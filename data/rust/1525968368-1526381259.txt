Thanks! Will check it out :\-\)
At those points, it doesn't look like there's much of anything to unwind. In my case, I've already allocated some mem by the time I would want to exit. init_logging, which probably does some allocation, returns before the exit is called, so it's already been unwound.
Right, but that's because I made it so. Check out what I made out of your code below.
That works and is basically all I cared about doing. I can call usage from main, thanks!
Oh absolutely, great advice! I'm sadly not very good at coming up with problems to solve, but I do think its a good approach, so if I think of any I'll get on that. I suppose I'm also a bit worried about doing things the *'wrong'* way \- or at least in non\-idiomatic ones \(and thus developing bad habits\) for whichever language I end up using, so I figure before I try jumping into anything too complex I should have some level of understanding first \- but then I don't want to learn a language for a project only to realise I've made a poor choice for what I want to do and I'll have to start over! :p
Seems you have (:
Ok, that was kinda funny (in the vast contrast between reality and fantasy). But, also depressing because completely misrepresenting facts like this seems to be a game lately - bloggers and some 'news' outlets seem to be testing the limits of what they can get away with. What I think I need is a better filter for my browser. Perhaps the ability to globally rank and personally filter: * by a person's name or handle(s) - possibly associated with a website For example, I may not want to uBlock forbes.com, but I would want to block certain writers for forbes.com. 
One of the gRPC crates is from the pingcap guys and I think they use it in production in some way on Tikv. Not 100% sure.
Calls to action: * Try running it on non-Linux platforms * Vote for TODOs and another features to be implemented next * State and file issues
Non-Markdown version of the links In this comment: **Link Text:** explicitly advised not to **Link URL:** https://github.com/rust-unofficial/patterns/blob/master/anti_patterns/deref.md ^(Preventing misleading links on reddit by providing the links behind the markdown. **Why?** u/reallinkbot/comments/8igale/why_do_i_exist/)
Info: it allows bridging [WebSocket](https://en.wikipedia.org/wiki/WebSocket)s and other things (started console programs, TCP/UDP connections, stdin/stdout) together. It supports both connecting to and listening WebSockets.
/u/carols10cents /u/steveklabnik1 Congrats on shipping the book to the web site! Major milestone!
Same boat 
Thank you! ❤️
Long live impl Trait! Now I can get back to creating that abstract factory :-)
[Rust Belt Rust](http://www.rust-belt-rust.com/) ([@rustbeltrust](https://twitter.com/rustbeltrust)) takes place in the [US Rust Belt](https://en.wikipedia.org/wiki/Rust_Belt): &gt; The Rust Belt begins in western New York and traverses west through Pennsylvania, West Virginia, Ohio, Indiana, and the Lower Peninsula of Michigan, ending in northern Illinois, eastern Iowa, and southeastern Wisconsin. It was in Pittsburgh in 2016 and Columbus in 2017. I think it's going to be in Ann Arbor in 2018, but I can't find where I saw that. I'll bet if I mention /u/carols10cents, she will correct or confirm that. 
Looks like an awesome release! Lots of "quality of life" improvements that have been long-awaited. Go team :-)
[std::panic::take_hook](https://doc.rust-lang.org/1.9.0/std/panic/fn.take_hook.html)
I really hope that it demysitifes generics for programmers who have only used dynamic languages before. It would be nice to have a label for the idea that connects with people who are less familiar with traits and type theory. Existential types is a bit scary. The best I can think of right now is Traitor, but I guess that's scary too!
Finally 128-bit integers. Now it should be possible to write high performance bignum libraries in pure rust.
[Unfortunately, Rust 1.25 has regressed on three architectures](https://buildd.debian.org/status/package.php?p=rustc&amp;suite=experimental) so that it [Debian unstable is still on Rust 1.24.](https://buildd.debian.org/status/package.php?p=rustc&amp;suite=unstable). I really wished Rust wouldn’t break so often with updates :(. It’s frustrating trying to keep up with fixing these things. 
Wow, this has got to be the most impressive Rust release since 1.0! Well done to everyone involved! :-)
I have mixed feelings about the dereference in a match, but everything else is just superb!
I am not sure I get the advantages of `impl trait` from the release notes. I think it would make more sense to compare it to generics rather than to trait objects.
s390x, PowerPC, and Sparc? Unless there's a dedicated investment by some company willing to devote employees or funding to these platforms, I don't see them ever moving out of tier-2. I wouldn't even know where to begin to even get access to such hardware.
Regarding fs::write, it would be nice to have a second example that writes a `&amp;str` to show off the power of `AsRef\&lt;\[u8\]\&gt;`: fs::write("bar.txt", "boom")?;
From what I remember futures is unstable atm and they are planning for some big changes at some point but also aim to keep the current API as stable as possible for about a year, probably less, before they make major breaking changes. For more details [see this anouncment](http://aturon.github.io/2018/02/27/futures-0-2-RC/). Naturally, the rest of the async stack is reliant on futures so nothing will be stable until that is. However, there is a lot of care taken with this whole stack so breaking changes generally involve a minor version bump and so locking to minor versions should provide you will some stability in the short term while things mature towards 1.0.
What specifically do you mean by "generics" here?
This: fn foo&lt;T: Trait&gt;(x: T) { 
[This behavior](https://play.rust-lang.org/?gist=ccadba488e9120ff29235cbdfdca799b&amp;version=beta&amp;mode=debug) doesn't seem correct to me. Count the number of digits in the padded outputs... it's 2 short.
Did you all report this upstream? I don't remember seeing it.
Indeed, the new book is quite the achievement. Hats off to both Carol and Steve!
You should file a bug!
I'd happily take a PR for that.
ok, [done.](https://github.com/rust-lang/rust/issues/50621)
I suspect it's including the '0x' notation? (don't know if that's a bug or not)
Just preordered the book! 
&gt;So, compared to that, the only difference is syntax. Nothing changes. Not quite true. You also can't call it as \`foo::\&lt;SomeType\&gt;\(...\)\` anymore.
That's a possibility, but it's not intuitive to me, at least.
You also can't use the same type for multiple parameters that way, or use it in more complicated bounds. Kind of wish `impl Trait` had just stuck to return types, or even that we'd been able to bypass it for module-level named-but-inferred types, which we need anyway. Would have been a simpler language that way. :(
Yes, I know. But it's basically analogous to embedding in go
&gt; I wouldn't even know where to begin to even get access to such hardware. Second hand shops. Recycling.
Horray! :tada: Already started to replace some uses of `Box&lt;Future&gt;` with `impl Future`. **Shameless plug** [combine](https://github.com/Marwes/combine) is now a lot more useable as (almost) all parsers can now be written as functions returning `impl Parser&lt;Input = I, Output = O&gt;` instead of using workarounds such as the `parser!` macro https://github.com/Marwes/combine/pull/156 !
[Done](https://github.com/rust-lang/rust/pull/50624)
Ah, I see. But why couldn't the same syntax be expanded to return types? I assume there must be good reason but I can't see why right now. fn foo&lt;T: Trait&gt;() -&gt; T {
For `impl Trait`, is it impossible to name the type it returns? Like what would a fully qualified `let` statement look like if it were coming from a `fn` that `-&gt; impl Trait`? If this isn't possible, then does that mean that you're out of luck if type inference doesn't work for some reason?
For main returning a `Result`, what error code does the process return? Do we have any control over this without reverting back to the `run` pattern?
The s390x is an architecture found only in IBM mainframes. Best as I can tell, the entry-level price is $75,000.
&gt; There's no case where you can get stuck; the caller does not ever get to choose the type, and so there's no way to annotate it to get a different type out. Oh, I see, so the syntax's only purpose is to hide the concrete type, and not necessarily something that would allow, e.g., letting the caller choose which concrete type to use. Good to hear that type inference cannot fail in this case. Thank you!
Yup! If you wanted the caller to choose, you'd use a type parameter, rather than impl Trait. Any time!
The full release notes mention that the `Cargo.lock` file is now included in crates. Does that mean that even libs you download as dependencies will always use the version the lib author used?
No, this would only affect the semantics of `cargo install`.
This doesn't really make sense: the gist talks about the inconsistency of adding a "module level" type that would be required to implement the alternative meaning of "impl Trait", but that's not how it would work. When talking about "universal" vs "existential" types, those phrases are being used in the context of a set of generic parameters. Let's take an example: impl&lt;T&gt; Foo&lt;T&gt; { fn bar&lt;U&gt;(p: impl Trait) -&gt; impl Trait; } In this case the "generic context" is `for&lt;T, U&gt;`, so when we say a type is existential or universal it is in that context. The return type is existential because for any given combination of `T` and `U`, there is a single return type. The argument is universal, because even for a single combination of `T`, and `U` (let's say I pick `T=u32, U=u32`, I can still pass in any type that implements the trait. The "mathematical law" from the gist does hold, but then you're changing the context: `impl Trait` in parameter position is universal precisely *because* it doesn't appear in the generic context. If you apply the law, then you've changed the context, because now it does appear there: impl&lt;T&gt; Foo&lt;T&gt; { fn bar&lt;U, V&gt;(p: V) -&gt; impl Trait; } Now p does have an existential type, because for any combination of `T`, `U` and `V` there is only one type that `p` can take (and that is `V`).
So it looks like Rust now has 3 ways to write parameters constraints? `fn func&lt;T: Trait&gt;(arg: T)` `fn foo(arg: impl Trait)` `fn foo&lt;T&gt;(arg: T) where T: Trait`
rls is a million times better than it was a few months ago. I used to be hyper critical of it but it’s ok now. 
How can we tell if a trait object is returned with `impl Trait`? In the first example: ``` fn foo() -&gt; Box&lt;Trait&gt; { // ... } fn foo() -&gt; impl Trait { // ... } ``` we see boxing. But in the second one we don't: ``` fn foo() -&gt; impl Trait { 5 } ``` I feel like this is could be hiding an allocation... or not.
`impl Trait` does no boxing for you. That said, in theory... trait Trait {} impl Trait for i32 {} impl Trait for Box&lt;i32&gt; {} fn foo() -&gt; impl Trait { Box::new(5) } But this doesn't hide allocations any more than `-&gt; Struct` hides an allocation if `Struct` contains a `Box` inside.
If you were to use a regular `for` loop with a `ParallelIterator`, you'd be losing all the parallelism. The standard way to iterate in parallel is to call `.for_each` or similar. Someone should correct me if I'm wrong, but I think if you have to get the result back into a standard `for` loop, you need to use `collect` to assemble the values into some kind of list first.
That's correct.
I would still consider it a bug, because you are adding the 0x. 
128\-bit integers are nice. But what about 128\-bit floats?
My comment was mainly hinting at PowerPC though. 
Having read through both, you and the others work on it did an absolutely fantastic job updating/improving it.
Abstract Data Type ;)
Curious that you're skeptical about achieving this in Node? So far, my Node projects have been the *only* ones where I've been able to share complex functions between the browser (front-end) and in the node.js back-end, and it's been glorious. Of course, you have to design for this, and adhering to a very functional style of programming really helps, but the seamless re-use between front- and back-end is a real, everyday occurrence with Node-based systems.
I'm not aware of any plans at this time. If someone wants to work on it, I'm sure it's a possibility! Long ago, we did have them. I think the major blockers have probably improved since then, though I'm not sure.
Looking at PR, the changes to rust/examples/array\_from\_builder.rs and rust/examples/array\_from\_vec.rs show that end user code will have to deal with ugliness caused by Any. There is no change to examples to show how end user code will become better after this.
A new release of hyper is coming in a couple days (it was ready earlier this week, but as we're now updating our use in Conduit, I figured I'd wait to see if the upgrade showed any rough edges in the API.) It will work with the new Tokio by default (and include a way to disable any runtime, if you need to use an older version, or something else entirely). If you want to build some serious async HTTP code right now, I'd suggest trying using hyper's master branch. It shouldn't see any sudden breaking changes, unless something horrible is noticed, which so far doesn't seem like it has been. It should be released to crates.io early next week.
An allocation could also be nested in the return type, even without `impl Trait`.
What should I read to understand the difference?
&gt; fn foo() -&gt; impl Iterator&lt;Item = i32&gt; 😍this great for functional programming. Before, we had three poor options to compose iter operations: 1. Write them all in one function 2. Unnecessary `Box` 3. Unnecessary `Vec` Now, Closures are copyable and we can return Iterators easily! I am so happy.
Definitely. It just seems to me that `Box&lt;Trait&gt;` was explicit about an allocation in the return. Now we should assume that `-&gt; impl Trait` still returns an allocated value but it may not. Unless I'm missing something.
[https://www.reddit.com/r/programming/comments/8igiwq/announcing\_rust\_126/dyrqwrt](https://www.reddit.com/r/programming/comments/8igiwq/announcing_rust_126/dyrqwrt)
If `foo` gets to pick, then it already knows what type it wants to return and could just put that type as the return type no? 
It *never* does any allocation (unless the thing it's hiding allocates explicitly, but that's no different than a struct).
I absolutely hate `impl Trait` in argument position. We already have a clean, perfectly good syntax for universally quantified generics. Using `impl Trait` for both just confuses its meaning.
I sincerely appreciate your efforts! :) My comment isn't trying to say that rarer architectures are unimportant, only that we don't have the resources to gate our build infrastructure on them. We obviously don't try to break them on purpose, but it's going to happen, and it's not always going to be obvious when it does.
It might be useful to communicate this (that impl trait returns an anonymous type) more clearly. I actually can't even find any documentation for impl trait. 
Yes, the docs don't exist yet. They will soonish. Working on it. This is a release post, not new docs :)
Reading it that way, I totally get it :)
In most cases, yes. But sometimes the type it picks is literally impossible to write down (e.g. a closure, which has an anonymous type that implements the `Fn` trait(s)), and sometimes the type it picks is just really really long (e.g. a chain of iterators).
But it is consistent with decorated LowerHex, for example `println!("8 {:#08x}", 7);` prints `8 0x000007`.
It only makes sense to distinguish existential vs universal quantifiers when dealing with prenex normal form - you're right that if you don't normalise the expression then the quantifiers flip based on the variance of the types, and so it's not a particularly useful distinction. Given how generics are implemented (via monomorphisation) and that this is not purely an implementation detail, programmers will tend not to think of "impl Trait" as existential. The conversion to prenex normal form happens as part of this compile step.
Anyone know how to enable this with Vim and https://github.com/autozimu/LanguageClient-neovim?
Sure, but I wanted to put this out there because I find the mental model of "always existential" to be *easier* because the "translation" is always as local as possible. Literally only the `impl Trait` form itself has to be translated; after that, its the usual normalization step, which we had beforehand anyway. In any case, monomorphisation has nothing to do with this. I can monomorphize a Σ type just as easily as a Π types, and that's really what's going on here. You can think of it as a variation on currying.
The latest one is easier to read, "this function takes an argument that implements `Trait`", much simpler.
This is a correct understanding, both of the distinction and of its application to Rust. The point of this article was to point out that universals and existentials are more interconvertible than some people seem to realize; you can always think of `impl Trait` as an existential (as locally as possible) and it will always work.
Why was 64 bit integers not enough for that?
Monomorphisation is important because the compiler cannot generate code for existential quantifiers. It can only generate code for the non-existential portion of the function signature. This is what drives the requirement to convert to prenex normal form in the first place, that way the compiler can "cheat" by just evaluating all of the variations of types that actually end up getting used, and plugging them in to generate code. Because the programmer is "aware" that these multiple versions are being generated, that impacts how they think about these quantifiers in a practical sense.
Can someone pitch to me why we need `await!(foo)` syntax and cannot just do `foo.await()?`?
The very last 32-bit PowerPC Mac was released in May 2005, and only a handful were released after April 2004. `ppc64` still seems to be building just fine. I honestly don't believe a computer from 2004 is worth recycling with Linux, since the hard drives are surely on the verge of failing, and replacements for those hard drives are not forthcoming. Do you want to give someone a computer that will fail weeks after they get it? The hard drive that has been sitting idle for a decade might work fine for a few hours while you set things up, but I struggle to imagine that it'll continue working fine for any reasonable period of time.
I'm trying to argue that normalization is an implementation detail at best. Here's another way to put this: `impl Trait` is *always* sugar for (type T: Trait, T); that is, a pair of a type (erased at runtime) and a value of that type. The compiler can monomorphize this just fine regardless of its position: if it's in an positive position, just copy code to whatever versions of it are needed, and if its in a negative position, just typecheck with an unsolved metavariable and check that by the time typechecking ends that metavariable has been unified with something contextually concrete (and then generate code for that thing).
This is a convenience API that isn’t meant to cover every case. `std::fs::OpenOptions` is still there if you need it.
Good to know!
&lt;3
Alright, but why do we need a keyword then? What does it do that a method cannot? 
I think there was an option 4. Write more boilerplate code. I think `a::foo` below is equivalent to `fn foo() -&gt; impl Iterator&lt;Iter = i32&gt;`, though writing it is less ergonomic. I still find myself using this pattern for things that impl Trait does not handle, for example impl Trait can handle returning `impl Into&lt;i32&gt;`, but not returning `T where i32: From&lt;T&gt;`. mod a { mod hide { pub struct ImplIteratorI32(i32); impl Iterator for ImplIteratorI32 { // ... } pub fn foo(i: i32) -&gt; ImplIteratorI32 { ImplIteratorI32(i) } } pub use self::hide::foo; } 
i havent played since early access. is rust better now? like more things to do? less hackers? new maps?
This is what I call a major release! Congratulations. Question to /u/steveklabnik1, now that the book is being finalized, would it continue to be maintained? For example I've noticed that the book probably hasn't demonstrated the usage of `impl Trait` yet. If the book is continually updated, would it be really pragmatic to release the book in the form of physical books?
&gt; the hard drives are surely on the verge of failing, and replacements for those hard drives are not forthcoming I bought an iBook G4 and replaced the hard drive (which was working okay) with a CompactFlash adapter. Also dremel'd a hole for swapping the card without disassembly :D There are also adapters for SD cards, (m)SATA drives, etc. The problem is performance, G4s are slow as heck. If you want a fast PowerPC, you need a PowerMac G5. Preferably a Quad. They're big, noisy, power hungry, and Quads are rare.
es and no. Think of it like release trains; the second edition has left the station, and so isn't being updated directly. It's actually pinned to 1.21; it left the station a while back. Work on the "2018 edition", which is the next version after "second edition", is just starting. It will be getting updates as new stuff lands, though there may be a bit of lag. In general, docs are going to be a bit weird up to the Rust 2018 release; it's all coming together, but slower at first, faster at the end. (This means that, as of right this moment, there aren't great docs for impl Trait. I'm working on them right now.) &gt; would it be really pragmatic to release the book in the form of physical books? Backwards compatibility means that everything you read in the printed book works and is still useful, perpetually, into the future.
Do all compile to the same, or are there subtle differences?
that's /r/playrust not /r/rust
What about 16\-bit floats?
I still have a G4 running debian (mostly for hardware I/O related projects and for ham/amateur radio stuff). I've not used it for anything rust related yet but I definitely had a few ideas! Most probably I'll soon get something low powered to replace it but for anyone in a similar position... It doesn't look great.
At its simplest, \`foo.await\(\)\` would have a signature like \`fn await\(...\) \-\&gt; T\`. But, of course, this would require blocking on a future, which defeats the entire purpose of futures. \`await!\(\)\` the macro coordinates with the \`async\` keyword to completely rewrite the function body to only appear to be blocking.
Those numbers are so huge that it breaks the webpage on mobile!
This is not /r/playrust.
Same deal.
They compile to the same thing.
https://github.com/rust-lang/rfcs/blob/master/text/2394-async_await.md#the-expansion-of-await My understanding is that it can't be expressed as a function, because it has a control flow in it, `yield`. This is similar to the `try!(...)` macro that needed to be a macro because it contained `return`. But apparently it's worse than that. The `yield` in an async function is not something that can actually be expressed by the language yet anyway, so it can't even be expressed as a macro. `await` needs to be a compiler built-in. So that's what it's purposed to be, even though it looks like a macro.
YEEEEAAAAHHHHHHHH BOOOOOOOIIIIIII
Is there a timeline on fully switching over to the http crate for request/response/header etc types?
That ship has sailed, hasn't it? No point rehashing it. I'm not sure I like it myself, but now that it's in stable it's here to stay.
PowerPC was used by Nintendo for its Wii U console until 2017. PlayStation 3 also used PowerPC and was produced until 2017. It is also still quite popular in Aerospace/Defense industries. And this is not mentioning IBM's servers, BlueGene supercomputers, and OpenPower initiative. 
The new version has changed over :)
Such a shame! Thanks for explaining!
You have made my day. Awesome!
Maybe I'm being obtuse but I'm not sure I understand why these are different in the inclusive range example: `0..256` and `0..=255` Shouldn't they both have a range `(1,2,3....254,255)` ?
Yep, I quite like these wrappers for common use cases as builders are bit inconvenient IMO.
Definitely. Futures and Iterators will be so much better with impl trait. However, I think that would be a breaking change for people that use the concrete return type. We also cant bind existential types to thing with the type keyword.
And ppc64 Rust is compiling just fine, as I pointed out, but yeah.
Yasss! 
Yeah, it's long since sailed. I just wanted to complain one last time before sucking it up :P
&gt; We also cant bind existential types to thing with the type keyword. Hopefully in some future release, we could do something like this: trait Foo { /* ... */ } trait Bar { type Output: Foo; fn enfoo(&amp;self) -&gt; Self::Output; } struct Baz; impl Bar for Baz { type Output = impl Foo; // redundant much? fn enfoo(&amp;self) -&gt; Self::Output { // return some existential type that implements Foo } } That way we'll have a way to refer to that existential type - `Baz::Output`. 
Who said anything about rewriting unmarked methods? I'm talking about removing `await`, not removing `async`.
I still think we shouldn't allow `impl Trait` for arg types because it doesn't add any expressiveness, just adds a redundant (non-orthogonal) and less powerful way to express universally quantified types.
From my point, rust became more complicated to use, 
If your question is "how do I specify what exit code to use after a panic", I think this version is the best: fn main() { if let Err(e) = panic::catch_unwind(|| { run_program(); }) { process::exit( /* something determined from e */ ) } } ...here we make sure `process::exit` are called after all `Drop`s are run. In case `run_program()` starts additional threads, make sure you wait for them to join.
There's an RFC for that: https://github.com/rust-lang/rfcs/pull/2071 It even extends to non-trait methods so you can give a name to any type you would use with `impl Trait` today.
This is a pointless distinction to make. In the "universal" case, you can always freely move the quantifier between the "existential" position and the "universal" position. Calling it "universal" is useful because it gives you a way to distinguish it from the case where you *can't* move the quantifier. So think of it as "existential" if you want, but there's no need to go telling people the "universal" interpretation is "fundamentally incorrect."
&gt; that would be a breaking change for people that use the concrete return type and that's a sad thing, because that means std can't be changed to use `-&gt; impl Iterator` for `map`, `filter`, etc.
256 is zero, thanks to wraparound. So you’re ranging from zero to zero. Think of it this way: ranges are a struct, like any other. To construct one, you pass in start and end sizes. To iterate over u8s, you pass in two u8s. 0u8 is zero. 256u8 is also zero. So you pass in two zeroes and your range is now from zero to zero, aka empty.
Good question...I'm not sure. I see that the current implementation spins up a bunch of separate threads, though, and the async delivery works. \(I've tried it from a Python client.\) It's just that each client puts all the commands in the same queue so it processes each individual client sequentially.
Okay that makes sense now. Thanks for explaining it, my confusion was why it would ever get to 256 in the first place for the exclusive range, but your explanation makes sense. Thanks
You’re welcome!
Curious what you mean by the Javascript way. Are you referring to async stuff? Javascript and Rust seem like very different beasts. I do hope networking/web stuff progresses on for Rust though. Less worry about overflows on open ports. 
Yeah, but other people can use impl in their libraries on top of std. One day when we have a rust epoch we can break it.
You might want to check out GObject and [gnome-class](https://gitlab.gnome.org/federico/gnome-class/).
Figuring out this issue when I got to it gave me a much better understanding of what I had passed around, given Rust was the first language I'd used with references.
I added that to my vscode user settings, no change. Anything I should try?
actix-web has an unmentioned [async web client](https://github.com/actix/actix-web/blob/master/src/client/mod.rs). I've used it briefly and found it pretty useful though I haven't done much with it yet. Getting it to use ssl required: [dependencies] actix-web = {version="0.6", features=["tls", "alpn"]} otherwise it just hung trying to connect to an https url. 
so you dont play it here? you just talk about updates? wat?
`Map`, `Filter`, `Reduce` and friends conditionally implement some traits based on the type they're parameterized over. You can't do that with `impl Trait`.
I don't write much Rust and I haven't seen the earlier discussions, but at first glance it seems fine to me because it reminds me of function subtyping, where the argument type is implicitly contravariant and the return type is implicitly covariant.
Any word on how long the Early Access for the print book will last? 
Thanks to whoever finally took an interest in slice patterns. I think I can ditch advanced... then I can have all of my programs on stable.
What is current state for named argument vs anonymous struct? It seems they're competing proposals and I'd like to check them both. Where can I find the latest proposals?
I’m not sure, you should talk to No Starch.
Will do! Looking forward to reading it. Congrats! 
At least on Windows an error returns 1, which is a sensible default. But it will be nicer when it's able to be specified by the application.
&gt; Using `impl Trait` for both just confuses its meaning. Does it though? Comparing it to Java, you can take an interface as a function argument or return it as a return value, and in both cases it means exactly the same as it does in Rust. Taking an `IFoo` as an argument means that the function accepts *any* type that implements `IFoo`, an returning an `IFoo` means that the function returns *some* type that implements `IFoo`. Replace "`IFoo`" with "`impl TraitFoo`" and it's exactly the same in Rust, just statically checked instead of dynamically.
You also need inline assembly to get at the carry bit register. Is that in stable yet?
How long does it take to compile rustc? :P
We get this unfortunate confusion regularly. The programming language Rust isn't related to the game in any way, they just happen to share the name. This thread is about todays update to the programming language making development easier. Like others already mentioned, you probably want r/playrust.
whats a programming language?
i was stunned the other day that i could fix warnings in vs code with mouse clicks. nice!
By coincidence, a colleague introduced me to `cargo outdated` today. Which, while not providing the actual build times of crates, does help with sorting out and optimizing the dependencies of a project.
The patch notes say otherwise, but I feel this is wrong. In both cases, `impl T` is existential. But this is isomorphic to the generic syntax. It's the difference between `forall a. Eq a =&gt; a -&gt; B` and `(exists a. Eq a) =&gt; B`. So while yes there are two ways of expressing things, this is no worse than the situation in logic.
It's not really the same. Java's version is much more like `dyn Trait`. For example: Java lets you do the following Serializable f(boolean b) { if (b) { return new Integer(1); } else { return new Boolean(b); } } whereas `impl Trait` does not fn f(b: bool) -&gt; impl std::fmt::Display { if b { 0 } else { true } } error[E0308]: if and else have incompatible types --&gt; test.rs:2:5 | 2 | / if b { 0 } 3 | | else { true } | |_________________^ expected integral variable, found bool | = note: expected type `{integer}` found type `bool`
Don't forget #5: It's new syntax, so if you start using it, people on older compilers, or who are trying to maintain back-compat guarantees for older compilers, can't keep using your crate. If you're going to add it to an existing crate, make sure you bump the major version. If you don't *need* to use it, consider not using it. (When `?` was introduced, I had a few libraries I couldn't use any more on older projects because they *immediately* jumped on it, back-compat be damned.)
If you're going to give a statement "for the type theorists" you should make sure your statement is correct. Existential types have a very definite meaning, and the patch notes made an incorrect statement, regardless of whether or not it affects any programs you'd write. The typical programmer won't know the difference between those names anyway. So don't dismiss this as nitpicking. For the record, the difference is illustrated cleanly in this Haskell- (or Idris-) like syntax: This is universal. The variable `a` is in scope for the entire type: `forall a. Eq a =&gt; a -&gt; B` This is existential. The variable is limited in scope and only exists inside the parens: `(exists a. Eq a) =&gt; B`. They achieve the same goal. They are isomorphic as types. They are, however, not the same thing. And anyone who is interested in learning type theory coming from Rust should hopefully not have to reconcile what that statement means, since it is simply incorrect. 
[removed]
[removed]
[removed]
Whether requiring a new compiler version constitutes a semver breaking change or not is [currently a controversial question](https://github.com/rust-lang-nursery/api-guidelines/issues/123) so I left it out above. But I definitely agree with your point. I don't think this question can be fixed by policy alone, as if there is one single violator in your crate graph, `cargo update` breaks. This needs technical enforcement.
Can’t 3 be solved by saying fn foo() -&gt; impl MyTrait + Debug + whatever else ??
This would assume that `Debug` is always implemented. Sometimes that's no problem to assume, but in the example I gave `#[derive(Debug, PartialEq, Eq)] Foo&lt;T&gt;(T);` , `Debug` and the other traits are only implemented if `T` implements it. With ` fn foo&lt;T&gt;(param: T) -&gt; Foo&lt;T&gt;` there is no requirement, you can use the code whether `T` implements `Debug` or `Eq` or none of the traits. With ` fn foo(param: impl Debug + Eq + [...]) -&gt; impl MyTrait + Debug + [...]`, you'll have to decide whether add those types as required bounds or whether to disallow code to assume that those traits are implemented on the return type. This is the decrease in expressiveness that my point 3 was about.
I migrated my blog to GitHub pages now that they support SSL with custom domains .... but it hasn't gone well. The non SSL version is currently working. http://andygrove.io/2018/05/apache-arrow-traits-generics/ This weekend I plan on putting together an example that builds a complete RecordBatch and does some processing, to demonstrate how this all fits together.
Why is turbofish syntax banned with impl trait? Was thia an intentional design decision, or is it simply not implemented yet?
You are correct. It's a downgrade in the sense that it has reflection, which means it loses parametricity (“free theorems”).
Higher-kinded, no; higher-ranked, yes, but you have to write your quantifier in a record or first-class module, so it can be a little awkward.
Higher-kinded, no; higher-ranked, yes, but you have to write your quantifier in a record or first-class module, so it can be a little awkward.
If it's not global, I wouldn't really call it HM. The whole point of HM was that *let* could be generalized.
Is there a way to turn on a feature of a dependency based on a particular feature being turned on for my crate? Is there a way to turn on a feature based on the rustc version?
&gt; https://andygrove.io/2018/05/apache-arrow-traits-generics/ It seems like the main problem stated in this article could be solved using the `num` crate and traits for generic arithmetic.
&gt; It's the difference between forall a. Eq a =&gt; a -&gt; B and (exists a. Eq a) =&gt; B. This is exactly universal vs. existential qualification.
You should ask in /r/playrust
Doesn't a Github search using `language:rust stars:&gt;0` do the same thing, apart from that it shows all of them, not just the top 1000
&gt; There's not really a way to do that, so that's why we're not. Donating to BountySource top rust project bounties would be interesting :) Or supporting some devs/maintainers of popular Rust projects, I know some that that could use the extra development resources or financial support(dev behind nalgebra/ncollide/nphysics setup a patreon and is doing part-time work instead of full-time work now so that they can commit more time to work on those Rust crates despite the financial impact in doing so).
The game Rust is made with a programming language, it tells the computer how to do all the game stuff and render it for you on the screen etc. The game Rust isn't made with the Rust programming language though, they just share the same name, like uhh.. a person with the name Mark and the saying "X marks the spot".
Is `fn foo&lt;T: Foo&gt;(v: T)` exactly equivalent to `fn foo(v: impl Foo)`? And is there a reason to prefer one form to the other in _new code_?
There was an RFC not long ago to add support for a widening add/mul, i.e. add(u64, u64) -&gt; (u64, u64).
Except for the turbofish question those two syntaxes are equivalent, to my knowledge.
There's no need at all, it just saves characters. 
&gt; impl&lt;T&gt; Foo&lt;T&gt; { &gt; fn bar&lt;U&gt;(p: impl Trait) -&gt; impl Trait; &gt; } &gt; &gt; is *exactly the same thing* as &gt; &gt; impl&lt;T&gt; Foo&lt;T&gt; { &gt; fn bar&lt;U&gt;(p: ∃ &lt;V : Trait&gt; V) -&gt; ∃ &lt;W: Trait&gt; W; &gt; } Is it really? Take for example this signature: fn mwahaha(p: bool, x: impl Debug, y: impl Debug) -&gt; impl Debug; If we substitute each occurrence of `impl Debug` with an existential like you do, this would give us: fn mwahaha(p: bool, x: ∃&lt;A: Debug&gt;, y: ∃&lt;B: Debug&gt;) -&gt; ∃&lt;C: Debug&gt;; And you'd think that you'd be able to write this function: fn mwahaha(p: bool, x: impl Debug, y: impl Debug) -&gt; impl Debug { if p { x } else { y } } [But `mwahaha` doesn't compile.](https://play.rust-lang.org/?gist=ce13758e2e3c6a0b4d5452b2db005f9f&amp;version=stable&amp;mode=debug) While this function does, and you can even use it: fn whee(x: impl Debug) -&gt; impl Debug { x } To me this suggests that your attempts to render the quantifiers explicitly in pseudocode is getting their scopes wrong. My `whee`, you'd say, has this type: (∃A:Debug. A) → (∃B:Debug. B) But in reality its type is this: ∀A:Debug. ∃B:Debug. A → B ...where the quantifiers must scope over the function arrow, and the universal over the existential (because `B` can be instantiated to different types at different call sites that choose different instantiations for `A`, as the playground link demonstrates). And then the reason my `mwahaha` above doesn't compile is because its type is this: ∀A:Debug. ∀B:Debug. ∃C:Debug. bool → A → B → C ...but then the `if` expression doesn't type check because `A`, `B` and `C` don't unify.
See https://github.com/rust-lang/rfcs/blob/master/text/2005-match-ergonomics.md.
I didn’t have a time for that yet. I was busy with other upstream projects. But I will get around doing that. I will cross-compile new binaries for the affected architectures over the weekend and see if that helps.
The patches for SPARC come from Oracle with some fixes from me. PowerPC and IBM zSeries are maintained by IBM. If Rust wants to succeed as a low-level language, it has to improve for at least some of those targets.
Debian blocks transitions only for the release architectures. Those are the upper ones in the build overview. Allowing a compiler to pass transition for a next Debian release with the testsuite failing without at least looking at what’s the problem, isn’t exactly the best idea. Also, if you work for RedHat, you should know that both POWER and IBM zSeries are supported targets in RHEL, so I don’t think RedHat is going to ship anything untested.
This part of the announcement confused me: &gt; It’s important to note that sometimes trait objects are still what you need. You can only use `impl Trait` if your function returns a single type; if you want to return multiple, you need dynamic dispatch. Because, for example, is `whee` in this code returning a single type or multiple types? use std::fmt::Debug; fn main() { println!("whee: {:?}, {:?}", whee(5u32), whee(6u64)); } fn whee(x: impl Debug) -&gt; impl Debug { x } You might say it returns "multiple types" because one of its calls returns `u32` and the other returns `u64` (or stated more generally, because `whee`'s body's type is polymorphic). But no, [the snippet works just fine](https://play.rust-lang.org/?gist=b45cfc1e513985b7a2c9c1538fb248f4&amp;version=stable&amp;mode=debug). (However, [this more complex example](https://play.rust-lang.org/?gist=ce13758e2e3c6a0b4d5452b2db005f9f&amp;version=stable&amp;mode=debug) understandably doesn't.) Basically, it sounds like: 1. The return type existential quantifiers scope over the function arrow; 2. The argument type universal quantifiers scope over the existential quantifiers. 
I’m one of the porters who takes care of Rust in Debian. And while I understand that breakage can happen, I think it happens very often on Rust as compared to other toolchains like gcc or OpenJDK where I am also contributing.
+100
I'm happy to be enlightened, but your argument is just hand waving. I'm pretty familiar with the market. Intel, ARM, and the new PowerPC stuff are the ones that matter most. There are some mainframes in use, both old and modern. MIPS occasionally gets mentioned, but less and less often. RISC-V will be a thing someday. AVR, MSP430, and other embedded targets are being worked on, and hopefully they will be better supported soon. Am I missing anything that actually matters? No one is using alpha or ppc32 or Itanium. If 5 people decide to start using some platform no one has ever heard of that C doesn't support, is that suddenly C's problem? No. That's their problem, not C's. Rust can't support everything ever made. There's no one volunteering to maintain those ports, and there's no real benefit. Idk.
I think it should be noted that while panics can be used for cleaning up files and do other things, it's not something to rely upon as an unhandled signal or power loss may occur at any point of time with no unwinding as a result.
I don't know why anyone would change from FooStruct to impl Trait (r 1)? If that's the only possible return type, that should be the signature. And if you suddenly start returning other types, it can't be FooStruct anymore anyway.
https://github.com/rust-lang/rfcs/pull/2417
Mate, I'm just giving them options on where to buy a cheap PowerPC based computer on which to code on. You can then target routers, clusters, or whatever else for all I care.
Yes. `fn foo(v: impl Foo)` and `fn foo&lt;T: Foo&gt;(v: T)` denote generics (what C++ would call function templates, monomorphization) while `fn foo(v: dyn Foo)` and `fn foo(v: Foo)` denote trait objects (vtables, dynamic dispatch).
Good to know, thanks.
With `u128`, can we now add a method on `Duration` that returns the number of milliseconds? Currently you have to do something like this: (d.as_secs() * 1_000) + (d.subsec_nanos() / 1_000_000) as u64
For `impl Trait`, I've always been most excited about the error messages for iterators, futures and more generally types that generically build upon other types. It is demotivating and hard to decipher an error message when the typenames fill most of your terminal. I'm looking forward to having _much_ more precise errors where they point directly to the problem.
I believe they're used in some GPU programming for performance. E.g. machine learning or graphics work.
Ok that seems like it would work, too. But if your `run_program` simply returns a `Result`, everything would've been `Drop`ed, too, right? So the panic doesn't really give something additional here, does it?
I think for the particular case of error messages we could come up with `#[dont_mention_in_errors]` attributes that turn off mentioning of the type in error messages (similar to how the `#[rustc_on_unimplemented]` attribute works which also only exists to make errors nicer). That'd need no `impl Trait`.
 &gt; Is the use of trait objects a good thing or a bad thing? Trait objects are just a tool. Tools aren't good or bad, its how you use them that matters. Trait objects (and virtual dispatch) buy you infinite future extensibility, which is something that makes sense to want for a GUI, but they aren't the only way to achieve that. &gt; I'm open to suggestions about how to best describe my prototype; if it's misleading to characterize it as ECS I want to move away from that [...] I'm interested in exploring this question more deeply. I think that you don't really sound to be too convinced about what is an ECS and what isn't. IMO if your experiment is tiny, it might be worth it to just try to go full-ECS (using something like `specs`), and then with both experiments on the table, choose what you like best and move on forward knowing that you explored all possibilities.
This is pretty much [Failure](https://github.com/rust-lang-nursery/failure). I'm just downloading 1.26 to see if `use failure::Error;` and `fn main() -&gt; Result&lt;(), Error&gt;` works.
Oh man this is great! I have had a case in the past where the prospect of matching `&amp; EnumWithWayTooManyComplexCases` actually affected the way I designed my code, and in the end it turned out to be painful in lots of other places because of the changes I made. Good riddance.
aaaaaaaand rust has broken clippy the very next day. But we can still use the `nightly-2018-05-09` toolchain to try out this functionality in rls.
&gt; Somewhat less obvious: changing `fn foo&lt;T: Trait&gt;(v: &amp;T) {}` to `fn foo(v: impl Trait) {}` is a breaking change as well because of turbofish syntax. A user might do `foo::&lt;u32&gt;(42);`, which is illegal with `impl Trait`. I propose calling such syntax a lead sugar. At first it nice and sweaty, but then you realize it's actually poisonous.
If you think the interface between caller and callee symmetrically, existential and universal are basically mirror images of each others. When passing a value to a function, the "receiver" promises to take any type (within the limits of the trait.) From the viewpoint of the "sender", this the receiver's type is universal. From the viewpoint of the receiver, sender has passed "some" type, an existential. When returning values, the roles just switch. The caller becomes the receiver and the callee becomes the sender. So it makes sense if you think the polarity of existential/universal distinction with regards to the direction of the data flow. An indeed, it doesn't make sense if you think it with regards to the call stack, because that's asymmetric.
Ok now it seams less like a bug and more like a inuntuitive feature.
Thanks for this summary! &gt; There is an exception of of this rule only in two instances: [...] and specialization Can you elaborate on how specialization is an exception? &gt; [...] code will always need to provide a codepath if a given derive is not present [...], hurting ergonomics and the strong compile time guarantee [...] Could anybody elaborate on this as well? Also: I remember reading a previous discussion here on Reddit on how `impl Trait` returned values kind of unergonomic when trying to store them in your own struct. \[*] Do you feel that's not a concern compared to your other points? \[*]: The rationale was (I think) that you're forced to use generics (until we get existentials (?)) and you leak implementation details.
It's not batteries and you should not fear to use cargo.
Imagine you have a function like this: fn foo(v :impl Sized) -&gt; impl Sized { #[derive(Debug)] struct T&lt;I&gt;(I); T(v) } A naive `println!("{:?}", foo(42));` would fail and complain that the return type does not implement `Debug` (even though we know it does because we know the implementation of foo). But with specialization you could do: trait MaybeDebug { fn maybe_debug(&amp;self); } impl&lt;T&gt; MaybeDebug for T { default fn maybe_debug(&amp;self) { println!("Can't debug, sorry :("); } } impl&lt;T: Debug&gt; MaybeDebug for T { fn maybe_debug(&amp;self) { println!("{:?}", self); } } This would allow you to use the Debug impl for all input params that impl Debug. See [this](http://play.rust-lang.org/?gist=733e7133e621413fd09de488e37fb78e&amp;version=nightly&amp;mode=debug) full code example. This example works because all trait impls of an `impl Trait` type leak with regards to specialization. Specialization "sees" when the inner type implements Debug and when it doesn't. This exception got added to allow for iterator specific optimizations to be performed. In this example, the codepath where the given trait is not implemented is in the `default fn maybe_debug(&amp;self) { [...] }`. You always need to provide a default impl even if you *know* that under certain conditions a trait is implemented for the return type. And for `NoDebug` you'd only get a feedback at runtime that the debug trait is not implemented. In this example this might be trivial, but it certainly makes refactors harder if you change your code to remove a trait impl and some places don't error. For reference: an impl Trait free version would look like [this](http://play.rust-lang.org/?gist=b178b84a1f5ab493a2308404ba3dc729&amp;version=nightly&amp;mode=debug). Here you get a compile error for the `NoDebug` struct which is better for refactoring. Of course, if a runtime error is what you want to have instead, use of specialization remains a choice.
I get why one would want to have both the first and the third form (the first form is more concise, the third one can get split over multiple lines), but I don't get the universal `impl Trait`. Why would one choose to use that?
`std::arch` isn't stable yet \(only beta\), and isn't portable, unlike `u128`.
We don't need `u128` for this. A `u64` can already hold 584942417.355 years in milliseconds \(if my math is correct\).
`d.as_nanos()` seems more natural?
Sure but the `std::arch` implementation of `mulx` can be done in portable rust just fine by using `i128` today. IIRC that's exactly what the `bitintr` crate did.
You are correct that pingcap is using it. They are wrapping the C library. It requires the go compiler for some of the generation step which is kind of annoying when you want to make a slim docker image.
But `where` was necessary to express things like `&lt;'a&gt;(..) where 'b: 'a`, but `impl Trait` for args wasn't necessary. Btw, how would the syntax look like for specifying `impl Trait` types within turbofish? This feels like a rabbithole that's too deep, being able to specify them within turbofish is kinda necessary now that `impl Trait` for args was allowed, but it just introduces more non-orthogonality..
That's not a big price for making `impl Trait` in argument position a sugar fully interchangeable with `T: Trait`. It's not like there are going to be dozens of `impl Traits` in a single signature, 1-2 at most (in sane code). I don't know why lang team hesitates allowing specifying type arguments for parameters introduced with `impl Trait`. To me it looks so obviously right that the whole situation kinda baffles me. C++ [allows this](https://github.com/rust-lang/rfcs/pull/2176#issuecomment-370228412) for `auto` and concepts in argument positions and I don't remember this choice ever being questioned or complained about.
Oh ya right. I completely forgot that it required go also to work. I was adapting a small calculator example for gRPC and encountered it then.
&gt;Horray! :tada: Already started to replace some uses of `Box&lt;Future&gt;` with `impl Future`. Don't forget to bump the major version in your crate then ;) 
I have an annoying problem with borrowing: fn foo(obj: &amp;mut Obj, x: usize, y: usize, z: usize) -&gt; usize { .. } fn bar(obj: &amp;Obj, x: usize) -&gt; usize { .. } I want to call `foo(obj, bar(obj, 10), bar(obj, 100), bar(obj, 1000));` But it raises error: `cannot borrow as immutable because it is also borrowed as mutable` So I have to call it like this instead: let x = .. let y = .. let z = .. foo(obj, x, y, z); Isn't there a better way? Doesn't it make sense for rust (because it is strict) to evaluate the arguments before calling function foo?
There is no changelog for rustfmt, actually. 
Not batteries? What does that mean? That t’s not as „complete“ as other standard libraries? (I.e. Python?)
Yes, but getting the first argument only means you put it on stack, so no mutation can happen to it, before all the other arguments also get evaluated.
Yep! Python is often described as "batteries included" meaning it's got a huge ton of stuff (xml, json, sql, string templating, compression, logging, curses, email, testing, gui, etc.). But a lot of those packages are not as friendly or well maintained as the external alternatives (the other thing that's often being said about Python's standard library is that it's where projects go to die). So Rust's standard library is much smaller, and people are encouraged to use Cargo for any extra dependencies they need.
And [here is why.](https://www.reddit.com/r/rust/comments/8ik620/notes_on_impl_trait/?st=jh1tfyf3&amp;sh=522db336)
Right. Normal `Result` handling is considered better practice when applicable. As a side note, I [just read about](https://blog.rust-lang.org/2018/05/10/Rust-1.26.html) a new feature in Rust 1.26 that might help - with being able to return a Result directly from `main` - returning an `Err` will then give an [FAILURE](https://doc.rust-lang.org/std/process/struct.ExitCode.html#associatedconstant.FAILURE) exit code. 
 &gt;My motivation is to reduce the executable size. You know, I almost fried when I saw this: ``` $ ls -lah a.out -rwxr-xr-x 1 woot woot 30K May 5 11:26 a.out ``` 30 KB of an unstripped executable. I wish Rust could produce such a tiny binary. source: https://arvid.io/2018/05/05/pdbs-on-linux/ 
Yeah. (I fixed the typo.) IMHO, the "goal" of Rust's standard library is to provide basic building blocks and common abstractions. For example: It doesn't have a module to read gzip-compressed files, but it does give you a `Read` trait external libraries can provide a common interface for reading and decompressing files.
Is there a case where `fn foo&lt;T: Trait&gt;(v: T) {}` is necessary and you can't use `fn foo(v: impl Trait) {}`, because the caller might not be able to specify the type without using the turbofish? I can't think of any but I can't say it can't exist either.
To determine how important this is, what are the main reasons people are on older compilers, and how many are there compared to the number that are on latest stable (or have no problem upgrading if a crate requires it to)?
I have a very simple function similar to `fn foo(a: usize, b:usize) {a &gt;&gt; 8 &amp; b | 15 }` Now I was thinking about using the `#[inline]` pragma. The problem is, in some places it is called multiple times with the same argument, so `let` would be perhaps more appropriate? * Can rust optimize multiple calls to function with same immutable arguments into a `let`, so that the function is not called twice? * Is it even worth it for such simple functions to do ^? * Will rust automatically inline those functions, if they are so simple? * Is there some rule of thumb, when to use the `#[inline]`? Does it fit my case?
But that's the thing: that's not how the compiler works. It re-borrows `obj` to create the `&amp;mut Obj` for the first argument. There is now a mutable borrow in existence. One of the explicit goals of the language is to ensure it is impossible to have a mutable borrow to something at the same time as immutable borrows, and that is exactly what happens next. It tries to take an immutable re-borrow of `obj` whilst a mutable borrow exists. It won't allow that. There are efforts to make the compiler more flexible about this, but like I said, that's not how it works right now. From what I remember, they revolve around trying to define a safe set of rules to let the compiler "juggle" borrows. More generally, there are *always* going to be programs that are (strictly-speaking) valid, but which the compiler rejects because it just doesn't know how to *prove* them to be valid.
FWIW main() doesn't even care that you return an error, it just wants a `Result&lt;(), T: Debug&gt;` or `Result&lt;!, T: Debug&gt;` (more generally it wants a value which implements [std::process::Termination](https://doc.rust-lang.org/std/process/trait.Termination.html) and that's only the case for the two results above).
And #6: Returning a newtype is more ergonomic for your users, because it can be put in a struct naturally, like this: struct Bar { foo: FooStruct } If the function instead returned `impl Trait`, your user now have two bad options: struct Bar&lt;T: Trait&gt; { foo: T } ...which means the user's code will now be cluttered with `T: Trait` every here and there, or: struct Bar { foo: Box&lt;Trait&gt; } ...and we're back to dynamic dispatch (which we wanted to avoid in the first place).
`impl Trait` is easy to spot. I predict the more interesting cases are going to be the new `match` semantics. That is, if you don't have CI setup to test on a specific Rust version, it seems exceptionally easy to accidentally bump your minimum Rust version that way. Folks have gotten better about adding a specific stable version of Rust to their CI so that it's at least a somewhat conscious act to increase it. But there are still many crates that just test on `stable`, `beta` and `nightly`. I expect something similar once nll lands too. I try to just encourage folks to pin a specific Rust version in their CI, so that it at least makes everything discoverable, without advocating any specific policy with respect to semver.
My project is quite small: https://github.com/abhijat/mosler What OS do you run? I have seen the slowness most of the time on windows. Also are you using the default settings for the plugin or did you tweak some of them? Thanks in advance!
Yeah, this is what I was wondering about. I can't imagine ever putting an `impl Trait` return type into a public API for this reason alone.
What is the best way to create newtype? I currently use [derive_more](https://crates.io/crates/derive_more) but there are some usecases that either require ugly `&amp;*` or some manual implementation, e.g. for struct Newtype(String); when function requires `&amp;str` or when I have `&amp;str` and want to compare with my `Newtype` but I think there are more cases to list
The seconds part of a Duration is already u64, so with milliseconds it can overflow (although not really an issue if you just measure timing).
Awesome!
Is there some RFC to make this work?
I'm super happy about `fn function () -&gt; impl Trat` and the fact that incremental compilation is now in stable!
I don't recall any one specifically, and a quick search didn't turn up anything likely. Anyway, until it actually lands and is stable, any proposed changes are just that: it doesn't really help. :P
I'm not aware of such a case. Sometimes you must specify a type because otherwise rustc can't infer it (I have encountered such cases but can't remember an example). Then you can simply do `foo({let v: TYPE = expr; v})` instead of `foo::&lt;TYPE&gt;(expr)` or use type ascription when it becomes stable.
&gt; As for numbers, I don't know, and I'm not sure how you'd even find that out. This'd be a good way: https://github.com/rust-lang/crates.io/issues/1198
&gt; Can rust optimize multiple calls to function with same immutable arguments into a let, so that the function is called only once? I believe these sorts of optimisations are handled by LLVM, not directly by the Rust compiler. LLVM is *absolutely* capable of doing this, though I'm not aware of any way to *guarantee* that it will do so. &gt; Is it even worth it for such simple functions to do ^ ? It's probably 100% redundant (see below). &gt; Will rust automatically inline those functions, if they are so simple? LLVM will (if it decides it should) inline any function for which it has the code. LLVM has the code for *all* the functions in the current crate. It also has the code for any generic functions from other crates, and the code from non-generic functions from other crates that are marked with `#[inline]`. So adding `#[inline]` to a non-exported function in the current crate is, to my knowledge, totally pointless. Also note the "if it decides it should" part. `#[inline]` tells the compiler to make the function available for inlining, nothing more. You can force it to inline a function with #[inline(always)]`, but the general advice is to never do this unless you've profiled the code and know for *certain* that it's better. LLVM is already pretty aggressive when it comes to inlining stuff. It's not perfect, and it doesn't *always* get it right, but you're *probably* better off just letting it do its thing. &gt; Is there some rule of thumb, when to use the `#[inline]`? Does it fit my case? If it's a function being exported from your crate, and it's small, and you want LLVM to be *able* to inline it for users of your library, add it.
Reading in parallel is easy, it is writing concurrently that is the goal.
... yes... although it wouldn't catch people in environments where they're using a local crate mirror. Or using package repository mirrors. Better than nothing, though, certainly.
Thanks for your time :)
That it contains a lot of useful traits (and blanket impls) for convenience. A lot of them integrate with the language and different syntax. You should really consider learning about them and implementing/deriving whichever ones are relevant for your custom types, especially if you are writing a library. They enable lots of interoperability between different crates in the Rust ecosystem. Often times, just implementing a single trait can get you huge swaths of functionality without you needing to do anything extra. Have a look at `From`, `FromStr`, `IntoIterator`, `FromIterator`, ... Implementing `From` allows convenient conversions between arbitrary types using `.into()`. `FromStr` allows using `parse()` from the standard library to parse strings into your custom type. `IntoIterator` allows using your type with `for` loops. `FromIterator` allows you to `.collect()` arbitrary iterators into your custom collection types. `Ord` enables sorting. `Eq` and `Hash` enable hash tables. Have a look at this blog post: https://llogiq.github.io/2015/07/30/traits.html It is quite old and there are some newer traits that didn't exist back then, so read the docs! It is always a good idea to implement relevant traits for your types whenever possible, *especially* the traits from `std`. (but also from other libraries, such as `serde`) 
Yeah, also, they're in the CBOR spec, so when making a CBOR decoder, you need to deserialize them into something :)
&gt; So adding #[inline] to a non-exported function in the current crate is, to my knowledge, totally pointless. If an exported inline function `foo` calls a non-exported function `bar`, LLVM will not be able to inline `bar` in another crate unless it is also marked as inline. So marking `bar` as inline is *not* pointless.
Hmm... good point.
There still could be useful lints, like: * Disallowing it for release builds (just needs a general lint for the feature). * Disallowing it for public APIs.
I've even seen people thinking about 8 bits floats for machine learning :p
If you have implemented/derived `Deref&lt;Target=String&gt; for NewType`, then you should be able to use `&amp;NewType` where `&amp;str` is expected. [Rust Reference page](https://doc.rust-lang.org/reference/type-coercions.html#coercion-types)
If you have implemented/derived `Deref&lt;Target=String&gt; for NewType`, then you should be able to use `&amp;NewType` where `&amp;str` is expected. [Rust Reference page](https://doc.rust-lang.org/reference/type-coercions.html#coercion-types)
The age of our universe is 13,800,000,000 years old. So an u64 in milliseconds is able to represent a 1/24th of that. With processor speeds in range of gigahertz, computers are able to measure things in sub-nanosecond precision. If we want a single unified time type in the stdlib that is able to represent huge and super small timescales, 64 bits is not going to cut it. (Whereas 128 bits are more than enough.)
Does this also mean i can replace Box&lt;Error&gt; with impl Error? or are errors boxed for an entirely different reason?
I am still personally against the `impl Trait` syntax in argument position. It solves no problems. It is just yet another syntax that doesn't make anything clearer or nicer or enable any new features. The old syntax with type parameters works just fine and doesn't have any limitations, unlike the new syntax which does (and also breaks other syntax like turbofish). It only makes things more confusing. Why? Just so that you can avoid having to come up with a name for your type parameter if you only have one. As if people aren't already just calling it `T` by convention, which is clear and simple enough. Also, now we have the same syntax `impl Trait` meaning two completely different things, depending on whether it is in the argument or return position. I have seen all of these concerns voiced by other people in the discussions before. The syntax was stabilized regardless. Me being more vocal about it wouldn't have made any difference, so I did not even bother. I am really not happy that it was added, yet alone stabilized. I believe `impl Trait` should be just for return values. This is genuinely one of the very few changes to the Rust language that I actively dislike. Ugh. /rant
&gt; if you don't have CI setup to test on a specific Rust version, it seems exceptionally easy to accidentally bump your minimum Rust version that way Yeah, I can't count the number of times that I've built, tested, clippy'd, etc. my code locally, only to push it and have it rejected by our CI server thanks to the new match semantics not having been supported. Thankfully, most of our stuff is internal, so now that the new semantics are stable, we'll just update our CI server and I won't have to worry about it anymore. But I can definitely see it being a pain point for public projects with compiler guarantees due to just how easy it is to mess up. Maybe some extra lints could be added to clippy to verify compiler compatibility? It would be nice to be able to just add `#![deny(rust_1.25_incompat)]` to a crate's lib.rs and have it get rejected by clippy without having to either wait for CI or to remember to do a full rebuild with the proper compiler.
One reason is to avoid repetition, you're writing `T` twice in the first form and three times in the third form. In reality you don't even care about that type, except that it implements `Trait`, so in the second form, you don't write it at all. The second reason is so that it looks similar to `impl Trait` in outputs. `fn func(arg: impl Iterator&lt;Item=i32&gt; -&gt; impl Iterator&lt;Item=i32&gt;` looks nicer and clearer than `fn func&lt;I: Iterator&lt;Item=i32&gt;(arg: I) -&gt; impl Iterator&lt;i32&gt;`. 
Like I noted in my comment: &gt; This was just my experience and I didn't closely watch the scene after that company The subsequent project took over a year to even start to see the light of production. The main objective was to allow page generation to happen service-side for SEO but re-use the same stuff for dynamic single-page-app type stuff when actual users were visiting (all using the same code). It's good to hear the ideas were successful elsewhere, do you have some links to some basic examples? I'm not a javascript guy so it will be hard to see how this affected design decisions in a more complete/complicated example. 
This would actually work with non-lexical lifetimes as `bar` is not holding on to a reference; it only returns `usize`. If you use a nightly compiler and add `#![feature(nll)]`, this works. As a workaround for this particular case to work, you could tweak `foo` to something like: fn foo(x: usize, y: usize, z: usize, obj: &amp;mut Obj) -&gt; Usize { .. } Then, the `bar` calls are evaluated before the `obj` parameter, so the immutable borrows are finished before the compiler evaluates the `obj` parameter. This workaround wouldn't be applicable to methods taking `&amp;mut self`, as there, `&amp;mut self` is always the first parameter.
I feel like long term abstract type covers all of the use cases of impl Trait anyway and is much less problematic. So that's probably what it'll be replaced by in some future edition.
Another reason is if you don't control the environment that builds your code. For example, there are several websites with AI competitions where you can use Rust but the compiler won't always be the latest and greatest version. I recently participated on [riddles.io](https://www.riddles.io/), which has rustc version 1.18.0, which is about a year old now.
One useful addition could be a single function on `Vigil` that calls `set_interval` but then resets it after the next check-in. In the doc comment you mention that changing the interval for a single long operation is the main use for it, so not having to remember to change the interval back afterwards would be nice.
Digging a little deeper into the nll angle, although `#![feature(nll)]` makes this code work, I think it is not strictly a non-lexical lifetime issue, but a two-phase borrow issue. Two-phase borrows are enabled with that feature gate. The main feature of two-phase borrows is that they enable nested method calls such as `vec.push(vec.len())`, which is the same as saying `Vec::push(vec, vec.len())`.
I have to agree, I'm not as strongly against it but I really don't see why impl Trait in argument position exists. It seems unnecessary and unhelpful to add another way to do this to the two we already have.
I would say one method that you could do to approach this would be to do something similar to what one might do in C or Fortran using OpenMP. If your `values_to_change` is a unique vector and sorted this makes things a lot easier. I haven't actually had to do something like this in Rust yet, but I believe you should be able to do something like the following to get it to work. So, you would want to split your `values_to_change` up into a number of unique slices of lets equal to the number of threads you have. You can do this using unsafe code using something like: [split_at_mut](https://doc.rust-lang.org/book/second-edition/ch19-01-unsafe-rust.html#creating-a-safe-abstraction-over-unsafe-code). I would probably store these mutable references in a vector. Next, you would want to split up your `grid` into the same number of chunks going from the minimum to the maximum index in each one of your previously chunked `values_to_change`. Once again, I would try and store the mutable references into a vector. By splitting the `grid` variable up into chunks you should be able to get around the data race situation. Afterwards, you should be able to use something like `rayon::iter::Zip` to then work through all of your chunks in parallel. I believe inside the zip you should be able to use the same method that you were using in serial. 
`where` is definitely necessary but I mean we don't need `&lt;T: Foo&gt;` if we have `where`. My best idea for supporting turbofish is just having each `impl Trait` read left-to-right being in the next "type" position. I don't think it'd be too problematic since any changes to the type signature are breaking changes anyways, but it might be awkward to explain and use? We could also just... not be able to use turbofish. It would suck, but as you said we can't use extra bounds inside the `&lt;&gt;` generics either, we have to use `where` instead. This seems more important since the library using `impl Trait` makes a decision to disallow a feature to users while `&lt;&gt;` vs `where` only ever matters to the person writing the code, but it isn't _too_ different.
I just updated yesterday. Will report back if I experience any slowness. I'm on Linux, btw.
What newer traits have I missed?
Yeah, but Rust does not get to abstract away memory management. Note that `let g = |y| f(10, y);` and `let g = Box::new(|y| f(10, y));` are not equivalent in Rust.
I think it is a big price, because it *isn't* fully interchangeable with generics (because you can't specify with it turbofish, meaning it is slightly less functional). The whole point of impl Trait is that the user can't specify the concrete type(useful in return types, ie for various Iterator structs). My concern is if you allow impl Trait type to be able to be specified in arguments, now impl Trait has two different meanings (specifyable vs not specifyable type) depending on whether it's an argument or return type. That just seems implicitly unintuitive to me
if you Box them because there might be multiple different types returned, then you still need `Box&lt;Error&gt;`. `impl Trait` only does static dispatch, so boxing is still useful for dynamic dispatch
Ah, okay, derp. I'm not familiar with Debian tooling...
[`std::mem::replace`](https://doc.rust-lang.org/std/mem/fn.replace.html) is Rust's best-kept secret. It's hidden away next to some scary unsafe functions, but `replace` – and its sibling `swap` – are totally safe and you should not hesitate to use them. They let you assign to a mutable reference while returning the old value instead of dropping it. A great solution to "cannot move out of borrowed content" woes.
Is there a linting tool that could check for these kind of best practices that are not directly connected to the code? It could read CI configuration files in your repo and suggest pining to a fixed rustc version. (It could also suggest adding beta.) It's not like bumping your required compiler version is an inherently bad thing it just should be a conscious decision. 
This is definitely something I'll keep in mind!
Way more than I anticipated the first go around.
&gt; Is there a linting tool that could check for these kind of best practices that are not directly connected to the code? I'm not aware of one. It seems like it'd be out of scope for something like `clippy`, but maybe there could be a more general `cargo lint` command or something that also integrates checks for broader things like CI configuration? Not sure what else it would be used for, though...
I have mixed feelings about it. On the one hand, those types can be nearly impenetrable, but on the other hand, they also contain a lot of information that can be useful when debugging. My gut feeling is that I'd rather have my libraries give me the verbose error, and then let me wrap it in functions that impl the trait. 
`Any`, `Read`, `Write` are missing, they're pretty important IMO.
I understand it's got some uses, but I really have to question adding language features where the advice is, 'don't use this unless you've got a really good reason'.
I only changed the examples so the public API remains intact, don't worry :). If and when I release 4.0 I may use `impl Trait` in the public API where it is possible (though many core parsers require an explicit implementation to get maximum performance).
An option for disallowing would be great!
How can I use the `?` operator in a function that returns a future? My use-case is for a route handler in Gotham. Right now I'm just calling `unwrap()` on everything, but at some point I will have to implement real error handling. My attempts so far have followed a similar progression to the way I remember Rust's `?` operator getting implemented. My first attempt (without any sugar) looked like this: match serde_json::from_str(&amp;req_body) { Err(e) =&gt; return future::err(e.into_handler_error()), Ok(val) =&gt; val, } Next I wrapped that in a macro like this: macro_rules! htry { ($x:expr) =&gt; { match $x { Err(e) =&gt; return future::err(e.into_handler_error()), Ok(val) =&gt; val, } }; } I am reasonably happy with that result, but I'm wondering if I can take it one step further and implement the `Try` trait so I can use the `?` operator. This is what I have so far: #![feature(try_trait)] use std::ops::Try; impl Try for IntoHandlerError {} Missing implementation aside (since I don't know how to implement it), I get this error: only traits defined in the current crate can be implemented for arbitrary types 
&gt; There is a lot of excitement around what returning an impl Trait implies for closures, but it's not clear to me why this is so. Is a closure describable by behavior? In other words, can I ascribe a trait to a closure? Closures implement `Fn*` traits but each closure has its own anonymous type. So before impl trait, you could only return closures as *trait objects* (generally boxing them). With impl trait, you can return closures "by value" without having to box them and it works properly.
Why?
Creating C bindings can be somewhat tedious. [I've been trying to make it easier with macros for distinst](https://github.com/pop-os/distinst/blob/ffi-macros_nobuild/ffi/src/lvm.rs#L40:L122), but `cbindgen` doesn't seem to work at all when you have macros generating functions. Theoretically, it should work with the expand option, but at the moment that option merely causes infinite compile times.
I seem to be doing something wrong trying to start using `rustc 1.26.0`. I actually had a similar problem when `rustc 1.25.0` was released, but couldn't figure out what was going wrong. Is there something I'm supposed to do beyond just `rustup update`? I've run `rustup update`, and it cheerily reports that I'm using the `stable-x86_64-unknown-linux-gnu` channel with version `1.26.0`, but when I try to use `rustc` or build a project using `cargo` with the new `1.26.0` features like `impl Trait`, I get an error that this is an experimental feature. Interestingly, I'm no longer having issues with `1.25.0` features like nested use paths using `cargo build` or `rustc`. I *am* able to make the code compile if I do `rustup run stable cargo build`, however. Is there something weird going on with my channels?
one thing which took me a while to realise is that if you use rustup all of the documentation is available offline as well rustup doc or in your project cargo doc --open
Yeah, I encountered that as well. It's the reason why I have a separate `swiggen` binary instead of making it something you can just add to the build script. [All it does](https://github.com/samscott89/swiggen/blob/master/swiggen/src/main.rs) is just call `cargo expand` first and then runs `cbindgen` afterwards (as well as the custom stuff to pull out the SWIG code as well).
I didn’t know that! I will remember that next time when I work without Internet.
Can quickcheck's `Testable` trait be implemented for `impl FnMut()` I wonder? I've wanted that since I started quickcheck over 4 years ago.
Exactly
I don't *think* so. You'd still be doing `impl&lt;T&gt; Testable for T where T: FnMut()`. Presumably you couldn't make it work due to overlapping `impl`s.
&gt; If the impl Trait at hand stems from a function like fn foo(v: impl Traita) -&gt; impl Trait so the returned type is of the form Struct&lt;V&gt;, Bar would need generics in the pre-impl Trait version as well. Do you mean `fn foo&lt;V: Traita&gt;(v: V) -&gt; Struct&lt;V&gt;` ? Sure, that returns something generic too. &gt; I have this certain feeling that the new system makes code easier to write if you don't want to understand how it works, but if you want to understand it, the system makes it harder. Maybe that's just me being new to the system though. I hope this will clear up a bit in the future. Every syntactic sugar - and `impl Trait` is basically just a poor man's newtype (well, and the dancing around the "closure types cannot be named" issue) - makes the language larger, because now you have to know both ways to write the same thing, in order to be able to read all code. 
Without with an explicit type each use of a combinator will create a larger type like `Skip&lt;Many&lt;(Char, Or&lt;Parser1, Parser2&gt;)&gt;&gt;` whereas if each combinator used `impl Trait` instead that would just be `impl Parser`. While the compiler can memoize the result of checking large types like that it still has a cost. With just `impl Parser` though the compiler can just immediately see that it is "some" `Parser` type which should be faster (since there is no large type to check).
I wondered what the arguments were in favor of positional argument, since I had seen lots of arguments against (at least on Reddit), but not really in favor. I checked out the RFC that added it, and it seems like it was 100% about "learnability." I'm not sure the arguments against were really given as much weight d they should have been. Even the RFC reflects this; there's a section that lists some of them very briefly, and then simply rebuts them, without acknowledgment. Additionally, I'm starting to think that the whole "learnability" goal might have been dangerous. It doesn't seem like an objective, measurable goal to rally around. It's something people can only guess about, with only their intuition to lead them. I can't imagine even something like user studies would be very helpful, since everyone learns differently.
I thought the compiler still knows what the actual type is (and checks it)?
In Haskell we'd use C preprocessor guarded on the compiler version for this. I'm curious how y'all do it.
enums are good when you got a handful of variants. But if you got, say 100 variants, it becomes a nightmare to maintain.
Ah, well saying "it's always existential" as a tool to explain impl Trait in argument position seems reasonable. And to be fair that's what this article concludes with, mostly. But it's very combative about it. And on top of that, as one of those who does complain about impl Trait in argument position, "inconsistency between input vs output" isn't really what I have a problem with, so it kind of feels like a straw man.
With cargo feature flags that trigger cfg statements in the code.
I did `rustup toolchain uninstall stable` and `rustup toolchain install stable`, which seems to have fixed it! No idea why it wasn't working but problem solved for now.
How is this a good thing?
&gt; I try to just encourage folks to pin a specific Rust version in their CI, so that it at least makes everything discoverable, without advocating any specific policy with respect to semver. Suppose I haven’t been doing this and want to start. Is there a reasonable way to figure out what version I should support, without doing a binary search on Rust versions?
Yeah, the conditional compilation not relying on CPP is pretty great. But I don't know how to tie it to a rustc version, or to the presence of a language or library feature.
I'd use a build script and the [`rustc_version`](https://crates.io/crates/rustc_version) crate to detect the version, and define a `cfg` flag if `u128` support should be enabled.
They each make sense in different situations. We *could* require you to always use the most complex syntax (the `where` clause) but then simple stuff looks quite complicated. We could require you to use the simplest syntax, but then complicated stuff looks terrible.
Yes, we're starting small and going from there.
How does the receiver know the size of the object? This is reason why it needed to be boxed in the first place.
Oh, you just can't as far as I know. Your CI can be set up to target an older stable as well as checking if you build on nightly, but basically Rust's stability story of "you can always bring old code forward" means that we're really bad at trying to remember how far back a piece of code can go. Rustup mostly only interacts with the branches in terms of stable/beta/nightly, you don't normally ever pick an exact stable version, you're assumed to always be on the latest stable.
Ok.
It needed to be boxed because you couldn't name the closure's type, and you can't return a trait object without some kind of indirection. In this case, `impl FnOnce() -&gt; T` is standing in for the name of the type, which the compiler knows. You can think of it meaning "I can't/won't tell you what this type is, but I *will* tell you that it implements `FnOnce() -&gt; T`", and that is enough for the compiler to validate how callees use the returned closure.
Download, audit, and vendor libraries if that's important. Cargo makes that easy.
No, sorry. :-)
I don't think so. I think it's just judgment. You could start with whatever your minimum actual version is today. To figure that out, yeah, I'd do a manual binary search or something.
It does, but consider what the compiler needs to do when encountering a `Skip&lt;Many&lt;(Char, Or&lt;Parser1, Parser2&gt;)&gt;&gt;` and needs to prove that it is a `Parser`. To do that it looks at impl Parser for Skip&lt;P&gt; where P: Parser { ... } thus to prove that `Skip&lt;P&gt;` is a `Parser` it also needs to prove that `P` is a parser. Since in this case `P == Many&lt;...&gt;` it then needs to prove that `Many&lt;Q&gt;` is a parser etc etc. Now the compiler can (and does) memoize the work needed to prove each of those steps which prevents this from going exponential but it is still some work to check this. On the other hand with an `impl Parser` the compiler does not need to prove anything at all, it can just immediately see that it is a `Parser` and does not need to check anything further.
What about something like \` fn main\(\) \-\&gt; Result\&lt;\(\), impl std::error::Error\&gt;\` ?
No, I mean something like: concat_arrays! { [0, 1]; A = [2, 3]; B = [2, 3, 4]; } You could generate `A = [0,1,2,3];` and `B = [0,1,2,3,4];` from that.
It's worth spending some time digging through all the million combinators for `Iterator`, `Result` and `Option`. You don't need to memorize everything but having some idea how they work will make you occasionally say, "hang on, isn't there a way to...", which will lead to faster programming and clearer code.
A function that returns an `impl Trait` still only has one return type, of a defined size. This type is determined by the compiler at compile time. Callers simply aren’t allowed to assume anything about this returned value other than the trait. This is extremely useful for returning closures because closures have types that aren’t explicitly nameable. So you had no way to put their name in the type signature of a function in order to return it. Now, the compiler allows you to say you’re returning an `impl FnOnce`, it determines the exact type and size of this return value at compile time, and the caller knows how much space to allocate in the caller. This *does not* allow you to dynamically return one of several types that actually implement the trait, which would indeed make it impossible for the compiler to know how much storage is necessary. 
What does `rustup toolchain list` say your default toolchain is? Maybe you need to `rustup default stable`?
That's why packages on Crates.io are open source, immutable, and require semantic versioning.
You don't even need a binary search - just push a side branch where you configure your CI to check all minor version since 1.0.0. It could take some time, but it's not that bad since you don't have to be actively involved.
hmm. I feel like writing to the grid once all new values are calculated shouldn't be ran in parallel. The time to just set a value in an array is negligable if you know what the value is. You'll just want to parallelize the *thinking* part of the logic. Am I missing something?
(It comes off as casual-toned / enthusiastic to me, which is probably a function of our having been predisposed to agree vs. disagree with it...) &gt; And on top of that, as one of those who does complain about impl Trait in argument position, "inconsistency between input vs output" isn't really what I have a problem with, so it kind of feels like a straw man. I remember you had a long and nuanced argument against allowing it in argument position (IIRC, it was basically against the TMTOWTDI aspect of it? but my memory's hazy); separate from that, I remember a lot of people being against it for roughly the "inconsistency" reason. (This is where the whole "let's have `some` and `any` instead" branch of the discussion came from, which, to be clear, I think would've been fine even if I don't share the motivation.)
It makes sense for the same reason you can technically put `()` in an argument position. `impl T` means something and you are free to use it wherever you might write a type.
A co-worker pointed out that the callbacks don't need to be `Sync` if I split up the data elements a little better. I've pushed version 0.2.0 with this change.
Good idea, I'll have a think about how to do that safely (the lock-free nature of the code makes it a little non-trivial). Shouldn't be too hard though!
Looks like it's already enabled in the `next` branch as of [this commit](https://github.com/autozimu/LanguageClient-neovim/commit/7b1f2b10de7e65d35a49fe2fbf4288027ef0e6fe) The README shows how to target the next branch if you use `vim-plug`: ```vim Plug 'autozimu/LanguageClient-neovim', { \ 'branch': 'next', \ 'do': 'bash install.sh', \ } ```
I agree with you. As I said previously, it does not even help learnability, since anyone learning Rust now still has to learn the old syntax, as the new one is more limited and there are many things it cannot do (besides, the old syntax will still continue to be found throughout the Rust ecosystem). So now newbies have to learn 2 syntaxes instead of one. It is just another new thing to learn for no reason, another source of confusion ("why does this exist?", "why do I have to learn 2 ways of doing exactly the same thing?", "when do I use `impl Trait` vs. when do I use type parameters?"). If anything, it makes the language harder to learn, not easier, since now you have to learn more things.
This is mentioned in the announcement I believe. It's a great little improvement, especially for newbies: the compiler is already scary enough!
&gt; where is definitely necessary but I mean we don't need &lt;T: Foo&gt; if we have where. Ah yes, but we'd still need to have `&lt;T&gt;` to introduce `T` into the scope. And using `where` for all trait constraints would be very verbose.. But using `&lt;T: Trait&gt;` for all such args wouldn't be more verbose than `impl Trait`..
Important ones: `Read` and `Write`, but these aren't new. It might be a good idea to add those to your post for completeness. Also `TryFrom` and `TryInto`, but these are still nightly-only. There are the various kinds of `Iterator`: `DoubleEndedIterator`, `ExactSizeIterator`, etc. but these are more niche. `Any` is missing, but I don't think it is important. I just skimmed through the `std` docs to make sure I covered everything, so I can give you the most useful feedback I can; seems like that's it! There isn't really as much new stuff as I thought there was. &gt; It is quite old and there are some newer traits that didn't exist back then, so read the docs! I didn't word myself properly. `s/are/might be/`. I wanted to encourage OP to read the docs to learn more and to be aware of any new developments, not to pick on your post for being incomplete!
No overrides reported, unfortunately.
Thanks for the pointer. Apparently this sets --cfg, not --features? Is there a way to enable a feature from build.rs instead? I don't see that in the docs, but I need to pass the feature `i128` to `num-traits` when this is enabled. Maybe I need to stop depending on `num-traits`.
I was hoping for this as well for a brief few seconds of my compiler as well
&gt; Additionally, I'm starting to think that the whole "learnability" goal might have been dangerous. It doesn't seem like an objective, measurable goal to rally around. It's something people can only guess about, with only their intuition to lead them. In my opinion, this also happened with the reserve `try` RFC. The main argument for try is that it has a rough parallel with exception handling in C++/Java/etc, and this makes it easier to learn. The opposing argument never fully formalized, but it was roughly that `try` models local control flow, and this fact is obscured by borrowing the exception keywords. The "learnability" goal seems to have had a similar effect. The argument for try could have stronger support because learnability is subjective. Niko Matsakis made a compelling argument for familiarity in the try RFC. Insofar as learnability is related to familiarity, it echoes the [Argument from learnability](https://github.com/rust-lang/rfcs/blob/master/text/1951-expand-impl-trait.md#argument-from-learnability) section of #1951: &gt; I think that there is a belief -- one that I have shared from time to time -- that it is not helpful to use familiar keywords unless the semantics are a perfect match, the concern being that they will setup an intuition that will lead people astray. I think that is a danger, but it works both ways: those intuitions also help people to understand, particularly in the early days. So it's a question of "how far along will you get before the differences start to matter" and "how damaging is it if you misunderstand for a while". &gt; &gt; I'll share an anecdote. I was talking at a conference to Joe Armstrong and I asked him for his take on Elixir. He was very positive, and said that he was a bit surprised, because he would have expected that using more familiar, Ruby-like syntax would be confusing, since the semantics of Erlang can at times be quite different. But that it seemed that the opposite was true: people preferred the familiar syntax, even when the semantics had changed. (I am paraphrasing, obviously, and any deviance from Joe's beliefs are all my fault.) &gt; &gt; I found that insight pretty deep, actually. It's something that I've had kicking around in my brain -- and I know people in this community and elsewhere have told me before -- but somehow it clicked that particular time. &gt; &gt; Rust has a lot of concepts to learn. If we are going to succeed, it's essential that people can learn them a bit at a time, and that we not throw everything at you at once. I think we should always be on the lookout for places where we can build on intuitions from other languages; it doesn't have to be a 100% match to be useful. 
I wonder if you have a parallel installation of `rustc` sitting around, which lags behind on the version updates. What's your `which cargo` / `which rustc`? Is that the same paths that rustup installed? How many Rust-related packages does your package manager show you have installed?
&gt; Maybe I need to stop depending on num-traits. Or send a PR to num-traits to similarly detect compiler support in its build script instead of requiring a feature (after checking with the maintainers to see whether they're interested). 
That's what it was! Looks like I had it installed through `pacman` at `/usr/bin/`, but I also had it under `~/.cargo/bin/`. Thanks!
i think you mean gpdr
Personally, I find the learnability of a language I already know inside and out quite importaint. The biggest reason is that the community around a language is so importaint. A bigger community is almost always better.
Nice! Note that you can install `rustup` via `pacman`, and if you do then it'll own the `/usr/bin/*` binaries.
Since `impl` has been recently stabilized, I wanted to share this since I found this concept by [withoutboats](https://github.com/withoutboats) insightful.
Agreed, but the verbosity reduction of `impl Trait` vs `&lt;T: Trait&gt;` is very small compared to that of `&lt;T: Trait&gt;` vs `&lt;T&gt;`+`where T: Trait`, so the latter has more reason for existing, and it keeps the constraint visually closer to the scope that introduces the type, making the code easier to read..
Currently the only work-around is this? Is that correct? It hits two other unstable features... #![feature(fn_traits)] #![feature(unboxed_closures)] struct MyFn&lt;T&gt;(T); impl&lt;T&gt; FnOnce&lt;()&gt; for MyFn&lt;T&gt; { type Output = T; extern "rust-call" fn call_once(self, (): ()) -&gt; T { self.0 } }
Yeah, build script runs after dependency resolution so you can't set features. 
It works strangely. Also, the search in issues is mostly useless.
3 times. Not bad. N and Author columns will be useful.
That's not true, you can't put it as the type of a struct member, or `impl Trait for (impl SomeOtherTrait)`, etc. And `()` means the same thing in argument and return position, unlike `impl Trait`. 
I don't see how it makes sense. fn foo&lt;T: Trait1&gt;(x: T, y: impl Trait2) Would the turbofish for `y` go before or after the one for `x`? 
Absolutely. - If you want two arguments with the same type - If the return type is some generic that depends on the actual input type - If you need to name the input type, say for a turbofish in the function body 
I think you misunderstood my question. The question is rather: if I use `impl Trait` in argument position, is there any time this will be problematic for the *caller*, because the caller can't use turbofish to specify the type?
Regarding the precision of measures, the most precise hardware timestamps I heard of had a precision of 1/10th of a nano-seconds (aka, 100 picoseconds). On the other hand, I am not sure if it's that useful to be able to add 100 picoseconds to 14 billion years and not lose any precision ;)
I guess it's one of those features like list comprehensions in Python that make no sense when you first encounter them, but are an amazingly useful tool after things click for you
Fair enough! :)
I was hoping I can make sometihng like `A = myarray!{ [2, 3]}; B = myarray! { [2, 3, 4]}` to achieve the same result. 
For example you are making macros which generates function with argument per each supplied keyword. And this arguments have unknown type, but implements same trait. Before, there was trouble with supplying different unique Type parameters per each argument, now you can just impl Trait.
You should be able to. `FnMut` is fundamental so you get to break the rules about negative reasoning for it. It's still overlap with any other blanket impls, but it looks like it's only implemented for concrete types
Oh, that's a good idea.
This would be excellent. We'll live with `impl Trait` for a while until we get a better replacement (abstract type, nameable `async fn` types), then hopefully start linting against it (at least in public APIs), and finally remove it in Rust 2021 or 2024.
Any is to be used with trait objects. 
Oh boy, you’ve invented autoconf! Maybe feature detection is the way to go, but it sure would be nice if Rust's conditional compilation just supported feature detection directly.
https://play.rust-lang.org/?gist=859f85b9388cc2b64df3b7757c7b00c6&amp;version=nightly&amp;mode=debug
Seconding this. My students write a lot of awkward pattern match code because they are insufficiently familiar with `Result::unwrap_or_else` and methods like that.
Yeah, I've used autoconf in the past, and was very probably influenced by that. :)
I have similar project https://github.com/Dushistov/rust_swig It also has similar name. May be we can cooperate ?
I don't think the situation is much different than what happens in Python or Node. Not to say that makes the situation any less dangerous, but it meets the demands of the status quo.
For some combination of "pretty important" and "not obvious": Sometimes you need to look carefully at the trait implementations that come with an object. For example, `File` implements `Write`, and `Write::write` requires `&amp;mut self`, so you might think that you can't write to a file through a shared reference. However, if you look carefully at the `File` docs, you'll see that `&amp;'a File` _also_ implements `Write`, so you _can_ write through a shared reference. A similar situation comes up with `Iterator` methods, where it looks like they all require `self`, but there's a blanket impl that makes any `&amp;mut` of an iterator also work as an iterator.
Is there a reason https://doc.rust-lang.org/std/primitive.str.html#method.get consumes the index? I've helped myself with cloning (not sure if that is expensive for a range), and I might be able to pass the original by value, but I'm kinda wondering if that is really necessary. Or should ranges just be copy? Thanks for any pointers :)
Link is working fine on my side.
Binary heap's [PeekMut](`https://doc.rust-lang.org/std/collections/binary_heap/struct.PeekMut.html`) are kind of cool. It fixes the heap on drop.
Ah it was my reddit client. Urlencoded # breaking the link.
`&amp;mut [u8]` implements `Read`. That's my favorite one by far.
Note that I did add to the API with new methods in `ToPrimitive` and `FromPrimitive` \(though they do have defaults\). I guess it doesn't make much difference whether that's enabled with a build script or a feature, and I can see how using a feature doesn't let *others* use a build script. Hmm.
This was actually fun! I started like "How hard can it be?", then I read a bit about macros and felt "Dude that's too hard for me!"... 2 minutes later I've got it working :D [This](https://github.com/KillTheMule/nvimpam/blob/master/src/carddata/part.rs#L10) is what I ended up with, seems to fit nicely. The worst thing is that rustfmt does not seem to work inside of macros :P
it's actually great even when you have internet: \- you are guaranteed to be looking at the docs for the version of the dependency you are \_actually using\_ \- the docs are \_really\_ fast since they are off your local filesystem \- you can lookup the docs using your normal workflow. I have `alias rs-doc='cargo +nightly doc --open -p'` This allows me to do` rs-doc rege`x
just released yesterday :\)
Using the build script can also have another temporary short-term minor advantage: the documentation on docs.rs will still be built if the docs.rs build system is slightly out of date, although the new features won't be documented.
Ok, thanks :)
You went &amp;mut type. You’ve created a miracle binding to an immutable reference. What you want is a binding to a mutable reference.
Yeah, the problem is conversion of error type in `?` operator using `From` trait. If there was a way to tell Rust to default to `impl&lt;T&gt; From&lt;T&gt; for T`, it'd work.
I have a stub project somewhere to define fs traits for the operations you want to actually perform (object, appending log, in-memory-db), etc. The idea is to help wean people off of POSIX fs semantics since it's almost never what anyone wants as it's almost impossible to correctly write a file. (e.g. are you checking EAGAIN? are you appending to a file on page/block boundaries to make sure each write is atomically handled so there's no corrupt data or dirty reads?).
It's not my crate, but have you seen https://github.com/darrenldl/reed-solomon-erasure ?
&gt; This does not allow you to dynamically return one of several types that actually implement the trait, which would indeed make it impossible for the compiler to know the size of the object. It could simply pick the largest one and do dynamic dispatch like it was an `enum`. Tip: if you are using `futures`, you can use `futures::future::Either` to return different type from different branches.
`whee` returns one type: the type of its argument, whatever that is. The more complex example, `mwahaha`, doesn't return one type: it returns either the type of its second argument, or the type of its third.
Imo there are not enough libraries in that list. Not meaning to discredit those awesome apps.
When you return `impl Trait` it's hiding a *single* type. If multiple error types can be produced at different points in `main`, this wouldn't work. 
&gt;I think it is a big price, because it isn't fully interchangeable with generics (because you can't specify with it turbofish, meaning it is slightly less functional). The whole point of impl Trait is that the user can't specify the concrete type Wait, I'm arguing that it *should* be interchangeable. `impl Trait` in return type and arguments are already different things and have very different "whole points", for arguments it's convenience/shortcut for single-use type parameters. What is counterintuitive is same syntax for these two different constructions, "dialectical ratchet", my ass.
In C++ you get explicit parameters first, then implicit ones in left to right order.
In C++ terms, you just did void change(const int* x) { x = &amp;42; }
They have their uses, mostly in graphics and other gpu programming. Basically, they allow compactly representing a high dynamic range number \(which is a float\) with few bits, to save memory.Usually you don't do arithmetic on them directly \(because their precision is so terrible\); they are just used for storage as a memory optimization. You convert them to a 32\-bit float for intermediate calculations, to do them with the higher precision, and only round back at the end when converting back to 16\-bit if needing to store them again. The loss of accuracy from the rounding errors is usually fairly negligible for graphics, because you wouldn't notice it when looking at the rendered image.
It could, but it doesn’t yet, and may never do so.
If you don't want to make it that complicated. I have a basic build script for the same purpose that just checks if rust is nightly: https://github.com/JelteF/derive_more/blob/master/build.rs
Is the standard library making use of `impl Trait`/existentials? If not, will it in the future or is this a breaking change?
It currently does not. Stuff that exists today can’t change, as that’d be breaking. New APIs could use it, though.
The point is that the "one type" vs. "multiple types" thing in the end doesn't really clarify an issue that boils down to quantifier scope. Let's do them in something like a lambda calculus with second-order universal and existential types. `whee` is like this (eliding the trait bounds which are not in fact relevant to the issue, and using `*` as the universe type): λA:*. (A, λx:A.x) : ΠA:* .ΣB:*. A → B `mwahaha` is trying to be something like this: λA:*. λB:*. (???, λp:bool. λx:A. λy:B. if p then x else y) : ΠA:*. ΠB:*. ΣC. bool → A → B → C ...but the conditional doesn't typecheck so it doesn't matter what type you fill in for `???`. I was confused because I somehow approached `impl Trait` with a preconception that `whee` would be like this, with the existential scoping over the universal: (B, λA:*.λx:A.x) : ΣB:*.ΠA:*. A → B I mistakenly thought that because I believed that `impl Trait` required the type `B` to be known at `whee`'s definition site. But no, it only needs to be known at each call site, which may independently instantiate it to a type that depends on `A`.
Along with `String` and `Vec&lt;u8&gt;` implement `fmt::Write` and `io::Write` respectively.
Nope, editions can’t make breaking changes in libstd.
While, as others have said, you can use mutable references, this is also a terribly bad practise. There are some exceptions, for example where you want to re-use a buffer, but in general you probably just want to return the value, or a tuple thereof. 
&gt; A function that returns an `impl Trait` still only has one return type, of a defined size. This type is determined by the compiler at compile time. I think we should consider explaining this in a different manner, because this explanation is easy to misunderstand. It focuses on the function definition, and to my ear it falsely suggests that it uniquely determines a return type. But here's [a playground example I did yesterday](https://play.rust-lang.org/?gist=b45cfc1e513985b7a2c9c1538fb248f4&amp;version=stable&amp;mode=debug) that disabused me of this misunderstanding: use std::fmt::Debug; fn main() { println!("whee: {:?}, {:?}", whee(5u32), whee(6u64)); } // I was slightly surprised you can do this: fn whee(x: impl Debug) -&gt; impl Debug { x } Note how `whee` has different return types with different sizes, but *in different call sites*. I think the better statement is this: the compiler must be able to statically determine a monomorphic return type *at each call site* of a function that returns `impl Trait`. But different call sites for the same function may yield different return types, and those return types may depend on the instantiations of type parameters.
OK, num\-traits 0.2.4 now automatically probes for `i128` in a build script.
There are `FromIter` implementations to go from an iterator over `Result&lt;T, E&gt;` to something like a `Result&lt;Vec&lt;T&gt;, E&gt;` (returning error on the first error element of the iterator).
logwatcher [takes a closure](https://github.com/aravindavk/logwatcher/blob/master/src/lib.rs#L78) not a funtion, [just use a closure](https://play.rust-lang.org/?gist=594304bf28b2e50b6d7319a3a6f38a74&amp;version=nightly&amp;mode=debug).
You can even [use the new and shiny impl Trait in argument position](https://play.rust-lang.org/?gist=2f75180caedb9bbc529424e05d9e2d53&amp;version=nightly&amp;mode=debug) to avoid having an explicit template parameter `F`. It looks a little bit neater.
That is an *excellent* point. 
Those additionally implemented traits are transparent to the language feature "specialization", just for that reason (iterators). See my [comment above](https://www.reddit.com/r/rust/comments/8ik620/notes_on_impl_trait/dysvm34/) for an example.
ask for a commit bit?
How are you creating your crate directory? If you are using `cargo new` one of the things it does is create a `.gitignore` file. This includes an ignore for the `target` directory which you should not check into version control, which might be the reason your `git add` is taking so long. 
Good tip about SSL. Took me a bit to figure that out. 
On the flip side, that's why I still reach out to python, rather than rust, when I want to experiment or hammer out a poc. The default HTTP and parser modules may be ill-suited for a public facing application, but they're more than appreciated for quick and dirty experimentation and one-off scripts. While people may be stubbornly against it now, I hope that rust gains some much-needed internal batteries in the future. 
Well, since it's unstable, it's not much of a work-around... also, since we now have `impl Trait`, the "currently" isn't true, either. But yes, that *is* how you would do it if we didn't have `impl Trait`, *and* the function traits were fully stabilised.
I know this is a couple days old, but I read this a while ago, and I'm wondering why proof finding is slow. (I know nothing about formal proofs ;) )
If you use `--lib` it adds `Cargo.lock` to the gitignore. Otherwise every project starts with the same. See: https://github.com/rust-lang/cargo/blob/827bf050dcff4315a997649023b245c22d78133c/src/cargo/ops/cargo_new.rs#L421
Curious: why gRPC instead of Cap’n Protp RPC? The latter supports way more features, is faster, and has a production-quality official Rust implementation.
The default `.gitignore` created by Cargo just contains /target **/*.rs.bk You can put this in any Cargo project you work on. You can put '/target/' or 'target/' in your global gitignore, which is risky since that name can be used by other things, but is sometimes convenient. I will admit to doing the latter, and also admit to occasionally being confused when working on non-Rust things. You can override in a non-Rust git working directory with a local `.gitignore` that includes `! target/`.
Why is one line a huge barrier here?
Thank you! This works because closures implement the Fn trait, correct? I guess I'm having trouble understanding what &amp;|| even means. What does it mean to borrow (take a reference to?) a closure. What am I borrowing it from? I ha
Cool, thanks :)
Does println!("cargo:rustc-cfg=feature=\"nightly\""); actually work?
Unless `cargo doc --open` does not work at all (Windows).
Skia has code written for this, so you might look into some of the bindings for it.
Isn't it obscure? I mean, you can't be suspicious all the time and check all cross-references like a "doc hacker".
Why would you need to vendor them when packages on crates.io are immutable? Unless you need to incorporate your own patches or make sure you can operate without crates.io I suppose.
One thing that's unclear to me is how to create a file-backed database, since both of your examples are of memory-backed databases. I would assume it’s similar, but I don't see how to specify the file name.
In C++ out parameters were used because of: a) before std::move (C++11) and before guaranteed RVO (C++17) to eliminate return value copying; b) before structured bindings (C++17) when you need multiple return values. That's all I can really recollect. In Rust you have move-by-default and multiple return values (tuples), so we're encouraged to return via return values. 
Was there discussion about this yet? What are the arguments against it?
&gt; Given that both the source and the library are proprietary, how would I go about providing information to the rust team to help narrow down the issue? Has anyone run into and debugged an issue like this before? If you can write a small, self contained example of the rust code which calls the C function, in a way that can be extracted from your proprietary code base, and still exhibits the issue, along with the Rust declaration of the C function, the original C header declaration of the C function, and the different assembly that each version of rustc compiles your small self-contained example down into, that would probably go the furthest towards providing information for others to be able to debug what's going on. You could also potentially reach out to the [community team](https://www.rust-lang.org/en-US/team.html#Community-team), who are responsible for contact with commercial users, to see if they could put you in touch with someone who might be willing to work on getting the issue debugged and reported privately, though you might have to pay for that service.
I know of some cheap hardware ($2) doing &lt;10ps precision, look for e.g. TI TDC (can't remember the exact ref).
You should be aware that there are many ways that the fault could be in the proprietary code, even though the only thing you changed is rust 1.25 to rust 1.26. For example, if the proprietary code assumes uninitialized values to be zero, or reads uninitialized memory, or reads or writes out of bounds, or really any kind of undefined behavior, it could have just coincidentally not been blowing up under 1.25. I think you're going to need to debug the crash, find the instruction that crashed, and find out why. If it's rust's fault, hopefully you can make a minimal piece of code to reproduce it.
You can use `git add src` or simply `git commit -a` if you want to commit all your changes 
But there is a natural choice for C here, and it is `C = A + B`, or in Rust terms, `enum _Anonymous&lt;A: Debug, B: Debug&gt; { A(A), B(B) }`. Now, there is a question whether `A + B` can meet the `Debug` bound. But because `Debug` is a ['negative'](https://ncatlab.org/nlab/show/negative+type) trait (it only defines methods with `Self` in the argument position, never in the return position), there is a natural way to define `Debug` for any coproduct (enum) of `Debug` types. (This wouldn't be the case with, say, `Default`.) And inferring `C = A + B` here [has been requested](https://github.com/rust-lang/rfcs/issues/2414). That your example doesn't compile is a limitation of the type *checker*, not of type *theory*.
Minimizing might be useful, although I'm not familiar with the tools typically used for it in Rust. I think extracting a small, reproducible test case may be tough since even inserting a couple of functions into the (safe rust) code to debug causes the problem to magically disappear.
&gt;language features where the advice is, 'don't use this unless you've got a really good reason'. The same is true for `unsafe`, and it is still a critical feature. Impl Trait can help in cases like iterators, closures, futures, etc. The problem is if you abuse it, but this can be said for almost every feature in the language.
Fair comparison would be FDB vs Redis Cluster. \- FDB guarantees serializable ACID transaction, transactions are limited to 5 seconds. \- Redis Cluster, no ACID transaction guarantee. Explicitly unsupported.
The two big things which help are: - Cargo enforces never updating dependencies or dependencies of dependencies unless you ask it to. Even if someone else clones your repository, your `Cargo.lock` will keep all the dependencies at the exact same versions - crates.io is immutable for published packages. With npm and other registries, authors are allowed to take down or republish crates - but the most you can do in crates.io is hide a package from search (it will always still be available). This is further guaranteed with `Cargo.lock` files containing hashes of the dependencies expected. Even if there was somehow an MITM attack (which there shouldn't be since it's all HTTPS), an attacker could not change actual crate contents. - Last, the source code of all crates.io crates is extremely easily browsable. You can open up the source files that are directly used to compile the dependency just be opening an editor in `~/.cargo/src/github.com-~~~/crate-name-version/`. This is more foolproof than github since it's literally the code compiled, and there's no way it can be anything different. (see: hashes in Cargo.lock).
Oh, you are the 25519-dalek people! Cool library! :-) So, those log-sized aggregated range proofs are said to *require* very long verification times. [Monero](/r/Monero) wants to implement them to enable more transactions per block, but they claim that they are that slow, that the nodes would be best off having a GPU to verify the transactions. What is the verification complexity according to you guys? 1~ms sounds very acceptable. (are we by the way talking about the same kind of proofs here? I assumed so by the name "bulletproofs", but yours seems to require *some* interaction, while I think Monero does it NI)
ACID guarantee isn't everything to everyone. If I have to take a 90% performance hit to gain a feature that my application doesn't even depend on, that's just a loss. There isn't any gain. For many applications, an ACID guarantee is important, but performance is also important. I know what features are touted. What I haven't seen is performance comparison.
Report to log_watcher that `watch` expects a function pointer rather than a generic function type.
If I access thr first element of tuple `( u32, u32 )`, will the whole tuple be cached on 64 systems? If I access the second element afterwards, will I have a cache hit? 
Maybe? Recent x86-64s should have 64B cache lines, so it really depends on whether the whole tuple is allocated on an 8 byte boundary or not. Also, it depends on what else the CPU is doing, how long you spend pre-empted, *etc.* Unless you're writing carefully tuned assembly, I wouldn't worry about it. Just try to keep structures as small as you reasonably can, and data as local as you reasonably can.
It sounds like your asking whether this protocol works similarly to Monero? I don't know the details here, but I think the"bulletproof" part just references how they use rust to guarantee correct local behavior according to protocol: "We wanted a foolproof model that would prevent users from performing any step other than the correct next step of the protocol. With this in mind, we modeled each party and dealer state as a distinct type, and used move semantics to enforce state transitions." I don't think it refers to the protocol itself :-)
yes, then you can use it like this: https://github.com/JelteF/derive_more/blob/master/tests/try_into.rs#L1
svgcleaner and SVG stuff author here. What kind of simplification do you need? - *svgcleaner* simplifies the path data notation, not the data itself - *usvg* simplifies the path data by converting all segment to only M, L, C and Z - *lyon* can do flattening, which will *simplify* the path data by converting all the segments to lines - *inkscape* simplifies the path data by removing as much segments as possible, but it isn't lossless (afaik) So can you specify what kind of simplification do you need?
No. Arrow functions always capture `this` from its definition site like a normal variable, it is never re-bound. It really is just a function, you can't use it as a constructor and it will not get a different `this` this if you assign it on an object and call it as a method.
Thanks for your response and the rust SVG stuff brought by you ：） What I need is similar to inkscape. Inkspace is great， but I need it more aggressive to remove overlapping segments.
Wow, that's some awesome response time!
Suggestion: use an empty cell for description instead of "None".
&gt; How this was possible without a link error I am not sure, perhaps there is something odd when calling into a C++ function that has default arguments. My guess is that you are using the C ABI to communicate between C and Rust, which strips all information about the function signature from the symbol, leaving only the bare name of the function.
Me too! Awesome! At least one oft my crates!
When you say out parameter, it makes it sound like you're using the pattern common in languages like C, where you want to return multiple values. If you're mutating an existing value, it's completely fine to take a mutable borrow. 
You can try to contact the original developer and maybe they move the ownership to your account.
&gt; When you say out parameter, it makes it sound like you're using the pattern common in languages like C, where you want to return multiple values. I did not say "out parameter", so I'm not quite sure what to make of that :D But the way the OP uses it seems to be a premier use for a mutable reference (even if in the case of usize it doesn't even matter because copy). Just like you're saying, he's mutating an existing value. 
Hehe, awesome. Now I want to know how their version scales. Pretty sure the Monero devs would be interested, although they use 22519 instead of Ristretto. Shouldn't be an issue though.
\`rust\-geo\` can simplify lines using \[RDP\]\([https://docs.rs/geo/0.8.3/geo/algorithm/simplify/trait.Simplify.html](https://docs.rs/geo/0.8.3/geo/algorithm/simplify/trait.Simplify.html)\) or \[Visvalingham\-Whyatt\]\([https://docs.rs/geo/0.8.3/geo/algorithm/simplifyvw/trait.SimplifyVW.html](https://docs.rs/geo/0.8.3/geo/algorithm/simplifyvw/trait.SimplifyVW.html)\). There's also a \[topology\-preserving\]\([https://docs.rs/geo/0.8.3/geo/algorithm/simplifyvw/trait.SimplifyVWPreserve.html](https://docs.rs/geo/0.8.3/geo/algorithm/simplifyvw/trait.SimplifyVWPreserve.html)\) variant of VW. \(I'm the author\)
I would take anything the Monero people say with a grain of salt. The project is not exactly known for high-quality cryptography engineering, with [critical bugs in its design](https://getmonero.org/2017/05/17/disclosure-of-a-major-bug-in-cryptonote-based-currencies.html) and a [questionable privacy model](https://arxiv.org/pdf/1704.04299.pdf).
I'm trying to build email verification for my website, but I can't get gmail smtp working. Lettre has been evolving quite a lot in a short period of time, so many of the examples are now outdated. Also none of any examples actually use external smtp server like gmail. It's finally compiling, but it's asking for STARTTLS: https://pastebin.com/2yqHRzgn I guess it has something to do with this, but I can't get my head around all this and I have no-one to ask: https://docs.rs/lettre/0.8.2/lettre/smtp/client/net/struct.ClientTlsParameters.html Also some information here: https://lettre.at/ It would be super helpful if there would be even one example that actually compiles and works by sending hello world from somewhere else but localhost (and wouldn't be a huge project with 10+ source files).
I have Vector3&lt;T&gt; where T is a number, and I want to be able to convert a Vecto3&lt;T&gt; to a Vecto3&lt;U&gt; where T is convertible to U. How can I achieve this? For example, I should be able to convert a Vector3&lt;u32&gt; to a Vector3&lt;i64&gt;, and for that matter, it would be nice if I could convert a Vector3&lt;u32&gt; to a Vector3&lt;i32&gt;, and panic if it fails, or get an Option&lt;Vector3&lt;i32&gt;&gt;. The main reason for this, is I want to be able to cross to unsigned Vector3s and get a signed Vector3 as a result. I already tried implementing From&lt;Vector3&lt;T&gt;&gt; trait for Vector3&lt;U&gt; where U: From&lt;T&gt; but it gave a weird compiler error. What is the best way to achieve this? (Here is my current progress: https://github.com/AradiPatrik/mini_renderer/blob/master/src/vector/vector.rs)
Excellent write up. Great to see what Rust and attention to the underlying hardware can achieve.
What happens is that `stream.read_line(&amp;mut buf)` will read a single line from the connection. If the loop is not there, the example will only echo the first line it'd receive, and then close the connection. The `read_line` function is blocking, but this is fine because in `main`, the code calls `thread::spawn` which spawns a seperate thread that runs things in parallel.
[removed]
For someone fighting with the same thing: just use this: https://pastebin.com/ts9npfrt Just remember to leave email part untouched and use smtp.gmail.com in smtptransport!
Nice work. I'd be curious to see how this compares to the [MatrixMultiply](https://github.com/bluss/matrixmultiply) crate which uses the underlying ideas behind BLIS, and if ideas from both of these sets of works can't be combined to give us a really nice Rust native matrix multiply library. I had a few other questions as well. Did you use any underlying BLAS library for Armadillo or was it just using its own implementation? Next, any idea how the algorithms compare FLOPs wise? I'm just asking because I'm used to seeing comparison charts for matrix multiply libraries in terms of FLOPs. It also usually gives me a better idea for how it might compare to MKL or OpenBLAS. 
A question about the Strassen algorithm. If I understand correctly, this algorithm generate less multiplication at the expense of more addition. But, in newish architecture, you can use the FMA instruction to pack an add and a mul. The Strassen algorithm should take longer. No? Did you check that you actuality generate FMA instructions? 
`impl Trait` is shorthand for `&lt;T: Trait&gt;`; it's still a generic function that will monomorphize. The distinction is that `Fn&lt;T: Trait&gt;() -&gt; T` cannot return immediate items as `T` or `Trait`, while `Fn() -&gt; impl Trait` *can* return immediate items that are at least `Trait`.
(FWIW I kind of find myself agreeing with chuecho here; while I like Rust's `std` being curated as a "here's what you need to stand up a program on your machine" in general, I still *really* think an official `ext` or something that is just a re-export façade would be useful for cases like, idk, working inside a room where internet access is a felony)
&gt; [..] but it gave a weird compiler error. If you don't show what the error was, or provide an easily compilable example of the problem, no one can easily help you.
tangentially, given ```u128,i128``` .. is there a chance of getting ```f16``` 
Iron has been around for a long time and become unmaintained only relatively recently.
Yeah, you can do that. The problem is that you cannot `get` several resources in parallel with the current version.
I imagine it could be implemented with a thread pool and atomic pointer swaps. But maybe there's another limitation in `warmy`.
I am not sure what you are trying to do, but [this](https://play.rust-lang.org/?gist=5aa137e25b1a4f60206d039a8e962887&amp;version=stable&amp;mode=debug) might work. It spins up a new thread with a watcher that will write changes to channel. I copied the code from the logwatcher crate. if you need to write to different channels, you need to choose channel without mutating an outer variable. This can be avoided by protecting mutation [with a Mutex](https://play.rust-lang.org/?gist=03bf11982019a9e5b08eeaec2a7027e5&amp;version=stable&amp;mode=debug) or a RefCell.
Yes. Sadly, macros are probably the most direct route to what you want. You could also implement your own trait that is functionally the same as `From`/`TryFrom`, but which you control. Or implement a non-standard method with the behaviour you want. Using `From`/`TryFrom` is canonical, but it's not *required*.
Any references to showing bad or awkward combinations done properly?
Place your mains in `src/bin` such as `src/bin/some_program.rs` then you can build them with cargo build --bin some_program You can then have `src/lib.rs` which they can both use via `extern crate &lt;yourcratename&gt;`;
Nothing too exciting. It's been in beta for almost a year and the API seems to have stabilized without too many complaints!
Thanks so much! :D
That sounds great, thanks! :D
Well this is indeed is sad :(. Thanks for the help Your Holy Macroness :)
I looked at the examples on the main page of documentation and then skimmed the API reference. If looking more deeply I would have found that, but I think it would be useful for users to see what I assume to be the primary use case in the first or second example.
Usually people want to do this to ensure that their build systems can build the project without Internet access, for additional guarantees about reproducible builds.
\**waves hand in an 's' motion, followed by a vertical line*\* May your code be meta, and your macros recursive.
Nice!
It's easy to miss but the key is that it's turning 8 recursive *matrix* multiplications into 7, which is what gives the asymptotic speed-up. Using FMA may helpful but only as a constant-factor speed-up for the base case of the recursion.
&gt; much-needed internal batteries in the future. I would hope that Rust doesn't add domain-specific logic into the standard library. When you have a language which supports modules, you should take advantage of that. A language that wants to stand the test of time will not do things that will make it become a fashion trend.
&gt; they'll require a download and an audit Thankfully with Rust, the compiler does the audit for you. You just need to examine the docs to figure out the API.
You'll need Internet access to install Rust, though. You could just make a Crates mirror with all the crates you want to use internally.
This is a proposal to add `Result`-like error handling to C++. It's got a lot of interesting background and motivation, as well as some interesting choices on the design of `std::error` that might be relevant and/or similar to `failure`.
Oh, I see! It was *matrix* multiplication. Thanks you! 
I haven't compared to anything else but Armadillo ‒ my intention wasn't exactly arms race against another implementation, more like using matrix multiplication as a kind of optimisation case study ‒ it could have been sorting of numbers, or something else. The comparison to Armadillo was just a fast experiment to see I'm not off the charts completely. As for Armadillo ‒ I think it uses its own implementation, but I haven't dug into it ‒ if it found and used something already on my system, I wouldn't have known. Comparing matrix multiplication libraries in terms of FLOPs? How does that work or say anything at all about the implementation? I mean, using strassen algorithm, you'll probably get lower FLOPs (because you do less work overall), but faster implementation. Also, at what size of matrix do you measure it?
I don't provision the SCIF I just work in it :/
It's a really solid crate. Thanks for writing it.
First, I haven't checked the generated assembler (I don't really know assembler). I *guess* this part actually can use it pretty easily, though: https://github.com/vorner/fastmatmult/blob/master/src/simd.rs#L32 And yes, as pointed out, you seem to have gotten confused a bit there (or I haven't written it clearly enough). You speak about the primitive instructions, while Strassen cares about matrix multiplications, on the big matrices. These are asymptotically more expensive than additions (while the primitive ones are constant-factor more expensive than additions). As an intuitive explanation (if you don't know how the asymptotic time complexity works): normal matrix multiplication is 3 nested for cycles, while addition are only 2. Therefore, if I double the side of the matrix, I expect the addition to take roughly 4 times as long, while matrix multiplication 8 times as long (if I ignore all the important details, like caches or costs to start up and finish up the algorithm). So while addition and multiplication can take about the same time on small a matrix, there'll be a difference on large ones, and the bigger the matrix, the bigger the difference. Therefore, if we have a really big matrix, trading one multiplication for several additions on this big matrix pays off. This is somewhat simplified (because with recursive Strassen algorithm, the multiplication is faster than O(n³), so the difference between multiplication and addition is slightly smaller), but I think this helps to build the intuition behind the idea. That O(n^2.81) comes from rigorous mathematics around recurrences… something that I don't find as fun or enlightening, but it can be found on the wikipedia page. In other words, Strassen is about doing smaller amount of operations, not doing them faster ‒ and it does smaller amount of both multiplications and additions.
Good point, I've added them. Though I believe these could be guessed quite easily anyway.
Neat! I added a similar proof of concept parallel Strassen impl to rayon a while ago. I ran the extra additions in parallel at the cost of more allocation. I also used rayon's lower level join API, which I thought was cleaner in this context than parallel iterators. I didn't know what I was doing for the lower level Z order multiplication and I'm sure left a lot to be desired there :) https://github.com/rayon-rs/rayon/blob/master/rayon-demo/src/matmul/mod.rs
The only reasonable scenario I can think of is `impl Fn() -&gt; T` or even `FnOnce`.
There’s some minor discussion but not even an RFC yet. One argument against it is that it reintroduces dynamic dispatch.
&gt; // This method is unsafe for thread-safety reasons, as the callback we provide // is not Send/Synd (it uses an Rc&lt;Cell&lt;&gt;&gt;). But our app is not multi-threaded // here, so this is fine. =) umm, you don't have to mark this function as unsafe? If you try to use an `Rc` across multiple threads, Rust will refuse to compile it, [as `Rc&lt;T&gt;` auto-implements `!Sync` for any `T`](https://doc.rust-lang.org/std/rc/struct.Rc.html#impl-Sync). So there is no reason for this `unsafe` here, unless I'm missing something specific to wayland.
&gt; Someone had upgraded the proprietary DLL without updating the bindings, so we were passing garbage for two new parameters that had been added. How this was possible without a link error I am not sure, perhaps there is something odd when calling into a C++ function that has default arguments. The linker knows nothing about the arguments taken by a function. That's specified in the headers, or in the case of Rust calling into C or C++, the bindings which were either manually written or automatically generated from the headers. The only thing that will cause link errors are if a function doesn't exist that is expected to exist. I guess with C++ name mangling there is type information encoded in the function names, so it's possible for changes to signature to cause linker errors, but in general, you can't trust your linker to catch these kinds of errors, you need to have your declarations correct (whether using the right header or the right Rust declaration) or you could run into undefined behavior. When writing `unsafe` code, even just calling out to a C or C++ library, you should generally audit your own code before assuming that a crash is due to a soundness issue. While there are a few soundness bugs in the Rust compiler, they're generally relatively obscure and hard to hit without trying, while there are many ways to hit unsoundness quite easily in `unsafe` code, or in the C or C++ code it calls.
More 1.0 releases are great! I realize that you probably didn't want to bump the minimum version of Rust that this builds with, but I can't help but notice that the readme examples would benefit from ?-in-main. :)
One of the best advices I have gotten in academia is how people usually read papers: Title -&gt; Abstract -&gt; Figures -&gt; Summary (-&gt; Body, if still interested) Often figures come before abstract. Its kind of funny because its true ;) 
Far as thread safety, the main contender was Eiffel's [SCOOP](https://www.eiffel.org/doc/solutions/Concurrent%20programming%20with%20SCOOP) which has been in production for a while. Someone also did a verification of semantics in Maude. Students at Meyer's university have also done a lot of small projects improving performance, catching deadlocks, etc. The earliest I saw was [Concurrent Pascal](https://en.wikipedia.org/wiki/Concurrent_Pascal). A more recent one is [ParaSail](https://en.wikipedia.org/wiki/ParaSail_(programming_language)) aimed at making parallel programs easier with race freedom. Whereas, Pony is using [different concepts](https://www.ponylang.org/discover/#what-is-pony) to get a lot of safety and performance with some uptake happening. 
[There is something specific to wayland](https://docs.rs/wayland-client/0.20.6/wayland_client/struct.NewProxy.html#method.implement_nonsend), or actually, specific to the fact that the wayland C libs I'm wrapping in wayland-client have not really been design with thread-safety in mind.
I intentionally did not update the examples because the feature is so new. Also, I'm not sure if it's a benefit. AIUI, `?-in-main` emits a debug error message, which isn't actually what you want in a real program... 
I did some digging. The [RFC for `?-in-main`](https://github.com/rust-lang/rfcs/blob/master/text/1937-ques-in-main.md) seems to suggest that the `Display` impl should be used. The tide seems to have shifted toward [using `Debug`](https://github.com/rust-lang/rust/issues/43301#issuecomment-358379109) a few months ago, and it appears that was done because `?-in-main` isn't intended to be used in polished programs. But the whole point of me writing the CSV examples the way I did was to show exactly how it would be used in a *real* program. If I didn't care about that, and instead wanted to let the reader figure out what a polished program looks like, then I wouldn't have even bothered with error handling at all. I would have just used `unwrap`. I guess `?-in-main` looks like it will primarily be useful in doc tests and unit tests, but I don't actually know whether that's supported yet. I can't ever see myself using it in examples unless I'm also OK with using `unwrap`, in which case, `?-in-main` is moot.
The thread pool part is already in my testing playground – with `futures-cpupool`. But it’s not that trivial, yeah, and the `sync`–ing stuff is hard to make cooporate with multithreading.
Okay, I guess I missed the point in your post when you were talking about this being an optimization case study. I would say that's a fair point to not comparing against other libraries as much. I meant FLOP/s earlier, so yes I would agree that FLOPs wouldn't make much sense to compare against. I'd say one of the nice things about doing it this way is that by measuring it over quite a few different size matrices you can see at what point your algorithm/program saturates over certain sized matrices. It also can show you whether or not your thrashing your cache. You already know that a matrix-matrix multiplication should take [2n^3 operations](http://cornell-cs5220-f15.github.io/slides/2015-09-08-matmul.html#/11). You'd time a number of runs of the program. If the total run time is below an already set time then you can increase your number of runs until you at or above the set time. I would say you could do this on a number of different hardware if you'd like, but it's not like these measure carry over to different platforms. Since, each platform has its own peak performance. The MatrixMultiply crate - I know it uses a lot of auto-vectorization for its micro-kernels. So, I'm curious how much using explicit SIMD calls might help it. 
You can't implement traits you do not own for types you do not own. And you can't implement traits you do not own for arbitrary types either, since that arbitrary type might be one you do not own. Both `Try` and `IntoHandlerError` do not belong to you, so I believe you'll need a little boilerplate to convert things you do not own into something you do own and then implement `Try` for that. Here's an outline of what I'm thinking, though I haven't followed it through to make a working example.. // a type you own struct HTry&lt;T&gt;(t: T) trait HTry { fn htry(self) -&gt; HTry&lt;Self&gt;; } // easily converted to even for things you do not own impl &lt;T&gt; HTry for T { fn htry(self) -&gt; HTry&lt;Self&gt; { HTry(self) } } // generically implement Try for HTry with your desired conversions impl &lt;T, E&gt; Try for HTry&lt;Result&lt;T, E&gt;&gt; where E: IntoHandlerError { // appropriate impl here } This should allow you to call `result.htry()?`.
Well, FLOP/s doesn't help much either ‒ as I said, even if I get lower FLOP/s with Strassen algorithm, it can be overall faster by requiring less FLOPs in total. If I just divide the speed by the 2n³ (and not measure what the *actual* FLOP/s were), I only make the line in the graph horizontal instead of going up. As a means for seeing where I trash the cache ‒ I can see it from how long the algorithm takes as well :-). Anyway, `perf` gives me much finer insight into that thing. I haven't highlighted that in the writeup much, but this is what I can get out of it, for example: https://github.com/vorner/fastmatmult/blob/master/presentation/presentation.md#perf This one nicely says where my performance problem is. If I wanted, I could trace that to specific lines of the code. And one that says I've fixed it (this got run on a larger matrix): https://github.com/vorner/fastmatmult/blob/master/presentation/presentation.md#perf
Why not use a memory mapping for the buffer? Since it's a temp file that only exists in memory it should be able to avoid *any syscalls at all* (unless it gets paged out), unless Wayland doesn't support this for some reason.
The link regarding the design bug is interesting, but seems to have been fixed in monero. Crazy to think that a security hole was hiding in plain sight in a pull request that fixed it only in one coin. Do you know if anyone ever used this attack successfully?
Just wanted to know that with the introduction of `impl Trait` on rust 1.27 , how will the compiler benefit (faster build time, less memory usage .... etc) from this point forward ? Can somebody from the rustc compiler team shed some lights on it ? 
Ah, I'd missed that addition. Still, it doesn't help in cases where the conditionally present traits are _required_, right? But at least with Iterators, those conditional traits tend to just be optimizations.
One danger I see in this is: Crate A is made to work with old compilers while also providing new feature for new versions. Crate B depends on crate A and accidentally uses the feature without realizing implications, because the author simply used the new compiler. The attempt to use crate B with old compiler fails.
Reading this makes me happy to be using result/option! What a nightmare. 
FWIW, it's already possible to re-implement `Result` and `Option` in C++ as well as `TRY()` macro (working exactly as `try!()` in Rust) with compiler extension to allow returning values from macros.
Now that Tokio has an abstraction for asynchronous filesystem IO, should I ever default to std::fs::File anymore? 
Ouch, so this isn't as useful for the CLI-WG as we thought it would be :(.
I think it was used in some shitcoins. (Bytecoin, if I remember correctly.)
&gt; I'm an idiot. Bulletproof is actually the name of the protocol, not just the first word of the title. Your comment is accurate. I was confused at first as well even though I knew bulletproofs is a name of a protocol.
I'm working on code that does this * reads lines from multiple inputs (stdin, tcpsockets, files) * for each input, send a line to a processing thread in a round robin fasion * each processing thread writes to multiple outputs I'm using the fact that you can clone txs to send clones to multiple threads. Do you know the reason for the compiler error re: the closure/function above?
&gt; to show exactly how it would be used in a real program Thanks for this by the way, makes it much more concrete seeing how to use it in practice.
Some people work for organisations who would object to that position. In particular ones which, if informed that the programmer's intention was to make use of external code that they had not personally read and could vouch for, would consider that a career-ending confession. Some of these are organisations who believe it is acceptable or defensible for a publicly funded entity to possess information not available at will to any member of the public, so they're not exactly known for a sense of humour or proportionality.
I wouldn't consider that to be composing more easily; whether you're awaiting the result of a try or trying the result of an await is an important semantic difference and it's very strange that those two orderings produce equivalent behavior. A language should strive to make order of operations obvious, rather than forcing users to remember to parenthesize everywhere for clarity.
&gt; Nothing too exciting. For better or worse, how many libraries have reached 1.0 is often used as a proxy to measure the maturity of the ecosystem. Having 1.0 regex and csv libraries is a small step for those libraries, but a giant leap for the perceived maturity of the Rust ecosystem :)
It's not strange at all, given how `try` works in C++. You don't `try` a value, you `try` an operation- the errors are on a side channel, not part of a value-level `Result` object.
Not sure if anything will come of this, but I did my best to organize my thoughts here: https://github.com/rust-lang/rust/issues/43301#issuecomment-388575730
Agreed. I also pushed out docopt 1.0 today! I think at this point, my most popular crates are all pretty much at 1.0. Feels good! Still have lots that aren't at 1.0, but they are more niche. :)
I didn't think too much about it -- `obj` seems plenty fast enough for such a tiny code snippet. Maybe `llvm-ir` would be slightly faster. You're right that `metadata` support is limited, but of course such old compilers won't support the feature we're probing anyway. 
And their core developers are also hostile to criticism of any form; claiming academic research is "FUD" trying to detail Monero.
No, the error is weird, but one problem I see is that you are trying to mutate the captured variable “iter” within a Fn closure, this is not allowed. See my second example.
 I see what’s going on, the crate is outdated. The owner have not published the [new API that takes a closure instead of a function](https://github.com/aravindavk/logwatcher/commit/c08f460021e3deb75b8a57c940499e1556d13516). Try to get him to fix it, in the meantime just copy his newest code into your code. 
Well, this is weirder. The code I have in the crate I downloaded is different from this: https://github.com/aravindavk/logwatcher/blob/master/src/lib.rs My version of the code says this: ~/.cargo/registry/src/github.com-1ecc6299db9ec823/logwatcher-0.1.0/src: pub fn watch(&amp;mut self, callback: fn (line: String)) { loop{ [...] Is the crate at crates.io out of date?
Or `inner!(rule, if Rule::Number)` with the [inner](https://crates.io/crates/inner) crate.
yes, [I noticed that too](https://www.reddit.com/r/rust/comments/8is38t/anonymous_function_turns_into_closure/dyvhxnu/).
Will do, thanks for your help. 
I love this crate. Great ergonomics and performance, with several different tradeoffs you can make. The only complaint/feature request that I have is atomic line-writing. I had incorrectly assumed that one `writer.serialize()` would result in a single `fwrite` so that I could share the writer across threads. Obviously I should not have assumed that given it's signature, etc. I ended up just having a writer thread with my consumers sending lines to the thread, but it'd still be nice to be able to share a writer directly.
Ohh that makes sense! thank you!
I'm not sure what you mean here. `try` in C++ is always followed by a braced block, unless there's some more exotic use that I've never seen. From the above two examples that you've given, I would intuitively assume that something similar to C/C++'s `if`-statement brace elision is in play, where `co_await try foo()` is sugar for `co_await try { foo() }`, and `try co_await foo()` is sugar for `try { co_await foo() }`. In Rust, the equivalent to the former would be `await!(foo()?)` and the latter would be `await!(foo())?`.
I ended up cloning the repo and changing the callback to FnMut(), and changing the borrow of the closure to &amp;mut ||, and it compiles :) Thanks, I learned some stuff - love it when that happens. 
&gt; Note that seeking is performed based only on the byte offset in the given position. Namely, the record or line numbers in the position may be incorrect, but this will cause any future position generated by this CSV reader to be similarly incorrect. What is this referring to? The API does not appear to expose anything related to record or line numbers.
This is just my pure opinion and rant. The world would be a better place if, people stopped adding more features to C++, and focused on better interfacing to other languages, stabilizing the ABI. If one were to start from scratch, nobody should pick C++ anymore.
Your braced versions are correct, and they showcase exactly what I mean. `co_await try foo()` means "call `foo`, propagate any exception that produces, then await its result." `try co_await foo()` means "call `foo`, await its result, and propagate any exception that produces." C++ has a separate "exception channel" instead of always materializing a `Result` in the "value channel," so `try` propagates any exception produced *anywhere* in the block/expression it's applied to- not just in its outermost/final position. That is, this propagates exceptions thrown by *any* of these function calls: try { int x = foo(); int y = bar(x); baz(y); } Coming from the other direction, it's as if Rust had an operator for "apply `?` to all `Result`-returning function calls in this block."
Frankly, that's precisely what they're doing here. This is a different implementation style for exceptions that makes it easier to interop with C through a stable ABI. The proposal discusses this.
It is referring to a Position, whivh can be retrieved from a Reader or either of the record types: https://docs.rs/csv/1.0.0/csv/struct.Position.html
Is there a way to ensure at compile time that no `println!` is used? I've been bitten by this again and again, but I'd be damned if I didn't need to spend an hour debugging before it dawns on me what's happening...
Thank you for sharing your opinion. After reading your comment, I spent some time thinking about how much value the community has and how much I appreciate the Rust community. Anything that helps grow the community is great. I agree with you. My previous comment shared a very naive viewpoint, which I can see how bad it can be if taken to the extreme. You kinda changed my mind. :) I used to be into Common Lisp when I was a teenager. I was really obsessed with the language after reading things like Paul Graham's essays. I stopped using it, because, while it is theoretically a great language, it is fairly useless in practice, because there aren't many libraries and the ecosystem around it is limited. Really shows how important the health of the community is. Although, I still dislike `impl Trait` in argument position. As I said in another comment, I disagree even with the learnability argument. Anyone learning the language still has to learn the old syntax, simply because there are so many common things that `impl Trait` cannot do. `impl Trait` syntax is only useful in very simple cases. A lot of code is going to keep using the old syntax, simply because it is *better*. And even if the new syntax was perfect and everyone switched to it, old code written in the old syntax will continue to exist anyway. This means that newbies now have to learn 2 syntaxes instead of 1. They still have to learn everything as before, but now they also have an extra thing to learn too, which isn't even that useful in practice, but it exists, so you have to know it. So no, `impl Trait` does not improve learnability, at least IMO.
I think I agree with you that impl Trait in argument position is a bit weird and hurts lernability by adding two ways to do something. I just wanted to address the whole "useability for power users matters most" thing.
Interesting! How did you even get into that situation though? The writer requires a mutable borrow, so you should never actually be able to observe non atomic writes. You could also just throw the writer in an Arc&lt;Mutex&lt;...&gt;&gt; in order to share it across threads, which would also of course give you atomic writes. But yeah, having a writer thread is also a pretty standard approach too!
Hello, sorry for hijacking a thread. I remember that some time ago you did some tests with hyper against your private web server. I've just noticed that in recent version 0.11.26 they added a [run_threads](https://docs.rs/hyper/0.11.26/hyper/server/struct.Server.html#method.run_threads) function to start reactors on multiple threads. Now I'm curios if this is ok solution, or would it still be slower than the code you showed at the end of your post.
Aha, I wasn't considering that the intention was for the error path of `try` to bypass `co_await` via the exception channel. But it sounds like the equivalence isn't due to any context-like nature of `try`, but to the existence of the exception channel. In other words, if `?` in Rust returned errors via a dedicated channel, then `await!(foo()?)` and `await!(foo())?` would have the same semantics as in Herb's proposal, regardless of the existence of any sort of "block-scoped-`?`". Conversely, if a block-scoped `???` operator existed, then `??? { await!(foo()) }` and `await!(??? { foo() })` would still have different semantics from Herb's proposal because of the lack of an exception channel.
I mean, sure, the exception channel how C++ accomplishes it, but your `???` operator would totally work the same way as Herb's proposal. As I said, it would just need to "apply `?` to all `Result`-returning function calls in its block." For example, `??? { await!(foo()) }` would become `{ await!(foo()?) }`, and `await!(??? { foo() })` would become `await!({ foo()? })`.
&lt;nods head&gt; yes...I see. Yup. I know some of these words. =P
There's a [clippy lint](https://rust-lang-nursery.github.io/rust-clippy/master/index.html#print_stdout) for that.
I was cloning the file handle (which is theoretically safe) because sharing the writer didn't work. That should have been my first clue :) Still, I kept assuming that it'd end up with just a single `fwrite` for some reason. Yea, I wanted to avoid lock contention since I was using rayon to read some 80 million files and write results based on that to ~6 files. 
Boy oh boy my C++ fanboy coworker is gonna be so pissed, I've been selling him errors as return values for the past month or so and have been met with biased criticism every time.
Oh I see. Interesting. In that case, it's probably not because of how `write` is called, but because writes to the CSV writer are buffered. Once the buffer fills up, one `write` syscall _should_ be used to push that buffer to the underlying writer. ([`write_all` is used](https://doc.rust-lang.org/std/io/trait.Write.html#method.write_all), which can technically call `write` many times, but in practice probably won't.) The buffer is what will defeat you here. You can't reliably predict when it's going to get flushed, which might happen in the middle of a record. If another writer gets flushed around the same time, then you'll wind up with gobbledygook. I'm pretty sure you'd wind up with the same failure mode with `fwrite`, unless you [set the buffering mode to line buffering](http://man7.org/linux/man-pages/man3/setbuf.3.html), and even then, that only works if every record you write spans exactly one line. If you meant `write` on the other hand, which does no buffering at all, then yeah, I think this would work as you'd expect. But! If line buffering is acceptable to you, then you can actually do this with the csv crate because you can tell the writer to [`flush`](https://docs.rs/csv/1.0.0/csv/struct.Writer.html#method.flush) at any point. So if you have multiple `Writer`s each pointing to the same file, then wtr.serialize(...).unwrap(); wtr.flush().unwrap(); should basically achieve what you want (with no rock solid guarantees), *at the cost of losing the performance advantages of buffering*. But that's pretty much intrinsic to your stated desire. That cost could be quite steep, so much so, that using an `Arc&lt;Mutex&lt;...&gt;&gt;` or a writer thread would be faster.
Oh okay I see, I thought our await macro was making an implicit closure which made it impossible to return from inside the await call, I had to go back and re-read the RFC to see that my mental model was wrong (I mean I think... the RFC is lacking in implementation detail). But sadly I think that means that I disagree just as much as ever, because I don't see where the problem with composition actually is in your mind? What doesn't compose?
By ABI performance benefits, is this referring to constructing the result as an outparam? I thought Rust already did that; AFAIK it's the secret first parameter to every Rust function (or was, when I last checked years and years ago).
Is there a way to (in effect) break out of a match block? I can't find anything on it except [this thing](https://doc.rust-lang.org/book/second-edition/ch09-02-recoverable-errors-with-result.html) in the rust book, with the `let f = match...` notation. Effectively, I have a big nested match block, and in just one of the cases I want to have it move on to handling more lines after the main block. match self.read_byte(true) { Ok(byte) =&gt; { match byte { SOH =&gt; // line 4 EOT =&gt; { match self.write_byte(NAK) { &lt;big code block&gt; } }, _ =&gt; Err(..) } } Err(e) =&gt; Err(e) } // line 15 In the above, I want to "break" from line 4 to line 15. Naively, I could just write a function for line 15+'s work, and call it at line 4, but I'll be doing this a lot. From the link, it seems like they're suggesting that I use `let` assignment on the match block, but this would mean converting all the `Ok`/`Err` return expressions into explicit return statements, like this: let foo : bool = match self.read_byte(true) { Ok(byte) =&gt; { match byte { SOH =&gt; true // line 4 EOT =&gt; { match self.write_byte(NAK) { &lt;big code block&gt; } }, _ =&gt; {return Err(..);} } } Err(e) =&gt; {return Err(e);} } // line 15: ignore foo from here on That gives a code smell, though. Am I missing a more sensible/standard option?
The paper proposes 1) adding more registers to the ABI, so the entire result can go there instead of as an out parameter and 2) storing the discriminant as a CPU flag, so it can be more easily branched on.
I barely know enough to help you but you are going to need to give a lot more information to have a chance to get your problem fixed. What is depending on ring 1.12.1? How far have you gone down the dependency path and the build scripts, and what have you found? --- stderr thread '&lt;unnamed&gt;' panicked at 'execution failed', C:\Users\tomer\.cargo\registry\src\github.com-1ecc6299db9ec823\ring-0.12.1\build.rs:636:9 note: Run with `RUST_BACKTRACE=1` for a backtrace. thread '&lt;unnamed&gt;' panicked at 'execution failed', C:\U run it with `RUST_BACKTRACE=1`
You could make your configuration its own crate in the same directory and point to it using path in Cargo.toml. I think that is overkill though, so I recommend not worrying too much about it. serde is something most people are pulling into their top level module these days due to always needing serialization.
All arms must return the same type, if non of the arms return anything you could do this `SOH =&gt; ()`.
But I do want to have most of the arms return something.
Then you might need use a enum, Option or Result so that all arms has the same type. Also, while all match arms must have same type, if you return out of the function you can return another type. fn foo(a: u8) -&gt; Option&lt;u8&gt; { let c = match a { 0 =&gt; return None; 10 =&gt; 100, b =&gt; b, }; Some(b + b) } here c is u8
That's what I'm doing in my second example (though I didn't know I could leave the `{}` off the return statements in the match blocks.
Awesome! Thanks!
Sounds like a weirdo c++ guy. When things are better, they are just better. Any c++ guys worth their salt should generally know that the language isn't perfect and that it is currently trying to improve itself... sometimes painfully. If someone is so set in their mode that they can't acknowledge that rust among others has better elements, then they aren't much of a c++ guy. 
&gt;I'm not sure this can be fixed, even if we convinced the lang team that `Debug` was not the right choice here. Thought experiment: in what way could someone be relying on the `Debug` behavior that it would break if they changed it to `Display`? I can definitely see how it could upset *expectations* \(regardless of how reasonable those expectations may have been\), but *behavior*? I'd like hear some possible scenarios for that.
I'm now probing two features, and anticipating more in the future, so I'm thinking speed could start to matter. But you might be right that it doesn't make any difference when there's no code to generate. I can't measure a significant difference.
Also, it seems the code smell was mostly avoidable -- you don't need all those `Err(e) -&gt; return Err(e);`s if you use the `match...?` idiom.
Does not work with MSVC though unfortunately.
Yea, after I saw the tearing in the result file I looked at the source and saw multiple calls to write methods and signs of caching and knew both would be problems haha I cannot find why I thought `fwrite` is atomic. I must be confusing it with `write`, like you say, which is only atomic on threads in the same process for POSIX compliant filesystems, apparently. Calling flush everytime seems like it wouldn't work without locks still, right? Each line was tiny, serializing two integer IDs. I'm glad I picked the right path :) though now I'm curious what the performance difference would be. I was under the impression that, despite the atomicity guarantees, that there was some kernel level IO buffering. Then userspace buffering wouldn't be much better than serial calls to `write` if it was in vDSO? (Is `write` in vDSO? Did vDSO get killed in the KPTI patches?) I've not had enough experience in this area (or with sufficient performance requirements) to have a good intuition for this yet.
I think this is normal. It certainly is normal for Travis CI.
You might want to build your own image with pre-downloaded dependencies, then reference this built image either in dockerfile ou gitlab-ci. You can test your setup in a brand new environment, if you use your setup in a new machine, it will have no choice to redownload everything. Same thing happens for gitlab-ci, no cache is kept. Now your issue is more about Docker and less about Rust, you might want to seek help to docker community.
I would be careful with which directories you cache 
I would be careful with which directories you cache, and when/how the cache is invalidated 
My gosh, why did no one tell me about this before?
If I have a tuple `(u32, u32)`, can I convert it to `u64` for zero cost?
You can define which files should be [cached] (https://docs.gitlab.com/ce/ci/yaml/#cache). I would suggest to just cache the "target" directory.
Your mininum version is probably going to have to be 1.20, since that's what latest bitflags requires, and bitflags is a transitive dependency of so much of the ecosystem (see Most Downloaded on https://crates.io).
They may be a "C++ guy", but they aren't much of a programmer. I use mainly C++ at work. I like some of it, hate some of it. Same with Rust for my hobby projects. Or Python. And Bash... no wait, that one can go in a dark corner and stay there.
:(
Ah, true.
Some Rust bindings for VirusTotal/Yara, a pattern matching library. My first FFI project! (and maybe the first crate I will publish on crates.io, if the result is good enough)
Uhh that's helpfull, thanks!
`std::mem::transmute` and `std::mem::transmute_copy` are what you're looking for. Note: these functions are unsafe, also check big endian vs small endian beforehand
I think `vDSO`s only work for simple functions that read stuff, like `gettimeofday`. If you're worried about the serialization time, I wonder if it might be better to serialize to memory buffers, then flush them out to disk from a single writer thread. There's probably a smart buffering scheme where you've got a fixed number of buffers that you pass to the disk writing thread then back, to avoid allocations and handle backpressure.
This feels like I'm totally missing something... I've got my code condensed to this: https://play.rust-lang.org/?gist=4ef473af70a642c3e32636c3b04221df&amp;version=stable&amp;mode=debug (I hope it's really a representation of my problem, the real code is of course somewhat more complicated). Now, the functions `takes_ref` and `what_here` are very similar semantically, so I'd really want them to have the same signature. I can make `takes_ref` take the `T` by value, but then clippy complains, and I agree that taking a reference is the fitting thing to do (it's used read-only anyways for sure). So, I'd like to make `what_here` take an `&amp;T` instead of `T`. I can't do it. The problem is that I need to keep `res` alive until the next loop iteration, but the value returned from `takes_ref` is inside the loop so I need to save it in a variable that outlives the loop... no matter what combination I try, no dice :( Thanks for any pointers!
If something is unsafe it should always be marked as such
Not `noop` but close enough. use std::mem; pub fn test_transmute(a: u64) -&gt; (u32, u32) { unsafe {mem::transmute::&lt;u64, (u32, u32)&gt;(a)} } ```
Thank you so much for all your work!
Yes, absolutely. I meant that I want if possible to better handle the underlying C lib so that I can provide the same functionality, but in a safe way. :)
My comment was aimed about 1 level higher, sorry about the confusion
Is this not the sort of transformation that LLVM could/would/should do automatically if it's profitable...? (IANALLVME!)
Yon van look at this [topic on the Rust users forum](https://users.rust-lang.org/t/my-gitlab-config-docs-tests/16396). In particular, you can see what I have done [here](https://gitlab.com/dzamlo/gitlab-ci-rust-test).
AIUI, the intention is to have most of this straightened out for the 2018 edition, which I think means "later this year."
That's okay though? The buffers aren't shared across threads. If we satisfy all the constraints in my comment, then in your example, thread1 doesn't write anything until the flush call, and thread2 doesn't write anything at all.
`Error` inherits from both `Debug` and `Display` (see https://doc.rust-lang.org/std/error/trait.Error.html), so, assuming the type is checked to implement `Error`, switching between them should be okay from a compilation-error perspective.
No clue. It's been quite a while since then so I assume it's working way better now (I stopped working on this server a year ago)...
No: each thread has a different `wtr` and a different buffer, so it's just the `flush`s (i.e. dumping each thread's buffer) between threads that interact, via the file on disc.
I'm not a kernel hacker, so my knowledge doesn't quite extend beyond the syscall. However, AIUI, the point of buffering is to amortize syscalls themselves, which can have significant overhead due to context switching and all that. It definitely isn't about amortizing physical disk writes, since I do believe you are correct about the kernel handling a buffer for that (where the buffer is essentially the page cache, AIUI). Either way, I think my suggested strategy would work. You won't have a deterministic order (but that's true probably even with your writer thread approach unless you went out of your way to enforce an order), but it should not need locks and should not result in interleaving records (assuming my constraints above about record size and implementation details of write_all). To do this, you would need to open your file and clone it for each thread. Each thread would then need to create their own csv writer. My guess is that if you tried it, the sys time on your process will shoot way up and you will be very unhappy. :)
It's not just about output, but it's an actual breaking change at the code level. Today you can return Result&lt;(), T&gt; from main where T impls Debug and not Display. Switching to Display would break that code. I'm not saying there isn't a way at all... Maybe specialization can help, or maybe there is something else that can be done. But that's beyond my paygrade. :)
I'm not worried. Haha just theory crafting. It was a rewrite of something that was going to take nearly two weeks. Mine took 20 minutes. When I added some periodic print statements into the writing threads, the buffers were between zero and a couple hundred, so the producers (the reading from millions of files) were the real limiting factor. Yea, you're probably right about not needing the CSV crate for two IDs. :) But I already had it for reading the other stuff and most of it (hopefully) gets compiled away.
Maybe? AFAIK clippy is okay with having lints that not everyone agrees with since folks can just disable them. But what you say does sound like what my decision procedure will be going forward. Internally though, impl trait sounds great. :)
The idea with vDSO is that the syscall is in userspace so no context switch is needed. Though, as others have mentioned, `write` and co probably don't make the cut for that. I think at one point I did that and was just unhappy with the code to handle 'get or create a file handle' atomically between threads. That might have been right before I switched to spinning up dedicated writer threads at the beginning.
I think the point is that for examples it lets you see what's wrong when it errors as it's already expected you know the basic error handling platters yo just need to know what to react too. It's very much part of the development process as a tool to use before polish. 
[futures-compat](https://github.com/seanmonstar/futures-compat) seems like a project that should solve your current problem. The last proposed roadmap regarding futures [by aturon can be found here](https://github.com/rust-lang/rfcs/pull/2418#issuecomment-386964959). Overall, it seems that the plan is to wait for `futures` `v0.3`. So in my understanding, most libraries are sticking to `futures` `v0.1`for now.
Yes, but like I said, if I accept that, then I might as well just use unwrap in all my examples. I also disagree that showing the Debug if errors is part of the development process. It certainly isn't part of mine. The first time you see the Debug of an io::Error, you are going to immediately wonder how to show a human readable message instead. Even as developer, you aren't going to understand what io::Error occurred just from its Debug message.
Okay, [issue filed](https://github.com/rust-lang-nursery/rust-clippy/issues/2756). 
A simple thing. Learn all functions available on [`Option`](https://doc.rust-lang.org/std/option/enum.Option.html), [`Result`](https://doc.rust-lang.org/std/result/enum.Result.html) and [`Iterator`](https://doc.rust-lang.org/std/iter/trait.Iterator.html) by reading every function a 100 times. That really helps you building amazingly concise function chains on these types.
And all are awesome! 
This sounds great. I've been watching from the sidelines to see how the embedded tooling side develops, and your posts have been most of my reading material! I've done a few simple blinken-lights projects with rust and STM32F4, but nothing serious yet. I'm increasingly sure of a rust based project in the next year.
Just an FYI, ADT stands for algebraic data type in the context of enums.
The problem seems more to do with type mismatch than lifetimes. Since t is the only non-owning reference used, just call `takes_ref` with t once before the loop begins. https://play.rust-lang.org/?gist=5c3176c75e5525f3a5bee0473633b7f0&amp;version=stable&amp;mode=debug
One more question: Does Rust will support `async/await` feature without extra crates (namely `futures`)? I really really want to reduce extra dependencies in my program, one Tokio is enough for me.
I would not recommend MVC for Snake. Don't get me wrong: I heartily recommend separation of concerns, but not into MVC for a small game. While I didn't look into the details, the only part that you will probably extend will be the gamepad support and the display hardware. That's however only my humble opinion, the [topic is highly controversial](https://gamedev.stackexchange.com/questions/3426/why-are-mvc-tdd-not-employed-more-in-game-architecture). ------- -------------- Apart from that, I would remove some of the superfluous `println!` in your code, as a `"Hello World"` might surprise a new player ;).
&gt; My viewpoint is that error handling is irrelevant to examples I think we are unfortunately at opposite ends of the spectrum here then. I wrote those CSV examples a year ago in exactly the way I did because I cared about showing examples that were representative of real programs. I don't use Rust for network services, but even when I use it for programs that are intended to be used by exactly one person (me), I would never even dream of printing the `Debug` of the error. I've definitely done it a few times accidentally, and as soon as I do something with my program that causes an error to be printed, the first thing I do is fix the fact that I printed an error message that I cannot easily understand.
What we do at work (albeit for a cpp project) is to create an image cached on the host. The dind thing mentioned on the gitlab doc is, imo, an over engineered and not even working solution. Have a dockerfile, set your yml to run it. Build the image locally on the machine (more secure since you run the image from the host and not from another container).
Why write a new macro when you could use the *general* [`named_block` macro](https://crates.io/crates/named-block)? 🙃 
"Oh, yeer. Yer macro system's busted. The, uh, the... \**sound of pages turned*\* *scranson's* right buggered. Honestly, yer better off just replacing the thing. "An' it jus' so 'appens I've got this cousin, see, who can get ye a *real* solid deal on a only *slightly used* second-hand one..." &amp;nbsp; ^^^^^^Disclaimer:&amp;nbsp;I&amp;nbsp;have&amp;nbsp;*no*&amp;nbsp;idea&amp;nbsp;what&amp;nbsp;accent&amp;nbsp;I&amp;nbsp;was&amp;nbsp;doing&amp;nbsp;there.&amp;nbsp;&amp;nbsp;*None.*
Very easy after all :D Thanks a lot!
Probably! I'm not sure how much inter-function stuff LLVM does though. I think it usually only does that kind of stuff within a single post-inlining function?
Async/await are becoming keywords and a compiler transformation of the functions the apply to. They are targetting the 2018 edition. This will stabilize a minimal future trait that does not include combinatorial, eg map nor does it include an event loop. Most projects will likely still need to include tokio and some sort of futures library. 
:-) I think the most interesting future work on this crate involves the following (all of which I think have issues): * Improving automatic serialization/deserialization. I think the recent "flatten" feature in serde solves a big piece of this. * Make handling maps easier. When you don't care about speed and don't care about the specific record format, it would be nice to get each record as a `Map&lt;String, String&gt;`. You can do this today for deserialization into a `HashMap`, but you can't serialize it for $reasons, mostly around choosing a defined order. Methinks the right way to solve this is to add a new concrete `MapRecord` type or something. * Add a "strict" parsing mode that fails if CSV data isn't valid according to RFC 4180. The utility here is somewhat suspect, but I've heard a few folks asking for it. I don't think any of these things will lead to breaking changes. I think the only possible future Rust feature that could *possibly* change how the CSV API works today is if we get ATC (associated type constructors). But that's very much a guess and is probably years down the road at this point!
I concur.
I love where this is going, and I hope that I can incorporate it into a future learning project.
I also come from a c background and I think I know what you mean. C hackers are often not as snobby about language things (I'm a recovering Haskell programmer so I have experience with that too)
Non Rust suggestions: I suggest to include "How to run" in the code itself (i.e. improve error message - I looked at the code first - and was wondering what device could be). `Rustfmt` :)
Your work is literally amazing! I wish someday AVR will get some love too
AFAICS this is the log file https://buildd.debian.org/status/fetch.php?pkg=rustc&amp;arch=s390x&amp;ver=1.25.0%2Bdfsg1-1~exp2&amp;stamp=1523059805&amp;raw=0
If you just need a timer, Tokio has an [implementation](https://tokio.rs/blog/2018-03-timers/) you can use.
Might I then recommend https://crate-ci.github.io/
As you figure this out (or anyone else familiar with it), please send a PR to document it at https://crate-ci.github.io/ I'm trying to centralize knowledge on these sort of things so people don't have to hunt for posts on the topic and figure out how to synthesize conflicting recommendations.
I mentioned the approach of putting a function call at line 4: &gt;&gt; Naively, I could just write a function for line 15+'s work, and call it at line 4, but I'll be doing this a lot. Combining the matches doesn’t do anything about one of them still needing to break. 
I think there should be another condition for structs to have this trait - that all the fields will be `pub`. Structs with private fields often have some invariants they hold, and guarantee those invariants - at least for safe code - by only allowing their private fields to be touched by module code that respects these invariants. If we put `ArbitraryBytesSafe` on such types, it will be possible to create invalid values of them without using `unsafe`. This may not break memory safety, but it will still harm the soundness of the type system.
Nice! When will _that_ be stabilized? impl traits are nice, but sometimes I *want* to name the struct I return (and give it additional methods, etc.), and yet have it be callable.
They are mostly a part of concepts (i.e. C++20 tentatively), see "Abbreviated templates" [here](http://en.cppreference.com/w/cpp/language/constraints). On "stable" C++ this shortcut (`void my_fn(auto x, auto y) { ... }`) can be used only in closures, but behavior with regards to providing generic arguments is the same.
Thought it was a shared buffer :)
&gt; Be strict in what you accept, and strict in what you output. I've always heard be liberal in what you accept and conservative in what you output. But I find that leads to maddeningly (and needlessly) complex parsing layers. For a networking layer you really only have two to four types of inputs you need to accept, and that depends on what you're doing: ipv4, ipv6, mac or domain name each with an optional (aka implicit) port. Maybe you deal with username and password in the identifier. I saw a post on another subreddit (I think it was PHP) where someone was talking about how you should accept first name, last name or full name and just map it to your data layout. First of all, no names are more complicated than that. Second of all, no the user can send me data in the format I expect instead of me guessing at it. Guessing at what users want is like going to China and speaking only Spanish to everyone. You might come across someone that understands it but I'd expect better results speaking Chinese. 
I agree the use\-case of the library is perfectly fine, it's just the "trust" thing that got me writing about the more general case.
What is the recommended way to generate Rust code? Let's say I want to declare a variable of type u32, with name "x" and initialise it to 17. How do I procedurally to from those data to the code `let x: u32 = 17`?
Put `Sized` only where it is needed. `split` method.
Why not manually implement `Debug` as something more appealing to you?
Either you can generate the code with a macro or you just write it out in your build.rs
Makes sense, thanks !
Even that doesn't work. I'm pretty sure `bool` is either `0` or `1`, and never `23`. Likewise for enums, they have a hidden tag (usually). By allowing conversions from raw bytes, you may end up with an invalid tag.
It's fast AF :-)
I am using it to write services that can meet the highest possible standards for safety and performance. It is too soon for me to tell you whether that has come true. Will see how production goes.
I just like a change in pace from Java, that I use for my job :)
See: - https://www.reddit.com/r/rust/comments/79om8y/why_rust/ - https://www.reddit.com/r/rust/comments/4l44z3/why_should_i_use_rust/ Both of these have some great answers which I agree with in my reasons for using rust. Also possibly applicable: - https://www.reddit.com/r/rust/comments/6ppptq/what_type_of_applications_are_you_using_rust_for/ - https://www.reddit.com/r/rust/comments/7ouqcy/what_should_rust_not_be_used_for/
How does enterprises think about using Rust? Is it mature enough for enterprise grade software?
Because it exists. No seriously, why better reason to use something than creative curiosity, right? The learning curve is steep (for me at least) but it feels great to wrestle with the compiler aimlessly before releasing the *exact* reason why something isn't working - that kind of learning is very rewarding IMO.
You're right, which is why it's not an auto trait like `Send` or `Sync`. The author of the type has to explicitly decide to impl or derive the trait.
At work, for reliable agents for which the cost of patching is prohibitive. At home, I'm making a game and I'm not a native code guru, saves me a lot of pain :) 
I dable on and off because it has a large number of features I want; I like the functional flavour whilst still being multi-paradigm. I hold hope that it could make collaborative programming less painful than C++.
Hmm, I was saying to put the match statement in a function, not the code after it, since you can return from a function anywhere. But maybe I didn't quite understand what you want to do? In your first example, if `self.read_byte` returns `Ok(SOH)`, then it will execute whatever code is on line 4 and then go right to line 15, because none of the other cases in the inner or outer match applies. So... unless you have more code in the `Ok(byte)` branch after the inner `match`, I don't see why there's an extra need to break. Also, in your first example you have some match arms that just say `Err(e)`. If this match isn't at the end of the function body (which it isn't assuming there's more code on line 15), then that doesn't do anything (for instance, it doesn't return `Err(e)` from the current function), which presumably isn't what you meant :) Can you clarify? 
I'm trying to expand my skills as a coder; Got too stuck on Python, so needed to improve my toolbox. Rust seems like the nicest systems-level lang.
It’s fast enough to use for systems programming, and it doesn’t force you to choose between abstractions and performance - you can have both. Having no GC is a big point as well. It opens it up to more platforms like embedded and WebAssembly.
Once the costs of badly approximating type system guarantees in a test suite are factored in, it's quicker and more comfortable than Python for the tasks I want to accomplish.
That's a little bit of a redundant feature.
How does it differ? If I declare `split` like so: fn split(&amp;self) -&gt; Vec&lt;Self&gt; where Self: Sized; the type that impls `BoundingBox` has to implement every four functions. This requires Self/the type to be Sized, anyway. Isn't it better to declare it upfront, so implementors immediatly recognize that the type has to be Sized? Or did I miss something?
My programs run fast, package management is amazing, tools like `#[derive]` and not worrying about concurrency make programming fun again.
If you do "inherit" `Sized` then the `BoundingBox` trait won't be usable as trait object. Otherwise only `split` would be impossible to call via trait object.
You don't need to mask the lower 32 bits, the cast does the truncating for you.
It's a lot like OCaml with no garbage collection, so it's perfect for games. And it's *rad as heck* and I meet so many awesome people.
Ah, yes. Forgot about that. Thanks for the reply!
Not at all, it can be used to easily break out of a specific loop. For example: 'outer: for x in 0..10 { for y in 0..10 { // Break out of the outer loop break 'outer; } } 
I'm using it to write a webserver with Rocket. I chose to use Rust over Python because I expect to maintain the code for a very long time, so correctness is important. It's been my experience that the code takes quite a while longer to write in Rust -- I think I would have finished the project many months ago if I had went with Python. I spent a lot of time writing manual deserialization boilerplate for many different types. On the other hand, in that whole development time, we've only had a single server crash (due to an out-of-bounds array index). The server is also very fast -- we can guarantee sub-100ms response times. Those were the goals, so it was a good choice in retrospect. Other things that have been positive are the Tera templating system and the testing framework. It's very easy to write tests, and we're very confident when changes pass CI.
It's like what c++ should have been - C with the bad bits removed, and nice bits added, not some backwards compatible bloated shitfest with super questionable features &amp; ever programming paradigm under the sun
Rust strength complement Python weaknesses better than anything else.
I am not very knowledgeable about multi-threading, but what you propose has some inherent problems. You may not care about old data (e.g. the music switching songs one frame too late), but when accessing your data from a different thread without any form of synchronization you will eventually run into problems where you access data that is in an invalid state. Maybe it's a partly initialized vector or the memory that you're trying to read has already been free'd/dropped. The easiest solution is to simply clone the complete gamestate each frame/update, but this may be too expensive. 
I'm excited for its application in the scientific field. I can easily integrate tests into my code base. If I need to bring a package into a project I just have to add it to Cargo. It's a nice change coming from Fortran and C. I've had so many errors caught from the compiler alone. The ease of unit tests has helped me find the rest of them fairly fast as well. I'm really enjoying the language.
I had spent some time with Haskell before getting into Rust, so for me one of the most appealing things about Rust is that I still get a lot of the high level features I enjoyed in Haskell (traits, a strong compiler, etc) but in a language that provides a more familiar style. Not having to wrangle with monads to do simple IO is nice. Basically the balance between high level and low level that Rust presents is really appealing to me. 
Suggestion: put a brief description in the reddit post, for people who don't know what term-table-rs is. "The purpose of term-table-rs is to make displaying table data in CLI apps easier." 
You can try flipping it into `RwLock&lt;Arc&lt;LocalState&gt;&gt;`. Instead of having a single `LocalState` that gets mutated in place, the writer would create a new `LocalState` for each update, probably based on cloning the previous one. Readers would only hold the lock for as long as they need to increment the reference count, then read from the `LocalState` at their leisure; if a new `LocalState` is pushed in the meantime, they can still read from the old data until they're done with it.
How it differs from prettytable-rs?
Good point, done
It's 1.26, not 1.27, but nonetheless: This is mostly a syntactic change. impl Trait in argument position is exactly the same as writing a generic function (inability to use turbofish syntax is the only difference). impl Trait in return position is mostly nothing new either, because you could have just named the exact type (e.g. instead of "impl Iterator" you could have exactly specified which iterator-type). Now, there are some cases where it's an improvement: Before "impl Trait" you could not actually return unnameable types (because they're unnameable/unwritable, duh), which mostly affected closures. Before 1.26 you had to box closures when returning them, arguably inflicting a small performance penalty due to following one more pointer. This would usually be negligible, though. The second improvement is actually backwards-compatibility. When specifiying the exact iterator type, changing the way you generate that iterator (e.g. before it was iter().map().filter() and now it's iter().filter().map()) would result in a different type and thus at best only a semver bump (and at worst it would cause already written code to fail to compile). With impl Trait it will never break as long as you keep the same trait.
I think the only pro to term-table-rs over prettytable-rs is that cells can span multiple columns which may be useful for displaying some types of data. Prettytable-rs seems to have a more complete API at this point but I'm hoping to add some more flexibility to term-table rs.
I'm playing around with making a compiler using llvm from rust, decided to make one for a toy language I called [Oxygen](https://github.com/Redrield/Oxygen). I'm working on expanding it to include loops n stuff
I always complained that with processing powers growing the past 20 years, the compiler could and should have been checking your code for the errors you do as a programmer in compile time. C++ has always caused me suffering, and Java didn't improve upon it either. Most languages don't solve many errors even when they use GC, and the languages that specialize in correctness handle performance or ergonomics badly instead. It was bad enough for me to want and start writing my own language, but before I could get too invested I found out about Rust that just became 1.0 and it had everything I wanted from a language, with the exception for some weird concept I agreed to give up on. I never wanted to use any other language ever again.
* Excellent type system * Syntax is reasonable * *Outstanding* package management * Fast programs fall out of good style - controlled allocations, lots of flat structures, and so on.
Primarily the type system. Though cargo and all other tooling around it, as well as the community are really nice as well
I'm afraid I don't understand. If we had gone with `Display`, to use ? in main you'd impl Display for the Error. You've indicated this is your preference. All you have to change with the existing implementation is to implement `Debug` - a one word substitution from what you wanted.
Rust lacks proper higher-kinded types though (generic associated types aren't full HKTs and they haven't arrived yet)
&gt; It's the ultimate open source language. Which langs+ecosystem do you have experience with?
Good example, but I believe the reason is primarily because most mutex implementations don't allow a different thread to release a lock than the one that acquired it; while I can't find this in the docs for `SRWLock` (mutex impl on Windows), [the docs for `pthread_mutex`](http://pubs.opengroup.org/onlinepubs/009695399/functions/pthread_mutex_lock.html) (mutex impl on *nix) specify this as undefined behavior.
Crossbeam has [`ArcCell`](https://docs.rs/crossbeam/0.3.2/crossbeam/sync/struct.ArcCell.html) which provides atomic reads and updates of an `Arc` pointer, so no locking is necessary.
I wanting to learn a relatively low-level language but one with a modern package manager.
You describe my situation as well
- I love, love, love the compiler. - I love typed languages. - I hate concurrency/multi-threading bugs. - I love monads. - I love tuples. - I like pattern matching. - I like speed. - I don't like garbage collection. To be honest, the only thing that i'm like "man, rust sucks at X", is the package, file, lib hierarchy stuff. I'm not talking about crates.io, Cargo.toml, etc - that stuff is AMAZING. I just don't like `extern crate use`, `pub mod`, and that stuff. I'm just getting around to knowing how to use it now that I'm completing one of my first big projects. I'm sure it has its pros and cons and stuff when you truly know it, compared to like Java's packaging system, but honestly at this point its confusing so any value that comes from it just doesn't do it for me.
Not exactly. This use case is where C++ programmers would begrudgingly suggest goto and recommend you refactor with subroutines. This allows you to keep your control flow local and easy to read.
The RFC addressed enums, but you may have a point about booleans.
Didn't you mention a custom derive that'll verify the conditions?
Thanks. I admit I was vague in my last sentence because I didn't know the details. So pthreads calls it undefined behavior, which is encoded in the Rust type system by making `MutexGuard: !Send`. 
I've been programming in C since the early 1980s and have never had a viable alternative for a lot of things I do until now. Rust gives me almost everything C would in a language that is so nice to use compared to the best the 1970s had to offer. The fact that it is reasonable for basically everything I used to do in Haskell and most of what I wanted Python for is a big bonus. Writing a TCP/IP service in Haskell was more than a little painful. Python is fun, but the 20x slowdown and lack of most static checks means that I'm going to choose Rust for most non-prototype things going forward.
In terms of library mostly. In terms of number of developer available to support it not quite there yet.
Rocket needs nightly. Not sure it is the best choice for long term stability.
Was using Go for a bit, their types sucked. Moved to Rust because someone said its type system was nice. They were right. Few years later I'm still here.
The plan is for Rocket to eventually compile with stable. There is [a tracking bug](https://github.com/SergioBenitez/Rocket/issues/19) for that. I also thought that would be an issue, but in practice it has not been.
I find it dramatically reduced cognitive overload versus C/C++ with regards to pointers, freeing memory at the right time etc. It's fast. Concurrency and threading is dead easy to manage. And it doesn't feel like something that's had bolt-on after bolt-on added to it over the decades - rather, it feels as if it evolves organically through what the community as a whole decides. Compiling it to WASM practically guarantees a fast and safe program running in the browser (to an extent, depending on what other requirements you have).
Rust is a great survival game with lots of different goals and options available. Can be pretty toxic tho. I use it to spend hours of my life drowning my sorrows
Faster and more expressive type system than Go.
This is a super cool project! It reminds me of the following talk (which everyone should watch, it's filled with interesting insights): https://www.destroyallsoftware.com/talks/the-birth-and-death-of-javascript Did it influence you, by any chance ?
It definitely did! After watching it for the first time, I realized that the joke idea that Gary proposed could actually work, and work well! I'd say that that talk is probably why I started writing Nebulet.
Incredible news. I've slowly been getting into Embedded development with the Teensy 3.2 (Cortex M4), and found myself missing many of the features that Rust offers! I cannot wait for easy Rust use on embedded systems. 
If you have comments you want to use often, you can declare a free function with that name and then just call that function! my_function() -&gt; i32 { todo_implement_this(); 0 } Just because something can be done, doesn't make it wise.
&gt; I know few apps use C++ for sharing the business logic. How useful would rust be in this case? Rust itself provides the same affordances as C++ for exposing code to other languages (ie. Declaring functions as `extern "C"` and annotating structs as `#[repr(C)]`). The ecosystem provides crates like [cbindgen](https://github.com/eqrion/cbindgen) (C or C++), [Neon](https://www.neon-bindings.com/) (Node.js), [Helix](https://usehelix.com/) (Ruby), [rust-cpython](https://github.com/dgrunwald/rust-cpython), [rust-swig](https://github.com/Dushistov/rust_swig) (Java, C, C++), and the developing WASM ecosystem as ways to abstract over the process of boilerplating past the limitations of the C ABI and verifying the safety of the conversions involved. (And, while it's not yet usable on stable-channel Rust, the GNOME community is working on [gnome-class](https://gitlab.gnome.org/federico/gnome-class/) as a way to expose a GObject Introspection API. The GIR system, in turn, will get you automatic bindings for just about any Linux development ecosystem you can think of, since GIR bindings exist for many of them, GTK+ is optional for a Glib-based application, and Qt has supported running on top of a Glib event loop since Qt 4.) That said, Rust is *much* better than C++ about developer convenience, since the entire ecosystem is built around a single best-in-class dependency management solution which also does build automation. (And, of course, Rust will do much more than C++ to catch problems at compile time, since it doesn't have over two decades of legacy support limiting its options.) &gt; Can I use the same thing code in mobile apps? Here's an [example project](https://github.com/kennytm/rust-ios-android) which demonstrates writing a Rust library to be used by both Android and iOS. 
I am so glad someone is doing this! I wanted someone to do this with Native Client back when people thought Native Client was cool.
Native Client is still cool! It's too bad it's pretty much gone now. 
As a web developer, it seems like a good option to delve into systems programming. I'm also dabbling with assembly and c, but most of my time is invested in rust at the moment.
Thanks. In the example, it still seems that it is up to me to get the Rust syntax right, though. Is there a way to get away from that? I did make an attempt with [`aster`](https://github.com/kbknapp/rust-aster). I didn't get very far, because I don't know how things work. Is that something that might be the right thing and that I should try again?
Because I want a fast, compiled language that gives low-level memory access, but I don't want to deal with C/C++ and its various outdated features (complex build system, header files, etc).
Actually yes! You can use wasm tables to indirectly call functions. This requires os support (and an abi to populate the table), but it can be done. 
Started out of curiousity, Stayed for speed and hand-holding. Stuck because I forgot how to live without enums.
- quick to start a project - nice config files - it Just Works(TM), don't need to worry about bugs if it compiles - nice package manager - modern (options, pattern matching, etc) - easy to write errors - static typing - memory safe (no null exceptions, no memory leaks) - fast - nice community
Yep! That's what low precision means. However, the error doesn't become larger for larger numbers... at least, not the error that's relevant to floats: the relative error is bounded, both 1000 vs 1001 and 1.000 and 1.001 have the same relative error. The power of floating point is having a large range with bounded relative error, and this is why they work for machine learning/graphics: errors can be (approximately) bounded by &amp;pm;x%, and so one can assess how precise one needs things. Floats are universal in hardware and software, but they're not the appropriate tool for controlling absolute error. For controlling absolute error, fixed point (including integer types) bounds the absolute error across the whole range of the type, but loses control of relative error. --- The relevance of relative error can be seen for, say, normalizing a vector (common in graphics). For a 2D vector `(x, y)`, it involves something like new_x = x / sqrt(x * x + y * y) and similarly for `y`. If we're using fixed-point with 2 bits of precision (i.e. numbers of the form a.b for some integer `a` and a single bit `b`) and the vector is `(0.25, 0.25)` (== `(0b0.01, 0b0.01)`), then `x * x = 0b0.0001`, but that rounds to 0! So, `new_x = 0.25/0`: oops, crash and/or infinite error! On the other hand, for floating point with 1 bit of precision (i.e. numbers of the form 0b1.b &amp;times; 2^e for a single bit `b` and (integer) exponent `e`), `x * x = 1.0 * 2^-4` and similarly for `y`, so we end up with `sqrt(x * x + y * y) = sqrt(0b0.001)`, which rounds to `0b0.011` == 0b1.1 &amp;times; 2^(-2) == 0.375. Compared to the true answer `sqrt(1/8) == 0.3535...`, this is wrong by ~6%. Of course, there's (tons of) examples where floating point has similar problems, so which trade-off is right is very dependent on what exactly is being done.
Rust strikes a great balance of innovation and pragmatism. I've learned a ton of languages for fun. But when starting out with Rust I slowly got convinced that knowing the language was going to be important for the future. It's having a huge impact on the state of the art, showing everyone that a safe language can also be highly productive. Now that I've learned it, I use it because I get stuff done \_faster\_ and \_better\_ than ever before.
I like Rust for a couple of reasons. First, it allows me to write high-performance close-to-the-metal software without giving up the expressivity of a higher-level language like Java, and with an even stricter compiler. Not everyone likes that last point, but I do. Second, it's still new enough that one person can reasonably have a meaningful impact on the ecosystem, so it's great for doing Open Source development. In Java, basically everything anyone cares about is a solved problem. When I got to Rust I needed a better benchmarking tool than was available in the stdlib, so I volunteered to take over Criterion.rs because there just wasn't anything else out there. Now I'm contributing to Accel because I need a good GPGPU programming framework and it just doesn't exist yet. It's exciting, seeing all the great ideas everyone else is coming up with and being able to participate by myself too.
Cargo, lifetimes, lack of bugs, semantic versioning, easy tests, documentation tests, easy documentation, spending &gt;95% of my time designing and writing, being able to write APIs that cant be misused and always follow RAII, having high level functional coding with low level performance and safe closures, being able to use Rayon to parallelize my code without any effort, using serde to be able to serialize data structures with only annotations, being able to make webservers easily with annotations for routes in rocket, being able to use high performance futures that really destroy the ones in C++ (boost's are a bit better with resumables), having crates (like serde and Rand) that give traits that everyone can use consistently, awesome crates like stdweb that let us write webassembly apps, powerful all-in-one crates like ggez, and the community is awesome. Once we have const generics and lots of good linear algebra stuff, I'll have no complaints.
I can confirm that that is a concern where I work as well. We definitely are more concerned about the lack of devepopers than anything else.
Hey dude, I love your videos; please keep it up! Bite-sized, accessible content like this is a fantastic resource to help new users of a language get over the fear of its intricacies and complexities. It goes without saying that Rust is seen by a lot of people as a complex language with some very unique aspects, and it could benefit a lot from just this sort of ice-breaker. Your videos are also just quirky and entertaining in their own right; they're no chore to watch! I really hope that you get the support you deserve from the Rust community on YouTube, because it'll only help Rust become accessible to the wider audience that *it* deserves.
I think C++ just acquired more and more syntactic baggage as time went on. When I have to teach people really strange things like the semantics of list initializers in different contexts or whether or not a templated method will get compiled, I wonder how they would ever be able to figure that out in the first place. How did I even figure it out? Why did we have to endure that language...
Hey, thats what happened to me too! Hello =P
It is sad that lots of calls end up going back through into JS in the browser. Hopefully more APIs will be exposed directly to WASM.
Rust really does deserve the wider audience. I was originally going to make the videos around JavaScript, but Rust saved my butt when JS things in my projects didn't work out. &lt;3 Thank you for the motivating comment! &lt;3 
Ever try Haskell? I tried following a tutorial and something in Cabal broke and I had some conflicts with versions of libraries. Very frustrating.
I'm looking forward to the next release!
to prevent myself from using Coq
I'm curious, how do you handle out-of-bounds memory accesses?
The WASM subsystem would handle that during interpretation, JIT or native compilation, the same way that WASM can safely be run in your browser. The idea of removing the hardware protection boundaries via safe languages has been explored [before](http://www-spin.cs.washington.edu/).
Maybe the Solution in this case is not really rust specific. I would at first determine if this happens in practice. Then maybe shared state is the wrong aproach and signal queues could help. If not, i would try to minimize the time, the consumers hold the lock. If that's not easy, i would split the lock into more fine grained locks and try again. Only when all that fails, i would look for another Solution. And keep in mind: measurement is the most important part of fixing any performance problem. Lastly, and i can not stress this enough, you never ever want to read potentially inconsistent data, because the number of things that can go wrong will explode (and so will your program most likely).
I missed that, therefore I take it back that it is redundant.
Right, I'm curious how it does that. SPIN only allowed the SAFE subset of modula-3, which doesn't have pointer arithmetic IIRC. Wasm code can access arbitrary memory offsets, and I'm curious how you handle that without privilege levels, especially with wasm's upcoming 64-bit support.
Please be civil. C++ is still a very successful language, and ensures productive employment for a lot of people. It's also weighed down by legacy baggage, yes, but that hasn't stopped it from evolving into something more modern. If anything, it's too powerful for its own good. That said, yes, Rust offers footgun-reduced low-level coding that is really a joy to work with, and the community is simply awesome.
This has been discussed a bunch of times before under the name of the Pod trait.
maybe i want more progressive programming language. c, c++ need to be fixed/evolved. rust, scala, elixir could be the alternatives.
Most ppl list speed as main advantage but I think that type system and borrowing are the killer feature. You can improve performance of any app in any language and reducing input size or improving algorithm gives always way better results than changing language. However without language support, you cannot forget about nulls, errors mutexes etc.
Couldn’t have said it better. I love this language so much in what it tries to achieve. Even though it’s relatively new (in terms of languages), it’s already showing amazing power. I think my favorite part at the moment is the no use of GC in the memory management, it making this language so fast and *small* in the memory.
It also took me a while to figure this out, I believe it's quite unique. 
Named loops are great but I only use them for `continue` instead of `break`. Mainly because with continue it's immediately clear where the flow continues (at the label). With break you have to find the matching close brace. If I need to break out of a loop I usually stick it in a function/method and use `return` instead.
The only thing that's unique is the reusing of the lifetime syntax for the loop labels. Otherwise you find loop labels in a lot of languages that have loops. I know Java and JavaScript for sure have them.
I was unfortunate enough to have to attempt to explain the difference between rvalue references and forwarding references to a colleague last week.
The following code gives similar assembly to transmutes for me. The `1u32.to_le() == 1` condition is to check for little endian. #[inline] pub fn to_tuple(s: u64) -&gt; (u32, u32) { if 1u32.to_le() == 1 { (s as u32, (s &gt;&gt; 32) as u32) } else { ((s &gt;&gt; 32) as u32, s as u32) } } #[inline] pub fn from_tuple(t: (u32, u32)) -&gt; u64 { if 1u32.to_le() == 1 { t.0 as u64 | (t.1 as u64) &lt;&lt; 32 } else { t.1 as u64 | (t.0 as u64) &lt;&lt; 32 } } 
&gt; I had thought that macros wielded infinite power and would allow all sorts of crazy manipulations. But after reading about them, I realised that they are restricted (I'm sure for a good reason) in that they don't allow arbitrary macro expansions. "declarative" macros (`macro_rules!`) very much so. Rust also has "procedural" macros where "procedural" Rust code is used to manipulate Rust AST in whichever way it want. I don't know if that's supported for anything but custom derives yet in stable though.
You only have to worry about byte order when it comes to `transmute()`; bitwise operations and safe casts always do the same thing regardless of endianness. You can also test endianness in a clearer way by using `if cfg!(target_endian = "little") {}`
This looks small enough that it could be added to itertools?
See also https://github.com/rust-lang/rfcs/pull/2173.
&gt; Over the summer, I'll be working on Nebulet for a Google Summer of Code project. Who is your sponsor?
Mozilla 
AFAIK the plan is that Nebulet only supports 32-bit wasm. It is a 64-bit kernel, however, so it has access to the whole massive address space available on modern CPUs. It compiles 32-bit wasm to 64-bit native code, and all the pointers in wasm are converted to offsets from a base address. Then it reserves a 4GB chunk of virtual address space for each process and loads every process at a different multiple of 4GB in the address space. Since wasm code is 32-bit, it can only access 4GB of address space anyway, so it cannot go outside of the 4GB region that the kernel gave it.
As a C++ dev by trade, true memory safety is a godsend. That, and a lot of constructs that make programs tend to work with very few bugs right after compilation.
I've done C/C++ in the past, I'm doing F# at work and have been for a while. I'm only doing toy stuff in Rust for now, but I enjoy it because I feel like it combines the best of both worlds -- the performance of a systems language with memory safety and functional features.
So what you can do is store your state in an `Arc`, and then keep a local copy in a thread-local. You also keep a global atomic version number, and a thread-local copy of the version. When you want to read the local state, you first do an atomic comparison of the thread-local version number against the global version number: if they are the same, then you can just return a copy of the thread-local Arc. If the version number is different, then you take a read lock, make the thread-local Arc equal to the global Arc, and update the version number. When updating the state, you take a write lock, store the new version, and then bump the version number.
Once I have implemented it. :)
Many seem to be replying by saying what they like about Rust, or by listing generic strengths of Rust. Perhaps that indeed is their reason. For me, I think what really got me to rewrite a nontrivial bit (10.000'ish loc) of C++-code to Rust was that Rust properly understood the notion of "handing off" data. When I'm done with some data half-way through a function, and pass it into a consumer, Rust 100% _gets_ that. For instance, in my C++-code that took an explicit `std::move`, which of course doesn't actually move, but merely type-casts to something you may move out of (or, rather, which allows using the overload that takes an `&amp;&amp;T`, of one exists). But of course casting doesn't change the type of the variable you apply it to. In pseudo-C++: void f(X *x, SomeConsumer *consumer) { auto y = Y(); use_x_and_y(x, &amp;y); consumer.hand_off(std::move(y)); use_y_again(y); // !!! } The `std::move(y)` doesn't touch the type of `y`, so `use_y_again(y)` at best triggers a warning (on a best-effort basis by the compiler). It's not a compile-time error, and perhaps even runs 'fine' if `Y`'s move constructor is friendly, but it's UB. If your application logic is littered with sending data through pipelines, this will happen, and there will be bugs. As silly as it may seem, Rust's ability to (force me to write a program in a certain way, and then) handle that logic is what made me switch to Rust 'for real', outside of just playing around with new languages. Of course, that's heavily influenced by the application logic I happened to be writing. Interestingly, I had some performance budget available, so I could even have switched to something like Java or C# from a performance perspective. In conclusion, I think I use Rust because it's the only practically usable language which has affine types...
Have you tried stack?
Thanks!
There are some niche applications where such an approach works well. A famous one is [Hogwild](the https://papers.nips.cc/paper/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent), which relies on stochastic gradient descent handling overwrites and stale values gracefully. I'm not sure, but there might also have been some MCTS implementations experimenting with such a lack of synchronization as well. It's important to note that all of these are plain numbers where the interactions between different memory locations are quite limited. But even in these well understood scenarios you have to pay a price, namely you lose determinism. This makes debugging and testing more difficult and reduces reproducability.
I'm tentatively aiming nebulet at server applications. Due to the overarching design choices, it could potentially be faster for syscall\-dense usage, like in a webserver, then the same application running on linux.
We're shipping a library that needs to be used from many different languages, and as we don't want to integrate a heavy runtime platform with GC, background threads etc. in our library, that leaves either C/C++ or Rust. Rust was an easy choice as C/C++ tends to produce code that crashes and has security vulnerabilities even when written by experienced developers while Rust allows programmers that are new to the language to start contribute quality code.
&gt;I've always heard be liberal in what you accept and conservative in what you output. But I find that leads to maddeningly \(and needlessly\) complex parsing layers. It also leads to exploits, e.g. there were situations where different browsers would interpret one SSL certificate in two different ways \-\- both spec compliant. The grammar was just ambiguous. \(And, yes, CFGs in general can be ambigious and there's no general method to check a grammar for non\-ambiguity \-\- however, whether a specific parse is unambigious is always well\-known \(unless you use an LL or something parser for something that's specced to be a CFG\)\).
Rather than revise for the two exams I have this week, I'm going to take some Lidar data (sort of a height map created by firing light at the ground) and making a 3d model out of it. My end goal is to be able to take a gpx file from a bike ride and plot it onto the terrain, so I can prove how steep the hills are! Unfortunately I decided to update my PC to Kubuntu 18.04 yesterday, and the amd drivers seem to be broken. I was getting 60fps on what I've done so far on Ubuntu 16.04 and now it's 15fps. I'll have to fix that first 
Oh? It is in the implementation stage already?! That is amazing! I was under the impression that it was still at the RFC-discussion stage and that it would be a while. I guess this means that something should be available in Nightly sometime in the next few weeks?
I *tried* Rust because it offers safety and performance without requiring grueling years of battle scars earned with other static languages. I *stayed* with Rust for the community. An entire community helped me along the way. There's something really special about a language-platform that so many people care about and want others to succeed with it. Rust is honey for high achievers who want to share its wealth with others. These are people who I want to surround myself with in life. 
Could someone look at [this commit](https://github.com/KillTheMule/nvimpam/commit/03ada03a56b56a74a42a2864aca95b43e2fb1eaa)? It does what it says, it makes the function a lot slower... the [corresponding benchmark](https://github.com/KillTheMule/nvimpam/blob/master/benches/card.rs#L33) performs roughly 6(!) times worse than without this change. I'm flabbergasted. What's happening here? I figured the thing might be a tad faster, because it needs 12.5% less comparisons, but seems I was very wrong. Can someone explain this, or give any idea what's going on?
I've been working on a small library as of late that might be of interest to you called [crossbeam-stm](https://github.com/k3d3/crossbeam-stm/). The idea behind this STM is that reads are always non-blocking and as fast as possible, at the expense of slower writes. Writes are never contended by readers, only by other writers, so if there's only one writer then you should never have contention on either side and everything should be decently constant-time. This is achieved by having an atomic pointer that always points to valid data. In order to update said data, it needs to be cloned (and modified if need be) after which the atomic pointer is then updated to point to the new data. Crossbeam-epoch is used to handle safely freeing memory of the old data. In my experience for this system, reads are about 30ns (around the same as parking_lot::RwLock on my machine, and about 4 or 5 times faster than std::sync::RwLock), and writes are about 2000ns (which of course is immensely slower than either RwLock implementation). There's currently [one issue with unsoundness](https://github.com/k3d3/crossbeam-stm/pull/1) in the library, which basically states that the STM doesn't restrict its internal type enough (needs to be `'static + Send`, but currently allows anything) however I should be able to get that fixed soon. 
Great video! Both funny and instructive. It gave me a better understanding of what Nebulet seems to be aiming for (as I'm not very familiar with protection rings functioning).
The RFC was merged last week. Hoping to have it in nightly by some time in June.
I don't think you should use some adapter that forwards Display to Debug just to get ? in main, but if you've already got your own error type (as most serious projects seem to) I don't see any problem with implementing Debug for it in a manner appropriate to print from main. I don't see how this is "a bug. Full stop." or "masquerading;" these seem like mismatched value judgments to apply based on the syntax used to execute a piece of code. In a parallel line of reasoning, I think Display is inadequate for very serious CLI applications which will want to do things like colorization, which is why I'm not very excited by the idea that requiring Display for ? in main would make these "real life applications."
Aha. Thanks for the status update! Sounds great! Looking forward to it! Thank you for all the awesome work you are doing BTW!
&gt; There are three kinds of lies: lies, damned lies, and statistics. For this benchmark we implemented Treap in a few classic (C++, Java, Python) and hyped (JavaScript, Kotlin, Swift, Rust) programming languages and tested their performance on Linux, Mac OS, and Windows (all of them running on different hardware, so the results should not be compared between platforms).
My guess would be that because it's looking at 8 bytes at a time, LLVM realises this is *effectively* the same as loading and comparing two `u64`s. When it's 7, at best, it's going to have to load and compare one each of `u32`, `u16`, and `u8`; that's assuming it doesn't fall back to 7 `u8` loads and comparisons.
[v0.18.0](https://github.com/iliekturtles/uom/releases/tag/v0.18.0) of [`uom`](https://github.com/iliekturtles/uom) (type-safe zero-cost dimensional analysis) was published to [crates.io](https://crates.io/crates/uom) over the weekend and includes 10 new quantities (energy, density, ...) along with their associated units as well as additional units for 7 other existing quantities. Compile times for this version are also significantly improved. Most of this work is thanks to PRs from /u/Aehmlo.
Please consider mentoring other programmers. Too much depends on your productivity. Life throws all kinds of unanticipated events our way.
Ok, that makes sense. Maybe I can get a speedup by looking at 4 bytes first, which is sufficient most of the time. Thanks!
Since school pretty much ended last week, I got some free time to continue learning rust. I added color support (on unix systems) to my very basic tictactoe game https://github.com/flofriday/tictactoe. Any advise will be appreciated. This week I will continue reading the book and maybe I will look into compiling to webassembly.
One thing that stood out for me is that the construction of a new node calls an rng. Not all rngs are equal, and there's probably a tradeoff of speed vs. quality there. Maybe that influences the benchmark unduly...
Can confirm: [Godbolt link](https://godbolt.org/#z:OYLghAFBqd5TKALEBjA9gEwKYFFMCWALugE4A0BIEAViAIzkA2AhgHaikCuAzkeT3RdSqbCADkABy4AjANTY2XALZyA0tgCeAdzKY5AUgDsAIQMAGAIJy5AeltyAclmwXrTl%2BTc2Awm0/ecgCyLDw8XlY2jjzKoRHu0bE8AEzxNvZyuEzYyopEgQDK6EwEmGlyABLYAB4syeblAAp59ACsTXksAGzlACrYRKT0DYH9gwAs5SY8SNhMfTNz5QWzTPOFqz0bcwAc5UE5MhSBJtgsysuSpMAyFycsx5FyBVcEHOUAUuhv/IFqNN82PsSDQ2L8ni9VpdVuhygBxFiScoASWUkli5QyABk3gBrQJZPHlLElNi45ak8mBRqUvoEbBlQIZRoPIhyADMABFqayiiVGU8WaQiNMoTzhWNSCwmrzJGLBayfOgeOyBe5may5MluQqJYt1rqiCs5gb3EKiAdlEdMQ5zXJ6DqzayTA8ZcLTuc3UbXu9xd7rrcvUEZDwrl6vj8vf9AUGZLjQUGQWCvb16WqbOaCrS/YSyV6SUS/TTC4aEUi3MZHW4CGimOotLpSPpjGYnvZbQ8eNg5Cw5HxSG9gHISHIcERsKRlG9uwQAGZyYh9oisnhybTEJDD2ZyXENvRydDz3uoB6YAB0gQMyXZBlaJjepNcrUdNmk8lnbDkkk7rnZPl6IByJYPAAErYLOBh/v2kG4DBEA8IBV5dL0ACUcgALQwXIADykhEAQ6BsJBPgaDoehYS2gQ2Lw3Zdkws4gCApGNpgjEAFSQa2lhUXI2RsqukGcn2Z6hAA%2BqQ4EQChIk8KJMiaOOPBSZxPF8bxiiGFywnZGwynslxPFzupn6CYJcjmIYRg%2BMYPjGZptlGJZXE2C5HhsK4Tw2JWChMF2Tk8TYal8JqZlITwt4mOYZ5nkYt7cvpFaeS5sREKgm7BcK/lJa5GTODgAUuTIV7JI42Gcrg6TFZp8UVUUuQQHl2AoeUrk2EVyTJD4pXlXYVWmeytXoPVfguM1BVtcVQSWAUBSVR11VYXV2AQCEYRjdlhXFY4BRTbNth9Vyi1DctiShOt7itXI7UlTt03JPt839YN9WnSk52XekDhZDkijjVdxUFNhWLIkJD3JAtA3PMdEB8qU70fddFS4AAGpY9RgxDz3LVUtT1PDl3XY0uCOL0bQY09UP1c0YJtPjrWE8TvSWF05OHZDS0QNTy5dHTrnXb0uC9CBwyszVlPLZKwy85tHUC0L4y9Y9bNYxAkrjNLE0dSYBRI1iivgxTHOinMGv/bLOu4HrotHfVvT6qb10W1iVsHWLHPGmsDsA7rXT65j4sw5sXsdU7Ox%2B4b0MezswfJEEuBBCYIHh8rAeWkcMcmLglhBHNBsp0bZzKDHBSNCBcImEE1vs5HVw3EXLX08VJiWEnuf%2B0bDzF6XyKOHCVcqy8A4cDHHzYT3vTJ270MRmCMdqKPPeTzby3Rm8MdBL02EfCT/ep0mRBd7rS/V/VkImw3fMA40uvYbv7tynM6Ax3CliNC5d/Q2WMfIkEjRTcfKtUTohYKbbEeI/rXUtj3NQACA65lxDHZ20DYEcwLGSYuwNHAwI/qfSkMdGiYOwa7ZenM8EXxlskXoyJcBCRQdDVMDJ8EtwnrAyiG1XJGR0vZLUCs2EXQ%2BlEQiHl%2BGXW8nMPyfCBGuTUpIaqhhkhdHCneegXRorJHGHFFSVg/quRSmlL8WURFSOui5QGwNuRKynlTXkxQ4bkIESYmw2tdbEJPstc0xsmCmwJlVGwcsQLcUsSQ80koQH2IRr45419LauJVpmB%2BXjwk%2BPmr4bCBRDpBLcZzRUypVTeMbikuQvQnaxIDiE%2B2SSCngxsE7LEpSOaZlWIknRVSXJxwTiBep0NzRp1IPky%2BhSnEty6dY90ndKkDOqU4rOQQRnuOdIXfpFCand17nM7JwpB6DiWZrapJcy4V3WfEgM9cWmTLadrUuRzWTBlDH0iZyybALxJtc4UM8D4PN2S5eeY9HCvKIKvNgOyzbVI3lvF5mS4k3P3sCxxwQTDzz%2BZCspNy4yglhZEop1CLF5ysfMiUaYMWFIKIQ/5WY8REqmVArB/z4GUpckgmlyKGmsjQQgz5IKXIEOgf84s6COVwpsC/Ro/yv4ctEv7Zw7kHmVhabK9hlkXwfQlRTKV2ByHyv4ZqryRgqy6sSpYN8cgPxyFiLuUSu4yJNikhhLCuF8KEWIsxcikNJEuRokuViIACJaP4WpZQXA2QyC4POMyBRBiDkYu5bQelnIuQIoxPghBdLSQkiwTAolHwQCQgGoNIbpJcDYNoKUkhY08WdU2Ri35SBdmzQo4Ns4UIVl1eIZqTAJCtHEOQNgEhzBdvQBIECvB%2BJCBEN2K87J6BdqIL21tzVcQgGvGeVo7JJ1dHZAold5guj0A6swCQ4wu3KBAK0BoPbxB9vIAO8QXaEINBnRe1t5A4CwBQBgHA%2BBiBkEoNQOgqRWAcBANwPgAhR2iAkEaxQKh6xWubKYJkDhGqBEauUEaOB9ihHCMhmIcRsNJFSAhzI2RchgkKLY9MlQah1BGIKFo7RqSdC2E8SWNH3BqymPqBY8p3Ae1NDUzYyxVh7ECL0qYhdLgnKmK6QoPpgCfEBOCdwgLgToFBIpmpCToSP3hIiFEaIMSEZxGSAk2YnhsopCWM0pm2NphtHIO0h1DSwwox4zjfpQlejPnx%2BzOSVQUY1JlbU7m3NOaaUGQ4jx1Qdkyg6P0LpIsZgWZ6P0WzfROdroGP0tywx%2BneVGAEbxYzxiBFl/eKZbMpes4l4U8D8xVZ88KPlVJSy6ebVWKwNZJB1grXBuN7YGt%2BV7P2Qcw50CjgGBOKc7kFzzkXBlIgq51xEE3Mt7slqWIHiPHIE8TYLxPAnRFB805NFPCNSa6tXZiIASAqBcCxFoIDTggheRyE0KYUhvagiRE/w9YovBpKHq6IMSYnuStIAOIJW0UlIKcieAyXEpJaSYl5KKTLdDgYdkzJw50mj7iSVOEaX6kJCyNkbKY7/HIMObrXJqp4mI3y47/tGKCsuTKoUFFKMitFHYmjId46MXo9KrO2TU9arlFwECtplQqnYeRuKSGNRjl1aXsv1loaauE66u1Zqq%2BZdDVaPAY7bV2rr%2BXWTXpG9ugUe6cv27Q1eskUBX1iN5ElyHIGINTd29PuRmOSNUbo1txHeqONqP4MZmTIP%2Bdul0fDyTZmDgjmMZjv4kWUe8WqwGEMcwKfBYgQVonvXtss/q0103J2Xvg/LU8SnivhezcqztqsYuusrbp5Ibxlvltfb1%2B98tD2PMy8h11mHXvVfA67HXvHROlfo/1V6RnGZ6R29ZI9KchVjtS7l0rivgeGX19GN2c3Vus%2BM/xa7iBHufdd8B1S8AEevyJ5j7n8td5c9nmn5IYC9em9t69GfxnkEDCkPskCXEfAAR3gkofJbLfDfvfDCM/K/O/HAZ/IiN/L/P/BAVkkArEE7nIEZviBvsVNStgigfVHSiAYyqQesmyhgsglgQPGQm7skNylgp/lkk1intisvusgwpgEwiBCwgwYYlIjNuTrZOoiIaIW5MIqIfThIkztIepGyLIuzoohFComohos%2BL6mcqaiwKlJuKoYoUoZyjUh7jin3hskaL7gKpEs4jEkXviiKBUnoV8n4nnoEg3iihKFnmEm4WYVEi4k4dYV5vSqkukpYePuaEqH5uEUUiUiEeUs3nYcSq3mSk0vEe0onP8gvqkVMkMp0kkc6OMgEYKpnNnP8mvuEfslfmSrJjUVvocsUZsvvlkZckUd4SysKNlvcmUZEs8r0P8m/vkd8s8v8t/qMcEL/hCl0d0tCqprPFMfCoirkWikCssVijQv8nwfESSjyi0UaEwQqsklSqSocRQf0YUlQf8rQcsawWoLyscYfhEoMnIMKqKmgeKpKkIjKvqicYqn9Nqsqj8dKgVMCTqkqpqtCVYGdp%2BGatgBaqDpgDau9hVJ9o6j9siX9nGtRH5EmoxD6nztIhjrmldCGnIuGkPKACANGrjhwgOiAEmm8FJGeGmhmlmjmoGuSY2meIWsWoiPSd8siVWj%2BHWl0A2k2toi2m2h2l2uepetekOnwH2GBuOteMkNOrOihM1LMOmhONQPOouqouMF0F0DsOyF0EYOyKaQAJzsg7BGB7DtriCHrdqzpXoSC3ogD3ranPqIAQBvpDSSAEDZAUBUAQAYBoihkGlDAKINCzihnjg1rUAyAekyBvAPCaASBTrkBRkkZEDYRsBMDZmPrkA4CxAcDZAemEASSoD4QABu2ACEZZNQ2AqAgaYg4guZPwcwHpEax6j6ba7AnAw6zABAIY8AzU6AeEX2LZ6E2Et6apDAsp4gna7pZZ16wGqU9oZ4CiZ4FkEAn6JApA8ik65AcgSo0ZYZZ59AaEypRAWpQ5RpyQrQy6tp5grQrQ9A9AOwP55g1466%2B6rp8pHp163pvpQ5/p8AgZaAwZMZ4Z1AUZIZYZQGu6XQCZSZE4CEEAaZZZGZbAWZOZXa%2BZeQRZJZNZOQI51ZZZtZ7ZjZzZHpbZHZ44xFlAYIfZZZA5fpAGo5IGJQk5kA05s5hE85i5oGwg4GjuwF65Cp/aEg25qAu5%2B5h5x5ZAZ5qQl58FN5E6jucgD5T5faOp5AC66iZ44w5gtp4w4wOwf5tltpRg66RgwFbpclnpN6AgPp5AD6Rlq5mp5Ax6yQRge5RgQVYVoVEVQVG5ipXp3lfpL6yAIAhAs4s4P6EATASACEjApAmVi6q5slYFEgnIc484ilylqiFkDZq4ZV9Ae5FVhlc6LlR6IAoVMU4V7VEV7I0V8lHld6cVz5wF7IoFm5sVPljVTZNaX2IA4wQAA)
Waiting with baited breath! Thanks for your work.
Personally, I use this. Essentially it's the same as what you have but it automatically lists the number of times something's been called, which is useful for catching when a failure state is being generated erroneously (but then your code handles it, so tests don't fail): macro_rules! debug { (@preamble) =&gt; {{ use std::sync::atomic::{AtomicUsize, Ordering}; static DEBUG: AtomicUsize = AtomicUsize::new(0); print!( "{:04} {}:{},{}", DEBUG.fetch_add(1, Ordering::Relaxed), file!(), line!(), column!(), ); }}; () =&gt; { debug!(@preamble); println!(); }; ($first:expr $(, $rest:expr)* $(,)*) =&gt; { debug!(@preamble); print!(" ("); print!("{} = {:?}", stringify!($first), $first); $(print!(", {} = {:?}", stringify!($rest), $rest);)* print!(")"); println!(); }; }
we should change the official name from monomorphisation to copypasta 
A whole bunch of the "batteries" are rust-lang/ and rust-lang-nursery/ crates, like regex or Rand. These are maintained by the libs team.
If your error type is `io::Error`, yes, but if your `main_result` that you're going to be calling from `main` already returns `Result&lt;(), MyCustomError&gt;`, you control the `Debug` impl. That's all I'm saying.
fighting the urge, and failing to point out that this is the only legit time to use the word "bated".
I'll spend some more time on [rustfix](https://github.com/rust-lang-nursery/rustfix/)
I only checked the ref\-counted c\+\+ implementation quickly, but it seems to make quite a few copies of shared\_ptr that are not necessary. I would guess the difference with the Rust ref\-counted version come from those temporary refcount changes
Yes, I believe so as well, which is why I want to write two extra implementations: 1. C++ without raw references (`&amp;shared_ptr` looks like a cheating to me). 2. C++-like implementation in Rust. Currently, as mentioned in the README, C++ is implemented rather differently from all the other implementations.
Still working on my \[GameBoy emulator\]\([https://github.com/drgomesp/oxiboy](https://github.com/drgomesp/oxiboy)\). Here's the change log since I last posted: \- Mapped wram and cartdrige addresses\- Fixed mistake on pop func\- Implemented basic breakpoint function on debugger\- Fixed flags affected by dec and inc and also implemented a few more instructions around that\- Implemented basic memory dumping on debugger\- Implemented a few more instructions; got to the vblank loop I'm basically now working on the PPU implementation to get passed the vblank loop, and the expected result is to have the Nintendo™ logo scroll down next time I post here :\) 
We expose add with carry via `overflowing_add` (we can't expose `adc` because it's stateful but doing `overflowing_add` and then adding the bool part of the result to an integer will compile to `adc`). Overflow-free multiplication isn't in std yet but I would love for it to be exposed. I think a nicer API would be to return a number with twice the number of bits, though.
I'd love if there was a way to be generic over integer size. LLVM supports arbitrary-size integers and libraries (including the stdlib) are already rife with `impl_foo_for! { u8, u16, u32, u64 }`. It'd have to be at least after Rust supports const generics though.
I made this flamegraph: https://svgshare.com/i/6cU.svg It seems that the recursive calls of benchmark_tree::merge and benchmark_tree::split_binary are the main time hogs there. However, I don't see anything obviously bad in the implementation. Btw. the C++ version has a quite different graph: https://svgshare.com/i/6f5.svg
. i.i.ilum.... jeg Limited kl.k.im.u.pk.lk..k..klom..ii..kl..kim.im i i Milk om.. jeg.[..mi.mm.im.. mm er m.kl.k......l.ml..kuu.k.uilm....ikm..m.. i lmk I'm.lum..mimik..il.l.m.i. ml milliliters i.ui..lm.mmi.i.. MLM the mm mmmm. I know ml.the UI know.. I have to get minim me..I'm.. much l.ii...il..i.ui...l...k..iilm...il.l.u..i.ml.ikm.u...l.......mimk. u.i.i. Mi minimal umm I don't minim it..m.m..l mm mmmm immediately imminent.il[we. I mm. i. er meget.er.med.m.u mi.. jeg mmol er det minimum mig.k.l..u..ml. i. I...mm.mi.li.l..mm MLM....med. i Ml.. jeg I.er i ikke.li.mp i a MLM u umm milk.iuk.l.k..imii.i.i.l. a good day at a NMM..the actual datem mom Monika.mm Mi like.i.k...iili. ok lmii.m.. Mi im it Lim the Ml agood agood.mm mmk millimeter..m. we Mii i.il..ll.m.immmm.im mm imidlertid minim me nu Lim the actual almost u I'm not momma. I'm on my way to the l.ml..milim..mi Mill Creek andI.my.l.um..i.i.i..i..mlm.l mm mm im it is i.m...l.i.m.m. Muk...I.I. I l.i...i.i.ikl.. might..I. Mii let i stykker.ilm.mim. i Uni i.i.i..i.. i il.limi.k.i.ll..ili.l.im..i.lm. Mim..imlm.il..m....i..I.er ml ikke.l.mim.lm i dag i Mimi Jakobsen i minimum i.i min i.mm.i.lumk... ml imiteret..I.. kan I ml.m i I Milk mml.... i.l.ll..l..iml......im. immun i.. jeg nu.mk..i.k.l. Mim link...i min.m i min. I dag.lim..mi. a. i går i.k..i..il.immiimim.i..i.l....l. i morgen i min m ok love mml. ii.im.im a m.ikmii..k.limlm a few I might i.l.klk..i.im theactual my.m u I am.m.m. mmk millimeter mom.i.m.m. I'm Mk.....I.l.mumum.i.mu.k..i...u u Ml ikm..I. jeg har mulighed.imil..m..i..m.. nu er det er en af de mul i morgen Milli..lige... mml minilæsser er meget.. Mim min lillebror kommissionen. Min.I...u.u.ml.km.u.m.im.m.mk..m. mm minimumskrav.er det enu Killin.im.mi.iml. LM not minimithe.. me. i.m. mm mm..immunization.i..iimk.. Lon I.. thanks I. I making.. I num m.mik.il.. mmk....I'm.l..pi..k... a milk.m minimum.iimi..l.m...m. Mimi and I iii.m.ik... u l.. I no..k.l..imi.mi.m. Mii i.mmi.ui. i ilum.k... MLM i.mk. li i.u..umm..imui.mu...mmimlm.i.ml mm imidlertid ikkeat.i.imk.m. i min.m a er. I. jeg kan lide.. jeghar. jeg. jeg har i.. in m. jeg kan ikke immunisering er unkinking immunologi unkinking to. I have a few i.mlm the mm mm mm i.mlk.. km lang.er ml.]().. jeg kan nu lukke dette er op om jeg.. I dag er det i dag så i Illinois m. og Mmm i dag &gt; i.immi &gt;m.mi.m..limm &gt;Men &gt; &gt;Jeg har en god mml. Mm.det her med det ml i i..med..er det... jeg. jeg. jeg l.m.mu..k.u I l.i..I. I love.. I.I. I. I mku.m.ii..muiill.km...i. nomatter... u to be okay.... U.km..i.l. I'll be more than happy to help you can m.m mm m not much just got home myself like l.mu.mimi...i'll.....i.l..mum..ikmi..m..u... Mk the. I might. I have mk.the... I'm unknowns the United and ll.k.i.ik..ll.kl..kmi...k...ui.l... Min..my.the..m..m.. Ml agood agoodagood.. agood I. i.k. jeg.l..l..l.mim. mm mm imidlertid immuniseringer lige..med.i....i....l.iik..i......i..kk.l.lmkmm.i.u.....u i dag i mm..lige..IMM....jeg.. i. i MLM..i.l..ll.im i i minimum lige.. jeg. jeg mig.. a MN i dag i.m..i.i.i.l...m..ill.imuk.kll.i. i dag I'm.. I l.m.i..ll.kl..l.mi...i..m..m a good laugh out.i....iii.um.u.ik..il.m.k.i.ul.like I'll.u.u.k.](). my phoneisnot.. I havealot
&gt; LLVM supports arbitrary-size integers LLVM-IR, the language, support these, but that does not imply that the backends lower these properly. For example, depending on the LLVM backend, trying to use 128-bit integers results in a code generation error.
Edit: hm, the "flatness" of the C++ recursive towers in the foot of them looks like the compiler would be just const-folding and inlining stuff until it has reached some threshold value. It would be better if the parameters would be handled from the input to prevent this.
I too would like to see a lint for unused loop / break names. Perhaps open a ticket in clippy?
I think we might just have it. One reason you might be getting that message is because the function lives in the `wasm.instance.exports` so you could try and change the getWasm code to look like this. async function getWasm() { if (!cached_wasm) { cached_wasm = await fetch('rust.wasm') .then(res =&gt; res.arrayBuffer()) .then(async buf =&gt; { let mod = await WebAssembly.Instantiate(buf, {}).instance.exports; }) .catch(e =&gt; console.error(e)) } return cached_wasm; }
I'm trying to get my material/crystallography orientation library ([mori](https://github.com/rcarson3/mori)) ready to be published as a crate. It's really close, but I just need to add the parallel library, and a few small orientation functions to it. 
So does this mean using generics over distinct functions does nothing to reduce code size, or are there optimisations available for generics that can’t be applied to distinct functions?
Tangential to the topic but what was the trick to removing mangled names in the flamegraph? I had no luck when I was toying with it a week or so ago, and couldn't find a global no_mangle.
Because it has the ecosystem (and thus similar productivity) of Python and static checking better than almost any language and the speed of C++. It's the best of all worlds, I love it.
Letting Tokio abuse me while working on a p2p project.
If an optimiser had a pass that could merge similar or identical functions, it would apply equally to all functions, monomorphised or not. If you have N instantiations of a generic function, then you're probably going to end up with N functions. This is why, in some cases, monomorphisation is explicitly avoided.
Addendum: LLVM generates even better code with `-C target-cpu=native`. We could never have used `mulx` in our assembly since we plan on distributing binaries and `mulx` is only available on Haswell and higher (post-2013). libbigint.so`disassemble: pushq %rbp movq %rsp, %rbp pushq %r15 pushq %r14 pushq %r13 pushq %r12 pushq %rbx subq $0x50, %rsp movq 0x10(%rbp), %r14 movq 0x18(%rbp), %r11 movq 0x30(%rbp), %rax movq %rax, %rdx mulxq %r14, %rcx, %r15 movq %rcx, -0x78(%rbp) movq 0x38(%rbp), %r10 mulxq %r11, %rcx, %r8 movq 0x20(%rbp), %rbx mulxq %rbx, %rsi, %r9 movq %rbx, %r13 movq %r13, -0x40(%rbp) addq %r15, %rcx adcxq %r8, %rsi pushq %rax seto %al lahf movq %rax, %r8 popq %rax movq %r10, %rdx mulxq %r14, %rbx, %rdx addq %rcx, %rbx movq %rbx, -0x70(%rbp) adcxq %rdx, %rsi pushq %rax seto %al lahf movq %rax, %rcx popq %rax movq 0x28(%rbp), %rbx movq %rbx, -0x60(%rbp) movq %rax, %rdx mulxq %rbx, %rax, %rdx movq %rdx, -0x38(%rbp) pushq %rax movq %r8, %rax addb $0x7f, %al sahf popq %rax adcxq %r9, %rax pushq %rax seto %al lahf movq %rax, %rdx popq %rax movq %rdx, -0x58(%rbp) movq %r10, %rdx mulxq %r11, %rbx, %r12 movq %r11, -0x50(%rbp) xorl %edx, %edx addq %rsi, %rbx adcxq %rdx, %r12 pushq %rax movq %rcx, %rax addb $0x7f, %al sahf popq %rax adcxq %rdx, %r12 setb %sil addq %rax, %r12 movq %r10, %rdx mulxq %r13, %r15, %rax movq %rax, -0x48(%rbp) movq 0x40(%rbp), %r8 movq %r8, %rdx mulxq %r14, %rcx, %rdx setb %al addq %rbx, %rcx movq %rcx, -0x68(%rbp) leaq (%r15,%r12), %rbx adcxq %rdx, %rbx pushq %rax seto %al lahf movq %rax, %rcx popq %rax movq %r8, %rdx mulxq %r11, %r9, %r13 addq %rbx, %r9 movl $0x0, %edx adcxq %rdx, %r13 pushq %rax movq %rcx, %rax addb $0x7f, %al sahf popq %rax adcxq %rdx, %r13 setb -0x29(%rbp) orb %sil, %al addq %r15, %r12 movzbl %al, %eax adcxq -0x48(%rbp), %rax setb %sil movq -0x58(%rbp), %rcx pushq %rax movq %rcx, %rax addb $0x7f, %al sahf popq %rax adcxq -0x38(%rbp), %rax movq %r10, %rdx movq -0x60(%rbp), %r10 mulxq %r10, %rbx, %rdx setb %cl orb %sil, %cl addq %rax, %rbx movzbl %cl, %esi adcxq %rdx, %rsi movq %r8, %rdx mulxq -0x40(%rbp), %r12, %rax movq %rax, -0x38(%rbp) addq %r13, %rbx movq 0x48(%rbp), %r15 movq %r15, %rdx mulxq %r14, %r11, %rdx setb %cl addq %r9, %r11 leaq (%r12,%rbx), %rax adcxq %rdx, %rax pushq %rax seto %al lahf movq %rax, %r13 popq %rax movq %r15, %rdx mulxq -0x50(%rbp), %r9, %r14 addq %rax, %r9 movl $0x0, %eax adcxq %rax, %r14 pushq %rax movq %r13, %rax addb $0x7f, %al sahf popq %rax adcxq %rax, %r14 setb %al orb -0x29(%rbp), %cl addq %r12, %rbx movzbl %cl, %ecx adcxq -0x38(%rbp), %rcx setb %dl addq %rsi, %rcx setb %bl orb %dl, %bl movq %r8, %rdx mulxq %r10, %rsi, %rdx addq %rcx, %rsi movzbl %bl, %ecx adcxq %rdx, %rcx addq %r14, %rsi setb %bl orb %al, %bl movq %r15, %rdx mulxq -0x40(%rbp), %rax, %rdx addq %rsi, %rax movzbl %bl, %esi adcxq %rdx, %rsi setb %dl addq %rcx, %rsi setb %cl orb %dl, %cl movq %r15, %rdx mulxq %r10, %rbx, %rdx addq %rsi, %rbx movzbl %cl, %ecx adcxq %rdx, %rcx movq -0x78(%rbp), %rdx movq %rdx, (%rdi) movq -0x70(%rbp), %rdx movq %rdx, 0x8(%rdi) movq -0x68(%rbp), %rdx movq %rdx, 0x10(%rdi) movq %r11, 0x18(%rdi) movq %r9, 0x20(%rdi) movq %rax, 0x28(%rdi) movq %rbx, 0x30(%rdi) movq %rcx, 0x38(%rdi) movq %rdi, %rax addq $0x50, %rsp popq %rbx popq %r12 popq %r13 popq %r14 popq %r15 popq %rbp retq 
I'm continuing to learn rust by reading the book while live streaming. I just finished chapter 8 so I'm going to be implementing what I've learned into the challenges at the end of the chapter.
Thanks! Looks like there are several things there that I was considering asking about here.
It was 1.20 for three crates, 1.22 for one, and I haven't looked into the others yet. I think bitflags was the reason for one of those. Good call!
A timely edit will probably save you some downvotes... ;)
That's what I mean, we can't use modern instructions in our inline assembly because it'll lock us into later processors and likewise we can't use it in our binaries. It's interesting nonetheless.
More work on [mutagen](https://github.com/llogiq/mutagen), [TWiR](https://this-week-in-rust.org), [RustFest](https://RustFest.eu) preparations and perhaps some small rust improvements if I find the time.
On crypto related code I've seen several times that `target-cpu=native` can result in a less performant binary, so it must be used with caution. One of the reasons can be [this](https://blog.cloudflare.com/on-the-dangers-of-intels-frequency-scaling/).
I've had to step away from trying Rust for the last almost year now. I've finally got my poop in a group such that I can dip my toes into Rust again. In real life I write a lot of SPA JavaScript applications and mobile apps. The announcements and progress of WebAssembly has got me all jazzed up, and finding out Rust has it built in is awesome. I came across \[yew\]\([https://github.com/DenisKolodin/yew](https://github.com/DenisKolodin/yew)\), and have been super excited about it. I'm working on porting a portion of one of my real time JS Mobile Apps to it just to see what it can do.
&gt; That would greatly improve performance once more. Why would you expect that? What makes 128-bit integers so useful for performance is that CPUs have an instruction that multiplies two 64-bit numbers and produces a 128-bit number (split into two 64-bit registers).
That was there so that I could see which functions were contributing the most CPU usage in Linux perf. When they got inlined I lost that breakdown. They weren't called that frequently so the performance impact wasn't measurable.
🍌🍌🍌❗️
Maybe I understood the article wrong, but as I understand it u256 arithmetic is done manually by using u128 numbers in Rust after this update. Wouldn't it be even faster if Rust native u256 types were used, that would do arithmetic operations on a lower level? 
You forgot painless concurrency, but after switching to Rust I've felt that I don't have to compromise anymore. Python\-like convenience without all the "footguns" of C... There is also something really nice about sheer simplicity of compiling into binary and being able to "just run" your program on a machine without installing something first... You don't know how much you miss it until you try it again... :\-\)
One of the benefits of Java is that it does do specific optimizations for the platform it detects that it is running on. So that's nice. But generally, it is usually easier to just target feature sets with the CPU. So you'll say something like --cpu=x64-SSE4-win32 or some other craziness. You'll base that feature set on roughly how old you want your CPU to be (so, for example, you could say SSE2 for CPUs built from 2000 up.)
you should donate your brain to science once the time comes...
This is something I've struggled with as well. However, I've come to the conclusion that what's happening is that Rust is making issues you'd face in other languages more visible. When working with tree structures in example code we often only need to walk the tree, or do simple manipulations of its structure. However, building something more complex around that data structure often requires maintaining 'views' or references into parts of the structure. Understanding the ownership of data in situations like this is complex nomatter what language you're in. I would highly recomend exploring the html5ever codebase: https://github.com/servo/html5ever It contains a DOM implementation that could be suitable for using as a building block in a more complex application (they do suggest not building a browser around it fwiw). That means that the code is not going to look like Rust you are probably familiar with. It was a shock to me! On the other hand, if you start to grok the codebase and use it in some programs, it becomes clear that this tree implementation is much friendlier to modify and hold references to. What it comes down to is that there are very good reasons to choose runtime checks like reference counting in some situations. While I often use the indirection (indices) pattern you mention when throwing something together, the more advanced use cases can still use pointers in conjuction with Rc or RefCell. Of course this is all my opinion and I'd love to here what I've got wrong.
This crate looks really promising - I'll have a closer look!
In the end all the code compiles down to instructions operating on 64-bit registers (Unless you go with SIMD, which is probably not useful for this scenario). So "native" types are only faster if the big integer library that ships with LLVM is better than what the optimized produces for the rust code. That may be the case, but I wouldn't expect a huge difference.
For me these issues aren't a source of errors, but they cause me to give up on projects. As I do not have the combination of time and talent to figure out how to make things work at all, and am usually dumbfouned by the array of options, having no idea which ones will perform well, or make the most sense. 
Type constraints in parametric polymorphism. Consider I have this: mod fib { pub enum Term { Number(i32), Text(&amp;'static str) } impl ::std::fmt::Display for Term { fn fmt(&amp;self, f: &amp;mut ::std::fmt::Formatter) -&gt; ::std::fmt::Result { match &amp;self { Term::Number(n) =&gt; write!(f, "{}", n), Term::Text(s) =&gt; write!(f, "{}", s) } } } pub fn nth(n: i32) -&gt; Term { match (n % 15, n % 5, n % 3) { | (0, _, _) =&gt; Term::Text("FizzBuzz"), | (_, 0, _) =&gt; Term::Text("Buzz"), | (_, _, 0) =&gt; Term::Text("Fizz"), | _ =&gt; Term::Number(n) } } } fn main() { for i in 1 .. 101 { println!("{}", fib::nth(i)); } } 1. This `fib:::Term::Number` should work with any type that has defined the `%` operator. How can I get such trait to put a restriction in `fib::nth`? 2. If I want `fib::nth` to work with only `u32` and `u64` only, how can I implement the function without having to repeat myself? For example, `fn nth&lt;T : u32 | u64&gt;(T: n) -&gt; Term` (conceptual syntax, obviously).
rust still depends on C++ (LLVM) 
Yes there are sadly a bunch of backends that don't support i128 right now: https://github.com/rust-lang/rust/issues/35118#issuecomment-361423792 Those backends are mostly tier 3 anyway so a lot of other stuff isn't working on them either. Generally there is little motivation in adding i128 support for those backends because often the primary target is to get clang working and C/C++ only support i128 on x86_64 (not even 32 bit x86). Of course this doesn't explain the atmel backend which was created specifically so that rust can target atmel devices, but here they wrote the backend before Rust got i128 and they also attempted to turn it on but apparently ran into bugs with the register allocator.
The problematic case is: 1. Thread A reads pointer 0x123 from cell 2. Thread B atomically swaps it to a different pointer, 0x456, and gets back an Arc pointing to 0x123 3. Thread B drops the Arc; this decrements pointer 0x123's refcount, and it gets freed 4. ...Thread A increments pointer 0x123's refcount There are a few potential workarounds. For one, you could use some approach that's similar to what `ArcCell` currently does but where a reader's CAS doesn't block other readers. For example, you could use some free bits from the pointer to store a temporary reference count, then have Clone do: 1. Read {pointer: 0x123, temp_refcount: x} 2. Compare-and-swap to {pointer: 0x123, temp_refcount: x+1} 3. Increment the real reference count at 0x123 4. Compare-and-swap back to {pointer: 0x123, temp_refcount: x} (if it fails, reread to determine what the new temp_refcount should be) …but then you need a "slow path" for when `temp_refcount` overflows (alternatively: make `ArcCell` 128-bit, make `temp_refcount` a full 64 bits, and use cmpxchg16b, but that's wasteful). And you need more logic to keep readers from fighting with writers. Even then, you end up with 3x as many atomic ops as the naive solution, and cache line contention means it won't be very fast if many cores are all cloning the pointer at the same time. (But incrementing the original refcount causes contention too, so fully avoiding this requires a totally different design.) A potentially more efficient alternative is [hazard pointers](https://en.wikipedia.org/wiki/Hazard_pointer), where each thread has a publicly visible variable that keeps track of what pointer(s) it's currently accessing – but there must be some way for writer to identify all potential reader threads, which is a bit messy, and there are other complexities.
1. The trait for `%` is [`std::ops::Rem`](http://doc.rust-lang.org/1.26.0/std/ops/trait.Rem.html), and you could restrict `nth` to only types that support that. However, you also need a way to get your hands on the constants 3, 5, and 15 in that type, and that's going to be harder. 2. It's ugly, but you can create a private trait that you impl for only those two types. Though if you really just want two definitions that are otherwise the same and don't want to repeat yourself, you could also use a macro.
Getting Rust project proposals from my students tomorrow. Hoping some of them want to produce something interesting. Thinking about how I can improve [bv](https://crates.io/crates/bv)’s documentation, especially with examples. Are you doing anything interesting with bit vectors that would benefit from slicing?
Wow! The RFC and the discussion makes `myprint!` look like a minion :)
There's even a followup RFC that isn't closed yet, so it may yet be added to core in some fashion.
Personally I don't think rust is in any way a replacement for C\+\+ because it's simply got a much higher learning curve and is more frustrating to learn
Working in bioinfo/HPC, so I needed something: - fast; - easy to build everywhere; - no GC (huge datasets). I, of course, started with the C++ I knew, then decided to try rust on a whim. I fell in love, then decided to keep it when my benchmarks displayed no deviation from C++. And I suck at coding, but still need multithreading and efficient memory management, and the compiler do it for me. So, what's not to love? Oh, and my colleagues _love_ the `git clone &amp;&amp; cargo run --release`.
Part of a server. The function call is on its own thread, so blocking it wouldn't be an issue for the server handling other things. The service requesting the resource _must_ wait for a response from this call. Does that help?
Are you you looking for [Rust the game](https://www.reddit.com/r/playrust/)? This is for Rust the programing language. 
I literally started using nom 3 yesterday, what a great timing! Now I get all the improvements without having to rewrite anything yet :)
The way to do this in the crates ecosystem is using `num_traits::FromPrimitive` (as well as `Rem`). Then you can get the numeric constants. 
What's your favorite framework for getting starting with game dev in Rust?
Learn both!
[removed]
Great suggestion, thanks! I'll look into it.
It depends - careful use of numpy/scipy and the available accelerators for number-crunching code can make the remaining speed difference negligible compared to development effort.
For low-latency I've seen custom network cards + user-space network stacks being used. I can see how switching everything to ring 0 would make it possible to just stick to off-the-shelf cards!
It sounds like you're excited about Rust. One of things that has always served me well is to, whenever possible, pursue things that I intrinsically find interesting. (Another angle that I've used is to find a way to *make* something intrinsically interesting, but this is hard for me articulate.) In my experience, when one finds something intrinsically interesting, the learning experience is not only heightened, but it's *fun*. Use that to your advantage and learn as much as you can. Do it in your free time, and let your classes teach you C++. Learning both will also be advantageous since they cover very similar domains, so you'll be learning very similar material from different perspectives. More connections in your brain should make the learning process all that more enjoyable. :-) I miss my time as a student. Even though I also worked part time, there was so much time left over to learn things!
&gt; The trait for % is std::ops::Rem, and you could restrict nth to only types that support that. However, you also need a way to get your hands on the constants 3, 5, and 15 in that type, and that's going to be harder. Couldn't one strict to `Rem&lt;usize&gt;`?
Yeah. So sleeping is probably more ideal than looping if you're not doing anything in the meantime, though it does depend on the interval. Too short and you're wasting cycles in the OS scheduler; your interval may also be rounded up if it's smaller than the resolution of the system clock. If we're talking milliseconds or seconds of sleep, that should be fine. If your server is already using Tokio then you could use [`Interval`](https://docs.rs/tokio/0.1.6/tokio/timer/struct.Interval.html) and perform your API check every time the stream yields. You could turn it into one combinator chain if you do something like: Interval::new(start, interval) .and_then(|_| api_call()) // where `api_call` returns a future .filter_map(|res| res) // if `res` is an Option .into_future() // we only care about the first valid result You can combine this with [`Deadline`](https://docs.rs/tokio/0.1.6/tokio/timer/struct.Deadline.html) to ensure you're not trying forever, or use `.take(n)` to limit it to so many attempts.
Nothing is stopping you from learning, or at least becoming familiar with, both. If you are excited by rust then I highly recommend it as you will be far more motivated to learn and delve into it in more detail. But if your course requires C++ then you might need to learn enough of it to get by and you never know you may find it interesting as well. Really there is nothing stopping you from getting your feet wet in both languages as required and they will likely not be the only/last languages you learn. It also becomes easier to pick up other languages once you have the core understanding of one as the fundamental concepts apply to most languages. So I would suggest you pick one - whichever you prefer and start learning that. Pick up the other as required or if your interests change or just you want to gain some more perspective on your chosen language. Once you know a bit more about both the languages you will be better able to answer this question. I am not really sure what else you are expecting from this sub as most people here are here since they like rust and so will be biased towards it.
a lot of people started to use nom 4 long before the release, since it was available as 4.0.0-betaX on crates.io :) So at least it's stable!
/r/playrust
I find the HTTP benchmark (bottom of the article) really impressive with regard to nom's performance!
Those numerical computation libraries tend to FFI into C for the actual number-crunching. The advantage is that end-users get to use a nice python API instead of trying to manage safety themselves.
Probably you don't have the time to learn both simultaneously. So my suggestion is to lean first mainstream languages and that would be C++ in this case. The reason is just you'll gonna need these mainstream languages in practice. C++ is also fun and interesting to learn. But after one or two years, you can start with Rust. BUT: This is just an opinion! I can't tell you, what really is the best for YOU!
&gt;We have a graphics course in our uni, and i am pretty sure they use c++ for that. You can do low-level graphics in Rust as well (if not better) as in C++ (check [`glium`](https://github.com/glium/glium), [`vulkano`](https://github.com/vulkano-rs/vulkano), [`gfx-rs`](https://github.com/gfx-rs/gfx)) The biggest "why I shouldn't prefer Rust" is that for some domains ecosystem is still in its infancy. (e.g. while I think Rust will be an excellent choice for robotics, but C++ has decades of library development behind it, so it's very hard to compete) Rust can be a very rewarding language if you like to write something from scratch and get to know stuff from inside out, but not so if you want quick and dirty solutions.
Clippy may check them, I haven't tried it. I really want it as a warning in the compiler; few people run Clippy, and in my opinion this is really no less of a "code smell" than an unused variable declaration. I have filed an [issue](https://github.com/rust-lang/rust/issues/50751) with `rustc`, although I fear the dreaded RFC process will be required.
Cool! What's the goal with these bulletproofs? Where do you want to use them?
I hope I can finish the man page for [commitmsgfmt](https://gitlab.com/mkjeldsen/commitmsgfmt)... which technically isn't Rust. I wish somebody would brave an AsciiDoc Rust implementation (ideally one that doesn't attempt to replicate asciidoc(1)'s idiosyncracies), that would make man pages so much easier to manage. I would if I were any good.
&gt; GameBoy emulator I'm also [working](https://github.com/gil0mendes/rustboy) on one myself, but is on hold for now :/ This week I'm working on the port of my old bootloader into Rust, little baby steps here 😅
&gt; **1. In your experience as Rust programmers, would you say I'm wrong to think this is a problem?** &gt; **2. What does Rust do to mitigate this problem?** This is definitely a downside of the single-owner model. I would love to see alternatives made easier to use, or at least easier to learn. Most people seem to learn the `Rc&lt;RefCell&lt;T&gt;&gt;` and index-based solutions fairly quickly. But you can still use an arena or even just raw pointers, which are probably the most common solutions outside of Rust. The fundamental question that all these alternatives are answering is "who's in charge of freeing these nodes?" Depending on the data structure, the answer to this question may be trivial, like with parent pointers in a tree- in which case wrapping up some `unsafe` raw pointers is probably fine. Or it may be more complex, in which case you may *need* `Rc` anyway. Or it may be somewhere in the middle, and an arena or index-based solution would suffice. &gt; **3. What else could Rust do to mitigate this problem?** The main area I see for improvement is around interior mutability. I think the naming there is kind of unfortunate- in these scenarios it's usually better to think of `&amp;T` as a "shared" reference and `&amp;mut T` as a "unique" reference, rather than anything to do with mutability. `Cell` is a fairly under-appreciated tool. `RefCell` is easy to get started with- just slap it on an entire node and `.borrow_mut()` away. `Cell` has less overhead, but requires modifying the type to wrap leaf fields, which is more intrusive than `RefCell`. But this isn't strictly necessary for memory safety- you can go from [`&amp;Cell&lt;[T]&gt;` to `&amp;[Cell&lt;T&gt;]`](https://github.com/rust-lang/rfcs/pull/1789). Ideally it would also be possible to do the same [on struct fields](https://internals.rust-lang.org/t/idea-derefpin-derefcell/7292). If `Cell` were also a little more transparent (`=` instead of `set`, for example) this would allow `&amp;Cell&lt;T&gt;` to work basically like a third reference type for "shared mutable" data, sitting somewhere between `&amp;T` and `&amp;mut T` in what it allows. With this new `&amp;Cell&lt;T&gt;`, we could have arena interfaces that just return `&amp;Cell&lt;T&gt;` on allocation, without requiring any intrusive changes to the definition of `T`. This would also work great with eventual GC integration, which would provide another option on top of arenas or index-based graphs.
I heard it's better, but I haven't had the chance to try it.
I feel reminded of Microsoft Reseach's .NET OS: [Singularity](https://en.wikipedia.org/wiki/Singularity_(operating_system))
I like [ggez](https://crates.io/crates/ggez/), but I'm biased since I'm also the primary maintainer. ;-) Amethyst also has a lot of really nice ideas and great people involved, but has a lot of work to do still as well.
Why can't the Rust compiler use soft-128 (or 256, 1024, etc.) bit integers where the arch doesn't support them?
Out of curiosity, any thoughts on Nom's future in regards to generators, and `impl Trait` compared with macros? 
&gt; We expose add with carry via `overflowing_add` Oh awesome, I somehow missed that was a thing. :D &gt; I think a nicer API would be to return a number with twice the number of bits, though. I agree up until the point where you need bignums anyway to implement it; if we wanted to implement `overflowing_mul` for `i128` for example we would need it to return an `i256` type, if we wanted `overflowing_mul` for `i256` we would need an `i512` type, and so on. Having `overflowing_mul::&lt;T&gt;()` return `(T, T)` is more composable, if a little less convenient.
That same talk inspired me to do something similar with lua. I got to the point where I was able to write to video memory using luajit's FFI, then shelved it when I realized I'd have to delve into actual operating system design. I wish OP the best of luck on his project though, seems nifty.
* Learn to be a programmer, which is someone who can pick up any language relatively quickly. (though Rust can be an exception!) * it is always good to know some C / C++ because it is foundational to much of the computing world and is one of the few languages that lets you get pretty close to the hardware, and really understand memory, intrinsics, SIMD, and so on. * Rust is amazing and I think may have a bright future, it is worth learning as well, especially if you are interested in fields where performance and/or correctness are very critical. space ships? cryptocurrency? financial industry? game servers? So yeah learn both. 
That was super well-written and clear. Thanks for sharing it! I tell my students that these days compilers are usually better at assembly than we are, but they rarely believe me. :-(
I had a lot of fun getting to that result! I'll probably release a blogpost to explain the process.
[nom works well with generators](https://github.com/geal/generator_nom) (and also [with tokio](https://github.com/sozu-proxy/lapin/blob/master/futures/src/transport.rs#L22-L45)). Regarding nom's future, I might change the internals to be able to build parsers without using macros (but probably keeping macros as a compatibility layer), but it's not planned yet. I have to do some tests first :)
I opened a thread at https://old.reddit.com/r/learnrust/comments/8jej7x/borrow_checker_problem_implementing_a_cursor_into/, could anyone help please?
AFAIK, this won't work because Tokio does not support futures2.
you could try https://github.com/antoyo/asciidoctor-rs . Not sure how complete it is, just noticed it while looking at relm stuff.
Yes, that's UB. When you call `drop`, the memory is deallocated, and `a` is now dangling.
We're one PR away from only fixing what the compiler/lint authors mark as "machine fixable" -- this will greatly reduce what rustfix fixes but to an equally large amount also what it destroys. The current implementation is actually quite conservative. It doesn't fix overlapping fixes, and it bails out and completely reverts the fixes when the result no longer compiles.
Yeah, I'm on Linux, that's quite interesting, at first from my experience in C\+\+ I was thinking that this example was UB, but that output tricked me and make me doubt about it, thank you.
No problem! That’s the tricky bit about UB: it can “work” sometimes.
Great to see 4.0 out! :tada: Somewhat unfortunately, my use case has diverged enough from general parsing that using nom won't work so well for me anymore. I've decided that I'm going with a [modal lexer](https://www.oilshell.org/blog/2017/12/17.html) (almost entirely with simple regexes) rather than the lexerless structure that nom lends itself to, and I'm parsing into an experimental [libsyntax2](https://github.com/matklad/libsyntax2) inspired tree, and one very tolerant of faults. I'm also setting up a `slog` logger to be passed through and `trace!` log the whole stack because my grammar is complicated enough that I need that. I'm sticking to the combinator style that nom introduced me to, though, and the testability it gives is unparalleled to any other solution I've used. My stubborn self wants a canonical grammar that I'm implementing against though so I'm still fighting ANTLR4 to do what I want; non-keyword keywords and whitespace yes/no sensitivity in only a few spots is so much easier in a handwritten parser than generated. Nom's plumbing has heavily inspired my internal plumbing, but it's just specialized enough and just different enough that the effort to use `nom` proper seems a bit much. ----- Now I get to finalize the nom-locate update to 4.0 as well. [Shameless `nom_locate` plug] (https://github.com/fflorent/nom_locate), a library for nom that makes tracking your column/row easier, and might as well [link my pet project] (https://github.com/nafi-lang/rust-nafi), though it's in a really messy state right now.
Here is the RFC link for anyone that hasn't seen it. https://github.com/rust-lang/rfcs/blob/master/text/2394-async_await.md
Nice! You should make an annoucement here when you release, I bet some people would be interested.
I've started to work on a [netlink](https://www.linuxjournal.com/article/7356) library two weeks ago, and I'll probably continue implementing packet parsing this week. I believe rust is a very good fit for networking protocols, but on linux, we lack a good interface between the control plane and and the data plane (aka the kernel).
Here's my experience report from porting over Askama just now: * I ran into some problems where I had a named lifetime specified for `named!` parser return types, like this: ``` named!(parameters&lt;Vec&lt;&amp;'a str&gt;&gt;, do_parse!( ``` While this worked with nom 3, it seems that nom 4 does not allow it anymore. The simple fix here was to remove the `'a` (since lifetime elision probably covers it, anyway). * The new version exposed some problems for me with unused variables (copy/paste error) that didn't turn up in the old one. Very nice! * Unsolved for now: I'm getting a whole bunch of errors for `Many0`. I think this might be due to the breaking change about `many0!` returning `Incomplete` on empty input or reaching end of input without an error from the child parser. The description of the breaking change here definitely doesn't match my intuition for what `many0!` is supposed to do... what's an idiomatic way of getting the old behavior of `many0!`?
There's a PR to httparse that adds SIMD support (both SSE4 and AVX for even faster), and it helps a lot! Just waiting for it to be stable...
I will! If you (or anyone else reading this!) want to help out, I've already opened some help-wanted issues :)
How viable is it to run Nebulet on architectures other than x86_64?
Thanks for the helpful reply! That combinator flow is really cool. I also like the idea of using an Option as a return type for my API call, that feels way "Rustier". I'll probably refactor my code to follow that pattern. Thanks!
Jesus how is D so quick? That's impressive.
Ironically enough `ArcCell` seems like a pretty good way to implement an RCU datastructure, since the `Arc` refcount will keep track of the current readers and free when there's no more. So you'd still have an ABA problem when deciding when to free the old version of the data, how is that addressed with RCU? The Wikipedia article doesn't go into details except that it seems to suggest the same kind of spin lock.
If you explicitly wrap the rotation count to up-to-16 like such xorshifted.rotate_right(rot &amp; 0b1111) you’ll get a `ror`. Not sure why LLVM cannot figure it out without this, why it doesn’t do the same when the rotation count is wrapped to 31 (`0b11111`), or why it is unable to derive the same information from the shift by `59` alone.
We have applied a few fixes here and there (enabled LTO for C++ and Rust, switched to `panic=abort`, and shaved some `.clone()`s in Rust and used `const shared_ptr&lt;&gt;&amp;` in C++) and the results are now that Rust is slightly faster than C++ shared_ptr version (0.37s vs 0.38s) though now it is weird to see that idiomatic Rust version is a little bit slower than ref-counted Rust version O_o
How about you guys collaborate?
No question: Learn both. But assuming you already know another language (C, JS, something like that), you might want to start with Rust!
You should submit a bug report.
Right now, it's not very feasible. While cretonne will support more architectures eventually, right now it only supports x86\_64.
Great news, congrats! I know what’s left to do with `glsl` then… ;)
While it is still small, consider ports for ARM and/or RISC-V, as this may be great for IoT and microcontrollers (larger ones, obviously) Also, I am not a graphic designer, but I have been struck with an Idea for logo, is svg ok?
Continuing work on [tarpaulin](https://github.com/xd009642/tarpaulin), tweaked the syntax analysis some more but it still requires a change to `RUSTFLAGS` to build. An issue was raised which showed that coverage doesn't work on nightly rustc versions after 2018-04-16. So I'm currently disassembling things and analysing what's changed. Hoping to fix it by the end of the week, but it might be a biggy
The reason I actually first got into writing Rust code is thanks entirely to Kiss3d (https://crates.io/crates/kiss3d). I needed a simple way to view a bunch of spheres in 3D space, and this crate allowed me to ignore almost all of the tedious and error-prone OpenGL boilerplate
I came here for Option and Result, stayed for the community. Seriously, why any new language will continue making the [billion dollar mistake](https://www.infoq.com/presentations/Null-References-The-Billion-Dollar-Mistake-Tony-Hoare) is beyond me. 
&gt; it was because he caught a few bugs for you lol you just assumed the compiler's gender
Refactoring [https://github.com/chmln/asciimath\-rs](https://github.com/chmln/asciimath-rs). Its a variable mathematical expression parser and evaluator focused on ease of use. So far it's been a great pleasure reducing loop blocks into a handful of iterator methods. It is also my first Rust project, so if you see any code that could be improved I'd really appreciate some feedback.
&gt;I tried to re-implement some structures (List, Queue, Stacks, Trees, etc) using all that I already know from theory and C++ (I already have them implemented in C++), but for that I need some freedom on my pointers Have you run into [Learning Rust With Entirely Too Many Linked Lists](http://cglab.ca/~abeinges/blah/too-many-lists/book/) yet? If not, it sounds like you're trying to rediscover what it walks you through.
oh, that's awesome, the code is really nice to follow!
Could well be! Hope that's how it will turn out. Certainly seems like you've done a great job of getting the experiment set up for the best possible outcome.
Filled https://bugs.llvm.org/show_bug.cgi?id=37461.
I think one of the main motivations is consistency and making it slightly nicer to ease into learning Rust. See https://github.com/rust-lang/rfcs/pull/2071#issuecomment-329026602 ("The Dialectical Ratchet").
I appreciate knowing the timetable too.
I like the indices solution. It makes your data structure serializable without much extra work.
Quoting from our previous blog post https://blog.chain.com/faster-bulletproofs-with-ristretto-avx2-29450b4490cd: &gt; Zero-knowledge range proofs are a key building block for confidential transaction systems, such as Confidential Transactions for Bitcoin, Chain’s Confidential Assets, and many other protocols. Range proofs allow a verifier to ensure that secret values, such as asset amounts, are nonnegative. This prevents a user from forging value by secretly using a negative amount. Since every transaction involves one or more range proofs, their efficiency, both in terms of proof size and verification time, is key to transaction performance. 
FYI, only a few dozen pixels at the top are visible for me. The rest is behind some sort of mask.
Very interesting. I recently was working on some optimizations for the `pcg_rand` crate and I got it around the same speed as xorshift \(from the Rand crate\). If you are interested in PCG have a look the pcg\_rand crate, I would love a second set of eyes to make sure my port is correct.Next on my TODO is to clean up the code and improve testing. I would love to put together an easy rust binding to a random number test suite to make sure the implementation is sound. Additionally I want to bring the simple C version in as a dev\-depenency as a sanity check.
Also, learning Rust will probably teach you a lot about modern C++ and why it is the way it is.
Since it's like reflection, but happens before run time, how about "preflect" for the name?
Right this (software fallbacks) are happening for 64 bit numbers on 32 bit platforms and 64 as well as 32 bit numbers on 16 bit platforms. This is partly LLVM's, and partly the job of compiler-rt. Rust ships with a RIIR of compiler-rt called `compiler_builtins` (still using stuff from compiler-rt). The algorithms are generic over the bit size and are thus shared. We then provide a C interface over to allow LLVM to call and/or inline them. But the backend still needs to support *some* things for i128 and wider number types, like handling it as return value or stack allocation. And that support is lacking in the non-main backends, see my other comment in the thread (currently above this one). A solution to this would be [fully "soft" emulation](https://github.com/rust-lang/rust/issues/45676) but right now it is not complete yet.
Rust and C++ are similar enough that most things you learn for one will be at least applicable to the other.
I would be delighted if it were somehow useful ^__^
While I dont like preflect much for the name, I do agree with the sentiment that this should be called something more than reflection. Compile time reflection or something entirely different would be a useful delineation for those coming to the language for the first time.
Now the blind can read my hex dumps! In all seriousness, this is actually very cool. Clever and fun application of rust, I might have students write something like this as a challenge.
Nope, I'd never used it before. Thank you!
True, but to an extent depending on your architecture, it is *so much* better than using straight JS (even though that bar is pretty low these days).
&gt; I'm trying to expand my skills as a coder This is a great reason to learn new languages. I recommend picking some languages that represent paradigms of computation that you're not as familiar with. For example, learning prolog to learn more about logic programming or learning Haskell to learn more about functional programming. Learning new models of computation is really mind expanding and also has practical value. For instance, it improves your ability to solve problems in new ways. Needing different compute models can show up in unexpected places too. C++ programmers don't need to know much about functional programming right? Well, if you're doing template meta programming, that's a purely functional compute model.
Thanks!
I don't know what your requirements are I guess, but you never know until you give it a try. Otherwise I would just wait for the async situation in Rust to stabilize.
If reflection is just asking about informtion of what something is at runtime, and this is just doing the same to resolve compilation; Then `const reflection` seems like a fine and non-confusing name.
Also the other way around. For me, studying about copy/move constructors and RAII in Modern C++ has tremendously helped me in learning Rust concepts. (Soon you figure out that Rust is what Modern C++ is aiming to be without all the old cruft...)
This is a (fair-to-middling quality, sorry) video of a talk I gave at the SF Rust meetup last week, hosted by CloudFlare. I'm actively looking for people to come help keep Rust programs fast! AMA, I guess? Slides: https://docs.google.com/presentation/d/1BEI7zXhEiCwEd93-UUpWv-Yv5azRmBa5caPH0rCAh_Q GitHub: https://github.com/anp/lolbench Original irlo thread: https://internals.rust-lang.org/t/help-needed-corpus-for-measuring-runtime-performance-of-generated-code/6794/31
rust-geo looks great and I'll look into it. Thanks for your information...
here http://namaristats.com/langs/Rust
I think this is somewhat disappointing. Lots of people had their hopes up about using Rust for small microcontrollers and now that support is given up so easily. I think Rust upstream needs to slow down a bit with adding new features and rather focus on stabilizing everything and increasing portability, so that the language has better chances of getting adopted more widely. Maybe LTS releases would be an idea.
With SIMD instructions, most likely.
I can very easily imagine a world where it is far easier to get a feature out (and thus share value) by adding it to a widely adopted language than trying to create a new one and convince people to use it.
You should make it an iterative solver that continually steps towards a more-fixed state... ... including program speed and clarity optimizations... ...and have it terminate only when it either reduces the program to `return Ok(())` or evolves sentience.
I greatly know the feeling! I read the Kademlia spec and said to myself "this seems simple, why is IPFS so complicated?" And decided to write my own distributed content-addresses THINGY to find out. I'm currently pretty sure I can make tokio do a ping request, so progress is strong and spirits are high. How's your effort going?
The idea of static reflection isn’t new. For example, here’s Herb Sutter’s Cppcon 2017 talk about his thoughts on static reflection for C++: https://m.youtube.com/watch?v=4AfRAVcThyA
Honestly, I think that you should learn C++ first, then Rust. C++ is mainstream, you will be ready for real projects if you learn it. Rust is great so I recommend learning it eventually.
I was going to suggest a framework to replace all references with pointers, run each emulator in it's own thread have them fight. Your idea is probably better. Unless they use unsafe poorly it would be a pretty tame cage match anyway.
Thank you for such a detailed reply. I will definitely look into this :)
I'm confused about all the comments on the `asm!` macro producing suboptimal assembly. If this is so bad, couldn't you provide a separate asm file and then link it in as a C symbol instead of writing it inline? Or is this not feasible in Rust? (Obviously the fast Rust-with-`i128` is a better solution in every way, but if they spent so much time hand-optimizing the asm it seems like this would be the obvious next step)
I'm definitely with u/burnsushi here—I typically use `failure::Error` in my high level code, and I consider showing a `Debug` implementation to a CLI user to be a bug, even in a fairly immature application. I feel like the Rust community is missing an opportunity to do something elegant and ergonomic here, and I don't expect to ever use the `Debug` version of ?-in-main. It's not a huge deal; I have a couple versions of a `quick_main!` macro that I've taught my co-workers to use. I'll just keep using it. But I think `Display` would have been a much better choice.
Just checked out the repository, cool project! Might have to build my own after I learn some more rust. 
Is there a concern that having a #[derive(ToOwned)] would facilitate suboptimal API designs? Because it seems like a pretty big hole in the baked in derive implementations as it would solve a lot of the issues people encounter w/ lifetimes 
This is the kind of case where I might reach for reference counting. Rather than generalizing over owned vs. unowned, you switch to having multiple owners and not having to care how many there are. You can even have `Rc&lt;str&gt;` or `Rc&lt;String&gt;` to store the input strings. The main wrinkles are lifetime rules (can’t put non-static references in there) and memory leaks (to avoid this, the ownership structure must be a DAG).
It looks like C++ turns its `split` implementation into a tighter loop because it's tail recursive there. Rust doesn't quite manage to get to as nice a loop as that if the Rust version is made tail recursive in the same way, but manually writing the loop version is slightly faster (~10%). A bigger problem seems to be the destructor for `NodeCell`: it is recursive and so isn't inlined at all, but is called a lot when assigning to the various `NodeCell`s that are `None`, which means there's a whole pile of function call overhead for what is essentially an `if x.is_some() { ... }` that is never taken. It shows up as `core::ptr::drop_in_place`, and seems to take 20% or more of execution time by itself. Addressing this makes the code essentially as fast as the C++ unique_ptr version: I did it by making the `left` and `right` fields `ManuallyDrop`, and adding a `Drop` impl that just runs `self.left.take(); self.right.take();`. This move the recursion out of `drop_in_place` itself so that the `Some` check there can be inlined. It seems like Rust should either investigate enabling [the partial inlining optimization](http://llvm.org/doxygen/PartialInlining_8cpp_source.html) ("typically by inlining an if statement that surrounds the body of the function": exactly this case) or change the code generation for `drop_in_place`, because recursive structures with `Option&lt;Box&gt;` are likely to all suffer from this. Also, as an "above and beyond" optimization that (slightly) changes the behaviour, switching to a `typed_arena` and `&amp;mut` instead of `Box` cuts the execution time even more: to &lt;10% slower than C++ "raw", and slightly faster than C++ "unique_ptr". (Presumably C++/other languages would also benefit from arena-allocation: I suspect D is benefiting from something along these lines, with its GC likely having relatively arena-like allocations and "deallocation"/reassignment not needing any sort of checks like `unique_ptr`/`Box`.)
PRs welcome ;) I’d probably start with a mode adds `&amp;`, `*`, and `unwrap` to broken programs until they compile, though…
Historical tidbit: Rust had reflection. [RFC 379](https://github.com/rust-lang/rfcs/pull/379), which proposed to remove reflection, has a description of what it had.
It seems like it should be helpful, but unfortunately, IIRC, there's not many SIMD instructions that are particularly helpful for high-bitwidth integers. On x86, there's [various `align` instructions](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=align) which allow effectively doing shifts/rotations on `u128` in a single instruction, but they only work in multiples of 8 (at a minimum), and the 256/512-bit versions don't do shifts of `u256`/`u512`. There's also some for shifting 128 bits at a time, by individual bits (`pslldq`)... but the shift amount has to be a constant. Other than that there's very little that operates on larger words, and there's even only 32-bit integer multiplication, not 64. There's also limited (automatic) detection of things like overflow, and there's no tricks like `adc`/`sbb`.
Easy, learn both. Both languages have their shortcomings compared to the other.
This code compiles: struct Foo; let a: [Option&lt;Foo&gt;; 4] = [None, None, None, None]; but this doesn't struct Foo; let a: [Option&lt;Foo&gt;; 4] = [None; 4]; error[E0277]: the trait bound `main::Foo: std::marker::Copy` is not satisfied --&gt; /tmp/a.rs:3:31 | 3 | let a: [Option&lt;Foo&gt;; 4] = [None; 4]; | ^^^^^^^^^ the trait `std::marker::Copy` is not implemented for `main::Foo` | = note: required because of the requirements on the impl of `std::marker::Copy` for `std::option::Option&lt;main::Foo&gt;` = note: the `Copy` trait is required because the repeated element will be copied I understand the error messages, but is there any way to initialize an array with None without repeating it?
&gt; You can do low-level graphics in Rust as well (if not better) as in C++. (check glium, vulkano, gfx-rs) Not exactly. None of these libraries is mature and stable. For example, gfx-rs is going through a big rewrite atm if I'm not mistaken. Feel free to correct me if I'm wrong.
Why decide? You will be best off if you do both.
If you are planning to write full-blown commercial game engine, then yes, you can't consider them mature and stable. But for university project I believe they are more than applicable.
No. I mean, you can do things like write macros to repeat the `None` over and over again, but they have to expand to `None` repeated the right number of times. You could write unsafe code to incrementally initialise an array, but you'd still have to implement it separately for every different size of array... or use a macro that expands to one `impl` for every size of array. As an aside, I'm assuming you've noticed the note in the error about `Copy`.
please file a bug at [https://github.com/rust\-lang/rust/issues](https://github.com/rust-lang/rust/issues)
I'd be interested to see if I could combine this with my work on `darling` to give people simple attribute parsing coupled with robust code generation. I struggled with both areas when I first attempted to write custom derives, and I suspect I'm still doing the latter poorly.
A big feature I know I am missing is jump ahead. Depending how long ago it was I hadn't implemented extensions yet. But yes if you are interested let me know when you have time and we can discuss improvements.
While the name sounds nice, I'm not sure it is technically right. As I understand it, while macro and const evaluation are both runtime mechanisms, they are distinct features. Macros are resolved before const evaluation. Maybe \`\` macro refection\` would fit better.
That's really nice! I'm going to follow the series and hopefully learn a few things from you. I'm still an amateur in emu development and this has been a crazy journey, but sure as hell fun!
I've used DPDK with Intel NICs... You don't need custom network cards to use fast userspace networking.
I've put together a library to do code generation that I use in reproto called [genco](https://github.com/udoprog/genco) \- For more complicated languages \(Java, C#\) I achieve correctness in a similar way: Instead of emitting quoted code directly, I build up type\-safe data structures corresponding to the code being generated. I quickly realized that in order to be general enough without having to reflect _everything_ in the languages I target, I had to provide an escape hatch permitting structural embedding. It looks like you are taking that one step further and describe _every_ statement as data structures through these "reflective operations". It doesn't look like the most natural way to accomplish this, but it certainly is the safest. I'll be following this project with interest to see if it's an approach I want to ~steal~ borrow eventually.
Your parse solution boils down to 'can i make a cycle ?'. The answer with regards to just pointers in safe rust is no. Rc is a good option. Another solution is to split up a run into multiple phases. Depending on the requirements you might simply try: while let Some(p) = self.unparsed_files.pop_front(){ ... parse p and push other file names .... } 
It’s going a bit better. I’m also working on Kademlia like UDP p2p registry thing. Finally got Tokio behaving well enough but now channels are being a pain.. slowly but surely. Loving Rust overall tho.
Thank you :)
&gt; Meanwhile the library is tracking the control flow and function invocations to build up a fully general and robust procedural implementation of the author's macro. So it's like symbolic execution? But presumably you can't actually symbolically execute Rust code, so how does it work?
what are your findings? If its still to early i would be happy to read about it later in a blog post or something ...
Now it says: WebAssembly.Instantiate is not a function
Thanks for the information. I found [this](https://stackoverflow.com/a/33764418) discussion to repeat block using compiler plugin. After several attempts, I could write the followings. #![crate_type = "dylib"] #![feature(plugin_registrar, rustc_private)] extern crate rustc; extern crate rustc_plugin; extern crate syntax; extern crate syntax_pos; use rustc_plugin::Registry; use syntax::ast::LitKind; use syntax::codemap::Span; use syntax::ext::base::{DummyResult, ExtCtxt, MacEager, MacResult}; use syntax::ext::build::AstBuilder; use syntax::tokenstream::TokenTree; fn expand_repeat_none(cx: &amp;mut ExtCtxt, sp: Span, tts: &amp;[TokenTree]) -&gt; Box&lt;MacResult + 'static&gt; { let mut parser = cx.new_parser_from_tts(tts); let times = match parser.parse_lit() { Ok(lit) =&gt; match lit.node { LitKind::Int(n, _) =&gt; n, _ =&gt; { cx.span_err(lit.span, "Expected literal integer"); return DummyResult::any(sp); } }, Err(_e) =&gt; { cx.span_err(sp, "Expected literal integer"); return DummyResult::any(sp); } }; MacEager::expr(cx.expr_vec(sp, vec![cx.expr_none(sp); times as usize])) } #[plugin_registrar] pub fn plugin_registrar(reg: &amp;mut Registry) { reg.register_macro("repeat_none", expand_repeat_none); } --- #![feature(plugin)] #![plugin(repeat)] fn main() { struct Foo; let a: [Option&lt;Foo&gt;; 4] = repeat_none!(4); // repat_none!(4); becoms [None, None, None, None]; // Actually, I want to write like the following, but I'm not sure how to do it.. // let a: [Option&lt;Foo&gt;; 4] = [repeat!(4, None)]; } Maybe there is a better way to do this.
&gt; I don't really see why this is a thing since it just adds yet another generics syntax that is strictly less powerful than the other two. This is just the dialectical ratchet for generics. New users can learn that traits are "like interfaces" and that `impl` means implement. They can use `impl Trait` everywhere without having to learn the kinds system (`&lt;...&gt;`) and they can get a lot of stuff done with that. This is similar to references, where new users can get a lot done with `&amp;` and `&amp;mut` without having to learn the kind system either. At some point they'll need to do something that `impl Trait` in argument position cannot do, in the same way that they will need to do something that just `&amp;`/`&amp;mut` cannot do. At that point, they can learn the kind system and discover that `impl Trait` in argument position is just sugar for the most common case for it, in the same way that `&amp;`/`&amp;mut` is just sugar for the most common cases of life-times. 
You get `ToOwned` for free if you derive `Clone`.
I had similar idea a while ago. I'm very happy to see someone implement it! Keep up good work!
That implementation of ToOwned just calls .clone\(\), which doesn't solve the lifetime issue. What I had in mind is a ToOwned implementation that provides a struct with all fields transformed to their ToOwned::Owned type. For example: #\[derive\(ToOwned\)\] struct Struct\&lt;'a\&gt;{ a: &amp;'a str b: &amp;'a \[u8\] } providing: struct Owned\_Struct{ a: String, b: Vec\&lt;u8\&gt; }
It is ok. I used it half a year ago to fix ~120 clippy issues in a few hours in my project. It broke some semicolons I think, but that was about it. Easier to fix that than going through clippy issues manually. Great tool for doing rough cleanup, but yeah, it may eat your code (but not substantially).
&gt;However, if I try to parse the DSL, read the imports of the parsed AST and add them to the DST vec, I will encounter errors as the DSL vec is borrowed by the AST vec and cannot be mutated. At some point in that while loop I'll still have to: let dsl = read_to_string(p); asts.push(parse(dsl)); // &lt;-- lifetime issue, dst gets dropped here. or: dsls.push(read_to_string(p)); asts.push(dsls.last()); // &lt;-- borrow of dsls here, but is mutable in the loop I don't immediately see how Rc's could help in this scenario. The issue is not with cyclic references, it is with caching structs (AST) that reference some &amp;str, in this case the DST, so the DST &amp;str has to outlive the Ast. There are no cyclic references as the ASTs do not reference eachother, they only contain references to their source file.
Next step: A library with serde support :P
Are there any benefits to using Rc\&lt;String\&gt; over just using String? As my goal is to cache the ASTs there should only be one reference to them. Not that struct Ast\&lt;'a\&gt;{ imports: Vec\&lt;&amp;'a str\&gt; } Does not hold any references to other ASTs, only a list of paths. 
Sorry, I'm French and with don't have any equivalent to the "it" pronoun so it's quite unnatural for us :)
Or how about having each major IC emulation in it's own thread? Scaling then would be amazing
Sadly, I have been hit twice already. Very recently with [this](https://github.com/SergioBenitez/Rocket/issues/635) one. I had to force a specific version of nightly+rocket. I understand there is a lot of work in that direction but i wouldn't recommend it for production just yet.
Writing parsers is a very interactive process: write a small sub parser, go through data, try again, build larger and larger stuff. I can definitely see how it can be fun!
Just read the slides, but they look nice. You might want one of the Rust versions of my [tic-tac-toe benchmark](http://github.com/BartMassey/ttt-bench) in your suite. It does something I really care about the performance of (game-tree search) but that is a bit different from "normal" benchmarks. I just freshed everything up this morning because I was thinking of doing a Reddit post, so it should be all ready to poke at.
Getting a bit offtopic, but especially if some fields have a different size in the two dumps vbindiff is probably the better choice. Very lightweight too. Hm. Maybe I should try reimplementing that in Rust sometime :)
I really wish there was some kind of standard for this in unix tools. base64 has the -d flag, xxd has the -r flag, uuencode has uudecode etc.
Not sure if you're aware, but reportedly [NetBSD has support for Lua in the kernel since 2013](https://www.phoronix.com/scan.php?page=news_item&amp;px=MTQ4ODk).
 pub enum Needed { /// needs more data, but we do not know how much Unknown, /// contains the required additional data size Size(usize) } This can be `Size(NonZeroUsize)`, cutting the enum's size down by `mem::size_of::&lt;usize&gt;()`. After all, *"needing 0 more bytes to continue parsing"* makes no sense at all and probably indicates a bug. The smaller `Needed` becomes, the smaller `Err` gets, and the faster passing around `Result`s will be. *(Hence why `failure` loves to box errors.)* Sorry that I did not catch this during the beta phase. I didn't even know about the beta, because currently I'm not doing any parsing.
if your array is small you can take advantage of Default: https://play.rust-lang.org/?gist=56239c8140a292a7fce0a139faf64d5e&amp;version=stable&amp;mode=debug
&gt; As my goal is to cache the ASTs there should only be one reference to them. There would be one permanent reference (the cache) and one or more transient references (as algorithms traverse the AST), correct?
You scientific guys should really think about naming your creatures! While [yaiouom](https://crates.io/crates/yaiouom) may be a great name like uom, I will never remember it without googling first.
yew is fun.
&gt; But that would require me to update all the parsing code, and would disallow the use of string slices where possible. Where would it be possible? You're already saying you want all ast's in a big vec, not just for caching but also analysis. In this scenario you always need the ast's to own their data, right? In any case, what generic parsers such as serde or HTML parsers often do is just generate a stream of events and something else is responsible for building a tree or whatever from that. It's a lot of effort for a single-use parser though.