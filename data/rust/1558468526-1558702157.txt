My spaghetti code is certainly art on it's own.
&gt;Unfortunately, what compilers most love in the world is to prove that something is Undefined Behaviour. Undefined Behaviour means they can apply aggressive optimizations and make everything go fast! Usually by deleting all your code. This article is really entertaining. &gt;In the section on untagged unions, I noted that in the extreme case you could make an UntaggedOption type. ... Well it turns out that's all that MaybeUninit is. [Mostly]... But that's it. The compiler doesn't even know about it as a special type. It's just a union with a dummy "uninit" case. The Rust Language developers are so clever. They keep coming up with things that make *so* much goddamn sense. Wow. This is awesome.
I agree with you, it seems like a very complicated solution to save two characters. This isn't unlike the recent `await` syntax battle that was started to avoid typing two parentheses (`(await expr)?`). I also find that the solution breaks *my* expectations of Rust: if a field is publicly accessible, I expect that I can modify it without causing problems. If not, then I'd rather have a getter, like `.len()` or `.capacity()`.
&gt; I would disagree with this. Most CISC CPUs are RISC behind the scenes with a layer made to appear like the CISC one. You actually agreed with it. This is exactly why CISC is great. Yes, the CPUs use RISC-like designs for the internals of the cores, because that is simpler and works better for hardware design, but *this is actually enabled by CISC*. The point is that *these details are not exposed to the outside world*. CISC makes for a great interface to the outside world, because it leaves the CPU designers free to do whatever they want internally (translate into any design, usually RISC-like, whatever they find works best, but keep it unknown to the outside world and hence free to change and improve with every CPU generation without breaking people's software). RISC instruction sets expose more of the CPU's internal design to the outside world, meaning that it has to stay compatible to avoid breaking people's code. Software must stay compatible. People need their software to keep running as they upgrade their computer hardware. Incompatible binary instruction set architectures are a massive hassle that fragments the world. This is far more important than the raw performance or simplicity or elegance of the design. This is why CISC wins. It allows the hardware to evolve without sacrificing compatibility. By hiding the low-level details behind an abstraction, they are free to change. A RISC ISA doesn't allow for such freedom. CISC is a fantastic abstraction that allows us to have 25+ years of software compatibility with ever-evolving CPUs with incredible performance. This is exactly thanks to our CPUs having become some truly amazing hardware JIT compilers. Nowadays we can fit massive amounts of transistors in a CPU to form very complex circuits, so having such a hardware translation layer is not a problem. Even ARM are moving to become increasingly more CISC-like over time.
&gt; This might be useful for future reference but I don't use C++ at all. Understood. My reply was just to point out (for future readers of this page) that "This C++ optimization seems like it leaves performance on the table" is not necessarily the case. The default for GCC's `shared_ptr` is to use atomics in the presence of threads, but there's a way to avoid that. i.e. "Instead of trying to do optimizations in the background allow the programmer to specify what he wants" (but obviously in C++ the compiler doesn't check you're not making mistakes). &gt; without the instances of "if you know what you're doing" and "so you're on your own" that you had to use. Yes, getting the most out of C++ requires you to know what you're doing. That shouldn't be a surprise to anyone, especially in r/rust :-). I'm not trying to have a language pissing contest, I'm just adding some additional information relevant to the article that you commented on. However, the "you're on your own" was in reference to avoiding the standard libraries (C++, C, and POSIX) and creating OS threads directly using system calls. My point is just that if you do that, don't expect the standard library to help. Again, that shouldn't be surprising.
Author here: Yeah, this was my consideration. However, I mostly write these articles for myself to better understand whats going on. The graphic stays the same, the layers and the process from TCP stream to Vector/String as well. The Syntax will change, and once hyper and reqwest adopted async/await and 0.3, I'll update the article(s)
I've spent the last week learning actix-web. I'm starting off with 1.0. I didn't understand how to work with Futures when I started. Until I got a grasp I ended up placing comments between all methods in a future call, with each one looking like &amp;#x200B; // (FrontendData, usize) / actix\_web::Error::PayloadError &amp;#x200B; as when the types don't match, the errors are very difficult to understand. Since I wasn't familiar with Futures and \`then\`, \`and\_then\`, \`map\`, \`map\_err\` I was very confused until I started doing this. Once I knew which methods change which side of the future, or which sides I could change, everything became much easier. &amp;#x200B; Since the manual isn't updated for actix-web 1.0, I mostly kept to studying the examples and looking through the source when necessary. [https://docs.rs](https://docs.rs) has the 1.0 generated docs, which is useful.
It is complicated and might not be the preferred solution for the majority of people for this particular use case. But as far as I understood the case study is more about what is possible and how you have to think and model the code in order to get there so that you can use these things for cases where they are really appropriate or beautiful.
Do these same arguments apply to `mem::transmute()`? With `transmute`, we can just as easily produce: &gt; * dangling/null references &gt; * null fn pointers &gt; * a bool that isn't 0 or 1 &gt; * an undefined enum discriminant &gt; * a char outside the ranges [0x0, 0xD7FF] and [0xE000, 0x10FFFF] &gt; * A non-utf8 str
&gt; If you read from uninitialized memory, you are essentially reading random bits, and so your program may behave randomly. &gt; ... &gt; So as a conservative model it's reasonable to just declare that if you do anything with uninitialized memory other than just copying it around, it is Undefined Behaviour. Full stop. The first paragraph seems incorrect. My understanding is that it is straight up undefined behavior. It's not only undefined behavior from a compiler perspective, or the perspective of C, but isn't it also undefined behavior from the perspective of Linux or any system that is possibly executing your binary? For example, Linux could, and does, possibly cause a seg fault?
&gt; Say you wanted to apply the simplification y &amp; x =&gt; y. That requires proving that x is all 1's. Oh it's uninitialized memory? Ok let's just assume it's all 1's. For y | x you can assume it's all 0's. Whatever's most convenient at the time! More controversially, one may also conclude normally impossible things, like x == x =&gt; false, by assuming the value is changing on each read. If the programmer creates an uninitialized value and _does not write to it_, then these compiler assumptions don't strike me as ridiculous. You write that: &gt; No, it doesn't matter that we didn't read the uninitialized memory. I don't entirely follow why this doesn't matter. If it's at all possible that a write to that uninitialized memory occurs along the control flow graph before a read, then it seems to me like it's a compiler bug to assume that the memory is still uninitialized at the time of the read?
Indeed! And that's why `mem::transmute` is `unsafe`. The difference is that it's possible to uphold these invariants using `mem::transmute`, whereas the value returned by `mem::uninitialized` breaks the invariants just by existing. For example, `let b: bool = unsafe { std::mem::transmute(2u8) };` is UB but `let b: bool = unsafe { std::mem::transmute(1u8) };` is not. On the other hand, `let b: bool = std::mem::uninitialized()` is always UB, and there's nothing we can do to correct that. By comparison, `MaybeUninit` lets the compiler pretend that the value is just `()` until after you've had a chance to initialize the memory to a valid value.
&gt; Ideally, you will also read or drop_in_place any initialized memory whose type has a Drop impl when you're done with it, although forgetting to is technically allowed. What's the reason behind this? I'm sure it's not too complex, but I have pretty much no experience with this.
&gt; For example, Linux could, and does sometimes, cause a seg fault? No. Linux does not know or care about the application-level concept of memory initialization.
i don’t like electron apps either but that isn’t what we are talking about here, just the aesthetics of non native looking uis.
again, we aren’t talking about web apps here, i don’t like web apps either. but that they are ubiquitous now suggests users would be fine with high performance uis that dont look native.
The semantics of uninitialized memory inside compilers are \*extremely\* subtle, but regardless, the first paragraph is simply describing why uninitialized memory is a thing from an implementation perspective. Say, if you were hand-writing assembly.
Undefined behavior means that the compiler can do whatever it wants. The actual machine code that it generates chooses a certain option, whether it's removing the entire code path, segfaulting, printing a warning, or emailing a rude message to the NSA. The option the compiler chooses isn't necessarily the same on any given platform. Reading or writing memory outside of your assigned address space is what causes a segfault. That means that reading uninitialized memory that you just allocated won't cause one, and neither will overflowing an array on the stack. But writing code that reads/writes out of bounds won't necessarily cause a segfault, since the compiler doesn't have to generate machine code that accesses that memory in cases where it would be undefined behavior.
The system, in this case the CPU, has no concepts of types when running the generated binaries, everything is a blob of data that is accessed by instructions. At most what could be faulty is to read from a random **address** or accessing some data with an instruction relying on a specific alignment not upheld from the address, but as far as the processor is concerned, there's no `bool` or Enums that must always hold some specific memory representation.
A seg fault is caused by accessing memory you don't have access to (either because it belongs to the kernel, or hasn't been mapped to the current process), or by accessing that memory in the wrong way (e.g. writing to a read-only address). Apart from that, though, the kernel doesn't really care about what you do with the memory that it's given you. --- Undefined behaviour, in this context, refers to situations that the compiler are allowed to assume will never happen. For instance, it's allowed to assume that `bool` will only ever be 0 or 1, so it can perform optimizations based on that fact. If somehow you manage to sneak a `bool` into the system that isn't 0 or 1, things might break unexpectedly. Further more, the compiler is allowed to assume that any code that breaks these rules is invalid and will never be executed, so can be optimized away. Obviously, code following the UB is never going to be run either, so can also be optimized out of existence! Less obviously, this works in both directions: if the UB code is preceded by some non-UB code, that non-UB code won't happen either. If there's an `if` statement, with one branch leading to UB code, the compiler will assume that branch will never be chosen and will optimize away your `if` statement. In theory, if all branches lead to UB, your program could be optimized away entirely!
Essentially we are passing down to llvm the following lines (pseudo llvm ir because I don't want to look up the proper terms): &amp;#x200B; var = allocate\_local\_var() \*var = undef assume(\*var &lt; 2) &amp;#x200B; // ... later we actually initialize it \*var = 1 &amp;#x200B; The compiler can look at the first three lines and instantly derive a contradiction (\`\*var == 3\` (from undef) and \`\*var &lt; 2\` (from assume)).
Two wild speculations to consider: 1) Have you cleared your your `Cargo.lock` for good measure? 2) Perhaps the problem arises from the fact that one of your crate's dependencies, `i2cdev`, also depends on a particular version of `nix`. I haven't used `patch` in ages, but it may be the case that you will also need to use `replace` to convince `i2cdev` that the git-based alternative is an acceptable substitute for the version that `i2cdev` expects. See also: https://doc.rust-lang.org/cargo/reference/manifest.html#the-replace-section
&gt; If it's at all possible that a write to that uninitialized memory occurs along the control flow graph before a read, then it seems to me like it's a compiler bug to assume that the memory is still uninitialized at the time of the read? Just the presence of undefined behavior means the compiler can do whatever it wants. Usually "what it wants" is to make the code fast, and it accomplishes that by assuming that the circumstances that would cause undefined behavior don't happen. [For example](https://rust.godbolt.org/z/pEVA6m): use std::hint::unreachable_unchecked; pub fn test(value: u8) -&gt; u8 { if value == 0 { unsafe { unreachable_unchecked(); } } 100 / value } `unreachable_unchecked` is a compiler built-in that says that it is undefined behavior to execute it. If you look at the generated assembly, it doesn't check at all whether or not `value` is 0, it just does the division. That's because, if `value == 0`, there is undefined behavior, so the compiler can act as if it's impossible for `value` to be 0. If you comment out the `unreachable_unchecked`, then it's no longer undefined behavior to pass 0 so the compiler generates code that checks for the attempted divide by zero and panics.
They aren't *just* case studies, it's real code written in real, published crates. For read only fields, [this one specifically](https://docs.rs/oqueue/0.1.0/oqueue/struct.Task.html) Neat for a case study, sure, but actually using it? Even as a macro? Just use a getter. If a field is public, it's supposed to be mutable. It breaks expectations and convention for no reason. The given justification is, oddly, "rust users understand fields", except now they don't, as the entire point of this case-study is to break that understanding using obscure hacks.
Technically speaking assembly language is not any lower than C and technically it never was. All assembly languages and C used be 1:1 mapping of how CPU will execute them. Which is not the case anymore and hasn't been for a long time. Essentially any language without runtime and with AOT compilation considered low-level today by pretty much everyone except a few snobs.
The API, especially in rust, kinda sucks.
Thanks for the reply. I know the basics, already went through a few Rust books. I looked at getting started and examples, and manage to piece stuffs from that, but I have no idea how stuffs are related to one another. For example, how does CookieIdentityPolicy should relate to CookieSession, or do they relate at all?
I am starting with 1.0 as well and the docs are not for 1.0. I think for basic request response cycle I am pretty okay with it, what I don't understand is when things get more complex such as having session with cookies. I looked at CookieIdentityPolicy and CookieSession from the examples and can pull it to work, but I don't know how they relate to one another. I looked at the traits and structs, got super confused.
Ok, I refreshed my memory. My understanding is that Linux does keep track of which pages of memory have been written to, and something like this can happen: 1. A page of memory is written to, and later freed, but Linux might not have actually gotten rid of it yet. 2. You can create a new allocation that allocates that same virtual page again. 3. You can read from the bytes that belong to that page of memory and maybe get some non-zero, old, values. 4. Linux, knowing that the page hasn't been written to since it was allocated, might free up the physical page of memory. 5. You can read the same bytes and get zeros, because there's no physical page there. Linux has decided that you're reading uninitialized memory. 6. Someone writes to the page (in a completely different area), and Linux allocates a physical page 7. You can read the same bytes again and get different non-zero bytes As far as I can tell, I was wrong about it possibly causing a seg fault, but this seems like a pretty clear indication that as far as Linux is concerned, it can do whatever it wants when someone reads uninitialized memory.
&gt; because that is simpler and works better for hardware design And more specifically power-efficient and cycle-efficient, also more easy to pipeline. &gt; this is actually enabled by CISC Hardly. This is like stating that linux is enabled by bash. RISC is efficient enough that you can simulate very efficiently a CISC system while still keeping the transistor count lower than you would with the full CISC (but still higher than if you only had RISC). Simulating a RISC on a CISC is simply not using all the transistors in there. Backwards compatibility has a cost that we pay. &gt; CISC makes for a great interface to the outside world, because it leaves the CPU designers free to do whatever they want internally Not true. First of all you can have a RISC interface and do whatever the fuck you want behind the scenes, it could all compile into even more trivial and simply microoperations (microcode). RISC still allows for shifting things around. Look at all the ARMS, they all have their own tricks, but the assembly is the same. &gt; RISC instruction sets expose more of the CPU's internal design Not true. RISC exposes a simpler instruction set that is easy to create in an optimized fashion. ARM CPUs both use less battery and transistors to achieve comparable performance *because* the target interface is very easy to optimize for. Intel instead needs to pull out a lot more expensive tricks to get things working. I'm skipping the next chapter which just repeats your point. RISC is still an abstract interface that you can change implementation underneath easily. It just so happens that RISC is easier to optimize. It's the same logic of why Java can only be so fast compared to C. Just want to note: &gt; without needing the complex translation layer Most RISC do still have a translation layer, at least one that reorders commands for optimal pipelining. &gt; CISC is a fantastic abstraction that allows us to have 25+ years of software compatibility Hardly. The fact that x86 became a de facto open standard was what allowed this to happen. But it was the fact that x86 became open, not that &gt; with incredible performance Again ARM and others showed that x86 isn't performant, it's just the CPU with the most transistors crammed in. Everything is fast with a rocket launched to it, that doesn't make the design efficient or powerful. &gt; This is exactly thanks to our CPUs having become some truly amazing hardware JIT compilers. That's not how it works. CPUs do many minimal optimizations, but they are not a JIT. &gt; Nowadays we can fit massive amounts of transistors in a CPU to form very complex circuits, so having such a hardware translation layer is not a problem. That's just wasteful though. We could make that layer instead be more CPUs, faster operations, more cache. &gt; Even ARM are moving to become increasingly more CISC-like over time. Not quite. I will grant you this, ARM is not the most RISC CPU out there. It does compromises and concessions in the name of practicality. Just like most LISP dialects out there are not purely functional. CPUs are expensive and not a lot of work on dramatic innovation has been done in the same way software has. Mostly because most dramatic innovation in the software world sucks more than the alternative for the first N versions, then is equal, and only after that surpass. And hardware, to get used, has a higher cost if people aren't compiling stuff for your architecture. Some great research and advance has happened, but when things come to pay for building a factory, most will go for something more "reliable". This has been in part because of Moore's law interacting with the Law of Scale. It's always cheaper to just crank out more of the same and every two years you double the power you can put on it. It has failed sometimes (ej. x86 was not effective enough at lowering power demands for phones, so ARM was able to advance). Maybe things will change in the future, maybe not yet, maybe not ever. We'll just have to wait and see.
Very exciting! It's very awesome to see Servo's work come to fruition. In the article, they mention that they support Nvidia GPUs the best. Does webrender manually call each different graphics backend itself? Or does it use gfx-hal? Cause, ouch, if it doesn't use gfx.
&gt; Linux has decided that you're reading uninitialized memory. Linux does not decide this. Anonymous pages are explicitly zero-initialized. They may be implemented via a ZFOD scheme, but that's transparent to the application. The physical page that Linux allocates will be filled with zeroes before it is mapped into the process's address space.
&gt; You know, your [run of the mill three-state boolean](https://www.boost.org/doc/libs/1_59_0/doc/html/boost/logic/tribool.html). I expected this link to go to https://thedailywtf.com/articles/OurBoolean, but having an actual `tribool` is such a popular library is even more amazing.
&gt; &gt; Unfortunately, what compilers most love in the world is to prove that something is Undefined Behaviour. Undefined Behaviour means they can apply aggressive optimizations and make everything go fast! Usually by deleting all your code. &gt; &gt; This article is really entertaining. The quoted paragraph may be entertaining but it's wrong. It's not really how compilers work -- they don't go out of their way to prove that something is UB. Their optimizers *assume* that no UB can occur and optimize based on that. (If it were about proving UB it would be trivial to have the compiler warn every time an optimization fires because of "proven UB".)
I know this isn’t a GUI framework, but I absolutely love [webview](https://github.com/Boscop/web-view). It’s like electron, except native to rust, tiny footprint (only 500kb compiled!) and very little CPU and ram usage. I always use this over electron. I love how it deals with communication between the frontend and the backend, too. All you do is `external.invoke(some_json)` in the frontend, which you can then extract from rust however you wish! Give it a try
I thought I was looking a Minecraft map first... Oof Very nice project though!
thanks! :D
At this moment I'm writing a CLI tool to download and search books from [libgen.is](https://libgen.is),because I'm lazy to do it myself
From my experience intellij has the best predictions of any ide but it's slow as hell
Qt has that string in their docs, yes, and it's correct on some level... and incorrect on others. While it's an Apple API, it's a black sheep if there ever was one. It's an old as hell API that was introduced with Carbon, and is woefully undocumented at best and unmaintained for new features at worst. It doesn't have feature parity with things like CoreText, either, last I checked... if you build against HIThemes, you're essentially getting hints on how something should look, but it's not perfect. It's the definition of uncanny valley. [Qt has bugs crop up due to Apple's mucking with it](https://bugreports.qt.io/browse/QTBUG-68891), and if you'll note in that issue, they're aware that pretty much every year Apple causes them to spin their wheels fixing things. [wxWidgets](https://trac.wxwidgets.org/ticket/18146) experiences issues with it as well, where HIThemes has no support for dark mode in the new macOS releases. If a toolkit relies on OpenGL, well, Apple has deprecated that in favor of metal (Piston will hit this, for instance...). Bugs in several applications have already shown up, with Apple mostly responding with "Metal is the way forward". Competing frameworks, like Lazarus, faced the same issue... it may have changed recently, but for the last few years their macOS support was actually Carbon based, not Cocoa based. Apple deprecated Carbon recently and they had to scramble to make a proper Cocoa backend. [Hell, Xojo (some random-ass framework I just found) also had to replace their HIThemes implementation recently, it seems](http://www.einhugur.com/Html/Grids.html). tl;dr Apple routinely provides frameworks that don't exactly do what they say they do, and Qt kind of hides that fact in their notes. It's not as simple as it sounds. Unless you're building _on Cocoa itself_, it'll be an uncanny valley. It sucks, but that's the reality.
&gt; random ass-framework *** ^(Bleep-bloop, I'm a bot. This comment was inspired by )^[xkcd#37](https://xkcd.com/37)
that's very clearly sarcastic hyperbole for comedic effect
My observation was a while ago. If I can still reproduce it, I'll file a bug.
Yes,exactly.
One would hope, but I have certainly come across lots of people who actually seem to believe it. (And thusly end up thinking that compilers and compiler writers who add aggressive optimizations are "the enemy".)
The IntelliJ Rust Plugin may be compiling your crate, which will slow things down until it finishes. **You** only imported one dependency, but `reqwest` has its own dependencies, which may have their own dependencies, and so on. And they all have to be compiled. It may take several minutes, or longer depending on cpu. After the dependencies are compiled they shouldn't need to be compiled again, as long as you don't delete the `target` folder or run `cargo clean`, so it should be faster when you edit your crates source(your crate will have to be compiled again, but `reqwest` won't have to be)
Although there are definitely some optimizations that *should* trigger a warning, like "[lol, I turned your function into an infinite loop, hope you don't mind](https://godbolt.org/z/fm-Bja)."
&gt; The first paragraph seems incorrect. My understanding is that it is straight up undefined behavior This section is from a "low level implementation perspective". There's no undefined behavior, there is no compiler to define what behavior is allowed :) Ultimately you can always write assembly to read from uninitialized memory. That will fine, and it will work, it just won't do anything useful. Which is what this paragraph is describing. You're right that there's a bunch of randomness that can happen from the kernel here. That's not the same _kind_ of "undefined behavior", in the context of compilers "undefined behavior" is not just for scenarios where the behavior isn't 100% defined, it's for scenarios where the compiler has full license to do whatever it wants, including replacing your stack frame with poo emoji. Here the OS does not have full license to notice your weird memory reads and replace your stack frame with poo emoji. This falls into _implementation defined_ behavior: There are specific constraints on what the behavior may be, but different implementations may have different choices. In this case, the constraints are that the OS may return whatever it wants on such memory reads (and it doesn't have to be consistent across reads). I don't think OSes really have the same kind of concept of UB, it's almost always implementation defined behavior.
Also IMO it is legitimately more instructive to regard the compiler as adversarially trying to find UB, as it disillusions oneself from the notion that "well even though there's UB, no compiler would ever miscompile this". &amp;#x200B; e.g. assuming \`x = \*ptr\` will reliably lower to a cpu load instruction, and therefore result in a hardware trap if \`ptr\` is null. "Why would a compiler ever break that?" Because it's a complex system designed to find UB and break your program if it finds it.
It [appears](https://github.com/servo/webrender/blob/cdadd068f4c7218bd983d856981d561e605270ab/webrender/Cargo.toml#L35) to use [gleam](https://crates.io/crates/gleam), “Generated OpenGL bindings and wrapper for Servo”.
I know,but just changing things in the code,or even calling autocomplete it's just a pain even if it has already compiled.
Just a small correction: &gt; wxWidgets experiences issues with it as well, where HIThemes has no support for dark mode in the new macOS releases. Latest master (which will become 3.1.3 release in the near future) does support dark mode and this is possible exactly because it does _not_ use HITheme APIs, unlike Qt, but uses the actual, real, genuine, 100% organic native controls directly.
Keep in mind that I've never actually used actix myself, so this is only my thoughts from reading over the documentation. [CookieIdentityPolicy](https://docs.rs/actix-web/1.0.0-rc/actix_web/middleware/identity/index.html) is a part of the [identity](https://docs.rs/actix-web/1.0.0-rc/actix_web/middleware/identity/index.html) middleware, which lets you track the identity of the user, normally with cookies. It looks from the way it's written that you could theoretically use some other kind of session backend with `IdentityService` (like recoding IP addresses?) but `CookieIdentityPolicy` is the only backend that's provided. [CookieSession](https://docs.rs/actix-session/0.1.0/actix_session/struct.CookieSession.html) is actually part of a different crate, [actix\_session](https://docs.rs/actix-session/0.1.0/actix_session/index.html). It looks like when actix went from 0.7 to 1.0, the crate got split up into a number of smaller crates, the main one being actix\_web. actix\_session provides the ability to identify users and store data per user by using cookies. `Sesssion` is the generic trait while `CookieSession` is the concrete struct that you'll actually initialize. At the end of the day they're both middleware which use cookies to track user identities. The `Session` middleware allows you to store data assocated with users while the `IdentityService` middleware does not. I'm not sure why `Session` gets its own crate while `IdentityService` doesn't, but it probably has to do with historical details.
Thanks for chiming in! I looked briefly to try and figure out if that was the case, but my searching failed me there it seems. Really appreciate it!
&gt; To quote the Rustonomicon, it is Undefined Behaviour to produce an invalid primitive value Why? Why did they define it this way, instead of saying that only reading invalid value is UB?
I wonder whether this can be compiled to Web Assembly and interfaced with WebGL2, so we can get consistent HTML rendering on all browsers…
I've looked at this a bit more and noticed that calling closure in a box always [moves it out of the box](https://godbolt.org/z/QRuF_u) first. Which is very unfortunate for me because I thought that it is possible to call a closure from heap without extra memcopies. It also means that I can just use `ptr::read::&lt;F&gt;(b.ptr)()` to call the closure and it will be as efficient as Box is now anyways.
I just make a main.rs file and put some junk in there. Then later on when I'm ready to publish I delete it or move it to the examples directory.
Are you using Rust Qt Binding Generator for the Qt/C++ part?
The plugin/intellij may just be slow, in that case.
Ooooohhhh.... Should this maybe be explained in the documentation? https://doc.rust-lang.org/std/option/enum.Option.html#method.copied
A "read", or "use" is a very intuitive thing that very quickly falls apart when you try to formalize it and people start asking you questions. let x: bool = mem::uninitialized(); Ok let's say that's not a read. Makes sense. Is this? fn foo() -&gt; bool { mem::uninitialized() } Is this? let x: Option&lt;bool&gt; = Some(mem::uninitialized()); Is this? let y = Some(mem::uninitialized).is_some(); Is this? let x: bool = mem::uninitialized(); let y = &amp;x; Is this? let x: bool = mem::uninitialized(); i_promise_not_to_look(&amp;x); Everywhere you say "no that's not a read" you add difficult constraints to the compiler and memory model for *very* little benefit.
I imagine for various reasons, from optimization to sanity to just being simpler For example, `std::mem::size_of::&lt;Option&lt;bool&gt;&gt;() == std::mem::size_of::&lt;bool&gt;()` because, since `bool` can only be 1 or 0, the compiler can use another value to represent `None`, like 2. If references can't be null/dangling, you and the compiler never have to check for that, they're safe by definition. If enums can only have the values you say they can have, you don't need to handle them being something else, even as an error, etc. The types are correct by their very nature, so you only have to worry when making them, and for primitives the compiler controls that outside of stuff like type punning, where you have to. Also, what consitutes "reading" a value isnt easy to define, theres an issue somewhere that describes this but i dont have the link handy
Looks great. Especially for cross platform. Thank you!
Ah, yes, this is a good point.
I’d say they go out of their way to prove that something is UB *for some possible inputs*, which then allows them to simplify the code by assuming those inputs won’t occur. That’s roughly equivalent to what you said, but I think it still counts as “going out of their way to prove UB”.
I'd like to point out that misuse of uninitialized memory has already caused [at least one security vulnerability](https://github.com/ruuda/claxon/issues/10). It's also notoriously hard to detect you're using it improperly - I literally had to [write a custom tool to do that](https://github.com/Shnatsel/libdiffuzz) and it's still a far cry from catching everything. The funny thing is, [requesting zeroed memory from the OS](https://github.com/rust-lang/rust/issues/54628) is usually exactly as fast as using uninitialized memory. So there is pretty much no reason to use `mem::uninitialized`, ever.
&gt; I wasn't 100% sure if I could claim that `arr[i] = x` doesn't create a reference, so I just used pointer arithmetic to be safe. If we're worried about that one, should we also be worried about `results.as_mut_ptr()` implicitly creating an `&amp;mut self`, since it's an `&amp;mut self` method? Vaguely related, does the language provide any way for me to get my hands on a `*const T` or a `*mut T` directly, without first creating a reference to something? Unrelated question: Is there any particular reason the `init` field needs to be `ManuallyDrop&lt;T&gt;`? Or does that just simplify the implementation, since writes to that variant don't have to worry about accidentally running `Drop` on uninitialized data?
What is the idiomatic (if there is one) way to express supertrait/subtrait bounds on a type parameter? I have a function that I'd like to make generic over types that implement IntoIterator such that the resulting iterator also implements DoubleEndedIterator. I know I can get the desired behavior by calling .iter() at the callsite and just having the DoubleEnded bound, but I'd prefer not to. Thanks!
My argument is not so much a technical one, as one of [values](https://www.youtube.com/watch?v=2wZ1pCpJUIM). I value performance and safety, and happily, so does Rust—a happy marriage there! I value simplicity; Rust not as much, but so far the complexity of Rust (and its ecosystem) have mostly been in the service of performance and safety—an acceptable deal. But it rubs me the wrong way to see code that relies on byte-for-byte struct equivalence (losing rustc's struct packing in the process), the implementation of a trait to "override" the behavior of `.`, conditional compilation, and procedural macros. And for what? To be able to type `vec.len` instead of `vec.len()`? People who value ergonomy above simplicity might agree; myself, I just have a hard time with it. Now, I get that this is a study, and from a purely PL perspective, it is interesting that you can have read-only fields in Rust without Rust having explicit support for them in the language. I just hope that this doesn't become something that people do.
a) It is perfectly fine to create an `&amp;mut MaybeUninit::&lt;bool&gt;::uninit()` because it *is* initialized -- to `uninit: ()`. b) For raw referencing, you want is this RFC https://github.com/rust-lang/rfcs/pull/2582 c) To the best of my knowledge the presence of ManuallyDrop is paranoia about well-defined language specification (e.g. they might as well put use it To Be Safe). Although I haven't been following the RFCs for making unions able to contain types with destructors, so it's possible the latest version requires this.
Better yet, we'll foil Chrome's monopoly on the web by just compiling all of Firefox to WASM and delivering it on every page load! :)
[Here's a graph](https://i.imgur.com/wq3on3Z.png) of some crates from [crates.io](https://crates.io). As you can see, while most files are in the 100-500 line range, it's definitely not uncommon to have a file that's 1000+ lines of code. I think that ultimately it depends on personal taste.
Sure, but isn't it more likely that the person who wrote the Rustonomicon, whoever they are, is just a huge dingus?
Thanks for the explanation, so what are the differences between both? Or do I have to integrate them together in a way? If I want to use the CookieSession to identify user, how would that look like? Given the example, I only see the way to store counter. Sorry for asking too much
( for those at home, i am the author of both of these things: [https://gankro.github.io/blah/](https://gankro.github.io/blah/) )
Meh, I don't care about Windows at all
As someone who's working on a procedural macro library and has therefore been waking up at night thinking about edge cases, this is very interesting stuff! Thanks. :)
If you're targetting Windows you might want to look at [my fork](https://github.com/Freaky/webview/tree/various-fixes) of the underlying library, which includes fixes to dialogues, the Windows message loop, and memory leaks in invoke. Most of it's pulled out of its sadly neglected PR queue.
Don't apologize for asking too much in a thread dedicated to asking questions :) If you want to use a CookieSession to identify a user, you'd have to have a global list of known user ids, and on a request, you would see if the session contained a user id which is in your list. If so, you know who the user is, and if not, you can assign them a user id so that you can identify them next time. Keep in mind if your user is using a browser which doesn't set cookies you'll see them as a new user for every request they make to your site. This is basically what IdentityService does for you, so if you want to identify users in that way I would use them both together. If you don't need to store axillary session data besides the user's identity you can use IdentityService on it's own. An example of a situation where you would use only CookieSession might be if you have a web app like a e-reader that stores user's preferences but doesn't require a login. You don't care who the user is, but you do want to store some data associated with them across requests. An example of a situation where you would use only IdentityService might be if you have a restricted access site that requires authentication. Once the user authenticates themselves by putting in their credentials, you'll want to keep track of them, but you don't need to store any additional information besides that. And of course you might have some sites which require the use of both. It depends on the needs of whatever you're writing.
I was expecting https://thedailywtf.com/articles/What_Is_Truth_0x3f_
Re: (c), there was a change to unions that just banned types with destructors, to simplify the rules around assigning to the union and/or its fields.
Why are the `f32::copysign` and `f64::copysign` not called `copy_sign`? Those names are kinda weird.
Stock-trading historical analysis program that tries a bunch of basic trading strategies on a given stock over time.
That's already done with Skia, which powers Chrome rendering.
Is there a particular reason that the ClipboardFunctions trait is Sized, and has its own `new()` function? Generally, it doesn't make sense for traits to have their own constructor! I would remove `new()` from the ClipboardFunctions trait, and allow ClipboardFunctions to be unsized.
Some previous discussion: https://www.reddit.com/r/rust/comments/bmy5ch/cargo_permissions/
&gt; On the other hand, `let b: bool = std::mem::uninitialized()` is always UB, and there's nothing we can do to correct that. I don't understand why that's always UB. I thought `uninitialized` was the same as something like this in C/C++, which I doubt is UB. bool x; x = true; What's the difference?
Dear lord, that first screenshot looks like the bastard child of OSX 9 and Windows 95. Come on people, we can do better than this.
Thank you,
In the example of writing bools to an uninitialized array of length 16, can you explain what could go wrong? (Assuming you do assign a bool value to every slot in the array). I've never understood what could happen, specifically, since every byte is overwritten before any are ever read. Also, in the case of an array or Vec of u8, what risk is there, if any, with using unitialized since the bytes are just bytes, if that makes sense?
maybe instead of `tribool`, they could have called it a [`łukasiewicz`](https://en.wikipedia.org/wiki/%C5%81ukasiewicz_logic)
There’s really nothing that prevents you from using any programming language as an embeddable scripting language. You could use C or Java as a scripting language if you wanted. If you want to do it with Rust then you should see if there’s an existing crate for parsing Rust code into an abstract syntax tree. If not, you could probably break out the parser from the current compiler. Though that could be a massive undertaking. Once you have an AST, you could interpret it directly or use something like LLVM’s JIT capabilities to convert into machine code and run it directly.
I need to ask a clang expert about this, but my gut reaction is that clang attaches metadata about bools to loads or something, but rustc doesn't because it has a different (in development) model of how memory and type layout works.
No mention of the rollout to OS X here, but it's cool to see progress for Windows and Linux.
Hmm definitely interesting for my purposes, but unfortunately this doesn't integrate with Rust on wasm yet :/
Yeah it's c++ so I don't think it's doable without crossing back across the boundary or using emscripten to compile it. Would love to know if that's wrong.
It's not UB in C/C++, but it is in Rust. On the other hand, some things are UB in C/C++, but aren't in Rust. Examples include signed integer overflows, integer division by zero and, from the blog post: type punning through unions.
It's a matter of language specification, and not some specific implementation issue. It's cleaner/safer to have a "no exceptions" model than to have some complex wart in the specification that Very Smart Programmers can technically use correctly but in practice is too hard to use. So: what can go wrong? Our specification says this is Undefined Behaviour, because it says so, and that means Everything Can Go Wrong.
I'm not sure if it was the only one, but I remember either C or C++ being at least the preferred language for targeting the QVM virtual machine used to run Quake 3 extensions. I don't know about now, but, back in 2014, Phoronix [reported on](https://www.phoronix.com/scan.php?page=news_item&amp;px=MTYzMDk) the Q3 descendant Unvanquished working to rebase that on Google's Portable Native Client (PNaCl) environment. (Which, together with Mozilla's asm.js, is one of the ancestors of WebAssembly.)
The risk is that before every value is assigned, the bools can be thought to be in a "Schrödinger state" of, for example, 0x02, which is not an allowed value for bools. The point isn't whether it actually is, but the mere possibility is UB.
I’m convinced that I learn more from these posts and threads than entire semesters of college.
How does one go about creating a menu bar and should it be native? Personal preference aside, is it sensible to draw up a custom menu bar and functionality?
I feel like while the semantics of rust are nice, the fact of it being so low-level, and depending on LLVM to compile, kind of negate that. Sure, you could try to embed rust, but it's not really going to be an easy task. Even with things like you mentioned: when the rust compiler interpret `let var = foo();`, it still needs to know the exact size of `var` on the stack, and it's exact type, to compile the program. And types still need to be known to the programmer if you want to store them anywhere, or return them. If you want to undertake this, by all means power to you! But just trying to say that it will be an undertaking. ---- If your end goal is just to get a good embedded language, though, there are some existing ones which model themselves off of Rust without being too closely tied to it. The one I know best is [gluon](https://github.com/gluon-lang/gluon), a staticly-typed, type-inferred and functional language. It's like a more Haskell-like Rust which can be used as an embedded language. There's also [dyon](https://github.com/PistonDevelopers/dyon) - it even has lifetimes modeled on Rust's! It's slightly less active, though.
There are three versions of `nix` in your crate graph (0.6, 0.10, and 0.11), none of which are compatible with 0.13. You will need to update your dependencies to versions that use 0.13.
Sure. You can make [native shared libraries](https://doc.rust-lang.org/1.5.0/book/rust-inside-other-languages.html) in Rust, which is exactly what JNI talks to.
I added this to my 3D ticket - I'll give it a shot! My hope is that if I can get the 3D stuff to work, this library will be blazing fast.
It's feasible, but I would search for newer Java input libraries first. JNI is a pain, and native binaries are more difficult to distribute than Java.
Thanks for the explainer and for the 30 minutes I spent fiddling on godbolt with `MaybeUninit` :)
While the performance equivalency maybe true for rust running in an OS, that isn't guaranteed outside of that. For example uninitialized memory in embedded systems can be faster since you would (on certain systems) have to be the one to zero it out yourself. However, if you are writing something that is running on linux/mac/windows, you probably should avoid uninitialized memory if for nothing other than security concerns.
Yo! So I went back and dug into this, and then went and dug around in the Qt repository. We're actually _both_ wrong (and Qt's website is wrong, to boot). As of Qt 5.11.x, `QMacStyle` does _not_ depend on `HIThemes` or Carbon. Why their docs are not updated is beyond me, but somewhere in the 5.11 changeset they moved to relying on AppKit and custom code for rendering. This is likely because Apple isn't doing anything with HIThemes. [The support to remove HIThemes landed almost a year ago](https://github.com/qt/qtbase/commit/38a16b8be0a1e408fae19611e68f2caa12440d68), but didn't go out until Qt 5.11 it seems (towards the end of 2018). It looks like, now, they draw using mostly-native backing layers (ala `NSView` and so on) but still do a bunch of custom styling/rendering at points. It would explain why it still feels off, at least to me - but it's nice to know it's closer than it used to be.
That's just the standard theme in the MATE desktop environment, as far as I can tell. You could always replace it with a [better looking theme](https://drasite.com/flat-remix-gtk), for example.
Hey, thank you so much for your feedback! &gt;Is there a particular reason that the ClipboardFunctions trait is Sized, and has its own new() function? I put the `new` function in the trait because I thought traits were like interfaces and I thought I need to all the common functions there. I think added Sized when I was trying to clone the callback function for the WndProc. I'll fix those! &gt; create\_window() function is not marked unsafe, but it returns a raw HWND Yes, I think it might be better to mark it as unsafe but it is not a public function and I do not use it anywhere else except that. &gt;You'd then need something like crossbeam\_channels to send notifications to a different thread. I see! I have never programmed with threads so I will have to look into it further. I really appreciate you taking the time to write such a helpful comment! Thanks!
Your desktop OS is?
Also just note that the community edition of IntelliJ is completely free &amp; open source, and has all the features needed for Rust development. The paid version (or in your case, the student license) doesn't add much for Rust.
In Rust, the equivalent is safe: let x; x = true; The difference is that, at least for Rust, there's a difference between "uninitialized" and "undefined". "Uninitialized" is tracked by the compiler. "Undefined" isn't. When you declare and assign a variable in two steps, you aren't promising that the value is well-formed in-between. (In fact, it might not even have space allocated yet, depending on the optimizer.) But when you do `let x: bool = mem::uninitialized();`, you're filling the `x` slot and promising that the value in the `x` slot is always a valid `bool`. And because we tell LLVM that, and `undef` isn't, UB results.
Your link is broken on Reddit's official mobile client, as it doesn't include the trailing underscore. Except it does in the comment editor view. Reddit, why do you have two separate URL parsers in your app‽
It doesn't use gfx-hal *yet*, but the work of porting on it has been going [for a while](https://github.com/szeged/webrender/projects/1). We want to see the unroll (to more platforms) going well before doing this radical change.
This is a decent amount of just "what can be done" theory, with a splash of practicality. These corners exist, so it's good to explore them. (You need to know `Deref` can panic, though it shouldn't!) And `#[repr(transparent)]` guarantees identical layout.
I'd love to see `ghost` in this case study. I understand the basics of type and value namespace but the rustdoc view of the type makes me have no idea how the type is zero sized and "uses" the type parameters.
You can also try Rust Analyzer aka RLS2 ([https://github.com/rust-analyzer/rust-analyzer](https://github.com/rust-analyzer/rust-analyzer)) with VSCode. Warning: it's work in progress.
Thanks. The vscode one seems somewhat dismal, but as it turns out a portion of that was just due to it going wonky from suspend/resume and also adding different workspaces. Once I got into the habit of identifying that and restarting vscode it became a lot more bearable. That said, the integrated build still seems fundamentally broken so I'm keen to try intellij
cheers. After doing some major fundamental rethinking, this makes perfect sense now!
When was the last time you have taken a look at [https://github.com/Boscop/web-view/pulls](https://github.com/Boscop/web-view/pulls) ? It's completely empty. I see that your fork is 4 commits ahead of master. Please consider making a PR? Disclaimer: I have no relationship to [web-view](https://github.com/Boscop/web-view). I just got curious, and followed the links.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/fnctl] [Announcing diesel-factories 0.1.0](https://www.reddit.com/r/FnCtl/comments/brk9am/announcing_dieselfactories_010/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
I haven't used it yet, but you may want to look into [rust\_swig](https://github.com/Dushistov/rust_swig) to generate the necessary rust and java boilerplate.
I've written a serial terminal in Rust. Annoyances i found were that GTK framework doesn't have first party packaged support for Windows. And the support that does exists requires packaging DLLs, you can't compile it statically as it's a non-pure-Rust solution. And finally it's internal logic is very closely tied to the C way of doing things and is not very Rust-y. A specific example is that storing program state is very painful. I think relm is much more compelling, but I've been too frustrated to decide to rewrite my app in relm instead. Additionally GTK contains a lot of things that already exist as separate Rust libraries, like getting the directory where user-specific cache files should go. This was commonly because libraries in C were painful, but managing them in Rust is easy. So there are some awkward decisions one has to make when writing an app here: do I use the corssplatform pureRust solution or use the GTK implementation. So that's a nice papercut as well. I think [druid](https://github.com/xi-editor/druid) is the eventual future, assuming it does develop into a general GUI framework.
It's quite beautiful if you ask me
&gt;some of lol, could you explain yourself as i'm super curious why you say this! =) thx.
This would be a great YouTube content series
Skia is a vector graphics renderer, not an HTML or CSS renderer. Converting CSS to vector graphics commands is a fairly involved process. On the other hand, WebRender's display list model is mostly a subset of CSS, so the conversion step from CSS is significantly simpler. However, WR has no layout at all, not even text shaping. So you would need to bring in a layout engine such as Servo to actually have a full HTML renderer. Alternately, one could imagine hooking up a minimal CSS-subset layout engine such as Stretch to WebRender in order to render a subset of HTML.
Yes, I should have made it more clear by what I meant by "powers Chrome", I didn't mean the full stack. It is actually very practical with wasm though, especially with canvaskit which makes it super easy to use. Its used often by graphic design programs such as 2dimensions, AdobeXD, and figma for that reason. All of them run it in the browser with wasm.
How can I be generic over borrows? I know that the traits `Borrow` and `AsRef` exist for this purpose, but I've read the official documentation, as well as some supplementary articles and stuff, but I just fundamentally don't understand how to use these traits. My problem involves taking a function with constraint `Fn(&amp;I::Item) -&gt; ()` where `I` is a simple `I: Iterator&lt;Item = T&gt;` (no constraint on `T`). The user would then supply a function or closure like this ``` fn go(t: &amp;T) { println!("Wow! Look at this {} I found!", T); } ``` (assuming `T: Display`, of course) This is okay, but I really want to be generic over the type of borrow, so that I can take advantage of the relationships between, for example, `String` and `&amp;str`, and `Vec&lt;T&gt;` and `[T]`. In short, I want to allow the user to decide the exact type of borrow that is preferred for their situation. Also, beyond just the problem at hand, I feel like learning about these traits (`Borrow`, `AsRef`, and friends) will really help my understanding of the language's system of type coercion, generics, etc. So what helped you guys understand these traits? And if you helped make the language, of course, where are the best places to learn about them. I just really don't understand their mechanics, and how I'm supposed to use them in this setting. Any help is appreciated! Thanks!
Okay so, this might be a slightly hackier way of doing it, but here’s a thought: you could load up some rust source file. This source file would expose all of the non mangled, public functions it needs to use. Then, you would invoke rustc on it telling it to produce a `cdylib` (dll or so file). Then, you could just dynamically import this and call any functions you may need. Probably the easiest way to do it
Python [`functools`](https://docs.python.org/3/library/functools.html) does have a [`reduce`](https://docs.python.org/3/library/functools.html#functools.reduce) function. Maybe it's that I came from functional languages, but I find that I use functional features of Python like `map` and `filter` (and `sum`) quite a bit. Paired with (more or less) functional things like list comprehension and generator expressions, I find that these function features are among the most powerful features of Python.
That sounds like a fantastic project, though I can't speak as to the difficulty of doing the glue code. You are going to need to figure out bridging the rust constructs in that library to C and then through JNI to be used in java code. Not impossible by any means, but not a trivial amount of work. If you end up giving it a shot always feel free to ask for help when you need it: one of our highly trained rustaceans will be more than willing to help you out.
I think mozilla is working on similar to skia regarding vector graphics. Called pathfinder. And it's an interesting project.
Hoof. I suspect there is a something better tailored to rust but but I often show people the [LINQ 101 Samples](http://linq101.nilzorblog.com/linq101-lambda.php) to try to explain the value and syntax of this sort of functional programming. (note I've provided a link to a translation that would closer maps to how it would be expressed in rust - the original is [here](https://code.msdn.microsoft.com/101-LINQ-Samples-3fb9811b)). And there's a great translation of these examples to rust [here](https://gist.github.com/leonardo-m/6e9315a57fe9caa893472c2935e9d589) If you're able to understand the C# linq samples (my hope is this would be pretty readable for non-C# people but hard for me to tell.) you could at least lookup the rust translation. It's a multi-step process and hardly idea but maybe better than nothing.
I don't think C has ever been a 1:1 mapping. Fortran and Basic maybe, but not C. Now, of course, there are layers and layers of machine instructions of which no one has full access to. So even some assembly languages are "high level", but that highly depends on which architecture you're assembling for. The low-level to high-level line is highly dependent on how hardware or software biased you are. I, as a software engineer, also think that C is a very low level language. Rust and C++ are somewhere in the middle and Java and a lot of modern languages are on the high-level end of the spectrum.
r/unixporn
Fantasque Sans Mono ;)
The IntelliJ platform actually supports creating Rust scratch files. (Unfortunately, I don't think they're runnable, though.)
r/titlegore
To clarify, in terms of specification, rust specification? Does the language also specify order of operations such that any read of the array (as written in the example) could only occur after they've been written to? I'm perfectly happy for the answer to be "this tool was prone to misuse and it was prudent to remove it." But I find the notion of what seems to be purely theoretical UB to be unsatisfying. Specifically: on tier 1 targets, if you completely overwrite an uninitialised array prior to any reads, are there any practical dangers? If so, what are they?
I found this dailywtf article a great way to explain why Option and Result exist and how nicely they compose.
I think that spring did this. Very intersting concept for sure
eBPF usage is now generalized everywhere in the kernel and no longer limited to network functions. It's used for tracing kprobe, uprobe... For example bpftrace is similar to dtrace but based on eBPF ([https://github.com/iovisor/bpftrace](https://github.com/iovisor/bpftrace)). Other usages: \- BPFilter to replace IPTable. \- Facebook, Google, CloudFlare... are all using eBPF intensively \- Cilium is mostly based on eBPF+XDP to provide their security solution for microservices running on k8s, mesos, ... JIT in the kernel is definitely a thing!
Although there could be an optimization to avoid the copy, semantically with `FnOnce` you’re consuming the closure’s captured data and passing it by value. In other words, `FnOnce::call_once` takes `self`. If the closure’s body doesn’t need to move stuff out of its captures, try having an `FnMut` or `Fn` bound instead. (`call_mut` and `call` take `&amp;mut self` and `&amp;self` respectively.)
Eh, I actually meant this one and copy-pasted the wrong URL :)
Isn't this was patch is supposed to do? Every time nix is supposed to be pulled from [crates.io](https://crates.io) it will be replaced with the patched version?
Alternatively, instead of thinking of the compiler as your enemy, think of him as an extremely misguided friend. "But I just want to make your code go fast!"
1. yes, but no change 2. When using this Cargo.toml \[replace\] "nix:0.11.0" = { git = "[https://github.com/nix-rust/nix/](https://github.com/nix-rust/nix/)", rev = "196b05b5c7b" } , I'm getting this error Updating git repository `https://github.com/nix-rust/nix/` error: no matching package for override `https://github.com/rust-lang/crates.io-index#nix:0.11.0` found location searched: https://github.com/nix-rust/nix/?rev=196b05b5c7b version required: = 0.11.0 I'm a little confused as to why it's "searching" the git link and revision. Isn't it just supposed to download and use it instead? Changing the version to 0.13.0 results in a "nix ignored warning", similar to my original post
I've got a data type which is created in main and some time later passed to a function which has the job to initialize this data type. Rust doesn't let me do that, warning about passing an uninitialized variable. How do I do that. Capsule the data type in an Option?
Wait, what? MaybeUninit has been stabilized!?
I can reflect on whether a type needs to be explicitly dropped using `std::mem::needs_drop`. Is there something similar for `Copy`? I'm building something that clones an object N times and then finally moves it. In the general case, I need to ensure the move happens _after_ any clones. If I knew ahead of time that a type has Copy, I wouldn't need to distinguish between cloners and the mover.
I'm an intermediate level programmer in Rust, and I must admit that I also find the `.await` syntax confusing. It seems to me like the main argument is that one can't easily concatenate it with the `?` operator: from my vantage point, this is a weak argument. There isn't a whole lot I can add after the many great comments here two weeks ago, so instead I'll just add an example of code which in my eyes makes a lot of sense: ``` async fn wait_for_me(var: i32) -&gt; Result&lt;Yay, Ohno&gt; { // Do some stuff Yay::new("happy"); } fn another_function() -&gt; Result&lt;Yay, Ohno&gt; { // Set up stuff let val: i32 = 159; let all_good = await wait_for_me(val)?; // Or the let keyword would simply not be supported, which is OK too! let all_good: Yay; await all_good = wait_for_me(val)?; // And this would make sense with the other syntax as well: if let Ok(result) = await wait_for_me(val) { println!("{:?}", result); } } ``` As others have mentioned, the prefix notation makes it 100% obvious that this is a flow-control keyword. As someone else also mention, if the `.await` syntax is selected tomorrow, I'll be very worried that we'll quickly see "annoyingly clever" code. This will make it more problematic for folks like me who need to enforce Quality of Code standards in corporate environments. Any chance you guys can push back the decision so more arguments can be made and the community more thoroughly surveyed? Many thanks, and of course, thanks for the great work you're doing on Rust.
I suppose that there are implicit dependencies on the system here but I find it stunning that this project uses not a single dependency from crates.io!
If you want to be able to build extensions in an efficient high-level language, support WASM extensions via [wasmer](https://github.com/wasmerio/wasmer). This will get you cross-language and cross-platform and sandbox your extensions pretty nicely, without losing much efficiency.
I would argue there still some distance between a general-purpose JIT compiler and a limited set of functionality exposed for the purposes of installing triggers and hooks at appropriate places. :-)
Why don't you just return it as a result from your initialization function? Is there some particular reason you need 2 stage initialization?
Same reason they have two separate Markdown parsers: raisins
&gt; Most CISC CPUs are RISC behind the scenes with a layer made to appear like the CISC one. This is exactly the argument I'm making. :-) Microcode in modern x86 processors is very close to a RISC instruction set in practice. The difference is that the CPU can make informed decisions about what microcode to generate on the fly, as part of the out-of-order execution pipeline. Itanium and PPC architectures proved that compilers cannot make better decisions, or at least were very slow to catch up.
&gt; That's just wasteful though It's not wasteful if something is actually gained by doing it, though. That seems to be the case.
Reducing ambiguity in contexts with defaulted params is nice, but I think a much more important feature of keyword params is documentative. The labels at the call site can be incredibly useful, particularly for parameters that are booleans, Nones, or integer literals (unlike integer variables, whose names serve a similar documentative purpose, but without which labels become necessary). Compare `copy(source: foo, dest: bar)` to `copy(foo, bar)`
&gt; requesting zeroed memory from the OS On Unix, this means using `calloc` instead of `malloc`. But `mem::uninitialized` and `MaybeUninit` are often used on the stack, where `malloc` isn’t used in the first place.
Linux does not decide to cause a segfault when your program does the wrong thing. Your compiled code runs on the CPU, and if it attempts to break the CPU's rules on memory, it will trigger a CPU exception. This causes it to stop executing your code and jump into Linux's exception handler code, basically just telling Linux "this piece of code just attempted to break the segmentation rules, what do you want to do?" Linux just reacts to the exception by sending a signal to the process killing it, then moving on.
alloca could have a similar effect on the stack (non-zeroed memory)
For the sake of completness, if you don't need dependencies you can just go with `rustc source.rs &amp;&amp; ./source`
not sure if you didn't realize you were replying to the author, or were making a tongue-in-cheek compliment...
&gt; However, the "you're on your own" was in reference to avoiding the standard libraries (C++, C, and POSIX) and creating OS threads directly using system calls. My point is just that if you do that, don't expect the standard library to help. Again, that shouldn't be surprising. While that's true, someone doing the same in Rust would most likely still use Sync/Send in the interface and get the same static checks of correctness in the use of Rc/Arc. Whereas in C++ you need to rely on the standard library to ensure correctness because the language won't help directly.
But again it's not like the compiler actively tries to prove "this loop is infinite" so that it can remove it. It's rather that it *assumes* the loop is finite and then bad things happen if it is not. See [this document](http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1528.htm) for why this particular clause is in the C standard, and the optimizations it enables. It is entirely unclear how to turn that into a warning. Compilers don't emit warning when exploiting UB not because they hate you, but because they don't even know that they are exploiting UB -- they just assume you did your job and avoided UB. Beyond toy examples, it's really hard for the compiler to tell when it *incorrectly* assumes UB-freedom.
line 24-25: let mut tortoise: &amp;T; let mut hare: &amp;T; In my experience, one does not usually declare variables like that. Instead, delete those lines and replace line 33-34 with : let mut tortoise = &amp;mcyc[0]; // Start at the begining of the loop let mut hare = &amp;mcyc[1]; It's strange that clippy didnt pick it up to be honest. I'm also not a fan of the algorithm cloning data with `.to_vec()` when it arguably doesn't need to. On big structures, you would end up cloning a lot of data. I'm pretty sure that using lifetimes, you could provide references to the elements forming your cycle. Of course, this doesn't matter if this algorithm is to be used with types of size smaller or equal to that of a pointer, since the cost is the same. One last thing, depending on the use case it might be better to refactor this to use iterators and make the function return an impl Iterator. It's also a good exercise.
Indeed, with [this RFC](https://github.com/rust-lang/rfcs/blob/master/text/2514-union-initialization-and-drop.md) we banned unions with fields that need dropping. That's why `MaybeUninit` has a `ManuallyDrop`.
It is a timer\_handler, which is not initialized (started) until a certain event\_function activates this timer. After initialization it is used within the main-function.
A type cannot be `Send + Sync` when any of its parts are `!Send` or `!Sync`. If you want to access it from multiple threads, you need to wrap it in an interface that _can be_ `Send` or `Sync`, and then implement `Send` or `Sync` for that interface.
Semantically yes it is passed by value, but the actual implementation of closure doesn't need to do this. The heap memory will not be released until the call ends, so there is no actual value in making a second copy on the stack, it's just a waste of time and space. For example here is [the same code in C++](https://godbolt.org/z/CbGkCI) and it is truly zero cost, there is no extra copies. I'd like to have this in Rust too.
&gt; If you're using a GNU only extension to the standard library apparently Yes, I'm talking about properties of GCC's `shared_ptr`, as was the original article. I don't think other implementations of `shared_ptr` have any way to selectively choose atomic or non-atomic for each `shared_ptr` instance, it has to be a program-wide choice.
I found that [https://crates.io/crates/iui](https://crates.io/crates/iui) is very easy to use, unlike conrod
Types are automatically `Send` and/or `Sync` if all their fields are. If that's not the case, you can still unsafely `impl Send/Sync` for your custom types. However, you should only do this if any usage of those traits is completely safe for your type. You should not impl `Sync` if your type has interior mutability that is unsynchronized. For example, `Cell` and `RefCell` have interior mutability, you can mutate the contained value using only a `&amp;Cell` / `&amp;RefCell`. But that mutability is not thread-safe, there is no locking etc. used by those types. Therefore, `Cell` and `RefCell` are not `Sync`. They are `Send` though, because it's safe to move them across thread boundaries. To move something, you need complete ownership of it, which means that no shared references can exist at that time anyway, and the borrow checker will make sure of this. So you can send a `Cell` to another thread and *then* start mutating it from multiple places within that thread via shared references, that's fine. Types should not `impl Send` if they themselves hold references to something that is mutated in a thread-unsafe manner. For example, `Rc` internally holds a pointer to a reference count that all the other `Rc` to the same location also observe, and they increment/decrement that reference count without using atomicn instructions, which is thread unsafe. So `Rc` is `!Send` because internally, it acts like a `&amp;` to something that is `!Sync` - the thread-unsafe reference count.
That's the point. You shouldn't study _only_ at universities, you should to study everywhere.
There was a proposal to add something like that to the GNU C library, which would also detect when it goes back from &gt;1 to 1, see https://sourceware.org/ml/libc-alpha/2019-02/msg00073.html That's still just a proposal for now, nothing has been committed. If it does make it into glibc I will change libstdc++ to use it, making the `std::shared_ptr` optimisation more reliable, and correct even is libpthread is only loaded via `dlopen` instead of being linked in when the process starts. Non-GNU targets would still use the `__gthread_active_p()` detection described in the linked article though, as we don't know any better way to do it semi-portably.
Pretty sure he's heard of it 😂
I'm having issues understanding "cannot borrow `*self` as mutable more than once at a time" error in my code and how to resolve it. Here is a minimal snippet demonstrating my issue: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=115f48e953805eefffac8300f2f8a5f2 How can I iterate over vector of traits and call functions on them passing the Main as a mutable reference to the function?
True, but the logic used in GCC's `std::shared_ptr` that skips atomic ref counting was originally added for GCC's ref-counted Copy-On-Write `std::string`, and then later reused for `std::tr1::shared_ptr` and then `std::shared_ptr`. I agree that the standard doesn't recognize any way to create new threads other than `std::thread`, but the GCC implementation goes further and works fine as long as threads are created by `pthread_create` (which is what our `std::thread` uses internally). If you work directly with syscalls and go behind the back of the standard library by using `clone` syscalls directly instead of `pthread_create` then you're on your own, the C++ standard library won't help, and maybe won't work. That's also true for the GNU C library: a thread that isn't created by glibc can't use **any** libc functions, and must use direct system calls only, because glibc relies on some per-thread initialization that won't happen if you bypass `pthread_create`.
I'm experiencing some strange behaviour with `rustup`. Whatever I do, and whatever `rustup` says is going on, `cargo` always runs rustc version 1.34.1. $ cargo rustc -- --version rustc 1.34.1 I want nightly, so... $ rustup override set nightly info: using existing install for 'nightly-x86_64-apple-darwin' info: override toolchain for '&lt;current dir&gt;' set to 'nightly-x86_64-apple-darwin' nightly-x86_64-apple-darwin unchanged - rustc 1.36.0-nightly (50a0defd5 2019-05-21) Rustup confirms that it worked: $ rustup which rustc /Users/peter/.rustup/toolchains/nightly-x86_64-apple-darwin/bin/rustc **However!** $ cargo rustc -- --version rustc 1.34.1 Any ideas?
The C and C++ standards are quite restrictive about what's allowed in signal handlers, and copying or modifying a `shared_ptr` object is not allowed, see http://eel.is/c++draft/basic.exec#basic.start.term-6 and http://eel.is/c++draft/support.signal Since it's not allowed anyway, it doesn't matter that the GCC `shared_ptr` optimization assumes it doesn't happen (this is C++ not Rust remember, so "that's undefined, your house is on fire now" is a commonplace outcome ;-)
Thanks, will do.
That's not what the C++ standard says: http://eel.is/c++draft/basic.exec#intro.races-21
The method `int` takes mutable references to both a `Component` (self) and a `Main`. But the `&amp;mut Component` that you pass to it is owned by the `Main`. That means `int` could mutate `Main` by removing the component. The borrow checker doesn't look inside `int` to see if it does this or not, but — because of the fact that it _could_ — it won't allow you to pass these two references. If it did allow this, it would be allowing the possibility of `int` inadvertently removing the component and then accessing a dangling pointer.
Right, this is a missing optimization. Wanna file a bug for it? My previous comment was intended as a suggestion for working around this in today’s compiler.
One of these similar SO questions might spark something: * https://stackoverflow.com/q/31281155/493729 * https://stackoverflow.com/q/56232556/493729 * https://stackoverflow.com/q/31281155/493729 * https://stackoverflow.com/q/50245626/493729
They're up there. [https://docs.rs/actix-web/1.0.0-rc/actix\_web/](https://docs.rs/actix-web/1.0.0-rc/actix_web/) There's even a version picker on the crate tab. [https://docs.rs/crate/actix-web/0.7.19](https://docs.rs/crate/actix-web/0.7.19)
You can enable it manually. Works great on my mbp so far.
`T: Send` means it's safe for a value of type `T` to be sent across a thread boundary. `T: Sync` means it's safe for a value of type `T` to be accessed from multiple threads via a shared reference `&amp;T`. These are "auto traits" in that types automatically implement these traits as long as *all data members* implement these traits. It should be impossible for you to create types that violate the Send/Sync guarantees without using the keyword `unsafe`. The point where things fail to be Send/Sync usually involves some form of mutation via a shared reference. Take `Rc&lt;T&gt;` for example: An `Rc&lt;T&gt;` is a reference-counting smart pointer to a `T`. When you clone such a thing, the reference counter has to be changed to reflect that. But `clone()` takes a `&amp;self` argument which usually wouldn't allow changing anything. And yet the original `Rc` and the new cloned one will share the same reference counter that now stores a different value than before. This has been achieved using "unsafe methods" in a way that makes it a bad idea to have `Rc&lt;T&gt;` implement `Send` and `Sync` because that could cause a data race on the reference counter. With some more synchronization around the reference counter, it can be made thread-safe. That's how we get an `Arc`.
Yeah I think WASM would be a good choice here.
If we were to start over with the `std::io::Read` trait tomorrow, would it make sense to instead have this function declaration? fn read(&amp;mut self, buf: &amp;mut [MaybeUninit&lt;u8&gt;]) -&gt; Result&lt;usize&gt;
I uninstalled Rust, and rustup then re-installed and everything now works. I have no idea what happened to get into that state though.
I both love and hate this message. Lol.
In my case, Linux, so I'm quite fine with GTK, but I've also used GTK apps on Windows and they look half decent. Windows is pretty schizophrenic about HCI though, so it might depend on what you're used to.
I agree. It would probably not be acceptable to use WASM in a similar context (i.e. eBPF) because there is no guarantee of program execution termination. However, the idea of using WASM to implement kernel modules might be an interesting idea to explore. WASM gives us a mechanism to expose a restrictive subset of kernel functions and a better portability (i.e. no need to compile your module with each different kernel versions you want to support).
With friends like that.
You might want to bound it with another trait that is orthogonal to `Read`, for example. impl&lt;R: AsRawFd /* + Read serves no role here */&gt; SomeWrapper&lt;R&gt; { fn do_whatever_with_fd(&amp;self) { self.reader.get_ref().as_raw_fd() ... } }
Probably you want `Deref` instead of `Borrow` or `AsRef`. I think it suits your need because it connects the types you mention using an associated type. For example `String: Deref&lt;Target = str&gt;` and `Vec&lt;T&gt;: Deref&lt;Target = [T]&gt;`.
Have you seen [gitstatus](https://github.com/romkatv/gitstatus)? You might want to use that instead of `git2-rs`. Or maybe even rewrite it in Rust :-).
Okay, thanks for the reply! I think I am understanding somewhat you are saying. But how could we make the borrow checker happy and allow the code to compile?
`rustup override set nightly` this makes cargo use nightly to compile the current crate only. If you are in the current crate folder, `rustc --version` should say nightly. In order to switch to nightly everywhere, you should use `rustup default nightly`.
The paid version of IntelliJ IDE does not bring much, but the paid version of CLion bring debugging.
Thanks, I've been reading through SO before coming here. The answer is probably somewhere in there, it's just alot to digest at once for me.
I am still unhappy with the current design. It makes working with uninit memory even more verbose than it already was... You say it's a good thing? No! I agree that 'uninit memory' is bad and should be shunned, however it is sometimes necessary to use as a _building block_ to create safe patterns. My number one use case for uninit memory are _out_ parameters you encounter in C APIs. One of my uses cases of Rust is talking directly to C APIs without first building a rigorous Rust wrapper API (sue me :P). Working with _out_ parameters from C API was already precarious and I wish I didn't need to use uninit for this case. But Rust's only support for this is forcing me to use uninit memory and this change makes it even more annoying to do so... Here's my wishlist for uninit out variables for the purpose of calling C APIs: // Notice Rust already supports uninit variables! // This is safe because the compiler checks all access. let mut x; // New! Let me create a raw pointer to uninit variables to pass to C APIs: unsafe { C_API(&amp;mut x); } // New! Let me tell the compiler that the variable has been initialized: let x = unsafe { std::mem::initialize(x) }; I would much prefer this over the current verbose dance with MaybeUninit which requires me to write down the types of the uninit variable whereas before with uninitialized that type could be inferred. I see this as a UX regression. I know that my suggestion is probably _much_ harder to implement as it would require adding special cases to uninit variables. We _should_ be looking where people are required to use uninit memory and figure out if we can capture these patterns in the Rust language itself where previously not supported instead of just slapping them around with unwieldly verbose syntax and just making it more unwieldly because it has some issues. Well that turned into a Rant... Idk how to communicate this and last time I had a chance to talk to RalfJung all I got was a somewhat condenscending 'you can't possible use this correctly therefore we must remove this functionality' when my point was, please enable me to call C APIs. I don't mind not using uninit, but please give me something to replace it.
I recommend reading some of those SO answers, to understand the different problems and proposed solutions, to see which one applies best to your situation. The answer will probably involve structuring your data differently. This could mean adding indirection so that the components are owned elsewhere and accessed by a key or index. Alternatively you could use interior mutability, via `RefCell` or similar.
Yep, I also tried that! I seem to have just got it into a weird state. Reinstalling everything fixed it and I'm moving on now :)
Thank you very much. One of the SO links looks almost identical to my issue and they are suggesting either cloning values (wouild be very expensive in my situation and I rather not do that), or RefCell:s. Reading up on RefCell now. I have run into the issue before with rust that my mental model of the program turns out to be flawed somehow and I spend days trying to convince rust my program is fine but eventually when I realized what rust is trying to tell me, the only solution was to rework the program. I'm just having a hard time understanding how to rework this stuff.
If that is a goal, it would probably be much simpler to decide on a stable kernel ABI, or alternatively provide such an ABI through a separate module. :-)
Perhaps a `unsafe fn read_unsafe(&amp;mut self, buf: *mut [u8]) -&gt; Result&lt;usize&gt;;` with better ergonomics surrounding creating raw pointers to uninitialized memory. (see my top-level post below) It could be implemented in a backwards compatible fashion with a default implementation which zeroes the memory before calling the `read` function on it.
Yes but dynamic libraries are not what I would call crash proof, exceptions don’t go through ffi boundaries, dynamic libraries are an OS concept and unloading may go from hard to impossible.
FYI, the use of String instead of OsString for cwd and environment variable values could lead to issues if you encounter an invalid unicode value which is possible in linux. You may want to consider vars_os and keeping things as OsString, then write to stdout directly instead of doing a lossy conversion to use print ln!() -- and let your terminal deal with how it should be displayed.
Be aware that calling methods across a JNI boundary is quite slow compared to Java-Java or native-native calls. Probably not an issue for high-level logic or input/event handling. :-)
Thanks for pushing me in the right direction. It looks like a good solution might involve re-thinking ownership and split the "Main" struct into multiple structs, avoding the need to pass &amp;mut self to the int().
Very good job. Wish I would have had this for the past semester!
No worries. I'm a big fan of list comprehensions too.
I know nothing of Rust but this: ``` pub enum ZeroMod8 {} pub enum OneMod8 {} pub enum TwoMod8 {} pub enum ThreeMod8 {} pub enum FourMod8 {} pub enum FiveMod8 {} pub enum SixMod8 {} pub enum SevenMod8 {} ``` is hideous.
Cmake says my c++ is broken on macOS when trying to use this crate. Any ideas ?
Looks amazing! A little super nit-picking suggestion but could you make it so that the transition functions can be declared something along the line of the example below so that it's more in line with the formal definition? transitions { d(q0, 'a') = q0 d(q1, 'a') = q2 ... }
Change the macro expansion engine from the rust options(per project) to the experimental one - for me it made the ide very responsive
Does println do the conversion explicitly or is it the Display trait of OsString that does it?
Reinstall build essentials
At least the result is a Boolean
I see a lot of tutorials still use `extern crate` and `#[macro_use]`. Why is that? I mean, shouldn't it be better to teach new people the 2018 edition right away?
Interesting stuff. Wish I had this to mess around when learning this stuff last term.
Ah yeah. I do like that more. I couldn't decide on a syntax that I really liked in the macro. I have changed it to this. &amp;#x200B; Thanks for the suggestion.
I'm guessing this still shipped with the shaking bug when browsing reddit using webrender enabled in firefox. Here is the bug I reported weeks ago. [https://github.com/servo/webrender/issues/3637](https://github.com/servo/webrender/issues/3637)
&gt; The funny thing is, requesting zeroed memory from the OS is usually exactly as fast as using uninitialized memory Not in my experience. I have some code that got a &gt;100x speedup from switching from doing `vec![0; l]` and subscripting to reserving and pushing. `__memset_sse2_unaligned_erms` dominated the profile.
for me it always takes 5min to initialize ("expanding macros... step 1/64")
&gt; Add entry-like methods to HashSet Hah, I was wondering about this not two weeks ago, and I'm glad to see it be implemented! :)
I took a course relevant to this a year and a half ago, but I also wish that I had this to mess around with. I have no idea why I had the random urge to revisit this material, but it truly was fun.
Isn’t build-essential something for Debian/Ubuntu ?
When an object is `!Sync`, you can still share it between threads using a `Mutex` (but not an `RwLock` -- this is an interesting difference between the two). However, when an object is `!Send`, you're much more restricted. If you were to put such a thing in a `Mutex`, you'd be giving other threads the ability to take it out via something like `std::ptr::swap` or `Option::take`, which would defeat the purpose of `!Send`. In general, when an object is `!Send + !Sync` you really can't access it across multiple threads at all. What's your use case? It's likely that whatever you need to do can be done a different way.
You're on Linux ...
It's hard to know if your proposal is workable without it having a more complete description of what the semantics would be. i.e. How does the compiler track the initialization status of memory if the mem::initialize() call is not in the same function as the uninitialized use?
Small typo; it's `memory-profiler` not `memory-profiles`.
In the code I posted, it turned a loop I wrote with a conditional into a hard-coded infinite loop, then deleted all of the code after it. It may not technically be accurate to say that the compiler is "proving" anything, but I would certainly hope _someone_ has proven _something_ about that optimization. Granted, this _is_ a toy example. But it's a relatively common mistake, and it's fascinating to me that GCC _has the ability to detect it_, but they decided that code was best put to use optimizing _infinite loops_, rather than putting it in, say, a linter. Because golly, if there's anything worse than an infinite loop, it's a _slow_ infinite loop!
That was actually one of the requirements of the project. Some dependencies were vendored, though.
wow dont know how to reply that one but even if i do you need to understand that its because some of us refuse to stoop down low to human levels because we know patience and persistence will get us there one day. so the reply is that i didnt even do JS due to the language it is so why will i ever pay any kind of homage to Node which patronized JS , JS that has kept some of us Genius savants out of commercial front end software development. We scarified our life fighting JS and kept our fingers crossed that one day our time will come, and that time is very very near when Rust gets into front end for real.
I'm not aware of any miscompilations, but what we were feeding down to llvm was seemingly giving it permission to consider the code immediate UB and do whatever to it, as described by the article. Best not to keep that ticking timebomb around.
We came up with the idea of making this our first public "corporate" blog post when we felt that the internal documentation we wrote for this process might also be helpful to others. Please let us know if you have suggestions to improve the code or the explanations!
How I describe it here, it would only be usable locally in a function but not in method arguments, yes. Rust really wants you to use Option/Result and straight returning your result instead of initializing a variable by passing a reference. Which is fine but this specific case doesn't work when the C API dictates the method signature. Thus the above proposed only does the bare needed things to get this particular use case to work, the point being that once you call the C API you immediately wrap things up how Rust wants you to do things. There have been proposals to extend the notion of out parameters in Rust itself through `&amp;out var` references that are references to logically uninit memory. However the story surrounding panic safety and requiring things to be initialized were never quite fully worked out. (reminds me of the trickery you can do in C# with the out keyword). There are other questions like 'what should the signature of initialize look like?' (probably `fn initialize&lt;T&gt;(T) -&gt; T` with special compiler magic to make it work with uninitialized variable). If you pass it something already initialized it acts as the identity function? Another question is what is the type of `let mut uninit; &amp;mut uninit` what is the type of the `&amp;mut` operator? Perhaps `*mut T`. What about the `&amp;` operator? Probably not allowed. All of this is a ton of special casing just for calling C APIs. Is it worth it? For me, of course yes :P but is it more generally useful? Perhaps in the definition of the `io::Read` trait where it is more naturally to express the more general idea of out parameters because you need to provide a buffer of bytes to read into. The idea here would still require unsafe where I think it would be preferable to have such an API not require unsafe code. All I can really tell is that sure, mem::uninitialized is problematic, but mem::MaybeUninit doesn't solve all use cases and makes the rest less ergonomic.
Reddit actually has two separate Markdown parsers, and the new one is written in Rust. :P
[htmlfn](https://github.com/partim/htmlfn) seems like the future of HTML in Rust, why do more code when we can get away with less
Fortunately I don't get the impression that MaybeUninit is designed to be the end-all solution to every imaginable use case for uninitialized memory. Rather it exists so that the Unsafe Code Guidelines group can formalize *any* well-defined semantics for working with uninitialized memory, which was impossible with the old API. Once we have a better-understood model of how we want unsafe Rust code to work, I don't see a reason why people couldn't consider additional APIs for making such use cases more ergonomic.
Is there an RFC introducing dot-await syntax? It may be more effective to bring these comments up there.
I feel the same as you, but I suppose no language is going to be perfect -- Rust already has a couple of rough edges. I strongly dislike the chosen syntax, and I believe it will be a barrier to learning the language for some, but in the long run people will get over this and continue using it. Maybe at some point we'll have a discussion about the preferred idioms in coding, but it seems we're not there yet.
There can be some differences between the MinGW toolchains which are part of Linux distros and the Rust MinGW targets, in particular the exception handling and the used compiler version/libraries. Rust doesn't bundle the complete cross toolchain on non-Windows platforms. You can use `cargo-cross` to build it (`cargo install cross`, then `cross build --target=x86_64-pc-windows-gnu`).
Have you read (this)[https://boats.gitlab.io/blog/post/await-decision/]? Pre- vs postfix `await` is clearly a subjective issue with arguments for either side. The language team decided that a postfix operator is the best option *for rust*, primarily for better composability with method chaining and the `?` operator. Prefix operators would force the use of brackets which would get extremely noisy with nested futures. You speak of 'severe' consequences but don't provide any examples, care to elaborate?
Ahhhh, now it makes sense. So basically CookieIdentityPolicy already do the heavy lifting compared to doing it myself using CookieSession. One last question, I saw the example on CookieSession on flash message. It seems that I have to manually check and clear the flash message on the routes I'm interested at. Is there a way to make this handled automatically? Maybe some kind of middleware Thanks a lot!
The positive thing in your complain is how it places Rust in Bjarne Stroustrup famous quote ‘there are only 2 kinds of languages : the ones people complain about and the ones nobody uses’
I cannot help you without the error message)
I have a weird deja-vu to the introduction of the `?` operator. There was a lot of talk about fracturing the ecosystem (`?` vs `try!`), readability downsides ("hiding" control flow included), more sigils being uncharacteristic of Rust, and strawpolls. And `?` is arguably the more severe case, since it's not only used in a (albeit sizable) fraction of code, but in virtually every Rust module that exceeds a few lines. I don't have to say much about how that decision turned out...
What is the difference between how Copy and Clone are implemented? I'm coming from C and my understanding of Copy is that it does a memcpy on the source and allocates memory for that on the heap with a malloc. How is Clone different?
What is the const folding for? Doesn't LLVM already handle that? https://github.com/rust-lang/rust/pull/60597
As someone who really likes the _idea_ of supporting chaining, I will say that in Rust a lack of chaining is far less painful because variables allow shadowing. Ie, in some languages a lack of shadowing sucks, as it forces unique naming and naming is one of the hardest things. This especially sucks when you don't care about the intermediate steps - await just forced you to name everything. Eg, in a fake other non-shadowing language: let thing_foo = await foo(); let thing_bar = await bar(thing_foo); let thing_baz = await baz(thing_bar); let thing = await bang(thing_baz); However, in Rust this is easier imo. Eg: let thing = await foo(); let thing = await bar(thing); let thing = await baz(thing); let thing = await bang(thing); If you need to name something you can. However, you don't have to come up with obscure names for a bunch of steps that are only named because it's an async action. Not only is this more readable, imo, but it lowers the crap you need to think about. To the programmer, the single name behaves a lot like `foo().bar().baz().bang()`. So while I like chaining, a lack of chaining is far less painful in Rust. I could support a prefix only keyword.
There's no separate RFC; the async/await RFC left the syntax to be decided before stabilization. There are threads where the decision is being made, but this kind of post was explicitly asked \*not\* to be made there. There is no new information in this post. All of these things have already been considered. Repeating them again is not going to change anything at this point.
Copy is always a memcpy on the exact bytes of the structure. There's no malloc involved, it's purely a memcpy call. &amp;#x200B; Clone runs arbitrary code to make a copy. This may be a heap allocation, it may be a memcpy, it may be "bump a reference count."
You'd have to ask those tutorial authors.
No, because a \`Copy\` type can always be copied again, and cannot run code when it's freed.
To be clear, gcc didn't decide this; the language specification did.
I think maybe you misunderstood. I am asking if there is a function something like \`std::mem::is\_copy::&lt;F&gt;() -&gt; bool\` which can tell me if type \`F\` is Copy
I'm not aware of one.
ok, thank you :)
alloca also has a host of issues itself, so you're really trading off one set of problems for another.
Good to know; thanks! Would you be able to link to the existing conversation from here?
Yep: [https://github.com/rust-lang/rust/pull/60445](https://github.com/rust-lang/rust/pull/60445) [https://github.com/rust-lang/rust/issues/53491](https://github.com/rust-lang/rust/issues/53491)
There is a potential benefit to the second version in terms of "cache locality". When you have something like a `Vec&lt;u64&gt;`, all of those `u64`'s are stored in a contiguous piece of memory. That's good for your CPU's cache, because you're not wasting space caching objects you don't need that happen to be adjacent in memory, and the objects you do need are more likely to have been cached already. Contrast that to a `Vec&lt;String&gt;`: here each string contains a pointer to it's own separate allocation. That makes it more likely that you'll jump around in memory as you read each string, and your cache won't help you as much. Also the additional pointer indirection itself isn't free, even if the memory it's pointing to is cached. That said, often times these performance benefits are too small to be measurable, unless this data structure is right in the middle of your very hottest loop. It can be a good idea to do whatever's convenient and clear first, and to only optimize later if your benchmarks suggest you should.
I would prefer not to; they're easy enough to find if you look.
I think in case of C++ it's a language feature, not an LLVM optimization, because even if you disable optimizations using -O0 flag, the `do_call` function still doesn't move any data out of the heap. I've posted the bug if it helps https://github.com/rust-lang/rust/issues/61042
Fair enough.
`Vec&lt;[i32; 60]&gt;` (or `Arc&lt;Vec&lt;Foo&lt;[i32; 60]&gt;&gt;&gt;`) contains all of its data as a single contiguous allocation. If you always process through every `i32` in a loop, this is the fastest option. `Vec&lt;Vec&lt;i32&gt;&gt;` (or `Arc&lt;Vec&lt;Foo&gt;&gt;`) on the other hand has multiple separate allocations. If you always touch every `i32` this will be much slower. On the other hand, if you need to access every `Foo` but only use the `Vec&lt;i32&gt;` for *some* of them, then this option might be faster because the overall memory bandwidth needed may be smaller as you don't need to bring every single `i32` to the CPU.
What event handling system are you using? If event_function is called directly from main, I would create and return the timer_handler from the event_function and use that. If event_function and main are concurrent, then the situation becomes more complicated due to rust's aliasing rules. You will need some synchronizing mechanism, like a Mutex, to ensure both can use it.
This script doesn't pretend to be: 1. Really good bash scripting example 2. Solve all your problems but if it looks useful to you, enjoy it and give me a feedback.
I figured you could via Nightly and about:config, I was just surprised that the announcement didn't mention OS X in the time-line for roll-out. Good to hear that it's been stable though.
You can modify the JVM vmoptions for intellij products to let them consume more RAM. By default they used to only use a small amount but it caused performance issues. Also if you've got any form of antivirus, make sure to let it ignore your IDE and compiler or you'll incur massive speed hits
Full disagreement on all counts. The two best ways to learn [insert language here] are: - Write programs/utilities/libraries/etc. that you use personally, and - Translate code (programs/utilities/libraries/etc.) that you wrote or are very familiar with from other languages. Go advertise elsewhere.
To be fair, even though I think `?` is very usefuly, I'm still a bit uneasy about how small it is, visually. In my own code I tend to insert a space: let something = foobar() ?; to sort of make it more readily visible. The `.async` syntax is from that point of view actually less controversial than `?` :-)
One of the reasons that Rust takes so long to compile is that rustc passes off too much work to LLVM that it could be doing itself. Since rust has _already_ done an analysis on the thing (or things) to be folded, it's silly to pass that to LLVM as is and then have it do a second analysis before it does the fold. If LLVM could just _only_ witness the folded version from the start its whole job will end up being faster.
It’s still helpful to optimize the generated LLVM IR, and rustc has a reputation for generating somewhat inefficient LIR. LLVM itself isn’t the fastest, so optimizing LIR can help with compile times.
AFAIK, you could compile a nearly statically binaries with *-windows-msvc targets by setting RUSTFLAGS="-Ctarget-feature=+crt-static".
&gt; That said, often times these performance benefits are too small to be measurable, unless this data structure is right in the middle of your very hottest loop. It actually is, unfortunately. Hence my obsession. It's already very fast, but I'm always trying to improve performance. This all started because for a brief moment I almost added a new nested complex data structure, we'll call `Foo { ids: Vec&lt;ID&gt; }`, and then it occurred to me that if I stored indexes of IDs _(which works in my case)_, it's far less storage and conceptually on the stack. But I still had `Foo { bar: String, baz: Vec&lt;_&gt; }`, so even if my ID code stays on the stack, String and Vec were causing me not to be. But yea, users are hitting an API needing a few million `Foo{}`'s compared to a few thousand `Bar{}`'s, and math created against the comparison. If this change netted me 1% it wouldn't be worth it, but anything in the double digits definitely would. I definitely plan on benchmarking if the consensus from this post was that it _could_ be meaningful. I just didn't want to _(at this stage in due dates)_ spend a couple days refactoring for comparison benchmarks. Appreciate the reply :)
It's wasteful if whatever you gained from it could be gained for less. And that's the point: there's a better way to get the same benefits.
&gt; I’m fortunate to be able to use Rust, in any form. I feel like it may be the only language I get to use in my career that is so closely aligned with my preferences for a language. I selfishly want it to remain closely aligned with my preferences, and so I've written this post, but I can’t help but feel that I am one of many. You're certainly not alone in general, but I do feel we are a bit outnumbered at the moment. Please don't be discouraged by downvotes or comments that seem dismissive. Things I/we would/could do are: * Make your own informed decisions and apply them to your own projects, in this case that would be a simple macro. * Advocate for it in projects where you feel it would have big value. Projects with mixed sync/async logic, with complex locking, or any case where maximum discoverability of suspension points/potentially blocking behavior really pays off. Of course this only really works with projects you're involved with. With others you can ask, but they might have different preferences. * You can also advocate for middle grounds. Variations are: `.await` and `.await?` only at the end of statements; limiting the number of `.await`s per statement; or/and always placing `.await` and `.await?` on a new line. * If at some point Rust has an official up-to-date grammar, it will be much easier to have independent tools to enforce stylistic choices that follow more strict rules. * If there's ever an alternative prefix `await` version (due to `.match` et al being introduced for example), it should not be too hard to argue for clippy lints to enforce consistency in both ways. But most of all, don't be discouraged from using Rust in general. There's a lot of great people doing great work on Rust (including the language team). On a personal note: Thanks for speaking up. I know that isn't easy. It might not be well received by everyone, but I can say with confidence that it is well received by some. We all care about Rust, but it's a lot easier to not get frustrated when you know others share your specific big picture concerns.
Still, the advantage of RISC is as an interface. Because you limit and specialize the operations you are able to optimize it very aggressively. You still have to waste silicon to expose features that the user may not want. Itanium proved that closed systems cannot work, that closed models will but catch up. ARM is open from the start, and x86 is open de facto. I'm not dating x86 isn't good, it's pretty good, but it also follows a very old model, and literally one command (MOV) is Turing complete, which means that one command needs enough silicon to form a small limited computer on it's own.
Also MIR is much higher level than LLVM IR and preserves all of Rust's semantics. \`const\` folding at the MIR level can do much more advanced folding because it much closer to just evaluating Rust at compile time. In theory this can even fold allocations or whole data structures.
* A driver for the RN2483 LoRaWAN modem: [https://github.com/dbrgn/rn2xx3-rs/](https://github.com/dbrgn/rn2xx3-rs/) * A parser for airspace files in OpenAir format: [https://github.com/dbrgn/openair-rs](https://github.com/dbrgn/openair-rs)
Interesting, appreciate the detail! I hadn't thought about avoiding memory to the CPU. For reference _(in case you're curious)_, dynamic user input _(which we'll call `Vec&lt;Bar&gt;`, and in the thousands of `Bar`)_ is being compared against millions of `Foo`'s. The comparison firstly equality matches per `(Bar.field, Bar.field).eq((Foo.field, Foo.field))`, and then does a bit of math against every `Foo` that matched the `Bar`. The math is done against the `Vec&lt;i32&gt;` values of `Foo` based on indexes provided by `Bar{ indexes: Vec&lt;usize&gt; }`. Any number `&lt; Foo.values.len()` of indexes may be used I don't a common use case at this time unfortunately _(not enough user traffic)_, so I'm mainly focusing around the larger amounts of `Bar` and `Foo` for my worst case.
I dislike how implementation details of compilers, mostly coming from the C and C++ world, are leaking into more safe programming languages API. From the programmer standpoint, there is absolutely no reason for the compiler to try to "destroy" their program by using absurd inferences on e.g. unobserved bools being invalid. In that regard, letting union doing type punning and not having the concept of active union members was the right choice. Arguably, invalid representations of bools yielding to UB is even a poor choice in the general case (I doubt non-trivial real world gains can be achieved thanks to that); likewise for uninitialized memory being considered so much tainted: modeling the real hardware and low level allocators implementations by considering the representation unknown but stable, would obviously yield to less risks, and I'd also be surprised if it currently being UB yields to that much interesting optimizations for valid code. I would have preferred to see a movement toward making mem::uninitialized well-defined and sound, rather than simply working around the issues (with the drawbacks explained, like it disabling other optims that can have an impact on memory consumption). Still, I understand that I'm nitpicking and I will happily use MaybeUnit if I have to, but I consider it to be a compromise, and not the *Right Choice*™.
&gt; they just assume you did your job and avoided UB Yes. The only thing is: that's a *very* bad assumption. We have several decades of practice and all the experts telling us that. Actually Rust would not even need to exist with that kind of assumption. So yeah, this is helpful for compiler writers to merely assume things instead of proving them. This is not for their users (maybe this is for a few video game programmers who do not have to care much about the risks, but who want the absolutely fastest possible binary, even if it yields to more actual bugs -- but for the general purpose programmer this is not helpful).
The language specification does not *mandate* that kind of insane optimization, and would not have prevented the gcc devs from doing something more useful.
Interesting! which platform and which Rust version was that on? Did `__memset_sse2_unaligned_erms` run in userland or in the kernel?
&gt; That being said, assembly is the lowest level a programmer can interact with, so we can call it low-level for that reason alone. Agreed. I just like to remind myself that there's a "life" below assembly, as understanding how the CPU actually executes instructions is necessary for performance work (cache effects, contention, ...).
Ignoring most university courses and only doing enough to get passing grades while learning from reading stuff online and contributing to open source has worked out very well for me. I'm way better off than people who studied more. But hey, maybe I just had a bad university.
Well, for the vector use case I intend for Pathfinder to compete with Skia. :)
Very cool, good luck with your delivery schedule :)
An example: Linux namespace handles are neither Send nor Sync and therefore cannot even be accessed from a secondary thread. https://www.weave.works/blog/linux-namespaces-and-go-don-t-mix
You can only patch compatible versions. If you want to patch an older version, you can branch the older version and backport any fixes to that branch.
&gt; I imagine random access from 10 users repeatedly comparing different sets of Bar's would result in benefits of locality to be inhibited drastically. I think usually a CPU core will sit on a single thread/task for [some number I don't know] of milliseconds, to minimize the costs associated with switching.
In c++ i have a T cast&lt;&gt;() function, which in debug casts a number and its static_cast&lt;T&gt; result into long doubles and checks weither cast indeed can be succeeds. Can this be reproduced in stable rust? Right now i have a choice between as with is useless unsafe garbage, From:: which is only implemented for casts that are always correct anyway, and an extremely unwieldy Try_from:: that also checks in release. Do i really have to make a bajillion cast_"from"_to_"to" functions?
Yes, this post is in part a response to withoutboats' post announcing the dot-await syntax decision. That post does not go over postfix vs prefix, but simply states that the argument had reached a standstill, and the language team had a majority preference for postfix syntax. I have criticisms of how method-like and macro-like syntax are dismissed as misleading, but field-like syntax is given a pass on being misleading because it is more teachable. I respect the argument in isolation, but it's not convincing that dot-await should thus be chosen; it only demonstrates that dot-await syntax is slightly preferable to two other poor options. The assumption that postfix syntax is superior is assumed from on the onset. This was purposefully assumed for that post, but I am challenging that assumption. The readability concerns I have are that postfix await makes it much more difficult to trace the yield points in a function. The Fuchsia example is illustrative of this: https://github.com/inejge/await-syntax/blob/postfix-field/bin/wlan/wlantool/src/main.rs. There are more examples of this in the Async-await experience reports thread on the Rust internals forum.
Wading into the waters of JNI can be a little intimidating, but don’t let that discourage you from giving it a shot. However, if you’re mostly interested in getting something working without writing a bunch of JNI glue and boilerplate, you should check out [JNR](https://github.com/jnr/jnr-ffi), which tries to handle a lot of that for you.
In my case, the patch is actually from nix0.13.0, when i2cdev used nix0.11.0. I'm not quite sure what you mean with "compatible"?
Depends on how the kernels i configured. Afaik you can even tell the kernel to not switch context untill the current thread is done. Wouldnt recommend that tho, for obvious reasons
It seems like this is what [FlashMiddleware](https://docs.rs/actix-web-flash/0.2.0/actix_web_flash/struct.FlashMiddleware.html) is.
I remain indifferent on this subject. I trust that the people involved have sufficiently explored the possible "syntax space" and weighed all their pros and cons. &gt; [...] For these reasons, my plea is that the language team reconsider their decision and ship a prefix version of await. While I'm not persuaded by what you have written. But, as far as I can tell, it's not entirely out of the picture to have both work, `await xpression` and `xpression.await`. Quoting [this article](https://boats.gitlab.io/blog/post/await-decision/): &gt; In particular, some members of the language team are excited about a potential future extensions in which some “expression-oriented” keywords (that is, those that evaluate to something other than ! or ()) can all be called in a “method-like” fashion. In this world, the dot await operation would be generalized so that await were a “normal” prefix keyword, but the dot combination applied to several such keywords, most importantly match: &gt; &gt; foo.bar(..).baz(..).match { &gt; Variant1 =&gt; { ... } &gt; Variant2(quux) =&gt; { ... } &gt; }
Like all other products, compilers are driven by the wishes of their users. In C and C++, where there are *multiple* free compilers, developers vote with their feet^1 and in practice, as it turns out: - Developers will regularly mention they use gcc over clang, despite worse diagnostics and in general slower compilation, because the generated code is better (faster/smaller) for their usecases. - Developers of icc generally pay for it, despite access to gcc or clang, because it generates faster code. So, let's stop this name-calling: **developers** are asking for faster generated code, and compiler-writers just follow their users. If anyone is misguided, it's the compiler users for asking for features that get them into trouble. ^1 *That is, they vote by using a given compiler over an another.*
You may enjoy this talk https://www.youtube.com/watch?v=WDIkqP4JbkE
Oh men. I replied to the creator itself. Didn't I 😭😭
Oh men. I replied to the creator itself. Didn't I 😭😭
Oh men. I replied to the creator itself. Didn't I 😭😭
I think the most important bit in the withoutboats post is this: &gt; but users should be realistic about the amount of consideration this question has already received and how likely it is that truly novel information will be introduced at this phase Are you *really* introducing *novel information* here now? I don't think so.
&gt; bugs accidentally fixed That's always a nice one :)
I'm assuming that you're trying to cast from large integer types to smaller integer types, where overflow may occur. When you say that `as` is unsafe, I'm assuming that what you mean is that it doesn't do overflow checking, not that it breaks the safety guarantees of rust. The idiomatic way would be to use `try_from()` everywhere and use `unwrap()` if you're sure it won't overflow. If you find that there is a legitimate performance increase from disabling overflow checks, you could write a function that uses [conditional compilation](https://doc.rust-lang.org/reference/attributes.html#conditional-compilation) to use `.try_from().unwrap()` in debug mode, and use `as` in release mode. You certainly don't need to write more than one function. That's what generics are for.
Linux is perhaps the most important platform to support right now. Windows and NVIDIA are doing pretty well and don't need much help. Their Linux port needs all the help it can get. I keep having to recommend Chrome to our customers and users of Pop!_OS because basic websites are too janky and laggy on their hardware, even if it's a recently purchased $2000 laptop or desktop.
I hope it does! Skia is annoying to build and maintain, it's huge. The smallest WASM binary we can make is around 3megs!! I'm a big fan of your work on Pathfinder and we definitely explored Pathfinder as a Skia alternative. Unfortunately iOS will never have WebGL2 though, which instantly disqualifies Pathfinder as it requires ES3 right? There are also so many useful features in Skia that would make it hard to leave. SkPicture is one that immediately comes to mind. The multitude of backends in Skia also allows us to have a native renderers, in metal for example, if we so choose. Not sure if PathFinder has any of the pathops tools like SkPath provides.
I *believe* that MIR can optimize generic code whereas LLVM only operates on monomorphized code. Therefore, if a generic method is instantiated 5 different times, MIR only optimizes once when LLVM would need to optimize 5 times...
...except you can't apply as to generics and num::Primitive trait is experimental. that's why i asked. or do you mean writing a macro that substitures with as in release and with try_from in debug?
It is just used to provide for a more readable error message during compilation.
So the point is to avoid `unsafe` here without the overhead of initializing memory, and maybe this could enable safe code in both consumer and producer side? fn read&lt;'b&gt;(&amp;mut self, buf: &amp;'b mut [MaybeUninit&lt;u8&gt;]) -&gt; Result&lt;(&amp;'b mut [u8],&amp;'b mut [MaybeUninit&lt;u8&gt;])&gt;
You're right, I didn't know that. Yeah, you should write a macro instead of a function in that case. Another option is using the `FromPrimitive` and `ToPrimitive` traits from the [num](https://github.com/rust-num/num) crate.
&gt; But most of all, don't be discouraged from using Rust in general. There's a lot of great people doing great work on Rust (including the language team). :+1: -- and thanks for the civil tone :) much appreciated!
Semver compatible. Cargo interprets 0.1 as not compatible with 0.2. There's more details on caret requirements in [the docs](https://doc.rust-lang.org/nightly/cargo/reference/specifying-dependencies.html#caret-requirements).
Just to give some background: The idea is to have generic interface which is low-level enough for user to write own abstractions without overhead, but abstract enough to not depend on particular codec. This is currently not really easy with existing libraries like flate2/brotli and etc.
If new tutorials are doing this, then I'd definitely file a bug. But existing tutorials might just not have had time to update yet, or might not be maintained.
Thanks, I've never had much experience linking between languages, just some exposure to it as a concept. Rust actually gave me most of that exposure from people using it to link to existing C libraries. I'll check those two out, I figured you could make mockups of the types you'd expect them to receive and use it as a boilerplate at the top of whatever source got provided. I hadn't really considered the issue of allowing shared resources/references, probably the smarter thing would be just sending Tags/IDs to the addon that could be passed back to actually handle the modifications. Traits at least should still hopefully negate passing the wrong type of ID around, and shouldn't add as much overhead as the entire type structure.
&gt;But yea, users are hitting an API needing a few million Foo{}'s compared to a few thousand Bar{}'s, and math created against the comparison. If this change netted me 1% it wouldn't be worth it, but anything in the double digits definitely would. &amp;#x200B; In that kind of comparison, shouldn't you be looking at a hashmap anyway?
Are you looking for the video game? That’s /r/playrust
Noo! I mean the programming language
Okay! There’s two discords: the community one and the official one, which is mostly used by the teams to coordinate work. I’m on my phone so I can’t find the invites right this second...
In that fuchsia file, I quickly found my eyes jumping to the `context()` call immediately after the await and then noticing await right before. Async await and combinators aren't exclusive. If they were, Haskell wouldn't have do-notation.
&gt; The readability downsides of dot-await are severe. I on the other hand find many readability benefits with a syntax such as `.await`. In particular, it fits with a left-to-right data-flow-order reading mode. Moreover, instead of having to do two separate scanning modes for `await e` and `e?`, you will naturally spot `.await?` which is visually quite distinct. Also, `.await?` is super easy to `grep` for. &gt; Explicit flow control is important to systems programmers. I can see why it would be substantially more important when working with fundamentally unsound languages such as C or C++ where you hack with fear rather than without. However, Rust's rich type system makes shooting yourself in the foot quite more difficult. Another case where it may prudent might be safety-critical software. In those cases I would approach things by considering formal verification or encoding more things in the type system. Can you speak more about what you interpret systems programming to be? (what are the boundaries?) Also, why should explicit control flow be more important for these systems programmers rather than say a Haskell developer (in which, aside from laziness, reasoning about side-effects is more explicit than in e.g. C)? Also, given the `?` operator, what does that suggest to you about "explicit control flow" and Rust? &gt; Hiding flow control is almost always a bad thing, and is one reason we don't have exceptions. Note that syntax such as `e?` and `e.await` is still very much explicitly denoted control flow. There are no exceptions here and I don't think that anything is hidden. &gt; There is not clear evidence that the majority of the community wants to write postfix / highly-chained code. This is something I largely ignore; I do not consider popular opinion in decisions about language design. I do consider elaborate and well thought out arguments, especially when they provide new information to the discussion.
yes i have webrender enable manually too on my old macbook white mid-2010 and works fine... it's not 60fps but is more than 15 fps is more fluid for an old piece of mac...
 let no_val_1 = node_conv.get(&amp;record.NO_KEY_1).expect("missing key")
Doesn't the Arc trash your cache when cloned/dropped?
You probably just want something like `map.get(key).expect("No key in map!")`.
Forming an opinion of a language based on a set of case studies of the strangest-of-the-strange edge cases is probably a bad idea.
Linux doesn't provide a stable ABI for kernel modules. This is one of the reason why I find WASM interesting to explore in this context. But anyway I'm not a kernel expert.
Yep that works, thank you
In this match match no_key_1 { Some(no_key_1) =&gt; (), None =&gt; panic!("missing no_key from node file: {}, for line ID: {}", record.NO_KEY_1, record.ID), }; `no_key_1` binding inside `Some` patter has type `u32`, but it doesn't affect `no_key_1` declared before. You want make this `match` expression to evaluate into inner `u32` like this let no_key_1 match no_key_1 { Some(no_key_1) =&gt; no_key_1, None =&gt; panic!("missing no_key from node file: {}, for line ID: {}", record.NO_KEY_1, record.ID), }; See, now new `let no_key_1` has type `u32`. Ofc `Option` already have methods that do just that - `Option::expect` and `Option::unwrap`. Where former allows custom error message.
Okay easy enough, thank you
I remember reading somewhere that Rayon will run things on a single thread when it determines that it is the best to do so. It looks like Rayon is unwilling to throw more threads at the problem after &gt;8 threads, not sure whether that's caused by the heuristic used by Rayon or overhead, I doubt it's the latter though.
That is exactly what I want! I'll have to look into it more, I never even considered using it outside of a web application. I don't know how many examples I'll be able to ~~shamelessly steal~~ model code after, I haven't seen too many examples where it's used outside of web/javascript interactions, but then I haven't been actively looking yet. The documentation seems a little sparse, but seeing as it's (at first glance) just 'Write the code in C++/Rust, Compile, Load' for any addon developer hopefully it shouldn't be too big of an issue. Especially considering step two and three could be handled by the main program using existing libraries.
Thanks for the explanation on match and Option, I was looking for something like that. I'm guessing that using expect is more idiomatic in Rust programming?
I don't think that's the case, given the documentation: https://docs.rs/rayon/1.0.3/rayon/struct.ThreadPoolBuilder.html#method.num_threads
Thanks for both your guys replies, as that is actually what I was thinking of when I imagined this. I was hoping there was a way to write it with it being less a ffi, and more a rust program ran on a separate thread using an API provided by the main program. I won't say I know anything about dynamic libraries, so I'll take your word on unloading being difficult. I've never had to mess around with them, but I'll put it on my growing list of things to look into
It slows down way before 8.
Can anyone [try this page](http://output.jsbin.com/surane/quiet) on WebRender please? Is it still slow?
It's difficult to answer that without knowing more about your OS etc. Rust as a language knows nothing about menu bars. There is no standard library support for GUIs and most likely never will be, due to Rust philosophy of keeping the standard library compact. See [Are we GUI yet?](https://areweguiyet.com/)
In a way it was. You could look at C code and eye-ball what instructions will be executed in which order. Which is not true for languages like ruby and C#. It all depends on the level of nit-picking you willing to commit to. You can start saying that C has a runtime and guess what? You will be technically right.
I find invite weird but... did you mean: https://discordapp.com/invite/rust-lang ?
There is a matrix room too. https://matrix.to/#/#rust:matrix.org
Interesting. Please do post the follow-up benchmarks!
And if you want to keep your extended info in there: map.get(key).unwrap_or_else(|| panic!("... {}", record...))
That's a fine opinion, but it would need serious data to back it up, and lacking that I could only cite the usual counter-examples about the projects migrating to clang. Which I doubt is highly relevant anyway - concerning the rational you want to give, because it is both not that fast anymore for compile time, and not that bad (actually even quite good) as far as codegen goes. I *think* it is slightly more careful in regard to UB than gcc is, but I'm not sure. Now of course there are gonna be some people citing benchmark or personal experience about binary optim to choose a compiler or another, but there are tons of other more convincing criteria to use gcc over clang or the inverse (or anything over anything, just using examples) : most Linux distro are widely build with gcc, so it is just easier to use that; on the other hand gcc is not very good for good PE and or MSVC compat under Windows, nor does it let use MSVC STL, while mingw (or the like) distros ship very incomplete standard libraries; yet another factor can be the amount of archi gcc can target; and so over. Also, I never saw benchmarks of the safety of codegen. People for which this is important would maybe *also* use them if they existed. I'm not advocating for non-optimizing compilers; I'm advocating against dangerous optimizations, and the idea that it is not a big deal to amplify bugs because they are of the fault of the users to begin with. Last, I've yet to see anything looking like a scientific approach confirming that exploitation of UB is *required* to obtain good perfs. On the contrary Rust seems to do extremely fine while having strongly reduced their number, and msot interesting optims often happen at an higher level nowadays (but Rust still can yield good codegen, even from safe sections). And yes, it is harder to do optims using more sound techniques. I'm not calling names; I'm recognizing this is hard. I stand my point that making the hypothesis that there is no bug for optimization purpose is an absolutely terrible idea if not approached very very carefully, and my opinion is the whole C and C++ ecosystem went waaay too far. I also stand my point that experts consider that it is impossible to expect programmer to be just careful and not write UB; that's complete wishful thinking, you might as well require that they never write any bug at all -- so yeah: good tooling is required and IMO static guarantees are to be strongly preferred in that domain (but dynamic is better than nothing). But there is hope; *some* recent C++ papers are appearing which would reduce the number of UB, and IIRC a working group more or less on that subject has been created.
Perhaps I'm reading the graphs wrong, but how is it that running with 2 threads gives more than a 20x speedup? And using 5 threads gives a 40x speedup?
Yup, and that's the primary reason for abandonig that.. LWJGLs controller support is very limited in various ways.. Thanks for the input, I'll look into JNR!
I have an issue for WebGL 1 compatibility: https://github.com/pcwalton/pathfinder/issues/139 When you say you want `SkPicture`, what does that mean exactly? Do you mean just the fact that sort of serializable and deserializable vector scene format? If so, the Scene object in Pathfinder could probably be made serializable. There are two "backend" abstractions in Pathfinder: Device and Renderer. A Device is a graphics backend, such as Metal. A Renderer is a complete reimplementation. I think it's useful to have both, because the higher-level Renderer logic is identical regardless of the GPU API in use. There are not as many path operations exposed in Pathfinder as `SkPath` presently, but feel free to file issues if you need some. Lyon has some more. It sounds like you're fairly wedded to Skia, however. Pathfinder isn't designed to follow Skia's API exactly. It will necessarily do things differently since it's designed differently.
I think one issue might be that rayon fork-joins until individual operations. When those operations are cheap (as it is in this case), this adds some overhead. Maybe consider `par_chunks` instead of `par_iter`. That would also allow vectorization within the chunks.
Enjoyed the post, thanks!
Thanks!!
&gt; and conceptually on the stack. But [...] even if my ID code stays on the stack, String and Vec were causing me not to be I find it helps to think about it as "inline vs. not-inline" instead of "on the stack vs. not on the stack". Elements which are behind a reference or a pointer are not necessarily on the heap, and elements which aren't behind some indirection aren't necessarily on the stack. Then, your initial question becomes "When storing large objects on the heap, eg Vec&lt;Foo&gt;, is there any benefit to _keeping the members of Foo inline_?". When phrasing your question like that, the issue becomes one of figuring out whether packing things tightly is better than spreading it out. You thus focus on what that trade-off entails, namely the issues of memory usage (more pointers = more space used; a pointer is 8/16/32/64 bits wide), cache usage (less cache lines' worth of memory that is accessed = less cache used = a larger fraction of your working set in cache = faster execution), cache misses (sequentially laid out memory is more-or-less automagically prefetched into cache when you read through it; but high cache usage leads to more cache misses), memory fragmentation (many small blocks vs. fewer but bigger blocks, one could write books about this trade-off), copying costs (Moving a Vec&lt;T&gt; entails copying 3 x 64 bits on 64 bit platforms, regardless of the number of elements. Moving a 1,000,000-element slice of `Foo`s entails copying 1,000,000 * `size_of::&lt;Foo&gt;` * 8 + 64 bits. If this slice is stored inline in a type placed in a `Vec`, each time the vector grows it has to move all those elements), etc. All in all, I think your reflex of asking the community, but wanting to benchmark it anyway, is great. If you write a bunch of programs using big collections of elements in the hot path in Rust/C/C++/friends (but not e.g. Java or Python or other languages which put anything behind a pointer anyway), you get some intuition, but once every so often your intuition will be horribly wrong anyway. So: don't do silly things (putting a `u8` in a `Box`, or a `[T; 5000000000]` inline in a vector that'll get resized constantly), use some sane defaults (remember that e.g. Java gets away with references everywhere), and benchmark as a sanity-check.
I understand your frustration. But that's now how compilers work. GCC arrives at this through a series of small transformations, and every one of them makes sense in some reasonable code and is by no means an indication that the code is doing something wrong. This is like the case where GCC miscompiled the Linux kernel by removing a NULL pointer check: after a pointer is dereferenced, GCC adds "this ptr is not NULL" is the set of known facts. Later, that fact is exploited to optimize away a comparison. Such redundant comparisons happen all the time after inlining, so that is not on its own enough to lint in a meaningful way -- but it *is* enough to optimize.
https://en.wikipedia.org/wiki/BLAKE_(hash_function)
&gt; Also, I never saw benchmarks of the safety of codegen. People for who this is important would maybe also use them if they existed. Simpler compilers that implement a version of C with less UB exist, like compilers used for embedded or safety critical software, including a formally proven correct compiler (CompCert). I don't know if there is a free one, but I'd expect them to be cheaper than ICC. &gt; Last, I've yet to see anything looking like a scientific approach confirming that exploitation of UB is required to obtain good perfs. On the contrary Rust seems to do extremely fine while having strongly reduced their number, and msot interesting optims often happen at an higher level nowadays (but Rust still can yield good codegen, even from safe sections). Unsafe Rust has (almost) all of the UB C/C++ have (except for boring stuff like integer overflow). Rust "solved" this not by making UB any simpler or making a less aggressive compiler, but by establishing a type system that is strong enough to build safe abstractions. So, the existence of Rust on its own does not provide any data here I think. But I agree that it would be interesting indeed to study how much performance C/C++/Rust get from the UB that they have -- how much they'd lose if certain assumptions cannot be made by the compiler any more. But given that every compiler developer's life goal is to improve performance of some benchmark suite by 1%, I am not very optimistic about the community as a whole accepting even a slight decrease in performance. :( (Yes this is a hyperbole, but it is not far from the impression I got when talking with various compiler folks -- mostly those that are as frustrated about this as I am.)
I'm no expert but that's how I understand it. `.expect("describe the invariant that will have been broken if it panic'd here")`
Also, keep in mind that `Some(node_conv.get(&amp;record.NO_KEY_1))` is taking the `Option&lt;u32&gt;` you got from `node_conv.get` and **puts it inside another `Option`** - so you get an `Option&lt;Option&lt;u32&gt;&gt;` (e.g. `Some(Some(42))`), and the outer option will never be `None`. You don't need to put the result of `node_conv.get(&amp;record.NO_KEY_1)` inside `Some(...)` to make it an `Option`.
Paying close attention in most university courses and diligently doing all the exercises while chatting with other students and searching online resources when the lecture did not satisfy my curiosity has served by very well. ;) But different people learn in different ways.
The chunk sizes you use are really small. Try making them x10, also align them to power of 2. Toy with it and see how it affect performance - find a point in which you get best perf.
For some more context, BLAKE2 is a modern hash function whose ancestor BLAKE came close to winning the SHA-3 competition. It's also the basis of the Argon2 password hash. BLAKE2 often tops the list of recommendations for migrating away from legacy hashes, for example recently: https://www.zdnet.com/article/sha-1-collision-attacks-are-now-actually-practical-and-a-looming-danger/
I think it's a cache invalidation (or [false sharing](https://en.wikipedia.org/wiki/False_sharing). The non-linear scaling of performance vs threads used including the massive loss at the end makes me think that is happening. This behavior reminds me of a talk given by Scott Meyers ([CPU Caches and Why You Should Care](https://www.youtube.com/watch?reload=9&amp;v=WDIkqP4JbkE)) where he showed a similar trend happening to a program searching for even numbers in parallel. He found that some threads were writing to the cache line of other threads, causing them to reload their cache line. This causes performance hits, which causes a non-linear scaling in performance. Granted, I could be wrong since I'm still learning more about this area of performance, but the cases seem pretty similar.
seems cargo is not switching CurrentWorkingDir to the sub-project, but stays in the root. I don't know a solution for your problem, but another detail is: Each [build.rs](https://build.rs) should communicate back to cargo at least a single "rerun-if-changed" otherwise cargo might monitor all files of a project for changes, causing erratic re-builds.
I'm not super up to date on how OMP directives work, but this seems to indicate that it has SIMD optimizations enabled? https://github.com/UoB-HPC/BabelStream/blob/master/OMPStream.cpp Maybe you could see if you could turn the SIMD off on those and re-run the test? Or try adding faster or a different SIMD library to your version.
Thanks for pointing that out, that's definitely not the behaviour I would want
&gt; Unsafe Rust has (almost) all of the UB C/C++ have (except for boring stuff like integer overflow). Rust "solved" this not by making UB any simpler or making a less aggressive compiler, but by establishing a type system that is strong enough to build safe abstractions. &gt; So, the existence of Rust on its own does not provide any data here I think. I think it does. Most Rust code is made of safe sections, and the code of those sections optimize well enough. I think the explicit distinction between the two makes it slightly less problematic to leave a few risk of UB in unsafe sections, but does the good perf of Rust really comes from the exploitation of the hypothesis they do not exist? Maybe a few are important, but I suspect (and hope) that some are irrelevant and will eventually be replaced by a model less insane. For example do we really gain anything by considering access to uninitialized memory UB (for optim purposes) rather than just containing "random" garbage? Or even just "existence" of objects with invalid representation, even if not accessed? I know what we risk, so it better yield insane perf optims...
Yeah, a more specific newtype is better. This is just the most general version because I have no idea what OP is actually trying to do.
Thanks for the feedback! I happen to be a big fan of encoding invariants in types, which is why I supported and pushed this approach, but I guess I cannot convince everyone. :) To reply to some of your concrete points: &gt; Arguably, invalid representations of bools yielding to UB is even a poor choice in the general case (I doubt non-trivial real world gains can be achieved thanks to that) Without that assumption, every `if` has to be compiled to a three-way test to panic/abort if the `bool` if neither `true` nor `false`. A jump table or two conditional jumps instead of one seem like some serious extra cost to me. &gt; likewise for uninitialized memory being considered so much tainted LLVM exploits `undef` heavily during register assignment, that's (one of the) reason(s) why "uninitialized" values can "wobble" in practice. Not doing this would thus increase register pressure, which is a precious resource. Unfortunately I am not aware of a systematic study of what these kinds of decisions actually gain in terms of performance. However, I have seen the struggle my coauthors had when trying to get the slowdown in LLVM for [our paper](https://people.mpi-sws.org/~jung/twinsem/twinsem.pdf) down into the low-single-digit percent range.
Aha! Of course, it makes sense to make that optimization after a function has been inlined. That would also explain dead storage elimination, too. Thanks for helping me understand.
We did have miscompilations with things like `mem::uninitialized::&lt;!&gt;()` (this was in generic code, e.g. in [`rc::Weak`](https://github.com/rust-lang/rust/issues/48493)). And I think we also had trouble around uninitialized references (conflicting with the non-NULL assumption).
I guess you'd be good to reinstall your corresponding package on MacOS then.
Sorry, 100x was a bug on my part when I rewrote it just now. The actual speedup is only about 10x. This is x86_64-unknown-linux-gnu, Rust 1.34.2 (I probably first did it on 1.32 though). It ran in userland, taking &gt;91% of the execution time. With reserve+push, it isn't on the profile at all.
The software z buffer is definitely interesting, especially because we have some very deep scenes. If we were able to ship a smaller wasm binary and not have to deal with emscripten, I think that would be worthy of some effort. For that reason I'm definitely keeping an eye on Pathfinder. Our vector renderer is relatively well abstracted anyway. SkPictures are great for caching or tiling work. We have some fairly intensive JS work outside of the wasm boundary generating draw instructions and it's trivial to draw on an SkPictureRecorder. Being able to replay those at different resolutions completely within the wasm boundary means any unoptimized JS code is eliminated from playback. Of course we could solve those problems in better ways and we plan to. SkPictures also automatically cull and they're useful in the SkDebugger for finding optimizations. I'll checkout the scene object in Pathfinder.
&gt; I think it does. Most Rust code is made of safe sections, and the code of those sections optimize well enough. That same code would optimize just as well if it was in an unsafe section, or in C++. Optimizations are not affected by whether you are writing safe or unsafe code. The good optimizations on safe Rust rely *heavily* on the assumption that there is no UB. For safe Rust, that assumption was proven by the compiler. For unsafe Rust, that assumption is up to the programmer to prove, like it is in C++. Safe Rust is like a version of C++ that statically detects all UB. Such a C++ wouldn't optimize any different, or have less UB -- it just would just refuse to compile a bunch of programs that are currently accepted by the compiler.
Oh, I didn't intend to throw shade at Rust, just the chunk of code I pasted there.
That’s the official one, yes!
Smooth for me on Firefox 67 / Linux / Intel HD Graphics 630 / Mesa 19.0.4 (WebRender force enabled).
&gt; Without that assumption, every if has to be compiled to a three-way test to panic/abort if the bool if neither true nor false. A jump table or two conditional jumps instead of one seem like some serious extra cost to me. Yes it would be bad. I was more thinking about something like byte 0 =&gt; false, anything else =&gt; true. Actually what bothers me is the kind of unbounded nature of UB, which sometimes argued as an all or nothing way to do optims for its proponents, and can yield to now famous insanity like https://t.co/mGmNEQidBT If some unchecked hypotheses are passed to overly aggressive optimizer, the impact of them being actually false should be considered very carefully. If panicking is actually too expensive in some cases, then a good compromise can be a deterministic outcome that is known to be implementable cheaply enough (Rust integer overflow are a good example of this). But if instead it really degenerates to "anything can happen", then well, anything can happen... I understand the some high-level choices are influenced by the LLVM implementation, and in practice I trust the people working on it to do the realistic compromises. I would just have liked an alternate history in which the outcome would have been different :P
He has the definition of speed-up on the y-axis label - n-threaded performance divided by single threaded performance. This is pretty typical in HPC research and generally more meaningful than showing raw numbers. This graph is the ratio of the rayon and non-rayon graphs you propose.
Postfix async has not been officially accepted yet. Accepting a PR for this would be the wrong move by editor maintainers until that happens.
I don't think there is much evidence to support the claim that the complexity of interpreting the x86 instruction set is a serious impediment on modern processors. Compared with all the other comparably much more complex features, I doubt that makes any measurable difference. Those Turing-complete mov instructions end up being compiled to different microcode instructions, and any out-of-order RISC-like processor with similar performance goals to best of class x86 processors will likely contain a similar amount of transformation.
I'm having trouble grasping this but would really like to understand. So if copy makes a memcpy call, like as in void *memcpy(void *dest, const void *src, size_t n); then something like the below Rust code happens: let x: i32 = 123; let y = x; So we had x stored on the stack, and now we have another variable, y, also stored on the stack, and they both hold the same value in separate places of the stack? In this case it would be equivalent to, in C: int x; int y; x = 123; memcpy(&amp;y, &amp;x, sizeof(int)); Am I understanding that correctly? And if so then Clone is different because sometimes it will do the above just like Copy, but will do other things in other contexts depending on some other factors?
You misunderstand me :) I mean that with two threads I would expect the program to be at most twice as fast. Yet in the graph it looks like going from one thread to two increases speed by a factor of twenty. That seems inexplicable to me.
Well, it's not a better way if the gains are not substantial enough to offset the considerable cost of switching to a different and incompatible architecture. Compatibility matters. Tooling matters. Cost matters.
Well, it's not a better way if the gains are not substantial enough to offset the considerable cost of switching to a different and incompatible architecture. Compatibility matters. Tooling matters. Cost matters.
Following this [Tensor's youtube videos](https://www.youtube.com/watch?v=EYqceb2AnkU&amp;list=PLJbE2Yu2zumDF6BX6_RdPisRVHgzV02NW), trying to work up to at least being able to follow the logic and decisions behind making a [toy blockchain](https://www.youtube.com/watch?v=U8GGZ4TqlQs&amp;list=PLJbE2Yu2zumDF6BX6_RdPisRVHgzV02NW&amp;index=18) in Rust. I am seeing that some blockchain technology (like Filecoin) is using Rust and I am intrigued.
When an assumption is proven I've got no problem with it. I even do not really call it an assumption anymore. But at this point this has become just an implementation detail inside the compiler. Once the compiler actually proves something (possibly thanks to an enforced type system), I've got no problem that it uses that proof. When this is exposed to the programmer, this is another story. You have to consider the ergonomics and the impacts of "errors", and even more interestingly if some things should even be errors at all (e.g. possibly an invalid object that merely exist but is never accessed should just not matter?) To be clear I'm not asking for anything magic like making even unsafe sections safe. Just as a user I've a different mental model than just C++ + static checks, and I question the interest of exploitation of some (most?) UB for optimization purposes even in C++ to begin with.
Okay, so, the other thing I need to point out here is that all of this is the discussion of the *semantics*. The optimizer takes in what we talk about here and does all kinds of stuff to it. It's possible that the final object code eliminates a lot of things. &gt; So we had x stored on the stack, and now we have another variable, y, also stored on the stack, and they both hold the same value in separate places of the stack? That's correct. Even in debug mode, `let x = 123; let y = x;` doesn't literally invoke memcpy; it does `movl $5, (%rsp); movl (%rsp), %eax; movl %eax, 4(%rsp);` But yes, as you can see there, you have to independent copies of the value `5`. &gt; In this case it would be equivalent to, in C: int x; int y; x = 123; memcpy(&amp;y, &amp;x, sizeof(int)); Yep. Hilariously, the codgen is *slightly* different, but basically the same https://godbolt.org/z/COE9Xl &gt; And if so then Clone is different because sometimes it will do the above just like Copy, but will do other things in other contexts depending on some other factors? Correct. Clone can run arbitrary code. But you can't change the semantics of `Copy`.
Fair enough!
Are you sure? How does 2 cores have 20x speedup?
Out of curiosity, what is your use case in particular?
Been working on my own Python-inspired language called [Mamba](https://github.com/JSAbrahams/mamba). Kinda like Python but with null safety and type safety, and perhaps someday some cool type refinement features and pattern matching (still figuring out how I'm going to do that one).
I haven't audited his calculations, but the speed-up calculation/graph is very standard. As for the 20x speedup, I agree it seems a bit strange and I would not be surprised if there was an error somewhere, either in the calculation or the experiment. I have no explanation for the results, but they don't really matter as my comment was purely about the presentation of the results.
Also curious about this, re:Amdahl's Law
Oh dang I didn't see that! Thanks a lot for your help!
Maybe not entirely relevant, as this is strictly pack/unpack on a buffer, but what do you think about Java’s abstraction over archives? It provides a `FileSystem` interface and then you create/use a zip/tgz/whatever implementation and just use the regular `FileSystem` methods, making it indistinguishable from working with files on your OS’s filesystem, re-using `Files.copy()` etc. Do you think something like that might work for Rust?
What do you mean by "arbitrary code" when you say: &gt;Clone can run arbitrary code.
To implement `Clone` for your type, you write: ```rust impl Clone for Foo { fn clone(&amp;self) -&gt; Foo { // your code goes here } } ``` Anything at all can go inside `//your code goes here`, as long as it returns a `Foo`. If you derive `Clone`, it makes a copy by recursively calling `clone` on all of the fields.
Interesting. Can you give an example where this happens in practice.
&gt; two 2.1 GHz, 18-core Intel Xeon E5-2695 That should be roughly 36 cores :)
Graphic design software. I can't share much more than that publicly (without getting permission) but I'm happy to share as much as you'd like privately.
Huh that speedup on unpacking docs sounds like what I got on Windows 10 by turning off real time virus scan during the rustup update. Glad to hear they found a way to do it w/o me having to do that :)
It sounds interesting. Do you plan on releasing that as free software, and protecting the freedom with a copyleft licence?
We're doing different things which mitigate but don't eliminate the delay which disabling the realtime scan for the rustup directory will do. Both will give you even more speedups. I've heard reports of as fast at 20s :D
Oh wow, that's really good to know. Thanks!
I agree completely. The improvements are not enough to offset the benefits of inertia, which are far greater. That is the whole reason behind worse-is-better. Even RISC is not the killer thing in a CPU, you have to look at the greater picture. This is why openness and many alternatives matter: it helps create more of a foundation.
&gt; When an assumption is proven I've got no problem with it. I even do not really call it an assumption anymore. But at this point this has become just an implementation detail inside the compiler. Once the compiler actually proves something (possibly thanks to an enforced type system), I've got no problem that it uses that proof. Sure. But this does mean that you cannot use safe Rust as a data point for "how fast would C++ or unsafe Rust be without all the UB". Safe Rust, as far as the optimizer is concerned, *has* all this UB.
Well, Rust has `Read/Write` which would work for just files. But handling multiple files/directories is quite tricky, and not all algorithms allow to put into a directory. I wouldn't say it is impossible, but my current goal is to work on library for HTTP related compressions
Writing an ActivityPub server. The current main goal is to refactor the Webfinger and mention parsing code
I agree that there isn't evidence that this is the reason x86 would fall (or that x86 is doomed at all). I mostly countered that CISC wasn't the reason for x86 success, but many other factors. RISC, while an improvement is not enough to defeat the other benefits. I think the difference is observable (as in we can see things get worse) but I do agree that it's not measurable (as in we can't really tell what would be the effect of changing this one thing, as so many other things change too). I would argue against "the other much more complex features" because RISC is all about avoiding those as much as possible and exposing a simpler interface, and let the complexity happen on a layer above (which ultimately the CPU must do). things such as pre-fetching and speculative execution, if happened at a software (or at least firmware/micro-code level) would be more manageable in security issues and challenges.
 #![feature(specialization)] fn is_copy&lt;T&gt;() -&gt; bool { trait CopyInfo { const IS_COPY: bool; } impl&lt;T&gt; CopyInfo for T { default const IS_COPY: bool = false; } impl&lt;T: Copy&gt; CopyInfo for T { const IS_COPY: bool = true; } &lt;T as CopyInfo&gt;::IS_COPY }
Why not `Result::cloned` and `Result::copied` too?
It sure would be nice if self-updating was the *first* thing rustup did, so that bugfixes and speed improvements like that work in time.
Okay. I think what you are saying is to write my own thingy for user clicky fun, then. Thank you! I will get right on it as soon as my heart rate is back to normal- just got back from counseling.
It's a keyword wherever it appears, so I don't see any issue TBH
then you should find the error and fix it, then rerun the tests.
Use an `Rc` over an `Arc`, if you can. Much faster to clone.
I'm not sure I follow - this isn't my work
Or just `src/bin/`
I think it has to do with the cache/memory usage when you increase the number of processors available. Given that these operations are thread-independent, having more cores will lead to having more caches (assuming this is how they are designed etc etc) Here's an SO article describing superlinear speedup: https://cs.stackexchange.com/questions/55433/what-is-meant-by-superlinear-speedup-is-it-possible-to-have-superlinear-speedup
Unpacking docs is much, much faster. Before it was upwards of a minute for me, now it's 8s. Entire update of nightly was done in under a minute.
ops sorry, though you where OP
Huh, apparently it was using `malloc()` + `memset()` in userland instead of `calloc()`. What was the type of the vector? It's [known to not lower to calloc()](https://github.com/rust-lang/rust/issues/54628) for types that do not implement the IsZero private trait. There is a [PR outstanding](https://github.com/rust-lang/rust/issues/60978) for some of them.
In addition to what was mentioned here, something I realized recently was that the `Option::ok_or_else` method lets you error handle your options in a nicely clean way. Before I knew that I would use `match` on things like `let foo = foos.get(1);` to return an error in the event of `None`. Now with `ok_or_else`, the big block of `match` is reduced to almost nothing; `let foo = foos.get(1).ok_or_else(|| Err("1 index missing"))?;`
Yes to the first, no to the second. Progress continues, and there are reasons to be very excited, but right now I'm not doing a lot of publicity.
It does not appear these graphs match the raw data. I think there is a bug in your plotting script. Specifically, I don't think the update in `get_speedup` `res\[i\] = res[0]/res[i]` is what you want. The first step sets `res[0]` to 1 and every step after that sets the result equal to the reciprocal of the runtime. Doing a few checks on your raw data shows more like 2x, 4x, etc. speedups for 2, 4, etc. threads. &amp;#x200B; Just doing a few spot checks on the raw data, it appears that Rust is actually pretty close to C++ in terms of speedup, although the runtimes are a bit slower.
Others agree with you (including myself) -- we have an issue open here: https://github.com/rust-lang/rustup.rs/issues/1838 and would welcome PRs :D
I'm really glad that it's a big improvement for you.
I think you grossly overestimate how significant this increased cache size due to using more cores can be. I think it extremely unlikely that this could result in a 10x speedup over the expected speedup even on very far-fetched microbenchmarks. I think OP is making some kind of error in their measurements.
How many cores does your machine have?
The problem with this is that if you want to compress/uncompress a collection of bytes (the most natural structure to actually compress/decompress) then you are out of luck. It's relatively easy to send a collection of bytes to a file but if you use a file as the level of abstraction then you can't do anything else. &amp;#x200B; A good example: I'm dealing with a format which has a binary header, a collection of key value pairs in ascii, and then finally a zip file stuck on the end. With a file based compression/uncompression system I would have to stuff the zip file that is at the end of this format into some kind of file as a temporary. &amp;#x200B; But with a Vec of bytes I can create adapters which allow me to read/write to files or to a memory structure or a network stack.
NVMe SSD finally getting to stretch it's legs, I suppose. I'm curious what change actually allowed this. Perhaps setting `mtime` or calling `stat` on every file previously was causing a full filesystem sync, leading to massive wait times (relatively) on every file.
In my experience, OMP SIMD optimizations are pretty similar to compiler auto-vectorization, so I don't think it's a major issue here.
The measurements seem OK - I believe the error is in the plot generation script. Specifically, it looks like his actual calculation of speedup is incorrect (it appears the result of his speedup calc is that speedup is set to the reciprocal of runtime).
16C/32T Threadripper 1950X. However, as stated in my other comment, I think the speedup is more to do with the removal of things that would hinder the performance of my NVMe SSD.
It doesn't go all the way to individual operations. The core way in which it works is https://docs.rs/rayon/1.0.3/rayon/iter/plumbing/trait.Producer.html and as you can see that switches to sequential processing as an iterator. This is confirmed by the `Splitter` type: https://github.com/rayon-rs/rayon/blob/master/src/iter/plumbing/mod.rs#L256
Yea, this seems really fishy, until op (or someone else, by running op's benchmarks) explains the results, i don't really know what to make of this.
\&gt; this option might be faster because the overall memory bandwidth needed may be smaller as you don't need to bring every single i32 to the CPU. &amp;#x200B; You shouldn't need to do this anyway, no?
In part, but also we now close handles in a threadpool. Since `CloseHandle()` is where inline virus checkers such as Windows Defender tend to do their work, this means we're utilising all of your cores to close files instead of doing it all on one core. At least that's how I understand the change that Robert made :D
Each "syscall" was actually resulting in several windows syscalls AIUI, the IO models are sufficiently different between POSIX and Windows. However as I explained below, the majority of the speedup likely comes in how we close handles now.
Yes, but that's only if you always access at least one `i32`. If in many cases you access zero of them (filtering by some other information stored in `Foo`) then that `Foo`'s `i32`s won't be loaded into cache at all.
It's a `Vec&lt;u8&gt;`. It lowered to calloc fine; it's `__calloc` that calls `__memset_sse2_unaligned_erms_`.
That is quite interesting. I'll have to keep that in mind for if I ever need to write that many files at once. Does `rustc` use any kind of optimization like that? That could probably help out a decent amount in some places, such as for incremental compilation. At least the initial run of it.
Hi, karma seeker here :) - I've done most of the improvement work; I thought I'd drop a quick description of what we've done, where we are at, and possible remaining work. In the previous release we removed extra copies of the files (was already in a release). In this release we remove unneeded syscalls - we no longer set the mtime for the files, and we no longer set the file attributes (tar-rs was setting the attributes unconditionally, but really only needed to set them when readonly was needed). We could probably add mtime setting back in, as part of this arc of work involved adding file-handle based mtime setting for both windows and unix to the filetime package and tar-rs. We also added juidicious read buffering to avoid IO contention. That work brought us down to minimal syscalls for unpacking - create, write, close - but close was still slow - 6 or more ms *even with defender disabled* - and easily up to 50ms with defender enabled - and as we're unpacking 20K files that becomes a significant wall clock time. So the final tweak thus far was to defer CloseHandle to a thread, which allows us to avoid blocking on that close call. On a single core machine, this will obviously have no benefit; on a 32-core machine, rather more :). I hope we can get some way to have defender do its scanning without blocking userspace, but thats a longer discussion - for now we have no low-hanging fruit for improving the return-to-user experience. We can do some other things though: - we could set the do-not-index attribute on the files, but then they won't be indexed; we do have a js index, but its not hooked into the windows index service: someone would have to hook that up: not low hanging fruit. Doing this would reduce CPU and disk contention, as indexing has some CPU work (obviously) and 200MB of content written all at once does take a bit to index. - we could background rustup for docs extraction - docs don't prevent compilation after all, so CI tasks or getting back into dev could proceed immediately and docs could unpack over the next minute or two. This needs to not block the console though, allowing it to be closed, and still be debuggable - not low hanging. - we could ship docs as a mountable object like an .iso or .zip - and mount them just-in-time for rustup docs calls; also not that low hanging - code signing may reduce defender overheads, https://github.com/rust-lang/rustup.rs/issues/1568 - we could change incremental updates to do vastly less work - https://github.com/rust-lang/rustup.rs/issues/1798 - wouldn't solve CI cases, but would make things much faster elsewhere (even on unices I suspect). This is our tracking bug on unpack performance on windows - if its still slow for you please contribute there - https://github.com/rust-lang/rustup.rs/issues/1540 (and hey, 'fixed for me' is a gppd thing to say too).
Can you run it under `perf record` and see the hotspots in `perf report`? This should give us a first approximation of where you're spending time.
&gt; It actually is, unfortunately. Hence my obsession. It's already very fast, but I'm always trying to improve performance. Rather than blindly consider design options that might improve performance, you should look at what's actually happening with a profiler. Modern CPUs have dedicated performance counting hardware you can use. You sound like you already know where the hottest code is, so the next logical step is using a profiler. You can see if your code is encountering cache misses, stalls, mispredictions, etc. On linux it's `perf` but there's also VTune and lots of other options.
I suspect Display either isn't implemented for OsString, or if it is, there would be a lossy conversion. I write direct to stdout/stderr.
https://www.reddit.com/r/rust/comments/brtec1/rustup_1183_released/eogpfgr/ has a bunch of the details. I'll probably write up a blog post too.
tar-rs was using fs:: calls rather than File:: calls - so this on unix OSes is a single syscall; on Windows it is Open; syscall (or multiple), Close(). tar-rs (and filetime) have been enhanced not to do that; and we've also tweaked how we use it to just not do things we don't care about in this context (mtime specifically)
Hi sorry, you're right these graphs are a little ooverstated. As you can see from this pull request, I fucked up in a fairly embarrassing way: https://github.com/andrewpsuedonym/Dissertation-Project/issues/1 All I can say is that it was the end of a long day. I'll post an update with better graphs tomorrow. Hopefully, the scale is wrong but the ratio is fairly correct. Apologies again.
I messed up the graphs a little bit https://www.reddit.com/r/rust/comments/brre8o/comment/eogt99y
On Linux with an NVMe SSD, it has always been a snap for me. It's ludicrous that a program has to call the OS in a contorted way to reach acceptable performance due to some ad-hoc hooks for real-time virus scanners (which themselves practically don't exist on Linux).
Okay so I just tried that and it needs a failure crate that can't be found on [crates.io](https://crates.io), not sure why. But I think this is my chance to write a middleware. I am looking for how this middleware look. My first instinct is to find any trait that does this but I can't find it in the docs, I am using actix-web 1.0.0rc. Any ideas?
/u/TheMiamiWhale is correct https://www.reddit.com/r/rust/comments/brre8o/comment/eogt99y
When I worked at McAfee, I would stop the virus scanner before a build and enable it later. It was not just the source code, the different executables (and the DLLs) that start and stop during build were getting scanned on every open!
Well, try running older rustups it on a Linux homedir mounted on NFS v3 backed by ext3 for instance; you might find its not as snappy as all that, even on NVMe. Its absolutely great that Linux with everything local was super fast even though rustup was being very inefficient with its syscalls (even in Linux terms this was true - way more dentry traversals than needed, for instance). But that doesn't mean that what we were doing was optimal for Linux, and some file systems (such as NFS) do report errors in close(2) on Linux, and do so by forcing writes to be flushed across the network - at least, thats my understanding. The hooks that are used for Defender on Windows are no more adhoc than device-mapper is adhoc on Linux - its a well defined layer model for IO. It is frustrating that Defender does interfere choose to block CloseHandle() in the process rather than blocking subsequent reads until the file has been vetted - but I wouldn't call it ludicrous.
Wrong subreddit
&gt; I definitely plan on benchmarking if the consensus from this post was that it could be meaningful. I just didn't want to (at this stage in due dates) spend a couple days refactoring for comparison benchmarks. I know this feeling! Unfortunately, you generally always have to do the careful, time-consuming measurements once you've exhausted the the initial low-hanging fruit. I've been surprised too many times by what the measurements showed. Fwiw, using a `Vec&lt;[i32; 60]&gt;` vs. `Vec&lt;Vec&lt;i32&gt;&gt;` is, by my instinct, absolutely within the realm of things that could have a significant impact, and my first inclination would be to reach for the `[i32; 60]`.
You should maybe edit your top-level post and let folks coming in fresh know to ignore it for now?
Wrong subreddit, I think you mean r/playrust
First of all, if you haven't done that yet, read the [Rust API Guidelines](https://rust-lang-nursery.github.io/api-guidelines/). Some issues: - Doc comments aren't properly formatted. Try running `cargo fmt`. - HebrewDate's `Eq` implementation is empty. - Except for HebrewDate there is no documentation for structs and enums. - Where does the ToraReading struct come from? - Your error enum variants should be documented instead in HebrewDate. Then you can just enumerate which errors occur. - ymd_unchecked should be marked unsafe as this method could panic or at least that should be mentioned (safety/panic section). - The `?` operator should be used in examples instead of unwrap. There are also things that could make your code easier to use. Some of your trait implementations like PartialEq could be dervied. Your HebrewMonth enum should derive Ord as you cast it to an i32 for comparisons to get its discriminant but deriving it on enums will order it top down. For your DayIsZero error you could avoid it by using a NonZero integer type. This way the user cannot pass a 0. TryFrom and TryInto seems more ergonomic. The main documentation is a little bit redundant in the usage section. The dependency should not be set to a wildcard as cargo could pull any versions. Either show the newest version or remove it (on crates.io is a copy button anyway). Most newer users will probably use the 2018 edition, so `extern crate` isn't realky needed. And importing stuff is shown in the example. Additionally, the front page is somewhat of an overview, so as an heading not very helpful. Another thing I would change is defining every enum in the prelude. Re-exports make it easier to change the prelude by adding or removing a line. I do not know what your MonthSchedule is exactly but wouldn't it be better to define a struct with three fields? To me it seems like you would need to always add combinations but maybe your enum never changes.
Thank you so much for all the Windows work!!!
Fixed. Thanks!
Good shout, have done that, going to sleep now. Will update this thread and post a follow up tomorrow.
Thanks! &gt; Try running cargo fmt I did. Does cargo fmt format comments? &gt; ymd_unchecked should be marked unsafe Should panicking code be marked unsafe? I thought unsafe is just for potential memory unsafe code? But I should document that it can panic. &gt;HebrewDate's Eq implementation is empty. I thought that Eq is just a marker which says that it's fully equal (if A==b and B==c then A==c). &gt; NonZero integer type. Never saw that before. Thanks. &gt;To me it seems like you would need to always add combinations but maybe your enum never changes. It doesn't change. These are the only possibilities. Thanks anyways
Thanks both!
Implement it whenever you have an error type. You should also implement `Display` whenever you implement `Error` to get nice error messages.
I don't want to be mean, but... Microsoft has the ability to take the coolest language and make it sound like Cobol... No offense meant to Microsoft or Cobol, but this video is really cringey, as my teenage kids would say.
Yikes. For reference in this case I meant Defender specifically, though most/all Virus scanners I assume have the same problem. Still, kind of amusing to hear that a dev MAKING the software going through that.
Glutin's DeviceEvents can probably do that.
I'd just like to say that data oriented supports `await` as a pipeline operation. Data orientation is quite related to function orientation. The model is one of pushing data through transformations to get it in the format you need at the end. Viewed from this pure view, `async` transformation is just another data transformation to turn a delayed computation into its result, and you're likely to need other transformations before and after this one. I suspect whatever the lang team chooses will become perfectly understandable an unproblematic for the majority of `async` Rust developers by "edition 2021". I think the proper model for `await` is that of a blocking call in a greenish thread context.
Nice, thanks for spreading the word /u/olix0r!
Hey, that's science :)
Uninstall Norton. Then reinstall rust.
Thanks for your work on this. Us Windows users greatly appreciate it!
Actual Windows developer here (speaking for myself, etc). I am explicitly not going to tell you whether SOP is to disable Defender on local builds of [parts of] Windows. Because it would be very silly to tell you that. BTW no need to shut off the whole thing for rustup; use PowerShell to add a directory exclusion.
Question: what do we gain by implementing the `Error` trait? In my crate (first Rust project) I have a few error enums that implement `Display` only and this seems to be good enough for me. I'd like to know what I can gain by implementing `Error`. &amp;#x200B; Also, the `cause` method from `Error` is deprecated, and the `description` method is soft-deprecated (whatever that means). This leaves us with the `source` method only. Is this the one you're recommending to implement?
&gt;So the final tweak thus far was to defer CloseHandle to a thread, which allows us to avoid blocking on that close call. On a single core machine, this will obviously have no benefit; on a 32-core machine, rather more :). I'm curious, why would a single-core machine not benefit? Assuming the 6ms on close is IO, then your CloseHandle thread should just block waiting on that IO for that 6ms and not tie up the CPU, and your original CPU-bound thread should be runnable, right?
&gt; I thought that Eq is just a marker which says that it's fully equal (if A==b and B==c then A==c). [It is](https://doc.rust-lang.org/std/cmp/trait.Eq.html). You are using the trait correctly.
_Computer_ science even.
You can just implement `Error` as so if you don't have a source impl Error for Type {} If you do have a source then you can override the default implementation of source like so impl Error for Type { fn source(&amp;self) -&gt; Option&lt;&amp;(dyn Error + 'static)&gt;; }
you can always contribute to other Templating projects
I hope that whatever you do to address the Windows docs complaints is a general efficiency improvement; I use rustup on a HPC system so the whole thing downloads in less than a second but the docs take 6 minutes because the filesystem really does not appreciate the barrage of small files. If you opt for code signing as a solution for the Windows people I'll still be kinda hosed. Also just as a side note, crushing the disk on a shared system like this is extremely silly because it (temporarily) degrades the experience of my fellow users and I can guarantee you I'll never touch those files. There isn't even a program on an HPC system to render HTML.
Unfortunately, it seems you have malware on your computer. You can learn how to remove it [Here](https://support.norton.com/sp/en/us/home/current/solutions/v60392881_EndUserProfile_en_us).
Nice writeup of a fairly epic project! // Requires a Result return type or unwrap, even though it won't ever fail. // Generates a bunch of garbage error handling LLVM needs to optimize out. writeln!(s, "mov eax, {}", val)?; If you're confident a call won't ever fail, just `unwrap()`. That's what `unwrap()` is there for. It is acceptable for a compiler to panic if `writeln!()` fails: as you say, that should never happen.
You could just use tree sitter for vim, I think it's out of box
A great way to earn developer mindshare is to require them to switch their entire fu$%$% operating system just to get basic autocomplete...
Ugh, way too much work. I don't even know which folder it is. I'm just a user.
I’m a neckbeard with an ego. Should I use mutable or immutable references?
Suppose I have a code in C like this: int total = 0; for (int i = 0; i &lt; n; i++) { for (int j = 0; j &lt; n; j++) { total += do_work(i, j); } } How could I implement this in Rust using iterators? My idea was to use zip and cycle to generate a iterable list such as ((1,1), (1,2)... (2,1), (2,2)...) but it didn't work. Enumerate also didn't work. Should I try to generate a iterator at all or just fill a list eagerly?
step 3, don't reinstall Norton.
I know it doesn’t really matter, I just find it aesthetically displeasing. Rust compile times are already slow enough and I cringe whenever I shovel yet more bloated code onto LLVM for it to delete, presuming it can even figure out it can delete the panic path. Even though I know it’s a drop in the bucket.
Well, thats the question. Is it IO ? :). I haven't cracked out WPA to dig into where and why CloseHandle is blocking. If it is blocking for IOPS to complete, then yes, some degree of threadedness will help even on a single core machine. However, we \*know\* that the vast majority of the blocking time - the \~50ms (when single threaded) blocks - is in Defender, and thats not IO, thats CPU. So strictly speaking I should say 'multiple threads won't get more work out of Defender on a single core machine'. But you're right, some users will probably both not have Defender and have single cores, in which case having more than one thread would benefit them assuming that those 6ms-11ms blocks were indeed IO, not CPU. If you know someone in that boat and they want a custom build / patch to experiment with this, let me know.
FWIW we know the powershell to do that, but if folk were to do that in any widespread fashion, it would become a malware target... so we really want rustup to be fast enough folk don't feel the need to exclude it.
There is a separate initiative 'profiles' coming to allow disabling the installation of docs entirely, for HPC / CI / other non-interactive-use-cases. That will help you a lot. That said, the threading code path is currently windows only, out of a desire to not impact other users... and I suspect your HPC system is some unix flavour. If you wanted to get a strace -c, or some similar summary performance analysis of rustup running on it, I'd be delighted to see if it would make sense to enable the threaded code path more broadly. It may (especially for older NFS's...). The reduced syscalls aspect of the performance tuning will have benefited your HPC setup somewhat I expect, but exactly how much is an unknown question; it will depend where the time is going - see under getting a performance summary. e.g. strace -o stats.txt -c rustup install nightly File a bug on [https://github.com/rust-lang/rustup.rs](https://github.com/rust-lang/rustup.rs) with stats.txt.
Not for me. Seemed like well done presentation.
A while back some malware was written in rust, pre-1.0. The quality of anti-virus products, and latest heuristics for malware analysis decided that because an executable binary file contained a few symbols (mangled function names) it was malware. These symbols effectively mean anything which was the output of rustc, I believe (but am not certain) it was the panic handler. In short, anti-virus is pretty dumb, and norton will continue to flag most rust executables as malware.
Background: I'm a Python developer seeking to expand my skillset to a growing language. Decided to check out Rust and see this at the end of the "Getting Started" guide. Now, while I'm not opposed to having a neuter crab as the mascot of a programing language (the Python pythons never specified their sex), this phrase stands out to me like mold on a piece of bread. So, my question is: is this phrase (e.g. the need to specify that you don't refer to a damn crab with specific gender pronouns and instead use bizarre plural pronouns) indicative of the development community as a whole, or is this more so the case of a single author taking a bit more creative liberty than should be exercised when talking about a mascot of a programming language?
Hi all, I wrote a little article based on some experiences I had mentoring a newbie recently. I hope it's well-received, and feedback is very welcome :)
Which exact bit? The [Code of Conduct](https://www.rust-lang.org/policies/code-of-conduct) does indeed say &gt;We are committed to providing a friendly, safe and welcoming environment for all, regardless of level of experience, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, religion, nationality, or other similar characteristic. &amp;#x200B; So, no discrimination of people based off of gender identity. Keep in mind that flamewars are explicitly banned (rules 3,4), so as long as you are polite and respectful, you shouldn't end up with too much of an issue. &amp;#x200B; Not sure if that's *exactly* what you were asking about, but eh, I tried. ¯\\\_(ツ)\_/¯
I'm in no way associated with the Rust project, but my views are basically: 1. the Rust project is explicitly, and proudly, inclusive 2. representation matters It costs nothing to make Ferris ungendered, and it helps further those goals, however incrementally.
Working on `yams`, Yet Another Multiboot System, which generates a multi-boot disk with a GRUB menu from an arbitrary set of disk images. First tidbit is going live soon over at https://gitlab.com/agausmann/yams , with a lot more polish coming later!
Looking forward to optional docs. I just use the online versions and don't really get value from them (in addition to improving CIs).
As a Python developer who's been lurking here since before 1.0, I have to say that it stuck out to me too. I'm all for being inclusive but, when I saw that, it just felt like a cheap token effort that would give newcomers the wrong impression of what the community spends its time on.
This is great to hear! Thank you!
Let me reiterate that I don’t have a problem with a mascot with an unspecified sex. It’s just the odd phrasing that’s a bit off-puting for me.
Excellent write up, and an incredible project for a single semester course. Nice work!
Cool. That was my main concern. I like to build and I like to be amongst people who like to build. If you can build and/or help me build, I don’t care at all about your politics. Discuss those with your friends at the bar, and let’s build together.
The complaint is that Norton interfered and now rustup is caught in between messages that boil down to "Can't install rustc. Parts of it are already installed." and "Can't remove rustc. It's not installed yet."
Well I think you would just use for loops for i in 0..n { for j in 0..n { total += do_work(i, j); } } If you want to do it on another way use [iproduct](https://docs.rs/itertools/0.8.0/itertools/macro.iproduct.html) from the itertools crate. A nested for loop is creating a Cartesian product of numbers. Example: iproduct!(0..n, 0..n).map(|i, j| do_work(i, j)).sum()
Sure, but please recognize that "I don't care at all about your politics" is a privileged position to be in. If the goal is to encourage participation by members of historically-excluded (for whatever reason) groups, small concrete and conscientious steps like the one you've highlighted can help. It doesn't mean anything to you, but it can mean a _lot_ to someone else.
Aside: this is the umpteenth time that I've gone crosseyed looking at weirdly parsed new-reddit-markdown. Mods, if you haven't already, could you give feedback to reddit about how awful and disruptive this is? It seems like the fix is that they should either delete old reddit (OMG please no), fix old reddit to recognize new reddit markdown, or give new reddit editing buttons that generate old reddit markdown.
[htmlfn](https://github.com/partim/htmlfn) is the best one, no need of real HTML in rust
OMG , i been searching and searching such templating in rust and now i found this in DuckDuckGo results, so sad of Google and Bing. This is the Future of Web Programming, Markup without tags. thanks [Utkarsh Kukreti](https://github.com/utkarshkukreti) for being creative and breaking the status quo with this
This person gets it.
There are many LGBT people who contribute. These are talented language and compiler hackers whose goal is to build a better systems programming language. But Rust is a deliberately inclusive community. You don't have to share any politics to contribute, but you do need to be respectful of people's personal expression, including pronouns.
Whatever you are, in Rust you'll likely want to use both.
Really cool! But why is it single threaded? How hard would it be to multi-thread?
Good contents but I was waiting for how the first Rust example would look like without clone. Also kind of painful to read thin dark-grey letters on grey background
I don't think the rest of the workspace is available when you download an individual crate from it. As such, the file does not exist.
There is definite Leftism in the community. At least one member has, prior to her induction, tweeted remarks to the tune of kill all white men. And yes, as you noted, there are occasional reminders that a certain subset of the community is not above leaving uncomfortable reminders of their moralist worldview. That said, I think the community is more diverse than that. (In the honest sense of that word). On a certain forums, I've seen one user occasionally open discussion about his or her moral concerns. And the response has typically been overwhelmingly moderate. It is unfortunate that we live in times where communities can't do a better job divorcing their work from their religion. But I feel the Rust language is strong enough to survive the current cultural era.
Training is not trivial to parallelize and inference is embarrassingly parallel so the library doesn't need to be multi-threaded.
Format the HD, avoid windows, if not possible avoid anti viruses
Small nit: you've got the `strcpy` arguments reversed.
These are the two correct ways to implement the `Error` trait currently. `cause` and `description` proved lacking in usability and should not be used. The `source` method allows you to expose error information from lower functionality layers without making all possible cause error types part of the public signature of the error, similar to how exception chaining is done in Java and Python. This has to be done with dynamically dispatched trait objects, so that any underlying type will be suitable as long as it implements the `Error` trait and is not bound to an ephemeral lifetime.
Thanks for the hard work.
That'd be right, I just decided to switch from glutin to glium before this for my display :) That looks perfect though. Thanks. Solves a big problem for me.
Not really. There are a lot of people here who are not virtue signaling neo-progressives. However, code of conduct do not protect discrimination based on consciousness. So in theory, you can be harassed for voting for wrong candidate, like someone discovers it on twitter or something like that.
I want to check wether a process exists. I only have the name of the process and not even the path. I tried googling for getting a pid, I tried googling for taking "process snapshots", didnt find anything. Target platform is windows only. How can I achieve this?
I think the underlying atomic variable's consistency mechanisms can be more subtle than that, but in essence yes, they do impose non-trivial performance overhead. If you don't have a reason to use \`Arc\` it should be avoided, and even if you do it might be worthwhile to scrutinize the design for elimination of concurrent access.
`core::mem::MaybeUninit`is stable from 1.36.0 onwards, and `core::mem::uninitialized` is deprecated from 1.38.0 onwards. If you still need to support an older version of Rust than 1.36.0, how should you wrangle things? [select-rustc](https://crates.io/crates/select-rustc) could help you support both manually, but it depends on a procedural macro, so I think that’d only gain you support back to 1.30.0 and the 2018 edition. If you need older, I suppose it’d take a build script of some form. This sounds like something that could beneficially be abstracted into a crate. Use `core::mem::MaybeUninit` from 1.36.0, an in-crate copy of it from libcore for Rust 1.19.0 (the first version with `union`) until 1.36.0, and an API-compatible wrapper around `core::mem::uninitialized` for versions still older than that.
Imagine some generic code, like `Vec&lt;T&gt;`. LLVM only sees monomorphized versions of it, so if you use `Vec` with 10 different types in your code, LLVM will have to optimize them one by one. Any optimizations that `rustc` can do on the generic `Vec&lt;T&gt;` code will avoid some LLVM work for each of those 10 types.
Regular political correctness crap, Ferris sounds like a girl though
I haven't really dug into the component data store at this point, but fundamentally you have a file on disk (the one that conflicted) that rustup doesn't think existed. The errors about uninstall that come \*after\* the conflict error are usually rollback side effect errors IME and can be ignored. Delete the windbg.cmd file and try again; this may be repeated a few times :/. Uninstalling the entire toolchain and reinstalling (rustup toolchain uninstall nightly + rustup toolchain install nightly) is probably simpler and more robust; if that errors during uninstall, just delete the entire toolchain subdir for it...
The only person virtue signaling here is you
It's just saying that the Rust community refers to Ferris as "they". It's not saying that Ferris self-identifies as anything in particular, just that for all intents and purposes gender isn't an aspect of the mascot that's being specified. I wouldn't characterize this as a single author's creative liberty as you suggest in your comment below; my interpretation is that Ferris's gender is explicitly unknown so that all potential members of the community can identify equally with the mascot. &amp;#x200B; So yes, it is very indicative of the community.
Please google definition
&gt;regular political correctness crap &gt;sadly &gt;ferris sounds like a girl Clearly not virtue signaling... When presenting the mascot it's important to describe it, like the name and yes, gender. But a lot of times gender is specified through the name. When that's not possible because of a genderless name instead of people using the wrong pronouns you specify it. Now yes every mascot endsup having a gender, why not genderless? Specially cause a gender role is a very weird thing for a mascot, so why not not having one? It's a normal thing, complaining about it is clearly virtue signaling...
Caring about someone gender and/or gender is not necessary virtue for normal people &gt;why not genderless? No one gives a crap about your mascot being boy or girl, but it is annoying when you need to point it out on each occasion to show how progressive you're Mascot needs to be cute in first place, gender is secondary
I'd definitely just use the loops like /u/ironhaven suggested :) But here's an option with iterators: (0..n).flat_map(|i| (0..n).map(move |j| (i, j))) I'm not actually clear on why the `move` keyword is required on the inner closure. Even though the tuple constructor `(i, j)` moves both `i` and `j`, it seems like the closure still only captures `i` by reference (because it's `Copy`?), and we need `move` to convince it otherwise. So captures are more complicated than I thought. Can anyone explain the exact rules for what determines when something gets captured by value?
&gt; use bizarre plural pronouns https://en.wikipedia.org/wiki/Singular_they &gt; The singular they had emerged by the 14th century, about a century after plural they. It has been commonly employed in everyday English ever since then It's not actually that bizarre.
&gt; So in theory, you can be harassed for voting for wrong candidate, like someone discovers it on twitter or something like that. No, you cannot. If you are, please report it to the moderators.
If you're extremely certain something won't fail, then you can even do ``` .unwrap_or_else(|| unreachable!()) ``` Which should definitely be optimised out by the compiler.
Yo me, the biggest difference is for people who are new. When I first saw a ? I knew I was seeing something weird, and went to read about it. If I had see x.y.await, I would assume it was just a twice nested member access. The problem with. await is if you don't know what it is, you have very little chance of knowing what you are seeing.
&gt; Clearly not virtue signaling... It is if you view a proactively inclusive stance as a bad thing; virtue signalling is not specific to a particular ideology.
You're unsafely marking the Subject as Send + Sync, but it contains an Rc&lt;RefCell&lt;Option&lt;BaseObserver&gt;&gt;&gt;. What is to prevent me sharing \`Rc\`s accross my threads then ? How do you keep that safe ?
Nicely written and well motivated. A small comment on wording toward the end, with me nitpicking essentially &gt; This makes unwrap move the reference out of the `Option` that `as_ref` returns. That is, it consumes the reference. I feel this may confuse new users a bit. Since immutable reference is `Copy`, it doesn't make much sense to say the reference is moved out of the `Option`, or anything consumes the immutable reference. Furthermore, `Option` implements `Copy` when the wrapped type implements `Copy` as well, so `unwrap` doesn't really consume the `Option` type either in this specific case. This doesn't impact anyone's understanding too much ideally, but I reckon it's best to keep things consistent especially for materials targeted toward new users so they can develop the understanding without much confusion later on.
I remember my first rust program being a awkward mess of `clone` and `fn abc(self ,.....) -&gt; Self`. This would have been an excellent article to find.
Can you not do let _ = expr; That will consume the result but do nothing with it. I’ve not tried in this case, but have used it in similar situations myself.
If you’re more extremely sure, you can use `std::hint::unreachable_unchecked`
Yeah, that'll work too. But you probably are better off leaving the check in in case you were wrong about "never".
I think with structured logging, part of the point is to instantiate logger objects capturing context of the message, and this dovetails with keeping them local to a thread, asynchronous task, and/or a data structure at the message site. We weave `Logger` clones in our application objects and task data, augmenting each instance with information relevant to the context. This way, it's easy to use them with no need for globally shared instances. One caveat with using slog in async code is, invocation of log statements should not block. We have settled on slog-async for the `Logger` frontend, backed by a logging thread that pulls messages from the MPSC queue to forward them to the logging backend.
I'd argue that "Rust await" is much easier to google than "Rust ?". Also, syntax highlighting.
Unless the OMP code is using `#pragma simd`, in which case it is doing explicit vectorization.
Please, edit your post and give us an update !
It is.
Maybe a per-thread connection using the [thread_local](https://doc.rust-lang.org/std/macro.thread_local.html) macro? This way you could avoid possible sync issues.
Sort of off topic, but what's the fastest way to multiply arrays of u64s? I've been looking around and I couldn't find a simd function multiplying packed u64s, just i64s
Isn't it quite obvious that the only people actively bringing this up are people who are somehow offended by the concept of not assigning a particular binary gender to a fictional crustacean?
Totally forgot that.
I am sorry that we had a bad interaction. Maybe I can make up for it a bit with this response? ### `mem::uninitialized()` The blog post here explains again why `mem::uninitialized()` as we had it was a mistake. I appreciate it was very convenient from an API perspective, but it destroyed some fundamental properties of our type system. For types such as `bool` or `&amp;T`, nobody had suggested a scheme yet that would be able to "salvage" the old `mem::uninitialized()` without seriously impeding the compiler's ability to use types for its reasoning. If there are proposals for this, I'd be excited to hear them! We just recently had a brief discussion [in the tracking issue](https://github.com/rust-lang/rust/issues/53491#issuecomment-494371045) delving into some way of saying "a bad `bool` is only UB when you use it", which unfortunately falls apart when one tries to define "use" properly. Maybe you are only using this with types consisting of integers and raw pointers. Then there are solutions that would keep `mem::uninitialized()` well-defined. This is being [discussed by the unsafe UCG WG](https://github.com/rust-lang/unsafe-code-guidelines/issues/71). As usual there is a trade-off here between allowing more code and making it easier to reason about code. ### `&amp;uninit` You are making a proposal here not to save `mem::uninitialized` but to provide a more ergonomic alternative. It is a variant of `&amp;uninit` or `&amp;out` which has been suggested several times. This is connected to the ancient discussion around "placement new" which goes back to pre-1.0 days. The reason this hasn't happened yet is not that people don't want it. Many people want this! But it turns out to be hard. [Here's a blog post giving an overview](http://blakesmith.me/2018/12/31/what-is-placement-new-in-rust.html). I can only encourage everyone with an interest in this to join the effort here. Getting a type-safe "placement new" for Rust would be truly awesome! However, given that "placement new" has been in the works for 4 years and `mem::uninitialized` has repeatedly caused soundness issues and crashes, it does not seem prudent to block deprecating `mem::uninitialized` on finding a solution for "placement new". We felt we couldn't wait another 2 years, we had to do *something*. Nobody is happy that this regressed ergonomics for some cases, but this is the best we were able to do. ### `MaybeUninit` However, I also have to push back a bit on what you say about the relative ergonomics of `MaybeUninit` for "out" pointers (though not all of this might apply to your particular cases). You say that with `MaybeUninit`, one has to explicitly give a type. That is not correct: ```rust use std::mem::MaybeUninit; unsafe fn mk_vec(ptr: *mut Vec&lt;i32&gt;) { ptr.write(vec![1,2]); } fn main() { let mut v = MaybeUninit::uninit(); unsafe { mk_vec(v.as_mut_ptr()); } let v = unsafe { v.assume_init() }; } ``` `main` consists of three lines corresponding exactly to the three lines in your wishlist. Admittedly, the first line is more verbose, but the other two are not. Also, while this scheme takes one line more than `mem::uninitialized`, it is also more correct! With `mem::uninitialized()`, if `mk_vec` panics before initializing `ptr`, `v` would drop an uninitialized `Vec`. To do "out"-pointers for types that `Drop` already required the use of `ManuallyDrop`, `mem::uninitialized` was not enough. Compared to that, `MaybeUninit` actually wins in terms of number of keystrokes. So, from what I can see, we have an ergonomics regression "only" in the case of types that don't need dropping and where all fields are integers or raw pointers. For all the other cases, the prior approach was incorrect and has lead to miscompilations already. For the mentioned cases, the regression consists in one added line, the `let v = unsafe { v.assume_init() };` in my example above. For this, you gain the aid of the type-checker to make sure you are not mixing up initialized and uninitialized values.
How large do you expect the values to be? There are `_mm_mul_epu32` and `_mm_mullo_epi64`.
There are several reasons why you may need a Box: \- shallow copy: if you move an object into a new scope (call function by value), the contents of that object need to be copied into the new scope, with the box, the object remains in its own heap address and you only copy the address of it. therefore faster for big objects. \- dynamic types. If you have several structs implementing a trait, you could have some function accepting a box of the trait and have polymorphic code. note that you could use a generic function, but in that case you can not take the decision during runtime. \- recursive types: you could have a struct, with a field of type enum, and one of the enum values contains a payload of the original struct. You need to add a box in there for the type system to be sound. This is a common pattern when building a tree structure. Boxes have an overhead as well: \- creating a box is heavier than creating a value. when creating a box, your program needs to agree with other threads where to put this new memory, this requires synchronization and is not as fast as just creating a variable in the stack¡. \- they introduce indirection, unless your contents are using some of the features above, you may be (slightly) better off. \- they may add memory fragmentation. If you put everything into boxes, you may end up with the data of your program spreaded all over your memory, and the cache system may have a bad time. &amp;#x200B; hope this helps
I use nightly fmt, so it does that for me. &gt; I thought that Eq is just a marker which says that it's fully equal (if A==b and B==c then A==c). Yeah, you're right. &gt; Should panicking code be marked unsafe? I thought unsafe is just for potential memory unsafe code? But I should document that it can panic. I was also unsure but in your case it is overkill. Panicking is safe. Writing reviews at night isn't a good idea anyways...
That's why I used "in theory". Personally I do believe in more charitable interpretation. That being said, when forging legal documents, and for all intents an purposes this is a legal document, missing such piece out is actually a non-trivial thing, despite of good faith of people.
It's part of a hashing algorithm so the bits of the u64 should be randomly distributed 1s and 0s (hopefully). I don't want to have a branch in the code to deal with u64s that overflow the i64s - it's in the hot loop so branching would be catastrophic for performance.
I guess you could have a macro that unwraps in tests and does the above in normal builds if you wanted the verification, but there’s probably limited benefit unless this is in a very tight loop.
The guy asked whether "progressive" people are majority of community so I let him know that he is not alone in being normal I'm not sure why you all need to aggro so much
This is actually the case I am referring to. I haven’t used OMP SIMD extensively but in my experience the effectiveness is very problem dependent (as one would expect)
It is bizarre for people with non-English native languages because #notall have singular they, so for us some weird literature quirk of `they` is at least weird
Wait, aren't the lower 64 bits the same for signed and unsigned multiplication?
Are Rust's data types `size` and `u8` absolutely compatible to the C ABI (`size_t` and `uint8_t`) or should one use the data types of the libc-crate to interact with C? In my tests, using the Rust data types work fine to send data from a C app to Rust, but who knows if that's always the case.
What if you unpack the files on a thread pool? If Defender blocks the thread doing I/O, it might improve the throughput.
What do I do if I'm merely *absolutely* certain?
I also forgot to write that you might consider implementing the Error trait for your conversion error enum. I'm not sure if that is super important in your case though.
Though I'd recommend falling back to `unreachable!()` in debug builds, so you can fuzz-test your assumptions. Well, okay, maybe not fuzz-test writing to `stdout`, but in general...
The default is always capture by reference regardless of copyability or not. I think simplicity and predictability is the idea. Imagine in the opposite case if you wanted to mutate `j` in the closure but it was implicitly copied instead. You'd be beating your head against the wall trying to figure out why it's not being updated. Of course `j` isn't even mutable here but that's an extra case to consider, both for the code computing captures as well as the person reading the code. It's just easier to say "always capture by reference unless `move` is applied", and reason about it too.
This seems like a false positive, not actual malware…
I do think the Rust-community is more affected by this current trends than others. But that's pretty much 2019. Movies, series, companies (Gillette). Inclusivity etc. are the new buzz words, while no one seems to be able to explain what real meaning they have. Did the epic ranter Torvalds or literally *anyone else* ever reject anybody's patches based on their group identity? You don't see people or know about them anyways, communication usually takes place over mailing lists and similar. Even less so on reddit. People are just a username here. I think many in this thread do not see a problem “Why are they so upset about inclusivity? What is wrong about being inclusive?”. For me it is that I feel these words are coming from dangerous groups, which push to make group identity paramount over individuality – in the USA and several common wealth countries very terrifying things are happening (Lindsey Shepard for example, human right tribunals in Canada for wrong pronoun usage...). So I will not obey to the ideological possessed words of these groups and I'd encourage everyone not to underestimate the danger political correctness carries. Politics does not belong into technology. A group working on a project should not consist of people equally distributed by their group identity, but of the most competent people – and if 100% of them were red heads no one should come up with inclusivity quotas... The only rule (code of conduct...) we need is: Be decent, or: Don't be an ass.
I must to confess that I spend an embarrassing amount of time figuring out what was defender. This is what happens after more than 20 years living only on Linux. Defender is the software that is part of Microsoft Windows that detect and isolate potentially dangerous programs (virus, spyware, etc). Sorry the OT, I only wanted to provide a bit of a context for this excellent post.
I'm sorry, but you just can't use phrasing like this and then accuse others of "aggro"ing. You are saying that people who don't see any issue with a crab of unspecified gender are ""progressive"" as opposed to "normal". I work with people who don't identify as any particular gender, and they are perfectly normal to me. If you are offended by them, or by the choice to let a fictional crab fall in a similar category, that's your problem, not a problem for the Rust community.
&gt; When presenting the mascot it's important to describe it, like the name and yes, gender. Why? Why not say "this is Ferris, the Rust mascot" Done. No politics, no tension, nothing. Just like technology should be: Free of all ideology, political swarming, current 2019 stuff going on. Nothing. Just a funny crab and a name.
&gt; So, no discrimination of people based off of gender identity. Do communities which do not have such a CoC have such discrimination and if yes, how does it look like? CoC does not have any use at all. No one reads this and thinks "Ahh, ok, I must not insult people based on group identity. Didn't know that"
I agree with most of this, except that software can adequately perform tasks like speculation, branch prediction, prefetching, etc. Those are inherently dependent on the runtime state of the program. If compilers emitted code that actively contained different code paths for all the possible decisions a CPU can make, you would get massively huge and less portable binaries. You would need to recompile the world for every new microarchitecture just to get barely passable performance.
To a large degree that is effectively what we're doing: CloseHandle is blocking until the submitted but not completed writes complete, and that blocking takes place in another thread. The actual time spent on the CreateFile and WriteFile calls is now much less significant. You can see here we track both the core extract loop and the CloseHandle blocking which is where stalling on the IO completing is happening: ``` info: installing component 'rust-docs' 11.1 MiB / 11.1 MiB (100 %) 548.8 KiB/s in 15s ETA: 0s Closing 8936 deferred file handles 8.7 Kihandles / 8.7 Kihandles (100 %) 943 handles/s in 8s ETA: 0s ``` ... but its not 0: so that is another source of possible further gains; as is moving tar decompression to a thread to allow concurrency with that... the question is how much we'll gain vs the effort. - there's additional complexity to deal with at this point: we're unpacking compressed tars, which means we have a synchronisation problem: tars are a serial format and we cannot read the data for the next entry until we have read the file content of the prior one.. So yes, its probably worth doing the experiment at some point, but tar-rs needs some ownership changes to permit it; another contributor has a draft patch to permit that; an alternative would be to make it possible to iterate entirely in memory then dispatch those in memory segments to threads; either way, further tar-rs work is required. A developer survey is probably useful: if most devs have 4-core machines, optimisations that really only gain a lot on 32 or 64 core desktops aren't going to be too useful. OTOH if most are on 8 or 16 core machines thats a rather different story... Current syscall timings with Defender enabled: (captured with procmon which adds some overhead...) for one file from docs... ``` 9:10:14.6855925 PM 0.0001911 rustup.exe 8896 CreateFile C:\\Users\\robertc\\.rustup\\tmp\\wkvcoctik\_mifljv\_dir\\rust-docs\\share\\doc\\rust\\html\\core\\core\_arch\\mips\\msa\\fn.\_\_msa\_min\_a\_d.html SUCCESS Desired Access: Generic Write, Read Attributes, Disposition: Create, Options: Synchronous IO Non-Alert, Non-Directory File, Open Reparse Point, Attributes: n/a, ShareMode: Read, Write, Delete, AllocationSize: 0, OpenResult: Created 9:10:14.6858162 PM 0.0000647 rustup.exe 8896 WriteFile C:\\Users\\robertc\\.rustup\\tmp\\wkvcoctik\_mifljv\_dir\\rust-docs\\share\\doc\\rust\\html\\core\\core\_arch\\mips\\msa\\fn.\_\_msa\_min\_a\_d.html SUCCESS Offset: 0, Length: 433, Priority: Normal ... 9:10:16.8442020 PM 0.0366417 rustup.exe 8896 CloseFile C:\Users\robertc\.rustup\tmp\wkvcoctik_mifljv_dir\rust-docs\share\doc\rust\html\core\core_arch\mips\msa\fn.__msa_min_a_d.html SUCCESS ``` Observe the time offsets - the CloseHandle occurred 2 seconds later, because it was stuck in a queue waiting for a thread to execute on (on a 64-core machine...). 0.1911ms to create the file 0.0647ms to pass the data to the OS 36.6417ms to close the file handle, even after the OS had had 2 seconds to scan it asynchronously. 0.1911 + 0.0647 = 0.2558ms * 20K files = 5.116 seconds, our runtime for the extraction loop is up at 15seconds (for the trace I grabbed these figures from) - see above) - so *at most* if we got perfect parallelisation on a 4 core system that would take 15seconds down to 11 - but, traces show Defender using multiple cores already with what we're doing - we don't need to submit the IO from multiple cores; what seems to matter is that when we force the driver to close the handle and immediately complete the scan that happens in the same thread we made the call (well, our thread -&gt; ntos -&gt; callback into Defender - but thats the call stack). We have another bug open to switch to a faster decompression method, which should save a good number of seconds, and if we save a few seconds a few more times, doing the work I mentioned above vis-a-vis tar-rs may well become much more beneficial in terms of relative benefit. OTOH we may also pass the point at which no one is bothered anymore :).
Because languages have gendered pronouns, so they are chosen. Sure some people may call "it" but in the end a gender pronoun will be chosen. So it makes sense to specify it. &gt;Free of all ideology, political swarming And why isn't your complaint ideological and political swarming? You are complaining about the gender of a mascot because that doesn't suit your ideology. If ferris was female you wouln't be complaining. How isn't your position ideological and political swarming? I've never seen anything free of ideology tho. Have you?
Do you read the subject of topic? Do not speculate about my thoughts. All I did is to indicate that this phrase is not indicative of the _whole_ community, which tends to be overly _progressive_, but then people appear to downvote and start saying stuff about me like they know anything.
&gt;Because languages have gendered pronouns, so they are chosen eventually. So it makes sense to specify it if there is a preference. #notall &gt;And why isn't your complaint ideological and political swarming? Can people get tired of your pronounce bullshit? Since when being annoyed by this gender ideology become political position? My position is only that Rust should have nothing to do with ideologies that are not related to programming in any way &gt;If ferris was female you wouln't be complaining. Even if ferris would be female, the pronoun you're supposed to use is `it` because he/she is used for humans
A code of conduct also serves a secondary purpose, which is to signal to various marginalized groups that they are welcome, and that discrimination against them will not be tolerated by the majority. It should not be necessary, but if it turns out that it isn't, it costs nothing. From personal experience, I can tell you that it is far from given that open source communities do not contain some pretty off-putting behavior from certain people.
It seems to me people who are somehow offended by political correctness spend far more time talking about it than everyone else. Do you fear a human rights tribunal because the Rust community did not sign a binary gender to a fictional crab? Come on.
I know the sign-bit would be shifted to the top of the i128, but my expectations would be that all other bits would be flipped when negative, and it would be a branch to conditionally invert the bits
It's true, not all languages have a gender-neutral third-person singular. German and Spanish, for example. English, however, does. I'm sure you say lots of things in English that would be awkward to translate into your native language.
I am reaching to your particular choice of words, which is clearly indicating that you believe that most people share your sentiment that choosing a gender-neutral pronoun is an indication that the Rust community is "overly" progressive. If that is not what you think, please feel free to express a different sentiment.
I
The existence of gender-neutral pronouns and people who use them is political only because some people would apparent deny that they exist or that their experience of life is less valid. Personal pronouns is often used for animals in English, especially pets and mascots. Even moreso for fictional animals. Do you call Simba from The Lion King 'it'?
IIRC, `rustup toolchain remove nightly`
Thanks for the elaborate reply! &gt; To a large degree that is effectively what we're doing: CloseHandle is blocking until the submitted but not completed writes complete, and that blocking takes place in another thread. The actual time spent on the CreateFile and WriteFile calls is now much less significant. Yeah, the parallel `CloseHandle` calls would come for free if the files were extracted in parallel. &gt; there's additional complexity to deal with at this point: we're unpacking compressed tars, which means we have a synchronisation problem Oh, right. I forgot about that. I made a quick test using `zip`: `tar cz` produces a 35 MB archive from my documentation directory, while `zip -9` makes a 54 MB one. I don't think it's work switching to a ZIP archive. For the record, `tar cj` (`bzip2`) yields 21 MB, while `tar xJ` (`xz`) yields 13 MB. Of course, the decompression will be slower for these two. &gt; traces show Defender using multiple cores already with what we're doing That's good. &gt; That said, the threading code path is currently windows only I didn't realize that.
No, it's not a false positive, Norton is malware.
I usually implement from&lt;error&gt; and it works fine. For example If I have lib1error and lib2error I make my own error type like this pub enum Error { Lib1Error(String), Lib2Error(String), Other(String), } And then I impl From&lt;lib1error&gt; and From&lt;lib2error&gt; for my Error
Oops, whooshed!
We use rustup a lot, and the largest perf bottleneck we have is that when some target or toolchain fails to download, rustup errors and we have to call it again. This is so ridiculous that we have a script to always call rustup in a loop... It would be nice if rustup would retry downloads "enough", or if we could instead specify how often should rustup should re-try failed downloads. I mean, if a download starts, but hangs in the middle, the file exist on the server, but the network connection somehow failed.
Not sure about the community as a whole (which is quite diverse and international), but pretty characteristic of the people responsible for the web site and governance.
Parallel file-systems like GPFS just don't like many small files. They are optimized for super-large but few files. Ideally, rustup would just put everything into a single file there, and software that needs to access that stuff, would mmap the parts of that file that they need to memory, and access them from there.
Inclusivity is not a meaningless buzzword or a leftist political plot. It's a core value of a community that wants to invite as many people to participate as possible, in order to foster the most widespread activity. It's about assuring that everyone know they are welcome, that their opinions are worth consideration and that their contributions are appreciated. &amp;#x200B; It's true that we can all just be usernames online, but we all carry with us our sense of ourselves and our relationships to the other communities we've been a part of. Some people have lived lives that make it easy for them to believe that, even in anonymous online communities like this one, their participation is unwanted and unappreciated. We should take every chance anyone who feels this way knows they are wanted here. &amp;#x200B; You talk about technology free of politics as if that's not an ideology in itself. It's a pretty scary one if you ask me.
Worked like a charm. Thanks.
It's been fixed (he packages the deps as WASM)
https://crates.io/crates/lru-disk-cache
No problem! 👍
No problem 👍
Use `unwrap()` or `expect()`, it's exactly made for this use-case.
/r/playrust
Jfc I’m dumb
I don't know which compiler you are using, but if clang does not vectorize a `#pragma simd` loop, you get a compilation error. That is, if it compiles, you get a vectorized loop. This feels as effective as it gets to me.
I think my original comment was not clear. What I was trying to communicate is that in my experience, sometimes compiler auto-vectorization (without OMP) is as effective as OMP SIMD vectorization. And the larger point I was trying to make was that just because you throw a `#pragma omp simd` into your code doesn't mean it is automatically way faster than if you didn't use the OMP pragma.
Well, I fear it is one small step into a direction. I know this sounds ridiculous to you, I really do. It's just a fictional crab, right, so why the outrage? I believe that all this PC / inclusivity tribalism is part of a political drift that has seized our culture, propagating that hierarchies of competence, harshness in dispute etc. are part of a corrupt system, discriminating and oppressing women and minorities, judging by the disparity in distributions, claiming they are all due to oppression and tyranny. And this is my fundamental issue with this harmless mascot and CoC: If you introduce such things you implicitely claim that without these propagations people are suspect to tyranny and exclusion.
Not my fault that `they` became incorrectly popular as gender-neutral. That's why its usage is associated with this ideology. Personal pronouns may be used for animals, but are not necessary too. So going with `it` is as well valid
Cool stuff! When you were deciding on how to write the parser, were you considering parser combinators as well?
Choosing gender pronoun in first place is not necessary for mascot These who do it, are doing so because of being overly consciousness of this crap. That's all I mean
English had `he` as gender neutral too, but then come wild you know who.
Do you think these people (who apparently identify by their group identity, rather than as individuals) would not feel welcome if there was no sign "You are welcome!"? I personally never have witnessed drastic behavior in communities, but I agree that this exists. Btw. I do not disagree with the rules as they are phrased in this subreddit, basically: Be polite. I do disagree with what I see as ideological fight expressions. It's not that big of a step between propagating inclusivity and for example saying "The Rust community jugdes the oppressive patriarchy which has excluded women for thousands of years". May be sound ridiculous to you, but that's common at most universities by now. I'm not always agreeing with i.e. Linus Torvalds harsh behavior most of the time, but I do also believe that competence and conflict are essential parts of human competence hierarchies. I think it has to be possible to judge ideas harshly and to have conflict-like disputes within a community without everyone ringing the bully-alarm and trying to forbid any hurt feelings.
Well, I believe we to some grade agree with each other. What I want is "This is Ferris, our mascot". Would you not agree that this is completely free of any current or past political trend? &gt; I've never seen anything free of ideology tho. Have you? Depends on the definition probably. The tech-mailinglists I read are what I perceive as free of ideology: It's just about code and machines. But I think I get your point, at one point or another ideology comes into the game; with "Free Software" for example.
&gt;We should take every chance to assure anyone who feels this way that they are wanted here. This is the point where we probably disagree. Is it up to the community to preactively assure and comfort everyone, or is it up to the individual to grow up, realize that they are individuals rather than represants of their group? You see, if we reach a point where we try to avoid to hurt anyone's feelings at all and make everyone feel welcome actively, we likely won't be able to say much anymore at all. Being alive means conflict most at the time, and safe spacing won't do people much good, I believe. I think you can say a short "everyone is welcome", but that should be it. It's really easy to get carried with it to an extreme, like so many institutions in the west already have.
Why would that ever be my goal? As I stated, my goal has always been “build things with people who also want to build things”. If someone doesn’t want to build things, I’m not going to force my them to do that. People are more than quotas to me.
`unreachable!` is still a panic, so that won't affect optimization, it'll only change that panic message.
You can also use `rustup show` to see the default toolchain, and `rustup override list` to see what directory overrides, if any, exist.
There's a bug open for automatic retry of some HTTP errors; I've observed some flakiness myself but haven't built up the activation energy to track it down... I completely understand the frustration, because I see exactly the same thing, when it dies, it dies a slow painful timeout, not a clean failure at all. Re: Wifi and planes and so on - we have bug reports asking for a mirror network for users behind the great firewall, or on connections with bad latency to the rust-lang servers; first-world internet is great, but there are quite a few orders of magnitude between that and the long tail that would cover (say) 95% of our users. Of course, I don't have actual data... Do VScode and other IDE's use the generated docs for pop-up information? Or is that all from RLS? NRC is working on profiles which will allow not installing docs; once thats done I imagine it will be easy to step from 'choose not to install docs' to 'don't install docs until they they are asked for', which would be better.
On the other hand, in this case `s` is a `String`, which [can't fail](https://doc.rust-lang.org/stable/src/alloc/string.rs.html#2301-2304) `Vec` has a similar [can't fail implementation](https://doc.rust-lang.org/stable/src/std/io/impls.rs.html#297-322) I assume other in-memory structures with `Write` impls are the same
Ah gotcha, yes that makes sense. The `#pragma omp simd` is not required for auto-vectorization, the compiler can sometimes do it on its own automatically. The pragma is however a guarantee that you want vectorization to happen, and often in specific ways (the pragma has lots of options), and if the compiler cannot do what you want, it will error.
Yeah, you're right, you should use [`unreachable_unchecked`](https://doc.rust-lang.org/std/hint/fn.unreachable_unchecked.html) like /u/flying-sheep noted.
&gt; Do VScode and other IDE's use the generated docs for pop-up information? Or is that all from RLS? That's RLS, it parses doc comments, and it provides those to tools. Generated docs are only used when reading them in the browser (they are html..).
yeah, it should be Arc instead, not sure why I didn't change it yet, thank you
SQLite wouldn't be a bad choice here.
If in doubt repeat this step.
How would you fix the first rust example?
&gt; and the webserver is another one, Its starting to become a lot of threads that needs to be aware of each other. Looks like you want a message broker. There are many available, search for mpmc on crates.io
"Windows Defender" is probably more googleable.
Huh. That is even more interesting. And you're on `x86_64-unknown-linux-gnu` so that's not some kind of musl weirdness either. Are you on recent glibc and kernel versions?
German does. The singular is "es". There is only one plurar that covers all genders. ("sie") My Spanish is a bit rusty ( no pun intended) , so O can't comment on that.
I guess it depends on expectations. Even zfs with journalled everything can make that million file directory in 44 seconds for me on spinning rust; and ls of that directory takes 4 seconds. An absolute age in CPU cycles, but quite tolerable in human interactions given what you're dealing with. Shrug. I will admit to curiousity as to why rust-doc is 20k files in size ;)
It's a crab. How many percent of Rust programmers would even be able to distinguish a male from a female crab? On the one hand it's a bit weird that Ferris has their preferred pronoun listed on the Rust website. On the other hand, you're making a big deal about having to know whether a bright orange crustacean is a dude or not...
You're assuming that I have a correct and complete understanding of the concepts of thread-safety, which is not the case. But I'll try my best to answer. Generally speaking, Rc is faster than Arc because the latter uses atomic instructions to provide thread-safety and has to do more work than Rc, which simply holds a counter of references. &amp;#x200B; Marking a struct Send + Sync, as mentionned in the [Rustonomicon](https://doc.rust-lang.org/nomicon/) is something that Rust allows to stay flexible, and is definitely not something to be taken lightly. Since what you are doing violates the invariant that an Rc and its clones can only be present in one thread, you should remove those unsafe impl and make the structure impl Send + Sync by using Arcs even if it comes with a small performance cost. Your Rc&lt;Mutex&gt; is a big issue.
As an implementation detail yes. In the absolute, nothing prevents other languages from proving properties, then internally optimizing by making the "hypothesis" (actually using the knowledge) that they are not falsified. I'm only concerned by the model presented to the programmer, because (short of compiler bugs) that is what can be dangerous.
&gt;they became incorrectly popular as gender-neutral It's not incorrect, languages changes, go read shakespeare to see how incorrect your writing is. By the way ´they´ has been used as singular pronoun since the 1300s, it's just wasn't formalized. But now with the knew debate about gendered pronoun it became formalized. It's actually recognized The American Dialect Society’s for example, it was their Word of the Year in 2015. It's a fairly ignorant thing to believe language is static and some things are wrong, native speakers know how to speak the language, it makes more sense to question the formalization than the native speaker...
Says the person that said "ferris sounds like a girl". Are you being overly consciousness of this crap?
I don’t think cloning is inherently bad. If you don’t have multiple references to data, you don’t have aliasing, which means you don’t have shared mutable state bugs. Copying data is a huge part of the Erlang model, where you only send copies of data to other processes to avoid concurrency bugs. Aliasing can really mess up code that’s not concurrent as well, so copying by default makes code simpler. Structs in Swift are copied on assignment / when passed as function arguments as well. It eliminates a whole class of bugs.
No. "He" is not and was never a gender-neutral pronoun. It was used for a while by convention in a time when men were assumed to be more relevant or more important. That is and should not be the case any longer.
My German is very bad, but I hear from LGBTQ+ Germans that the pronoun "es" carries an implication of "objecthood" or dehumanization, and is therefore rarely preferred by genderqueer / agender / etc. Germans. I could be completely wrong. :-)
\&gt; and most people who compile code for a living don't run AV Well, most people who compile code that are on Linux or Mac OS. There are a lot of developers on Windows, and not running AV on Windows is not a very smart move.
Additionally, just running \`rustc --version\` or \`cargo --version\` will show the toolchain.
&gt; I could be completely wrong. :-) No, your information is fully correct and it's a good point. My correction was more on a technical/grammatical level.
&gt; Do you think these people (who apparently identify by their group identity, rather than as individuals) I don't know who you are talking about here. It seems to me you are ascribing your own interpretation of the pronoun to them. I don't think you can derive any such meaning from the usage of a gender-neutral pronoun. &gt; "The Rust community jugdes the oppressive patriarchy which has excluded women for thousands of years". May be sound ridiculous to you, but that's common at most universities by now. Can you please clarify what it is about this sentence that you find abhorrent? I personally care about good code, good engineering, and great technical discussion. Oppressing and excluding women and minorities actively prohibits those things, because many great engineers and software developers fall into those categories. So actively rejecting the patriarchy 100% serves the interests of the Rust community, as far as I can tell. &gt; I'm not always agreeing with i.e. Linus Torvalds harsh behavior most of the time I just want to address this specifically: Linux Torvalds has a history of pretty terrible communication, but to my knowledge he has never used someone's identity as a reason to reject their code. It is still counterproductive, which is why such behavior is frowned upon in general, and also why Torvalds himself has apparently changed his behavior. But there is absolutely zero reason why any community should tolerate attacks on someone based on their membership of some minority. None.
 Maybe use HttpRequest::state() [https://actix.rs/docs/application/](https://actix.rs/docs/application/)
My best advice would be from Robert Greene's 48 laws of power: Law 38. Think As You Like But Behave Like Others. Speaking out against CoC will get you mobbed and banned around here. Instead, simply refrain from adding CoC to your own projects, speak freely in independent communities and vote right at your local polling station.
But see, that's the issue. There mere presence of a gender-neutral pronoun has you talking about "tribalism" and "political correctness". To some people, those things aren't abstract political debates, but real life. The implication of what you are saying is that the very existence of people falling into those categories is a topic for political debate.
Install a Linux distribution
CoC has a great use, bigots out themselves and actually leave the community because of them. The usage is not outside censorship, it's self censorship, bigots get offended because they can't handle some words and leave the community. [Check this for an example](https://www.reddit.com/r/programming/comments/8gl161/llvmdev_i_am_leaving_llvm/dyd9c5w/)
The allocation in `push_str` may fail.
&gt; It seems to me people who are somehow offended by political correctness spend far more time talking about it than everyone else. If I change it to &gt; It seems to me people who are somehow offended by sexual harrasement spend far more time talking about it than everyone else. It would still be surprising you?
Rustup installs all toolchains under &lt;your username&gt;\.rustup\toolchains, so .. try deleting the toolchain in there (or move it out and delete later). I think that should fix the problem.
&gt;Beware of bugs in the above code; I have only proved it correct, not tried it. &gt; &gt;– Donald Knuth
Windows Defender is on by default in Windows 10 (which works rather well) so I'd be surprised many devs using Windows turn it off.
No it can't. Maybe in theory, deep in the internals, but that is not something that Rust exposes. [`push_str`](https://doc.rust-lang.org/stable/std/string/struct.String.html#method.push_str) returns no errors, and the linked `Write` unconditionally return `Ok(())`. There is no observable failure.
But the whole point of the Rust ownership model is that you can't have shared mutable data by default, because shared references are read-only. You can send objects between threads safely without copying, because the ownership is transferred as well.
[Hmm](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=a9cb1253dcd802e627b129461e4b0d81). Are you sure?
Welp, I only shared my opinion how Ferris name sounds, plus it is good to troll fanatics of gender ideology :)
There's an EMACs LISP macro for that.
Anyone, like me, from Maryland! Of course, only from the bottom and I've never seen ferris from that angle. The thing that makes me uncomfortable is that his coloring makes him appear *cooked*!
No one use Shakespeare language for reason, it is crap to use and too complicated. They did formalize it recently, but it is no programming language to change immediately, maybe in hundred years `they` will be used as singular by everyone, but right now I still prefer to treat `they` as plural
Until introduction of gender ideology, it was perfectly fine to use `he` as generic pronoun when actual pronoun is not clear. For people of past it was simplest solution comparing to using `he or she`. Coming up with new neutral pronoun would be at least better than just reusing wrong pronoun :D
It sounds like you're looking for something like _mm256_mul_epu32? But in general when you multiply things, the result is twice as wide as the inputs, so I think the SIMD multiply instructions only go up to u32?
&gt; It's just easier to say "always capture by reference unless `move` is applied", and reason about it too. That's what I thought, but it doesn't seem to be true if the type of the variable is non-`Copy`. Like if I write a closure that puts two captured `String` values into a tuple, the compiler moves them into the closure without the `move` keyword.
Did you compile in release mode, right?
Rust has a steep learning curve. As a rust novice I was confronted with the decision: do I halt all progress in my project in order to learn how the borrow checker works OR do I add a call to `clone` in and keep moving? I opted to add some calls to `clone` in and keep moving. I know it's not ideal, but understanding the borrow checker and and related aspects of the type system takes time. If we accept sub optimal usage as part of learning rust, I think it would make it easier for people to get started.
yep. One one thread performance is quite similar. Rayon struggles to scale.
Your program can still abort, that is what I would consider a failure.
thanks for following up...you describe exactly the problem I ran into. what I would need is some parser that is equivalent to a regex like this: `.*(\d{2})-(\d{2})-(\d{4})\s+(\d{2}):(\d{2}):(\d{2}).(\d+)`. The backtracking behavior of regular expressions saves the day here. You point out a viable solution but I doubt I'm the only one who needs something like that so it would be nice to have it in nom itself.
We weren’t allowed to use parsing libraries so I would have needed to write my own, and to some extent I did. A lot of my helper methods are like parser combinators. I’m not sure how pure combinator approaches typically handle look ahead though, if it’s pure backtracking it’s bound to be slower.
Does rust-doc needs to be in 20k separate files? This reminds me of a similar problem encountered in the gamedev (there it was loading thousands of small files). It was usually solved by organizing everything into archives. Is there any reason this can't be done for rust-doc?
It's an up-to-date Arch, so fairly recent. glibc 2.29 and kernel 5.1.3.
Yes, as another Maryland-er, the only acceptable color for a crab is blue... unless it's completely covered in Old Bay.
Yeah. I have no idea why it doesn't do that already. What if some future Rust update would require newer rustup to install? Or if there was some subtle bug that could corrupt Rust installation but was fixed in the newer version? Qt Maintenance tool, for example, always updates itself first before upgrading other components.
And this is exactly what I mean with "cargo should do the right thing here", which is obviously including that file in the published package in a way that it is found when installing from crates.io!
&gt;why rust-doc is 20k files in size Every *module*, *struct*, *enum*, *trait*, *function* an *macro* needs a documentation page. And as I recall, everything in the `alloc` and `core` module is re-exported in `std`, which further increases the number of documentation pages.
The process to pack a Rust application as a Docker container is already very simple. All that's really needed is a Makefile that runs `cargo build --release --target=x86_64-unknown-linux-musl` and a `FROM scratch` Dockerfile that copies that binary in. I think hiding that in a bash script just makes your build process harder to understand for someone looking at your repository for the first time.
I dislike the idea of carry a logger around, because even more with a functional style (that I prefer) I need to "pollute" all. Also, because I'm building stuff that cross all the stack, not everything fit well the idea of carry a context.
Im that case you can just implement `Display` and implement `Error` like ```rust impl std::error::Error { fn source(&amp;self) -&gt; Option&lt;&amp;(dyn Error + 'static)&gt; { match self { Error::Other(_) =&gt; None, Error::Lib1Error(x) =&gt; Some(x), Error::Lib2Error(x) =&gt; Some(x), } } } ```
\&gt; I don't work with time-zones I don't blame you! [https://en.wikipedia.org/wiki/Israel\_Summer\_Time](https://en.wikipedia.org/wiki/Israel_Summer_Time) Until 2005, the start and end of IDT each year was established in an ad hoc fashion as the result of haggling between political parties representing various sectors of Israeli society.
It definitely was _not_ fine. Male-as-the-default is something that has shaped Western thinking for centuries, to its detriment. By the way, "gender ideology" was arguably introduced by the Old Testament, or something to that effect. The idea that women, for example, are less suited for technical challenges is frankly a ridiculous ideology that has been entertained for far too long. &gt; Coming up with new neutral pronoun would be at least better than just reusing wrong pronoun :D I'm not sure why you continue to insist that singular "they" is somehow wrong in English, when it has been in use for 700 years. It is a perfectly good pronoun, there is no reason not to use it.
How do I install the `x86_64-unknown-linux-musl` target?
I'm not sure what point you are trying to make here. Is political correctness as a goal somehow similar to sexual harrassment in your mind? 🤔
You can add targets with rustup: rustup target add x86_64-unknown-linux-musl
Thanks I will try.
&gt; My question is, what is the correct way to write variant tuples (enums with structs) in rust_swig? `foreign_enum!` is for C++/Java enums, so only C-like enums can be mapped to them, because of there are no Rust like enums in Java or C++. So you need write some wrapper code for them, what code is depend on how you want design your Java API. For example you can use `foreigner_class!` and write methods like button_pressed(&amp;self) -&gt; Option&lt;ButtonPressed&gt;; where ButtonPressed one more Java class that have two getters button and code.
You've completely missing the point. &amp;#x200B; People that whine about the use of of more PC terminology claim that PC minded folks are attacking/oppressing them for their thoughts or the way they express themselves. Yet the only people we usually see complaining about words someone else is using are the anti-pc folks taking offense at PC terminology.
I figured I would have to do something like that, but as a last resort in case there wasn't something in the macros. Thanks for your help!
\&gt; If you introduce such things you implicitely claim that without these propagations people are suspect to tyranny and exclusion. &amp;#x200B; No. That's you looking for a reason to take offense go on the offensive.
You don't need that much to keep multiple CPUs implementations optimal, things like OoO execution and such can really help with the speed up. The problem is that, of course, devs/compilers never really code to make their code easy to reorder and optimize. If they could things would be different. CPUs then also added the ability to execute things far into the future for long lines. Speculative execution.
&gt; Lindsey Shepard for example, human right tribunals in Canada for wrong pronoun usage... Source, please.
Hm, that's definitely more correct, thank you. I was trying to connect some dots about how consuming `self` works conceptually, but I think being correct is probably better here. I think I'll update it to include your feedback, and maybe add a note about "it might help if you think of it this way..."
Sure! My writeup was intended for the user who is being told "clone isn't the best option here" (and may already know it anyway) but isn't sure _why_. :)
&gt; Yes to the first, That's great to read; thanks! &gt; no to the second. Why not? If the concern is integration with otherwise licensed software, wouldn't weak copyleft (e.g. [LGPLv3](https://www.gnu.org/licenses/lgpl-3.0.html) or [MPLv2](https://www.mozilla.org/en-US/MPL/2.0/)) provide that?
Agreed, this is quite disturbing.
It may not be your goal, and it needn't be if you don't want it to be. It's one stated goal of the Rust project, however. I'm not going to respond to "People are more than quotas to me" because I feel you're intentionally misunderstanding me.
Aye: it was specifically designed to *not* split the ecosystem, by allowing packages to be upgraded individually while still being interoperable, and mostly just changing syntax, not semantics. Very well-handled.
How is this related to Rust?
What command do I run to just update rustup itself?
If match gets a dot operator version then await should get a prefix keyword version. Then everyone will be happy.
Majority of complaints about async.await come down to "it is hard to see it". We have syntax highlighting, and let's be honest a huge majority of us use it. Accessibility is a problem for color-blind people, but modern editors have different ways of styling beside just changing colors. Funny thought, maybe CAPSLOCKED dot notation would result in a least backlash :D `my_async_thingie.AWAIT.do_something()` &amp;#x200B; On a serious note, there is no single solution that everybody will like, and from what I see a lot of people who are against specific syntax would rather have something "NOW" than prolong discussion for a year. Not only that, but if they do not waste time on defending syntax choice, they can use that time to make other improvements to language or ecosystem surrounding it.
yaa good you do that reactive stuff, but just make sure you do that for Web pages instead of what Scala did and then died out.
OP is a spam(mer/bot).
&amp;#x200B; i agree with above , you should upgrade to Linux. btw the down voting bafoons, i bet they all use a Linux phone ;-)
Today I fixed an [issue](https://github.com/wahn/rs_pbrt/issues/96) about `Shape "plymesh"`. Indices can be `ply::Property::ListInt` as well as `ply::Property::ListUInt`. While investigating what goes wrong with a test scene I found (and fixed) another bug regarding **quads** (and turning them into triangles). Nevertheless there is something else going wrong with that test scene, and I hope to [fix this](https://github.com/wahn/rs_pbrt/issues/97) tomorrow.
inelliJ killed Scala, stay away from those EVIL corporate thugs, use atom ide instead
I am a native English speaker, not from America though and it's bizarre for me as well. If someone asks "what is Ferris?" is the answer supposed to be "They is a crab" or "It is a crab"? The second is what I and everyone ever from country would say but I think I've read "It" isn't appropriate...or something...I duno. I don't really care either. It only ever comes up in these posts so it's easy to ignore.
&gt; That's a fine opinion, but it would need serious data to back it up, and lacking that I could only cite the usual counter-examples about the projects migrating to clang. &gt; &gt; Which I doubt is highly relevant anyway - concerning the rational you want to give, because it is both not that fast anymore for compile time, and not that bad (actually even quite good) as far as codegen goes. I think it is slightly more careful in regard to UB than gcc is, but I'm not sure. I did not say that Clang was slower than GCC (indeed it depends on the usecase), or that it compiled faster (it used to, not sure today); I said that the *reasoning* I've seen from people who had the choice between Clang and GCC was picking GCC because they wanted better performance, and they were ready to endure worse diagnostics to get it. &gt; Also, I never saw benchmarks of the safety of codegen. People for who this is important would maybe also use them if they existed. &gt; &gt; I'm not advocating for non-optimizing compilers; I'm advocating against dangerous optimizations, and the idea that it is not a big deal to amplify bugs because they are of the fault of the users to begin with. There are known benchmarks: UBSan and SafeStack amount for less than 1% of difference, the former prevents a bunch of "local" UB, the latter prevent ROP attacks. And there are people advocating for curbing the trend back; John Regher is quite famous indeed. &gt; Last, I've yet to see anything looking like a scientific approach confirming that exploitation of UB is required to obtain good perfs. On the contrary Rust seems to do extremely fine while having strongly reduced their number, and most interesting optims often happen at an higher level nowadays (but Rust still can yield good codegen, even from safe sections). The most important optimization enabler, *inlining*, does not require UB. A whole host of strength reduction techniques is also UB free. On the other hand, aliasing is crippling; any kind of array read/write is crippled in the presence of aliasing, for example. Without aliasing information, say goodbye to auto-vectorization, hoisting, unrolling, etc... when working with arrays. It's bad. Rust handles aliasing through its Borrow Checker, to safely prove that something is either non-alias (and writeable) or non-writeable... however unsafe Rust requires upholding those rules manually. The argument could be made that vectorization could be handled in library code, certainly `faster` has demonstrated it can provide good ergonomic. &gt; And yes, it is harder to do optims using more sound techniques. I'm not calling names; I'm recognizing this is hard. I stand my point that making the hypothesis that there is no bug for optimization purpose is an absolutely terrible idea if not approached very very carefully, and my opinion is the whole C and C++ ecosystem went waaay too far. I also stand my point that experts consider that it is impossible to expect programmer to be just careful and not write UB; that's complete wishful thinking, you might as well require that they never write any bug at all -- so yeah: good tooling is required and IMO static guarantees are to be strongly preferred in that domain (but dynamic is better than nothing). I agree. I was very disappointed when C++0x was gearing up for publication to realize that new ways to shoot yourself in the foot were introduced (lambdas captures are gorgeous for lifetime issues) but no attempt was made at cleaning up. It actually prompted to look deeper at Rust. I can understand that some things are *hard* to prove: aliasing information and double-free/use-after-free are typical in this domain. However, C and C++ also abound with lots of paper cuts; see UBSan for a bunch of silly UB. &gt; But there is hope; some recent C++ papers are appearing which would reduce the number of UB, and IIRC a working group more or less on that subject has been created. That's excellent news, I had not heard of that!
I really hope so. These two things should not be mixed up together. No one knows what my skin colour or sexual anything is and I'm not going to tell them. It's not important for what we do here.
In actix the best way to do this is to have an DbExecutor actor that will maintain a connection pool rather than opening and closing on each request. For example see [the diesel actix example](https://github.com/actix/examples/blob/250fc6eaf1054df15e3b6a168bb8731363fb6812/diesel/src/db.rs)
Just chiming in with the HPC perspective: ls (with colors, so lstat on each file, not just a few getdents) on a directory with 471 files takes 8.2 seconds. Things get out of hand very quickly here.
I would also like to see the original example done without cloning. Or (pardon my newbie idea): We want the `AThing` struct &amp; impl to be its own object. So we HAVE to clone it to create a new `AThing` right? Otherwise what is the point of the struct &amp; impl? Why not just write a fn that gets the length of `name`? BTW this made me go and re-check the code I have done in Rust.
Why stop at 3 when you can have 5? https://docs.microsoft.com/en-us/dotnet/api/microsoft.office.core.msotristate?view=office-pia
Well thank you very much, the docs do install a lot quicker! Looking at the output it did make me wonder why you count kibi-handles instead of just kilo-handles? :P
The battery could also run dead, but the significance of what /u/CrazyKilla15 calls a "can't fail implementation" is that there is no possible scenario in which an Err variant will be returned.
Offline docs not being installed by default would be great. Never in 3 years have I needed offline docs in my actual work on in any CI run I've ever done.
Can't fail *today* :-). Hard to imagine why an error return would be added. Idk.
I think if you symlink the common file into each workspace crate, `cargo package` will include the actual file contents. I've seen this done a lot for LICENSE files.
In case anyone isn't aware, here's someone else to thank for their work on rustup. :) https://github.com/rust-lang/rustup.rs/graphs/contributors
Simply `rustup self update`
Just use `my_thing.name`?
Without knowing how AThing will be used later in the program, one way to fix it is by giving a borrowed reference to AThing upon construction. The downside to this is that you have to introduce a lifetime parameter, which is subjectively ugly: /// Our thing struct AThing&lt;'a&gt; { /// Some data associated with our thing pub name: &amp;'a str, } impl &lt;'a&gt; AThing&lt;'a&gt; { /// Do some operation upon this thing pub fn thing_len(&amp;self) -&gt; usize { self.name.len() } } fn main() { let name = "don juan"; let my_thing = AThing { name }; println!("{} is {} chars long", name, my_thing.thing_len()); }
If `Foo` has several attributes that are accessed *independently*, you may want to check a SOA (Struct of Arrays) approach as well.
I always install both nightly and stable through *rustup update* and then switch to the one I want with *rustup default stable* or *rustup default nightly*
I've found the std Error type pretty lacking in functionality, so I usually use [failure](https://crates.io/crates/failure) for error handling, it's great!
So psyched to check this out! I recently shipped a production forecast model coded in rust that used xgboost via the rust bindings. xgboost is an extremely impressive library in several respects -- arguably the most successful algorithm in real world contexts and a fast, battle-hardened codebase. But there were several times when I went into the xgboost code with the idea of tweaking it for my own purposes and ended up deciding not to, for several reasons. One, not nearly as well-versed in C as I am in Rust. Two, concerns that I would accidentally violate an assumption I didn't understand. A couple random thoughts: 1) single-threaded: interested to hear any thoughts you have on the multi-threaded design of xgboost and how you might tackle this in rust. The built-in multi-threaded training in xgboost is awesome, but there are also plenty of times when you are training many, smaller models and you could parallelize training across models. I've actually benchmarked xgboost regarding whether using it's builtin multi-threading is faster than training multiple models simultaneously for that use case. My findings were that there is a huge performance increase when training many small models on small data on many threads (vs sequentially training the same small models on small data with each xgboost model using many threads). But once the data gets above a certain size, it doesn't matter at all whether it's parallel at the model level, or parallel at the training level. 2) how was your experience developing this? what surprised you? it's interesting to hear about challenges faced on more substantial rust projects. 3) Regarding github readme: "There is no unsafe rust code in the library". Hopefully this does not overshadow my overwhelmingly positive reaction to this, but I don't understand this attitude. The rust standard library is built on unsafe! I'm by no means advocating a cavalier approach, but I don't think it should be boasted about as a badge of purity, either. There are legitimate uses of unsafe, places where the rust ownership model prevents sound code. Anyway, just my two cents - I just have been noticing a lot of this lately.
hmmmh... that's a solution but not nice. I would need 3 symlinks (README, LICENSE and build.rs) for each crate. That would be about 100 symlinks more in the repository (I have ~50 crates but I already have the README symlinked).
Still waiting for `cond.if { a} else { b} ` so we can finally have our bootleg ternary operator.
I believe it's because you're saying that `small_str` has the same lifetime as `_cow`, which isn't true since `_cow` lives longer than the lifetime represented by the function `use_two_cows`. If you decouple their lifetimes, it works: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=cac1551960fc2b39aee642ce211ffaac
You only use a single lifetime parameter, so the lifetimes of the two cows are connected. &amp;#x200B; You may want to change your use\_two\_cows signature to: ```rust fn use_two_cows&lt;'a, 'b&gt;(cow: Cow&lt;'a, Cow&lt;'b, String&gt;&gt;) ```
I'm sorry that I can't offer answer to the question, but I'm very curious why would you have a nested `Cow`? A `Cow` swallowed a `Cow`? :D Couldn't you just match the first `Cow` https://doc.rust-lang.org/std/borrow/enum.Cow.html and in each arm do something slightly different and unify to one `Cow`, or something like that? `Cow&lt;Cow&lt;_&gt;&gt;` seems like `Option&lt;Option&lt;_&gt;&gt;` - noone should have a need for that.
This is amazing, great job.
*checks crates.io for a crate named "bull"*
That's what [debug_unreachable](https://lib.rs/crates/debug_unreachable) is for!
It's good to see Rust in production.
But why is it different when using my \`CowWithoutToOwned\`? (also in my real code I actually use the fact that all Cow-s have the same lifetime).
&gt; seems like `Option&lt;Option&lt;_&gt;&gt;` - noone should have a need for that, and `map` or `and_then` should have taken care of that. I needed that today! It lets you express the difference between failing to produce something a and successfully producing nothing.
[https://gitter.im/rust-lang/rust](https://gitter.im/rust-lang/rust) [https://t.me/RustTalk](https://t.me/RustTalk) [https://www.rustaceans.org/](https://www.rustaceans.org/)
i would love to see a list of stuff they've ACTUALLY been protected on, e.g. 'oh shit i would've run that but AV flagged it and it WAS an actual virus, my bad'
I modified your function `use_two_cows_and_return_small` as such: fn use_two_cows_and_return_small&lt;'a, 'b&gt;( small: &amp;'a str, _cow: Cow&lt;'b, Cow&lt;'b, String&gt;&gt;, ) -&gt; &amp;'a str { small } It appears to run now. [Playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=fd4ab5f44782691cfcafb3d29610bdc9)
The actual [code](https://github.com/omerbenamram/evtx/blob/master/src/model/deserialized.rs) has a bunch of nested structs, and some of them have \`Cow\`s in them. The result is that you [can actually](https://github.com/omerbenamram/evtx/blob/master/src/binxml/assemble.rs#L175) have all combinations of Owned/Borrowed! &amp;#x200B; The minimal code for reproducing the error is using just the \`Cow\`s (but it's also a bit funny tbh 🐮🐮)
at that point you're almost better off making a more specific enum to convey that
I don't know your use case, but if I saw that in a code review I'd ask for `Result&lt;Option&lt;T&gt;, E&gt;` instead. Even if E is `()` it'd be much easier for a reader to understand what the signature means.
Thanks for the answer! However this fails to achieve what I want, as you can see in [this example](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=a0d9a4609a9edc0ccb1a0c2c4d069e37): fn use_two_cows_and_return_small&lt;'use_two_cows, 'return_small&gt;( small: &amp;'use_two_cows String, _cow: Cow&lt;'return_small, Cow&lt;'return_small, String&gt;&gt;, ) -&gt; Cow&lt;'return_small, String&gt; { if true { Cow::Borrowed(small) } else { _cow.into_owned() } } While my \` CowWithoutToOwned \` works with not problems
What about `Result&lt;Option&lt;_&gt;, ()&gt;`?
You can also stick a `rust-toolchain` file next to your `Cargo.toml` in which you write `stable` or `nightly`. Cargo will honour that with any command you launch.
Then you should probably not try to use a structured logger, which requires this passing around and building of context.
Check out multi stages builds to avoid the temp files : https://docs.docker.com/develop/develop-images/multistage-build/ Basically you have two containers with two `FROM` in one Dockerfile. In the first one you do all the building stuff. In the second one you copy the final binary from the other container. So no cleanup of any kind to do.
You are building with `--release`, right?
I'm not sure what you're trying to accomplish, but I wouldn't do that conditional there. You should return the string value then call `Borrowed`. In general, I find that sending references down more than one or two function calls leads to frustrating battles with lifetimes. The solution I went with is to return the value then deal with it. [Playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=5cf95fd7b885884e769fab5197dc2468)
yes When building without one cycle takes like 1 sec xD I am also running the C/C++ code with the release options (default settings by the STM32CubeIDE)
Then you should probably post the assembly you're getting, because it's hard to investigate without that. Also, have you tried clang instead of gcc?
I wrote this very simple test for prime numbers: Just read a (big) list of prime numbers from a file and see if the testnumber is in that list. fn is_prime(n: &amp;u32) -&gt; bool { for p in read_list_of_primes().iter() { if p == n { return true } else if p &gt; n { return false } } panic!("Couldn't handle number {}, it is too big.", n) } fn read_list_of_primes() -&gt; Vec&lt;u32&gt; { let file = File::open("primes.list").expect("Could not read file."); let buf = BufReader::new(file); buf.lines().map(|l| l.expect("Could not read line.").parse().expect("Could not parse number")).collect() } Now this reads the file every time I test for a prime. Is there a way to cache the result of `read_list_of_primes`? &amp;#x200B; I already tried the cached crate, but I wasn't successfull. Also, since the function doesn't take any arguments the approach of that crate seems to be a bit over the top. How can I make my code read the file only once?
Try enabling [LTO](https://doc.rust-lang.org/cargo/reference/manifest.html#the-profile-sections), and perhaps setting `opt-level = "z"` (optimize for size), although the latter can also regress speed
Never mind, solved it myself: I added an Unknown enum, and removed the Option&lt;&gt;, then replaced enums that weren't defined with that Unknown case.
Or if you have a HashMap of Option&lt;T&gt;, it allows functions like get() to express finding a key pointing to None versus a failure to find a key. Same with Iterator&lt;Item=Option&lt;T&gt;&gt;::next.
Thanks!
&gt;I would love to have and alternative implementation built on and existing stack, mostly for differential fuzzing, but also as a form of checked documentation. I have been following the development of Chalk, Polonius, and the SAT solvers in rust but have not yet succeeded at making it happen. I have just submitted a [PR](https://github.com/rust-lang/cargo/pull/6980) that uses a SAT solver for differential fuzzing. That is only a small part of what you asked for but progress is being made. Feedback is as always welcome.
!remindme 1 day
I will be messaging you on [**2019-05-24 20:40:52 UTC**](http://www.wolframalpha.com/input/?i=2019-05-24 20:40:52 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/rust/comments/bs7i94/call_for_contributors_kafkalike_service/eojwwb7/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/rust/comments/bs7i94/call_for_contributors_kafkalike_service/eojwwb7/]%0A%0ARemindMe! 1 day) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! eojwzfe) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
Can you check the timing when you don't do any work other than the delay? Maybe the delay timer is not calibrated correctly...
&gt; Fn* closure traits implemented for Box&lt;dyn Fn*&gt; Great, I just wrapped my head around this in TRPL...that said, it actually _is_ great!
Oh wow, just yesterday I was trying to do exactly that `map` example. Excellent service, I must say.
I know it's just a small helper, but it's still so cool to see the function that I wrote and stabilized finally make it out (Range::contains)!
I rejiggered it. I think it does what you want now? https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=e377256b1b583df72e395427547b6935 Problem was you didn't take ownership of the inner cow (also, pretty sure you had the lifetimes mixed up).
It could be worth checking that your processor architecture is correct. Not all ARM CPUs have built in division instructions, and both division and modulo operations like you are using will be a lot slower if not done using the built in instructions.
The project structure is really odd, where you have `src/benchmark.rs` have you considered using `criterion` for benchmarking? Also normally you would put the `voik` code into the `src/` top level directory. Also `segment.rs` should probably be `segment/mod.rs`. Would you mind if I put in a PR to put it in a more 'standard' format?
Isn't this also the first release to use the new Hashbrown/Swisstable based HashMap implementation?
Nope, Hashbrown will be included in 1.36.0.
And I'm glad it is! It was quite annoying having to manually bounds check if values were within ranges.
I think it's in the next one. The next two releases will be big ones. Hashbrown in 1.36 and async in 1.37.
Anyone else a bit miffed by the functions being called `copysign` instead of `copy_sign`?
Congrats! :)
Ah, my mistake. Time to revert these commits replacing Hashbrown with std HashMap :P
@zoonage, thanks for the comment. Feel free to open PRs for such things... &amp;#x200B; As for criterion, I haven't checked it out yet, my benchmark code is quite manual, just a draft. &amp;#x200B; The \`commit\_log\` is actually a sub-create, that's why is not in the \`src/\*\`, that was one of the recommendations of the Rust book/documentation to be honest. If I try to have a sub-crate under the \`src/\` folder, I would get some issues when referencing and/or compiling. &amp;#x200B; I saw a couple projects using the \`foo/bar.rs\` rather than \`foo/bar/mod.rs\`, it feels a bit more "natural". Either way, I would rather the rust standard one for sure. &amp;#x200B; Thanks! !
Have you tested this locally to check whether this an issue with the ARM instructions being used or whether its higher level?
IIRC it was discussed and they chose to omit the space for consistency with other programming languages
I have wanted that forever. Thanks!
What clock source do you use? Please check that it is the same in both cases. It's very easy to enable xtal and PLL in stm32cube, but in Rust you have to do it manually.
That's how it begins ... While there certainly are a lot of languages using `copysign`, it's hardly unanimous. Checking the top languages from TIOBE: - Java: `copySign` - C/C++: `copysign` - Python: `copysign` - VB.NET/C#: `CopySign` - Perl: `copysign` - GoLang: `Copysign` -
This will be great for some stuff I do with grids. A lot of checking everything is inside the grid. This should really clean up the code! Also, the source looks pretty fast too.
The fact that this is the first time I feel deeply conflicted with something stabilized into Rust goes to show just how much attention has been paid to all the things that were added.
Well now our `match` suddenly needs to handle an `Err(())` case. If you insist on `Result`, wait for the `!` feature and use `Result&lt;Option&lt;T&gt;, !&gt;`.
This seems exactly what I was looking for, thanks everyone!
You're introducing a new case: `Err(())`.
Please use code blocks for posting code. Don't use inline code.
`Option&lt;Option&lt;T&gt;&gt;` \-&gt; `Result&lt;T, ()&gt;`: * `None` \-&gt; `Err(())` * `Some(None)` \-&gt; `Ok(None)` * `Some(T)` \-&gt; `Ok(T)` Am I missing something?
My biggest annoyance is that the unittest library in Python does camelCase, where everywhere else is snake_case. If this is the only place where it's different, I guess I can live with it, but being consistent with other languages is a poor excuse IMO.
But the `Err(())` case is the point – to represent three distinct cases: * failure to produce anything (`Err(())`), * success without a value (`Ok(None)`), * success with a value (`Ok(Some(T))`), instead of having `Option&lt;Option&lt;T&gt;&gt;` and respectively `None`, `Some(None)`, and `Some(Some(T))`.
Uhh, you're right. Sorry.
Yeah, they did the same with `logging` and everything else that was brought over from older Python versions. If you're going to break backwards compatibility, at least make sure the naming is consistent.
I've wanted that *many* times. Thank you!
Using Rc means your struct can't be Sync. Accessing it from multiple threads will blow up in your face, so yes, you should use Arc here. Normally you almost never need to implement Send or Sync manually.
Sorry, stupid question: why isn't `contains` an element for some popular iterator's trait that `Range` and friends satisfy?
For it to be on an iterator you would have to run through the entirety of the iteration to prove that the range doesn't contain something. This implementation can just check the start and end instead. This is a provided method on the [RangeBounds trait](https://doc.rust-lang.org/std/ops/trait.RangeBounds.html) however.
Hashbrown AND futures.
You could add a static variable (or thread local) with something like refcell option vec, which you initialize to be None. At the entry of the read method you check if it is Some, and if it's not, read it and save it to so the next time it will be used again. (Sorry for formatting, I'm on mobile)
Have you looked into cargo workspaces? It might be a good option if the crate starts growing more. The `foo/bar.rs` file heirarchy for modules is the 2018 Edition way, and I personally like it more as well.
A concrete example for /u/Darksonn would be getting a database record by primary key: fn get_widget(id: i32) -&gt; Result&lt;Option&lt;Widget&gt;, DatabaseError&gt; As the caller, you probably want to know if the record simply doesn't exist versus a problem querying the database.
I'm curious, what kind of instruction set are you looking for that would be supposedly easier for CPUs to transform for OoO execution? What language features, beyond what current languages like C, C++, Rust, etc. offer, would you foresee? I guess I'm asking more specifically: what would be a better memory model in your mind?
Have you checked the compiled output to make sure it's hitting the FPU?
&gt;I'd like to know what I can gain by implementing Error. Your error will be useable in `dyn Error` trait objects, which can be handy for users who need to aggregate and handle multiple error types.
That's actually a valid case - when two different APIs are involved, etc.
Why not? structured logging is to spit out son(or similar) so is easy to parse and digest. &amp;#x200B; A \*separate\* concern is how call the library. Most logging libraries allow to just call log!() or similar. &amp;#x200B; Now the impasse is that slog can do that: [https://github.com/slog-rs/misc/blob/master/examples/global\_file\_logger.rs](https://github.com/slog-rs/misc/blob/master/examples/global_file_logger.rs) &amp;#x200B; but removing the ability to pass structured data! This is odd.
After following Rust since about 0.5 and coming to this sub since there was &lt;1000 subscribers I finally made it onto the contributors list
What are you using for leader election? Raft, or something else?
Ok, it's working now. I tried the uninstall and reinstall method with no luck but the Norton remove and replacement tool worked and an overnight scan found a virus and killed it. Thank's for the help. P.S I like that rust up now tells you the nightly update won't work and where to check for that last one that might.
You were absolutely correct. The only additional thing I had to do after it reinstalled Norton was to allow rustup to access the internet after it blocked it.
I suppose this is just not implemented yet. There are various "typemapping" for Option&lt;something&gt; here: https://github.com/Dushistov/rust_swig/blob/master/macroslib/src/java_jni/jni-include.rs but there is no Option&lt;foreign_enum!&gt; among them.
But `Result&lt;Option&lt;T&gt;, !&gt;` is just `Option&lt;T&gt;` since the `Err` constructor could never be instantiated.
I considered writing a backport PR for a rename to `copy_sign` but in the end I overslept the deadline and I felt bad about it for a minute and then I was like "meh".
Some comments about the name: - https://github.com/rust-lang/rust/pull/55169#issuecomment-430967367 - https://github.com/rust-lang/rust/issues/58046#issuecomment-459876564 - https://github.com/rust-lang/rust/issues/58046#issuecomment-477828500
Thanks, I was searching mainly for instances of copy_sign and I guess I didnt look at the tracking issue.
This is all integer math...
I'm sorry, but I was making a joke. The joke being that Norton itself is more malware than anti-malware, and that you should uninstall Norton in favor or something like Windows Defender or maybe ESET (what I personally use). I'm apologize if you wasted time on that, but I hope everything goes well now.
I find it a bit odd that consistency with other programming languages is so important for something as simple as `copysign`, but not for something like `await`. I'd think it would be the other way around.
If I had to do a low level assembly that worked with modern considerations. Commands would not be a single list of commands but instead a series of pipelines, a list of commands that must be run in strict sequence. A CPU would try to interleave the commands of multiple pipelines into a single serial execution. Pipelines could "fall asleep" (a bit flip) until an event triggers them. Pipelines may also depend on the output of other pipelines which means the CPU will ensure ordering of pipelines. Compilers want to split code into as many pipelines as possible to ensure the CPU had enough to optimize and always be doing something. External events would have two forms. The old interrupt system, but also a wake of a pipe that knows that an event just happened. I would do away with the idea of direct RAM access. Assembly would instead load from addresses at a specific cache page into the CPU registers. There would be a command that would either load pages from RAM into cache or write from cache to RAM. When it happens it would activate a specific pipeline. With the knowledge of what page it did if any. Notice these are not language features, and they're not meant for you're every day programmer. They're meant for compiler and OS people that already have to think of when things go in and out of cache, of his things get moved around, paralellized etc. The thing is that because the compiler can explicitly explain how things can be run in order or out of it, the CPU doesn't need to be too smart to make sure how things are run. Instead it's up to the compiler to understand semantics. The nice thing is that you can choose when to use the feature aggressively and when to avoid it fully, because the compiler can have greater context than the CPU. Notice that this is not a specification of how a CPU is supposed to work, but instead how code for a CPU is written. Behind the scenes it would all compile into something similar to what we have now: sequentially pipelines microcode that with NOOPs injected whenever we can't use the CPU and just have to wait. This doesn't change how Rust, C or C++ work (though they may want to expose semantics that allow programmers to direct the compiler explicitly in this events, like pragmas) as the language compilers into the same thing. The main advantage is that this removes silicon used to translate between assembler and real microcode and it also is easier for CPUs with widely different compromises and implementation details to run the same code optimally (which means compilers don't need to handle many differing CPU concerns). Is this a good idea? No, the amount of infrastructure, compiler development, tool building and programmers changing mindset means that this probably would take years, if not a couple decades to become competitive. The benefits are questionable. And the fact that it's so wildly different means there may be surprised down the line that will have to be worked around (if they're possible at all). But I'd love to explore this at least Ave see what we'd learn.
The irony of Python code not using snake case is strong.
The Rust Book says: &gt; One special lifetime we need to discuss is 'static, which means that this reference can live for the entire duration of the program. All string literals have the 'static lifetime... Don't string literals adhere to the same scoping rules as everything else? Since they'll be within some function or block somewhere, won't the leave scope / existence at the end of the block?
Glad to see this land. It always felt like a glaring hole when stuffing a mutable closure in a box couldn't be called without jumping through some hoops.
&gt; no need for zookeeper Sounds like no distributed consensus is desired. I'm a little fuzzy on what Kafka is supposed to do so maybe it still makes sense. Or maybe this will end up implementing some brand new distributed consensus algorithm that's not Raft/Paxos/Zab.
Slow and steady, ya know?
Lol, wow, yeah it is. Let's just pretend I'm not an idiot, just for funsies.
So I think I remember hearing that Python's use of camelCase was, ironically, to maintain consistency with JUnit. It's ~turtles~consistency all the way down.
... no, because all string literals have the `'static` lifetime. String literals don't live on the stack; where would they come from?
I guess that's my fault, although I'm pretty comfortable with the outcome. I do think consistency with other languages is important here.
Yeah, it puts Rust ahead of API's like Java's \`Map\` API where a \`null\` from its \`get\` function is ambiguous.
thanks, this make sense
I personally dislike the idea of consumers keeping track of their own offsets for this. Balancing the consumers becomes harder and HA becomes more difficult too.
I know the bytes of the string literal are in the program code itself, but does that really mean they can be accessed outside of their scope?
what do you mean for web page ? perhaps you mean wasm ? still think about it, but not yet, I'm on making this stuff work first and used it in my side project.
Well, in `owned_double_cow`, the lifetime `’a` in `use_two_cows_and_return_small&lt;’a&gt;` is not actually the `’a` in `use_two_cows&lt;’a&gt;`. I think there is an implicit coercion when you call `use_two_cows_and_return_small&lt;’a&gt;`. The coercion is not possible for built-in `Cow` because a trait (`ToOwned`) is involved. pub enum Cow&lt;'a, B&gt; where B: 'a + ToOwned + 'a + ?Sized, { Borrowed(&amp;'a B), Owned(&lt;B as ToOwned&gt;::Owned), } So, if you add a dummy trait to `CowWithoutToOwned` like [this](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=216133d0e216d6335dd113566cdf8fe5), you’ll get the same error. pub trait Trait{ type T; } impl&lt;X&gt; Trait for X { type T = X; } pub enum CowWithoutToOwned&lt;'a, B: Trait&gt; { /// Borrowed data. Borrowed(&amp;'a B), /// Owned data. Owned(B::T), } For the same reason, in `double_cow`, this will work (still in [here](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=216133d0e216d6335dd113566cdf8fe5)) fn use_two_cows_and_return_small&lt;'a, 'b&gt;( small: &amp;'a str, _cow: Cow&lt;'a, Cow&lt;'b, String&gt;&gt;, ) -&gt; &amp;'a str
Can someone help me understand when I would use `ptr::hash`? Here's my reasoning: &amp;#x200B; I thought it was really important (and `HashMap`/`HashSet` make certain of this) that some key, (`key: K`) and a reference to a key (`key: &amp;K`) have the same hash value. &amp;#x200B; Wouldn't this feature make it so that if I have one `ptr` to the value `5`, somewhere in memory, and another `ptr` to the same value `5`, that these would hash differently? I feel like this breaks a useful invariant.
I’d love to contribute. One suggestion is that you create a few issues for things that you’re planning to implement next. Having issues with a good description helps new contributors to understand the direction in which the project is going.
Why not? What issues would that cause?
For readability, please format your code using four-space indentation so that reddit can mark it up appropriately.
Yeah, so the only 64bit mul is _mm256_mullo_epi64 and _mm512_epi64 which both sign extend into 128 bit results (great) but then return the lower 64 bits of each mul (dang). Now, this may work (though the 512 requires simd512e), but I'm dubious about whether this will return the right result at the bit level. I have another thread above, I'll try and find out if it actually returns the right bits or (if I naively just assumed) the bits are flipped if the u64 -&gt; i64 conversion overflows.
Really new to Rust. Looking for something similar to Clojure's partition. Basically I have a string (or a vector of chars) to split into lengths of 3. I could do it with a regular loop but hoping there is something already there to convert to an iterator so I can pass it further into a map to calc statistics on it.
What is `a`? `&amp;str`? `a.as_bytes().chunks(3)`. But I don't get what that has to do with partition...
A is a String. &amp;#x200B; In Clojure if I wanted to partition a string into groups of 3 characters, I could do: user=&gt; (partition 3 "Hello, there!") ((\\H \\e \\l) (\\l \\o \\,) (\\space \\t \\h) (\\e \\r \\e)) &amp;#x200B; Basically looking to do the same thing in Rust with a string. Got a solution, pretty sure it's not the right way.
It probably seems like it doesn't matter, but *why* do you need to split into fixed char-length strings? The reason why utf8 is used is that there should be no legitimate reason to index string data quickly by character. Consider whether you really needed a `[char]` to begin with, or whether you're actually dealing with ASCII, or you actually want to count graphemes not `char`s.
I find it funny that my original hn comment that triggered your hn response, that led to this blogpost (that I was eagerly looking forward to) expressed the same position as the GP. It's like a digital fractal of the two positions, forever going in an ouroboros of reasonable advice that prima fasciae seems contradictory but is actually reasonable and nuanced. ^_^
Not all structs are small enough that `Copy` is cheap (hence them only implementing `Clone`) and not all structs cheap enough to freely clone without dealing with borrows are marked `Copy` (like `Rc`, where both borrowing and `.clone()` are cheap, just semantically different).
Counting codons from genomic sequence, so I want to ultimately go from a string such as: "AAATTTAAAGGGAAA" to { "AAA": 3, "TTT": 1, "GGG": 1} and am used to using map/reduce type systems (probably will end up using rayon) since the data file is &gt; 5Gb. &amp;#x200B; So I'm splitting the string into char vector so that I can split it into 3's back to a string to perform the counting. But... new to rust, probably a better way than what I'm doing.
This is a classic case of "username checks out."
You don't want strs and chars then, you want u8s. Conveniently, that's exactly what my original solution did :) `char` and `str` are for Unicode data. When you get a byte other than your nucleotides (or whatever), you don't really care if it's the start of a 1-, 2-, or whatever-byte utf8 sequence; it's wrong anyways. But answering that question for every byte in your string is literally the only thing that using `char` is for.
I'm confused... If you asking a string literal to a variable inside of a block, you're telling me I can access that variable from outside the block without returning it?
Awesome, cheers. To read as u8 looks like I'll have to rework the struct/impl and change read\_line to read\_until as that mutates a u8 buffer, should be straightforward enough. (Have to read in the header and process that a bit, before getting the actual sequence I need to operate on). &amp;#x200B; Really appreciate the help.
No, not the variable. The string literal, as a byte slice (reference).
But if I'm using a reference then the variable still needs to be in scope, right?
I was really looking forward to this post! One of the things I've spent (an inordinate amount of) time on is making the compiler give suggestions that make sense to newcomers and experienced developers alike for most cases, slightly biased towards problems newcomers will face with &gt;80% chance. Because of this I consider it an outright bug when a *common* problem is displayed without either clear wording explaining possible solutions or outright suggesting the correct code. This leads me to my (seemingly diametrically opposed to yours) advice of "`.clone()` to your hearts content if it will make the problem go away", as it is predicated on `rustc` actually being helpful for the cases when that is actively bad advice. Looking at the first example, I would say that is a reasonable place for a newcomer to rely on `.clone()`. For the case of fn make_lipographic(banned: char, line: String) -&gt; String { line.as_str().chars().filter(|&amp;c| c != banned).collect() } I consider this a linting problem, where we have `line: String` and its only use is `line.as_str()`. I don't know how to fix the general case, but this is something that `rustc` could/should complain about for at least `String`/`&amp;str`. This leads me to extend my "rule" to "use `.clone()` if it helps you (and it's not a loop). Reach for `&amp;str` over `String` when possible, but don't be scared to use `String` if things get tough." If a newcomer had used `line: String` first, [they would have gotten](https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018&amp;gist=85d74ad0b18456c5f53b76703a0bb308): error[E0308]: mismatched types --&gt; src/main.rs:7:38 | 7 | assert_eq!(make_lipographic('e', passage.clone()), passage); | ^^^^^^^^^^^^^^^ | | | expected &amp;str, found struct `std::string::String` | help: consider borrowing here: `&amp;passage.clone()` | = note: expected type `&amp;str` found type `std::string::String` Note that neither the compiler nor clippy suggest removing the useless `clone()`, but probably should. For the following problem ("`Hashmap`), the proper solution to a human in this case would be to use `&amp;str`, but for the general case it would be possibly incorrect if the function were to return the `Hashmap` (invalidating the keys). That being said, [if we were to rewrite almost all the `String`s but `quote`, the error is mystifying](https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018&amp;gist=fd8f6dbd5776b7362ec6bc36e3bc7eaf) and doesn't suggest the [use of `.as_str()`](https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018&amp;gist=d3fe1b5474a5760241cb4cc1fda09c20). For the `Vec` problem, I think it would be valid (and relatively easy to add) to emit: error[E0382]: borrow of moved value: `bad_letters` --&gt; src/main.rs:6:5 | 2 | let mut bad_letters = vec!['e', 't', 'o', 'i']; | --------------- move occurs because `bad_letters` has type `std::vec::Vec&lt;char&gt;`, which does not implement the `Copy` trait 3 | for l in bad_letters { | ----------- | | | value moved here | help: consider borrowing here: `&amp;bad_letters` ... 6 | bad_letters.push('s'); | ^^^^^^^^^^^ value borrowed here after move | = note: probably some explanation about `Iterator::into_iter` For the `Option` and `Result` cases, `rustc` already has [some hacky](https://github.com/rust-lang/rust/pull/51100) [support to suggest](https://github.com/rust-lang/rust/pull/57158) `.as_ref()` under limited circumstances: error[E0308]: mismatched types --&gt; src/main.rs:6:25 | 6 | opt.map(|arg| takes_ref(arg)); | --- ^^^ expected &amp;Foo, found struct `Foo` | | | help: consider using `as_ref` instead: `as_ref().map` | = note: expected type `&amp;Foo` found type `Foo` I would wish to extend that support to the presented case of `E0507`, if possible.
You *could* just turn the `str` into `[u8]` as soon as you get it, or even every time you use it since the conversion is literally a no-op, but if you're working with big data sets best to avoid the wasted up-front cost of Unicode parsing for parts of the read data which are going to panic on non-ascii anyways.
Links on the [official standalone installers page](https://forge.rust-lang.org/other-installation-methods.html#standalone) are up-to-date! **Thank you** to whoever is maintaining them. Instead of going over all standalone installer types like I usually do, I'll just focus on *additional target installers* since I don't believe they have been documented sufficiently on the website yet. If you have installed Rust through [a standalone installer](https://forge.rust-lang.org/other-installation-methods.html#standalone) and would like to add additional compilation targets to your installation (e.g. `x86_64-unknown-linux-musl` or `wasm32-unknown-unknown`), you can download standalone target installers. These installers work in the same way your normal standalone installers would: Just download, run the install script, and your set. Since a page where all target installers are listed hasn't been made yet, you can download them (and their pgp signatures) manually by following a url with the following pattern: * `https://static.rust-lang.org/dist/rust-std-1.35.0-{TARGET-TRIPPLE}.{EXT}` * `https://static.rust-lang.org/dist/rust-std-1.35.0-{TARGET-TRIPPLE}.{EXT}.asc` As a concrete example, you can install musl (staticly linked linux binaries) by downloading: https://static.rust-lang.org/dist/rust-std-1.35.0-x86_64-unknown-linux-musl.tar.gz Note that the extension for all target installers is 'tar.gz' or '.tar.xz'. Also note that a list of all platforms supported by rust can be found at https://forge.rust-lang.org/platform-support.html. Finally, browsing all 1.35.0 installers (both host and target variants) can be done by visiting https://static.rust-lang.org/dist/2019-05-24/. If you have any questions about stand-alone installers or additional compilation targets, please don't hesitate to ask here. Cheers!
A reference to the variable, yes (`&amp;&amp;str`). A reference to the string literal, no (`&amp;str`). You may be confused about the difference between variables, values, and items. Really, any data works this way, not just DSTs like `str` and slices: fn test() -&gt; &amp;'static usize { &amp;5 } And in fact when we move away from DSTs we can rewrite that to see what's really going on: fn test() -&gt; &amp;'static usize { static VALUE: usize = 5; &amp;VALUE } You can't write that directly for `str` and `u8` (`static VALUE: str = "foo"` doesn't compile) because they're unsized. But the compiler can do it for the implicit version (`&amp;[1,2,3]`). In the example, `VALUE` is an *item*, not a *variable*. Totally different things in rust. (`const` also denotes an item with slightly different semantics. No such thing in rust as "const variable", which many people get wrong — and finally, a language where that oxymoron is not present.) Items don't have lifetimes the way variables do, nor do they "go out of scope" the way variables do (although the names are still scoped exactly the way variable names are; a confusing overloaded term).
Actix\_web and cassandra (cpp driver) itself have thread, so there will be no conflict?
Ok, just give it a try.
Maybe this is more in line with my application, but also need try it.
Potentially useful if you know that the pointers are already uniquified (e.g. they point to storage for interned strings). This doesn't break HashMap/HashSet -- they should continue to work as they work today. What you can do is create a newtype wrapper around &amp;T or pointers which uses this as a hashing strategy instead.
Yeah. I like your approach of reading it as u8's and then converting the header line to a legit string when needed (small data, infrequent, and why not), with the bulk of the data remaining u8. I think that'll pay off, especially once I'm past the small test set. &amp;#x200B; I was able to figure it out too, hit a borrowed/ownership issue but was able to solve it (I'm celebrating the small victories). &amp;#x200B; Cheers, and thanks for the help.
it simply means i was chasing this hype in Scala for years with lot patience , but it never arrived for the web page development. And f its n Scala now i dont care because i have moved on to a better platform than bytecode.
Yeah hahah I mean, both are true and good! Hell, even if a newbie reads my article and goes "wow, I don't understand a _bit_ of that except that cloning copies things wholesale, I'mma keep cloning" but is then enticed to think more deeply about memory in the future, that's a win :)
Which crates? Crates have unique names, there can't be various crates called `touch`. If you have some complaint about it, maybe you should open an issue or send the maintainer a PR?
IntelliJ is ok, But VSCode is killing it. Have you guys seen the remote feature? Makes working in docker containers a breeze. I would personally suggest everyone use vscode over IntelliJ. I have used both over the course of several months and can personally say vscode does many things better than IntelliJ. Just feels snappier too.
Really good write-up. One of my personal areas of interests. I’d love to hear the follow-ups you mentioned at the end of the blog, too :)
Whoops! Thanks. I shoulda checked. :)
I'd recommend you measure the runtime with the in-built DWT register, which will give you a precise cycle count. I think the STM32F3 has one, but I'm not 100% sure.
Check the type of the map. I expect it to have `&amp;str` type of keys and/or values. If that is the case then changing that to String will help. But note that vector also owns its contents, so the implicit ownership move from vector to map does not happen.
I've updated the text to incorporate this corrected view of the semantics. Thanks!
&gt; This feature makes it so that if I have an address pointing to a value, and another address pointing to the same value, that these would hash differently? You can have two pointers pointing to the same location in memory?
I've updated the article to answer this question... before I noticed that @jrop [beat me to it](https://www.reddit.com/r/rust/comments/bry7ya/rebuffing_the_attack_of_the_clones_a_newbies/eojcjqf/) :)
Great work as usual! Keep it up team. Sorting autocomplete based on type information will be nice.
I think you're looking for [`Vec::remove`](https://doc.rust-lang.org/std/vec/struct.Vec.html#method.remove). ```rust fn test_funct() -&gt; HashMap&lt;String, i32&gt;{ let mut keys: Vec&lt;String&gt; = [1,2,3,4].iter().map(|x| x.to_string()).collect(); let values = [1,2,3,4]; let mut hashmap = HashMap::new(); for i in 0..keys.len() { hashmap.insert(keys.remove(i), values[i]); } return hashmap; } ``` Note that the only reason you don't get the exact same error when you insert `values[i]` is because `values` is a list of (probably) `u32`, and `u32` is `Copy`.
Some of us are still trying to wrap our minds around it
yep. cloning vec values `hashmap.insert(keys[i].clone(),values[i]);` works. Or the better way u/thenewwazoo suggested [playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=fc4a5a7de4ee79e2809e75b37c845ff1) Or the better way as [playground2](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=9241c3f13f4648e05a3dcdb232aaa5d4)
Username checks out.
into_iter() will remove the need to clone. And you can zip the two collections together and use from_iter or collect to make the hashmap
 for i in 0..keys.len() { hashmap.insert(keys[i],values[i]); } is a bit of a code smell in Rust. A more idiomatic construction that would also fix your issue is: for (k, &amp;v) in keys.into_iter().zip(&amp;values) { hashmap.insert(k, v); }
 keys.into_iter().zip(values.iter().cloned()).collect() It's easy to move out of a `Vec`, just use `into_iter`. It's much harder to move out of an array though.
I'd like some prefix await for consistency with the other languages, pretty please.
I knew that I could do it myself, but this is such an isolated and common problem that I wanted to go the rust way and _not_ do it myself. lazy_static worked instantly, thank you. My code is crazy fast now :) I will have a look at once_cell also. I should work on knowing more crates.
Sure you can ([Playground](https://play.rust-lang.org/?version=beta&amp;mode=debug&amp;edition=2018&amp;gist=a85953a2920fb0680ebfbccc0f741141](https://play.rust-lang.org/?version=beta&amp;mode=debug&amp;edition=2018&amp;gist=a85953a2920fb0680ebfbccc0f741141)) &amp;#x200B; `ref1` and `ref2` refer to the same location where the constant 5 lives, and `ptr::hash` hashes them the same.
The example about the language runtime makes a lot of sense. Thanks. Sorry, I didn't mean to give the impression that I thought this would break HashMap/HashSet. I was just pointing out that this particular hash breaks the invariant that they seek to guarantee.
Have you checked the RAM memory usage of VS code ? Even if it has a lot of features, it will still be a resources hogger for me. :)) The difference between Intellij and VS code is that Intellij is snappier, it's not just a feeling.
Now that we can call boxed closures, shouldn't FnBox be removed in this release? As far as I can tell it was never stabilised and is just sitting in the compiler behind a feature gate.
I think this is the first time clippy changes made it into the release notes. Cool! There is still room for improvement, so for anyone who wants to chime in, there's your chance to write release notes history. In related news, great job everyone. Another release with many improvements, and looking at the current beta and nightly, the next releases will also be something to watch out for.
Fantastic work !
Sorry, one final nitpicking. I'd change &gt; The key here is that references are `Copy` ... to &gt; The key here is that immutable references are `Copy` ... since mutable references are not `Copy`. Other than that, it looks perfect.
&gt; Support postfix await syntax Yes! Quick question though, how can I syntax-highlight `await`? It doesn't seem to use the "Keywords" highlights, and I haven't seen an option in the other "Color Scheme" settings either. https://i.imgur.com/anwvugU.png
I understand that, yet I would really appreaciate not having to register consumers, less toil. Also, consumers are more flexible on operation to point to the horizon of the stream whenever they want to, not depending on state maintained by the engine. The only state I want the engine to have is the stream content, to be honest.
The main idea is Raft, since Paxos seems like an overkill, but I have to study a bit more before making any conclusions. Open to suggestions or tips, also, I’m not looking into implementing the whole consensus from the scratch, I’m on the hope there are crates for Paxos already there, although, I would have to evaluate them still.
I haven’t checked, but I will. Thanks for the comment!
Or maybe create a new post. It’s hard to understand which comment here refers to which version of this post.
It depends on the plugins. VsCode without plugins, or for some of the mature supported languages (e.g. Typescript) can be very snappy. However for Rust with Rust Analyzer that's currently not yet the case, and IntelliJ often feels more responsive. However that's known to the Rust Analyzer team, and the situation might improve in the future.
It all depends what you mean by *the same*. What’s important is that the behavior of a `Hash` impl is consistent with that of `PartialEq`. In particular, that `x == y =&gt; hash(x) == hash(y)`. But it can be valid to decide that values of a given type are “the same” only if they have the same address. In that case you’d implement `PartialEq` for that type based on `std::ptr::eq`, and `Hash` based on `std::ptr::hash`.
Not only \`.await\` shows no error anymore. It even resolves the types correctly, from \`impl Future&lt;T&gt;\` to \`T\`. Awesome!
I'd also like to see the first example written with `Box` in addition to the plain ref you have now.
In Rust, `Peekable` is defined as such. pub struct Peekable&lt;I: Iterator&gt; { iter: I, /// Remember a peeked value, even if it was None. peeked: Option&lt;Option&lt;I::Item&gt;&gt;, } This is because there are two possible `None` states, either `peek` function wasn't called or the `peek` function returned `None`.
It could be, but iterator traits would only ever guarantee O(n) at best, might consume the iterator, and would mutate it (except for DoubleEnded which could move back and forth I guess, I don't think it guarantees to yield the same items on every back and forth but that would be a fair implication). That seems like large side-effects for something people would generally expect to be side-effects free.
&gt; My biggest annoyance is that the unittest library in Python does camelCase, where everywhere else is snake_case. Likely because of the Java / xUnit origin. `threading` was / is the same, though they've since added snake_case aliases to the old camelCase APIs.
It will be removed soon, but for now it's staying to let nightly users remove its usage.
/r/rustjerk ?
If you want to use ptr::hash with HashMap and HashSet you still have to implement a wrapper type with the right hash implementation, and for that wrapper type the invariant still holds.
got any sample code at hand how to set the clock to 72Mhz?
hmm, didn't know that such exists, thanks
Wow this is great! Do you have any idea why associated types cause invariance? (using the trait itself doesn't seems to be a problem) pub enum CowWithoutToOwned&lt;'a, B: Trait&gt; { /// Borrowed data. Borrowed(&amp;'a B), /// Owned data. Owned(B), // Blows up! // OwnedButInvariant(B::T), } In this example it's seems to me that the compiler can prove that **T** is just the same as **B**, so there shouldn't be any problem. Is there an example where that's not true? (so the compiler can't know - if the compiler tell for a given type that covariance don't hold I would still expect it to allow for this case) &amp;#x200B; (About 'a not being the same in both: I know, but I thought it was simpler to just use a single 'throwaway' name)
Seems like \`rustc\` guide has an answer! [https://rust-lang.github.io/rustc-guide/variance.html#addendum-variance-on-traits](https://rust-lang.github.io/rustc-guide/variance.html#addendum-variance-on-traits) [https://rust-lang.github.io/rustc-guide/variance.html#variance-and-associated-types](https://rust-lang.github.io/rustc-guide/variance.html#variance-and-associated-types)
I'm not sure if it is to maintain consistency per se. There was not PEP8 at this time, and they might wrote what they felt natural.
I wonder if it’ll have an auto-insert-await mode.
But what's the relation of Rust vs C++? Is C++ ratio a ratio over single-threaded C++, or single-threaded Rust? It could be that A takes 1s in single-threaded mode, and B 10s, while in multi-threaded A scales to 0.25s, and Bto 1s. B scales better, but A is faster in all cases. /u/andrwpsuedonym Any updates? The suspense is killing us. :D
## Implement datastruct with Rust!
Looks like the board support crate &lt;https://docs.rs/stm32f30x-hal/0.2.0/stm32f30x_hal/rcc/struct.CFGR.html&gt; has support for it in via the `CFGR::sysclk()` method.
Thanks for the reminder! I’ve submitted https://github.com/rust-lang/rust/pull/61113 to emit a deprecation warning. Since it’s unstable we can technically remove it any time. But it doesn’t hurt to leave remaining users some more time, while nudging them with the warning.
Sure. But why go to the trouble of modelling an error when the only variant would be `NotPresent`? These kinds of types arise a lot in generic code or when combining third party structures. For example, if you retrieve a value from a `HashMap&lt;i32, Option&lt;String&gt;&gt;`, why awkwardly convert the outer option to a `Result`, when you wouldn't do that for a simpler value type?
I don't really see a problem with larger projects like the ones you've mentioned. For me, one amazing thing about rust is that high level abstractions can be very easy to use and mesh well with smaller projects. Consider node and js. You could write a program with the http module and manage http resources how you'd like. An equivalent could be hyper. You could also go higher level and use express which might be analogous to something like rocket. Or in node you could use something like SailsJS which might be equivalent to Actix in that it has lots of high level concepts for elegant code even if some implementation details are hidden.
What is your *actual* concern?
In the case of Microsoft and actix specifically, I believe that the primary author works for MS. That likely explains some of their lack of fear =P
Can I use the debugger in idea or do I need clion for that?
Looks really cool. The problem though, is that I can't actually play it. Does it require multiple people to play? I went through and changed some of the controls (the defaults were a bit awkward looking imo), then I started up the game. I got through the main menu, chose my character, then just couldn't progress after that. The game would not allow me to actually start.
*Finally* being able to use `Box&lt;dyn FnOnce(..)&gt;` on stable. ❤️
Hm, the game requires at least 2 players (haven't gotten to implementing computer players). Once 2 or more players have confirmed their character selection, then the next `Attack` should let it proceed. Thanks for trying it out =D
Sustainability. There're several reasons a maintainer might have to leave the project, e.g. new job or burnout (hopefully not, but in OSS not uncommon). Fixing stuff or contributing to those projects is beyond my skill level and what I could handle beside my day job. P.S.: I'm aware that I'm benefitting from OSS, but that is another story. I don't intend to somehow reduce the great performance of those maintainers. I just wonder how others handle this particular situation for those widely used libs.
Board support crate for the STM32F3DISCOVERY: [https://docs.rs/f3/0.6.1/f3/](https://docs.rs/f3/0.6.1/f3/) Example for setting the clock: [https://docs.rs/f3/0.6.1/f3/examples/\_03\_blinky/index.html](https://docs.rs/f3/0.6.1/f3/examples/_03_blinky/index.html)
I guess I can't try it out yet then. You can definitely count on me trying it though, when you get computer players implemented.
Yeap, at the moment I was handling those as items on a roadmap, to have a sense of sequence, but I can move that to issues. Makes total sense!
&gt; Fixing stuff or contributing to those projects is beyond my skill level and what I could handle beside my day job. So only use stuff that's used by enough other people such that the chances of somebody else picking maintainer ship are larger ?
You need CLion for the debugger. Personally, it was very much worth to buy jsut for that.
Just keep on mind that `remove(0)` in a loop makes this algorithm O(n\^2), and it'll be really slow for longer vecs. That's because on every iteration all the data needs to be moved one position left. I like u/boomshroom's solution with `into_iter`, which avoids this problem.
Hadn't seen the \`dbg!()\` macro before, looks super convenient!
&gt; Is there a way to "move" the ownership of the memory in keys and values to the hashmap without cloning? Yes, it's possible to move stuff out of a Vec. You should use iterators for that. A `Vec&lt;T&gt;` offers four kinds of iterators: * `iter()` taking `&amp;self` giving you an `Iterator&lt;Item = &amp;T&gt;` (borrowing the Vec) * `iter_mut()` taking `&amp;mut self` giving you an `impl Iterator&lt;Item = &amp;mut T&gt;` (borrowing the Vec) * `into_iter()` taking `self` giving you an `impl Iterator&lt;Item = T&gt;` (consuming the Vec) * `drain(..)` taking `&amp;mut self` giving you an `impl Iterator&lt;Item = T&gt;` (borrowing the Vec and leaving it empty) So, for moving the `String`s out of your `Vec&lt;String&gt;` you could use either `into_iter()` or `drain(..)`. I suggest `into_iter` because (a) you don't have any use for that `Vec` afterwards and (b) draining is presumably more complicated. But you don't even need to collect the strings into a Vec and then move them out again. You can create this HashMap directly based on a longer chain of iterators: fn test_funct() -&gt; HashMap&lt;String, i32&gt; { let values = [1,2,3,4]; let keys = values.iter().map(|x| x.to_string()); // Iterator&lt;Item=String&gt; let pairs = keys.zip(values.iter().cloned()); // Iterator&lt;Item=(String,i32)&gt; pairs.collect() }
&gt; without implementing the struct in java? Like some kind of pointer? Also, how &gt; would that look in rust_swig? In JNI common way to use pointers is Java "long" type. So you allocate something in "native" heap and return long to Java world. By Java spec long is 64 bit integer, so it works on all common cases (32 bit or 64 bit CPU). If you look at generated Java code by rust_swig, you can also see that it uses this technique via Box/Rc/Arc::into_raw. So with rust_swig you can use jlong_to_pointer / as jlong to work with it. But it is unclear why you need it. You loose type safety for what?
&gt; Compilers want to split code into as many pipelines as possible to ensure the CPU had enough to optimize and always be doing something. Compilers actually already somewhat do this. The problem often is 1. Lack of context, so "pipelines" can only be generated for short sequences with no opaque calls that may have side-effects. Things like strict aliasing might help. 2. Interconnectivity. "Pipelines" would often be much shorter than you might expect, and would be serialized anyway. The performance benefits of inlining actually often come from increasing the compiler's ability to analyze these things, moreso than avoiding the branch instruction. &gt; I would do away with the idea of direct RAM access. Assembly would instead load from addresses at a specific cache page into the CPU registers. I have actually made this argument in the past, but it turns out that it would probably not work well in practice. The compiler would need full transparency for every function being called. You would get code that thrashes the cache (by calling a library function, for instance) by accident, where today's cache heuristics perform very, very well. &gt; Instead it's up to the compiler to understand semantics. The nice thing is that you can choose when to use the feature aggressively and when to avoid it fully, because the compiler can have greater context than the CPU. And the downside is that programs compiled for such an architecture will be less portable. They would need to be recompiled for each microarchitecture. Separate binaries for AMD/Intel, various ARM vendors, and for each generation of chips from each vendor. The compiler has _much_ less context than the CPU. It generally only knows the context of the translation unit that it is working on. Full-program optimization (such as LTO) is the closest thing we have, but it still doesn't take external libraries into account. If you have ever worked on a large project, you will know that LTO is definitely not a trivial thing to enable, not least because it imposes extreme requirements on memory usage and increases build times substantially. The compiler knows close to nothing about external libraries, other than the names of the symbols being invoked. It could get this information through annotations in the function signature, attributes, whatever, but it is definitely not clear that this will be sufficient (library functions are allowed to call other libraries, etc.). And lastly, the compiler knows nothing about the runtime state of the CPU. As you say, the benefits of such a system are questionable. :-)
What? Of course Actix (one maintainer) has a much bigger risk of collapsing than Rust itself with the current size of the contributing community. I don't really get that you are arguing against that…
actix has dozens of contributors, most of which are members of the actix org
&gt;mpmc How does this differ from crossbeam\_channels?
Clippy can calculate cyclomatic complexity and will give a warning if your functions are too complex.
&gt; Actix has dozens of contributors and maintainers I'm sorry but : Actix: 57 has contributors, but fafhrd91 has 541 commits, and the second biggest contributor [only **27**](https://github.com/actix/actix/graphs/contributors) Actix web: fafhrd91: 2,070 commits, the [second one 52](https://github.com/actix/actix-web/graphs/contributors) Saying that Actix has dozens of maintainers isn't a realistic take on the situation, at all. I really like Actix, but I totally understand the op's worries. And saying “meh, you can't find a 100% reliable project anyware” just totally misses the point.
&gt; And saying “meh, you can't find a 100% reliable project anyware” just totally misses the point. Saying "I'd like things to be better, but I am not willing to do anything to improve things", isn't a very convincing concern either.
thanks
good idea, fixed it :)
So, if the intent of the `AThing` was to do some operations on the `name`, like extend it with more data, you would have to clone it at some point right? I am still trying to understand the rest of the article. I get the point of not cloning except when you explicitly need to. If I were to summarize your point in my own words: - only clone when you need to modify a copy of the data - otherwise use a reference (or borrow?)
Shoot! I totally forgot about the clocks. In hindsight it makes perfectly sense. The STM32CubeIDE knows about the Discovery board and therefore sets good defaults. The Rust crates do now know that there is an external oscillator available and therefore have to default to the internal 8MHz RC. &amp;#x200B; Thanks, this seems to be the main culprit! I used the code from the 03\_blinky example and set the speed to 16Mhz. The calcuations now took only \~196ms. The STM-IDE has a default of 48MHz, which is 3x the speed. 196ms / 3 = 65ms. I don´t think that will be the case, but it will definitely be something around the times C/C++ got. Anyhow, I was unable to set it to 48MHz, so I couldn´t test it. I just replaced the 16.mhz() with a 48.mhz() and then the LED never did light up... I´m new to embedded Rust and am probably using the API wrongly. (And I´m having a hard time reading the docs. I feels like there is so much code and little desciption or examples on how to use it ...) Is there a more detailed description what this CFGR::sysclk() does behind the scenes and what parameters are valid?
Little fighter 2! I love it! Gona defineatly download and look forward this project
crossbeam_channels provides implementation of mpmc. Use any one you like, this is probably a good choice.
Ah, I see, there was a package called mpmc on [crates.io](https://crates.io), but I guess you meant the concept of mpmc
Thanks for your reply! I'll look into using a long. Basically, the library Gilrs has struct called gilrs::Gilrs initialized in the beginning and used in almost every method. It controls events, and such. I dont really know if this pointer fixes the problem, because that wouldn't make it last the whole lifetime... Any suggestions?
This is a pretty disingenuous take. Importance has to be balanced against other stuff. Just because most languages use prefix await and Rust (appears to be) choosing a postfix await does _not_ mean that consistency with other languages was not very important. In fact, if you read some of the arguments from lang team members, one of the strong pieces of criteria in its favor was that it was similar to how many other programming languages implemented the same feature. We don't have to, nor should we be, harping on singular issues. PL design is a series of design trade offs. Just because "language similarity" is a deciding factor in one case doesn't mean it's a deciding factor in _every_ case. This is _not_ odd at all. It's completely normal and a sign of a health design process.
I'm trying to replicate this code with iterators: let mut ret : Vec&lt;usize&gt; = Vec::with_capacity(20); for i in 0..40 { if i % 2 == 0{ ret.push(i); } } return ret; I have come up with something like: let ret : Vec&lt;usize&gt; = (0..40).iter().step(2).collect(); The problem is `0..40` does not implement `Iterator`. What is the most straightforward way to generate a iterator of numbers? Is there some way to have an iterator from 0 to infinity, use `.take(40)` and then `.step()` as in the code above?
&gt; I dont really know if this pointer fixes the problem, because that wouldn't make it last the whole lifetime Obviously if you want pointer and pass it back and forth, you need "boxed" it and pass pointer back and forth. Like `Box&lt;Girls&gt;` `Box::into_raw as jlong` and `jlong_to_pointer` to convert it back. But this is exactly what rust_swig do. So it is unclear why do so problem thing by hands.
How does it fare compared to vscode these days? I have a jetbrains toolbox subscription (mostly for IntelliJ Ultimate and Datagrip), but last time I tried it there wasn't a compelling reason to use it for Rust over VSCode
I’m not sure I follow. Speed up is calculated with respect to the language’s single threaded performance. For his analysis it wouldn’t make sense to use the ratio of multithreaded C++ to single threaded Rust. The goal is to see how well each language scales as the number of available threads increases. Either way, as stated in a few posts here, you can just go to his GitHub repo and look at the raw results. Rust performs similarly to C++.
This kind of makes sense to me, so rust_swig has this functionality built in. I'll attempt to put this in, and if it works I'll check back. Thanks so much!
You had me quite confused there. Ranges do implement `Iterator` in Rust, so you don't need to call `iter()` on it. Now the error you were most likely getting is the fact that the `step` function is actually called `step_by`! It's not necessary anymore now, but you can of course also use infinite ranges: let ret : Vec&lt;usize&gt; = (0..).take(40).step_by(2).collect();
Many extremely good repositories are not well maintained and have a big backlog. As a user I feel that the best way to support those libraries is through writing blog posts that showcase the usefulness of the repos. This will help other developer (hopefully more capable developers) get initiated into the repo and the tool and thus increase the lifespan of the tool.
With rust_swig if you wrap struct/enum with foreigner_class then it automatically convert it to/from jlong pointer when it see this struct/enum as argument of method. This is basic functionality, I myself don't use it, if it have no such functionality. But you describe something different, you want pass struct/enum without defining it as foreign_class. It is rather strange.
The situation is not unique to Rust; GnuPG or GNU Parallel are, as far as I understand, one-man shows. For a couple of years, groff was without a maintainer. And I'm sure that there are widely-used libraries in Node, Python, Ruby, etc. that depend on the heroic efforts of a single contributor. When you pick a piece of software, you need to ask if your life is better using it, even if that means you might be left stranded in a couple of years, or if the risk of an unmaintained project is too great, in which case you should find an alternative.
Why not "pass ownership"? I'm not a native English speaker, but it sounds fine to me. As for moving values, it's at least a well-established, if not straightforward, term (cf. C++11).
&gt; I bet Rust loses a lot of frustrated newbies because of this. I'd like to see numbers for this claim. "Move ownership" may not be idiomatic English, but engineering terms are not always idiomatic English. It is chosen because it combines the operation (`move`) with the thing it operates on ("Ownership"). "Pass Ownership", as /u/WellMakeItSomehow suggests is also in use at some places.
&gt; Why not "pass ownership"? "Adopt" does the job with one word instead of two. No need to add the word ownership because "adopt" already entails that. &gt; As for moving values This is about moving ownership, not moving data. Yet another reason why the term "move" creates confusion.
I don't see what's so frustrating about the phrase "move ownership".
I find much higher success rate using IntelliJ for my code (completion, error highlighting, type hints) but YMMV
It's not the same, because it's in going in the different direction. Just as you pass ownership, you _give for adoption_, which is even longer.
&gt; I'd like to see numbers for this claim. Well you've got one frustrated newbie right here. If you want more numbers than one, great, knock yourself out designing that double-blinded study. More power to you.
Did you try to change the frequency by steps, like change it to 16MHz, wait for stabilization, then to 48MHz. I think the datasheet say something about the max clock change at once.
No, you adopt. I am a native English speaker.
Is this supposed to be 100% safe code? Because if you look at the stdlib's implementation of `VecDeque` there's some clever tricks it does with leaving chunks of the underlying memory "invalid" as they're beyond the bounds of where the "real" data in the queue is. You might want to look into that, it's worth sorting through it.
&gt; I don't see what's so frustrating about the phrase "move ownership". Had you ever used the phrase "move ownership" before encountering Rust? Ever?
Do you use the phrase "move ownership" outside the context of Rust? Ever?
Clojure has a similar complaint all the time about "unmaintained" libs -- where small libraries are actually just mostly functionally complete and the authors don't feel the need to rewrite the API every 6 mo. That's not to say that there are _never_ improvements that could be done with a more active maintainer, but things "just work" and don't tend to need active bug fixes. I can see Rust ending up in a similar situation. Rust has more churn due to being a younger language, but benefits from a high degree of stability/safety and has a culture of small packages with clean APIs and minimal scope.
I find the whole "moving/passing ownership" nomenclature confusing too, but from different reasons (or perhaps not so different). In my mental model, you just move things from one place into another — if I have a brick and give it to you, then you have a brick now and I don't. Note that I'm talking about moving _things_, not _ownership of things_. If I try to imagine how "passing ownership" would look like, I picture me giving a slip of paper to you saying that you now own the brick, without moving actual object. I think it's needlessly complicated and in my mind it feels like passing some `&amp;mut T`s, not `T`s. So, your proposed single word "adopt" definitely reads better in my eyes, because it's a single verb which refers directly to the object (and not indirectly to an ownership of such). I think the same could be said about bare "give", "move", "transfer", "throw" etc.
&gt; Had you ever used the phrase "move ownership" Sure. Ownership is often conceptualized as a "thing" (a legal entity, etc.). I've definitely discussed moving ownership of an internal library to another team, for example.
I hear you but it seems to me you're just moving the work to the user of the service. Kafka keeps track of these offsets in its own topic and that was a change in later versions to make it easier on the user. I honestly can't see myself using a service that doesn't have this functionality today.
Reducing the number of words should only be a goal if the original text was too long. We're not playing Text Golf here.
How about transfer ownership? You transfer ownership of a house, boat, car... I think that that makes more sense than move ownership tbh.
Any reason you picked a cons-list implementation for stacks instead of a vec-based one?
I love rust language, so I'm trying to implement [howdoi](https://github.com/gleitz/howdoi) in rust. After the practise, I found that when my code can be compiled, the program can lessly raise `panic` (May be it can refer to `RuntimeError` on other language). Which make me so exciting! Ofcause I can't say that I'm fluent with rust(especially something about `lifetime`). And when I come to design internal API, it's hard to decide the API should accept `reference` as parameter, or accept actual object(which will give up the owner). It's hard for me to say that which is better :( But anyway, I love rust very much. It force me to concern about error-handling process. Which makes me relealize that there are many corner case we need to handle in one program.
it not supposed to be safe, just a simplified version of std.
You said "I bet ... a lot". This is much different then from "this frustrates me". Claims of wide impact should _always_ be supported. &gt; If you want more numbers than one, great, knock yourself out designing that double-blinded study. More power to you. In case it interests you, yes, we do speak to teachers and educators regularly.
You can use vector as stack directly. It shows how to implement it with linked list :)
I've used Kinesis for a long time, and I have to admit it was much easier as a consumer to keep track of the cursor... much more when you re-shard your streams. Either way, it's not something I am strongly against, it's more a UX mindset. The main goal is to learn, so having such discussions is awesome!