Much better utilization of multicore hardware (already demonstrated). This means better battery life on mobile too. Better security through memory safety (eliminating ~50% of Firefox security critical bugs). Generally a chance to do experiments and make architectural changes that you just can't do in a huge 15 year old C++ codebase with 500 million users.
I was especially thinking of the `handler.get("database").unwrap()` thingy. If you ever make a typo in the name of what you want to retreive, or if you don't pass the middleware, it is only detected at runtime. Without callbacks, you are free to create your own threads and the database connections, templates cache, etc. become regular local variables. I had something else in mind, but I don't remember right now. 
There's `.count()` too.
FWIW, there's two contexts for lifetimes and the subtle difference can be slightly confusing: - In a type, e.g. `&amp;'static i32`, `'static` means that the data it points to *actually* lasts for the whole execution of the program - As a bound, e.g. `fn foo&lt;T: 'static&gt;(...`, it means that `T` is *allowed* to last forever, but doesn't necessarily do so. Something that contains lifetimes shorter than `'static` isn't allowed to last forever. E.g. `i32: 'static` because it is safe to hold it for arbitrarily long, but `&amp;'a i32` isn't `'static` because it is illegal to hold it for longer than `'a`.
What is he using? Enlightenment?
I think so, yes (the username of the YouTube account hosting the video in the article is [e17](http://en.wikipedia.org/wiki/Enlightenment_%28software%29#E17)releasemanager). 
I wrote an intro to some of the problems that Servo is trying to solve [here](https://www.catalyst.net.nz/show-and-tell/blog/two-and-twenty-years-browsers-or-jankfree-animation) (feedback would be appreciated)
Well zmike *is* a principal dev. http://en.wikipedia.org/wiki/Enlightenment_%28software%29#Principal
Hah, fair point
Except that I'm on OSX, so the easiest way for me to get started is to install a different operating system... :P
&gt; Except for the JavaScript engine (written in C++) which is shared with Gecko, and will be sandboxed within Servo using NaCL. We don't use NaCl, or plan to. I don't know our exact planned sandboxing story, but https://github.com/pcwalton/gaol is relevant.
There shouldn't ever be a Rust 2.0. Major backwards-incompatible changes can really hurt a language. Having half your API deprecated for 5 years isn't nice either. All of this doesn't instil in me any confidence that Rust is actually stable despite it being post-1.0.
&gt; There shouldn't ever be a Java 2.0. Major backwards-incompatible changes can really hurt a language. Having half your API deprecated for 5 years isn't nice either. Yes, right (Sorry, mahacctissoawsum, no offense meant, m'kay?).
Pity, isn't it? If only Rust had defined a *proper* concatenation operator... oh well. Spilled milk and all that. :)
I don't understand: is this about deprecation through a compilation warning or real removal of stuff from the language? Doesn’t the latter contradict the promise of full backwards compatibility in Rust 1?
I like "bring to production" as an alternative. :)
Just deprecation through a compilation warning. No API can be removed until the mythical Rust 2.0.
Should I use tuples or arrays for coordinates? Is there a way to add tuples or arrays together without dissecting them, a la MATLAB?
All of what? Every language adds new APIs over time, some just do a worse job than others at deprecating old ones. If you're going to claim that Rust isn't stable because of that, then you might as well claim that Java isn't stable.
Allowing a `--target=&lt;x.y&gt;` compiler option would take care of that. The deprecation attribute would then become `#[deprecated(v1_2)]` or something to that effect.
Thanks for the summary. But do you know what happens at the hardware level? Say I have a relatively thick wrapper struct for 16 doubles Double16 and derive the copy trait for it. If I then define the method foo(bar: Double16) -&gt; Double, is the struct at the hardware level passed by value or reference? I would hope that the reference and move semantics of Rust are purely at a language level, which would leave the compiler free to choose how to actually pass data around (especially since we can never have mutable aliasing), but I don't know if this is actually the case.
There is a difference. Java (at least until version 9, when Jigsaw hits) keeps 100% backwards compatibility; they still have all those deprecated APIs. At JavaLand 2015, Marcus Lagergren had a demo of a Java 1.0 app that ran on 1.8 unmodified. Also Java only very conservatively takes features away, given that both `javac` and `java` allow to specify target versions. I think the former (allow code of any version of the language to interoperate, and to define a target version) is a good model to emulate.
That seems like a really bad idea. I mean, `&gt;&gt;?:^)` takes a while to type, *and* it has an unbalanced parenthesis. Definite "no" vote from me. &amp;nbsp; ^^^/s
Thanks for input! &gt; `.get(path: Into&lt;&amp;XPath&gt;) -&gt; Snippet ` where XPath implements From&lt;&amp;str&gt;`, and may also have a builder interface I meant more as in preferred usage (I'll change post to reflect that). E.g. let doc = str.parse(); doc.get("/A/B") but doc.get can accept Into&lt;XPath&gt; interface. &gt; .replace(…)? .cut()? So if I understand you correctly, you'd want to do something like: let doc = // &lt;h1&gt;&lt;head&gt;foo&lt;/head&lt;/h1&gt; doc.replace("head") = "bar" //doc is now &lt;h1&gt;&lt;head&gt;bar&lt;/head&lt;/h1&gt; &gt; I once wrote a static analysis tool (i.e. lint) for a XML-based programming language. I used a SAX parser. Cool! What did you use most often in that parser? Was there anything you wish could be easier?
&gt; What did you use most often in that parser? I serialized the XML into my own pseudo-AST that was enriched with flow / scope / type information, but the XML parser itself was a boring standard SAX2 number, so I mostly used the element, attribute and characters-callbacks. That said, think about how the DOM handles manipulation. Look at JAXP or python's ElementTree API. Those are examples to emulate.
D2, Ruby 1.9 (this ended up pretty okay), Python 3000. Perl 6.
I would argue that you should use a `struct` for coordinates, actually.
I would say 'productize,' personally.
One could argue, that these hard breaks have been necessary because the languages have not evolved incrementally. It's another question, if a language can evolve incrementally and have a big footprint in the industry. 
Or, heck, even `arr.[i]` or something (or `func![generic]` for generics, which looks extra readable...). Of course, it's way too late now :/ ... **Edit:** Forgot that `identifier![...]` is valid macro form. Well, `array.[index]` and `ident[instance]` still look fine to me. Better than `ident&lt;generic&lt;special&lt;instance&gt;&gt;&gt;` at least...
`"42".to![int]()`? But yeah, too late now :/ ... **Edit:** `"42".to[int]()` looks better to me than angle-bracket versions. Forgot that `ident![...]` was valid macro invocation.
Thanks for the answer :) I'll look into negative impl!
Go did have "gofix" for that, when it was pre-1.0. That tool automatically fixed the source code wherever possible.
It may have been down voted to try to encourage keeping the discussion in one place as cmr encouraged. (I agree that the comment didn't deserve down voting for its content.)
*Every* long term ecosystem has deprecated APIs (even super-stable ones like C/C++/Java), often they're only deprecated by documentation or even word of mouth but Rust provides the option for the compiler to help too. Whether or not we use that option, there's absolutely no question that use of certain functions will be discouraged. There is definitely scope for optimising how we handle such things, but improvements are a fact of life. (As one point, I assume we will be landing bug fixes/maintaining even compiler-noted deprecated things.)
To be clear, Rust will also be keeping all its deprecated APIs. The team is committed to stability.
A simpler version of the same idea. When you call `borrow` from a `clone` function you end up with both a mutable and an immutable reference of the same object: thread_local!(static TLS: ExCell&lt;QQ&gt; = ExCell::new(QQ("Init".to_string()))); impl Clone for QQ { fn clone(&amp;self) -&gt; Self { let mystr = &amp;*self.0; println!("Before: {}", mystr); TLS.with(|t| { t.borrow(|s| { s.0.remove(0) }) // this line mutates self, which is currently borrowed as immutable }); println!("After: {}", mystr); QQ("".to_string()) } } fn main() { TLS.with(|t| t.clone()); } 
No
Lately, I've been using a sort of inherited struct pattern in rust; I'm not sure if it's for the worse but it really simplifies things for me. I'll first create a [trait](https://github.com/PistonDevelopers/mush/blob/master/src/uigraph.rs#L9) which requires the access to the [base-struct](https://github.com/PistonDevelopers/mush/blob/master/src/uigraph.rs#L76) which is to be inherited. With the [trait in scope](https://github.com/PistonDevelopers/mush/blob/master/src/uigraph.rs#L102) I can use the [accessor methods](https://github.com/PistonDevelopers/mush/blob/master/src/uigraph.rs#L126) All that is required is a place for the inherited struct to [live](https://github.com/PistonDevelopers/mush/blob/master/examples/demo.rs#L42) and the [accessor impl](https://github.com/PistonDevelopers/mush/blob/master/examples/demo.rs#L63) It has been my work around, though I wonder if it's a bad pattern. 
I really like to know how they will handle C++ inter-op. 
&gt; No API can be removed until the mythical Rust 2.0. But it can be version-gated (and thus effectively be removed, unless your code targets an older version).
I'm not entirely sure what you're asking - all three code examples you posted should work. If you're wanting to get a `Box&lt;Error + Send + Sync&gt;` when only using `x: Box&lt;Error&gt;`, the reason for this (I believe) is that `Box&lt;Error&gt;` and `Box&lt;Error + Send + Sync&gt;` are completely different incompatible types - if you declare it to be `Box&lt;Error&gt;`, it will be exactly that, no more, no less. Also, if `Box&lt;Error&gt;` did imply `Send + Sync` if the type was `Send + Sync`, then it would lead to some very confusing error messages if later on someone uses `let x: Box&lt;Error&gt; = ... (some type which isn't Send + Sync)` and it doesn't work the same way your code would.
I'd argue that `.count()` would still be a bit simpler/more concise, but that would also work.
One of the Sandstorm developers is working on [Cap'n Proto in Rust](http://dwrensha.github.io/capnproto-rust/). I wouldn't be too surprised if Sandstorm started moving in that direction eventually.
In case you missed it, this was a reference to a running gag that /u/Manishearth started.
Hah, also you should consider including [Ferris](https://www.reddit.com/r/rust/comments/364jhm/rustacean_mascot_name/) as the main avatar... ;D
Cool! I would love to see a Rust port of the Haskell [diagrams](http://projects.haskell.org/diagrams/) library, as well.
At [Maidsafe](http://maidsafe.net/) as I understand it, their c++ systems were re-written in Rust with great success. A couple of the projects can be found here: https://github.com/maidsafe/routing
Good username / comment synergy.
&gt; It should be possible to make a Rust library with the same API as a C library, although we are not aware of any attempt at that project right now Servo is a good example. It can [replace embedded Chromium](http://blogs.s-osg.org/servo-the-countdown-to-your-next-browser-continues/) without recompiling. Here's the [video demo](https://www.youtube.com/watch?v=KR7RwaZ_xrw).
If any projects have people similar to me, then its really early. The guys that love languages are just starting to do their home evaluation. From there, tests will be done, starting small and growing. Then suggestions will be made at work, so over the next year, smaller projects will be done in Rust, and as its a win, bigger and bigger ones will be. The C guys are almost impossible to bring over, but C++ is a prime candidate. 
I don't know if the current stability guarantees allow for breaking changes that can be remedied by adding an attribute. One could argue that they do in theory, but I'd want to see this spelled out explicitly. Until then, I am acting under the impression that version gating isn't possible without bumping the major version.
No, I mean the inverse: If the developer specifies a version, we can make items unavailable for that version. If no version is specified, all items should be available.
I've been in touch with many people in industry who are evaluating Rust now that it's post-1.0, including a few at those enormous web companies which I won't name but whose names you know regardless. I wouldn't expect any announcements for a few months, however (and only if the experiments turn out favorably, naturally). In the meantime, for anyone here interested, I'm offering my services as unofficial liaison between corporate users and the Rust developers. I think it's important that the Rust project begin to cultivate a relationship with companies using Rust, and creating a rapport is the first step! Feel free to PM me on here or contact me via the nick "bstrie" on IRC.
That build process is the bit that scares me off. The build script maintains its own cargo and rust install. I really don't want to have to use Servo's cargo to build my entire project. If there was an easy way to let Servo build using the system rustc and cargo that would be great, but I can't see any documentation on that, or how one would go about using Servo as a library. If this works, it would be really great for embedding game UIs.
That is a design implementation in Iron, to use a HashMap for extensions. It's in no way required to use the callback approach. I agree that using strings is less desirable; as I build a framework for use in Mozilla webdev, it's something I'll avoid as well. In fact, you can use local variables that are captured by a closure just as you mention an iterator can. The only thing you need to do is to make sure they are `Sync`, since the closure is shared among threads.
If it's funded when will this book be published?
Great, that explanation helps a lot, thank you. I'll try to edit the post a bit. That code actually is reduced a bit, I just wasn't sure what else could be taken out. edit: So, I have nightly installed, how can I use ScopedPool?
I thought they were only thinking about it, with the C++ core going on as usual?
Yes, Nov. 2015 is what I put on Kickstarter but I expect to finish it before that.
FRP has made haskell a viable choice for GUIs although no large-scale projects exist that I know of. Pandoc shows that haskell can be a great choice for document processing.
Dropbox is using Rust for the next gen exabyte+ block storage engine. Although it's more about writing it from scratch rather than switching. [proof #1](http://www.reddit.com/r/rust/comments/2xvtll/getting_acquainted_with_mio/cp5tfa4) [proof #2](http://www.reddit.com/r/programming/comments/374mre/we_just_switched_from_rust_to_nim_for_a_very/crk48jw)
No, but you can implement trait for `Foo` that will do almost the same (except introducing new syntax inside macro).
Package names are lowercase by convention. You should name it `rurtle` instead of `Rurtle` if you want to conform to the convention.
https://air.mozilla.org/rust-release-party/
Does Rurtle really need GLSL 1.40? My video card only supports up to 1.20 (OpenGL 2.1).
Yeah, C++ doesnt have a defined ABI like C. Thats going to slow down adoption a little, but shouldnt be tooo bad. 
If I was allowed to post memes in this subreddit, I would post one of the aliens guy that said "MONADS".
For my own curiosity, why do that over a macro that simply accepts a `Foo`? ...not to start a UFCS discussion :P There was a [Pre-RFC discussion](https://internals.rust-lang.org/t/pre-rfc-macro-improvements/2088) about improving the macro system. One of the topics was namespacing, although this isn't exactly what you're asking about.
A clever idea coming from Nim is to target C++ as codegen backend. This way, final compiling and linking is done by c++ and you don't have to worry about ABI.
Still just a patch in the bugtracker. It hasn't been merged yet.
I understand crowdfunding non-commercial projects and investing in commercial ones but crowdfunding commercial projects looks like a scam to me.
To the extent that some are just pre-orders at a discounted price, with the caveat that if there isn't enough interest your money will be refunded instead, it isn't really a scam. Though, I can't imagine any good reason to contribute, for example, $5 when the minimum price to get a product is $15. Nor do I see spending $15 on this when it looks like the price going forwards is likely to be $15 anyways (judging by his previous book). The corporate sponsorship options are especially not scam-like, just a venue for advertising...
&gt; Not likely to happen except as an effort to refactor Tor into different modules and rewrite piece-by-piece. OTOH, there's nothing stopping somebody from doing a compatible implementation in whatever language they like. Rust enthusiasts: Prove the superiority of your language by writing a simpler, better, bug-free Tor today! ;) I loved this, haha.
never mind then! that is annoying... I mean I can see how those two things are separate types, but it would be nice if that would work... I would suggest filing on issue on the rust github repo for this, it seems like something isn't working entirely as nicely as it should be.
LLVM does not have a C backend in-tree anymore, at least. I don't think it's maintained.
I think that boat has sailed.
Laundry eating has always been an [important consideration](https://github.com/rust-lang/rust-wiki-backup/commit/25421ebae90ef3458a91baddda2179d7a1745e51).
I guess I'm familiar with neither the challenge in implementing office suites nor the strengths of Haskell, and how the two line up.
I had no idea there was a "part 1." Shows how much I pay attention. Anyhow, this looks incredibly interesting even though Rust isn't quite web-ready. I think I'll give this a read!
Scala calls it mkString.
What derailments are you referring to?
So our next catchprase is: **Rust – eating laundry since 2010**, right?
I think there is a section on that on the rust book.
/r/playrust
Hi there. You posted to the wrong subreddit. You're looking for /r/playrust instead of /r/rust as this subreddit is for the Rust programming language.
oh wow thanks for changing that for me! i'm sorry!
https://doc.rust-lang.org/book/crates-and-modules.html
The section is this one: https://doc.rust-lang.org/book/crates-and-modules.html
I don't think you can make a Box&lt;Path&gt;, because Path is unsized. Use a PathBuf instead? http://is.gd/6AELAI
You don't need the `Box` since PathBuf is `Sized`: https://play.rust-lang.org/?gist=946e817a0fe21855e27e&amp;version=stable 
In addition to the section of the book on crates and modules, you should also read the section on cargo. You mentioned rustc; if you don't know, you should really be using cargo, which is a build system that handles linking crates together for you, as well as downloading third party libraries (its also a test runner and other stuff). https://doc.rust-lang.org/book/hello-cargo.html
http://en.wikipedia.org/wiki/2015_Philadelphia_train_derailment
The left hand side of each argument in a function definition is a pattern; therefore instead of this: extern "rust-call" fn call_once(self, args: (N,)) -&gt; Self::Output { $name::new(self.amount * args.0) } You could write this: extern "rust-call" fn call_once(self, (n,): (N,)) -&gt; Self::Output { $name::new(self.amount * n) }
That's fair. I was getting the impression that PathBuf was meant to be used as a builder pattern: you create one, keep adding components, and then get a Path out at the last second. And now it makes slightly more sense why it's a good idea for it to implement Deref&lt;Path&gt;. Thanks for the help.
Rust might hit v1.0 stable already, but there are lots of things, as seen in API docs, marked as 'unstable' and marked for RFC approval/disproval in the future. This may be a blocker for lots of corporations.
There is, but you need to have your project opened as a folder. Once have the root folder open, then you can press Ctrl-Shift-B and it will come up with something to configure the task runner. It gives you a typescript one by default but it's pretty easy to make a cargo build one. The examples show how do to most things.
Aye, thanks. I've taken your advice and [posted an issue](https://github.com/rust-lang/rust/issues/25982).
Generics are monomorphized (have a non-generic instantiation generated for each type they are used with), which means that if you do a runtime test for zero size, it will end up as a test on a constant in each case, and the optimizer will effortlessly eliminate it. The standard library also does this (e.g. `Vec`).
Syntex is cool, alas it's no use for us lint writers. Still, we can live with the small amounts of breakage (in about 4 weeks, we only had two instances that were easily fixed).
Syntex is cool, but it needs more examples/documentation imho :( Is there a good source of documentation for it (or rust-quasi/rust-aster)?
What the hell is "hamster consumption" and "eating laundry" ??
I've definitely read it before but I totally forgot about it when I wrote the readme
I've only heard it exists, I haven't actually tried it yet.
Interesting post with some valuable information on how to extend information and error messages. One thing we should definitely do is expand the documentation for `try!` to mention that it requires the return type of the function to be `Result&lt;T,E&gt;` (and maybe don't omit the type signature in the mentioned [File docs example](https://doc.rust-lang.org/std/fs/struct.File.html)).
Hmm, I don't know. I think the [try! documentation](http://doc.rust-lang.org/nightly/std/macro.try!.html) is quite clear about this when it says that the macro can make an early return. IMO, the error message can be confusing, but I don't see anything wrong with the docs. Is it just me?
It has build commands built in, but for Rust you will need to make a task.json file that describes how to run cargo i imagine. It supports grunt and gulp out of the box, and although they could be used to build any kind of project, they are usually used for javascript stuff. Im going to try integrating this weekend. They actually have a pretty nice simple interface to integrate command line tools. EDIT: https://code.visualstudio.com/docs/tasks EDIT: this is working for me to start... EDIT: Use bytemr's more complete config file: https://gist.github.com/Mr-Byte/855cae109e8687cc22c3 
I think you should be able to do it without macro: ecs.collect_with::&lt;(Position, Velocity)&gt;() Maybe even with inference, just ecs.collect(); “Just make some traits” for handling those tuples and call it a day! (I know that it may probably be not so trivial)
Any time. I also always forget its name too :(
Yeah, it's just you. I thought abount sending a pull request last night that just explicitly calls out that you can only use `try!` in a function whose return type is `Result`. I feel like it's the combination of wanting to experiment in `fn main() {}` and often seeing advice to people saying "Oh you can clean up your code by using `try`" without the nuance that that advice is only valid because the code in question is a method that returns `Result`. It's very subtle and should definitely be called out more clearly in the docs. In fact, I'm going to go do that right now :)
I have written up an [RFC](https://github.com/llogiq/rfcs/blob/master/text/0000-deprecation.md) to capture the consensus of the discussion and hopefully give us something we can actually implement.
See this crate! &gt; &lt; rustbot&gt; [num_cpus (0.2.6)](https://crates.io/crates/num_cpus) - Get the number of CPUs on a machine. -&gt; https://crates.io/crates/num_cpus
We should have done a test drive before we bought it.
[**@hannover\_rs**](https://twitter.com/hannover_rs/) &gt; [2015-06-03 15:05 UTC](https://twitter.com/hannover_rs/status/606114614997303296) &gt; Surprise, surprise! We'll run another @rustlang meetup at the @EDELSTALL in Hannover. 19 o'clock 23rd of June! DON'T MISS! ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://www.np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
Convenience, mostly. I don't want to write, say, an oauth library, or similar things; primarily because I don't think I'm good enough a programmer to be writing reliable libraries. An ORM would be nice too.
Very cool use of mathematical notation to explain iterators. Aside: Does rustdoc format LaTeX-style formulas in docs? I could easily see things like this augmenting the docs.
I didn't really promote the first article (or this one, for that matter). Thanks Carol for submitting this!
See my edit, I've added the ability to match out failed assertions in tests as a part of `Ctrl+Shift+T`.
I think the thing that got me was that it wasn't apparent to me that try! was the thing that was impacting the type signature. In hindsight, of course, its apparent that returning early would do this, but in the moment it wasn't.
Coming from a Java background, I fight this same intuition.
The docs for `try!` are far too sparse for its importance IMHO. You should totally do that PR, /u/carols10cents. Or I will! :)
It sounds like you want a function that takes a string slice as input and returns a Dataset, and then a function that takes a Dataset as input and returns a String. That would look similar to this: impl Dataset { fn encode(&amp;self) -&gt; String { String::new() } fn decode(string: &amp;str) -&gt; Dataset { Dataset { value_a: String::new(), value_b: false, value_c: String::new(), value_d: false, } } } You might also be interested in using the Option type to represent values that may or may not exist. 
Yeah, I'm not sure if it's an issue with VS code or an issue with the tasks.json file. I'd like to see them split out test reporting into it's own pane, though.
Is it me or Rust would have prevented a significant number of these problems?
I noticed that, barring special cases, you get the printable character associated with the keypress by converting to u8 and then to char. What happens if I type a 16-bit character like λ?
ahh I'm thinking of `impl !Sync` and `impl !Send`, you're right.
Rust doesn't (nor should it) prevent global variables, or keep you from writing spaghetti code. Rust warns if you ignore a Result, but it lets you call .unwrap() on it.
What about foreign keyboards? :)
_Simple_ keylogger in Rust :P
Thanks! I'm disappointed in my google skills for not having found anything about it.
Why not use MathML directly?
MathML unfortunately [doesn't seem to have great browser support](http://caniuse.com/#feat=mathml), and is apparently quite annoying to write by hand. (One could fix the latter by copy-pasting out of a LaTeX -&gt; MathML converter of course.)
Of course it would be appropriate! :) I would say though that reading through your example, it seems like type specialization is actually what you want. That would be the other side of loosening the coherence rules: rather than declaring traits mutually exclusive, allowing a subset of types bound by a parameter to implement different behavior. Under specialization rules, `Thing&lt;U&gt;` would be more specialized than `U`, and its implementation would take precedence. There aren't any RFCs yet discussing specialization, but I think some people are working on the problem.
It wouldn't notice if you pressed a key like that, because it only checks values of `i` in the range `8..190`.
Although Rust requires you to use `unsafe` to actually mutate any of those globals, but I get the feeling that these developers wouldn't have minded that too much.
Here's what I was thinking about: https://gist.github.com/krdln/57ebc1f1940cdd434646 (it would probably simpler if I assumed Key type = Value type (which it looks like you do)) I have never used ECS before, so forgive me, if my idea is unapplyable to your code.
Also, if you don't care about allocations, it's clearer: s.chars() .group_by(|c| c.is_whitespace()) .filter(|&amp;(is_whitespace, _)| !is_whitespace) .map(|(_, chars)| chars.into_iter().collect::&lt;String&gt;()); 
Done!!! :) https://github.com/rust-lang/rust/pull/25990
it's okay, google is terrible at rust stuff, given how much things are changing
Global variables are a way of life in embedded systems.
Because this: https://en.wikipedia.org/wiki/MathML#Example_and_comparison_to_other_formats The problem with writing MathML directly is that no one, in their right mind, would ever write MathML directly. Give me TeX, or give me death! ...or something at least reasonably approximate, no need to be *too* extreme.
Well definitely more concise. Thanks for the comparison!
A lot of the issues in the Toyota stemmed from concurrency. The Toyota had many different computing systems in it executing code, and all of these systems needed to communicate. No program language (afaik) can enforce that separate processes use reasonable communication protocols that have some safety and liveness properties. In this sense, I agree the problems with the Toyota went well beyond anything at the programming language level.
I can't imagine that's the case. There are programs such as [keyboard maestro](http://www.keyboardmaestro.com/main/) for OS X, which allows for things like bringing applications to the foreground.
You could simplify the duplication of the write! calls with a let binding on the match. let key = match k { 32 =&gt; " ", 8 =&gt; "[backspace]", ... _ =&gt; // char to str }; write!("{}", key); 
I don't know what this is supposed to imply but I've spent the last 6 years writing a lot of embedded code. I haven't found the need for global variables in embedded development anymore than I did for regular desktop development. A lot of it has to do with how you structure your code from the start. A poorly structured program will likely accrue a number of global variables because there isn't a sane way to pass data around the program. I actually argue that global variables aren't really bad except that they generally indicate poorly-designed programs. And really I don't want those kinds of programs anywhere near my throttle control or airbag deployment.
Thinking more about it, type specialization would not be as good a solution. If you have two variables `a: Vec2&lt;N&gt;` and `b: Vec3&lt;N&gt;`, then `a * b` would give you a type mismatch error if implemented using mutually exclusive traits. If it were implemented with type specialization, you would end up with something of type `Vec2&lt;Vec3&lt;N&gt;&gt;` which would lead to an error or weird bugs later on, making it harder to figure out what actually went wrong.
Could you please explain what is going on there, method by method?
I have a similar project: Tickeys. (For Mac). http://www.yingdev.com/projects/tickeys 
The most important extension might be triple backtick and default to rust syntax colors. I don't know, is that an extension? Oh and is it possible to secure MIT/apache dual licensing to be fully compat with inclusion into rust? Not sure how important that is.
Syntax coloring is usually provided by third parties: all the Markdown/CommonMark renderer does is respect the language annotation, translating it to an HTML class. Then, some third party JS magic takes over for the actual syntax highlighting; it is not the Markdown/CommonMark renderer's responsibility to know about each and every language. The triple backtick is not part of Markdown, but it is part of CommonMark.
* `char_indices`: Gives you `(pos, char)` pairs for each `char` in a string. * `group_by`: Groups successive elements which has the same "group key" together, giving `(key, vals)`. Here, the key is "is it whitespace" =&gt; groups characters into sub-sequences of whitespace/not-whitespace * `filter`: keeps *only* sub-sequences that are *not* whitespace * `map`: turns `(key, [(pos, char)])` into a slice of the original string using the `pos` at either end of the sub-sequence. Input is a string, output is an iterator of whitespace-separated words in that string.
Were it really the devs? I mean, as devs we usually have to fight management. Their deadlines. Their unproductive meetings to increase productivity. Their 10-minute schedule on interrupting focus. Their opinions on how to design a system. Sadly, we will never know in the case of Toyoto. I'd gladly spend a few thousand dollars more on a car if it means the difference between life and death.
Could I suggest ```String::new()``` or ```"".to_owned()```? I thing ```"".to_string()``` is a bit of an antipattern since it goes through the ```format!``` machinery.
It is known that rustc is far from optimal. 
But actual engineering practice for safety-critical systems encourages global variables (with formalisms which modules/functions affect which variables). The reason is that dynamic memory allocations are not available, and stack space is typically rather limited. Even without the latter restriction, passing around pointers to local, stack-allocated objects, just for the sake of avoiding global variables, is not an improvement in terms of code readability. In addition, offsets for global variables are stored in ROM code and presumed incorruptible. This way of writing software is *very* different. One criticism of Toyota's code was that it did not implement *mirroring* of critical variables. This technique is apparently considered mandatory for safety-critical software, and involves keeping consistent copies of variables: writes target both copies, and reads access both copies, compare them, and the system is supposed to enter some fail-safe state if a mismatch is detected. (Of course, C compilers are free to optimize away the redundant copies because their existence has no provable effect under the semantics of the language.) I find this very difficult to believe that anyone would do that, but this technique is described in [slide 35 of this PDF](http://www.safetyresearch.net/Library/BarrSlides_FINAL_SCRUBBED.pdf), apparently from the same person who is quoted in the article. It's a totally different way of writing software.
The generated LLVM-IR that `rustc` produces is not very good, and LLVM has to work quite a lot to turn it into efficient code. And LLVM does so amazingly well (which is why compiled and optimized Rust is comparable to C w.r.t. speed), but it makes compilation take a little longer. Now that the language has reached 1.0 I expect there will be more time to make improvements in this area. Having said that, it is possible that the slowness originates somewhere else. Perhaps you could try compiling with the `-Z time-passes` option (I believe that is the correct option) and see if one of the passes takes a long time.
I'll add the note that it's not the most efficient or brief way to perform split whitespace. But it might be useful for other things.
Not *any*: one can mutate `Sync` types, like atomics. Also, considering that, it's worth noting Rust only takes an opinionated stance on (some) globals because they can cause memory unsafety, not because they're bad software development practice. use std::sync::atomic::{self, Ordering}; static X: atomic::AtomicUsize = atomic::ATOMIC_USIZE_INIT; fn main() { println!("{}", X.load(Ordering::Relaxed)); X.store(1234, Ordering::Relaxed); println!("{}", X.load(Ordering::Relaxed)); } prints: 0 1234 
&gt; stack space is typically rather limited Also, not just limited, but much more non-deterministic than static allocations, and aspects of the system that aren't fully controlled are always bad in safety-critical devices. (Of course, there are additional ways to attack the non-determinism: outlawing recursion and dynamic stack allocation allows for literally computing maximum possible stack depth.)
Lints like that are never going to be perfect: there are always ways to trick the compiler into helping you less. Notably, all of those are a lot more suspicious for code-review than just `returns_result();`, which is what would be written without the lint.
Well yes, use &amp;str when possible. But when you do need to convert a slice to an owned string, I suggest using ```.to_owned()``` over ```.to_string()```. See my other comment for reasons.
Think about the writer too. It's not easy to write a good quality blog post and it's sometimes easier to break the topic into two parts. Ending on a "cliffhanger" means you will come back for more. Also in a hot topic like rust I'd consider publishing as soon as possible just so nobody can scoop the topic earlier. A lot of blogs are about self-promotion.
It is well known that we rely on llvm to deconstruct our wrapper types and functions though. Part of that will always be there as a cost of high level abstractions.
Lets distinguish between global and static variables. Static variables stay at a fixed location in ram and let you avoid stack allocation. You can still program modularly, but you lose reentrance. I use them all the time on tiny systems to keep the stack under control. Global variables go beyond that and bring all the problems of shared state. These you of course want to avoid.
Nice post! I've been meaning to read up on how to write lints, but there aren't many docs. This tutorial-style writing is exactly how I like to learn about this stuff! :) &gt; Oh, and if someone knows how to get correct/styled HTML output from github/jekyll, please PM me :-D I think I can help you. I'll take a look at your source, and either PM you or send you a PR. *Edit:* [PR](https://github.com/llogiq/llogiq.github.io/pull/1)
I still find it interesting that people at google are looking at rust.
I don't find it very descriptive: I would expect `&amp;T`, when a function `.to_owned()` to be called, to make a copy and return a `T`. Substituting `str` for `T`, this would mean that `&amp;str`, with a `.to_owned()` called on it, would return a `str`. On the other hand, `to_string()` describes exactly what it returns: a `String`, adjusted for the different convention in naming.
The behaviour of `thread::scoped` is irrelevant; what matters here is the method `rend.spawn`, and it takes a function which has been explicitly constrained `'static`. If you want it to be able to cope with non-`'static` data, then you instead want it to take a function of any lifetime and return a join guard of that same lifetime, just like `thread::scoped` itself does: pub fn spawn&lt;'a, F: FnOnce(Rendered) + Send + 'a&gt;(&amp;self, func: F) -&gt; thread::JoinGuard&lt;'a, ()&gt;;
I agree with what you said, but &gt; So we should just give up on making our tools better? Is not what I am implying. The point I was trying to make, and maybe failed to make, was that improving tools and having tools that help prevent issues is only useful when the developers using the tool want help. Better lints and language features absolutely help the "brilliant but distracted" developer from making mistakes. But the issues outlined in the article were not some single mistake that somehow made its way past lints and multiple code reviews. Lints and warnings can be ignored. Warnings elevated to errors can be dodged. Code reviews wont catch these things when the organization chooses not to have code reviews. What occurred and resulted in a death was the deliberate and willful disregard for code quality and diligence. And I stand by my assertion that no tool or language can help this problem. 
Maybe theres too much pessimism, but I think my point was lost; I'm not saying rust or the desire for better tools is pointless. I've tried to elaborate and explain what I mean to /u/kmc_v3 [here](http://www.reddit.com/r/rust/comments/38esyd/toyota_unintended_acceleration_and_the_big_bowl/crvawzt).
Merged! And thank you! I'm currently in the process of extending the post to make the lint actually useful. Btw. this is getting more complex than it looked when I started...anyway I'm here for learning, too.
I am *not* using thread::spawn
Short answer: http://doc.rust-lang.org/stable/book/lifetimes.html#lifetime-elision medium answer: elision doesn't kick in when you have zero arguments, but does when you have one argument. long answer: Rust has a feature called 'lifetime elision' that lets you not write lifetime annotations in certain cases. In the case of the function, elision will not kick in. This is because elision kicks in when you take arguments and return references. In this function, the only possible lifetime is `'static`, so you could have written `fn word() -&gt; &amp;'static str {` as well, and it would have worked. 
thanks a lot)
I'm glad I could help. Oh, it seems that I broke the URL you posted above, though. I removed the `date` field from your post front matter (as it is part of the filename), but apparently that was 2015-06-03 while the filename is 2015-06-04… Edit: Speaking of dates, Jekyll hides posts with dates in the future and drafts, you post is neither; i.e. your post is visible on your start page. Changing the date changes the URL currently, though. ([Here](http://jekyllrb.com/docs/permalinks/) are the docs on how to change the URL schema.)
Any time :)
And it's not like it would be impossible to make the `must_use` lint smarter -- either within the compiler or as a plugin. This is small potatoes compared to some of the pattern matching that sophisticated tools for C can do. We want to put that power in the hands of *every* developer, for free.
Programmers run Windows a lot actually. Many of us, me included, prefer Linux over Windows, but even those who primarily program in Linux will have to test their code in Windows.
Also on the front page as http://www.reddit.com/r/rust/comments/38hft5/new_commonmark_parser/
 let mut my_array: [[u32; 21]; 21] = [[0; 21]; 21]; 
That's exactly what I wrote.
 let mut my_array: [[u32; 21]; 21] = [[0; 21]; 21]; Otherwise the compiler says: error: use of possibly uninitialized variable: `my_array`
Hah, I just realized the same thing as I was reading your original explanation. I'll update the original example.
I had a copy/paste/internet error, please refresh.
I had a copy/paste/internet error, it now says the exact thing you said :)
Is there an RFC for this or some other source of information?
Thanks (also for the PR)! I sure plan on finishing the post (however I'm currently stalled on the issue on how to match an actual rustc::middle::ty::Ty. I have a [stackoverflow question](http://stackoverflow.com/questions/30648412/how-to-find-out-what-type-a-rustcmiddletyty-represents) to that effect and am investigating further. However, I think I'll first improve what I have by following your suggestions – especially around testing and finding out structure.
Answered the SO question :P
Hoedown has a single license (which looks similar to MIT), and that doesn’t seem to be a problem for using it in rustdoc.
Thanks! :-D
I don't have a definitive answer to your question, but [this article](https://opensource.com/business/15/6/rust-6-week-release-cycle) may shed some light about the rust release cycle, especially : &gt; beta and stable releases are only updated as things are backported to their branch So maybe there just hasn't been any updates (edit : I meant, backports) to the beta channel. If you really do want a feature that hasn't landed in stable rust yet, tell us what it is, and maybe someone more informed than me can help you. Or if you just want the latest code, you may consider switching to the nightly channel.
[I feel like I'm having difficulty saying what I mean once more...](http://www.reddit.com/r/rust/comments/2zd797/would_rust_have_prevented_heartbleed_another_look/cphwgnt) I'll try once more before I give up (at no fault of anyone but myself). There is a very big gap between &gt; forced them to use Rust (or s/Rust/any other tool/) and &gt; forced them to use sane software engineering practices I will totally concede that better tools can lead to better code quality and less bugs. I will concede and believe that Rust is an excellent tool that will help with code quality and bugs. But I do not believe that any tool would have stopped or prevented the issues outlined in the article posted by /u/lapinrigolo. Regardless of language, something with a brain made the conscious decision NOT to follow sane engineering practices. * Rust would not have prevented those engineers from deciding to use thousands of global variables. * Rust would not have prevented the design decision to allow for single point failures in the system. * Rust would not have prevented the engineers from consciously choosing to do nothing with operating system error codes (see the comment by /u/thiez) * Perhaps Rust would have restructured the watchdog to be bug free, but Rust would not have prevented the lack of tests proving the watchdog was or was not working. * Rust would not have prevented the spaghetti code function Task X, at blame for the death of a human being, from being overbloated, untestable and unmaintainable. * Rust would not have prevented the engineering group from choosing NOT to do code reviews. To put it simply, a tool is only as useful as those who wield it. And since these blatant issues are language agnostic, I strongly believe if they had wielded Rust, those individuals would not have made Rust useful.
I left a [comment](https://users.rust-lang.org/t/new-commonmark-parser/1690/13?u=brson) about the license. MIT does need to be added, for the same reason Rust is not Apache 2 only - it's not GPL2 compatible. Alternately, we can and do include single license BSD/MIT style code into Rust (as /u/SimonSapin pointed out), but I'd prefer to just use the same dual license everywhere when possible.
That makes sense. I was kind of hoping the compiler performance improvement would continue on the beta branch, since that was one of the things that appeared in the release notes of beta 1.1 that came out just after 1.0 stable.
Unfortunately you cannot simply pass "&lt; FILE" into your command's arguments since file redirection is done by your shell, while the Command API simply spawns a process with the given arguments. Thus you would have to do the redirection yourself. Given the current state of the API you could accomplish it by reading in the file contents yourself and sending them to the child's stdin. The example below reads in the file's entire contents at once, but you could definitely used a buffering strategy: use std::process::{Command, Stdio}; use std::fs::File; use std::io::{Write, Read}; fn main() { let mut child = Command::new("/bin/netcat") .args(&amp;["domain", "80"]) .stdin(Stdio::piped()) .stdout(Stdio::piped()) .spawn() .ok().expect("failed to spawn process"); let mut file = File::open("FILE").ok().expect("failed to open file"); let mut contents = String::new(); file.read_to_string(&amp;mut contents).ok().expect("read from file failed"); child.stdin.as_mut().unwrap().write_all(contents.as_bytes()); let output = child.wait_with_output().unwrap(); println!("{}", String::from_utf8(output.stdout).unwrap()); } I've written up an [RFC](https://github.com/rust-lang/rfcs/pull/1055) that seeks to make file redirection more painless, if you are interested in leaving some feedback for a future version of the API. But for now we have to live with the old API.
Sorry. Thank you.
I think the example under [*.mend_slices()*](http://bluss.github.io/rust-itertools/doc/itertools/trait.Itertools.html#method.mend_slices) is shorter. If not that, I at least love this one since mending slices again is so cool, and it's allocation free. The char -&gt; to char slices iterator mapping is a pretty cool trick of its own too.
Whoa, that's brilliant! Thanks!
There's also GCC's similar `warn_unused_result` for C, though that's declared on functions rather than types.
I hadn't heard of the ability to mutate `Sync` types, that's interesting. Though originally there certainly was some aversion to global variables on philosophical grounds, as Rust didn't have globals in any form until 0.2 or something.
Couldn't you just explicitly call `drop()` on it? See [this](http://rustbyexample.com/trait/drop.html). edit: another way to do it is to enclose your file handling in its own block (like the `_c` and `_d` variables in that example).
You can use `std::mem::drop` to immediately drop (and close) the file instead of waiting for its scope to end.
I can certainly empathize with developers putting out sub-par software in order to meet tight deadlines imposed by an uncaring manager. I've been there. However, we have to see a practical difference between software that is life-endangering and software that isn't. If a flaw in your software is likely to get someone killed (and the flaws in this software *did* get someone killed), then we as responsible engineers have an obligation to report reckless managerial practices to the authorities.
I prefer use std::process::{ Command, Stdio }; use std::fs::File; use std::io; fn main() { let mut child = Command::new("/bin/netcat") .args(&amp;["domain", "80"]) .stdin(Stdio::piped()) .stdout(Stdio::piped()) .spawn() .ok().expect("failed to spawn process"); let mut file = File::open("FILE").ok().expect("failed to open file"); io::copy(&amp;mut file, child.stdin.as_mut().unwrap()).ok().expect("Can't copy from FILE"); let output = child.wait_with_output().unwrap(); println!("{}", String::from_utf8(output.stdout).unwrap()); } to this. It won't read the whole file to memory, which might end up [crashing your code with large files](https://www.codeschool.com/blog/2015/06/04/how-a-bug-in-my-ruby-code-cost-code-school-13000/). Also, file.read_to_string(&amp;mut contents).ok().expect("read from file failed"); [errors](http://doc.rust-lang.org/nightly/std/io/trait.Read.html#method.read_to_string) If the data in this stream is not valid UTF-8 then an error is returned and buf is unchanged. which is probably undesired if you're sending binary files.
Yup, rustc is incredibly naive at the moment. Nothing concurrent or parallel, nothing cached, nothing incremental. I expect huge gains in the year to come as these are addressed.
What kind of control do you need? The file will be closed once the scope of its owner ends. If you need it to live for less time, you can introduce a new scope where it will live. If you need it to live for more time, you can move the ownership of the file to a new owner. 
Bringing down compile times is a large focus right now. Trust me, the Servo and rustc devs are suffering more than anyone from this. :) From what I've heard, compiling on today's nightly Rust should take 70% of the time that compiling on 1.0 takes. And we're still not doing anything clever like parallel codegen or incremental compilation, which should both produce huge wins on their own.
No need to hope that it's being worked on, there are indeed several projects in the pipe that should begin bringing down compilation time dramatically. At least two 10% speedups have landed between today and 1.0, along with numerous other minor speedups. nrc is looking into reviving the parallel codegen patch from last year, brson is working on optimizing linking, pnkfelix is working on non-zeroing drop (which isn't intended as a compilation-time improvement, but will probably result in one anyway), niko is looking at removing accidentally-quadratic behavior in typechecking, and I'm idly toying with a compilation mode that in the short term should let you typecheck your entire dependency tree without needing to do any codegen or linking at all, and in the long run this same work can be leveraged in order to perform building dependencies in parallel efficiently. You're right in that there's no fundamental reason that compiling Rust needs to be slow. If D can compile quickly with DMD, then there's no reason that rustc can't be just as quick (though the effort certainly isn't trivial).
TIL about [Iota in Go](https://golang.org/ref/spec#Iota). While I can't really see myself using this very often, it looks like it solves its problem elegantly. Perhaps it would be helpful to show a Rust-to-Rust example in the docs also. Show how it would need to be done without using Iota, then introduce it and show how it cleans up the code.
I was promised zero cost abstractions!!!
I'm very interested in this as well, though it may still be too soon for this use case to have seen any degree of polish.
`./mach build-cef` will produce shared libraries named `libembedding-*.so` in the `ports/cef/target` directory. You can use these through the CEF API. This is a C API, so these are C-compatible shared libraries, not rlibs. This isn't very documented yet, but you can find some nascent docs at in the [README](https://github.com/servo/servo/blob/master/ports/cef/README.md) and (possibly outdated) example code in [miniservo-gtk](https://github.com/glennw/miniservo-gtk). You can also compile Servo as a Rust library, though the [API](http://doc.servo.org/servo/struct.Browser.html) it exposes is not guaranteed stable and is probably not complete, and not really designed for embedding the way CEF is. But if you wanted to expand it, [servo/lib.rs](https://github.com/servo/servo/blob/master/components/servo/lib.rs) is the starting place. [doc.servo.org](http://doc.servo.org/servo/) has API docs for Servo internals and dependencies. Each folder in the `components` directory is a regular Cargo crate, and you can point Cargo at them with "git" or "path" dependencies. Note that Servo uses unstable Rust features, and will probably only build with the version of Rust specified in the `rust-snapshot-hash` file, or possibly a nearby nightly.
I wanted to keep my example simple for the OP, but those are all definitely valid points! Also I wasn't aware of `std::io::copy`, thanks for pointing it out!
I cleaned up some of my code in servo really nicely after reading that article. Thank you!
I'm on nightly and I'm still spending nearly as much time compiling as I am writing code. Glad to hear that there's work being done though.
Yes! It means grabbing a copy of the source code somehow and building it yourself, shoving the resulting binary somewhere, and then configuring Atom's racer plugin to look in that somewhere for Racer.
Good idea!
But given that the company that we are discussing did not use code reviews, that is irrelevant. I'm not saying `#[must_use]` is useless, I'm saying it would not have made a difference in this case. The compiler warnings are cool when you actually pay attention to them, but I do not believe using Rust would have made a (positive) difference for Toyota.
Hey, it looks Rust's std caught up! https://github.com/rust-lang/rust/pull/24029
There is no single file that is kept up-to-date, but I just ran this: $ git grep '#!.*feature' | cut -d: -f2 | sort -u #![cfg_attr(not(target_os = "android"), feature(path_ext))] #![cfg_attr(target_os="linux", feature(page_size))] #![cfg_attr(test, feature(alloc))] #![feature(alloc)] #![feature(box_syntax)] #![feature(box_syntax, core, rustc_private)] #![feature(collections)] #![feature(collections, exit_status, fs_walk, path_ext, slice_patterns, test)] #![feature(convert)] #![feature(core)] #![feature(custom_attribute)] #![feature(exit_status)] #![feature(filling_drop)] #![feature(hash)] #![feature(libc)] #![feature(link_args)] #![feature(negate_unsigned)] #![feature(optin_builtin_traits)] #![feature(path_ext)] #![feature(plugin)] #![feature(plugin_registrar, quote, plugin, box_syntax, rustc_private, collections)] #![feature(rustc_private)] #![feature(rustc_private, ip_addr)] #![feature(slice_patterns)] #![feature(start)] #![feature(std_misc)] #![feature(step_by)] #![feature(step_trait)] #![feature(str_char)] #![feature(thread_local)] #![feature(unicode)] #![feature(unsafe_no_drop_flag)] #![feature(unsafe_no_drop_flag, filling_drop)] #![feature(zero_one)]
Neat.
I think `git grep` does not also search our Cargo dependencies.
No, the authority in this case would be something like the [National Highway Traffic Safety Administration](http://www.nhtsa.gov/).
&gt; I've seen this before. This must mean main is returning the result of File::create. I was thinking it would return nothing, but I guess I didn't really think that through. What if I add a semicolon? I think the copy/pasted blurb already has the semicolon by mistake.
It blew mine
One of the biggest improvements will come from improving the way data is stored internally. Right now, we have most of our data stored in one big object that gets passed around. A lot of this data can be stored on a per function basis, as it's only relevant on a per function basis. This allows many parts of the compilation process to be done in parallel over individual functions. 
&gt; But it would be rather annoying for programmers to have a smarter must_use lint This entirely depends how we make it smarter ;) e.g. I could imagine detecting cases like `[result()]` and `(result(),)` would not annoy anyone (well, they deserve to be annoyed if they are using those to silence the warning).
&gt; To me, any attitude that says that Servo will never exist on stable Rust is tantamount to saying that Rust will never be fit for use in Big And Important Projects I don't share this opinion. As long as Rust is continuing to add new features that provide benefits important to a large systems project, Servo will probably use them somewhere. Expecting all of Servo to work on stable Rust is, to me, expecting Rust to stop adding features that are interesting for a big project like Servo. That said, most of our dependencies work fine on stable Rust and I expect that to continue to expand over time.
Yes
Great writeup! Regarding compile speed, there's some good discussion happening in [this thread] (https://www.reddit.com/r/rust/comments/38hyyk/is_this_normal/) about all of the low hanging fruit waiting to be picked now that 1.0 is out. I'm also eagerly anticipating the upcoming proposal to add a `cargo check` command, which would run all the syntax, borrow, and type checking but not do any of the code generation or optimization that accounts for the vast majority of compile times. That should make iterating on your code much easier.
 21 21 $ 0 21x21 array of zeros. in J. wait where am I?
Couldn't servo eventually have a train model where stable servo works on stable rust and so on?
&gt;cargo check god yes please Where can one track upcoming features like this? I don't even know when the next release is.
Sorry if I'm repeating what others are saying, but thank you for posting that. That is...I mean...I just...wow.
In my opinion there is always room for more static analysis. Rust's type system helps with memory safety and thread safety, but there are other forms of correctness to care about. You could even push this to the level of having codebase specific static analysis tools to ensure that YOUR code has certain properties that you get to define. 
Well, the compiletime checking _is_ static analysis, so I'm not sure what exactly you mean.
```Perhaps you could try compiling with the -Z time-passes option``` How to pass arguments to rustc through cargo ? :(
Well, racer does work on windows, but couldn't get the atom plugin to work [windows 8.1 - 64 bit] :( . Tried the sublime plugin - it works , but sublime text ain't free :P . edit: got the atom plugin to work finally , somehow.
Enums where you need specific byte patterns? something like consts!{ FLAG1: u64 = (iota!()+1) * 0b1000000000000; FLAG2: u64 = (iota!()+1) * 0b1000000000000; ... } maybe? So here you can delete any FLAG without having to renumber anything. 
After Rust's unsafe semantics are formalized, it should be. The Rust compiler might not be able to do the proofs itself, which is where the static analysis would step in.
rustc_private is just a collective feature tag for all the stuff used in `rustc` like `middle::ty` and friends. Getting plugins to work on stable is hard; some schemes were fleshed out at Portland but nothing that could be done now. However, I bet that Servo could drop its plugin/rustc_private deps if we use syntex and disable all the lints. So Servo the browser engine might compile on stable some day. Servo the development environment (i.e., lints) might never do so.
Here's a solution for the `step` function that requires no unsafe, and doesn't do any unnecessary clones ([runnable source on playpen](http://is.gd/ZNXCEm)): use std::borrow::Cow; fn step&lt;'a&gt;(table: &amp;'a [String], state: &amp;mut Vec&lt;Cow&lt;'a, str&gt;&gt;, results: &amp;mut Vec&lt;Vec&lt;String&gt;&gt;) { if table.len() == 0 { results.push(state.iter().map(|s| (**s).to_owned()).collect()); } else if table.len() % 2 == 0 { // some complicated condition state.push(table[0][..].into()); step(&amp;table[1..], state, results); state.pop(); } else { let s = "some new thing".to_owned(); state.push(s.into()); step(&amp;table[1..], state, results); state.pop(); } } fn main() { let table = vec!["a".to_owned(), "b".to_owned(), "c".to_owned(), "d".to_owned()]; let mut state = vec![]; let mut results = vec![]; step(&amp;table[..], &amp;mut state, &amp;mut results); println!("state: {:?}", state); println!("results: {:?}", results); } The trick is to use `Cow`; although we can't get the lifetimes to match up, since `s` isn't borrowed, we don't *need* to. For reference, `Cow&lt;'a, str&gt;` is (with appropriate substitutions made) the same as: pub enum Cow&lt;'a, str&gt; { Borrowed(&amp;'a str), Owned(String), } 
You would use cargo, just like any other rust project. It should work correctly with the 1.0 stable rustc and cargo.
You can hit the github "Subscribe" button on this issue: https://github.com/rust-lang/cargo/issues/1313
&gt;You could even push this to the level of having codebase specific static analysis tools to ensure that YOUR code has certain properties that you get to define. Cf. [code-aware libraries with Roslyn](http://www.infoq.com/news/2015/05/Code-Aware-Libraries)
/r/botsrights
It isn't impossible, but it is hard and overly complicated, and thus hasn't been built yet. Pretty much by definition unsafe code is code which the compiler can't guarantee is safe.
&gt; Google has not helped. `rustdoc` search &gt; Google, most of the time, at least for Rust for now.
You can; just not with stable Rust. Servo builds with cargo normally (`cargo build` in components/servo, ports/cef, or ports/gonk depending on what you want to build). The caveat is that we pin to rustc versions, but these days it's much more flexible and variations in your rustc version are somewhat okay. If you use the same or similar compiler hash that we do (the nearest nightly almost always works), using Servo as a cargo dep with the paths set should work. We don't expose a rust embedding API though IIRC (libembedding from ports/cef is a C api). We may in the future. Edit: Also, you can just piggyback our build system. `./mach cargo` and `./mach rustc` provide cargo/rustc using the downloaded snapshots, and can be used anywhere.
cargo rustc
That’s correct. Fixing it would need cooperation from Cargo: https://github.com/rust-lang/cargo/issues/1179
Adding new APIs is good, continuously replacing existing APIs instead of improving them is not. Sure, you should deprecate the API if you're truly abandoning it in favour of something new and have no intention of maintaining it, but preferably just stop replacing stuff with new-and-shiney. It's like ADD.
I don't think that works on 1.0
C.f. [Servo's lint army](https://github.com/servo/servo/tree/master/components/plugins/lints) :)
In a recent discussion, someone posited we were not doing "real static analysis", their counter-example being Coverity, because it's so much more complex. :-P I share your sentiment, and yes, `humpty_dumpty` is at least a few orders of magnitude more complex than any lint in rust-clippy – it does actual flow analysis. I just wanted to bring the point across that the things we do here are mostly low-hanging fruit (in order not to scare away newbies :-D), and we are already catching a good number of error classes.
"For our 2400 loc it takes 20s for a dev build and 70s for a release build." :( :( :( 
But I love iterators! ;-)
&gt; but half the API shouldn't be deprecated right out of the gate (1.0). I agree! Fortunately, there's only 4 deprecated things in 1.0's standard library: - [`std::collections::btree_map::Entry::get`](http://doc.rust-lang.org/stable/std/collections/btree_map/enum.Entry.html#method.get) - [`std::collections::hash_map::Entry::get`](http://doc.rust-lang.org/stable/std/collections/hash_map/enum.Entry.html#method.get) - [`f64::is_positive`](http://doc.rust-lang.org/stable/std/primitive.f64.html#method.is_positive) - [`f64::is_negative`](http://doc.rust-lang.org/stable/std/primitive.f64.html#method.is_negative) The first two are deprecated because it was found that they don't work well in practice. And, yes, the second two are named wrong, but this was only discovered close to the 1.0 release, so, to avoid leaving people high and dry, they were deprecated instead of removed. Doing the latter would've been worse. &gt; If it never worked, then it was wasn't properly tested, and shouldn't have been marked stable to begin with. We should never have a scenario where we go Experimental-&gt;Stable-&gt;Deprecated in the span of a few months. "Stable" is supposed to mean its safe to use, not that we're going to have the rug pulled out from underneath us next month. Definitely agree! The 1.0 release was very much a special case: it didn't have the full train process operating (because it was the first one), and the new subteam organisation should help too. Also, there's always the possibility for mistakes. Even C++ has defect reports.
That works - thanks!
&gt; Edit: It appears to be back[1] . NB. that thread was started 6 months ago. Compiling the following indicates it's still gone: fn main() { vec![0u8] + &amp;[]; }
And then let's examine the Linux kernel or firefox and see if they can build without compiler specific extensions!
Move along, move along; Nothing to see here, just some guy's brain exploded after it failed to launch from his cranium and instead leaked from his ears
That's exactly my point - the compiler already does so much static analysis, is there room for third party tools to improve upon it?
Good point. Since the compiler has to take a step back for unsafe a static analysis tool would be useful for that.
&gt; flow control analysis and static evaluation which would allow us to find even more bugs. Sounds awesome. I look forward to the blog post.
"Static Analysis": Analysis of code without live execution of the code. Technically syntax highlighting is a static analysis. Anybody who invokes the "No True Scotsman" argument against it **really** needs to define a new term for state of the art static analysis.
Thanks, I'll give it a look. edit: Grey on white though, might consider changing that for people like me with poor vision :P
For Rust 1.x, it'd be analyzer only since disallow leaks would be backwards incompatible. Depending on how leak analysis works and how ergonomic it is to prevent them, it could possibly be integrated into a Rust 2.0 like thing. But without spending a decade on getting it ergonomic, I don't think you'll see it in a language.
Fixed, now it's (almost) black. Sorry, I was just using the default styles so far.
There is a ton of useful static analysis that could be done, even of safe code. For instance, doing flow control analysis of anything that could produce a panic, such as indexing past the bounds of arrays, trying to index into an `&amp;str` with an index that isn't guaranteed to be on a codepoint boundary, etc. Adding in, well, static asserts, and then doing analysis of whether they are possible to trigger. The static analysis features of Rust the language can pretty much only cover ground that it is realistic to require every program to pass. However, there are far more guarantees that would be nice to provide statically, but would be far too cumbersome to work with for every piece of code, or wouldn't be able to cover all uses of the language but would be able to cover some useful subset of programs. It's true that several Rust features mean that the bread and butter of static analysis tools, such as chasing down uninitialized data, use after free, double free, buffer overflows, lack of null checks, and so on don't really apply. But I'm pretty sure that there is plenty of room for things that the language itself still can't cover, such as array bounds checks (in Rust, it's guaranteed that that will panic rather than corrupt memory, but you'd rather find it out at compile time), analysis of unsafe code, pointing out code paths that can lead to a panic, etc.
Nice! On a related note, one of the things that slows me down when working with vecs is remembering what I can do with an iterator, and what I can do with a slice (or more specifically, finding the docs that remind me of the method names). For some reason I could only ever find the page http://doc.rust-lang.org/nightly/std/primitive.slice.html by accident, so I bookmarked it specifically (though now that I check again, it appears to show up as the third result when you search for "Slice").
The easiest solution would be to unwrap the results: fn main() { let num1str = "123"; let num2str = "345"; let num1 = num1str.parse::&lt;i32&gt;().unwrap(); let num2 = num2str.parse::&lt;i32&gt;().unwrap(); println!("{} + {} = {}", num1, num2, num1 + num2); } Since the use of unwrap ist not really nice, you should handle the errors (see https://doc.rust-lang.org/std/result/)
I suppose using .ok() would be the safest route?
Well, "safe" is relative. `.unwrap()` is safe in the sense that it doesn't allow you to ignore errors; it just kills your thread in the process. So let's say: `.ok()` is *less crashy* than `.unwrap()`. :D The *safest* approach is really to either handle the errors, or pass them up to the caller.
Wouldn't fine-grained unsafe blocks help isolating what's actually unsafe? Or do you think they add too much noise? But perhaps the editor/IDE could highlight what are the actual unsafe operations inside a large unsafe block.
And that expands, the reason (at least the way you can think about it, idk how the compiler really works :p ) that references work the way they do is because `&amp;` implements `Copy` so it can be copied, and if you try and drop it, it does nothing (like you would expect with a `Copy` value) and `&amp;mut` does not implement `Copy`, so it has the same move semantics as a regular non-ref value. I just love how everything works the same in Rust, very little special cases.
http://blog.burntsushi.net/rust-error-handling/ is what you want on that topic.
`partition` is generic over it's output, it returns anything that is `Default + Extend&lt;D::Item&gt;`. Do maps require something more sophisticated?
In the same sense that it is possible to prove things about C code. For example, the [seL4](https://sel4.systems/) kernel is [formally verified](https://sel4.systems/FAQ/#verif) to meet its specification, and its compiled binaries are proven to be correct translations of the source code. To do something like this you need a formal model of the programming language. There is an attempt to formalize (a reasonable subset of) Rust called [Patina](https://www.reddit.com/r/rust/comments/31kq70/patina_a_formalization_of_the_rust_programming/). Instead of using an external static analysis tool, an approach is to embed the proofs in the Rust code itself (perhaps using `#[attributes]`, comments, or new syntax). That's what [LiquidHaskell](http://goto.ucsd.edu/~rjhala/liquid/haskell/blog/blog/2013/01/01/refinement-types-101.lhs/) does, encoding refinement types in Haskell comments. LiquidHaskell's refinement types let you assert [things like](http://goto.ucsd.edu/~rjhala/liquid/haskell/blog/blog/2013/03/04/bounding-vectors.lhs/) "this function receives a vector of size len, and an index between 0 and len": your program will fail to compile (with a type error) if the LiquidHaskell's SMT solver can't prove that the function is never called with an index that *might* be outside the range (it might be conservative and reject some correct programs, but must never let a program with a type error to be compiled). Appropriate use of such techniques can prove the correctness of some unsafe programs (for example, that array access without bounds check is correct). A yet another approach is to embed a full blown theorem prover in the language; that's what [ATS](http://www.ats-lang.org/) does. ATS is as low level as C but it has a formal specification, and it also let you write proofs about the correctness of its programs. [Here](http://bluishcoder.co.nz/2014/04/11/preventing-heartbleed-bugs-with-safe-languages.html) is an article on how proofs written in ATS could rule out, at compile time, a bug similar to Heartbleed: &gt; The code does runtime checks that constrain the bounds of various length variables: &gt; if payload_length &gt; 0 then &gt; if data_len &gt;= payload_length + padding + 1 + 2 then &gt; ... &gt; Without those checks a compile error is produced. The original heartbeat flaw was the absence of similar runtime checks. The code as structured can’t suffer from that flaw and still be compiled.
I think you're right. It could be helpful to know that the unsafe bits happen in one of those 3 points, but in this particular example it isn't. It also breaks the flow of reading the function. If a tool can tell which expressions need the unsafe block, one can have coarser unsafe blocks and mark the specific unsafe calls with syntax highlighting. Just like you can have a tool that tell the type of each expression under the cursor (like Haskell's ghc-mod) without littering the code with types.
Awesome thanks :D That worked, will remember the part about lexical borrows.
Yep, as mentioned elsewhere in this thread, `drop` simply takes ownership of a thing and then does nothing with it. References don't own what they point to, so it has no effect (it would be obviously unsafe if it did).
Thanks! 
Actually, the `char` type takes up 4 bytes, and encodes the Unicode code point. `str` is a variable number of bytes (as evidenced by the unsizedness).
Parallel codegen (among other things) to the rescue: https://github.com/rust-lang/rust/pull/26018
That's indeed serious, I hope this can be fixed in a backwards-compatible way (but a case can be made that this is just a bugfix).
Thank you!
It would be super cool, for sure 👍
Nope, its a normal struct. pub struct Texture { id: GLuint, width: u32, height: u32, } Comes from https://github.com/PistonDevelopers/opengl_graphics/blob/master/src/texture.rs
You could write your own closeable trait and impl it for Files. Something like `fn close(self) { }`, which is the same as drop but can be called as a method, preventing the bug where you accidentally drop a reference instead of the actual file.
I take it by your phrasing you don't have much hope for me. :P
I would absolutely **love** to undergo a project like this! Unfortunately, I'm entirely bound by my internship this summer. This is the reason I posted!
[indeed](http://i.imgur.com/YwGX0Is.png) ([here](https://github.com/rust-lang/rust/pull/26023#issuecomment-109295341))
I guess I'll start compiling a list of rust code I'd call 'complex' and put it up somewhere where the community could 'translate' it at will! Hmm. If only there were some kind of website on the internet which allowed you to make posts and get feedback as comments...
Wrong subreddit, you want /r/playrust, this one is for a programming language.
Can't any of those trait parameters become associated types? 5 parameters is a bit too much..
My understanding is that you require HKT to generically handle any of the handler/proxy types we use pervasively throughout collections. Off the top of my head this means: no `iter`, `iter_mut`, `drain`, or `entry`. For a vague reason, consider that all of these types have a lifetime equal to the borrow that occurs when they are created. Now try declaring that lifetime with respect to the Map, and no actual code in sight.
crates.io is updated: https://crates.io/crates/piston_window The piston core was also changed a bit in to allow using the `From` trait. Window back-ends now uses OpenGL 3.2 by default if it is not set in `WindowSettings`.
Good job. Some tips from a fellow sodoku solver: * make your brute force act over the fields with the least number of possible combinations *first* * bench against [this sodoku](http://pastie.org/pastes/10226024/text?key=zvbtpaj3itc32cjkqvtaq) which I've found to be the most difficult one possible to brute force (after looking up quite a few) Let me know if you find a harder one! * hashsets are allocated; use a [bool; 9] in your field stuff. (though I guess you might want to be generic over the sodoku size) You can start making a lot of stuff Copy and work recursively over the stack. Very efficient. Here's a (crappy) [solver I wrote](https://github.com/ebfull/bad-sodoku-solver) a while ago, I just got it working on the current rust. Surprisingly [few breaking changes](https://github.com/ebfull/bad-sodoku-solver/commit/98d3abd8a02390d8460f2020b020597da76de782) in rust since then!
&gt; PistonWindow uses Glutin as window back-end by default, but you can change to another back-end, for example SDL2 or GLFW by changing the type parameter At some point I'd like to see a high-level overview of the Piston ecosystem to give me a better idea of what all the moving parts are and what their respective roles are. It seems like it spans several layers of abstraction, so it's hard to tell what parts are "high-level" are what parts are more fundamental.
If the only expected effect of `drop` is moving ownership, I think passing a reference (so that will not cause any ownership movement) can be detected as an error by a static analysis pass. 
Now I totally want to use this to make a puzzle generator, and use this to solve/provide hints! I haven't dug too deep, but is it possible to specify the sudoku string with some sort of delimiter? Something such as: "___|2__|_63 3__|__5|4_1 __1|__3|98_ ___|___|_9_ ___|538|___ _3_|___|___ _26|3__|5__ 5_3|7__|__8 47_|__1|___"; Its slightly easier for my brain to debug instead of counting `_`s :)
Which are practical numbers (or estimations) on how much cpu/memory/io does Rust impl save compared to Go in your scale?
In `src/lib.rs` there is a `from_reader(..)` function, where you could filter `line` by `|c| c != '|'`, then adding pipe characters doesn't change parsing.
Yeah, I see that the `eclectic` library did it for me.
For those wondering: the Rust versions are being compiled with either `opt-level=3` or `--release`. I also had a quick look at the Rust Brainfuck benchmark; there doesn't appear to be any algorithmic differences between it and the better performing D or Nim versions.
If there's an otherwise safe operation that is unsafe to do because of some unsafe calls prior, until it is safe to do those operations again, it is still unsafe. The idea here is that I should be able to do anything I'd normally do outside of an unsafe block. E.g., imagine returning before the reference drops in the blogpost's example. I've now got an unsafe struct claiming to be safe caused by doing an otherwise safe operation in "safe" code. Why? Because I didn't actually guarantee that my unsafe blocks left the world in a safe state.
I like this idea... I just implemented it (see the [example](http://aochagavia.github.io/sudoku/sudoku/#example) in the documentation)
Matmul is doing a bunch of inefficient things with vectors. (EDIT: fixed part of it: https://github.com/kostya/benchmarks/pull/29) Cryptographic hashing could be an issue for hashset. Also, I didn't look closely but the memory measurement seems to be done for the entire process, which might count binary size and stuff. I shall look more closely when I'm on a computer
IMO, this should have been asked on StackOverflow or IRC.
&gt; Can we please have explicit links to the docs for primitive data types in Rust's docs? [It already has.](http://doc.rust-lang.org/std/#primitives) 
I love Rust, but I find it worrying that it is often blown out of the water by D, Crystal and C++ in the benchmarks I linked, when it emphasizes performance. It's almost **three times slower** than Ruby in the Base64 benchmark! Why is that?
The Json test should be using Serde, it's well known that the older implementation in libserialize is not as fast. Brainfuck test uses HashMap, I know it's rather slow: http://www.reddit.com/r/rust/comments/2l4kxf/std_hashmap_is_slow/. Might work better with BTreeMap, I dunno. Just my two cents.
/r/playrust
I may be wrong, but I think is this because: 1) an API added to the standard library is harder to change because of the stability promises 2) Since `std` is statically linked, wouldn't adding more and more things to it increase binary size significantly? Like you, I'd love to see more stuff readily available in the standard library, but crates offer more room for change. EDIT: missing word
&gt; 1) an API added to the standard library is harder to change because of the stability promises There are good reasons to put std APIs through a rigorous process, but *when we begin to add wrong reasons to it* (like some people saying "nah, let's not put it into std **because we can put it on crates.io**", replacing a logical agrument with some kind of strange crates.io-policy or std-fear) *then we're taking it too far*. &gt; 2) Since std is statically linked, wouldn't adding more and more things to it increase binary size significantly? No, LLVM pulls only the used functions into the binary, AFAIK. That's not any different from pulling in the functions from the crates.io crates.
It's okay, it happens fairly regularly.
&gt; Brainfuck test uses HashMap, https://github.com/kostya/benchmarks/pull/26 Diff: +3, -3 Twice as fast. I didn't try any other ones, but given the timings in the README, I estimate it would make it roughly the same as the C++, with no other optimizations.
&gt;It's almost three times slower than Ruby in the Base64 benchmark! Why is that? I believe the Base64 demo would benefit from using serde. As for how Ruby is so fast: [pack.c](https://github.com/ruby/ruby/blob/trunk/pack.c#L953).
We must read different things. Also, remember that the demographics are rapidly shifting post 1.0. When these kinds of decisions were made, basically everyone was on board. Maybe today it's different, but it's not really possible to tell.
Serde isn't yet ready for the standard library, it still needs time to bake. This applies to most things that have been proposed for inclusion in the standard library. It's no secret that living outside of the main tree makes for faster iteration and better competition. APIs will be added to the stdlib in time. Given that the APIs we do add will be around for effectively ever, it only makes sense to take a measured approach.
usually I'm told that I should be using less.... :)
You mean after they see &lt;anon&gt;:1:1: 1:24 error: use of unstable library feature 'rustc_private': deprecated in favor of rustc-serialize on crates.io &lt;anon&gt;:1 extern crate serialize; error: aborting due to previous error ?
Piston describes itself as a user friendly game engine but it has grown into something that is not straightforward to use. There are so many crates that do different things and depend on each other in various ways. Just finding the documentation for a function called in one of the examples can be time consuming. Anything that makes it a bit easier to use is good.
Yes, they'll use the one on crates.io after they see the error message suggesting that they should. :) *edit*: to be clear, that internal `serialize` only exists because the compiler needs it, it's not part of the standard library, so I don't really understand your point.
We have the obvious best way to work with JSON, it's Serde. But it's not there yet. librustc_serialize is there. Newcomers to Rust keep using it without strong technical reasons to, because it's in the ~~standard~~ official library documentation. Perhaps it's just a temporary issue!
Then it should be in the standard library when it's done and the API is 100% stabilized. That is, until someone writes a possibly better crate...
You'll only get one, because I'm morally opposed to microbenchmarks and think they should all be consumed by gaping fissures in the earth's crust: https://github.com/nsf/pnoise Now, I hear that people are working on getting Rust into the TechEmpower web framework benchmarks (https://www.techempower.com/benchmarks/#section=data-r10&amp;hw=peak&amp;test=update), which probably meet enough criteria to elevate it from a microbenchmark to a minibenchmark and thus I don't wish it a fiery home in the planetary core. And we're probably going to get hammered at first because none of our web frameworks use async io yet (see that bit above about implementing different algorithms). But we'll get better at that, and it may very well prove actually useful!
No, because then you should be enabling LTO and other things to reduce binary size. Rust doesn't create a small binary by default.
&gt; There are good reasons to put std APIs through a rigorous process, but when we begin to add wrong reasons to it (like some people saying "nah, let's not put it into std because we can put it on crates.io", replacing a logical agrument with some kind of strange crates.io-policy or std-fear) then we're taking it too far. That argument has to be considered with context. People stating it won't necessarily write down the full background each time (maybe they should). The biggest factor for things to not be in std is the flexibility/maintanence/versioning: once something is stable in std, it's there forever, needs to be maintained forever and the std developers are on the hook for it. There's rightfully trepidation about adding functionality. And, anything in std needs to follow the same semantic versioning as the rest of the standard library (which means no breaking changes). On crates.io, a crate is versioned independently from the rest of everything else: it's possible to tweak designs and explore the space fairly freely, even maintaining multiple incompatible versions of the same functionality (e.g. `foobar 1.6`, `foobar 2.2` and `foobar 3.0`). This extra flexibility is *really* nice. Of course, it's nice when things are in std, but it is not free to add them. And, as soon as you're using cargo in your project, it's really easy to use crates from crates.io.
All languages read file as one piece but only C++ Implementation reads line-by-line...
&gt; It's almost three times slower than Ruby in the Base64 benchmark! Why is that? rustc-serialize is hamstringing itself: the base64 decoder handles fancy stuff like skipping newlines in its input, whereas the equivalent D/Ruby/... decoders only handle raw base64 (with or without padding). Removing that functionality (or changing how it is implemented) would allow it to handle characters four-at-a-time like all the others, and hence make it much faster.
&gt; "Broadly speaking, the Rust community prefers the standard library to be minimal at this time" You're leaping to broad conclusions here about the direction of the standard library based on incorrect assumptions. This quote is from Steve, and Steve is not on the libs team. Instead, try asking any of the following people about their philosophy on stdlib inclusiveness: * Brian Anderson (brson) * Alexis Beingessner (Gankro) * Alex Crichton (acrichto) * Steven Fackler (sfackler) * Andrew Gallant (burntsushi) * Marvin Löbel (kimundi) * Aaron Turon (aturon) * Huon Wilson (dbaupp) These are the people at the moment with the final say on what goes into the Rust stdlib. One of these, Andrew Gallant, is right here in this discussion saying things that you yourself agree with: https://www.reddit.com/r/rust/comments/38s1n3/why_is_rust_much_slower_and_memory_consuming_than/crxfi6h So please, just relax. :) The stdlib will grow as needed, when libraries have proven themselves mature and when a need to bless is felt. Your worries here are unfounded.
Thanks for getting it through, @dbaupp! I feel I understand better now the motivation of some of the Rust library authors regarding their RFC downvotes. --- On a side note, crates.io usage isn't as bright currently as it could be. As a library user, I see a big potential for the dependency hell in the current crates.io organization. Why, I've been bitten by it a few times (r2d2 using a wrong version of postgres, https://github.com/sfackler/r2d2-postgres/pull/4; Serde using an old version of something; crates using a different version of a common crate)! I had to adopt a different infrastructure (Docker) simply to have a stable foundation to work on, in Docker image having a working snapshot of Rust compiler and dependency graph, but every time I want to add a crates.io dependency I cringe in fear of solving dependency and dependency graph issues. Adding insult to the injury, every time I upgrade Rust I know I'll have to spend a day fighting Rust stability issues in the dependency graph. 1.0 haven't fixed this, there are still things to do when switching between releases (stable-&gt;beta, beta-&gt;nightly; example https://github.com/ArtemGr/rust-smtp/commit/b5aaece4b6d9a7a921fe05467b48c8c53b3137bf). Hopefully something like the deprecation RFC (https://github.com/rust-lang/rfcs/pull/1147) will fix this. That's not the primary reason why I want the standard library to evolve though. The primary reasons are: 1) If the standard library does something, it better do it good. If it offers a buffered reader, it better have an effective convenience method that people need around I/O all the time. Otherwise we're splitting the functionality, spreading the concern over different crates, making it harder to reason about and work with the API. 2) The new Rust users naturally look up to the standard library and there is currently a lack of documentation in pointing them to the crates.io. Using crates.io is a common motto in the community, but it isn't as obvious what to do when you're just using the Rust site. I think the most natural thing to have is for new users to find what they are looking for, e.g. the best tools in the standard library. As a Rust citizen I would like to hear that the Rust is going there and that keeping standard library absolutely minimal is not some new fashion in the Rust language design. And second, provided we're not yet there, I want to point out the gap (between the temporarily minimal standard library and the things on crates.io). Maybe it could be closed in other ways, adjusting the documentation, the Rust book, the front page, etc.
&gt; So please, just relax. :) All right ) Thanks.
Stuff like http://xania.org/201505/on-rust-performance is more useful for practical Rust speed measurements than microbenchmarks.
&gt; &gt; As a Rust citizen I would like to hear that the Rust is going there and that keeping standard library absolutely minimal is not some new fashion in the Rust language desing &gt; It's not a new fashion... it's been the intention for a while. It was never a goal to pile on a whole set of new functionality after 1.0, instead using crates.io to incubate, possibly adopting excellent crates into std possibly "promoting" them in other ways. (The timeframe being months/years not weeks.) To me it's several different and distinct categories: a) Keep std minimal. b) Add useful features into std after a careful evaluation (possibly from existing crates.io crates). c) "pile on a whole set of new functionality after 1.0". I think that (a) is a new fashion. In a software design it's an old idea certainly (reminds me of OS micro-kernels, BeOS, FreeBSD), but in Rust it's something that still emerges. I can hear people thinking (yeah, maybe I'm a telepath, heh heh): we're doing so good with crates.io, so why not make a micro-std? I'd like, however, for Rust to stay on it's track to (b) or at least not to have a split-mind between (a) and (b), because in the case of split mind it's likely the end users of the language and RFC authors who'd suffer. (c) I see more as reductio ad absurdum example, at least personally I never meant it that way.
&gt; to be clear, that internal serialize only exists because the compiler needs it, it's not part of the standard library, so I don't really understand your point. My point is that people search for json support in Rust, they find json::serialize here https://doc.rust-lang.org/serialize/json/, they switch to crates.io in order to fix the error the compiler gives. For all practical purposes they're using the standard library. Or if you don't like that definition, they're using a crate blessed by standard library documentation. They're still in the "i'm testing Rust standard library" mentality, that librustc_serialize is somewhere on crates.io is just an unfortunate detail. Finding Serde or something like that on crates.io needs a leap of faith (or extensive research, like finding all those Serde blogs). That people load librustc_serialize from crates.io because compiler told them to - it doesn't mean they really use crates.io. Not in the sense that they'll find what they're missing from the standard library there.
&gt; My arbitrary precision arithmetic library, Ramp, is about 2-3 times slower than GMP, which is pretty much the fastest library of its kind. Whoa, that's impressive. But are you saying 2-3 times slower for relatively small numbers? GMP is supposed to use different algorithms for larger numbers.
The biggest reason Rust doesn't produce small binaries is because it doesn't dynamically link, which you can do with -C prefer-dynamic and immediately puts the binaries in the C or C++ ballpark. The reason this is not the default is because static linking makes for easier deployment and doesn't break on ABI changes (Rust doesn't have a stable ABI). The reason Rust doesn't default to LTO (which, BTW, does not in my experience result in really tiny binaries) is very simple: it's incredibly slow.
That's it, and this is fine. I believe there are more important things to worry about.
I wonder what would happen if I took the Base64 encoding/decoding functions from the C benchmark and use them in the rust version. Also if the Base64 Ruby version is using more C code than Ruby code then what are you comparing? 
There are a couple of other things that bloat Rust binaries: all the landing pads from panics, and the __morestack checks. The latter can be turned off with -C no-stack-check and after stack probes are added should become the default. The former are something you still see in C++ but not C, and it makes the assembly output very difficult to read. However, it usually only results in double or less binary output, so even with this it's still in the same ballpark as C or C++. When the panic =&gt; abort toggle is added, this will become much easier to turn off. You can also sort of turn them off with -Z no-landing-pads, but I'm not sure this actually works without LTO and I think it ends up keeping a lot of the code around.
An issue is that language changes may prompt API changes, but it might not be possible to do it backwards-compatibly. For example, if HKT is ever accepted, it's possible that a lot of APIs will want to take advantage of it. Those in crates.io will be able to do so quickly, but for those in the stdlib it's more complicated. Which isn't to say this can't happen. In Haskell, recently we had a generalization of the list functions in the Prelude (not because language changes, but because people were in the mood for it), which is analogous to generalizing functions if HKT lands. But GHC doesn't have semver guarantees; if it had, that (and the Applicative-Monad proposal) would be a moment to call it GHC 8.0.
Since Option&lt;V&gt; cannot be easily size-optimized for non-pointer types, would it make sense to create a `SentinelVecMap` that can hold all but one sentinel value that is used in place of `None`? (For java, there are a few high-performance hash maps, e.g. Goldman-Sachs, fastutil, trove, that all have maps that make use of this, IIRC)
I've got two PRs that bring it _well_ below C++'s ~~speed~~ runtime and only 1.5x compared to the C one: https://github.com/rust-lang/rustc-serialize/pull/118 https://github.com/kostya/benchmarks/pull/30
I think it's subjective. IMO Piston is more of a framework that enables you to make your own engine easily. Kind of like XNA. You really can't compare it to something like Unreal or Unity in terms of abstraction.
This is already happening, over at [RFC PR #1122](https://github.com/rust-lang/rfcs/pull/1122) (language semver) and my own [RFC PR #1147](https://github.com/rust-lang/rfcs/pull/1147) (APi deprecation).
That appears to me to also be a microbenchmark (it's a port of a 99-line program).
Huh, I thought it was much larger. Sorry.
You mean "above"? Or "below C++'s *run time*"? **EDIT**: wait, that's still rustc-serialize. Nobody should be using that if serde works for them.
That second link is not to the standard library: it's documentation for the crates.io crate. This is true conceptually but also literally: the internal and external versions of many of the Rust Team's crates have diverged. (The standard library is rooted at /std, it would be /std/json if it were part of the standard library.)
Further, I'd bet the Ruby Base64 implementation is actually done in C, not in pure Ruby.
Will you call Serde an official library? Which one of the two projects would you recommend new users to use, Serde or librustc_serialize? What project, if any, you'd like to include in the standard library in the future, Serde or librustc_serialize?
Thanks
Thanks. &gt; I tell users to consider both. Usually something like 'rustc-serialize is the standard right now, but next-gen work is happening with Serde and you may want to see if it works for you.' Do you think if it would make sense to mention Serde on https://doc.rust-lang.org/serialize/json/ so that new users would consider using it instead of blindly using the older package? I consider making a PR.
Code size is not a relevant discriminant for a microbenchmark. What seperates different benchmark classes is the time it takes for one iteration. [Alexey Shipilëv](http://shipilev.net) (aka. /u/shipilev) lists the following classes in his 2103 Benchmarking talk ([PDF slides](http://shipilev.net/talks/jvmls-July2013-benchmarking.pdf)): * Kilo- &gt;1000s e.g. Linpack * ___- 1-1000s e.g. SPECjvm2008, SPECjbb2013 * milli- 1-1000ms e.g. SPECjvm98, SPECjbb2005 ("not really hard") * micro- 1-1000µs e.g. a single webapp request ("challenging but OK") * nano- 1-1000ns e.g. a single op ("...are the damned beasts!") * pico- 1-1000ps e.g. pipelining ("...") As the hot loop in this case falls into the millisecond range (if I have my math right), it straddles the line between millibenchmarks and microbenchmarks.
Why is nim so fast when it compiles to c? Every example that includes c it does beat nim but nim routinely beats out c++. Is c really that much faster than c++.
Using a BitSet for initialized state is akin to tombstoning, right? I would advise against that, from a memory locality/cache friendlyness standpoint. I have read (as well as written) a few hashmap implementations (in Java), and reordering (e.g. via Robin-Hood) consistently won out against tombstoning.
Laundry eating is back! Delicious :-)
Default static linking and default lack of LTO. Both can be flipped via a command line switch, giving a much smaller binary.
&gt; For all practical purposes they're using the standard library. Or if you don't like that definition, they're using a crate blessed by standard library documentation. They're still in the "i'm testing Rust standard library" mentality, that librustc_serialize is somewhere on crates.io is just an unfortunate detail. No, the more flexible versioning of crates.io crates is a *huge* pratical difference between things truly in the standard library and a crates.io crate like that. It is an "official" crate (i.e. maintained by rust-lang), but it's not part of `std`.
I may be wrong, but here's my take: C has a much slimmer stdlib than C++. It is therefore commonplace to make up for the lack of standard vector or string classes by allocating and manipulating memory yourself. Since you're doing it manually, all the details are exposed and performance pitfalls which would be very easy to disregard when using a fancy class, such as reallocating much more often than necessary instead of allocating the right amount of memory upfront, can be avoided more easily. That's just an example, but you get the idea. C++ abstracts more away from you and encourages you to use tools (standard classes) which may not be the best performance-wise but are more convenient to use. 
Filed. https://github.com/rust-lang/rustc-serialize/pull/120
Yeah, I kind of dislike that IO is so heavily influencing these benchmarks. For node's json example I can tell you that the benchmark is really for readFileSync. If it's native json was done well or poorly it won't have much effect on the test.
Wow, this is quite comprehensive, thanks Guillaume! My French isn't so great anymore so I can't parse most of this; but you seem to have covered most of the important stuff. I clicked the link expecting a blog post and instead I get a proper book thing :D
I sort-of expected a tutorial on Rouille.
I'm using [RustDT](https://github.com/RustDT/RustDT) and it works perfectly, including Racer. It even works with the Ellipse built in debugger. It is a work on progress and a lot of stuff is still missing. You can't for example run unit tests from Eclipse, which is a drag. But it's better than anything else out there (at least for people that prefer a GUI).
I took inspiration from the rustbook and mixed it with my webcv (the rustbook is too white !). Thanks for your comment ! =D
My understanding is that tombstoning is bad because they clog up the map and require you to traverse over a bunch of tombstones to find anything. However this isn't a problem for VecMap because stuff can only be in one location (its index). This is more like pulling the Option's descriminants out-of-line and compressing them. For insertion and random indexing *if the value was there* this will indeed probably incur a penalty. However if you random index and the value wasn't there, then you only hit the BitSet. In theory if you anticipate a lot "not theres", this could be more performant as more bits will fit in a cache line than Option&lt;V&gt;'s by a mile. However it should improve memory usage and *iteration* speed when Option has a non-zero overhead since you need to walk over all the memory anyway, and this can only improve the amount of memory that's walked over (and therefore the amount of cache lines accessed). Further, if the map is sparse you may get additional gains as a string of 0's allows you to actually *skip over* entire cachelines in the Vec. Of course you're incurring overhead doing a lookup into the BitSet at all, so that might wash out any cache benefits your access pattern has. Even such a trivial structure has such subtle usecase tuning!
And that piece of code I saw related to b64 decode isn't even doing any crazy tricks (in fact it is not optimized very well), it just uses a lookup table per input byte and branches like crazy. There are methods to do this faster - you can do a lookup per 2 bytes of input, branch only once per the input 4 byte sequence. And you can even do this with a whitespaced b64, since the result code of the 2byte lookups can inform you to do a slow decode using the method ruby decoder uses.
&gt; That's true, but we quite often see Rust behind C, C++, Nim and D in benchmarks that are posted here. To my impression this is often due to the fact that the benchmarks often do not measure the language performance directly but how good something has been implemented in some library. It is not surprising that Rust does not shine in a regexp benchmark when the Rust candidate is using a newly written library while everybody else is using a battle tested, highly optimized library written in C. Same thing regularity happens to benchmarks using HashMaps. These are not testing the language speed at all. They are just testing the speed of different hashing algorithms.
The Ruby implementation of base64 uses C. 
Hi! I updated this code to work with Rust 1.0: https://gist.github.com/audebert/d21572d389b7d6cdbb6a I'm also trying to write a generic lexer that takes either `std::io::Chars` or `std::str::Chars`, any idea on how-to that idiomatically in Rust? 
 thanks a bunch! That worked.
Hear hear! Btw. there is [awesome-rust](https://github.com/kud1ing/awesome-rust).
You are right! I found it. I am pretty curious about performance improvements
Serde is currently broken on the latest nightlies due to const fn changes in libsyntax. [Issue 85](https://github.com/serde-rs/serde/issues/85) is concerning, as well.
The implementation in the Ruby standard library still does all the error checking to recognize bad data, and even the variant that recognizes data in the original base64 specification is only 20% slower. The benchmark even triggers garbage collection in the Ruby case. No way this should be faster than the Rust implementation (even though the inner loop is written in C).
It's been improved [by /u/nwydo](http://www.reddit.com/r/rust/comments/38s1n3/why_is_rust_much_slower_and_memory_consuming_than/crxknij)
True. That was dumb copy-paste work.
The problem of C++ is backward compatibility. When the C++ Standard Library was assembled in '98 it juxtaposed 3 pre-existing parts: - I/O streams - String - STL A couple of adaptations were added to String and I/O streams to make them "STL" compatible, which is the reason `std::string` has a dual interface (indices/iterators), and the I/O stream layer was left as-is, maybe the only part of the standard library (of '98) to use inheritance. And in the name of backward compatibility, 17 years later, things have not changed, and C++ is cursed with its I/O layer. Why cursed? Because unfortunately its I/O layer does not follow the C++ philosophy of "You don't pay for what you don't need": - C++ I/O streams are all stateful, and must all use those "facets" things when formatting/parsing; there are some run-time optimizations there but... - C++ I/O is by default synchronized with C I/O; to do so, with the C I/O layer being oblivious, requires the C++ layer not to buffer [You can tune things](http://stackoverflow.com/q/5166263/147192) (*yes, it's been bothering me for nigh on 4 years already*), and then C++ I/O can perform well, but I wish fast I/O was the default.
&gt; That is, until someone writes a possibly better crate... Yep, that's what worries me. I/O streams in C++ are maybe the worst part of the standard library, but in the name of backward compatibility, they are here to stay.
mmmmmmm that sounds delicious. Even just highlighting the end of something's lifetime would be well worth the effort.
[Done](http://redd.it/38tx2j) !
Please help me out by submitting blocks of demonstrative code that would be helpful to have translated!
#####&amp;#009; ######&amp;#009; ####&amp;#009; Section 8. [**Singular they**](https://en.wikipedia.org/wiki/Gender-specific_and_gender-neutral_pronouns#Singular_they) of article [**Gender-specific and gender-neutral pronouns**](https://en.wikipedia.org/wiki/Gender-specific%20and%20gender-neutral%20pronouns): [](#sfw) --- &gt; &gt;Since at least the 15th century, *they* (used with verbs conjugated in the plural, as with *you*), *them*, *their*, *theirs*, and *themselves* or *themself* have been used, in an increasingly accepted fashion, as singular pronouns. This usage is often called the [singular *they*](https://en.wikipedia.org/wiki/Singular_they). It is widely used and accepted in Britain, Australia, and North America in conversation. At least one style guide has, in the past, advised against this use. &gt; &gt;* I say to each person in this room: may __they__ enjoy __themselves__ tonight! &gt;* Anyone who arrives at the door can let __themself__ in using this key. &gt;* "If *a person* is born of a . . . gloomy temper . . . __they__ cannot help it."— Chesterfield, *Letter to his son* (1759) &gt;*They* may be used even when the gender of the subject is obvious; *they* implies a generic (or representative of type class) rather than individuated interpretation: &gt; &gt;* 'Tis meet that some more audience than a mother, since nature makes __them__ partial, should o'erhear the speech — Shakespeare, *Hamlet* &gt;* There's not a man I meet but doth salute me / As if I were __their__ well-acquainted friend — Shakespeare, *Comedy of Errors* &gt;* If some guy beat me up, then I'd leave __them__. &gt;* Every bride hopes that __their__ wedding day will go as planned. &gt; &gt; --- ^Interesting: [^Gender-neutral ^language](https://en.wikipedia.org/wiki/Gender-neutral_language) ^| [^Singular ^they](https://en.wikipedia.org/wiki/Singular_they) ^| [^English ^usage ^controversies](https://en.wikipedia.org/wiki/English_usage_controversies) ^| [^Pronoun ^game](https://en.wikipedia.org/wiki/Pronoun_game) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+crxt0bb) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+crxt0bb)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
&gt; micro- 1-1000µs e.g. a single webapp request ("challenging but OK") sub-millisecond webapp response? 
I think that /u/pnkfelix is based in France, perhaps you could ask him to proofread if you need collaborators. :)
He doesn't speak french very well (I help him to translate stuff time to time). I'll ask to [/u/SimonSapin](http://www.reddit.com/user/SimonSapin) instead, if he doesn't mind.
Yes, it does, unless the function(s) that the address is passed to are inlined and their accesses to the pointer are of a form that can be 'canceled out' into direct references to the variable. edit: which is why a useful future optimization for immutable references to small objects (1 or 2 pointer-widths) might be to transform them at the IR level into pass by value - this is valid unless the object contains interior mutability.
Oh now that you mention it, "without interrupting his workflow" does probably indicate a person rather than a thing. Is singular "their" commonly used? It feels a little weird to use in certain phrases, but I guess it's because I'm not very familiar with the term :)
Could you add an example to show how you want it to look?
After seeing so many Rust for X developers (where X = some other language), it took me a good 5 seconds to realize that the title wasn't referring to the [French language "developers"](http://www.academie-francaise.fr/).
&gt; I'm morally opposed to microbenchmarks and think they should all be consumed by gaping fissures in the earth's crust: Who wants to make a micro benchmark suite and call it Korah?
&gt; this is valid unless the object contains interior mutability I'm not so sure it's valid. For example, the references can always be converted to raw pointers and compared with each other. Does Rust have any guarantees about distinct nonzero-sized objects having distinct addresses or something similar?
Ok, that's good to know. Thank you! I never imagined I would learn something in another language than Rust on this forum ;)
I think the author (who is french actually) did not write this intentionally. In fact, the french word for "user" is masculine and so the word-by-word traduction implied the use of "his". Your comment is still relevant. 
http://gradworks.umi.com/33/52/3352854.html
I want to say it came from the fact that as Rust was developed out in the open, it was not a good candidate for a "Good First Language", so it had to be a second/third/etc. language, and teaching that is more easily done by comparing it to another language. Rust isn't the first language I've seen like this. The things I've seen from Elixir that aren't starting from scratch either angle off of Ruby (because surface syntax similarities and author [Jose Valim] being one a large Rails contributor) and Erlang (because Elixir sits on the Erlang VM).
I didn't see it from this point of view. However, that's true : I don't think Rust should be a good choice to start learning IT (but that's another debate !).
It's easy to tell if you are comparing pointer values to each other or not. The compiler has the information - if the pointer values are useful, then it won't optimize into pass by value. Structs initialized at different places will have distinct addresses, even if they are zero sized. This is so that they can be freed independently of each other. The way of getting around that is to use multiple immutable references, Rc&lt;T&gt;, or other similar trick.
OK, yes, different things: you are upset that official but non-std crates still suck users away from better but non-official ones. This seems to go back to the discoverability problem. The alternatives I can see are: - not officially maintaining/publishing anything outside std - eagerly adopting alternatives to put them all on an equal footing The first is not workable (you yourself were rightfully annoyed about this happening previously), and I strongly suspect the second will result in lower average quality at the moment.
"This way, the user activity (and so the domain) can automatically be detected without interrupting his workflow." This reads to me as "without interrupting [the user's] workflow". And I can get that, but in most cases it literally takes a minute or two, to be more inclusive to everyone. It was important enough that the Debian project put it in their manual, I think it should be important enough for us.
Gaol is a (really interesting) library to create applications who sandbox themselves. StemJail brings some standalone tools to create custom jails for any applications. Moreover, it rely on a unique policy whereas applications using sandboxing techniques (like Gaol) brings their own policy. The two projects have different but complementary purposes. I plan to use Gaol to harden StemJail. More documentation (in french) : https://www.sstic.org/2015/presentation/stemjail_cloisonnement_dynamique_d_activites_pour_la_protection_des_donnees_utilisateur/
I actually ask this section because I'm a total noob in this ^^'. I just heard someone explaining Rust while he was explaining Heap/Stack and it was making some Rust concept obvious (because some Rust concept come up from Stack/Heap management).
You're right, it's fixed :)
Yeah, turns out a PR was all that was needed. I guess I should stop talking, doing works better :P
I really like this! Your writing style is really clear and the code seems very elegant. The only improvement I could come up with would be to replace the `#[doc = "blah"] ` attributes with doc-comments on the previous line, as in: `/// blah`. Thanks for sharing! :)
It *is* the case that something like buff[i++] = trans[077 &amp; (*s &gt;&gt; 2)]; in Rust will end up a fair bit slower than in C because the lookups will potentially panic and indexing with `i++` (especially the C-style `*ptr++`) doesn't really have a direct Rust analogue. The Rust code at that time used something like buff.push(trans[077 &amp; (*s &gt;&gt; 2)]); which isn't *expensive* but adds up really quickly in a tight loop. However, using iterators can get most of the way there. [The new encoder](https://github.com/rust-lang/rustc-serialize/blob/adbd254b14d8abb6ad5c9704af7a433eb3afcb00/src/base64.rs#L93) does this - the main relative speed and complexity losses to Ruby come from using a configuration object and the extra bounds checks on each call to `next` that the C can elide.
The pipe to Python trick is cute!
Ahh, indeed, good clarification.
How does the four-at-a-time approach work? shifts plus bitwise ors? Edit: Probably needs a table lookup in between the shifts and the ors.
I was referring to the entire apparatus with that statement. :) And I happen to disagree with TeXitoi there. I'd be happiest if the LTO option were removed from the compiler entirely.
Yep, it inlines the bitshifts and table lookups rather than separately pushing each one to a buffer and only handling that on each fourth repetition. (The C code used by Ruby linked above/below demonstrates the idea.)
3, three devs, ah ha ha ha!! (boom) I'd be keen.
Whhaaaaattt! Hey! 
You've hit upon the proper solution, which is to use the inline attribute. If a dependency isn't using it, that's a bug in the library just as much as any other performance problem would be. Papering over that bug with the nuclear hammer of LTO builds would only penalize us in the long run.
If you want the comment to be in the same line as the variant, `/** doc */ Variant,` might work as well.
&gt; I wonder if I believe programmers to think more positively of Rust than they really do. Don't worry too much about what other people think of the technologies that you use. :P Good products will speak for themselves, so focus on making something awesome if you feel the need to prove yourself.
I'm not looking for affirmation, I'm hoping software will have fewer security vulnerabilities in the future and still be resource efficient. Programming is far from my best skill, actually. Doesn't mean I don't try to stay aware of what's going on, but that's true for a lot of disciplines with me.
Overuse of `inline` is a really common antipattern in the Rust community at the moment. For one, it is very easy for it to pessimise code: overriding the compiler's own judgement. But more than that, it results in code bloat and slow compile times by forcing them to be instantiated in all downstream crates always, including non-optimise compiles. (The latter could probably be fixed, but it seems moderately nontrivial.) LTO completely removes the latter problem, making them entirely opt-in for when performance really matters.
I think those allocations aren't included in the timing loops. (It's `O(log2 n)` allocations, not `O(sqrt n)` btw. I.e. on the order of 23 rather than 3000.)
(A more sensible intro I guess - one that doesn't reference the count from sesame street) - I've been working on a small app for windows, using some of the windows libraries. I've found it a bit hard going on :) I'd be keen to catch up with Any other Rust devs in Wellington. 
"Overuse of `inline`" is only possible at the moment because the attribute is overloaded to mean both "make it possible to ever inline this item across crates" and "I want you to please try to inline this item". This is a bug in the language, and attempting to remedy this with a compiler flag is unwise. And the fix is simple: just one new attribute that means "export this for inlining, but don't I wouldn't be so insistent here if LTO wasn't so consistently unusable. And by unusable I mean *unusable*. And by *unusable* I mean **unusable**. Here's the time that it takes to compile `fn main() {}` on my machine: $ time rustc -C lto empty.rs real 0m5.630s user 0m4.567s sys 0m1.010s That's almost six seconds to compile a one-line file. That's absurd. Last I asked, nobody had ever successfully built Servo with LTO. Suggesting usage of LTO isn't just unwise and wasteful, it may very well literally be impossible if your project is large enough. Its very existence makes us a laughingstock and trying to suggest that anyone use it, especially use it casually, is harmful to the language.
In Rust `&amp;T`s are expected to be immutable which is backed up by LLVM assumptions. Writing code that breaks this using raw pointers can be UB. If you plan on using raw pointers, don't have any Rust pointers to the same thing alive at the same time.
*grins* I know the Enterprise gets a lot of shit, in many cases justifiably, but it's also fair to say that large companies have things they need to worry about that aren't so much on the radar of OSS developers. Since a lot of big systems software is developed by such companies, it's probably worth considering how our direction as a community affects uptake in those environments.
Pretty sweet! However, I think your claims of solving the TSP are a bit too strong. AFAIK, a genetic algorithm will generally only give you an approximation to the optimal solution, without performance guarantees. Still, cool to see this implemented in Rust!
&gt; In this case, you could also take an iterator How would that affect the body of the function? I have tried, but I can't make it work
Me (as of 2 days ago) and aatch (not sure of reddit username). edit: u/aatch according to rustaceans.org
I ran it through gzip or something: #![feature(core)] pub fn least_square_slope(points: &amp;[(f64, f64)]) { let x_mean = points.iter().map(|&amp;(x,_)| x).sum::&lt;f64&gt;() / points.len() as f64; let y_mean = points.iter().map(|&amp;(_,y)| y).sum::&lt;f64&gt;() / points.len() as f64; let sum_x_diff_squared = points.iter().map(|&amp;(x,_)| (x-x_mean)*(x-x_mean)).sum::&lt;f64&gt;(); let sum_x_diff_y_diff = points.iter().map(|&amp;(x,y)| (x-x_mean)*(y-y_mean)).sum::&lt;f64&gt;(); let slope = sum_x_diff_y_diff / sum_x_diff_squared; println!("Slope: {}", slope); } Only works on nightly, and is slower than your code (until LLVM merges the iterators somehow), but smaller code is more good, right? ;) EDIT: &amp;amp;
`IntoIterator` is cool, yes. But in this case it would not help much at the call site because a *borrowed* slice/Vec will only convert into an iterator over *borrowed* items and a Vec's `IntoIter` is not `Clone`.
What is an obsidian?
Thanks. :)
 points.iter().map(|&amp;(x,_)| x) I totally forgot that you could do this! Destructuring FTW! I don't have the right mindset yet.. I would have used `sum()` too, but I don't like to use "unstable" methods. So I stick with what is in "Stable" at the moment :) Generally I like shorter code, but only if it is easier to read. In this case I think I prefer @sellibitze way, because it looks so elegant and it seems like it's iterating a lot less, but I could be wrong because I don't know the internals of `fold, map, sum, ...` and how the compiler optimizes them
What would I have to change from @sellibitze's answer to make it work with `IntoIterator` ? I tried multiple things, but always get an error like: error: the trait `core::iter::Iterator` is not implemented for the type `[(_, _); 2]` [E0277]
Yes.
Very nice explanation! Thank you :)
Sorry for the click bait title! :)
Global variables can be structured with name prefix, and most languages have module-level global variables with some level of access control (so *global* refers to lifetime, not scope). Some coding guidelines require that the dependencies and effects functions have on global state is described in the function signature, and there are tools to enforce that the function implementation does not access global state in ways not expressed in the signature.
as marcusklaas pointed out, it doesn't really solve the problem, more like finds a sub-optimal route over period of time. Finding an exact solution to the problem is computationally impossible over a non-trivial number of cities. But now that you asked, I'm going to play with adding more cities to see how the algorithm behaves.
Raw pointers: - `*mut T` - `*const T` 'borrowed reference', aka a reference: - `&amp;T` These three are a single pointer. ------------- 'borrowed traits', aka, a trait object: - `&amp;Trait` slices: - `&amp;[T]` these two are a double pointer
Inline asm isn't part of the C spec either. Firefox uses loads of GCC / Clang extensions to C++ including custom static analysis plugins. So does e.g. the Linux kernel. Does that mean ISO C/C++ is "never fit for use in Big And Important Projects"? Well, maybe, but it doesn't matter a whole lot.
Alternatively you can use for-loops. This is more verbose, but possibly also more transparent. fn least_squares_slope(points: &amp;[(f64,f64)]) -&gt; Option&lt;f64&gt; { let mut sum_x = 0f64; let mut sum_y = 0f64; for &amp;(x,y) in points { sum_x += x; sum_y += y; } let n = points.len() as f64; let mean_x = sum_x / n; let mean_y = sum_y / n; let mut sum_xx = 0f64; let mut sum_xy = 0f64; for &amp;(x,y) in points { sum_xx += (x-mean_x)*(x-mean_x); sum_xy += (x-mean_x)*(y-mean_y); } if sum_xx &gt; 0.0 { Some(sum_xy / sum_xx) } else { None } } fn main() { let data = vec![(0.0,2.0),(1.0,4.0),(2.0,6.0)]; let slope = least_squares_slope(&amp;data); println!("Slope: {:?}",slope); }
I don't have an answer, but rust has nice benchmarking tools, so you can check for yourself. :) http://rustbyexample.com/meta/bench.html 
My point wasn't that you can't do a similar thing to `buff[i++]`, but that people *don't* because it's not idiomatic. The point being that the de-facto implementation in Rust ends up being slower than the de-facto implementation in C. Further, `*ptr++` doesn't have a Rust alternative, and does seem more optimal than indexing. Luckily iterators do near-enough the same thing as long as you don't need random access.
Hm, that doesn't help me. How about a tutorial for Rust developer who want to learn French instead? Seriously, this looks amazing!
&gt; Structs whose last field is dynamically sized, the extra data is that of the last field Is there an example of how to do that in Rust 1.0? I tried to do that (just playing around) but I couldn't figure out a syntax that compiled successfully.
I was kind of joking but not joking at the same time-- it does sound like there's a need for someone to bundle the stdlib and a set of libraries that would make it useful for a variety of enterprise applications and commit to keeping it up to date and supported. I'm not sure if volunteers from the community are the right people to do this though; I'd love to see a company sponsor/run this.
I actually wholeheartedly agree. I find them neat (hence writing about them), but despite numerous attempts I've never been able to do anything with them that couldn't be done another (simpler) way. There's a [well-known article](http://nothings.org/gamedev/l_systems.html) on this point that you might find interesting.
Nice catch on the `&lt;title&gt;` tag. I wrote my own Pelican theme, and I clearly missed some things. I'm a little unclear right now as to what is more idiomatic --- `vec![]` or `vec!()`. Is there a community preference?
I'm glad you learned something :) A good book showing what can be accomplished when modelling plants is [freely available online](http://algorithmicbotany.org/papers/sigcourse.2003/2-1-lsystems.pdf). It goes through the math a little more thoroughly than I have as well.
Nice to see people interested in that matter! Your readme is full of interesting details and is worth reading.
On that point, I can't really do much... Sorry :p
Nice! I'd be interested to see the performance of these implementations compared to ones using Rust bindings to OpenBLAS, etc.
 struct Foo { things: [u8], } fn main() {} 
I believe it actually has way more optimization opportunities due to not having mutable pointers aliasing for the most part.
If you refuse to use third-party libraries implemented in Rust if they use unsafe code unless they are C bindings, then people will just keep relying on those implemented in C. That's what I meant. &gt; The stdlib is well-audited and bugs are guaranteed to be fixed because bugs in the stdlib are effectively bugs in the language as far as Rust's safety guarantees are concerned. As far as Rust's safety guarantees are concerned, the same is true of third-party crates. I agree in general that the standard library is probably better audited, but it's by no means perfect. I suppose I can't convince you otherwise, but I *have* to use unsafe code in my crates at times (either in order to achieve acceptable performance, or in order to achieve anything at all), so what you're effectively telling me is that you refuse to use my crates. I hope you understand why I think that this is a too-extreme stance, though I certainly don't begrudge you the opportunity to make this choice. To be clear, I would *love* to be able to express everything I want in safe Rust, but I feel that the language requires extensions in order to be able to do so. (Not that I think my crates are currently in a state of readiness that I'd encourage people to use them, but I cannot envision a future where some of them are finished and do not use *any* unsafe code).
I must say that 90% of the time the long codegen times in Rust don't bite me, because I can go for hours writing hundreds of lines of code without even trying to compile. Then, I try to compile, taking 1-2s because of the almost inevitable type/symbol errors, fix the first few errors, compile again (~10 times). Once it compiles, it usually works at the first try. No other language I have used has been able to provide this kind of experience, not C(++), not Java, not Python, and definitely not JavaScript. (I haven't tried Haskell yet though) Rust can, and it's awesome. As for `javac` being "instantaneous": I reliably get these kinds of timings on [https://github.com/CrystalGamma/java-playground/blob/master/src/NestedClasses.java] on both my laptop and my desktop: $ time javac -d ../out NestedClasses.java real 0m0.541s user 0m1.327s sys 0m0.067s It's slightly worse when I use IntelliJ to invoke the compiler. This is ridiculous, considering how little a Java compiler actually has to do (seriously, google/duckduckgo/whatever for JVM Internals, Java instantly feels like assembler for the VM).
Yeah, I have a similar experience when coding on rust-clippy (apart from the long codegen – I have yet to write so much code). As for java, you should try ecj (eclipse's compiler for java) – it usually runs circles around javac. And also works in IntelliJ. Edit: While I do have some experience with Haskell, I think rustc is much more user friendly than either hugs or ghc. Apart from that, the lazy evaluation makes for a hard-to-understand runtime profile.
You may. ;-)
It looks like you're messing up something with pointers vs. values in your function signatures. What are the C signatures of the two functions you're trying to access through the DLL?
It's not a C dll that I'm trying to load, it's a Rust dll. The signatures match the two type defs, but should that matter? The crash happens at compile time, not at runtime, so if the function signature in the dll doesn't match what I have declared I shouldn't have a crash until runtime.
And you compiled both with the same version of rustc?
Yes, I agree: this is almost certainly a Rust compiler issue. The way to search for / report bugs is to go to github.com/rust-lang/rust/issues and search or create a new issue, with the test case and output attached. *Edit:* It looks like it's probably this issue: https://github.com/rust-lang/rust/issues/17257.
That's true. I meant it more as a put-things-in-perspective exercise.
Yeah, but sometimes it doesn't work after it compiles. I've been working with rust code to deal with an SQL database, so the compiler can only do so much. When there's an error I need to recompile for tiny changes to the SQL statements, every time. It definitely interrupts the workflow to some extent. It's such a great language I don't mind too much, because it made everything else so much easier (*so* much easier), but it is a pain.
Yes, though I'm still not sure how that would matter.
Didn't talk much about rust, to be honest. Just about flubs, which he could just call academic languages. It's true that languages like Haskell are often praised but rarely used, but that's kind of the point of rust, to be an academic language that feels like you're writing in something that's easier to use.
https://github.com/rust-lang/rust/issues/17257 looks like it's related to what I'm running into. I've added my situation to the list of causes.
Here's hope for `cargo check` that would just run the checks of the compiler without actual codegen.
Rust does not have a stable ABI, so a dynamically linked library isn't guaranteed to work unless they were compiled with the same version.
If this issue is occurring at compile time, not when you run it, could you add the definition of `Engine` and `Window`? I can't seem to reproduce using just `struct Engine; struct Window;` and then the code you pasted.
 pub struct Engine { window: Box&lt;Window&gt;, renderer: GLRender, resource_manager: Rc&lt;RefCell&lt;ResourceManager&gt;&gt;, systems: Vec&lt;Box&lt;System&gt;&gt;, transform_update: Box&lt;System&gt;, light_update: Box&lt;System&gt;, audio_update: Box&lt;System&gt;, scene: Option&lt;Scene&gt;, close: bool, } pub struct Window { pub handle: HWND, pub dc: HDC, pub messages: VecDeque&lt;Message&gt; }
The 17th is a Wednesday, btw. Also, the 18th is the resurrected PostgreSQL user group day too, I just realised.
&gt; To use Rust for writing programs that can afford the overhead of garbage collection would be premature optimization. This is a 'meme' I see a lot, and I don't get it. I *like* writing Rust more than I like writing most other languages.
Some JOOQ-like abstraction over the DB that does some checking at compile time would help there, right?
Only if the changes were due to syntax or simple naming errors (unless you utilize model checking to verify correctness of your SQL statements with respect to your overall abstract data model, which is still very much active research territory--but even then, you might want to tweak the statements for performance).
Rust has (limited) operator overloading, which also makes for some expressive APIs. There are the `From&lt;_&gt;/Into&lt;_&gt;` traits which make for an awesome adapter pattern. There are iterators, local type inference (though it has failed me once or twice), `Deref` and friends, ... which together make Rust more expressive than other languages with its scope can achieve. Also expressiveness is not conciseness. Else we'd all programm in J.
Isn't image.draw() generic over a trait for the texture parameter though? I think it'd be fixed/asking for opengl Texture in particular because that is the 'Texture' type on the graphics variable used, but it is still a generic function. " texture: &amp;&lt;G as Graphics&gt;::Texture," is the signature you posted, so it isn't exactly a trait but it is an associated type in the method. Here's a post I made a while ago asking about auto-dereferencing with traits: https://users.rust-lang.org/t/pre-rfc-auto-dereferencing-parameters-passed-to-generic-functions-as-well-as-concrete-ones/702
I am, literally only been back 2 days, house is still full of boxes, but I have internets!
You're looking for /r/playrust. This subreddit is dedicated to the Rust programming language.
I'm not an expert, I can only tell you what I heard. I think yes, it was DMA. There already were some boards with 64 core epiphany test productions sent to kickstarter backers. I think I heard the problem with memory was actually even worse because the bus still was the same and shared between all cores. I guess they will address these problems in the future
If your data is in fact sorted and you want to dedup it, just call https://doc.rust-lang.org/std/vec/struct.Vec.html#method.dedup If your data is not sorted, consider just sorting it and calling dedup. If you want to modify dedup to have special logic, just grab the source and do so: https://doc.rust-lang.org/src/collections/vec.rs.html#1118-1201 If your data is impractical to sort, consider: let set: HashSet&lt;_&gt; = vec.drain(..).collect(); // dedup vec.extend(set.into_iter()); Otherwise if you just want a "cleaner" solution than calling `sort` and modifying dedup, consider: let set: BTreeSet&lt;_&gt; = vec.drain(..).collect(); for x in set { // data comes in in sorted order so you can further // process adjacenct elements like this if let Some(last) = vec.last() { if predicate(last, &amp;x) { continue; } } vec.push(x); } Or if you want a full view of the data: vec.sort(); for i in (0 .. vec.len()).rev() { if analyze_around(&amp;vec, i) { // O(1); but will scramble up the order of stuff we've // already seen vec.swap_remove(i); } } All in all there's quite a few ways to accomplish this, with the best choice depending on your data and constraints. 
Can you please change `&amp;amp;` to `&amp;`? It would make your code *so* much more readable. Also - can you please read your own post after posting and edit any *obvious* mistakes?
Well, this adage basically boils down to "Don't write a line of code unless your life depends on it", so I see little value in it. The bigger problem I see is that a single reported compile-time measurement (which devs really should look into, make no mistake!) as **THE TRUTH** about rustc's speed. Here's another measurement. [rust-clippy](https://github.com/Manishearth/rust-clippy) `cloc`ks in (sorry for the pun) at 1.8kloc, and `time cargo test` (which uses rustc for compiling *and* compile-fail testing, no less!) tells me: real 0m0.946s user 0m1.884s sys 0m0.560s So obviously there are places within rustc that deserve some more love, but I don't think that everyone should expect collossal build times.
Well, operator overloading is a delicate balance. If it's overdone (e.g. scala, some C++ codebases), it makes code really hard to read, and leads people to do *clever* things ("well, at least I thought I was clever when I wrote it"). Doing it right requires *taste*, which as of today is hard to teach. Other languages that allow for limited overloading like lua or python have seen good success with it. I have faith that Rust will continue to fall in that category.
&gt;&gt; I think yes, it was DMA wow ok, that's a bit worrying. 
&gt; We probably have to be careful not to be seen as the next Lispers or Haskellers. Do you have a proposed course of action for that?
OP specifically uses `vec![...]` as an example, but that's a good point!
&gt; How many different usecases can you cover with that single function? In a language with ad-hoc overloading you can cover ALL POSSIBLE use-cases IN THE ENTIRE UNIVERSE with a single function. ;-P
yeah, but your type signature is like fn accumulate&lt;'a, T: Monoid&gt;(tuples: &amp;[(&amp;'a str, &amp;Fn(i32) -&gt; bool)], i: i32) -&gt; Option&lt;T&gt; where &amp;'a str: Into&lt;T&gt; which is actually longer than the body of the function
LOL, I misunderstood your previous comment. I thought you say it is unlikely that we **won't** be seen as the next Lispers/Haskellers.
Good god. :-D I sure hope this doesn't come off as zealotry. If so, it's certainly unintended.
Perhaps [lombok](http://projectlombok.org) can help alleviate at least a bit of the pain? 
Personally, I enjoy writing Rust a lot, but I'd rather not have to think about memory management for 90% of the things I code...
Yeah. Rust is self-handicapped with the "no GC" and "static memory safety" constraints and *still* manages to be more usable than most established mainstream languages. ADTs, generics with traits, and so on.
Android/JNI soup? That doesn't sound too tasty :-P You can still make delombok part of your build. :) Note that it's not only about generating getters/setters/equal/hashcode/etc., but also hiding it from the code (but e.g. showing it in the outline, where it's actually useful). Less code to scroll, to read, to maintain.
I usually start with a unit test and then every Ctrl+S (also every focus switch between the windows) runs the compilation and testing in the build section of the editor (Sublime Text at this time).
Code that visually matches the wolfram or wikipedia definition is sometimes very satisfying to write. In Java, we don't have operator overloading (unless you use a compiler plugin), and it's good enough for my work. On the other hand, I also have written vector code in Lua, where operator overloading made for quite readable code (at the cost of a lot of allocations).
Oh, cool: things must've improved!
This is really cool, glad its no longer slow :p If the upper bound of your lazy prime iterator is the size of an integer (u64 it seems in your code), then could it be possible to adapt another library, such as [Ramp](https://github.com/Aatch/ramp), to allow bignum primes? I know it will be slower because there will be allocations involved, this is mostly just a random question :)
The lazy prime iterator is actually memory-linked in a relatively subtle way: it has to store most of the primes it generates internally, because they're required for sieving. So, unfortunately, it is not possible to extend it to bignums. (An alternate implementation strategy may solve this, I've got one in mind, but I haven't tried it at all and I suspect it may be much slower.) Also, it's going to take a while to do anything with primes larger than 2^(64) &amp;#x2248; 2 ⋅ 10^(19): running the iterator up to 10^(11) takes 80 seconds on my computer, and the runtime is super-linear, i.e. extending out of a `u64` is going to take at least 400 years and a huge chunk of memory. (Hardware improves, and it's actually somewhat parallelisable, so this could be reduced, but still... that's a lot of primes.)
Loop removal like that should work just fine if you take the greedy approach and collapse each loop as you come to it. You'd need to re-scan the loop and set the "last seen location" back to None or to its previous location when you remove the loop, though.
`&lt;3` I want this in some sort of FAQ.
Granted, our use case is fairly uncommon. Not everyone is doing as much AST matching. But I'll wager that whoever gets 20s build for a 2.4kloc project without counting dependencies is doing something fishy...
good info. Swings and roundabouts here deciding if I should get one :) i'm hearing '32k isn't much' a lot from people, but I've programmed machines with that much memory total, lol, and certainly machines with only that much cache or local memory. (admittedly workloads were simpler back then..). It seems the FPGA it comes with is quite capable too - I'd long been wondering about dabbling with FPGA's too - so it might be a nice board to just think differently with
Is it cheating if I abandon my own prime library for Project Euler?
Gah, cryptic errors, filed https://github.com/rust-lang/rust/issues/26100
Yeah, what I was getting at was that a good comparison would be to using the standard dense-matrix solvers in LAPACK, not the actual least squares solvers. But I mean, realistically I can already tell you what will happen... comparing to an optimized LAPACK implementation, for small problems there won't be any measurable difference or the Rust implementation will be marginally faster because of overhead, while for large problems LAPACK will absolutely crush it by order(s) of magnitude. That's not really that concerning though, LAPACK and BLAS are probably the most highly optimized codes in existence and it's pretty hard to even get into the same ballpark without specialized magic.
[**@evanm**](https://twitter.com/evanm/) &gt; [2014-12-15 18:41 UTC](https://twitter.com/evanm/status/544562872114610176) &gt; @carllerche @smllmp Yes. I'm shooting for more than just arg-parsing, no non-std deps, and a toolkit of functions rather than codegen magic. ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://www.np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
I think that was my problem, I didn't want a bunch of copies and allocations so I'd define it all on pointers, but returning a vec so... &gt;v4=&amp;(v1+&amp;v2)*&amp;v3 was just ugly to me, but I probably did it wrong.
I believe that's a super-power, actually... `MS VC++` has me firmly believing this after I got to compare its with a couple of `rustc`'s errors/warns... Unrelated question: If I have a suggestion/sorta-RFC but can't write an actual RFC (i.e. got an idea seed but none of the technical prowess it requires), can I post somewhere for anyone who might adopt it?
Just to answer your question about how to do `list.erase_between(list.find_first(item), list.find_last(item))` in Rust: Your `erase_between` function is called [`drain`](http://static.rust-lang.org/doc/master/collections/vec/struct.Vec.html#method.drain). Unfortunately it's not available on stable yet. While finding this function by a method name may be hard, it's easy to find when you search for `remove` on `Vec`'s page. Similarly, you can search for `find` on that page and find `position_elem` and `rposition_elem`. They're also unstable, but can be easily replaced by their `Iterator`'s counterparts.
I think it's more about how much duplicate code you have to write than how the function is called.
Would you comment on this part: "It seems a little silly, but I tried adding .write(true) to the chain of functions, and it worked. It seems like .append(true) should imply .write(true), but I guess it doesn't." Not sure if I was missing something or not, but this was pretty surprising. 
Yeah, like I said, totally understandable. Computers are hard :( I'll take a look at improving the docs on `OpenOptions`. Feel free to make any more issues where the errors aren't particuarly helpful: this kind of polish is something that we want to focus on in the nearish future. :)
In that case, just post an issue to the RFC tracker. 
I'll post this here since it came in quite handy for me not too long ago: you can still have default-implemented trait methods, that call other methods on that trait. The C++ equivalent would be: struct Render { virtual Point point_at(int u, int v) = 0; template &lt;typename R&gt; Point random_pos(const R&amp; rng) { return point_at(rng.rand_int(), rng.rand_int()); } } [Here's](http://is.gd/rdWmRp) a small example in Rust. My only problem is that it's pretty difficult to mix methods that take `&amp;self` and `&amp;mut self`: [example](http://is.gd/clA8h0) EDIT: Hitting the `unreachable!()` is completely reasonable, but I don't really see a way around this problem. 
[Umm... ??](https://encrypted.google.com/search?q=rust+rfc+tracker)
Swift seems to be quite similar to Rust. 
The main designer of Swift was apparently inspired by Rust to some degree.
Swift only uses ARC for Class types, Struct types are copied.
&gt; I think the API[3] looks okay, and that larger incompatible changes are unlikely. If I were to start over maybe I had changed MessageItem a bit, to make it contain only valid objects (e g, an MessageItem::ObjectPath should only be able to contain valid object paths, not any String). But at this point I think people prefer me to keep it the way it is, instead of drawing the rug underneath their projects. One way to do this change would be to have it on a separate branch, and ask the current users of your library at crates.io to evaluate it. The *future* users might as well thank you. I'm not sure whether Cargo supports a `git://github.com/&lt;user&gt;/&lt;project&gt;.git#&lt;branch&gt;` URI for its git dependencies, but I think it should (npm does for example).
You have it correct. Note that this only applies to Class types. Struct types don't do reference counting, instead they're always copied.
Really nice article :) I might just try F#. It seems to fill that void for a language with a Rust-like type system and feel (ADTs! Match! Functionalish stuff!) but also without the memory management overhead. I mean, at this stage I rarely have to worry about Rust memory management; the ownership and borrowing is mental metadata on variables as I type stuff, but still being able to program fast and loose is always fun. These days I've been programming in Java and C++ for work, and I sorely miss a lot of Rust features; features which F# seems to provide too by looking at your post. Which gives a nice option for me when I want to write good code, but don't care about perf or memory management so much. Then again, Rust *usually* fills those needs well for me. Still, certainly should try my hand at it :) &gt; If you are a functional programming fan, you might be skeptical of Rust and its claims. Try to think of it like this: Rust agrees that mutability is a problem -- it is simply offering a different solution to that problem. :D This is what I end up saying often to FP folks who ask about Rust's mutability story. Many have come back and said that they liked the "different solution". &gt; Rust does type inference within functions, but it cannot or will not infer the types of function arguments or function return values. &gt; &gt; [Stuff about the type system] I don't think the type system complexity makes it _impossible_ to do inference past function boundaries; rather it is a deliberate design choice based on readability concerns (and readability may be related to the type system's complexity, of course)
&gt; have the code scanned to ensure it doesn't obviously contain copyright-breaking material, But code from stdlib also largely relies on contributions so I don’t see why it’s supposed to be immune of this kind of thing solely because it’s stdlib.
&gt; But a little expectation setting is appropriate here. Some programming languages are built for the purpose of making programming easier. [..] Rust is not one of those languages. Actually, Rust is trying to make programming easier. The kind where you have high performance, concurrency and no memory safety issues, that should be easier. After all Rust is a tool. By definition it is making things easier, if you use it for the right task.
Interestingly, Swift 2.0 is using a traditional try/throw/catch error handling model https://developer.apple.com/swift/blog/?id=29
Excellent writeup with a good sense of humor. Thanks for sharing your experience! Do you have any criticism?
What does mlp stand for?
I did mean criticism of Rust. Well, I'm glad you enjoy it!
Point taken. I re-worded that paragraph several times, and it looks like I ended up with a claim about Rust that may be worth arguing. In my original wording, I focused on comparisons to other languages which were specifically designed to make programming simpler, such as Go, Python, or Visual Basic. Such languages make different tradeoffs than the designers of Rust have chosen to make.
Neat. I've a game currently running all the major platforms in mostly pure Rust with tiny platform stubs: Linux, Windows, OS X, iOS, Android.
&gt; The new error handling model in Swift 2.0 will instantly feel natural, with familiar `try`, `throw`, and `catch` keywords. Assuming your comment was based on the same paragraph, the use of traditional keywords doesn't *necessarily* imply a "traditional" exception handling model. The [Trait-based exception handling](http://github.com/rust-lang/rfcs/pull/243) RFC I submitted for Rust also (potentially) uses these keywords for familiarity, but departs from tradition in many ways. If anyone finds more details about Swift 2's approach I'd love to hear about it :) (I don't know anything about `NSError` though.)
Swift is mainly an applications language. It has exceptions, and it uses reference-counting (though the incref/decref are not dynamic, instead they're statically injected by the compiler's first phase, then the optimiser attempts to remove as many reference count operations as possible)
&gt; | F# | Rust &gt; -|-|- &gt; Unit type|()|() Technically Rust has "custom" unit types via empty structs no? Also a nitpick: &gt; | F# | Rust &gt; -|-|- &gt; Naming types | CamelCase | CamelCase that's PascalCase, really.
&gt; I might just try F#. It seems to fill that void for a language with a Rust-like type system and feel (ADTs! Match! Functionalish stuff!) but also without the memory management overhead. Why not just take a look at OCaml? It's the same family (ML derivative with imperative and object capabilities) but based on native code generation. Also, MirageOS.
That's a good point, it will be interesting to see more details.
&gt; That said, I haven't quite figured out how to get overflow checking to happen on casts. I want the following code (or something very much like it) to panic: Sadly this semantics for `as` was decided against in the run-up to 1.0. If I remember correctly the rationale was along the lines that `as` would remain for unchecked casts, and a different form for checked casts (perhaps a generic `.to()` method or something similar) could be added backwards compatibly at a later point. [Relevant discussion.](https://internals.rust-lang.org/t/on-casts-and-checked-overflow/1710)
I don't know swift that much but for me it seems that it's mostly just the look that's similar. Rust is designed for zero cost abstractions, multithreading and safety. A lot of features are very powerful but quite complex to learn and needed a lot of time and work until the final version has evolved. Swift often goes the easy way. It is easy to learn, you can quickly write code that runs quite but not perfectly fast. The compiler choose an implementation strategy and in doubt often defaults to a simpler aproach (objects on heap and reference counted or type erase instead of monomorphization) whereas rust lets the programmer decide.
NP-hard just mean that it's at least as difficult as any NP-complete problem (maybe more difficult). A NP-complete problem ("the hardest problem in NP") cannot be solved in polynomial time unless P = NP (P != NP is a conjecture). NP is the set of problem that you can verify a solution in polynomial time. P is the set of problem you can solve in polynomial time (P in NP). So you can solve the TSP. An easy algorithm is to test each permutation and take the better. But it's (at least) an exponential algorithm, thus it'll be intractable in practice when the number of cities grown. There is an efficient (but still non polynomial) exact method for the TSP with triangular inequality on the distances between the cities. Instances of several thousand of cities can be solved. (I think it uses column generation Edit: no it's a branch and cut).
&gt; Technically Rust has "custom" unit types via empty structs no? Yes. And custom void types too. ;-) e.g., `enum Foo {}`. No value with type `Foo` can exist.
That could work too. F# sounds like it hooks into C# libraries, which I like. C# is pretty great (with an IDE), especially now that you don't need Windows to run it.
They look like excellent resources. Thank you very much for sharing. Particularly cool to see the Rust &amp; Python combo :)
Seems as if we're coming at this from different directions and meeting in the middle. I prefer to think of the complexity classes very formally, in terms of reductions, determinism and non-determinism, time and space. It's important to note that your definition of NP is another, equivalent definition, arguably more popular. I think, when complexity is being introduced, it is perhaps easier for students to grasp the concept of verification in polynomial time on deterministic machines. It's a neat definition in that it describes a property which all NP languages have, and can therefore define the set. I've always favoured the more direct definition -- it's right there in the name "NP." The two definitions are lined by consequence of how non-deterministic Turing machines are defined.
&gt; You can take a slice of every element, so not just a part. Again, mega-nitpicky, sorry :) If you want to get pedantic then surely 'the entire array' is merely a particularly big part.
Yeah, it depends on how you define words, words are hard.
That is not what I said, nor what I wished to imply. In fact, I think the points you brought up are helpful to consider, and are by no means worth less than what I said. I only wanted to clarify that the definition and consequences we both posted are two sides of the same coin, two different ways of looking at the world. "Computationally infeasible" is a statement that deliberately goes outside the rigours of theory and appeals to the muddy reality of practice. I thought it was clear that infeasibility does not equal impossibility. For example, a problem which would have taken a century to solve on a PDP-8 would be regarded as infeasible at the time when such machines were popular, but that doesn't mean it is impossible to solve. The infeasibility of NP-complete problems comes from the sheer resources (time, in this case) required to solve them as the problem instance grows asymptotically. Nonetheless, they are still computable problems.
Not at all. Not in my book anyway. Generating lots of primes super efficiently isn't a necessity for project Euler any way. Once you've proven to yourself that you can implement a sieve, I wouldn't feel bad about using an other.
Unit *is* a distinct type in Rust. Alway has been, always will be (as long as Rust aims to be typesafe).
Also, iirc, rustc was originally written in OCaml.
The unit type is no longer distinct from an empty tuple.
[My Little Pony](http://mlp.wikia.com/wiki/Maud_Pie_%28character%29)
Great news! `slow_primes` was the first external crate I ever used (and yes, for Project Euler). While I kinda grew attached to the old name, I guess we're not *that* slow anymore ;-)
&gt; You are referring to a change in the parser that broke some macros. I'm referring to https://github.com/rust-lang/rust/issues/18614 and https://github.com/rust-lang/rust/pull/18752 , which has comments like https://github.com/rust-lang/rust/pull/18752#issuecomment-62324656 &gt; It still might be nice for pedagogical reasons to refer to unit as a kind of separate type in the documentation, since it's so common. (Internally though this is a great refactoring.) It is entirely possible that I just have some misunderstanding about this history and what's meant here, but all of this implies to me that they were considered different types, and now are not.
In the way it unifies a slew of modern language constructs together. It's targeting application level programming however. So I think it will be picked as a tool for very different projects.
That is the thing i was referring to. Absolutely nothing in the Rust type system changed in that pull request.
Cool. Then what part am I missing about why it's being discussed as a separate type? (I'm not saying you're wrong, I've just _really_ wanted to clear up my understanding here, there's several open bugs with docs and such here)
&gt; As someone who was looking for a better C++ I still don't think that Swift is going to be able to replace C++ in its full capacity.
&gt; you could even make an argument that stable Rust isn't truly systems-y right now, since things like inline assembler aren't stable yet. As far as I'm concerned, stable Rust isn't a complete language yet. Like someone mentioned here a couple days ago, the real sign that Rust is a complete language is when Servo can build on stable.
Exactly. The confusion here comes from the fact that the Rust team works on both the language spec and the compiler simultaneously and it's not always obvious what's being changed. The title of the PR ("Remove unit") refers to something that exists only internally in the compiler.
Excellent, thank you. That fits my brain quite nicely.
`cargo bench` putting separators between triples is already [being fixed](https://github.com/rust-lang/rust/pull/26068).
Another option is to look in Elixir (which sits on the Erlang VM), although it's not statically typed. Pattern matching everywhere though.
Fair enough, and if you're not, /u/gsingh93 , I apologize. The way you worded it and with the discussion I'm having over in proggit, it read pretty aggro, but it's possible that's an incorrect reading. I myself have a high-ish level of understanding of this stuff, as these kinds of guts haven't been my day-to-day in a while. If you asked me to write one of those files, I'd fail miserably. With Rust, one of the main jobs of the runtime is to set up [landing pads](http://llvm.org/docs/ExceptionHandling.html) for `panic`s, which is the job of files like https://github.com/rust-lang/rust/blob/master/src/rt/rust_try.ll. Another is stuff like http://doc.rust-lang.org/stable/std/rt/args/ . In a certain sense, you can think of the Rust runtime as the implementations of all this stuff: http://doc.rust-lang.org/std/rt/index.html this is the interface to all of the specific details in `src/rt/`. With C, http://stackoverflow.com/questions/2766233/what-is-the-c-runtime-library has lots of good answers that match my understanding. It's the same kind of stuff: setting up the stack, allocating local variables, etc. The lowest level of the language.
&gt; that's PascalCase, really. FWIW, "[CamelCase](http://en.wikipedia.org/wiki/CamelCase)" is quite vague on the matter. It's right, even if underspecified.
I personally found it interesting to do that stuff from scratch, trying to eek out as much performance as possible. But no, it's not cheating.
I'm sorry it came off that way, I didn't intend it to (I still don't see how it did though). Was this question supposed to be very obvious? I didn't know things like the heap and command line arguments classified as runtime. Edit: I also know nothing about this proggit discussion.
It was. YRC.
This might be a stupid question and I only skimmed the article, but do I need a Mac to do this?
Yes.
Thanks for this! &gt; However, it gets quite hard to provide your own analysis (showing that a token is redundant, for example, or measure imbalance in order to parallelize). I don't quite know what you mean by this, but I'd be interested in trying to solve these problems. At the moment the operation can be easily parallelized, since you can just partition the current axiom up into manageable bits without worrying about eating another part's lunch. The `Iterator` trait even facilitates doing this, and I can see it as a possible improvement in the future. &gt; Can you handle parametrized Lsystems that way ? By adding a parameter to the sum type ? I alluded to this a little in the post, but yes, it can handle parameterized L-systems (see [example](https://github.com/atheriel/lsystem/blob/master/examples/parametric.rs)), or really any 0L-system at all. The cases it can't handle at the moment are context-sensitive L-systems (such as 1L-systems or 2L-systems). &gt; How do you handle non-totality in the rule set (like if there if there is no rule for B) ? Using `match` statements also lends a natural way of handling non-totality (constants). See [a line in this other example](https://github.com/atheriel/lsystem/blob/master/examples/penrose.rs#L112). &gt; It seems to me that the function that act as a rule is far too free, you are not limited to simple rewriting. It's not really a Lsystem now, just a repeated flatmap. :p I think this is basically true, but I see it as a good thing! A repeated flatmap with a well-defined set of tokens just looks like a flexible generalization of an L-system to me.
Quick question about this: &gt; There’s three traits, and so seven non-empty sets of traits that could possibly be implemented… but there’s actually only three interesting configurations: &gt; &gt; * Fn, FnMut and FnOnce, &gt; * FnMut and FnOnce, &gt; * only FnOnce. &gt; &gt; Why? Well, the three closure traits are actually three nested sets: every closure that implements Fn can also implement FnMut (if &amp;self works, &amp;mut self also works; proof: &amp;*self), and similarly every closure implementing FnMut can also implement FnOnce. This hierarchy is enforced at the type level, e.g. FnMut has declaration: I don't get this. It seems to me this is completely backwards. If I have type `T` which is mutable, then I can obviously borrow it to get `&amp;T` or `&amp;mut T` but if I have `&amp;T`, then I'm not allowed to do `&amp;mut T`. I'd have to do `&amp;mut *T` to reborrow it and requesting `mut` might just get me a compiler error because the variable wasn't mutable. What am I misunderstanding?
The proof is talking about borrowing a `&amp;mut self` to a `&amp;self` (i.e. "if the closure works with `&amp;self`, then it also works with `&amp;mut self`, because the latter can be converted into the former): fn call_mut(&amp;mut self, ...) { (&amp;*self).call(...) }
Initial implementation of `const-fn`? Awww, yiss. I wanted this very recently. :)
Oh, I see. Yeah, I interpreted it exactly the opposite way you meant it, ha. Thanks.
I'm excited about `IndexAssign`. It'll help prettify a lot of code using common library types like `BTreeMap`. Also, `str::chunks`—I was wishing for this just a few days ago!
It's [pretty limited in what it can do](https://github.com/rust-lang/rfcs/blob/master/text/0911-const-fn.md); it's pretty much just useful for basic initialization of private fields in structs, and basic arithmetic on other consts. You can't use any kinds of conditionals, loops, side effects, or almost anything else in const functions. It's not at all an alternative to syntax extensions or preprocessors, which can use the full language.
No, I don't know Haskell, I tried reading a Haskell book but it was too hard for me, and it wasn't a good book too. I know Ruby and a little C (Arduino), but I'm moving away from Ruby, looking into Rust or Nim to replace it. I use Ruby mostly for command line apps, and that's why I'm toying with Rust now, but it's not easy. :) As for Rust's Windows support, I think it's very good, I really don't see a difference (at the basic level that I am using it) between Windows and Linux. And about those tools, you would probably distribute them as binary executables, so people wouldn't even need Rust installed to use them.
How do you run tests in the playpen? I can't find any way to do it.
Why? Are there any details about Swift yet?
Swift is a truly good enough systems programming language for Apple themselves because all Apple platform code are written in RC semantics from the skin to the core. But I don't think Swift fits to every low level problems. Swift can access memory directly, but without RC, Swift has no memory safety at all -- it's just a re-syntaxed C at least currently. I think Swift can appeal to many existing iOS app developers who are looking for good-enough language to write some server-side code. And number of them is pretty large. Swift is not the best for systems, but provides good enough features and production-ready IDE. (though it's very buggy...)
. | F# | Rust - | - | - Compiler strictness | Extremely strict | Even stricter I chuckled. :-) &gt; Rust is a whole new level above that. Compared to Rust, regular pointers are simplistic. Gosh, that's incredible! We broadened the evil empire of the pointer land. I can already hear freshmen trying to grasp what Rust's pointers are screaming. &gt; That said, I haven't quite figured out how to get overflow checking to happen on casts. I want the following code (or something very much like it) to panic: That was a thing, but it changed not to panic right before the release of 1.0. IIRC the main reason was that in Rust bit fiddling is quite common, so using `as` as the purpose should be ergonomic.
Agreed. Swift can be a very good introduction to Rust :) If a Swift programmer look for a ~~better~~ language that allows do more things with minimal learning efforts*, Rust will be the first candidate.
You can provide the format string at runtime. :P
Oh that's nice, I didn't know about that one. Is the use case phantom types?
You may want to leave a :+1: in the RFC comments.
I think /u/lfairy meant that as "The original author clearly enjoyed using functional language, so it's unsurprising there was a functional influence", which seems like a reasonable idea to me.
&gt; I miss printf-style format strings. Isn't the rust formatting syntax based on python? i.e. it offers very much the same facilities as printf-style formatting?
That doesn't have anything to do with the format string format (…) though
He expands in an other comment that it's more of a habit issue.
Cheers for the heads up. I'll try to bear that in mind!
While I'm sure it doesn't compile (Don't have a compiler on me) and this wastes way more cycles then needed, does something like this seem sane? pub fn niave_cut_cycle(path : &amp;mut Vec&lt;i32&gt;) { let mut count = HashMap::&lt;i32, i32&gt;::new(); for item in path { count[item] += 1; } let mut it = count.iter().find(|&amp;x| *x &gt; 1); while !it.is_none() { let (item, _) = it.unwrap(); let start = path.iter().position(|&amp;x| *x == item); let end = path.iter().rposition(|&amp;x| *x == item); for item in start..end { path.remove(start); } //Recalculate the count map and look for the next iter (We may have inadvertantly removed doubles already) count.clear(); for item in path { count[item] += 1; } it = count.iter().find(|&amp;x| *x &gt; 1); } }
&gt; If you want to modify dedup to have special logic, just grab the source and do so: https://doc.rust-lang.org/src/collections/vec.rs.html#1118-1201 By the way, any reason why we don't have `if ln &lt; 2 { return; }` [here](https://doc.rust-lang.org/src/collections/vec.rs.html#1178) ? 
Have you published it on GitHub? I would love to take a look at it
Ahh ok then.
The parent comment would be fair if it said "a project of the size of Servo". It's unfair because it doesn't factor in choices made by Servo to make a sweeping statement that amounts to "Rust will never be complete in a useful sense" because of specific decisions made by Servo that may not be made by other projects of that size. Note that it's not necessary for the stdlib to be all the things. In Rust it's quite common to offload libraries from the stdlib to external things.
I see you're working from an OSX build machine. Make sure you had your `ar` set properly when building the library, not just the executable. The system `ar` on os x won't create anything valid for `arm-none-eabi`. After you set this in `.cargo/config` for your target, just run a clean build and you should be fine. I ran in to this issue when I was working on zinc. For those interested in fully rust bare metal solution: [zinc](http://github.com/hackndev/zinc).
https://users.rust-lang.org/t/formatted-strings-and-debug/1726/1
I think you can enable debug assertions in a release build? It's a bit strange of course, but I think you can have that, and still have optimizations turned on. (And it will have a performance impact).
&gt; It's easier to use C a good deal of the time. It's harder to use C *correctly* a good deal of the time, though.
If your goal is just to fiddle with Rust, I wouldn't bother with staying on stable. Nightly is just more pleasant to use. And I feel that `drain` will become stable soon (RFC was accepted). Your solution with `remove` in loop is O(n²) (if `from..to` can be big), so I wouldn't recommend doing it that way. You can use iterators: new_path = path.iter().cloned().take(from).cloned().chain( path.iter().skip(to).cloned() ).collect(); *path = new_path; Or let tail: Vec&lt;_&gt; = path.iter().skip(to).cloned().collect(); path.truncate(from); path.extend(tail.into_iter());
Doesn't a '+1' pollute people's inboxes? &lt;rant&gt; What's so hard about adding an 'upvote' feature, dear GitHub? &lt;/rant&gt;
I guess it's ok. Although you could update the hashmap instead of creating it from scratch. Also, if indexes in your graph are bounded, use `VecMap` (or just `Vec`). &gt; Don't have a compiler on me https://play.rust-lang.org/ On reddit, please prepend all lines in your code snippet with four spaces (you see what happened with first two lines). Also, while !it.is_none() { let (item, _) = it.unwrap(); ... } should be better written as: while let Some((item, _)) = it { ... }
More constructive than a simple +1 would be an elaboration of their motivation for desiring the feature. It always bothers me when RFCs devolve into popularity contents rather than reasoned discussion.
Why is there a `Copy` restriction for the implementation of `Extend` for collections?
&gt;prevents all memory corruption bugs. Rust, in most cases, prevents *many* memory corruption bugs that are defined as security issues. I wouldn't necessarily state that it is provably preventing all possible memory corruption bugs, even within the subset of vulnerability classes. Awesome post, though. You've managed to represent a lot of information very clearly. It's interesting that you found it to be difficult to learn. I found it to be one of the easiest languages I've picked up. It took considerably less time to jump into rust compared to Javascript (perhaps ironically, as Javascript is always seen as 'easy'), because everything just made sense. There's some overhead in terms of thought process, but I've found the code to be very readable. I think there really just needs to be more diverse documentation out there, with many different explanations of concepts like lifetimes, including videos (rust has so few good videos on youtube post 1.0). I had to read a dozen different articles on Monads before I understood them when I was learning Haskell, and it wasn't that one of them was better, it was that each one had some new interesting way of formatting the information. I could read one, reread another, and get more out of it that second time. Rust doesn't have this right now - that is where I see the learning curve. I'm also the same way about superhero movies.
So one might say, that while Rust doesn't actually make programming easier, it makes *reckless* programming harder.
That reminds me: I've recently seen http://crystal-lang.org/docs/syntax_and_semantics/macros.html which shows how Crystal includes itself (really?) as preprocessor/templating language with delimitters like handlebars. Nice thing is, apparently, you can use this syntax everywhere. It's a bit like using mako, but also having access to the AST.
&gt; Omitting such a feature would make them less ergonomic Why is that?
That's what extend already does. The RFC is about adding another additional version of extend.
I have ar set in my target json to the toolchain one, is that not having an effect? EDIT: just realized that ar is not a property settable with the target json. That's kind of stupid. I'll try setting it in my cargo config in a bit.
Or just throw it on SO and let Google sort it out.
That is a pretty killer idea. Even though it sounds difficult to implement I hope they at least give it serious consideration.
I've thought about this. The problem comes down to this: const fn foo() -&gt; usize { !0 } const N: usize = foo(); Specifically, what happens if you're cross-compiling. This is just the simplest example where it all goes wrong; imagine trying to return a structure with platform- and os-dependent fields and types. That basically *eliminates* the possibility of JIT compiling const code. But you can't interpret it, either. You want access to files, but that means having access to system APIs, and how are you going to reconcile the `usize` you return from your const code and the `usize` you use when making a syscall? You'd have to more or less build an entire *platform* that's completely independent of the target and host platforms... at which point, const code becomes unable to deal with platform differences. You're pretty hamstrung either way. I don't think there *is* a good solution to this.
I would have probably never found this out, if I hadn't been told. I was looking for a "Test" button next to the ASM/LLVM IR buttons. Still a great feature in any case. Being able to write tests in playpen is great and something I've wanted.
Have you considered `[T; rand()]`? What semantics could that *possibly* have?
Is it fundamentally different from simply writing a different program to calculate your constexpr, then pasting the result into your code? :-)
A different one every time you compile! Every compilation will have it's own unique flavour!
Consider making `[T; rand()]` part of `rustc`. Every programmer should have a unique compiler.
Yeah, this sounds really scary and not fun to debug. It may be less of that it is particularly hard to implement, but not a good language design idea. If it was a good idea, it would be in any of the languages you mentioned.
Yikes, my bad. It's uploaded to cmr now.
&gt; &lt;rant&gt; What's so hard about adding an 'upvote' feature, dear GitHub? &lt;/rant&gt; Years ago, they had it. It wasn't actually particuarly useful, so they dropped it.
PR submitted. :-)
Yup! It looks ok. But of course I still have some nitpicks :) 1. It's ok to call `.unwrap()` if panic would mean bug in algorithm. So I'd unwrap before assigning to `start_op` and `end_op` and get rid of following if-lets. Similarly in hashmap-updating loop. 2. Without those if-lets, I think you could declare `skipped` without `mut` or initial value. I think that Rust should be smart enough figure out `skipped` is never used uninitialized (but I might be wrong). let skipped : Vec&lt;i32&gt;; 3. Even better, move the `for item in skipped` before updating of path, so you don't have to collect them. After doing so, you can use while-let sugar. 4. `insert()` after `get()` is an antipattern when dealing with maps (you should be able to do it with `get_mut()` or `entry()`). Also, you forgot to actualy substract one :P 5. Drain would really shine here, squishing about 10 lines into 3. for item in path.drain(start.. end) { *instance.get_mut(&amp;item).unwrap() -= 1; } **Edit** I see that you're using `let mut x = default; if let ... { x = y }` pattern. Although in your case it seems that providing defaults is unnecessary, when you need to provide default it's better to avoid mutation by using `or` method, for example: let current = instance_map.get(&amp;item).cloned().or(0);
&gt; I am seriously impressed with Rust. Then again, I thought that Eric Bana's Hulk movie was pretty good, so you might want to just ignore everything I say. :-)
Stability is desired in the ecosystem, but it's not expected for everything right now. If you really want to code complex things in Rust, you will probably need to do it on the nightly channel. With feature gates you will know what breaking changes to expect in the language going forward -- giving you foresight for design/dependency decisions. You can minimize some breakage -- Cargo.lock is intended to prevent unexpected breaking changes in your dependencies, and multirust will allow you to install multiple toolchains and upgrade your codebase when you need to fix bugs or use more recent compiler features. Don't be ashamed of nightly!
Depends on the distribution :P Now seriously, as erkelep says it would have whatever semantics rand() gives you in whatever platform you are (distribution, engine, seed...). There is a trend in some replies that I don't like tho. It is worth to remember that just because one could do something stupid with it doesn't mean that the feature itself is stupid. In particular, what I'm proposing is completely opt-in, the programmer tells at the expression evaluation site that the expression has to be evaluated at compile time. So maybe I should ask you, why do you want a `[T; rand()]`? I can give you other examples, but not one for that :P Say you are writing a game, and have a directory with entity descriptions in json. You could, for example, `[FileName; get_no_entities_from_dir_(...)]`, and you get a stack allocated array of file names for your current build. You could even have stack allocated strings since you can get the file names at compile time. This requires I/O during compilation. Others will need Network I/O, interprocess I/O, a random device, ... There are alternative solutions to all these problems (code generators, execute arbitrary python code inside rust macros, ...). This is a language-level alternative that some at least the CTFE problem forever. I think it solves it nicely, since you can reuse your rust code for it. But I respect that others disagree.
Oh excellent! I love Recife. Fun bit of trivia: Porto de Galinhas is actually sort of why I have a job working on Rust, in a way.... &gt; sorry my poor english Your English is totally fine :) &gt; i was looking for a "hello_parallel_world" anybody can help? What about this? use std::thread; fn main() { for i in (0..10) { thread::spawn(move || { println!("Hello: {}", i); }); } thread::sleep_ms(50); } 
Funny you mention that. That's what I tried just now, only to get an error.
True that. I just added a use case from internals to the discussion.
&gt; They cant choose to use stable right now. Who told you that? Please don't make such statements about Servo unless you know what you're talking about :/ The majority of our unstable feature usage is convenience. We use plugins because they're convenient (the lints can be turned off, and the codegen could be done with python like we already do). We use box_syntax because it's convenient. We use a bunch of niche APIs because they're convenient. Remember that both Servo and Cargo were written back in the pre-featureflag days when breaking changes were common. At that time, there wasn't any "stable" to aim for. We used whatever APIs made sense. During rustups we generally did the minimum effort to get it compiling/working again, and then sometimes fixed stuff up later. So if a feature like `box_syntax` was put behind a gate, we would just enable that gate and move on. A bunch of our library flags could be removed by replacing it with an import from the std facade. The `collections` (and `core`) flag is an example of that. We import `VecDeque` from outside of the std facade because that's where it used to be, and we don't care about having Servo compiling on stable rustc so nobody put the effort into fixing that and removing the flag. There's a _lot_ of this in our feature flag usage. At the same time, we _have_ been putting effort into making our deps compile on stable, and it's pretty easy to do so. To be clear, we could probably remove a lot of the feature flag usage from Servo with some tedious work; without really compromising on code quality. I might do this later. However, we do need nightly features for: - Plugins. Some of our codegen (and html5ever's codegen) is done by plugins. We _could_ use syntex or python-fu to do this like most C++ projects, but plugins are nicer. We would also lose some lints. No biggie. But having them is better. Plugins are something that will never really stabilize for a long time in their full power (because they hook into compiler internals), and most other languages don't have an analogue of this, so these shouldn't really be a part of the stable-unstable comparison. A language can be complete without plugins. - Some small library uses here and there. Not much. I had a hard time finding anything from digging into the repo for five minutes; most of them were trivially-fixable things. Technically these could be replaced by our own implementations. They're not std-crucial libraries, just random edge libraries that need stabilization - New language sugars (slice patterns, box syntax): Again, these are in no way necessary for Servo, we use them because these are nicer and there's no point putting effort into replacing them. - Some small feature flag usage of language features like filling drop. I think these aren't necessary, but I didn't investigate. A lot of this is due to the history of the project, as mentioned above -- we used to freely use those features and migrating away would be a waste of time for us. So, it is _possible_ for Servo to migrate to stable and retain the functionality. We would just view it as a whole lot of work for little gain. We do migrate deps to stable whenever people ask for it (there's a gain there -- library reuse), but for Servo to migrate the only gain is that we don't need to rustup anymore. Given that our rustups are mostly painless now (last two rustups were just some plugin one-liner fixes), and that we would lose a lot of not-necessary-but-nice-to-have features like plugins, there's little reason for us to do this migration. -------------------------------------- Besides, my contention was against the "the real sign that Rust is a complete language is when Servo can build on stable" -- I'm saying that it's likely that this will happen way, _way_ after Rust's std library gets filled in and can be called "complete", and this is because of Servo's choices to use and dogfood shiny new features -- a choice which may perhaps be the only choice now, but won't be the only choice in the future. My issue was that it was unfair to use Servo as a yardstick for stable-completeness because Servo will likely use unstable features way after Rust is "usably complete" because it's our choice to do so, not because it's the only choice we have.
No need to go that far, if IIUC you get the same problem with floating point arithmetic at compile time within a `const fn` (the result is architecture dependent). &gt; what happens if you're cross-compiling. &gt; You want access to files, but that means having access to system APIs, and how are you going to reconcile the usize you return from your const code and the usize you use when making a syscall? If you are not cross-compiling, accessing a system APIs is not a problem, your rust std library is there, and can be used on your platform. If you are cross compiling, and linking against a rust std library from a different platform, you cannot (obviously) use that standard library at compile time in your platform (because it's different). We should definitely add a lint for that! I think you raised a very good point. Cross-compilation is a problem. However, some problems are not worth solving. Don't use system specific features at compile-time if you intend to cross compile. If one were to do those things at the build system level it would be a hard problem too. Consider the case of MPI. The MPI standard specifies constants like `MPI_INT` as magic. It is a macro, and it can expand to... an enum, an int, a long, a pointer to a struct, ... You need to compile in the host against a ABI compatible MPI implementation of the one installed in the target. My point being, one already has to be careful while cross-compiling. &gt; You'd have to more or less build an entire platform that's completely independent of the target and host platforms... at which point, const code becomes unable to deal with platform differences. &gt; You're pretty hamstrung either way. I don't think there is a good solution to this. I think the solution is just "don't do that", and we should have a lint for system APIs to warn of their usage within CTFE, if this were allowed. From what I have lived, when a language gets a new feature there are 3 phases: - go overboard with it and use it everywhere (which surely will happen if such a feature gets introduced), - land in "feature"-hell, and finally - learn to use the feature judiciously. When C++11 constexpr functions where introduced (whose body is a single return statement containing a single expression), some people rewrote ~~almost~~ the whole standard library to make it constexpr, including constexpr random number generators (see sprout). So yeah, people can go overboard with CTFE. I think that in this particular case (cross-compilation), it really makes sense that you cannot find a file on a linux system by using the windows sys API, so it shouldn't be an unexpected error. Anyhow, i've proposed opting into CTFE by telling the compiler at expression evaluation site that you want the expression to be evaluated at compile time. The compiler can, of course, try to evaluate other stuff at compile time if possible, but that is a compiler heuristics problem, not a problem with the feature itself.
&gt; and not fun to debug. It is probably funnier than debugging python code, since it's a Rust interpreter, may the types be with you!
Awesome Steve!! You are the best, always helping me! Thank you 
Sorry, but the point you've just made is that you cant use stable. 'we could use stable exect for...', well dang! I dont want to argue about it; servo doesnt want to use stable? fine. ...but *no one is using stable at all, for anything serious!* ...and edit, because heck, edits are all the rage rather than replies apparently: &gt; -- a choice which may perhaps be the only choice now, but won't be the only choice in the future. My issue was that it was unfair to use Servo as a yardstick for stable-completeness because Servo will likely use unstable features way after Rust is "usably complete" because it's our choice to do so, not because it's the only choice we have. Right, and fair enough. The point of distinction I'm making simply being the difference between *right now* and 'some point in the future when stable has more stuff in it'.
And Steve, i was wondering myself if i can request some Rust swag.. Is that possible? I'll be talking about rust at two events: FISL (http://softwarelivre.org/fisl16) and Campus Party (http://recife.campus-party.org/). Can you help me (again)?
According to [this blogpost][], it looks like `try` is like Rust's `try!` macro, or `?` in the exceptions proposal, and acts to explicitly propagate failure. `do { ... }` blocks scope failure, the way functions do in Rust, and `try { ... }` does in the exceptions proposal. You handle the failures in `catch` blocks chained after the `do` block that each match the error against an enum variant representing the error, which you can elide if you want a catch-all case. `throw` does what you'd expect and raises an error. Unlike Rust, Swift 2 does not appear to encode the error type into the return type of a function. Instead, you just use the keyword `throws` in the signature. It's unclear from the blog whether it's possible to explicitly specify the error type. The blog mentions using enums to allow exhaustiveness, but doesn't specify how the error type of a called function is determined, so I'm not sure whether it's somehow globally inferred, or else if the exhaustiveness consists merely of requiring a catch-all at the end of the `catch` chain. It's also not clear if potential failures are first class values like in Rust and in the exceptions proposal. There's no apparent carrier type (like `Result`), and it's unclear if you can elide a `try` in order to handle or ignore such a value directly rather than unwrapping it and propagating any errors. My first impression is that you can't, though. Edit: They've added an [Error Handling][] section to the Swift book. It apparently does try to infer exhaustiveness, but the conditions where it can do this aren't specified. There's no apparent possibility of specifying the error type. [this blogpost]: http://www.hackingwithswift.com/swift2 [Error Handling]: https://developer.apple.com/library/prerelease/ios/documentation/Swift/Conceptual/Swift_Programming_Language/ErrorHandling.html
"rustc is the best fuzzer ever"
There is a strong pressure in the Rust community against "implicit deep copies". Evidently we have a lot of shell-shocked C++ devs who fear default/copy/assignment/move constructors with super elaborate logic that is a huge perf/correctness footgun. Instead in rust to do a general deep copy you need to call `clone`. However we generally agree that for "plain old data" (e.g. primitives or simple structs of primitives) this concern is irrelevant and it should be fine to implicitly deep copy them everywhere. This is why we have the separation of Copy and Clone. This RFC in particular extends this reasoning to the Extend trait on collections. Where currently you can do: foo.extend(vec![1, 2, 3, 4]); one would also like to do foo.extend(&amp;[1, 2, 3, 4]) as this avoids the need to allocate or mutate the source. Particularly in the case of Plain Old Data, this should be "fine". You can of course do this today with foo.extend(&amp;[1,2,3,4].iter().cloned()) So this is just an ergonomics boost.
That is maybe a bit over-exaggerated. `iron` and `piston` work on stable. Both pull in quite a lot dependencies. `mio` also runs with stable. There are only a few things that really don’t work well with stable (like Rc::make_unique being unstable). Most things are just annoying but easy to work around. Besides that, the really interesting things (like associated consts) don’t event work with nightly yet. So no reason to abandon stable rust.
I target stable with what I do at least. You could look for Travis links and see which crates are regularly built on stable. Frank brought up something useful: the crater tool knows which crates and versions build on stable. Cargo could also record which version it was checked with during upload.
&gt; It says that of the 2196 known crates, 837 work on stable, while 215 more are known to work on nightly but not stable When I first saw these numbers, I thought "oh, half of crates.io works on stable," but this actually means that 80%[1] of crates work on stable. Seems good. 1: 837 / (837 + 215) = 0.79562737642
Tooling and library are transitory differences, so that's irrelevant since things will change. But other things are language design. For me ownership in Rust is not such a burden (since I care about it in C++ as well and it requires mental effort either way).
Pretty sweet! Now fixed in case anyone missed it.
Me too. That'd be a good improvement. 
Especially because you'd ideally run 🏃 a benchmark on a machine without any other (demanding) processes.
You're looking for /r/playrust. This subreddit is about the programming language Rust.
I'd like to pass my proposal through a mentor of some kind to make sure the abstract lines up with community interest. Anyone interested in helping me out? Just reading an abstract and providing feedback before I send it through the official process.
Ah, yes, with that interpretation it's much more reasonable.
It's not open source. Sorry!
Wow, there are multiple Rust (research) dialects by now? Patina, and now Impala; any others?
Par contre le font-weight &lt; 400 pour le corps de texte c’est pour moi à éviter de toute urgence…
So it's like [this](http://is.gd/nWvKLb), but I don't think you need a lifetime specified, why not copy Line? Also, remove that unsafe block, that seems unnecessary 
That's also my opinion, but after close to 8 years of using C++ professionally, and a significant time investment in understanding as much of its nitty-gritty details, I've pretty much integrated the ideas of ownership... ... it just was really neat to see Rust managing to put *words* and *explanations* on my intuitions :)
Fantastic! Not sure how I missed that!
Why put a Vec in an Option? What is the semantic difference between a None and an empty Vec?
I don't think that's right. The broken list contains a lot of crates that work fine on nightly -- just look at the top of that list, those work.
Well, remember, that report wasn't done today: if those crates were updated within the last fortnight, they may have been broken for the report but fixed now. It's also true that the broken crates may or may not be built right by crater.
You might enjoy Scala, you can connect to a database during compilation in Scala. Personally, I am *very* leery of impure builds: - compilation might fail after a certain data, or only work from 00:00 to 01:00 in your locale - how do you know you should re-compile? how is the build system supposed to track the changes of that distant github repository? should we integrate all kinds of network/file transfer protocols in the compiler so it *can* track them? - how do you do cross-compilation (sorry, it's *mandatory*) Now, those don't actually necessitate `const fn`; the necessity of `const fn` seems to stem from `Rust` *explicitness policy*: you don't want a program to accidentally compile, and not compile with the next 3rd party library version because now they do something "not kosher" during that function call. `const` is a contract. On the other hand, I agree with you that splitting the functions in 2 is a harsh penalty. --- Regarding the evaluation strategy; in my latest "plans" I was thinking that what would actually be required would be to restore LLVM meaning =&gt; LLVM is after a low-level virtual machine, so it should be possible to: - write an interpreter for LLVM code (LLVM code comes with all you need to know about integer/pointer sizes) - write "shims" to abstract the OS away (memory allocation/thread allocation) The idea of intervening at LLVM level (and not Rust's) is that this greatly ease interaction with other languages (such as C).
You might enjoy Scala, you can connect to a database during compilation in Scala. Personally, I am *very* leery of impure builds: - compilation might fail after a certain data, or only work from 00:00 to 01:00 in your locale - how do you know you should re-compile? how is the build system supposed to track the changes of that distant github repository? should we integrate all kinds of network/file transfer protocols in the compiler so it *can* track them? - how do you do cross-compilation (sorry, it's *mandatory*) Now, those don't actually necessitate `const fn`; the necessity of `const fn` seems to stem from `Rust` *explicitness policy*: you don't want a program to accidentally compile, and not compile with the next 3rd party library version because now they do something "not kosher" during that function call. `const` is a contract. On the other hand, I agree with you that splitting the functions in 2 is a harsh penalty. --- Regarding the evaluation strategy; in my latest "plans" I was thinking that what would actually be required would be to restore LLVM meaning =&gt; LLVM is after a low-level virtual machine, so it should be possible to: - write an interpreter for LLVM code (LLVM code comes with all you need to know about integer/pointer sizes) - write "shims" to abstract the OS away (memory allocation/thread allocation) The idea of intervening at LLVM level (and not Rust's) is that this greatly ease interaction with other languages (such as C).
As long as you don't try to use some system routines in a different system _at compile-time_ cross-compilation still works. Maybe I'm too pragmatic here but I fail to see why this problem is worth solving. Can you help me with more details? The only thing I can say is that as /u/Quxxy points out I would be very wary of trying to use different versions of the libraries for CTFE and run-time and have bridges between them. I don't think something like that would be feasible to implement. For me it is fine to just say that "using windows file system API on linux doesn't work". It doesn't matter if you happen to try to use it at run-time on linux, or at compile time because you are cross compiling from linux to windows and using some file-system API via CTFE. Maybe I didn't got your point but as I said above, we can lint that.
&gt; compilation might fail after a certain data, or only work from 00:00 to 01:00 in your locale Why are you leery of this? I think this is just fine. It is opt-in after all. If the programmer explicitly opts into evaluating an expression at compile-time, then it is their own fault if they launch a program at compile-time that never terminates. &gt;how do you know you should re-compile? how is the build system &gt;supposed to track the changes of that distant github repository? &gt;should we integrate all kinds of network/file transfer protocols in the &gt;compiler so it can track them? Why should the build system have to know anything about CTFE? &gt; how do you do cross-compilation (sorry, it's mandatory) I learned that today :D I answer you in a different comment :) &gt; Regarding the evaluation strategy; in my latest "plans" I was thinking that what would actually be required would be to restore LLVM meaning =&gt; LLVM is after a low-level virtual machine, so it should be possible to: &gt; write an interpreter for LLVM code (LLVM code comes with all you need to know about integer/pointer sizes) &gt; write "shims" to abstract the OS away (memory allocation/thread allocation) &gt; The idea of intervening at LLVM level (and not Rust's) is that this greatly ease interaction with other &gt; languages (such as C). I think this is a nice idea. I was kind of thinking more about compiling the code of the expression to be evaluated into its own binary, link it, execute it, fetch the result, and continue compilation of the original code. This would be overkill for "automatic CTFE" (the one the compiler does behind the scenes like constant propagation), but for a call-site evaluate expression at compile time type of CTFE where the user explicitly tells the compiler to evaluate some code at compile-time it would be nice. I don't want the compiler trying to call some server somewhere and waiting until giving up because it tries to CTFE a function automatically.
Or if !prod.is_empty() So you save two characters. In return, you get a weird API with two different representations of the same thing, and any operations on the Vec becomes much more cumbersome since they're wrapped in an Option.
Right, they seem to be build failures on crater, I guess some crates are just so volatile they break often. I know I waited for quickcheck_macros to be updated etc.
Right, but the stdblib is one organisational unit. If I'm approved to use a language I'm approved to use its stdlib, and as new versions come in they'll get approvals done too. It's not immune to code scans, it's immune to me have to request and organise them :-)
My bad, the comments in each columns of their corresponding tables was also a comment of the fields of the structs. I'm too lazy to shortened the example, since i do just copy paste the code.
I can't find any information on Impala other than a reference in a DSL paper, can you link to more info on it?
let's ping /u/brson, he would know
I can ship Rust stickers if you email me a shipping address, phone number, and desired quantity to banderson@mozilla.com.
This page may help you: http://www.ncameron.org/rust.html
That's my limited experience porting Servo deps and watching other ports. Usually it was just the flick of a switch, perhaps accompanied by some minor path-changing (eg `collections::` to `std::collections`)
Agreed that it might not be the best design and it is totally contrived to begin with ... ;) Nevertheless, what I still want to learn is how to specify, if possible, the explicit lifetimes in such a way that all of the initialization code (including lines 19-28 from my original code), can be moved into a constructor function. All answers appreciated.
They (the lines from your initialization code) cannot be moved into a constructor function. A constructor function must return the constructed item, which implies a move. But since the initialized item contains references to itself *it cannot be moved*. The struct, as you have designed it, cannot be moved after initialization. The modified version that is in my code can do what you want. Move line 28-37 into `Triangle::new` and it should work.
&gt; I can't figure out how to call the closure inline let x = { some_code(); value }; ;)
So let's say you pick the "executed at first instantiation and frozen" semantic, with unspecific order. You would then want to JIT part of a crate to get a type-level constant used in other parts of the crate. That requires you to be able to "carve out" a DAG and type-check it and compile it separately. This is quite problematic for syntax extensions (which is why you don't *yet* see syntax extensions being written in the same crate as the code using them). You could argue that doing it at type-check time requires less juggling, but you still have to run all the compiler passes on a subset of the crate *whilst converting a type from AST form*. That bit is done during type collection, which is a *prerequisite* for the very next step of typeck (function-local inference/type-checking). Not doable in the current compiler by any stretch of the imagination. A theoretical one that spawns a process and starts JITing into it as needed (you don't want to deref raw pointers in the same process as the compiler)? Maybe. I guess that's my verdict. Maybe. I was hoping I would convince you this is an impossible feature, but I convinced myself that someone dedicated enough could make it happen. The only consolation I have is that Rust's philosophy goes a bit against such impurity in the type system, so it's unlikely it will happen. It does however simplify one dilemma I had in a theoretical Rust dialect (where the distinction between types and values is blurrier, but still being a superset of Rust 1.0's stable features). I thought I had to distinguish between `const fn` and `fn` (e.g. `foo(x) = x` and `bar() =&gt; print(x)`), but... If I "just" allow impure execution with few guarantees, I could make the following work, presumably: chance(x: T, y: T) -&gt; T { if oracle() { x } else { y } } Joker(T) = chance(Vec, |T| [T; chance(1, 2)])(chance(T, (T, T))); Though writing this, I realized there still has to be a distinction because of the "open function" problem (observable body vs hidden body), and your `expr` vs `const expr` distinction. So what happens is that `Joker(T)` can be one of 8 types, but once `Joker(T)` is instantiated with a given `T`, further instantiations produce the same result, whereas: Joker(T: K) -&gt; K { chance(Vec, |T| [T; chance(1, 2)])(chance(T, (T, T))) } Will give you a type that you only know is a type, if you pass in a type. And multiple applications will have different results. E.g. with that last definition `Joker(u8).to_string()` could be `"Vec(u8)"` on a first call and `"[(u8, u8); 2]"` on a later one. Oh and just because this may be peculiar: that last signature is parametrized over `K` which was `Type` in this case but could be something else: let mut v = Vec(Type).new(); v.push(u8); v.push(u16); v.push(u32); v.push(u64); v.shuffle(); (v[0], v[1], v[2], v[3]) With this occasion, I've also picked method calls on types instead of the `::` associated item syntax. No idea how it would work though. This post is getting long, so long - and thanks for all the ~~fish~~ fun.
Reading the paper, not that well-versed on type theory. Is Thorin a memory safe IR (or potentially a memory safe IR)?
Nailed it! And that is why 1-to-1 translations don't always work between languages. I just thought it was a nifty pattern, and it got the wheels spinning. Now here we are, with a more Rusty solution :D thanks!
&gt; Owned and driven by a company rather than community. So just like Rust?
A compiler does random IO on the host system all the time (by writing object files!)! Anyhow, it is fine that we disagree here! I am taking an extremist position in this discussion after all! FWIW the way I see it is as follows. The moment one has a Rust interpreter, and can interpret rust code and JIT it, the interpreter can perform IO just like the python interpreter does. If that would exist, the feature just runs the rust interpreter "within" the compiler, so it isn't really the compiler which is doing "random IO", but actually the rust interpreter being run. 
Yes, for us C++ interop is the killer feature that would make us switch from Obj-C to Swift. Our software core is written in C++ and the UI code in Obj-C. Replacing the mushy dynamic Obj-C UI code with strongly static typed Swift code would be great. But currently the Swift &lt;-&gt; C++ interop is just too much trouble.
&gt;We wanted a language in which you could system software I wish I could system software :p (Just joking about the typo here.)
I think that deterministic builds are better than this stuff (so one should provide the build system a .sql file that creates the schema and check using it, but not connect to an external database).
&gt; So let's say you pick the "executed at first instantiation and frozen" semantic, with unspecific order. &gt; You would then want to JIT part of a crate to get a type-level constant used in other parts of the crate. &gt; That requires you to be able to "carve out" a DAG and type-check it and compile it separately. &gt; This is quite problematic for syntax extensions (which is why you don't yet see syntax extensions being written in the same crate as the code using them). &gt; You could argue that doing it at type-check time requires less juggling, but you still have to run all the compiler passes on a subset of the crate whilst converting a type from AST form. &gt; That bit is done during type collection, which is a prerequisite for the very next step of typeck (function-local inference/type-checking). &gt; Not doable in the current compiler by any stretch of the imagination. So allowing only calling code that is in a different crate or module would make this much more feasible? I think this would already be huge. &gt; A theoretical one that spawns a process and starts JITing into it as needed (you don't want to deref raw pointers in the same process as the compiler)? Maybe. I was thinking from the very beginning about a rust interpreter, JIT, VM, or similar. Thanks for the rest of the post. It is a bit late over here and I have to read it a couple of time tomorrow morning before I can grok it, but I will and will let you know :)
Personally, I think the highest priority should be improving compile times. The sooner you do that, the more time you save!
There's a workaround: return `Box&lt;Iterator&gt;` instead of the concrete type. You lose a bit of performance from that allocation and indirection, but it's usable in most circumstances.
The faster we compile the easier we make it for people to improve Rust itself!
You're probably right. That's a wart even bigger than this one, although it's not really a problem with the language itself.
This looks pretty far along (ha, like I can tell). Is this work being tracked somewhere? Is there somewhere I should have looked before opening my big mouth? :)
I don't see what that has to do with the discussion, but my embedded experience is mostly with 16-bit microcontrollers where the software is running on the bare metal. I understand the embedded moniker to refer to systems running either no operating system or a very minimal real-time one where the only processes are those directly in use by the system.
The pattern is, after all, a hack to get expression semantics in C++, while Rust comes with them.
It's worth noting that there exist [GNU C extension](https://gcc.gnu.org/onlinedocs/gcc/Statement-Exprs.html#Statement-Exprs) that enables you to do: x = ({ some_code(); value; }); And it's quite broadly used in the kernel.
Those commits should be enough to use `-&gt; Foo&lt;impl TraitA, impl TraitB&gt;` (and simpler cases) in free functions and methods in `impl` blocks. The trait signature form is not implemented (as it means something else, namely "anonymous associated types". oh and please use "anonymized" instead of "abstract" because trait objects are also "abstract"). You could have asked compiler devs on IRC. My branch may very well rot before it has a chance at being merged, but I did find a couple of interesting issues with `impl Trait` (life and type param variance) which I could raise on a new RFC. There's also the thought of allowing `impl Trait` in an associated type in an `impl` and methods in the same `impl` block could set the concrete type (they would have to agree on it, of course). /u/quxxy has been kind enough to write me [an initial testcase](https://github.com/DanielKeep/calendar-example/blob/ef955326e7d5ddb251c33d8775304e0505569e63/bin.rs#L379) so you may peek at that to see what I'm talking about. I've been meaning to implement that part for a few days now, maybe I'll get around to doing it tonight. No promises though.
It lets me do cool match statements when I do this. Can I do that with an empty vec?
Interesting, I'm from Brazil too, I live in Rio currently, and I made a [presentation on Rust](https://docs.google.com/presentation/d/1-rmMUmnZi1gMDbs52Q7BrdaMWk6QPLCafhrU-xgx5zE/edit?usp=sharing) for an assignment of my Programming Languages course. It has a rather fun tone and was kind of rushed, had to present in 15min. If that has any use feel free to consider it CC0. I'd be glad to join RoR, and quite possibly Mozilla, but I feel like I'm completely out of spare time until I graduate in December.
thaks Steve 
No it wouldn't, because `String` doesn't impl `Write`. Even if it did, that's not how `fold` works; you'd have to mark `a` as mutable, do the write, unwrap the result (or rewrite the closure to return `Result&lt;String, io::Error&gt;`, then explicitly return `a`. And since `String` doesn't impl `Write`, you actually have to replace it with a `Vec&lt;u8&gt;`, and do the UTF-8 conversion explicitly afterward... ...which was *way* too much complexity for a attempt to *simplify* the example, which is why I didn't do it. Anyway, a `for` loop would probably be easier and cleaner, which makes for a poor example of functional constructs in Rust. :D
There's a difference between libraries built using stable, and *end-to-end applications* of moderate size built on stable. Cargo, Servo. Those don't work on stable. Got some examples of ones that do? I'm not arguing that you *can't* build (some) things using stable; just that people *aren't* building things using stable; for all the annoyances of syntax, missing apis, etc. it's not worth bothering with using stable at the moment. (... Certainly some things you can't. For example DynamicLibrary...)
Crates that just don't work in general don't count? (I hope we can delist crates like this at some point...)
An IR based on the lambda calculus requires an explicit nesting of scopes (regardless of whether it uses names or De Bruijn indices for variables), whereas Thorin doesn't even have nested scopes.
&gt; no one else is either. Do we have numbers on this? I'm certainly trying to target all my libraries to stable, and succeeding, so far. The major web frameworks and server libraries already target stable. It certainly looks like stable is usable, and so far I've seen no indication that it's not *used*. Brash statements with no supporting data are quite misleading.
&gt; Don't be ashamed of nightly! For pet projects, experiments, or fun, I pass no judgement. But I have very real concerns about using unstable libraries or tools for production intent *work*. This concern is amplified by this whimsical attitude and encouragement in this community that nightly is acceptable for such things. And I'm seeing this behavior more and more. Why is this? 
Would this make it possible to do fn do_filter&lt;I: Iterator&lt;Item=u8&gt;&gt;(i: I) -&gt; ??? { i.filter(|x| x) } *and* preserve the potential double-endedness of the iterator?
This is not really a satisfying response. The whole reason 1.0 was such a big thing is that we could finally commit to a stable Rust; it should be our number-one priority to get as much as we can working on stable as soon as possible. We shouldn't be saying "just use the nightlies" anymore.
The lifetime bounds are there because there are no sane defaults that I can think of that would be based on lifetime or type parameters in scope. Right now `impl Trait` is actually behaving like `Trait`, in that it will default to `'static` (and types included will need to be `'static`) or it can be written like `impl Trait+'a` which will also require bounds. Type parameter variance is trickier. I would suggest something like `impl Trait+PhantomFn(T) -&gt; U` where the anonymized type is covariant over U and contravariant over T, but that's quite verbose. The current implementation treats all type params in scope as covariant. I have also considered `impl&lt;T&gt; Trait` to specify that the type `T` is captured. This could be used like `impl&lt;fn() -&gt; T&gt; Trait` to create contravariance, I guess.
yes indeed, we'll [see](https://github.com/rust-lang/rust/pull/26164) 
I wrote [some slides on this topic](http://kmcallister.github.io/talks/rust/2015-contributing-to-rust/slides.html). They're now a bit out of date, but [pull requests are welcome](https://github.com/kmcallister/kmcallister.github.io/blob/master/talks/rust/2015-contributing-to-rust/slides.md) :)
BitC was another systems level language with a region story for memory management. But where did Rust succeed and BitC failed? (I don't know about the specifics of BitC) PS: I can find [this post from 2013](https://www.reddit.com/r/rust/comments/1jvhxq/bitcdev_rust_gc_and_language_politics/): in the later days of BitC, Jonathan Shapiro (BitC's designer) was aware of Rust's design choices. It was in response to ["Removing Garbage Collection From the Rust Language"](https://pcwalton.github.io/blog/2013/06/02/removing-garbage-collection-from-the-rust-language/) which may also serve as archeology at this point.. edit: on the mailing list archive, there is [this comment chain](http://www.coyotos.org/pipermail/bitc-dev/2013-July/003578.html)..
Kind of. The only way to implement such libraries is going through hackish ways.
I can't answer your question directly, but there was a thread on the rust-dev mailing list back when BitC went belly-up in 2012: http://thread.gmane.org/gmane.comp.lang.rust.devel/1407/focus=1409 (though obviously note that the version of the language being described there is heavily outdated).
Thanks.
It means you'd have to have `fn morph&lt;M: HasHKT, T&gt;(m: M) -&gt; M::HKT&lt;T&gt;` instead of `fn morph&lt;M: HKT, T, U&gt;(m: M&lt;T&gt;) -&gt; M&lt;U&gt;`. And you'd have to establish the identity between `Self` and `Self::HKT&lt;Self::ArgT&gt;` somehow without tripping up the compiler.
No. You could probably debate the level of control Mozilla has over Rust, but it's ludicrous to say that Rust, open source long before Mozilla was officially involved, is "just like" a language that is only now publically planning to go open source.
That sound like a pretty fundamental omission. If I understand this right, I can see one immediate use case where allowing to be generic over `Self` would be useful.
From my perspective there are two ways to implement HKTs. Right now trait dispatch requires that Self be a concrete type (int, bool, etc). If we add the ability to declare higher kinded types so: fn hkt&lt;F&lt;_&gt;&gt;() { ... } When we go to declare something like a Functor: trait Functor&lt;F&lt;_&gt;&gt; { fn map&lt;A, B, G : Fn(A) -&gt; B&gt;(self, fun : G) -&gt; F&lt;B&gt;; } In the above scenario how to we constraint `self` to be `F&lt;A&gt;`? If we continue to allow the ability to declare traits only on concrete types we must then do something like this: trait Functor&lt;F&lt;_&gt;&gt;{ fn map&lt;A, B, G : Fn(A) -&gt; B&gt;(self, fun : G) -&gt; F&lt;B&gt; where Self = F&lt;A&gt;; } Now this isn't too bad to get extension methods (for collection types, or something similar) but what happens when I want to manifest an single `impl` given just a HKT? Do I pick an arbitrary inner type? What if I want a struct parametrized by a smart pointer type? struct PointererList&lt;A, Ptr&lt;_&gt;&gt; where Ptr&lt;A&gt; : SmartPtr, Ptr&lt;PointererList&lt;A, Ptr&gt;&gt; : SmartPtr { head : Ptr&lt;A&gt; tail : Ptr&lt;PointererList&lt;A, Ptr&gt;&gt; } I think overall the design becomes cleaner if we allow for `Self` to become higher kinded, for example: trait Functor { fn map&lt;A, B, F : Fn(A) -&gt; B&gt;(self : Self&lt;A&gt;, fun : F) -&gt; Self&lt;B&gt;; } and: struct PointererList&lt;A, Ptr&lt;_&gt;&gt; where Ptr : SmartPtr { head : Ptr&lt;A&gt; tail : Ptr&lt;PointererList&lt;A, Ptr&gt;&gt; } This doesn't require us to have a dummy type parameter and enables us to freely apply Self to type arguments in multiple locations. 
I'm not sure it's ever *impossible* to express an HKT pattern through HKT associated types, it just forces you to privilege one instantiation of the HKT as Self, and have the rest accessible via instantiating an associated HKT. Ensuring the compiler can reason about it correctly might be tricky though. Edit: I'd definitely prefer a more elegant solution though.
Congrats! :)
I agree that is arguable. It's very hard to say how two companies feel differently to me. But with my observation for several years, I think Rust is trying to provide a proper solution for community (or at least for myself) when Swift is trying to force their strategic advantage to community. Maybe just Mozilla's strategic advantage or goal is coincidentally overlaps to mine. Though I still love Apple products so much, but I am getting tired and disappointing on their SDKs.
Yeah I should probably revise my RFC which is pretty much completely written and submit it if not just for archival reasons. If I have sufficient free time over the summer I may take another stab at implementing it. I believe my official work for the summer will be more focused on internal refactors and optimizing compiler execution, but I would be happy to work on this provided the chance.
Eheh, thanks :)
Note that the type looks much less dirty when you stop using absolute paths (I assume this is from rustc output). In most cases folks will use `type FooIterator&lt;...&gt; = long thing`. With typedefs you can use these types with ease. There are still some cases where you need abstract return types, but typedefs simplify most cases. The rest can be solved by paying a small cost for using `Box&lt;Iterator&lt;Item=Attribute&gt;&gt;` or whatever. It's not really meme-worthy because there are better ways of doing it. There are plenty of other things that need priority. Compile times (folks are working on that), lib stabilization, docs, diagnostics, etc. There are no workarounds for many of those. Still, you're right, this is a wart that needs fixing.
If you're just looking for the syntax of how to create a closure and then immediately call it: let z = (|| { some_code(); value })(); This can be useful combined with `try!` (assume `func1`, `func2` and `func3` all return some `Result` whose error can be converted to a `String`) : match (|| -&gt; Result&lt;i32, String&gt; { let a = try!(func1()); let b = try!(a.func2()); let c = try!(b.func3()); Ok(c + 3) })() { Ok(i) =&gt; println!("Oh yes! ({})", i), Err(e) =&gt; println!("Oh no! ({})", e), } EDIT: Added type annotation to the closure to make it compile
Do you require implementing types to have an identical kind to the Self type? That would be unfortunate, since it would require newtype wrappers in a bunch of places. On the other hand, instantiations of implementing HKTs need to have exactly one HKT they're an instantiation of for type inference purposes, so allowing greater flexibility is also problematic. For example, if you do this via type lambdas (`impl&lt;K&gt; Trait for &lt;V&gt; Map&lt;K, V&gt;`), you need to make it illegal have implementations of a trait on different type lambdas which directly invoke the same type constructor (`impl&lt;V&gt; Trait for &lt;K&gt; Map&lt;K, V&gt;`). This is one of the weird design decisions that makes people want to put off a final design until they have a better understanding of the use cases.
I think it's easy enough to understand such desire. Isn't it nice to "go to" a method by clicking it and to rename all occurrences of a variable easily?
Not sure if it's currently possible to build a bare-metal binary using cargo, but my notes on the same ARM architecture: http://www.hashmismatch.net/2015/05/18/pragmatic-bare-metal-rust.html Basically you'll probably need to build both .o and .rlib files for your Rust builds and link the objects at the end.
&gt; being confident that I can learn anything given enough time ...this would be nice.
&gt;&gt;git clone http://github.com/xojoc/snake-piston cd snake-piston &gt;&gt;cargo run &gt;&gt;# Have fun... If this is the future I like it.
Can one write a syntax extension that evaluates run-time rust code at compile time? I thought that one would end up reimplementing the rust compiler inside that syntax extension but I definitely have to check into that.
I think that if you want to do complex stuff at compile time, build.rs may actually be better than adding a more complex compile-time machinery to the language. It's already written in Rust and reuses as much as you would like from your code. And better yet, it's already available in Rust stable. And it can do I/O too.
I'll check that out! thanks! If it is able to parse my code, find expressions tagged with an attribute, and compile the code necessary to run those expressions into a binary, run it, and substitute the result, then it does a lot of what i want. For type-level integers I don't know how that will pan out, since different results produces different types.
Aaron just nails it as usual. I think we as a community should be quite happy to have him. Naturally it's mostly a fluff piece, but articles like this shape the image of Rust in the minds of readers.
Perhaps even `join` the threads instead of sleeping?
Yes. Just pointing out that early return from a closure can be useful sometimes, which you can't do with a block expression. It's hard to give a realistic but still short example. It's possible to write my example above with a block expression too, I *think* it would look something like this: match { func1().map_err(|e| e.into()). and_then(|a| a.func2()).map_err(|e| e.into()). and_then(|b| b.func3()).map_err(|e| e.into()). map(|c| c+3) } { Ok(i) =&gt; println!("Oh yes! ({})", i), Err(e) =&gt; println!("Oh no! ({})", e), } But in some cases the `try!` workflow would be more ergonomic.
I have a [gist](https://gist.github.com/llogiq/3cf1d2ef58aa816a5dab) of a naive DefaultVecMap (just insert and get for now) that performs a little worse than VecMap, but could probably made faster if it didn't need to clone() when extending the map. `with_capacity(…)` should probably pre-populate, too.
It's not: `ld: cannot find -lfreetype-6`
I'm not seeing this behavior any more than it ever was. It's always been acknowledged that it will take time for the library ecosystem to transition from "everything breaks all the time" to any degree of stability. Furthermore, when people use nightly it provides valuable feedback on which features need to be stabilized, which ends up improving the language faster for people who do want to use Rust in production.
A big +1! This is great!
LLVM yanked support for the C backend a few years ago. I heard recently that there was an effort to revive it, but I have no idea how that's proceeding.
With slice patterns you can do something like this: #![feature(slice_patterns,convert)] fn main () { //let vec : Vec&lt;i64&gt; = vec![]; let vec = vec![1]; //let vec = vec![4,76,200]; match vec.as_slice() { [] =&gt; { println!("empty") }, [_] =&gt; { println!("one element") }, _ =&gt; { println!("many elements") }, } } 
Then surely 1144 crates shouldn't show up on a crates search? If they don't work on stable, or on a post 1.0 nightly, then surely they don't work and aren't of any value to anyone, and are just noise? How to verify that crates compile for those that have non-rust dependencies is an interesting one. Require that crate authors provide a suitably set up Travis link or similar to verify? A lot of this is just down to presentation of packages on crates.io, and having the information to do that presentation properly. Broken crates shouldn't show up (they aren't of use to anyone), nightly-only crates should either be clearly highlighted as such, or should only show up when you tick a box. Only building on nightly is less ideal than building on stable, and if its presented as such the it's a bit of a nudge. Like a compile warning - doesn't *force* you to do anything, but might make you feel a bit bad for leaving it lying around.
Looks great, I'll refactor, thanks.
There is nothing in using peano numbers as integers that creates ad-hoc types or does anything with the macro system. [You can see for yourself](https://github.com/paholg/dimensioned/blob/master/src/peano.rs). The only ad-hoc type created is the unit system that one creates to use. This is a necessary feature of any unit system if it is to be general. The macro is only there because it's all boilerplate code with somewhat messy syntax; basically the perfect place to use a macro. You could certainly just type it out yourself; if you don't care about printing, it's about 30 lines plus one line for each type alias you want and another for each constant. Also, allowing integers as part of type signature wouldn't help this at all. If I had any complaint it would be that using traits as type functions doesn't have the best syntax; &lt;A as AddPeano&lt;B&gt;&gt;::Output gets messy, especially when `A` or `B` is the output of some other operation. It couldn't be too much better, though, as all those tokens (`A`, `B`, `AddPeano`, and `Output`) are required. A final note, I've heard people praise F# for including units of measure in the language. However, briefly reading its documentation, I don't see any functionality that it has that dimensioned either doesn't have or won't have soon. I do see a couple features that dimensioned offers that F#'s units lack: * fractional powers (useful, at least, for [CGS](http://en.wikipedia.org/wiki/Centimetre%E2%80%93gram%E2%80%93second_system_of_units)) * functions: Because they are types, you can implement, for example, `Display` for dimensioned things, and Rust will generate a function for printing the specific unit you wanted at compile time, allowing you to print unit data which can be quite handy for debugging. I'm sure other uses will be found for this as well. * Non-primitives. Dimensioned doesn't care too much what you use it with, allowing you to use it with other types that only accept primitives (see the first example [here](http://paholg.com/dimensioned/work-with-others.html)). I see this as a huge advantage. I don't think that implementing this in the language would have made a lot of sense, but I am very glad that Rust has the feature set that enables it as a library. I hope to get dimensioned to the point that using it is so smooth that it's hard to notice it isn't part of the language. Edit: Fixed a couple typos.
I have noticed that peano numbers are quite slow when they get large. Some time ago, I tried incrementing by one until I hit 63. It *is* an N^2 operation (depending on which side the one is), but I would expect it's a rather simple one. Do you have any idea why it takes so long? I agree that units are probably not something that should be in the language, although I'm quite happy that the language allows their implementation. Most of the features that I want to add to dimensioned seem to be on their way to Rust. I would love to see the ability to embed numbers in types, but it wouldn't even make that big a difference to users of dimensioned --- clearer error messages and faster compile times in edge cases are about it.
Servo’s rust-geom has "typed units" to avoid mixing e.g. lengths measured in CSS `px` units and lengths in device pixels, but there is no fancy unit analysis like speed * times = length. https://github.com/servo/rust-geom/blob/master/src/length.rs
&gt; Do you have any idea why it takes so long? Yes, it does not only take N² time, but also N² space, because all generics need to be instantiated while typechecking the code. And this happens at every use of the type, over all variables. Don't look at the generated IR, it's horrible! Next, LLVM needs to munch all that gunk (because formatting is an operation on the types, it all ends up in LLVM IR) and optimize it into something useful, which I imagine takes the most time – it's a very general mechanism that has to deal with a very specific edge case. That said, perhaps binary numerals as in shoggoth.rs *could* be faster, as they should be within N*logN – there's only one way to find out (*BENCHMARK!*). Also negative trait bounds might be useful to reduce the search space for typeck. As Kirk Pepperdine famously says: "Measure, don't guess."^TM &gt; I would love to see the ability to embed numbers in types, but it wouldn't even make that big a difference to users of dimensioned --- clearer error messages and faster compile times in edge cases are about it. Full ack. It would mainly improve ergonomics for people using specialized fixed-sized container types, e.g. ring buffers (which can be currently specified as fixed-size-array types anyway).
I'm in! Who wanna play with me? My username is `true_tekken_novice`. :-)
&gt;I've been thinking about this the last month a lot, and what I actually wanted is not having to think about constexpr at all, ever again. When I want something to be evaluated at compile time, I just want to tell that to the compiler, and have it compute it. I don't want to think, "is that expression constexpr", or even worse, have to rewrite a part of my code to make something constexpr (sometimes at the expense of making the run-time performance actually worse). This. +inf. I hate spamming everything in my own code with `constexpr` and basically duplicating parts of the STL in order to do really trivial compile-time computations (think `find` or `sort` on small arrays, and not even being able to use `std::vector`). I don't understand Bjarne's apparent repugnance for more compile-time stuff. The `goto` example may seem trivial, but it is enforcing a particular coding style on *every* user. And the whole STL is so incredibly underpowered compared to the language. Just look at MS with their C++11 only `constexpr`: they support essentially the entire C++17 STL with regards to `constexpr`. Just actively enabling the current language in the entire standard library would be a giant leap forward.
A tuple of double f64 could be anything. Commonly, tuples are used to keep types together for a while. Imagine that you need to return a data structure and another thing, like a log, from the same function. You would use a tuple, just to keep them together and, then, separate them. What I mean is: a tuple isn't good to represent a data structure, like a point or something else. Because it could be anything. On the other hand, you have tuple structs! If you just want two numbers to represent a point, without saying anything more, you might as well use a tuple struct. And so, now that you know it's not just a tuple, but a `Point`, you can sleep quiet.
This is interesting, because the given example implies that the pattern above can be rewritten to the callback style: fn with_field(a: A&amp; T, f: fn(&amp;int)) { f(&amp;a.t); } Not sure this can be applied in general. It doesn't look ergonomic though.
You are allowed to make borrows as many times as you want to vec and anything in it. This is safe because altough vec has multiple aliases there cannot be mutation.
The various stages of analysis we do are not lightning fast, but they're not slow either, and nowhere near a bottleneck. This is just FUD.
Thank you! fn main() { let mut x = vec!["Hello", "world"]; let k = &amp;mut x; let m = &amp;mut x[0]; } Complains with: &lt;anon&gt;:4:18: 4:19 error: cannot borrow `x` as mutable more than once at a time &lt;anon&gt;:4 let m = &amp;mut x[0]; As expected.
&gt;draw functions accept f64 parameters so I can avoid type casting. You avoid type casting at the cost of this monstrosity: ` (1.0,0.0),(2.0,0.0),(3.0,0.0),(4.0,0.0),(5.0,0.0),(6.0,0.0),(8.0,0.0),(9.0,0.0),(10.0,0.0),(11.0,0.0),(12.0,0.0),(13.0,0.0), (14.0,1.0),(14.0,2.0),(14.0,3.0),(14.0,4.0),(14.0,5.0),(14.0,6.0),(14.0,8.0),(14.0,9.0),(14.0,10.0),(14.0,11.0),(14.0,12.0),(14.0,13.0), (1.0,14.0),(2.0,14.0),(3.0,14.0),(4.0,14.0),(5.0,14.0),(6.0,14.0),(8.0,14.0),(9.0,14.0),(10.0,14.0),(11.0,14.0),(12.0,14.0),(13.0,14.0), (0.0,1.0),(0.0,2.0),(0.0,3.0),(0.0,4.0),(0.0,5.0),(0.0,6.0),(0.0,8.0),(0.0,9.0),(0.0,10.0),(0.0,11.0),(0.0,12.0),(0.0,13.0), (7.0,7.0), ];` All those `.0` are unnecessary.
Why use tuple structs when you have regular structs? `point.0, point.1` is worse than `point.x, point.y`
Multiple traits are not currently implemented, though that's not particularily hard - the problem is that your suggestion would produce a type that is *always* double-ended, whereas /u/Veedrac wanted to make it double-ended *only if* the input iterator was also double-ended.
You should try compiling rustc and you'll want the same thing.
I wasn't surprised at all: - goto is the only control flow structure not supported by constexpr, - goto is extensively used by at least 3 STL algorithms (it makes their implementation clearer _and_ faster): - `nth_element`, - `stable_partition`, and - `is_partitioned`. One would think that the STL implementors would chime in, but the sad truth is that this feature is discussed in the language Evolution Working Group (EWG) while the library implementors sit in the Library Evolution Working Group (LEWG). I spoke with the author of that proposal and he said that there was vocal opposition to goto in constexpr, and that the opposition is unlikely to go away almost regardless of the arguments/motivation for it. The feeling it gave me is that it was more a matter of principle (goto is wrong), than a technical decision. But then again, the STL implementations use goto more than most programmers, so it just seem a decision that is completely disconnected from reality. If we get constexpr lambdas in C++17, some use cases of goto will be covered by these.
There is a discussion about replacing FreeType here https://github.com/PistonDevelopers/conrod/issues/321, but there seem to be no short term solution. Feedback/input is welcome!
Now that is a circular argument if I ever saw one ;-)
Oh. I see.
That's why we need to get C++14 style data structures and algorithms (however piecemeal, just `array`, `find` and other simple stuff) first, this will act as a driving force to increase language support (lambda, goto, virtual, dynamic allocation etc.). The I/O, threads, warhead launch buttons etc. can come later. 
&gt;slice matching become stable again, it will be possible to easily get multiple mutable references. Can you give an example, please?
Thinking about it, you are damn right! I'll change it. EDIT: changed `f64` to `i8`
&gt; you can't efficiently or safely mutate a UTF-8 string in-place Yes, this is becuase you could replace a single-byte character with a multi-byte character, and that would then cause the size of the thing to change, which would then make a copy, which is then basically `&amp;str` -&gt; `String` anyway.
Also, this: fn index(&amp;self, i: isize) -&gt; usize { ((self.tail.len() as isize + i) as usize + self.headi) % self.tail.len() } 1. Whaaaa???!? Seriously, what are you doing here? 2. You only use this method in one place.
I agree `point.x` is nicer but see the `level1` function I would have to do `Point{x:0,y:0}` instead of `(0,0)`.
I meant, how to use it go get multiple mutable references to elements of the slice?
I was treating the snake's tail as a circular buffer to avoid useless allocations. I now replaced it with ``` g.snake.tail.pop(); g.snake.tail.insert(0, xy); ``` to make things simpler.
An IDE puts a lot of information at in your face and makes things much more visual, which is great for a lot of people. In the same token, a culture of people who think extra, more powerful tools are unnecessary will serve to do nothing but drive the language in to the ground. You think java is so big because it is the best language? Tooling and compatability got java where it is and these two things should be rust (and its early community) primary focus. Between swift, D, num, haskell etc etc and the plethora of other languages, the one that will "win" might not be the best one. It will be the one that has the best, broadest tooling first.
I guess you are referring to [this](https://github.com/draperlaboratory/llvm-cbe)? Seems actively worked on, thats good i guess. 
 if let [ref mut a, ref mut b, ..] = *vec { ... } Here's the tracking [issue](https://github.com/rust-lang/rust/issues/23121) with a link to RFC.
&gt; A simple example: std::mem::size_of::&lt;*const T&gt;() will report a different size on a 64-bits host and a 32-bits target. Which value do you use during CTFE? This happens with cross compiling independently of any CTFE, and the compile-time sizes of pointers are already those of the target platform. The only interesting situation is what happens during the rust interpreter at compile-time? Does it emulate the pointer-width of the target? Doing that is possible (VMs do this all the time). I would be fine if the compiler would just emit a lint and say "you can't do this if you want to cross-compile". More extreme cases are cross-compiling from linux to windows, and trying to use the windows fs API at compile-time on linux. Some people argue ad nauseum "How can we make this work?" and argue that we can't easily do so and that this CTFE idea is thus flawed. My answer has always been: "not all problems are worth solving". A lint can just tell the user that he can't do that if he wants to cross-compile. As repeated ad absurdum above, just because you can, doesn't mean you should. If you want to write a portable GUI application and decide to use Cocoa for your GUI (which works only on MacOS), you can certainly complain to the Rust devs that your App doesn't run on windows. That doesn't mean that it isn't your own fault that it doesn't. It also doesn't mean that the Rust devs should fix this either. In the same sense, if you need to cross-compile an application, and decide to execute platform specific code at compile-time, you can certainly complain that your code doesn't work on the target. It doesn't mean that it isn't your own fault that it doesn't. I think that these two problems are extremely similar. And for the cross-compilation CTFE problem, we can add lints that prevent you from writing non cross-compilable code. I don't see any reason for these lints not to be enabled by default.
Jai, last I checked, also lets you execute arbitrary code at compile time. In one of its demos, the author switched his game to run at compile time, and at run time it just prints the score you got at compile time. That said, I think more deterministic builds are generally better.
Couldn't you have a new method for your strict that initialized all fields to default values?
I maintain that this should never work. It would imply the compiler knows far too much.
The fact that reproducibility already exists in the presence of `build.rs` or compiler plugins is an excellent point.
You mean that non-reproducibility already exists right? I am using the examples of file I/O and network I/O to drive my point that in my opinion CTFE should be able to do anything. I think that these examples represent extreme cases. Reproducibility is good, but if you need a non-reproducible build, then that is what you need (being completely pragmatic here). There are good reasons for non-reproducible builds in some situations, and yes, this feature would make that easier. I don't expect non-reproducible builds to be a common thing tho.
&gt; The only interesting situation is what happens during the rust interpreter at compile-time? Yes, that was the underlying question. You cannot JIT the code because JITting requires the pointer size to be that of the host while for semantics it should be that of the target. The problem of designing a "sub-par" CTFE though, is that once again you split the eco-system between libraries that can be used in CTFE contexts and those that cannot. I agree that reasonable limits need be placed (and saying that FFI can only work if the right libraries are provided works for me). However I am afraid that pointer-size adjustments (and other C integrals) cannot be hand-waved. I may be afraid unduly, mind you, however I'd like people with cross-compilation experience explaining what the difference between the ABI on their host and on their target actually look. Deciding it's too rare to be worth supporting is making a trade-off, not even considering the issue seems irresponsible.
I think functionally, so I don't see any problem since I can unpack it with patterns. :)
Yes, I share your fears, pointer arithmetic should be the bare minimum that a full CTFE system should support for cross-compilation. Right now it seems to me that anything harder than that would be unfeasible in terms of manpower. If we get type level integers and if const fn can be use to compute them, we'll run into the first issues with cross-compilation already, whatever we learn there might help here.
If I find the time I'll whip up a simple benchmark.
Hmm, maybe this works? impl ::std::default::Default for foo { fn default() -&gt; foo { foo { a: "Yeast", b: "Funghi" }} } 
What I meant is, `x` and `y` are better names than 0 and 1. `x` and `y` are good names for **coordinates**. 0 and 1 can be anything. That's the advantage of regular structs.
Every project has different requirements. Nobody should *want* to use a tool in production that they don't need -- that includes Rust itself. If you've decided to use Rust, you've decided you gain more from its memory safety and other features than you lose from its still-developing ecosystem, waiting 30 minutes every time you compile, etc. In most cases it's probably not a good idea to use nightly Rust in production if you can help it. But right now you need nightly for huge complex projects, and if you've decided that your particular project benefits from Rust enough, it might not be such a burden.
The example isn't mine, I ripped it from comments on the RFC. Someone came across it, and I just used it as a dramatic "bad example".
I see.
Everyone here has nailed it on the head, so I'd just like to reinforce a key point here: * &amp; is a share, * &amp;mut is a borrow, Borrows are exclusive, shares are not. So really your question is "Why can I share a subelement of a vector that's already shared?" Which is hopefully self-evident as "you can always share more!" :D
It's not really a pet issue, it's just the most horrific thing of which I was aware. I'm worried that removing "horrors" from the language is not as high of a priority as it should be. I hoped I was wrong which is why I phrased my objection as a question, inviting people to comment on precisely what was more important than this issue (and by extension, other, lesser pain points). It does appear that at least one thing is currently more horrific (compile times), so I am satisfied.
Possibly we can do better about de-emphasizing broken crates, yes. It's a tough question though because it's not trivial to write automation that can make that determination reliably. When crater matures we might be able to surface compatibility information derived from it on crates.io in an advisory capacity. We could also as you suggest support the display of travis build results.
&gt; (I do want to switch to stable at some point, but that's quite far in the future) But then who would test nightly? ;)
&gt; in the macro, then use Meter2 in the function type signature. Not trying to take your motivation to do this way. But why isn't possible to define something like `int!(1)`? Can't we have both? I still find `Meter2` ugly. If we can have something like `Fruit&lt;int!(1), int!(0), int!(0), int!(0), int!(0)&gt;`, then I'm sold and I won't whine about integer/primitive compile-time-value-constrained generics again.
Video is planned!
Glad to see my fix worked for you! EDIT: You're also using the libcore providing package built for zinc. Cool that this stuff is applicable elsewhere as well. Look out for a crates.io published version in the near future.
I'm guessing that the given example predates associated types, which means that many of those types would have shed some of their type parameters since then.
Indeed, but I wonder how much work it would be to just provide a minimal pure-Rust implementation that's nowhere near as sophisticated as freetype. A cargo feature could be used to override the pure-Rust implementation if you want to install the library yourself. I know in my case, I have no use for freetype but I'm forced to install it anyway to compile Piston, which is somewhat annoying.
In general, if you see /u/strncat make a technical claim about Rust then it's pretty safe to assume that it is valid. I think he explains his point quite well in the thread, is there anything in particular that you want more information about? Note that Java has a very forgiving memory model so not having stronger guarantees is nothing to be ashamed of.
I find it very interesting how many concurrency features from the Java ecosystem end up as extensions or libraries in other languages. E.g. in Rust we already have Vyukov-style ring buffers (in java, there is LMAX' disruptor and Nitsan Wakart's JCTools), a fork-join framework (centered around closures, no less!), the usual threads + messaging primitives (although Rust affords more freedom because its design makes more operations safe).
Note that Java went through two memory models and is now at #3. They have made good use of two decades of experience in the wild. If anything, Rust makes using the model *incorrectly* a bit harder. Though today's Java ecosystem also has it's share of static analysis tools that reduce this particular gap. As I'm working in Java professionally for more than a decade, I am quite happy that Java is becoming more Rust-y every year 
You should get wih @WindowsBunny on IRC, he's leading the charge on implementing Windows APIs in Rust. I don't think he's on Reddit though. https://github.com/retep998/winapi-rs 
From a Java intermediate to a Java professional, are there any good Java APIs left for desktop development or is it all web and Android now? I'm trying to get into C# + WPF but there's just something about it that turns me off.
&gt; it's pretty safe to assume that it is valid I... disagree. He knows his stuff for sure, but he's very confrontational in technical arguments, and will distort the facts to further his personal agenda and vendettas. The name of the game is often [fear, uncertainty, and doubt](https://en.wikipedia.org/wiki/Fear,_uncertainty_and_doubt). Bullying people into agreeing with you is not a sign of a strong technical argument. Since you are singing his praises, I hope it's not against the rules for me to share a differing opinion as a counterbalance. And it is just a personal opinion. I am no longer a Mozilla employee; a major reason for leaving is that I want to be able to speak freely about Rust community issues.
I don't know what's going on but yeah, speak freely. Unless it devolves to slander or something somehow, an opinion like this has its place. Especially since it's the opinion of a person on the personality/actions of another: it's inherently subjective, can't not be.
*cough https://github.com/retep998/winapi-rs/blob/master/src/shobjidl.rs cough* With that if you have a `whatever: &amp;mut IWhatever` you can call `whatever.SomeFunc()`, and if `IWhatever` inherits from `ISomethingElse` then you can pass an `&amp;mut IWhatever` to a function that takes a `&amp;mut ISomethingElse` thanks to cross borrowing. That said, my COM is a very thin lightweight abstraction, so there's plenty of room for someone (like you) to build a fancier abstraction on top of it.
To my knowledge, Java's top-of-the-class garbage collector enables *way better* concurrent data structures than can be easily written in Rust. Most of the literature apparently relies on a garbage collector to handle ownership problems in a concurrent context. For instance, consider a simple singly-linked-list. You win whatever race to pop off a node from the list, but other threads may still hold a reference to this node. This is fine since its tail points to a valid list continuation (or null), but it means that you can't free the node yet. If we have a garbage collector it literally doesn't matter: just forget the node and it will go away when it's unreachable.
I would not like to enable that for a couple reasons: * Fundamentally, `One` is a type. That is lost if it is replaced with a macro, and I find it makes expressions much less clear. * Regular macros cannot be used in place of types (which is the reason I have not yet implemented the macro for making derived units), so `int!(1)` representing `One` would require a syntax extension, if even at all. What I would consider is renaming the type aliases shipped with peano.rs. For example,`Zero`, `One`, `Two` could become `Peano0`, `Peano1`, `Peano2` or even `P0`, `P1`, `P2`. That still leaves a difficulty with negative integers. `P-1` could not work. I could use `P1` to mean "positive 1" and `N1` to mean "negative one", which is what tylar does. However, that still has the difficulty of what do you call `Zero`? I would want something of a similar form, but there's nothing that really makes sense. It's also quite easy for a user of dimensioned to make their own type aliases, so I'm not sure it matters all that much what they are. The only one that is not an alias is `Zero`. Also, in practice I believe that you'll find that you interact with these numbers so little that it doesn't make much difference what they're called.
See [this chapter of the reference](https://doc.rust-lang.org/reference.html#ffi-attributes). It seems like it's possible to specify link args for an extern block. You can also try exporting `LD_LIBRARY_PATH` and `LIBRARY_PATH` with the snappy/lib directory included.
The claim misses the forest for the trees. In particular, one of the important aspects of Rust's concurrency story is the `Send` and `Sync` traits, which prevent you from sharing objects between threads that are not thread-safe. Now, it's true that "thread safe" here basically means that there is *some* form of "synchronization" (which could simply be unordered atomics). But merely by requiring that much, you instantly rule out a huge class of mistakes where you've sent a value that has no synchronization across threads. This is something that Java, and other languages, don't currently give you much help with. (You just have to read the docs for a class saying: "This operation is not thread safe".) One of the clearest examples here is the difference between `Rc` and `Arc`. Both provide reference counting, but the latter does so using synchronized operations (which are slower). In many other languages, it would simply be too dangerous to use something like `Rc`, because it's too easy for such a value to make its way to another thread and thereby wreak havoc. But in Rust, you can fearlessly use `Rc` in single-threaded contexts because you know the compiler will catch any attempt to do otherwise. So in terms of hard, pedantic, technical guarantees, both Rust and Java provide the same thing -- no *undefined behavior* due to data races. **But the key point is that Rust *also* marks types as thread safe or not, and enforces this at thread boundaries for you, which in practice is a huge win.** You can still create racy code and deadlocks, but only by writing code that is explicitly using concurrent (e.g. atomic) operations. I've laid out this and some other high-level aspects of Rust's concurrency in a [blog post](http://blog.rust-lang.org/2015/04/10/Fearless-Concurrency.html) that might be helpful.
True. I suppose I was imagining type-level numerics and arithmetic being implemented at the same time, but there's no reason numerics would require arithmetic. Edit: And just to be pedantic, you would want something like fn multiply&lt;T, N, M, K&gt;(a: [[T; N]; M], b: [[T; M]; K]) so the columns of one matrix match the rows of the other.
If you want to use this with a cargo project, simply adjust any of your relevant cargo profiles like so: [profile.release] codegen-units = 4 lto = false LTO and parallel codegen don't play nicely together, so LTO must be disabled. `codegen-units` in cargo is currently undocumented, but I found the issue on the cargo repo yesterday and I'll finish writing this up and open a PR tonight :) Speaking of LTO, in another thread the other day, people were saying that it is very slow. It should be noted that [an RFC for a faster "ThinLTO" implementation has been proposed](http://article.gmane.org/gmane.comp.compilers.llvm.devel/86282) on the llvm mailing lists.
Thank you... I had not seen or tried `link_args` yet. When I do try it, I hit a different error: error: unstable feature #![feature(link_args)] note: this feature may not be used in the stable release channel error: aborting due to previous error \(edit) Yes I'm able to get it to work if I compile with LIBRARY_PATH=snappy/lib cargo build and then I can test it with LD_PRELOAD=snappy/lib/libsnappy.so.1 cargo run but dang, I wish there was a way to handle this in the build system.
Some platforms are not supported by LLVM and/or come with their own proprietary C compiler.
Wrong subreddit. You want /r/playrust
I think we're in vigorous agreement :)
I would say something a little bit stronger. In Java, if you try to use a HashMap in multiple threads without synchronization, it is possible for the data to become corrupted. This is not UB in the very narrowest sense, but as a practical matter, you need to understand whether a given type is threadsafe to know whether you can safely use it without synchronization. In Rust **any safe type that Rust allows you to use across multiple threads is safe to use across multiple threads without additional synchronization.** As Aaron said, this is because Rust defines Send and Sync traits that are the way to articulate, as the author of an abstraction, that specific guarantees apply. This means that if someone authors an abstraction using unsafe code and claims to be threadsafe (via the traits, **not via documentation**), it is a clear and serious bug if they are wrong. This is a huge improvement from my experience of trying to reason about threadsafety in other languages.
The question is, is this really necessarily the case? For example, if I opt to restrict myself only to numeric types, I can make a memory-safe hash map that cannot cause UB when accessed from multiple threads, but *does* have races. It would not be a very useful one, granted, but there is nothing in the language preventing you from building something like that. What Rust brings to the table, IMO, is: 1. Relatively easy to reason about thread safety (`Send`, `Sync`, lifetimes, explicit atomics, etc.) 2. The lack of a garbage collector, and support for things like unaligned access and fat pointers, makes it really hard to write a memory safe, concurrent data structure of any sort. The combination of those two mean that there's a relatively clear separation between single-threaded and concurrent data structures, and that separation can be enforced by the compiler. Thus, there's no need to have structures that are really single-threaded but have to be safe when accessed concurrently (i.e., nobody is going to bother building the racy hash map I described above).
I admit to having been slightly over-strong in my comment above. The point I was trying to make was that the relevant caveats don't properly capture the extent of the improvement to local reasoning. It's similar to Rust's story about memory leaks. It's certainly possible to leak memory in Rust (especially when Rc is involved), but the fact that the language improves local reasoning about leaks so much makes it tractable for a mere mortal to diagnose and fix leaks. The fact that "tractable" is less sexy than "perfect" is unfortunate, but it doesn't materially change the extent of the improvement in my experience.
(And this is all analogous to safe abstractions around unsafe code; you have to "ask for trouble", and marking your code safe is promising a certain contract.)
Check out build scripts: http://doc.crates.io/build-script.html
With that attitude, I hope you're not using any of crates with a 0 major version. There's no practical difference between using a crate like that, and unstable API in the standard distribution.
&gt;When you call a method on a value x, and the compiler can't find a suitable method, it tries again on x.deref() Does this continue on down the chain, so to speak? Will the compiler check x.deref().deref() etc? 
Yes.
Regarding corrupted HashMaps, that's what the `Concurrent*Map` classes are for. You wouldn't share an `Rc&lt;_&gt;` in Rust, either (well, fact is you cannot, without unsafe). Also even normal HashMaps try to throw `ConcurrentModificationException`s on a best-effort basis. However, I'd argue that syncing on the map is a bad idea in many cases anyway. In most algorithms, you are not content with stopping just short of clobbering that map, you also want to uphold some ordering of events, so that if two events race, always the same one ends up in the map. Or you want elements to be unequal or something. Recognize that even an `Arc&lt;Mutex&lt;HashMap&gt;&gt;` provides no such guarantees. So you will need to introduce some machinery (which itself is usually synchronized) to uphold those stronger guarantees anyway. At this point, you can safely use a HashMap, since your machinery will take care of synchronizing access to it.
I couldn't compile my project with `codegen-units=4` because I hit [an ICE when building the `rand` crate](https://github.com/rust-lang/rust/issues/26206).
Check out [serde](https://github.com/serde-rs/serde), which lets you mark fields that will use the default value if it's not specified. 
It's a bit stronger: fn main() { let mut x = vec!["Hello", "world"]; let k = &amp;mut x; let m = &amp;x[0]; } Complains: &lt;anon&gt;:4:12: 4:13 error: cannot borrow `x` as immutable because it is also borrowed as mutable &lt;anon&gt;:4 let m = &amp;x[0]; 
Thanks for this, I will have a look at that :)
It seems serde and rustc_serialize is having an overlap. Which library should I use for doing json serialization?
Programs written in languages like Go have to constantly ask themselves "Is this safe?" and "Is anything still using this?" while running. Taking the time to do that adds up. Rust proves, at compile time, that certain types of unsafety are impossible and figures out, at compile time, exactly when to free memory, which means it doesn't need to insert those time-consuming checks. (C and C++ also don't insert those checks for you... but you have to do what Rust does yourself and, if you make a mistake, your program will crash or leak memory or corrupt data or do something else horrible.) The downside is that you'll need some practice to get used to the rules Rust enforces to make those compile-time checks possible.
Niko Matsakis apparently has a patch that reduces compile time of the largest servo crate (script) by 33%.
The `use` statements tend to be relative to the module. The `extern crate` imports are in main, so (if I remember this right) you can do `use ::hyper::{...}` in server.rs, or `use super::hyper::{...}`.
I didn't even know the original RFC was closed, since i considered the RFC my own personal number 1 RFC to wait for :( Sure, everyone has other priorities, but this this should really be much higher on the list and not on the "probably some day" list. This is an almost meaningless classification that will always be overruled by other feature requests. At least that's what it is communicating to me and other developers.
I absolutely agree with this RFC... this is exactly the kind of breaking change we can tolerate. Low risk, many benefits down the road, no burden on users. I understand it's not to resolve a safety concern, but quite frankly, I think the language will suffer if a change like this can't pass community muster. Should we strangle ourselves in pursuit of stability?
Note that this isn't planned, but suggested. It's a breaking change which doesn't have any breakages on crates.io, and it makes things a lot less confusing, but it isn't necessarily going to be accepted, it's still an open issue.
Tough decision. These changes look great, but maintaining strict semver adherence is important too. Going to 2.0 over such a small change would probably be a PR nightmare. Personally, I think it is acceptable to make a small breaking change to gain a ergonomic win.
This might interest you: &gt; @nrc made some great progress on tracking performance metrics for the compiler, and the frontend [is looking quite nice](http://www.ncameron.org/perf-rustc/)! &gt; &gt; – https://github.com/rust-lang/subteams/blob/master/tools/reports/2015-06-10.md
Aren't you talking about nrc's parallel codegen? I have a patch that should give 20% improvements on no-opt, however.
I wonder if using a Python-like `from __future__ import ...` mechanism (like somebody in the issues suggested), or something similar, and then badgering the writer with compiler warnings might the way to go. That way there would be no breaking changes but the issue still get's sorted. Obviously you'd be adding a bit of boilerplate but I don't think that's too much of an issue. I think the biggest issue would be that you can always trust people to ignore the warnings rather than fix the issue.
There are infinite small changes that would be ergonomic wins -- why ever stop? If BREAKING changes aren't the "line in the sand" -- what the heck is? 
From the README, this appears to be a network layer library to use for NAT piercing in a P2P context. It sure advertises many features, but there is little information on the interaction between them. Suffice to say I'm probably not part of the target audience.
Please read the whole document. It even says &gt; We reserve the right to fix compiler bugs, patch safety holes, and change type inference in ways that may occasionally require new type annotations. We do not expect any of these changes to cause headaches when upgrading Rust. &gt; The library API caveats will be laid out in a forthcoming RFC, but are similarly designed to minimize upgrade pain in practice. You can now read the semantic versioning policies here: * [For the Rust Language](https://github.com/rust-lang/rfcs/blob/master/text/1122-language-semver.md) * [For the API](https://github.com/rust-lang/rfcs/blob/master/text/1105-api-evolution.md)
I'd be inclined to agree, except for the following: * I've seen two cases in the last week where the current rules bit newbies *hard*. I haven't been looking for instances; that's literally just periodically checking the IRC channel and looking at Stack Overflow. * A comprehensive regression test against all the crates on crates.io turned up *zero regressions*. Not a *comprehensive* audit of all extant Rust code, but reasonably compelling. I feel like this *particular* change might be OK because the current rules are clearly not working, and you can make a decent argument that the current behaviour is not being widely used. I mean, is it really a breaking change if it doesn't break anyone's code? :P The concern, of course, is that there's code out there (not on crates.io) that *is* going to break. Come to think of it, it might be a good idea for the core team to put up a set of precompiled binaries and ask people to test their code against it to see what happens. If few enough people break, the problematic code could be fixed for the sake of everyone else. Might also be an idea to issue a 1.0.1 release which *just* adds a lint that detects this situation and warns about it for anyone who uses a 1.0.x compiler in the future for any reason.
Time to switch to Servo! Just to check the compile time, I'd be surprised if that page worked ;)
No idea: I've only seen him mention the results, no real specifics. I'm sure a PR will appear if/when it is ready. (I have the impression that it is adding caching/tweaking code-paths inside the type checker, but I'm not sure.)
[It doesn't quite support enough JS](http://i.imgur.com/r8GHGCz.png).
I don't see what's wrong with doing quick versioning on a programming language, just like Chrome does.
Cool. 33% would be huge for me.
Note that the reason to avoid *breaking* changes is because they tend to *break* things. Now I have argued elsewhere that if the cost of breakage resulting from a misfeature is greater than the cost of breakage resulting from fixing it, it should be fixed. In this particular case, the breakage resulting from the change is certainly going to be cheaper than keeping things as is. I don't care much if we bump version numbers for this or not. No code could be found to depend on the feature, and the people who have shown up to argue against the change (myself included) have done so from general principles, not because of the change itself.
How would badgering people reduce the cost for programmers as opposed to just fixing the feature and adding suitable compiler errors? In fact, this would be the worst of both worlds. As an aside, it was I who made the suggestion.
Which **"reserved right"** are you invoking? Are you claiming this is a compiler bug? Safety hole? Type inference tweak? Point me to what specifically makes this acceptable breakage... because I fear I simply do not understand something you obviously think you do. Enlighten me. 
In general, the problem is not with quick versioning, but with breaking changes. If there is too much churn, the cost of keeping Rust code working correctly will be too high. As for this particular change, it may be a breaking change, but it will probably reduce breakage.
If you just have the `from __future__ import ...` mechanism then there's a good chance that people will either just not bother to use it (like the whole Python3 fiasco) or won't even know of it's existence in the first place. Emitting compiler warnings when people don't use the new functionality will at the very least solve the second issue and hopefully help with the first issue as well.
&gt; Now I have argued elsewhere that if the cost of breakage resulting from a misfeature is greater than the cost of breakage resulting from fixing it, it should be fixed. But... that isn't the bill of goods we were sold. I am fine with that being what you think it should be -- but that isn't what was messaged to the community. You seem to be in favor of any degree of breakage in the wild as long as it is a community-net-positive (helps more than hurts), which would be a never-ending-set of breakages... much like pre-1.0 Rust. 
Exactly my point. As an aside: I don't see Python3 as a fiasco. Breaking compat was a tough decision made for technical reasons and it is only due to the strength of the python community that they are slowly crossing that chasm. ~~Now let's not talk about Perl6~~
It was a incredibly tough decision but it should not have taken almost a decade for widespread adoption, in my eyes that *was* a fiasco.
Any change on the licensing? The company still wants to _own_ all intellectual rights for the project? https://github.com/maidsafe/crust/blob/master/CONTRIBUTOR &gt; you hereby assign to us with full title guarantee all the intellectual property rights (existing and future) in your contributions http://maidsafe.net/network-platform-licensing &gt; we believe that the potential for the SAFE network to become so pervasive throughout peoples lives, means that no person, company or organisation should own the technology.
&gt; A comprehensive regression test against all the crates on crates.io turned up zero regressions. Not a comprehensive audit of all extant Rust code, but reasonably compelling. Then would you be fine with this as the "new" policy? As long as you can show 0 regressions against crates.io you can ship any breaking changes because they likely fall under "reasonable private repo breakage"? If that is your argument, it is interesting and I would like to hear more... at least that would allow some level of coherency rather than a series of one off decisions against promised policy. It could additionally lead to some interesting solutions. IE: If you do something a little more unique or different, you can put an example in crates.io to force breakage of stuff like this to protect your internal systems. You end with crates projects JUST to demonstrate "bugs as features" or odd internal uses. Honestly -- might be a better way forward for the Rust community -- crates.io as a universal breakage check. 
&gt; You seem to be in favor of any degree of breakage in the wild as long as it is a community-net-positive No. I'm actually in favor of stability – note that I authored [RFC PR #1147](https://github.com/rust-lang/rfcs/pull/1147) that is about deprecating `std` APIs while keeping backcompat. I do however think that language features should be allowed to change *iff* the resulting cost of the change is marginal *and* the cost of not moving is proportionally much higher. I will still oppose vehemently any backcompat-breaking change where any of these two requirements doesn't apply. &gt; [...] which would be a never-ending-set of breakages I doubt that. This particular case has been shown to *only* trip up people. No code that relies on the feature has been found on crates.io, and so far no one has stepped up with more than the same slippery slope argument you invoke.
&gt; it should not have taken almost a decade for widespread adoption So you think all those third-party libraries upgraded themselves?
&gt; if the resulting cost of the change is marginal and the cost of not moving is proportionally much higher. Which is fine and fair... but I feel that wasn't what was messaged in http://blog.rust-lang.org/2014/10/30/Stability.html -- I might have misinterpreted it, but that was not at all my understanding. As I mentioned in [my reply](https://www.reddit.com/r/rust/comments/39f2t7/planned_breaking_change_in_rust_11/cs2y1zz) to /u/Quxxy -- maybe there is a better way to measure breakage than simply saying "not allowed"... maybe crates.io representing all features people depend on is a better way to handle stability. Private companies with internal repos could even upload "examples" of exotic feature use to crates.io specifically to trigger breakage and signal dependency. 
The current beta is 1.1, versioning is *inherently* tied to the trains. Patches can be backported to beta (especially now, since the process is just starting: the first few cycles are not normal).
IMHO, crates.io is a *very* biased sample of rust code. It mostly carries libraries that have been around for some time, if I'm not mistaken. &gt; Private companies with internal repos could even upload "examples" of exotic feature use to crates.io Many companies are very secretive about their code, so I doubt this option would be used much.
My point is that the problem of missing libraries was entirely predictable (I posited this in 2004, but cannot find any link to my statement. Pity).
&gt; No code could be found to depend on the feature I can practically guarantee to you that there are people watching this situation and giving marking Rust down in their minds. We were told we'd have stability and no breaking changes and now here we have a breaking change. If you have Rust code in your private repos that happens to have a pattern that no one uses on crates.io, you can't depend on your code compiling tomorrow.
&gt; It could additionally lead to some interesting solutions. Heh, that's actually quite an interesting approach... It also has the benefit of being "self-conservativizing". That is, as the language matures and attracts more and more crates, changes automatically become more and more sensitive to being "breaking", as there's a much bigger corpus of packages to test against. Of course, all these changes are just breaking in the compilation sense. I could see there being changes that still compile but introduce logic bugs.
It looks like the aim is to go the MySQL route: GPL software but all rights to contributions are assigned to the company so they can sell proprietary versions.
Yup! It's working today, thanks.
Practically speaking this change looks pretty risk free. Philosophically why go to "1.0" if you're willing to make breaking changes?
&gt; Philosophically why go to "1.0" if you're willing to make breaking changes? I have a number of thoughts here, but I'll just point out the biggest thing: Pre 1.0, this change would have just shipped. Now, because it's post 1.0, there's a lot of thought and discussion, and it may or may not actually ship. That's a very big difference.
That isn't how the JIT worked. It was still module-at-a-time.
Fair point. I guess I'm more a curmudgeon when it comes to semver.
I am too, actually. I personally lean towards -1 on this, but what I'm mostly concerned with are the social aspects of this: the reason to take a hardline approach to breaking changes is because doing work for upgrades isn't great. But, if this change is technically breaking, but basically affects no-one, that addresses the root concern.
I don't know, I think we're taking stability very seriously! In my view, stability at this point is absolutely essential for Rust to succeed. It's the only way to achieve widespread adoption amongst production users, after all. &gt; Actual SemVer adherence was never going to happen. To make any sense of SemVer, you first need a definition of what is and is not a breaking change. We've already accepted two RFCs ([1], [2]) that describe more precisely what stability means for Rust (including outlining areas of the language that we still consider explicitly unstable). Not to mention that we are also regularly testing nightly builds against all the code that is available to us (and making efforts to increase the size of that set beyond crates.io) to detect regressions. We really don't want to see breakage. Now, it's true that I've proposed a change that constitutes an exception to the above policy. You might consider this evidence that we're not committed to stability, but I disagree. Consider that we're talking about a change that breaks *no known crates* and which basically everyone agrees is an improvement. Even so, it's up for debate whether it's worth making the change. If that's not evidence that we've made a big shift in our approach to stability, what is? :) More seriously, the RFC that proposes the change also goes to great lengths to elaborate on why the impact is believed to be negligible (or possibly even positive, as others have argued). [1]: https://github.com/rust-lang/rfcs/blob/master/text/1122-language-semver.md [2]: https://github.com/rust-lang/rfcs/blob/master/text/1105-api-evolution.md
The problem isn't the ease of developing the tool, but the ramifications of doing so. Keeping the opt out explicit is a deliberate choice that reflects (1) the belief that very, very few crates (perhaps none) will actually need to opt out and (2) that the revised rules for lifetimes greatly reduce confusions and footguns, so we really want to move code in that direction. An explicit opt-out helps acknowledge the fact that an important improvement has been made and one should move code over to it ASAP -- and I don't think that it's really any harder to add than running a tool would be. (FWIW, I also don't anticipate adding this kind of opt-out very often at all. The trains and stabilization process, going forward, should give us more time to vet things like these defaults; 1.0 was a special case because so many different features needed to coalesce at the same time.)
Some tangentially-related Rust history: `crust fn` was the original name for what became `extern "C" fn`. Despite being something that was always understood as being needed, the use case that drove its implementation was the first pass at libuv bindings, which was also my first PR to Rust! &lt;/wistful-rumination&gt; https://github.com/rust-lang/rust/pull/1878/files#diff-af4102b893eaf6be433b86f6ae4e0031R463
That's actually (partly) nrc's fault, he used experimental browser APIs with no polyfills.
If you let your barber cut your hair, pretty soon he'll chop your whole head off!
You're looking for /r/playrust
The JIT just used LLVM in the mode where it writes machine code to memory instead of to disk. Same slow compile pipeline, working on a whole crate at once.
I'm impressed that this kind of post still happens.
I plainly didn't say that everyone against this RFC is a whiner. But I'll draw your attention to comments like &gt; If this is implemented -- Rust's "Stability as a Deliverable"[1] will be looked at as a pathetic joke and nothing more. or &gt; why ever stop? If BREAKING changes aren't the "line in the sand" -- what the heck is? Maybe you can lecture those people on being "constructive".
&gt; Even one breaking change can be a big problem if you have a lot of code - you don't want to spend x many hours/days/weeks fixing your codebase needlessly. That may not happen at all in this case, but the principal still stands. Yes, I meant to define the "rate" of breaking change by the amount of code that's affected and not (say) the number of commits to the compiler.
Ok, so "1.0" promises a certain level of stability, and a breaking change in the very next minor version destroys that promise. But I think we could possibly have our cake and eat it too. Python has something called "\_\_future\_\_". It's basically an opt-in statement that individual coders add that pulls in newly added breaking behavior before the next major version is released. Seems like adding something similar would allow us to have breaking changes added without actually forcing the breaking changes on all users. Although it does increase the amount of code that developers need to maintain, since they need to maintain both the old behavior and the new behavior. Cheers!
I said that having breaking changes this early was inevitable, not that you don't take stability seriously. I just don't understand the point of certain people on the Rust team making a big deal about following SemVer when that was clearly never going to be the case. Why even invoke SemVer when you aren't actually following it? Why not just say exactly what you ended up doing, that you take stability seriously and the bar from breaking changes in a minor release is substantially higher, but not infinitely high? That seems more honest to me.
I should totally write `--pretty` and `-Z ast-json` commands for my Sublime, great idea! :D
Or compiler can just tell you to add exactly this line so that you don't need to investigate it.
The github discussion doesn't sound like detecting this situation would be feasible. If the compiler only knows that it doesn't compile, but not the deeper reasons that would be required to give specific advice.
Huh, it is refreshing to see a Rust-related program written in WPF. It looks clean and simple.
What is a "real 1.0" exactly, if not this? All sort of 1.0 releases are still considered "evaluation phase"- 1.0 is what allows that to happen, since before that you're not even sure what you're evaluating. Further, Rust has a very specific definition of what "1.0" means, as well as a very strong case that it's being followed. [The stability guarantees announcement](http://blog.rust-lang.org/2014/10/30/Stability.html) says "We reserve the right to fix compiler bugs, patch safety holes, and change type inference in ways that may occasionally require new type annotations." This is a type inference change. It has only been stable for a few weeks, and it breaks *literally nothing* on crates.io. The only thing it *could* break is private code that is still under active development anyway due to its recent stabilization. There is still far more pre-1.0 code that people want to use but can't because it's broken. So what exactly do you want the Rust devs to say? Issue a public apology for lying about the 1.0 release, and that it should actually be 0.14 or something? That accomplishes nothing and only muddies Rust's definition of stability.
Let's go to 2.0! We could have more parties!! :)
&gt; Now, it's true that I've proposed a change that constitutes an exception to the above policy. You might consider this evidence that we're not committed to stability, but I disagree. To me it looks like the goalpost of *stability* are being moved. Having policies of what "stable" means is fine. Just plain "stable" might be idealistic - you might want to have wiggle room for breaking changes for bug fixing. But now it seems like the goalpost of "doesn't break existing code" is being moved to "doesn't break existing code *that we know of*". What's the next step? A new proposal for a breaking change might justify it with "is as good as trivial to fix for users". Maybe the next is "isn't hard to fix for users". And so on. Maybe I'm not an expert on *semver*. But past a certain a point, it feels like one is playing some lawyering game over definitions. Why not just bump to 2.0.0 after this proposed change? That might cause some people to be like *ho hum, Rust and stability*. But at least it's very consistent. As opposed to making exceptions (which you seem to be saying here).
Technically the goalpost for stability always included inference changes. Now whether or not this is an inference change is debatable but it's pretty close nevertheless. Besides, the linked RfCs mention worse breaking changes that are considered stable (eg changes that require UFCS to clarify a previously-inferred trait). Over here just adding an attribute to the top fixes it.
Looks nice, you're not a newbie anymore after having accomplished this. Using `return` on the last line of a function is not rustic.. just put the value you want to return in a terminal expression in the function. *Without semicolons*. Not sure why you use `T: Copy` in insert. A quick look suggests it should compile without it, what am I missing? You can use `debug_assert!` for assertions that you want to have removed when compiling for benchmarks or in release mode. You can add `#[derive(Clone)]` to your `struct Node` and then the whole structure can be cloned. This code: match search(key, root) { None =&gt; return false, Some(_) =&gt; return true } This is exactly how `Option::is_some` is implemented, so you can replace this with just `search(key,root).is_some()`. Option has a lot of methods that are just simple applications of match like this. Bonus fun: You can use the `rand` crate or even `quickcheck` to fuzz test your structure if you define some testable properties. 
While this is true, there's also fairly little backporting already. There's only 17 new commits on `beta`. While this will drop pretty close to zero, it's still not many. There are 848 commits on `master` since the fork.
on the contrary, if the change is breaking but affects exactly zero third party libraries, what's bad about it?
The condition you mention cannot be guaranteed, and will still break promises. What's the use of ensuring no third party code breaks if first party code will still break? Merely checking crates.io guarantees nothing. The 1.0 milestone was exactly where these kinds of discussions should have ended, regardless of whether you are in favor or opposed to this suggested breakage.
of course it will. i don't argue otherwise. i'm saying if there's a time for taking this tradeoff, it is now, while the language is still hot and can bend easily. i wasn't a fan of saying that rust will be 100% rock-solid stable post 1.0, because the waterfall model does not work for software. this discussion is just another example of it. IOW i don't care for the promise to be broken when it makes sense. i don't want the promise to become the rule, because that means stagnation. imagine if every java release was compatible with earlier versions. it's stagnating as it is, it'd be frozen in mediocrity forever if sun kept their promises. 
i think that rust is in a place where if you have a breaking change that improves the language while not really breaking that much in practice and you can't decide, you should default to 'let's do it'. otherwise you'll be stuck with a tool that can't be improved because promises even if there's no other argument. 5 years from now, yeah, we're waiting for rust 2.0. today, just fix the problem, whatever it is.
Compiler can check if code compiles according to the old rules but does not according to the new rules. Although this doesn't allow to distinguish between situations when user has upgraded compiler and when this is a newly written code.
what's the definition of a breaking change? this change is technically breaking, as in there is a possibility that some code breaks, but practically it isn't, because code that would break doesn't exist. i prefer the practical approach.
so, does the promise hold for you if the change gets merged into 1.1?
there's a huge quality difference between &gt;Before Rust stable - better fix things now, before it's too late. and &gt;Shortly after Rust stable - this is a great time to fix things, since the damage is likely to be minimal. if the damage is not likely to be minimal, the breaking change is a no-go. but if it is? is a breaking change that doesn't break any code in practice a breaking change? what's the cost of not making the change? is an in-practice-non-breaking change a promise-of-stability breaker? the only habit i'd like to see in rust is of answering such questions for every technically breaking change. i'd also like that to be in the definition of *stable*. i don't want *stable* to be equal to *not-improvable-for-any-reason*.
&gt; Right, but a `&amp;str` is UTF-8, not ASCII. It seems that `to_ascii_*` ignores non-ASCII characters. I guess that there is no risk for single-byte ⇒ multi-byte conversions. [Playpen](http://is.gd/CAK03t)
I think that keeping things stable as much as possible is good but should not be surprising that there are some breaking changes popping out with the first couple of stable versions. Imho it's better to fix it now than have it messed up until 2.0 or forever 
I actually only test in Firefox, so I'm glad it works in Chrome. It should be fine in Firefox nightly, I haven't tested in anything more recent. Perhaps its just because I'm running on a fast machine?
Right, but you can't know that for an arbitrary `&amp;mut str`.
Indeed, the whole point of the RFC process is to foster discussion and make the direction of the language transparent. I encourage people to comment on the RFC directly if they would like to give feedback.
I was recently [gilded](https://www.reddit.com/r/rust/comments/37ujvx/i_wrote_a_website_in_rust_and_lived_to_tell_the/crq2t47) for mentioning that I'm working on a "cargo check" subcommand that would typecheck your entire crate hierarchy without bothering to perform any code generation, which should drastically improve feedback cycles for when you just want to continually perform sanity checks on your code. I suppose I have until the gold expires to implement it. :P
I believe https://doc.rust-lang.org/stable/std/sync/mpsc/fn.channel.html is similar. Here is an [example](https://gist.github.com/hjr3/14a23e3f1872633ed5fb) of the telegram problem solved using channels in Rust.
I'd be quite interested to see your revised RFC.
Well, for those crates, you can continue using the version you're using, and nothing will change. A new nightly version of rustc, however, could break everything.
We do plan to use some kind of rustfix tool to help address breaking changes in the future, but more of the kind of breaking changes which would take use from 1.n to 2.0, rather than this kind of thing or adding the #[legacy...], stuff. In particular, it would be hard to use here because it doesn't exist yet :-)
&gt; In the face of ambiguity, refuse the temptation to guess. -- Tim Peters, "The Zen of Python" You're right. This is not good design. It would solve our immediate problem by creating a host of new ones (apart from the implementation being intractable). Luckily the core devs appear to be warming up to my suggestion of using target version declaration (see [RFC PR 1147](https://github.com/rust-lang/rfcs/pull/1147), which, while written with the `std` API in mind, also works well with language features. I believe this is the best solution: We break absolutely no code, we are free to implement both behaviours depending on target version (and we can also allow explicit opt-in/out) and it is always 100% clear which behaviour should be invoked.
&gt; Customer shall pay to MaidSafe 1% of Qualifying Revenue.
&gt; nor parallel codegen which certainly does (but isn't on by default). I'm having difficulty thinking of why. Is there a problem with it that stops it from working for all projects? Or does it just need some time to reach a good level of reliability?
This may be biased toward the language people are more familiar with, posting it here may not give you the answers you want.
Yeah, its nice for internal company versions and for when things are moving slower, but at this stage I think its ok to diverge a little from strict SemVer. 
I have not written code that would break with this change, but I *have* written code in which I needed to add `+'static`because of the current setting. Trying to pass references to boxed traits to APIs that require the trait be `'static` is much more common than trying to pass references non-`'static` boxed traits (because of how uncommon it is to define traits on references).
That .1% should be reserved for severe bug fixes and security issues, not changes like the proposed no matter how positive or unlikely to actually break things. But if a non security/severe bugfix breaking change needs to be applied, that's totally fine - just bump the major version. If that's doable because of PR, wait until you have several of these changes. If that's not doable, or you don't want to, just don't say you're using SemVer. Period. It seems kind of black and white to me.
&gt; is a breaking change that doesn't break any code in practice a breaking change? You know that it doesn't break any code on cargo. You don't know that it doesn't break *any code* in general. &gt; the only habit i'd like to see in rust is of answering such questions for every technically breaking change. i'd also like that to be in the definition of stable. i don't want stable to be equal to not-improvable-for-any-reason. Your wishes are different to what the definition of *stable* Rust outlined to be, officially.
I almost see it the other way, as low reward/high risk. You risk adoption, breaking promising, etc. The reward? Better ergonomics...? That can wait. What's the risk if we don't make the change, newcomers getting frustrated in a somewhat edge-case? They already do :P I would prefer Rust have a reputation of a reliable release model than implementing every new idea.
&gt; imagine if every java release was compatible with earlier versions. Actually it (mostly) is. Marcus Lagergren had a demo prepared for JavaLand 2015 that would run a Java 1.0 applet on a Java 8 VM. They use the `--source` and `--target` command line flags for this, which is where I got the idea for target versions in [RFC PR 1147](https://github.com/rust-lang/rfcs/pull/1147).
Judging by this thread, Rust Stable might as well have been called Rust Flobbernobber. At least with that people could ask "what is Flobbernobber?" and the Rust people could give their exact, precise definition of what such a *Flobbernobber* release means. And people wouldn't get their hopes up by associating it with a word like *stable* (which looked precise enough before 1.0, but is now looking more and more watered out). Instead the attitude from some people is more "Really, did anyone really expect 'stable' to mean """stable"""? Hah! Of course it doesn't at this stage, who would be that naive...".
There are thousands of py2 only crates on pypi; the python community is still struggling to deal with them. *thousands*. Watch the 2015 pycon keynote; tje python ecosystem will continue to be broken for years to come. There is no solution. No one even know how to *start* fixing the problem with longtail dependencies. Thats the fiasco. Thats the issue here; if you make an breaking change, abandoned crates stop working. If a breaking change doesnt cause a regression on crates, its actually fine (imo) to make it. Its irritating, obviously, but we *currently* have 50% of listed crates broken. Making the situation worse would be a disaster. 
That was in response to this specific sentence: &gt;If anything, it's proving how little this breaking change matters. No one has gained ergonomic benefit from the default inference; my point was that cases where the inference is an anti-pattern are much more common. :) I don't want Rust to get a reputation for never stabilizing or being unready, and I see that other people are having that reaction, but I really don't find that response credible.
&gt; it seems like the goalpost of "doesn't break existing code" is being moved to "doesn't break existing code that we know of" We're actively working with proprietary users of Rust to see if this would break any of their code. So far every company I've talked to has said that this won't break any of their code (save for one case where they've said that they don't know if it would break any of their code but they still want us to go through with it). If you're using Rust in production, feel free to PM me and I'll add you to the spreadsheet. :) It's easy to fret about slippery slopes, but the degree of caution being exhibited here should be enough to demonstrate that nobody is taking potential breakage lightly.
What exactly are you referring to by "that"? If you're referring to marking types as thread-safe, then no there can't be a C++ library with the same power as the Rust version (the type system doesn't quite handle static guarantees like this). If you're referring to the `Arc`/`Rc` split then I'm sure there's libraries that offer non-atomic reference counting (`std::shared_ptr` is atomic).
Just interested in the syntax preference of Rust developers
It's OK, I'm interested in everyone's opinion and am asking the opinion of other devs as well.
Ah, I didn't know that. I wonder if there's some intermediate form that doesn't produce less optimal code. But reliability should be paramount, so I can't blame you for being cautious. Great work!
Minor breakages every couple of versions are okay-ish, but not if the release-cycle is 6 weeks. If I only have to care about something like this every 6 months that's tolerable, but every 6 weeks?
Interestingly, also on reddit today is the change log for ocaml 4.0.2.2. It nicely and explicitly marks all the changes which can break existing programs. OCaml is 19 years old. Just sayin.
You probably don't want to compile the code twice whenever an error occurs. When there are more changes introduced this quickly doesn't scale anymore. Generally you'll need *n!* compilations to figure out which combination of *n* features triggered an error. Checking both cases in a single compilation would need more widespread changes, according to the github discussion.
A couple of random thoughts. First off, I prefer the second example. * Things like `save toDo to the database` scare me. What does that translate to? * ``` Ask for ToDos with "Enter a ToDo:" Check errors ``` Again, what does this mean? * I think testing is one place where semi-natural language works, because it's a very constrained set of operations. When this, ensure this is true. Outside of testing, there are just too many possible things to do.
`-Z no-trans`
Yes, but Java 8's `javac` still supports compiling Java 1 code with `--source` (most of it even to newer targets) while `java` will happily run bytecode targeting older versions of the language, allowing you to freely mix and match code over different language versions. Otherwise, changing java versions would become infeasible for most users. This is the model of backwards-compatibility I want Rust to follow, if perhaps with better ergonomics.
`serde` is the new library which seeks to replace `rustc_serialize`. `rustc_serialize` isn't in development at all, and was built to have some sort of serialization library in rustc (it is meant to be replaced when new libraries (serde) are ready).
You are right. `&amp;str` can be anything which implements ` Deref&lt;Target=str&gt;`, not just `String`.
I think you should implement deletion, since deletion is the difficult operation to implement for AVL trees.
The Rust programming language is free software, and can be downloaded at http://www.rust-lang.org/install.html . The source code is available on GitHub.
I did not test this extensively, so I’m not sure whether this is signal or noise, but for [this crate](https://github.com/ruud-v-a/robigo-luculenta) I got a slowdown of 15% with `codegen-units = 4` on an i7 920, from ~48 seconds up to ~55 seconds. By far most time is spent compiling the [image](https://github.com/PistonDevelopers/image) crate and its dependencies.
The high temperature and water content in Steam leads to increases in oxidation of iron causing more rust. These rust parts are often sold for cheaper price.
Well, given a wide adoption, they could get that without this license agreement. Like bitcoin: you can create your own copy of the network for sure, but most people will still use the main network.
Just a slight aside: is it possible to pass generic variables into build.rs from the cargo? I.e. I want to pass in some Cmake parameters.
&gt;You know that it doesn't break any code on cargo. You don't know that it doesn't break any code in general. yes. &gt;Your wishes are different to what the definition of stable Rust outlined to be, officially. yes. that's why they're my wishes. i also wish the definition of stable changes according to my wishes, so we can make progress instead of wasting time discussing a storm in a teacup.
You don't even have to invoke *slippery slope*. If you choose to break your promise about stability here[1], then the whole *promise* of stability has been broken. Simple enough. And *slippery slope* only looks ridiculous when you take it to extreme and absurd conclusions. But it isn't hard to make a convincing argument of concrete downsides of a *slippery* mindset: first it is breaking changes that doesn't *seem* to break any code. Then it is breaking changes that doesn't break code *that much*. At every step, the burden of maintaining the code gets heavier, and the trust of "stable" gets less. [1] Assuming for the sake of argument in this case that this would do that.
Yes, you can fork without merging back. But such an approach is _not_ Free Software. It also contradicts to the text I quoted above.
Doesn't look like a very useful thing yet, since it doesn't actually implement NAT traversal nor anything but TCP.
I don't know what you are talking about. It seems to be dual-licensed. GPL3 qualifies as free software by any definition I know of.
&gt; Isn't it at the cost of performance as well? If you are using an array, where the size is know at compile time, the compiler should be able to elide the bound check.
They intended to dual-license it, but: 1. They included both a `COPYING` file and a `LICENSE` file but they have different content 2. The GPL is in `COPYING` where less experienced people are more likely to overlook it 3. The proprietary license is in `LICENSE` where you're liable to look first That's the problem. People see nothing about the license, see a `LICENSE` file, and don't look further. I just [advised them](https://github.com/maidsafe/crust/issues/131#issuecomment-111439137) about two hours ago to make their dual-licensing intentions clear and suggested they add a shields.io badge to show that GPL is the license for people who haven't chosen to buy their way to proprietary licensing.
&gt; the discussion of what "stable" means ought to be had This already happened long ago. https://github.com/rust-lang/rfcs/pull/1105/files?short_path=237a6e1#diff-237a6e1cd0ae879ccb2a1c71d4b505bf https://github.com/rust-lang/rfcs/pull/1122/files?short_path=0c44dba#diff-0c44dbabd227602ba87d5e6ed221b54c Note that "type inference changes" was a part of the language RfC. It even links to an rfc about default type params.
Can the next one take place [in Germany](https://www.google.ch/maps/place/Rust,+Germany/@48.2612133,7.7174525,14z/data=!3m1!4b1!4m2!3m1!1s0x47913981b6a81ead:0x41f6bb7a5df8620)? (plus, europa park is great!)
AFAIK, Rust's `Send` and `Sync` system is relatively unique (at least, it wasn't deliberately copying something else), but you'd have to ask Niko for sure.
&gt; What I meant is that clearly - as seen in the thread - there are many who have assumed or thought that this is a breaking change according to the original definition Then those people didn't read the official definition: &gt; **This RFC proposes that minor releases may only contain breaking changes that fix compiler bugs or other type-system issues.** Minor or Major changes to language, may or may not be breaking. In context of Rust, it makes little sense to talk about SemVer, because it would cause Rust's version to change erratically. And it's not like people adhere to it always. 
Yea I have found this as well. However, the Cmake flags are hard-coded into the build.rs
Ce n’est pas une question de point de vue, ça rend le texte difficile à lire et – sur mon moniteur en tout cas – il y a certains fûts qui font moins d’un pixel sur l’écran avec l’anticrénelage sous-pixels. http://abload.de/img/capturediscp.png ^ Par exemple le fût horizontal de la lettre « e » sur cette capture d’écran.
Bon ce n’est que la table des matières, je peux faire avec.
Thanks a lot, this kind of "you could do that easier with the std lib" and "the rust way is" was exactly what I was looking for. 
I know, and I will implement deletion as well. However I felt this was a good time to gather some feedback on how to write decent rust code.
Have you tried the section the explains "Inputs to the Build Script"?
There is a [proposed replacement](https://github.com/rust-lang/rfcs/pull/1084), which is a fairly significant API change. It sounds like it needs some more work and iteration, so it's not likely to be ready in the near term.
OK, so we'll get something functionally equivalent but with a different API, right? That works for me.
It's not a bad solution. However, I don't like that every crate needs to have an annotation for something that is extremely unlikely to affect them. It'll also add some legacy crust in the compiler. We'll clearly have to compromise though, and this looks like one of the better proposals.
I don't think that's very true, even languages with very strong backcompat guarantees make exceptions, see the comments in this thread about Java, for example.
Not n!, but 2^n. That's a lot better, but still absolutely infeasible.
Good question. There's no way to tell whether such code exists.
Thanks! Just one nit: Not every crate needs to have an annotation. Only those who choose the new behaviour.
My understanding is that it's not intended to do anything except TCP: it's the lowest layer that you build on top of.
I recently had to do some changes that I believe could all be automated by a linter. Maybe you'll find useful: https://github.com/maidsafe/crust/commit/ae11ea75d2f68dc0d5436ebf55195c81bc431bde
Yep. Personally I have found that it replaces almost all uses of thread::spawn (and Arc) so I hope the API is finalized sooner rather than later, or at least exported to a crate. It makes multithreading easier, faster at runtime, and less noisy (no Arc / clone spam).
The problem is not finding things that *might* be improved – the problem is giving useful advice without too many false positives. These methods can be combined like Lego blocks to produce any behavior you could think of. To be reasonably sure of your match, you need something akin to symbolic execution.
Small correction : Swift doesn't have atomic reference counting, but rather automatic reference counting.
I added [this](https://pay.reddit.com/r/rust/comments/39db12/help_test_parallel_codegen/cs2hsqq) to my Cargo.toml and then ran `time cargo build --release`. These are the results: units | run 1 | run 2 | run 3 | average | speedup ------------|--------|--------|--------|--------------|-------- 4 | 51.028 | 51.397 | 51.683 | 51.4 ± 0.3 | -11 % 2 | 48.516 | 48.886 | 48.266 | 48.6 ± 0.3 | -6 % 1 (no lto) | 45.786 | 45.658 | 45.786 | 45.74 ± 0.07 | 0 % default | 45.724 | 45.662 | 45.756 | 45.71 ± 0.05 | I tried with no modifications to Cargo.toml (default) and setting `lto = false` (1 (no lto)). Is LTO disabled by default? There appears to be no difference. (My platform is Windows 7 x64.) I also benchmarked this for [Claxon](https://github.com/ruud-v-a/claxon). This is just a small crate, so the results are not as surprising: units | run 1 | run 2 | run 3 | run 4 | average | speedup ------------|-------|-------|------ |-------|-------------|-------- 8 | 7.893 | 7.800 | 7.738 | 7.753 | 7.80 ± 0.07 | -13 % 4 | 7.473 | 7.488 | 7.457 | 7.504 | 7.48 ± 0.02 | -9 % 2 | 7.347 | 7.363 | 7.363 | 7.363 | 7.36 ± 0.01 | -7 % 1 (no lto) | 6.864 | 6.832 | 6.833 | 6.849 | 6.84 ± 0.02 | 0 % default | 6.817 | 6.833 | 6.818 | 6.817 | 6.82 ± 0.01 |
The question is no longer about the change itself, but on how to best introduce it. Edit: not that I'm for waiting two years to introduce it, but as this is the first breaking change post-1.0, we should set up a good process for introduction of breaking changes without breaking anyone's code.
Why stop at 2.0 – we can have multiple parties! Let's start Rust 3000. No, wait...
&gt; Your feels are hurt? Yeah, I'm not gonna play that game. &gt; If that isn't the case, if you're running code in production and the entire success of your project at this point depends on a strict backcompat promise, then you are an irresponsible engineer, straight up. Irresponsible for trusting the Rust "org", I guess? 
That's because `set_raw` returns `()` aka nothing. Try let mut headers = Headers::new(); headers.set_raw("X-IAUTH",vec![value]); let res = client.get(url).header(headers).send() .... http://hyper.rs/hyper/hyper/header/struct.Headers.html#method.set_raw 
Your `headers` variable is bound to the value returned by the `set_raw` method called on a newly constructed `Headers` instance. Since `set_raw` doesn't return anything ([docs](http://hyper.rs/hyper/hyper/header/struct.Headers.html#method.set_raw)) -- which in Rust corresponds to the unit, i.e. `()`, type -- `headers` is then also `()`, leading to the compiler's message. What you want is to do: let mut headers = Headers::new(); headers.set_raw(...); Also, note that the `RequestBuilder` (the instance returned by the client's `get` method), has both a `header` and a `headers` method -- in this case, you want the latter.