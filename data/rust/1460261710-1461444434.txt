Damn, that chapter in the Book is great! Haven't seen it before. That's much better than anything I could come up with. ;-) Also, I deliberately didn't use a trait object, but I'll be sure to look at quick-error. I also didn't know that `From` was called automatically. Not sure if I like that; either way, it's good to know.
Please keep it civil. Also as I wrote elsewhere in this thread, C++ is still going strong. It has a large ecosystem with multiple compilers, mature tooling (if a little baroque), lots and lots of libraries. You may feel lucky not to need it, but a sizable fraction of programmers doesn't share that distinction.
It only changes the signature for internal functions, as LLVM can see, and modify, all the calls to it.
LTO is off by default: http://doc.crates.io/manifest.html#the-profile-sections
&gt;but I wonder how complicated it is to write them and how much meta-information they can use. They provide VS tooling + it's a managed API - you get AST and symbol resolution and it plugs right in to VS.
I think just a reference to chars().count() in the API doc of `String::len()` would be help.
Thanks for the info. So no type information nor CFG? And it probably won't work with run-time generated byte code. Still having such an API is a nice starting point. On the other hand, we have a very capable API in Rust that breaks every one or two weeks (but fixing is usually straightforward). Plus the syntax, type system + borrowck already catch a lot of things that other languages will need lints against.
&gt; Also the few C# devs I know use git. But perhaps that's a European thing. The company I work for (~5,000 developers) use C++ and Java almost exclusively (60/40) and is moving toward git. But it's still going to be git behind a corporate firewall.
I think Rc behaves the same way as Box. Probably a special case the other way (for &amp;, &amp;mut).
And I am looking forward to how much Singularity and Midori can still contribute now; the last I heard from Joe Duffy there were still things cooking (I am thinking underflow/overflow improvements notably).
Perhaps it's because Java devs are given more leeway, but I've worked with both Mercurial and Git in the enterprise. And you are asking me to believe that the C++ SO ranking overstates its popularity while the C# ranking is understating theirs? Note that we're talking about a ranking, not raw numbers (which is why they're so stable, btw.), so that's a tall order.
&gt; Value types are common in C and C++, so they've been around for some time. The characteristic I am referring to is reified-generics-and-value-types, i.e. one thing. This is futuristic because it allows a GC'd VM to avoid allocation from a Lisp-style uniform data representation where almost everything is boxed by default. C++ is nowhere near having this, of course. C++ doesn't even have a GC. Some C++ compilers tried to move in a similar direction with link-time optimisations but reified generics is a much better overall solution in terms of compile times and run times. &gt; And reified generics only makes sense if you use runtime reflection (right?), No. Nothing to do with reflection. &gt; so while it's a fair match with VM languages, it won't fly in a systems (i.e. near-zero-runtime) language. You need a compiler at link time. Nothing to do with the run-time and perfectly applicable to systems programming languages. The problem is: nobody is learning from Microsoft's pre-existing solutions. 
&gt; Are C# and Java displacing C++ if there's the same amount of C++ being written and it just cohabits with C# and Java? If that were true then no but that isn't even remotely close to being true. &gt; For example, AAA games are still written in C++, Which is why you had to cherry pick such a niche of a niche example. Not just games programming but AAA games. &gt; whereas indie games False dichotomy. Just because a game isn't AAA doesn't mean it is "indie". 
Is there a method on Vec that allows me to pop multiple elements returning a borrowed slice to them? My use case is I have to 'consume' a Vec starting from the end, take a slice to the last `n` elements, feed that to some function for processing and then pop those values from the Vec. Repeat that until the Vec is empty. let at = self.vals.len() - 2; let vals = self.vals.split_at(at).1; work_with_vals(vals); self.vals.pop(); self.vals.pop(); It doesn't look like there is, but a proposal could look like this: `fn Vec::&lt;T&gt;::pop_n&lt;'a&gt;(&amp;'a mut self, n: usize) -&gt; &amp;'a [T]` it would subtract from the the underlying vector's length by n (but keep that memory allocated), and return a slice of the split off n elements at the end. Since this is a reborrow of `&amp;mut self` you cannot modify the original Vec until your borrowed slice goes out of scope which makes it safe. Or am I overthinking it and there's a much more elegant solution?
&gt; Even then how'd you explain the positions of C/C++ on the StackOverflow index? Just look at the questions. From a search for C++ now: * "C++ cannot convert std::string to HANDLE". This problem doesn't exist in C#. * "sorting in single linked list c++". The OP wrote a pile of pointer mutating crap to sort a list. A non-problem in C#. Call `List.sort` from FSharp.Core. * "split string containing many whitespace in c++". Call `.Split` in C#. Again, a non-problem. * "Why can't void parameters exist in functions with more than one argument?". A retarded language design flaw that C# sadly copied. These are all easy low-level problems problems that C++ makes hard. In comparison, questions tagged c# are much higher-level, about things like RESTful APIs. &gt; I'd expect C# to score exceptionally high on that index, considering they're a .NET shop. You're assuming people ask as many questions about C++ as C# so you're assuming people get stuck as often with C# as they do with C++ so you're assuming the productivities of these two tools are comparable. They aren't. The productivity isn't even close. C# is maybe 10x more productive than C++. People get 10x more done with C# before they get stuck so they ask 10x fewer questions. That is precisely why next-generation languages like C# and Java displace the previous generation like C++. F# is another 10x more productive above and beyond C#. 
I think to_owned is clearer here, as you already have a string (slice), and the important part about the return value is here that it is a owned copy.
Or maybe your claims of the big takeover by C# are bogus. 
There will still be domains where C# is unsuitable anyway (when one cannot afford a GC, for example, either because of small memory or because of the need to interact with a GC'ed language). Furthermore, once it's implemented in the open, there's no reason for Rust not to get inspiration from it :)
All our enterprise customers use their own source control stacks, we were never allowed to bring out own stacks. Usually we adapt to what their IT allows us to do. No, what I am saying is that numbers can always be manipulated and SO, Github and many other social sites for developers are usually ignored by the traditional enterprise developer. Those Fortune 500, whose main business isn't software development. I have met many enterprise developers on my career that don't care one second for such activities. And I also have worked with customers where such sites were blocked. So from my point of view there are more C++ developers that care about such sites and work in environments that allow its use, than the typical enterprise developer that works with the tools of the dark side of the force. Just search for C++ jobs in any international job board, they are usually games, compiler development, HPC, HPT, device drivers, embedded stuff or related to porting existing applications into another stack. None of the typical CRUD, batch processing, desktop and web UIs of the enterprise stuff. I like C++ a lot, even used to teach it at some point, but I don't see C++ job offers going up.
Yeah, I guess you are right. Microcontrolers would be cool, for example.
You basically have Roslyn compiler API with project information so you can get semantic analysis (with type information) as well as AST - in that regard it's probably similar to Rust but they add in VS integration stuff
Does this work on Windows because all the other audio crates don't. 
Don't get me wrong it's my absolute favorite editor for those exact reasons, but holy GOD does it burn up power. It's like using a semi truck to putz around town.
Yeah, Nick Cameron has some [webpage](http://www.ncameron.org/perf-rustc/) for this.
I am confused as to why the dereference of `Option`'s `&amp;self` is valid as my assumption is that all dereferences transfer ownership. I would have thought the ownership moves out out self and `rustc` complains. pub fn as_ref&lt;'r&gt;(&amp;'r self) -&gt; Option&lt;&amp;'r T&gt; { match *self { Some(ref x) =&gt; Some(x), None =&gt; None } }
baal depend on portaudio and libsndfile. both work on Windows/MacOsX/Linux. it must be possible to make baal work on windows too. But there must be other audio crate working on Windows: * [rust-portaudio](https://github.com/RustAudio/rust-portaudio) as portaudio is cross platform * [ears](https://github.com/jhasse/ears) * [openal-rs](https://github.com/jpernst/openal-rs) * [cpal](https://github.com/tomaka/cpal) (win32)
http://burntsushi.net/rustdoc/fst/trait.Streamer.html
No, the dereference only consumes the value if it doesn't take a ref. Since you do take a ref, the contained value is effectively *reborrowed*.
It'd be interesting to see what percentage of Fortune 500's are 'highly invested in Windows'. I've only worked with two dozen of the European ones and only one of them asked us to use Microsoft products. But as a Java dev, my sample is probably biased.
I added a REST API and web server to https://github.com/llambda/rreverse (simple offline reverse geocoder). Next I'd like to optimize it to not reload the CSV file and tree each request and improve the unwrap logic.
I was actually having a conversation with a professor trying to convince him that he should teach his concurrent programming course in Rust. His strongest argument against it was the lack of courses meant to actually teach you how to code in Rust. Now I seem to have something that I can show him.
Unfortunately, GPU-related APIs suck. I'm sorry to say it, but if you're used to your good-old UNIX style of having sane, free software tools everywhere so you can just build something simple without a lot of hubbub, well, that's not gonna happen. Vulkan is one NICE idea so solve all this, but it has so many competitors: DirectX, Metal, CUDA. Heck, even OpenGL and OpenCL are still independently developed along side it because some important companies decided that OpenGL/CL would be ENOUGH. If you're not a Microsoft/Nvidia fanboy and you like GPUs or computer graphics, my friend, you're screwed.
A rusty parallel ECS
Entity Component System. It's common in game development. Basically an entity is just a unique identifier that has components attached. Components are pure data and are processed by systems.
Entity Component System. A design paradigm of sorts.
Neither Github nor Crates.io links to documentation. Could you fix this? Reading docs is my usual way of learning new frameworks :P Also, you might consider changing the name of your project, because it strongly collides with Haskell universe ;)
Thanks!
A while ago I came across [OMeta](https://en.wikipedia.org/wiki/OMeta), which is a PEG parser generator that's been implemented for a few host languages like JS, Squeak and C#. I personally got so inspired I went off to write [a Pythonic variation](https://gitlab.com/Screwtapello/python-omega) on the same ideas. The big ideas behind OMeta are: * The same language should be useful for all compiler passes, not just the initial parsing phase. Practically, this means the parser is defined over iterables of arbitrary data like an AST, not just over strings. * Grammars and DSLs should be a handy, available tool like regexes are in many scripting languages, not just for special occasions. I haven't really thought how those ideas could be applied to Rust—OMeta itself really wants scripting-language things like "there's a literal syntax for every interesting data type" and "lists can contain arbitrary types, not just a single type". Rust's type-ascription syntax definitely collides with OMeta's syntax for naming terms to pull out of a pattern. And of course, the "O" in "OMeta" is for "Object-oriented", and Rust doesn't do that. I expect there's some cool way to Rustify the ideas behind OMeta, but I haven't given it much thought yet. I really hope somebody does, though - after playing with OMeta, I never want to deal with slogging through shift/reduce errors or the verbosity of parser combinators ever again.
&gt; Yes, it does have to do with reflection, where "reflection" means "RTTI". No. Generics can be reified at link time. You don't need run-time anything. &gt; Rust doesn't have much if any RTTI. Doesn't matter. &gt; From e.g. http://gafter.blogspot.com/2006/11/reified-generics-for-java.html &gt; The list of operations are (1) T.class; (2) instanceof; (3) arrays of T. All of these are either fundamentally RTTI operations or RTTI operations in Java. The fact that Java struggles has nothing to do with reified generics. .NET and HLVM implemented reified generics without using RTTI. There is no inherent relationship between reified generics and RTTI. The fact that Java happens to be trying to mimic this feature using RTTI is irrelevant. 
&gt; Really, if you live inside a bubble that is very new and very web orient6ed i guess you can easily think c++ is gone or being replaced, but that is still just your bubble. "Being displaced"? C++ lost the fight almost 20 years ago now. C++ isn't "gone", it is transitioning into an undead language like COBOL that won't go away for decades because of legacy code. People like Herb Sutter like to pretend there is a C++ renaissance because of low power requirements but the reality at the coal face is that the C++ job market for new applications has been in free fall for a decade now. 
[piston_meta](https://github.com/PistonDevelopers/meta) is a parser generator based on ideas from OMeta.
I don't understand how any of this is responsive to my point. Reified generics are about operations that require RTTI. Without RTTI, it doesn't matter whether generics are reified are not. Indeed, monomorphization like Rust does guarantees that generics are reified no matter what, so this whole argument is a bit pointless.
When I try and implement Copy on the Coefficient, which is defined like this: #[derive(Debug, Clone, Copy)] struct Coefficient { str_rep: String, } I get the error: src/main.rs:1:24: 1:28 error: the trait `Copy` may not be implemented for this type; field `str_rep` does not implement `Copy` [E0204] src/main.rs:1 #[derive(Debug, Clone, Copy)]
Well, that's very interesting. As you say, it's based on ideas from OMeta, but it's also [very deliberately](https://github.com/PistonDevelopers/meta/issues/1) doing some things differently, although I don't really understand the reasoning in that second-last paragraph. At a glance, I find piston_meta's syntax difficult to follow; the original OMeta syntax tries very hard to be like friendlier, fluffier regexes. But it's quite possible this new syntax has advantages I just don't understand yet.
I think you should reconsider the name. Parsec is a very popular parser combinator library for Haskell, so I believe the name will just confuse people looking for a similar library in Rust. Just a suggestion, though.
This is really cool. I am rolling my own ECS (single-threaded), so it is interesting to see other strategies for implementing one. Like some of the other people in this thread, I was also confused by the name. I thought it was somehow related to the Haskell parsec library.
Huh. I mostly do F# but it's never occurred to me that someone might call stuff from FSharp.core exclusively from C#.
I think you can find a few PEG libraries in crates.io
I take it parsECS was taken?
This is really cool but the naming is going to confuse a lot of people, as other commenters have mentioned. Parsec is probably one of the most common Haskell libraries. When I glanced over the headline I assumed this was a port. Please seriously consider renaming.
Ok, I didn't know about the Haskell's lib. We'll change the name upon moving into an organization.
I actually came here assuming it was some Parsec port from Haskell.
&gt; Still not true. Reified generics have nothing whatsoever to do with RTTI. &gt; Oh, I see. I think you've taken some balked retrofit done in Java and assumed it is the be-all and end-all of reified generics. That's a really bad place to start learning about generics as Java did a really poor job implementing generics in the first place. Yes, they do have to do with RTTI. And I don't need to "start learning" about generics, thanks; I implemented much of them in the Rust compiler twice. This is becoming an unproductive conversation. &gt; That leads to bloat and slow compile times and does not improve performance significantly over the .NET solution which is to copy code for value types but reuse code for reference types. That's the characteristic I am referring to. This is of course totally wrong for Rust, which uses static dispatch, doesn't use a JIT, can't use inline caching, and so can't do speculative devirtualization. Reusing code for reference types in Rust would be disastrous for performance. F#'s decisions are tradeoffs and don't always make sense in languages with very different characteristics. Allow me to be blunt, but: you're continuing to do what you have done for a decade, which is to troll other language communities to transparently try to spread F#. Please stop doing it here.
Thanks for the suggestion. The other difference is the closure is passed as 'mut setter'.
[Sure](https://github.com/dragostis/Parsh). Unfortunately, it's pretty unstable right now, especially the error reporting part. The project is my bachelor's thesis that I will finish until the middle of June. A rewrite in Rust is probably not plausible until then, but a reimplementation might come in handy. Take a look over this [JSON example](https://github.com/dragostis/Parsh/blob/master/spec/json_spec.cr). Currently, it only works with ASTs, but I plan to add lambdas to process captured tokens.
Threw my html macro up on github. https://github.com/zvxy/rust-dom The macro part is pretty broken at the moment, because I've been working on the core library, but the source is there for browsing. #[derive(Clone, Default)] struct App { n: usize } impl App { fn click(&amp;mut self) { self.n += 1; } } impl Component for App { fn render(&amp;self) -&gt; Node { dom!(&lt;div onclick=|| { this.click() } /&gt;) } } I've been working on closures for properties (like onclick events), and found a cool trick. Although, I'm not sure whether to be proud of myself or ashamed. Currently closures can't take arbitrary arguments (either number of arguments, or type) so I used a macro to help with this. The state for each struct is stored as a trait object, and the events are stored as boxed closures. From there we execute the closure with the trait object as its argument. The scope macro downcasts the trait object and creates a synthetic closure. scope!(|this| { this.onclick() }) gets converted to: Box::new(|this| { let mut r = this.clone(); if let Some(ref mut this) = r.downcast_mut::&lt;Self&gt; { this.onclick(); } r.to_owned() }) So then for dom representation we can use the macro: dom!(&lt;div onclick=|| { this.click(); } /&gt;); and it gets converted to: Node { selector: "div", state: Box::new(Self::default()), properties: props! { onclick: scope!(|this, event| { this.click(); }) }, children: Vec::new() } 
That is not a difference justifying the creation of a separate function, the function signature remains the same. The parameter is moved in both cases, the `mut` annotation only concerns whether the binding is mutable. Equivalently you could drop the `mut` from the argument definition and use `let mut setter = setter;` within the function body.
Thank you. I will check and improve the code :)
Still working on [GBArs](https://github.com/Evrey/GBArs). I'm still doing my planned-for-last-monday unit tests for THUMB disassembly, which is right now almost done. Instead I built a new and better REPL last week. My next step is THUMB state instruction execution, combined with a big refactoring of the whole ARM state stuff.
And then there's [`io::Error`](https://doc.rust-lang.org/std/io/struct.Error.html) at the other end of the spectrum...
*Especially* once you realise it includes things like OS errors; it's not just every I/O failure, it's every OS failure, too! :D
Working on my game project that uses Vulkan, and in vulkano in parallel. Vulkano is almost out of its "initial implementation" phase. Only command buffers, descriptor pools, queries and memory pools are hacky or unimplemented. Some parts of the code are already robust, like image creation. Unfortunately some parts of the code are blocked because [of](https://github.com/KhronosGroup/Vulkan-Docs/issues/155) [issues](https://github.com/KhronosGroup/Vulkan-Docs/issues/164) in the Vulkan specs. 
Depends on what you're thinking of. In rustc, the MIR is a representation that is higher level than LLVM, but lower level than the AST, which is useful, because an AST is a terrible way to represent a program. You may be thinking of "Machine IR" which is indeed a sort of "second IR" for LLVM, but is not what I'm talking about here. 
&gt; wat. C++ is untouchable by Java and C# in many niches. Some niches, yes. &gt; There is no fight, but even if there was, C++ is far from down and out. The fight was about the most common types of software: GUI apps, database apps, front- and back-ends and so on. 20 years ago people tried to write all of those in C++. C++ was touted as a language for application programming. But C++ sucks at application programming and people jumped ship to Java, mainly for memory safety, first chance they got. C# offers a slicker development process so many Windows shops use that. Today, most people write front-ends as web apps. Those that don't write native apps using Java or C#. Very few people are using C++ for GUI app development any more. Back-ends are written primarily in Java or C#. Database apps are written almost entirely in Java and C#. Scripting has emerged as a common way to write code. Nobody writes scripts in C++, of course. &gt; No, it really isn't. C++ is still king in so many areas. And they aren't just little niches, they're important niches. Operating systems, Firstly, the Windows, Mac OS and Linux kernels are all written in C, not C++. Secondly, these are exactly the kinds of legacy code bases written in undead languages that I was referring to. &gt; real-time systems, vehicle control systems, some embedded stuff, medical devices, etc. Those are all the same niche. Again, C is preferred over C++ because destructors avalanching in C++ cause unwanted pauses. People used to regard mobile phone apps as embedded programming but it isn't really: phones are more powerful computers today than the desktop computers we used when Java overtook C++ in the 1990s. &gt; These types of systems aren't going anywhere, and nor is development or production slowing. Yes, of course. Web browsers, databases and run-times for modern languages are other examples of code bases we use that were written in C++. Again, this is just legacy. 
&gt; The fight was about the most common types of software: GUI apps, database apps, front- and back-ends and so on By "database apps", do you mean databases, or programs that use them? I was under the impression that most databases in widespread, serious use are written in C or C++. &gt; Firstly, the Windows, Mac OS and Linux kernels are all written in C, not C++ Linux is all C, yes. The Windows kernel is C too, but a kernel does not an operating system make, and much of the OS surrounding the kernel is C++. The OS X kernel is also not entirely C - much of the driver system is C++. &gt; Those are all the same niche. Again, C is preferred over C++ because destructors avalanching in C++ cause unwanted pauses. [citation needed]. Yes, C is widely used, but C++ is certainly around as well. The F35 II for example, a lot of NASA stuff, and I believe Airbus uses it too. &gt; Web browsers is another example. Many browsers are still written in C++. Again, this is just legacy. I'm sorry, but you can't claim that *all* in-use software in a niche is legacy. Legacy *by definition* means that it has been superseded. Current web browsers have not been superseded. Not has C++ been superseded.
Not a question per se, just wanted to say that I started learning Rust last month at some point, as more or less my first programming language -- in a serious attempt anyway. Half the time I feel like I'm just putting in &amp;s and muts until the compiler stops complaining, so I know I should probably actually read about the language more. I felt it was hard getting a grasp on things from reading the "book", and it often left me with more questions and not enough easy examples showing off the code succinctly. It feels everything is written towards someone who already knows another language? It has been pretty fun working with it though. I would love a very quick code review of the project I started out making if anyone has the time to just look over some things. I feel like I might be getting into some bad habits, as half the time I don't know the best way to do something, and might not know about all the various functions available to me. I'm not sure if the code is clear enough to get an idea of what I'm doing, but in short I'm just trying to make a crate library that can deal with the binary files from a small game, specifically its level and replay files. I've currently been able to parse through a full level file and get the correct values expected (yay!). I feel the code is extremely bad however :) Here's what I've got so far: [https://github.com/hexjelly/elma-rust/](https://github.com/hexjelly/elma-rust/) One question I wondered about: is there any point making a test for a ::new() constructor thingiemajig and the expected values, if you're just using very simple types? One would assume you never actually get the wrong values returned?
Thanks, that looks like the cleanest solution!
As always, improving [Serkr](https://github.com/mAarnos/Serkr). Last week I ironed out most of the bugs left in the CNF transformer. Serkr can now parse and CNF transform 15820 out of the 15858 CNF and FOF problems found in the TPTP library. Unfortunately the rest (excluding a few) are rather nasty to fix so I do not know when we reach 100%. However, out of the 15820 problems, Serkr can only prove around 3766 if given 60s per problem, which is around 24%. This is quite bad so I am improving it this week. More specifically, I am improving the heuristics for proof search control. After we read a problem into memory, we negate it if necessary, and transform it into CNF. This gives us some amount of clauses which we put into a set called unused. Simplifying it a bit, the proof procedure picks one clause from unused, puts it into a set called used and performs all possible inferences with that clause and used (including itself), and puts the inferred clauses into unused. Then it picks another clause etc. We continue until we derive the empty clause (contradiction) or we have performed all possible inferences (we found a model - note that given unlimited time and memory this might not happen, unlike with contradictions). The better we are at picking the next clause the faster we can do both of these things. Currently the proof procedure just picks the smallest clause, measured in the symbol count. This has all sorts of problems, but it is easy to implement and works reasonably well. So this week I am implementing what is called the 'standard clause selection heuristic', which is a combination of picking the smallest clause and picking the oldest clause. This works a lot better than just naive symbol counting, and so I hope after this Serkr can prove maybe 30% of the problems. The implementation won't take long though, so probably I will also start implementing proof output this week as well. The next step would be to use machine learning to build some system to recognize different problem types, and pick the best heuristic for that problem type from some set of heuristics found by something like stochastic local search. I will probably do this someday, but right now I have bigger problems, including the fact that I don't know that much about machine learning. 
A graph, I would imagine, since you may have mulutiple references to a single structure. Kind of the exact purpose of RC.
General graphs (with cycles) are difficult even with `Rc`, because you can leak nodes unless you implement some sort of cycle breaking strategy. (`Weak` only helps if you know certain things about the graph's structure in advance.)
&gt; By "database apps", do you mean databases, or programs that use them? I meant applications that use databases, i.e. CRUD apps. There is a *huge* amount of code using the Java and C# interfaces to various databases in industry, of course. &gt; I was under the impression that most databases in widespread, serious use are written in C or C++. Technically the vast majority of "databases" are lashed together using completely inappropriate tools like Excel. I know of retail shops that keep their entire inventory in an Excel "database". You probably mean common relational databases like SQLServer which are written primarily in C++, yes. Modern alternatives like Cassandra (Java), Dynamo (Java), Riak (Erlang) and Datomic (Clojure) are written in more modern languages. I've built custom databases for the financial industry in F# before. The main benefit over C++ is massive reduction in software development cost, of course. &gt; Linux is all C, yes. The Windows kernel is C too, but a kernel does not an operating system make, and much of the OS surrounding the kernel is C++. The OS X kernel is also not entirely C - much of the driver system is C++. Ok. &gt; &gt; Web browsers is another example. Many browsers are still written in C++. Again, this is just legacy. &gt; &gt; I'm sorry, but you can't claim that all in-use software in a niche is legacy. Legacy by definition means that it has been superseded. I am using [legacy](http://www.merriam-webster.com/dictionary/legacy) to mean handed down or inherited rather than superceded. Those haven't been superceded (although the tech that underpins them, like C++, has long since been superceded). &gt; Current web browsers have not been superseded. Not has C++ been superseded. The source code to all mainstream browsers is very old code that has been inherited. When you inherit a code base like that it is rarely feasible to rewrite the whole thing in a more modern language. The same is true of COBOL. 
Oh wow, pub(restricted) is making its way in! This makes me so excited. 
It's been a slow week because it's the first one, and because I've been working on non-doc responsibilities. As for TWIR, not many doc-related things are always appropriate for TWIR, but for a more specialized version, it might be. This is just always true with any kind of summarized content; the more general it is, the bigger of a deal it has to be. We can pay more attention to smaller things with a focused version. Finally, we're trying to get more people involved in docs, so we're working on building out that infrastructure. It's only a little now, but we hope it's gonna be a lot more in the future.
I also don't think changing the name is necessary. There are plenty of generic names that mean different things in different language ecosystems. Take "[diesel](https://github.com/search?utf8=%E2%9C%93&amp;q=diesel)" for example: &gt; Greenlet-based event I/O Framework for Python &gt; A safe, extensible ORM and Query Builder for Rust &gt; Diesel gives your Rails engines power.
Wait. How does this work? Are you running Rust code in the browser via emscripten?
Spent the last two weeks working on perf for the underlying library for my 2d computer-aided-design program. In a benchmark testing polygon-distance calculations, I went from [94ms / iteration to 2.4ms / iteration](https://github.com/TyOverby/implicit/issues/8) with the help of SIMD, Rayon, and my instrumenting profiling tool [flame](https://github.com/TyOverby/flame).
I agree... but considering how many people upvoted the root comment, and complained about the name on other channels, it seems like Haskell's stuff is too big/famous to interfere with.
I didn't test on https yet. I'll try to fix this as soon as I can. EDIT: Fixed. Thanks @Ruud-v-a!
RFC 1444 says it's going to be implemented as the contextual keyword `union`, why does TWiR say `untagged_union`?
I'm still working on the diary implementation for [imag, the personal information management suite for the commandline](https://github.com/matthiasbeyer/imag). Also, I'm working on a rewrite of the linking library to support external links in the format which is defined in the specification/documentation document. And whenever I don't like to hack on these two, I write code for the `libimaginteraction` library, a library for interactively interacting with the user and asking her for data or something.
For something like a graph, there's not really any way to avoid using reference counting or garbage collection. Actually, it seems like any graph like data structure is pretty much *the* number one use case for things like Rc. If you aren't doing a ton of mutations or if you want to keep old versions of the graph alive, you can actually just use something like [mempool](https://doc.rust-lang.org/regex/mempool/index.html) to avoid the overhead of reference counting. So far for me this has been more than adequate for all my use cases. The thing it recognize is Rc allows for shared ownership, but if you have a graph data structure then the graph itself can actually hold ownership of the nodes (ie. there's no need to have nodes own each other).
Someone sent a PR to fix this: https://github.com/cmr/this-week-in-rust/pull/196.
Thanks for the tip, and sorry it took me a while to get back to you. I'm just about there, and I got everything working with a 32-bit texture (Rgba) but I can't seem to get gfx's `update_texture` function working with a `u8`. It crashes when it calls `update_texture`. Here's an abbreviated version of my setup—what type agreement am I missing here? gfx_pipeline!( pipe { glyphs: gfx::TextureSampler&lt;u32&gt; = "t_Glyphs", ... } ); // setup code let tex: gfx::handle::Texture&lt;_, f::R8&gt; = factory.create_texture::&lt;f::R8&gt;( kind, 1, bind, gfx::Usage::Dynamic, Some(cty)) .unwrap(); let resource: gfx::handle::ShaderResourceView&lt;_, u32&gt; = factory.view_texture_as_shader_resource::&lt;(f::R8, f::Uint)&gt;( &amp;tex, (0, 0), gfx::format::Swizzle::new()).unwrap(); // in the loop, where d: &amp;[u8] encoder.update_texture::&lt;f::R8, (f::R8, f::Uint)&gt;( &amp;glyph_tex, None, info, &amp;d[..]).unwrap(); 
All the problems with Rust documentation I'm aware of have *everything* to do with rustdoc being lame, and nothing to do with markdown. (Github Flavour) Markdown has two basic avenues for extension -- named code fences and URIs. We use code fences a bit, and don't take advantage of custom URIs at all. Styling is a matter for CSS. Markdown is your copy, and *should not* mandate styling in any way. 
[removed]
the only issue with rST is shitty inline link syntax, apart from that it’s very easy and not far from MD. if we provide a simple cheat sheet, people have an easy time: * external links: a link_ and a `multiword link`_ .. _link: http://foo.bar .. _`multiword link`: http://example.com * code: prefix with two colons:: hi("there") * rules: * ``` ``inline("code")`` ``` * ``:math:`x = y^2` `` * item link rules. a `~` removes the modules and only leaves the name. code | result ------|----- ``:mod:`std::env` `` | [`std::env`](https://doc.rust-lang.org/stable/std/env/index.html) ``:struct:`~std::string::String` `` | [`String`](https://doc.rust-lang.org/stable/std/string/struct.String.html) ``:fn:`~std::env::args` `` | [`args`](https://doc.rust-lang.org/stable/std/env/fn.args.html) ``:fn:`~std::string::String::new` `` | [`String::new`](https://doc.rust-lang.org/stable/std/string/struct.String.html#method.new) ``:trait:`std::borrow::Borrow` `` | [`std::borrow::Borrow`](https://doc.rust-lang.org/stable/std/borrow/trait.Borrow.html) ``:const:`~`std::time::UNIX_EPOCH` `` | [`UNIX_EPOCH`](https://doc.rust-lang.org/stable/std/time/constant.UNIX_EPOCH.html) * …
I think rST worked out pretty well for the Python community, although the syntax is kind of horrible IMHO.
with styling i meant text roles. sphinx (rST based technical doc generator for python) has several roles that create links with classes that would (for us) be styled like the corresponding trait/fn/constant/struct definitions (green, orange, …) and have convenience stuff, like described [here](https://www.reddit.com/r/rust/comments/4easyf/this_week_in_rust_docs/d1ystb7)
I won't dispute your conclusion, but don't put too much currency in upvotes.
Ahhh that's not Deref though. Deref stuff is shown. That's extension traits. Regardless, it is a problem. And it's tough. The plan is to work on it, but it's not clear what to do yet.
Also the jeans label. It was an example in the big trademark discussion. Things are inevitably named the same, for being named after the same worldly concept independently.
I don't have any experience with this personally, but there was recently a [post about deploying an Iron server in a docker to google container engine](https://www.reddit.com/r/rust/comments/4cyuti/deploying_rustiron_to_google_app_engine/). I imagine most of it is transferable to other cloud hosts. 
Is there anything in particular you're concerned about?
I guess the gold standard would be: get the webapp building continuously on travis or similar, use terraform or cloudformation to build the following stack: ELB -&gt; ASG with 1 or more instances -&gt; Launch Profile, with user data in the launch profile so the instances use cfninit to run chef or puppet to download and start your webapp. then add cloudwatch and sensu as desired. if that's too much, just create an instance, and run the webapp process using supervisord or systemd or similar. alternatively, try amazon ECS to run a docker containerised version, or go with heroku.
I usually recommend [Rust By Example](http://rustbyexample.com/) as a more distilled tutorial for experienced programmers, but it tends not to dive very heavily into detail. For example, the [chapter on Strings](http://rustbyexample.com/std/str.html) glosses over quite a bit. 
Yo! Yeah it's been a bit; crazy busy time of year for me unfortunately. The stream is far from dead, but I can't really say when I might stream again. Might be in tomorrow, might be in 3 weeks. We'll just have to see :)
What's the most common clippy warning(s) that you've encountered in the wild?
I love that you're actually discovering issues with the specification as a result of writing this library.
Ok, here's the rub with Strings: `String`s are mutable entities, akin to a `StringBuilder` in Java, whereas `&amp;str`s are immutable slices of the contents of `String`s (or of string literals). IMHO the best resource for turbocharged learning Rust is to join a project! For example, look at [This Week in Rust](https://this-week-in-rust.org), it has a "Call for Participation" section. Also other projects (for example [clippy](https://github.com/Manishearth/rust-clippy)) have mentored issues (and with clippy we even have a Workshop in Frankfurt am Main/Germany this Friday where I will mentor in person).
Well, that's a tough one to answer, because the code bases I'm looking at are very diverse. `let_and_return` and `match_bool` are pretty common with newbies, `map_clone`, `cyclomatic_complexity` and `option_map_unwrap_or` are more common in projects of more seasoned Rustaceans (if there are warnings at all). Also some of the new lints (like `doc_markdown` and `similar_names`) produce rather a lot of reports. They are nice enough when you think about them, but also have the unfortunate tendency to swamp the output.
A bit of a last-minute announcement because of wild chance: I'm in Philly right now at the Emerging Technology in the Enterprise conference, talking about Rust stuff, when they emailed me to ask if I wanted to swing by Philly to talk about Rust stuff! Short notice, but if anyone is around, would love to see you!
Yea good points. Mainly doing work by myself, and still learning the very basics of things, I've been bad at getting into testing since there's no consequences for anyone else involved. I figured it'd be a good habit to get into more from the start, specially with Rust. I appreciate how easy it was to add tests in it, as well as documentation, really love these features being built-in! I figured that was the case, as long as they don't rely on anything too fancy or other complicated functions or results. Thanks :)
Well, "strings are mutable" is really only *one* of the things I'd like to dive into about them. For example, how are they represented internally? UTF-8, UTF-16, something custom? Can I get the length in O(1) or is it O(n)? Is the length a representation of the number of bytes, unicode code points, characters, or rendered glyphs? What about indexing strings? Is that done by byte, code point, character, or glyph? I will want to know similarly detailed things about the rest of the language's constructs. How does run-time polymorphism work? Overloading and overriding methods? Does Rust support dynamic dispatch like other big OO languages (e.g., Java, C#)? How are parametric types implemented (obviously, there's a world of difference between Java's type erasure and C++'s templates)? These are the kinds of things that I would really like for a book about a language to dive into. When a book spends a full section explaining what immutability means, it makes for a frustrating read for me and leaves me wondering if it will ever get to the more interesting topics. I'm not sure I want to dive right in to contributing before I know more about the details of the language and how the pieces fit together. And FYI, your first link is broken. 
Thanks, that book looks like it may have potential. Unfortunately the preview available is not enough for me to decide yet, and the two reviews are not exactly helpful.
I appreciate you taking the time to answer my questions. I'm just hoping to find a resource that takes the time to answer questions like those without wasting my time explaining things like "String interpolation is a computer science term that means 'stick in the middle of a string.'" Or including entire sections explaining very basic topics like "Using a variable before initialization is a compile-time error," which are adequately conveyed in a single sentence to people with some experience. Maybe I'm just impatient, but it's not very fun slogging through explanations of the basic programming building-blocks I've been using my entire career. If that were the only problem, though, I could probably just deal with skimming it quickly, but the book is also missing a lot of important information! I just read the section on strings, where I'm told that Strings don't support indexing. Okay, fine, I understand the reasoning there. But then the section ends without actually addressing any alternatives! It's frustrating reading a book that spends all its time on mundane details that are common to many languages while spending very little on what's useful to me as an experienced programmer. Anyway, it sounds like the answer to my question of whether there's a better learning resource for me is "not yet." If I choose to adopt Rust in my projects, maybe some day I'll try to contribute to the available documentation to make it more useful.
Any generic item (with type parameters) is eligible to be inlined anyway. *Edit:* and `#[inline]` marked functions too (the attribute enables, does not force inline).
For a start, try to read the file as a byte vector and not as a string. Strings are UTF-8 encoded in Rust and `read_to_string` performs UTF-8 validation in addition to just reading the file content.
You can reserve capacity in the `Vec`/`String` by first doing `f.metadata().unwrap().len()` to get the file size and then `Vec::with_capacity(size)`.
&gt; Styling is a matter for CSS It's more than just surface styling - the actual UX of Rustdoc needs to be looked at and possibly rethought with an eye for improving the daily workflows of users. It's a challenging, non-trivial design problem though, and hard to offload to volunteers.
Without having more information, I can't really help much. From your other post, it looks like you're trying to make a syntax extension? I think you might be trying to use the methods to see if you can parse something, which isn't how it's designed. `parse_ident` will parse an identifier, or fail. And there's no backtracking. Same with `expect_gt`, if the next token isn't `&gt;` or `&gt;&gt;`, then `expect_gt` will return an error. There's an `eat` method that returns true or false depending on whether the next token is the same as the token you passed in, bumping the parser to the next token if it was.
I'm curious what is generally considered more idomatic. I'm pattern matching against the enum Event and looking some some specific combinations. So I may write: match a { Event::A(State::Pressed, Some(Condition::Foo)) =&gt; ..., Event::A(State::Pressed, Some(Condition::Bar)) =&gt; ..., Event::A(State::Pressed, Some(Condition::Baz)) =&gt; ... ... //Other events handled... } but my question is it generally preferable to nest the matching or to be specific in the root cases. So should I be using the above or something like: match a { Event::A(State::Pressed, Some(condition)) =&gt; { match condition { Condition::Foo =&gt; ..., Condition::Bar =&gt; ..., ... } ... //Other events handled... } The second form is less repetition, but I think overall maybe a little harder to read? I'm curious what people find to be more idiomatic in this situation. Thanks!
 use std::io::prelude::*; use std::fs::File; fn main() { let name = std::env::args().nth(1).unwrap(); let mut f = File::open(name).unwrap(); let file_len = f.metadata().unwrap().len(); let mut v = Vec::with_capacity(file_len as usize + 1); f.read_to_end(&amp;mut v).unwrap(); println!("{}", v.len()); } Use read_to_end (not read_to_string) because it doesn't zero the buffer. Allocate a byte extra in the buffer! This means that it will not need to reallocate after the last byte is read only to find it's at EOF. (huon's insight). This is not intended as “this is how to write rad Rust”, just a story of what de facto turned out to be the fast way in current Rust nightly. And some avenues for bug fixing!
Ah, gotcha. Thanks. I was wondering if it was that way by design or not. [Yeah I already got it all working](https://github.com/zvxy/rust-dom/blob/master/macros/src/parser.rs) I was just wondering if I was being an idiot.
The two pieces of code are doing different things: the C++ is asking to read to a given length, while the Rust is asking to read the whole file. In particular, this means that the C++ stops when it's read enough, while the Rust will try to read some more, to see if the file is finished. Reading more requires allocating more memory, typically done by doubling the size of the vector (to ensure the function is O(1), amortized), and doubling the size of 1.5GB vector takes some time. If the Rust is changed to match the C++ more closely, so that it is just reading a fixed length, I see much improved results: use std::io::prelude::*; use std::fs::File; fn main() { let mut f = File::open("data.txt").unwrap(); let length = f.metadata().unwrap().len() as usize; let mut s = Vec::new(); s.resize(length, 0); f.read_exact(&amp;mut s).unwrap(); println!("{}", s.len()); } That code takes 1.14s, while the C++ takes 1.16s, on a file of the same size (with a warm file system cache).
`struct X;` and `struct X {}` are not equivalent, only the former creates a constant `X`.
The loop is from calling v.resize() in .read_to_string(). That was profiling the original .read_to_string() code. The linked issue shows how resize() sometimes regresses like this depending on various factors (and does not use memset). That loop fragment is visible in nightly/release/asm here http://is.gd/GZIZQx Kind of hard to point out exactly where, but if you get the same loop labels it's at .LBB0_26:
no, you killed a hacker on /r/playrust
I didn't know about using metadata to determine the size of the file. I was seeking to the end of the file, and then back to the start to determine the buffer size. Thanks!
&gt; Reading more requires allocating more memory Should the code not postpone allocation until after it knows that there's actually more data? Reserving memory up front to known size and expecting it to not cause reallocation seems like a common expectation
It definitely depends on the situation. In your case, I'd probably prefer the second option, since you only care about a specific sub-pattern. It also lends itself better for refactoring, you could move the code handling `condition` out to another function.
They support range indexing, but not individual byte indexing.
&gt; Is there a quick, easy way to turn those off that don't involve editing source? See that message that says #[warn(unused_variables)] on by default do $ rustc foo.rs -A unused_variables A is for 'allow'.
True. And this is an important distinction, one can take a slice (=defined start, defined end) from a `str`, or iterate over `.chars`, but it makes little sense to loop over a range + index .
You can set rust to allow unused variables within your code: http://is.gd/6Fn0RK `#[allow(unused_variables)]` you can also prefix your variables with an underscore, to ignore a single variable. However, usually, unused variables are a sign of something really wrong
… and Markdown is much better? `[text](href)`, or is it `(text)[href]`, *&amp;c.*
Any idea when you there will be binaries to be download an installer
Practice. All the reading and research is good, but you won't get better at programming unless you program. Start small, and incrementally increase your projects in size. Try to follow language standards when you can so you learn good habits, and just keep programming. If you really want to work on Rust as a core dev, you have a lot to learn. I am not particularly well versed in language design myself, but you will want to be comfortable with Rust, how compilers work (LLVM specifically), programming language theory (if you want to be able to contribute constructively to RFCs), and then a whole gamut of other programming concepts you may not have run into if you were just using Python for small projects (data structures, pointers, and stack/heap allocation for example). Additionally, you may want to read up on exactly how a computer actually handles it's instructions in the CPU. Not 100% necessary I don't think, but very useful and can help a lot in understanding low level details. There's a lot of stuff here, and I wouldn't be surprised if someone more informed on the topic told me I didn't touch on 50% of the knowledge required. Having a goal and working towards it is great, and I don't want you to feel discouraged. That being said, you should probably be aware that you are going to have to learn a LOT if you want a core dev position and it's likely not going to be something that you are doing in a couple months. You can definitely contribute to the language's development without being a core dev, but those core roles are people whose job is to ensure that all of the other work being done meets the language's standards, and they are expected to have a very strong understanding of at least one specific aspect of the project, and (I would assume) a general understanding of everything.
IMHO - the addition of `pub(restricted)` is going to blow the module system's complexity budget. Modules are a hard problem to solve.
I'm fairly certain you want [RFC #560](https://github.com/rust-lang/rfcs/blob/master/text/0560-integer-overflow.md). Most of the issues here seem to be raised by this RFC, so I'd recommend getting aboard the issue commenting train.
Reading the file metadata works and doesn't cost much on small files - it's probably worth it performance-wise (edit: or was before https://github.com/jemalloc/jemalloc/issues/335 got fixed - maybe not now...). I submitted a PR in this direction a while back, but it was rejected on the grounds that it could interfere with syscall whitelisting.
If you're using stable, you're almost certainly also getting bitten by this jemalloc bug: https://github.com/jemalloc/jemalloc/issues/335 . I believe the fix for that will be included in Rust 1.8.
File size is around 1.6GB, but I edited my starting post. Using f.metadata().unwrap().len() + 1 solves my problem (the + 1 part).
thanks for the link, I read over it a bit. I'm under the impression this has been accepted and possibly implemented? I think it's a bit too general for what I was suggesting, but definitely related. In particular, with an actual primitive the compiler has enough information to know ahead of time what the correct behavior is, and I think it should be treated as a special case when it's a primitive for that very reason. 
What is a basic strategy for selecting Rust libraries in the situation I'm not able to evaluate their quality by myself (being new in the language and/or not being deep in lib's domain area, just wanting to pick it up and use)? I suppose that crate.io downloads count might be an approx criteria, and also presence in https://github.com/kud1ing/awesome-rust list. Other options?
Ah, I see.
That's because `&amp;'a T` is covariant over `'a` but `EmptyTrait&lt;'a&gt;` is invariant over `'a`. See [Subtyping and Variance](https://doc.rust-lang.org/nomicon/subtyping.html) chapter of Nomicon for details. _EDIT:_ See [this example](https://play.rust-lang.org/?gist=38663f257c45c1614117fa11528908c8&amp;version=stable&amp;backtrace=0) which shows why it should be invariant.
[June.](https://groups.google.com/forum/#!topic/mozilla.dev.servo/dcrNW6389g4)
Do we have an issue for this bug?
you forget to link your libextern.a to the application. try to add `links = extern` in Cargo.toml `[package]` 
Hey, sorry for the delay. Lets start with describing seastar itself and ideas behind it, just to be on the same page and give some introduction for people not familiar with it. The way seastar apps are written is very different from usual network apps and very similar to HPC apps. There are a lot of different technics used here, lets talk about some them: kernel bypass networking, data locality and thread affinity. None of them are new and were used for at least a decade, but it's a first time to my knowledge that they are used for high level apps. Kernel bypass is an approach when instead of default networking stack a user space stack is used. It allows to get rid of copying from nic to kernel and from kernel to userspace. Typical app will also use no interrupts and instead use polling. No mutexes, no atomics, the goal here is to remove as much overhead as possible. Without kernel bypass you just can't saturate 10/40gbit network. (More details here: http://queue.acm.org/detail.cfm?id=2103536) It's not a secret that context switch is very expensive for certain apps. Synchronization is even more expensive. To get rid of both of them every app thread acts completely independent and is pinned to it's own core. Usually you will see 1 core left for OS and master thread, while other cores are 1:1 to work threads. Kernel is also forbidden from scheduling any work to those work cores. Contemporary NICs have multiple queues and those queues are also pinned to specific core/thread. Meaning that if a connection was handled by core 3 it will always be handled by core 3 and other threads will know nothing about it. (Google Receive Side Scaling to read more about balancing) Given that all of our threads are independent now no locks are required at all. For those cases when some form of communication is necessary SPSC queues are used between every thread. It's also important to note that data locality and shared nothing approach is crucial for NUMA systems, where attempt access other CPU's memory will result in huge performance penalty. In such systems all memory is usually preallocated. Let's get back to seastar. It uses Intel's DPDK for kernel bypass and thread affinity. hwloc is also used in some cases. Seastar provides it's own TCP/IP stack implementation. For fine grained parallelism futures/promises are used. To summarize Seastar is awesome. It brings ideas previously seen only in HPC to a world of general purpose apps. But I think that we can do better (with Rust) and that is why couple month ago I started prototyping my own framework. The idea is much older than that, but I had no time to try it out. By design it's not a big general purpose framework, but rather a set of specialized libraries that can be used independently. Let's go through it layer by layer. While seastar uses DPDK I decided against it and instead wrote my own kernel bypass lib. The reason behind it is that DPDK is just too big and requires tons of configuration, not to mention that it's a C dependency. Unlike DPDK, Netmap or similar libs my lib doesn't require kernel module. It disconnects NIC from kernel, allocates hugetlb memory for DMA and mmaps registers and buffers to that memory. It works and I can receive packets from NIC, but it's far from alpha version and I'm in the process of designing proper buffer abstraction. Beside a couple of basic structures there is no TCP/IP stack yet and it's probably the hardest and most time consuming thing to write here. The higher level is more researched tho. At first I started by mimicking seastar and using futures/promises, but soon realized that even with helpers like do_until, repeat and similar my code was becoming a mess very quickly. I was spending more time thinking about composing futures then writing code and even for simple cases it required me to be constantly focused to avoid bugs. Also it's very hard to make futures fast, probably unless you introduce custom allocator it's not possible to get rid of allocation per future. There is no way to create a pool, because futures are generic. So I decided to go with coroutines. It significantly simplified my code and removed tremendous cognitive load. You can have a glance at before/after here https://gist.github.com/rozaliev/9d61d680a6fbab00d594653bcb28503d Even for a simple case code is so much more readable. Coroutines lib is the only one that I published so far, it's pre-alpha, but good enough to get the idea. https://github.com/rozaliev/monkeys At the highest level there is a lib that helps you to setup all underlying components and allows to write apps. It has swappable network stack, so you could use basic epoll/kqueue while developing. While the main goal is to create set of libraries for higher level apps, all underlying libs can and should be used independently. For example once NIC level lib is complete it can be used to write efficient routers, firewalls and all kinds of software you will typically see in big ISP data centers. My current plan is to publish libs as soon as they stop crashing and at least basic primitives are in place :) Once libs are published we can have a productive discussion about this approach. Meantime I'm eager to know how many people are interested in this kind of libraries and would like to participate in development in future. 
well, your cargo.toml is wrong, you don't have `build="build.rs`. Anyway see https://github.com/alexcrichton/flate2-rs/blob/master/miniz-sys/Cargo.toml for example how to use build script. edit: (and I hope you don't literally write `build="build.rs` and complain about TOML syntax error.) 
I think you can say that `struct X;` is equivalent to `struct X { }; const X: X = X { };`, i.e. you have both a type and a constant with the same name.
Really detailed and long blog post, was a really good read!
Can you (or somebody else) explain why + 1 doubles performance?
Are you planning to reuse any of the existing game engine development work in Rust like Piston?
IIRC rust disabled some `noalias` optimization a couple of months ago because LLVM wasn't behaving properly.
Yes, that's what an awesome rustie proved to be the cause too (see the linked issue). It's really quite unfortunate. It's unfortunate because now our best bet for "noalias" like things is just to inline everything, then the compiler can prove it for themselves.
Looks interesting. You might want to x-post this to /r/rust_gamedev as well. 
quadtrees/octrees are graph and you can actually represent them with indices or with Box + raw pointers safely.
This looks very good! I read it but I'll definitely have to give it a closer read later on. That said, I'd like to go into a bit of detail on the Linux Kernel's allocators: &gt; These requirements make good allocators pretty complex. For example, jemalloc has over 30.000 lines of code. This complexity is out of scope for most kernel allocators. Even the Linux kernel uses a much simpler buddy allocator internally. Most of the complexity just isn't needed for kernel allocators, since kernels differ greatly from common applications. For example, kernels normally don't do heavy concurrent allocations. They allocate only when it's absolutely necessary. You mixed-up the allocators a little bit. The Linux kernel uses a buddy-allocator for it's *frame allocator* (Or page allocator, or page-frame allocator, etc.). For it's general purpose kernel allocator `kmalloc`, there's the `slab` allocator, the improved `slub` allocator, and I think one other option (Sorry, I can't quite recall). These are quite complex because the kernel actually does handle a lot of concurrent allocations and speed can be a problem. You're entirely right when saying you only allocate when absolutely necessary, but allocation does become more common as you go along. In a lot of cases the Linux Kernel uses separate pools of memory for things that end-up being allocated often. By having separate pools, allocating from those pools doesn't have to hit `kmalloc` if they already have enough memory to handle the request, so concurrent allocations from separate pools isn't as much of an issue. For your uses right now though, you're definitely right in saying that it's not worth worrying about. Those allocators are actually fairly similar to your linked-list allocator, so in the end your approach isn't tons different. I implemented a similar allocator in my kernel (You can see it [here](https://github.com/DSMan195276/protura/blob/master/src/mm/slab.c) if you want). The big difference between these allocators and your allocator is that mine and and Linux kernel's allocator only allocate one size of object per slab. My [kmalloc](https://github.com/DSMan195276/protura/blob/master/src/mm/kmalloc.c) implementation uses multiple slabs of different sizes, and then allocates from the smallest slab that still fits the size of the object we want. The reasoning is that, while using multiple slabs like this will use up a bit more memory (Though really it's not much), it's actually *much* simpler and *much* faster. Since each slab only fits one object size, you never have weird-sized holes and you never have to search the list for an entry of the correct size - every entry is the same size. Allocation is simply removing the first entry in the list of free entries, and deallocation is simply adding the entry to the list of free entries. A nice side-effect is that all entries are also automatically aligned to the size of the entries. That said, there's nothing wrong with the allocator you created. The description is on-point and the diagrams and such are very good and well done :)
It doesn't, yet, as it's very early stage, but I've been considering an ECS precisely in order to make sure high performance is achieved.
I personally think that you should not stop dooing what you love because some guy on the Internet does not like you. Because then this person winns. Keep up the good work.
We just happen to present a performant ECS called `specs`, if you are interested - https://github.com/slide-rs/specs
Fully aware of it! Kudos for brining this to the community. As I said, I first need to see if Anima will be specialized enough in order to use it instead of a really naïve implementation. This is definitely a gem in case Vulkan is a go.
Definitely a bit different. Amethyst seems to be very focused on parallelism, while for Anima I'm only considering more aggressive parallelization only when rendering, if needed. Amethyst also appears to be more specialized. I'm definitely going to something that is very simple to use. Basically being able to write the whole game in Ruby. (without really losing performance) I still have the source of the old engine written in Java. That version had a focus on graphics and high performance rendering and I'm planning to keep this too, but in a less broad sense. Amethyst is talking about DX12, Vulkan, Metal. I want Anima to talk about cascading shadow maps and global illumination, without really wasting time on APIs. Amethyst should be able to use [mrusty](https://github.com/anima-engine/mrusty) which I developed outside of the engine especially because I saw their plans to use mruby. :)
Glium is a wonderful library, but it would limit your "performance"-oriented abilities for not being able to render in parallel. And that's not just because of OpenGL - one could still benefit from multiple threads for caching and state verification, like we do ;)
Yes, I just reported the bug in gfx yesterday and it's fixed and working now.
Thank you! This came up for me when adding an enum variant (containing a trait object) in one place caused lifetime errors way far away, by suddenly making a different struct invariant in its lifetime parameters. Or so I understand now :) Phew!
For easy issues, there are some nice links for this on a mozilla employee's blog: http://edunham.net/pages/issue_aggregators.html (towards the bottom).
See one of neutralinostar's (and/or my) comment above. (On mobile, so can't link to them directly easily, sorry!)
To be fair, I didn't expect a cross-post on /r/programming . Here in Rust community we are all (mostly) kind and welcoming even the earliest stages rough prototypes with joy. In the outside world though, people see Rust and expect something grand right off the bat.
They are doing it entirely differently - the use the same GL context from different threads, while we allow you to populate the command buffers (emulated for GL, native for DX11 and others in future) on separate thread and submit them for execution on the only thread to own the context. As for exact benchmarks, I don't think there is anything that is parallel... For single-threaded benches, Piston can be used for it has backends for both GFX and Glium.
I'm not the author, BTW, but that's a good suggestion.
Also, could you share the link to what you have so far anyway, if only so I can subscribe and see how it's going?
Are you planning to let the scripting be abstract? Ie integrate with lua or whatever else binding is available: js, python Having to use ruby for logic would be a blocker for some people (like me).
Thanks for the clarifications. I don't know DX well, and I had no idea about the use of command buffers in gfx. Honestly I now feel that the comparison between glium and gfx is not adapted. It seems that gfx is higher level than glium for the following reasons: * glium match the OpenGL APIs and would probably be a poor interface for something like DX11 or Vulkan * Command buffers and sending data from multiple threads could be done on top of glium for cheap. From there the question is more, do you prefer to use raw OpenGL or glium? Probably it doesn't make sense to use glium within gfx because you can avoid some runtime checks performed by glium as you have more information. Also if you don't provide a full access to the OpenGL API, for graphics API compatibility reasons you can be even more greedy in checks performed. Going back to the original problem of dragostis, I guess it make more sense to use gfx as it provides higher function that have been proved to work well and doesn't block on a specific API. Glium seems more appropriate when you *specifically* want to play with OpenGL or that you are not interested in alternative back-end for rendering. What do you think?
Yes, but I'm not planning to implement anything else soon. I am actually considering also writing a Dart backend myself, so that's probably going to be the second one.
This is pretty close to my reasoning. The reason I mentioned Glium is because it appears to be a more pragmatic answer. I'm quite sure that whatever the API, paying attention to the algorithms you're implementing will have a bigger impact than the API chosen. Still, gfx is a great idea. Having one abstract graphics API is precisely what I was dreaming about so many years, and Vulkan appears not to be the answer yet. Still, I'm a bit worried about the fact that GFX still has a lot of work to done to offer what its ideals point to, some things proving to be quite intricate. :)
To clarify, my opinion on this topic is that a safe API, like glium, should be as low-level as possible in order to avoid bugs and make corner cases predictable. For example if you read an article saying that OpenGL uniform buffers are faster than regular uniforms on some architecture, you can do so with glium because it exposes these concepts. If you want to write an engine with multiple backends, you are encouraged to do so on top of glium. This is especially important because there are many small differences between APIs which makes, in my opinion, the idea of a low-level wrapper around multiple backends impossible in practice. In the end if you use gfx you will probably have to write API-specific code in order to fix these differences (the most blatant example being the fact that you have to write your shaders multiple times). Instead I think that the right thing to do is write a high-level API with multiple low-level wrappers (such as glium) as backends. 
You are right, algorithms are more important. Especially for new projects (or restarted =) ). But in tight loops, algorithms matters less than numbers. You can end up with a very different algorithm than originally designed because it performs better. For instance, bubble sort would be better than a quicksort if your don't have much data or if your data is almost always sorted. Whether you should go with glium or gfx (or both) is mostly the same question. It does not really impact your algorithm (well not entirely true, it impact the code). So a choice based on a performance argument can only be driven by numbers and profiling. You know for sure that your render loop will have, at least, to be executed under 16ms. Once you get a pretty good idea of how well your engine performs and what the data looks like, you'll be able to know which API suits best your needs. And maybe at that point, you'll even realize you need to dive into unsafe yourself to save those ms. But until you get that data, you can't really take that decision based on performance. As (for now) you don't have any numbers yet, I would advise you to choose the library based on: * The API that's solves problems you have now (like platform compatibility, specific pipeline you want to experiment, etc..) * The API that contains features you want to experiment with Finally, you can also experiments with both at the same time! (More fun! :p)
&gt; IIRC, Glium allows you to draw from any thread, and it just sends the closures to the real thread owning the context (which I'm not sure you even has access to). That has been fixed 6 months-1 year ago. You are now restricted to a single thread due to the fact that the context doesn't implement `Send` and `Sync`. &gt; It's worth noting that Glium also supports native NV command buffer extensions, but I don't know if anyone is using them. No, they are not supported and they will never be as long as only nVidia implements them or the extension is not standard. 
Sorry for being a PITA here but I don't consider Glium and GFX to be interchangeable. GFX forces you to think in terms of command buffers and encourages parallel design. That might be important before you go too far with a single-threaded rendering pipeline ;)
&gt; IIRC, Glium allows you to draw from any thread, and it just sends the closures to the real thread owning the context (which I'm not sure you even has access to). I think this is no longer true, but I haven't checked the code recently. &gt; it has to re-validate the state and the cache upon execution the hypothetical command buffers. It make sense from the point of view of glium and I understand that it's really something that you want to avoid in gfx. Thanks again for the clarifications! =)
Check out /r/playrust 
You're making a really good point. The only real issue here is the lack of decent APIs on Apple' ecosystem. The issue with the shaders would be solvable with a unified shading language and I think it would nice to work on that, but how realistic would it be to say that this is the only big issue for this approach?
&gt; Finally, being a "core dev", basically means being a Mozilla employee (though it's not technically necessary), just so you know. It is not a requirement. Currently one third of [the core team](https://www.rust-lang.org/team.html#Core) (3 out of 9 people) are not Mozilla employees. I’ve heard from people on that team, both Mozilla employees and not, that they want this proportion to go up with time. (For the same reason that the project is not called "Mozilla Rust" and Mozilla is not mentioned on the home page of https://www.rust-lang.org/ : it’s more healthy for the language to not be controlled by a single organization) Though of course spending so much time working on Rust can be hard unless you’re paid to do it.
I'm not very familiar with DirectX, but here are some differences between APIs that I can think of: - There's a half pixel difference between DX9 and other APIs because of the way sampling works in DX9. - Sometimes the y coordinate -1 is at the bottom and sometimes it's at the top. - OpenGL's `gl_VertexID` and `gl_InstanceID` don't exist in Vulkan. There are equivalents that are almost similar but have a slightly different behavior when it comes to some corner cases. - Occlusion queries sometimes return an approximation of the number of samples, sometimes the exact number, and sometimes only true or false. - OpenGL doesn't allow drawing at the same time to the color buffer of the window and an in-memory depth buffer, while modern APIs do. - 32-bits indices may have a maximum of 0x1000000 sometimes, and sometimes not. - Vulkan doesn't provide some major features (yet? I'm not sure), like conditional rendering. - DX12 doesn't support render passes. There are of course many other differences, but I was trying to think of the ones which wouldn't be easy to fix. 
What's the current idiom for converting between bytes and numeric primitives in safe code? My instinct is expecting: `u32::to_bytes_be(self) -&gt; [u8; 4]` `u32::from_bytes_be([u8; 4]) -&gt; u32` These would compile down to swaps or no-ops easily enough and, unlike `u32::to_be(self) -&gt; u32`, are type safe in addition to being memory safe. Unfortunately, I can't see that being terribly useful without a standard safe way to cast `&amp;[u8]` to `&amp;[u8; n]`. (Maddening, because it compiles to a mere bounds check... plus some kind of error handling not yet specified. Could be macro'd though.) The rustc-serialize and byteorder crates both feel heavier than necessary for simple file/network tasks. So it's tempting as all heck to just bounds-check (if applicable) and transmute. But I thought I'd ask in case I'm missing something obvious.
Please post to /r/playrust
It is meta-crate that only reexports sub-crates. 
That's usually what is wrong. :p
Aw shoot. Thanks!
Nice article! Just one small style nit: You could have used `.map(_)` instead of matching the `Option` manually.
At least they have their agenda back. Now if only someone gave them a few items so they could meet...
&gt; I would love a very quick code review I'm a beginner too (kinda - I know a little bit of a lot of languages) but I'll give it a peek. &gt; making a test for a ::new() constructor thingiemajig I suspect there's a lot of confusion behind this question, so I'll err on the side of being more basic than verbose. Rust isn't an object-oriented language. It's much closer to functional, and sometimes it's best to forget or redefine OO terminology. "Constructor" is one of those words. In a functional language, a constructor is something like `Point(x, y)` -- it doesn't make sense to have only one of the fields filled in. You need them all. There's another kind of constructor, the one with the curly-braces that looks like this: let x = LvlHeader{ title: t, width: w, height: h, tileSet: tlset ... }; In either case, you must fill in all the fields (safety says~b). But in this second kind you can't fill in the fields you don't see. This limits construction to code that can see the private fields of the `struct`. Also in a functional language `Point(2, 3)` is a completely different value than `Point(3, 3)`. We don't think of it as an updated version of any point. It just is what it is. Rust doesn't force all of your types to act like this, but types that do are typically easier to work with. One of the things they usually don't need is a `T::new()` function - it's only required if you have to do validation or maybe some kind of precomputation, etc. As a point of style, I do provide `T::new()`or other appropriate names if T is a type of things that remember what you do to them. A state machine should start in a starting state, not whatever random data external code decides to use. In the simple case, no, it generally doesn't need to be tested by itself. But my tests are going to use it to test more complex behavior. E.g. if I'm parsing a file, this might be a test of error handling let t = super::Parser::new(); t.write(INVALID_HEADER_1).unwrap(); assert!(t.isHalt()); assert_eq!(t.err(), super::ERR_BAD_HEADER); Thus `new()` is covered even if it doesn't get its own test. ---- Anyway, `git clone` looks good so I'll be back in a bit.
It's really hard to decide. Supporting a lot of different APIs and edge cases would mean investing more time in this than I actually want. What do you think about using OpenGL 4.1 on desktop and ES 3.0 on mobile until Vulkan gets to be more popular?
[Link](https://github.com/rust-num/num/pull/142) for the curious. It looks nice, things such as `align_down` really belong into some library. However, the num crate does not seem to support no_std crates… Or did I miss something?
Thanks so much! I definitely want to write more :) I'll try to update the series more often again (if I have enough time).
You probably mean `bad_bit_mask` or `min_max`? I don't think we lint `x != _ || x != _` yet but that'd be a good easy issue for Friday's workshop!
You're welcome. Perhaps it's good to show the `match` too, as that's exactly what `map` does.
A great way to become a better programmer is to tackle the bug tracker. You'll learn the lay of the land, start seeing common patterns, and have a solid goal through the process.
Long ago, I wrote a blog post: http://words.steveklabnik.com/how-to-be-an-open-source-gardener
I remember that post! Personally I think the workflow of submitting a bug to is a bit of a mess. Users are expected to search for the issue before writing out the bug report and I think this is the source of many duplicate reports and wasted triage time. The best fix I've seen is how Uservoice searches for similar issues while you write the report - e.g. [Podcast Addict's Uservoice page](https://podcastaddict.uservoice.com/forums/211997-general?query=add%20swipe).
You might find [`std::ascii::escape_default`](http://doc.rust-lang.org/std/ascii/fn.escape_default.html) useful, which I use all the time for printing out `&amp;[u8]`. It might actually do exactly what you want in this case.
A version with a few more rusty constructs: pub fn hexdump(data: &amp;[u8]) { for chunk in data.chunks(16) { let mut numeric = String::with_capacity(50); let mut ascii = String::with_capacity(16); for (i, &amp;b) in chunk.iter().enumerate() { numeric.push_str(&amp;format!("{:02X} ", b)); if i == 7 { numeric.push(' '); } ascii.push(match b { 0x32 ... 0x7e =&gt; b as char, _ =&gt; '.' }); } println!("{:50}{}", numeric, ascii); } } (It also prints the ASCIIfied chars for the last, maybe incomplete, line.)
awesome, thanks! I was wondering if there was an enumerate like python's. Very useful. Looks a lot cleaner
Yeah, that might be helpful. I don't mind the dups, personally, but I'm a weirdo.
thanks! I'll check it out.
In my experience, `std::io::copy` is often faster that `read_to_end`. At least if inlined. Unfortunately if you use it too often, it won't be inlined anymore and then it's slower... It's kind of a lottery.
Most of the time it shouldn't matter, I/O time should overwhelmingly dominate. That's the ideal world. If there are any codegen/library bugs that make nice loops compile to slow misunderstandings of said loops, the I guess there can be problems. You should profile the slow case.
Thanks, I'll try to read through that a few times and try to make sense of it :) My brain is really working with the bare minimum when it comes to programming. I feel I would probably benefit a lot from reading some general non-language specific programming concepts, but I tend to feel a bit lost without actually trying out code, and I can only go through so many "hello world" examples and nonsensical snippets where all the variable and functions names are some variation of foo, bar, baz (I sure wish they would stop being used in language examples!).
Maybe you should actually start by coding. Try to find easy to tackle bugs in the issue tracker or features in the roadmap that are simple and try to gain enough knowledge to get it done.
Would you like me to help write a section on syntax extensions? I haven't had any experience writing an attribute extension or linter, but I can do a writeup on `syntax::parse::parser` and the AST.
I imagine that doing this reliably in the general case would be impossible, because operator overloading allows `!=` to do arbitrary things. Even assuming that it implements a nice partial or total order as demanded by the documentation, it could perform IO as part of its operation (e.g. write a line to a log-file). But I suppose that for most sane implementations of the equality operator, it would make sense to warn, so the false positives may be worth it.
If you want to become a core dev, then a good path to take would be to contribute good quality pull requests and keep doing that for an extended period of time. Also, you should be good at communicating. If your contributions are of high quality, then - perhaps - you will gain developer/contributor access. Who knows? :) You must work on making yourself irresistible.
That's pretty subtle! Does that give you some extra advantages over just `struct X { };` ?
A library can be great without being production ready. Further, MD5 may be necessary for backwards compatibility in a project.
Code review, part quickie uno. I read the level parser and associated data types. Clear code, reasonable choice of local variable names, *extremely* strange file format in places, but I'll assume that's not your fault. let poly_count = (buffer.read_f64::&lt;LittleEndian&gt;().unwrap() - 0.4643643).round() as u16; And the weak spots are array manipulation and error handling. Let's think about what the parsing task involves: - grab the entire file (as big as the OS says it is) and copy it into memory. - Create a mutable data structure with growable parts to accommodate the variable-sized elements of the level - Copy data from the buffered file into that data structure, converting as needed. Plus some wasted work copying around intermediate values, and a very unnecessary dependency on libc for CString. The type you want working for you is`&amp;[u8]` - slice of bytes. This has a lightweight value: inside there's a base memory address, and two indices. Instead of copying data around, let's just call out slices of the data that's already loaded. // Remainder of file let mut rem = self.raw.as_slice(); let (version, rem) = rem.split_at(5); self.version = match version { ... } let (_, rem) = rem.split_at(2); self.link = rem.read_i32::&lt;LittleEndian&gt;().unwrap(); One of the subtle rules of `let`-without-`mut` is that it can give an old name to a new value and new location. So it turns out that we don't need to make `rem` mutable for the splits, but we do need it to use the read methods that `byteorder` patches into the primitive types. (Aside: don't worry about running out of space in the virtual CPU. LLVM is very good at forgetting values that won't be used again.) (Aside #2: this is one reason why I don't think `byteorder` is particularly clean. It's clever, sure, but I'd like to see some other solution. Also, patching types belonging to other crates is poor form.) Before getting too carried away, I'll point out that there's no error handling yet. `split_at` requires that the slice be long enough to split. If it's not, it panics and kills your thread. The parser function deals with outside data, so it definitely should defend against errors. On the other hand, the only consequence is a panic (same as `unwrap`) so you can leave it as is. The guards might look like this if rem.len() &lt; 2 { return Err(Error::UnexpectedEOF); } let (head, rem) = rem.split_at(2); Or let somethin = !try(rem.read_u32 ... These require a `-&gt; Result` function signature. --- Okay, so that's the basics of reading bytes and numerics from the buffer. Let's consider strings. The short version is that you probably do want to copy data out of the all-in-one buffered file. This makes it easier to keep the borrow checker and allows you to drop the raw data if you choose. (`level.raw = vec![]`) It's likely possible to keep pointing into the raw bytes, but I'm not going to try and figure it out now. So, we'll need a new memory location. What's the easiest way to get memory for a variable number of things of the same type? Always think vec first. But before then, it's necessary to slice off the trailing null. That's all that `CString` is doing for you. Do it instead with a slice manipulation: let (name, rem) = rem.split_at(51); for name_trimmed in name.splitn(1, |c| c == 0) { self.name = String::from_utf8(name_trimmed.to_vec()) .unwrap(); } Which, okay, apologies for the functional programming. The star of the show is `split`, which starts at the beginning runs tests on each element. Here that test is `|c| c==0` - let the element be named `c`, is it equal to zero? `split` then splits out a slice that points to the first run of false elements. `for` names that slice `name_trimmed`, then: - `to_vec` copies those bytes to a new area of memory, returning a Vec that manages it. - `from_utf8` validates UTF8 coding and creates a String pointer that owns the Vec that owns the memory that contains the copy. - `unwrap` asserts no error - `self.name =` mutates `self` Then `for` calls back into the `split` iterator. However, this is a `splitn(1, _)` n = 1, so it only yields 1 trimmed name. (I do however applaud you for finding a way to do this, even if it was with CString. Functional programming is *not* particularly obvious at first.) --- One last thing because it's easy: The uint types don't require explicit wrapping methods, you can just use `u16` and ordinary operators `+ * % ^` to generate the toy stream cipher.
Actually `octavo-digest` should be production safe (but only this one). It is still marked as one as I need more tests against known test vectors and some randomly generated tests (via great `quickcheck`) to be sure that my implementations match specifications. Also I am thinking about setting up AFL to fuzzy test against any vulnerabilities. There should not be many of such, but in future when I start work on TLS implementation it will became useful.
I like that suggestion. I like my particular workaround in this case better just 'cause it lets the user use negative values in ways that intuitively make sense, but I'll keep that in mind for other stuff I do. Thanks!
Oh right I just understood the difference! I would find it way more intuitive if `struct X;` and `struct X {};` where completely synonymous (like mentioned in the alternatives). I don't really get why this caused problems
Thanks for the thorough reply, it's very appreciated. Yes, the file format is certainly... special. Most of the examples for it is in C as well, so I sort of frankensteined it the best I could. It's also my first attempt at Rust ever, so it's been an adventure trying to understand anything at all :) let poly_count = (buffer.read_f64::&lt;LittleEndian&gt;().unwrap() - 0.4643643).round() as u16; As for this particular thing, I believe the reasoning was something about byte alignment (which is way beyond me, so I have no idea if there's any sense at all to this or someone played a joke on me). It's stored as a double for whatever reason, with 0.4643643 added to it. I cast it as u16 because the max value should be 1000 (another arbitrary thing). I'm not sure if there's any sense in that or not. I read (or think I might have read) that it can be faster storing numbers in the native memory size? Seeing as u16 can be way more than 1000, perhaps it's more sane to manually check that it's below 1000 anyway and use... uhm usize? I don't have the best understanding about all of this. Thanks for all the examples. It was a bit of a pain trying to get that Reader thing to work for me, so if that's not needed at all that'd be great. The problem with being new to the language is it being hard enough to even make something work, and even harder to know what the better way would be to do it. I will definitely check out your examples some more and try to figure out how it works, it's been a nice help. Yea the error handling I sort of skipped initially, because I couldn't for the life of me figure out what kind of result it wanted me to make, and I wrestled for a couple of hours trying out random things that never ended up working. I do think I have a bit better understanding of it now, so maybe it's worth a revisit. When it comes to strings... Oh well that was an adventure as well :). It might boil down to me just not simply understanding the differences, but as I understood it, Rust only deals with utf-8. As I want to also save/write a valid file, I need to convert that back to u8's. Maybe this is not an issue at all, but since the file format only expects chars and ascii, I couldn't see any easy way of doing that? Comes back to examples being in C, so when I saw the CString thing I just immediately went for that. Which I then had to make that helper function for since it didn't like it if there were multiple null bytes in the provided byte array (which was also made better by another helpful person which I got a pull request from after this post, my initial method was even worse and would certainly not be applauded :). For the cipher thing, hmm. Well that's what I initially attempted to do. Maybe I will go back and try again, and it was just a problem with casting in the wrong places. I struggled with a lot of... uhm I forget the error message now, something about arithmetic and unreachable code. The wrapping functions sorted those out, so I went with that. Anyway, thanks again for all the help, gave me a lot of things to think about and improve!
but also likewise, I would _not_ be employed by mozilla if some other organization wanted to pay me to do the work that I'm doing. Not because I hate mozilla, but because of this principle.
We have https://doc.rust-lang.org/book/compiler-plugins.html already, but I'd bet it's out of date. Maybe check it out and send in a PR?
&gt; This has a lightweight value: inside there's a base memory address, and two indices. a slice is a pointer and a length, not a pointer and two indices. &gt; One of the subtle rules of let-without-mut This is true of `let` _with_ `mut` as well. &gt; Also, patching types belonging to other crates is poor form I would be interested in hearing more about why! The usual objections I'd have to something like this are solved by the coherence rules.
Yeah this is where I first started out, but this doesn't go into how to use rust's ast parser. Actually, there's hardly any on how to use the parser. I'll probably send in a pr, just not sure when.
For fun I ported a friend's python script to generate Taco Bell menu items. It was super simple with the `rand` crate. Check it out on [on crates.io](https://crates.io/crates/tacobell). I'm also working on https://github.com/rust-lang/cargo/issues/1570 as my first contribution to Cargo, since I ran into that issue last week. There were existing PRs for it to take over, so not a lot of work to do. Just a matter of making time
Can you report the full error message?
Alternatively, `::std::u16::MAX` works.
rust-gdb? holy shit, I've been missing out. lol, still glad I got this though because the bug was dead obvious once i saw the hex and ascii side by side.
seems up my ally
Is the explanation that the extra byte is for EOF? If so, I'm still confused. But shouldn't rust determine whether a byte is EOF before trying to write to the output buffer? Is this a bug? It's pretty unintuitive IMO. Edit: is it that when read() returns 0 we can't tell whether it is because the buffer is full or EOF so rust always trys to resize the buffer first?
The link appears to be fixed in the [beta version](https://doc.rust-lang.org/beta/book/getting-started.html#closing-thoughts); it was supposed to link to the [Guessing Game tutorial](https://doc.rust-lang.org/book/guessing-game.html), but I guess at some point the chapter was renamed but the link wasn't fixed.
::std::u16::MAX worked, but I was unable to get std::u16::MAX working when I placed it at the top of the file. I'm not sure why, but I understand why ::std::u16::MAX works. Since I dislike it being so wordy I just ended up doing let u16MAX = ::std::u16::MAX; and used that. Thanks for the help!
Indeed, thanks!
What catches most people at first is the different resolution rules in `use` statements, vs. in paths within modules. Let's start with describing the module namespace. In your top-level module, generally `lib.rs` for libraries or `main.rs` for executables, the compiler acts as if the following implicit declarations are present, declaring the `std` crate and importing [some of the most commonly used traits and types](http://doc.rust-lang.org/std/prelude/v1/): extern crate std; use std::prelude::v1::*; What this does is populate the top-level namespace of your crate with the following names: std Copy Send Sync Sized // ...etc... However, in each other module, only the implicit `use` is inserted: use std::prelude::v1::*; Which means only the following paths are defined: Copy Send Sync Sized // ...etc... So from the top-level module you can use `std` directly, while in other modules you have to explicitly import it or use a fully qualified path. The part that then further makes things confusing is how path resolution works, as it's different in `use` statements vs. in bodies. In a `use` statement, it defaults to resolving from the crate root; as you frequently want to `use` things that have been imported from `extern crate` statements in the root, or from various other modules via absolute paths. So a statement like this: use std::io::stdin; Will, from any module, start from the crate root, resolve `std` based on the `extern crate` declaration, and walk its way down from there. If you want to resolve relative to the current module, you have to explicitly use `self` or `super`: use super::foo; On the other hand, paths in any other location than a `use` statement refer to the namespace of the current module. Thus, the following: let max = std::u16::MAX; Will only work if `std` has been imported into the current module's namespace; either via the implicit `extern crate std;` in the root module, or with an explicit `use std;` in any other module. If you haven't done that, the only way to get at it is to explicitly qualify that you want to start resolving from the root module with `::`: let max = ::std::u16::MAX; I find that this can be a little surprising at first, but if you think about it makes good sense; in `use` statements you much more often want to bring things from far away, like external crates or other modules, into your namespace, so starting from the root makes sense, while in the body, you much more often want to just refer to local things or things that have been explicitly brought into your namespace with `use`. The convention that I generally is to import individual modules within `std` with `use`, in every module that I use them, like so: use std::u16; let max = u16::MAX; This works out well when I use several modules, since I have one big import list at the beginning, and then nicely short but qualified names throughout: use std::{io, error, mem, u16, u32}; fn foo() -&gt; io::Result&lt;()&gt;; mem::drop(blah); if x &lt; u16::MAX as usize { }
I'm of the impression that no one knows how to make a constant-time RSA library that also performs well. My understanding is that mature RSA libraries generally do a variety of tricks to make it very hard to get useful timing information, but that its not really constant time. Are you aware of any fast RSA implementation that is constant time? Right now, anything that isn't ASM is really hard to guarantee executes in constant time, at least for certain problematic algorithms such as AES. So, "pure Rust" really could be replaced with "non-ASM".
Does the Iron benchmark (55000 requests per second) refers to Iron on top of hyper? (i.e. the usual usage) I was confused because the one labelled "Uwsgi" (22000 requests per second) probably means Iron on top of uwsgi, but they are both running "Iron"..
Yes, sorry I should have mentioned that the Iron benchmark is running on top of hyper
&gt; But... why? This feels like a wart. I'm not sure. It would maybe make more sense to have the implicit `extern crate std;` in every module. I thought it might be because you couldn't have more than one instance of `extern crate x;` in a given crate, but that doesn't seem to be true. So yeah, it would make sense to insert that implicit `extern crate std;` into every module just like is done with the prelude. Except... that would be a breaking change, because it's possible for people to have imported other modules named `std`. So not a wart that's fixable until Rust 2.0, I don't believe. &gt; I wouldn't mind having to write `use ::std::u16`, just to have a bit of consistency :( I don't think the consistency here is at all helpful, outside of the first time you're surprised about this and then learn how it works. In particular, `use` is used to bring something into the current namespace, while other uses of paths are there to refer to things currently in the namespace. They are two very different kinds of operations, and it is common for languages to have `import` or `use` statements that bring something in from some global namespace or search path to the local namespace, while the local namespace can only access those things that have already been imported somehow.
Due to improvements in Rust (i.e. Rust didn't permit these in the past and the codebase never upgraded): Elision, match_if_let Stuff people miss: Let_and_return, iteratory things, monadic optiony things, cyclomatic_complexity Though I think people don't yet have an intuition for "new elision" so even newer codebases don't utilize it to its fullest extent.
The builder pattern is better suited for cases of optional values, but in your case you certainly want the user to apply all three values, so it's more appropriate to have something like: impl Rectangle2D { fn new(location: Point2D, height: f64, width: f64) -&gt; Option&lt;Rectangle2D&gt; { if height &gt;= 0 &amp;&amp; width &gt;= 0 { Some(Rectangle2D { location: location, height: height, width: width }) } else { None } } } 
&gt; In this case, because negative dimensions are a logic error, I think you're fine using asserts and panicking if the value is less than 0. Please don't panic in such a case. Return an Option or Result and let the user decide what to do.
I know. I've never even remotely considered Octavo as anything-ready project. I've started it as I wanted to learn cryptography, and to be honest it is still learning project. I would be happy to see it peer reviewed, fuzzed and production-ready, but honestly I do not believe that this can happen sooner than 5-10 years since now. It is still learning project where I test my wild ideas like random-testing against OpenSSL (which I will probably replace with Ring) or making hash function independent HMAC without runtime allocations.
Sooner or later, if Octavo want to be serious, it will need to include some assembly. It is impossible to do otherwise as currently there is no way to use AES-NI without it. It doesn't include any ASM now as I couldn't find way to do so in way that will satisfy me. Perfect solution would be that `rustc` would understand assembly (just like `clang` and `gcc` does) and allow us to assembly without external dependencies. For now I want to find how much can be done without single line of assembly and in future add what only as much as absolutely needed minimum. **EDIT**: About RSA, I am working on `bn` crate that will provide BigNums without allocs. De facto static array of limbs with defined addition, multiplication, subtraction and modulo. This should be safer than allocating BigNums but not such flexible, which is perfectly fine RSA case.
I'm not a new Rust user, but borrow checker still confuses me sometimes...this is a distilled example (the real one is more complex) : use std::collections::BTreeMap; use std::sync::Arc; use std::borrow::Cow; struct MyMap(BTreeMap&lt;Arc&lt;Cow&lt;'static, str&gt;&gt;, String&gt;); impl MyMap { fn remove&lt;'b&gt;(&amp;mut self, m: &amp;Cow&lt;'b, str&gt;) -&gt; Option&lt;String&gt; { self.0.remove(m) } } Fails with: error: cannot infer an appropriate lifetime due to conflicting requirements [E0495] note: first, the lifetime cannot outlive the lifetime 'b as defined on the block at 8:65... note: ...so that expression is assignable (expected `&amp;collections::borrow::Cow&lt;'_, str&gt;`, found `&amp;collections::borrow::Cow&lt;'b, str&gt;`) note: but, the lifetime must be valid for the static lifetime... note: ...so that trait type parameters matches those specified on the impl (expected `core::borrow::Borrow&lt;collections::borrow::Cow&lt;'_, str&gt;&gt;`, found `core::borrow::Borrow&lt;collections::borrow::Cow&lt;'_, str&gt;&gt;`) ...I could change this to `m: &amp;Cow&lt;'static, str&gt;` and have it working, but `BTreeMap::remove` shouldn't require this, I believe, it should work with borrowed slices too. 
Thanks, that looks like it will really help.
User/logic error/contract violation is precisely what panicking is for. That's why index-out-of-bounds is a panic. Granted it's not always clear where the contract lies, especially when it comes to validation: If `height` comes from user input, then somewhere up the line you need to check `height &gt; 0.0` only for the builder to check again `assert!(height &gt; 0.0)`, duplicating the constraints and leaving the door open for them to get out of sync. A `Result&lt;Rectangle2D, ValidationError&gt;` would allow bubbling that up. If `height` comes from some mathsy calculation, then dealing with `Result` is just a loss in ergonomics with no benefit. Error handling is hard. In this case though, of low level primitives, I expect `panic`-s, but there's no hard and fast rule.
Trees are not inherently unsafe. A tree structure meshes perfectly with ownership, so they can be expressed in safe rust; on the other hand ownership semantics will typically have you use the friendly functions replace and swap a lot. However, if you introduce parent pointers or some kind of crosslinks, then the tree becomes a de facto graph.
After fixing a few obvious errors in the code you provided (missing `main()` and imports) I get this error: a.rs:23:57: 23:67 error: cannot borrow `node` (here through borrowing `node.right`) as mutable more than once at a time [E0499] a.rs:23 match (&amp;mut node.left, &amp;mut node.right) { ^~~~~~~~~~ a.rs:23:57: 23:67 help: run `rustc --explain E0499` to see a detailed explanation a.rs:23:41: 23:50 note: previous borrow of `node` occurs here (through borrowing `node.left`); the mutable borrow prevents subsequent moves, borrows, or modification of `node` until the borrow ends a.rs:23 match (&amp;mut node.left, &amp;mut node.right) { ^~~~~~~~~ a.rs:23:68: 23:68 note: previous borrow ends here a.rs:23 match (&amp;mut node.left, &amp;mut node.right) { ^ I’ll assume you’re talking about this. `&amp;mut T` is called a mutable reference or mutable borrow, but really more than mutability it’s about unique access. You can safely do anything with data that you’re the only one accessing. Here you want to uniquely/mutably borrow two components of `node`. That’s OK since the two are disjoint (separate with no overlap), but a current limitation of the compiler makes consider that `&amp;mut node.left` borrows all of `node`, not just `node.left`. Maybe this can be improved in future version of the compiler. In the meantime, you can work around this limitation by using a struct pattern. That way the compiler seems the two borrows made at the same time and can more easily realize that they’re disjoint: https://play.rust-lang.org/?gist=683b83fff40c1db2cf61ff14fb176c46&amp;version=stable&amp;backtrace=0
I see. Well I suppose the reverse thing is my potential problem, if it actually would be one. While it'd work fine parsing the files and getting the information from them, I would assume writing utf-8 to ascii is more troublesome, or? As the plan is to abstract the weird format of these files into more easily editable structs and whatnot, at some point a user is going to be able to manually write strings in some of the fields. So if someone wrote some hiragana characters, or any non-valid ascii... I am not sure how to handle that? They would if I remember correctly from just googling it one second ago, take 3 bytes for each character? And now just having written that and tried some search keys, is the solution to just use the std::ascii::AsciiExt trait?
The real problem why the compiler complains is that `node` is an `&amp;mut Box&lt;Node&gt;`and it won't allow you to borrow the box more than once. If you add `let node = &amp;mut **node;`directly before the match ([playpen](http://is.gd/apFjJM)) then `node` will be an `&amp;mut Node` and everything will work.
The actual problem here is that node is a reference to a Box, rust if perfectly happy with mutable borrows of different components of a struct.
Really appreciate your help. I didn't know that it borrows all of the struct and it seems like a good idea to use a struct to trick the compiler. Do you think the way I'm implementing `remove` is correct? I get a feeling there's a better, more Rust friendly implementation.
Ah, that makes more sense. Thanks for the explanation! So something closer to the original code can be written as: let node = &amp;mut **node; // Get &amp;mut Node from &amp;mut Box&lt;Node&gt; match (&amp;mut node.left, &amp;mut node.right) {
But wouldn't it cause problems that I would have to manage lifetimes of the nodes manually?
Um, if you're making references from mut ptrs the lifetimes will be as long as they need to be. I don't believe you'll encounter any problems you wouldn't encounter with manual memory management elsewhere. But I'm not sure that the Option&lt;Box&lt;Node&gt;&gt; signature is what's causing you friction, here. No need to go fully manual.
Not exactly. Yes you can run it without using the hyper http server, but the uwsgi-rust plugin still implements hyper traits such as NetworkStream to get iron support. And you still need hyper for iron for things like the header macro, and so on
That's exactly what I'm trying to do, but it's very hard to have both cross-platformness and low selection of APIs. Right now, I'm tempted to place Vulkan support on hold and concentrate on OpenGL/ES and implement a pipeline on top with some amount of extensibility. I think I can get away with implementing something that's as simple as possible with the best possible performance without sacrificing simplicity. I want something that just works and is not a headache. The way I see it, the big game engines suffer performance penalties because they're focused on productivity (i.e. implementing a lot of graphics just in time for the game to ship and look pretty), other times they don't have a clear focus (they start with graphics then chop it down). There's also most definitely a penalty because of the size of the project. The more people work on something, the less they think about performance.
Have you tried doing that exact thing? What does the compiler say? EDIT: Right, so because you can do something like impl Expr for i32 {} the compiler (rightly) does not let you write code that could result in an ambiguous implementation of a trait. One way around this would be having a struct like struct Expression&lt;E: Expr&gt;(E); and passing that around instead of the types implementing `Expr`. This means that there will only ever be one unambiguous implementation of that trait at resolution-time.
Slightly off-topic, but Is there a .mobi version of the Rust book I can download?
See my edit, I didn't realise until I tried it in Playground
That is pretty, but (+/- LLVM optimisations) horribly inefficient, because it allocates a vector (heap-allocated collection) of Strings (heap-allocated collection) and then joins them together after the fact. You'd be better iterating and `write!`ing.
I view it as a matter of preference, and for me I lean more towards errors appearing as early as possible so that a developer can quickly find the exact line where something is going wrong (one example: a builder is started somewhere else in a utility class the developer is unfamiliar with, and for example if that utility class experiences bit rot, that line number will tell them where to go immediately) To me it's just as readable - if you are in a situation where you can't use the type system for validation (e.g. you are receiving inputs and plugging them or some derivative into this builder), and there are lots of cases where those inputs can cause an error, I think it's actually very self evident what the code is doing if you reflect that each function call receiving those inputs can result an error.
I think that it is reasonable to allow false positives here, since it is a violation of the principle of least surprise anyway to have operators have arbitrary side-effects since the inputs are (normally) shared borrows. Silently bailing out if it finds side-effects could lead to false _negatives_, which in a linter is arguably worse.
As other comments said it’s not really about the struct. Rather it’s the `Box` that can only be borrowed as a whole. Dereferencing it separately works around that. Regarding `remove` on a higher level I haven’t looked closely, but you’ll probably want to use recursion at some point. 
Why don't just have another different check that warns on side-effecting "operators" that should not have side-effects? This is orthogonal to tautological comparisons. Zero false positives is a goal checks should strive to. By using two different lints for this, one gets a warning about the fundamental issue first (a side-effecting comparison operator), and in case the comparison is tautological, one will get another warning once the big issue has been fixed.
I think the compiler should only error out unambiguously, without false positives, but lints are totally OK to false-positive if it is a choice between that and false negatives. I mean, `println!` is a side-effect, but you don't want to silently not warn when an operator uses that and you don't want to special-case `println!` either.
Writing the 1.8 release notes today, came back to find this this time, ha!
&gt; This is true of let with mut as well. Mm, yes, good point. To check my understanding, and try to phrase it in a way that's easy-to-understand: `let` always creates a new location\* (or locations). This location lasts until the end of the current block. The name -&gt; location binding lasts until: - it ends at the end of block - another `let` replaces it - it is shadowed inside an inner block (temporary) (\* the language spec uses the term *lvalue*) Beginners would benefit from diagrams to explain shadowing. Bare `let` creates a location that can be stored to once. `let mut a` allows a later `a =` to overwrite the same location. In either case, the compiler verifies that all possible code paths make an initial store before any use or borrow. &gt; a slice is a pointer and a length Good catch. I was lazy and didn't check and assumed that like a GC language you might want to have the base pointer in memory. On the other hand, it might be better to explain slices conceptually as two indices or a bracket selecting part of an array. Especially with pictures. --- &gt; [monkey patching] Experience with Ruby demonstrates that patching runtime dispatch tables that are valid everywhere an object is use leads to insanity, baldness and sudden death. Rust is trying to bring safety to this wilderness. There are quite possibly conceptual bugs that we don't know about yet within that system, but let's assume that it both works and always does what you'd intuitively expect. Cool. That means every (namespace, type, method) triad maps to exactly one `fn` definition, and this is computable at compile time. You still run into difficulty if you decide to move code between modules. Something like `byteorder` really isn't that bad (and you don't have to use the extensions, if I understand correctly, but with a lot of remote `impl` blocks, maintainability could suffer a death of ten thousand papercuts. Macros need special handling too: do they use the dispatch rules of their defining namespace or the target namespace? (This is probably something that macros think about; I haven't dug into the nuts and bolts of defining macros yet.) Ideally I'd like the standard *prelude* to have a reasonable opinion of how to convert between primitive types and bags-of-bytes. Those are primitive operations after all! What keeps that future opinion from conflicting with `byteorder` traits? Waiting for the compile to break? Not great. Worse: `byteorder` wants me to think about `std::io` readers and writers. Readers and writers are stateful objects and this task doesn't require statefulness .
This is the commit in which I switched back to the ::std::u16::MAX; placing a 'use std;' directly under the 'mod address_mode' did not enable me access the std::u16. https://github.com/mreiland/trustiNES/commit/bf29638c40616cffd526c848e80b7427de585b6d I don't recall the version of the rust compiler off the top of my head, but it was stable from a month or so ago.
They could always make it an opt-in lint.
I just tried placing `use std;` directly under the `mod address_mode {` line, and changing the references to `std::u16::MAX`, and it compiled and tested just fine for me. I notice that you have a `use std::u16` in the top level module in the file, but that doesn't help for bringing `std` or `u16` into the namespace of the modules within that module like `address_mode`; I wonder if that might have been your mistake? Anyhow, give it a try yourself, you should see it working.
&gt; What is the 'base pointer' here? Imagine the only thing keeping an array in memory is a slice. If that were the case, you'd need a pointer that could be passed to `free` when you're done with the array. This has to be the original pointer from `malloc`. But I guess Rust doesn't work that way. Slices only borrow memory, so they don't have to call `free` or decrement a refcount when dropped, so no need to hold the base memory pointer.
That's pretty cool! :) More of a gfx-rs question, but i see you have written glsl and hlsl shaders in a lot of the examples. Does gfx-rs simply load the correct one depending on platform it's compiled for? 
I tried it at the top of the file when I realized I couldn't get it to work in the address_mode mod and apparently forgot to remove it. I'll play around with it when I get home tonight. Thanks for taking the time to look at it.
Ah yes, right.
Usually when I file a bug it's a dup. But I wouldn't know it because it's filed under a different name and not easy to find. On the one hand, I kind of wasted my time reporting; on the other hand, if it's not filed I'd feel bad; on the third hand (?!) it would waste more of my time to go through all the bugs to make sure it hasn't been filed before. So I don't know what the best solution is.
As far as the Rust project as concerned, and as someone who does a lot of triage, I prefer that if you're unsure, or don't have time, that you just file an issue. I'd much rather have a bunch of dups than not know about an issue in the first place, and as someone who does a lot of triage, it's not too bad for me to remember "oh yeah this has been filed." Other projects may work differently.
That depends on what you mean by static method. Edit: If you mean a method that doesn't take `self`, it's exactly that.
having literally every single instance of "struct" and "impl"and whatnot highlighted in that yellow background really didn't sit well with me... it was distracting. but that could just be my personal preferences.
using the escape character (\\)? that seems like an odd choice. I could understand using the standard path separator (/) possibly\*, but even then, the paths to what you want to access are not always so easy to define, especially with external crates and such. \*no, I'm not dumb. I realize Windows uses backslash, but effectively no one else does, and backslash causes **so** many problems when used for anything but escaping characters that it shouldn't even be under consideration. Moreover, Windows actually deals just fine with forward slashes most of the time.
It wasn't me actually, I just made the `dxgi+sciter` backend and UI for the original gfx-rs examples. But yes, those shaders are shared between D3D and GL. And not only shaders — all samples can be runned on both backends. Moreover, those crazy guys do not want to [stop](https://github.com/gfx-rs/gfx/issues/352) :)
Haha, what a fun crate. I'm excited to order a "Double Cantina Doritos Locos 5-Layer Burrito". 
And this thread (just one topic of many) https://users.rust-lang.org/t/floating-point-traits-roundtable/5382
I'm not sure copying the entire tree for each modification is the way to go... and what's up with the let new_bst = new_bst; new_bst pattern you use at the end of your methods?
Because in D I've used a dynamic array, that is similar to a Rust Vec.
Forward slash parses weird: `let x = std/u16/MAX - 1;` is that division? `let x = std\u16\MAX - 1;` this parses unambiguously PHP also uses backslash: namespace Aura\Filter; use Aura\Filter\Failure\FailureCollection;
So can I go to https://rustup.rs from a RasPi and install Rust directly now? I'm a bit confused with those target triples.
Thank you for your answer. Sorry for my delayed answer, I have had to study some of Rust in the meantime, to understand your answer, and to give you a good answer. This D line of code: auto a = new int[10]; In general should be translated to this Rust line of code: let mut a = vec![0i32; 10]; So in this D code 'a' isn't a stack-allocated fixed-size array: a[] = 5; // Set all items of a to 5. So this line of Rust code isn't doing what it should, you can't use this: a = [5; 10]; // Set all items of a to 5. So I think the original D code: void main() { auto a = new int[10]; auto b = new int[10]; a[] = 5; // Set all items of a to 5. b[] = a[]; // Copy slice a in b. b[$-2..$] = a[$-2..$]; // Partial copy, $ is contextual length. auto c = b.dup; // Whole array duplication. auto d = a ~ b; // concatenation. } gets translated to this Rust code (if you see how to improve it, please tell me, but at least I think this code is correct, it gives the right results and the right data structures in memory): fn main() { let mut a = vec![0u32; 10]; let mut b = vec![0u32; 10]; for i in 0 .. a.len() { a[i] = 5; } // Set all items of a to 5. b.clone_from_slice(&amp;a); // Copy slice a in b. let b_len = b.len(); b[b_len-2..].clone_from_slice(&amp;a[a.len()-2..]); // Partial copy let mut c = b.to_vec(); // Whole array duplication. let mut d: Vec&lt;_&gt; = a.iter().chain(b.iter()).collect(); // Concatenation. } In D the "$" of b[$-2..$] saves you from creating the b_len variable. To create "c" in Rust you have to use to_vec() because otherwise in Rust it isn't a "whole vector duplication", it's just a move, and you can't use "b" any more. The use of "$" in D is really handy, and I hope Rust will gain the same functionality (perhaps using the # symbol instead of $), I have discussed it elsewhere. The "a[] = 5;" is rather commonly used in D (and in similar ways in Python, where you need an iterable on the right). &gt; Only one is longer than D for no reason other than language complexity (which I think is a compelling enough reason to not include it) &gt; and one is longer because Rust doesn't want to hide the allocation from you That D code doesn't hide allocations, D programmers know that the use of "dup", "idup" and "~" on dynamic arrays always performs allocations (and if you don't spot that, there are two other ways to spot them in D, annotating the function with @nogc, and using the "-vgc" compiler switch). Rust is a very good language, perhaps it's even overall better than D, but when you compare it to other languages you need to be very careful, and you have to be very honest in recognizing where Rust is worse or about equal :-) If you write lot of of slice-heavy code (like the typical numeric/scientific code often written with NumPy), you quickly see how important compact&amp;handy array handling syntaxes are. I hope Rust will be good to write such kind of code too :-) Currently few pieces (like user defined "$") are missing. In the Rust version of the code the common use of "$-2" is not nice. The slice copying is worse. And the array concatenation is even worse, despite it's a quite common operation to do. In this code D was better than Rust.
Yeah, they parse unambiguously when you do `::` which is a huge downside. At that point might as well do `//` like `//std//u16//MAX` since we're typing two symbols now. it was only done that way because of C++, but it looks ugly to me because I don't do C++
Yes, that should work. multirust quick install should work as well. I don't have an RPi, but both methods work on my Odroid.
I actually added documentation right after I posted this. Good thought about the `length`/`data`/`crc` variables. I wanted to avoid temp variables but I guess having immutability is better. Thanks!
We call these 'associated functions'. Methods are also technically associated functions, but are a more specific name, so you tend to use it instead.
&gt; I would assume writing utf-8 to ascii is more troublesome, or? Yeah, so general unicode isn't representable as ASCII. But in the end, it's all bytes, (`u8`), so you would write out something as unicode as a sequence of bytes. 
Nah, that's a fair criticism. I haven't written a lot of blog posts with a large number of language keywords in them, so I never had a lot of `&lt;code&gt;` tag styling to deal with. I'll dig around in the CSS and see if I can come up with something that lets me call out `code` but be less jarring on the reader. Thanks for pointing that out.
You are not forgiven. **Take him to the rancor pit!**
I would just panic as if for division by zero. If it's meaningless, your program is incorrect by trying to compute it. One should check their inputs before calling such functions. AKA: let the user decide what to do. Here's my rule of thumb: If you can look at the only the input values to a function and know that it will fail, then it can freely panic. Errors are for the case when you can't tell this, like if you don't know if a file is locked or not before you try to open it. Also, it's almost always correct to panic on f64::NAN rather than allow your user to transparently propagate NANs through their calculations. It will eventually blow up anyway. (Unless you've got performance reasons to allow it.)
If it's a function where someone calling the function might reasonable be able to make better code by passing a variable to your function unchecked for if it's &lt;1, then None makes sense. For example, if the parameter is a variable which might just require some other calculation if it doesn't fit into this function, None might work. However, if the parameter is more like a constant "setting" of sorts - something that might be a compile-time constant in some places or maybe a user-defined setting, panicking or returning a Result::Err would probably be more reasonable. If you would expect the caller to already have checked the bounds, either a Result or panic would be good.
`None` forces the user to deal with results, but only after you've performed a completely unwanted and invalid calculation. They should sanitize their inputs ("assert preconditions") before doing calculations so as to avoid invalid states. NAN, on the other hand, really has no place in software. It's not a number, and really shouldn't be a value for a numeric type. It exists because it's implemented in hardware, and hardware cannot be in an invalid state, so it has to have a way or representing failed computations. Software doesn't, as it can just crash when a computation fails. And again, if a calculation returns NAN, you have to deal with it *after* you've already spent processor time computing a completely useless function. If you know your function is invalid for some input, assert that. Don't try to return a nonsense value and pretend your user is bright enough to not pass it through into another dozen functions. If they knew what they were doing, they wouldn't have passed you an invalid input in the first place. The only exceptions are if you've got some highly-streamlined optimizations going on, or if you're trying to export the IEEE 754 interface yourself so other people know what they're working with. (IEEE 754 probably doesn't cover your math function, so there's no need to return NAN from it.)
IMHO - NaN is a historical accident; it should be avoided in almost all cases. When you need it, you'll know it. Using options to represent possible failure is unidiomatic - you should prefer the Result type. Panicking is best kept for truly exceptional situations having to do with unrecoverable states at runtime - memory corruption, OOM, etc.
When will OpenBSD be supported then? :&gt;
What's the best way to deal with multiple function calls that return result within a function that doesn't. I want to catch if any of them fail immediately and then just return an empty Vector. Right now it looks like: pub fn list_ports() -&gt; Vec&lt;PortInfo&gt; { let mut vec = Vec::new(); if let Ok(context) = libudev::Context::new() { let mut enumerator = libudev::Enumerator::new(&amp;context).unwrap(); enumerator.match_subsystem("tty").unwrap(); for device in enumerator.scan_devices().unwrap() { if let Some(_) = device.parent() { if let Some(path) = device.syspath().to_str() { vec.push(PortInfo { port_name: String::from(path)}); } } } } vec } You can see I've tried to use the `if let` syntax to make things a little cleaner, but even that fails because you can't do the negation of it (I'd like a `if not ` or something). I've only seen one [article]( https://mortoray.com/2015/10/21/messy-error-handling-in-rust-with-try/) that complains about this, but I've seen other places deal with it by using a helper function so that the `try!()` macro can be used. But I feel like that is some substantial boilerplate. What I really want in this use case is an exception that I can catch! Anyone have any advice for this? If someone suggests using a helper function, I think it'd force me to write an RFC for Rust that can address this cause I see it as a big problem with the current error-handling within Rust.
It is already static, you just didn't know yet what to look for :) Static methods in Rust do not need a `static` keyword, because the explicit `self` argument allows rewriting all method calls into "static calls": `obj.method()` is nothing different from `ObjType::method(obj)` (possibly taking a reference to `obj` as needed). Ergo, since the `default()` method defined by the `Default` trait doesn't take a `self`, it can be called a static method. Since it's on the trait, it's on every type that implements the trait. 
Do you think that every time the user calls the function, they would like to decide at runtime what do to in case of error? If yes, you should return `Option` (or `Result`). The trouble with returning them is that making the programmer decide what to do every time makes your API unergonomic. Specially because the programmer might think his code is right and then call `.unwrap()` on it anyway. I think it's okay to panic on programming errors. That is, if the programmer did a nonsensical thing - like dividing by zero, or out-of-bounds array access - then the program should just panic, instead of propagate the error somewhere else. So returning `NaN` is just a mistake IMO. There could be another function to make a "checked" operation that return `Option` instead of panicking, just like [`checked_div`](https://doc.rust-lang.org/std/primitive.u32.html#method.checked_div) or [`get`](https://doc.rust-lang.org/core/slice/trait.SliceExt.html#tymethod.get). But if the failure mode here is a programming error, then the panicking operation should be the "default" one. Anyway, I will second the suggestion to disallow this programming error at compile time (using Rust's powerful type system, such as more specific trait bounds), instead of checking it at runtime. Even if it makes the API more complex, it's worth it IMO.
`f64::NaN` undermines the type system as the user can treat it as an ordinary float. This is comparable to `null` in languages like Java: followup calculations with `NaN` or `null` will result in garbage or crashes. Therefore it's probably better to use `Option`/`Result`.
https://github.com/rust-lang/rfcs/pull/1510 should fix this issue with dylibs.
We don't even tend to use the "static" terminology. This naming has always been a bit awkward, but the official term is "associated function", of which all functions defined in an `impl` block are, with or without `self`. But the ones with `self` are usually called 'methods', which is a more specific term.
MIT for my libraries, as mentioned in toml. And yes, Sciter is not opensource (we omiting holywars about it). Its license placed at [sdk](https://github.com/c-smile/sciter-sdk).
&gt; This is something I'm having troubles with -- the difference between str and String. If I'm not mistaken a String is allocated on the heap and owns the contents whereas a str is referencing some other immutable memory, correct? If that's correct then it definitely makes more sense to return a str. Yes, that's correct. &gt; So this got cleaned up a bit (however shit would still hit the fan if one of these operations failed but for my own purposes I'm assuming they're not going to). You still use both `unwrap()` and `ok()`. If you want to assume nothing fails, you should always use `unwrap()` to handle the cases where it *does* fail explicitly. Another thing I failed to mention before is that `read` can read less bytes than the buffer size. To catch this case as an error, you should use `read_exact`. &gt; I have some questions though: &gt; &gt; I made my Chunk's name field a Box&lt;[u8]&gt; so that I could easily just clean up that code without having to store the chunk in a temp var and I can make it immutable... I don't really like this though because the name is a fixed-size and Box&lt;[u8]&gt; implies a heap-allocated dynamic-length array correct? I'm not sure if it's better to use this approach or keep the fixed-size array on the struct, make it immutable, and just fill it in directly. Yes, `Box&lt;[u8]&gt;` is heap-allocated. Its length is determined when the box is created, but cannot be changed afterwards (otherwise you'd use `Vec`). But the type doesn't know the length, so it has to be on the heap. You could make the data a `Box` as well, this would save a pointer-sized "capacity" field in the struct. &gt; Does this do what I think it does? &gt; &gt; let length = match input.seek(SeekFrom::End(0)) { Ok(l) =&gt; l, _ =&gt; return chunks, }; I can only describe what it does :) It tries to seek to EOF, and if that works, sets length to the file position there (aka file size), otherwise returns an empty vector. BTW, you might want to calculate and name `(length - (size_of::&lt;u32&gt;() as u64 * 4))` before the loop for clarity. &gt; And finally, how does the compiler know that the Vec can be auto-derefed? Is that one of the traits? Yes, Deref (and DerefMut) are implemented for Vec, you can see it (among lots of other impls) [here](https://doc.rust-lang.org/nightly/std/vec/struct.Vec.html). This also enables you to call slice methods on vectors, which is most methods that don't mutate self (see the "Methods from Deref&lt;Target=[T]&gt;" section). 
Panic when it is reasonable for the user of the library to do the bounds checking themselves, `Option&lt;T&gt;` when it is unreasonable, `Result&lt;T, E&gt;` when there are multiple failure states or when the failure state could not be described as "there is no meaningful output for these inputs" (tip: almost all single failure cases _can_ be described as this).
&gt; What's the best way to deal with multiple function calls that return result within a function that doesn't. If I understand this correctly, you want to get rid of the `.unwrap()`s and make it return an empty `Vec`? That sounds very much like these are error conditions and you *DO* want to bubble those up. If you insist on doing it this way you'll need to `match` the potentially erroring results and `return Vec::new();` in the `Err` match arm. Yeah this is ugly and verbose and probably by design. Rust really wants you to do the Right Thing™ and if you want to do something else, sure that's fine, but you'll miss out on some ergonomics. You can probably also just add an `else` clause after the `if let ...`.
Here's an intuitive explanation for the pumping lemma for regular languages: Pick any regular language and construct its DFA (doesn't have to be the minimal one, any DFA will suffice). Now pick a string the language accepts whose length is greater than the number of nodes in your DFA. Because the length is greater than the number of nodes in the DFA, the path starting from the initial node to an accepting node will contain a cycle. You now have two options: * Make a new path by removing the cycle. * Make a new path by taking the cycle as many times as you want. You've now ended up finding as many new strings as you want that are accepted by the language.
Alright, thanks for the clarification! Interesting way of dealing with object/class methods :)
&gt; Systems programming is programming where you spend more time reading man pages than reading the internet. This is definitely me, except I have 10 open tabs on Opengroup and http://man7.org instead of reading manpages.
&gt; If it's meaningless, your program is incorrect by trying to compute it. That's somehow right. Sometimes early panicking is nice to catch errors quickly, but it might also mean a panicking application for the user, which looses his previous state. So there might be something in between, having asserts in the debug build and explicit error handling in the code. &gt; If you can look at the only the input values to a function and know that it will fail, then it can freely panic. The problem is, to be always aware of the valid input values. If applications get more complicated this gets a lot harder. If multiple persons are working on one code base, then not everyone is in the same way familiar with all parts. I really don't think that this kind of solution will scale very well, therefore I highly prefer the explicitness of error cases. 
The users may not be able to fix it but they might want to work around it.
I'm pretty sure one of the errors calls them static methods. Edit: This one https://doc.rust-lang.org/error-index.html#E0424
The only repo from my work on this that is public at the moment is RustType. One that is nearly ready for public release is my constraint solver library, based on the Cassowary constraint algorithm. The actual UI repos are not public yet. Everything will be MIT/Apache though. In terms of dependencies, I'm designing the core libraries to be as backend-agnostic as possible. Instead individual backend crates would provide the link to native UI kits. As a result I fully intend to support cases like native Win32 UIs, as well as hardware graphics ones using Glium or GFX. At the moment the first backend I'm working on is an OpenGL one using Glium.
Meh, I thought this was about the nix package manager. :( (That post was nice, though).
…but not if you have FreeBSD on your Pi. =(
I dunno. It's hard for me to relate to this problem, since I've known fork returns three values for over 30 years. But yeah, wrappers around syscalls to return `Result&lt;T&gt;` is probably progress.
&gt; I would like to warn you that any system where you "steal" multiple smaller blocks from larger blocks is a pain to write and debug (or at least, that was our experience using C). Buddy allocators can do this, and generally are very effective and fairly simple - they only do size doubling though, so for example you can't take three smaller blocks to make a larger block (You'd have to make one that's 4 smaller blocks inside. And the next possible size after that would be double, at 8 blocks). This simplification means they tend to be much faster then other types of allocators, since they can find large contiguous blocks with little to no searching (Allocation and Deallocation is `O(n)`, where `n` is the number of *block sizes*, not the amount of memory. This essentially makes both operations constant-time `O(1)` operations in implementation). Still, separate slab allocators can be more effective then a buddy system. The big advantage the buddy system has though, is that you can put all the memory into a single pool, so the memory can be used in small or big chunks as needed. A slab allocator requires multiple slabs for different sizes, each with a separate memory pool, so you have to decide how to distribute the memory to the pools. Slab allocators end-up extremely nice for managing memory that you can you can pull from another allocator though, because then you can just grow or shrink the pool as needed. So a combo of a buddy and slab allocators can work quite well, depending on the situation. Obviously, on low memory situations like yours the extra overhead isn't practical (And the issue of sizing the memory pools isn't as much of a big deal)
Note that these binaries are (a) unofficial and (b) require glibc-2.19+ installed on your system (OTH, the official binaries only need glibc-2.14+). If you are OK with those, then it's a good option. If you want to wait for official releases, beta rustc/cargo binaries should come out today, and the stable binaries should come out in 6 weeks.
Yeah, we are not generating rustup (AKA multirust-rs) binaries for *BSD just yet. But multirust.sh should work, yes? EDIT: Oh no wait, the triple would be something like arm-unknown-freebsd. That's currently an unsupported triple :-/.
We are generating the *BSD binaries via cross compilation from Linux. I think no one has tried to cross compile rustc from Linux to OpenBSD using rustbuild yet but according to [this comment](https://github.com/rust-lang/rust/pull/32942#issuecomment-209708777) it may be difficult due to the fact that OpenBSD uses older/custom GNU ~~binutils~~ toolchain but I don't know the details. Once we overcome that hurdle we should be golden :-).
can rustc/cargo be installed and work on coreos? A couple of months ago, I tried to install rust with rustup but failed because there is no "file" command on coreos. 
Yes, sometime stuff has slipped through. Rust has changed so much for so long that it took a while to determine what to call even things like `&amp;T`. Not perfect at it. We should fix those.
A lot of the Rust community uses Travis, and so you can find out some of this information by looking at `.travis.yml`. Good projects should document what Rust versions they support. In the case of a project like this, which hasn't had a commit in a long time, look at `main.rs` or `lib.rs`: if it has a line like this: https://github.com/mchesser/snake-rs/blob/master/src/main.rs#L1 it uses unstable features, and so I'd try a nightly build around the day of the last commit. If it doesn't have unstable features, well, it should almost certainly build with stable Rust today, and so I guess you wouldn't have that problem in the first place.
&gt; hopefully someone else can chime in! The turbofish (`::&lt;&gt;`) is only neccesary when you have some kind of chain of method calls, and you need to disambiguate in the middle. Otherwise, declaring the type of the binding works too.
Beautifully demonstrates the value of using ADTs over magic sentinel values. Great article!
I think it succinctly implies one of the great downsides of treating zero as false and non-zero as true. Languages like Rust *force* you to look at the man page since integers aren't implicitly converted to booleans in conditionals. It's a fairly common rookie mistake, one that I've most likely made, so if this prevents just one program from accidentally killing all the processes on my machine when I run out of memory, then I'm all for it.
The best way is in my opinion a clean `goto` / `break` construction. This allows the success path to flow straight and level down the code. Since Rust considers `goto` harmful, the next options are `let if-let (if) else` and the more pedestrian `let if return`. Like so: fn maybe_give() -&gt; Option&lt;u8&gt; { return None; } fn main() { let thingy = if let Some(x) = maybe_give() { x } else { return }; println!("Heyo: {}", thingy); } Note that the `else` branch must do one of three things: - Evaluate to a default value of the correct type. - `return` / tail-call. - `panic!` / similar. Otherwise it won't compile. Option 2, very pedestrian C-flavored: let x = maybe_give(); if x.is_none() { return; } let thingy = x.unwrap(); Both of these keep the flow flat, much more readable. Option 3 is the `try!` macro. It's okay, but you need to write a different macro or function wrapper in this case. Option 4 is to do cute things with iteration. for x in result Or // iterate over Some(iterator), ignore None for thing in maybe_iterator().flatmap(|i| i) { Only recommended if you and everyone you work with are fluent in functional patterns.
Username checks out.
Is it relevant? Benchmarksgame uses Rust source code, and it is not what is being optimized.
Yup! It's kind of a shame in a certain sense that it's not on the exact same day, but the train model is too good for other reasons :)
There's an extra space before AddAssign in the code sample, is that wanted?
So in 1.7 one of the most important things was that it contained a breaking change, and it was a test of how Rust handled that sort of thing. I didn't see even one person express a negative outcome as a result of that change, so I would say that Rust passed the test, and Rust's strategy for small inevitable breaking changes so far is successful!
Is there some way of saying a type can have the trait `Add` if and only if it has `AddAssign`? Can you automatically derive the implementation for `AddAssign` from an implementation of `Add`?
Do you know what's the state of rust on Freebsd 32 bit? [Last time I asked](https://www.reddit.com/r/rust/comments/4bo5xy/how_long_before_rust_will_support_freebsd_32_out/) it was very close to be supported. I can't wait for it :&lt;
Under "Library stabilizations" it says the release includes "various improvements to CString, used for FFI", and later on "see the detailed release notes for more". That links to https://github.com/rust-lang/rust/blob/stable/RELEASES.md#version-180-2016-04-14, but I can't see any references there to CString or FFI (in the 1.8 release). Anyone know what those improvements are? 
Ok, I just sent one.
Hey, /u/steveklabnik1, I wanted to give you a shout-out, by the way, for your awesome talk, "Rust In Production", on Tuesday at Philly ETE. It was exciting to see such a diverse group of great minds in the tech industry come to the dreary/boring Mid-Atlantic region in addition to the more fun/common locales of the Valley, Austin, D.C., and Boston. :) You had actually mentioned that Rust was about ready to release a new version during your talk, whilst discussing the recent integration of crater. I found this part of your talk/the Rust ecosystem insanely cool because, as far as I know, there isn't much similar in the C/C++ world (maybe Boost?). I suspect that the lack of this kind of tooling could be a cause why the language committees are so careful when proposing changes / features as it can be hard to predict the full scope of changes without actually having some data on diverse real-world code-bases and regression tests with which to prototype changes. I actually spent some time on Wednesday doing some more research on Rust's cross-compilation story and apparently it has been continuing to improve as there is this project on Github (https://github.com/japaric/xargo) that seems to meet some of my common platform needs. I'm hoping to get some more time this week to dive back into Rust again. 
I think the release notes and download page should mention rustup.sh, even if it is *beta*. It's just too good to not get noticed.
You need to run the program twice (once to get profiling information, once to actually use it). It's not "just like any other kind of optimization."
The real issue is that you'd need to run the application to determine the hot/non-hot paths. The issue is baking this feature in would involving shipping a mini-LLVM with each executable. So you'd dynamically JIT compile the LLVM-IR **AFTER** the application had started. So you'd be adding spin up time. The real goal should be to find a way to get JIT optimizations passively without spin up time. Maybe if some type of path hinting was integrated? But then you'd need to modify the LLVM's input to accept this path hinting, or does that already exist? I don't know.
Care to answer, why did you rename multirust to rustup?
/u/brson might be able to give you a better answer, but I'm pretty sure it's "it's a better name". I know a few people disagree with that though.
Yeah, it's not like I'd advocate doing this within the scope of the benchmarksgame; just as an additional piece to show that in certain conditions we can be even faster with relatively little effort.
Alright guys, that's it! First you change somewhat long chapter of your book just after I've read it and now you release a new compiler version just after I rebuilt the previous one with important tools from source! On a serious note, keep up the good work and thanks! ;)
Is it just me or `rustup` doesn't install newest nightly `rustc 1.10.0`?
All breaking changes are on a relative scale. There have been minor breaking changes in most Rust releases after Rust 1.0. Most aren't noticeable. See the [compatibility notes](https://github.com/rust-lang/rust/blob/stable/RELEASES.md#compatibility-notes) for Rust 1.8 for a whole set of minor things that have changed, by the way.
As long as it is clear that it was renamed in the later announcement
&gt; Longstanding issues with the performance of microkernels (which require many more context switches than monolithic kernels do) may also make Redox a hard sell for the more lucrative server segment Are there comparative benchmarks for this? I'm sure there's some mechanisms that help to compensate for this, maybe via fast userspace-only IPC or something like that. Shared memory, maybe? I imagine it's a lot safer in userspace, since a bug in accessing it won't risk corrupting the kernel state.
&gt; Well, the source code is already under the org I am not sure what you mean by this. https://sh.rustup.rs (what is said to curl on https://rustup.rs) has no obvious connection to rust-lang.org or github.com/rust-lang &gt; Once it is, it'll be linked off of r-l.o, for sure. ok, cool. I hope by this you mean that rust-lang.org says to curl from another URL hosted on rust-lang.org, as opposed to rust-lang.org saying to curl from sh.rustup.rs. I would trust the former version implicitly, yet the second would require a google if I saw these installation instructions from another source (say, someone giving me this link on IRC)
Hooray! Been waiting for `drop_in_place` to stabilize so I can stop leaking memory in [embed_lang](https://github.com/Marwes/embed_lang). I saw no mention of `pub extern crate` being implemented in the release notes. I changed my crate re-exports to to `pub extern crate` and I am no longer seeing any warnings and it using the re-exported crates works so I guess its in stable now? Or did it slip through by error somehow?
Ah, for that I would either return Option::None, or better an unimplemented panic (https://doc.rust-lang.org/std/macro.unimplemented!.html). You probably can find it without a closed form -- bisection on the CDF, or even samples if nothing else. If it truly didn't exist then f64::NAN seems the better choice, because it is undefined in the same limiting way that many other math results are modeled using NANs in IEEE arithmetic.. (Ignore the people saying NANs were a mistake or shouldn't have been created. It indicates they don't understand numerical computing.)
It's really not. It's just that the IEEE types are emphatically not the real numbers. How can they be, when you have a limited number of bits? It's more comparable to "coalescing" nulls, or the nil object in Objective-C or Smalltalk.
Indeed in C++, binary operator overloads are conventionally implemented in terms of compound assignment operators: class T { T&amp; operator+=(const T&amp; that) { // … return *this; } }; inline T operator+(T a, const T&amp; b) { return a += b; } 
Which night? :D For me it is "tomorrow" now (UTC+2).
Aren't most breaking changes justified as bug-fixes? Although they break compatibility, the things they break shouldn't have been possible in the first place.
Yes absolutely, they are justified. The claim that Rust 1.7 had the first intentional breaking change is not really correct. Maybe it had the biggest intentional breaking change yet, with real implications. There's also been some unintentional breaking changes that had real implications for very rarely occurring code.
I see you made use of `!variable.is_ok()` instead of `variable.is_err()` which is more idiomatic. The less you use `!` in code the better. You might also want to implement your own trait for `Result` and `Option` so that you can handle them better without having to use `unwrap()` or resort to a lot of matching, which is bad. Something like this: /// Extension for Option-like types pub trait OptionalExt { type Succ; /// Unwrap or abort program with an exit status of 1. fn try(self, message: &amp;[u8], stderr: &amp;mut io::Stderr) -&gt; Self::Succ; } impl&lt;T, U: Error&gt; OptionalExt for Result&lt;T, U&gt; { type Succ = T; fn try(self, message: &amp;[u8], stderr: &amp;mut io::Stderr, ) -&gt; T { self.unwrap_or_else(|e| { let mut stderr = stderr.lock(); let _ = stderr.write(b"program-name: "); let _ = stderr.write(message); let _ = stderr.write(e.description().as_bytes()); let _ = stderr.write(b"\n"); let _ = stderr.flush(); process::exit(1); }) } } impl&lt;T&gt; OptionalExt for Option&lt;T&gt; { type Succ = T; fn try(self, message: &amp;[u8], stderr: &amp;mut io::Stderr) -&gt; T { self.unwrap_or_else(|| { let mut stderr = stderr.lock(); let _ = stderr.write(b"program-name: "); let _ = stderr.write(message); let _ = stderr.write(b"\n"); let _ = stderr.flush(); process::exit(1); }) } } Then you can just use it like so: let value = function().try(b"this error occured", &amp;mut stderr);
I worked professionally developing low level stuff with hypervisors an (micro-)kernels. IMO performance is not such a big of a problem. Writing userspace is. If you're utilizing the full microkernel model of IPC for all communication, you're basically writing a fully distributed SOA system. It's super complicated. And things like profiling and debugging get extra complicated too. Also, if you have a buggy driver in userspace, what practical help is it for your that it's going to be restarted and hit the same bug soon? Also microkernel does not protect you from most of real problems with system stability. On typical system a misbehaving DMA device can easily crash your system, no matter if you're running ukernel, or fatkernel. That's why typical mission-cricial OSes don't really deal much with DMA. Hardware IOMMU/SMMUs help with that issue, but then again why do you need a ukernel?
If we weren't talking about math, I wouldn't say to panic, but mathematical calculations *are* invalid when you don't assert their domains. For example, if you're writing a division function and you forget to assert that your denominator is non-zero, then you haven't written a valid division function. Thus your source code is incorrect, not the user, and trying to recover from this is the wrong response. You should write a correct function, not ways to protect the rest of your program from all the invalid functions you end up writing. Because if you go down that road, then all functions should return `Result`, because you're not willing to make any guarantees that any of them are written correctly. Basically, your program isn't robust anyway if you allow such invalid functions, so there's no argument for robustness by propagating errors when you encounter one. Additionally, returning `Result` makes it hard to compose your functions, and that's something to avoid.
The problem is with lifetime parameters. `'b` must live as long, or longer, than the BTreeMap. I'm not sure exactly why this is, possibly a bug, but BTreeMap requires a key to search the map and then returns the result. For some reason the compiler thinks the key needs to outlive the result. I looked at the source and it doesn't seem like the function still needs the key, and merely uses it for a match: pub fn remove&lt;Q: ?Sized&gt;(&amp;mut self, key: &amp;Q) -&gt; Option&lt;V&gt; where K: Borrow&lt;Q&gt;, Q: Ord { match search::search_tree(self.root.as_mut(), key) { Found(handle) =&gt; { Some(OccupiedEntry { handle: handle, length: &amp;mut self.length, _marker: PhantomData, }.remove()) }, GoDown(_) =&gt; None } } Anyways, here's the corrected code: use std::collections::BTreeMap; use std::sync::Arc; use std::borrow::Cow; struct MyMap&lt;'a&gt;(BTreeMap&lt;Arc&lt;Cow&lt;'a, str&gt;&gt;, String&gt;); impl&lt;'a&gt; MyMap&lt;'a&gt; { fn remove&lt;'b: 'a&gt;(&amp;mut self, m: &amp;Cow&lt;'b, str&gt;) -&gt; Option&lt;String&gt; { self.0.remove(m) } } The important bit is `remove&lt;'b: 'a&gt;`, which means the lifetime of `'b` lasts as long, or longer, than the lifetime of `'a`
Hehe, I probably should!
Ah, gotcha. Cool, looking forward to it.
Nothing yet. That work is still ongoing.
Great, something I've been wanting for a while. &gt; There’s one issue with the process: you need two compilations and a profiling run to generate the final executable. Really, there are two problems (or more, I'm sure). PGO can be great for benchmarks, or in a situation where you can predict your data consistently. However, once you compile the program - that's it, you've optimized for that data. If the data changes you need to recompile. Granted, variable data will mess a JIT up as well, but I can see PGO potentially slowing programs down because it's based on what the developer feeds it, as opposed to a JIT which literally is optimizing itself on the production data on the fly.
Yeah, I'll have to add some recurring event to my calendar, like &gt; Warning! Incoming Rust release! Don't you dare mess with anything! ;)
In principle it makes sense that when passing a reference to an `&amp;mut` method the implementation could store the passed reference internally. Therefore the reference must outlive the struct lest that stored reference be left dangling.
The comments are pretty painful. People seem woefully uninformed. I dislike the URI concept (I can't imagine reading the RFC for URI and liking it) but a lot of the detractors are talking nonsense.
There aren't really docs, no. Part of it is also optimizing compilers; so like, the code it translates into then gets transformed by the optimizer, so it may be totally different than what you'd expect. Specifically, on the two things you asked about: `match` statements will differ based on how complex the thing you're matching on is. Using `@` and complex stuff? Might be slow. Just matching types? switch statement, basically, which can be a jump table. `unwrap` looks like this: match self { Some(val) =&gt; val, None =&gt; panic!("called `Option::unwrap()` on a `None` value"), } that's it.
I understand compilers are going to optimize things, but how does one get a good feel for performance in rust? I've seen a lot of claims about rust's speed, but I've not seen a lot of on the subject of writing speedy rust.
I think it should be `--print target-list` in Misc section, not `--print targets`. Also the link for "Errors for non-exhaustive `match` patterns.." in Misc section seems to point to a wrong pull request.
To understand better how Rust works you can also check the more advanced tutorial (rustonomicon), if you didn't yet: https://doc.rust-lang.org/nomicon/
&lt;&gt; - type parameters filled in at compile-time. () - value / reference parameters filled in at run-time. Now that I think about it, the idea of type objects in Python and Ruby is almost perfectly misleading. "Typeobject" and "typeclass" are terminology from functional programming languages. Sorry for the confusion. The relationship (behavior &lt;-&gt; category) is a typeclass. And the relationship (behavior &lt;-&gt; specific value) is a type object. 
Bit of a ~~no obligation^(urgh autocorrect)~~ noob here, excuse this if it's a stupid question-- Are there plans to slow the release cadence right down? It seems crazy fast at the moment, and unless you're actively developing all the time I don't know how you'd keep up. C's decade long release cycle might be a tad slow, but even then the language struggles with codebases and compilers not making use of the new specs. For a systems language, being stable and consistent seems hugely important to me, as code written a decade ago to what we're then best practices shouldn't, simply by virtue of things changing, be a danger to use now. 
you've ignored a large part of my opening post. I don't know how match unrolls underneath. steveklabnik1 has told me that in simple cases it's analogous to a switch statement, which partially answers my question. But there is nothing like match statements in C++. Nor do we have anything like rust enum's, and I have no idea how they're implemented. I specifically mentioned unwrap() calls. Do those incur a runtime overhead, and if so, how much? I'm not trying to be rude, I just find it somewhat annoying when I get a response that is nothing but generalities that don't actually answer my question. "It's like C++" doesn't help me understand how the very non-C++ like semantics get boiled down.
actually I haven't, I've skimmed parts of it but I got the impression it was more for FFI type stuff. I'll take a closer look, thanks.
That's what I was thinking too. Also, [hello again](http://i.imgur.com/KwtwDKm.gifv)
Rust is built on the concept of zero-cost abstractions (http://blog.rust-lang.org/2015/05/11/traits.html), which in many ways is quite similar to C++. A good example of this would be heap allocation. Unless you use a Box, Vec, or other heap-allocated structure, you know that no heap allocation will occur. So in terms of simple performance, I'd try to program it like C and C++. The Rust documentation also includes some nice info in this regard, like big-O performance for collections. For specific performance, it helps that Rust is more library code than compiler features. So you can just go and read the code, and work out what it's actually doing (The api documentation even has a [src] link to read the source code). Also keep in mind that Rust undergoes a lot of inlining during compilation (crates are compiled into AST-like structures, then those are combined). So while Rust uses a lot of composition, there is not much of a price for this in most cases. The static-dispatch nature of rust would also help with this.
Well I don't see how to answer your question any more specifically. How can we answer it not-generally? Steve explained how a match statement works (as well as unwrap), but it seems like you want to understand a lot more than that, but you haven't really said what. To give you a rundown of how every bit of rust code compiles would be pretty difficult, and would change over time as compilers change, implementations change, and the language changes. General rules seem far more useful, since you can then look up the implementation and use those rules to understand. And since you have C++ knowledge it seems much more relevant. I don't think your question is very clear, so perhaps if you're annoyed that people (or perhaps it's just me) are not answering it to your satisfaction you should specify what you're looking for. That said, if you really are not interested in general rules, but implementation details, ask away. I just didn't get that impression from the post. edit: It seems like you're looking for the C++ style talks or documentation on "how to write fast C++"? I don't think anything like that exists for rust. Perhaps one day we will have a Herb Sutter of our own to do great talks and posts about writing fast code.
Gotcha. And to be clear, I totally think you're pointing out something legitimate here: it can be tough to know in a language you don't know well. At this time, we just don't have a lot of this kind of stuff written down, so it's hard to do more than give general answers or respond to specific questions with answers.
Hi! I'm one of the organizers of Rust Belt Rust, and I'm so excited you're submitting!!! &lt;3 &lt;3 &lt;3 I'm going to be the person making sure the talk proposals are sufficiently non-identifying for our blinded first round of review, so I'm not going to be participating in that round-- but I have reviewed proposals for past conferences. The things I personally look for most are: - What will the audience get out of this talk? It's even fine to explicitly say "At the end of this talk, you will be able to...", but even if it's not that explicit, I should be able to fill in that blank after reading your proposal. - Is it concrete enough? I totally understand writing a proposal before you've totally thought through the talk (very valid thing to do!), and you don't have to give away everything you're going to talk about in your abstract, but one "For example, when you're trying to do X, you can use technique Y" goes SO far. I also agree with pretty much everything [this post of Schneems'](http://schneems.com/blogs/2016-04-07-conference-proposal/) says. Best of luck, looking forward to meeting you at the conference!!!
This entire post is one big strawman, I won't be responding to you further.
It would be useful to have some sort of literature on how things shake out underneath. As it is now, I don't feel confident that anything I write is performant whatsoever outside of the natural performance you get for compiling to native code. Atleast with C++ I can make informed design decisions based upon my general understanding of how the code will be transformed, with rust I can't do that and thought it was time that I start learning how. edit: for example, I cringe at how often I'm writing unwrap or try! or things similar to them. In C++ I don't cringe when I use the various smart pointers because I understand how the compiler optimizes a lot of it away.
thank you for the link, that's the sort of thing I was asking about. It also apparently links to an article that describes how closures are implemented, which will give me more information. If you have any other resources like that, I'd love to see them.
http://doc.rust-lang.org/reference.html there are a few other few documents on doc.rust-lang.org that may be useful.
where can I expect to find your talk once it's released?
I _think_ it's going to be on InfoQ. It'll get linked on this subreddit as well.
This sub is for the Rust programming language. You're looking for https://www.reddit.com/r/playrust Please remove this post. Thanks!
Well shit. My bad. 
&gt; Which isn't also to not counter-imply that we won't So much negation...
It does seem to be the case that `pub extern crate` is fixed, [treated the same as other items](https://github.com/rust-lang/rust/pull/31362). 
I suspect you're not: the `for&lt;&gt;` syntax isn't explained anywhere particularly prominent, nor is it often encountered, so it easy for someone writing Rust to not know about it. Of course, as the article's problems/your solution demonstrates, it is highly useful in certain situations.
If you are really curious, you can always write some simple example code and then look at the assembly generated by the compiler. That will always tell you exactly how things are optimized. You can even do it on [this site](https://play.rust-lang.org/) without downloading anything.
Making it clear that it's an official project is [one of the blocking issues for making it an official project](https://github.com/rust-lang-nursery/rustup.rs/issues/256)! :) Edit: At some point we're going to integrate it into the main website too. It's not clear what will happen to rustup.rs after that. 
From a production point of view, a program tends to run predictably: the same kind of data, the same kind of web traffic nearly all the time. The outliers may mess up with PGO, but they would mess up with the JIT as well. It's best to optimize for the most frequent case.
Right, they can be any type, but what I'm really asking is why they can't be used as a constraint if it would make sense. In other words, if a type alias refers to a trait, why can't it be used as a bound?
Very minimal? I'm using it with "RustyCode" (or similar) which adds support for autocomplete, formatting, jump to (and that cmd+hover/f12 mode), and trivial linting. Using racer, cargo, and rustfmt under the hood, if course.
&gt; http://www.cplusplus.com/reference/stl/ Although https://doc.rust-lang.org/std/ doesn't have universal complexity coverage, it should cover those cases that have unobvious performance. Anywhere this is missing is a documentation bug. In particular, https://doc.rust-lang.org/std/collections/ has good advice for collection choice. It's worth noting that (at least in theory; I haven't seen any actual timings) Rust's collections are more optimal than C++'s. Its hash map uses Robin Hood Hashing, not linked-list backed buckets and it has a BTree instead of a (probable) Red-Black tree, for instance. `Iterator` is lazily composed, so much more akin to C++ ranges than traditional C++ algorithms, and so is much more performant. &gt; http://stackoverflow.com/questions/22295665/how-much-is-the-overhead-of-smart-pointers-compared-to-normal-pointers-in-c `std::shared_ptr` competes with `Rc` and `Arc`; unlike C++ you get to choose whether you require atomic operations. Rust's should be negligibly faster. `std::unique_ptr` competes with `Box`. Both should be equivalent, though Rust's has no deleter function. Rust's main advantage here is that it frequently makes it easier to borrow data raw. This is important because complex lifetimes in C++ often fall back to `shared_ptr` as ownership with raw pointers is less safe. Rust lets you always choose the faster option. &gt; http://thbecker.net/articles/rvalue_references/section_01.html Rust doesn't need rvalue references because rvalue references are an ungodly hack. Rust just has what I like to think of as the `memcpy` operation (used with `=`), and move semantics are just the result of applying the borrow checker to it. http://stackoverflow.com/questions/31168589/forcing-a-move-for-an-implemented-copy-type/31171431#31171431 &gt; I would then recommend they read over the CPP Core Guidelines. &gt; &gt; https://github.com/isocpp/CppCoreGuidelines That's... an 80 thousand word document. That's a long book, despite being incomplete. What I've skimmed doesn't even seem to mention anything specific wrt. performance. The closest I found is this diagram: https://github.com/isocpp/CppCoreGuidelines/blob/master/param-passing-advanced.png The Rust equivalent is more like: ||`Copy`|`!Copy`|expensive to move :-:|:-:|:-:|:-: In &amp; Keep|`f(X)`|`f(&amp;X)`|`f(&amp;X)` In &amp; Move|`f(X)`|`f(X)`|`f(&amp;X)` In/Out|`f(X) -&gt; X`|`f(&amp;mut X)`|`f(&amp;mut X)` Out|`f() -&gt; X`|`f() -&gt; X`|`f(&amp;mut X)` although you'll find the type system pretty much guides you to this solution anyway. &gt; What I wouldn't do is tell that person they're asking for a "rust-centric answer", or spend a few hundred words explaining to the questioner why their question shouldn't be asked. You've heavily misunderstood what people are saying. You seem to be taking the commentary in the worst possible faith. If people say "I don't see how to answer your question any more specifically", they are asking for clarification, not trying to dismiss it. Calm down and stop acting like we're all out to get you. 
Assuming you compile with optimizations, the `unwrap` method will probably compile down to a single (highly predictable) branch on the happy path. When it `panic`s it is more expensive (like throwing an exception in C++) but since that is not the common case I wouldn't worry about it. Really, a lot of these things depend on how ambitious LLVM is feeling, the position of the stars, and the phase of the moon. I've seen a case where LLVM was able to compile a statement like match some_char { 'a' | 'x' | '%' | '*' /* many more cases here */ =&gt; do_something(), _ =&gt; do_something_else(), }; to a single branch by interpreting `some_char` as an integer and using it to index in a bitmap. It was quite brilliant, but it's really hard for a programmer to predict exactly how something will get compiled. I think others have said it before, but I'll repeat it: if something in Rust is significantly slower than equivalent code in C/C++, this is considered a bug. So I wouldn't spend too much time worrying about the low-level details, if you minimize allocations and use efficient algorithms you're usually fine. If something turns out much slower than expected, try profiling. If you're unsure about some code you've written, you can request a code review. Usually several people around in the Rust community will be prepared to take a look and offer feedback. If you decide to make use of this, it really helps to include a few test cases, they demonstrate how your code should be used and offer a reviewer an easy way to verify that the code still works as expected while tinkering around.
&gt; Thus your source code is incorrect, not the user, and trying to recover &gt; from this is the wrong response. My point isn't to recover all these cases, yes sometimes there's no point to continue, but especially the low level blocks of your application should IMHO be explicit about their input values and error cases. In higher level functions you can still call `unwrap` or `expect`, when there's no way to continue. So if you're writing the code you're still more aware about what could go wrong and can then decide for the particular case how you want to handle it, or if you just want to "crash" here. I'm really not arguing for having `Result` or `Option` everywhere, but to think where they might communicate something useful to the user, which might not be the case - if the user is even aware about it - if the function can panic. You're right that mathematical functions might be a special case, and unwrapping devision results would be quite annoying, but I also had quite a lot of fun to hunt the origin of NANs in applications. Yes, sometimes a panic would have been better, but if the NAN only results into a somehow wrong visualization - which certainly is bad by itself - but the application still runs and the user can finish his work, this also has its value. It's a hard problem, it depends on the kind of application which is the better behaviour and even inside of one application sometimes one might be better then the other. 
Next nightly should arrive in about 4 hours I think, maybe a bit less?
Another stepping stone to get there is merging now: [PR 32779](https://github.com/rust-lang/rust/pull/32779)
But Java do it if it has a warming step.
Clarification: since the 1.0 release last May, Rust has had few very serious breaking changes. :) In the years before then, not so much...
Not just that, the Rust developers often submit PRs themselves to correct for and preempt any breakage that they detect.
It's probably due to the fact that `type` declares a **type** alias.
I'm not sure that'd be a great idea, tests are by nature atypical uses of the library, to a lower extent that may also be the case for benchmarks. As such, they don't represent an &gt; expected, real world load which ought be used for PGO builds.
IIRC this means that redox no longer needs a fork? I don't recall all the reasons for why they needed to fork rust, but the set seemed pretty small and fixable.
it's weird that you can write fn new() -&gt; Self { MyType { x: 0 } } but not fn new() -&gt; Self { Self { x: 0 } } I think I want to create a Pull Request for that. Do you think this would require a RFC?
I use regression tests for PGO with some success in C++ so maybe libtest could separate between regression and unit tests somehow and use that for PGO builds. Otherwise a way to cherry pick with test/bench should be used for PGO could help.
https://github.com/gchp/rustbox
&gt; a more advanced Rust talk would be doing their audiences a disservice What is more advanced? Ownership or generics? One does not need to understand ownership to see value in Rust generics, and the same applies to error handling. Their learning curve is actually lower because people have seen generics and error handling before. IMO the disservice is leaving out these cool parts for the sake of explaining ownership. A question I get from C++ devs is what is so cool about Rust? They think it is just C with stricter pointer semantics and maybe a better switch statement. I always answer concepts! and concept maps! and concept based compile-time and run-time polymorphism! And error handling! And macros! And plugins! And iterators! And variants! And tuples! And pattern-matching! And destructuring! And Concurrency! Yes, it also comes "with a static analyzer on steroids" that catches things like iterator invalidation, but _that's dull_! Going beyond that line actually requires too much time and concentration from both parts to explain and understand it. I don't really think it belongs in a pitch beyond that. On the other hand, by explaining how Rust does the cool stuff they already know better than their language it is actually easier to get them interested. Once they are interested pointing them to Niko's talk and letting him explain ownership in 30 min is actually easy and people will go through with it because they see a lot of things that Rust does actually better already. "Perfect static analysis on top?" That's just icing on the cake.
Are you looking for /r/playrust?
You know how a photon can act as both a wave and a particle? Traits are a bit like that. They are not just interfaces, but also types that can be (indirectly) instantiated. Take `&amp;MyTrait`, for example. It's a reference to a `MyTrait` object, where `MyTrait` acts like a type. You can also `impl MyTrait { ... }`, which will implement functions for the `MyTrait` type. A type alias for a trait will therefore be an alias for the trait type, and not for the trait interface.
&gt; It seems that the Redox team is chasing after 1st-generation microkernel goals (viz. increased security and reliability by applying the MMU to a subset of kernel functions) with 1st-generation methods (writing things into ring 0, then moving them out using an ad-hoc interface) and recent tools designed to solve security and reliability at the level of implementation language. Why is Redox not designed to be in the L4 family (2nd-gen)? I don't know much about kernel internals, really. But, as someone pointed out in comments, the the so called [3rd-gen microkernel(s)](https://en.wikipedia.org/wiki/Microkernel#Third_generation), specifically [seL4](https://wiki.sel4.systems/FrequentlyAskedQuestions#What_does_seL4.27s_formal_verification_mean.3F), seems to be very (or should I say, the most) promising, and addresses the goal of soundness in a formal and systematic way.
&gt; Also microkernel does not protect you from most of real problems with system stability. Are all these issues (debugging of drivers etc.) fundamentally applicable to all kernels, or all ukernels? What I'm curious about is: Does [seL4](https://sel4.systems/) help solve any of these issues (apart from [the issue with DMA](https://wiki.sel4.systems/FrequentlyAskedQuestions#What_about_DMA.3F))?
I recall that building redox required a clone of their rust. icbw. This was a while back, too.
&gt;Reimplementing ZFS is a monumental undertaking, not something one could do in a couple of weeks The original implementation of ZFS was written by a 2 man team over the course of 12 weeks.
What's the proper way of doing it ? PS: I have no idea about Rust at all, just asking for the unaware audience.
&gt; What is more advanced? Ownership or generics? It's tricky: Lots of languages have generics, so most devs will be familiar with the concept, therefore you might think that it is less advanced. Though I like what I see of where Rust takes it. On the other hand, from what I can see, ownership is really fundamental to Rust, so you have to start there, therefore you might think that it is less advanced. But as you say, it's less cool.
How is there still no way to efficiently get multiple adjacent entries in a BTreeMap? 
I think you missed my point, I gave an overview of the big performance related items in C++. If someone "didn't know what they didn't know" this gives them a direction to go in to start learning more about C++ performance. You're doing the same thing the other guy did, thinking I want a primer on how to write C++ in Rust. What I want is to better understand the performance characteristics of the Rust code I write so it doesn't seem like "magic". &gt; You've heavily misunderstood what people are saying. You seem to be taking the commentary in the worst possible faith. I don't need a lecture about why my question isn't answerable, especially when several others appear to have understood what I was asking (the top voted comment, in fact). I could spend my time trying to defend myself, or not. I chose not. It isn't personal, and it isn't me being mean or acting in bad faith. Neither of us were going to come out of that conversation happy. Look at it like this. staticassert and myself have not gone off topic. You chose to comment on how you perceived my post... and we're off topic. I didn't make this thread to argue with people about whether or not my question is answerable, or whether or not I'm taking what a random internet denizen said in "good faith". With that in mind, I won't be responding to this line of thought anymore, I see no point. I appreciate the information you've given me, it'll give me something to think about. I asked the question because I wanted to learn and I wasn't sure what was out there to learn from.
Slices (`&amp;[i32]`) are the way to go, because `&amp;Vec&lt;i32&gt;` can be converted to it directly, and they also handle other data types, like `[i32; N]` or `&amp;'static [i43]`.
These are alternatives to GLFW and similar, they're not complete abstractions like bgfx is, or even OSG/Magnum. That's why i mentioned those.
I agree that the outliers would be a problem for both. What I mean is that in the case of JIT, the optimization happens on production data (because it's at the last second) whereas PGO you will have to run on old or non-production data first and then push to production where those optimizations have to hold So if your old production data is going to be similar to the new production data there's no problem, and I think generally that's going to be the case since real world data tends to follow patterns.
Also, https://github.com/Ticki/termion for a pure Rust implementation of TermBox. Disclaimer: Not my library and I haven't used it so I can't comment on its completeness/quality.
this conversation is over.
Proppy doesn't have a single line of rust (python/typescript). We actually talked yesterday how it would be nice to replace some part of the API by Rust once our backend is stable. Sorry if it was misleading! I'll definitely use Rust for the backend of my next pet project though
Well, just as `\` looks ugly to everyone who doesn't do PHP. Module paths are not file paths, and making them look like they are is not a good idea.
What makes this different from something like liquid-rust? 
Nice, thanks mate! I don't need rustup, pkg or ports are completely enough. I need it because I run CI server on 32 bit CPU with FreeBSD, otherwise I code on 64bits Linux.
That's actually far from the only problem with this video. I have accumulated a small list over watching it: * Basically telling people Rust warnings are more annoying than useful * Using `if(cond) {}` instead of `if cond {}` * Confusingly also `if(cond_a) &amp;&amp; (cond_b) {}` though * Repeatedly shows using `loop` instead of `for` to go through iterators * Actually never shows using `for` on iterators directly (slices/vectors through `into_iterator()` though) * Uses `Option::map_or()` instead of `Option::and_then()` * The usage is arguably strange either way, since it ignores the value of the `Some` case * Claims `Copy` semantics (can?) only apply to "primitive values", without a definition thereof * Uses `&amp;Vec&lt;T&gt;` instead of `&amp;[T]` * Concerning `pub` in `impl`s: "public means it can be used outside the `{}` that surround it" is just plain wrong * "There are no books on Rust, and the documentation is considered kind of weak", I pretty much lost my ability to even
&gt; He's not entirely wrong. Keep in mind it's more of a casual introductory video to just get started or familiarize with the constructs. Documentation could always be better. In Rust's case library docs in particular. So if that's what you mean by not entirely, fine. You can easily find 2 Rust books on Amazon, though. Then there is "The Book" as part of the upstream documentation, which IMO is far better than what most languages have to offer. Overall I find his statement devaluing hard work that people have done. So allow me to be by and large irritated by it.
thanks for the reply, I'll dig into this tonight when I have more time.
It's true that Rust has quite a few nice features, but there are still important features of C++ (and D) genericity that Rust lacks: values (especially integers) as template parameters, variadic templates, and template template parameters. The lack of the first hurts most IMO. That said, Rust is great 
actually, I went ahead and took a closer look at it, that's super helpful in understand what's going on. Do you have any other resources that go further into the bowels of the sum type implementations?
The lexer/parser panics on error so those will be shown in the terminal. The idea is to not have invalid templates at runtime, you still get potential errors in rendering (like a struct multiplied by 2) but those are at runtime, can't do anything about it. The `render` method does return a Result this time though
Yep. Would be awesome for lalrpop and other compile-to-rust things as well.
Depends on how big the ecosystem grows. Fundamentally, not hard to do, though it will get more expensive as time goes on. We could always limit it to the X most downloaded crates or something if that happens.
As basic introductions go it's okay, provided you don't assume the author is writing particularly good or idiomatic Rust. The bit at the beginning is kind of insulting to the people who've put so much work into making Rust's documentation as good as it is, though. Disabling warnings should raise an unsilenceable warning that you've disabled warnings. It's a shocking antipattern.
You could write a hyperscript library but that might be too verbose to actually use
Completely deserved!
Minus the source maps, doesn't [horrorshow](https://crates.io/crates/horrorshow) do exactly that?
There would be [`Wrapping&lt;u8&gt;`](https://doc.rust-lang.org/std/num/struct.Wrapping.html) if you want that. Please leave the rest of us using unsigned integers for things like buffer sizes, string lengths and counters happy by panicking on overflow (in debug, at least). I don't want to have to deal with the subtleties of floating-point arithmetic everytime I use a number. Edit: Also, calling `u8` `p8` would be incorrect, since the positive integers don't include 0. I believe what you mean is ‘non-negative’.
Heh, I even +1'd that!
You can use `std::num::Wrapping` if that is what you prefer: fn main() { use std::num::Wrapping; let a = Wrapping(1u8); let b = Wrapping(0) -a; println!("Heyo: {}", b.0); } I'm not sure why `Wrapping&lt;T&gt;` does not implement `Display` but I'm sure that can be fixed. Integer overflow is often a bug, and by using `Wrapping&lt;T&gt;` or any of the `.wrapping_*` methods you can signal that overflow is in fact intended.
As far as I know, computations that require wrapping operations for correctness are much rarer than bugs caused by unexpected underflows / overflows. Better type a few additional characters than spend time chasing down those bugs. And, as /u/CrystalGamma said, there's `Wrapping&lt;T&gt;`. But if you _really_ want to disable overflow checks, you can. Last time I checked, they were tied to the [`debug-assertions` profile setting in Cargo](http://doc.crates.io/manifest.html#the-profile-sections), so you can write this in your Cargo.toml: [profile.dev] debug-assertions = false ...and your integers should wrap around without panicking. In release mode, debug assertions are disabled by default so you don't need to do anything special.
Please consider the way that your language choices devalue the hard work that other people have done. Panicking on overflow for primitives in debug builds was a carefully considered design choice balancing the trade offs of a wide variety of use cases. Just because it makes your use case less convenient doesn't mean that was dumb or ignorant.
By the way, this even works in the playground: you can switch "mode" to "release" and the code example you gave will compile and run without errors.
Unfortunately there's not much documentation like that that's also current. But it seems (to me and I'm not an expert) that Rust is like C++ -- the instructions do not matter anywhere near as much as the cache misses, not until you're in really tight loops where LLVM is still likely better than you. Thus 100% of our job in writing something fast is [the right memory layout](https://www.youtube.com/watch?v=rX0ItVEVjHc). 
This is really cool. I wish it didn't abandon type safety though. It'd be really nice to have a template engine that knows the types that go into it, and ends up exposing a single function (which takes known arguments, not a k/v store) to the rest of the Rust code.
&gt; I'm not sure why Wrapping&lt;T&gt; does not implement Display but I'm sure that can be fixed. So, `Display` is for end-user output. It's implemented for primitive types, since there's only one way to really do it. But for more complex types, it's not so clear, so they're left unimplemented. This kind of type walks that line...
I presume that you would need to concoct a library that compiles on unstable and exposes a C interface, and link that library into your stable project. Intrinsics are one of those few things that will probably never be stabilized, due to the possibility of precluding any alternative compiler implementations that are not based on LLVM.
&gt; `match mem::transmute::&lt;Void&gt;(()) {}` That one is new to me. Got any more tips like this?
I expect `Wrapping(5)` to display as 5. Is there any other way to print it that makes sense? The `Wrapping &lt;T&gt;` wrapper affects only the semantics of arithmetic operations, but not the value, so I think the `Display` trait should be implemented and show only the value. 
&gt; `match mem::transmute::&lt;Void&gt;(()) {}` Is this any different from `*ptr::null()`, i.e. undefined behaviour?
I'm not sure, but it woudln't surprise me.
I agree with everything you've said, so I think we are on the same page. I can see an argument for your visualization example, but if you're providing the function as a part of a library, you won't know whether the user intends to use it that way or not, and the user can always wrap these functions in `if input_would_cause_panic {return Err()}` at the place where they do the visualization, where the semantics of "this shouldn't panic" are obvious, rather than in the library you're using, where they are not. Plus you force your user to actually know what would actually cause a panic, which can be a good thing or a bad thing. I consider it a good thing, personally. One shouldn't be using math functions without knowing what they are for. *Especially* statistics functions. :)
Yes, template template parameters roughly correspond to higher kinded types. They're most often brought up in relation to collection libraries, but have many other uses too, mostly for library writers. Integral values as template parameters, which I mentioned first, are widely used in scientific libraries to type fixed size (usually small) matrices. It was stated (correctly, IMO) in the thread "Are we numeric yet" that currently Rust is not that appealing as a language for scientific computing because it lacks certain features. Rust will never be Julia, but I think a few more features will close the gap with C++ there so that it will be acceptable. Take a look at a C++ scientific library (say, Eigen) and while you may not see HKTs but you'll definitely see integer template parameters. 
/r/playrust
[Any](http://doc.rust-lang.org/std/any/trait.Any.html) puts the downcast methods on the trait object which is necessary as the methods aren't object safe. I am not aware of any other traits which does this however.
https://github.com/brson/multirust This is a fantastic tool you can use to pin specific versions so that you can try the latest and greatest vicariously and not worry about not being able to go back with to the toolchain that worked for you last. This is fantastic for tracking nightlies and you rely on unstable features. Follow the instructions to install, ctrl+f "pinning" for how to pin versions and enjoy unlimited freedom from breakage and upgrade headaches forever.
Would be nice but not sure how that would work in practice :/ I'm open to ideas!
&gt; LLVM understands that it can't actually match on Void because it is an empty enum Rather, rustc understands this and emits an unreachable intrinsic (or whatever the current behaviour is). LLVM doesn't know anything about matching, per se.
There is a crate of LLVM intrinsics called [llvmint](https://crates.io/crates/llvmint), which simply wraps them like extern functions. However, it is unstable since rustc explicitly checks for `llvm.*` link names and feature gates them in consideration of the future existence of non-LLVM backends for Rust.
I have actually been working on something like this in the shadows: https://github.com/Ogeon/symbiosis It can actually generate code for any language (currently Rust and JavaScript), since it breaks the templates down into HTML aware tokens. Some parts are still quite rough, but it works reasonably well, and I have been thinking of making a first release soon.
I'm pretty sure the null dereference in your example is undefined. For a transmute from `()` to `Void`, I'm less certain.
I disagree with how much of the discussion focused on performance. It's an important factor, but it moved past the "why should numbers act a certain way" really quickly and left the pure mathematics behind. Use-cases were limited to what the compiler standard library does. No, really, mkauffman said: &gt; One assumption of the this RFC is that most integer arithmetic does not expect wrapping behaviour to behave correctly. This assumption was proven when doing a test implementation of this RFC in rust-lang/rust#20795 , only very few changes were necessary to adapt the codebase to the new behaviour. Wrapping is presumed always wrong, because bounds checking and memory safety are assumed to be the primary purpose of integer math. mahkoh called this out on 13 Jan 15, in the first unequivocal opposition. By that time consensus had set in, and non-helpful answers like "you can always use `wrapped_add`" were advanced with apparent sincerity. Here's Tiny Encryption Algorithm in C. The Feistel network, the two arithmetic-heavy statements, use eight wrapping additions in four lines. void encrypt (uint32_t* v, uint32_t* k) { uint32_t v0=v[0], v1=v[1], sum=0, i; /* set up */ uint32_t delta=0x9e3779b9; /* a key schedule constant */ uint32_t k0=k[0], k1=k[1], k2=k[2], k3=k[3]; /* cache key */ for (i=0; i &lt; 32; i++) { /* basic cycle start */ sum += delta; v0 += ((v1&lt;&lt;4) + k0) ^ (v1 + sum) ^ ((v1&gt;&gt;5) + k1); v1 += ((v0&lt;&lt;4) + k2) ^ (v0 + sum) ^ ((v0&gt;&gt;5) + k3); } /* end cycle */ v[0]=v0; v[1]=v1; } It's clean and compact, if only the `+` operator is defined as modular addition. In C that happens when you call it `unsigned`. If I understand the history, early Rust wasn't shy about defining new kinds of pointer, even breaking with C's `~` operator to free up a sigil. I don't seen the part where adding another set of primitive types (they don't even have to be in the prelude) was considered. `Wrapping` is not the same as a primitive - the explicit type coercion is a real eyesore, and you're still missing unary `-a`. This... is disappointing. Since nobody was seriously writing fixed-point non-integer arithmetic last year, or didn't speak up, or wasn't considered important, we're stuck (at least until the next RFC) with a language in which non-integer arithmetic is unnecessarily difficult. Honestly, it makes the language feel like a single-minded toy (memory safety over all) and tempts me to lob snarky off-the-wall observations like: --- It seems to me that the concern is an overflow could allow an array index (or unsafe pointer math) to wrap around and thus pass a bound check that would otherwise catch a logic error. **So, why not go with f64?** It has an integer-perfect range of +/- 9 quadrillion. That's more than large enough for memory addressing and what would be a catastrophic overflow of i64 would hardly scratch its absolute range. And if you overflow further you get an Inf. Low-overhead lazy checking would Just Work. (You still need to check memory bounds, but this guarantees that the comparison operators will see good values.) Plus, 32 bit PCs have hardware support, heck even the original 16-bit PC with optional 8087 can manipulate them with decent speed. If this sounds crazy, remember that lookup tables in the OpenGL shader language are indexed using f32s. --- The casual ignorance of modular arithmetic in all these discussions feels every bit as frustrating to me as that proposal sounds to you, or anyone with a bit of sanity. (But, seriously, pure integer math in floating point gives pure integer results. Try it yourself.) Scoped attributes was better. If I had free reign, I'd have four families of fixed-point primitives: checked signed, checked non-negative, modular, checked fractional. Bottom line: it felt more like an internet discussion than a math or computer-science discussion. All-in-all the process gets a B-; I've experienced a lot worse in person. The actual conclusions I disagree with, but I'm done fighting. I feel those decisions will either prove false in due course, or be a factor that hinders adoption and contributes to the language's slow slide into obscurity. History be the judge.
 It looks very promising. Do you have any roadmap?
I put an example here: [Link](https://play.rust-lang.org/?gist=52e2818898a9c5234d86f6194a5aba62&amp;version=stable&amp;backtrace=0) The method of iterating over the nodes (as_mut().map(|node|... etc) I copied from [Learn Rust with Linked Lists](http://cglab.ca/~abeinges/blah/too-many-lists/book/), which, again, I strongly recommend. This example prepopulates the list with some values, because it can't handle the case of inserting into an empty list. If you want to use this code for real, you'll have to fix that.
[removed]
I think /u/gnzlbg's point is interesting. Reading it first I interpreted it the same way as you did: "the basics are boring, let's have talks about the more advanced stuff!", but I think he actually had something else in mind: &gt; When I have to explain Rust to somebody I start with Traits and Error handling and how they are awesome, and what does Rust offer in parallelism and Concurrency. That is, when they explain rust *to beginners* they take a different approach. I think there might be more "rust intro to beginners space" to explore before settling on leading with ownership and memory safety. Traits are a very cool design pattern. I miss ADTs and exhaustive pattern matching and non-null types when I go back to ruby. The error handling is dreamy. Concurrency is a hot topic these days. None of these necessarily are "advanced" rust. That is, I think it's possible to give a beginner talk that focuses on some other features of the language than the "trifecta" that typically gets played up. I guess it depends on the audience. C/C++ programmers are intimately aware of ownership issues and memory safety, so a "beginner rust" talk to them might focus on the usual stuff. But if you're a higher level programmer watching a rust talk for the first time because maybe you want to start getting more bang for your buck on your hardware, you don't really know what you don't know, and so memory safety will probably not be such a selling point compared to some of the other cool things rust offers.
Moore's law, it'll be fine
There are many important issues that the rust core team and community could be working on, but there's only a finite amount of time that the finite amount of people doing the work have. This may be the most important issue for you, but every issue has to be weighed against all the other issues. Please be patient.
Just so you know, https://www.rustup.rs/ is in beta now, and eventually will replace multirust.
&gt; yours seems to be modular arithmetic My passion at the moment is wrestling with this language, both its good parts and bad. Happily the good parts are winning. I'm skeptical of the RFC process. To the extent it revolves around convincing, well formatted natural language, that would disenfranchise ideas that are better expressed with convincing, well-commented code. I probably do need to skim more, and find one I feel was a clear improvement. If I get to the point where I can show how ugly and unworkable `Wrap` is, maybe. *Is* it ugly and unworkable? I'm not proficient enough to say.
I want a template engine that works on both server and browser (Javascript), but doesn't depend on node.js. What are my options?
&gt; you still get potential errors in rendering (like a struct multiplied by 2) but those are at runtime, can't do anything about it Well one could be creative in employing trait bounds to disallow some errors like this at compile-time as well. It could make the types (and specially the type errors!) a bit more complicated, but seems worth the trouble. I mean, I prefer a hairy type error than a runtime error..
Don't forget [maud]! [maud]: https://github.com/lfairy/maud
This is cool! Why does it need nodejs? (Does it do server-side initial rendering by any chance?) edit: actually I think it's just for running npm
No, not yet. There is an equivalent way to do this on Windows, so there aren't cross platform issues either. It's basically just that nobody has implemented this yet. Since `File` does expose ways to get the raw fd or handle, you can easily write your own function to do this in a cross platform manner, or even make a library on crates.io for this, which would be faster than waiting for an implementation of this to go into std and ride the trains to stable.
I was thinking about that and my conclusion is that we should provide standard way to allow compiler extensions in stable if we want other compilers.
You might be interested in [maud] and [horrorshow], which both do compile-time checking. [maud]: https://github.com/lfairy/maud [horrorshow]: https://stebalien.github.io/horrorshow-rs/horrorshow/
I tend to find that ownership is "more boring" to C++ programmers because they know the C++ smart pointer types and have experience with some static analysis tools that are "similar in purpose". My one minute description of the borrow checker for C++ programmers is the iterator invalidation example for a vector and I stop there. Everybody gets a "wow" moment because they know that the static analysis required to catch this in C++ is not there and probably won't be there any time soon. I don't explain them ownership and sharing, I just tell them Rust catches this error at compile time with 100% reliability and not only in vector but also in more complex data-structures like trees/graphs... That's enough to make them see value in ownership and want to learn it but for that there is already a lot of good material available. I'd rather move to other parts of Rust to make them want to learn _all of Rust_ and not only ownership.
So this could in theory be worked around by renaming the (extern) functions?
The only thing /u/annodomini didn't mention about enums which I think you will be interested in is the [null pointer optimisation](http://is.gd/YrV1pK). It's most useful when working with `Option`s but it does work with your own types as well as long as they follow the same format, the compiler can be [quite clever about it](http://is.gd/Qr9Zzg) as well.
I'd guess that an RFC to add modular arithmetic primitives won't be accepted, because "it's not common enough". But that doesn't stop you from writing your own crate which defines `M64, M32, ...` which overloads [these binary and unary operators](https://doc.rust-lang.org/std/ops/index.html#traits) so that it's painless for you. I just did a quick search on crates.io and didn't find any crate which does that.
You couldn't actually do that without going through clang-compiled C code.
For 2D, SDL2. For 3D, there is Glium, wrapper for OpenGL
How one doesn't notice that a subreddit is completely different from where they want to post in.
But as OP said, it doesn't have inverse methods. I.e. `Neg` is not defined for `Wrapping&lt;T&gt;`.
Any particular intrinsic you need?
&gt; For example, here's how I'd write the following: `bg_color` is a `Color` not a `String`. These are actually [`ToString::to_string`](http://doc.rust-lang.org/std/string/trait.ToString.html#tymethod.to_string) calls. &gt; You can convert this into the following: Depending on `f` it might be better to keep the intermediate `String` buffer and do the base writes in that (this needs benchmarking) with just a final write in the underlying `f`.
I think the nix library has a nix::sys::uio::pread function. But this will only work on a *nix os
I also think habdlebars-rs should be mentioned here. I haven't seen it in any of the comments
You'd still have to write a wrapper function, `rustc` will simply not let you import a symbol with a name starting with `llvm.` into a Rust crate, in any way that LLVM could interpret as an intrinsic (e.g. you can try to call it from inline assembly but then you'd get a linker error or some internal LLVM error).
This is cool! I'm not a babel expert but I thought that stage 0 features are totally experimental and should not be used in production. Is that correct? 
Does that mean that all of the output HTML is correct by construction?
&gt; But I don't get how to run use your library. What is the library supposed to do, should it be a standalone executable? Reading up on lemonbar, it takes bar description info as stdin format commands. This library would be the commands formatting part, so you'd use it to build a binary generating commands sequences to stdout, and pipe that into the `lemonbar` utility, rather than use a bash script to generate commands. https://wiki.archlinux.org/index.php/Lemonbar#Examples makes it clearer than lemonbar's own readme (which had me completely lost).
I think an even better handling of that part would be to apply the same pattern you did afterwards: `write!` the info directly (where and when needed) instead of allocating, so let background = match self.bg_color { Some(ref color) =&gt; Cow::Owned(color.to_string()), None =&gt; Cow::Borrowed("-") }; would be match self.bg_color { Some(ref color) =&gt; try!(write!(f, "%{{F{}}}", color)), None =&gt; try!(write!(f, "%{{-}}")), }; (with some reordering to put the content formatting last)
I don't think that something that does not come built in with scene handling, preset cameras and similar features could be considered a high-level graphics library. Take a look at OSG.
This is impossible without a js engine. Reacts "templates" are really just JavaScript.
Agreed. Though with inlining it should not make any difference after codegen.
Are you going to be stealing my 'multirust run stable cargo test' command?
It could be made a lot smarter, like caching identical crate compiles (from dependencies), so you can probably reduce the needed work to less than 50% of what it is now.
i believe stage 0 features might go away or change at any point, so using them in production could be a bit adventurous. by all means stick to established es7 features if you need more stability.
 trait OptInNeg { fn neg... } impl&lt;T&gt; Neg for T where T: OptInNeg { fn neg... } impl&lt;T&gt; OptInNeg for Wrapping&lt;T&gt; { fn neg... } Problem solved?
I tried to implement `-a` for `Wrapping&lt;T&gt;` and it pretty quickly fell apart. I'm not good enough at generics to know if it's possible or estimate how much I'd have to learn, so I gave up. At the machine-language level, the difference between signed and unsigned numbers is a few (not all) instructions. A reasonably complete bit-twiddling type would need access to signed operations too. Extending a sign bit, shift right with extend, and signed comparison are expected primitives. Finally it also needs to do byte-reordering and transmutation to *all* primitive numeric types, including IEEE floats.
if it fails you see the values in the panic massage
This comment is great, you should consider sending it as a PR to the book or at least making a blog post!!!
Where's #1? :) This is the wrong place! Go to /r/playrust
They very much are graphics libraries, it's just that you are looking for something much more high-level.
Maybe, but it's not.
Ah, right. That was something I had meant to mention, but missed. Thanks for pointing it out.
`.expect("Everything will be fine.")`
It's not, unfortunately. gfx-rs receives both HLSL and GLSL and decides which to use depending on the backend it runs. [This issue](https://github.com/gfx-rs/gfx/issues/71) talks about the possible selections. (I think that it can even receive different GLSL versions, and select between them at runtime, depending on the OpenGL version of the machine it's running. At least Glium does this) You can abstract the shader yourself: there are HLSL-to-GLSL compilers on Github [such as this](https://github.com/James-Jones/HLSLCrossCompiler), [this](https://github.com/aras-p/hlsl2glslfork). You can write your shaders in hlsl, run one such compiler at compile time, and provide both HLSL and GLSL to Gfx. Ultimately I think that, since SPIR-V is available and is supposed to become the industry standard, we should compile Rust to SPIR-V and write the shaders in Rust (with a model kind of the one described [here](http://www.g-truc.net/post-0714.html)). I asked about such a thing [here](https://www.reddit.com/r/rust/comments/42efnk/how_feasible_would_be_to_provide_a_spirv_target/) and /u/kainino0x talked about trying to do this.
It works perfectly with: `fn createHandler&lt;'a&gt;(&amp;'a self) -&gt; Box&lt;Handler&lt;T&gt; + 'a&gt; {`. It just needed some help explaining the relationship of the resulting trait object to the source object.
As /u/masklinn points out, lemonbar is a status bar for X that takes a formatted string from `STDIN`. Currently, this library is for the generation of those strings. Most people use Bash for this purpose, a few use other scripting languages. I wanted to see if I could write a Rust library to do it because I have some more complex blocks that I want to write and didn't want the performance hit associated with Ruby (my usual language).
The test is in a sub module, so it will automatically have access to everything in its parent module.
The HTML is syntax checked while it's parsed, so it will be able to catch some mistakes, but not all. The final output isn't validated, though. The template may be a stub, so it's hard to know how much to check. The parameters are at least type safe, so there can't suddenly be a number where you expected a list.
It's not really my project, but I'd like to at least get a cursor so you can select which elements you are modifying. Right now all keyboard presses are global to the entire application, which isn't very useful, and more (any at all) documentation is always nice.
On reflection, it makes a lot of sense. I suppose that just didn't occur to me.
That's useful for PGO-ing rustc, not for PGOing arbitrary rust software which I understood was the subject here.
Fail at spelling too desaster -&gt; disaster lols
Too true. Edit: thinking a bit more about it, I made the mistake of thinking I could teach well because I usually can explain well. Boy was I wrong. ;-)
Thanks, gonna fix soon (on mobile now).
&gt; lols Not funny, just German.
Why were you voted down?
Yeah, that's my general plan, too. :-)
Nope, putting a ; after each statement causes a syntax error, and putting a ; in the end of the match statement doesn't change anything.
Sorry, see my edited comment.
I've found doing dry runs with people before hand useful. Kind of hard if you don't have many people to try it out on though, or don't have the time. I guess it must get easier to prepare the more you practice.
But storing the structs directly would require to call clone() right? Because I had a [version of the code](https://github.com/Darthkpo/utudb/tree/414aa27152bff8cc260d240fda7648748eb09f2a/src) that did that and it worked fine, but I tried using references to save memory and improve performance.
`-n` is that number such that `n + -n == 0`. This is well defined for wrapping numbers. I think wrapping numbers should have negation. It just seems needlessly difficult not to. `Wrapping&lt;T&gt;` is already saying "I know what I'm doing with overflow", so I don't even see a safety aspect.
And then live-coding is waaay harder than just explaining/presenting/teaching. It looks real easy when you see people used to it presenting so you think you've found a great idea, but as soon as you hit a snag things just go completely off the rails unless you have a ridiculous amount of experience, because you're now debugging code (which is stressful enough) in front of a live audience, and while you're acutely conscious you're now wasting the audience's time and losing their attention, that sucks out your debugging focus and makes it even easier than usual to miss the obvious. I made that mistake once, I'm happy I learned that lesson but boy is that one of those recurring "brain decides to reminds myself of all my shameful moments around 3AM" sessions. 
It doesn't actually :) I used a similar pattern somewhere but the traits I implemented things for were local so it went much better. Here the coherence rules disagree firmly with my attempt at blanket implementation ([E0210](https://doc.rust-lang.org/error-index.html#E0210)).
&gt; _**eddyb** commented on 23 Dec 2015_ &gt; &gt; Is there any reason why `Wrapping&lt;T&gt;` doesn't implement `Neg`? # &gt; _**aturon** commented on 31 Dec 2015_ &gt; &gt; **@eddyb** Not that I know of. Probably just an oversight. https://github.com/rust-lang/rust/issues/27755#issuecomment-166981405 ...and then it got forgotten again. I wrote up a new issue. Hopefully this is uncontroversial. https://github.com/rust-lang/rust/issues/33037
First, a disclaimer: optimising this may well not be worth it: when dealing with allocation in Rust/C++ it's very often the case that you have to trade off number of allocations with complexity of lifetime relationships. In Rust you fortunately have just borrow checker errors to deal with if you pick the latter and get it wrong, while in C++ you tend to get million dollar vulnerabilities. But, let's say we do want to avoid allocations here. I think your best bet here is to go with `Vec` indices. Since your entire universe is in three `Vec`-s (it often turns out that way with parsing problems), this should work out quite well: pub struct Hora { pub id: u32, pub comienzo: u32, pub duracion: u32, pub materia_index: usize, } pub struct Dia { pub enum_id: Dias, pub nombre: String, pub hora_indices: Vec&lt;usize&gt;, } pub struct Turno { pub id: u32, pub nombre: String, pub dias_indices: [usize; 7], } By using indices as proxies for references, you make the lifetimes/ownership semantics simple and clear: the three `Vec`-s own the data, and nobody has references to anyone. Note though that you have introduced some complexity and made it somewhat harder to understand---that's always the tradeoff. The alternative I mentioned, which you may be happier with, is to use `Rc&lt;Dia&gt;`, `Rc&lt;Materia&gt;` etc. In that case, you'll be using reference-counted pointers (a simplistic form of GC, esentially) and you can call `.clone()` without actually copying any data. The downside is that if you need mutability, it will become pretty cumbersome (e.g. `Rc&lt;RefCell&lt;Dia&gt;&gt;`, which is often a code smell in Rust).
No problem! Really happy I could help!
Same in Chrome on Linux. In the GNOME Terminal, they are rendered correctly and can be individually selected.
What's the best way to extend a struct in Rust? It seems that it isn't directly possible without specialization? Anyway, are there any common patterns one can use instead of doing that?
Also, in the last paragraph, "fist" -&gt; "first", but don't worry about it - I thought it was a joke because you'd mistyped the filename... :P
&gt; And as everyone knows, Germans don't know funny ;-P Exactly!
I was hoping for this exact feature just yesterday when I wanted to try out [sanakirja](https://crates.io/crates/sanakirja)... This is great!
I made enough mistakes to no longer worry about them. Still, fixed. And thank you.
Specialization is about behavior, a struct is about data. So do you mean extending behavior or data?
Ah, right. I mean data.
cool, I was able to download the source for alsa-sys which is actually embedded in another project on github, making it hard to find. 
This is exactly what I needed! Unfortunately it's slightly broken: https://play.rust-lang.org/?gist=425adc686ca375ec44cfcaf8b0adbd79&amp;version=nightly&amp;backtrace=0
I'm not at all a fan of his videos. Mostly because he rushes through everything and while he's good at teaching syntax, he's not good at teaching semantics, idioms, or really what certain features are useful for. Personally, I'm a fan of tutorials that have a project in mind and teach portions of the language necessary to complete the project.
Thanks! Maybe I'll start from there :)
I believe you have the wrong subreddit, friend. you're looking for /r/playrust 
I wonder how one can't notice that this isn't the correct subreddit :P
The correction was given in way that seems unnecessarily aggressive/rude: it is doesn't take much effort to phrase it more nicely (probably less) and this sub wants to encourage that.
What information are you referring to? Also what's changed?
Whats a good pattern for storing external configuration, like api keys, and other sertings that you may not want to check into source? 
Ok. It's not.
What kind of GUI are you looking for? Something with a "native look and feel"? Or something for a game? Or something else?
what are 1-3 the most popular GUI libraries or frameworks in Rust nowadays ready for production 
like GTK, wxWidgets. There're a few libraries but what are the ones ready for production and most popular?
The `unreachable` intrinsic tells LLVM's optimizer to assume that that code path is literally unreachable, opening up more opportunities for optimization. The `unreachable!()` macro just panics when that code path is hit -- it has no effect on the optimization level of code where you use it.
You said to update X information about the most popular GUIs, there's nothing to update, making it confusing. Then claimed something was changing rapidly making even more confusing. In my personal opinion for not good reasons, i would say one of the best GUI toolkit in Rust is [gtk-rs](http://gtk-rs.org/). But again stuff like this is very relative, there is not usually an obvious best choice.
nothing like that exists. It has never existed. It still does not exist. The work in progress and pretty decent gtk-rs and piston are still your best bets; they are not production ready and not strongly cross platform. If you want to use rust and write a serious cross platform UI application I recommend you write reusable libraries in rust and call them from [QT](https://www.qt.io/developers/) (ie. write the actual UI in C++).
There isn't really that much ready for production right now, but if you could specify what exactly you're looking for, maybe we could actually help. "GUI Library" is a super broad term, and without you giving more information there isn't an answer to your question. If you really do want a generic "top 3" of graphics libraries, I'd say: pistoncore-2d_graphics, gtk-rs and conrod. I wouldn't say any of these are stable or production ready, but they are probably the top 3 most used.
It wasn't bad at all. Very interesting in fact. Next time, just call it live coding. Also, don't show weakness by apologizing to the attendants. I might ask for a refund! :)
In rust, there aren't any ready for production yet. It's getting there, but there is still a lot of refining and designing before rust has a stable GUI ecosystem. gtk-rs is probably the closest to what you're looking for, but it is, as with most graphics libraries, not yet stable. There's also conrod, a rust-native GUI, but I really don't have the experience with it to say what its state is.
Yeah, sorry, my domain expired. Here http://159.203.118.196/blog/?id=iron_on_uwsgi
I want to like Rust, but I'm really having trouble getting started. The syntax feels ugly and overly concise, and its semicolons and generics/templates remind me of C++. I dislike that I can't easily do something like getting a slice of a string. I'm scared to do anything beyond basic number manipulation and I'm usually right. Go was designed to make every feature fun to use, but I'm not getting that out of Rust. I don't have much motivation to continue. But there is one thing that can convince me. Has mastering and internalizing the borrow checker helped you make the code you write in other languages faster or safer?
Yeah, my typing is slow and I forget things, so I've found having lots of files at various stages helps - sometimes with things I can uncomment along the way, to show compiler errors, etc.
I'd love to answer this in the affirmative, but the truth at least for me is no, I'm not good enough to internalize the borrow checker. The compiler is clearly better than me in upholding rules across any nontrivial codebase. In fact, I think this is nigh impossible without mostly memorizing the whole codebase, because this is what the borrow checker works with. Of course, it needs lifetime annotations to help, but even with those in my non-Rust code, it'd be a very taxing, yet boring task. Note that this is different to internalizing the borrowing rules, which should be standard issue for every C++ coder. Luckily for me, the syntax grew on me, I think string handling in Rust is better than in, say, Java (although Java 9 may change that) and while I don't make a living writing Rust code, I can code Rust in my spare time.
The original author answered it in the first comment on https://blogs.msdn.microsoft.com/vcblog/2016/03/31/announcing-the-official-release-of-the-visual-c-build-tools-2015/ To save clicks: &gt; the phrase “go-live license” is about support. When the Build Tools were in preview they had no support associated with them. If you found a bug, you were on your own. Now that they are go-live, we’ll support the Build Tools like we support any other VS product. &gt; &gt; As for licensing, my understanding is that you do NOT have to buy an additional license of VS2015 for your CI server. Our goal with the Build Tools is that if you have a license for VS already (that is, you bought Enterprise or Professional for your company, or you are using Community for yourself or your small organization) that you can use the Build Tools on your build server or continuous integration server. And my understanding of the Build Tools SKU license (https://www.visualstudio.com/en-us/support/legal/mt644918) seems to support that (compare to the Enterprise/Professional license here: https://www.visualstudio.com/en-us/support/legal/mt171576, see the section on “Use Rights”.) But I’m not a lawyer, and I’m not giving legal advice here, so check with your own legal team if you have concerns.
Why is an AST a terrible way to represent a program?
Thanks for the comment. I agree that it would be nice to move operator tokenization to a separate function; however doing a `HashMap` lookup for _every single operator or delimiter_ would probably be way more expensive than a simple match-based lookup table, so I'm not keen on using a `HashMap` for that purpose. One of the main selling points of Go is that it compiles quickly, and I would like to preserve that in `rgo`.
I've been having a lot of fun with the new support for completely static Rust binaries! We've built a couple of small Rust-related tools at Faraday, including [credentials-to-env](https://github.com/faradayio/credentials_to_env), which allows us to write Docker containers that can fetch various secrets from either environment variables (Heroku style) or from Hashicorp's Vault (which provides security policies, audit logs, and auto-expiring credentials). But these tools are much easier to deploy if they don't have any external dependencies, especially if we want to build tiny Docker containers using [Alpine Linux](https://hub.docker.com/_/alpine/). So I built a Docker container which installs a musl-gcc toolchain and the appropriate Rust target, and which also includes a static version of OpenSSL so that popular Rust crates like hyper work right out of the box. Building should be pretty easy, especially if your account has uid 1000: alias rust-musl-builder='docker run --rm -it -v "$(pwd)":/home/rust/src ekidd/rust-musl-builder' rust-musl-builder cargo build --release I'd be happy to add static versions of other popular C libraries, so that the most popular Rust packages work correctly without further tweaking.
That was a proposal that hasn't been implemented. With data, you use composition.
I for one really liked the workshop! 👌 Doing pair programming with a total of three people is a rather though nut to begin with. Nothing you had any influence on though. There'd been three additional colleagues of mine who were interested, but given the date (Friday evening) and the short notice (of mine) had other stuff planned already. So maybe we should reconsider the choice of day in order to strengthen the overall attendance of the group? Having written a small compiler plugin myself as well as a tiny rustc PR, I was already familiar with its overall internal API. But without any prior contact I reckon it'd have been rather difficult to write a lint, even in pairs. 90% of time would probably have been "wasted" searching the api docs for (mostly wrong) functions/types, etc. That's why I suggested to do the first lint by observing your steps over the projector. So your point two ("The second was entirely my fault. […] Instead I let myself talk into live-coding the lint myself") is entirely on me. 😇 (I still believe it was not that bad of a choice, given the low and odd number of attendance.) For real pair programming you'd have had to be part of one of the two pairs. so every time you'd "be running around and give advice where needed" you'd have had to abandon your pairing partner. Had we been an even number (plus you), things would have been entirely different, no doubt. Some weeks ago I was hosting a Swift Dojo at the company with roughly 15 people of which about 10 were new to the language. I had prepared a pure functional impl of command-line Snake with katas of increasing difficulty (ranging from "write a struct for a point { x, y }", up to "write an ascii rasterizer/renderer for the game using nested maps"). For every kata I had prepared the usual unit tests. I then also listed all required doc urls in comments above, as well as hints on possible approaches to use (again with links to docs). And last but not least I provided an additional/optional empty skeleton of the given function/type, as well as a second full implementation of it. Both "hidden" through rot13. This way one could quickly get help/hints once stuck, without sitting there waiting for the instructor to come around and also not spend 90% of the time trying to get familiar with the structure and depths of the documentation. Getting to know the documentation is crucial for learning a language, but a bit much for a dojo of just two hours max with people mostly new to the topic at hand. A similar approach might be a good fit for a future lint workshop, maybe? Last but not least lemme repeat this: "I for one really liked the workshop! 👌" (also I finally made an account for writing this, if that means anything. 😉)
Hey, guys! I am new to rust and I have a question now, I don't know how to solve it. Suppose we have let veca : Vec&lt;i32&gt; = vec![1, 2, 3, 4]; let vecb : Vec&lt;i32&gt; = vec![-3, 5, 7, 8]; I'd like to write a line of code to create a new vector which is the result of these two vectors, like this: let vecc = veca.mapi(|a, i| a+vecb[i]).collect::&lt;Vec&lt;i32&gt;&gt;(); where i is the index of a. I'm looking for a function can do things like "mapi", the method map don't work here. Is there such a method? 
I see. I should have read that more carefully, sorry. I was about to type up a long response questioning how composition works in a few cases. While doing so, I realised I was being silly and still thinking about it from an inheritance perspective. Looks like it will take me a while to get my head around it, but hopefully it'll become second nature soon. Do you know if there any bits of documentation or blog posts around design patterns in rust? I feel like it might be nice to have something showing a few obvious OO patterns and how you'd go about implementing the same behaviour in rust. Perhaps it could be a new section of the Effective Rust book? Thanks for the help llogiq.
* Your parser seem to panic when it encounters something unexpected which is not very friendly. Instead return a Result to indicate parsing failure, or if you want to be a bit fancier store you can store all errors and pretend it parsed fine so that multiple parsing errors can be found. Note that the second method can be tricky to get right since you need to be careful not to get a cascade of bogus errors. * I would consider using an iterator or some method like an iterator for getting tokens into the parser instead of pushing all tokens into a `Vec` before even starting to parse the file. I am actually not sure which is faster but by reading in tokens only when they are needed you can at least save some memory. * As /u/pjmlp says you should definitely use an enums for your AST. There is usually no need to have a function for each variant as you can just put the implementation directly in place. If you ever get stuck, feel free to look at and steal ideas from the compiler I am working on [embed_lang](https://github.com/Marwes/embed_lang) or the older compiler for (parts of) [haskell](https://github.com/Marwes/haskell-compiler) I made (it has recursive descent parser very similar to what you are making if nothing else).
* Error reporting is awful right now, indeed. However, while I'm a big fan of `Result`s, I'm not sure using them is obviously the right thing here. I'm reading the `rustc` source while writing my compiler, and I noticed it uses panics for fatal errors. One advantage is that compilation is faster (since you stop at the first fatal error anyway), but I'm not sure by how much. * I'm working on that too, however I'm worried about cache friendliness (I'll have to profile to be sure, of course). Shouldn't it be faster to read the source string linearly, tokenize it, then read the tokens linearly, instead of going back-and-forth between parser and lexer, thrashing the cache in the process?... &gt; There is usually no need to have a function for each variant as you can just put the implementation directly in place. What do you mean by "you can just put the implementation directly in place"? Thanks for your links!
&gt; When viewed from that perspective a panic is not especially friendly. Well, from the point of view of the user, there doesn't have to be any difference between panicking and using `Result`s. It is possible to [override the default panic hook](http://is.gd/faTQfA) to prevent an ugly message from being printed to stderr, and to use [`recover`](http://doc.rust-lang.org/std/panic/fn.recover.html) to capture the error carried by the panic and display it in a user-friendly way. I'll try using `Result`s, though, and see how that turns out. &gt; It feels so wasteful to create such a large vector only to throw it away immediately :/. It's a trade-off: memory versus speed. Given that memory is plenty cheap and hardware speed improvements are slowing down, I'll happily choose the latter. But if you care about memory usage that much, I guess you could use a fixed-size token buffer, and get the best of both worlds.
That would have been my guess too. You can take it even further with `debug_assert!(value.is_some())` would help the optimizer to assume things (similar to the [`__assume`](https://msdn.microsoft.com/en-us/library/1b3fsfxw.aspx) hint in ms' C++ compiler). But this [playpen](http://is.gd/wSPeib) shows that it still emits panic checking code (Release build and inspect the generated code). I can understand why Rust wouldn't want to do this, at least not in safe code but it would be nice. It's trivial to make an assertion and then to not follow it, would result in Bad Things (tm) in Release builds.
I started to play around with [qmlrs - QtQuick bindings for Rust](https://github.com/cyndis/qmlrs). This seems quite usable with QML. Can someone comment on the limitations of this GUI? Is it usable on all platforms which are supported by Qt Quick?
Thanks so much :)
[removed]
Trait objects can be an order of magnitude slower than ADTs (enums in Rust), and it's not like you can encode interesting patterns with just trait objects. To implement visitors or folders, you can use a macro to automate parts of it, or you can do it manually. Either way, you get to keep a lot of polymorphism and low-overhead static dispatch.
Er it was a joke, continuing the theme of "failing". The OP clearly wasn't offended by it, either. Don't be so thin-skinned?
Wouldn't it be better to use `Cow` instead of `Box`?
Awesome. Thanks! Spotted a typo in the last link to rustonomicon. Looks like it should point to https://doc.rust-lang.org/nomicon/lifetimes.html
&gt; I haven't compared performance of ISPC vs. Rust but I'd expect it to be similar to the numbers they show vs C++ How about comparing with [huonw/simd crate](https://github.com/huonw/simd) (also see [bench](http://huonw.github.io/blog/2015/08/simd-in-rust/#benchmarks))? I dunno whether it is still maintained. Is it easier to program using that crate or in ispc?
&gt; To me doesn't really make sense to define unary minus on an unsigned number I'd explain it as "unsigned means no sign: the type is unable to distinguish numbers greater than zero from numbers less than zero." Using `u8` as an example: 0 == -256, 1 == -255, 2 == -252 ... 255 == -1 Fortunately we can still count on the identity `a + -a == 0` (and so can the optimizer).
You are looking for /r/playrust. This is /r/rust, a subreddit about the Rust _programming language_ :)
&gt; since you'd be doing a bounds check each time that you did a lookup on vecb[i] That cost should be negligible-to-zero depending on how much inlining happens. `zip` is still more readable.
Please let me know if you run into any problems, or if you need more static C libraries, or if I could improve the docs in any way. Note that this container is set up for musl-libc x86_64 builds.
You can use `env!("CARGO_MANIFEST_DIR")` to build a PathBuf to your helper binary with an absolute path. [Here's I how do it in my project](https://github.com/yberreby/rgo/blob/f4f994ebfc1690b08adc233a5615ef0d2e4e94ca/tests/all.rs#L7-L13).
Cool, I've been trying to find some more Rust videos to watch where people actually code. There's never anyone on livecoding or twitch doing Rust, and I'd rather watch someone work on something real instead of all the repeated tutorial videos that are on youtube. Look forward to checking this out later!
Exactly.
Missing from Rust (and many other high-level languages). I think language people consider this problem out-of-scope, like bundling data with your program. Plain-text configuration file with minimalistic syntax. TOML, YAML, XML if you're in a nasty mood. Cargo uses TOML. Locating configuration files is platform-specific.
Could you give a more concrete use case? This feels very object-oriented: b = a.createHandler(); As if you want to create a back-door `b` that inspects or modifies the state of `a` but *also* want to continue using `a`. Rust won't let you do this unless you prove to its satisfaction that you haven't created a race condition. (You may create deadlocks.) If you only want to "zoom-in" and interact with part of `a`, then zoom back out before working with all of it, that's much easier to make safe and acceptable. That's what /u/joshmatthews is suggesting.
I came across that last month, and is the reason I actually picked up Rust in the first place :) Sadly there hasn't been much action for a while, so I've been looking for something similar without much luck.
I was thinking of something like this: let a = huge_expression(); let b = &amp;a + x; let c = &amp;a + y; Wouldn't it make sense to use `Cow` here, so `a` does not have to be copied?
These are the sort of comments that I am referring to. &gt; I feel like my six year old brother is correcting my algebra homework in crayon. &gt; adding overflow checking to types that do not overflow is a terrible idea. &gt; like designing an aeroplane for pilots who don't understand aerodynamics. Even if you succeed you've still done something very dumb. &gt; it makes the language feel like a single-minded toy &gt; The casual ignorance of modular arithmetic in all these discussions &gt; it felt more like an internet discussion than a math or computer-science discussion. &gt; All-in-all the process gets a B- &gt; a factor that hinders adoption and contributes to the language's slow slide into obscurity Most of these comments are belittling and condescending, a few of them are just unnecessarily melodramatic. Several of them are placed in prominent positions in the texts, and they sum to an unmistakeable rhetorical technique. I get the clear impression that he thinks this decision proves that Rust is an amateurish language designed by people unfamiliar with computer science (a belief that should be easy to dispel). I am not trying to shut /u/glasswings down. I am trying to encourage him to use language that will be more effective in accomplishing his goals. These quotes sapped me of any interest in engaging with his proposal, and I imagine I am not alone - this is not in his best interest.
ISPC is probably easier. It's almost like writing regular sequential code. It's not Rust, though. :) For comparison: - ISPC: https://github.com/ispc/ispc/blob/master/examples/mandelbrot/mandelbrot.ispc - huonw/simd: https://github.com/huonw/simd/blob/master/examples/mandelbrot.rs
One thing I found helpful is to take screenshots of each of the significant steps of the live coding. It both helps me focus on what I want to talk about, and gives me a backup when things inevitably goes wrong. There's never enough time in the day :(
Wow. Impressive. Thanks for adding them
https://crates.io/crates/dotenv
you know, if Rust and Go were direct competitors I would say about your project something like: *I also like to live dangerously*. But, that is not the case :)
&gt; they sum to an unmistakeable rhetorical technique. Rhetoric that is sincere - melodramatic yes - but sincere. But please don't overgeneralize: one amateurish decision does not make a language amateurish. To give some other examples: null pointers are a terrible idea, but that doesn't make C a terrible language, and having way too many keywords is no fun, but I still loved QBASIC. I do not couch my opinion in whatever terms seem most likely to get me what I want. I am expressing how I feel interacting with the compiler, a real-life use story. I am also expressing that the RFC process doesn't give me the warm fuzzies. I honestly and sincerely judge that the consensus reached regarding bounds-checking was immature and foolish. I'll even make a generalization: the process and its conclusion was beneath everyone involved. That is: you could have picked a random contributor, given them responsibility for making the best decision possible, and had a better chance at doing the wiser thing. Why? Because when you give responsibility to someone random, their natural attitude will be "I don't understand this, I hope I don't screw it up." Not "I think the language really needs change X and I will champion it." This is why dictatorships work so well in the free software world. The kind of leadership talent that a dictator needs is more common and more useful than the kind of leadership talent that makes friends and influences people. Furthermore, it's a lot easier to have respect for a Python decision (e.g.) that overtly boils down to "it's not a bad idea but Guido doesn't like it" than it is to have respect for "all opinions were considered" when in fact: all opinions that were voiced at the time in the right tone were considered. I don't think my ideas are unique. I'm not that clever. Someone else must have been thinking them, but they weren't involved or they weren't comfortable speaking up, or they simply weren't asked. This is not to say I advocate an anything-goes attitude in discussions. I will cut down ideas unapologetically but people deserve better. If I've been too *personal* (not too *critical*) I apologize.
My first outline of the function goes like this, (operate on `&amp;str`): initialize hashset for i in 0 .. word.len() if word[i] not codepoint boundary, continue let right = word[i..] let k = 0 for j in 1 .. right.len() if right[j] not on codepoint boundary continue k++ if k &gt; max_len, break insert right[..j] into hashset return hashset i and j are bytes, k counts codepoints. Special-casing / guard conditionals are done with continue/break. Fortunately, [the decision was made to stabilize](https://github.com/rust-lang/rust/issues/27754#issuecomment-195478632) `is_char_boundry` so this approach is reasonable. &amp;str vs String boils down to this: `String` owns memory, `&amp;str` doesn't. Use `String` when you need to own memory, `&amp;str` when you're just looking at it. Yes this is not the most obvious thing to think about, especially if you come from a garbage-collected world. As I think about it, I believe it's best to create a copy of the input (converting `&amp;str` to `String`) at the top of the function, then store &amp;str slices looking at that copy in the hashset. This is because the caller would reasonably expect that the hashset is a new independent object, not subject to the lifetime restrictions of the original borrow. Memory is cheap enough to avoid that hassle.
For borrow checker POV, the last `&amp;mut` lives as long as self (`'b`), not as long as `'a`. There may be another solution that I don't know but I think you need to use unsafe to convince him that it does live for `'a`. Here is the `vec_decque` `iter_mut` implementation: http://doc.rust-lang.org/src/collections/up/src/libcollections/vec_deque.rs.html#1813-1816 
&gt; Rhetoric that is sincere - melodramatic yes - but sincere. Biting my tongue so hard. Most of what you write is either wrong or unsupported, void of any value gut judgements. Nobody cares about sincerity, it's never been an excuse. &gt; one amateurish decision does not make a language amateurish You didn't say Rust committed one-off mistake, you implied the community involved in the design does not understand modular arithmetic which is quite another thing. Of course they understand it and this should give you a clue that there are trade-offs that you should maybe try to understand, before you come here to pontificate patronizingly about why everyone is wrong. &gt; I am also expressing that the RFC process doesn't give me the warm fuzzies. The point is to make documentation tangible and not he-said she-said inaccessible fragments in a cloud somewhere that no one can follow. &gt; immature and foolish Stop it, it's cringeworthy. &gt; So, why not go with f64? Because it's slower. Rust is about memory safety and zero-cost abstractions (aka speed), in this order, then comes everything else. Using floats liberally would mess with design priorities. 
You know, community is important to me. To the point that I've never done Java work because I disliked the communities fascination with being better than C++ (late 90's, early 2000's). And being honest, you have single handedly caused me to question whether or not rust will be the second community I choose to actively avoid. At some point caring about the quality of rust means not saying anything if someone expresses themselves a little more colorfully than you would like because the idea's and discussions they're bringing to the table will ultimately make rust better. Because at no point was /u/glasswings toxic, he was flowery. He used a bit of flair in his expression. For example, the crayon statement. You take it as him insulting rust, I took it as him saying he felt like rust was doing too much handholding (treating him like a child). And lastly, I'm going to compare this with a study that came out relatively recently. Whether you agree with the study or not, it's the idea I'm trying to convey. Emphasis mine. http://www.mirror.co.uk/news/world-news/people-who-swear-most-cleverer-7011464 &gt; That is, a voluminous taboo lexicon may better be considered an indicator of healthy verbal abilities rather than a cover for their deficiencies. &gt; "Speakers who use taboo words understand their general expressive content as well as __nuanced distinctions that must be drawn to use slurs appropriately__.” He's obviously passionate about this, and that's what you want in this community. People like /u/glasswings actively make rust a better language, and shutting him out because you don't like the flair he gives his words is an injustice to rust and its community. It would be one thing if he had actually been toxic, but he wasn't, and it was unfair of you to continually attack and dismiss him. edit: And also, rust *is* an amateurish language. It's new, it's untried, and it's still changing rapidly. That isn't a disparagement, rust has some really smart people working on it. Someday I'm sure it'll be an established, solid piece of technology, but for now very few people use it for production work (although that's changing). It isn't necessarily a bad thing to acknowledge that point
I don't think you've said anything that needs apologizing for. I don't have an opinion on whether or not consensus vs dictatorship is the right way to go, although I think your observations about the downsides are generally correct. however, one sentence in particular stuck out to me. &gt; Furthermore, it's a lot easier to have respect for a Python decision (e.g.) that overtly boils down to "it's not a bad idea but Guido doesn't like it" than it is to have respect for "all opinions were considered" when in fact: all opinions that were voiced at the time in the right tone were considered. I was having a discussion with someone just the other day in which I stated "I prefer it that way" is a valid reason to want to do something. It's also honest. The ones who never say "that's just how I want it" typically can't be trusted because they have a tendency to try and cover their preferences in technical reasons. These people can't be reasoned with because they'll just find another technical reason when the initial one is shown to be faulty. It's actually a *good* thing when people say "I prefer it that way". It implies an understanding that their preference can't hold up to strong technical opposition and they realize it. You can have better discussions with such people, and they tend to make better decisions in my experience. I know that wasn't the point you were trying to make it, but it jumped out at me.
&gt; You didn't say Rust committed one-off mistake, you implied the community involved in the design does not understand modular arithmetic which is quite another thing. Of course they understand it and this should give you a clue that there are trade-offs that you should maybe try to understand, before you come here to pontificate patronizingly about why everyone is wrong. actually, he's been pretty open about looking into the reasons surrounding the decision. &gt; The point is to make documentation tangible and not he-said she-said inaccessible fragments in a cloud somewhere that no one can follow. You've misunderstood his complaints about the process.
I believe that you are communicating in good faith, but some of your claims strain credulity. You consistently characterize glasswing's statements to make them seem more favorable than I think is reasonable, and characterize my statements in a way that I think is totally unreasonable. Some examples: &gt; the crayon statement. You take it as him insulting rust, I took it as him saying he felt like rust was doing too much handholding (treating him like a child). The statement was that _Rust_ was like a child, childishly ('in crayon') making incorrect edits to glasswing's 'algebra homework' (something a child is not educated enough to understand). I don't feel like I am going out on a limb by saying your reading is not the common reading, not the intended reading, and not a reading consistent with the text itself. &gt; And also, rust is an amateurish language. It's new, it's untried, and it's still changing rapidly. Intentionally or not, this is equivocation. The sense in which Rust was said to be amateurish was in the sense that there was a claim its designers were ignorant about basic concepts from discrete math like modular arithmetic. That sentiment is both false and disrespectful. &gt; shutting him out because you don't like the flair he gives his words is an injustice &gt; it was unfair of you to continually attack and dismiss him. I asked glasswing to consider the way their choice of words was rude. They responded with more content which was rude in a similar manner, so I again asked them to consider the way their choice of words were rude. I have not in any manner silenced them, nor do I have any particular influence or power by which I could. There is no injustice here. I asked someone to consider their language and they refused to. I wish it had gone differently, and I don't know why the idea of maintaining an atmosphere of respect is unreasonable to some people.
I am getting Error 522, connection timeout. Anyone knows a cached version?
I'm going to continue to [clippy](https://github.com/Manishearth/rust-clippy) various projects I can find. Also working on a super-secret lint that was started during the Rust Lint Workshop.
I quoted two things you said which I thought mischaracterized my prior statements. There was a period when formatting issues concatenated the two sentences, and I think you think I intentionally tried to combine them into a single sentence. That was not the case, and I edited my post to fix the formatting issue before your reply was posted. I don't think the formatting issue significantly impacted the meaning of my comment. Your response has me very confused. EDIT: I guess what I would ask you to do is to step back and have some perspective on this conversation. First you accused me of attacking, silencing, shutting down, and so on, for asking someone to think about their language. Now, because I forgot a newline character in a markdown, my post is as egregious as "throw[ing] a racial epithet as a black person". This is not a reasonable way to respond.
&gt; actually, he's been pretty open about looking into the reasons surrounding the decision. He replied with "casual ignorance" bit after he was pointed to RFC. The language feature isn't the issue, everyone makes amateurish mistakes, it's that he called people discussing RFC ignorant. &gt; You've misunderstood his complaints about the process. Have I? He seems to be against "well formatted natural language".
You'd use `Rc` for that, now `Cow`. Using `Cow` still requires a clear owner of `a`, which is tricky when it's contained in two independent ASTs.
If it's any help, [the whole Neg implementation for Wrapping&lt;T&gt; is fixable.](https://github.com/rust-lang/rust/pull/33067/)
Coincidentally I've been working on the same project hehe. I'd be interested to see your implementation, since my benchmarks are slower. 
Why is `steal_a_var` named like that? It just borrows its parameter: fn steal_a_var(o: &amp;u32) { println!("{}", o); } Also the comment // steal_a_var()s scope ends, killing all the variables inside it... // c goes away Seems wrong to me, since you can happily use `c` after it. See this [playpen](https://play.rust-lang.org/?gist=59bd7c4c5f553767549cc0dc7baaeefe&amp;version=stable&amp;backtrace=0).
I am writing a PNG decoder for fun but also with the end goal of it being the PNG decoder used by Redox. I have never written an image decoder before so it is presenting itself as a nice challenge. The spec is fairly easy to digest too. I currently (after about 2 weeks of limited actual time spent on it) can decode Truecolor+Alpha 32-bit PNGs and 4-bit Indexed (paletted) PNGs. Once I have all critical chunks, color types and bit depths supported I will push it to GitHub.
I think the easiest way here is to use [Cell](http://doc.rust-lang.org/std/cell/struct.Cell.html), like this: type Sudoku = [[Cell&lt;u32&gt;; 9]; 9]; /* ... */ impl&lt;'a&gt; Iterator for RowIter&lt;'a&gt; { type Item = &amp;'a Cell&lt;u32&gt;; ...that will allow for mutation of the `u32` without using mutable references at all. But it should also be possible to do this (without unsafe) by using [split_at_mut](http://doc.rust-lang.org/std/primitive.slice.html#method.split_at_mut) - something like this (sketch only) : struct RowIter&lt;'a&gt; { sudoku: &amp;'a mut [[u32; 9]]; y: usize, } /* inside next() ... */ if self.sudoku.len() &lt; 1 { None } else { let (a, b) = self.sudoku.split_at_mut(1); self.sudoku = b; a[self.y] }
One idea is to add a struct that represents an update to the board (That is a value that you know for certain) struct update { x: usize, y: usize, value: u32} Then you can do let mut updates = Vec::&lt;Update&gt;::new(); loop { get_updates(&amp;board, &amp;mut updates); apply_updates(&amp;mut board, &amp;mut updates) } This will be slightly slower, but it allows you to parallellize you code in a simple way. If you want great speedups, then you may wish to look at constraint programming and pcp. https://github.com/ptal/pcp
What you wrote is short for fn next&lt;'x&gt;(&amp;'x mut self) -&gt; Option&lt;&amp;'a mut u32&gt; and the reference you create by &amp;mut self.sudoku[x][y] is basically of type `&amp;'x mut u32`, not `&amp;'a mut u32` where `'x` does not outlive `'a`. However, you can solve this *without* writing your own unsafe code by using `slice::IterMut&lt;'a, [u32; 9]&gt;`. It already solves this lifetime problem for you: use std::slice; pub struct RowIterMut&lt;'a&gt; { columns: slice::IterMut&lt;'a, [u32; 9]&gt;, y: usize, } impl&lt;'a&gt; Iterator for RowIterMut&lt;'a&gt; { type Item = &amp;'a mut u32; fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; { self.columns.next().map(|col| &amp;mut col[self.y]) } } where `yourstate.sudoku.iter_mut()` will give you such a slice iterator.
ward of warning about using twitch: you will get swatted. it's when a viewer on twitch calls the SWAT team on the streamer (pretends the streamer has killed his whole family, etc) and the SWAT breaks down the door. It's happened to me when I streamed League of Legends.
Best advice is not to give your full name.
That's odd, seems fine for me here :/
Or to live outside the US.
I merged a big bunch of PRs for [imag, the personal information management suite for the commandline](https://github.com/matthiasbeyer/imag) which improved code quality (Enable lints, refactor some code, etc). Some more are coming up in the next days. I also got the rewrite of the link library merged this weekend, so I'm happy with this piece now. Besides that, I'm still working on the diary implementation (yeah, blame me...) I decided to re-write it almost entirely. I really hope I can get the diary working this week and also maybe the libimagentryview, which is not even started yet but also rather trivial. I got a friend doing that but I guess he is stuck and as he didn't push his commits to his repository I have to start over I guess. --- And I'm going to [write another blog post about imag](http://beyermatthias.de/tags/rust.html) at the end of the week. I also want to advertise it much more, because I'm **still looking for contributors** - maybe some of you people here are interested? I have a bunch of _complexity/easy_ labeled issues in [the issue tracker over at github](https://github.com/matthiasbeyer/imag/issues). Feel free to stop by and ask questions!
Thanks, fixed.
I'm not familiar with SAT solvers, but one option could be to temporarily take the vec out of `self`: let watch = mem::replace(self.watches[false_lit.as_usize()], vec!()); ...and then if ever `find_viable_lit` returns `None`, put it back: None =&gt; { self.watches[false_lit.as_usize()] = watch; return false } Now this does not work in case `find_viable_lit` actually needs to inspect `self.watches[false_lit.as_usize()]`, but if it can actually return a `lit` that's equal to `false_lit`, then you have a possible infinite loop...? 
 pub fn run(&amp;mut self) { loop { let StateFn(state_fn) = self.state; if state_fn.is_none() { break; } self.state = state_fn.unwrap()(self); } } Just FYI, this would probably be a lot more idiomatic as pub fn run(&amp;mut self) { loop { match self.state.0 { None =&gt; break, Some(state_fn) =&gt; self.state = state_fn(self), } } } 
You've found *lexical scoping*, which is how rustc currently handles lifetimes. Scopes always go on until the next closing bracket. However, it's possible that this may be relaxed in a future rust version.
I suppose if the struct is not in the same module, you'd have to write accessor methods and call `Self::find_viable_lit(self.member(), self.clauses(clause))`?
That's what I initially thought, but the Rustonomicon said "The borrow checker always tries to minimize the extent of a lifetime" in its section on the subject, so I rewrote my blogpost. Darn! Thanks for the help.
Yes, this looks ok. But please reconsider. A solution without `unsafe` is possible, too.
Cool! Last time I looked https://github.com/PistonDevelopers/image-png used https://crates.io/crates/flate2 which uses C libraries for decoding DEFLATE. Would be nice to have it all in Rust!
Trying to come up with ideas on how to make Tera (template engine) typesafe, if you have ideas please post on https://github.com/Keats/tera/issues/24 Development in on halt until this question is solved. Lastly, looking for contributors to help as well once a decision has been reached on the type safety
Could anybody explain me how to wrap [this piece of code](https://github.com/klingtnet/rsoundio/blob/master/examples/sine.rs#L13-L28) into the function idiomatically to init audio and return just `OutStream`? All my attempts to make it straightforward and then just follow compiler hints on errors failed. `sio` and `dev` still don't live enough. How to make them outlive the function? Wrap everything in some struct and return together, but not just `OutStream`?
What projects' sources would you recommend to read to learn idiomatic Rust?
Why multi-tape?
That's what I really want, thanks
I'm writing an arithmetic expression evaluator and I'm working on the lexing part. In C/C++ I'd do something like this bool lex_literal(const char* in, double* out) { char* end_ptr; *out = strtod(in, &amp;end_ptr); return in != end_ptr; } Well the part I'm interested in is where I want to see if a string starts with an f64 literal as well as tell me where the literal ends. `FromStr` just errors if you do this: fn main() { println!("{:?}", "12.4 + 2".parse::&lt;f64&gt;()); } How do I get `strtod`-like behaviour?
Thanks a lot. I wasn't ready to dig into the standard library to find it. I don't have a GitHub account yet, so I hope you don't mind if I mention it here: Wrapping(self.0.wrapping_neg()) would avoid creation of a `Wrapping(0)`. I guess it shouldn't make a difference. Since LLVM IR doesn't have separate `Neg` and `Sub` opcodes and `Wrapping&lt;T&gt;` is a zero-cost wrapper, they'll both compile to a subtract from constant zero in the IR. 
I'm writing an arithmetic expression evaluator and I'm working on the lexing part. In C/C++ I'd do something like this bool lex_literal(const char* in, double* out) { char* end_ptr; *out = strtod(in, &amp;end_ptr); return in != end_ptr; } Well the part I'm interested in is where I want to see if a string starts with an f64 literal as well as tell me where the literal ends. `FromStr` just errors if you do this: fn main() { println!("{:?}", "12.4 + 2".parse::&lt;f64&gt;()); } How do I get `strtod`-like behaviour?
What guarantees do you strive for?
Really nice work! 
Perhaps [a closure](https://llogiq.github.io/2015/08/19/closure.html) helps?
Perhaps nom it? Or chomp on it?
Does Rust have Fortran matrices or just standard arrays of arrays? Any plans to add them? I haven't been able to find a straight answer from where I looked.
Thanks, but the point here is to write it myself as an exercise. Looking at those crates an oh boy, maybe for another day. Atm just going with calling `libc::strtod` and updating the iterator as needed. Uses unsafe but oh well.
Short Answer: No. Rust bundles a bigger smarter allocator with itself. Which wastes a lot of binary space. Long Answer: Yes. Rust auto builds as static builds not dynamic builds. So all dep's/libraries (except Libc) are packaged into the binary. This could be changed with a lot of sweat, blood, and tears. Longer Answer: No. 5-10MB binaries aren't *as much* of an issues in $Current_Year. Embedded platforms maybe, but on x86_64 PC's. You easily have 1,000x more RAM so keeping your ELF's small isn't as big of a deal as it was in the 50's when dynamic loading was invented. 
You can see the divide and conquer part [here](https://github.com/AtheMathmo/rusty-machine/blob/paramul/rusty-machine/src/linalg/matrix/mat_mul.rs#L171-L234) - though it is likely to change soon after posting. I'd really like to see your implementation too!
From how I see it, you could add a line describing types in a template, for example `{% types user=User count=i32 %}` and it would generate functions to render template with the correct arguments and then you get rust compiler safety. Not sure if that would work though. The tricky part is that kind of type safety makes it unusable without rustc and `build.rs` which means it couldn't be used by a static website generator for example
Perhaps that was the intention of the secrecy? Edit: No, there's really something shady going on. :-)
woah, yeah that def saved a lot. I found [this link](https://doc.rust-lang.org/book/custom-allocators.html) which explains `alloc_system` pretty well. Thanks for pointing me to this :D
I will be in France, then Lithuania, then Belarus. Back Monday morning. Whew! So I might not be on IRC that much, but I have a lot of book work to do, so maybe that's good for me, ha!
Me and retep998 did this; I got it down to 257 bytes, and they got it down to 1,536 bytes, for Unix and Windows respectively: https://github.com/retep998/hello-rs
The language itself only has one dimensional chunks of memory (slices &amp; arrays), one has to use types implemented as external libraries to get any fancier nD types beyond plain old arrays of arrays. Rust is designed to be powerful enough for library types to be just as performant as baking things into the language and hence there is no plans to add fancier array types.
Having a single piece of data of interest that is tied by lifetime to various other set-up/management values is definitely annoying. Depending on the details of the situation, sometimes it is better for that sort of library to swallow an allocation and use `Arc` (o r `Rc`) internally. However given the situation, one approach (which is somewhat of a generalisation the CPS---continuation passing style---transform which I believe llogiq is suggesting) is to leave the set-up code where it is high on the stack and instead move the code after the set-up into separate functions that take the arguments they need. If the types are specific enough, then this is hard to screw up: the compiler tells you what you need to pass where. The compiler essentially uses this structure internally: the various phases are implemented as a series of functions that take the various parameters they need (such as arenas used to allocate internally). The caller of these functions is then automatically forced to create the arenas before calling any of the main passes, and the types help the programmer link everything together right. (This answer is not as satisfying as it could be, I know.) Another approach is that which `owning-ref` (on crates.io) best exemplifies: it uses the knowledge that certain pointer types are stable (never change addresses) to do some magic about tying ownership of some original value to a value derived from it, i.e. the parent object is automatically kept alive by the child, the inverse of Rust's default approach. (I'm on mobile at the moment so can't make an example, sorry.)
Are you aware of https://github.com/pcwalton/parng ?
I'm not making significant changes to the book, as I'm .... re-writing it. Which is significant. Heh :) Cow's docs in general have been on my todo list for a long time, and so it might be better to fold it in there, for now.
I'm not sure, but I believe that there is `-C prefer-dynamic` which should accomplish (part of?) your goal. I couldn't find any real official documentation only a related rfc: https://github.com/rust-lang/rfcs/blob/master/text/0404-change-prefer-dynamic.md
Oh dear the formatting is pretty mangled. I'll try to fix it. Sorry
You can use your own enum.
AFAIR, jemalloc was always used for executables. Maybe this item from [the changelog](https://github.com/rust-lang/rust/blob/master/RELEASES.md#version-180-2016-04-14) is responsible? &gt; When using jemalloc, its symbols are unprefixed so that it overrides the libc malloc implementation. This means that for rustc, LLVM is now using jemalloc, which results in a 6% compile-time improvement on a specific workload. I see the same in 1.10 nightly. Switching to the system allocator with #![feature(alloc_system)] extern crate alloc_system; make valgrind "see" the leaked 100 bytes again.
Your benchmarks, is that for the f32 or f64 version? Just curious. There's the possibility that matrixmultiply's version of one compares better to blas than the other. Great numbers anyway.
No worries. It's in a separate repo, and I haven't been talking about it because I haven't made a ton of progress lately. Editing takes a while... (And glad you like my stuff, thanks)
Any chance of a link?
https://rust-lang.github.io/book/
my sibling gave you the rendered output, repository is here: https://github.com/rust-lang/book
Why do you want to be able to take `File` or `&amp;File`? If you have a `File`, you can convert it to an `&amp;File` trivially. "Ownership polymorphism" doesn't make sense as an actionable concept to me, an `&amp;` is basically already ownership polymorphism. Cow is useful for the ability to _clone_ a reference when you want to mutate it (without cloning when you have ownership). Incidentally, you can't clone a `File`, so maybe that was a bad example. Are there types for which you want a clone on write capability but they don't meet `Cow` right now? There may just be an impl missing.
check [this](https://github.com/pegasos1/rsmat/blob/master/src/dot/dot.rs) out. Seems like we took a similar approach. I'm using naive dot product implementation though. matrixmultiply is cool!
For the same reason as the original article: &gt; This implementation has a big limitation; `token` cannot outlive `secret`, and that means `token` can’t escape this stack frame. That's the problem with always taking a reference. Callers can't insert references to local values into my struct, if they're going to return my struct. I could use `libc::dup` under the covers to sort of make a clone of the file, but secretly dup'ing other people's file handles isn't ok. (It might represent one end of a pipe, where keeping it open causes something else to deadlock.) So I need something that matches the ergonomics of `Cow`, but without the ability/requirement to convert a reference into an owned value. I'm leaning towards defining my own enum, like /u/neutralinostar suggested, but I think exposing a nice API around something like that is going to require at least two separate impls for every type I want to handle, and that feels gross.
matrixmultiply gave even larger performance gains than I expected! It's definitely worth trying it out. Our approaches are practically the same :D. The only difference is that I also split along `k` and add the two matrices together. This can also be done using the mutable view. [See here](http://i.imgur.com/RJwCE9r.jpg). The first two rows you've covered. In the final row you can use your _init_ view for the left matrix in the diagram and use some other uninitialized memory for the second. Then do init += a_2_b_2 I need to make some changes to rusty-machine before I can pull that bit off though : (
&gt; `'a` - `B` cannot contain a lifetime that outlives `'a` That's exactly backwards. `B: 'a` means `B` has to live at least as long as `'a`. That is, `B` cannot contain a lifetime that is shorter than `'a`.
Or, alternatively, you could use links (either symlinks or hardlinks) to have a single physical executable act as multiple programs. This is what Busybox does, and it saves significant amounts of both disk space and deployment complexity over using dynamic linking for that.
Fun, fun. I'm still working on mine too. Good first project, isn't it?
The stream is over, but was recorded. You can see the VODs at https://www.twitch.tv/seantheprogrammer/profile
Interesting. thanks for the link. 
Got a chance to put the benchmarks after the work done on stream into context. Diesel's query builder is now faster than a SQL string literal. https://twitter.com/sgrif/status/722228823308107777
I see now. I didn't consider the fact that the enum could be instantiated with the `'static` lifetime for owned values, but a reference cannot. If you could come up with a snappy name and a handy interface, the generalization of this type seems like it would make for a useful crate.
Yes, I will. I just need to look up how and remember to do it. And make a youtube account I guess
Oops! Wrote that down backwards... will update the article. Thanks!
There is no such thing as (classical) OOP in Rust, which adopts a more compositional or functional (think C or Haskell) approach to code reuse. Say you want both `Car` and `Bicycle` to be `Vehicles`, you'd do something like this: trait Vehicle { fn they_see_me_rolling(&amp;self); } struct Car; impl Vehicle for Car { fn they_see_me_rolling(&amp;self) { println!("They hear: vroom, vroom!"); } } struct Bicycle { sound: String, } impl Vehicle for Bicycle { fn they_see_me_rolling(&amp;self) { println!("They hear: {}", self.sound); } } In other words, `Car` and `Bicycle` _are not_ `Vehicle`, as much as they have a behaviour that fits the __interface__ `Vehicle`. You may manage to find some way to do OO in Rust, but I would personally discourage it: Rust itself is not a classical OOP language. If you approach it this way, you're going to hit a wall at some point. Do you have an example of a situation where you feel like you must use OOP? EDIT: Fixed code styling (Reddit doesn't like triple backslashes, I get caught every time -.-)
Nick Cameron was writing a guide on [design patterns](https://github.com/nrc/patterns) with a Rusty flavor (but it's incomplete, with no new commits since 3 months ago). Also keep in mind the [style guide](https://aturon.github.io/) (it has nothing to do with OO per se, but many customary things in other OO languages are done differently in Rust) Rust doesn't do inheritance. Think the "a `Car` is a `Vehicle`" toy example. In class-based OO, they are both classes and `Car` inherits from `Vehicle` (inheriting both code and data). Rust doesn't do that. If you don't need to share data but just an interface (that is, call the methods of vehicles on cars), then `Vehicle` should be a [trait](https://doc.rust-lang.org/book/traits.html) and `Car` should implement this trait. (it's analogous to `Vehicle` being a Java interface -- but Rust traits are way more powerful) If you need to share data to from all vehicles, then `Vehicle` should be a struct, and `Car` should be another struct that includes the vehicle in a field (this is called [composition](https://en.wikipedia.org/wiki/Composition_over_inheritance)). AFAIK, it's not usual to write wrapper methods that merely call the contained object (that is, implement a `car.start()` that just calls `car.vehicle.start()`). There are some topics on how to apply composition in Rust, like [this](https://www.reddit.com/r/rust/comments/372mqw/how_do_i_composition_over_inheritance/). [ edit: I wanted to link [this comment](https://www.reddit.com/r/rust/comments/372mqw/how_do_i_composition_over_inheritance/crjp10m) ] edit: another thing: I think your main abstraction should be traits, use composition only if you really need to compose data. Think of not reusing data as a plus: it gives the implementer of the trait more flexibility, and whoever is reading the code doesn't need to be puzzled thinking where that member came from. Also: if two method name conflicts (perhaps because your struct implements two traits, that define the same method), you can always disambiguate between the methods using the [universal function call syntax](https://doc.rust-lang.org/book/ufcs.html).
You'll probably have to specify the target explicitly: ``` docker run --rm -it -v "$(pwd)":/home/rust/src ekidd/rust-musl-builder cargo build --release --target x86_64-unknown-linux-musl ```
This includes [Rust expression parser in Bison](https://github.com/tromey/gdb/blob/rust/gdb/rust-exp.y), for example.
[yes](http://rustbyexample.com/trait.html) example: trait Bar { fn bar(&amp;self) { println!("Bar!"); } } struct Foo; impl Bar for Foo {} fn main() { let foo = Foo; foo.bar(); } 
&gt; I certainly won't be willing to tolerate a 50GB system image because the userland was rewritten in rust That's not how this works. The 5-10 MB binary size is a constant offset, not a linear multiplier. Rust scales like C when it comes to binary size, just that the initial binary size is larger due to static linking of the stdlib and jemalloc. So Rust's program size will be `(5MB + size of same program written in C)`, not `multiplier*(size of same program written in C)` In case you're actually doing embedded programming you can turn off a bunch of things and get tiny binaries if you need. That constant offset is the default because most programmers don't need to care about it, but it can be removed if you do.
I was not aware of this. It certainly looks good. I did search around and only found one other PNG decoder that didn't just wrap libpng. I won't be micro-optimizing to quite the level that parng is, but I hope it will be decently performant in comparison to other libraries (it certainly seems quick in its current state.. but its not finished yet).
That would be a dream come true for me.. except someone above posted a PNG decoder that is micro-optimized for speed written by someone who works for Mozilla.. so its very doubtful my little decoder lib will make it that far :P I can dream though can't I? :D
That doesn't seem to help. The binary resides in a directory called x86_64-unknown-linux-musl so the compiler probably figured out the correct target
I imagine by classical OOP you mean what Java, C# and C++ developers know about. Papers about trait based OO go back to the late 70's.
In case of a Rust userland that wouldn't be an issue, since you would be linking dynamically to all these. C binaries are tiny by default because they link dynamically to system libraries. Rust doesn't, because the Rust stdlib dylib isn't found in distros (and even if it was the ABI isn't yet stable). But if you plan on having tons of Rust binaries around there's nothing stopping you from linking dynamically.
Thank you! Just curious, why is TypeNum striked out?
Super interesting –- not just seeing you do stuff but also hearing your reasoning. I just watched the second (recorded) half at breakfast, it was pretty late hear in "UTC+2 with day job"-land…
I would like to have some pointers about the rules of concurrently accessing memory from several threads, and how these relate to the rules which have been addressed by the memory models in C++11 and Java. ''Given that Rust is so young, is concurrent memory access well-defined in all cases?
Thanks for your answer, but i am a little lost, the TcpStream don't implement the clone method anymore (i found an example with this method from one year ago but it doesn't compile anymore). What i am missing to clone my TcpStream ?
You also don't have inheritance in all OO languages.
Tip for compiler designers out there: if you use `Add(Box&lt;(Expr, Expr)&gt;)` in place of `Add(Box&lt;Expr&gt;, Box&lt;Expr&gt;)` you halve your allocations!
You where mentioning Java/Python style OO. Traits and impls dont make anything classes can do impossible, they are just a way to template your object. On the other hand good luck following inheritance based patterns without inheritance support.
If I understand the changes correctly then it will give you possibility to view internals of the Rust structures (most importantly containers) without `rust-gdb`. 
I built this branch and tested it out in printing a string and a vector of ints. vanilla GDB 7.11: $1 = {vec = {buf = {ptr = {pointer = {__0 = 0x7ffff6c20008 "abc"}, _marker = {&lt;No data fields&gt;}}, cap = 3}, len = 3}} $1 = {buf = {ptr = {pointer = {__0 = 0x7ffff6c21000}, _marker = {&lt;No data fields&gt;}}, cap = 4}, len = 3} vanilla GDB 7.11 plus rust-gdb: $1 = "abc" $1 = Vec&lt;i32&gt;(len: 3, cap: 4) = {1, 2, 3} GDB from this branch: $1 = collections::string::String {vec: collections::vec::Vec&lt;u8&gt; {buf: alloc::raw_vec::RawVec&lt;u8&gt; {ptr: core::ptr::Unique&lt;u8&gt; {pointer: core::nonzero::NonZero::NonZero&lt;*const u8&gt; (0x7ffff6c20008 "abc\000"), _marker: core::marker::PhantomData::PhantomData&lt;u8&gt; {}}, cap: 3}, len: 3}} $1 = collections::vec::Vec&lt;i32&gt; {buf: alloc::raw_vec::RawVec&lt;i32&gt; {ptr: core::ptr::Unique&lt;i32&gt; {pointer: core::nonzero::NonZero::NonZero&lt;*const i32&gt; (0x7ffff6c21000), _marker: core::marker::PhantomData::PhantomData&lt;i32&gt; {}}, cap: 4}, len: 3} The expression from this branch is certainly much more correct than the vanilla GDB one, but also much less helpful by default than what rust-gdb gives us. However, I'm not sure the level of pretty-printing that rust-gdb does is suitable for default GDB behavior.
You should be able to call functions and evaluate arbitrary expressions.
Wasn't it always faster than SQL string literal?
No what I mentioned is that there are other models of OO and that I understand classic by what Java, C# and C++ developers know about. There are quite a few other models of OO that go back to the late 70's, discussed in SIGPLAN papers, including conceptual models similar to Rust traits. This was my point.
Thanks for posting this comparison!
What about impl lifetimes? Can you explain those too please?
I don't think that's easily possible (yet), and it'd be a huge project. The problem is that the Android API is mostly Java-only (plus JVM languages), even C/C++ has to communicate with Java via JNI a lot. Google doesn't really want you to use the NDK for app development, if you can avoid it. &gt; Before downloading the NDK, you should understand that **the NDK will not benefit most apps**. As a developer, you need to balance its benefits against its drawbacks. Notably, using native code on Android generally does not result in a noticable performance improvement, but it always increases your app complexity. In general, you should only use the NDK if it is essential to your app—**never because you simply prefer to program in C/C++**. When examining whether or not you should develop in native code, think about your requirements and see if the Android framework APIs provide the functionality that you need. -- [NDK](http://developer.android.com/tools/sdk/ndk/index.html) Unless anyone mirrors the Java API to JNI wrappers, we might have to wait till Google decides to use a natively-compiled language for Apps. There was talk about using Go, Dart or Swift for Android development, but I'm unware of any official plans beyond [Flutter](https://flutter.io/). You can use Rust crates in your app, or use Rust to drive a [NativeActivity](http://developer.android.com/reference/android/app/NativeActivity.html), but that's not going to get you very far if you want the native UI.
I wish rust let you do #[derive(Bar)] for traits that provide all methods
Do you know about: https://www.livecoding.tv/ ? it is basically Twitch for coders
As others have said, Rust uses a weak memory model very similar to C++11. Rust guarantees memory safety, which translates to outlawing *data races* in concurrent code. There is the possibility for other problems such as (non-data race) race conditions, dead locks and, relevantly, misuse of fences/atomic orderings. Unfortunately, one of the best answers for "how do I know my atomic orderings are correct?" is "just use seqcst everywhere", and another is "think hard about edge cases, and think hard about why they're OK". A compiler verifying atomic orderings would basically require global reasoning about everything that could interact with shared data... Hard work! In fact, I believe it is so hard that it is still one of the cutting edges of programming language theory research. However, not all is lost in terms of static guarantees. Rust at least allows focusing on the areas of concurrent code that will be the most dangerous/hardest to debug (memory corruption dependant on precise thread timings). The only way to get that memory corruption (aka memory unsafety) via shared mutable state is to have `unsafe` somewhere, hopefully throwing up a big red flag: if you have atomics and `unsafe` in the same piece of code, that code needs a *lot* of attention. (To expand, the core cause of the double-checked locking problem is publishing a pointer in some shared location, without guaranteeing that the pointer's data written before publishing will be available for other threads to read if they can read the published pointer value. Rust highlights the issue because the only ways to atomically publish shared pointers in Rust (e.g. `AtomicPtr`) require `unsafe` to actually read the data they point to, effectively the compiler is asking for an assertion from the programmer that the data is in a fit state to read, that any necessary fences exist, etc.) On the topic of STM, it already exists: there is at least one crate for it on crates.io. I guess you're really asking if the standard library will include STM support, but this seems very unlikely to me, at least for the near future: the standard library is trying to not grow fast at the moment (anything added will have to be supported forever), and STM isn't necessarily the best core abstraction for a systems langauge to provide (it gives a nice API but, I believe, often isn't particularly scalable since the bookkeeping involved modifying globally shared locations).
Go has more convenient embedding, though, so you can write something like this to achieve the same result: type Barer interface { Bar() } type DefaultBarer struct {} func (db DefaultBarer) Bar() { fmt.Println("Bar!") } type Foo struct { DefaultBarer } Which hypothetical Go 3 version of default implementation might desugar to. But I'm not holding my breath, because that would defeat the purpose of ad-hoc interfaces (breaking source code dependency). And, while Rust has some neat stuff in it (such as generics), Go's interfaces and embedding are more convenient way to reuse code.
&gt; As far as safe rust code goes, you don't have to worry about anything here, shared memory will work as expected. You will get compile errors if you try to do anything with a data race. This is misleading/wrong: the core problem with the double checked locking is not a data race (at least, not as C++/Rust define it). You can still get unexpected/wrong results by omitting fences, even if you're not going to accidentally get memory corruption. More specifically, given shared locations `x = 0` and `y = 0`, where all reads/writes are *relaxed* atomic instructions, then running x = 1 y = 1 and while y == 0 {} assert_eq!(x, 1) concurrently, may fail the assertion. Weird, huh? (Sorry for the pseudorust, on a phone so a working example is too much typing.) Fences/non-relaxed orderings on instructions protect against this, by ensuring that when another thread reads the new value from `y` it will also read the new value from `x`. Getting the right orderings (and fence locations) to give the results one expects is often not easy, nor is it something Rust itself (language or standard library) provides any assistance with, beyond of course providing a few abstractions so users often don't need to write atomic code themselves. &gt; Rust's stdlib has a `Once` type which uses sequentially consistent atomics that can be used in lieu of double checked locking. I suspect it could be made to use acquire/release semantics (might need a fence?) but I'd have to think about it. I'm not sure "in lieu of" makes sense here: double checked locking is an optimisation one can do on top of any implementation of `Once`, no matter what atomic orderings it uses. In fact, Once *is* double checking, and SeqCst effectively makes this automatically correct (the difficulty with double checking is that it is a little subtle with weaker orderings).
Saturday and Sunday, I think? I would love to come back to Ukraine, Kyiv is one of my favorite places on earth. But I don't have any current plans to :(
I think the main thing is that it exposes a Rust-like expression syntax. This means that you can examine objects in a way that is similar to what you wrote in the source. For example, you can print a tuple member with "print x.0", or you will be able to make a new array slice and use it, like "call mumble(&amp;array[0..7])" (I say "will be" because I'm still implementing slicing in the expression evaluator). Other things are a bit more Rusty as well. For example "ptype" tries a bit harder to print in Rust syntax.
Just a note on formatting, Reddit doesn't support the ``` notation, you've just actually done (``) (`line ... `) (``) and used the inline code formatting, with two empty strings on either side. It's easier if you use the 4 spaces indent to mark something as code. Example: This is a code block. Note the newline before it starts.
One thing I'm slightly confused about: this is a copy of GDB on your personal gihub, right? So it's not like this has landed upstream yet, it's just in-progress? How likely is it to land upstream, do you think?
Thanks for the clarification! That's exactly as I thought it should be.
For full queries including deserialization, yes. This is just the query construction though.
Yeah, I think the time that I streamed on Sunday will probably be the norm on the weekends. Hits more of the globe. Hard for me to stream at a good time during the week though. I'm full time OSS, but during the day I'm usually just doing issue triage which would be a much less interesting stream.
Specifically, the benchmark was https://github.com/diesel-rs/diesel/blob/master/diesel_tests/tests/bench.rs#L29-L31 for the query builder case. The sql case was the same code, but using the equivalent sql string directly (with one minor change to the code to force the SQL string to be cached, as we normally don't cache SQL literals as they may be dynamically generated and would cause unbounded memory growth in the statement cache)
I plan to bundle it up and submit it upstream when it is ready. I think the odds are good that it will go in, though probably after a bit of wrangling.
Currently Rust doesn't have its own language spec. The standard library refers to the underlying LLVM memory model: http://llvm.org/docs/LangRef.html#memory-model-for-concurrent-operations LLVM in turn says that its similar to `C++0x`. That doesn't mean a lot to me, but if you have a deep understanding of synchronization issues the standard library source is probably the best entrance to the topic. Use github blame to find out who the major contributors are and they'd be the best to ask. 
Is it because of the macro's language?
Well, I mean... *yeah*. There's nothing *else* to blame. It can't do it because it doesn't have any constructs that would allow *allow* it to do it. \**shrugs*\* 
Awesome! I guess this explains the torrent of debuginfo bugs filed by /u/tromey. It'll be great to have first-class debugging support for rust! Also, I wonder if any parts of the [Rust Language Server](https://github.com/rust-lang/rfcs/pull/1317) will eventually be useful for debuggers? Perhaps you could call out to the compiler for help with things like parsing and trait selection?
How about this? fn add(x: Value, y: Value) -&gt; Value { use Value::*; match (x,y) { (Int(i), Int(j)) =&gt; Int(i + j), (Str(s), Str(d)) =&gt; Str(s + &amp;d), _ =&gt; panic!("Type error!") } }
You can also scope things like function declarations and struct declarations in the same way. It can come in handy when writing helper functions, sometimes.
&gt; I used gdb this morning to find out where an unwrap panic was occuring, for example As an aside, if only the "where" part is important, then you can simply set "RUST_BACKTRACE=1" while running the program... I have felt that a debugger actually slows down the debugging process in (safe) Rust, where logical errors are the main (only) concern...
Normally gdb goes its own way in expression parsing. There are a few reasons for this, some ok, some just history. The ok reasons are that debugger users usually want some language extensions: * It's useful in the debugger to be able to violate protection mechanism, like using private fields. * gdb provides convenience variables; and it isn't perhaps widely known but this makes the gdb expression language more dynamic than many of the languages it targets, because a convenience variable's type isn't fixed. * gdb provides convenience functions, which can be written in Python. Historically there wasn't a way to reuse compiler bits in the GNU toolchain. Also, gdb originated as a C debugger, where this problem is just not difficult to solve. This changed a bit in the recent past -- see the gdb "compile" command and the "libcc1plugin.so" that ships with GCC. This project lets gdb reuse gcc, while still providing type, variable, and function definitions from the inferior. It's still pretty early though (like, no direct integration into breakpoint conditions, etc). This could certainly be done for Rust, and I can explain how at length (I wrote a lot of the GCC and GDB code for this); but it's pretty complicated and I don't have any plans to look into it.
As mentioned by Effnote you can actually scope pretty much anything: new structs, new functions, new traits, `use`, ... if it makes sense, try scoping it and see if the compiler complains :D
Very cool, I was thinking of getting into rust by implementing Matrix, but it seemed too complicated as a first rust project.
[Rust guidelines](https://github.com/rust-lang/rust/tree/master/src/doc/style) now lives in rust repo itself. Unfortunately, it is rarely updated. There are too few "best practices"-style reading for Rust.
Someone mentioned in on Twitter. Is there any specific reason that it'd be better or worse than Twitch? I don't have an particular preference
lol what?... pretty sure its actually growing, esp if you look at github and many trending projects are in Rust. I wish more companies adopted it though.
/r/playrust
Here's a [recent thread](https://internals.rust-lang.org/t/need-help-with-emscripten-port/3154) describing some of what's involved in building for emscripten. I suspect that everything has again bitrotted to the point that it will be non-trivial getting it working again. The good news is that the Rust team has committed to adding emscripten as a target, so once we *do* get it working again we can start moving to get it under CI so that it works for good. I suspect that the most immediate thing that needs to be done is that emscripten needs to forward port their LLVM fork to catch up with ours, but I have not looked at the state of it lately.
Go to "IV. Trending Tech on Stack Overflow" and click on Losers. It's losing about -5.9%
Thanks, this is helpful! I also think that using lock-free data structures in many cases is not worth the work it needs with stress-testing and very thorough testing of edge cases. What proved really useful for me was to have a "simple" mutex-protected implementation of a data structure, and another lock-free implementation with exactly the same interface, which could replace the first without any other change. As far as I understand C++, as soon as one uses mutexes, one is safe because they include the correct fence semantics. And another thing I learned, lock-free implementations of more complex structures (say, a double-ended FIFO) quickly do not only become very complex but also require so costly atomic operations (in terms of cache line flushes, worst-case latencies and so on) that their performance advantage to lock-protected structures can become questionable. 
-5.9% is in the noise. That's in the same order of magnitude as the change of Visual Basic for Applications (2.5%).
Ah, thanks. I agree with /u/pcwalton here...
Damn... That's weird.. Just noticed that
It may be declining, and it may be worrying if you have a requirement for your tech stack to be trendy :)
I agree, and to expand on that, I think that moving all the things to Rust might not be the smartest choice - unless the application has very few dependencies on the platform. The Rust wrappers would have to be very actively maintained for a comfortable UI development, and even then I doubt that we can reach the comfort of UI development of something like Android Studio. What would be amazing, though, is to have an easy way to call back and forth over JNI between Rust and Java, with a tool similar to [djinni](https://github.com/dropbox/djinni) (well, I am repeating what I posted in other answer, but it's just too exciting). That way the UI could be done with the tools that are native to the platform, and we could still benefit from Rust where necessary.
Noise is much lower level, yes. It is concerned more with the details of how messages are exchanged over the network. Matrix is more akin to XMPP than it is to something at the layer Noise is.
"Kids, did I ever tell you the story, when back in 2015 I was fiddling with that one programming language… the one with the oxide jokes…"
Not a big deal. But I'm wondering why `Self` can't be used in the commented line here: https://play.rust-lang.org/?gist=bf5c48757a0fe28ecfc785be53aa5c6a&amp;version=nightly&amp;backtrace=0
`¯\_(ツ)_/¯` It's a streaming platform, whether it's originally for gaming or not doesn't really affect anything. I have no strong opinions but I'd have to do a bit of setup to switch, and I haven't seen any specific reasons to do so.
That just means you need to ask and answer more questions!
There are plenty of languages that people love that don't get widespread use. Look as Smalltalk, Scheme and Haskel for example. 
Usually folks on Linux use gdb
IMO, it should be allowed.
Good. I don't like overly hyped up programming languages. :) However, in term of substance, I have a strong feeling that Rust is on the rise.
Hmm, Matrix appears to structure comments as a graph at the lowest level, is this ever made available to users? Will clients ever be able to render conversations as a tree?
There are two things, in my mind, that primarily influence language representation on stack overflow. Complexity and popularity. For a while, rust was changing really rapidly (which mean high complexity, lots of questions). Now that it has stabilized, I think it is somewhat expected that number of questions would go down. What will be interesting is next year's poll. I would expect that rust questions would go up as popularity gains (especially since the language isn't experiencing as much flux). Just so long as we don't land in Dart territory, I think rust will do fine.
If you're asking about whether or not messages can be structured in a nested-comment format like on reddit, I don't think so. The graph aspect is used to resolve conflicts in the view of the world between homeservers, but the conversation it ultimately represents is linear.
Whilst Matrix is an application layer protocol rather than a transport layer protocol like Noise effectively is, it's worth noting that the parallels with XMPP are pretty superficial. You can use both Matrix &amp; XMPP to build chatrooms, but that's about it. Matrix is effectively a decentralised database with open federation for exchanging and persisting any kind of realtime data with eventual consistency semantics. It's all about synchronising conversation history between different servers and clients. XMPP instead is all about message passing rather than synchronising state. If anything, Matrix is more like a CouchDB style document database... but with open federation and a single global footprint.
&gt; Edit: is it that when read() returns 0 we can't tell whether it is because the buffer is full or EOF so rust always trys to resize the buffer first? Yes, exactly. If the buffer is full (i.e. no space to write) then `read` always returns `0`, whether or not the file is ended, so Rust needs to make sure there's space or else calling `read` is pointless.
Allowing that probably hasn't really crossed anyone's mind. However, if we consider `Self` a special scoped type alias (i.e. as can be manually defined with `type`) then it should probably work. Seems like something that one could open an issue and/or RFC about.
I think that Conrod is designed to create in-game UI that doesn't need to give the same look like other parts of system. So answer to Your question is: depends what you need. 
Depends on your use. If you need to do normal gui things like open a file system "open" dialogue box, ~~or have more than one window open at once~~ then gtk is it. If you want cool sliders and breakpoint envelopes and the ability to write customizable widgets at a low level then go with conrod. Edit: piston_window can open multiple windows if needed.
Huh, I could swear there was a lockfree ringbuf in there. I knew that it switches based on the situation, but perhaps I mistook Buffer for a lockfree thing. Might be fun to implement a proper lf ringbuf in a separate crate then.
&gt; have more than one window open at once conrod doesn't allow that?
It's still a work in progress, but I think I have enough written to ask for feedback. I am writing a simple command line tool to interact with an API. I am most curious about the code in check_venue.rs and the custom decoder I wrote. I am not sure if there is a better way to get that working, although I tried for several hours. 
*Someone knocks on the door. Moments later, a man walks into a room, carrying a huge pie.* "Hey, I'm here for the Pie club!" *The room turns to stare at the newcomer, their expressions blank.* "Uh... this is the Pi Club." "Yes, that's right." "Pi. As in the mathematical constant?" "...the what now?" "It's written on the door. 'Pi Club'. I don't know how you missed it." "I assumed it was a typo." "The walls outside are covered with pictures of pi, and stylised pis, tons of circles and sine waves..." "I just ignored all that. Didn't seem relevant." "We're in the maths department of a university!" "Are we? I didn't notice." "You didn't notice the enormous monolith out the front with 'Maths Department' written on it in gleaming, cut aluminium letters?" "Not really." *The math fans stare at the pie-bringer.* "... so did no one else bring any pies or what?" *The math fans proceed to hurl the interloper out the window to the sound of crashing glass and screaming.* "God damnit, that's the third this morning!" "We *really* need to start locking that doo—" *The sound of someone knocking on the door interrupts them.* "Oh *noooo,* not *again!*" &amp;nbsp; /r/playrust 
I think your `CheckApiResponse` type would be better as a `Result&lt;(), String&gt;`, so that you get all of the built-in methods on `Result` to use. I'm not entirely sure why you're even using it. There's nothing stopping the API from going down between when you check it and when you use it.
It doesn't look like you've pushed `check_venue.rs` to github yet.
At least Mozilla, Samsung, DropBox, MaidSafe and some other companies are investing in Rust already. So yes, Rust is serious, and it's here to stay. Also the team has made good on their stability promise so far.
&gt; I haven't even tried it yet, but if future versions of this language came out which made current rust programs broken when that happens, that could be an awful lot of code to re-write. The team regards [stability as a goal of Rust](http://blog.rust-lang.org/2014/10/30/Stability.html), now that 1.0 has been released.
Currently in the middle of testing a scraping tool for all of the issue/PR/comment activity from rust-lang/rust on GitHub. It's been interesting to do in Rust, partly because GitHub's API is so clearly made for consumption by a dynamic language, partly because learning hyper &amp; diesel has exposed me to a whole new part of the ecosystem.
&gt; And another thing I learned, lock-free implementations of more complex structures (say, a double-ended FIFO) quickly do not only become very complex but also require so costly atomic operations (in terms of cache line flushes, worst-case latencies and so on) that their performance advantage to lock-protected structures can become questionable. The performance questions about atomics/lock-free data structures and lock-protected ones is a tricky one, depending on the exact algorithm and even the platform they're running on. However, even the most expensive lock-free algorithms are generally more scalable as they don't inherently serialise everything as a lock does, that is to say, lock-free structures are likely to work better if you're interacting with a data structure from a lot of threads. Furthermore, locks can suffer from significantly more contention, as all interactions modify the same locations (the internals of the lock), while lock-free data structures can and often are explicitly designed to avoid this. That is to say, lock-free data structures align better to the underlying architecture. (Also, locks themselves have to be implemented in terms of atomic operations, and, as you say, include fences internally, hitting some of the same sort of costly atomic operations.)
&gt; Yes, and I then go on to talk about non-seqcst atomics and mention that you need to think about them explicitly :) Everything else is pretty straightforward to reason about. I'll clarify. Ah, that "doesn't use atomics" clarification was fairly critical. ;) &gt; I meant "in lieu of manually implementing double checked locking". Makes sense, although the fact that it happens to use SeqCst internally is a bit irrelevant (they could be weakened and users would only know because their code got faster).
 struct Foo { bars: Vec&lt;u8&gt;, // imagine this is a list of "big structs" bar_index: usize, } impl Foo { fn baz(&amp;mut self) -&gt; &amp;u8 { self.bar_index += 1; &amp;self.bars[self.bar_index] // don't want to return a copy of a "big struct" } fn qux(&amp;mut self) { // use the baz [helper] method more than once let b1 = self.baz(); // previous borrow of `*self` occurs here; println!("{}", b1); // use b1 let b2 = self.baz(); // error: cannot borrow `*self` as mutable more than once at a time println!("{}", b2); // use b2 } // ^ previous borrow ends here } fn main() { let mut foo = Foo { bar_index: 0, bars: vec![1, 2, 3, 4, 255] }; foo.qux(); } Is there a way to call a method that takes &amp;mut self and returns a reference, more than once in the same scope/calling method?
_Definitely_ use GTK for a desktop app. Conrod is a game-driven GUI toolkit and it will look extremely strange (think early Java Swing applets) if you try to use it for native applications.
Reviewing would be easier if the code at least compiled (as /u/minno remarked). Regardless, I've sent you a PR.
Yeah, you have to manage the framerate yourself I think, like 60fps + sleep the rest of the time. Stuff like that is usually done by a game anyway, so conrod kinda expects you to have that already.
If this is the case, it's probably a bug - the `all_widgets` example sits at about 5% cpu for me (also on OSX). Feel free to post an issue with your problem code. BTW, conrod doesn't provide any event loop itself - it just reacts to events, so the problem is likely more related to the window backend you were using at the time (for the examples, the backend used is [piston_window](https://github.com/PistonDevelopers/piston_window)).
Yes that's correct, however having to explicitly annotate the type of `e` every time I pass in a new instance to `foo!()` is exactly what I was trying to avoid, given that the type of `a` is already known and in my application it will be the same.
You have a (probably unintentional) extra level of indentation here: https://github.com/aaronmell/rust-stockfighter/blob/master/src/check_api.rs#L13-L15 Also, the common convention in Rust is to place braces on the same line (Egyptian braces) instead of placing them on the next one (Allman style).
Actually, you can do this instead: fn main() { let a: Result&lt;u32, &amp;str&gt; = Ok(0); let b: Option&lt;u32&gt; = foo!(a, Err("error")); }
Since it's web stuff and Atom/VSCode are also web apps, it shouldn"t be too hard to integrate this frontend in those editors, right?
Check this issue, you probably need to enable valgrind support for jemalloc: https://github.com/rust-lang/rust/issues/28224#issuecomment-138725566
Debug mode you say? In debug mode you'll go through all the `Wrapping` wrapper functions without inlining, which will be slower by far.
Ah! I didn't fully grasp your intentions. That particular way of doing it is not possible as far as I am aware... Primarily because you cannot write down the type of all expressions. E.g. `typeof!` (if existed) would not work for closures. But I guess you can pass in a type as a macro argument using `:ty`, though I wouldn't encourage that if it would go against readability...
This works: macro_rules! foo { ($src:ident, $val:expr) =&gt; ({ match $val { Ok(val) =&gt; { { let mut _type_hint = &amp;$src; _type_hint = &amp;val; } Some(val) } Err(_) =&gt; None, } }) } 
In debug mode it will be slower, but in release mode it will simply become an `add` assembly command. AFAIK: In debug mode: - default add = add with a check for overflow - wrapping add = add with a check for overflow and a special case when it happend In release mode: - default add and wrapping add become assembly `add`, no overflow checks are done for both. The wrapping operator is there to just tell the compiler that overflow is a wanted behavior
Sorry about that, I'm new to git on the command line, and I didn't realize that I hadn't added that file
Question: why introduce a new command for this? I feel like a target would have been perfectly fine...
An Android package can contain multiple binaries for multiple targets (arm, x86, mips, each 32 or 64bits). Therefore it's cleaner to treat an apk as a layer above targets.
&gt; it's a lot easier to get big swings in usage for new languages, because they don't have as many users in the first place Good point.
I'm excited for this as I think it'll also pave the way for packaging tools for distros like RPM and DEB. Supporting packaging natively in the build toolchain should drastically improve the ease of installing software as people won't necessarily have to build from source. It's a common complaint of mine on Linux, where people stop at supportin g compiling from source when really they should stop at packages. 
&gt; I'd also like to use this as a platform for code exploration - implementing some of the features from DXR such as 'jump to definition', 'find uses', and 'identifier search'. I just happened to stumble upon [grasp](http://www.graspjs.com/) (and somewhat similar [es-search](https://github.com/jdonaghue/es-search)) today, and immediately thought that it'd be cool to implement something similar for rust, especially as afaik the rustc parser should be pretty usable as-is.
I've been struggling with autocomplete in sublime / vim / atom for sometime now, I've got a semi working setup based around racer - but I've not been able to tell it about dependencies sourced from cargo. Anyone have any suggestions? 
You need to set `CARGO_HOME=&lt;user home&gt;/.cargo`
Yeh, a macro was probably the wrong thing to invoke here. It would probably just have to be some kind of special Rust syntax. I see what you're saying about closures. Perhaps this magical `typeof` feature would also be smart enough to return the type of a trait corresponding to a given closure since the unique internal type of the closure itself wouldn't be very useful.
Yep. `Ok(())` for success, `Err("something's fucky".into())` for failure. That way you also get a warning if you don't check the return value of the call.
I'm still a bit baffled that we're ~~polluting~~ populating the cargo namespace with platform-specific commands. But then, I know nothing about Android dev or the engineering concerns that led to this, for all I know this might have been the sanest design decision. Good on you I guess.
&gt; http://redditmetrics.com/r/rust Um, how does that show any kind of decline? I see a noisy graph going straight to the right, maybe with an ever-so-slight upward curve.
&gt; since the time you really want to be reminded of overflow is when you don't want it to wrap. yeah, that's why you use wrapping to tell the compiler that, in debug mode it's alright that it overflows.
Overflow checks are disabled in release mode because they have a significant runtime performance cost. But you still want them on by default in debug mode and when running your test suite (which compiles in debug mode) so you'll be aware when your program is performing unwanted wrapping. It's relatively rare that wrapping is a desired behavior, hence the explicit `Wrapping` struct (aside: there are also `wrapping_add` and `wrapping_sub` methods, which I think are a bit more user-friendly). Ideally overflow checks would be able to be kept on in release mode, and there have been efforts to reduce the cost of overflow checks to make this possible, but right now if they were kept on, people would be complaining that Rust is "slower than C++" in release mode.
It seems to be working fine after a `cargo clean`, but when I change something and rebuild without cleaning, I'm getting the error.
This works for me if I pass an `Err` but not if I pass an `Ok`: fn main() { let a: Result&lt;u32, &amp;str&gt; = Ok(0); let b = foo!(a, Err("error")); // works let b = foo!(a, Ok(1)); // error: mismatched types }
Here's a shorter and more generic version of your `type_hint` function, which works for both the `Ok` and `Err` cases: macro_rules! foo { ($src:ident, $val:expr) =&gt; ({ fn hint&lt;T&gt;(_: &amp;T, v: T) -&gt; T {v} match hint(&amp;$src, $val) { Ok(val) =&gt; Some(val), Err(_) =&gt; None } }) } fn main() { let a: Result&lt;u32, &amp;str&gt; = Ok(0); let b = foo!(a, Err("error")); let b = foo!(a, Ok(1)); }
Schopenhauer is turning in his grave. Look at what you've become. A shill for feel-good american sentimentalism.
What I'm saying is if packaging is going to become part of cargo then it should be a first-class citizen. Where cargo add the `package` command as part of its standard set which can then call into any of the multiple package formats that might be desired (apk, rpm, deb, and zip to name a few). I would think this is nicer than stuffing all of these into the root namespace. This is really a bike-shedding issue, so it's not terribly important, but I'd really like packaging to be a first-class citizen with Cargo to facilitate people distributing their applications in an easy-to-use format.
The error "failed to read bytecode" seems to indicate that one of the rustc compiler's intermediate artifacts has gone missing unexpectedly. I've seen this when I've had two instances of cargo running at the same time; eg: running in another terminal. Might not be the case, but worth checking.
[What do you mean, _no_?](https://play.rust-lang.org/?gist=971606d4b311a2841ed9565c26f462c4&amp;version=stable&amp;backtrace=0) (cc /u/dragostis)
I do have cargo compile whenever I save files for syntax checking, could that be affecting the build?
Could both be supported? (or is LLDB already supported?)
Is it possible to integrate it with `cargo test`? For example, it can highlights failed assertions with the same view used for syntax errors
Additionally, those builds distinguish themselves by disabling jemalloc. On an untested platform like ARM (for both rust and jemalloc) that's a big plus. And they're built with clang.
C++ was never used for OS development because it's too complex under the hood. Because of RTTI and other complex language features, when something goes wrong in the kernel (especially with memory access) it's nearly impossible to debug because of the complexity of the underlying implementation.
The scoping is indeed special. I can declare a trait function to return `Self`, and I can construct the return value using a type alias to `Self`, even if the alias is defined within the function, but if I try to assign to the alias to `Self` itself (`type Selph = Self;`) then I get E0401 and E0071.
&gt; I can construct the return value using a type alias to Self, even if the alias is defined within the function What does this mean if not `type Selph = Self;`?
That statement triggers E0401 (somewhat nonsensically). You have to write `type Selph = Foo;` with the real name of the struct.
discuss @ [RFC repo issue](https://github.com/rust-lang/rfcs/issues/1588)
CPython (the official implementation) is called C-Python for a reason, it literally has its entire ecosystem in C. But that's rather portable C so you can probably can run with Newlib. Also there's a ton of alternative Python implementations, so if CPython doesn't run as-is then one of the other ones might be easier to run on Newlib. Go, on the other hand... I don't think I can explain it any better than [this OpenSolaris guy did](https://youtu.be/TrfD3pC0VSs?t=1h7m59s).
&gt; That statement triggers E0401 Yes, I read that clause of your first comment, hence my confusion/question ;) Thanks for clarifying what you meant.
Yes, if some of those builds are overlapping with each other, or with your manual builds.
I work for a company that develops security systems that run on roughly Raspberry Pi-ish class of hardware and in such setups kernel drivers actually become a solid chunk of attack surface, and very risky one at that because they're in ring0. We would love to ditch Linux for something that has isolated and mostly memory-safe drivers. Redox looks like the perfect fit, except I'm somewhat concerned about ZFS being the only supported filesystem. Last time I checked, Solaris's ZFS implementation that got ported to BSD and Linux required multiple gigabytes of RAM just for its internal caches and whatnot, and we have at best 1Gb of RAM for *everything.* 
&gt; Through a weird trick, it would capture the arguments that rustc passed to the linker and instead manually invoke the linker itself Google devs hate him! Jokes aside, I didn't know you could build android native binaries from rust. Does it interop with the android NDK?
Probably, since the NDK is listed as required in TFA.
I would worry about the namespace if this were an official tool being distributed with default cargo, but this is from a third party and will only be installed by people who specifically need to package for Android.
Is there a way to run syntax/type/lifetime checking without saving files for my editor? 
I was able to run rustfmt no problems. Can clippy only be compiled from the nightly? Tried adding it to my project, which is using stable, and it fails to compile. 
Could someone provide an example of how a Trait Object could be used with Cow? Why (and how) would you "write" to a Trait Object?
**WHO DISTURBS MY SLUMBER?** just kidding, I've been awake for two hours already. 1. I heartily recommend rustup.rs – if you don't yet have it, give it a try! This way you can get nightly when you need it. 2. If you want to put clippy into your project, do it as an optional dependency. Otherwise `cargo-clippy` is the easiest way to run clippy, and you don't need to change your project. However, you'll lose the ability to `#[allow(..)]` or `#[deny(..)]` clippy lints. My personal preference is to use `cargo rustc ..`, but that's mostly because I sometimes test unmerged PRs or uncommitted changes.
Couldn't cargo clippy maybe, perhaps, process the `#[allow(..)]` stuff? Perhaps it could bundle a nightly rustc or something.
wrong rust reddit.
There's a directory for 64bits MIPS in the NDK, so I added it.
The FAQ says &gt; The unverified x86 version of seL4 supports VT-d extensions on the experimental branch. The VT-d extensions allow the kernel to restrict DMA and thereby enable DMA devices with untrusted user-level drivers. We are currently working on providing similar verified support for A15 ARM boards with SystemMMU. So they are working on IOMMUs, but they haven't verified this code yet.
You need much more than the parser, but the compiler has a load of infrastructure to make this kind of information available to tools - it is what powers DXR, for example.
&gt; It is important to note, however, that no function will allow you to get a native Android UI. This build system and this library are meant for applications that handle everything themselves, like video games. If you want a regular app you should write in Java. Why? And is this permanent advice, or just “there aren‘t any bindings for this kind of thing yet”?
I'll try a quick explanation as I am a bit in a hurry. The above example works if you are modifying a single value. But let's assume x is the head index of a ring buffer which is accesses by several threads. Now, the lock in the first code snippet warrants that x is not modified by any other threads. For the second code snippets, there is no such guarantee - the code could succeed even if the ring buffer index was modified multiple times. Imagine that we have such a ring bugger with 16 entries and a thread "one" tries to modify the head index by incrementing it from zero to 1. It reads the value of zero, and tries the compare and swap operation, but before that operation completes, it becomes blocked. Now a second thread does the same access and does that 16 times, so that the ring buffer wraps around. And then the first thread resumes, sees that the value happens to be zero, and the compare-and-swap operation succeeds. In reality, one entry of the queue is lost. Bang, if the value stored in the ring buffer is a pointer, we may have a memory leak or even a invalid pointer access later. This is the so-called [ABA](https://en.wikipedia.org/wiki/ABA_problem) problem. A related problem is that in more complex data structures, you probably do not have a compare-and-swap instruction which is able to atomically change all essential related data at once. For example, you can compare-and-swap 64 bit but you have 128 bits of data which must be kept synchronized. This means that you need to design the algorithm so that all changes become only valid at a certain point which is defined by an atomic operation. Then you get to algorithms like this one: https://www.researchgate.net/profile/Claude_Evequoz/publication/4376046_Non-Blocking_Concurrent_FIFO_Queues_with_Single_Word_Synchronization_Primitives/links/0c960527ca83d0249d000000.pdf You will find many similar publications if you search, for example, scholar.google.com . Now if you implement the second algorithm in the above paper naively on a x86 system with atomic CAS instructions, you will find that it works - but only in most cases. There is a problem with synchronizing the head index which needs to be addressed extra. In short, lock-free structures can be really tricky. 
Its in exactly the same sense as C, since the compiler optimizes loops based on this assumption. IIRC there have been various memory safety bugs related to refcounts due to this. It isn't as bad as pointer UB in C, but it isn't fundamentally different IMO. Edit: I'm totally wrong, overflows are defined as a panic or a wrap depending on your flags. I thought they were UB in the sense that we tell the optimizer to poof them at will. We don't.
I can imagine someone creating something similar to [goconvey](http://goconvey.co/) using that, which would be really nice
Implementation defined doesn't mean undefined behaviour.
You can specify the target directory with the `CARGO_TARGET_DIR` environment variable. Just be aware that Cargo wasn't really designed to have multiple crates emit to the same directory. There may be weirdness.
Woah, woah, integer overflow in Rust is not in any way, shape or form undefined behaviour like in C. If `x` is signed, the comparison `x + 1 &gt; x` can and [does](https://godbolt.org/g/m9wwLS) compile to true in C (i.e. gives the wrong answer for `x = INT_MAX`), but not in Rust. The memory safety problems with refcounts have been *humans* incorrectly assuming overflow won't happen, not the compiler.
It's been a long time, and I studied the C++ version of this book, but doesn't it use the visitor pattern?
I'll give that a whirl tonight. Thanks!
yeah, it's not clear to me how the event loop would work. I believe you would need a parent and a child window.
Lisp style. :D I sort of prefer exporting them outside since it makes macros more readable.
Hey there! I'm learning Rust and I have some problem on understanding the error thrown by this code: fn main() { let mut x = 42; let y = &amp;mut x; println!("We have {} and {}.", x, y); } The error is src/main.rs:5:36: 5:37 error: cannot borrow `x` as immutable because it is also borrowed as mutable [E0502] src/main.rs:5 println!("We have {} and {}.", x, y); I see the problem is on printing `x` but I cannot understand why :(
Awesome. Thanks for publishing this!
You say that "you can deploy your machine learning applications to almost any machine and device" and that "any combination of platform and computation language (OpenCL, CUDA, etc.) is a first class citizen in Leaf" but readme in your repo [says](https://github.com/autumnai/collenchyma-nn#provided-operations) that only CUDA is supported. Which info is correct? Are OpenCL and CPU supported?
&gt; A lint that false-positive's a lot will just be ignored. If you're doing a seemingly tautological comparison for its side effects, you damned-well deserve a warning. If you're doing this *a lot* you should probably rethink what you're doing.
The first statement is more conceptual while the second is more practical. The thing is, for the Neural Network algorithms (Collenchyma-NN) there are currently no OpenCL kernels linked. Do that and you can run your Leaf application on OpenCL devices, too. Pull Requests are welcome. The OpenCL kernels should be available somewhere e.g. via [DeepCL](https://github.com/hughperkins/DeepCL)
&gt; you damned-well deserve a warning. And that warning should tell you what you are doing wrong. That is, having comparison operators with side-effects. 
Well, not exactly *the* secret service...
It's confusing, because this is one of the few cases in which LLVM's "default" semantics differ from those of C. In most cases they match up, as LLVM IR is basically a C compiler IR (insert link to Dan Gohman's post here). In LLVM, integers have no intrinsic signedness, and therefore addition has well-defined wrapping behavior. There is a special flag, `nsw`, that the frontend can insert in order to specify that wrapping is UB, but rustc doesn't insert it. (The other major way that LLVM's semantics differ from those of C is that there is no such thing as type-based alias or strict-aliasing in LLVM by default—the front end has to tell LLVM explicitly about the aliasing rules for the language to allow it to optimize based on aliasing.)
So, we needed a new build tool to replace _make_ for Redox. The goal was to create a simple and fast build tool, so I came of with _cake_, a build tool based on the Rust macro system.
Or [Cake](https://www.npmjs.com/package/cake) the coffeescript build tool.
I'll just sue them all, right?
One possibility is that a previous file with that name has been deleted, but is still held open by some other process.
&gt; For the extra helper macros, see the rendered docs. A link to those rendered docs would be nice. 
[Rayon](https://github.com/nikomatsakis/rayon) is also an excellent choice.
So the chain of reasoning is this: First we type check overloaded operators uniformly. This means that `i32 + i32` isn't checked any differently to `Vec3 + Vec3` (for example). We special case primitives a little later so they aren't translated as method calls. This is why you need the trait implementations to start with. Since you have presumably implemented `Add` as: impl Add&lt;i32&gt; for i32 { type Output = i32; fn add(self, other:i32) -&gt; i32 { self + other } } The implementation of the `add` will use the "base" implementation of addition. By default, Rust checks for overflow in non-optimised builds, which triggers a panic if overflow occurred. Hence why you need the `panic` language item. Interestingly, even if you use something like the `overflowing_add` intrinsic in the `Add` implementation, you still need the panic language item as `i32 + i32` won't actually use the implementation.
Or [CakePHP](http://cakephp.org/) (which is not a yummy cake I admit).
Nah, there's loads of nondescript names of random things. It's just the short, obvious ones that are taken
Rust doesn't automagically take care of concurrency patterns. In this case it's (unintentionally) saving you from doing something silly: If you have multiple threads on multiple cores writing to the same memory region simultaneously you're likely to cause cache contention and lose most of the benefit. Apply the map-reduce pattern. All your worker threads can look at the source matrices, but each only computes part of the result. They send those parts back to an assembly thread (via `mpsc`?) which puts them together. Note that parallel matrix multiplication is only faster than single-threaded if your matrices are big enough. Also since you're compiling through LLVM, [SIMD is worth thinking about.](http://llvm.org/docs/Vectorizers.html) Bottom line is there's all kinds of picky details involved; this is why most people let a library do it. (Haven't gotten around to this in Rust yet. Best of luck Parallel stuff in Scala is quite a lot of fun though, and is all about the map-reduce.)
ah okay, thank you!
It's not really a bug. The requirement of `Add` to exist is more or less an implementation detail for the purposes of type-checking. The thing is, you don't really want all arithmetic to go through the method calls, since it would make compile times explode. Think of it as requiring the implementation of the operator traits on primitives to be just the expression and nothing more. So using `overflowing_add` would be "wrong" since only `self + other` would be considered correct. It's basically answering "what if people do X?" with "Well those people are wrong".
simplerustbuildtool?
The easiest way to make a simple traffic generator: Take a 4-port network switch. Plug two ports into each other. Plug the third packet into your sniff port. Transmit some DNS packets out to a MAC address that isn't on the network. You'll get back line-rate copies of those packets, as the switch does unknown-unicast flooding as quick as it can. At 1gbps, you can probably keep up, so you may want a faster NIC to test ... and I assume most people don't have 10gbps or faster gear laying around (though nics are only $50 on ebay if you get one cheap)
For a slightly smaller codebase that might be more understandable, my OS project, intermezzOS, also can't 100% use Cargo, but uses make to drive Cargo: https://github.com/intermezzOS/kernel/blob/master/src/Makefile Things it does that Cargo can't: * [use nasm to compile assembly files](https://github.com/intermezzOS/kernel/blob/master/src/Makefile#L61-L64) * [Clone down](https://github.com/intermezzOS/kernel/blob/master/src/Makefile#L48-L50) and then [build a custom libcore](https://github.com/intermezzOS/kernel/blob/master/src/Makefile#L54-L56) * [using that libcore is easier with make calling Cargo](https://github.com/intermezzOS/kernel/blob/master/src/Makefile#L59) * I think Cargo can technically do this, but it's easier to [just call ld to link it all together](https://github.com/intermezzOS/kernel/blob/master/src/Makefile#L45-L46) * [build an ISO with mkrescue](https://github.com/intermezzOS/kernel/blob/master/src/Makefile#L36-L41) * [run the final output through qemu](https://github.com/intermezzOS/kernel/blob/master/src/Makefile#L31-L32) I love Cargo, and use it as much as I can. But there's nothing the matter with using another build system on top as well! Basically, use Cargo for all of your Rust stuff, use the other system to integrate everything together.
It is *very* not true.
Name it take because it will take the pain away! Muahaha
It's 64 bits. It's *always* 64 bits. It's right there *in the name*. [There is no way for `f64` to be anything *other* than a 64-bit double-precision IEEE754 floating point value](http://doc.rust-lang.org/nightly/reference.html#machine-types).
Ah! So the "very" was only pointing at `*size` primitives... I got _very_ confused! :)
You're on Windows, so it doesn't apply directly, but, on Linux, [`rr`](http://rr-project.org/) is a great tool that works well with Rust, it's even had some adjustments to accommodate Rust better! (I wrote [a bit](http://huonw.github.io/blog/2015/10/rreverse-debugging/) about it, if you're interested.)
Ah! You said "_very_ not true", I read it as "not _very_ true". Sorry, my bad!!
Ha, awesome! Thanks for the tip! I haven't heard of that trick. I've got a switch right here but that's 1000Mbps as well as my onboard NIC. Maybe I could see if there are any spare parts at work I could borrow.
I only skimmed your blog post but rr looks really nice. I'll add that to my long list of reasons to give Linux a proper try for programming.
Const gets copied into each location it's used. Your probably wanted static, not const here.
 fn add(self, other:i32) -&gt; i32 { self + other } That's a recursive definition without a base case (it basically restates that `self + other` is `self + other`). The compiler would need to special-case the `add` method to say that inside it, the `Add` trait is not used to expand the meaning of `+`. Instead, it special-cases `i32` everywhere.
I guess the code is for the algorithm courses? That would possibly mean that you cannot rely on the correct and better-written functions from the standard library. That said, some random notes that do not depend on that fact: * [`if rng.gen::&lt;i8&gt;() &gt; 0 { ... }`](https://github.com/dingari/grr-verk1/blob/master/src/array.rs#L31) is better written as `if rng.gen() { ... }`; it generates a random `bool`. Well, unless you *did* mean to get the exact probability of 49.609375%... * [`print_slice` and `println_slice` functions](https://github.com/dingari/grr-verk1/blob/master/src/array.rs#L83-L97) are redundant, `print!("{:?}", &amp;a[i..j])` and `println!("{:?}", &amp;a[i..j])` likely will do the same. (The original code is also incorrect if `i == j`.)
You are correct that it is for an an algorithms course. We had to implement the sorting algorithms from the book's pseudocode. * Thanks for the hint, I didn't know about that. Was totally going for ~50% probability. * Yep, that makes perfect sense. At the time I didn't realize to use debug printing. Thanks for the feedback!
There's no particular reason to us not posting; when I edited TWiS I used to usually post it, but now Lars does, and he doesn't post it. I'll try to post it more regularly then. See also: /r/servo (though that's missing the latest one too :P )
I notice you use a lot of indexing where you could be iterating over references instead. E.g. pub fn clone(a: &amp;[i32], b: &amp;mut [i32]) { if a.len() != b.len() { panic!("Array lengths must be equal"); } for i in 0..a.len() { b[i] = a[i]; } } could be written as pub fn clone2(a: &amp;[i32], b: &amp;mut [i32]) { if a.len() != b.len() { panic!("Array lengths must be equal"); } for (from, to) in (a.iter()).zip(b.iter_mut()) { *to = *from; } } Note that your print_slice function will index -1 if you pass '0' to both `i` and `j`, and will generally misbehave if `j &lt;= i`, because it will always print at least the final element.
I've got gdb and the VSCode Debug extension installed. I've also got my project's launch config setup (check my config above). When I hit debug, the preLaunchTask builds my project properly then launches it through gdb and I get the debug console in VSCode. What *isn't* working is the breakpoints, locals, etc.
I basically have the same launch configuration you posted, the only difference is I didn't configure the preLaunchTask. It's doubtful it's the reason why breakpoints and locals work on my setup though...
Cargo recently added a feature that can help with this: https://github.com/rust-lang/cargo/pull/2385 You can add to your `Cargo.toml` file a `[replace]` section whose entries say "whenever *this* version of *that* library is to be used (no matter how deep in the dependency tree), get it from *there* instead." *There* can be any kind of source Cargo supports. For example a git branch, possibly the one for a not-merge-yet pull request.
You mean TWIR? Because that is what I have been talking about. TWIS is still on the blog, but TWIR for this week does not exist.
er, yeah, typo
* Some of the type-hinting can be removed and inferred, or value-specified e.g. in the first repo's main, you declare `a` and `b` as `[i32; SIZE]`, but the functions invoked on these take `i32` so the array types are naturally inferred. Similarly for the argument parsing, or in the second repo you can type either the Vec or the parses, but you shouldn't need to type both. There's a bunch of other places where you specify value types explicitly and don't need to. There's even places where the type-hinting is at best actively harmful, in `fill_almost_sorted` you can just remove all type specifications and casts inside the loop and everything will work out fine: `gen_range(usize, usize)` will generate a `usize` which you can just pass directly to `swap`, no need to convert back and forth between usize and u32. Likewise `shuffle`. * A few bits of code could be improved with standard macros e.g. `let mut p: Vec&lt;f32&gt; = Vec::new(); p.push(0.0);` =&gt; `let mut p = vec![0.0]` or let mut tmp_str = String::new(); tmp_str.push_str("\n"); tmp_str.push_str(&amp;n.to_string()); f.write(tmp_str.as_bytes()); =&gt; write!(f, "\n{}", n); * you're using debug formatting (`{:?}`) in way more places than you need e.g. `println!("Running with n = {:?}", n);` * `&amp;Vec&lt;T&gt;` parameters are usually considered bad form and to replace by `&amp;[T]` (that's more flexible on the caller side) * there are places where you ref a match parameter for no good reason e.g. match &amp;self.root { &amp;Some(ref node) =&gt; false, &amp;None =&gt; true } where match self.root { Some(ref node) =&gt; false, None =&gt; true } would do, similarly match file { &amp;mut Some(…) &amp;mut None } would be more readable as match *file { Some(…) None } * for matches with a single branch or when matching Options (which you should generally avoid as it has plenty of nice higher-order functions) `if let` can make the code simpler/clearer e.g. match &amp;self.root { &amp;Some(ref node) =&gt; self.gather_heights_rec(&amp;self.root, &amp;mut depth), &amp;None =&gt; {} } =&gt; if let Some(ref node) = self.root { self.gather_heights_rec(&amp;self.root, &amp;mut depth); } or match target_node { &amp;mut Some(ref mut subnode) =&gt; { subnode.insert(new_val); }, &amp;mut None =&gt; { let new_node = Node::new(new_val); let boxed_node = Some(Box::new(new_node)); *target_node = boxed_node; } } =&gt; if let Some(ref mut subnode) = target_node { subnode.insert(new_val); } else { let new_node = Node::new(new_val); let boxed_node = Some(Box::new(new_node)); *target_node = boxed_node; }
It was drafted on the 13th though, so I doubt it's up-to-date.
Actually you can just require &lt;T: PrimInt + Unsigned&gt; http://rust-num.github.io/num/num_traits/int/trait.PrimInt.html PrimInt is always Bounded and it is also NumCast which is [ToPrimitive](http://rust-num.github.io/num/num_traits/cast/trait.ToPrimitive.html). That has the to_f64 that you want.
In the first repository there are also a bunch of utility functions which could be either simplified or removed using the standard library: * `array::swap` is a less generic version of `std::slice::swap` * `check_sorted` can pretty trivially be implemented in terms of `windows`: a.windows(2).all(|it| it[0] &lt; it[1]) 
1) Lifetimes: The Book talks about lifetimes as if they're a property of references, but I find it more natural to think of them as properties of the referred-to values. What am I missing? As for defining them, is it safe to think that for `fn f&lt;'a&gt;(x: &amp;'a)`, `'a` is a function parameter initialised with the lifetime of whatever `x` is going to be referring to? 2) Internal vs external mutability: The language I've found describing it is very confusing. Is it equivalent to C++'s `const std::unique_ptr&lt;T&gt;` vs. `std::unique_ptr&lt;const T&gt;` distinction? Is the 'handle object' relevant in any way? 3) The mechanics of &amp;*some_str still puzzle me. Can someone please break it down for me?
&gt; I'm not aware of an IDE... Wouldn't any interface over GDB do the trick?
Sorry, I don’t know what "version locks" refers to. Maybe not related, but Cargo creates `Cargo.lock` files that record exactly what versions are being used, so that things don’t change unexpectedly. http://doc.crates.io/guide.html has some more explanations.
Well, I used to use emacs (M-x gdb), it's pretty good and offers everything you would expect from a full-featured debugger. But more generally, I know the latest versions of Visual Studio have gdb support, and many other IDEs as well. I don't think it's any harder to build a graphical interface over GDB than other debuggers, really. I'm no expert but the native debugger on VS doesn't look that different from GDB, cli-wise.
&gt; I don't think it's any harder to build a graphical interface over GDB than other debuggers Well, all of those I found parsed GDB's text output and made sense of it (to highlight breakpoints, program counter etc.) which is ugly. Whereas in LLDB, you can actually create a debugger session from _inside_ python since LLDB exports a shared library (and a python module) using which you can do _everything_ without having to depend on the LLDB executable. (The reason why I built [lldb.nvim](https://github.com/critiqjo/lldb.nvim) instead of gdb.nvim) Though they haven't been updating the python interface for years (still uses Py2), and the interface itself is not very Pythonic. Better than nothing...
Re "version lock": &gt; (the inability to upgrade a package without having to release new versions of every dependent package). Cargo does in fact do this. You specify a range for your dependencies, and so when new verisons come out within range, they work. However, as /u/simonsapin mentions, there is a 'lockfile' which does record the exact versions, and so they won't change. But you can run `cargo update` or `cargo update -p some_crate` to say "ignore the lockfile, and please upgrade to the latest versions in range. This gives you reproducible builds while letting you easily upgrade if an important bugfix comes out.
You could call it Oak - Oak bark tannin is used in corrosion control formulas to convert iron oxides (rust) into a protective chemical barrier. I like that name better than 'Cake' :)
You might want to look at my parallel Mandelbort benchmark, where I compare various Rust crates that support parallelism: https://github.com/willi-kappler/mandel-rust (Unfortunately I didn't have much time lately to work on it, but will continue)
If breaking changes are not required, then you can just run `cargo update -p &lt;crate_name&gt;` as soon as a fix is published for the vulnerable package, and you will now have the fix. Intermediate dependencies don't need to be changed at all, assuming they are using normal dependency constraints like `crate_name = "2.3.12"` (which is shorthand for `crate_name = "&gt;= 2.3.12, &lt; 3.0.0"`) or `other_crate = "0.3.1"` (which is shorthand for `other_crate = "&gt;= 0.3.1, &lt; 0.4.0"`). Fixes only need to "propagate" through the dependency chain if they require actual breaking changes, or if some intermediate package has unusually strict semver constraints on its dependencies.
[removed]
It just hasn't been designed and implemented yet: https://github.com/rust-lang/cargo/issues/545
Agreed except it doesn't handle properly autocompletion on extern crate dependencies so far for me on OSX. It has a few gliches on hierarchical cargo file also. Other than that it's the best editor i've seen so far in terms of Rust support. 
When Cargo generates or updates a lockfile, if multiple crates share a dependency, and their version constraints are compatible, then Cargo will choose the highest available version that satisfies all the constraints. When building, Cargo only pays attention to the lockfile of the top-level crate.
Wow, this could be a really important crate for Rust. I gather that MQTT is the IoT protocol, but I have no idea if it gets real use or not. Is this appropriate for any pub/sub use case? Would I want to use it instead of AMQP generally?
Is this really a needed feature in Cargo? I feel like this is a possible first step for falling into a very deep hole. Imagine that I want crate A to have a post_build.rs and a pre_build.rs and crate B uses crate A and then I want to configure that crate B pre_build.rs should be executed before crate A's post_build.rs but after crate C's post_build but only if it is on a specific platform with specific features because otherwise I want something else... things can get complex quickly. I feel like this is a convoluted way to describe your dependency tree and it is not supposed to look complicated. If we really want to have a way to describe the dependency tree then it should be a proper solution and not this ad hoc thing. I never thought of cargo as a general make replacement and I have no idea what it really is supposed to be, so it is possible that I'm wrong about this.
Great, you can still fix the typo I just found then :p
And now I'm ashamed
/r/playrust is &lt;-- thataway
It's adoption is certainly increasing -- AWS has their whole IoT pub/sub gateway they launched recently and it only speaks MQTT. The SDK they are shipping uses MQTT as a result. It also has the benefit of being able to really efficiently convert MQTT-SN messages (much lower overhead protocol for super tiny devices) to MQTT and mux them over a gateway connection. I've just started looking into the protocol for a project at work and it's super exciting to see a Rust implementation!
Will this be the first These Weeks in Rust then?
1) That's not a correct way to think about them. You may have a reference to an object that does not live as long as the object itself (but it can't live longer, of course). For example: ``` let mut x = 42; { let r1 = &amp;mut x; } let r2 = &amp;x; ``` Without that inner scope, there would be an error about a mutable and immutable reference existing at the same time. But the inner scope limits the lifetime of `r1` so it passes the borrow checker. 2) If you have a struct in a mutable binding (like `let mut s = S::new();`) or a mutable reference to one (`&amp;mut S`), then you can modify the members of `S`. If you have an immutable binding (`let s`) or a shared reference (`&amp;S`) you can't. Unlike C++ you don't mark specific members as const or not. That's exterior mutability. The exception to this is interior mutability, where the struct contains a member of type `Cell&lt;T&gt;` or `RefCell&lt;T&gt;` or a few related, less common types. Those types have so-called "interior mutability" because you can safely mutate their contents even through an immutable binding/reference. So a struct may appear to be immutable while invisible updating an internal counter or something using a `Cell`. 3) `&amp;*x` is `&amp;(*x)`, so it dereferences `x` and then takes a shared reference to that. In the case of `String` specifically, it implements `Deref` which overloads the `*` operator, so `*s: str` where `s: String`. Then `&amp;*s: &amp;str`.
I've considered writing a library for [wamp](http://wamp-proto.org/). Do you have any thoughts on MQTT vs wamp? I haven't looked much into MQTT, but I'm *very* familiar with wamp (I use it in a few projects).
For (2), internal vs external mutability is pretty analogous to the C++ notion of "logical const". External mutability maps to not having `const` in C++; you can freely change values. Internal mutability via `Cell` and `RefCell` maps to having `mutable` on a member in C++; you can change those values even through a constant reference/function.
&gt; although from those slides it looks like it's aiming at text-based payloads You can set whatever payload you want. The protocol standard officially supports JSON and MsgPack, though in theory you can have your router use whatever you like. The general flow is to develop your app in JSON so it's easier to debug, then release with MsgPack to reduce network and processing overhead.
You want to go here: /r/playrust Please ensure before posting that the subreddit is actually the one you are looking for...
Can anyone compare Rusty Code to Rust/RustAutoComplete on sublime?
I recently learned that `ref` patterns can be used in formal parameters. If T is a copyable type, these both do the same thing (pass-by-reference)... `fn foo(a: &amp;T)` `fn foo(ref a: T)` with the only difference being that the first one expects the caller to use`&amp;` (or an existing reference) and the second one forbids it. A quick playground experiment shows that if `T` is a *non*-copyable type, the second signature doesn't borrow. It always causes the actual parameter to be moved-then-dropped. Huh. Doesn't seem too useful, which is probably one reason why I haven't seen it in the standard library. Are we stuck with moving being the default behavior, and `&amp;` on every function call that only wants a borrow? Is there a way to write a function for copyable or non-copyable types that treats T and &amp;T the same way? (Thus encapsulating pass-by-reference vs pass-by-value as an implementation concern.) 
&gt; If T is a copyable type, these both do the same thing (pass-by-reference) It might look like that but, this is not true. The 2nd signature `fn foo(ref a: T)` is 100% pass-by-value. However, inside of the body of the method, a is a `&amp;T` in both instances. &gt; Are we stuck with moving being the default behavior, and &amp; on every function call that only wants a borrow? Yep. This is an explicit design goal of rust. &gt; Is there a way to write a function for copyable or non-copyable types that treats T and &amp;T the same way? You can't treat `T` and `&amp;T` in the same way in rust. &gt; Thus encapsulating pass-by-reference vs pass-by-value as an implementation concern It's possible to do, but it uses traits. Here's an example from the Iterator trait: impl&lt;'a, I&gt; Iterator for &amp;'a mut I where I: Iterator + ?Sized This means that every `&amp;mut I` is an `Iterator` if `I` by itself is an `Iterator`. The reason that this is cool is that you can have a function fn foo&lt;I&gt;(iterator: I) where I: Iterator&lt;Item=i32&gt; {} and that function can take it's iterator by value, *or* by mutable reference.
Even if the current editors stepped down I doubt TWiR would ever be outright discontinued. It's already a community project, all that would be needed would be to find new volunteers to pick it up (which has already happened at least once, remembering that cmr produced the first fifty or so entirely on his own).
I think you meant to post this to /r/playrust
# CheckTheSubredditBeforePosting # AlsoDontShoutWeCanHearYouJustFine # IncidentallyThisIsMarkdownNotTwitter
Absolutely :)
What is e0505? I'm using capn proto and see E0505, but google can't seem to find this error any where. cannot move out of `message_reader` because it is borrowed [E0505] I tried a jan nightly, and an April 20, 2016 nightly. Both say E0505. 
I suppose but it seems like circumventing a lot of cargo then.
&gt; Yep. This is an explicit design goal of rust. Ah well. Rust is still better than not-Rust, warts and all. This is warty because, for values that cannot be mutated, references and values are semantically equivalent. They're *so much* equivalent that functional languages (Haskell etc.) conceptually pass (immutable) values only and guarantee that you won't notice if the compiler substitutes references. If I may ramble for a bit, I really do like Rust, but sometimes I run across things and think "now, how will I explain this one to someone I'm teaching?" For example, this let sum = 0; for i in 0..100 { sum += i; } vs. let sum = 0; for i in slice { sum += *i; } Especially since the reason is: &gt; Slices can contain types that have to be moved. If you were iterating over that kind of slice the second loop would incorrect and should generate `cannot move out of borrowed content`. To help remind the programmer of this possible error, Rust makes you perform the last dereference explicitly Okay, yes, I sorta almost believe that. But my clever student says &gt; So, why not add a coercion rule that maps `&amp;C` -&gt; `C` where `C: Copy + Sized`? It would make lots of things Just Work, and if C is a large type the compiler is going to turn it into a pass-by-reference anyway. I dunno. The rule is `&amp;&amp;T` -&gt; `&amp;T`, just because. Anyway, in a case of answering my own question, I [found this](https://github.com/rust-lang/rfcs/blob/master/text/0241-deref-conversions.md) and I'll try to digest it. (edit: water under the bridge, but using the `move` keyword to invoke the relatively less common functions that take ownership of an argument - e.g. `vec.push(move x)`- would have been elegant)
This is a great example, thank you!
[Not at all](https://cmr.github.io/blog/2013/11/09/these-weeks-in-rust/).
This is very interesting. I'm curious why it wasn't made possible to replace a version of a crate with another version -- it's directly acknowledged in the pull that this is something you might want to do, which is explicitly not supported. One thing I've wanted to do in the past, which I couldn't figure out how -- but maybe it's already possible -- is the following: Constrain the version of a particular crate, ANYWHERE in the transitive dependency tree, to be a particular version or range of versions. (Of course this can be done by munging Cargo.lock directly, but that's not a great approach....) We ran into this because an indirectly-dependent crate broke semver and stopped building (with our rustc version) in a minor update.
E0505 is a Rust compiler error code. In theory, for every error code there is a detailed explanation of the error [here](https://doc.rust-lang.org/error-index.html). Unfortunately, there is no explanation for E0505 yet. If you can post code demonstrating the error, hopefully someone will be able to provide a bespoke explanation for you.
There's a good reason for this "wart": Rust has an explicit goal of zero-cost abstractions, and while LLVM is able to elide some copies, this is not the general case. By making `Copy` the exception, rather than the rule, that cost is made explicit and can be contained by careful coding. Apart from that, the default non-`Copy` also allows `Drop` by default, which is the magic sauce that makes implicit RAII in Rust possible. Also note that `&amp;T` implements `Copy`, so if you `move &amp;T`, are you moving the `&amp;T` or the `T`?
&gt; (edit: water under the bridge, but using the move keyword to invoke the relatively less common functions that take ownership of an argument - e.g. vec.push(move x)- would have been elegant) We tried it; it wasn't.
Visual studio code with the gdb/lldb extension
Any time. I'm still not totally happy with this Makefile, but haven't had enough time...
You can of course use [clippy](https://github.com/Manishearth/rust-clippy) and `#[deny(shadow_reuse, shadow_same, shadow_unrelated)`, but that'd require nightly Rust. Or at least use it (via [rustup](https://rustup.rs) and `rustup update nightly`) with cargo clippy (`rustup run nightly cargo install cargo-clippy; rustup run nightly cargo clippy`) and `-W shadow_same -W shadow_reuse -W shadow_unrelated` to get warnings. For the record, I used to worry, too, which is why I wrote the `shadow_*` lints. Today, I'm pretty relaxed about it.
You can actually do something like this in a round-about way with `str::from_utf8`: as [the error it returns](http://doc.rust-lang.org/std/str/struct.Utf8Error.html) indicates the prefix which is valid UTF-8, so one can use that to compute a length for which `from_utf8` will succeed (aka for which the unchecked version will not be skirting undefined behaviour) and the count codepoints etc on that. If double processing the array isn't fast enough, [the internals of `from_utf8`](https://dxr.mozilla.org/rust/rev/a5d1e7a59a2a3413c377b003075349f854304b5e/src/libcore/str/mod.rs#1094) are a good (and efficient!) place to start with a custom version.
Alternatively, there's bjorn's [fast and economical UTF-8 decoder](http://bjoern.hoehrmann.de/utf-8/decoder/dfa/) though it's a bit weird. In my tests it's quite a bit slower than [the stdlib's decoder](http://doc.rust-lang.org/src/core/up/src/libcore/str/mod.rs.html#328-330) but it includes validation, I don't know if `str::next_code_point` would be as fast if it had to validate its input.
Perhaps I've misunderstood the problem. OP has some stream of bytes and wants to extract Unicode snippets?
I think sqlite3 does in fact have such an interface, and generating the correct byte code for their VM without parsing should save *some* time. Edit: Apparently Diesel doesn't even need that to somehow be faster.
Isn't SQLite just a C interface basically?
&gt; I don't know if Diesel automatically prepares and caches all queries. All that are "safe" to do so (we don't cache queries containing an `In` node, or a `SqlLiteral` node, as those two could result in unbounded memory growth). We also don't cache non-select statements, as they're generally not a bottleneck. &gt; An other interesting possibility might be a non-text interface to the database The majority of the cost isn't actually in parsing, its in the query planner anyway.
Yes (which sucks sometimes)
&gt; allegedly faster We do have benchmarks if you're skeptical. ;)
You can call `.as_str()` on `Chars` to get the remaining slice. You can then substract lengths or pointers to get the number of consumed bytes.
Thanks for the answers. But if the lifetime refers to the reference, then what's the point of having `fn f&lt;'a, 'b&gt;(x:&amp;'a, y:&amp;'b)` ? Both refs are parameters, so both their lifetimes are going to be the same as the overall function's, so why have separate lifetimes for each? As for 3), this is the first time I've seen a `str` without its ampersand. Can you really have a plain `str`? This (`&amp;*`) is the only place in the language I've encountered, where there being a plain `str` matters.
I found the [benchmarks](https://github.com/diesel-rs/diesel/pull/283): name old ns/iter new ns/iter diff ns/iter diff % pg::connection::tests::benchmarks::prepared_statement_lookup_query_builder 1,048 191 -857 -81.77% pg::connection::tests::benchmarks::prepared_statement_lookup_raw_sql 319 318 -1 -0.31% sqlite::connection::tests::benchmarks::prepared_statement_lookup_query_builder 743 123 -620 -83.45% sqlite::connection::tests::benchmarks::prepared_statement_lookup_raw_sql 183 180 -3 -1.64% [And here is the benchmark script](https://gist.github.com/sgrif/24ed31f66246d71a2f7356a806ebc57a). So I guess the performance gain is because it's quicker for Diesel to construct a query string than it is to do nothing? If the benchmark isn't hitting the database, what is the [prepare](http://www.postgresql.org/docs/current/static/sql-prepare.html) statement doing?
No, viewing pointers as integers is fine, it's going the other way---creating a pointer from an integer (and then dereferencing/reading from)---that risks memory unsafety.
One can also recompute it by summing the [`len_utf8`s](http://doc.rust-lang.org/std/primitive.char.html#method.len_utf8) of each yielded `char` (less efficient of course). *e*: and I shouldn't forget to mention [`char_indices`](http://doc.rust-lang.org/std/primitive.str.html#method.char_indices)!
The database does get hit in order to create the prepared statement, we just aren't executing it.
Prepared statements have costs so I hope their usage can be controlled. Prepared statements use a small amount of memory at the database, can make the database use a worse plan due to not knowing the parameters at plan time, and do not play well with some connections poolers (PgBouncer, I am looking at you).
I fully agree that `from_utf8_unchecked` is not the correct solution here. Disabling UTF-8 parsing can be done on subexpressions – so one should be able to parse a byte stream, enable UTF-8 within some subexpression that is captured and get out a `&amp;str` slice from the capture. Caveat emptor, I haven't tried this yet. Perhaps /u/BurntSushi can help?
&gt; Disabling UTF-8 parsing can be done on subexpressions I think you meant to say *en*abling UTF-8 parsing? Either way, it seems a byte regex with enabled unicode support would work&amp;mdash;something like `(?u:.{n})`&amp;mdash;although this is likely to be ridiculously slow if the number of code-points `n` is dynamic (string formatting plus regex compilation for each parsing), or maybe one could just forgo enforcing that count initially with `(?u:.*)` and then later trim to the prefix of the desired `char` count (a downside with this, like with reading the error from a `from_utf8` call, is wasting time overvalidating, if there's a long suffix that happens to be valid UTF-8 but isn't of interest).
You could use termbox or ncurses bindings: https://crates.io/search?q=termbox https://crates.io/search?q=curses or you could use https://github.com/Ticki/termion
Html5 link http://player.twitch.tv/?html5&amp;channel=seantheprogrammer
1. It's not about the lifetimes of references vs. lifetime of struct. It's about whether Rust will understand that these two references have different types. It actually doesn't matter if you only read from those references. It matters when you try to store them somewhere. Take a look [at this example](https://play.rust-lang.org/?gist=a923e7b24690bd7c43d0f8c15a79c39f&amp;version=stable&amp;backtrace=0). This compiles fine, but if you uncomment the commented line, your program would create a dangling pointer, so Rust doesn't let the program compile. But, if you made made `struct Refs&lt;'a&gt; { ... }`, Rust couldn't differentiate between the types `refs.a` and `refs.b` (because you said that they're both using the same lifetime `'a`). So Rust has to make a safe assumption -- that both `refs.a` and `refs.b` live as short as `inner`, and thus both assignments are illegal. 2. The same as with the struct, if you're just going to read from those references, there's no difference. It only matters if you want to store those references somewhere. But this function doesn't have any third parameter (eg. self) where you could store those references, so here, it totally doesn't matter. 3. It says that the returned reference will have the same lifetime as argument -- that means that: * the caller may assume that the returned integer will live at least as long as the integer from the argument and also * the implementation must guarantee that behaviour.
Same, I don't really actively use it for anything, but I love reading what people are working on with rust.
There is a CharIndices iterator which does that, and actually does so by [wrapping a Chars and checking by how many bytes the underlying slice iterator advanced for each `char` yielded](http://doc.rust-lang.org/src/core/up/src/libcore/str/mod.rs.html#473-476). 
The use of generic syntax `&lt;...&gt;` is not a coincidence, all of the cases described are type-level parameters and can be described as: 1. `struct Stuff&lt;'a&gt; { a: &amp;'a i32, b: &amp;'a i32 }` means "every `Stuff` contains two references, and the `i32`s they point to must be valid for the same lifetime, which is the `'lt` in `Stuff&lt;'lt&gt;`", while `struct Stuff&lt;'a, 'b&gt; { a: &amp;'a i32, b: &amp;'b i32 }` means "every `Stuff` contains two references, each with their own lifetime for which the `i32` inside is valid, and they can be specified independently". The consequence is that the latter definition is strictly more flexible, as `type Stuff1&lt;'a&gt; = Stuff&lt;'a, 'a&gt;;` gives you the same behavior as the former definition. With the two independent lifetime parameters, one can have a method taking `&amp;'a mut self` and returning `Stuff&lt;'a, 'static&gt;`, which means that the `a` field borrows the value the method was called on, while the `b` field can be kept around forever. 2. In that specific example, because you're not actually using `'a` or `'b` more than once, both examples are equivalent to `fn func(a: &amp;i32, b: &amp;i32)`, i.e. the two reference types in the arguments have anonymous lifetimes and the function can't keep them beyond its own body (i.e. it can't put them in a global or in a detached thread, as those require `'static`). If you were to, say, return one of `a` or `b`, the definition with two lifetime parameters forces you to choose one of `a` or `b` at the type level, i.e. if you write `-&gt; &amp;'a i32`, you can only return `a`, and if you write `-&gt; &amp;'b i32`, you can only return `b` (or, in either case, some unrelated `&amp;'static i32`). Whereas the definition with one lifetime lets you use either of the references as the returned value because they have the same type, and that single lifetime is effectively the shortest of the two lifetimes your function gets called with. The drawback there is that the result of the call keeps borrows for both arguments alive, at the call site. To exemplify, `fn func&lt;'a&gt;(a: &amp;'a i32, b: &amp;'a i32) -&gt; &amp;'a i32 {...}` `let mut a = 0; let mut b = 1;` `{ let c = func(&amp;a, &amp;b); /* can't mutate a or b here */ }`. 3. That means that the function can be called with a reference to an `i32` of any lifetime, and the result is a reference that is valid *for that same lifetime*. It can be some `&amp;'static i32`, like I mentioned above, or the argument. If you had some other arguments and `'a'` was known to be no longer than the lifetime of some `i32`in those arguments, you could also return those.
They can live longer than the other, but you *can't* know which is longer, so you effectively get to work with the shortest of them in a generic way. It's always safe to store `a` in `set`, technically speaking, the problem arises where `set` may be used, because a `HashSet&lt;&amp;'a T&gt;` is only required not to live longer than `'a`, and if any of the elements points to something shorter than `'a`, you'd be accessing data which is no longer valid (or not even part of memory, which would crash). Even if the memory is still valid, you can also lose track of borrows this way. This also illustrates a less obvious property of lifetime parametrism: `&amp;mut HashSet&lt;&amp;'a T&gt;` is "invariant" over `'a`, like `&amp;mut &amp;'a T`, but opposed to `&amp;'a T`: that means you can't shorten `&amp;'a T` if it's behind a mutable reference, because it would allow writing some reference pointing to a value of a shorter lifetime in there. `&amp;mut &amp;'static i32` can't be `&amp;mut &amp;'a i32` unless `'a = 'static`. If it could, I could put some `&amp;temporary` in there, then extract it where I know it to be `&amp;'static i32`, pass it to another thread, and then end the current one, which would be pretty bad.
Yes, but I was talking about the implementation of these cell types, hence my wrappers aside remark. Looking this stuff up, it looks like UnsafeCell is only useful in unsafe mode. I assume the other stuff is implemented in terms of it.
by the way, there's also a compiler option for this `rustc -Z print-enum-sizes`
eddyb is referring to concepts of [subtyping and variance](https://doc.rust-lang.org/nomicon/subtyping.html). In simpler terms, since `set` is like `&amp;mut T`, the lifetime of `a` _must_ be at least as big as the scope of the owner of `set` (from which it was borrowed). But if the function was this: fn contains&lt;'a&gt;(a: &amp;'a i32, set: &amp;HashSet&lt;&amp;'a i32&gt;) -&gt; bool { set.contains(a) } then even if `a` did not live as long as the owner of `set`, it should be ok (the elements' lifetime will _appear_ to have shrunk inside that function)... See [this example](https://play.rust-lang.org/?gist=a5b955e3308493275086e518c8495816&amp;version=stable&amp;backtrace=0). Each successive `insert`s inserts a reference of smaller lifetime than the elements it already contains, but it's okay as long as the owner's scope is smaller. And `&amp;d` is something that cannot be inserted into the `set`. _EDIT:_ updated example
Just brainstorming here, but wouldn't our TypeId stuff be able to solve that (at least for 64 bit platforms)? Instead of taking 0 and 1 to represent our enum variants, we'll just take two available numbers from our TypeId pool. (I'm not sure I would recommend it in general, as I suspect this will lead to bigger code size, but just as a thought...)
I'm confused because Rust's ref types seem very unlike the refs of other languages. Maybe similar to C++ refs, but not enough to be comparable imo. Anyway, given this code: fn f&lt;'a, 'b&gt;(x: &amp;'a X, y: &amp;'b X) { } let a = X{}; let b = &amp;a; f(&amp;a, b); How are the refs passed in? Are they copied into f's locals, lifetime and all (1), moved (2), or are f's parameters references to the passed-in references, i.e. &amp;(&amp;a), &amp;(b) (3)? If it's case 1, then does that mean that `y`, having the same lifetime as `b`, is going to live for as long as `b`, even though f's execution will have ended by then? If it's case 2, wouldn't that imply the destruction of `&amp;a` before entering the function? I've looked at the Rust lang ref, and it says that temporaries live until the end of the enclosing statement. I've only added case 3 for completeness's sake, as it makes little sense to me. As for str, thanks for the heads-up, I'm going to check out what unsized types are about. ~~You typed in single quotes instead of backticks at the end btw.~~
Well, I suppose it is safe. It still feels... Immoral. Pointers are for C programmers and OS designers.
&gt; But won't that always be the lifetime of the function, given that it's a parameter? No. But first: the scope of the function is a different lifetime every time the function is called; so even if that were true it would still be different each time. Here are some examples where the lifetime of the reference is different from the scope of the function: let y = 0; let x = &amp;y; foo(&amp;y); foo(x); For the first invocation of foo, `'a` is just the length of the call. But for the second, it begins at `let x = &amp;y;` and ends at the end of the scope, because its the lifetime of `x`. There are ways this gets more complicated, but this example hopefully is enough to show that the lifetime `'a` isn't the lifetime of the function. &gt; Like, if the pointee lives in a different thread? More commonly it occurs if the pointee would be deallocated before the lifetime would. A very trivial example (which is honestly a bit silly): let x: &amp;i32; let y = 0; x = &amp;y; Because variables are deallocated at the end of the scope in reverse order of their declaration,`x` lives longer than `y`. This one is easy to avoid, but it can sometimes happen in more complicated ways.
(hopefully not a too stupid question)... I searched book and SO, but couldn't come up with a answer: Basically I've five different structs. Is there any way how I could insert them in a vector? (Something like e.g. `List&lt;Object&gt;()` in Java.). Right now I use a another struct to encapsulate the five struct's and each one is marked as `Option`and an enum marks which one is set.
Shouldn't [`#[repr(packet)]`](https://doc.rust-lang.org/nomicon/other-reprs.html#reprpacked) do what the OP wants? Or it works only for structs?
Only for structs :(
The first step would be "RFC: repr(packed) for enums"
&gt;But for the second, it begins at `let x = &amp;y;` and ends at the end of the scope, because its the lifetime of `x`. Assuming that `x` gets copied into `foo`, why is `x`s lifetime still relevant? Shouldn't `foo` then have a perfectly working local reference to `y`, independent of `x`? Your main point here did get across though - different args, different lifetimes. As for your second example, I was aware of it before, but was thinking more in terms of function calls - you can't pre-declare function arguments: let foo::x : &amp;i32; // can't do this let y = 0; foo(x = &amp;y);
I think `bytes::Regex` is useful for extracting UTF-8 snippets out of a `&amp;[u8]` for some set of predefined patterns. The OP somewhat suggests they might want to do that, but the OP seems to be lurking more closely to writing a parser, which might mean regexes probably aren't as good of a fit. Without more requirements from the OP, it isn't really clear. I think what /u/dbaupp said is otherwise accurate.
There have been attempts to allow for loading/unloading code at runtime, however those should be deemed *very* experimental. The big problem is that Rust as of now has no defined ABI.
Rust uses semver, so it's just gonna go to 1.10, not 2.0. Think of the version number not as a real number, but a list of integers separated by a dot.
You can't write `const A: i32 = b;`.
Rust 1.9.0 will be followed by Rust 1.10.0, because version numbers aren't decimal fractions, just like IP addresses aren't decimal fractions. :P See also the Linux kernel, which is on something like version 3.18.687. There are no concrete plans for 2.0 at the moment, as first we'd probably want to exhaust all of the opportunities to advance the language that don't require breakage. Several years off, at the earliest. There are some tentative things listed at https://github.com/rust-lang/rust/issues?q=is%3Aissue+label%3Arust-2-breakage-wishlist+is%3Aclosed , though this is not a comprehensive list.
This is a [lint](https://github.com/Manishearth/rust-clippy/wiki#let_unit_value) in clippy. Clippy has many, many excellent warnings, and can be installed for use in your projects like any other crate. It does require nightly rust, though, since it is a compiler plugin.
_academic reader high five_!
The Rust support isn't better than Atom, however.