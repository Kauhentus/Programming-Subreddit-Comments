Yeah, I had to just decide to go with something and build up the code for it. I'm working on a pair of blogposts today that explain how I went from 0 to HTTP serving like that. There's actually a lot of very simple code powering all of this. 
It's been a ride. Favorite project to work on so far tbh.
Yeah, I'm working on a network card driver for nebulet now. Once, that works, I can use smoltcp as the network stack and have it serving and receiving tcp soon after that.
Thanks. Even if I could see the entire type, I'm not sure it would tell me what I want to know. I tried reversing the order of `Map`'s type params to see what closures look like, and the part of the type I see is: &lt;Map&lt;[closure@src/main.rs:53:28: 56:10 f:[closure@src/main.rs:76... My real question is how to compute the type-length of the closure type.
Fantastic. I don't mind the `.zip(array.par_iter())` method calls. It reads a bit more explicitly to me. But it's great to know different ways to do things. Again, thanks so much for all your help :)
I think you meant https://github.com/actix/examples/tree/master/diesel ? :)
Anyone who is currently considering clicking on this link: Just go to https://os.phil-opp.com/ directly, the post has basically no further content
Which other programs use simd for fannkuch-redux ?
You're running into a fundamental limitation of &amp;mut references. If you have something like a `&amp;'a Vec`, it's perfectly fine to pull out references that live longer than 'a. But if you have a `&amp;'a mut Vec`, it's not fine, because that `&amp;mut` is supposed to be the unique reference to its contents. There's a similar problem with a more detailed answer here: https://www.reddit.com/r/rust/comments/931kel/hey_rustaceans_got_an_easy_question_ask_here/e3mngte/?context=1 Another way to understand the problem the compiler has here: If your code worked as written, would you be able to pull out two &amp;mut references to the same item in the Vec? I believe you would, and that would violate all the guarantees of safe code.
Actually, I thought that the comment section is quite interesting. Reading that, I found out about [monotron](https://github.com/thejpster/monotron).
&gt; The futures async/await support is actually disabled again Are you saying that the two async examples in the link do not work?
Ah, that's fair; nostd does still have all language features. But you still end up losing parts of the standard library (including some like `Read`/`Write` traits which are not in core for [reasons that are not obvious](https://github.com/rust-lang-nursery/portability-wg/issues/12), though hopefully that could change). And in general only crates specifically designed with nostd in mind will work. But again, this isn't entirely avoidable. And if D has anything similar I would expect it has similar issues. I haven't really used D.
Yeah, the Rust Evangelism Strike Force needs a few moments to get going, what with the compile times and all. :-)
You can create a `BufRead` out of a `Read` by wrapping it in a `BufReader`, if you want lines, for example. Also note that `Read` has a read method: `fn read(&amp;mut self, buf: &amp;mut [u8]) -&gt; usize` which fills up the given buffer &amp; returns the number of bytes read.
The async/await one doesn't work anymore unless you use my futures 0.3 branches of everything. The other one works fine, also on stable.
So much negativity and overreaction (-42 downvotes, really?) to an amusing way of saying, "Hey, that kinda sucks, this sounded like a neat project". Wow! Easily triggered.
I changed the `impl MyTrait&lt;...&gt;` to `Wrapper&lt;Map&lt;T, impl Fn(Option&lt;A&gt;)-&gt;Option&lt;B&gt; &gt;&gt;` and the example worked, even with a couple more opt_map's thrown in. (It timed out on the playground, but only took a couple dozen seconds to run locally. It took 2 minutes with 2 extra opt_map's added.) Perhaps type size is the actual memory used to store the type information, and impl Trait used in your way can trigger exponential space? In any case, the compile time on this is still embarrassing, and likely to be your next bottleneck. FWIW, I forced a type error (with my change in place), and this was the type returned: Wrapper&lt;Map&lt;Map&lt;Map&lt;Map&lt;Map&lt;Map&lt;Map&lt;Map&lt;Map&lt;Map&lt;Map&lt;Map&lt;Map&lt;Map&lt;Map&lt;Map&lt;Map&lt;Value&lt;std::option::Option&lt;{integer}&gt;&gt;, impl std::ops::Fn&lt;(std::option::Option&lt;{integer}&gt;,)&gt;&gt;, impl std::ops::Fn&lt;(std::option::Option&lt;{integer}&gt;,)&gt;&gt;, impl std::ops::Fn&lt;(std::option::Option&lt;{integer}&gt;,)&gt;&gt;, impl std::ops::Fn&lt;(std::option::Option&lt;{integer}&gt;,)&gt;&gt;, impl std::ops::Fn&lt;(std::option::Option&lt;{integer}&gt;,)&gt;&gt;, impl std::ops::Fn&lt;(std::option::Option&lt;{integer}&gt;,)&gt;&gt;, impl std::ops::Fn&lt;(std::option::Option&lt;{integer}&gt;,)&gt;&gt;, impl std::ops::Fn&lt;(std::option::Option&lt;{integer}&gt;,)&gt;&gt;, impl std::ops::Fn&lt;(std::option::Option&lt;{integer}&gt;,)&gt;&gt;, impl std::ops::Fn&lt;(std::option::Option&lt;{integer}&gt;,)&gt;&gt;, impl std::ops::Fn&lt;(std::option::Option&lt;{integer}&gt;,)&gt;&gt;, impl std::ops::Fn&lt;(std::option::Option&lt;{integer}&gt;,)&gt;&gt;, impl std::ops::Fn&lt;(std::option::Option&lt;{integer}&gt;,)&gt;&gt;, impl std::ops::Fn&lt;(std::option::Option&lt;{integer}&gt;,)&gt;&gt;, impl std::ops::Fn&lt;(std::option::Option&lt;{integer}&gt;,)&gt;&gt;, impl std::ops::Fn&lt;(std::option::Option&lt;{integer}&gt;,)&gt;&gt;, impl std::ops::Fn&lt;(std::option::Option&lt;{integer}&gt;,)&gt;&gt;&gt;
Question, do gtk apps work on Mac and windows like qt apps?
Someone asked about Box&lt;Trait&gt; and asked if there was a name for that. Isn't the name for that "Trait Objects"? Or has that terminology been abandoned?
And here you are, commenting.
Hmm, sure that Gtk has accessibility on Windows? I need to check this out now then. Thanks a lot. 
I use GIMP on windows. As GIMP is gtk.
Does that mean that windows has native GTK support, or does GIMP just kinda bundle the entire gtk library in the program?
&gt; What makes you think that high performance and systems programming are at odds? A programmer that has very tight control over the machine upon which it is running may be able to perform some operations much more efficiently than one which doesn't have such control. In such cases, the compiler and language do not provide performance--they provide the control with which the programmer can achieve the performance. Compare that situation to e.g. a programmer who needs to perform a sequence of operations on a number of rather large matrices. In many such situations, programmers don't want to have to think about how such operations might be most efficiently converted into a sequence of vector instructions on the target CPU, especially since the most efficient way to perform the required operations might change multiple times over the lifetime of the code (e.g. because of hardware upgrades). The goal of a high-performance compiler is to take code which is written to describe all of the operations that will need to be done, and figure out how to rearrange and consolidate those operations so as to best fit the target platform's capabilities. As a simple example, consider the following: void foo(int *ip, float *fp) { for (int i=0; i&lt;10000; i++) { ip[i] = 0; fp[i] = i*2.0f; } } On some platforms, if none of the items accessed via `ip` and `fp` alias, it might be most efficient to interleave the operations precisely as shown, or consolidate all of the writes to `ip` into a single block-clear operation, or consolidate groups of four or eight operations on `fp` to exploit SIMD instructions, or perhaps alternate between doing cache-line-sized operations on `ip` and `fp`. A high-performance compiler would be expected to take the above code and rearrange the operations into whatever sequence would be most efficient on the target. I don't think there's any reason why compilers intended for high-end programming should be completely unsuitable for systems programming, nor any reason why compilers intended for systems programming shouldn't be able to perform many useful optimizations. With the code as written above, even a compiler that's intended to be suitable for low-level or systems programming may be reasonably entitled to assume nothing accessed through `ip` will alias anything accessed through `fp`. If, however, code had done something like: void pegValues(double *array, uint32_t n, double pegValue) { if (pegValue &lt; 0) pegValue = -pegValue; // Get top half of bit pattern of pegValue, with sign bit masked off uint32_t pegHi = ((uint32_t*)pegValue)[1]; uint32_t *p = array+1; // Upper word of first element for (uint32_t i=0; i&lt;n; i++) { // Get upper part of array element i, mask off sign bit, and compare if ( (*p &amp; 0x7FFFFFFF) &gt;= pegHi) { double *dp = (double*)(p-1); double thisValue = *dp; if (thisVal &gt; pegValue) *dp = pegValue; if (thisVal &lt; -pegValue) *dp = -pegValue; } p+=2; } } a compiler intended for low-level programming should treat the cast from `double*` to `uint32_t*` as an indicator that succeeding operations on `uint32_t*` might involve the same storage as operations on `double*`, and it should avoid reordering such operations unless it fully understands how they might behave on the target platform. The purpose of the (untested) function is to peg values in an array of `double` to the range +/- pegValue, in a case where most elements are expected to be within range. On a 64-bit implementation, it would probably be faster to compare the absolute value of each element against pegValue, skipping the 32-bit comparisons. On a 32-bit implementation, however, the outer `if` check could likely shave an order of magnitude off the time that could be produced without it. As said before, I would not regard the fact that a function like the above could perform well even on a system with no FPU as an indication that the language or implementation is efficiently processing floating-point numbers, but rather that it is allowing the *programmer* to perform the operation in an efficient manner. 
Will the new global allocator work make it easier to use the standard library for bare metal (OS) applications?
So this is like the cgi-bin stuff, just translated to WebAssembly?
Yes, the next step is to have this fed by Kafka or Redis Streams instead of HTTP.
https://github.com/internet-of-plants/server
As I explained on rust-users, there's a (not submitted) single-core version. Those are all I know of.
Sorry for stupid question, but I did not get what does it mean. Does it mean no more "This week in Rust" posts on Reddit? 
&gt;That's true, but since the length of the type grows exponentially Linearly, no?
Yes, I believe it is (and someone mentioned it later on but it wasn't recorded)
I remember that I switched to my linux when I am playing with gtk-rs but I couldn't remember why.
It seems many crates are in this situation right now, but async/await in stable Rust is still relatively far away. Based on your experience, what do you think is the best approach here?
&gt; Something like deadline on encoding time per frame? [It already has this.](https://stackoverflow.com/a/30467104/3878168)
Certainly an issue with the dlls. Everyone has this issue: just put them in the same directory and it should work just fine.
Small ones but we still have a long way to go. You can find such changes in [here](https://github.com/gtk-rs/examples/pull/188).
It allows you to use some gnome libraries in rust, not sure if it is what you call a wrapper...
perfect, thanks
Windows support is theoretically there but very poor; it is quite difficult to distribute GTK programs on windows.
There is actually a guide on how to do that: http://gtk-rs.org/tuto/cross
Hi everyone! I'm building a toy web scraper mostly as a learning exercise for the new async/await syntax. The code works and is clearly running in parallel, but I just wanted to post it here to see if I'm doing anything obviously wrong. 😊 &amp;nbsp; The main async logic is as follows:&amp;nbsp; async fn hyper_async(n: u32, state: Arc&lt;Mutex&lt;Shared&gt;&gt;) -&gt; io::Result&lt;()&gt; { let client = Client::new(); let url: Uri = format!("{}/{}", BASE_URI, n).parse().unwrap(); await!(client.get(url) .and_then(|res| { res.into_body().concat2() }) .map_err(|e| println!("Error: {:?}", e)) .and_then(|chunk| { let doc = Document::from_read(&amp;chunk[..]).unwrap(); let links = doc.find(Class("sprite")) .filter_map(|ele| ele.attr("href")) .collect::&lt;Vec&lt;_&gt;&gt;() .iter() .map(|uri| uri.to_string()) .collect::&lt;Vec&lt;String&gt;&gt;(); state.lock().unwrap().extend_links(links); Ok(()) })); Ok(()) } fn hyper_run(state: Arc&lt;Mutex&lt;Shared&gt;&gt;) { tokio::run_async(async move { for n in 1 .. 10 { let state = state.clone(); tokio::spawn_async(async move { if let Err(_) = await!(hyper_async(n, state)) { eprintln!("Hyper Async -- failure!"); } }); } }); } And [here is a link to the entire program](https://gist.github.com/michael-grunder/c5d6a8d0742499683c708be87b2cf8e7), and to the [Cargo.toml](https://gist.github.com/michael-grunder/743a6abc2a92499ee3ad5e62f6dcbc80)
I like it! :)
Thanks, I've found that with `BufReader`, the number of read calls is small enough that using a box doesn't affect performance in any noticeable way!
Dependency injection in Rust is a bit of a different beast compared to languages like C# that are more well suited to indirection. Something you could do is make `MyStruct` generic over its `File` rather than using trait objects, which will then propagate through your code wherever you use it. The type signature becomes more complex, but I find by keeping structure nesting minimal and passing them as arguments where possible lets you hide a lot of those complex types behind other traits when they're passed around. You could also create a non-generic type alias for your generic type that expects a `File` if you only need a different type in unit testing `MyStruct` itself. Alternatively, you could define your own trait that provides the functionality you need, and implement it for any `Read + Write + Seek`. You could then use it as a trait object but would need to decide upfront whether your structure owns or borrows its input (through `Box&lt;dyn Trait&gt;` or `&amp; dyn Trait`). As a wider pattern though, the way I've implemented DI in Rust previously has been through a more service-locator-esc approach than the sophisticated containers we're spoiled with in other languages. I have an (unfortunately a little outdated now) [example application](https://github.com/KodrAus/rust-web-app) that uses it. I have some notes in the readme there about how it's structured.
Are you allowed to add service workers? Because that is a serious security issue. You can then run your javascript on any other docs hosted on docs.rs 
[https://github.com/ryanmcgrath/jelly/](https://github.com/ryanmcgrath/jelly/) - though I'm biased, and it's from a (slightly) older actix-web. Shows sessions/users/etc tho.
You can take this one step further, avoiding the user requirement to import any traits or awkward syntax [playground](https://play.rust-lang.org/?gist=8e34a2141ce6b0f875f87f2780361d81) mod details { // Need a type to implement traits pub struct Foo; // The trait implementing the overloads through generic parameters pub trait Overload&lt;T, U&gt; { fn overload(self, tee: T, yu: U); } impl Overload&lt;i32, bool&gt; for Foo { fn overload(self, tee: i32, yu: bool) { println!("i32 and bool"); } } impl&lt;'a&gt; Overload&lt;&amp;'a str, f32&gt; for Foo { fn overload(self, tee: &amp;'a str, yu: f32) { println!("&amp;str and f32"); } } } // The magic of where clauses to hide the implementation details pub fn overload&lt;T, U&gt;(tee: T, yu: U) where details::Foo: details::Overload&lt;T, U&gt; { details::Overload::overload(details::Foo, tee, yu) } fn main() { overload(42, true); overload("Hello world", 13.0); } 
It means inside the TWiR post, there are no longer links to the reddit comment sections next to each of the blog posts it links.
ping /u/nightcracker (the talk mentions `slotmap` around 34:20)
I agree. Also: SQLite is actually used as a backing store for mentat.
This is the technique that I use, its a bit of a code smell as one normally shouldn't need overloading but in certain circumstances it is useful. For instance I am using it to define a dot and wedge product between different grade basis elements of a vector space. It's really nice how the Rust type system can determine what the result type of a calculation will be without having to work it out by hand. 
What improvements are you thinking of specifically? \`Rc&lt;RefCell&lt;\_&gt;&gt;\`-like patterns are a result/requirement of the callback-based nature of GTK for events. While still staying close to the GTK API there's not much that can be done about this, but you might want to take a look at \[Relm\]([https://github.com/antoyo/relm](https://github.com/antoyo/relm)) which tries to solve this by providing a higher-level API on top of this.
In a different, non-child module where `Abs` is *not* in scope: #[inline(always)] pub fn f32_abs(v: f32) -&gt; f32 { v.abs() } Then call that.
&gt; &gt; How many languages are there which can run code on "bare metal"? &gt; There have been OSes written in C# (Midori)! And otherwise, the likes of Ada, ATS, D, Rust, Zig all fit the bill. C# presupposes the existence of a managed code environment, and a fair number of aspects of the .NET Framework are baked into the language with core features like the "for-in" loop, "destructors", etc. and even more are baked in with Linq. One might be able to build a minimalist C# implementation that could work on an ARM with 32K of ROM and 4K of RAM, but it would be a really stripped down language. Ada would probably be a good choice, but I've never used it. ATS and Zip I've never heard of. When I looked into D, I thought it would be really cool as soon as implementations for more machines than just x86 became available, but I gave up waiting. Are there more platforms yet? Rust seems like it might be workable, but the design seems to be a bit more focused on making it difficult to do things in ways that aren't inherently thread-safe than in allowing code to use the minimum degree of synchronization needed to meet application requirements. &gt; And that's not counting Forth, a champion of minimalism. For many purposes, "semi-portable C" is a much more practical language than Forth, and on many platforms would likely yield better performance as well. I'm also not aware of any Forth implementations that aren't based upon the idea of user code being accepted from some kind of I/O device and used to build up data structures in ROM--an approach which really doesn't work on a microcontroller with a read-only code store. Perhaps ROMmable versions of Forth exist, but I've not seen them. Using my C development system for the ARM, on many targets, I could arrange to have the processor start executing a C function directly when coming out of reset. If a project is going to spend most of its time in minimal-power sleep mode, and a typical wake-up event will only need to do a couple things before going back to sleep, the time required to set up a typical language's run-time library could exceed the time spent in user code before the system goes back to sleep. While the Standard doesn't require that any particular C implementations be able to build their entire standard library entirely from C code, any implementation that can't will make it necessary to write the library in some other language. I would regard an implementation that implement all library functions within C code as being more suitable for systems programming than one which cannot.
This seems to work fine for me: [https://play.rust-lang.org/?gist=27008d78733f331eb973fb31664a44c1&amp;version=stable&amp;mode=debug&amp;edition=2015](https://play.rust-lang.org/?gist=27008d78733f331eb973fb31664a44c1&amp;version=stable&amp;mode=debug&amp;edition=2015) &amp;#x200B; You don't even need the disambiguation, as inherent methods are preferred to trait methods: [https://play.rust-lang.org/?gist=8550a7aebeafc1c42d25ec7d33050225&amp;version=stable&amp;mode=debug&amp;edition=2015](https://play.rust-lang.org/?gist=8550a7aebeafc1c42d25ec7d33050225&amp;version=stable&amp;mode=debug&amp;edition=2015)
Thanks
Why does moving my test module to its own file break an import to a common test module? For example, with the file `tests/common/mod.rs` in place, this works: **tests/lib.rs** mod common; #[cfg(test)] mod sibling { use common; #[test] fn sample() { assert_eq!(1, common::return_one()); } } But when I try to split out the sibling mod into its own file like below, it fails with "unresolved import: no common in root". **tests/lib.rs** mod common; #[cfg(test)] mod sibling; **tests/sibling.rs** use common; #[test] fn sample() { assert_eq!(1, common::return_one()); } 
This is the first time I've seen this syntax in a where clause. Is there a name for using the where clause like that? 
That's awesome. Side question, what kind of shell or terminal is that? I always see it in various videos/gifs.
I don’t really get it, sorry. I read about the first half of the first blogpost too and it didn’t really clarify for me.
Why's the value limited to f64? I am not familiar with timeseries db, but could I insert something like raw Bytes?
Can you be more clear what your talking about here? I don't see how the Rc&lt;RefCell&lt;&gt;&gt; can be avoided. The changes that you linked to seem to be more about how cloning widgets for each handler isn't necessary. Do they implement `Copy` now?
Every `.rs` file in the `tests` directory is compiled as a unit test. It's complaining because `sibling.rs` is probably being compiled as both a module of `lib.rs` *and* as it's own, self-contained test program. You need to either manually specify all tests in `Cargo.toml`, or put `sibling.rs` in a subdirectory.
It's not just this post - the hackaday blog comments are pretty horrible most of the time.
If you're talking about the shell prompt, it's called powerline.
Thanks!
A wrapper being a piece of software that translates your code to talk to the library rather than just implementing the library in Rust.
Yeah, I tried that and it ended up throwing a bunch of runtime errors because it couldn't read the PNGs that were.. embedded in the executable. No idea what's up with that.
It's zsh with the Powerlevel9k theme 😉
OS of my choice is risky business. I can have a custom call that can be invoked pretty cheaply. 
As a workaround, could you create a symlink (I forget the windows term for it) on the C drive to where visual studio install is? Maybe that's enough to make it think it's on the C drive still?
I decided to go with current stable Tokio but prepare the APIs already for async. There will be another breaking change eventually, but I hope it will be relatively small. I actually went ahead and re-implemented the resolver with current Tokio yesterday. It was surprisingly painless but I didn't quite finish it.
Disclaimer: I'm not certain this is a drive issue, I just think that's the most likely culprit. That is pure baseless conjecture. The internals of how Visual Studio are located are arcane dark magic, and the VS team won't even try to help you if you've ever even looked at the install directory in the wrong way (or at least it feels like it). The first issue I ever had with Visual Studio (it was in the other direction, where VSI was convinced that an install was present but failed to do anything with it), the ticket was closed as "we can't diagnose a corrupted install", though I don't think I did anything specific to mess it up. As for actually using a symlink to move Visual Studio: maybe. If when this happens again I might try making a junction, or honestly I might just install on C:\ to see if it's even the issue. (Again, I have no proof, just a vague discomfort.) Due to how installations are managed, however, it would almost certainly require installing on C:\ first, and then moving and symlinking it afterwards. The reason I'm loathe to go behind VS's back to symlink onto a non-C:\ drive is that there are certain resources which aren't relocatable. Things like `%appdata%` Microsoft _specifically_ tells you _not_ to symlink or otherwise change its location, otherwise your system could be unstable. I don't think symlinking Visual Studio should ever be _that_ dangerous, but given the fact it manages unrelocatable SDKs, I don't put it past them. (I think on Windows the general term still is symlink, with a Junction being the most common symlink, presenting a "remote" folder as if it were inline. If I remember properly, which I don't guarantee.)
It's still easy to miss to ship a library when the dependencies change etc. (that happened to neovim-gtk recently).
What made you change your projection of async await being done from end of October to when "hell freezes over"?
Great talk! From the talk alone, I did not understand why generational indexing is useful. After reading the documentation of the slotmap crate, I got it. For reference: If you just use a vector with indices, you end up with indices referring to different entities when you reuse slots in the vector. The generation value allows the data structure to detect that the key refers to a remove entity.
Ah? You sure you called the right method? From memory and from disk are two different functions.
Then I don't know.
Actix supports 'sync actors', which are pretty much worker threads from my understanding. Also, I've just noticed there is no link yet to https://actix.rs/docs/databases/ -- it explains quite a few important details
It was more or about the `downgrade`/`upgrade` than the `Rc&lt;RefCell&lt;&gt;&gt;` itself.
I believe it's just called a where clause, they're super powerful and can do way more than simply constraining the type parameters themselves. You can put these where clauses in surprising places; functions, impl blocks (both trait impl and inherent impls), structs, traits, enums. Every one of these places admits syntax like I've shown here.
When I posted a blog post exploring overloading in Rust [I got quite a bit of feedback](https://old.reddit.com/r/rust/comments/83g847/exploring_function_overloading/), an important feeling was that you shouldn't overuse this trick for the purpose of overloading. I've actually gone back in my own code and removed some places (but not all) where I used this trick.
Great news, now waiting for the update of relm :) /u/antoyo 
Question: Does the gtk-rs includes the webkit-gtk?
The release doesn't. webkit-gtk is inside gtk-rs but in a different "scope". Its update should follow not too long after.
Well, there is still the hope that similarly to NLL this kind of patterns get some borrow checker love. This makes using Rust an hindrance for GUI programming.
It's a binding/wrapper to the GTK (and other) library, it's not a re-implementation of GTK in Rust.
NLL can't really help with this. The problem is simply that there is no information in the type system that would allow to infer the lifetime of signal closure data, and for the general case you'll always need the data at least stored on the heap (signals are not emitted from the scope where you connect to them). That said, many signal handlers could be a `FnMut` instead of `Fn`, which would at least solve part of the problem (if you don't need refcounting, otherwise you're back to `RefCell&lt;_&gt;`). So generally, I'm not sure what kind of improvement/solution you're thinking of. If you have any ideas, please let us know :) But I don't think it is possible to get rid of heap allocations for any callback-based/signal API without restricting how it can be used a lot. And in the end, that's also what you do in other languages with such APIs, just less explicit or less safe.
If you can send `&amp;mut T`, you can effectively send `T` because the receiver might use `std::mem::replace`.
&gt; as then two thread have mutable access to the same data at the same time, which is where data races happen. The whole point of a mutex is one thread can lock it at a time to gain exclusive access to the data. While the other comments have pointed out the issue of `mem::replace`, this particular issue seems like this wouldn't happen? Since the mutable reference is moved into the child thread, lifetime analysis will prevent additional mutable references from being created. In essence, the parent thread has "given up" its ability to create more mutable references to the original object for the duration of the child thread.
You are right, I miss took the question.
Quick follow up: Commenters have pointed out the obvious: there are ways to move out of an `&amp;mut T`. Is this, then, something that could be solved by the eventual completion of `Pin`, which is a reference that must never be moved out of its container?
Not the OP, but perhaps it would be possible to add some sort of language support for cases where almost everything is `Rc&lt;RefCell&lt;&gt;&gt;` wrapped. It might be antithetical to the "everything explicit" principle of Rust, but then again Rust is about choosing your tradeoffs, and one of them is syntax overhead...
What would happen if you'd send `&amp;mut Rc&lt;i32&gt;` and then clone it (accessing the reference count unatomically from another thread)?
Not really, as /u/Lucretiel points out [here](https://www.reddit.com/r/rust/comments/9e6mj0/pwnies_0014_docsrs/e5ni9uo/).
Will Crichton, any relation to our /u/acrichto?
I believe your problem is because this is `#![no_std]` code where the floating point types _do not have any methods_! This means that your call will always refer to your trait method, which is recursive. The solution is that floating point math is not no_std compatible.
Batching isn't a bad idea if it doesn't complicate your use-case too much. That would let you amortize the work done during pushing and popping, namely allocations.
`!Send` and `!Sync` have nothing to do with the bytes of the moved/copied object itself. Why should they? If that object is moved the old object is no more and thus there can be no races, and if it's copied the new object is a separate entity from the old object and thus there can't be races between them. If it was just about the bytes of the moved/copied object, that is. But it isn't. `!Send` and `!Sync` objects are usually _handles_ (or they contain handles that will be moved/copied with them) - they contain data that allows - via their methods or via external functions - to interact with other things. It could be a reference, which is an handle to something else in the memory - something which isn't moved/copied with the handle. Or it could be a connection, or a file descriptor, or a lock. These things may or may not be safe to move to other threads, because then another thread will have an handle to an object still related to the original thread, or - in case of `Sync` - copying the handle will mean two threads will have handle to the same object, which - depending on the type of the object and the handle - may or may not cause races. `!Send` means it's not safe to move the handle to another thread. Even though the old thread no longer has that handle, it's not safe to have it in a different thread than the object that it refers to. But if you have a mutable reference to the handle, you can usually do with to whatever it refers to pretty much everything you could do if you had the handle itself - so why should it be `Send`?
For non-trivial code, it's better to make a review request into its own post so the discussions are well encapsulated. It would also get the attention of people who are subscribed to /r/rust but don't regularly browse this thread.
Weird question about this: would lifetime analysis prevent this from working? Like, if you sent an `&amp;'b mut MutexGuard&lt;'a, T + 'a&gt;, is it possible (even in theory) to replace it with another guard matching `MutexGuard&lt;'a, T + 'a&gt;`? Even if you sent two guards, they'd have different lifetimes and wouldn't be "swappable", right?
Not `std` directly but it would let you use types from the `alloc` crate, such as `Box` and `Vec` along with all the other collections types. 
It is programming that many practitioners of believe is much more difficult than other programming domains and that it requires special knowledge and talent when in fact it is simply a different domain of programming that has similar levels of complexity as all other programming. Many practitioners of "System Programming" believe that the knowledge required for "System Programming" is much more specialized and difficult than other programming domains. This is actually not the case. Every programming domain has similar levels of complexity and concerns, and, in fact, the farther you are from the hardware the more (generally speaking) concerns you have that must constantly be considered, including: performance, security, maintainability, etc. Low-level programming is not actually more difficult than high-level programming, it just requires you to focus your time on the concerns of low-level programming as much as you've possibly focused your attention on the concerns of high-level programming. &amp;#x200B; You too can be a "Systems Programmer". Just dive in and start learning.
You **can** call `clone` on a `&amp;mut Rc&lt;i32&gt;` and get a new `Rc&lt;i32&gt;`. You can have many `Rc&lt;i32&gt;` pointing to the same heap-allocated `i32`, even though only have one `&amp;mut Rc&lt;i32&gt;` to every one of the `Rc&lt;i32&gt;`s.
This is indeed the case, thank you.
thank you killercup, i hadn't seen this page :/
I’m not sure, I think it depends on the variance of `MutexGuard` with respect to `'a`. If it is co-variant and you could somehow obtain a value of type `MutexGuard&lt;'static, T + 'static&gt;`, I think that would be a subtype of `MutexGuard&lt;'a, T + 'a&gt;` for any `'a` and so would be an acceptable argument to `mem::replace`. (I might be getting any part of this wrong.)
Ah, I believe I misinterpreted the context, thinking that the query was whether sozu itself spoke FastCGI, to sit in another server by that means. Either that, or I was thinking you were talking about embedding something Rust in sozu. But it’s a week ago; I’m not certain how I interpreted it at the time. If indeed you’re dealing with PHP, then I’m not aware of anything better than php-fpm. That’s what I use in the few places I’m maintaining something written in PHP.
So I wrote a tetris clone in typescript. Now I'm re writting it in rust to learn rust. I can tell you rust is much harder than a scripting language for doing the same thing. 
Ah, I see what you mean. Yeah, that's a good point.
This would be clearly against the rust philosophy but it wouldn't the first one (even though I don't like the idea), however adding a brand new language feature just for a crate seems a bit overkill...
&gt; Rust's main strengths come from writing functional code, which conveys more information to the optimizer. This isn't really true, unfortunately. It's barely true even in Haskell, and it's pretty much the opposite in Rust, since even Rust's "zero-cost" abstractions take nontrivial levels of optimization to unwind.
You seldom do the same thing in system programming though
&gt; I can't help but think how different the compiler landscape would be if GCC or LLVM were rewritten in Rust, it would basically make the memory safety issue of writing compilers go away And this would be possible to do gradually (one compiler-pass at a time) , I think!
Fair :-). I'm using `nginx` as a reverse proxy in front of a couple of services, some of them speaking HTTP, and others FastCGI. So right now I can't replace it with `sozu` on my server.
That's probably mostly because `rustfmt` has a pretty simple interface and no side-effects... 
&gt; Difficult-to-avoid pitfalls in unsafe compiler implementation languages—commonly, null pointer references and use-after-free errors. Are these really such big problems in a compiler? I don't think Rust tries very hard to defend against compiling malicious crates, and I think Rust has many more "internal compiler error" panics from Rust code than segmentation faults from LLVM.
&gt; even Rust's "zero-cost" abstractions take nontrivial levels of optimization to unwind Are you saying that the compiler can't/doesn't always fully unwind those abstractions into "zero-cost" generated code, or just that it just takes a long time to do so during compilation? IIRC, Rust's documentation does clarify that "zero-cost" actually means "zero-_runtime_-cost", but it's human nature to enjoy oversimplified quotes. With short compilation time being such a desirable compiler feature (as with Go), the trade-off should be discussed more.
Now that school has started who knows how much time I'll have but I'm working on a gtk DND character sheet. I'm writing the actual logic in a different crate so that I can write multiple frontends. I haven't decided how much logic I want there to be (leveling up, using items, etc.) Cause DND is such a freeform game it would be hard to capture all of that functionality.
I ran into this. I think I had to explicitly `.opaque_type("size_type")` in addition to the std glob.
It's less explicit in GC languages, or less safe (but explicit) in languages like C :) Apart from that I agree with you, but I think what is needed are "simply" different API design approaches like the one from Relm.
I had the same issues with gdk_pixbuf :/. Works fine with cargo, trying to distribute it blows up. Not sure whats with that
Are you using [include_bytes!](https://doc.rust-lang.org/std/macro.include_bytes.html)?
 People make games in c# and javascript with unity. And also with c++. AI is hugely popular with both c++ and Python. Rust is one of the most popular wasm languages and you could just use javascript instead. There is lots of overlap. 
I couldn't get that, or just set_icon_from_file. See my previous post: https://www.reddit.com/r/rust/comments/9co1n1/distributing_a_gtk_app_on_windows/
Thank you for your answer, I still have the error :( I've added `.opaque_type("std_size_type")` as well but no luck
This paper lays out a general path that I envision Cranelift following. I believe superoptimization will change a lot of our assumptions about what optimizers can and can't be made to do, by making it easier to build up new ones, and by making it easier to make changes once they've been built up.
&gt; Are you saying that the compiler can't/doesn't always fully unwind those abstractions into "zero-cost" generated code, or just that it just takes a long time to do so during compilation? Both. Optimizers only put a finite amount of effort into optimization, and the harder you make its job, the longer it will take to get a less-ideal outcome. Rust still does fairly well, but there are limits.
gtk-re really needs first class support for MSVC toolchain. Without this it is limiting itself to a limited class of uses. Rust GUI situation is precarious, the best shots here seems gtk-rs and relm. Qt support is worse, the best alternative is still writing UI in C++ with qt-bindings-generator or even qmlrs. And the one big thing limiting gtk-rs is MSVC support. Doing this via mingw/cygwin is just not right (it is an unecessary added complexity). Luckily, there is a solution. I did succeed in doing this pretty easily, the problem is that I still don't have enough time to make it a seamless and painless process. 
Hm I'm not sure then because I double checked and that was indeed what I did. I get the error `error[E0428]: the name `size_type` is defined multiple times` though, not `std_size_type`. Is `std_size_type` an alias? I don't believe there is a `size_type` in the root `sys` namespace.
`&amp;Trait` is also a trait object (and possibly called `&amp;dyn Trait` now). Is that just a way of telling Rust your function accepts any reference to a trait, including `Box&lt;Trait&gt;`, `Rc&lt;Trait&gt;`, etc?
I like the litmus test that you can build a memory allocator in that language. I think a systems programming language *can* provide high levels of abstraction and a high density of instructions per statement, as long as it gives you fine grain control over memory layout and performance.
My main goal for the week is to start a GUI for the music [synthesizer](https://github.com/raphlinus/synthesizer-io). I'm doing Windows-first (largely because performance and responsiveness matters), which leaves a nice chunk of work porting it to other platforms later.
From now on I refer to this trick as the Detector pattern...
&gt; gtk-rs really needs first class support for MSVC toolchain. Without this it is limiting itself to a limited class of uses. What's missing for support of the Rust/MSVC toolchain? I didn't try up to GTK, but GLib/GStreamer applications build and work just fine with the Rust/MSVC toolchain.
&gt; The new procedural macros (which allow you to define your own #[attributes] and function_like!() macros) are basically stabilized! So, to be clear: * attribute-like macros will be able to be defined on stable rust in 1.29, this week * attribute-like macros will be able to be used on stable rust in 1.30, in six weeks * function-like procedural macros do not have a timeline for stabilization yet.
I improved my \[MegaZeux world viewer\]([https://github.com/jdm/mzxview](https://github.com/jdm/mzxview)) by evaluating some Robotic programs to generate screenshots that better reflect the initial state when actually playing the game. Now going to build on my abstract rendering background to bootstrap an \[SDL-based version\]([https://github.com/jdm/mzxplay](https://github.com/jdm/mzxplay)) that can process input rather than generate static screenshots.
For an application development I would go with (A)Rc&lt;dyn Trait&gt; approach for all high level components (repositories, services, etc) and with generics in "util" code. Generics CAN work in some cases (your example can be one of them), but may leak types in others. A good example is Actix where you need to pass an address of another actor in a message but if actor itself has parameters you have to add them to a message. 
It looks like it comes from `std::string::size_type`, can't it? `std::size_type` didn't helped. I'm trying a regex: `.*size_type`. 
Really? That's weird... `set_icon\from_file` won't work if it's installed with cargo but `include_bytes` is (I'm using it in `process-viewer`).
All the libraries? The best exercise is, clone relm and try to build the simplest example with a msvc toolchain. This should work seamless, anything else is bullshit. Then, you go to gtk-rs repo and look what is needed to build on windows: http://gtk-rs.org/docs/requirements.html Most will just give-up at the cargo build step, some may get to this page and then a good % of these will close the page and search another gui or language more able to do gui stuff.
I believe being generic over `AsRef&lt;Trait&gt;` should accomplish that.
The rust-enabled lldb finally made it into rustup, but of course not without problems. So, my first task is cleaning these up. I'll post on the discourse when this is finally ready. It's been such a long slog :( Second, I'm working on my patch to improve enum debug info. This is very close to being ready.
So let's not start another spiral of rewriting fannkuch-redux programs (this time to use SIMD). The benchmarks game tasks that already have programs which use SIMD are still fair game. *iirc* For many years one claim has been that Rust needed SIMD to compete on n-body, with the counter-claim that it was really all about LLVM loop-unrolling. 
I’m putting the finishing touches on the first version of a library I’ve written for modeling the polarization of a laser as it passes through various optical elements ([Jones calculus](https://en.m.wikipedia.org/wiki/Jones_calculus) for those in the know). The plan is to finish adding documentation, then start work on wrapping it in a CLI tool so that you can specify an optical system in a TOML file, read it, maybe sweep a parameter, and write the output to a file.
Can you file bugs about this with details? As said, works fine for me with glib/gio/gstreamer/pango/cairo. So let's try to find what goes wrong for you and can be improved
I already tested the [byteorder!](https://crates.io/crates/byteorder) crate, but problem I have, is the fact that the protocol is so dependent on the "C" way of doing thing (size of array in different place than array itself). Secondly, the byteorder is an encoder, as pointed out, I would try to rather somehow use basic `repr(C)` than involving additional overhead. But the `str::as_byte` is actually really interesting for this usecase, will try it right away! Thanks!
This will probably only be easy for those steeped in the work of the macro namespace modularization stuff: On nightly, is it possible to prevent macro\_rules macros from bubbling up to the root of the crate? E.g. declare a macro\_rules macro inside a module and have to import it from that module when calling it from another crate? I've been reading through this thread about the modularization/namespacing on Github, [https://github.com/rust-lang/rust/issues/35896#issuecomment-395557096](https://github.com/rust-lang/rust/issues/35896#issuecomment-395557096), and it's still hard to figure out what changes apply to macro rules (in addition to the decl &amp; proc macros)
I took a week off to work on something completely different, and ended up RIIR'ing [minimp3](https://github.com/lieff/minimp3). The result is [way less complete than I expected](https://github.com/icefoxen/rinimp3), and doesn't reeeeeally work yet, but it's now 90% safe code and has tests suites comparing its results against the original C code (via the [ffi crate](https://crates.io/crates/minimp3) ). Just have to work out a bunch of panics that are due to the original code freely treating `float[8][4]` as `float[32]` whenever it's convenient and such, which doesn't work so well when you turn the arrays into bounds-checked slices. ...then I'm sure I've introduced piles of bugs that need to be found. Ah well! Anyway, it was fun, and maybe someday I'll write about it. For now, back to ggez!
&gt; function-like procedural macros do not have a timeline for stabilization yet. Oh, good to know. Had that remembered incorrectly. Thanks! :)
One thing that might matter here is that GStreamer provides Windows binaries for all that (except for gtk/gdk as those are irrelevant in the context of GStreamer). That makes everything on Windows relatively easy apart from Windows' general development unfriendliness. Are there no useful binary distributions for gtk for Windows?
To find a good term is important, so it can be easily communicated :)
Could you send a closure that can produce the io::Write instance inside the correct thread instead?
I'll give an example of a structure that is never allowed to be accessed from a different thread: Linux namespace handles. There's a good example here: https://www.weave.works/blog/linux-namespaces-and-go-don-t-mix The current Send semantics effectively remove that problem. `!Send` effectively makes sure that data local to a thread remains there and also access remains there. You can still recreate `Send` by providing an appropriate wrapper type if you need to, at the danger of having to make sure that this actually holds for all situations.
&gt; The underlying type isn't actually "sent" between threads; no bytes are copied. It is, because you can easily replace the value within the mutable reference with a different one to extract it. Here's the problem: let x = Rc::new(1); let y = x.clone(); let y_ref = &amp;mut y; thread::spawn(|| y_ref.clone()) this lets us `clone()` an `Rc` off the main thread. Things which are not `Send` shouldn't be accessed from other threads, period, that's the guarantee of `Send`. You may find [this blog post of mine](https://manishearth.github.io/blog/2015/05/30/how-rust-achieves-thread-safety/) helpful
Exactly, this is the root issue. Cargo could do this via build.rs or something else when building gtk-sys, gdk-sys and so on. The first error we get when trying relm button example is a missing gtk-3.lib. I would hope gtk-sys would be able to bring it in. That can and should be automated.
No problem!
Code generation is indeed what we're currently focused on, and what our current goals require. But we're also keeping in mind [where things might lead](https://github.com/CraneStation/cranelift/blob/master/rustc.md). Also, code generation, in particular instruction, can benefit from superoptimization too!
I'm reviewing the various rfcs on named arguments, anonymous structs and default values (for structs / arguments / named arguments). I think I have an opinion as to what the best proposal would be, but am polishing my review. In addition, I'm working on a small proof-of-concept to demonstrate what it would feel like to use said proposal, albeit with very heavy restrictions (uses nightly and some macro related feature gates).
You can think of `Send` as "**Exclusive** access is thread-safe," and `Sync` as "**Shared** access is thread-safe." `T: Send` means it's safe for one thread at a time to have unique access to `T`. This means: * You can send `T` by value. * You can send unique pointers like `&amp;mut T` or `Box&lt;T&gt;`. * You can share `Mutex&lt;T&gt;` because it enforces unique access to `T` even when shared. `T: Sync` means it's safe for multiple threads at a time to have shared access to `T`. This means: * You can send shared pointers like `&amp;T` and `Arc&lt;T&gt;`. * You can share `RwLock&lt;T&gt;` because it allows shared access when shared. If a type is neither Send nor Sync, then no access is thread-safe, and it can only be accessed on the thread where it originated.
Look forward to reading it :)
Why not use [`()` instead of `Foo`](https://play.rust-lang.org/?gist=46adc2078787b670d6c57c13781b2d64&amp;version=stable&amp;mode=debug&amp;edition=2015) though? 
&gt; Here’s a hot take: functional languages like OCaml and Haskell are far more systems-oriented than low-level languages like C or C++. Without disputing the hotness of that take, I think the exact same logic lets you say that Java is more of a systems programming language than C++. Which makes it seem like the definition of systems programming he's working with there isn't super useful. I think whatever definition we want to settle on -- and the article does a good job of pointing out that there are many possibilities -- it has to _include_ low level programming as a necessary part. Otherwise the concept ends up meaning something totally divorced from how anyone actually uses the term.
What is the problem? Cloning a `&amp;usize` is a pointer dereference. It's not expensive. Where is the waste, and why is it massive?
&gt; Branan Riley is working on the fourth chapter of his Exploring Rust on Teensy series, which will focus on using futures-rs to represent DMA transfers High-level abstractions, meet systems programming.
IMO systems programming is what you end up doing when no libraries exist to achieve it.
&gt; Alternatively, you could define your own trait that provides the functionality you need, and implement it for any Read + Write + Seek Wasn't there a crate with a macro for specifically this use case? (It's not the extension-trait crate.)
Is there a timeline for attributes on expressions? 
Yes, [and then](http://static.comicvine.com/uploads/original/11114/111142536/4137641-lets-get-ready-to-rustle.jpg) you can re-use your API types between backend and frontend (which I'm doing in a webapp that uses Websockets where I have the backend and frontend in the same workspace and the common types in a shared crate).
&gt; I think I want something like "int" that is the same as the default in assignments. Well, `i32` *is* the default in assignments on all platforms. There are arch-dependent integral types `isize` and `usize` though.
The raytraced pictures look pretty good!
The paper mentions some optimisations which are clear wins, and then getting to the level where things start to be a trade-off and are more complicated to judge and "it depends". It seems to me that many Rust optimisations at a high level should hopefully be in the "clear win" category, for a large part at least. Definitely as soon as CraneLift is working as a backend, even at the most basic level, that means there's a motivation to do more "clear win" style Rust high-level optimisations, as they will give immediate benefits. So hopefully even a little progress here will be self-reinforcing. This may even improve LLVM-backend build-times as a side-effect.
Not the author but the repo uses: https://crates.io/crates/notify which appears to use the OS notification systems like inotify, etc.
Possibly deserves a stack overflow question... but, I have a service that returns CSV files, and some of the fields store arrays. The service represents these array fields by repeating the column, once for each item in the longest array in the dataset. A side effect is that the number of columns in the CSV file varies based on the query. What’s the best way to reconstruct these records with array fields? Naively using the CSV crate with serde gives duplicate header errors, so while testing I’ve just added 1, 2, 3 suffixes in my test data, but I’d like to be able to process the raw data from the service easily.
I just loaded a glade prototype.
Finally i decided to make a little journey through game development (after some time of reading about in general to build a solid foundation of knowledge) with Rust as my language of choice, so i'm starting to create a map with Blender and bringing it to screen with Amethyst.
How do I start making a game? What I mean is, how would I go about getting the key presses? This has always puzzled me. I can understand that a game is just a state machine that either changes over time or changes every time the user inputs something. For example: if I make a space invaders clone, how do I fetch the key press event and handle it? Thanks in advance
&gt;But if I do: &gt;let x = 6; &gt;The compiler can default to something. I think its probably i32 and it might be platform specific. It's not platform specific, the type of `x` is inferred by the compiler. It defaults to `i32` if it can't figure it out (unlike in C and C++, where without a suffix it is *always* an `int`). But the compiler is pretty smart, for example: fn foo (x : u8) { println!("{}", x); } fn bar (x : i32) { println!("{}", x); } fn main() { let x = 6; foo (x); bar (x); } This won't compile, you get `error[E0308]: mismatched types expected i32, found u8`. If you swap the order of the function calls you'll get the same error with the types reversed. The compiler infers the type of `x` based on the first time you use that alias after assignment. 
Can you link to the code or docs?
Great work, renders look gorgeous!
LLVM unrolling is more optimized on newer CPUs – I get much better code for my skylake (which is two gens old), than your penryn (which is ancient). I can counter that by writing SIMD code in Rust, or I can live with it and accept that the benchmarksgame won't show what performance is possible using Rust. As you have taken the former option from me, I am left with the latter. Also, as I've written elsewhere, please document this new rule.
Thank you, just let it run over my code :)
Thanks for the feedback. Based on my testing, TimeTrack is very light on resources. On your algorithm question, the system considers any two changes that happen within five minutes of each other to be part of the same session, as long as the changes are part of the same project. For example, if you make two changes on the same project, and they are 4.5 minutes apart, then that counts as 4.5 minutes. If you make changes in separate projects, then it splits the time between projects (you don't get credit for the same time on two different projects). The algorithm isn't perfect, and I believe tends to underestimate the amount of time I am actually working, but it is close enough for my uses. I'd be open to pull requests improving the accuracy, but at some point I think you just have to make some assumptions. 
As /u/swarthy_io said, this uses OS notifications, so it should work no matter what you are using to change the files. 
Ok well then I'm going to use i64 and if anybody complains I'm not being idiomatic I'm going to blame you.
What you are describing sounds a bit like [selfspy](https://github.com/selfspy/selfspy) and [activity watch](https://github.com/ActivityWatch/activitywatch/). I haven't used them personally, but based on what you are describing you might be interested. 
&gt;the Rust developer immediately talking about Go not being systems 🤣
Why isn't the context part of the return value?
It's really just a matter of practice and what you're used to. For me setting up the TypeScript toolchain was quite a pain, while in Rust it's a breeze. What libraries are you using for the game? If you're using something like SDL the code should look pretty much the same as using HTML Canvas.
Could you show us your code? naive response: use a struct with methods rather than plain function
&gt; for elem in v { &gt; vec.push(elem); &gt; } You can use [`Vec::append`](https://doc.rust-lang.org/std/vec/struct.Vec.html#method.append) or [`Vec::extend_from_slice`](https://doc.rust-lang.org/std/vec/struct.Vec.html#method.extend_from_slice).
Just return both stream and context in the same struct? Also have you tried [`rscam`](https://crates.io/crates/rscam)?
Thanks for those. Neither mention tracking joystick activity not visible to X11, and, according to Activity Watch's comparison chart, selfspy isn't extensible and Activity Watch already has a browser extension, so I guess I'll be investigating Activity Watch first.
Good point, I should've read the end more closely.
Nice! Here is my version of the same thing https://github.com/termhn/rayn Overall I'd say that your code isn't bad but it follows c/c++ style conventions (for example returning a bool and passing a pointer to some data to be modified instead of just returning an Option&lt;Thing&gt;), probably because that is what was used in the book. Also doesn't take advantage of the functional-style programming offered by Rust's awesome `Iter` api.
There are many mechanisms, but building on top of consensus algorithms such as Paxos and Raft would be a key approach.
I think its a great technique and implementing it really helps understand how traits work. It definitely helped clean up my maths operations. Your article was really well written as well!
What BSDF did you use for the metal?
Thanks! That is quite big chunk of code from real life example 
Nice, I'll keep that in mind for future embedded projects. And I did not knew how "little" is required to write a working programming language interpreter. I read/skimmed trough the whole source and it is very easy to understand (except for the nom-part without reading how nom works) and nicely written. One note: You can write!() directly into a String instead of push\_str(&amp;format!()) :) What I like about the language is it's simplicity. What I don't like is the silent generation of None-Values on unsupported operations. I like it if I trigger errors on unintended behaviour as early as possible. In no-std projects, it also uses the global allocator. That's probably ok for most cases, but I try to imagine how hard would it be to create an interpreter that uses a fixed amount of memory? Is it possible to call functions even after all statements have been executed?
I wouldn't be surprised :)
I am writing a tool to get, list and set MacOS screen resolution(s) on MacOS. It uses core\_graphics crate from [core-foundation-rs](https://github.com/servo/core-foundation-rs). In a way, it is a clone of a previously exisiting tool written in C ([https://github.com/jhford/screenresolution](https://github.com/jhford/screenresolution), not maintained anymore). 
That's pretty smart. I'd support changing the wording to be more descriptive, but I could see there being some resistance to change since the phrase is fairly well established in Rust's branding.
I tries `rscam`, but currently it doesn't seem to work with MacOS.
Hi, first time asking a question here :) &amp;#x200B; I would like to `match` over two `Rc&lt;i32&gt;`'s and am a little confused as to why I cannot match against the tuple I would like to. use std::rc::Rc; struct T(i32); // This function compiles just fine fn try_match1(x: Rc&lt;T&gt;) -&gt; &amp;'static str { match *x { T(0) =&gt; "zero", _ =&gt; "non-zero" } } // This function doesn't compile because "cannot move out of borrowed content" // matching against x and y individually will also compile just fine, but together in a tuple they do not work fn try_match2(x: Rc&lt;T&gt;, y: Rc&lt;T&gt;) -&gt; &amp;'static str { match (*x, *y) { (T(0), T(0)) =&gt; "both zero", _ =&gt; "at least one is non-zero" } } Adding `#[derive(Copy,Clone)]` fixes this in this sample example, but for my real datatype, I do not want Copy. Using `as_ref` on the Rc also fixes it, but I think that it would be nicer to be able to use the same `*` deref in both places. Does anyone have any solutions? Thanks! :)
This doesn't make much sense though, your program either needs i64 precison or it doesn't, having the upper limit be different depending on the CPU you're compiling for doesn't seen helpful in most scenarios.
&gt; This is what I'm getting at, assignment to variables has a sensible default type, why not assignment to parameters? (I know rustc likes to use function signatures to figure out what types everything is, but I can't think of why it can't be a little presumptuous here.) So would you prefer this instead: fn foo(&amp;mut self, x, s: &amp;str) { ... } I think it would be quite weird and inconsistent if Rust required type annotations for most arguments, but it would assume `i32` when the type annotation is omitted. In addition to looking weird, I would imagine it could be a source a bugs (e.g a developer intends to have a `usize` parameter but forgets the type annotation, resulting in weirdness). I think in this case consistency and clarity wins over terseness, especially when `i32` arguments are quite rare in practise. &gt; This doesn't sound right to me, i32 has to be an optimization in some way, otherwise we'd just all use the biggest int that fits. Maybe its a very low level thing, like an operation with i64 has to be split across more instruction cycles than a i32. I'm not on the language team nor was present when the decisions were made, but I think `i32` is the default because of both historical and practical reasons: * On 32bit and 64bit systems in all modern C++ compilers `int` means a 32-bit signed integer. Rust is both inspired by and designed to replace C++, so it makes sense to follow the lead. C++11 also has type inference using the `auto` keyword (like `let` in Rust), and AFAIK it infers `auto x = 10;` as an `int`. * Rust is heavily inspired by OCaml, and the first few versions of rustc were written in it. OCaml uses 31-bit integers as the default integer type (1 bit is used for other purposes, Google for more info). `let x = 1` has the same semantics as Rust. * C# and Java are both incredibly popular languages, and Rust has been somewhat inspired by C#. Both languages use 32-bit integers as the default number type (`int`). There are probably half a dozen other examples. So, why a signed number? I think people [new to programming] would expect negative numbers to work just like on a calculator, so a signed integer makes sense. Why 32 bits? Aside from the aforementioned historical reasons, performance _is_ a factor. 32-bit operations are fast on both 32-bit and 64-bit architectures, and 32-bit CPUs have been the (minimum) standard for at least 25 years, so it makes sense to target them as the default. If Rust used 64-bit integers instead, operations would be much slower on 32-bit hardware (and more importantly, when running 32-bit binaries) with very little actual benefit for most programs. If on the other hand Rust used `isize` as the integer default type, your programs would be less portable since they might behave differently on 32-bit and 64-bit platforms. 32-bit integers also consume half the memory, which can be a significant difference when you have a tight memory budget or you want to optimize CPU cache usage. Besides, they're big enough for most everyday tasks. Unless you're generating gigabytes of data or counting the number of people in the entirety of Asia, you'll be fine with 32-bit.
It's just Whitted basically
This works, but I don't know why (basically sums up all of my Rust experience) https://play.rust-lang.org/?gist=385f0e05e51c07f1a180dcb6a5ca5db6&amp;version=stable&amp;mode=debug&amp;edition=2015
this will also probably be faster
found this with my trusty let me google that for you skills &amp;#x200B; [https://docs.rs/corona/0.4.0/corona/](https://docs.rs/corona/0.4.0/corona/)
Neat! I also wrote a take on the raytracer from _Raytracing in a Weekend_ when I was learning Rust. Yours is better, but if you’re interested mine’s [on github][mobula]. [mobula]: https://github.com/isaacazuelos/mobula
Doing `&amp;*x` converts an `Rc&lt;T&gt;` into a `&amp;T`, which makes sense. I don't know why the single one works, though, unless Rust is implicitly adding `&amp;` to the match variable (it does this in the arms, at least - in the second function, if you put `&amp;` in front of both of the `T(0)`s it still works.)
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/raytracing] [Raytracer in Rust](https://www.reddit.com/r/raytracing/comments/9erur7/raytracer_in_rust/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
I'm surprised people keep building stackful coroutine libraries. None have caught on and none will. It is a tempting but ultimately a bad solution. More RAM usage and less safe. 
In this case what's happening in Rust is indeed "zero-cost abstractions" because e.g. iterators get optimized into magical constructs that use some voodoo you don't understand. Until they don't get optimized and you get performance as horrible as you'd expect from the naive implementation, and you don't have a clue why. And performance also changes from one rustc version to another, and sometimes regresses.
Well then just move the cloning a little closer to the `usize`s, like [so](https://play.rust-lang.org/?gist=1a9df69563f4f8f63ecd85be3e29a750&amp;version=stable&amp;mode=debug&amp;edition=2015).
It sounds like you tried to use rental to solve this problem, but your posted example doesn't use rental, so there's no way for us to see where you went wrong. This kind of "returning multiple things that borrow each other" is indeed something that rental tries to help you solve.
Look at a library like [glutin](https://github.com/tomaka/glutin), it provides a list of key presses to you. In terms of making a game all you need to understand is an API like this. The more fundamental answer is the window manager provides them to you. I'm not familiar with X11 but with wayland you connect to a unix socket the window manager opened (think "tcp - but only local" if you aren't familiar with the concept) and it passes you events including keystrokes and mouse movement. The window manager get's the events from the kernel, by reading from the /dev/input/eventXX files. The kernel gets them via interrupts, when I press a key it sends an eletrical signal to the CPU that says "new keystroke here", the CPU stops what it's doing, and runs an interrupt handler that reads that signal and stores it somewhere. Then returns to what it's doing. It later makes it available on the previously mentioned files.
Wow this is really nice and seems to confirm the suspicion that the x86\_64-pc-windows-gnu toolchain just straight up never has the RLS component. Interestingly the i686 version seems to have it.
&gt; I think Rust has many more "internal compiler error" panics from Rust code than segmentation faults from LLVM (and often only on invalid code). As someone writing a safe LLVM wrapper, I have to say that LLVM will sometimes segfault at the most unexpected times. While it's true segfaults are usually from invalid code, this is all too easy to accomplish. Also its global state leaks all over the place sometimes making testing really difficult (different results in single vs groups of tests). Meanwhile, I don't think I've ever encountered an ICE outside of nightly since rust 1.0
&amp;\*x does essentially the same thing as `x.as_ref()`. I think this must be an error/oversight in the new match semantics? 
If there are no constraints from type inference, the default for integers is `i32`. The default for floats is `f64`. Yes, you declared `y` to be an unsigned 8-bit integer. The colon is used for a "type annotation" which is used at compile time; there's no run-time overhead.
Thank you, this answers all of my questions :)
On a side note, why is PDF still alive, do people still use paper?
I can't speak for the rest of the world, but I use PDFs on my [reMarkable tablet](https://remarkable.com/) so that I can make notes or editing marks.
Wrong subreddit. Check out /r/playrust
Oh thx!
It's consistent formatting and it's all packaged up in a single file?
How do you solve the problem of trying to read a4-formatted pdf on a phone, and that phone's app trying to show a4 on a small screen? That has always been the biggest problem with pdfs for me - not being able to just read normal text flow with normal text size, like reading html-formatted text.
No, I don't think it's an error. Reading the [reference page on match expressions](https://doc.rust-lang.org/reference/expressions/match-expr.html), the match head is either a place or value expression, and has different semantics based on that. \`\*x\` is a place expression, while \`(&amp;\*x, &amp;\*y)\` is a value expression, so they are treated differently - you can match on \`\*x\` fine because its ownership isn't being changed by that, whereas if you try match on \`(\*x, \*y)\`, that's essentially creating a temporary tuple, attempting to move \`x\` and \`y\` into it, and matching on that (which you can't do.)
PDF is _the_ way to share documents. You do not share source files (Word, Pages, TeX, GDoc, whatever format you use), you share the finished product, which is a PDF. It's consistent formatting across machines, doesn't suffer from proprietary software requirements, and is (marginally) more tamper resistant. If a document is final, it should be in PDF format in most cases.
No. Zero-cost abstractions would be closer to coding with NumPy's abstractions, where even the most skilled Python programmer could not hand-write code with performance comparable to the optimized C implementations the abstractions rely on.
Cool project. I'd like to see your code when it is complete. 
All good points, but how do you do the very thing that text documents exist for - that is, read it? It's too small on a phone/tablet, it really doesn't like adjusting to viewer app's window size because viewer apps can't (won't?) rewrap text on the fly, etc. It's just a huge pain in the ass to view/read it, compared to just opening a markdown file on github, for example. Pdf, from what I can tell, was designed to be formatted to a specific piece of paper and then frozen like that. That's why I asked about people using paper - paper is dead, so freezing text to imaginary piece of paper only makes it unusable on anything but PC displays.
[C](https://pastebin.com/3YvWQa5c)ode from Jonathan blow [https://pastebin.com/3YvWQa5c](https://pastebin.com/3YvWQa5c) that you may find useful. 
&gt; How do you solve the problem of trying to read a4-formatted pdf on a phone Not with PDF. &gt; not being able to just read normal text flow with normal text Many documents contain things besides "normal" text.
How does this differ from pandoc? This seems like a less flexible version of it with the minor advantage of no runtime dependency on LaTeX. 
But it is consistent across all devices, so that it can be read by all devices providing the same rendered image, which is useful for stuff like mathematics and science. There are programs that will pull the text off a pdf document and present that, so if you need to read it on a mobile device you could use those.
Then don't read it on a phone. PDF is not the format you're looking for, so don't complain that it's not what you want it to be. Different formats exist for different things.
Delete this post please. 
I mean, if pdf is unreadable on mobile devices, then how can it be the way to share documents? Documents are meant to be read, and this is a direct contradiction. I thought I was missing something but apparently not. Thanks for verification.
&gt; it is consistent across all devices Kinda like png files are. But we don't read from png files, even if every pixel in that image is exactly how the author wanted it.
There are two different classes of "way to share documents". Reflowable, and layout-preserving. PDF is "*the way* to share documents" in the layout-preserving class. HTML is probably the closest thing to a universal "*the way* to share documents" that the reflowable class has, but there's no true, unarguable standard because no single format is accepted by all devices without conversion. (ie. Kindles want MobiPocket, other eReaders want a restricted subset of HTML inside a Zip file called ePub, and desktop PCs don't come with something capable of reading either of those formats, but do come pre-loaded with an HTML reader.)
I kind of think of pdfs as being more like image files than paper (sometimes pdfs are basically image files), but image files that can have other types of content. You'd use a pdf for many of the same reasons you'd want an image file, but when you might have text and so forth that's embedded. There are many cases pdfs are irritating to me, and I'd prefer it be html or whatever, but there are other times I'd prefer a pdf. Highly technical stuff, for example, I generally prefer on pdfs. Sometimes when a pdf isn't viewable properly on a device, it means the device isn't really capable of viewing the content as it was meant to be viewed. That is, it might be a limitation of the device rather than the file format. 
PDF is a wonderful publishing / printing format. For those interested in the problems it was designed to solve, [Computerphile](https://www.youtube.com/watch?v=48tFB_sjHgY) did a great video on it.
Type inference starts off by calling it a generic integer, which you can see in some error messages (try `let x: () = 10;`). If you use the value in a context that requires a certain type, and that type is an integer, then it infers that the value is that type. In your example for `y`, the assignment requires the right hand side to be a `u8` since the variable it's being assigned to is also one. If there is nothing saying what kind of integer it is, then it just goes with `i32`, like with `x` there. Explicit data types have no impact on performance. The language is compiled, which means that the type information is contained within the low-level machine instructions that it generates. It generates those same instructions no matter how the compiler determined what that type is. There are some performance differences between different number types, but in almost all circumstances they're either identical or so close in performance that they're indistinguishable.
It depends. There's a very popular pair of formats for reading comics and manga electronically, `cbr` and `cbz` which are just sequentially-named JPEG or PNG files inside renamed RAR or Zip files, respectively.
/r/playrust. Probably shouldn't heart the community if you don't even know what community it is. 
I wouldn't consider that a minor advantage
Offtopic, but I have a few lengthy documents I've been meaning read, and was looking at sony's A4-sized offering until I learned that it needed custom software just to load pdfs on the device. That is a deal-breaker for a linux user like me. Does the remarkable appear as a mass storage device when plugged into a PC or does it also require some sort of custom software to work?
`lazy_static` is a good solution for singletons. Do you need to construct more than one `VideoStream` over the course of the program's execution?
Do you own a smartphone?
Who says it needs to be generated on the same computer as it is viewed? I also think such a package could be transformed into a library crate easily which one could use in other standalone rust programs.
PNG files are an extreme waste for text and are also not at all accessible.
Will there be any performance penalty for doing DMA this way? Using futures to represent embedded tasks like this would be absolutely amazing, but I can’t help but think that there has to be some cost vs. doing it the “normal”, unabstracted way.
Forget gstreamer and use ffmpeg libraries. Using gstreamer only brings pain and disappointment. 
Cool project! You may also take some inspiration from [Tectonic](https://tectonic-typesetting.github.io/en-US/) (TeX engine written in Rust) and [Rinohtype](https://rinohtype.readthedocs.io/en/latest/index.html) (RST to PDF renderer in pure Python)
A couple ideas: * First would be to do what you're doing now; which is one column per element in the array. Is there a definite upper bound on the number of elements in the array? If so, maybe it'd be simpler to just always have the same number of columns, everytime. (e.g 20 columns for the array field, irrespective of the length). * Another thing you could try is storing the array as a string inside one cell of the table. (Super gross, but it could work). * Spread out each element across many rows. For example, if the array had five elements: ID | some_field | some_array_field ---|---|--- 0 | "foo" | "a" 0 | "foo" | "b" 0 | "foo" | "c" 1 | "bar" | "a" 1 | "bar" | "b" In other words, making the table "long" rather than "wide". The disadvantage here is that you have duplicated data in each row (see how `"foo"` is repeated three times), so if you wanted to maximise throughput this might not be the best idea. * Send multiple tables in the response; the first could look like ID | some_field ---|--- 0 | "foo" 1 | "bar" And the second could look like ID | some_array_field ---|--- 0 | "a" 0 | "b" 0 | "c" 1 | "a" 1 | "b" This way, you aren't sending the non-array fields more than once (although you still have to send the ID many times). You could extend this to many array fields by either sending one table for each, or by using an extra column for the name of the array. ID | which_array | value ---|---|--- 0 | "some_array" | 21 0 | "some_array" | 22 0 | "array2" | 90 0 | "array2" | 91 (I've omitted the the values for `ID=1` due to time but hopefully the idea is clear. * Finally, any reason not to use JSON, or some other format that has better support for variable-length values?
I think it is good. It's a scientific approach to try alternatives even if at first sight they may be a bad idea. After N stackful coroutine libraries we can say that we tried the concept in various ways and the community doesn't prefer it. Or maybe they will!:)
It gets a bit tedious when you can’t see beyond simple dynamic formatting for information exchange. PDF is about *maintaining* formatting for many of us. I constantly deal with custom fonts before Unicode in areas such as linguistics, so obscure one-off orthographies are plenty (I.e. no Unicode equivalent). Saving as a pdf might not make it machine readable for those orthographies, but a human brain can make sense of the message.
Not being able to constrain `F` in the type itself seems silly. Is there really no way to make that work nicely? Why?
Well, a lot of stuff isn't just text. Consider a document like [this paper](https://arxiv.org/abs/1809.00738). How do you make that look ok and reasonably consistent across devices without resorting to LaTeX, and by extension PDFs? HTML rendered by browsers isn't good enough yet: [nlab pages](https://ncatlab.org/nlab/show/end), for instance, don't look the same across different browsers on my phone. It's totally unreadable in some. Also, I don't agree that paper is dead or dying, but that's not a point I'm going to argue.
how does this manage font rendering, line balancing (justified text, orphans and widows) and hyphenation? those are the areas where i perceive the largest advantages to latex or libreoffice as dependencies, and that's where a good substitute would be good to know.
Well done! I feel this deserves mention in TWiR!
I read papers on my phone in PDF format at least twice a week. It's working fine for me. The format has issues but being workable on a phone isn't one of them.
Not arguing against that. For me the higher level languages are used wherever they can be used, and the low level ones for performance. I mean a lot of games use scripting languages for the game logic. And a high end graphics engine to do the heavy lifting
The paper mentions that the mentioned techniques can be applied to any compiler IR, which could include MIR
I think the plan is that rustup will only upgrade to nightlies without broken tools if you have them installed, but there were bugs in the implementation of that.
I think "inherent method" is the term you're looking for.
Reddit doesn't use GFM, but the (slightly modified) [original Markdown syntax](https://daringfireball.net/projects/markdown/). Use four spaces to format your code.
That's possible, yes. I don't have an example for that specific case, but [one](https://github.com/sdroege/gstreamer-rs/blob/master/examples/src/bin/appsink.rs) that takes raw audio buffer by buffer. This could be changed to a different pipeline, e.g. `uridecodebin uri=file:///path/to/video ! videoconvert ! jpegenc ! appsink` and then you would get one JPEG per buffer. However as you target WASM, GStreamer is probably not the solution you're looking for. It's a C library and these are only Rust bindings around that C library, not a Rust-only re-implementation. Compiling GStreamer to WASM is probably going to be non-trivial and will require code changes, I'm not sure if someone ever tried that.
Thanks, I'll think about this. I think I might just pay for the big clone, and benchmark to see if I need to do this more complex thing, as it is reasonably horrible :)
So I have a crate ([reffers](https://crates.io/crates/reffers)) of a few interesting smart pointers I made, and I have just freshened it up to take advantage of new Rust stuff such as the new stable allocation API. However, docs.rs now refuses to provide docs for my crate because [its Rust compiler is too old](https://github.com/onur/docs.rs/issues/233) and now I'm not sure what to do about that - should I 1) make some workaround specifically for docs.rs, or 2) set up github pages and provide docs that way 3) Find an alternative to docs.rs (is there one?) 4) just wait and hope that the docs.rs compiler is updated soon? 
Thanks! I'll give it a go! Also I wasn't expecting an explanation from the os's perspective too! You're the best :) 
&gt;Probably shouldn't heart the community if you don't even know what community it is. What is the point of this, other than being incendiary? They obviously thought it was a Rust community and was mistaken.
Yes, but id like to note that epub, markdown, and most HTML is responsive to screen size, while PDFs are not. I would never distribute anything I want people to read anywhere in pdf format. When you consider the number of non desktop devices, I have to agree with the other poster, fixed layout format like PDF should have died a long time ago. 
Rust is a hugely popular game. So presumably \_most\_ of that community is doing the right thing, or we would see \_many more\_ posts like this here than we do. There's always going to be some rate of careless folks, no matter the community.
I agree that it being as ridigd as it is (and also inherently page focused) is less than ideal, but pdfs via latex are the only consistent way to typeset mathematical papers that I know of. This *is* changing, but I don't agree that the support for rendering formulae in epub or HTML is good enough to be a serviceable replacement yet.
The explanation for why matching one works, but matching tuple does not: The main point is this: `match x { ... }` does not move `x` (as long as you don't move out parts of `x` in patterns). For example, [this works](https://play.rust-lang.org/?gist=9f7ad0faf07f3a6b0f270abeda147031&amp;version=stable&amp;mode=debug&amp;edition=2015), you can still use `foo` after matching on it: #[derive(Debug)] // not Copy enum Foo { A, B } fn main() { let foo = Foo::A; match foo { Foo::A =&gt; println!("Foo::A"), Foo::B =&gt; println!("Foo::B"), } // match did not move foo, can use it here println!("foo = {:?}", foo); } When you do `match *x { ... }` it desugars to `match *x.deref() { ... }`, where `deref` returns a reference. Because your type is not `Copy`, you cannot just dereference that reference and then move the value out, but because you are just `match`ing it you are not moving it anyway. When you do `match (*x, *y) { ... }` it desugars to `match (*x.deref(), *y.deref()) { ... }`. Now dereferences are not argument of match, but (kind of) arguments of tuple constructor. Here the compiler does it in a straightforward way - it tries construct the tuple, and then use that tuple for matching. However, to construct a tuple you need to actually move the value into the tuple, but you cannot do that - the place which you are moving from is borrowed, not owned by you. The compiler could cheat when you construct a tuple to immediately match it, and compile it to something similar to matching values independently. However this would put a weird case in the language where matching some constructs would work differently that others, so it probably won't ever be implemented. Solution proposed by /u/therico works because it constructs a `(&amp;T, &amp;T)` to match on instead of `(T, T)` - references to the values instead of moving the values into the tuple. Due to recently stabilized *match ergonomics* feature you can match references directly with a pattern `T(_)`, so it works without modifying match branches (at least in this example).
Yes, because they're all semantic markup. You're specifying the *meaning* of the information and letting the reader software apply a stylesheet to get appearance. That has two problems: 1. It's still not as expressive as PDF for certain formatting details. 2. It requires that the program the user used to create it has a certain semantic awareness, or a means to fake it. That second part is a real killer. PDF is based on PostScript, which has been and continues to be the native language of various higher-end printers. A print driver which losslessly transforms from your application's Print command to a PDF is not that difficult and can be guaranteed to work. Doing the same conversion to get the kind of HTML you want isn't as easy. (Yes. It is theoretically possible to get close... but it may involve generating an HTML+CSS mix that uses absolutely-positioned bits that don't reflow and text that "l o o k s l i k e t h i s" because it was outputting each glyph separately at absolute coordinates and who knows what else. Garbage in, garbage out.) That's why there are several LaTeX-to-HTML converters and all have limited compatibility. Arbitrary TeX commands don't have an inherent semantic meaning that can be automatically translated.
I've been wanting to get more into VDFs, especially the "trustworthy generation of public random numbers" and their application. It's over my head atm, but looks very interesting.
Many markdown readers and writers have latex support. 
Great so see a new approach! It would be awesome to have a backend that ca be used by [mdBook](https://github.com/rust-lang-nursery/mdBook).
Never had a problem with PDF on a phone. Consider graduating from babyphone maybe?
Huzzah, thank you very much!
Oh man if we only had this in the 2000s when each and every client insisted on some PDF features on their little websites...
Wondering the same thing. I wish stuff like ePub or other packaged HTML+CSS solutions were more prevalent.
Thanks for sharing. Working on a new language as well so it is great to see how other folks approach certain problems. Good luck!
In Ruby, there's a method `Object#tap`, which is basically just `Iterator::map` called on exactly one item: `self`. This has a fairly trivial implementation in Rust: pub trait Tap: Sized { fn tap&lt;T&gt;(self, func: impl FnOnce(Self) -&gt; T) -&gt; T { func(self) } } impl&lt;T&gt; Tap for T {} Is this already in the standard library or a commonly used crate?
C has quicker compile times than Rust, generates smaller binaries, and you can just link to the distro-provided precompiled libcurl. I had a library from the pre-reqwest days that used hyper 0.9 directly to download stuff. One day I updated my distro and the C dependency broke... but hyper 0.9 was obviously far too old to receive a fix in its dependency tree for that openssl bug. Instead of using reqwest, updating my minimum rustc version requirement, and possibly getting another compilation failure, I opted to using [libcurl](https://github.com/est31/test-assets/commit/797b4ac853a4370a3e4fbb60abfff41278447453) instead. Just look at the number of added lines vs the number of removed lines in [this](https://github.com/RustAudio/lewton/commit/b10623e2b4678fb8ff6d6009833fbadcd2f39a4c) mostly Cargo.lock commit. I didn't need to handle 10 thousand requests per second... this is just a utility of a utility of my actual program. So I guess I opted for stability, and libcurl is more stable than hyper. If I'm getting any issues with libcurl as well, I'm seriously considering shelling out, aka invoking whatever http(s) binary is available (curl, wget, etc) from rust and providing a library-esque interface around it. Maybe something like that exists already.
I entered a special kind of hell when I built a compiler in FSharp that interacted with LLVM many years ago. Would get random segfaults all the time, especially if you gave it invalid code :(
Impressive results! What surprises me most is your development time (really short)
ffmpeg and gstreamer has some feature overlap, but you can do much more with gstreamer than ffmpeg. They are not very comparable.
Thank you, good luck with yours too!
This confirms that pretty much only the macOS toolchains have lldb... This needs to be stickied or something! 
This reminds me of how Kotlin handles full stack projects: a Kotlin/JVM component for the backend, Kotlin/JS or Kotlin/Native part for web and mobile frontends... Except this looks like it's handled better by cargo
Oh, well, it seems that the rendering engine is, indeed, implemented (derived from XeTeX) in C. Good catch, I didn't notice that.
&gt; I think I want something like "int" that is the same as the default in assignments. There is not really a default. `let x = 6;` means `x` can be any primitive integer, Rust will try to infer which one based on how `x` is used elsewhere. If it can't infer a type, it defaults to `i32`. (This is a design decision, there was a time where compilation would just fail instead.) In Rust, type inference only works locally, for function arguments all types have to be specified. Again, this is a [design decision](https://www.rust-lang.org/en-US/faq.html#why-arent-function-signatures-inferred), there are other languages which have global inference, including for function arguments.
The reason for using Foo or self is simply because it's adapted from code which has a meaningful self argument. You're right that for free standing functions you can just use `()` as a stand in. Using macros for different number of arguments is cheating though :P The 'canonical way' to support multi arguments is (ab)use tuples even in the argument position but this makes invocation a bit more ugly. The Fn traits are actually defined using tuples for arguments underneath but this is hidden with stability attributes and syntactic sugar. I'm not sure about associated_type_defaults, it's not stable so uh idk? I also don't know why you need it? Further you aren't necessarily restricted by 'type parameters must be in the input, associated types must be the output of the function', you can mix it up however you wish. Here's an edited version: [playground](https://play.rust-lang.org/?gist=e78f33ec7803aaff9d6dfbc7c65df1e6&amp;version=stable&amp;mode=debug&amp;edition=2015).
Also GStreamer can make use of ffmpeg for supporting various formats. The GStreamer ffmpeg plugin is one among many provided by the GStreamer project.
Nice pictures! :) I just looked into some of the code files and skimmed the content. #Rust style: You use `match` on an Option which could also be an `if let` to reduce rightward drift and save two lines since you don't care about the `None` case: if let Some(mat) = hit_record.material.clone() { let (scattered, attenuation, valid) = mat.scatter(ray, &amp;hit_record); if valid &amp;&amp; depth &lt; 100 { let col = color(&amp;scattered, world, depth + 1); return Vector3::new( col.x * attenuation.x, col.y * attenuation.y, col.z * attenuation.z, ); } else { return Vector3::new(0.0, 0.0, 0.0); } } Even though cloning an `Option&lt;Rc&lt;_&gt;&gt;` is very cheap, you could still get around to doing it: if let Some(ref mat) = hit_record.material { // now, mat is of type &amp;Rc&lt;Material&gt; } You could also leverage a new "match ergonomics" feature: Rust kind of automatically adds `&amp;` and `ref` into the pattern in certain cases: if let Some(mat) = &amp;hit_record.material { // &amp;Some(ref mat) is the equivalent pattern // now, mat is of type &amp;Rc&lt;Material&gt; } As for parallelization, I would guess your `main.rs` would get simpler if you used the `rayon` crate and its parallel iterators. #Correctness concerns: This function and its use in other places look wrong: fn random_in_unit_sphere() -&gt; Vector3&lt;f32&gt; { Vector3::new(random::&lt;f32&gt;(), random::&lt;f32&gt;(), random::&lt;f32&gt;()).normalize() } `random::&lt;f32&gt;()` gives you a random value from the half-open set [0,1) with a uniform distribution. You do this for every dimention ending up with a "non-negative" cube, then you project all points onto the sphere surface. This gets you a probability density function that's 0 for 87.5% of the sphere (since you don't have any negative numbers) and a positive but *non-uniform* density for the non-negative sphere coordinates since the points outside of the sphere skew the distribution. You use this function for creating "lens samples" (which I don't understand the motivation of. Shouldn't you sample a disc instead?) and you use this within `impl Material for Lambertian`, too, which seems wrong. For the Lambertian you should probably use importance sampling based on the cosine factor which turns out to be pretty simple and easy. Again, you would uniformly sample a 2D unit disc: fn uniform_random_2d_disc_sample() -&gt; Vector2&lt;f32&gt; { // There are many possibly more efficient ways of doing this. // This approach is "rejection sampling". loop { let v = Vector2::new(random::&lt;f32&gt;() * 2.0 - 1.0, random::&lt;f32&gt;() * 2.0 - 1.0); if v.magnitude2() &lt;= 1.0 { return v; } } } fn index_of_smallest_magnitude(vec: Vector3&lt;f32&gt;) -&gt; usize { let ax = vec.x.abs(); let ay = vec.y.abs(); let az = vec.z.abs(); let tmp = if ax &lt; ay { (ax, 0) } else { (ay, 1) }; if tmp.0 &lt; az { tmp.1 } else { 2 } } fn orthonormal(a: Vector3&lt;f32&gt;) -&gt; (Vector3&lt;f32&gt;, Vector3&lt;f32&gt;) { let mut b = Vector3::new(0.0, 0.0, 0.0); b[index_of_smallest_magnitude(a)] = 1.0; let c = a.cross(b).normalize(); b = c.cross(a).normalize(); (b, c) } fn cosine_weighted_hemisphere_sample(normal: Vector3&lt;f32&gt;) -&gt; Vector3&lt;f32&gt; { let (u, v) = orthonormal(normal); let r = uniform_random_2d_disc_sample(); u * r.x + v * r.y + normal * (1.0 - r.magnitude2()).sqrt() } I didn't test this but I hope you get the idea.
The following compiles. The `Box`es and the 2 lifetime parameters on `VideoStream&lt;'a, 'b&gt;` seem less than ideal, but it's probably the best you can do with the current libuvc-rs API. Personally, I'd avoid doing this if possible, and keep it all on the stack (ie pass the stream to another function or to a callback instead of returning it). These self-referential structs are a weak point of rust. #[macro_use] extern crate rental; extern crate uvc; use uvc::{Context, Device, DeviceHandle, FrameFormat, StreamFormat, StreamHandle}; use rental::RentalError; rental! { pub mod device { use uvc::{Context, Device, DeviceHandle, FrameFormat, StreamFormat, StreamHandle}; #[rental(debug)] pub struct VideoStream&lt;'a, 'b&gt; { ctx: Box&lt;Context&lt;'a&gt;&gt;, dev: Box&lt;Device&lt;'ctx&gt;&gt;, devh: Box&lt;DeviceHandle&lt;'dev&gt;&gt;, streamh: StreamHandle&lt;'devh, 'b&gt;, } } } pub fn get_stream &lt;'a, 'b&gt; () -&gt; device::VideoStream&lt;'a, 'b&gt; { device::VideoStream::try_new( Box::new(Context::new().expect("Could not create context")), |ctx| ctx.find_device(None, None, None).map(Box::new).map_err(|e| format!("Could not find device: {}", e)), |dev, _ctx| dev.open().map(Box::new).map_err(|e| format!("Could not open device: {}", e)), |devh, _dev, _ctx| { let format = devh .get_preferred_format(|x, y| { if x.width == 640 &amp;&amp; x.height == 480 &amp;&amp; x.fps == 30 &amp;&amp; x.format == FrameFormat::Uncompressed { x } else { y } }).unwrap(); devh.get_stream_handle_with_format(format).map_err(|e| format!("Could not get stream: {}", e)) } ).map_err(|RentalError(e, _ctx)| e).unwrap() } fn main () { let mut streamh = get_stream(); }
The following compiles. The `Box`es and the 2 lifetime parameters on `VideoStream&lt;'a, 'b&gt;` seem less than ideal, but it's probably the best you can do with the current libuvc-rs API. Personally, I'd avoid doing this if possible, and keep it all on the stack (ie pass the stream to another function or to a callback instead of returning it). These self-referential structs are a weak point of rust. #[macro_use] extern crate rental; extern crate uvc; use uvc::{Context, Device, DeviceHandle, FrameFormat, StreamFormat, StreamHandle}; use rental::RentalError; rental! { pub mod device { use uvc::{Context, Device, DeviceHandle, FrameFormat, StreamFormat, StreamHandle}; #[rental(debug)] pub struct VideoStream&lt;'a, 'b&gt; { ctx: Box&lt;Context&lt;'a&gt;&gt;, dev: Box&lt;Device&lt;'ctx&gt;&gt;, devh: Box&lt;DeviceHandle&lt;'dev&gt;&gt;, streamh: StreamHandle&lt;'devh, 'b&gt;, } } } pub fn get_stream &lt;'a, 'b&gt; () -&gt; device::VideoStream&lt;'a, 'b&gt; { device::VideoStream::try_new( Box::new(Context::new().expect("Could not create context")), |ctx| ctx.find_device(None, None, None).map(Box::new).map_err(|e| format!("Could not find device: {}", e)), |dev, _ctx| dev.open().map(Box::new).map_err(|e| format!("Could not open device: {}", e)), |devh, _dev, _ctx| { let format = devh .get_preferred_format(|x, y| { if x.width == 640 &amp;&amp; x.height == 480 &amp;&amp; x.fps == 30 &amp;&amp; x.format == FrameFormat::Uncompressed { x } else { y } }).unwrap(); devh.get_stream_handle_with_format(format).map_err(|e| format!("Could not get stream: {}", e)) } ).map_err(|RentalError(e, _ctx)| e).unwrap() } fn main () { let mut streamh = get_stream(); }
Tldr; RustByExample and other code snippets are super important.
Gstreamer solves all your media problems. Side effects include: * Suicidal thoughts from trying to get it to work. * Heightened blood pressure from reading unhelpful error messages.
I have got a question, not specifically about Rust, but more generally about asynchronous programming. In my specific case, I am doing IO on several USB endpoints and doing so synchronously is not trivial, because of the ordering of messages on the endpoints that can be non-deterministic. But the USB library that I use doesn't have an async API, so I can only do blocking request that timeout if not successful. I thought about this a bit and wondered if I could do it differently: store in a queue all of the different messages I am expecting at the moment, along with a continuation function for each of them, cycle through this queue and when a message is ready, execute the continuation function associated with it, which in turn might push more requests to the queue. I have many questions, because I don't really understand async yet: Is this a very crude way to do async, over a sychronous IO library? Would it actually help me? Is there a way to do it better? How does that differ from the official implementation of async/await and how would that work if the USB library had an async API?
Sure, it works quite well for *writing* documents. I typically write things in pandoc's markdown, and convert it to pdf, myself. But I'm sceptical about markdown readers. Mainly because we run into consistency/standardisation problems: Do markdown readers that support latex generally agree on the syntax? On the subset of latex supported? Do they render the things they support in the same (or a comparable) way? Work is being done on standardising both markdown and math typesetting in epub and on the web. But we're not quite there yet, imo. I do hope we'll get there though. Pdf/Latex forcing a page based notion typesetting down my throat gets on my nerves constantly.
The use case is actually the opposite of UFCS; allowing functions or closures that take a named `self` parameter to be chained as if it were a method. It's especially useful when dealing with a chain of methods on a tuple. A contrived example: assert_eq!( ("ello", 5), ("hello", 6).bind(|(s, n)| (s[1..], n - 1)) );
Hi! What is the best rust IDE with debugging for windows?
Sorry, I misremembered what `Object#tap` did; I have since edited my post. Thank you for the long and detailed answer, though.
It would be _awesome_ to have that as a reusable rust crate :)
Interesting read but why not use an existing parallelized PNG compressor e.g. [oxipng](https://github.com/shssoichiro/oxipng)?
I also think they've fixed it, but not everyone wants to stick to the latest nightly :)
About number generation (random numbers beacon) you can take a look on Unicorn protocol from the [paper](https://pdfs.semanticscholar.org/f9b1/d432d2b8098be527f1ece5706b7d328753dc.pdf) about Sloth VDF, it's pretty straightforward and can produce publicly verifiable randomness.
I think we need: * boxes-and-glue system * knuth’s line breaking algorithm * all the settings and customizations for this system based on the microtype package * the ability to slightly compress and expand font shapes based on those computations ([python impl 1](https://github.com/akuchling/texlib), [python impl 2](https://github.com/brandon-rhodes/python-bookbinding/blob/master/bookbinding/knuth.py))
I'm guessing existing parallelized PNG compressors (e.g. oxipng) were avoided because the goal was to generate a PNG quickly rather than generate an optimised PNG?
The short answer is that it doesn't. Currently it just writes words to a line until it overflows, and then moves to a new line. I want to get around to implementing cool stuff like that, but currently my focus is on generating PDFs for proofreading, which doesn't require there text to look beautiful.
nice job! would be nice to see some more benchmarks also with Rust libs like oxipng and png crate ([https://github.com/PistonDevelopers/image-png](https://github.com/PistonDevelopers/image-png)) from Piston which is used in image lib [https://github.com/PistonDevelopers/image/blob/master/src/png.rs](https://github.com/PistonDevelopers/image/blob/master/src/png.rs) in the end if mtpng is way faster i might replace png encoder in my own project
I think this study points out that documentation should be more like it used to be back in the printed documentation days for proprietary languages: * Volume I: Intro to the Language (TRPL is here) * Volume II: Advanced Usage (RustNomicon is here) * Volume III: Standard Library/Language Reference (Rust Docs) * Volume IV: ... Here with Volume III it is important that the "reference" include explanation AND example of every single class, function, method, macro, etc. with some context for usage. Rustdoc provides the infrastructure to accomplish this systematically, but, it is up to the developers of crates to ensure good examples. &amp;#x200B; Part of the problem, to my mind, is that it is not more highly promoted this "Volume" sort of organization of the learning materials. It really does exist, but, it could be made more prominent within the community if you asked me.
I haven't seen this myself; it sounds like a bug to me.
On my machine oxipng is *way* slower, but produces better compressed PNG even using the lowest possible settings e.g. the 4k screenshot of mtpng's samples is 1.6MB base, still 1.6MB after mtpng, 1.5MB after oxipng on the lowest possible settings (or `--zc 1 --zs 0 -f 0` at least which seems like that) and 1.2MB after oxipng using default settings. However this is done in respectively 1s (as reported by `time`, mtpng claims ~500ms); 6s and 12s (I removed the entire thing and cleared my buffer but it was about that).
Does anyone know if the 3-part book series is available in hard cover?
Not really a question but I just realized that there's an edge case where Rust's abstractions aren't zero-cost yet 😛. You can't write a type-safe zero-cost function with the signature `Vec&lt;NT&gt; -&gt; Vec&lt;T&gt;` where NT is a newtype wrapper around T.
I’d guess very limited. One of the dependencies is https://crates.io/crates/pdf-canvas whose README says &gt; Currently, simple vector graphics and text set in the 14 built-in fonts are supported. Its text APIs appear to be along the line of “render this string at those (X, Y) coordinates using this one font”, with an implementation that maps each Unicode code point to a single glyph and moves the cursor by the glyph’s advance width. So no kerning, no ligature, etc. This places an upper limit on the complexity (quality?) of text rendering and layout that could potentially be achieved `pdf-canvas`. But `mdproof` so far is 5 days olds and has 30 commits, so it probably only makes straightforward use of that API. ---- I do not mean to bash on either of these projects! Quality layout and rendering of international text is *hard*. While the Rust ecosystem already has some of the very low-level pieces that would be needed (the `xi-unicode` and `unicode-bidi` crates, Harfbuzz bindings, …) we do not yet have a good solution for gluing them all together.
Really enjoying these.
also, inline errors in editors are v important
I've commented similarly before. I, too, share this sentiment. One of the reason's I do any programming at all in Rust (mostly use JS/TypeScript for work) is because of how its constraints force you to deal with fundamental programming concepts. Sometimes The goal for me is to "have a bad time" so to speak. Much to be learned!
Sorry, I should have been clearer! I’m only writing the code to consume the export service, so I don’t get to control the data sent from the server. I know that the service does have some JSON endpoints, so I think the best bet might be to just see if I can get the data I need from them instead, and ignore the CSV ones!
Can you give us some more context on this? How does \`append\` make it faster?
&gt; I'm working on a path-handling library, I'm interested in writing one as well. I'm curious, what are your goals for your library? I'm wondering if there might be room for collaboration.
&gt; Out of curiosity, what prevented you from using the full Rayon Iterator solution? I'm guessing they didn't want the dependency, seeing this excerpt from the section on C: &gt; I would either have to implement some threading details myself, or find a library to use… which might introduce a dependency to the GNOME platform.
So i think RustByExample and the Cookbook needs a prominent placement on the new Rust website for the 2018 Edition. 
Thanks :-)!
That might be worth setting up, but it looks somewhat different than what I was getting at -- r2d2 is a connection pooling library, so you don't open too many database connections. I was planning on doing database transactions in a threadpool so they don't block the event loop. You probably want both in a production app :-)
Considering this question on stack overflow: [https://stackoverflow.com/questions/42309597/cannot-infer-an-appropriate-lifetime-for-lifetime-parameter-while-implementing-d#42310309](https://stackoverflow.com/questions/42309597/cannot-infer-an-appropriate-lifetime-for-lifetime-parameter-while-implementing-d#42310309) &amp;#x200B; We've got this code: use std::ops::Deref; pub trait Trait {} pub struct S {} impl Trait for S {} pub struct Container(S); impl Deref for Container { type Target = Trait; fn deref(&amp;self) -&gt; &amp;dyn Trait { &amp;self.0 } } This does not compile as the "lifetime must also be valid for the static lifetime". Where does this static come from? The definition of Deref does not require this.
Yes, I think that's fixed. Rustup not updating nightly preemptively because some component was missing was a nice surprise a few days ago!
NM, figured it out: https://play.rust-lang.org/?gist=e11836f0212da810c8b3a6232bbbc043&amp;version=stable&amp;mode=debug&amp;edition=2015
A good modern reimplementation of Knuth's hyphenation and line breaking algorithms is in [Minikin](https://android.googlesource.com/platform/frameworks/minikin/). Fun story, I actually prototyped the hyphenation algorithm in Rust to experiment with CPU and memory use (on Android, memory is dear), then reimplemented in C++.
`+` isn't defined between `i32` and `u8`, so type inference will find which integer type can fit as the second argument to `+` with a `u8`. The only type is another `u8`, so that's the type it infers for `2` at compile time. (And then constant propagation and folding will change this to let x: u8 = 5; let z: u8 = 7; and just inline their values wherever they're used.)
Sorry fellas to I haven't read the description like you said! Im sorry about that a /\*reeeeeeeeeeeeeeeeeeeeeeeeeeeeee\* moment for me lol have a good day will not spam again 
That's not particularly surprising. \`mtpng\` seems to be a simple PNG encoder focused on speed. \`oxipng\` is focused on optimizing PNG files for size, and while it attempts to be fast and uses parallelization, it performs a lot of extra checks to try and reduce the filesize. I would consider \`mtpng\` to be more of a competitor to the \`png\` crate than to \`oxipng\`. That being said, the techniques from \`mtpng\` could probably be applied to give some speed boost to \`oxipng\`.
&gt; One way to make compiler error messages more helpful to all users would be to link the relevant conceptual information to the error message. This way, when users do not understand an error message, they are able to read about the underlying problem, rather than receive additional unhelpful information. That looks actionable. We could add a line to the top of each EXYZW explanation like: &gt; Understanding this explanation requires understanding: &gt; &gt; * ownership: &lt;link to the book section on ownership&gt; &gt; * traits: &lt;link to the book section on traits&gt; &gt; * associated types: &lt;link to the book section on associated types&gt;
The problem is basically what the error message says: your trait impl claims that output is `Point2D&lt;T&gt;`, so coordinates have to be of type `T`, but they are of type `&lt;T as Add&gt;::Output` - nowhere you say that those two have to be equal. First step to fixing this is changing trait bound on T from `T: Add` to `T: Add&lt;Output = T&gt;`. Once you do this, you will face another error: you are adding references, but to add `self.x + other.x` you need to move `self.x` and `other.x`, which you can't do. One possible fix to this is changing the bound to ask for addition over references: `&amp;'a T: Add&lt;&amp;'b T, Output = T&gt;`. Another could be restricting addition only for points where `T: Copy`, but I think this would be more limiting than previous alternative.
It takes half a second of reading to realize this sub isn't that one. This is a "Don't measure, cut freehand... While distracted" approach to karma whoring. 
FYI I implemented Unicorn protocol example for verifiable random number generation: [https://github.com/eupn/unicorn-p2p-rng](https://github.com/eupn/unicorn-p2p-rng/)
Maybe in addition to that, the output of all the `--explain` messages could be put in some way on the Rust doc website? With a bit of luck, those would end up getting good Google-fu and become the first hit (or at least in the top hits) for people who will keep on googling rustc's error messages.
An answer has already been provided by /u/jDomantas, however, it might be beneficial to look at the Rust documentation, as the std::ops::Add trait describes a very similar example: https://doc.rust-lang.org/std/ops/trait.Add.html#implementing-add-with-generics
Its already there: https://doc.rust-lang.org/error-index.html
`'static` come from `type Target = Trait;`, which actually means `type Target = dyn Trait + 'static;`. Now, because of lifetime elision, current signature says `fn deref&lt;'a&gt;(&amp;'a self) -&gt; &amp;'a (dyn Trait + 'a)`, but it should be `fn deref&lt;'a&gt;(&amp;'a self) -&gt; &amp;'a (dyn Trait + 'static)` - the returned type has to be the same as `Target`. Thats why both of these work, because they have the exact same type as `Self::Target` in the signature: // writing out exact same type as Target fn deref(&amp;self) -&gt; &amp;(dyn Trait + 'static) { ... } // just using Self::Target directly fn deref(&amp;self) -&gt; &amp;Self::Target { ... } The error message is somewhat confusing to me - it's not actually about lifetime inference, just elision. The actually relevant part of the error is the note at the end: = note: ...but the lifetime must also be valid for the static lifetime... = note: ...so that the method type is compatible with trait: expected fn(&amp;Container) -&gt; &amp;Trait + 'static found fn(&amp;Container) -&gt; &amp;Trait
I'm assuming the terminology is borrowed from Haskell -- there a newtype is primarily a structural thing. It may not be behaviourally the same. For example, the standard library has a max heap. Say you want to put costs (say an integer) in the heap and pick out cheapest things quickly. You have to "define a newtype" (in Haskell terms) with a reversed Ord impl to get a min heap. If you're saying that Haskell and Rust use the same term "newtype" to mean different things, then TIL thanks!
Thanks. I'm coming from a mainly C# background so perhaps I'm thinking about things the wrong way. It seems odd that I'm allowed to store a reference to a single trait but not multiple, but I like your suggestion of creating my own trait as it would allow me to work around this. Making it generic sounds good in theory but in practice wouldn't this mean that any struct that also references it would need to inherit the same generic params, and those of any other generics it references? This is going to be more of a utility struct but perhaps when the application grows, I could use a service locator pattern to avoid the above.
I'm not experienced with (coding) this kind of software. What specifically about Rust is it that makes/made you switch over to Rust? Thanks in advance! :-)
Oooh, no you're right. I just couldn't think of an example. Rust has [Reverse](https://doc.rust-lang.org/std/cmp/struct.Reverse.html) to implement this reverse ordering you describe. In this sense I'm not sure how you can make sense of `Vec&lt;T&gt;` &lt;-&gt; `Vec&lt;NT&gt;` conversions in an automatic fashion. It will certainly heavily depend on the particular type and its newtype to determine if this is a sane thing to do.
`std` is whatever is in [the `std` crate](https://github.com/rust-lang/rust/tree/master/src/libstd). Basically, everything you see in the docs for `std` that isn't also in `core` (which `std` re-exports for convenience). `no_std` exists because `std` assumes you have an operating system with things like a memory allocator and a file system. This assumption is not true on things like microcontrollers, or when you *are* the operating system. Rust automatically links `std` by default, so there has to be a switch to turn it off, or Rust wouldn't be usable in those places. As for what people do when they write `no_std` code... they either don't use those constructs, or write their own.
Interior mutability is `Cell`. But the typical way to do this is to shorten your slice! Parsing in Rust is typically represented as fn parse_T(&amp;[u8]) -&gt; Result&lt;(T&lt;'_&gt;, &amp;[u8]), Error&gt; where the result is parsed off the front of the buffer and the remaining unparsed part of the area is returned. Then, rather than passing around an offset into the slice everywhere, you just only pass the slice after that offset; you don't need the slice before the offset anyway. (If you need some context from early in the pipeline this is either managed by the call stack context or passed in as a separate argument.) More sophisticated parsing will often use some sort of `Cursor&lt;'_&gt;` or `ParserState&lt;'_&gt;` passed into and out of these parsing functions, but it's typical to model parsing as pure functions like this. Check out the way that `syn`, `nom`, or `combine` structure their parsing framework. If you're feeling ambitious, check out `pest` or even `lalrpop` or `libsyntax2`. You might learn a few cool tricks.
Why are you using `std::list`? Is there a reason to not use `std::vec`?
`std` is the standard library of Rust. `#![no_std]` applications are used on bare metal devices because those devices generally don't really have an OS, or a network, or a memory allocator, or anything else the standard library kind of expects to be there, since it's mostly targeted at general purpose computers. You can do anything without the standard library that you can do _with_ the standard library - in fact, the standard library itself is written in Rust - but having it already be there is generally more convenient. Vectors and other data structures are also part of the standard library, but they are technically separated into a lower level library called `alloc` that can run on anything that provides a memory allocator, so you can use them without pulling in the rest of `std`.
The [pull request](https://github.com/projectatomic/rpm-ostree/pull/1509) talks about this a bit. More rationale: We currently have a small C library, though our primary interface is via DBus and the command line. Having a C library though rules out us carrying along a language with a garbage collector. Even if we didn't have that C API though, it's more like: how could one write systems software like this and not spend every day wishing Rust had been invented a long time ago? =) Another strong point of Rust is interop/FFI with *other* C libraries - the libdnf (RPM) stack is primarily C (although that [may switch](https://lwn.net/Articles/750355/), and so is libostree (i.e. the package and image libraries). But it's really mostly about being software that manages the root partition of my hard drive and our customer's, and wanting that to be in a modern memory safe, efficient language.
Your answer is very helpful! Returning remained slice makes sense to me. 
Yes.
[The Rust Cookbook](https://rust-lang-nursery.github.io/rust-cookbook/) presents useful examples and introduces some crates.
Thanks for the explanation. It makes total sense and is along the lines of what I expected to hear. 
I see, thx
`no_std` can also be useful in programs that work in std. I work on the [nom parser combinators library](https://github.com/geal/nom), and being able to ensure no allocations would happen in a parser (which is usually in the hot path of a program) is very useful for performance.
I hope not! One of the truly amazing feats of Rust is that many of its abstractions/concepts are Zero-Overhead. It is not the only language which has such abstractions, of course, however it has so far managed to nudge the bar that much further. Statically guaranteeing the absence of data-races is *rare*. The `Future` trait itself imposes no run-time penalty. The `-&gt; impl Future&lt;Item = ...&gt;` feature itself imposes no run-time penalty either... so then it's up to the programmer not to introduce any :)
Plus, we also do casual lunches every Friday in Kendall Square in Cambridge, if that's more your style: https://www.meetup.com/BostonRust/events/rpmnfqyxmbsb/ . Hope to see you there! :)
Doesn't that apply to everything and nothing at the same time?
Just to correct `std::vector` is the right name.
hmm? If what you do is so custom that there does not exist a library to build around, then you are designing a new system. Sure, the kernel uses some libraries but the really core parts (maybe comparing to microkernel?) are low level stuff that simply have no libraries and you design it from the ground up, often even dropping down to assembly since C is just more pain at that level of interaction.
To be precise, no-std means you can't allocate from the *heap*, like with `Box` or `Vec`. The heap lets you go "I need this much memory!" at runtime, and later "I'm done, you can have it back!" Microcontrollers still have memory that you can allocate in the general sense of the word, since you can reserve space for things, and you can still put values on your stack. It's just that sometimes you don't have a heap, so you basically have to manually say where things go.
&gt; Thank you! Stupid question, how do you program without a memory allocator? The memory allocator is for creating new heap allocation, so you either don't do heap allocations (only stack) or the caller has to provide buffers which they're responsible for allocating however they can and give you.
Sweet tool and long needed. Thanks for sharing. Maybe you can incorporate some of this logic directly into rustup.
Missed the point again
Thanks for this. It was a good reminder that I wanted to check out the source blog. 
If you intended to provide a link for the document to look at, it's not in your comment. I imagine if you're going to convert markdown to pdf (like OP's program does), you don't really need pdf for tricky math formulas. http://jkorpela.fi/math/ this shows that you can render math in html as plain text, I'm not sure how flexible that is, but every device has a modern browser with javascript support and unicode fonts, so it should be enough? As to how do you distribute that - I'm sure there was some format that packed html/css/whatever as a single package. Mhtml?
I was going to go but I start my new job Monday so I'll be in SF for the week before being back in Boston :( I'll try and stop by this Friday at least!
True, embedding fonts is cool. Not the same as formatting though, why would you want to freeze word wraps, hypens? Using html allows you to have both arbitrary *and* mandatory wraps if you so desire, which is much more flexible and human-oriented.
It might, theoretically, but currently the standard libtest do allocate dynamically.
Fixed, thanks :).
I'm a bit confused about what your goal is based on your code. You want to be able to pass in a function that does what exactly?
Your todo says you plan to speed it up by not using `regex`. It's possible that you can be faster than it, but profile first! I suspect regex is nowhere close to being the slowest part of the tool.
Cool! I'm probably doing something wrong, though. Clippy should be available, by the looks of it, However, running Nightly 2018-09-11, and doing `cargo install-update -a`, Clippy fails to compile. I used to think it was just not available currently, but your table indicates otherwise.
You can actually use the simple hack you used for the 2d disc sample to get a (for this case) cosine weighted sample as well. If you pick a random uniformly distributed point `r` inside the unit sphere by rejection sampling, then arrange it such that a point `p` is on the surface of that unit sphere, if you cast a ray from `p` through `r`, you get a cosine weighted average around the vector from `p` to the center of the sphere. This is what is done in the book, replacing `p` with the intersection point and the center of the sphere with `p + n` where `n` is the normal vector from the point.
So I was reading the [nomicon](https://doc.rust-lang.org/nomicon/transmutes.html) and according to it transmuting the vector should be UB. &gt; Transmuting between non-repr(C) types is UB
Here's a practical example of the appeal of Rust in our use case: [https://github.com/projectatomic/rpm-ostree/pull/1508](https://github.com/projectatomic/rpm-ostree/pull/1508). This patch adds support for downloading files from the Internet using the [libcurl](https://github.com/alexcrichton/curl-rust) Rust bindings. We could've used those bindings directly ourselves from C, but using the `curl` crate instead means we get all the safety guarantees Rust provides (assuming the bindings are well implemented of course) at least for that part of the program.
Yeah, it's a different thing.. the webpage in the topic shows you what's there in rustup, i.e. if you install your components with the `rustup` tool, like `rustup component add clippy-preview`
Nice article! Mind to explain why Rust's standard library as a dependency is less of a problem than C++'s standard library? Is it because other GNOME parts already use Rust's standard library so you can reuse it?
no\_std produces a compile error when you try to compile though, some kind of allocator that always failed would be at run-time right? &amp;#x200B;
Interesting, so I can reproduce, but it's also not clear to me that this would work generally; aren't you still supposed to set `#[start]` somewhere? not 100% sure
If you find use in this library, then it's more than likely that someone else would as well. So I would say go ahead and publish it once you polish it and add documentation like you said. :) Some quick pointers about the code: - Some negative unit tests would bump up the coverage. - Since `Format` implements `FromStr`, you can call `"some string".parse::&lt;Format&gt;();`. - You might try and cut down on `String` allocations in `read_format` by returning a `Format` that has the same lifetime as the input slice. I would say you should at least eliminate the calls to `unwrap` here. Overall, the code looks great! Thanks for sharing.
Thank you, this helped me a lot!
42 MB is the RAM usage (at runtime). This is roughly how much a QT Quick application is using. The code size (binary size) is roughly 5 - 6 MB.
I could be mistaken, but I think the #[start] attribute would be an alternative to implementing the C-runtime functions by hand… I have used the blog by Philipp Oppermann as a guidance (https://os.phil-opp.com/freestanding-rust-binary/)
Ah that's probably right, yeah. Anyway, it sounds like you should file a bug?
I'm sure that it'll get sorted out in the issue :)
That's a good idea I'd say, let's see if I can get it integrated locally and send a PR on success :)
&gt; If you intended to provide a link for the document to look at, it's not in your comment. Ah, no, I just didn't want to link directly to the pdf file in case someone hit the link by mistake. The download link for the paper is in the sidebar on the right. Should probably have made that clear. My bad. &gt; I imagine if you're going to convert markdown to pdf (like OP's program does), you don't really need pdf for tricky math formulas. I frequently write things in markdown with latex spliced in for formulae and the like, using pandoc. But if you're not looking to to typeset that sort of thing, then yeah, pdf is mostly useful if you're interested in printing the resulting document. Or if you want to be sure that the document *doesn't* change depending on how you're displaying it. Which may be a desirable property in some cases. &gt; but every device has a modern browser with javascript support and unicode fonts, so it should be enough? Sadly, no. At least not yet. The nlab page I linked to looks ok on my phone using Firefox, but not using Chrome (chrome dropped support for mathml, I think). Trying to display maths that way still feels too brittle and feature-incomplete to me. Don't get me wrong, I'd love to have a better alternative than turning everything into a pdf. I don't like how printing focused the format is either: the way figures get shuffled around to reduce wasted page space (why do we need pages if we're not printing something?) will never cease to frustrate me. But the alternatives aren't up to feature parity yet (in my estimation, at least). So we're stuck with this for the time being.
OK, so it means that the command `rustup component add clippy-preview` is available, though it might not compile? 
Your `impl` blocks should use `fn` not `Fn`. `fn` is the type of a function. `Fn` is a trait representing callability and is implemented by both functions and closures.
CLion supports debugging Rust on Windows if you use the MingW toolchain which is the secondary one now. You'll need to install GDB in MingW manually. It's quite the hassle. I personally prefer to just develop in IDEA and then open the executable in Visual Studio (not VSCode) to debug. The experience isn't 100%, the Visual Studio debugger doesn't visualize enums well, but with some tinkering you figure out how it's interpreting the data and it works well enough for most things. I used it to debug some changes I made to `rustc` earlier this year.
Thanks for asking this, reading the comments helped me as well. I thought using `#![no_std]` would be "unsafe" or something if did not know what you were doing, but the only "hard" thing would be dynamic allocations if you would need those it seems. Will need to play around a bit.
[removed]
Oh, dear! I must somehow have had Clippy installed the wrong way, after all. After removing it, and then installing it with `rustup component add clippy-preview`, it runs! Now, how do I keep it updated?
Same error. You mean like: impl MyTrait for fn(f64) -&gt; &amp;'static str { fn get_ret(&amp;self) -&gt; String { format!("f64: {}", &amp;self(1 as f64)) } } right?
It’s usually compiled in statically.
Oh, cool! I didn't know. And that's what the OP was going for, apparently.
Pretty sure this: char* file_content = (char*) malloc(file_size * sizeof(char*)); should be this: char* file_content = (char*) malloc(file_size * sizeof(char)); ^ Nice post!
It is incorporated into your rustup tool.. If you are using the latest nightly (i.e. your default target is "nightly-x86...", not "nightly-2018-09-11-x86...") everything will get updated automagically when you run \`rustup update\`. If there's no clippy shipped with a new nightly, rustup won't let you update at all, so you're somehow safe. &amp;#x200B; If you're using a fixed nightly version, e.g. "nightly-2018-09-11-x86...", than you're more or less on your own, that is you have to install newer versions manually, and when you install a new toolchain you have add components again from scratches, and that is exactly the use-case for my [web page](https://mexus.github.io/rustup-components-history/): it lets you know in advance which nighly version of rustup contains all the packages (components) you are looking for.
To really understand German I have to read it out loud (yeah I know), but this was quite a good article, so I followed through :) Good to see that Rust is starting to get on a solid footing in Germany.
At our company we must work with no std of a special feature in Intels cpu called SGX. This SGX does allow regular std due to security https://github.com/enigmampc/enigma-core
Now that [https://github.com/miquels/webnis/](https://github.com/miquels/webnis/) is almost done, I've started to work on a [NNTP server](https://github.com/miquels/nntp-rs/). The plan is that it should be able to process a few 1000 articles/sec, taking in 5-10 Gbit/sec and pushing out 20-25 Gbit/sec of traffic. If the attached storage is able to handle hundreds of MB/sec, ofcourse.
They are [capturing sub-groups](https://github.com/themasch/nginx-log-parser/blob/9968d88d61dfcda1aebaaec54f6ec21a6a14465c/src/format.rs#L74), which is unfortunately an under-optimized part of the regex crate. It is probably not too hard to beat it with a hand rolled parser, although the performance difference may not be huge.
Huh, you're right. I found this which you might find helpful https://internals.rust-lang.org/t/implementing-traits-for-function-types/5888
Probably right. I guess there's really not much in std that's not in core that a parser could realistically use that doesn't allocate at any rate.
Sounds complex!
I didn't look closely at the code, but I compiled and ran it, and on my machine there is no noticeable lag. I didn't see any easy way to add circles to the grid though, so I was unable to see if circles made the difference as you had mentioned
&gt; Thank you! Stupid question, how do you program without a memory allocator? Carefully. (but seriously, you can still store stuff on the stack and as global variables) &gt; Is there some collection of common non-std crates people use? Yes, there's also a no-std tag on crates.io so that you can find them more easily.
I have checked that some time (and a few llvm versions) ago, and while it benefitted n_body, the other benchmarks were more or less unchanged. I should probably re-check.
Yeah, it's more syntactic sugar than adding functionality.
You could use [Vec::from_raw_parts](https://doc.rust-lang.org/std/vec/struct.Vec.html#method.from_raw_parts) also. [The generated code](https://play.rust-lang.org/?gist=eb4cc3831253c02099659b87520269c5&amp;version=stable&amp;mode=release&amp;edition=2015) has a bit of unnecessary data shuffling though.
&gt;TWi thanks! I think I'll make a proposal to spread it out on the next week :)
This book is fantastic, surely one of the best.
Thanks for the encouragement and the review. Making the Format borrow from the input is definitely an option. And I really need to get rid of those unwraps! When I wrote them, I thought that I really should replace them when cleaning up. Guess I did not ;). Thank you very much!
Yeah, I think (or hope) I do not need the full power of a regex engine here, although I may overestimate the benefit of a parser written by hand, especially by my hand :D Thanks for the comments, very much appreciated! 
Working on giving type-level context to stuff using wrapper types (for data parsing using the scroll crate). I created some nested wrapper types as part of this, and then nerdsniped myself trying to see if I could unwrap everything recursively... and now I'm trying to [nerdsnipe y'all](https://www.reddit.com/r/rust/comments/9f156o/is_it_possible_to_make_wrappertinto_inner_and/) because I couldn't figure out a way to do it.
thanks, that's really helpful
Oops, forgot to set a circle as the default element. I don't have an opportunity to edit the code at least until tommorow, so, please, do it for me: Replace line 40 of ./[game.rs](https://game.rs) which looks like this: self.game\_board.set\_if\_free(row, column, GameElem::X); with this: self.game\_board.set\_if\_free(row, column, GameElem::O); &amp;#x200B; I don't notice the delay with crosses on my PC too, but on my old laptop it's noticeable. As for circles... Even GTX 1060 with Intel Core i7 can't handle them :). The delay occures after you have placed at least 10-20 elements. Try clicking fast on random cells for a while, with circles you surely will notice it. 
Fedora Atomic is the coolest idea for a distro. I love the idea of a read-only OS image and having all apps sandboxed. I hope it succeeds, and we can all have super secure, stable, consistent OS behavior. The last time I tried it though, I couldn't even manage to install chrome. I'm switching over as soon as it's useable.
Adding support for GitHub to our [ghostflow stack](https://gitlab.kitware.com/utils/ghostflow-director). I may have [submitted some PRs](https://github.com/tomhoule/graphql-client/pulls?utf8=%E2%9C%93&amp;q=is%3Apr+author%3Amathstuf) to the `graphql-client` crate to further this end :) .
Has anyone considered making a WAI (Web Application Interface) like Haskell has. Not all frameworks use it, but it allows there to be an optimized server that can run an compliant web Application. If I was able to I’d give it a go, but I’m not sure how to do it in rust.
I'm really not enjoying VS Code. 90% of my development experience in the past 7 years has been in vi / Geany. 
That's essentially what the section titled "The basic service API" is about. It doesn't mention WAI specifically, but mentions it's counterparts in other languages, WSGI, Rack, etc. 
I don't know what kind of horrid documents you're stuck reading but I've never struggled to read papers with my phone held in landscape; the text is just not that small.
This is the goal of the Tower project: * [https://docs.rs/tower-service/0.1.0/tower\_service/trait.Service.html](https://docs.rs/tower-service/0.1.0/tower_service/trait.Service.html) * [https://medium.com/@carllerche/announcing-tower-a-library-for-writing-robust-network-services-with-rust-67273f052c40](https://medium.com/@carllerche/announcing-tower-a-library-for-writing-robust-network-services-with-rust-67273f052c40)
Doesn't seem to make a difference here: [Rust #3 vs Rust #4](https://benchmarksgame-team.pages.debian.net/benchmarksgame/measurements/rust.html). So what's the basis of your suggestion that LLVM unrolling is a problem that needs to be countered with SIMD for Rust fannkuch-redux programs?
What about two-column layout? You gonna scroll the left side of the ~~picture~~ pdf, and go up again and scroll the right side... it's so silly and inconvenient that I'm surprised someone still tries to defend it.
isn't there a mysql_async crate or something similar to that name?
Hi! I'm lucky to be writing Rust at work, so I wanted to share some details about the structure and a few patterns in the Rust code we've added to our commerical product this year. I hope there's something you'll find interesting or useful in there. I've got a few more of these to share in the future; the next one will look at how we do FFI between Rust and .NET :)
While the slogan "a rising tide lifts all boats" is catchy, I hope Tide will also lift ... our very own withoutboats.
Rocket has a chapter about pooled connections with Diesel.
The URL's not being blue is weird for me.
I use Fedora on all my laptops, and am seriously considering adopting the Atomic Host/CoreOS merger on my servers. Equally excited for the Fedora Silverblue desktop OS - finally, we've come up with a theoretically-unbreakable automatic updater!
&gt; Fedora Silverblue desktop OS Oh cool, I was wondering why I couldn't find the desktop version anymore. So that must be the new name for it. When I used it, it was called "Fedora Atomic Workstation".
Can't wait for rust to become popular in germany! As of now, I'm still having a hard time finding any internship offers that even mention Rust as an optional skill :/ Will probably have to try and write a lot of speculative applications.
This is where I got: [https://play.rust-lang.org/?gist=93d0c90ddc6e39a5d9a67263371d663b&amp;version=nightly&amp;mode=debug&amp;edition=2018](https://play.rust-lang.org/?gist=93d0c90ddc6e39a5d9a67263371d663b&amp;version=nightly&amp;mode=debug&amp;edition=2018) &amp;#x200B; I feel like it should work. It cites error E0275, but that error seems to be talking about an infinite, expanding type situation where this example is 'unwrapping' the types so it is necessarily bounded.
I want to create something that consumes Tide services and maybe cleans them? Or maybe validates them? I call dibs on the name `tide-pods`.
Why isn't it allowed to move out from a mutable borrow if you move something back in? fn foo&lt;T, F: FnOnce(T) -&gt; T&gt;(x: &amp;mut T, f: F) { *x = f(*x); } This example doesn't compile. Is there a way to achieve this?
`rust-postgres` has async support but hasn't been [updated](https://github.com/sfackler/rust-postgres/issues/334) to Tokio 0.1 yet. For querying from a thread pool there's the [`blocking`](https://docs.rs/tokio-threadpool/0.1/tokio_threadpool/fn.blocking.html) API in `tokio_threadpool` (announced [here](https://tokio.rs/blog/2018-05-tokio-fs/#blocking)).
You'd probably be asked to change it. "Tide" is just a word, but "Tide Pods" is specific enough to be a trademark.
Tower ([https://docs.rs/tower-service/0.1.0/tower\_service/trait.Service.html](https://docs.rs/tower-service/0.1.0/tower_service/trait.Service.html)) is designed async first and handles streaming.
You want something that integrates Kubernetes pods with Tide. Then the name makes sense.
Since you have a `std_list` and the bindings generated for that, could you potentially just follow the prev or next pointers to traverse the entire list? Eg I see the bindings generate this (rearranged for clarity): ```rust pub struct std_list { pub _base: std___list_imp, } pub struct std___list_imp { pub __end_: std___list_imp___node_base, pub __size_alloc_: std___compressed_pair, } pub type std___list_imp___node_base = std___list_node_base; pub struct std___list_node_base { pub __prev_: std___list_node_base___link_pointer, pub __next_: std___list_node_base___link_pointer, } // std___list_node_base___link_pointer is an alias for the below with several indirections: pub struct std___list_node_pointer_traits { pub _address: u8, } ``` Unfortunately that does mean re-implementing the cpp iteration. A better option might be to build an ffi-exposed function in cpp-land to do the conversion into a `std::vector` as mentioned, which is likely to be a lot simpler to convert into a rust `Vec`.
It was a godsend in the codebase I used to support. However.... I accidentally got promoted to maintenance lead, and suddenly all the people that work for me want VS Code.... so I need to get comfortable with it. 
It's alive because people don't like tiffs
TIL
Cool, not familiar with what the others are. Thanks!
True. That's hard to swallow.
Or they went to the reddit submission page, typed /r/rust, it completed to /r/rust, and then they posted. I've made this mistake before and it's embarrassing but not a big deal.
[removed]
Not tested but the following should be equivalent. objects.iter().filter_map(|x|{ x.intersect(ray).map(|i|{(x, i)}) }).min_by(|(_, dist1), (_, dist2)|{ dist1.cmp(dist2).expect("Got a distance of NaN!") })
Nothing at the moment. I'll probably look into the source code of unitr and see if I can extend it a little bit, with different systems of units.
Ah awesome, I can't thank you enough. Didn't realise you could call `map` like that on `Option`s. Thank you, works perfectly!
If `f` panics, then the scope that owns the value that `x` refers to will try to run its destructor (i.e. `Drop` impl) if it has one, thus it needs to always be valid. If `T` was `Copy`, this would be a non-issue but the compiler is forced to assume `T` might have a destructor because this is generic code. If `T` implements `Clone` you can clone the value out before replacing it so you have two valid instances, but cloning can be expensive and you might want to do this with types that cannot implement `Clone` for one reason or another. The typical generic solution is to take `&amp;mut Option&lt;T&gt;` instead; you can move out of it with `.take()`, do your thing, and replace the value: fn foo&lt;T, F: FnOnce(T) -&gt; T&gt;(x: &amp;mut Option&lt;T&gt;, f: F) { *x = Some(f(x.take().unwrap())); } 
For the option unwrapping part of the iterator chain, you can do .filter_map(|(o, x)| x.map(|inner| (o, inner))) What the inner function does is transform `(object, Some(t))` into `Some(object, t)` and `(object, None)` to `None`. If you pass that in to `filter_map`, then it will give `(object, t)` on the cases that were `Some(object, t)` and skip the ones where it was `None`. You can also skip the `zip` and instead start with a single-element iterator over `objects` and then use something like `.map(|obj| (obj, obj.intersect(ray)))` to make the double-element iterator.
Thank you very much. A very good explanation of how your and /u/christophe_biocca 's solutions work.
I never had any issues with two-column layout pdfs on my phones. Wishing you luck flowing math symbolism and various graphs/diagrams. Really excited to use your better solution once you develop it. 
I'm trying to use FFI in Windows (Cygwin)
Nope, it's a Tide ad.
I originally wanted a type that represented a guaranteed absolute path, with symlinks resolved and (on Windows) using extended-length path syntax, so that I could work with deeply nested paths and not have to worry about "magic names" like AUX or CON. Then I discovered the [path_abs](https://github.com/vitiral/path_abs/) crate, which did some things I wanted and some things I hadn't thought of, and I've since been working with u/vitiral to see where my ideas might fit in with his project. I'm sure he'd be more than happy for `path_abs` to get more attention!
I'm finally moving my [selenium-rs](https://github.com/saresend/selenium-rs) project to a state where I feel good sharing it. It currently supports most of the basic functionality needed to do basic test driven development and browser automation with rust, and I'll be continuing to add improvements until I've finished the spec!
No, we have to pick ONE WAY to solve the Web. 
[ALE for Vim](https://github.com/w0rp/ale) already supports rustc/cargo/rustfmt quite well, though I can't testify for other editors.
I don't see anything particularly egregious in your code, but the piston 2d graphics code is not really meant to be used that way, and is overall not optimized and pretty slow. It doesn't internally batch draw calls at all and I'm not sure what else exactly, but I've seen lots of complaints about it being slow in situations where it shouldn't be. Honestly I would recommend trying to write it using ggez (and possibly its MeshBuilder and SpriteBatch APIs).
PM me if you're looking for one in Berlin
:) They actually already have a tool in their CI automation \`pro\` (which is extensive and impressive) called \`tide\`. I believe it manages the massive merge queue and combining pulls, etc.
The pods are delicious!
You say that Fedora CoreOS is succeeding FAH, yet the Team Silverblue website is devoid of literally any useful information, and Googling pulls up articles that say Silverblue is succeeding FAH. The Fedora CoreOS page is also rather light on pertinent information. Meanwhile, I still have no idea what Fedora CoreOS even means, or how it differs from Silverblue. Similarly, are system containers sticking around? What about the rest of the \`atomic\` tooling that was awkwardly branded and tied into the OS/container. I understand the CoreOS acquisition is still settling, that rebasing Openshift is a herculian task while also having to integrate Tectonic, while also re-engineering your entire provisioning story... but... it's really hard to understand where things are going and it's disappointing to walk up to Fedora right now and have no idea what's going on. I feel like we're in a holding pattern...
What build system are you using to glue together rust and .net?
Yes, Atomic Host is EoL - but its core technologies like `rpm-ostree` are living as part of the new Fedora CoreOS. 
Not sure if OP is the author, but I think having `-rs` as part of the crate name is considered an anti-pattern. It looks like https://crates.io/crates/selenium is still available, so why not use that for the crate name?
Oh hey, that's really cool - I have a coworker who's been using [puppeteer](https://github.com/GoogleChrome/puppeteer) to screenshot websites for a project they're on and I've been holding back on telling them to rewrite it in Rust - now I am going to let them have it! Just kidding, that would incredibly rude of me. Great project though - I like the api-feel. I haven't heard much about Selenium so it's all new to me :)
Tower Web uses the macro approach: [http://github.com/carllerche/tower-web](http://github.com/carllerche/tower-web)
[removed]
It seems like FAH -&gt; Fedora CoreOS and FAW -&gt; Fedora Silverblue. \`rpm-ostree\` is indeed interesting, albeit less interesting to me personally, given the existence of NixOS.
If you are a Rust beginner you should not be jumping into these right away. 
/u/nicoburns I'm using this small helper, which turns any piece of synchronous code with owned data into a future: use futures::{Async, Future, Poll}; use std::result::Result; use tokio_threadpool; pub struct BlockingFuture&lt;F, T&gt; where F: FnOnce() -&gt; T, { f: Option&lt;F&gt;, } impl&lt;F, T&gt; BlockingFuture&lt;F, T&gt; where F: FnOnce() -&gt; T, { pub fn new(f: F) -&gt; Self { Self { f: Some(f) } } } impl&lt;F, T&gt; Future for BlockingFuture&lt;F, T&gt; where F: FnOnce() -&gt; T, { type Item = T; type Error = (); fn poll(&amp;mut self) -&gt; Poll&lt;Self::Item, Self::Error&gt; { let f = || (self.f.take().expect("future already completed"))(); match tokio_threadpool::blocking(f) { Ok(r) =&gt; Ok(r), _ =&gt; panic!("`BlockingFuture` must be used from the context of the Tokio runtime."), } } } pub struct BlockingFutureTry&lt;F, T, E&gt; where F: FnOnce() -&gt; Result&lt;T, E&gt;, { f: Option&lt;F&gt;, } impl&lt;F, T, E&gt; BlockingFutureTry&lt;F, T, E&gt; where F: FnOnce() -&gt; Result&lt;T, E&gt;, { pub fn new(f: F) -&gt; Self { Self { f: Some(f) } } } impl&lt;F, T, E&gt; Future for BlockingFutureTry&lt;F, T, E&gt; where F: FnOnce() -&gt; Result&lt;T, E&gt;, { type Item = T; type Error = E; fn poll(&amp;mut self) -&gt; Poll&lt;Self::Item, Self::Error&gt; { let f = || (self.f.take().expect("future already completed"))(); match tokio_threadpool::blocking(f) { Ok(Async::Ready(Ok(v))) =&gt; Ok(Async::Ready(v)), Ok(Async::Ready(Err(err))) =&gt; Err(err), Ok(Async::NotReady) =&gt; Ok(Async::NotReady), _ =&gt; panic!("`BlockingFutureTry` must be used from the context of the Tokio runtime."), } } } 
Yeah..I guess. To learn I am creating basic rust cli tools like interacting with various APIs, GitHub , telegram , reddit etc and invariably I come across tokio and futures, although I am using hyper most of the time. Still I feel the need to understand what's happening behind the scene and thus looking for accessible info on those.
You can compile c++’s stdlib statically too, right?
I don't have an answer, just a collection of thoughts and questions! Is there a way of doing a non blocking read on a USB device (I think there is a way to do this for network connections which is what mio uses)? Does mio perhaps already support this type of read attempt? If all else fails, one approach could be to spawn another thread and have that do blocking reads from the USB device, and then hide that behind a chain and some futures to give it an async interface.
As far as I've seen the repos are either name or name-rs but the packages are always name
Cargo was stripping away the -rs suffix in the past. But that's not the case now.
For local development we use msbuild to invoke Cargo commands, track modifications to Rust files, and copy the local .dll, .dylib, or .so into the .NET build output folder. That makes the F5 experience pretty nice. For CI we just use a script rather than msbuild, because we have to build for multiple targets. I was planning on writing some more about this.
Gotcha, will definitely change, thanks for the heads up!
I can't help but feel you should file a bug. At the very least, the error messages aren't helpful.
Thank you, I will give it a try the next time I need something like that.
Is it possible to use a trait that has generic methods (without an alternative trait generic impl that redefines every method)? Specifically I'm using Alto for audio in my game engine and wrapping the Source trait which can be either a StreamingSource or a StaticSource, but I can't make it a member or box it or anything, and I don't want to have two structs for it.
&gt; Is there a way of doing a non blocking read on a USB device [...]? Apparently [libusb](http://libusb.sourceforge.net/api-1.0/) offers an asynchronous API, which [libusb-rs doesn't yet implement](https://github.com/dcuddeback/libusb-rs/issues/4). They also say that "the asynchronous interface means that you usually won't need to thread", which I understand as "if you can only use the synchronous interface, threading might be an option", which goes in the same direction as your suggestion (which looks like something I had envisioned as well (except that I need to query at least two endpoints on the same USB device, and I don't know if I can do that concurrently)). Time for trying!
Very true. I used rocket mainly because I forgot the terminology used in the article and was on my phone and because rocket pioneered it
Here's another crate on the same subject, I've not contributed, think they're waiting for Mozilla to update their webdriver crate to use sere instead of rustc_serialize. https://github.com/jonhoo/fantoccini
The term you are looking for is "method". There's no specific term for it. "Trait method" just means a method of a trait. Both are just methods. And "native" makes no sense afaict.
&gt; In short, there’s already a very basis for a service abstraction in Rust, Is this supposed to be: "In short, there’s already a very basic service abstraction in Rust," ? 
I prefer `mem::replace()` in most cases but it may be difficult to get a replacement value in a generic context like this. You could do: fn foo&lt;T: Default, F: FnOnce(T) -&gt; T&gt;(x: T, f: F) { *x = f(mem::replace(x, Default::default())); } But that restricts this function to types that implement `Default`. 
I'm assuming this is just a curiosity or you are trying to get a head start on the obfuscated code contest. As a design decision this is horrible. Without pretending that I know exactly where this breaks down, I do see a hole in your requirements. let x : u32 = v.into_inner(); // Suppose this works. and someone comes along and: impl IntoInner for u32 {...} What do you want to have happen? 
True, that's an error! It's been [fixed](https://github.com/Hywan/gutenberg-parser-rs/commit/0786b9ee5fcf1ea767719ae1eedbb3a8b7b49667). The blog post has been updated too.
Could using 1 thread pr USB endpoint be a solution ? Then you could coalesce all the reads into as single mpsc channel that you process in your main thread ?
I can see it is; however, for this I had to look at the code. With just a bit of context, like what you added to your post, people can know right away if they can/want to help you.
Well, I understand the issue with panicking, just hoped there was a nicer solution. So, I basically have a trait like this: trait Foo { fn foo(&amp;mut self); } And I try to implement a state machine with and enum that implements that trait, and switches state inside the method: enum State { A(SomeData), B(SomeData), } impl Foo for State { fn foo(&amp;mut self) { *self = match *self { A(data) =&gt; B(data), B(data) =&gt; A(data), } } } So, to use the workaround with an `Option`, I should either: 1) change `SomeData` to `Option&lt;SomeData&gt;`, which is bad because I don't want to handle these `Option`s everywhere. 2) or change `impl Foo for State` to `impl Foo for Option&lt;State&gt;`, which is bad because sometimes I only have a `&amp;mut State` 3) or use `SomeData: Default` or something, but it also seems like a hack (basically I would need to handle this default value like in case with `Option`). It is funny that I have mutable access to the state, but cannot actually modify it :)
Oh hey now this interests me! I wanted to use Selenium for a project recently and i didnt find anything around. I look forward to this crates progress! Definitely something i'll keep in mind.
Tokio has a "book" https://tokio.rs/docs/getting-started/hello-world/ these concepts are hard though, expect to struggle for a while before it clicks
Sorry, must have forgotten. Added now.
This question motivated me to answer it in the reference. https://github.com/rust-lang-nursery/reference/pull/415
I agree it uses more RAM. Ultimately, I think that is a tradeoff issue. If you don't need absolutely minimal RAM usage, you might prefer more convenience and faster development. I've discovered you're usually OK with 32kB stacks. If you expect to have 1000 connections, 32MBs is not that a big issue. The documentation discusses this tradeoff. &amp;#x200B; Can you explain what you mean by less safe? I could have made a bug in the code in there, I admit that, but even stackless coroutines/futures contain a lot of unsafe code. If you refer to the problem with \`!Send\` data on the stack that other stackfull coroutines suffer from, this is not the case with Corona, simply because Corona never migrates the stacks between threads. That has its own downsides, but doesn't introduce unsafety. &amp;#x200B; Or is there something else I'm missing?
There's a note here: https://rust-lang-nursery.github.io/api-guidelines/naming.html &gt; Crate names should not use -rs or -rust as a suffix or prefix. Every crate is Rust! It serves no purpose to remind users of this constantly.
That's a fair question. We touched on it [in another post recently](https://blog.getseq.net/native-storage-in-a-new-seq-5-preview). The tl;dr is that we're able to utilize resources better with a storage engine that's very specialized for log events, and it gives us more design freedom for future features.
Yeah, I think I'm gonna write a whole C ABI wrapper for this :(
The important thing is not that the DB library is async internally, but that it interfaces well with async code that wants to use it, without blocking the threads that the async system expects to be constantly handling events. This could mean combining a connection pool with a thread pool and a task queue.
Excellent, thank you!
&gt; What do people end up typically doing without std? Helper libraries often just don't need anything from `std`, so use `no_std` to enable themselves to be used as broadly as possible. Another option for libraries is to use passed-in buffers, which might have come from the heap if the binary using the library has one, but can also be non-heap. And as a final thing, one can often make _traits_ that are `no_std` to serve as common abstraction points, even if many _implementations_ of those traits (in a different crate) end up using `std` stuff (directly or indirectly).
You can write high-performance, low allocation C#, but in my experience you end up fighting against the language and CLR. Performance characteristics tend to be non-local, which makes them difficult to reason about and easy to wipe out. On the other hand Rust and its ecosystem are very nicely geared towards this sort of code. We don't have to work in a subset of the language or make other compromises in the way we build the system. We do use some of the newer . NET features like ref structs in the FFI layer though, which are are nice to work with.
Thanks, I been looking for something like this in rust. I have been using gimli which is a Ruby gem. I like to make my handouts with vim and then convert them to PDF to be printed. Any chance it takes a CSS config file?
“Performance characteristics tend to be non-local, which makes them difficult to reason about and easy to wipe out.” Can you please elaborate?
Maybe "a very (good|firm) basis"
How about the asset pipeline? It's an important part of eg Rails and Django. Another thing to consider would be the story around "universal" Rust (in the sense of universal JavaScript). I've very much enjoyed sharing code across the client and server (business logic as well as React components) in my work as a Clojure(Script) developer.
The content is every well written and the series is great! Congrats! I wonder if this universe travels could expand for the Java universe as well. I'm still learning about JNI, and while familiar there are quite a few differences and I would love to read on this post format
It would be great if you could use syntax highlighter in your cod examples. It would be much readable.
PM me if you are close to Aachen.
Woops! Thanks for pointing this out, [there is now a PR open to include that](https://github.com/rust-lang/rust/pull/54150)! Unless there is a big issue with the pre-release we're not going to backport that PR to the stable branch though, it's not worth doing a full rebuild just for that.
&gt;Making the sequential nature of access implicit, with an intermediate variable, solves the problem that I thought I didn’t have. &lt;...&gt; Can anyone explain this? This issue will be [fixed](https://play.rust-lang.org/?gist=24792fe622c28701f3ced1653381057f&amp;version=nightly&amp;mode=debug&amp;edition=2015) with [NLL](https://github.com/rust-lang/rfcs/blob/master/text/2094-nll.md). &gt;Another puzzle I think you wanted to make `World` generic over types which implement `Rng` trait, the correct way to do it is: pub struct World&lt;R: Rng&gt; { random: R, } Alternative option is to use "trait objects" (but you will pay for dynamic dispatch): pub struct World { random: Box&lt;Rng&gt;, } 
Just learned about tower-web, and it looks pretty awesome. I'm using actix-web right now but definitely bookmarked it for later exploration
I am not sure that I like the name clash of flatten with itertools. Is there an RFC that addresses how name clashes for imported trait could be avoided?
I published the first working and usable version of [ptree](https://crates.io/crates/ptree). It's a library for pretty-printing tree-like structures, but it allows global user configuration, so that all applications using the library will look similar. Of course, applications themselves can override some or all aspects of the output styling. 
Yeah that is one of the blockers and unfortunately no one has time to champion that work (outside of Mozilla, I mean). I'd love to get involved again though, the project has potential and doesn't require Java!
Is actix a good web framework to make a status page in? I would like the web framework to have features like http2, but I don't know if actix is unnecessarily hard to work in because of it's speed. Does actix sacrifice usability for speed?
That article is really interesting, and it does seem to mostly translate to Rust (a lot of the points about python being slow don't, but the key point that ACID grade locking is more expensive than in-process locking and is likely to be the bottleneck seems like it does). However, we also don't really have async connection pooling yet in Rust. There seem to be a few efforts (see: https://github.com/sfackler/rust-postgres/issues/233), but no one standard like we have in r2d2 for sync connections.
A discord-irc bridge would be very appreciated. :)
A discord-irc bridge would be very appreciated. :)
This is really annoying. I generally want the crates.io name to match the GitHub name, and on GitHub I do not want binding crates like "libsomething-rs" to just be called "libsomething" — that looks confusing.
Ah yes, but alas Rocket is still synchronous (and in fact slow, although some of the other sync web frameworks are much faster), so I don't think that will really translate to the async frameworks.
I've had nothing but nightmares with selenium, and I'm having fun imagining the picture of you staring off into the distance, was taken after toiling with web driver for 7 hours straight.
This sounds really cool, out of curiosity -- what's the use-case? &amp;#x200B;
My only issue with using windows() iterator, is that it makes a copy of the window of data as it iterates; which could be expensive in some cases. Have you tried the peekable() iterator &amp;#x200B;
I didn't look through a lot, but many of those reverse dependencies seem to be "*-sys" crates. So to be fair to the "low-effort" packagers, the [convention for crates named "*-sys"](https://doc.rust-lang.org/cargo/reference/build-scripts.html#a-sys-packages) is they _should_ be thin, even auto-generated, wrappers. If you or someone else then makes a safe rustic API, it would depend on those low-level bindings and not have the -sys suffix. That's the theory, anyway. 
Hmm... I don't find it confusing if the github entry has a proper description that mentions rust and it has the "rust" tag. I'm looking at C libraries on github and it seems like most of them don't mention C or CPP in the name. Then there are popular projects like youtube-dl where the lack of "python" in the name doesn't seem to cause any confusion.
Remote internship? How should that work?
I think they've tried to cater to those that already know a language, sometimes c++, but things are well explained and will serve on the journey towards systems programming anyway. There will be some repetition already knowing some rust, but I guess that would the case with any book. Explains rust concepts clearly and succintly, has additional subjects vs the rust book, an excellent book imo.
Yes, it does, but how is that different from stackless coroutines or no coroutines at all? You can overflow any stack you have (no, Rust doesn't have segmented stacks). The library uses a guard page to detect the overflow and at least crash in well-defined way. The same way as normal rust threads do (with the only difference of not having the nice error message). So, probably the only difference is you'll be more likely to choose a smaller stack size than with threads (which you also can configure, [https://doc.rust-lang.org/std/thread/struct.Builder.html#method.stack\_size](https://doc.rust-lang.org/std/thread/struct.Builder.html#method.stack_size)). However, I do agree that in general Rust's stackless coroutines will probably be the long-term solutions for *most* of things. Once they are here, which will probably still take some time. And to follow up on your starting point ‒ I did write the corona library longer time ago. The 0.4 release is only porting from `tokio-core` to `tokio`. I probably wouldn't start writing it now as the stackless async-await gets nearer, but spending some time to keep it up to date still felt it's worth it. It can be used until async-await lands and after that for some niche cases. The goal of corona is *not* to replace tokio (unlike some other stack-full libraries), but complement it in cases where it makes sense.
I'm talking about **binding** crates. Of course `libcurl` shouldn't be `libcurl-c`, just as `rayon` shouldn't be `rayon-rs`. But I think e.g. `openssl-rs` should be `openssl-rs` everywhere.
&gt; `Cell&lt;T&gt;` now allows `T` to be unsized. Clicking through this one leads to magic I never dreamed was possible.
&gt; Take this code, which is the simplest solution I could find to get some timing in milliseconds: &gt; &gt; let start = SystemTime::now(); &gt; /* do stuff */ &gt; let end = SystemTime::now(); That's not the proper way to measure time. `SystemTime` can go backward or fast-forward. Use [`Instant`](https://doc.rust-lang.org/std/time/struct.Instant.html) instead.
What makes me more skeptical is that the study has 10 participants. That's not a very large sample size.
Presumably that's what yew is for.
wondering if there will be an equivalent for python? :3
Yeah, it'll be interesting to see if people started using it to do server side rendering of components.
We found only 11 crates broken by this through the Crater run, and we sent PRs to all of them to fix the broken code. While Crater doesn't test everything (yet) it showed us the impact was really small (I was surprised by the small number of broken things), so we decided to stabilize the method.
And `Instant`'s can be subtracted from each other to yield a `Duration`, which can't fail, unlike `SystemTime::duration_since()`. I agree that `Duration` is a bit of a PITA to work with, but at least you never(ish) have to worry about integer overflow while using it.
wasn't it announced at RustConf right before the closing keynote that procedural macros had been stabilized? it might have even been /u/steveklabnik1, but I can't remember for sure. I just don't see it on the release notes.
That’s true, but that’s 1.30, not 1.29. 
[“It looks like you are trying to write bad code ... “](https://vignette.wikia.nocookie.net/sonicfanchara/images/2/20/Clippy.jpg/revision/latest)
It's a subslice. There is no expensive copying.
I believe so, but it's not the default. On my Linux machine, a C++ program automatically links to `libstdc++.so.6`. Though a Rust program still links to `libc.so.6`, so, idk.
Care to provide sources for the performance claims? Maybe even comparing it with popular non-Rust web frameworks? &amp;#x200B; Not that I don't believe you, just interested in the data.
Well, no. Is that relevant? The `take_mut` crate lets you temporarily move out of a mutable reference at the cost of escalating any panic that may occur to an abort.
[removed]
I haven't tested this myself, but remembered seeing it on reddit a few times from people who had. I found: https://www.reddit.com/r/rust/comments/6rxjlt/iron_vs_rocket_performancebenchmarks/ However there is also: https://github.com/SergioBenitez/Rocket/issues/315 So things may have improved.
&gt; It is also worth mentioning that NLL does not solve the problem if the mutable reference is the first parameter of the function. Is this because NLL doesn't allow re-ordering of the expressions at the call-site?
Ohhh. This is so awesome! Finally this is included!
I thought it was obvious that I was talking about opportunities, not specifically about internships.
Stack/heap - Localhost/cloud of memory Ownership/borrowing - Owning a router/connecting to a router (with/without admin access - mut/non-mut)
Stack/heap - Localhost/cloud of memory Ownership/borrowing - Owning a router/connecting to a router (with/without admin access - mut/non-mut)
Just asked in case I miss smth. And that `take_mut::take` looks like what I was looking for, even exact same function definition I wrote! Thanks!
Stackless unless you have an ugly bug you're not likely to overflow. Stackfull you don't really know until it crashes.
What are the use cases of catching panics with unwinding? Isn't panic supposed to mean programmatic error, so why does it not abort always? `take_mut` crate mentioned in another comment here solves my problem in, I think, a less hacky way than an extra variant.
Make it future-proof and call it clippy-chain
"... would you like me to trust you know what you're doing?" 
that's what \`unsafe\` is for :)
Which I guess points at a failure of documentation somewhere. Maybe something to add to the Cookbook?
Basically me. I feel [this hazard symbol](https://xkcd.com/2038/) captures the dangers of unsafe code perfectly: if you slip up, people will get [irradiated](https://en.wikipedia.org/wiki/Therac-25), electrocuted or burned with lasers, and you will have a really unpleasant encounter with hostile organisms.
Great write up! If you find this interesting, you may also enjoy reading the wasm-bindgen guide's internal design section: https://rustwasm.github.io/wasm-bindgen/contributing/design/index.html
I assume that `v` is of type `Wrapper&lt;u32&gt;`. What I would want to happen is that Wrapper::into_inner() would call u32::into_inner() -- though you haven't specified what its associated type and everything is. Assuming that it returns u32 as your statement suggests, then it'd work just fine. If it returned anything else, then it'd type-error. Note that unless u32.into_inner decides it wants to (try to) do some sort of recursion there too, it'd stop there. If it *did try to recurse* to somewhere higher in the chain (via `Wrapper(self).into_inner()` or similar) I think the compiler would error because the return type of `u32::into_inner()` depends on the return type of `Wrapper&lt;u32&gt;::into_inner()` This is mainly just a curiosity, though I started on this track because I was dealing with tagging data at the type level with wrapper types where I want a struct field to hold a value of `LengthData&lt;EndianWrapper&lt;u32,BE&gt; /* Length of the following data field */, UTF16&gt;` (so it'd work with preexisting derive macros) but after deserialization all I want back is the type that UTF16 wraps -- a String. However, when talking about this to other people, it's easier to talk about the case where there's a single wrapper type that has what would be the necessary behavior (because I don't know if the data inside a wrapper is going to necessarily be another wrapper or a piece of data) In short, while I did think about this while working on something, I'm probably not going to use it there. But I think it should be possible and have been thoroughly nerdsniped.
Pick a tool you like to use, like grep, vim or git, and try to rebuild it piece by piece using Rust.
I don't think is a good idea. Scala has xml literals and that is largely considered a misfeature by [the](https://softwareengineering.stackexchange.com/questions/179699/whats-the-problem-with-scalas-xml-literals) [Scala](https://dotty.epfl.ch/docs/reference/dropped/xml.html) [community](https://contributors.scala-lang.org/t/proposal-to-remove-xml-literals-from-the-language/2146/6). Most of the issues you've pointed out are a result of the implementation of their `html!` macro which is based on the declarative `macro_rules!` which shipped in Rust 1.0. Procedural macros [are coming](https://github.com/rust-lang/rust/issues/38356) and I think they will address #1, #2 and possibly #3. I don't think #4 would be improved in any meaningful way by adding XML into the language. I think, at most, you'd just get nicer syntax highlighting. 
It looks like [they've switched to serde already](https://hg.mozilla.org/mozilla-central/file/tip/testing/webdriver/Cargo.toml)? I'm guessing the next version they push to crates.io will have that in it.
Even the stdlib docs could be improved here; I made this mistake myself recently.
Good eye! Thanks for catching that. https://hg.mozilla.org/mozilla-central/rev/7f2c542df6fd appears to be where that change was finalized and I've updated [the relevant ticket in fantoccini](https://github.com/jonhoo/fantoccini/issues/20#issuecomment-420743360).
Custom test frameworks, yeah! &amp;#x200B; [https://github.com/rust-lang/rust/pull/53410](https://github.com/rust-lang/rust/pull/53410)
C++ is just as picky, and it'll bite ten times as hard when you screw it up.
the time crate has a `Duration::span(|| { ...code here..})` just for this, pretty nice, but the API is borked.
Haha, this was motivated primarily because most of the ways I've used selenium felt horrendous, and I wanted an easy way to do it!
Are you determined to use an associated type? It becomes trivial if you use generics instead: [https://play.rust-lang.org/?gist=93d0c90ddc6e39a5d9a67263371d663b&amp;version=nightly&amp;mode=debug&amp;edition=2018](https://play.rust-lang.org/?gist=93d0c90ddc6e39a5d9a67263371d663b&amp;version=nightly&amp;mode=debug&amp;edition=2018)
Terminology thing, `&amp;` is a borrow, not *really* a reference (I think more specifically almost everything is a fat pointer), which is why there is just `.`. Without the borrow, the object address may change to something suitable to the function (a move), with the borrow, the address doesn't change. In both cases you're referencing something at a memory location, hence `.`. Ownership around borrows is done so the compiler can control when things move around, and also why we now have the Pin api, to enforce that borrows are against something that will never move. Blog posts that helpfully will explain this better than me: https://boats.gitlab.io/blog/post/2018-01-25-async-i-self-referential-structs/ https://boats.gitlab.io/blog/post/2018-01-30-async-ii-narrowing-the-scope/ https://boats.gitlab.io/blog/post/rethinking-pin/
Are you familiar with `Iterator`? Its combinator-based API is very similar to what you end up using for futures so it's not a bad place to start getting confortable with that part.
Hmm, I think `&amp;` is a reference. And `&amp;mut` is a mutable reference. `String`s, `Vec`s and trait objects use fat pointers, but I think `&amp;` and `&amp;mut` compile down to regular pointers.
Maybe that should have an exception for crates that provide bindings to non-Rust codebases?
That doesn't quite make sense though, because if I just use the second implementation for Wrapper and `impl IntoInner&lt;Target=u32&gt; for u32`, (or RWrapper vs Wrapper, as earlier) it works just fine. Wouldn't these have the same issue of `(R)Wrapper&lt;(R)Wrapper&lt;...Wrapper&lt;u32&gt;...&gt;&gt;` implementing `IntoInner&lt;Target=u32&gt;`? RWrapper and Wrapper: https://play.rust-lang.org/?gist=4826046ffed8bb7202dc2f0cbde6ab5f&amp;version=nightly&amp;mode=debug&amp;edition=2018 Wrapper and u32: https://play.rust-lang.org/?gist=43ef98e1e3428c8a27163d4777560b17&amp;version=nightly&amp;mode=debug&amp;edition=2018
I don't disagree, tbh. I'll probably try and bring it up in a few more days or something.
Docs and blog posts definitely do a better job of explaining it than I do, but due to lifetimes and ownerships, borrow is used because it better explains what is going on. The object at that memory address is being borrowed for some lifetime, during which it cannot be moved/destroyed, and possibly not mutated. Thinking in that terminology also makes program layout and compiler errors a little simpler to understand. You're probably right though that they're regular pointers and not fat pointers.
&amp; and &amp;mut are operators that borrow data by taking a reference or mutable reference to them is the correct way to think about this. Reference is not incorrect terminology by any means. Borrowing data is a more abstract concept and one of the ways you can do it is by taking a reference to data.
So there's no reason this couldn't have been done with C++ if the author used G++'s \`-static-libstdc++\` flag?
Awesome :) I just figured I'd double check and perhaps ripgrep around and see how much work it would be, but it seemed to be done :) Glad I could help!
There is work to be done in fantocinni if you're interested in that!
Sure. Python has a standard module for FFI. That's not my priority right now, but I can try!
Huh, you've got me there. I have no idea what's going on... probably a specialization related bug then?
`Borrowing data` is a pretty core concept in rust, and for those coming from languages where a reference is just a reference, it can lead to unnecessary compiler wrangling, especially if they're used to just handing out references as a means of optimization. Taking a look back through the rust book, the key difference is whether or not they are function parameters ``` We call having references as function parameters borrowing. As in real life, if a person owns something, you can borrow it from them. When you’re done, you have to give it back. ``` So you call `&amp;` in isolation a reference, but a `&amp;` as a function parameter is a borrow.
Subtracting a later `Instant` from an earlier will actually panic, like any underflowing arithmetic.
Thanks for the tip, I'll look it up :)
I felt like [exercism](https://exercism.io/) helped a lot. I didn't end up finishing, because after a while it seemed like the problems all required the same Rust knowledge and it was no longer helping me learn Rust, but the first 10-15 were a good way to quickly practice and then see how others solved the problem differently.
&gt; though you haven't specified what its associated type and everything is Specificly i was going for the paradox of something like. impl IntoInner for u32 { type Target=bool .... } impl IntoInner for bool { type Target=u32 .... } --- &gt; I think the compiler would error because the return type of u32::into_inner() depends on the return type of Wrapper&lt;u32&gt;::into_inner() This is not possible in the way compiler/type checking works as far as i understand it. Broadly speaking the type checker is trying to 'make something fit your requirements' by applying the rules you give it. Generally speaking it doesn't trace what rules it has applied , so it doesn't detect recursion. But even if it does, it is not an obvious decision to make it invalid. Some very long type like Wrapper&lt;Wrapper&lt;Wrapper&lt;T&gt;&gt;&gt; might have a specific rule that can break the infinite recursion. 
From what i understand, #1 is traded for #3. i.e. it's possible to allow `&lt;Tag&gt;&lt;Tag/&gt;` instead of `&lt;Tag,&gt;&lt;Tag/,&gt;` but it increases the compilation time.
The intention is that the name of any &amp;T is a “reference”, but one of the properties of references is that they borrow.
I'm working with Rust at a German company. The job ad was for C / embedded but I convinced them to let me use Rust instead (it's a small company). But it seems like the boss doesn't want me to use Rust for future projects because of the bus factor (I'm the only one at the company who knows Rust, and the others don't want to learn it), so I'm keeping my eyes open for other opportunities..
Or do something like https://github.com/Stebalien/horrorshow-rs. 
True, I was a bit confused about the name at first. Maybe selenium-rs could be renamed to Pinocchio :P Good find!
Yeah, I'm getting more accustomed to it each day. Thankfully there are VI bindings.
Between Corrosion and VSCode + RLS extension, which do you prefer and why? List any additional plugins/extensions you're using.
&gt; Will it ever be allowed to use this sugar for custom impls? No idea, but it sounds unlikely? I am still waiting on being able to implement the fn traits on my own structs though. &gt; So that I don't have to type -&gt; () (or even more verbosely, -&gt; Self::R) when I just want to use the default return type (()). I don't "need" it but it would be less verbose with type R = ();. Curiously you don't ever need to type `-&gt; ()` since the lack of return type is simple syntactic sugar for the unit tuple return type! That's actually pretty cool (see my playground example). &gt; (...) I used an assoc type for the return type because there should be only one return type for every impl / every arg tuple. (...) Would you have done it differently? The cool thing is that this doesn't necessarily need to be the case! It's not that by definition, inputs should be generic and return types associated. Instead I think about who is responsible for defining what the generic types are. Generics allow the caller to specify the type, associated types are what the function itself defines the types to be. Eg. crazy stuff like `fn foo&lt;T: Deref&gt;(a: T, b: T::Target);` are perfectly valid. I'm not saying this is useful (I've yet to find a use case for this), I'm just saying that this is really flexible. This has nothing to do with your (or mine) example, just a cool thing I wanted to bring up. 
Really only laziness in terms of learning a new programming model (actors). I did try creating an actix project, and it worked. But it was pretty complex for what it was, and it was pretty slow to compile as well. It's fine for what it is, but it's definitely not as simple as something like express or flask or sinatra.
Ha didn’t know window even existed. That’s very useful .
Not yet! This is a very early prototype. I agree that styling is important, but I'm not sure that CSS would be the best way to handle it. Markdown, specifically commonmark, only lets you add classes and ids by putting text inside of a `&lt;div&gt;`. I'm open to discussion about it. If you want to add your comments on github, I think [this issue](https://github.com/Geemili/mdproof/issues/3) would be a good place for it.
Is `init_or_die` really necessary? Kind of feels anti-idiomatic...
Here it is: https://github.com/gtk-rs/gtk/issues/702 If we get all this done, things will wildly improve.
Oh snap. TIL! Thank you.
It looks like you can just do `Config::init()` and handle the error yourself. Or are you questioning the relevance of just offering that method?
You can diff the two directories. Most of it so far has been backfilling features that have stabilized since 1.21. There’ll be a new macro chapter soon.
What happens if you try to use a type which implements a trait at runtime on a container with static dispatch? Eg, the serviceloader pattern in java for instance.
I personally hope the router can be data based instead of macro based. With data based one, we can: * Get the handler for particular url from runtime * Better compose ability * Reflection like swagger api generator 
&gt; `init_or_die` *somebody* was a perl programmer...
I was searching for a workable TCP client example and found the ecosystem is completely destroyed by the reform. Most existing code doesn't work. Perhaps it's not the time to dive into tokio. And we'd better wait for futures 0.3 and corresponding tokio version.
Do you have the code up anywhere? Hard to tell what might be happening from here. Although looking at benchmarks it seems iron is way down at the bottom, so this might be expected for it? https://www.techempower.com/benchmarks/#section=data-r16&amp;hw=ph&amp;test=plaintext
Iron is a synchronous web server - that means that each thread can only handle one request at a time. This is fine for handling a small number of requests very quickly, but you won't have nearly as high throughput as asynchronous servers for really high numbers of requests. There are a few different asynchronous servers you can try - tokio-minihttp for example is known to be one of, if not *the*, fastest web servers in the world (in terms of throughput).
actix-web is the fastest web framework, as you can see in the techempower benchmark, and it's fairly mature at this point. Warp and tower-web are two other ones designed with speed in mind that are still under heavy development. It's not possible to write fast code in every language, as JavaScript can only be so fast, but it is possible to write slow code in any language.
&gt; I'm gathering that a series is one "column" only. Is multiple column series something you would consider or possible with the current design? the "master" version has this feature now! However, the client API doesn't implement it quite yet. See the [Readme](https://github.com/njaard/sonnerie/blob/master/README.md) to see how it works. I'm interested in your feedback!
The 2018 edition is the more recent one and the one everybody should be reading. I want to have the print book and I have a question. When Rust 2018 comes out and the book gets updated, will it be more printed editions of it? If I wanted to buy the printed edition, should I wait to get the 2018 one, will that be sent to print?
How does this even work? like for `Cell&lt;[u32]&gt;`, which is unsized, where is the actual data stored? the Cell holds a fat pointer?
We do want to release more printed books in the future, but have no current timeline for when the next one would be. If you want a print book, get one now.
I can’t. Just diff the dirs, you’ll see the changes.
Until [recently](https://github.com/iron/iron/commit/9ad396d17733f3e082b91dbf18daceabd961f4c6) Iron was unmaintained, so it fell behind the newer web servers.
As long your actix thread pool is big enough a few slow tasks won't starve all the threads. Actix web is async and multi threaded. Best of both worlds like tomcat.
I think the goal is that a `&amp;mut [u32]` could be transmuted into a `&amp;Cell&lt;[u32]&gt;` by a new safe API function. My understanding is that this helps avoid the perf hit from bounds checks in cases where you want to iterate over the same list in multiple ways at once.
My initial testing has the actix hello world example on par with fastify's JSON example. Could I be hitting the limits of my machine in terms of HTTP throughput?
&gt;Stack/heap - Localhost/cloud of memory That would immediately make me think about access latency which may or may not be right, depending on your memory access pattern.
I made a pull request implementing making the format borrow from the input. Unfortunately, it did not seem to change performance (I also added benchmarks). &amp;#x200B; [https://github.com/themasch/nginx-log-parser/pull/2](https://github.com/themasch/nginx-log-parser/pull/2)
Ok so you're dealing with a few different levels here. The first is that the async to sync bridge is a bit wonky. The second is that the actix web diesel examples use the actix actor framework to do that async to sync bridge, which if you're not familiar with can be daunting. However, in actix-web, each `req` object actually has an associated cpu pool you can use to perform sync functions with, leaving the http thread to serve requests in an async fashion, and running the blocking code in a background thread. There are a couple of issues you need to overcome. The first is, you have this r2d2 pool that does need to get into a request to be used. There are 2 ways I know of to pass this around: * Use lazy\_static and just have it as a global variable. * Save it within the App State when you start up (i.e, App::with\_state(my pool)) So now you have a db pool that you can use with diesel, but you need to bridge the blocking and non-blocking code. So you create a function that overall looks like this: * Receive request * Execute Diesel Code * Send Response That 2nd part is blocking, and needs to be done in a background thread pool (or async if you are adventurous), so you take your db connection pool that you've stored in your state, and use the cpu pool via `req.cpu_pool()` available via the request, using the `spawn_fn()` method to turn a `Result` blocking method/closure into a `CpuFuture`. Here is a rough example of what that may look like, but you will obviously need to deal with a few things (like the error type, etc.. Use the failure crate.): fn index(req: &amp;HttpRequest&lt;Pool&gt;) -&gt; FutureResponse&lt;HttpResponse&gt; { let db_conn = req.state().clone(); req.cpu_pool() .spawn_fn(move || { //this closure needs to return a Result posts.filter(published.eq(true)) .limit(5) .load::&lt;Post&gt;(&amp;db_conn) }) .and_then(|val| { Ok(HttpResponse::json(val)) }) .responder() }
&gt; I... would not expect a newtype to change the Ord implementation. Please say hello to my little friend [`NotNaN`](https://docs.rs/ordered-float/0.5.1/ordered_float/struct.NotNaN.html) :-D
&lt;3
Perhaps the best way to figure out what's new in te 2018 edition is to read the the [2018 Edition Guide](nursery.github.io/edition-guide/rust-2018/index.html). IIUC, it also covers new features that haven't been documneted in the 2nd edition of the book.
You can set the cpu pool count with the env var `ACTIX_CPU_POOL`, by [default](https://github.com/actix/actix-web/blob/master/src/server/settings.rs#L29) it's 20 `with_state` will prevent you from using global vars which are normally a bad idea from a decoupling point of view. Sometimes you can't avoid them though. In this case you readily can.
Porting over the codebase for my university’s robotics team. Begone C++!
Why? Diesel example in "examples" repo already has r2d2 integrated. https://github.com/actix/examples
This is actually exactly what I was doing but I just think it's too verbose and indirect &gt;.&lt;
Most of the compilation time is spent on optimizations, actually. Parsing wouldn't increase it significantly.
You should format your posts properly; use backticks for code. Anyway, the issue here is that references in Rust are not "transparent". You can't write code as though they don't exist; you can't replace a value with a reference and have everything still work exactly the same. Yes, `+=` has to fetch the existing value to add to it, but that has *nothing* to do with `i` being a `&amp;mut i32` reference. `+=` is backed by the [`AddAssign`](https://doc.rust-lang.org/std/ops/trait.AddAssign.html) trait, and the relevant implementation here is `impl AddAssign&lt;i32&gt; for i32`. In order to use that implementation, you have to use it on an `i32`, *not* a `&amp;mut i32`, so you have to dereference `i`. Also, the non-`+=` version would be `*i = *i + 50`. You have to deref `i` so that you're assigning to the value behind the reference, not the reference itself. `&amp;i` in an expression *creates* a reference. *Wait, couldn't `AddAssign` be implemented for `&amp;mut i32`?* Yeah, probably. But it isn't. So that's not relevant. :P As an aside, I don't think it's possible to explain Rust to a 5 year old.
So I'm trying to implement this now but I'm getting a weird error message. Any ideas? ``` error[E0277]: the trait bound `models::ObjectUpdateProfile: futures::Future` is not satisfied --&gt; src\main.rs:320:20 | 320 | req.cpu_pool().spawn_fn(move || { | ^^^^^^^^ the trait `futures::Future` is not implemented for `models::ObjectUpdateProfile` | = note: required because of the requirements on the impl of `futures::IntoFuture` for `models::ObjectUpdateProfile` ```
You can specify additional requirements for each method with where clause: trait A { fn a(); } trait B { fn b(); } struct Wrapper&lt;T&gt;(T); impl&lt;T: A&gt; Wrapper&lt;T&gt; { fn wrap_a() { T::a(); } fn wrap_b() where T: B, { T::b(); } }
Since you can't hold an unsized value directly, the fat pointer comes with the indirection, like \`&amp;Cell&lt;\[u32\]&gt;\` or \`Box&lt;Cell&lt;\[u32\]&gt;&gt;\`.
ah, so the Cell itself becomes unsized too at that point, and also can only be referred to behind a pointer?
Cool! Reminds me of compression/decompression with the `flate2` crate
I am a data performance engineer. You can’t really make a database async for queries. You might be able to setup a queue ahead of the database yourself and that can be asynchronous. Regarding query performance, you will simply need to data model correctly. Putting indexes in the right place or partitioning. For more reporting queries you can consider using materialised views or similar summary table methods. 
&gt; `cryptostream` does not yet support any AEAD cipher suites, but support for those will be coming. That part sounds really important, so much so that you might want to discourage using other parts of your API once AEAD is ready. Do you have a particular design in mind for [breaking up the stream into authenticated packets](https://www.imperialviolet.org/2014/06/27/streamingencryption.html)? 
That's a *lovely* idea. Thank you, Niko!
C# and Common Lisp don't have "references everywhere", value types and stack allocation are also supported.
Cool project! I think you have a mistake in your example code with brace at the end. `let mut decryptor = read::Decryptor::new(src.as_slice(),`
Yes, exactly.
Did they fix the perf issues with SmallVec moves already ?
Your `spawn_fn` closure needs to return a `Result`, or any other type of struct that implements the `IntoFuture` trait. You will need to take out the `expect("Error")` portion at the end which will panic the thread, and probably replace it with a `map_err`. You might be able to throw in a `map_err(error::ErrorInternalServerError)`
I also don't get this. It is not uncommon to get significant speed ups when rewriting stuff from i.e. Python or Java to Rust, but this seems extreme. I cannot imagine that the basic algorithm and data structures are the same in both implementations.
I think the point was that `init_or_die` isn't doing anything that `init().unwrap()` doesn't do, in fact I think it's doing even less. 
Not if you can reach higher numbers with something else. Just to be sure, did you compile with the `--release` flag? Without it, the resulting binary is not really optimized and you can expect bad performance.
&gt; there's no way for the compiler to tell which variant is active Damn. We speculated that one could fix the performance for `SmallVec` by just copying the active `enum` variant, which is something the compiler could theoretically do, but for an `union` that's something that only user-written code can decide and we need full-blown move constructors for that. Damn :( 
Can I ask why you can't make it a boxed trait? Is it because of the allocation? What about having an concrete enum type that wraps StreamingSource or StaticSource as its variants and implementing Source for it? (when you say you don't want to have two structs, you mean you don't want the containing struct to be generic over Source or anything like that right)
For the purposes of optimizing away moves in the future, all that matters is the compiler seeing the same union field/enum variant being accessed multiple times. Enums can be optimized more readily because it's UB to look at non-active variants, while unions currently are allowed to soundly "transmute" data. If we had a per-union-definition opt-in that made accessing a certain field UB unless it's the last one to be written to, we could use the same optimizations for *those* unions. If I understand the context correctly, `SmallVec` could use such a (more restricted) union, as it never does (or has a reason to) transmute between `Vec&lt;_&gt;` and `[_; N]`.
There is https://asquera.de/, located in Berlin. Afaik they also do Rust Trainings etc.. Its Founder Florian Gilcher is in the Rust Community Team, so ist might be worth contacting them :) Florian is also on Reddit, but i don't remember his handle 😅
I wrote an allocator that on every call to alloc_... and dealloc, it inspects the backtrace, and trims it to the last stack frame that's in a certain crate (one can set up the crate name to inspect globally), so that you get, for each allocation and deallocation, the layout being allocated/deallocated (bytes, align) and the line of code within your crate that caused the allocation/deallocation. 
Is there a reason this post recommends doing extern crate smallvectune; and then rewriting the `use` statements? Shouldn't a simple `as` here save a lot of path changes? extern crate smallvectune as smallvec;
Author of the blog post here. I can assure you that the Clojure and Rust versions run the same basic algorithm. The acceptance tests are a one-to-one translation, and with the same parameters the results are very close. They are not identical because there is a degree of randomness. Both versions use the same approach to manage the terrain; a large array in which creatures and plants are kept (at a typical world size of 150x150 the array has 22500 elements), plus a second array with references to all locations in the large array that have creatures on them (typically less than 150). This allows for accessing plants and creatures by coordinate in O(1) and for iterating over all the creatures in O(n), where _n_ is the number of creatures, not the size of the world. That said, there is one aspect of the implementation that I suspect does make a difference. In fact, it was this aspect that I didn't like about the Clojure implementaion, even though it feels very idiomatic in Clojure. In the Clojure version the creatures are modelled as maps and the processor part of the simulation needs to do some `assocs` and `dissocs` to change values like the program counter or the creatures' energey level. In the Rust version the creatures are structs, the processor has mutable reference to it, allowing for changes in place. You can see this, for example, in the implementation of the instruction execution function. In Clojure, it looks like this: ```(defn- exec-instr [creature] (let [instr (c/current-instr creature) with-inc-pc (c/inc-pc creature 1)] (if-let [instr-fn (get instr-functions instr)] (apply instr-fn [with-inc-pc]) (throw (Exception. (str "Unknown instruction; found" instr))))))``` In line 3 you can see that the `inc-pc` function returns a new map for the creature with the value of the program counter increased. That new "copy" is then passed to the function implementing the actual instruction logic, and it is also returned from the function shown here, which means some later code will have to replace the map previously held in the terrain with the updated map. It is difficult to quantify the impact of this difference. In any case, as far as I can tell, it is the idomatic way to do something like this in Clojure. Maybe the problem is suited better to a language like Rust. 
`Arc::make_mut` makes a clone if there are other references, so it is not actually borrowing, but is basically same as `Cow::to_mut` Maybe make a trait for this clone-on-write behavior: impl&lt;'a, B: ToOwned&gt; ToMutMaybeClone&lt;B::Owned&gt; for Cow&lt;'a, B&gt; { fn to_mut(&amp;mut self) -&gt; &amp;mut B::Owned { Cow::to_mut(self) } } impl&lt;T&gt; ToMutMaybeClone&lt;T&gt; for Arc&lt;T&gt; { fn to_mut(&amp;mut self) -&gt; &amp;mut T { Arc::make_mut(self) } } impl&lt;T: BorrowMut&lt;U&gt;, U&gt; ToMutMaybeClone&lt;U&gt; for T { fn to_mut(&amp;mut self) -&gt; &amp;mut U { self.borrow_mut() } } Although this does not compile, just like your example :) | impl&lt;T&gt; ToMutMaybeClone&lt;T&gt; for Arc&lt;T&gt; { | ------------------------------------- first implementation here | impl&lt;T: BorrowMut&lt;U&gt;, U&gt; ToMutMaybeClone&lt;U&gt; for T { | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ conflicting implementation for `std::sync::Arc&lt;_&gt;` | = note: downstream crates may implement trait `std::borrow::BorrowMut&lt;_&gt;` for type `std::sync::Arc&lt;_&gt;`
And, I think, implementing this for `Arc` doesn't make sense actually, since it makes user code not obvious: let a = Arc::new(path); let mut b = a.clone(); b.your_method_that_takes_mut(); // a and b are now pointing to different paths assert!(!Arc::ptr_eq(&amp;a, &amp;b));
I suggest the following: * Join and Follow the Rust [Internals](https://internals.rust-lang.org/latest) and [User](https://users.rust-lang.org/latest) discussion forums * Read ALL the [documentation](https://www.rust-lang.org/en-US/documentation.html) * Don't forget to read (and re-read as necessary) the [FAQ](https://www.rust-lang.org/en-US/documentation.html) * See [Contributing](https://www.rust-lang.org/en-US/contribute.html) if you really want to get your hands dirty * Go read the history of Rust through [TWIR](https://this-week-in-rust.org/). Read through most/all of the referenced articles and RFC's (even declined RFC's) to get a better understanding of the history and current state of Rust When I first got into Rust, reading through all the old TWIR's and the referenced RFC's and articles helped me to quickly (for some definition of quickly) understand the full range of what Rust was about.
Ownership: Objects in Rust are like bricks*. If you move them from one place into another, they are gone from the previous place. Also, moving bricks is easy and cheap, you don't have to create a copy, you just move. Borrowing: You get a piece of paper describing where the brick is (and whether you can just look at it (`&amp;`) or do something more (`&amp;mut`)). (note: `&amp;mut`-describing paper is certified to be unique and you can't easily forge a copy). Heap&amp;stack: Don't think too much about it. Conceptually, there's only stack. Heap is only an implementation detail. For example, a String's data is allocated on the heap, but you shouldn't care too much about it, you should only care that you get a stack object, which is a handle to that data. ^ *Some simple objects, like `i32` are like piece of papers with the value written instead. Instead of moving, you just take a new piece of paper and write the value there. (These types are called `Copy` types).
Sound neat, did you publish the tracing allocator somewhere?
I don't think the "big" value here is to help one person with a specific problem – although that is quite nice – the main benefit here (that i see) is to get a more in depth feeling of how and why are people struggling with things. When you work full time on something everything becomes "obvious". rustc is just "software" with the rare exception that its user are also programmers. But to view this from another perspective its the same relationship with programmers and customers (of that program). Its is very insightful to sit besides the customer of the program you wrote and every features was 100% "obvious" in your mind – because you designed it. But seeing people struggle with it and why can help immensely to get yourself into the perspective of someone who don't know every ins and outs and just want to use it for their specific task. By getting to know what people really struggle with you can help a lot more people. Because the thing we (as programmers) really forget is the fact that it is hard for a customer (or any person for that matter) to express what their problems really are. And that is with software or any problem in your life. It is really hard to know your own problems and express them. A survey is cool but if the people who you are surveying don't know what is causing their struggles it can get problematic. Seeing people – live – what they're struggling with can be way more insightful. 
So is this a "live session" thing where it matters which timezone the participants are in? Neat idea in any case!
RLS is *very* brittle. At this point, I'd set your expectations to: "better than nothing". If it works, great. If not, oh well. If you want to know what methods are defined for a type, check the [API reference docs](https://doc.rust-lang.org/std/string/struct.String.html). Remember, you can document your own code by running `cargo doc`.
IntelliJ also has pretty good Rust integration if you're used to it.
Or use the stopwatch crate 
That sounds amazing. Although when I was looking at the current questions for september, I found `Ramana Venkata - Debugging Rust with GDB`, and I kind of want to know what will be said. I completely understand the need for a 1 on 1 talk, but maybe the idea could be expanded? For example what if we would gather questions/topics every month, collect a core group of volunteers that are capable of answering some of those questions and then host a live event every month(week?) on something like twitch + discord? Questions: * Debugging with gdb: Asked by Tom and answered by contributers A, B ,C starting at ~1pm * Workflow in vim: Asked by Peter and answered by contributers C, D, starting am ~2pm * Advanced lifetimes: ... * rustc mir: ... * Contributing to rustc: .. The person can ask the questions live or just write them down if he/she is not comfortable to speak in front of an audience. I am sure some of the viewers can also help answering some questions or they may also have some follow up questions. I think the major benefit of this is that it is more informal than a blog post. You don't necessarily have to write a rigorous blog post, and you can just say something like "I often find a, b, and c super useful when using gdb with rust", and the other contributors can also chime in. I definitely would **love** to watch something similar, it is often hard to get information about workflows from blog posts compared to talking to a person directly. Sorry I didn't want to distract from your project, this was just something that popped into my head.
Completely true, but the canonical C# style is heavily reliant on heap allocated objects. In the vast majority of C# codebases I've seen, structs and (very rarely) stack allocated buffers are only used as a last-resort optimization, which is a shame. The language is moving towards more stack allocation, less pointer chasing and less defensive copying with the addition of recent language features and libraries, though.
Congratulations on publishing your first crate! That's a great milestone. &amp;#x200B; Looks like a useful crate, I might stick the file into my Dropbox and have essentially the same as Authy but without another service. Any chance that this could be integrated with GPG/PGP so that the file could be encrypted via PGP and then it could use \`gpg-agent\` to ask for the password? I think that'd be neat.
Thank you! Didn't know that either! Would be a great clippy lint!
Thank you. Then, I will configure a tool for reading API reference docs.
How would clippy know what you want the extern crate to get renamed to?
So do you want to run it in a route handler, or do you want it to just run as a background task? 
Sounds like you just want std::thread::sleep
No, Rust is memory safe.
From the pile of renaming use statements? 
A background task.
Kinda. But more user-friendly. `sleep` can be stopped only with SIGKILL (afaik), which is not what I'm looking for. A want to be able to stop the process while it waits. 
Rust is not magic. If you want to persist the internal data structures, you have to go out of your way to make sure that happens. If you haven't already, the easiest way is to slap `#[derive(Deserialize, Serialize)]` on everything, and make sure you have some kind of clean shutdown/startup command that serialises the current state to disk, then reloads it. Of course, this raises major questions with regards to migration. I don't know that `serde` provides anything for handling that. Actually, you could maybe look at (I believe it's) protobuf. One of the RPC/serialisation formats has support for versioned migration. But short version: probably yes.
Then a separate thread sounds like it shouldn't be a problem, as long as you set a SIGINT handler that shuts it down cleanly. Alternatively, maybe move it out to a different process entirely, or even a cron/systemd timer job? 
Thank you very much! :) You know that is not a bad idea at all. That would be very interesting to implement and as you say, probably useful. Instead of typing in the password, you could use the gpg-agent. I have to look into that, I created an issue to keep track of it. Thank you for the idea! :)
Or `unsafe core::hint::initialized_range(&amp;u)` that tells you which bytes are initialized (which is then used as the argument to memcpy)
You probably meant /r/playrust
I never published it and its not open source yet. I might publish it at some point when I have enough time. 
&gt; First, differentiating the data by constructor doesn't neccessarily give you any value The line of code in which a vector is constructed for the first time denotes the start of a vector life-time and the call to `drop` its end (unless its leaked, in which case it never ends). &gt; I wanted to avoid the performance impact both of getting the initial backtrace and keeping an additional identifier Unless constructing and reallocating vectors is a hot-spot in your program, I doubt you can measure the impact of this. I wrapped the global allocator in my tracing allocator and did this on every call to `GlobalAllocator::alloc` and couldn't really measure the impact of it in release builds. Not that the impact would matter much at least for me since I only use this for memory-profiling, not time-based profiling. 
This is not true, sleep and can is interupted by all of the typical signals. You might have something else in your application blocking SIGTERM and SIGINT but it is not `std::thread::sleep`. It will block the thread from continued execution until it elapses so you cannot use the thread for anything else until it returns but it will not block external signals. But it does block the thread that called it which may or may not be what you want.
Yeah, I made sure to use the `--release` flag. That's the first thing I checked when I saw the numbers.
Your text is worth the introduction of TRPL 
Actix is not much harder than other Rust frameworks, IMO. However, for something like a status page, maybe it is better to generate static files, and serve them using an external service (like S3, GitHub Pages, etc).
I see. Then it was due to `loop` with `sleep` in it.
It's not just that `Arc::make_mut` is heavy. It's use also has _irreversible effects_ In general, I would expect `let _ = foo.borrow_mut()` to not modify `foo`. In the case of an Arc, that detaches the Arc from the others. That's pretty different.
You must scroll right...
As I said, you can write c style code, no problem. Simply chose to not use those features and you're golden. If you learn c, you can't move to these features later. 
I have a macro related question: I want convert user input like: byte_pattern![0x9, 0x5, "?", 0x0] into a `Vec&lt;Option&lt;u8&gt;&gt;` using a macro. Basically it should push `None` whenever the token `"?"` is encountered and `Some(expr)` otherwise. Is that possible? I tried for quite a long time, but the macro syntax is so alien to me, I couldn't come up with a working version.
While this looks nice to the beginners, I really wouldn't suggest it for any beginner. According to example, it's still very low-level. One has to provide IV (how can beginner know how to do it correctly?), it provides no authentication (without authentication, oracle padding attacks are possible!), no key exchange (what if beginners just hard-code the key without knowing what they do? How they are supposed to implement key exchange?) and no replay protection (the beginner likely won't even know such thing is needed!). It's also unclear what happens if one calls `flush()` - will the message be padded appropriately, flushed and the stream reinitialized? With the same IV?! If not, `flush()` is likely useless and breaks code that depends on it. This looks like a good base layer for encryption, but it needs to focus on being really secure - otherwise, you're doing people disservice by providing false sense of security. I'd love to help with this since I consider it much needed project, but I'm somewhat low of time. I think you should at least put a disclaimer there that using the library correctly requires knowledge, so that people don't get burned by it.
&gt; The cool thing is that this doesn't necessarily need to be the case! Sure but that's how it is with C++ function overloading, and that's what I was trying to emulate. Of course if I wanted to allow different return types for the same arg type tuple, I would use another type variable for the return type. &gt; Curiously you don't ever need to type -&gt; () since the lack of return type is simple syntactic sugar for the unit tuple return type! That's actually pretty cool (see my playground example). I know that it works without `associated_type_defaults` but I was wondering why it doesn't it work **with** `associated_type_defaults`..
I've had VScode Vim installed from the start, I wouldn't be able to use vscode without it. 
Yay! Been running Webrender on all of my machines for a while now with no major issues - big props to pcwalton, Gankro and everyone else for getting it stable so fast.
Note that `rust-crypto` is effectively abandoned, [RustCrypto](https://github.com/RustCrypto) aims to succeed it and become a foundation for pure Rust cryptography ecosystem.
Thanks a lot! My (non-working) solution wasn't even close.. I can see where your user flair is coming from ;)
Links to some further Webrender discussion/news: * https://www.reddit.com/r/firefox/comments/9f2fyk/webrender_on_nightly_restricted_to_desktop_win10/ * https://www.reddit.com/r/firefox/comments/9fezel/webrender_enabled_in_nightly_for_qualified_users/ * https://mozillagfx.wordpress.com/2018/09/13/webrender-newsletter-22/
This project uses unauthenticated encryption (AES-128 in CBC mode). [I've written about this issue at length](https://tonyarcieri.com/all-the-crypto-code-youve-ever-written-is-probably-broken). Cryptography using unauthenticated modes is malleable and potentially vulnerable to plaintext recovery attacks (i.e. chosen ciphertext attacks). I would consider this project cryptographically broken and unsuitable for use. The real challenge when designing a streaming encryption mode is incorporating an authenticated construction into a segmented scheme and in such a way that it resists reordering and truncation attacks. Fortunately cryptographers have studied this problem and come up with both a proof framework and provably secure schemes. My multi-language cryptography library [Miscreant](https://miscreant.io) implements one of these schemes: [STREAM](https://github.com/miscreant/miscreant/wiki/STREAM), a scheme designed by cryptographer Phil Rogaway. The Rust version of Miscreant, [miscreant.rs](https://github.com/miscreant/miscreant/tree/master/rust), provides STREAM in conjunction with two different authenticated ciphers, AES-SIV and AES-PMAC-SIV, with plans to add AES-GCM-SIV in the near future. tl;dr: do not use unauthenticated AES-CBC
I'm curious about the Nvidia limitation. What's the issue with other GPU vendors? Driver issues? And about when they are expected to be on a good enough level to enable them on default as well.
As I noted in my [other comment](https://www.reddit.com/r/rust/comments/9fcwts/transparent_encryption_and_decryption_in_rust/e5wjj02/), not using authenticated encryption modes makes this project cryptographically broken, and adding support for authenticated encryption modes is a non-trivial problem with the potential for reordering and truncation attacks if done incorrectly. [My library Miscreant implements a secure segmented AEAD construction](https://docs.rs/miscreant/0.4.0-beta2/miscreant/stream/index.html) and I would strongly recommend it over anything based on unauthenticated modes.
Just a note that in this case "desktop Windows" really does refer to non-laptops.
&gt; I'm curious about the Nvidia limitation. What's the issue with other GPU vendors? Driver issues? No issue. It's an incremental rollout. They started with a discrete GPU to reduce potential driver issues iirc. There's a more eloquent statement on that somewhere, but don't have it on hand. &gt; And about when they are expected to be on a good enough level to enable them on default as well. Last I read they were hoping to have WR enabled by default on 64 (which is currently Nightly), so it would basically ride the trains to release. There's certainly no guarantees.
Trying to release a new crate by the end of this month
I know nothing about how the AWS IoT stack works but have you looked at rusoto's support for it? https://rusoto.github.io/rusoto/rusoto_iot/trait.Iot.html Not sure if it covers the work you need to do.
Munching isn't really required here. macro_rules! byte_pattern { (@replace "?") =&gt; { None }; (@replace $elem:expr) =&gt; { Some($elem) }; ($($elem:tt),* $(,)*) =&gt; { vec![$(byte_pattern!(@replace $elem)),*] } } 
There is a [crate](https://crates.io/crates/take_mut) that implements `foo`, calling it `take_mut`, but as /u/DroidLogician says the key is you have to do something if the function panics. And the only sensible thing to do is abort the program, which explains why this isn't a super common pattern. 
Correct. As I understand they are not toggling it on for laptops. However, any Nightly users can turn it on by toggling `gfx.webrender.all` in about:config . I expect you'll get the same experience then, they just didn't want to be a big battery drain for laptop users.
One day we'll be able to write `$elem:literal`, though I don't think it matters a lot since there are no multi-token literals (?). 
You can always split up a large query into several queries and fire those off at the database. How is that async though? As in, you are not holding for the query to come back..
Maybe they want to exclude configurations that switch between an integrated GPU and the discrete GPU automatically? Just speculating.
It is astounding that there are so many different ways to do the same thing.
What are some cool things that WebRender does better (faster, nicer rendering, ...) as opposed to Gecko? I've enabled it now, and I'd like to see it shine somehow.
Beautiful! Thank you.
https://hacks.mozilla.org/2017/10/the-whole-web-at-maximum-fps-how-webrender-gets-rid-of-jank/
Very nice. I recently needed something like this (although in an async program).
Oh woops, I definitely got those names confused.
Well, that's how it works, but it doesn't really give any concrete webpages were you can see it shine, afaict?
&gt; laziness in terms of learning a new programming model (actors) It's not that different/difficult, and there are a lot of examples.. &gt; it was pretty slow to compile Such is life.
Not exactly the same, it doesn't take a function to regenerate the value, but the cached crate has a TimedCache which has worked well for me.
Before anyone brings up Serde 2.0 -- I am quite certain that the approach in miniserde does not extend to the range of use cases that Serde targets. This prototype considers only a small subset of Serde use cases and explores one interesting possibility for addressing those use cases in a different way than Serde. The design is neat and potentially useful so I felt it was worth sharing!
Note: the second "try it on play" link in the First draft paragraph leads to the same example as the first one.
I’ve heard of doing more work on energy consumption before enabling on laptops. Right now Firefox with WebRender enabled can drain your battery faster than without it.
Here's a version that compiles: https://play.rust-lang.org/?gist=7206d0b545cd92282a5dacd7da18674f&amp;version=stable&amp;mode=debug&amp;edition=2015 The first thing I did was change `*tmp_ptr = n;` to `*tmp_ptr = *n;`. This is because `for n in x.iter()` iterates over references to the values in the array rather than the values themselves, so you need to dereference `n` to get to the actual bytes you want. The second thing I did was cast `tmp_ptr` from `*const u8` to `*mut u8` so that the compiler wouldn't complain about me trying to assign through an immutable dereference (which is an error I'd never actually seen before, so that was interesting).
omg dude, awesome. you are the king.
Do you have any benchmarks for compile time differences?
I need to mutate a struct's field from callback assigned in struct's method, the problem is that callback belongs to external api and has a 'static bound. Any ideas how to achieve this? [https://play.rust-lang.org/?gist=664b05e5bb2209bb083dd2653956d5b4&amp;version=stable&amp;mode=debug&amp;edition=2015](https://play.rust-lang.org/?gist=664b05e5bb2209bb083dd2653956d5b4&amp;version=stable&amp;mode=debug&amp;edition=2015)
As the person who (usually) writes the embedded wg newsletter, it always makes me feel a little "meh" when we get lots of upvotes on Reddit, instead of starting a discussion. So instead, I wanted to share some of the stuff that we've done with the embedded newsletter, and see what you think about it. We originally started by keeping all of our newsletters as markdown documents in our main wg repo, but ended up moving [to a dedicated blog](https://rust-embedded.github.io/blog/), mostly for two reasons: First, it gave us a way to offer the newsletter as an RSS feed (we had requests for that), and it gave us a single place to point people to (vs twitter, i.r-l.o, u.r-l.o). The blog is a static site based on Gutenberg (in Rust!), so you could probably fork https://github.com/rust-embedded/blog and have your own pretty quickly (just replace the title, links, and logo :) ). Second, we hope to expand to add more content, or allow "guest blog posts" in the future. We'll see when/where that happens :) Having a tracking issue for the next newsletter works okay, but it becomes a chore to push people to add stuff there. If you have a couple people that reliably post cool stuff that they see, that helps. At this point, I get most of the content for the newsletter from people that mention @rustembedded on twitter, and I use that to fill in the content. Let me know if you have any questions, I'm excited to see more projects offering a newsletter, and I generally get positive reactions and feedback about it.
That's was the main idea of this \`init\_or\_die()\` function: to display a friendly error. Thanks for your support:) &amp;#x200B; Maybe, I'll just put an example in README, how one can handle the error. It's not that big deal.
Also, don't forget to mention your newsletter on your twitter! We have lots of responses from people that saw it from our twitter!
Sure! Here is what I tried: [gist](https://gist.github.com/rust-play/a5a5b6fbb21a20296315ddee3b44592f). These are the same data structures from [miniserde/benches/](https://github.com/dtolnay/miniserde/tree/master/benches). I ran each of these several times: rm -rf target/debug/libcompile-bench.* target/debug/deps/compile_bench-* target/debug/incremental/ &amp;&amp; time cargo build --features serde rm -rf target/debug/libcompile-bench.* target/debug/deps/compile_bench-* target/debug/incremental/ &amp;&amp; time cargo build --features miniserde rm -rf target/release/libcompile-bench.* target/release/deps/compile_bench-* &amp;&amp; time cargo build --release --features serde rm -rf target/release/libcompile-bench.* target/release/deps/compile_bench-* &amp;&amp; time cargo build --release --features miniserde In debug mode serde is 2.6 seconds and miniserde is 1.3 seconds. In release mode serde is 12.0 seconds and miniserde is 2.1 seconds.
Replied below, but drop us a line at office@ferrous-systems.com if you want to have a chat about Rust in Germany :)
It seems like the beginner slots are consistently at 10 in UTC-4. It might be worthwhile to vary the assignment of level to time—I would pretty much never be able to make that slot.
Got help on discord, the idea is to use Rc/Arc with a Cell, this way reference to Cell can be cloned: [https://play.rust-lang.org/?gist=3631ef43886d2c213b41cc7afd1221d4&amp;version=stable&amp;mode=debug&amp;edition=2015](https://play.rust-lang.org/?gist=3631ef43886d2c213b41cc7afd1221d4&amp;version=stable&amp;mode=debug&amp;edition=2015)
No probs. Also, you could make the function accept `ptr` as a `*mut u8` to begin with too instead of `*const u8`, since that way the signature of the function communicates that `ptr` is pointing to data that will be changed.
I've been using it on my laptop for a very long time, and haven't seen issues in months.
On second thought getting global visibility would greatly enhance the utility of this crate, and I begin to suspect my performance worries are overblown. I'll look into implementing your suggestion.
Great job in getting fix and clippy on stable! It feels a bit unfortunate though to have clippy as the name instead of lint. All previous official cargo commands use straightforward descriptive names: init, build, check, run, test, update, etc.
&gt; the main benefit here (that i see) is to get a more in depth feeling of how and why are people struggling with things. This is one of the reasons I spend so much time on IRC (though not anymore) Discord, Reddit, Hacker News, and Twitter. Helping others and then taking their feedback to the docs is the only way to write great docs, IMO.
Displaying the error quantity at the bottom would be useful
Well, I think many compilers operate in the same exact way when printing errors. I am used to checking out errors top to bottom.
This is an endless debate. Some languages do it one way, some do it another. I find myself wanting both behaviors at various times...
Even then, there are problems: https://twitter.com/withoutboats/status/1027702538563477505
From the article: &gt; ####The whole web at maximum FPS: How WebRender gets rid of jank Jank refers to non-smoothness, stutters, etc. &gt; WebRender is known for being extremely fast. But WebRender isn’t really about making rendering faster. It’s about making it smoother. &gt; With WebRender, we want apps to run at a silky smooth 60 frames per second (FPS) or better no matter how big the display is or how much of the page is changing from frame to frame. And it works. Pages that chug along at 15 FPS in Chrome or today’s Firefox run at 60 FPS with WebRender. So, on-screen movement and animations of any sort should be buttery smooth. Perhaps more readily observed would be the same quality of frame rate when scrolling the page.
I almost always scrollt to the top.
I wrote mysteriouspants_throttle a while back, you could use that to slow down a while-true loop. https://github.com/mysteriouspants/throttle It’s dead simple, but I hope reasonably well tested. If you do use it I’d love some feedback on your experience with it and how easy the api was to both learn and use.
Nah, raw pointers are raw pointers. The difference between `*const` and `*mut` pointers in Rust is more like a lint than anything and it's fine to cast between them. Granted, this mainly applies to pointers that are truly raw, such as ones received from C. If you try to violate the semantics of Rust references through raw pointer casts then sure, you'll hit UB. But that's because of the rules governing references rather than the raw pointers themselves.
Thanks, I filed [dtolnay/miniserde#1](https://github.com/dtolnay/miniserde/issues/1) to track this. I don't want to do it right now because currently on stable Rust it is not possible to rename derives when you import them. Also derives are required to be imported at the crate root. Together these mean that if anyone needs to use Serde's derives and Miniserde's derives in the same crate, those derives need to have different names. Rust 1.30 will bring the ability to rename derives when importing as well as importing derives at the submodule level. So after 1.30 is stable we can have Miniserde expose `Serialize` and `Deserialize` derives and leave it up to the user to de-conflict by renaming in the case that they also want to use Serde derives.
Hi all, author of the original post and code here! I'm not very active on reddit so didn't see this link and comments until today. :)
Or php
Rayon's parallel iterators look great when all the data is available up front; I \*think\* I could rig up row-by-row parallelism within chunks (overlapping the filter and deflate stages) by more carefully using the iterators within each chunk. I'll investigate this later, but still have to wrap my mind around iterators that don't have all their input data yet. :)
:tada: [my improvement](https://github.com/rust-lang/rust/pull/52239/) has landed! Non-'static Once is such a small difference that means a lot in possibilities to what's possible to do with the stdlib Once. It's great to see small changes that anyone could have done the research and PR to get changed featured in the (detailed) release notes! (But really, this was a six line change after all the previous Once work since 2014; the hard part was justifying the change, not doing it.)
I didn't even thought they could potentially be different oO
By the way: wasn't there an option so that the compiler omits warnings and errors that are only one line long? This option could be useful for example during refactoring sessions.
Yep. :D &amp;#x200B; I'll add benchmarks with the \`png\` crate when I get a chance, I'm already using it to load samples since I haven't implemented a decoder yet.
Agreed! Anyone have recommendations on best Rust-capable syntax highlight plugin for WordPress? Otherwise I'll google it up when I have a chance. :)
And while you're at it, any reason why you wouldn't just do this and dispense with the temp variable? pub extern "C" fn myFunc(mut ptr: *mut u8) -&gt; u8 { I know the optimized output will be the same, but to me this is more readable.
I feel like that’s the behavior I’m starting to adopt :) I mean, it’s minor mental effort figuring out how far to scroll up each compile something goes wrong, but adds up enough for me to mention here. Was curious if I was just crazy.
Aside from actually decimating colors etc, the best way to improve compression on a given PNG is usually to change the filter selection. mtpng can compress the dual4k.png image much better with `--filter=none` and `--strategy=default` options, but it's not yet smart enough to try that on its own. ;) The default heuristics recommended by the PNG spec and used in libpng (which I've copied) are particularly bad at truecolor screenshots because they have long runs of solid colors interrupted by high-frequency spikes in the color values (eg, switching from solid black to solid white and back over the course of three or four pixels, with no intermediate gray)... When using the "none" filter these compress GREAT! But the heuristic only works on the non-"none" filter modes, which will produce more data per hard-edged pixel in your screenshot. For instance if your input data is `(0, 0, 0, 255, 0, 0, 0)` your filtered data with "sub" mode is `(0, 0, 0, 255, 255, 0, 0)` because it has to bump _up_ from the previous pixel value and then bump back _down_ on the next pixel. 
This is similar to the (minor) irritation that you have to fix all your parse errors to get a compile cycle where it tells you about actual code errors. It's obvious why it happens, but not obvious what a solution would look like.
 cargo check 2&gt;&amp;1 | less -R Stops you from reading warnings/errors from a previous run too.
Makes sense. Good thinking!
I just use `cargo run | less -SR` a lot. :)
I don't think rls-preview was ever available for windows-gnu, at least according to this https://mexus.github.io/rustup-components-history/x86_64-pc-windows-gnu.html. It should be there for windows-msvc though.
Thanks heaps for all the advice! ✨
Amazing work, as always! 
What? How is it not UB to make a *const a *mut? Are you saying that if you receive a *const via FFI you can make it mut? And the question wasnt about UnsafeCell, it was about why can the compiler code transmute a *const to *mut without UB (primitives are implemented like that)
Then it seems I was wrong, I'll edit my above post. At the very least it seems like it hasn't been in for the past few nightlies.
I'm not sure if I'm doing it right, but I can't get `cargo clippy` to work at all. Here's what I tried (I'm inside a working project directory) $ rustup update stable info: syncing channel updates for 'stable-x86_64-unknown-linux-gnu' stable-x86_64-unknown-linux-gnu unchanged - rustc 1.29.0 (aa3ca1994 2018-09-11) $ rustup component add clippy-preview info: component 'clippy-preview' for target 'x86_64-unknown-linux-gnu' is up to date $ cargo clippy error: no such subcommand: `clippy` This is on the WSL by the way, so it is a bit of a weird system.
I thought that rls was not distributed in stable toolchains: I think this is because it is picky about working and so would block updates often.
It's been on the last few stable toolchains at least. But for my target, it apparently hasn't been available on nightly either.
Have you updated rustup recently?
Well, the status page has to talk to health monitors on other servers, so i need to do some networking, do i not need a web framework for that?
I hadn't, but now that I have it works great! Thanks.
In this situation, sure. The user in C is passing in a pointer to data that they expect will be mutated, so they probably aren't passing in a pointer that's qualified as `const` in the C code. And if they are then they should probably change that. Meanwhile on the Rust side it would probably be better for the function to simply accept a `*mut u8` to begin with since mutation is expected. But since both sides agree that mutation is supposed to be happening then casting should be fine as-is.
Cargo check is much faster than run, unless you actually want to run the program ofcourse.
&gt; cargo doc has also grown a new flag: --document-private-items. By default, cargo doc only documents public things, as the docs it produces are intended for end-users. But if you’re working on your own crate, and you have internal documentation for yourself to refer to, --document-private-items will generate docs for all items, not just public ones. This is great for when you want to provide clear documentation to other developers wanting to hack on your code; it's also a great incentive to *comment everything as if it were public*.
In the instance I was thinking of I *did* want to run it, but that's a great clarification. Prefer `check` over `build/run` if you just want compile errors!
It's something that I've been meaning to expand to more cases, but it is hard to do so, particularly [when recovering from parse errors](https://github.com/rust-lang/rust/pull/46732). The compiler itself has in its type system a special type `TyErr` which is [used throughout](https://github.com/rust-lang/rust/pull/53371) to [elide cascading errors](https://github.com/rust-lang/rust/blob/b24330fb7d9e89d63eb03d81fe577172aad49525/src/librustc_typeck/astconv.rs#L1217).
that makes sense. Correct me if I am wrong but.. enums are objects whose variants can have different data types structs are also objects but its variants all have the same data types &amp;#x200B;
Then with a consistent api we could even end up with some mini-serde-json, and mini-serde-bincode, .. crates. And that could provide a practical way to switch serialization implementations while still having fast compile times =)
But actually Ruby:)
Ah OK - I must be misremembering something I read! Sorry
Huh, I didn't realise Serde was adding that much to my compile times! I guess because it's a macro then incrementalncomp doesn't do much to help either?
(I found https://github.com/cedaro/shiny-code which is very much a work in progress but seems to get the job done with a few tweaks. Switching the back posts in the series to use the syntax highlighter now!)
I see, because UB only happens when derefing it, right? So if you don't deref in rust it's ok. But you do have to make sure the C side doesn't change the variable, right, otherwise it's UB?
Yup, it would be UB if that were to happen. But I have the C source for the library I'm working with so I can make sure that the functions I call into are well behaved.
Make sure your `PATH` has the directory that contains the `cargo-clippy` binary.
I haven’t checked out your code yet, but this is very much worth doing.
Can someone explain to me the implications of allowing the `T` in `Cell&lt;T&gt;` to be unsized?
I'm seeing a ~1% performance improvement in this version again: http://chimper.org/rawloader-rustc-benchmarks/ Worst slowdown is ~4%, best speedup is ~3%. There have been some outliers but in general rust performance has been very stable over time.
Yeah, that would be the best way to do it.
 &gt; ## Different: JSON only &gt; &gt; The same approach in this library could be made to work for other data formats, but it is not a goal to enable that through what this library exposes. Maybe the crate name should reflect this. One of my favorite features of Serde is how easy is to support multiple formats. I understand that *mini* can imply *only one format*, but it was unexpected.
The code as it stands lives up to the name. I'm currently working on a version that operates with a user provided storage and, you know, *actually switches.*
I like this idea!
One use that I'm finding interesting is that, thanks to this, you can &amp;mut [T] -&gt; &amp;[Cell&lt;T&gt;] which enables things like this: https://godbolt.org/z/MbpnTV The RFC goes into details, this example is taken from it: https://github.com/rust-lang/rfcs/blob/master/text/1789-as-cell.md
At first I was about to disagree with you as I personally have come to like the name clippy, but I think you are right. For newcomers the name would be confusing and lint would be much more discoverable. 
When I see your name in this thread every week, my eyes always lighten up, you make so much cool stuff and nice to read writeups on your blog/wiki. I also love your work you do on ggez, hope to use it some day for a hobby project.
I think on the whole I like the un-namespaces layout of crates.io, but this is definitely one of the downsides :p
Yep, when I want to be told about the errors in my program the first thing I think of is the stuff that collects in the dryer filter (and sometimes my belly button amongst other places) that I need to throw away periodically.
gnu nightly didn't have RLS for like one or two months now. I assumed it never had it, but I guess that's wrong. It definitely has been reported before, but no one really seems to care.
Yes!
`cargo check --message-format=short`, but it is only available starting in 1.30.
I'm assuming tongue in cheek but in the context of computer languages lint has a different, well understood meaning. If cargo lint could empty my dryer filter for me I'd be grateful though! 
woops, I am mistaken it's just warnings that aren't counted
Gankro says right now it's neutral to negative performance because it's not caching as heavily as the current setup.
\&gt; Q: Why do they not have licenses? \&gt; A: Suggest to me a good one. I don't know which to pick. &amp;#x200B; The most common licensing for Rust Crates is that same as Rust itself. I.e. Dual license MIT OR Apache 2. See Serde for an example of the \[wording in the README\]([https://github.com/serde-rs/serde#license](https://github.com/serde-rs/serde#license)) and the license documents themselves: \[LICENSE-MIT\]([https://github.com/serde-rs/serde/blob/master/LICENSE-MIT](https://github.com/serde-rs/serde/blob/master/LICENSE-MIT)), \[LICENSE-APACHE\]([https://github.com/serde-rs/serde/blob/master/LICENSE-APACHE](https://github.com/serde-rs/serde/blob/master/LICENSE-APACHE)).
I'm embarrassed to admit that I had no idea what a lint was before I started with Rust. (I have no formal CS education.) I did find both clippy and lint to be a bit confusing but certainly nothing too difficult to figure out.
I almost missed this because it's only on the full changelist, not the blog. I'm sure I've wanted this a couple of times.
Nothing to be embarrassed about. You are right that it is quickly figured, though I wouldn't say that's a reason to not improve it. Definitely up to debate. I don't feel strongly about this but maybe others do and would find it helpful 
I also wonder if your issue is related to `str` being recognized as a type (or something other than an identifier). I'm using [YouCompleteMe](https://github.com/Valloric/YouCompleteMe#rust-semantic-completion) with Rust completions from [racerd](https://github.com/jwilm/racerd), and I have the same problem when I re-type your example. If, however, I change `str` to `s`, typing `s.c` triggers the completion popup [as expected](https://imgur.com/Dtz0w6w).
PSA: You can integrate clippy into RLS by putting the following at the top of your crate: \`\`\`rust \#!\[allow(unknown\_lints)\] \#!\[warn(clippy)\] \`\`\`
Here's a few more options I extended from FenrirW0lf's https://play.rust-lang.org/?gist=720d1b97ce5a1eade344a03cf1c66ad2&amp;version=stable&amp;mode=debug&amp;edition=2015
If I recall correctly, the Async Embedded WG's lead is working on their own no-std executor for async await: [https://rust-lang-nursery.github.io/wg-net/embedded-foundations/](https://rust-lang-nursery.github.io/wg-net/embedded-foundations/) &amp;#x200B; They said it wasn't ready yet in their Discord chat: [https://discordapp.com/channels/442252698964721669/474974009667223565](https://discordapp.com/channels/442252698964721669/474974009667223565) &amp;#x200B; But I'm also fairly sure this is it if you were curious! [https://github.com/Nemo157/embrio-rs](https://github.com/Nemo157/embrio-rs)
I can't use Source in any way. It can't be a member, can't be boxed, can't be returned from function, or stored in a variable. It's only usable in calling its methods. An enum is even worse. Having an "InternalSource" that simply reimplements Source generically would mean I'd have to declare/define each and call the original inside each. With an enum I'd have to do the same, except also matching in each function (and it would be slower and bigger). It's a pretty big trait so I'd like to know of another solution. By not having two structs I meant I want it to be a single struct that can be either. If it had a 
That *is* the etymology of a compiler lint though. Just like a computer bug was defined as an actual bug inside the hardware.
Really? Emptying the dryer lint filter is almost as satisfying as when `cargo clippy` comes up clean
Thanks for bringing this to my attention! I'll definitely be keeping an eye on it. I hope to be able to differentiate mine by being minimalistic and providing multiple executors for different needs.
In my experience, "Dumb" is a great place to start designing a library.
Alternatively couldn't `cargo lint` be aliased to `cargo clippy`
This is definitely needed for the space, as different targets have different hardware (or no hardware!) for supporting concurrency. I'll take a look at your code later :)
Clear your buffer entirelly each time and scroll top using HOME. If you are using iTerm, `cmd + K` do the job :)
The same thing can be done from `Cargo.toml` instead: smallvec = { version = "0.0.1", package = "smallvectune" }
By specifying the targets, you've broken the 'paradox' though afaik. Wrapper&lt;u32&gt;::into_inner() would return bool and Wrapper&lt;bool&gt;::into_inner() would return u32. Now, if you made the implementations depend on each other or on itself, then it breaks, ofc. That said, I *am* newish to rust, etc.
casting &amp;T to &amp;mut T is *always* UB. Raw pointers can be freely casted. Note that UnsafeCell’s get method returns a *mut T https://doc.rust-lang.org/1.21.0/std/cell/struct.UnsafeCell.html#method.get
"Always fix the first error, then move on to the next", that's what my professor always said in college. It's true in any language pretty much.
C#'s assumptions are basically the opposite of Rust. Rust assumes value types first, and heap allocated types with dynamic dispatch are built from that. C# assumes heap allocated types most of the time, and value types are more-limited special cases. Honestly don't know about the guts of modern Common Lisp's or what facilities they have for working without automatic memory management, I confess.
Not everyone uses `rustup`.
will do, pardon me. :)
What about flattening nested tuples?
Whatever process installs cargo clippy would be a good candidate.
The worst part of "JSON only" is that there will be no [CBOR](https://cbor.io).
Something I have noticed is I start to figure out what sorts of errors to expect based on how long it takes to produce them, even before I look at them. :-p "Oh, it's instant, I made a syntax error typo somewhere" vs "oh it has to think for a fraction of a second, theres a borrowing problem". https://xkcd.com/1172/ is relevant, I expect 
That explains why my Firefox suddenly got colour management wrong on my wide-gamut display! 8(
It’s in the release notes, but not the blog post. It’s a little too much of an implementation detail for the blog.
Thank you for your incredible work on all of your libraries /u/sebcrozet. Besides producing high-quality libraries that fill a huge need in the Rust ecosystem and provide resources to devs to create awesome stuff, you also have stellar (and beautiful) documentation for each of them on their own domain. (To anyone who hasn't seen them, I urge you to check out r/https://nphysics.org/, r/https://nalgebra.org, and r/https://ncollide.org/). They've got beautiful documentation, tons of working examples that run in the web browser via WebAssembly, example code, and just look and feel so pleasant to use. You engage with the community, answering questions and resolving bugs. And, to top it all off, you make it all open source and available to the world for free. You and people like you are, without exaggeration, the heroes of the software development community. You're both a personal inspiration and a role model, and I thank you so much for the contributions you've made and continue to make.
It’s brave using a name that many/most people are going to associate with Microsoft’s annoying paperclip assistant 
Thanks! :D
so happy about clippy in stable!!!
First time I've seen compile times advertised as a crate feature.
I don't see how it saves any time. * If your code doesn't compile then `cargo build` stops after checking. It doesn't build. * The dependencies will be built, but only the first time. Again *'I pretty much never want to compile without getting a build or running'*. So I do want the dependencies built. I don't see how any time would be saved using `cargo check`.
It's a design decision you will have to make, there is no right answer here. You should use what you're comfortable with using and what you expect to grow your application into. The actor model is a great way to scale architecturally for some scenarios. Actix Web uses the actor framework underneath and so the examples notably will use the same paradigms as the framework. If all you're doing is simple CRUD then it might be overkill, but when you start scaling your app, you'll need to look at the architecture and a way to decouple components from eachother. The actor model may fit that quite well, but it might not.
This sounds great, thank you. Is there WIP that I could take a look at?
It would be nice to meet up and talk more about projects and/or do some actual programming because currently a big chunk of the existing meetups consist of reading through release notes point by point. Not too interesting when I'm sure most of us have already read them or know about them.
When will this be on Linux
Looks like something wrong with performance of the 1.29. My code moved from 5.76s to 6.86s: [https://benchmarksgame-team.pages.debian.net/benchmarksgame/program/knucleotide-rust-8.html](https://benchmarksgame-team.pages.debian.net/benchmarksgame/program/knucleotide-rust-8.html) No dep crates where updated.
I hope you'll find more rusty opportunities coming your way soon. What kind of architecture did you target in the project? --- Actually, I haven't tried my hand at producing DnB since my early teenage years playing around in FL Studio, so I doubt that was me. We're both referencing the same novel though, cyberpunk classic *Neuromancer* by William Gibson.
From what I gather, and from attempts to use each in a project, stdweb is for making web pages and doing front-end work in Rust. Bindgen is for passing Rust (Or perhaps C) functions and data structures to Javascript.
But etymology does not mean its the associated context whenever its used. Just like how i conceptualized "bug" different depending on the context whether its software or not. Dryer lint is not usually thought of while using a linter since there is a closer context for your brain to associate language with
I wish `.expect()` was actually called `.or_die()`...
&gt; If you don’t have it already, you can get rustup from the appropriate page on our website ... &gt; To install Rust, run the following in your terminal, then follow the onscreen instructions. curl https://sh.rustup.rs -sSf | sh ... &gt; This downloads and runs rustup-init.sh, which in turn downloads and runs the correct version of the rustup-init executable for your platform. Wow, that's dangerous. 
Don't reach for minihttp, as it doesn't implement most parts of HTTP. But don't forget hyper, it's [no slouch](https://www.techempower.com/benchmarks/#section=test&amp;runid=a1843d12-6091-4780-92a6-a747fab77cb1&amp;hw=ph&amp;test=plaintext&amp;l=yyku7z-1)!
"Fastest" probably needs some context. Recent benchmarks suggest [hyper is there](https://www.techempower.com/benchmarks/#section=test&amp;runid=a1843d12-6091-4780-92a6-a747fab77cb1&amp;hw=ph&amp;test=plaintext&amp;l=yyku7z-1).
Tokyo Rust Meetup is looking for co-organizers! Please feel free to contact me, or contact the organizers on meetup.com. Note that there’s also another, mainly Japanese meetup which is much larger. https://rust.connpass.com/
&gt; I was talking about when the compiler casts a &amp;T to &amp;mut T (implementing primitives and shit). I'm not sure what you're talking about. &gt; And what do you mean it can be freely casted I mean that casting `*const T` to `*mut T` and back is never UB.
It looks like you don't eagerly evict expired entries and instead wait until the next lookup. That could be a surprising memory leak. The tricks used by Java's Caffeine are: For a fixed duration like your case, you can use a queue. For example expireAfterCreate is merely a FIFO where the head is the oldest and the tail is the youngest. To expire eagerly, you just peek at the head to prune. Similarly expireAfterWrite reorders the entry on a write, and expireAfterAccess on a read or write. The only problem is you then have to deal with doubly-linked lists. For per-entry durations you want to retain order. A heap is popular, but O(lg n). Hierarchical TimerWheel is amortized O(1) by using hashing instead of comparisons. It's a fun little data structure.
This has been discussed multiple times, please just do a thread search of this reddit to see why it's considered ok
File a bug!
This submission does not mention Rust or appear to mention any project written in Rust, and therefore triggers our off-topic heuristic. Please leave a top-level comment explaining why readers of /r/rust might find this relevant.
I don’t know what you’re taking about re primitives. And yes, you can do those casts, but they aren’t *inherently* UB. &amp;T to &amp; mut T inherently is.
We upgraded LLVM, maybe that has something to do with it. Someone else said performance increased for their stuff...
I just said it may not be primitives, the question was about UBs that the compiler causes but are "checked"...
I’m not aware of any such thing.
I second this. rust tooling shouldn't assume that the installation was done through rustup.
Thanks!
Hopefully helps make some more low level safe APIs, hard to have memory safety without a memory model
You can do this yourself if you want; just symlink the `cargo-clippy` binary as `cargo-lint` somewhere in your path.
Thank you for those incredibly positive comments! The Rust community is amazing and it feels great to be part of it and contribute to its development. I take great pleasure on doing those developments and on seeing they can be useful for some and help the Rust language to be more widely adopted in the long term.
`i` isn't a number, it's a number on a box. You need to open up the box in order to add to it. `*` in this case opens the box for you. It's this ELI5? :P
Cool idea! What's the reason for the "CSV" file being a S(emicolon)SV, not a CSV? One could also include column headings, and append to it (but only if `seek(Current(0)) == 0` when it is opened) instead of overwriting to make it easy to run a program multiple times and summarise the distribution across all of them. 
&gt; I find myself wanting both behaviors at various times... So I'm not alone. Although I must say that given the choice, I prefer the one error at a time approach. Like the OP mentioned, the second and third errors may be due to the first one, so there's no point in even considering them. It also makes compilers simpler, because they can just display the error and exit rather than find a synchronization point and attempt to continue. (I believe that rustc already does this at the lexer level: https://play.rust-lang.org/?gist=6b5b82b4781cd968d0c0760ddd237344&amp;version=stable&amp;mode=debug&amp;edition=2015)
[https://learning-rust.github.io/docs/d4.crates.html](https://learning-rust.github.io/docs/d4.crates.html) might be helpful for you :)
Nvidia first, not only
`stdweb` also does all the work for passing `Rust` functions and data structures to JavaScript too, though. In addition, `wasm-bindgen` [wants to expose all APIs necessary to interact with the web](https://github.com/rustwasm/wasm-bindgen/issues/275). My understanding is that `wasm-bindgen` is a new repo reimplementing all of `stdweb`'s functionality just with a slightly more segregated repository structure.
Great thanks! This is the what I need.
Sure, but if you've never heard of what a lint is in terms of a compiler, you're going to think of what you do know, which is dryer lint.
Hey, Thank you very much for the link. However, I already did have a nice look at that before posting here. Do you know about any other resources ( such as a book ), which mainly teaches Rust projects ( including crates?). &amp;#x200B; Please let me know if you do. &amp;#x200B; Thank you. &amp;#x200B;
If you use cargo to make your projects you already have a crate, it's just not published. https://doc.rust-lang.org/cargo/reference/publishing.html talks about publishing a crate, but the whole document is worth a read.
If you already have code you want to publish, https://rust-lang-nursery.github.io/api-guidelines/ is a good resource for making it nice to use as a library crate. If you're wanting to write something but don't have ideas for what to build yet, https://github.com/not-yet-awesome-rust/not-yet-awesome-rust lists things that crates don't exist for yet! You could look through and find something that looks interesting to you to start.
The eternal rhetorical question.
Not that this is a reason to be embarrassed, but linting is not part of a CS curriculum. As least as far as I'm aware. 
 As said by alexcrichton in the comment, "there's a lot of improvement for the working group to improve communication between the two projects". I really want to see the status of stdweb project in "*This Week in Rust and WebAssembly*" in near future :)
True. What does clippy suggest then? Since it doesn't have a cs origin it helps nobody right? 
It is rolled out incrementally. They begin by releasing it to a limited audience of Nightly users, in this case only people using Windows 10 with an Nvidia GPU. This will help discover problems with that specific configuration. Later it will be enabled for more kinds of setups until eventually it is enabled for all Nightly users. They won't enable a huge change like this for every user at once.
&gt;[https://github.com/not-yet-awesome-rust/not-yet-awesome-rust](https://github.com/not-yet-awesome-rust/not-yet-awesome-rust) Thanks for the link! It helps. 
Thank you very much. This really helps. 
If you enable it manually: right now If you wait for it to be enabled automatically: as soon as it is included in a Firefox release (version 64 according to current plan, but that might change)
Thank you for the link. It really helps 
I doubt that many people these days would even have seen the actual assistant, much less know its name. I'm also not too sure if the functionality of this program is the same as the actual clippy (suggesting to reformat a document or give helpful tips when writing a certain type of document). I would expect something like "It looks like you are trying to write a parser, have you looked at the following crates?" from it, not actual linting.
Just left over from when I was going to use a counter to step through the vec when I just though why not use iter()?
There's a setting in the RLS VSCode settings that seems to have the same effect: ``` "rust.clippy_preference": "on" ``` I've had it in my settings file for ages but it never worked for me, as soon as I installed Clippy on stable it sprung to life :)
Glad I could help. These "minor" tooling bumps make a big difference!
I'd suggest looking at the functions attached to `Iterator`, for example, `min()` and `max()`, finding the largest in a randomly generated list could be achieved with the below. extern crate rand; use rand::{thread_rng, Rng}; fn main() { let mut numbers = [0u64; 9000]; thread_rng().fill(&amp;mut numbers[..]); if let Some(largest) = numbers.iter().max() { println!("Largest: {}", largest); } }
I think the idea is that you want to be able to click on any definition and jump to the exactly right definition, even if its inside one of your dependencies.
How is this a challenge? He comes up with constraints that fit Julia and every language either has to reimplement these in a DSL or in a bunch of macro. What does this prove, that the DSL of Julia matches the use-cases it was designed for?
So you're not using an editor that can parse the error messages? Because then it would just automatically jump to the source with the first error.
Why does this require rustup? `cargo` will look for subcommands in your `CARGO_HOME`, so you can add the symlink there independently of how you installed it.
https seems broken on nalgebra.org and not fully supported on [https://ncollide.org/](https://ncollide.org/) and [https://nphysics.org/](https://nphysics.org/). &gt;nalgebra.org uses an invalid security certificate. The certificate is only valid for the following names: ssl3.ovh.net, [www.ssl3.ovh.net](http://www.ssl3.ovh.net) Error code: SSL\_ERROR\_BAD\_CERT\_DOMAIN &gt; &gt;nphysics.org Parts of this page are not secure (Such as images)
Did someone create a ticket for the openssl crate to ask about this? Just writing about this on reddit is probably not going to magically solve the problem :)
No this is not the case, the function uses a mask: &amp;#x200B; `impl&lt;'a&gt; _MODEW&lt;'a&gt; {` `#[doc = r" Writes raw bits to the field"]` `#[inline]` `pub unsafe fn bits(self, value: u8) -&gt; &amp;'a mut W {` `const MASK: u8 = 0x03;` `const OFFSET: u8 = 3;` `self.w.bits &amp;= !((MASK as u32) &lt;&lt; OFFSET);` `self.w.bits |= ((value &amp; MASK) as u32) &lt;&lt; OFFSET;` `self.w` `}` `} l`
I'm pretty sure there's an option for that. Search for rust in the ide options and disable something that sounds like type hints (sorry, not on my computer right now to give specific instructions) 
&gt; self Then you might have to open an issue on [svd2rust](https://github.com/rust-embedded/svd2rust) !
I’m unable to locate the Tokyo Rust Meetup on meetup.com - is the group private? Can you send me an invite? Via DM 
Of course he want's to showcase the features of Julia but he also said that he is interested in what other PLs are capable of. It would be even more impressive if there was a solution close to Julia, but written in Rust (or D or Nim). Being able to implement these kind of things in a DSL or macro is a valuable feature of a PL, IMHO.
What did it do? I wonder what tool would document private items before?
In this "rant" he makes so many false statements... it's just weird. It's an interesting dive maybe into game programming problems in general.
I found a workaround: call the original method on a trait with bounds, like so trait InternalSource: alto::Source {} impl&lt;T: alto::Source&gt; InternalSource for T {} // with an InternalSource alto::Source::play(source);
Will do.
Because when your method returns a Foo::Bar (or requires me to pass a Foo::Bar in as a parameter), and I can't click on Foo::Bar to see what I can do with it then the documentation becomes rather frustrating to use. This is especially the case if I wouldn't be able to see what crate Foo::Bar is defined in, and what version of that crate.
What false statements does he make?
Yeah I had a short look. I appreciate that you can probably turn them off, but then I won't get them at all... I still want to see them, just wouldn't mind if they collapsed super long ones!
Does the Julia version work on sets and trees as well?
The part when he claims she 'turned the borrow checker off' because she's built a custom memory allocate is misleading. The borrow checker is still doing it's thing. He is right that she has managed to avoid the borrow checker, but that's because her simple ECS system has a single point of ownership. The ECS system. Everyone else then borrows from it.
Is there documentation anywhere about what each rustup component adds? E.g. what does `rustup component add rust-analysis` add and how does it differ from `rustup component add rls-preview` or `rustup component add clippy-preview`?
That's not what he meant. What he meant is that the EntityId's are effectively pointers, with the same problems as handing out a pointer. The difference is these "fake references" are not borrow checked, resulting in the same problem as just passing out pointers -- we've not solved the actual problem, just pushed it back further. That said, I think he doesn't quite understand how powerful Rust's type system is, and I think Rust's story on solving the actual problem he identifies is pretty decent.
https://www.meetup.com/fr-FR/Tokyo-Rust-Meetup
I think he misunderstood the distinction between safe and correct. Using a Vec and indices like your own allocator and pointers may result in incorrect and undesirable behaviour, but it cannot cause memory unsafety.
For example he confuses "memory safety" and "business logic safety". He says that "borrow checker helps to achieve a better design" is a "Stockholm syndrome", but it's clear that it prevents users from naive approaches, which in C or C++ can lead to memory safety issues. Yes, using vector+indexes can be viewed as pseudo-custom allocator, which is prone to business logic errors in the same way as raw pointers, but it's much-much better deal from the memory safety point of view. And yeah, he says "crashes are not so bad as you think", I wonder if he includes here random crashes from memory issues lurking somewhere deep in your program...
I think he was ignoring the memory safety aspect and talking just about the correctness. If the generational index is misused you can still "point"" to the incorrect slot in the vec and get either nothing (easy bug to fix) or the incorrect element (harder to fix). Both of these issues are also present in the "c++" way, except that now there's 3 potential issues: stale data, garbage data, or a crash (preferable).
Thanks for that. Interesting to see you used criterion since one of my local dev branches uses it as well. I think I do not want to pull it, tough since the speed for parsing the format itself isn’t critical and I’m not happy with going unsafe for that. But definitely thanks for the suggestion! I’ll comment on that PR when I have access to a proper device again, 
I'm assuming his premise is that the borrow checker doesn't solve the entity reference coordination problem. I've not actually seen the original rustconf closing talk, so I'm also assuming that the solution proposed in the talk is to not use raw pointers but indecies into an array (/Vec) that holds the entities/components instead. I'm well aware that this might not be the actual implementation from the talk, but this doesn't matter to make Jonathan's argument work, because it is true that the naive implementation satisfies the borrow checker. (you'd probably make the Vec reference counted as well, but I'm ignoring that here). Going from what was said in the original talk (or rather cited in this video), namely that this kind of implementation is the standard, I'd even argue this is the a minimal solution satisfying the borrow checker. It's important to note that you still have the bug when "freeing" references (and reusing the same heap space), because entity referencing the component only knows it index. It is totally obivious to any internal state changes of the referenced object, or even memory overwrites. Going from all of this, it is certainly true that the borrow checker did not force you to implement a correct solution to the entity reference coordination problem. But I'm not sure,m if I would follow his reasoning to the point where he dismisses the/a borrow checker as a useful tool (for game engine programming). Jonathan seems to have a good understanding of ownership and "borrowing" as in "(implicit) use dependencies". For him the borrow checker doesn't seem to have that much of a benefit. But a programmer who isn't used to think about this concept might very well profit from a 'harsh' compiler doing these things. I'd say it ultimately comes down to personal preference. A functional programmer thinks that imuatablity is great, because he doesn't have to worry about side effects. A imperitive programmer would propably feel to restricted from making 'low level', performant code. It's the same with the borrow checker. As a rust programmer, you'd think it's great that the compiler takes care of stuff like use after free problems, and making mutable function parameters explicit and so on. On the other hand, if you already think about that stuff already, an overblocking borrow checker is useless or even restrictive. In both cases the paradigm doesn't solve the hard problems for you. But it can at point some obvious things, and make you more confident in the correctness of your program.
His comments about weak pointers are just plain wrong. You typically just keep a counter of how many weak pointers are remaining, not a complete list of them. This is the case for both Rust's `Rc` and C++'s `shared_ptr`. &amp;#x200B; In Rust it's also impossible to use a weak pointer incorrectly as calling `upgrade` on it returns an optional strong reference. &amp;#x200B; However, his comments about loss of performance compared to using a hash table (or vector) might be true although it would be interesting to see some benchmarks.
I would say still go through the new changes but in 10 minutes or less. I swear one time we spent like 30 minutes talking about "? in main" which is a great feature but not a great conversational topic. I've also found the Japanese meetup to be nicely geared to beginners. They do code exercises and have knowledgeable people walk around to give help if you get stuck.
Ah, thanks.
TLDW: person that hasn't used Rust complains about a 30 second excerpt from a presentation about writing a game (not about Rust) and then goes on to explain how Rust the language is bad because it doesn't solve 100% of his problems. I'm not even sure he watched all of the presentation because the presenter speaks about all of his complaints later in the talk. I looked at his language, the easy way of switching between SOA and AOS looks useful, I wonder if you could do that in Rust.
12 mins in, i got bored
I think when he mentions "crashes are not so bad as you think", he means that crashes are better than subtle logic error or silent data corruptions. This is somewhat consistent with Rust's fail fast story with panic that if we know there is a bug in the program, we should just fail fast and loudly instead of trying to recover from it. In addition, he is not denying the Rust's benefit of replacing run-time crashes with compile errors. Instead, he is trying to argue that the way ECS talk changes from using pointers into indexed vectors are not making the program more correct, but just make crashes into logic errors, which he then argues may not be actually better.
He doesn't, the whole reason he says "crashes are not so bad as you think" is because it eliminates some of the random memory bugs that lurk deep. I'm not sure he meant that the better design due to the borrow checker in general is stockholm syndrome, but more in this case because he suggests that instead of really solving the memory safety issue she just moves the safety problem to the business logic so the borrow checker can't get at it anymore. I agree though that he completely skips over the fact that her idea makes the crashes nice and memory safe instead of deep and lurky.
I don't think he says Rust is bad, he says he's still not convinced that the increased friction is worth his time when programming games. 
yeah, I think that'd be cool. I created [an issue](https://github.com/intellij-rust/intellij-rust/issues/2850) for that. I found that when they get in my way (like when showing the code to someone else) I can toggle quickly by right-clicking on the hint and 'Disable Hints' and then Ctrl-Shift-A, 'hint', 'Enable Parameter Hints' to bring them back. There appears to be some sort of blacklist possible but that seems like it's not implemented yet for Rust.
They absolutely are better, though. You can't corrupt your application state with a panic. I'd much rather get an error log with a stack trace that points to the actual error than have to debug memory corruption. Also, lots of games are networked; I don't want to write a game with latent buffer overflows that ends up getting all my users pwned.
You can literally turn off the borrow checker, and building a game engine seems like a good reason to turn it off and people still don't, they look for patterns that *involve* the borrow checker. Not sure if that's how stockholm syndrome works, it's like the kidnappers leave the door open and tells you can go and you still stay.
I got bored 3 minutes in. An hour long video is not a good way to make a point, write a blog post please.
and quite frankly, I don't think that's a bad concern to have. I personally think the increased friction is somewhat useful, but I totally understand not liking it, and I certainly don't enjoy it all the time. 
Seriously. I understand he's considered to be somewhat brilliant, but his point of view is so narrow and he's really not open to entertaining ideas that aren't his. Working for him must be a nightmare. His code also always looks like a giant mess, but that's something else entirely.
I find all of Jonathan Blow's videos to be interesting artifacts. I can't identify with his approach to software at all, games programming really does seem to be it's own little world. The video itself is an hour long, full of long pauses, lots of chat, digressions, bald statements of opinion ("Vec" is a bad name for a vector, apparently). The last 20 minutes is just random, mostly off-topic questions from viewers. If it was boiled down to a blog post it could be read in 5 minutes (but would probably take longer than an hour to write). I think this form of creating and consuming media probably only makes sense when you view it as Twitch-like entertainment. Probably lots of the viewers are gamers first and foremost who want to know a bit more about the development process. As for the content - meh - he has his own style of programming which is completely antithetical to how Rust was designed. This is clear from comparing Rust with his own language Jai, which has zero interest in memory safety or correctness and mostly aims to be a de-crufted and more productive C++. He does state at one point that he has never written Rust beyond hello-world, and it shows. I think he should write some more before he makes any more "rants". I don't think he would change his mind about it, but it might stop him wasting his time making videos that will be easily dismissed by the community.
I think this is a good talk for anyone interested in game engine development and Rust. &amp;#x200B; I have previously been perplexed by his claim that Rust is not providing a much better solution comparing to other programming languages (like C++), because I feel even not counting algebraic types, generics, or macro system comparing to C++, the borrow checker alone is a great help in improving software quality. After this talk I still thinks Rust has more to offer and there is defnitely a lot of room to explore for using Rust in game programming, but I kind of understand where he is coming from. The degree borrow checker interferes with the traditional game engine patterns (one of the example is [trouble in vulkan binding](https://github.com/vulkano-rs/vulkano/blob/master/TROUBLES.md)) may make the trade-off a little less appealing than other areas of system programing. I appreciate this kind of thoughtful disagreement with clear elaboration. I can actually learn a lot from a different perspective even when we disagree.
The difference between a panic and a segfault is huge though, a panic gives you the reason in plain text, and a backtrace. 
You can’t turn the borrow checker off in Rust, not even in unsafe Rust.
That’s the only “useful” thing in its language. There are proc macros for doing that in Rust.
Start here: https://youtu.be/4t1K66dMhWk?t=36m15s
Learning Rust I realized that I'm writing really bad code. As in I was taught to write bad code, and even at work I was shown bad coding guidelines and styles to follow. Error-prone, lazy and dirty. Then we get 5-10 year old bug reports, sudden crashes from customers and all that jazz. Even if you don't plan using Rust professionally, it's a very good idea to learn it, just so you can pick up a few skills and practices that will make you write programs with a better mindset around safety. That's what I think at least. I agree wholeheartedly that compile-time errors are far better than runtime crashes/errors. Even if they both take the same time to debug, I'd rather have my errors handed to me during build, than during a runtest. 
 fn borrow_checker_go_home&lt;T&gt;(t: &amp;T) -&gt; &amp;'static mut T { #[allow(mutable_transmutes)] unsafe {std::mem::transmute(t)} } Where is your god now, etc
In fact he's actually saying a few good things about rust, and even praises it as a new language, as it doesn't just bring in flavor and design differences from other languages, but actually a new thing unique to it's language. I don't completely agree with what he says, but I don't find him as critical as others either...I think he's just talking very 'honestly' and 'dryly', which can rub a lot of people the wrong way...
I don't think he is arguing that replacing crashes with panics is useless, in fact panic isn't really mentioned. In this particular case the logic error doesn't trigger a panic, it just overwrites data from an other (valid/live) object. 
Let me get this straight. &amp;#x200B; He posted an hour long rant in response to a forty five minute long talk. One which he evidently didn't even bother to watch closely, because he goes "I think she implies in this minute of speech that that's the \_only\_ way to do it?" and goes off on why that's wrong, and ... she hadn't? &amp;#x200B; What the hell.
You didn't turn it off, you used a special compiler intrinsic that bypasses it, for exactly the usecase of bypassing it. The borrow checker is still doing it's thing, you just used a blessed function that the checker has been hard coded to trust completely.
Yeah but the difference isn't gigantic. You can still end up with stale data, and you can still end up with a form of garbage data (still type safe, but could be anything), and you can still end up with a program crash (if the component was supposed to be there, but wasn't and you panic). It's better than raw pointers, but it's still god awful.
&gt; And yeah, he says "crashes are not so bad as you think", I wonder if he includes here random crashes from memory issues lurking somewhere deep in your program... I think it's important to remember the context he works in: single player games. In a single player game, crashing and losing your progress is the absolute worst thing that could happen and it's just not that bad in the grand scheme of things. Once your code opens a network socket though, things change very quickly. With network code, crashes (and I mean actual crashes not aborts) usually indicate a memory safety issue that's only one carefully crafted network packet away from being a security issue. 
Right. It's still annoying to have it called vec when you already have some other objects called vec everywhere. 
You haven't "turned off the borrow checker", you've just changed an `&amp;` ref to an `&amp;mut` ref which is violates the memory model and is UB. `unsafe` doesn't turn off the borrow checker: fn main() { unsafe { let mut x = 1; let y = &amp;mut x; let z = &amp;mut x; //ERROR cannot borrow `x` as mutable more than once at a time } }
The borrow checker checks references and is always active, but your code uses raw pointers (that’s how transmute is implemented internally), so which the borrow checker does not check. Basically, your comment is like saying that you can turn off air space regulations by traveling by car. Just because you can travel by car and Aerial regulations do not apply to it does not mean that they are not still active and still applying to airplanes.
They kind of are tho. In practice weak_ptr keeps a pointer to the object and the control block and checks the reference count to see if the object has been allocated. It even has its own weak reference counter to prevent the control block being deleted when there are still weak_ptrs around. ***** Jon blow weak pointer implementation is not the usual one, and is terrible, but sometimes it's the good solution because it's the only one (afaik) that doesn't leave allocations when the object is deleted. Typically weak_ptr can not free the control block (and the object if you use merged allocations) until all the weak_ptr have gone out of scope. This means that it is really hard to guarantee that you memory consumption won't go over some limit, which can be really bad.
\&gt;His code also always looks like a giant mess, but that's something else entirely. &amp;#x200B; But the results are way way above average in quality. Maybe that means something, about worrying about what code looks like via some aesthetic intuition, I'm not sure. &amp;#x200B;
It's a small wart, but it's still a wart that doesn't need to be there
Is this a lot? How many questions do competing languages have? 
I was going to make a quip about just using the abstracted n-dimensional name for a vector but apparently those are just vectors also. Fair enough, Vec could have had a better name
Why do you think that this patterns should have a special language-level support? I think they are already abstract pretty well behind safe API using existing tools, see e.g. `specs` crate.
&gt;I looked at his language, the easy way of switching between SOA and AOS looks useful, I wonder if you could do that in Rust. Not as easily, but custom derive/proc macro (code generation in general) allow you to get halfway there: [https://github.com/lumol-org/soa-derive/](https://github.com/lumol-org/soa-derive/) (full disclosure, I wrote and use this =))
I don't I'm just asking. 
I understand Jonathan Blow has produced some great games over the years and appreciate that he's at the very least playing devil's advocate amongst a community of evangelists, but I feel like his rant is a bit rambling without stringing together any concrete points about precisely why Catherine's approach is sub optimal. Just a lot of disparaging words re: Rust in an attempt to proselytise his own language it seems.
Maybe it's some #ifdef in the header?
I think Rust isn't terribly popular on SO. If we assume that Rust is 3 about years old, that's about 9 questions each day. Which is not a lot. Would be interesting to see if there is a upwards trend. 
&gt; The 1.29 release is fairly small; Rust 1.30 and 1.31 are going to have a lot in them . . . Is there somewhere where we can view a roadmap of planned features in future releases?
JB went on a memorable rant a while back because he made the claim that most UIs are not responsive because they're written in managed languages. It was interesting to witness someone who is such an expert in a single domain yet so insanely ignorant in another. I got called an idiot for disagreeing and providing plenty of counterpoints. He seems like a nice guy.
&gt; I can't identify with his approach to software at all, games programming really does seem to be it's own little world. I believe he is in his own little world even among game programmers.
Well, in theory, by definition a weak pointer shouldn't prevent the referenced memory from deallocation, that's why they are "weak". In practice, the C++ std is full of gotchas. I think a weak\_ptr can prevent memory deallocation if the pointed object was created with make\_shared instead of new. But honestly I try to stay away from the std as much as I can so I'm not really sure.
We don’t do releases by features, but by time. So there’s only a general roadmap for the year, and then we do as much as we can, and features and and stabilize whenever they do. However, you *can* always tell what’s coming in the next release, as it goes into beta six weeks before release. We haven’t put the notes together yet though. These two releases are also a bit odd thanks to the edition; so we know a bit more about them than usual, as they’re the culmination of this year’s roadmap.
It gets rid of both type hints on variables and parameter names on function calls. I wish I could just disable the function calls! 
Long functions are sometimes preferred in industries where safety is paramount. An interesting read on this idea by John Carmack: http://number-none.com/blow/john_carmack_on_inlined_code.html 
This is a clip from a livestream, all he's done is dump it from twitch to YouTube.
| rust | 10,008 | | go | 32,298 | | swift | 206,808 |
From my personal experience (I'm watching the tag and answering questions for roughly 2,5 years now), there is an upwards trend, yes. I feel like today, there are way more daily questions than only a year ago. Right now 9 questions per day are... roughly accurate. But there were already a bunch of questions before the Rust 1.0 release. I'll try to generate a nice graph on [data explorer](http://data.stackexchange.com/) later :) 
stdweb seems far more polished and easier to use for me. That js macro is sweet. 
you are a god
How can we see what is being merged in a version and how is it decided? I am asking because I got a commit merged into master which fixes an issue I have in production, but it's not in 1.29.0.
wasn't the reason for those "gigantic functions" was to make the code more readable? basically you'll understand exactly what the code is doing without looking at comments kind of thing