I think a video would be best effective if coupled with bullet points (sections) on the major themes covered in the video. Ideally each point would be accompanied with a short description (or perhaps a transcript of the video -- you need a textual transcript to provide subtitles anyway) and any relevant source code. Bonus point if the user can click the section heading to jump in the video. Perhaps of the above suggestions, the "subtitles" part is the most important. I can read and write English, but I struggle to understand spoken English.
~~The problem is that the pointer must both be a `const char *` so that the FFI can use it as a null-terminated string, and also be a pointer that Rust can again pick up and destroy.~~ ~~The only way to do that, AFAIK, is to use `Box&lt;[c_char]&gt;`.~~ EDIT: This won't work (see minnos comment).
I'm not sure it's safe to transmute back and forth like that with fat pointers. [`Box&lt;[c_char]&gt;` and `*const c_char` have different sizes](https://play.rust-lang.org/?gist=0407ec531b3bc0411c04&amp;version=stable).
&gt; Is this more efficient than assigning it to a variable? Compiled languages are generally smart enough to make those sorts of optimizations for you. In this case, I suspect not, since the program will need to keep a counter around either way to tell when it's done the loop enough times.
Rust isn't inherently lazy, and other types described as monadic in Rust (like `Option&lt;T&gt;` and `Result&lt;T, E&gt;`) are also evaluated eagerly. The function passed to `and_then` is still only evaluated if the monad is in a 'success' state. I have not heard it said before that laziness is fundamental to monads rather than to Haskell, though I admit that I avoid reading about monads when I can.
Within your system, you could commit to only using Burrito to perform IO. I don't really see the impact of possibly depending on crates that perform IO another way. The impact is that there could be composition problems for libraries expecting IO to be performed using `std::io` (e.g. those that take type parameters bound to `Read` or `Write`). I think serde may be such a library, for example. This problem exists for all third party io libraries, though, such as mio.
What if you had a trait for converting a Result/etc into a throw::Result, and throw::Result implemented it to return itself, while Result implemented it to do the conversion?
Execution isn't the same as lazy. Its the fact that your monad isn't pure, which makes it useless as I can't construct an action and send that to something else, without executing it.
That would work, I did start with such a ThrowBehavior trait, but because of coherence, you can't `impl&lt;T,OldE,NewE&gt; ThrowBehavior&lt;T, NewE&gt; for Result&lt;T, OldE&gt; where OldE: Into&lt;NewE&gt;` - which means you can get it to work with one macro, but then you can't use the same From/Into conversions which you can use in try!().
That might be good - one thing is that both trace and mark in that case would propagate the error upwards, and mark an ErrorPoint on the Error. I mean both of them would do both actions, so I'm not sure it would be as clear which one is which.
I don't feel any need to tell you not to make them ;)
Same in Haskell, actually!
This produces nested vectors right? Is the compiler smart enough to "inline" them next to each other as a single large block, or will they be disjoint? The latter isn't very cache-locality friendly. 
Yes, as long you include some complicated examples. I've been using Rust for over a year and I still can't figure out more complicated "conflicting lifetime" issues. Like many others, I prefer text over videos, but sometimes when the text doesn't get through to you, videos work much better.
I think that a video series with a lot of abstraction and graphics that slowly transitions to the actual code would not only make Rust more accessible, but also finally teach me how the fuck lifetimes work exactly.
In that case, I'd guess a diagram could visualize the concepts just as good as a video. But I'll be gladly proved wrong.
Fair enough.
[Like this](http://cslibrary.stanford.edu/104/) ;)
Videos are good because they can be watched and rewatched until it sinks in.
So I just started learning rust ~3 days ago. The concepts of ownership and borrowing seem pretty simple in theory, but I struggled with it in practice. Part of this was really the iterator APIs, e.g. there is no nice way to remove elements from an iterator as you go through it, after thinking a bit this started to make more sense, since I was working with a Vec and removing things from an array is expensive, but even the linkedlist collection did not have that. Once I found https://github.com/contain-rs/linked-list my life got much simpler. So far I have not had to do anything particularly complicated with lifetimes. I did one exercise where I denoted that two lifetimes in a function signature were the same, but that was about it. I wouldn't really want a video series though. Good description + diagrams + exercises would be a lot better. I'm actually really looking for good exercises to do that will help me get a better grip on the language.
Hi Everyone. I am the lead developer for the [ArrayFire library](https://github.com/arrayfire/arrayfire). Right now OP, me and another one of our engineers are trying to create the rust wrapper. We are all excited to work on Rust. But we are all new to it. So any help and / or feedback is greatly appreciated.
I am not too familiar with rust (as mentioned in the other comment), but if Rust can dump LLVM IR, there are ways you can compile them into CUDA and OpenCL kernels. Alternatively, you can write CUDA and OpenCL "C" kernels, stringify them, then compile and run them using the CUDA and OpenCL runtime libraries. With that said, *most* people do not want to write low level code. This is the reason ArrayFire exists providing us, the developers of this library, with a day job. :)
It doesn't seem entirely infeasible to create a lint that ensures that you don't call impure code without lifting it, with external functions assumed to be impure. A step up from that would be a syntax extension which rewrote your crate so that all references to external code are automatically lifted (unless that crate is also a pure crate).
Regex seems overkill for this. Any reason you didn't use split_whitespace[1] for this? I'd be interested to see the benchmarks between using the two approaches. [1] https://doc.rust-lang.org/nightly/std/primitive.str.html#method.split_whitespace
thanks, updated
Is such kind of style a prescribed standard or just a preference? For instance I find 4 spaces to be too much and prefer to use 3.
Never said anything contrary to that.
I've had a similar thing in my toolbox for years. I call it `f` (for "field") and it's a short perl script that I [copied from here](http://perl.plover.com/classes/mybin/samples/slide010.html): #!/usr/bin/perl # http://perl.plover.com/classes/mybin/samples/slide010.html my $field = shift or usage(); @field = map { $_ &gt; 0 ? $_ - 1 : $_ } split(/,/,$field); while (&lt;&gt;) { chomp; my @f = split; print join(' ', map { $f[$_] } @field), "\n"; } sub usage { print STDERR "$0 fieldnumber\n"; exit 1; }
The patterns are: fn do(&amp;self, &amp;other) -&gt; &amp;thing assumes you're borrowing something out of `self`. fn do(&amp;stuff) -&gt; &amp;thing assumes you're borrowing out of `stuff`. fn do(&amp;stuff, &amp;thing, &amp;more) -&gt; not_a_reference assumes all of the lifetimes in the parameters are distinct. If any of those assumptions are violated (e.g. `fn do(&amp;self, other: &amp;u32) -&gt; &amp;u32 { other }`), then you need to include explicit lifetimes to disambiguate it.
As superlogical mentioned I wouldn't clone the Vec in parse_args. Though I'd also not remove the element as was suggested. Just pass in a slice of what you need and simplify your parse_args function. You also explicitly create new variables and explicitly return them more often than you need to. You only need the return keyword for early returns. If your last expression is your return value, just omit the semicolon fn parse_args(arguments: &amp;[String]) -&gt; Vec&lt;usize&gt; { if arguments.len() &lt; 1 { usage_err_exit(); } arguments.iter().map(|x| { let col_arg = FromStr::from_str(x); if !col_arg.is_ok() { usage_err_exit(); } col_arg.unwrap() }).collect() } fn main() { let args = vec!["fff".into(), "1".into(), "2".into(), "3".into()]; let columns = parse_args(&amp;args[1..]); } If you mark your usage_err_exit function as divergent ( -&gt; ! )you can even use it in match arms to get a bit more idiomatic fn usage_err_exit() -&gt; ! { println!("usage: nth &lt;columns&gt;"); exit(1); } fn parse_args(arguments: &amp;[String]) -&gt; Vec&lt;usize&gt; { if arguments.len() &lt;= 1 { usage_err_exit(); } arguments.iter().map(|x| { match FromStr::from_str(x) { Ok(col_arg) =&gt; col_arg, Err(_) =&gt; usage_err_exit(), } }).collect::&lt;Vec&lt;_&gt;&gt;() }
In C++ there are a whole variety of such styles in use, and in my experience they are just a preference and there is no one generally prescribed way, but within one project it's recommended to have one style for simplicity. I guess Rust can have a similar approach.
It's not preference. http://static.rust-lang.org/doc/master/style/style/whitespace.html
I'm using `OsStr` because it's a type that implements `AsRef&lt;Path&gt;`. It's what my core functions expect, and it's the most efficiant way to handle all relavant types (it's usually a zero-cost). I think `CStr` is more of a transitional type that's only useful for converting C strings into something Rust understands; namely, a `[u8]`. That said, I would gladly use it if it implemented `AsRef` for `Path`.
I didn't mean this as a matter of my personal preference, but as a preference of the community. I've found the Rust community to be much less prescriptive about style compared to say the Go community where everything runs through gofmt and you can't even change the indentation style through options anymore. I definitely agree with having a stated style guide and personally follow the guide to the best of my knowledge.
When I've read about the arrayfire a few weeks ago, then I thought that this was the next logical project to do. I've postponed all my experimental side projects when a need for code to be run on the GPU requires learning another 2 languages(OpenCL/C). With this project, I can continue to develop it using rust alone.
I've been having some trouble answering this question as well. I have a very strong preference on indentation which does not match Rust's style guide. For now, all my personal projects are indented via my personal style, but if I were to try to recruit a community around a project of mine, I would re-think the indentation style. I've adopted all other aspects of the Rust style guide. Further, I'm somewhat waiting for good editor support. I use vim to edit code, and I am not aware of any good support for space-based indentation in vim. I realize there are related settings, but they just don't work well. I have some ideas on how I would solve this problem myself, but it's far down my to-do list.
&gt;there is no nice way to remove elements from an iterator as you go through it let mut index = 0; let mut length = vec.len(); while index &lt; length { //iterate here. use unchecked_get() for performance, YOLO //if you don't remove element, increment index //if you remove element, decrement length instead } Granted, it's not as nice as an iterator. :-)
`Option&lt;Error&gt;` doesn't get linter love to make sure that users check for the errors.
I have had a go at it as well, [here is the result](https://github.com/Byron/nth/blob/5f586bf01c987ddf643bd861f9796456f855555d/src/nth.rs).
I think Rust can't be described to have a "small" foundation. Among other things, it has one of the most complex type system.
Always give a transcript unless you hate deaf people. Assuming you had already written down your plan for what to present (which is a good idea for unrelated reasons), you may even get a transcript for free!
Rust, the Language for Fungus Growth.
&gt; A lazy IO type would allow each thread to bundle it's IO actions into a single object that it can then execute all at once when it gets IO access, rather than having to have IO access the whole time it's executing functions that might perform IO. This would not be possible without referential purity and universal laziness. These are features of Haskell's IO type which are unrelated to its being a monadic type. But monadic IO has other attributes. It influences how code is written: IO errors are easier to ignore and then handle at a single point, rather than throwing try!s and so on.
Sorry, I somehow assumed that you would use it for more than just file paths. I did some more digging and, although the documentation was a little vague, I realize that using `OsString` in this case seem to actually give the best result. UTF-8 is then only required on Windows, so it's the most tolerant format.
I see the current interest in new language features come down to low cost abstractions around memory and threading. I think the next frontier is unlikely to be more language features, but will instead be interactivity. I see languages like rust as being highly evolved, which makes me feel that there won't be a tremendous amount to gain in productivity when it matures. If rust has fast compile times and easy parsing of the language built in, then I could see more and more tools built around it, which is really where C++ shines.
So here's a question: What about *business* growth, with respect to growing a program's responsibilities? I'm by no means a *rustacean*, but I'm following things here in an academic way to get a better idea of where Rust is positioned with respect to other languages. The impression I get is that Rust is awesome at handling a host of low level concepts while maintaining the utmost in memory and type safety. So in many ways, I'm left with the feeling that it is like a much more sane C++. So what of higher-level concerns, like adding a whole new suite of capabilities to a web app, or taking on new dimensions in data for an accounting desktop suite? Does Rust help or hinder this kind of non-CPU bound growth with ease, as one can with Go or Python? Can the programmer easily put together easily understood high-level constructs that make this task both safe and simple, or is it slow going like attempting the same in C++? TL;DR: is Rust a good fit for business apps, and would such solutions be too low-level to be maintained with minimal effort?
That's only because `rustfmt` is still being worked on, I'll imagine once we have it we'll generally be the same way. It's one of the most often requested features.
Thanks for posting this. I will like to see a more detailed article from this if you have the time. For example, it's not clear from above how these two are different: fn do(&amp;self, &amp;other) -&gt; &amp;thing And: fn do(&amp;self, other: &amp;u32) -&gt; &amp;u32 
We haven't fully promoted `/style` to a real thing because it's not totally complete yet. It does render properly, as /u/eddyb points out below.
This changes semantics ever so slightly: https://play.rust-lang.org/?gist=14e49a59961d07f6e402&amp;version=stable
But how does this help someone? Like, in what situations do you *have* to use explicit lifetimes, and how would you use them to solve that problem.
Well, you can't really remove traits from the language without coming up with an alternative to Copy, Drop, Send, Sized etc. And the lifetime system is basically a consequence of supporting safe references. Without it the reference system would be severely lacking. So, I don't see how you can remove either feature. You can replace them with something else, but if that is simpler or not depends.
Rust does have excellent support for various forms of polymorphism, and is going to gain even more. When the processing of a type of data can be bound to the data itself, a trait can be used. Traits are probably most closely analogous to Java interfaces, except that dispatch can happen either at compile-time or at runtime. If you want to extend a system without recompiling it, use dynamic. When the processing of a piece of data is not so tightly bound to the data type, an enum can be deployed; when a new variant is added to an enum, the compiler will statically guarantee that all variants are handled, alleviating the long, tedious hunt that's associated with adding a variant to a C enum. While traits are most suitable for interfacing between subsystems, enums are more suitable for adding features to a single subsystem, such as adding a new state to a state machine. Based on requirements of Servo's DOM, support for implementation subclassing is going to be added to the language, though it's exact form has not yet been decided. TL;DR: Rust's type system is a lot like Haskell's, which is lauded as making extension very easy to get right, specifically less error-prone than Python or Go.
I personally would prefer zero configuration options. "internal consistency amongst languages" doesn't play to each langagues' strengths, and leads to weirdness when communicating with projects outside of the organization. A big part of wating `foofmt` is that you don't spend time arguing over formatting. Configuration flags just introduce another thing to argue over. &gt; why even allow code to compile if it doesn't match the output of theoretical rustfmt exactly? One tool per job. This also lets other formatters be developed too, like your theoretical in-house one.
The language originally had just those things: simpler (non-trait, built-in-to-language) rules around copy/drop/send-ability, and simpler (second-class, parameter qualifier) safe references. It was made more complex in these two departments over time, absorbing the features while making them more powerful and complex. Some areas of complexity were reduced (the runtime model; typestates and effects) but I think there's been a modest increase in net complexity since the beginning. But it's also much more powerful, flexible, general and well-founded on solid theory. The initial versions were more muddled. Overall, Rust has never aimed to be a "minimal" language, but a "medium sized" one. We hoped to come in well under the complexity budget of, say, PL/I or Ada or C++; closer to that of, say, Sather, Alef, Java or Ocaml. I think this goal was met, despite the cognitive strain of explicit lifetimes. But Rust is not minimal in any sense. A complete spec will probably be many hundreds of pages. Rust is decidedly not a Forth, Scheme, Self or Oberon. The word "practical" was often used during design: we didn't want to force things to be "built from minimal primitives" if it was impractical (inefficient, incomprehensible, fragile) to do so.
Maybe a better way of describing it is that Rust's type system, while relatively complex, maps very simply and directly to the things you want to describe with it. This is in contrast to C++, for example, where the type system is often used to describe the same things in a very roundabout way (e.g. move semantics).
If you operate a data pipeline where you ingest some data from the outside world then try to act on it, it turns out you care a lot about whether the data is "bad" since it means you need to spend time cleaning it. Maybe Rust isn't the language I would write that in, in the first place, but I wouldn't give up my production stack traces for such domains.
I think you're overstating the benefit of the delimiter chosen for indentation in regard to a languages strengths and/or communication with projects but we'll have to agree to disagree here. I also think tools like gofmt have just changed time spent arguing over formatting with time spent complaining about it. Maybe that's a win for someone but I haven't found it to be a compelling reason. Having that tool is valuable but I don't believe developer productivity went up due to less arguments about formatting. Either way, I'm still of the opinion that in the absence of `foofmt` or compiler requirements that the style guide is strictly preference.
You silly goose. You know not to do that with Ruby ;)
`(())` freaks me out
I've found the [uutils coreutils](https://github.com/uutils/coreutils) project a helpful point of reference when creating small shell utilities in Rust. Its version of [cut](https://github.com/uutils/coreutils/tree/master/src/cut) for example might be able to provide some inspiration in this case. And since I like solving the occasional shell puzzle, here's a BASH-flavored version of `nth`: nth() { [[ $# &gt; 0 ]] &amp;&amp; awk "{print $(printf '$%s,' $@ | sed 's/,$//')}"; } $ nth 2 4 &lt;&lt;&lt;"a b c d e" b d
No need to be sorry. If anything, I should be thanking you for trying to help improve the library.
cargo uses `docopt`, it might be worth checking it out too.
great post (just glanced through .. maybe tomorrow i have more time ) but i have found one little typo: let text; //my_txt ? { let mut editor = TextEditor::new(); editor.add_char('a'); editor.add_char('b'); editor.add_char('c'); my_txt = editor.get_text(); } //Variable editor gets destroyed. println!("{}", my_txt); //Use after free. Not possible. 
I'd recommend to either use an algebraic data type for the structure of your individual options or to use a syntax extension to parse the strings at compile-time. It doesn't seem ideal to defer potential errors like "this is a nonsense string and I can't get an option out of it" until runtime when the vast majority of programs have a set of options that is known at compile-time.
Thanks! Was my first attempt and there were a few issues here and there, but glad to hear positive feedback.
Both `getopts` and `pirate` are fairly equivalent in what they provide for the user, only differing in methodology. Whereas `getopts` has you mutate the state of the `getopts::Options` struct using methods, I personally prefer defining your options using a specific grammar then parsing it into a data structure. To each their own! Recently I stumbled upon the `clap` library. And while it's incredibly powerful, I also prefer the relative simplicity of `pirate`. While `clap` may have a lot more features, it seems to be a bit more complicated to use. However, I don't have any experience using it with a project, so what do I know?? However, I do like the idea of sub-commands, which I may see about implementing...
If you like the available C++ tooling, you'd *love* what the Java community currently has to offer.
Fixed now. Thank you.
I'm not sure what you're asking exactly. But maybe this helps: One example would be a standalone function taking multiple references and returning a reference, e.g.: fn find(source: &amp;Container, key: &amp;Key) -&gt; &amp;Value { ... } Here, Rust wouldn't know how the lifetimes are put together. It doesn't know that the lifetime of the referenced value you return should be tied to the lifetime of the container you receive, and that the lifetime of the key you receive is only relevant for the duration of the function call. So you'd have to tell Rust about the lifetime relationships of the references in the signature: fn find&lt;'src&gt;(source: &amp;'src Container, key: &amp;Key) -&gt; &amp;'src Value { ... } I picked this case because this is a pattern you might actually have worked around. If you look at the first pattern by /u/minno above, methods default to assuming returned reference lifetimes are related to `&amp;self`. So if you put the function above in a trait and implement that for the `Container`, or have a wrapper type encapsulating the 'Container' and implement the method on that, everything works without specified lifetimes. Another issue for explicit lifetimes would be where the elision rules do apply, but they do not reflect the intention. A simple example might be a `RandomSelection` with a method like: impl RandomSelection { fn select(&amp;self, list: &amp;[Item]) -&gt; &amp;Item { ... } } Here, Rust will assume the lifetime of `&amp;Item` relates to the lifetime of `&amp;self`, but we actually intend to return a reference into the slice that was passed in as an argument. So when we try to compile that, Rust will complain about the mismatch between the signature and the function body. So we'll have to correct Rust's assumptions here: impl RandomSelection { fn select&lt;'src&gt;(&amp;self, list: &amp;'src [Item]) -&gt; &amp;'src Item { ... } } Are those the kinds of scenarios you were after?
Lifetimes are. Not. Like. Cake. Lifetimes are like onions! End of story. Bye bye. See ya later.
You might want to take a look at [this](https://www.gulshansingh.com/posts/how-to-write-a-rust-syntax-extension/). However, I'm not sure if a syntax extension is really needed. Ordinary rust macros are already pretty powerful. And given that syntax extensions are still unstable (and will continue to be for some time AFAIK), it would be pretty useful to have an ordinary macro, which compiles on stable...
Yes, exactly what I had been looking for, thanks. Is there a way that rust could, safely, infer these things in the future? Or is this simply a limitation.
Maybe so although that isn't an option for me. I don't think though that the tools for C++ are great necessarily, just that to me they were the minimum for real work. Basically syntax checking, auto completion, step through debugging, and profiling. What I actually think would be much better would be the ability to freeze the state of a program at any point, have a form of ouput, and change the source for the portion in between live. Ideally while being able to see the values of variables and expressions at the same time. 
When someone _complains_ about formatting he wastes his own time. When someone _argues_ about formatting he wastes both his time and others'. This above all else is what I like about `gofmt`: it protects me from getting dragged down into quibbles about things I don't care about. Meanwhile, people who want to rant and rave about whitespace may continue doing so—safely on their own time.
Is using compiler plugins a good general approach? Or it's a result of language being young and lacking some features? Also, are these plugins compiler specific? I.e. let's say another compiler will implement Rust front-end (such as gcc), will it mean that current Rust compiler plugins will be useless with it?
Thanks. I have a follow on question that may be I can get some help with. pub struct IndexItem&lt;'a&gt; { pub file_list: Vec&lt;&amp;'a str&gt; } impl &lt;'a&gt; IndexItem&lt;'a&gt; { //... } Why is ``&lt;'a&gt;`` repeated twice in ``impl`` declaration? Why do we even need to have lifetime in ``impl`` let alone twice? I mean, can you have different implementation for the same type but for different lifetime parameters?
What I found hardest to get about lifetimes was this: at first, I thought annotating lifetimes was a way of extending the lifetime of references beyond their original scope. Obviously this doesn't actually make sense when you understand that lifetimes are (essentially) associated with particular stack frames, but just starting out I was confused when my lifetime annotations didn't just *declare* that my data would live long enough, come hell or high water.
Can someone explain to me what a monad is in simple terms?
If you're talking about the "compiler backend" (means: machine code or LLVM IR generation &amp; optimizing), yes. As far as I can tell, compiler plugins are completely executed during the front end phase of the compiler. 
Yup, this is a very common misconception.
Why not use [BufReader](https://doc.rust-lang.org/nightly/std/io/struct.BufReader.html)? I apologize, I haven't tried compiling this but in theory it should work fn handle_input(column_numbers: &amp;[usize]) -&gt; Result&lt;_,_&gt; { for line in BufReader::new(stdin()).lines() { handle_line(try!(line), &amp;column_numbers); } } 
Hey thanks for explaining, I can see how they'd be useful in error handling situations.
They're also used in languages like Haskell to manage all kinds of stateful operations, not only errors (in Rust, the most commonly used monadic types are `Option` and `Result`, so it'd be fair to say Rust mainly uses monads to handle errors).
Well, I'm a bit out of my depth here. There might be additional lifetime elision rules that will prove valuable with more widespread use, but I personally think the obvious ones are covered by the simple rules we have. Although one thing that might improve is error messages. While Rust won't use the function body to infer information about the signature, it can always become smarter about telling you what you're probably looking for.
After the first 25 days, you're done dealing with windows ugliness.
Not sure about maps, but don't destroy his [trees](https://www.youtube.com/watch?v=guEea3Nk_RA&amp;t=206). (I don't play the game, just a random video I just found..)
[Here's a quick example I made.](http://is.gd/86kJWz)
Sorry, missed that. https://github.com/rust-lang/rfcs/issues/843 (codahale-metrics-esque library) seems like a great starter fit. Network communication w/ graphite, working with (somewhat) shared state, etc.
I was also surprised by this, I thought automatic implementation of Copy had been removed.
 I think the issue could be when you have something like this fn some_fn(first: &amp;SomeStruct, second: &amp;SomeStruct) -&gt; &amp;SomeStruct { if (someconditional) { first } else { second } } Now you have a scenario where you can have two different return types `&amp;'lifetime_of_first SomeStruct` vs `&amp;'lifetime_of_second SomeStruct` so you cannot know the exact return type at compile time.
Didn't the OIBIT RFC specifically change this? &gt; A struct or enum type is considered to implement the Copy trait only if it implements the Copy trait. This means that structs and enums would move by default unless their type is explicitly declared to be Copy. https://github.com/rust-lang/rfcs/blob/master/text/0019-opt-in-builtin-traits.md
Right, so I guess you've found an edge case here. Interesting. I mean, there's nothing inherently unsafe about this code, so I don't think it's a _problem_, exactly, but it is strange in some sense. The problem is in _reading_ memory after it's been moved, assigning to it isn't really a problem, as it's unreachable anyway.
So this is my understanding, somebody else might be better at explaining this than me. For the sake of lifetime elision, just think of the lifetime as any other type signature. In the context of a function that takes a single reference argument (it can still take other args by value), there is only one lifetime to reason about (the lifetime of that single reference). So there are now only two ways that function can return a reference: * Creating a new reference variable, but any reference variable created in the function will only have a lifetime till the end of that function and so cannot be returned since it won't life long enough. * Returning one of the parameters (or a reference that got its lifetime from one of the parameters). So if the function returns a reference, and only has one reference argument, its safe to assume that it has the same lifetime as the argument. Once there are two or more reference arguments for a function and the function returns a reference as well, you need to specify which one of the function arguments the return value will get its lifetime from. Otherwise you run into situations where its impossible to know what the lifetime of the returned value is at compile time. Here's an example of not being able to determine the lifetime of the return value at compile time: fn some_fn(first: &amp;SomeStruct, second: &amp;SomeStruct) -&gt; &amp;SomeStruct { if (someconditional) { first } else { second } } or with the type signatures for the arguments but not the return type fn some_fn&lt;'a, 'b&gt;(first: &amp;'a SomeStruct, second: &amp;'b SomeStruct) -&gt; &amp;SomeStruct { if (someconditional) { first } else { second } } So is the return type `&amp;'a SomeStruct` or `&amp;'b SomeStruct`? We cannot know at compile time, so we are forced to specify it as part of the function's definition. However, this following snippet will still error fn some_fn&lt;'a, 'b&gt;(first: &amp;'a SomeStruct, second: &amp;'b SomeStruct) -&gt; &amp;'a SomeStruct { if (someconditional) { first } else { second } } since not all the return path's return the correct type (second is now &amp;'a SomeStruct). This is the same error as trying to return a float from a function that specifies an int return type. So now finally, we change our function to this and it should compile fn some_fn&lt;'a&gt;(first: &amp;'a SomeStruct, second: &amp;'a SomeStruct) -&gt; &amp;'a SomeStruct { if (someconditional) { first } else { second } } The final lifetime elision rule is just for convenience. Basically, if a function takes a &amp;self reference, it will just use the lifetime as self for the lifetime of the return value since thats probably what you want more often than not.
After you do`let a_ = a;`, `a` is left basically as an uninitialized variable. This: let mut a = A { val: 1 }; let a_ = a; is really equivalent to this: let mut a: A; let a_ = A { val: 1 }; It *is* surprising that rust allows setting values, but whether or not it is intended it is valid to do this: let mut a: A; a.val = 2; although trying to access this value will result in an error: struct A { val: i32 } fn main() { let mut a: A; a.val = 2; println!("a: {}", a.val); } // &lt;anon&gt;:10:23: 10:28 error: use of possibly uninitialized variable: `a.val` // &lt;anon&gt;:10 println!("a: {}", a.val); // ^~~~~
It's valid to set members of an uninitialized variable as well, which is strange: let mut a: A; a.val = 1;
Well, you can have the lifetime of the returned reference be a subset of both input lifetimes: fn some_fn&lt;'ret, 'a, 'b&gt;( someconditional: bool, first: &amp;'a i32, second: &amp;'b i32 ) -&gt; &amp;'ret i32 where 'a: 'ret, 'b: 'ret { if someconditional { first } else { second } } fn main() { let a = 23; let b = 42; some_fn(true, &amp;a, &amp;b); } But that is actually one point where I believe Rust shouldn't try to do elision, because code like this would compile even in wrong cases, and could sometimes even pass tests. So always requiring people to be explicit about this seems the safest route to me. Note: I've seen you used a single lifetime for both in an example for both, which of course also works. No idea how I forgot about that. I'll blame it on the late hour :)
Do you have any interest to machine learning? Write bindings to libsvm and liblinear!
Yes. They both have layers. Edit: Thinking more about it, that actually makes some sense. If there is popular interest, I may whip up a blog post explaining lifetimes in terms of ~~ogres~~ onions.
Once racer has type suggestions, this will be less of an issue. But yes, currently writing fn signatures has to be my least favorite part of rust coding.
I love analyzing the performance of these little programs. I run them, then I run the standard equivalent (e.g., `awk`), and wonder: why is `awk` 5 times faster? I ended up getting your program to run faster than the equivalent `awk` on my system. The most dramatic performance problem is unfortunately a result of how `io::Stdout` is currently implemented. In particular, it uses a `LineWriter`, which flushes every time a `\n` is written to the buffer. This ends up executing a syscall for every line written. Owch! A temporary work-around for now is wrap stdout in a buffered writer: `io::BufWriter::new(io::stdout())`. On my system, this brings it close to `awk` performance, but still worse. The second trick is to reduce allocation. In particular, this line: let column_vals: Vec&lt;&amp;str&gt; = read_buf.split_whitespace().collect(); This is going to allocate a new `Vec` for every line. We can replace it with a simple state machine that captures the start/end byte offsets of each column without allocating a fresh vec on each line. Full code is here: https://gist.github.com/anonymous/fd7ebce6874ed786d58c N.B. I skipped an obvious intermediate step that removes the vec allocation without using a state machine. That step is to maintain a `Vec&lt;&amp;str&gt;` as scratch space, but the `&amp;str` inside the vec is tied to the lifetime of `read_buf`, which means you'll have to fight the borrow checker. Regardless, my way was more fun. :-) I can think of a couple other ways to go faster: 1. We're paying for Unicode with the `is_whitespace` call, which uses binary search on the `White_Space` Unicode property. If I, say, change this to `c == ' ' || c == '\t'`, then I get a small boost. 2. One I didn't try: borrow an idea from `grep` and stop reading one-line-at-a-time. Instead, read a whole bundle of bytes and run a slightly expanded state machine over that. This relatively complicates things, because now you need to be able to handle partial lines. 3. It looks like `read_line` could also be sped up by using something like `memchr` instead of `position_elem` (which I think is just a byte-by-byte loop).
Functions are the worst offenders. Especially due to bare functions having different syntax than closures (but just slightly, so it's easy to get confused), every closure being of it's own type, and necessity to explicit cast to pointer type to drop the body content from the function type signature. 
[Not entirely](https://play.rust-lang.org/?gist=93ecb1dba137a1665def&amp;version=stable). See what happens if you comment out the declaration or either of the two assignments.
Thanks, thats good to know. I'd still wrap it in a `BufReader` though for the massive ergonomic benefit. Why does it not implement `BufRead`? 
Yep that's what I ended up doing.
Obv. commenting out the decl. breaks everything down. But commenting out one of the two assignments *also* breaking it down makes it very strange that your example (answering the OP) errors the same way. Rust is going through the flow of the program and sees the variable initialized before use in all paths of the `if`; so why is your earlier code not working (it's got one path!)?!
I misunderstood it in a different way, heh.
You mean `as`? It's not. https://github.com/rust-lang/rfcs/blob/master/text/0401-coercions.md
`use` means reading, not writing, basically.
Thanks for the tip on buffered i/o, got similar gains on adding it.
Here's a bunch of tools that could help generate maps :) https://github.com/georust
You could do what I did to learn Rust: write [a command line tool](https://github.com/coledot/nth).
You could write bindings to some of the stb stand alone C files. Those get used in all sorts of projects. Beind able to easily load and save a png file is pretty useful. Anything that is missing from the C++ STL including algorithms and iterators can be useful. Functions to do simple SIMD operations across a vector of memory would be useful too. 
This is great! Thank you! Comming from the machine learning world having a good array library is a must. I have been wishing for something like this to come to rust for years.
Ah, that makes sense. Another option, if you need to write more of these, is to implement a method or maybe a From that casts everything but the option that has a parameter and panic on the one that does have an option. You can then default to that method/From so that at least don't have code duplication. I get that that isn't what you were going for but I guess it's the best you can do.
I'll add that a monad not only wraps a value, but gives it a context. So in the case of `Option&lt;T&gt;`, that context is that the value might be there and might not. This context is used by the monad's `bind` operation to decide what to do with the wrapped value and with the function passed to it, and what to return as a result. For `Option&lt;T&gt;`, this means calling the function with the unwrapped value if there is one and returning the result of this directly, or else returning None if there isn't a value. Another example is an `Iterator`, whose `bind` operation is called `flat_map` in Rust. The context is that there might be any number of `Iterator::Item` values. This context is applied by calling the function supplied to `flat_map` once for each item, potentially producing even more items, since each item in the original iterator gets mapped to another iterator. So now you have a bunch of iterators, one for each item in the original iterator, and you chain them together. So ["1 2", "3 4"] might be mapped to [[1, 2], [3, 4]], and then flattened to [1, 2, 3, 4], which is why the operation is called `flat_map`. You can try it out if my explanation isn't clear. Another use of the context provided by a monad is to introduce a side-channel for data. So, let's say you wanted to compose a bunch of functions. Normally, you can trivially compose a `Fn(A) -&gt; B` with a `Fn(B) -&gt; C` to get a `Fn(A) -&gt; C`. But, if all the functions in the chain need to share a value that's to be determined in the future, how do you compose, say, a `Fn(A, R) -&gt; B` with a `Fn(B, R) -&gt; C`? Well, if you curry those, it looks like `Fn(A) -&gt; (Fn(R) -&gt; B)`, which looks an awful lot like the functions you pass to `bind`, since what you really care about are the `A` and the `B`, but the return value wraps the `B` in a more complex context. That means that your monad is `Fn(R) -&gt; A`, with the value being an `A` and the context being that you need an `R` in order to retrieve it. Then `bind` has to compose those curried functions with your monad in such a way that it acts like normal composition and eventually gives you a `Fn(R) -&gt; C`, where the `R` value you pass to it gets fed to each function along the way. This seems a little abstract, but it might give you an intuition about how side channels can be used to create more complex behaviour, such as wrapping system input and output in a way where you don't actually have to do it until after you've composed all the actions together.
&gt; impl From&lt;String&gt; for Box&lt;Error + Send + Sync&gt; That's a coherence violation, so it's not possible to implement.
&gt; so it's easy to get confused I certainly am. Conceptually a closure is a bare fn + environment, so the concepts *are* somewhat similar, which would explain the similar syntax. But I have yet to find a place where I needed to cast to pointer (perhaps this is because I usually use closures with functions requiring a Fn or FnOnce, so the casting is implicit). Do you have a code example where this hits?
Yeah, I should browse the rustbyexample, it seems to provide some different explanations for things from docs and it helps to wrap my head around them. I think that the section about conversions would be valuable even if it's short. At least it could have a description of "as" operator and where it could be used. Also, more examples on differences between String and &amp;str or array and slice, maybe. About the third paragraph. Wouldn't it fail at compile time so I will not be able to walk the binary in the debugger? What IDEs are good to work with Rust? Currently, I work in vim (from remote access) or in atom.io (that has plugin for syntax highlighting).
There is a drain() API on Vec which will remove things from the Vec as it is iterated over, but it is not yet stable.
Or heck, just port the stb libraries to Rust. :)
This is done easily with match bindings. fn trans(a: MyEnum&lt;u32&gt;) -&gt; MyEnum&lt;String&gt; { match a { MyEnum::D(i) =&gt; MyEnum::D(i.to_string()), case @ _ =&gt; { // case == MyEnum&lt;i32&gt; // do stuff with case }, } } 
&gt; // do stuff with case The question is: how to turn this `MyEnum&lt;i32&gt;` into a `MyEnum&lt;String&gt;`, to match the return type of the function?
&gt; Functions to do simple SIMD operations across a vector of memory would be useful too. Most your math will do this automatically. The LLVM prefers SIMD instructions over standard. If every you make a group of equations like let a :f64 = constant + 50; let b :f64 = constant + 30; You'll use SMID. The LLVM is actually very SMID heavy and sometimes in compiler ~~pissing contests~~ technical arguments this is brought up for/against it depending on the requirements of the project. 
&gt; if you have a vector of Ts, iter should return an iterator of Ts. Actually `Vec::&lt;T&gt;::iter()` gives you an iterator of `&amp;T`. Shameless attention grabbing: Maybe if we could only [be allowed to document this.](https://github.com/rust-lang/rust/issues/25928)
~~I see. Doesn't that mean that the docs are factually wrong?~~ It sure is easy to overlook.
Maybe using the log crate and using `warn!` is what you want? 
For simple copyable types or similar, use `.iter().cloned()`. This gives you an iterator of `T` for Ts that are cheap to clone. For other kinds of types, use `Vec::into_iter()` to consume the vec and receive its elements by value. This consumes the vector, since you move out all elements.
&gt; So the first issue I have is with the closure. According to the docs, the closure should be FnMut(&amp;I) -&gt; bool, meaning the pattern match should be enough to dereference x. However, it only works if I dereference x once more. Why is x: &amp;&amp;i32? The iterator used here yields `&amp;T`, and filter takes its input by reference so that it can work with iterators that yield other things than `&amp;T`. &gt; Why did it become a vector of pointers to ints, when it started as a plain vector to ints? Again because the iterator yields `&amp;T`. The subsequent filtering only affects which elements will be yielded. If you want plain `T`s then use `into_iter()` or `iter().cloned()` instead. `into_iter()` will consume the vector though. 
This may be helpful: https://github.com/bibhas2/stride
&gt; return Result&lt;ParseResult&gt; where ParseResult is struct ParseResult(RootNode, Option&lt;Warnings&gt;) Why force warnings to only occur on success? It seems like `(Result&lt;...&gt;, Option&lt;Warnings&gt;)` would work better. If you want something lighter, how about `Warns&lt;Result&lt;...&gt;&gt;` where `Warns` is `Deref`. That should keep things syntactically lightweight when you want to ignore them.
That'd probably work for a console application, but if you'd embed the library in some web browser for example, you'd want to show the warnings in a different place.
There aren't any good IDE's that I know of yet. It's just something to want in the future. [Visual Studio Code](https://code.visualstudio.com/) just got Rust support but it's very preliminary. All they have is syntax highlighting basically. You might be able to get a debugger up and working with Eclipse. I'm not sure. Nothing super great and easy to setup yet. A graphical debugger with the ability to inspect variables would probably help a lot. It would be really great.
It will link to the playpen, but the playpen can't use external crates, so it's kinda moot.
I guess it's not then!
To fix the overflow panics in the generate_random_value function, try looking into the wrapping types. (http://doc.rust-lang.org/std/num/struct.Wrapping.html)
Looks great. I'm trying to build some ML to reorient networks. They're not quite the same, but it may interest you? https://github.com/insanitybit/Cricket The current code is for demonstration, and I've only outlined the ML part, have not gone too deep into it. Maybe you'd be interested in this sort of thing? Either way, thanks a lot, this is going to be a fun read.
In general, C types are in the libc crate: https://crates.io/crates/libc That said, there's an art to getting the C types to and from Rust types, which often requires knowledge about both's representations.
 macro_rules! partial_enum { ($name: ident, $some: ident, $($none: ident),+) =&gt; { #[derive(Debug)] enum $name&lt;T&gt; { $some(T), $($none),+ } impl&lt;T&gt; $name&lt;T&gt; { fn convert&lt;U&gt;(self) -&gt; Result&lt;$name&lt;U&gt;, T&gt; { match self { $name::$some(x) =&gt; Err(x), $($name::$none =&gt; Ok($name::$none)),+ } } } } } partial_enum!(MyEnum, D, B, C); fn trans(a: MyEnum&lt;u32&gt;) -&gt; MyEnum&lt;String&gt; { let a_split: Result&lt;MyEnum&lt;String&gt;, u32&gt; = a.convert(); match a_split { Ok(is_none) =&gt; is_none, Err(not_none) =&gt; MyEnum::D(not_none.to_string()), } } fn main() { println!("{:?}", trans(MyEnum::D(13))); } 
See also https://github.com/bjz/noise-rs for robust, well-optimized implementations of various coherent noise.
anything that involves machine learning is pretty interesting, so that's a pretty cool project, I introduced the NEAT algorithm because its what intrigued me the most and seeing it in Rust would be pretty sweet.
My experience is also that compilers will vectorize trivial loops well, but there seem to me to be caveats which make it difficult for me to be certain that it is happening without investigating, which is an unfortunate use of time I think. The major part though could be a library that is initialized once and chooses the widest SIMD lanes possible. It could also be made to use SIMD and OpenMP style parallel threads at the same time without too much trouble. Then there are other functions like pow that would be useful as well. Mostly I think it would be great to wrap up some of the functionality of ISPC in a very basic way, since ISPC can be ridiculously fast but can also be a pain to use. At the same time, most of the gains I have gotten from it have been from the most basic usage since getting the most out of SIMD usually means trivial operations over linear memory (at least until we see skylake's gather scatter performance). 
Pointers are mutable by default in c so those should be `*mut ...`.
I'm using the project to teach myself rust, ML, among other things. If you're interested in contributing I'd be happy to help in any way I can. There's tons of work to be done :P A lot of NEAT looks like what I'm doing, except with weighted connections, not that that can't be implemented.
On a raspberry pi 2, Raspbian upgraded from wheezy to jessie, using your snapshot binaries, I built and install both rust and cargo designating the armv7l as the host and target. Rust took about 16 hours all total and cargo about an hour. So far, I've only run 3 simple Rust programs, hello, calculator and dining philosophers. I'll be trying complex code in the near future.
Because I got overflow panic when I ported it to Rust. /u/adudney has linked the wrapping types which I missed when browsing the docs myself though, which will almost certainly fix this.
Aha, thanks! I knew there had to be something but I completely missed it when looking myself.
Yeah, I've seen this, and it's definitely the better choice for serious uses of noise. I implemented my own noise more for educational reasons. I'll probably end up making a noise provider that uses this, too.
Uh, that's terrifying!
The windows XP changes seem like a pretty big deal! How far is XP from being an officially supported platform now?
XP is not slated to ever become a first-class platform, but more of a 'we won't automatically crash if you even try' platform. It's missing a number of important operating system primitives that can be shimmed, but with significantly worse performance.
Clearly the above is next week's QOTW.
Loving the "Final Comment Period" section, it makes it really easy to keep up with the most important RFCs.
The success of crates.io has marginalized all the other package registers, not surprising.
If this happens, I would probably be interested in doing voice-overs for it, if that was needed. I do semi-pro voice acting work for fun (including a bunch of internal videos for my employer), have a good home studio, and of course know Rust better than most professional voice talent.
If anyone is looking for an automated way to build documentation: https://github.com/huonw/travis-cargo
I've talked a bit about this [before](https://www.reddit.com/r/rust/comments/3a3ftu/this_week_in_rust_83/cs8xz7t), and I think it'd be best to start from scratch. Personally, I'd like to have something *less* strongly typed in the interested of flexibility and ergonomics. Even if it just ended up being a fancy string builder, if it's convenient, I'd use it.
I was wondering about that. Thanks.
Got an unrelated question: When I have questions as a newbie, like above, is it better (for me, for the language) to ask here/on-Internals/on-StackOverflow?
Just talked to jroesch on IRC; defaults-affect-inference may land as soon as this week!
Oh, don't get me wrong, *reading* them is *so good* but *writing* them sometimes makes me want to strangle a compiler.
Yeah, but attribution is really hard, since just about everyone is asking this.
&gt; the best system programming language Someone posted a students paper which described Rust as not suitable for developing a OS kernel due to how allocation works. It actually seemed like a huge pain to write a kernel in Rust, full of unsatisfying workarounds that should not be necessary. So I would not describe it as "the best system programming language" until these issues are resolved, which will take a while.
`Box::from()` can work in some cases too: [another reddit question](https://www.reddit.com/r/rust/comments/34qu0w/error_handling_via_from_into/)
The code is available at GitHub: https://github.com/faern/forkjoin. Benchmarking code is in the separate repository: https://github.com/faern/forkjoin-benchmarking.
Stack overflow or users.rust-lang.org are probably the best, though reddit really isn't that bad for this. I wouldn't use internals.rust-lang.org, as it's for more compiler-internals-related questions, users.rust-lang.org is better for this type of question.
Has the well run dry? Is no one making pithy comments about Rust anymore?
The "95 pull requests were merged in the last week." doesn't work unless logged in on GitHub.
I sometimes live stream game development using SFML in Rust. Izze234 on Twitch if you're interested!
Hype Hype Hype!! Not only can we move hashmaps to libcollections, but if we add allocator parameters, libcollections could then depend only on core (and libstd would default to liballoc's allocator).
I remember me and [SimonSapin talking about things like this in Kuchiki](https://users.rust-lang.org/t/kuchiki-a-vaporware-html-xml-tree-manipulation-library/435/28), and he said, other members of Servo team were very much [against the idea](https://twitter.com/nick_r_cameron/status/580106932297465856) of using `Deref` magic as polymorphism. Something because it only works on references.
I used cachegrind and callgrind to profile rust code. Very helpful, managed to significantly reduce waiting on mutexes and vector copying. Actually forced me to understand how rust accomplishes its threading, and that I really needed to think more about copies. Kind of made me realize how much I abused C++, I'm seeing more and more how some of what I had done had left the program in poor state, or wasn't as safe as I had thought. Haskell helped here a lot too, but Rust has added quite a lot to how I think about structuring a program.
BTreeMap is in libcollections and can be used on bare metal, in the meantime.
There are a few attempts at defining inheritance in Rust, but it will probably take some time to pick one and finalize it. For your specific problem, I'd recommend implementing an Entity-Component model. Look at http://gameprogrammingpatterns.com/type-object.html
Just a question, is this the case? Even for builds at maximum optimization level?
As far as I know, yes.
Thanks a bunch :D BTW how do you set up projects to automatically test with new nightly?
That's not actually right; you'd need to make `lt` and such panic too to fulfil the `Ord` contract. An easier way is just to panic on construction with `NaN` (`is_nan`).
I did https://www.codementor.io/officehours/3223519064/live-coding-rust-with-steve-klabnik a while back. I'm so bad at vim.
This gives me the amusing parenthetical note, courtesy of Google Translate: &gt; (wonderful Bowen: [Why do I say Rust is fly programming language](http://blog.csdn.net/liigo/article/details/45757123) )
I'm referring to http://hyper.rs/hyper/hyper/server/trait.Handler.html#method.check_continue I was seeing a similar delay when posting with curl, until I added that method. Perhaps the default is missing something NET expects? Should be fixed. 
I wonder why .NET uses `Expect: 100-continue` by default ... it *does* add another round-trip to each request ...
I like the name, but it is easily confused with [plex.tv](https://plex.tv/) 
There's also the [Very Sleepy](http://www.codersnotes.com/sleepy) profiler. It understands both PDB and DWARF debug info.
My favorite section in the post-1.0 world of rust. Helps me keep up on what's potentially coming down the road.
Very interesting read. Getting a 21x speed-up in a 32 core machine is impressive, congratulations!
I filed https://github.com/hyperium/hyper/issues/592 to look into this.
`0./0.` is actually NaN, and `f32::max` forwards the non-NaN arguments.
I don't know. I thought NaN might be a possible invalid answer, particularly because it goes so well with the max function, so I don't need to use options.
Cool, thanks.
That macro worked perfectly, thanks! Yay, no more C, the entire library is now 100% Rust!
I don't know the exact reason, but attributes cannot be specified on statements (not sure if that will ever change). That sais, I like the idea of having explicit vectorization (and a compile error when that fails).
It's all good. The HTTP RFCs are really, really long, and have lots of details.
I'm not saying `Expect: 100-continue` doesn't have valid use cases. All I'm saying is that if your requests are &lt;1KB (The majority AFAIK) it hurts both latency and throughput, which is why I am confused why this would ever be the default in any HTTP library.
Yeah, httpbis helped a lot too
I agree. I was implying it was a stupid decision, which it is, IMHO.
It's worth noting that perf has minimal overhead, like, less than 10% (at least in my experience), while callgrind often seems to have orders of magnitude more. Of course, the interfaces are different, so one might get more useful info out of callgrind, but perf is at least good enough to find the hottest functions.
https://github.com/rust-lang/rfcs/pull/1183 is related but not the same thing, exactly.
Thank you. I actually got it up to 26x at some point, but those results were not achievable at the end of the project when the data for the report was collected.
I appreciate where you're coming from, but I'll keep using `vec![x; N]`. `vec![]` is no worse for opinions about Rust than `println!`. For better or worse, macros are an integral part of the language, and it's idiomatic to use them to make common functionality easier.
By the way, the source for this presentation is on github, and its built with Rust: [whistler_rust_intro github link](https://github.com/pnkfelix/cyot/tree/master/src/tutorial/whistler_rust_intro) (Well, Rust plus pandoc plus reveal.js. Maybe I'll get rid of the pandoc dependency at some point, but for now it gets the job done.) The main goal I had when I drafted this was to: 1. Put all of the presentation content in markdown, and 2. Unit test as much of the code as I could manage, automatically. I accomplished those goals via some semi-clever use of cargo's build-script feature. All of the `.md` files are mapped to `.rs` files as a prepass for each build. So `cargo test` builds those files and runs their unit tests. (And `cargo bench` will run the embedded benchmark that I showed on the last part of the presentation.)
&gt; I appreciate where you're coming from, but I'll keep using `vec![x; N]`. Me too probably, since it's best way to do it currently. I've shown the collect version mostly to show that it's not the only way. &gt; `vec![]` is no worse for opinions about Rust than `println!` I disagree. `println!` has to be a macro to be able to check format string at runtime (we're not C++, our template system can't do that via a function. yet). But `vec![]` could be replaced by `Vec::new()` or iterators or something else. It's just violating "one way to do it" principle.
There's also an effort to build a rust client for the Google APIs. Would probably be worth checking out how the rust code is generated from the JSON schemas that they publish? https://github.com/Byron/google-apis-rs
There are several existing implementations already. - [ecs-rs](https://github.com/HeroesGrave/ecs-rs) (I wrote this) - [simplecs](https://github.com/kvark/simplecs) - [eccles](https://github.com/TomBebbington/ecs.rs) (possibly out of date) And possibly one or two that I haven't heard of yet.
`Vec::new()` can't replace `vec![]`'s use as a collection literal. Collection literals are common in many languages, and macros are just how Rust emulates them.
If you really want to do this and find the functions are too large to stick `#[inline]` everywhere, you can consider this hack: #[inline] fn large_function(mut self, other_arguments) -&gt; Self { (|| { ... // long code here })(); self } 
[`.as_bytes()`](https://doc.rust-lang.org/std/primitive.str.html#method.as_bytes)
It's a neat trick, but I suspect LLVM may see through it: it will see the closure is called once, and so may decide to inline it anyway (of course, it may not happen... it probably depends on the precise implementation details/pass ordering). We'd need some way to specify `inline(never)` on closures for it to be reliable.
Yeah, I was hoping LLVM would inline the tiny functions first. After doing a few small tests it seems you're right, though. I've done some more tests with an inner function and `#[inline(never)]`, and I'm still getting lots of `memcpy`s. Perhaps this is above LLVM's pay grade after all...
So I guess that's because the functions were inlined? That's already pretty cool :)
Ok, so when inlined, the functions are nearly free, but when not, I pay half of the price (only one copy instead of two). Thanks!
Instead of a closure, having a non-chainable function that takes a &amp;mut is definitely an option. Would having the inlined, chainable one call the non-inlined, inplace one solve this problem? Something like that: http://is.gd/mJN1xX
I had been using Rust CI, but that seems broken. So now I'm looking to put [this](https://github.com/neoascetic/travis-cron/blob/master/travis-cron) into play somewhere.
`vec![a, b, c]` can't be replaced, but `vec![elem; len]` [can be](http://is.gd/kB67tM)...
Quick questions: * Are you compiling with optimisations enabled? * Are you using safe (bounds-checked) access, or unsafe (non-bounds-checked) access?
I'm compiling with opt-level=3 and even targeting my specific cpu with cargo rustc --release -- -C target-cpu=broadwell. I think at that optimization level bounds checking is disabled with the normal access operator []
[Working on a multipart/form-data parser for Iron web servers](http://twitch.tv/hytosys). I'm trying to stream my web dev work often :)
&gt; I think at that optimization level bounds checking is disabled with the normal access operator [] You think wrong. Rust *never* disables bounds checks. *Ever*. There is no switch to turn them off, either, since disabling them would basically invalidate its memory safety guarantees. If you want unchecked access, you need to use something like [`slice::get_unchecked`](http://doc.rust-lang.org/std/primitive.slice.html#method.get_unchecked). I should also point out that iterators also typically optimise down to unchecked accesses, if you're doing sequential access.
Do you have any code? I do agree that the naive two-dimensional approach hurts performance, but you can use something like `Vec&lt;&amp;'a [u64]&gt;` to simulate exactly what the chessprogramming article is doing. I don't know about the access pattern of `bishopAttacks` and `rookAttacks`, but that would be much better than `Vec&lt;Vec&lt;u64&gt;&gt;`. The contrived translation would look like this: struct SMagic&lt;'a&gt; { ptr: &amp;'a [u64], // if you are using a static array as a backing store, this can be just `&amp;'static [u64]`. mask: u64, magic: u64, shift: usize, } impl&lt;'a&gt; SMagic&lt;'a&gt; { // shared impls for bishopAttacks and rookAttacks. // if mBishopTbl[X] and mRookTbl[X] bounds check dominates the performance, // you may want to factor this out. fn attacks(&amp;self, occ: u64) -&gt; u64 { // wrapping_mul required for correctness; overflow check disappears on --opt-level 3. let occ = (occ &amp; self.mask).wrapping_mul(self.magic) &gt;&gt; self.shift; self.ptr[occ] // bounds check here, you may want to make this unsafe } }
Maybe you mistook boundary checking for integer overflow checking. The later is indeed disabled in release builds.
`flying` should be `reliable` here, lol.
I don't know an exact number, but if the hot code fits into the instruction cache you should benefit a lot from branch prediction always predicting that the index is inside the bounds. And given a modern superscalar CPU that handles multiple instructions at the same time, this bounds checking should not cost much. That's the theory. For actual numbers you should probably measure whether it makes a noticable difference.
One thing that is directly hurting your performance is the vec of vec's. The problem is that you have one list of pointers, where every entry points to a new list. Every time you need to access an entry, you first needs to find the pointer to the second list and then walk that pointer to get the actual value. Far better would be to flatten the list and find the correct index through some arithmetic. Say e.g. that the list represents a chessboard. So you would have a first level vec with a size of 8, which points to 8 vecs each with 8 entries. In our example, you would do `vec[2][4]` to get the fifth column of the third row. This is one access to get the pointer to the row and a second access to get the value of the column. If you change this to a single vec, you would get `vec[2 * 8 + 4]`, saving you that one access.
Translation?
The slice iterator in `libcore` handles that by using the `arith_offset` intrinsic like this: ::intrinsics::arith_offset(ptr as *mut i8, $by) as *mut _ So it treats the pointer as an `i8` pointer which results in the desired offset of one byte. `arith_offset` uses a `GEP` instruction, which yields better code optimizations than the `ptrtoint`/`inttoptr` code that the casts produce. And the iterator just returns `Some(&amp;mut *(1 as *mut _))` as long as there still items to be returned. The address of zero-sized items is deemed to be insignificant. Unfortunately, as of now, the `arith_offset` intrinsic is not exposed as a stable function.
Can bound checks be auto-vectorized by llvm?
What about the name has you so attached to it? I'm mainly commenting because the name really doesn't help me make any connection to what it does. 
Go and Rust may not be direct competitors but their target audience surely has *some* overlap. Dealing in absolutes to say that they compete or not is less useful than identifying the domains where each language shines and learn from each others' strengths. Rust exists both at a lower and higher level than Go. The offered synchronization primitives afford fine-grained control, the language allows special allocation behavior like arenas. The whole power of the standard library can also be wielded by third-party library authors. On the other hand, the type system is much more powerful with generics and associated types. Compile-time checks galore! Go streamlines and simplifies wherever possible. There is usually one obvious way to do it (perhaps even more than in Python), even if it's ugly at times. `gofmt` has one true style. The compiler does the least amount of work needed to produce a static binary or an error in the blink of an eye. As a language, Go is an attempt to codify a minimum viable product – with an emphasis on *viable*. For server apps, I'd currently choose Go, because support is a lot more mature than in Rust. However, for performance- and correctness-critical applications, Rust is becoming my go-to language (despite the fact that it lacks goto ☺).
&gt; Would having the inlined, chainable one call the non-inlined, inplace one solve this problem? That's the one I was talking about still getting lots of `memcpy`s for. When you do that, you get something like lea r14, [rbp - 16032] mov edx, 4000 mov rdi, r14 mov rsi, rbx call memcpy@PLT lea rbx, [rbp - 4032] mov edx, 4000 mov rdi, rbx mov rsi, r14 call memcpy@PLT mov esi, 5 mov edx, 8 mov rdi, rbx call A::set_inplace::h536b286a90330385taa per call, whereas using `set_inplace` directly gives you just mov esi, 2 mov edx, 2 mov rdi, rbx call A::set_inplace::h899910b9183105e5taa LLVM is getting confused for some reason. Perhaps it thinks the pointer might get stored (although Rust knows full-well it can't be).
Of course there has to be some overlap, but I'd be surprised of many projects ended up with Rust and Go on the language short-list.
Looks really good, like what I used to do with SQLAlchemy
yes!!..they compete with philosophy and ideas, probably many people coming from an empiric background doesn't notice these differences so drastically, but both languages are totally different in essence, where go bet for a simple language where types exist more for performance consideration than for type safe, rust is a full safe typed language. Go thinks than while more simple is the language more simple is programming with it, I'm not a big fan of this perspective, but it seems reasonable in some cases, instead rust is totally different, rust combine paradigms and tools from them creating a language which could be more dense but also more rich in ideas, I think than is better have a rich set of tools and you, as developer, take the decision which is the better one for x job Rust takes programming ideas and concepts from the last 20 years, you've option types, type class, macros...instead go takes C as base and add channels for concurrence and a GC trying to be more simple personally I think than although they can be used in the same task in some cases, those differences in concepts/ideas will make than many go developers don't feel comfortable using rust and many rust developers don't feel comfortable using go, if you like both then great!!...
Go feels like a bunch of older C programmers listing their issues with writing concurrent C code. Rust feels like a bunch of Haskell programmers listing their issues with C++. 
Gopher here: Am I reading this correctly - the weight of this wrapper is to bounce a zero-terminated c string to a mutable buffer that can be assigned/copied to 'path'? If it helps ease the burn at all, that's par for the course when wrapping C with any language that doesn't share its attitude for memory management. Even C++ has a hard time the second you involve `std::string`. For fun, take a look at the Go implementation (line 106) - it's not much cleaner and returns a `[]byte` instead of a `string`: https://golang.org/src/syscall/zsyscall_linux_amd64.go If Go had mutable/immutable annotations, it would probably start to look like the Rust version. :)
`scoped` won't work in this case, because I guess the thread is intended to last as long as the object, not only as long as the scope. Of course, it's probably possible to redesign so the `scoped` can be used, at the cost of less ergonomic interface.
The problem is that the lifetime of the spawned thread isn't bounded by the lifetime of `self`. First, `start_thread(&amp;mut core)` isn't the right syntax. You want `core.start_thread()` (or `FCCore::start_thread(&amp;mut core)`). Regardless, the real problem is that you're passing a pointer to a struct on the stack to another thread and then moving the struct. You should read the concurrency chapter in [the book](https://doc.rust-lang.org/book/concurrency.html).
Honestly, how many of these comparisons between Rust and Go do we need before we know that they are not competitors? This is only the two-millionth blog post posted here or on /r/programming that comes to this conclusion.
No, there has been a misunderstanding here. I see you edited your original post, which turned it incorrect. If you look in my thesis under 4.4 Hardware for benchmarking, you will see that all benchmarks were executed on a 36 core machine. Then under 4.6.1 Fibonacci, you can see that I scale ~8x on 8 threads (on a 36 core machine), but above that I start to lose the linear scale up results, on 32 threads I have a scale up of 21x, which I thought you meant in your first post. I have occasionally reached 26x scale up on 32 threads is what I meant in my reply.
Huh, I remember `from_elem` being removed. It seems it was added back [when its functionality was added to the macro]( https://github.com/rust-lang/rfcs/pull/832). I would guess it's not in the docs as it's not intended to be called except from the macro.
&gt; I'm looking at Windows You should be looking at Linux... I believe the only invalid byte/byte sequence in a path is NUL. There's a strong convention for UTF-8 encoding, but it's not guaranteed.
&gt; While your point is valid that string conversion is par for the course, a wrapper can hide this conversion for us. Go, Java, Objective-C all do this. This is something I am talking about: https://golang.org/pkg/os/#Getwd[1] . Hm... It's not clear to me how that Go function differs to `std::env::current_dir` and why &gt; The problem is such wrappers do not always exist, or exist in inconsistent location, not easy to find (if I search by getcwd) and not that easy to use. Trying get a simple String or &amp;str from current_dir() will still take jumping through a few hoops. doesn't apply. As one specific point, Go doesn't try to solve the Unicode problem: it punts it entirely to the programmer. Go `string`s aren't required to be UTF-8, they're just a collection of bytes with a UTF-8 convention. As such, using the return value of `Getwd` as if it was reliably a Unicode string is just as incorrect as it is in Rust. (I don't mean to rag on Go, but it seems that correct interaction with the OS is a hard problem. ;) )
Also, I want to mention [AMD's CodeXL](http://developer.amd.com/tools-and-sdks/opencl-zone/codexl/). I've had better experience with it, compared to VTune.
The author of this post gives it some extra significance, though.
Fair enough. If it makes you happy, it's a good name for your project.
You're totally right as far as I know, but I think that's an orthogonal topic. I like `scoped` and its semantics, and I'm happy to use it (surrounded with explicit `unsafe`ty) in my own projects. I was using it in a perfectly sane way. I'd like to implement it on top of `spawn`.
I think you missed my point slightly. Let me rephrase. :) Why do you need a `String` in Rust? That is, why are you trying to assert that the path is valid Unicode. The API Rust provides is careful to return exactly what is correct: a file path can be represented faithfully as a `Path`/`PathBuf` but this isn't guaranteed with a `String`. The Go API is "tricking" you, by returning a type called `string` that has different semantics to the Rust type of the same name. Basically, Go is lumping text (i.e. Unicode data) with paths etc, which are just a collection of bytes, without guarantees of validity. If you print a path, it may be just dumping a pile of raw bytes to a terminal/file. Generally, when handling file paths Rust encourages one to use the path types, since the `String` type has extra constraints. Of course, you're free to not be very keen on the more strict use of the type system :) (personally I like when it catches my misconceptions).
Now *that* made me chuckle :D I would point out, however, than I am mostly a C++ programmer listing my issues with C++... ;)
&gt; The GC, channels, and goroutines all stake out error-prone territory in a way that's hard to screw up - heading off any half-baked library implementations of the same. My only issue here is that the Go channels do not prevent data-races, and the resulting crashes. Go makes a best-effort attempt at detecting data-races at runtime with its built-in detector but it's not guaranteed, so Murphy's Law guarantees that some people will run afoul of the bug. Oh, and by the way, I seem to recall that data-races may lead to crashes down the road.
What I think of it? Well it's essential to know how threads work if you want to know parallelism. However, threads are very low level and don't give the programmer any easy way to express the parallelism she wants to have, unless the problem at hand is really basic. Therefore there are lots of abstractions on top of threads to provide this, for example: thread pools, fork-join libraries, message passing libraries etc. etc. My library uses thread::spawn and join underneath, but hides it in a thread pool with an abstraction that provides the programmer with a nicer interface to fork and join the computations with. Welcome to the Rust community and language :)
As I said in another reply here, if you look at the forkjoin code, look at the develop branch as it's quite far ahead of master.
Right, yeah. That's the problem with not having static arrays I guess.
I did a bit more research on Linux path naming and consequently I now understand your point. As it turns out UNIX does not enforce any encoding for file names. It stores them simply as a sequence of bytes. (I mistakenly thought they are saved as UTF-8). So, yeah, conversion from path name to ``&amp;str`` or ``String`` can surely fail.
Yes.
.NET has a built in multi dimensional array; they actually do this. Beyond that it shouldn't really be a problem to implement a multi dimensional `Vec`. However, having this as an optimization is not possible. The biggest problem I can see is that the compiler won't know the bounds of the 2nd level arrays. So it doesn't know whether they are all equal and what their size is, so it can't do the calculation to get the correct index and size. I think a multi dimensional `Vec` implementation is your best bet.
Rustorm looks really cool. Definitely alot of work going into it. Under suggestion from /u/gsingh93 in the projects thread, I just started building a SQL builder called Puddler that is decidedly not an ORM. I haven't put it up online yet, but since it doesn't care about your program, or Rust types at all, it allows you to do things like let query = Puddler::select("*").from(table); let result = DB::execute(query); which feels more ergonomic than let products:Vec&lt;Product&gt; = Query::select() .enumerate_table_all_columns(&amp;Product::table()) .from::&lt;Product&gt;() .collect(&amp;pg); since you don't have to mess with all the types everywhere and pass it to whatever ORM or straight db connection you want. Granted, your example does more, but I think the point comes across. It seems like Rustorm works super hard to integrate with your program directly as opposed to being a library you can call to get something done and then move on. In your examples you pass it your object, you pass it your postgres connection (which you handle), you have it build the query, execute the query from there and all in one place/chained statement. That's cool, but in a super strongly typed language like Rust it's gonna be an eye sore, require passing things everywhere, fighting the type checker along the way, and have a learning curve. Would it be a good idea (and I haven't put much thought into it) to separate the parts out into something like: sql builder, database control/execution functions (if you even want to handle that?), and then giving the user the ability to use your functions to easily build a "record" struct as a harness that they can define arbitrary methods on? That would allow much less internal complexity, allow you to work with just strings on the sql builder side, and let people handle their structs how they want to and only pass it to the record harness when they want to do something db related like get/save/update/any arbitrary thing they choose. It just feels like a more ergonomic work flow that, while a bit more work for the end-developer initially, would also make things more flexible and (I think?) get rid of alot of the checker fighting that might come of passing everything to your ORM. Anyway, just some random thoughts I had while thinking about building an ORM in general and then seeing yours. I'm gonna focus on the sql builder and getting it (and it's API) perfect, so I doubt I'll get near an ORM for a long time, and by then Rustorm will probably be at the point where that's no longer a needed project! Rustorm looks nice and I'll be watching with real interest :)
But Rust has static arrays, with any number of dimensions you can wish for. `[[[[[usize; 5]; 6]; 7]; 8]` et cetera
And you are figuratively Ken Thompson. ;-)
Yes, but I wouldn't call that a bug in Go or even the standard library. After all, it's not really the channel's job to prevent races as it's just a mutex-guarded queue. If *all* the goroutines in the program are blocked on channel I/O, the program panics since there's literally nothing it can do (like a checkmate). The alternative, would be to just sit there idle, forever. I'll take the crash and dump from the scheduler any day of the week - a running but non-functional process can lead to data loss, so I don't see this as a flaw. The only thing that can be done here, and should be done, is to avoid blocking calls, which isn't that hard to do (any channel can be written to or read from in a non-blocking way). I have some small gripes with Go's approach here too - there's room for improvement: * Go tutorials strongly encourage data copying for channels, rather than moving pointers around. This helps side-step data sharing issues, but isn't an all-round best practice once you get into performance sensitive areas * Goroutines are not garbage collected. My suspicion here is: since the GC is a conservative mark/sweep, it's not really possible to tell if a goroutine is blocked on an, otherwise, collectable channel. A rising pattern in the Gophersphere is to build rudimentary job control into goroutine instances where this is likely to happen (like passing in a 'cancel' channel), so that you never block on a channel read and can bail out of the goroutine responsibly. * The "data race" problem you mentioned. Go does give you just enough rope to hang yourself. I'll argue that you're using a branch closer to the ground when doing so, than with some other languages. This problem is easy to identify on code-review, and there are patterns already published to work around it. I really like Go, so please don't take the above as condemnation of the language. Compared to the state of the art 10+ years ago, this is a better place to have problems than where we were back then.
Everyone else: - *Oh Bell Labs guys? Then it's like c + garbage collection and concurrency.* - *Oh stuff inspired from statically typed functional languages? Then it must be like Haskell because that's the only one of those I know (of).*
People should just stop writing about them in the same articles and discussions altogether. *That's* how you stop this overplayed Rust vs Go trope.
That's a nonsense question, but if LLVM proves that accesses are inbounds, it can remove the check and *then* vectorization can occur.
&gt; I'd rather focus on making them both awesome so that everyone stops using C and C++, regardless of their sensibilities. Why? If it's about memory safety, it's not like everyone cares enough to prioritize that guarantee over other priorities (obviously). There have been other garbage collected, natively compiled language through history. 
I've always wondered why no language has a syntax just for method chaining. It could be a version of "." that says ignore the return value of the method and instead all this method on the original object.
I agree it should be possible, I have some proof-of-concept code that does it myself: https://github.com/Kimundi/scoped_thread_pool_cache/blob/master/src/lib.rs The issue is that the old scoped API is unsafe if you leak the RAII guard for it, and the new proposed API (that the code above also uses) runs into a compiler bug where a lifetime restriction does not actually apply, allowing dangling and racy code. The later is just a bug that will be fixed at some point though.
So I'm chiming in after admittedly not using it since about 0.8, but I totally think that with a lot of development on top of the existing core language that Rust could obliterate Go in all areas except build speed. But I don't see it happening too soon, as I still see the library infrastructure as too low level. It would be lovely to be able to use things like Akka's actors with some (fairly) transparent clustering and fully asynchronous web servers, etc.. Go is a very simple language, and that can definitely be a plus for some things, but it is just too hard to build higher level abstractions without things like generics and pattern matching and ADTs, etc. A huge killer application that needs to happen yesterday: a competitor to the Xamarin and Unity platforms. I get that C# is a very strong language, but on a hardware limited device, getting rid of garbage collection makes a monumental difference in responsiveness. 
I'm really excited to see Puddler :)
I don't program Go or Rust, but personally I never got the impression they were competitors, they just target similar domains. Go seems like C and Python had a baby with a focus on concurrency, and Rust is a new low level language in the same space as C++, emphasizing safety and other lessons we've learned over the decades from C/C++ and functional programming. They are tools well suited to a similar set of domains, but so are Python and C#.
TIL: The funny-sounding word 'fungible' is actually really scary.
There are different causes of Inf. Divide by zero and overflow both produce Inf, but are not equal or comparable.
I doubt it. Vectorizers are pretty dumb. I don't really know what I'm talking about, but [this reference](http://llvm.org/docs/Vectorizers.html) doesn't mention anything like that.
http://cglab.ca/~abeinges/blah/turpl/_book/vec.html#iterating-zero-sized-types
They are only accused of being competitors because they were both announced publicly around the same time, and described by their respective authors as 'systems languages'. There was also a bit more overlap when Rust was doing the green threads thing and therefore had a small runtime; at that time it wouldn't have been a directly viable competitor to C++.
I don't care *why* people want to use Go. I just want them to *not* use C. If Go's all glitter and marketing crap, then that's fine with me as long as it gets the job done. Enemy of an enemy, and all that jazz.
It probably isn't excess memory allocation though, as much as two hops of indirection instead of one.
You also can't have a 0x2F ("/") inside a file name, only between file names. You can, however, encode either NUL or / using an overlong sequence, which can get amazingly confusing if the application does not reject it (most do, some don't).
FWIW, this also applies to command-line arguments (argv) and environment variables. They're actually not Unicode-enforced, meaning that an incorrect environment variable can severely break your program if you assume them to be always valid strings. That's why `std::env` in Rust has two different method sets (normal ones returning `String`, and OS-specific ones returning `OsString` (which is equivalent to a sequence of bytes on Unix)). Surprisingly this "silent breakage" happens on many languages, including Python, and even Haskell. They just give you a broken string if that happens. For e.g., $ python3 -c 'import sys; print(repr(sys.argv[1]))' $(python2 -c 'import sys; sys.stdout.write(b"\xff")') '\udcff' /u/mitsuhiko has written excellent articles (or rants) regarding this, check this out if you are interested: http://lucumr.pocoo.org/2014/5/12/everything-about-unicode/ EDIT: I agree that this can be annoying, as shown in your example. But it is an unavoidable consequence considering the nature of Rust - it has to deal with the low level details correctly and explicitly. Maybe there could be "high level" methods on top of that someday (which always assume strings, or panic).
If there was Onion for Programmers this would be the headline: Rust and Go competing and how non-competitive with each other they are.
Nice, I'll have to check this out. Thanks k you.
The OP is closer to the truth by citing ML as an ancestor of Rust (though whether Rust is C++-flavored ML or ML-flavored C++ is open to interpretation). Rust may have taken some inspiration for traits from Haskell's typeclasses, but the lack of laziness, purity, and higher-kinded types (among other advanced typesystem features) make Haskell a poor point of comparison.
I never saw them as direct competitors. Go always seemed to me as appealing to Java programmers, and Rust as appealing to C++ programmers, just like the article mentions. Or to put it shorty, Rust is system programming language and Go is not. That sums it up. That said, having an option of green threads as a first class citizen in Rust could be very good (not as a replacement of native threads, but as another choice). I think it's a major upside to Go in comparison with Rust at this point. Right now anything of that sort in Rust would be highly manual and not integrated with the language at all, same as it would be in C++.
You could use Rust with Sailfish. But the main downside now is the lack of proper Rust + Qt integration.
It's not even that. It's literally just a sequence of bytes. Of course, usually they will match the user locale (since that's the lens through which the user is saving/creating files), but there's no hard guarantee they'll be valid in the locale.
IMO Rust is a much better survival game whereas CS GO is fun to watch as an esport but not as fun to play.
Additionally, the writer is [soliciting input](https://twitter.com/mjasay/status/616599198264242176) from Rustaceans on why Rust is important.
If I would have known that this was a mini conference and not a camp, i definitely would have clicked on any of the previous rust camp links and come to this. Cool looking talks. Will they be recorded?
Yikes, my bad. Thanks for the correction.
I put a code sample up, I hope it's not too complicated. MAP is a Vec&lt;u64&gt; which is 840 kB in size. This may not be the best way to do this, but the only other alternative is having to pass a lot of parameters to every function
When you were first developing prototypes, did you develop the grammar by writing idealized example programs of what you believed the language should eventually look like? If so, I'd be very interested to see those example programs, for historical reference. :)
The generic in the signature is predicated, so you *must* accept `T = String`, or `T = HashMap&lt;i32, f64&gt;`. This tells you that you're probably not going about this the right way. To constrain this to a small, fixed number of types, you should make multiple constructors. I suggest `from_u8` and `from_u16`. This would tie in well with the [`std::convert::From`](https://doc.rust-lang.org/std/convert/trait.From.html) trait.
Ah, I see. Well, shoot. I tried to expand my version to do that, but what I ended up with isn't much less verbose than your version. It does have the advantage of returning `None` rather than panicking if it encounters a NaN. Here it is as a function: fn partial_max_by&lt;I:IntoIterator, F, B&gt;(i:I, mut f:F) -&gt; Option&lt;I::Item&gt; where F: FnMut(&amp;I::Item) -&gt; B, B: PartialOrd { let mut iter = i.into_iter(); iter.next().and_then(|fst| { let ffst = f(&amp;fst); iter.fold(Some((fst, ffst)), |max_opt, x| max_opt.and_then(|(m,fm)| { let fx = f(&amp;x); fx.partial_cmp(&amp;fm).and_then(|o| match o { std::cmp::Ordering::Greater =&gt; Some((x,fx)), _ =&gt; Some((m,fm)) }) })) }).map(|p| p.0) } 
I'd love to see better input multiplexing in rust, or maybe there is and I'm just bad at looking for things.
-&gt; https://www.reddit.com/r/playrust/ Sorry :/
Is that because it needs to deref everytime I use it?
Don't forget the returned path need not be valid UTF-8.
As an addendum to /u/Veedrac's answer: you can write a trait that represents the constraint. pub trait CanGoInABuf { fn new_buf() -&gt; Buf; } You can then implement it for all appropriate types: impl CanGoInABuf for u8 { fn new_buf() -&gt; Buf { Buf::C8(vec![]) } } // ... And then, constrain methods appropriately: pub fn new&lt;T: CanGoInABuf&gt;() -&gt; Text { Text { buf: T::new_buf() } }
You beat the garbage collector but.... Can you survive the Borrow Checker? duh duh duh 
"Vectorize the comparison" meaning... what? There's one comparison, not many.
Please note that Moz and Mozilla are INCREDIBLY different entities. https://moz.com/. Moz cannot be trusted.
That joke flew right over your head.
There is a Twitter account called @ProgrammerOnion or something along those lines. Also, s/and/about/.
Thank you for your feedback, this is valuable to me. The Query API does more than I presented in the front page, The example I have presented was using structs that have been generated from their corresponding tables in the database. I made too much assumption that users will adopt the code generator. I'll put up an example where user's have the freedom to create their own structs and not using the code generator. &gt; I'm gonna focus on the sql builder and getting it (and it's API) perfect I've built a sql builder before (https://github.com/ivanceras/keywordSQL) , where users have to freedom to create whatever sql statement their platform supports,the code is very siimple, however I come to the point where the sql builder is nothing more like a string builder with predefined keywords provided by their corresponding functions, and it looses it's core features which is supposed to be cross platform and can execute on any database platform supported by the library, this is not the case with sql builder since, you loose the ability to keep track which parts of the sql are the keywords, functions, the table, the aggregates, the common table expressions, the filters. If you do keep track of these parts, you will be back to the Query API, where it has strongly typed values, which you can then transform these parts of the query that conforms to the underlying platform the user chooses. 
&gt; quite a few high profile databases written in Go Name one? I'd have thought most of them were written in C or C++. But perhaps our definition of high-profile differs.
In the words of This Week in Rust: &gt; Rust is: &gt; a systems language &gt; pursuing the trifecta: &gt; safe, concurrent, and fast. Rust protects the programmer against themselves where possible, and does so without garbage collection and (typically) at compile time. It also matches well with typical hardware architectures such as AMD64, unlike even purer dependent typing languages. While Googling for the 'trifecta' phrasing I encountered some people [talking](https://news.ycombinator.com/item?id=8040809) about how Ada may meet those goals as well, but from the sound of it was a little bit more limited in its support for concurrency in the sense of shared access perhaps. I know little about Ada though.
The article appears to give a high-level overview of Rust, picking some features to showcase to argue that Rust will make programming safer (as in absence of some classes of bugs). Edit: Oh and a PDF tag would have been nice.
Ada has a few nice things like ranged types (e.g. you can define an integer type from 0..999 and get a guarantee that the value will never fall out of the range), and support for verification (Spark). However, while it's quite a capable language, it has largely failed to attract developers outside of government-sponsored or extremely security-minded (e.g. aeronautics) circles. I guess the bureaucratic background of its inception has overshadowed the good job its creators have actually done.
A database driver has nothing in common with a database engine. Drivers can be made in any language. Engine are far more complex. Of course you can do a toy database engine, but if you want a database engine with top performance, Go seems a poor choice since it lacks low level control.
&gt; you can define an integer type from 0..999 and get a guarantee that the value will never fall out of the range That sounds incredibly slow, no? At least modern architectures have overflow flags when you do this for hardware-supported data types IIRC. &gt; I guess the bureaucratic background of its inception has overshadowed the good job its creators have actually done. Guess so, I thought it was pretty slow for no good reason.
Rust isn't important (in the sense that fighting disease or world hunger is important). However, Rust is a really cool language. It's pragmatism belies the lofty concepts underneath that even prior academic research ruled out as unusable, yet with just the right application of said concepts, out comes something that is very good, perhaps surprisingly so. If one follows the history of how Rust was made what it is now, you see a lot of churn, experimentation, iteration and hard design work, so the surprise at Rust's effectiveness is probably wrongheaded. After the lamentably failed BitC attempt, Rust is the first principled low-level language available that lays off the shackles of C, and allows for fine-grained low-level control with much less undefined behavior, yet much more powerful compile-time support (e.g. type checking, borrow- and lifeness-checking, lints).
Someone submit this to QOTW.
I suppose an argument could be made that even without language support it can be implemented by the programmer, something I'm not sure the programmer can do manually using only language-internal constructs using [overflow flags](http://en.wikipedia.org/wiki/Overflow_flag) for native data types on an architecture without Ada, Rust or LLVM changes in those languages.
It requires range checks in the right places, yes, which slows things down. The SPARK subset can avoid them by proving correctness (incl. absence of exceptions, such as overflows) at compile time.
I know one: [InfluxDB](https://github.com/influxdb/influxdb) is written in Go.
I'm excited to see that as well, I can't wait to copy your "ergonomic thingies" in mine. :) 
It's not closed sourced any more.
It builds on 1.3.0 nightly, though (a fresh clone built for me on `rustc 1.3.0-nightly (2671e8cee 2015-06-28)`), so you may want to use nightly rustc. EDIT: also builds on stable.
&gt; showed that the performance did not increase very much at all when using more threads than physical cores &gt; Going from 4 to 8 threads on a 4 core machine with HT only gave a few percent increase in performance A single thread cannot use all the parallelism available in a single core, that is why it does increase. As the number of cores increases, this difference increases too. Linear scaling from 1 thread to N threads is a nice-to-have property, but scaling from a serial application using 1 core to an application using all cores available independently of the number of threads is always the desired outcome. It is typical for embarrassingly parallel calculations to get a 28x speed-up from 1 core 1 thread to a 24 core machine using 60 threads. I don't get this with OpenMP (I barely get 20x speed-up with it) but I get this with TBB and for 80 threads it goes up-to 28x speed-up on a 24 core Haswell E5-2680v3 node. IBM recommends 2-4 threads per core on BlueGene Q machines which have Power6 CPUs. This is because 1 thread cannot use all the available SIMD vectors and ILP available in a modern superscalar CPU core. In perspective, my dual-core laptop running MacOSX has currently 1200 threads in flight...
It has build on stable, beta and nightly on my machine (tested via multirust).
Yeah it actually seems to be working on my machine now too, on both nightly and 1.1. I think I had a problem elsewhere.
The [latest version](https://github.com/DaGenix/rust-crypto/commit/5571cb41690b9cee12025192393ea7df0eddc21b) on github seems to build and test on stable: ➜ rust-crypto git:(master) cargo build Compiling libc v0.1.8 Compiling gcc v0.3.8 Compiling rustc-serialize v0.3.15 Compiling winapi-build v0.1.0 Compiling winapi v0.1.23 Compiling rand v0.3.8 Compiling kernel32-sys v0.1.2 Compiling time v0.1.30 Compiling rust-crypto v0.2.31 (file:///Users/byron/Documents/dev/rust/rust-crypto) ➜ rust-crypto git:(master) rustc --verbose --version rustc 1.1.0 (35ceea399 2015-06-19) binary: rustc commit-hash: 35ceea3997c79a3b7562e89b462ab76af5b86b22 commit-date: 2015-06-19 host: x86_64-apple-darwin release: 1.1.0 The latest crates.io release, v0.2.31, does so as well. Which OS are you trying to compile for ?
I just tried, and it does build on stable Rust (`rustc 1.1.0 (35ceea399 2015-06-19)`). I think your Cargo.lock is locked on an older version. Run `cargo update` then retry building.
Yeah it looks like that was the problem.
OS X, but it looks like my problem was I had an outdated Cargo.lock.
If this cannot happen due to economic reasons, would it be possible to donate particularly for this purpose? E.g. would kickstarter be a viable option or would it crash against some Mozilla Foundation rules? I know the standard answer is "donate to the Mozilla Foundation", but professionally recording high-quality video and audio synched with slides is very expensive. Still, I think that these recordings are very important for the Rust community as a whole. Not everybody can attend the conference, and it would be a shame if the information delivered will be lost.
The web is ultimately dictated by the popularity of its various implementations, and the only force that has ever been capable of overriding a popular implementation is the standardization of an alternative that has broader backing. When it comes to EME, it reached a point where not only was it supported in Chrome, IE, and Safari, but it was also being used by one of the most popular websites in the world, *and* it was a W3C standard. This more than constitutes "acceptance". You're welcome to dislike it, I do too. But if you feel that strongly about it then you're free to continue using Firefox 26 or whatever, which is exactly the version of Firefox you'd be stuck with in the alternate universe where Mozilla chose to die on this hill.
&gt; The web is ultimately dictated by the popularity of its various implementations Absolutely, that's what I'm saying. Hence the continued need for Mozilla's dissent regarding EME. &gt; This more than constitutes "acceptance". Acceptance isn't black and white, and while there is a major user, it's not used all over the place. &gt; alternate universe where Mozilla chose to die on this hill. Oh, come on. You're a mod. No need for this unfriendly language.
I think traits were first introduced in Smalltalk, no? At least that is the impression I got from reading the traits paper.
&gt; Parallelism in c# is pretty powerful, it has strong async primitives You're confusing parallelism with concurrency. Parallelism is about doing work literally simultaneously. Concurrency (control) is about not screwing up when shared mutable resources are manipulated during overlapping time intervals. Async and thread pools are about the latter. &gt;&gt; The traditional memory-wayR is a nightmare for straightforward parallelism. &gt; &gt; ? If unsafe memory management is already hard for non-concurrent, non-parallel programming; it is exponentially worse for programs that use concurrency, parallelism or both.
&gt; Why not have it in the language? Why have it in the language ? It sounds like an edge case you'd want to cover in a library. 
As someone who likes Rust (Go, well, not so much), all the incessant attempts to make it perfectly clear that "we're not competing with Go" make me worry. I can't help but read it as a sign of weakness: "We're afraid of Go eating Rust's lunch." Don't get me wrong, I do understand the technical essence of the argument, which is basically "Go isn't a systems programming language in the traditional sense of the term (C, C++, Ada, etc.)", and I actually happen to agree. However, even in technical discussion, there's much more to an argument than the technical content. Especially when the argument needs to be repeated over and over. What I read between the lines is this: "We're worried that, under the current system of incentives for programmers to choose languages, many would pick Go even when Rust would be a technically superior alternative." Rust community, tell me with a straight face that this isn't what you really mean.
Apparently. I checked the user's history briefly and it suggested it *wasn't* a joke.
Internet Explorer 54%? Bullshit.
I've found Mozilla cool since forever.
Uh, when is it? It would be nice if the dates were listed.
BS doesn't have to be untrue, sadly.
Please post to /r/playrust
1st August. It's on the [main page](http://rustcamp.com/)
&gt; That sounds incredibly slow, no? At least modern architectures have overflow flags when you do this for hardware-supported data types IIRC This isn't about overflow, this is about data-integrity. E.g. in Delphi you could say type TDayOfWeek = (Mon=1, Tue, Wed, Thu, Fri, Sat, Sun) TDayOfMonth = 1..31 TMonthOfYear = 1..12 TYear = 0..3000 TDate = record weekday : TDayOfWeek; day : TDayOfMonth; month : TMonthOfYear; year : TYear; end And then every time you mutate a value, the compiler will inject an if statement to ensure it is valid. It's the sort of thing you'd have to do yourself otherwise. The important thing is that since the compiler is checking it, you don't have to worry about adding the exact same if-statements everywhere the data structure is used. The downside is if an invalid value is provided, the injected code throws a dumb unchecked `ERangeError` exception, so you need to guard for that. The Rust equivalent would be `newtype` wrapper for every int, but that requires much more coding.
Potato potahto. In order to implement parallelism you need some form of syntax to tell the program to run in parallel, such as async and threadpools. In c# it's about as easy as "\#pragma parallel for" to achieve parallelism using these. 
&gt;We seek to foster knowledge of the language at the University of Victoria and are working on developing distributed consensus algorithm &gt;language at the University of Victoria and are working &gt;University of Victoria &gt;[Victoria](http://36.media.tumblr.com/f6aba722d30c5646d16868d43acfbb9a/tumblr_nbfv5sqMJl1qe6jr8o1_500.jpg) how topical! edit: but seriously, as a beginner with rust, I think this is a nice article.
&gt; Oh, come on. You're a mod. No need for this unfriendly language. I am directly paraphrasing Andreas Gal here, the CTO of Mozilla at the time of this decision.
Cool! This looks pretty powerful and flexible, but also complex.
Well, its also unpolished and potentially still buggy. :) Without the compiler fix, I can't really test it or let others review it.
:)
I've spottet two errors: a missing `mut` in `let foo = String::from("foo");` in section 3.2 and "Documentation comments *are can* be placed…" in section 4.1. Anyhow: Nice overview!
There's a reference to it [in its code](https://github.com/toshok/rust-mode/blob/c413caa8f23ad03f6fdd69bfd7637752c7f8ebc3/rust-mode.el#L272). And [it's declared in cm-mode.el](https://github.com/toshok/rust-mode/blob/master/cm-mode.el#L11), which is part of the rust-mode repository: I'm having the same issue. I have a feeling it's some version mismatch between the installed cm-mode and rust-mode. But I can't get to solve it
Fixed! Thank you!
&gt; Rust isn't important (in the sense that fighting disease or world hunger is important). You can't have political freedom without privacy and freedom of speech. And you can't have those in the 21st century without a secure basis for computing.
I'm paraphrasing a private correspondence, but it's no surprise that he's used that term elsewhere.
Outside of corporate environments, standalone email clients have been largely replaced by web-based clients.
The idea that adding an instance of a trait would not break existing code was something that Rust *used* to adhere to, but it was abandoned about a year ago (IIRC), because it was too useful for Rust's typechecker to be able to use information about what traits are implemented. I believe this is not a bug. Coherence is used to ensure modules can always be *linked* together; it's not designed to reduce typechecking problems.
Refinement types exist for the same reason that strong typing exists: better static guarantees about the behaviour of the program. How many real world domains do you know of where the integer range happens to be exactly -2,147,483,648 to 2,147,483,647? Regular system integer types are the true edge cases, but we've dealt with their many problems for decades because we fear the impact on performance. If I had to make a tradeoff between safety and performance for a trading system, I'd probably choose performance, but if it were a space shuttle, I'd probably choose safety. Ada allows for the use of refinement types during compile time but which can be elided for a release binary. It doesn't have to be slow. In fact, there are many potential optimizations, with a very large impact on performance, that can be enabled with just a few constraints on domain range. If you know that there are exactly 3 potential outputs of a function instead of 4,294,967,295 or 65,535 or even 256, there are a ton of things the compiler can do to optimize that, anywhere from invariant elision to branch prediction to loop reordering/interchange. 
Good to know, thanks
**Nobody** is arguing that Rust is magic security dust that solves all problems. I am sick of attaching a disclaimer of this to every comment I make about Rust. I don't *care* if a troll jumps out of the bushes and strawmans me. The "tools don't matter" crowd had their chance and now the secret police have hacked everything (using my tax dollars I might add). Let's make the twin ideas of "tools DO matter" and "no tool is sufficient by itself" the *starting point* for any discussion of the software security crisis. I donate a lot of money to global humanitarian causes, but I also care about what I can do *personally* in the way of nonviolent active resistance to the military-surveillance-torture-incarceration complex that dominates politics in my country, especially the foreign policy which in turn effects the whole world.
I don't think data races are easy to identify in code review. Empirically, if they were, we'd have a lot fewer data races. Golang has a race detector for a reason (and I'd argue that a race detector isn't enough when you're making heavy use of shared memory for parallel performance gains).
Sooooo I asked about this, and it turns out, we are going to be recording for sure. So thanks, but it won't be neccesary. :)
Overall, I agree. I was (attempting) to note that it's relatively easy to identify when a channel is being misused on inspection (blocking read/write, buffered vs non-buffered, etc). Naturally, there are many other kinds of cases that must be considered to be completely race free.
True, I was only considering platforms where Thunderbird is currently an option. Though I'm not sure how you'd spin the development of mobile Thunderbird as advancing the open web, and I have no idea how much of Thunderbird's code could be reused for a mobile version.
Much of Rust is a bet that the cognitive costs of safe manual memory management are outweighed by the benefits it brings you, for many projects. If GC had strictly better performance properties than manual memory management and if data races were indeed not a problem with a race detector (at least, not enough to justify the added cognitive overhead), then Go (and Java, and C#, etc.) would be better languages than Rust. During the '90s and early 2000s, in fact, it was kind of received wisdom that GCs and Java's shared memory model were just plain better in all cases. Some people do still think that. Nowadays we have pretty good evidence that GC is situational: it does have a performance cost. We also have a lot of evidence that data races are really painful. So the case for a Java-like memory model has dwindled from "it's clearly better" to "it's better for lots of apps, but you're going to pay a cost, and that cost really hurts some projects". At the same time, the explosion of languages on the server has rekindled interest in libraries that are language- and platform-independent, bestowing a significant *practical* benefit to being freestanding and interoperable. Interoperability in an environment where the lingua franca is C makes it hard to use a GC effectively. That said, I personally think that it's a leap too far to conclude that Rust's memory management story obsoletes GC. We now largely understand that GC is no free lunch, but sometimes it makes sense to give up performance for being able to write code without having to think about your memory management. Same story with data races: lots of programs aren't parallel or aren't using shared memory. If you're just writing a command line tool or Web server, you may not care about interoperability with other languages. So there certainly is a role for languages like Go, and Java, and C#, and Python and the rest. And that's fine. It's not the '90s anymore; there is so much software out there for which Rust is a great fit that there's no need to worry.
Awesome :)
Interesting. It seems to be mostly focused on the robotics and animation, but the toolchain is also interesting. I might have to check that out - it looks like a runtime similar to something I put together for grad school. The one I had used icc (we had a site license, and it was awesome), OpenMP for intra-computer communication, and then Open-MPI for the inter-node communication. But again, I'll have to check that out at some point. As I use Rust, I see that they kind of give us a way to interact with low-level code via the unsafe keyword, and I could probably build a lockless ring-buffer queue for the one way channel, and then some sort of fibonacci heap or different hash map for the distributed communication. I've thought about those in the past, but all of my stuff in is C++. Right now I am really a C/C++ programmer (yes I know they're different) learning other languages. I tried Go, but I didn't necessarily like it due to the runtime being stop-all, everything being garbage collected, and it not coming even close to the performance of C or C++. I'm also looking at Nim, which compiles .nim files into C and then compiles that. I could use my shared data structures there.
I don't know who downvoted you, but you show a lot of insight. Increasing overall information security is a good cause, and Rust could certainly play a role in this endeavor (I remain doubtful this can make a dent in the global surveillance regimen, alas). Also in some industries (e.g. automotive), if a more reliable approach becomes cheap enough to use (as it does with Rust, IMHO), it may literally save lives. Thus, I agree that Rust *is* a pretty big deal, as far as programming languages go, but let's not forget we're still solving the problems of the (relatively) well-off. Those are good problems to solve, but they're not going to cure cancer. Tangential to this, I recently found out while on vacation that cargo requires an internet connection to build a project, even if it has built it before. This leaves out those without reliable full-time internet access. Apparently multiple issues asking for cargo features to rectify this have been closed. I'll let you judge the picture this paints. 
Next time someone asks me what features Rust has that make it a good language, I'll send them this and say "Rust has some cool features, but part of what makes it awesome is what it doesn't have".
Can you point me to the documentation that world allow me to unstated this code? 
&gt; a == operator you can easily typo as = and still compile I think you meant this the other way around!
No, I think that's correct. For example in C you could do if (x = 3) do_something(); which wouldn't work in rust as `x = 3` is not a Boolean expression. (Not tested, on mobile)
No. == compares for equality, and is sometimes mistyped = in other languages.
I'll raise an issue once I'm home and have updated cargo – if the problem persists. Edit: Apparently the issue got fixed between 15.6. and now.
Ah well, always going to be an issue with young compilers. Thanks for the help :)
No, the section he puts that in is "things I'm very proud that rust shipped 1.0 without". It's not true that Rust shipped without a == operator, it shipped without a = operator.
We'll see. &gt; python, R or matlab Which are mostly implemented in C. I am *really* excited about the kinds of super high level yet lightweight languages that we can now build with such an improved foundation. Linear algebra, complex analysis, and differential geometry are due for a syntax refresh just as much as C. We don't have "full stack scientists" anymore because the abstractions of logic, math, physics, etc are just stacked too deeply. This is a reflection of the complexity of our world, for sure. But the lesson from software engineering is that we *also* need to simplify our abstraction stacks now and then.
It shipped without an == operator *that can be confused with =*. Not without an == operator altogether.
&gt; There's one, global namespace for all the crates, and it's as bad idea to have it there as it is to have it in the language, for exactly the same reasons. Why is it a bad idea?
You want to create a package named "foo" that wraps the popular Foo C API? Tough luck, someone took it already. Their package doesn't work at all and was abandoned years ago. Still, you're SOL. Anyone searching for "foo" looking for such a wrapper will find the non-working package instead of yours. Shared root namespaces are a Very Bad Idea™. You quickly get into the same position we are in with the `.com` namespace: anything even remotely sensible is already taken. The link I provided praises the "creativity" of the name Nokogiri for an XML parser, without realizing that - nobody will _ever_ figure out what Nokogiri is at-a-glance, - you can be just as creative in a non-crowded namespace if you wish.
Many of the missing features are present in languages from Ada./Pascal family. It is very nice people are being made aware of them, but many of us that didn't drank C kool aid, known them. 
But... it did ship with a `=` operator.
I think users are cities in the analogy.
I actually use `"\a"` to sound an audible beep in command-line programs, but I certainly won't miss `"\v"` (which I hadn't realized Perl got rid of already!) or `"\f"` (who uses text-based printers anymore?).
Neat!
In that case double the kudos. I've got a CS degree and more than a decade of work experience and I know I couldn't have done a better job.
According to the author of the SEME RFC, [this is doable](https://news.ycombinator.com/item?id=9827582). Makes sense, because the proposed borrow checking algorithm is based on a graph-of-basic-blocks representation and uses the concept of dominators rather than any structural features.
But = is not an operator in Rust. let a = something; a = something_else; Those are statements. = cannot exist within an expression.
I see now, I didn't read it like that.
Sorry. I did not mean to downplay the badness of what Stasi did. Actually I wrote this to highlight the fact that the Stasi needed a gigantic apparatus with snitches everywhere to do what they did; they were hobbled by limits of technology. Today, technology means surveillance is pretty much unlimited. And don't forget the extradition and 'lawful killing' stuff. I shudder to think how effective the Stasi could have been had they the tech and budget of the NSA.
Just curious but who chooses the speakers? Whoever wants to talk or is there some extended process?
We had 31 people submit talks. Then a number of people (I forget how many) did a blind review, ie, not knowing who the submitters were, rating the talks from one to five. Finally, four or five of us reviewed those votes, and selected the final program out of them. They were laregely correlated with but not the same as the highest-voted talks.
Actually it can it's just not very useful. `let a = x = y` is valid, it's just that `a` will be nil. 
Ah, interesting.
It's a thing on my todo list. IO got entirely re-written fairly recently, so there's not a lot out there other than the function list.
Related: http://programmers.stackexchange.com/a/17415
&gt;my point was since when you're not using an IO monad in rust, IO can still happen Right, that's what I'm getting at. Monads are not a way to achieve IO, they're a way to achieve IO with purity. If Haskell had an impure way to handle IO, it would not make monads less useful. The only reason rust benefits less from monads is because, essentially, it already has them. If rust didn't have a pure way of handling/ exposing IO/impurity, this would be great. But it does.
CockroachDB
I mentioned drivers yes. But there are databases and key value stores. CockroachDB is a major project with a lot of backing. 
Alternative implementations of standard libraries
Actually, that's the point; the `let y` is in the same lexical scope, but the goto lets us jump over it. Apparently, though, it'll be possible once we get non-lexical scoping.
Rust already has non-lexical scoping for variable initialization: fn f(c: bool) -&gt; u32 { let x: u32; if c { x = 0; } else { x = 1; } return x; } Making borrow scopes non-lexical will not allow the use of undeclared / uninitialized variables. Functions like this will still be rejected: fn f(c: bool) -&gt; u32 { let x: u32; if c { x = 0; } return x; }
I think it's because Unicode makes viewing strings as arrays of characters inaccurate.
&gt; Anyone searching for "foo" looking for such a wrapper will find the non-working package instead of yours. Keyword searching, which we have, addresses this issue.
You got me interested, so I googled around and found [this thread](https://www.reddit.com/r/rust/comments/30c6yb/why_is_accessing_a_char_from_a_str_so_annoying/) from a few months ago. It's from when they made char_at() unstable, and it has a lot of discussion of the reasons for doing so. Edit: Also, looking at /u/EcoGiko's [answer](https://www.reddit.com/r/rust/comments/30c6yb/why_is_accessing_a_char_from_a_str_so_annoying/cpr5lm5), it looks like you can do this to access characters in ASCII strings in O(1): `let a = "hello world".as_bytes()[4] as char;`
It just work. But still need optimization.
consistency. sdl2_name, or new_sdl2 or mint_sdl2. Is sdl2_net now a network sublibrary or an sdl2 binding by 'net'? How about blessed packages? rust_sql; is that a blessed package? or just a random name because someone already camped the sql name? Properly owned, properly namespaced crates is the solution... I really fail to understand the reluctance people have for it. Not having these properly namespaced is going to make things increasingly rubbish over time.
Oh, my bad, then! But this is literally the first time I've seen this being said by a Go proponent. Most Go programmers I've interacted with simply think the conceptual complexity of Rust's fine distinctions (pointer types, static vs. dynamic dispatch, etc.) outweight any benefits they may offer. So, for them, it isn't "Go and Rust aren't competing - they're for different use cases", but rather "Rust isn't worth the hassle, I'll stick to Go".
In my previous post, I haven't expressed any particular preference between automatic and manual memory management. I happen to prefer safe languages (in a broad sense, not just memory-safe) to unsafe ones, regardless of their memory management strategy, but that's besides the point. My point wasn't a technical one. From a purely technical standpoint, it's pretty clear that Rust and Go represent different approaches to solving different problems under different constraints. But, even then, all languages aiming at mainstream do compete for programmer mindshare. In this context, we can ask the following questions, for any pair of languages X and Y: 1. How likely is a X programmer, who isn't interested in programming languages for their own sake, to learn Y seriously? 2. If a X programmer language does learn Y seriously, how likely is he or she to continue using X seriously? 3. Does it make sense to implement different parts of the same project in X and Y? Depending on X and Y, we can get all sorts of different answers. For example, if `X = Python` and `Y = C`: 1. In the foreseeable future, there will always be Python programmers who are annoyed enough with (C)Python's performance to consider learning C to rewrite the performance-sensitive bits of their applications in it. 2. Very likely. C simply isn't productive enough to replace Python in all cases. 3. Yes, in some well identified cases: scientific computing, game engines, etc. On the other hand, if `X = C++`, and `Y = Rust` or `Y = D`: 1. Depends. Some will be attracted by features like actual module systems (both), superior metaprogramming (D), better static safety guarantees (Rust), etc. Others are too dependent on C++'s ecosystem to switch. 2. For new projects, not very likely. Large existing C++ projects probably won't be ported, though. 3. In the vast majority of cases, no. The main exception is providing bindings to existing libraries. What do you think the answers would be if `X = Go` and `Y = Rust`, or vice versa?
What if he's scared of snakes? 😨 I am! But yeah, sticking to one language until you can say you're no longer a beginner is the best way to go. Otherwise, beep something and then try to fix it Edit I mean break, not beep
Yeah, then the compiler gives out points (and perhaps a small amount of RustBux™) on successful compiles, and when you have enough points, you level up and the compiler becomes stricter and stricter until it won't compile anything without you paying RustBux™.
Jup. and then there's still combining characters. Also to have any semblance of consistency, you'll have to normalize beforehand. (Combining diacritics + u == ü, but the sequence of code points has to be normalized before this is true for the machine)
I'm a Java programmer listing my issues with C++ programmers ☺
I agree with the other commenters that you should stick with Python until you feel pretty comfortable with general programming concepts. Then, if you want to branch into understanding lower level programming concepts - like memory management - Rust is a good language to learn to get an understanding of how the system really works without the sharp edges of C.
&gt; someone didn’t understand how to get a language compiler to generate stackless coroutines effectively Stackless coroutines transparently store state beyond the expected lifetime in the current thread. This would be very difficult in a language as obsessed with lifetimes as Rust. In this case, what is easy in garbage collected programs (that don't offer any guarantees about lifetimes) would require so many restrictions that it would highly restricted/impractical/impossible Rust. It's very easy in Rust to do coroutine-like work if you *explicitly* store the state in a closure or struct. The desire to implicitly store state is where the problem occurs. &gt; they should have taken the opportunity to replace libuv with something better (hint: ASIO I'm sure someone will. But it's a tough task. Have you read the code for boost::asio? It's big, fussy, filled with platform-specific quirks and has required 8+ years to get into a good, relatively bug-free state. If you're after documentation on the old M:N threading, try here: http://static.rust-lang.org/doc/0.9/green/index.html It wasn't an API for async IO but rather an API for doing blocking IO in a single OS thread.
Doesn't enabling debug defeats the purpose of profiling? Or can Cargo / rustc generate debug symbols even when compiling for "release"?
I looked at `is_permutation_fn::four_iter_impl`. There are two uses of goto, and I think both could be replaced with labeled break/continue. (The second with a labeled `continue`. The first by wrapping with the equivalent of `do { } while (0)` - however you want to write that in rust). I think C++ could get labelled breaks. It'd just be very ugly to read. I.e. how would you know whether a label is going to be used with goto, or to break/continue the loop. The other use of goto I'm familiar with is in C, for cleanup on error. It's a manual implementation of RAII.
That's nice! I this this merit a paragraph [in this book section](https://doc.rust-lang.org/book/hello-cargo.html) saying that the main difference between debug and release is that debug builds are faster (but produces less optimized code, with overflow assertions etc), not that they have debug symbols -- since one can set `debug = true` on release builds. Or perhaps somewhere else, I'm not sure whether the book discuss Cargo options elsewhere.
Boy am I glad Rust on Windows uses native stack probes instead of morestack and doesn't have this issue. (Although neither does it have signals).
But that's probably inherited from C, where `goto` *can* be useful in some cases.
Would this help solve the issue with windowed mode applications on Windows not having a stderr to print a error message to?
MSVC nightlies. :D
I'd like to see some of your code. 
Try setting the sigaction struct to all zeros instead of leaving it uninitialized. I have been messing with signals in C, and had something weird happen if I didn't. (I don't remember what, though!)
I think so (the issue about this even referenced the RFC at some point). You could use a silent handler to disable the printing altogether, or instead log to a file. With panic handlers you have full control about what is logged, where it is logged and what else happens.
The [Plex](https://github.com/goffrie/plex) project was announced here 3 days ago, and the [PEG](https://github.com/kevinmehall/rust-peg) parser has been around for a while. There are probably more, but those are the two I'm aware of.
Whats wrong with continue?
The [cfp page](http://cfp.rustcamp.com/events/rustcamp-2015) is still up, even though it's now closed, if you want to get an idea of what Rust Camp was looking for. I believe there were 6 or 7 people doing the blind reviews.
Did you delete this thread? :-/ Others may find it useful!
I don't know of any guides, but I do know of some examples you could look at to help you write your own: * [libsyntax](https://github.com/rust-lang/rust/tree/master/src/libsyntax), the rust code that parses and lexes rust itself, is a useful example (even though the rust-parsing-rust part can make things a bit muddled sometimes, in my brain at least ;)) * [pulldown-cmark](https://github.com/google/pulldown-cmark) is a pull parser for Markdown, if that's a direction you'd want to go * [peresil](https://github.com/shepmaster/peresil) is a simplistic string parsing library on top of which Jake has built [sxd-document](https://github.com/shepmaster/sxd-document) for parsing XML. 
I don't know if you think that psychological or physical destruction is worse, but CIA and NSA pretty much exist to destroy dissidents (I hear the term 'terrorist' is popular these days)
&gt; would be very difficult in a language as obsessed with lifetimes as Rust So I guess it'll require **many** releases before we see coroutines again. =(
thank you, I found this solution too, but this approach leads to another problem: I cannot keep Buf private.
There's a functional parser/lexer for a Lisp-like language [here](https://github.com/JamesOwenHall/rust_arithmetic) that you might find helpful.
Building that cache takes a lot of memory. The Vec will use 16 bytes of memory for each grapheme, even though most of them only use a single byte. UTF-32 (like Vec&lt;char&gt;) is less absurdly storage-hungry, but it still uses 4 bytes where 1 is usually enough, and fails on multi-codepoint graphemes (like Zalgo text).
[Piston-Meta](https://github.com/pistondevelopers/meta) lets you change the syntax at run time, and it also is pretty good at error reporting. It is an experimental library though, and you need to learn the meta language to use it.
[Cargo](http://doc.crates.io/) is Rust package manager. Rust book has a [whole chapter on it](https://doc.rust-lang.org/stable/book/hello-cargo.html).
I'm working on a coroutine based handling for `mio` which will allow handling async IO transparently in fiber-like coroutines: https://github.com/dpc/mioco It's working, but far from complete.
Rust's package manager is [Cargo](http://doc.crates.io/guide.html). Cargo should come installed with Rust by default if you used the downloads from the [official Rust website](http://www.rust-lang.org).
Uh, didn't you want your project to utilize Hyper? That's what the configuration means. Do I correctly understand your question? Hyper is an HTTP client/server library.
Right, good point. I thought it was a reserved word for the config file. Now, where do I put the contents of the library?
I had an idea, could you tell me what you think? A while ago, as a learning experiment, I made a Python script to represent my idea of "commands with parameters", or an instruction set. It was 4-bit and wasn't at all realistic, but it taught me a lot, and I enjoyed building it over the course of a week. I was thinking about rebuilding it, but better, with Rust. Although not practical, I could learn a lot and get more comfortable with Rust. What do you think?
It's more than 6 characters long. I initially published the language with keywords all 5 characters or less (with a couple exceptions available for "things you shouldn't do": `unsafe` and `mutable`). This was relaxed to a general 6-character limit across the keyword set after we contracted `mutable` to `mut` and people complained too much about `ret`, and we admitted `return`. I then enforced a cranky veto to extensions past the 6 character limit for several years, including some of the most [bikesheddy bikeshed discussions we ever had](https://github.com/rust-lang/rust/issues/2229). A month after I left, [`continue` arrived](https://github.com/rust-lang/rust/issues/9467) and subsequently the dam kinda broke and a bunch of longer words got reserved and/or assigned (`virtual`, `abstract`, `offsetof`, `unsized`, `override`). Thankfully few of these are used, I hope no more ever will be. It's not a big deal, at the time I felt it was probably sort of the team's way of establishing their own aesthetic and judgment in my absence, shaking off my compulsive and at times overbearing nit-pickery, pedantry and foot-dragging. Which is a personal problem of mine! I gather I can be quite a cranky tyrant to work with sometimes. I just notice every time I write the word `continue` now :) But any current or future rust maintainer who wants to redirect a user's complaint about the prevalence of short keywords (`pub`, `mod`, `fn`, etc.) can direct the ire of the complainant at me and perhaps even this comment. The short keyword thing is entirely my fault, whether you like it or loathe it.
Interesting. I actually really like it, especially for things like mut and fn, since it reduces visual noise in common situations. 
No Copy constructors! No Move constructors!
Smalltalk has this feature. They call it a message cascade.
(Generally speaking, specifying a version will help avoid accidental breakage when upgrading dependencies, and so is recommended.)
Does AWS support content negotiation? I assumed they would support JSON for all APIs.. Chuck it up on Github that would be cool.
they both have curly brackets, then they diverge.
If you enjoy building something, go ahead :) Doing something that you find useful or fun is the best way to get into new aspects of programming. Aspects... Is that the right word? *Goes off to make coffee*
Advertising. Lots and lots of advertising.
If you feel comfortable with programming in general, then yeah go for it. But there's a lot less resources for learning Rust than Python - pretty much [the Book](https://doc.rust-lang.org/stable/book/) and the IRC channel are it. Rust will definitely teach good practice that can be applied to other languages - even garbage collected ones - because the restrictions it enforces to prevent memory corruption also encourage good design in a lot of ways.
Is there a way to avoid the downloading and store the library locally? Can it be referenced with some file:// URI? It can be useful for cases when there is no direct connectivity.
I think I understand where you are coming from, but I'm not buying that not agreeing with the government makes you a terrorist (in the government's eyes).
There is also a [paper](http://spw15.langsec.org/papers/couprie-nom.pdf) about nom that may be of interest.
There are two ways. One is directly specifying a local package path in your `Cargo.toml`: [dependencies] hyper = { path = "/home/username/libraries/hyper" } The other is using Cargo's dependency overriding. Create a `.cargo` directory and place a `config` file in it consisting of: paths = ["/home/username/libraries/hyper"] The location of `.cargo` can be any subdirectory between `/` and the project directory. For e.g. if your project is in `/home/username/projects/my-project`, any of `/home/username/projects/my-project/.cargo`, `/home/username/projects/.cargo`, `/home/username/.cargo`, `/home/.cargo`, or `/.cargo` will work. The latter is useful because it doesn't require your `Cargo.toml` to be modified. It works globally. However, if the location of the library repository is persistent (e.g. you're using an internally developed library) the former method might also be preferable. More information [here](http://doc.crates.io/guide.html).
Thanks for the pointer!
Just a nit, naming the document `paper.pdf` makes it lose context when downloaded (likewise to `plos2015.pdf`).
That's fine, but there can be a scenario where connectivity isn't available to begin with (for example firewalled environment and so on). So one common way in such case is to deploy libraries locally and then point your project to them. That's what I commonly do for C++ libraries in such cases.
You may try this https://github.com/zonyitoo/simplesched Based on https://github.com/rustcc/coroutine-rs 
I wrote the beginnings of a very small lisp interpreter a few days ago. As others have noted, the syntax is so regular that you will never need a "real" parser. Even for dotted pairs, you would just a) cons the next item onto your list and b) check to make sure the next item is a right parentheses, signaling an error otherwise. Here's where you would get into trouble, though. You want to read your Lisp program into a Lisp data structure, but Rust Vecs and Lists are not nested. Here's what I came up with for parsing (a shameless ripoff of Norvig's lispy): https://gist.github.com/anonymous/baf155489c75f109f0b0 And here's what I came up with for lisp data. https://gist.github.com/anonymous/84941660a9dd99b86bfd Support for dotted pairs just involves a couple of lines in the while loop. Exercise: implement binding environments, eval, and apply. EDIT: It occurred to me that some of the naming is a bit dated. Nreverse just means reverse in place ("Non-consing"), car and cdr are head and tail respectively, and an atom is a non-cons (because it is not decomposable into cars and cdrs (except the empty list, whose car and cdr are Nil!)) 
I suspect that's one if Gilad Bracha's contributions. He has a deep background in Smalltalk related languages (Strongtalk Newspeak).
https://s3-us-west-1.amazonaws.com/freebsd-repos/edenbsd/All/cargo-0.4.0n_5.txz https://s3-us-west-1.amazonaws.com/freebsd-repos/edenbsd/All/rust-nightly-1.2.0n_5.txz To get next build change _5 to _6 and so on. 
Thanks a lot! I am sorry to bother beause this is just FreeBSD staff. I find another solution now, "pkg fetch" can work with my broken pkg.
&gt; EDIT: I could really feel your pain with each line :) You learn so much about programming and PC's in general in 100 lines of Rust code which other didn't even in 150k loc in other languages – or never did. The pain comes from your brain try not to burst by the amount of knowledge streaming into! :D
I see. I bought my Jolla in order for me to play with it and try to develop something but I just never find the time or excuse to really to it :P. At least the dev kit supposedly works good and if you want to start playing with the OS that's the place to start. Or really go hardcore and find a Mer based distro for armv7 :P
I didn't track Nemo Mobile development lately (should probably revisit it), and KDE Plasma Active kind of stalled too after project Vivaldi sunk. It's a pity Sailfish isn't really properly open yet. Though there seems to be some [movement in that direction](http://techcrunch.com/2015/05/28/jolla-pushes-brics-partnerships-to-target-android-in-emerging-markets/).
The *important* thing about this for me is having rust-mode match the indentation portion of the chosen style, to the minimum details. It would also be nice to indent the current function (or method, or type definition) using `M-q`, to comply with local style guidelines that go beyond indenting. Perhaps rust-mode could extract the current function, send it to (a future version of) rustfmt and get it properly formatted.
Let's say you have three crates, **A**, **B**, and **C**. **A** relies on a trait having only a single implementation in order for type inference to work. **B** creates a second implementation of the trait. If **A** doesn't depend on **B**, then this isn't a problem. Nothing in **B** will be considered while compiling **A**. Now, if **C** depends on both **A** and **B**, that still won't be a problem. The fact that **C** depends on **B** doesn't affect the compilation of **A**. 
Call me a radical, but recently I've been using a style like this for long function declarations: fn run ( trie: $trie_type, key: Self::Key, value: Self::Value, mut key_fragments: NibbleVec ) -&gt; Self::Result { } I was changing the parameters to functions a lot and found that the parentheses kept getting in the way (both directly and indirectly via indenting). (code example from: https://github.com/michaelsproul/rust_radix_trie/blob/master/src/traversal.rs)
In case you're still looking for a simulation of higher-kinded types that works, I've got an implementation on monads that seems to work. I have two versions, [one that binds `Fn`][Fn] and [one that binds `FnOnce`][FnOnce]. The latter allows some clones to be removed, but it's more restrictive, since it wouldn't allow an iterator, for example, to act as a monad. It also relies on the `FnBox` feature, which isn't stable. I do find your encoding appealing, in that the *type constructor* and not the constructed type, implements `Monad`, but if the inference issue can't be surmounted, then this works too. Ideally, there'd be native support which erases this distinction, though. I ran into a couple bugs writing these. One is that, if you aren't careful about using `move` closures, you can end up with dangling references to the stack as a result of an issue which I think was discovered during the [`thread::scoped` debacle][debacle]. Also, impls on `Box&lt;Trait + 'a&gt;` don't seem to bind `'a` properly. [Fn]: https://play.rust-lang.org/?gist=a34f1e8f78d21c9202d8&amp;version=nightly [FnOnce]: https://play.rust-lang.org/?gist=29e7d6053f47143deee8&amp;version=nightly [debacle]: https://github.com/rust-lang/rfcs/pull/1084
What about this? fn run( trie: $trie_type, key: Self::Key, value: Self::Value, mut key_fragments: NibbleVec, ) -&gt; Self::Result { } Joining the closing `)` with the `-&gt; return type {` might be odd, but Rust kind of follows this convention when writing `} else {` instead of inserting a new line there. (PS: I didn't find `} else {` being discussed anywhere in the style book, but I searched Rust code and everyone writes this way. Which is kind of odd because I never considered not adding a newline after the `}`). Another thing is having the `,` in the last line, which may be odd but enables you to move parameters or insert new parameters freely (it appears it's a consensus in the Rust community). I don't actually use this style, but it minimizes diffs when reordering/adding/deleting parameters, which is nice. If I need to break a long function definition line, I end up doing this: fn run(trie: $trie_type, key: Self::Key, value: Self::Value, mut key_fragments: NibbleVec) -&gt; Self::Result { } But out of laziness: I just press enter after the first parameter and press tab to indent to this point. I'm unlikely to do anything else unless rust-mode do it for me.
Thanks for catching this! I'll activate the community team and we'll organize carpooling to Berkeley.
I usually try to put things like function headers on as little lines as possible – they are getting unwieldy enough as it is, what with all the messy type declarations. When I run out of space, I continue with double indentation. The opening brace comes at the end. On one hand, the proposed style has the benefit of laying out arguments quite neatly (however, examples fail to show arguments whose types doesn't fit on one line). On the other hand, it takes up a lot of vertical screen estate. FWIW, I don't buy the "minimize diffs" argument. I'm absolutely ok with breaking at `where` where applicable and putting it into its own line. But I feel taking up so much space for function args doesn't give enough benefits to outweigh the cost.
There's also `doto` in Clojure.
If the function has only a couple small arguments, spreading them to separate lines seems unnecessary.
I may be biased from a PHP/Zend background, but I feel that the opening brace always has to go on the next line for functions, but also for structs, implementation blocks etc. - basically everything but control structures (if for while loop match...). This is much more "compatible" (for example not requiring 2 levels of indentation) with breaking long declarations up into multiple lines and doesn't "cost" much for short declarations. Luckily Rust allows me to violate the recommended style if this won't become mainstream. Unlike Go.
I just prefer not to have special exceptions. Either always use hanging braces or always break before brace. Then there is never any confusion (accidental or intentional).
`let u = url.map_or("".into(), |url| match url.port().expect("Buuuh!") { ... }; url.to_owned())`
Looks like the `OpCode` struct needs a `#[derive(Copy, Clone)]` annotation--if it supports it, that is.
Implementing `JoinGuard` would probably be the hardest part of this - here's the JoinGuard in std::thread: http://doc.rust-lang.org/src/std/thread/mod.rs.html#709. Besides that, I *think* it would be possible to `mem::transmute` from `F + 'a` to `F + 'static` for using `thread::spawn()`.
I prefer this fn foo&lt;T&gt;(x: T, y: A) -&gt; T where T: Bar { // Stuff } And fn foo&lt;T&gt;( x: T, y: A, z: T ) -&gt; T where T: Bar { // Stuff }
The error is very weird, but essentially the issue here is you're returning an `OpCode` (owned) even though you only have "reference" access, so the compiler is saying that you can not move the `OpCode` outside of the `Vec` (the indexed content). There are two ways to fix this: * as /u/DroidLogician notes, make `OpCode` `#[derive(Copy, Clone)]`, that way `fetch` will copy the indexed opcode and return the copy * have `fetch` return an `&amp;OpCode` instead of an `OpCode`, in which case you'll also need to reference it: fn fetch(&amp;self) -&gt; &amp;OpCode { &amp;self.program[self.pc] } I don't understand why `self.program[self.pc]` returns an `OpCode` rather than an `&amp;OpCode` though, as far as I understand it should be using impl&lt;T&gt; Index&lt;usize&gt; for Vec&lt;T&gt; type Output = T fn index(&amp;self, index: usize) -&gt; &amp;T (edit: ah apparently it's to match the user-interface of most other languages which return a value rather than a ref, `T[Index]` is equivalent to `*T.index(index)`, and alternatively `index` can be called directly) You may want to report the error on the Rust bug tracker too, maybe in https://github.com/rust-lang/rust/issues/24407, it's in dire need of a diagnostics improvement.
Also note `.port()` returns the port-part of the URL, if the URL has no port part (e.g. `https://reddit.com`) it's going to be None. You may not want to *mandate* a port part, or you may want to call `port_or_default()` which is going to return a port number for all non-`file:` schemes (e.g. for `https://reddit.com` `.port()` should return `None`, `.port_or_default()` should return `443`)
I think computers should format code rather than humans. This allows the layout rules to be more complicated than would be possible to put into a style guide. It also allows you to tweak the rules more easily as you gain experience. I've been very happy with clang-format in the C++ world.
Unless it doesn't do it anymore, the compiler will probably suggest `#[derive(Copy, Clone)]` for `OpCode` anyways. It's such a simple enum, it's probably faster to copy it than operate on a reference to it and deal with ownership.
Considering how small a struct `OpCode` is (8 bytes) you're probably better off maying it copyable actually. Also you could make `eval` slightly less noisy by deref'ing the result of `state.fetch()`: fn eval(state: &amp;mut MachineState) { match *state.fetch() { OpCode::PSH(val) =&gt; push(&amp;mut state.stack, val), OpCode::POP =&gt; println!("{}", pop(&amp;mut state.stack)), OpCode::ADD =&gt; add(&amp;mut state.stack), OpCode::SUB =&gt; sub(&amp;mut state.stack), OpCode::MUL =&gt; mul(&amp;mut state.stack), OpCode::DIV =&gt; div(&amp;mut state.stack), OpCode::HLT =&gt; println!("HLT"), } }
Hmm. Well it used to, at least. Maybe they figured it was better if people decided for themselves if they needed copying instead of moving semantics.
That's amazing! I would've never known. And please, criticize all the way you want. :) I didn't know about `iter()` works just the same (I've now fixed it now). And by searching, I actually meant the usage of `in` keyword. In Python, we can use it for searching and iterating (among other things). For example, 'h' in 'hello' ... which we can't do in Rust.
Ah, I see. I misinterpreted the sentence.
No worries, and thanks for the wonderful reply :)
Rust has type inference for local variables, not anything at the top level of scope, including functions, consts, and statics, among others.
Hi! I'm trying to wrap my head around this can came to the following reasoning: self.program[self.pc] uses impl&lt;T&gt; Index&lt;usize&gt; for Vec&lt;T&gt; see https://doc.rust-lang.org/stable/std/vec/struct.Vec.html and https://doc.rust-lang.org/stable/std/vec/struct.Vec.html The index returns a reference &amp;OpCode as by the signature fn index(&amp;self, index: usize) -&gt; &amp;T But your function signature is a plain value OpCode. So in order to fullfill your signature, the value needs to be copied. So long story short, and as masklinn already answered before: Either make it copyable or change the signature to return a reference. 
Every statement in Rust is an expression yielding `()`, isn't it?
Hey, I got it to work! The problem is that the compiler can't infer from known associated types what the parent type is, so the signatures for MonadApplied and its methods needed to be changed to work in the other direction. The resulting type inference is reasonable. Not as good as you can get in a language with native support - some hints are required at times - but pretty decent nonetheless. The example code shows a few ways of using the API that work. https://gist.github.com/dylanede/3eaea1b0fa8901422add
I think you're right. Having a hanging `(` isn't exactly beautiful, but I can get used to it. I really would like to have my editor to make this and other style choices though.
Nit: you are shadowing `bytes` :p def shift(bs, amount): return bytes((char + amount) % 256 for char in bs)
Neither. You're not actually creating any variables with `Some(443)` so there's no place needed to allocate memory to. If it was `Some(port_number)`, then `port_number` would be located on the stack. Unless you use a type that puts things on the heap, values are put on the stack. Some types that do put things on the heap are Box, Vec, and String.
I always feel rigid style guidelines are harmful. Formatting can be a syntactically lightweight way to convey some meaning (e. g. grouping x, y coordinate parameters on the same line, or matching the length of 'width, height' parameters with a space so the lines look alike). Guidelines also often increase line count/length, producing unnecessary clutter when skimming through code. Humans can judge how to make the code most readable (for humans) better than machines can. The proposal seems to me too well-specified. I don't feel hurt right now by not having cool precise specs of fn-decl formatting. Rustfmt probably needs to do something though, but that could be a Rustfmt thing not a Rust RFC, no?
If the line is not too long (but would be too long in the first example), this is also acceptable: fn foo&lt;T&gt;( x: T, y: A, z: T ) -&gt; T where T: Bar { // Stuff } 
In general, you should think about the logical ownership of your data. Does an object logically shared between multiple owners, and is destroyed only when all of them are done with it? If yes, you should definitely consider Rc. If no, then Rc is probably not the best solution. Are you familiar with C++? If so, the thought process you'd use for choosing between unique_ptr and shared_ptr is almost exactly what you'd use to decide between Box and Rc. 
Interesting. I could use `bytes` (yeah), but I thought of demonstrating it with strings first and later, point it specifically to the reader, that I've changed from writing some piece of junk (just to make things work), to writing efficient &amp; readable code in my *rusty* life (in this case, the usage of bytes) :P
where are people staying in the East Bay?
The only way I can make this work is if the cache remains immutable after it is created (or once all the borrows go away). If you need to mutate the cache while having active borrows (such as a `Cube`), then you're out of luck. `Rc` seems the reasonable way to go since a shared ownership model seems to make sense here. Although, if you want to enforce that `Cube` cannot outlive the lifetime of its contents (for example the `Texture2d`) from the cache (when the cache is freed), then `Rc` has some of its own issues. **EDIT:** To more generally answer your question, you probably want to stop and consider: * Will this pointer need to be shared? * Should this pointer not outlive the original? This can potentially help inform you which route to go. For example, if you must absolutely guarantee a `Cube` cannot outlive the cache where the `Texture2d` is stored, then you'll need to get really creative. If that guarantee isn't needed, but you still need to share a `Texture2d` across `Cube`, then `Rc` is the way to go.
I really like your second solution here. Instead of storing the `Texture2d`, store the key into the cache. Although, I'd want to benchmark this in the long run to see if the lookup overhead is impacting performance, but I doubt it'd be excessive.
Try /r/playrust!
Try building with `--release`. That should make your base more resilient and more performant. 
You are probably right about the benchmarking. Perhaps the Textures could be stored in a `Vector` (or `Hashmap`). Then each gameobject could contain an index (or a hashable key) allowing for faster lookup. This approach could be faster, but it would also be less clean. I don't know if it is worth it. BTW: This thread would be a great example for a book about "Real world examples of legal and illegal data structures in rust".
You can do the same in Rust: trait Animal {} struct Dog; impl Animal for Dog {} struct Cat; impl Animal for Cat {} fn main() { let mut v: Vec&lt;Box&lt;Animal&gt;&gt; = vec![]; let d = Dog; let c = Cat; v.push(Box::new(d)); v.push(Box::new(c)); } [playpen](http://is.gd/U7zpRC) You need to use a pointer instead of just doing it by value because the values inside collections in Rust need to be of the same size. We can not statically guarantee that Dog and Cat are the same size, so we store pointers to the object. I used Box here because it simplifies the lifetimes, but you could use `&amp;` references too if you feel like dealing with lifetime stuff. ~~(pure question: is this not mentioned in the book? I have not read through it since its inception, so I do not know.)~~ edit: [found the section of the book](http://doc.rust-lang.org/stable/book/trait-objects.html#dynamic-dispatch) e2: felt like wasting more time and showing how to do it with shared references: trait Animal {} struct Dog; impl Animal for Dog {} struct Cat; impl Animal for Cat {} fn main() { let d = Dog; let c = Cat; // `d` and `c` need to outlive `v` because `v` does not have ownership of its contents. let mut v: Vec&lt;Box&lt;Animal&gt;&gt; = vec![]; v.push(&amp;d); v.push(&amp;c); } Oh, and I should say that 'polymorphism' is not quite the correct word to use, as what you asked for is just 'Dynamic Dispatch', polymorphism would be using generics.
Perfect, dynamic dispatch is exactly what I was hoping for. Thanks. Is there any benefit to using &amp; over box?
Perfect, thanks.
Just for completeness. If staticassert only wants to deal with cats and dogs, then they can do the following instead struct Dog; struct Cat; enum Animal { Cat(Cat), Dog(Dog) } fn main() { let mut v: Vec&lt;Animal&gt; = vec![]; let d = Dog; let c = Cat; v.push(Animal::Dog(d)); v.push(Animal::Cat(c)); } Playpen: https://play.rust-lang.org/?gist=8fc8cb80512b46a451c2&amp;version=stable 
Definitely, and you should try to do this! Dynamic dispatch is definitely something you should do as a last resort. Enums are a great way to accomplish this.
If you don't garbage-collect your cache, you can use `typed-arena`. If you do, you need `Rc` as ownership is dynamic.
funny, just rustc src/main.rs -O -L syscall.rs/target --emit obj ld main.o -e main gets the same 1.1 kb executable to fiddle along with 
Goddamnit... My bad!
&gt; start with Rust as first language. My advice: Be prepared to learn a lot of concepts to even get started. I know many people who easily get confused by having to grasp multiple ideas simultaneously. I'd say you need to do just that to learn Rust.
I'm a bit surprised that I/O is not the bottleneck here (but maybe you're reading from a tmpfs or have the relevant data cached in memory). Anyhow, I would expect that sending every single word over the channel to a separate "serializer" thread would be unnecessarily heavy. How about letting every thread have its own hashmap, and once the thread is done analyzing, it would lock the final hashmap and just add its contents into it. (Or extra bonus points for transferring things at every ten files or so, and only do so if a try_lock succeeds. Or something.)
`#[derive(Clone)]` is pretty naive when it comes to cloning (it just creates a new struct instance with clones of its fields) so we should actually [look at what criteria `[T: 4]` implements clone for:](http://doc.rust-lang.org/nightly/core/clone/trait.Clone.html) impl&lt;T: Copy&gt; Clone for [T; 4] So currently, arrays only implement `Clone` if their contents are `Copy`. `Option&lt;Vec&lt;i8&gt;&gt;` is not `Copy` so it looks like we're SOL with automatic derivation. Now, you *can* implement `Clone` for arrays with a little bit of unsafe code, but you can get into a situation where your array is only partially initialized with valid data and you have to drop it without causing undefined behavior, maybe because the `Clone` impl for one of its elements panicked. It looks like the Rust team elected to simply not go down that rabbit hole. Restricting it to only `Copy` data means the `Clone` impl can simply dereference and return a copy of `self`. 
Thanks for this note. Although this is not production code, the port thing is something that I missed...
This seems to answer your question: https://users.rust-lang.org/t/string-type-coercion-in-rust/1439 You could have also said this successfully: let tcp_listener = TcpListener::bind(&amp;*socket_addr); EDIT: Or you could drop the to_string instead and just use the &amp;str: use std::net::TcpListener; fn main() { let socket_addr = "127.0.0.1:1337"; let tcp_listener = TcpListener::bind(socket_addr); println!("{:?}", tcp_listener); }
&gt; How can I use the [T; n] syntax for T's which are not Copy? At the moment, I don't think you can, besides writing a syntax extension to make, e.g. `[my_struct.clone(); 4]` expand to `[my_struct.clone(), my_struct.clone(), my_struct.clone(), my_struct.clone()]`, since you can't create a macro to repeat something X times without having X expressions in the macro input. 
&gt; Fixed-size arrays are very much second class citizens in Rust at present. Which is a bit sad, since the stack can be so much faster than heap allocated vectors and lifetimes should make accessing a popped stack impossible. But of course it has to be correct as well. I wonder whether stack allocated vectors are possible in rust (see `alloca (3)`) or even better those which can auto-migrate to the heap when they grow too large or are initialized too large during run-time (`std::dynarray`**?**).
Those would be awesome. I think we are going to see those kinds of things in the future (I think they definitely align with Rust's design goals), but that's going to take a long time, since there many more urgent things that need to be done.
You're right, it doesn't matter, and I'm glad you told me about it :)
First, they're not built into the language, really (it isn't even special syntax, like unit structs). They exist as a side-effect of what enums are; an enum must be set to one of it's discriminant, so if there are no discriminant, it cannot be set to anything. Since any existing value must be set to something, no value with type `Void` can exist, where Void is this: enum Void {} As for use cases, it allows you to fill in a generic value that you will not use. A Result&lt;T, Void&gt; can never have an error, since constructing one would require constructing a Void, a Result&lt;Void, E&gt; always has an error, Option&lt;Void&gt; is always None, a Vec&lt;Void&gt; is always empty, etc. The compiler, and you, can optimize based on this: The one special case I know of for no-discriminant enums is that the empty match statement is considered diverging, like a trivial infinite loop or a panic. This function type-checks: #[inline] fn unreachable(void: &amp;Void) -&gt; ! { match (void) {} } Since rustc emits an LLVM unreachable intrinsic in the default position of a match (in this case, that's the entire match), this function compiles down to zero instructions: #[inline] fn fast_unwrap_ok&lt;T&gt;(r: &amp;Result&lt;T, Void&gt;) -&gt; &amp;T { match (r) { Ok(val) =&gt; val, Err(void) =&gt; unreachable(void), } } [And this already exists as a library on Crates.IO.](https://crates.io/crates/void)
The problem is that the discussion here is aimed towards human formatters who require simple rules. A tool like rustfmt can use a scoring system to determine where to split lines. For the example in the link I would probably format it like this: pub unsafe extern "C" fn foo&lt;A, B, C&gt;(x: SomeType, y: Vec&lt;B&gt;, z: SomeOtherType&lt;B, C&gt;) where A: Foo + Bar, B: Baz&lt;B, C&gt; { ... } But I maybe if the column for bin packing the arguments is too narrow a formatting like this would be better: pub unsafe extern "C" fn foo&lt;A, B, C, D, E, F, G, H, I&gt;( x: SomeType, y: Vec&lt;B&gt;, z: SomeOtherType&lt;B, C&gt;) where A: Foo + Bar, B: Baz&lt;B, C&gt; { ... } But because the rules are aimed towards humans, people here are preferring things like: "Always use 1 argument per line starting on a new indentation when splitting arguments over multiple lines" because they don't want to deal with special cases when formatting code. I suggest watching this presentation on clang-format: https://channel9.msdn.com/Events/GoingNative/2013/The-Care-and-Feeding-of-C-s-Dragons
If you can store the data in a place where all its users have access to (directly or indirectly), then you don't need `Rc`. For example, suppose you have a struct and implement some methods for it. If only those methods will access your data (and whoever needs it have access to the struct) then the struct can own the data on one of its fields, and no `Rc` is necessary. In doubt, always start with a single owner for your data. If you want to mutate the data elsewhere, you can pass a mutable reference of your data to it. You can't have two mutable references at the same time, but if the owner of the data is responsible to call all its users (and after the call is done, the user won't need it anymore), you can nearly always do it: just take a mutable reference, pass it to its consumer.. and when it returns you are free to offer mutation to someone else. If the owner is responsible to decide who uses the data and when, then we normally can do single ownership. But suppose we have this situation: a section A of the program creates some data (so by default they are owner of it), then it passes it to B. Both wants to use the data, and for a long time they keep using it. At some point in the future, A will decide it doesn't need the data anymore, and will want to get rid of it, but B might still be using it. But of course at some point in the future B will decide it doesn't need it too. We don't know what will happen first. In this case, even though A created the data, the ownership is truly shared. We can't deallocate the data if just A or B doesn't need it anymore - we need to wait for both to drop it. In this case, A creates a `Rc` and passes a clone to B. (and if A and B are in different thread, you use `Arc` instead).
Here's an NLP Library I've started to build for Rust. I'm going to use it on a side project of mine. But I couldn't find any native NLP libraries for Rust, so I'm building one out. Currently, the tokenizing is mostly working. More to come.
It's not clear to me, can the GC move the data around? (the appeal of tracing GCs for me is that compacting the heap might improve cache locality) If yes, how do you achieve it, through a double-pointer access every time you dereference?
Like others said, auto-coercion happens only when the targeted type is "clear". For functions that accept generic parameters, it does not happen because there could be some ambiguities. (Remember C++ function overloading resolution rules?) See: https://gist.github.com/barosl/390952faeaf7f184aab0 ([Playground](http://is.gd/fgNuWu)) Does this make it clear?
Thanks! I just finished and pushed the third part about macros. I'm also translating the Rust Book (and will translate the series) to French. Any idea on whether the core team is interested in internationalizing the Rust doc?
IRC and Rust by Examples (sometimes even more than the Rust book) are good places to start. I learned through Rust by Example and by asking stupid question to older Rustaceans. Turns out people don't bite, quite the contrary. So, go for [Rust by Examples](rustbyexample.com)! Also, shameless plug: once you feel like you've grasped the basic concepts and wonder about how it all fits together, I'm writing a [short tutorial series](http://jadpole.github.io/arcaders/2015/07/04/arcaders-1-0.html) on building a small game in Rust 1.1. There, I talk about lifetimes and macros in practice. There's still work to do, but it would be nice to have a newbie's opinion on what is clear and what isn't. :P
Isn't that kind of how it is with almost all programming. My advice is to just keep at it. The thing with programming is that you only learn a few things at once, but can't understand them because you lack the context. As you keep going at it, suddenly you have enough knowledge of everything to get context, which leads to this "feeling of enlightenment". In short be persistent and patient with yourself, and then one day it'll get there. Be it Rust, or any language.
I don't know about that. My first language was C and, although I never managed to install the SDL dev binaries on Windows, it wasn't _so_ bad to begin with. Now, I feel like Rust is even better because it catches a lot of mistakes that you might make, from the beginning. One place where it _might_ bite, though, is with the mood. Would be a shame to experience the 5 stages of Rust grief before being able to write a few command line tools. :-°
There's enough effort involved that when Manish doesn't make it happen then it often doesn't get done, unfortunately.
Yes! We can add a link at http://doc.rust-lang.org/nightly/#community-translations
Nice! Once it's good-enough, I get back to you.
I understand that position, but from experience, I find Python to be a better teaching language than C/C++. I have been through a 101 course with both (don't ask), and with Python, the class learned more and better.
Do you have a programming environment set up? I mean, the compiler, a reasonable programming editor, and all that. If you don't, use the [Rust Playground](http://play.rust-lang.org). Start making small programs (some random ideas: a program to multiply two polynomials, a program to reverse a vector, etc), it doesn't matter what. Ask Stack Overflow for any questions you have, people here tend to be very quick in answering. By browsing [the Rust tag](https://stackoverflow.com/questions/tagged/rust) on Stack Overflow, I can notice that otherwise experienced developers are very confused by lifetimes. I suggest that you don't do complex borrowing in your first programs (such as having structs with lifetimes), and prefer copying instead of borrowing. Also, it's *expected* that if you write a long piece of code, it will be littered with errors the first time you compile. Ideally you should write a very small amount of code, then try to compile it right away to check if there are any errors before you proceed. If know you will need a function but don't know yet how to write it, you can do this trick to write a "stub": fn function(parameter: SomeType) -&gt; Blablabla { unimplemented!(); } This makes the function compile as if you had already written its code, enabling you to focus on other parts of the program first, even if they make calls to this function (but if you try to run it the program will crash)
Yeah, I'm not really arguing about C over python as a first programming language. Python clearly wins there. It's more of a comparison with Rust. I started from a systems programming perspective, so I'm probably biased a bit toward systems programming languages, but here's my point: Perhaps beginning this low-level possibly isn't for everyone. However, Rust doesn't feel as bad as C. You don't have to deal directly with pointers: you have safe references; there is also pattern matching and destructuring which are by far my favourite features in a programming language. I am and will always be a static typing kind of guy, whether explicit or implicit. Now, python does an o.k. job at this (it doesn't have implicit conversion everywhere) but I like the mental discipline provided by a good ol' static type system. I tend to think of a process in terms of the types that it uses, and to design my types before my program logic. Now, python is extremely good for quick prototyping, or even for shipping products for that matter, but I feel like one should have solid grounds when starting programming. That is, if you are serious about software development. If you go in physics or biology, then I have no beef with python at all. Or perhaps I am misleaded. Like I said, pretty biased toward systems language and explicit type definitions. :P
Sounds good to me. One thing that might work could be: * python (getting the ideas across) * racket (&amp; functional mindset) * rust Or Rust as a second language. With a good library you can get most of the lifetime business out of the way. You talked about using C as a second language but, if we teach Rust with little wheels at first, one can learn the basics quite fast, and then temper with more complicated (and cooler) things such as lifetimes and macros. You don't learn C++ by beginning with pointer arithmetic; you hopefully start with stack allocation, then references in functions and smart pointers in structures, then references in structures. Also, I feel like having iterators all over the place, between other things, make it pretty expressive. Closure are still weird from an OCaml or a JavaScript perspective (and that's _the_ place where I found that the error messages were the worst, yet), but it's something we can eventually work on. Also, using Some/None seems simpler than having a `null` pointer (at least, that's my opinion). My view is that Rust is made simpler than C/C++ (or even JavaScript) by removing most of their gunfoots. It is complex, but not (uselessly) _confusing_.
&gt; This is going to sound mean and cruel, but in short: If you have to ask, then you shouldn't. That's not mean at all, and it makes a lot of sense. But all of these great answers are helping me narrow down my problem: &gt;but we don't know how long each user is going to live. Sometimes I think I do know how long things are going to live. I think it's OK to store references in certain places because the way all the lifetimes are setup, it should be fine. But it turns out that my brain doesn't borrow check as good as the compiler and things aren't guaranteed to live as long as they normally do. At this point I'm stumped on whether I can convince the borrow checker that things actually will live long enough, or switch over to `Rc`. I think this means I just need to get better with reasoning about lifetimes. When reading a function signature with a lifetime, it takes me a while to figure out what the lifetime is actually implying about the life of the variables they refer to. I think as I get better at this with practice, I'll be able to more easily decide whether it's necessary to have shared owners of the data. 
Yeah, I see your points, Im just being a little cautious with placement of Rust :) I also completely agree with functional as #2, I was just being conservative. I think we still have some time until we can rush on Rust as a #2 language. It could definitely be one, but Maybe in a year or two.
Having that level of visibility doesn't mean that one has to type out that information by oneself. With inference comes the possibility of automatically generating a number of artifacts, executable or not: declarations, documentation, interface definitions, module manifests…
See `SmallVec`. Servo could not achieve competitive performance without it. https://github.com/servo/rust-smallvec I wouldn't say fixed-length vectors are second-class citizens; we just don't have type level numerics at the moment.
AFAICS It doesn't even have to be static. He could just load all the textures in to an immutable cache at the beginning of the execution. As long as the cache is immutable, rust will not complain about references . The "problem" is that gsingh wants to load the textures lazily, and that requires the cache to be mutable. One cannot add a reference to a mutable object without borrowing it, and one cannot access an object mutably if it has been lend out as a reference. I wonder if gsingh could use a lazy loader, that behaves like an immutable variable. I just found the following project on github, and I haven't tried it, but it looks really cool https://github.com/reem/rust-lazy
I'm writing a stack-based interpreter and want something akin to Javascript objects. Right now I think I will use `Rc` and have an additional counter to count how many times the object has been inserted in the stack. When this count reaches zero I know the only references to it (if any) are held from other `Rc`s and trigger a cycle collection. I *could* avoid this extra work with a `Gc`, and compacting is often described to be its great advantage and so on. But that doesn't mean that a tracing GC is actually the right solution for me..
How about something like this ? pub unsafe extern "C" fn foo&lt;A&gt;(x: SomeType) -&gt; i32 where A: Foo + Bar { 42 } You can have arguments on multiple lines, but my main point is: function attributes(unsafe, pub, extern, fn...) should be on their own line, I think it's a good visual cue. And return type should have its own line too, same reasons, readability.
The code on my answer shows a solution for that kind of problem. You could make it static, in that everything has to share it, but it consumes too much memory to be useful. `Rc` can be used in a similar way to the example I gave. The problem with lazy is that, if I understand correctly, it runs a function once and only once, so it runs it at initialization, but it never becomes dropped. With heavy resources you'd want a way to release them once they aren't in use, and `Rc` is the simplest way for that. Notice that it uses `weak` which is not stable yet and probably will change soon.
&gt; But I couldn't find any native NLP libraries for Rust I think that rs-natural is somewhat similar. https://github.com/cjqed/rs-natural 
I've bee looking through ways I can have shared connection pool in my webapp. I thought using Arc:Mutex on a shared pool would be enough, but no. It is far more complicated than I thought. I just encoutered r2d2 by your post, which provides connection pools for each database platform I could have known earlier. When you search for rust connection pool, it refers to the game server of the game rust. Thanks for the post.
 fn database(req: &amp;mut Request) -&gt; IronResult&lt;Response&gt; { let pool = req.get::&lt;Read&lt;AppDb&gt;&gt;().unwrap(); let conn = pool.get().unwrap(); let stmt = conn.prepare("SELECT id FROM messages;").unwrap(); for row in stmt.query(&amp;[]).unwrap() { let id: i32 = row.get(0); println!("id: {}", id); } Ok(Response::with((status::Ok, format!("Db: {}", "ok")))) } I put in the type of conn let conn: PooledConnection&lt;PostgresConnectionManager&gt; = pool.get().unwrap(); since `conn` is of type `PooledConnection&lt;PostgresConnectionManager&gt;` but I don't understand why you can call functions (i.e. `prepare`) from it as though it is postgres::Connection. Thoughts?
Can this be used to recover from a failed memory allocation?
Really cool! Thanks for sharing! Someone should add this detail to the docs.
How about: each indexer has its own hashmap, and when it's done with a file, it sends the map to a merger thread that combines the output of all all indexers. Would this be more efficient?
This is really nice and easy to follow! Thanks again.
I don’t see any mention of `#` in there.
Very good !!! Thanks!
[The following in the playpen](http://is.gd/IOuudP) looks like it might give you what you want: #[allow(non_upper_case_globals)] #[no_mangle] pub static weechat_plugin_name: [u8; 27] = *b"Weechat Rust Sample Plugin\0"; fn main() {}
It seems to work indeed, thanks :o But now I've another problem: I want to generate this thing with a macro, like: weechat_plugin_name!(b"Weechat Rust Sample Plugin\0"); Aaand, I'm stucked as I must know the size of the string at compile-time, which isn't a function provided by some macros or other in the libstd/rustc :( That's could be solved by a syntax extension, which means nightly-only :( 
&gt; I don't understand why you can call functions (i.e. prepare) from it as though it is postgres::Connection. Thoughts? If you check out [PooledConnection's docs](http://sfackler.github.io/r2d2/doc/v0.6.0/r2d2/struct.PooledConnection.html), you'll see this: impl&lt;M&gt; Deref for PooledConnection&lt;M&gt; where M: ManageConnection type Target = M This means that the PooledConnection can have methods called on it that get delegated to the underlying `PostgresConnectionManager`, since method calls do a deref. 
~~http://doc.rust-lang.org/std/fmt/index.html#sign/#/0~~ EDIT: nightly: http://doc.rust-lang.org/nightly/std/fmt/index.html#sign/#/0
You're welcome! If you find anything unclear, let me know. Better fix the quirks as soon as possible. ;)
Thank you. So I learn that rust can do that neatly, awesome :). In other languages I have to re-enumerate the function names and call each of those, passing all the arguments, which is tedious
How long has this been around?
[About 7 hours.](https://github.com/rust-lang/rust/pull/26828)
Sent in https://github.com/rust-lang/rust/pull/26850 today due to this thread.
What does PSA stand for?
Praise be! Thanks for the PSA.
I don't think so, IIRC `std` [aborts on OOM](https://doc.rust-lang.org/src/alloc/lib.rs.html#121-126), and aborts are unrecoverable. 
I just did it last week: https://github.com/rust-lang/rust/pull/26678
I was really happy that my PR merged last week: [*Update substring search to use the Two Way algorithm*](https://github.com/rust-lang/rust/pull/26327). Finally we have decent substring search again. Next stop would be to add vectorization improvements.
&gt; You are better off calling chmod directly from libc crate. But the function is in a strange module. `libc` functions and types are re-exported at the crate root, so even when you do need to use `libc` directly, you don't actually need to deal with `libc::funcs::posix88::stat_`: extern crate libc; fn main() { let result = libc::chmod(...) }
I wrote [this](https://play.rust-lang.org/?gist=608ceb6f4da1ae7f38c0&amp;version=stable) `Vec2&lt;T&gt;` type that is indexed like a `Vec&lt;Vec&lt;T&gt;&gt;` but is stored as a `Vec&lt;T&gt;`. It's totally incomplete and not terribly useful (eg. no iterators), but it demonstrates that you can have this without a language built-in. Rust is being super powerful here. It turns out that nalgebra's [`DMat`](https://mcanders.github.io/bevy/doc/nalgebra/struct.DMat.html) already exists and does exactly this. Except it is indexed as `mat[(i, j)]` instead of `mat[i][j]`. Adding an `impl&lt;N&gt; Index&lt;usize&gt; for DMat&lt;N&gt;` with `type Output = [N]` would fix that, letting you choose between the two indexing styles.
thiez, Others have already convinced me that a complete rewrite into Rust is not a sensible idea - however, given that: - the code needs to be maintained for the foreseeable future, I think (perhaps naively) that it should kept so that it will run on the latest version of Java - the existing code needs extending to store chatbot&lt;-&gt;people dialogs in some sort of conventional DB - I want to move this application into the MaidSafe environment (partly for security of dialogs) - I have started learning Rust but will never be interested in learning Java it seems sensible to consider the possibilities of using Rust for some of these things - and if the possibility arises of converting some existing modules in the future - for sensible and pragmatic reasons, then all well and good . . The existing bot already needs speeding up - for the current application (acting as my own personal Avatar) that does not matter so much but if and when I want to offer that same (commercial?) service for a greatly expanded number of other people, any performance and efficiency improvements will need to be looked at. Does this make sense? Thanks, Phil.
I like this resource a lot. This or something like it might be worth linking from [rust-learning](https://github.com/ctjhoa/rust-learning#cheat-sheets).
The owner of `result` is the scope in which it is created, and you are trying to make it outlive its scope. You need to transfer ownership out of the function, so you should be returning an owned type instead of a borrowed one. In this case, I believe you should return the `RocksDBVector` instead of `deref`ing it and returning `&amp;[u8]`. ~~In fact, you can probably replace the whole function body with `self.db.get(&amp;key).ok()`, as that will return `Option&lt;RocksDBVector&gt;`. http://doc.rust-lang.org/stable/std/result/enum.Result.html#method.ok~~ EDIT: I assumed `RocksDBResult` was a wrapper around `Result`, which is incorrect, so please ignore my suggestion about `.ok()`.
For me it's a lot closer than "twice as fast", something like 11s vs 13s. From playing around with the code a bit I am guessing it's something between rust's debug formatting not being super optimized and rust carefully running individual destructors and freeing memory whereas python's gc might save itself the effort, but I dunno. Basically the difference disappears for me if I print the length of the returned vec instead of a string representation of the vec, and use `std::mem::forget(x)` to leak the vec instead of having its destructor run. :( Edit: It's been pointed out to me that python buffers writes across iterations whereas rust does multiple write syscalls per iteration. Just using libc's `puts()` function makes it faster.
&gt; As you can see, there are no lifetimes at work here Anytime you have a reference, there are lifetimes-- in this case it is elided, and thus not apparent
Thank you. Please feel free to mention additional system calls that are commonly used. I will try to research and find the Rust wrapper equivalents.
For me the compiler optimization flag makes a huge difference. If I compile with `-O` then Rust is about half as fast as Python. Without that flag it's like a hundred times slower :p If I abort the `get_per_cpu` function early with an empty result, I get some interesting results about where Rust is spending its time. Aborting before the `BufReader` is created makes the program take almost no time at all. But aborting right after the `BufReader` is created makes the program take about 50% of total time. So something about either allocating or deallocating that reader (or something the constructor does to the file handle) is expensive. The next line that matters is `lines.next()`. If I abort before that, I'm still at about 50% time, but after that the program takes almost the full time. So something about reading the very first line is expensive, but reading subsequent lines not so much. (And parsing/allocating `CpuTimes` objects seems to be pretty cheap.) If I rewrite `get_per_cpu` to just read the whole file instead of doing any kind of buffering (and keep the `-O` flag), that speeds things up so much that Rust is about 25% faster than Python: pub fn get_per_cpu() -&gt; Vec&lt;CpuTimes&gt; { let mut cpus: Vec&lt;CpuTimes&gt; = Vec::new(); let mut contents = String::new(); File::open(Path::new("/proc/stat")).unwrap().read_to_string(&amp;mut contents); for line in contents.lines() { if !line.starts_with("cpu") { break; } cpus.push(CpuTimes::from_line(&amp;line)); } cpus } I'm super new to Rust though, so I don't really understand why it's faster. Can someone with more experience jump in? (Edit: This was with rustc 1.1.0. Haven't tried with the beta.)
Use callgrind to see where your program is spending most of its time.
Confirmed for my tests dropping the buffered reader dropped it down to real 0m4.049s user 0m1.051s sys 0m2.996s That's definitely more satisfying. I'm curious why the BufReader is so much slower.
`&amp;mut self` has some other unnamed lifetime (see https://doc.rust-lang.org/book/lifetimes.html#lifetime-elision). This signature is equivalent to `fn trade_with&lt;'x, 'b&gt;(&amp;'x mut self, other: &amp;'b mut Person&lt;'a&gt;)` and `fn trade_with(&amp;mut self, other: &amp;mut Person&lt;'a&gt;)`
Yeah, another commented discovered most of the time was spent in the BufReader. Removing it increased performance considerably (3x)
@gkoz it appears that your reply got truncated.
Number of cents as a u32?
I just looked at the implementation for BufReader, and it does look like using `BufReader::new` would be suboptimal for your use case, to put it mildly. `BufReader::new` allocates a `Vec` of length 64k and then proceeds to zero that vec (using a method that [seems not to be ideal](https://github.com/rust-lang/rust/pull/26849), perfomance-wise). Since `/proc/stat` is pretty short, (787 bytes on my machine), this is a *lot* of work wasted in every iteration.
The application tracks fuel purchases. Average amount will be ~30 bucks, I'm sure. My initial thought, as /u/Tyr42 mentioned, was to just store *cents* as some integral value, but I thought I'd try to do something closer to what I'm accustomed to.
Noted for next time. This would've been easier :-D
No.
Actually pushes are O(1) amortized, I believe. The cost of a copy is O(n) and the frequency of copies is O(log n).
You're talking about the amortized marginal cost of pushing one element. They're talking about the overall cost of pushing N elements. It's just two different ways of saying the same thing.
Thanks, that makes it clear! I kinda thought it had something to do with the generic parameters. I saw in the link of hatesseattletraffic's comment that this use-case is already documented in the nightly book: http://doc.rust-lang.org/nightly/book/strings.html In a way the problem is similar to the C++ template specialisation http://www.gotw.ca/publications/mill17.htm
It's just valgrind --tool=callgrind PROGRAM ARGS kcachegrind &lt;outputfile&gt;
64k?! That sounds a little bit massive. I can understand the reasoning behind zeroing out the buffer to avoid undefined behaviors, but I think the primary reason for using `BufReader` (and `BufRead`) is for line-by-line reading, at least in my case. In that case, as the length of a line rarely exceeds a few thousands, a default buffer size of 4k (or even less) could be sufficient.
There used to be comments above that [line](https://github.com/rust-lang/rust/blob/f72b1645103e12b581f7022b893c37b5fe41aef7/src/libstd/old_io/mod.rs#L311-L312): // libuv recommends 64k buffers to maximize throughput // https://groups.google.com/forum/#!topic/libuv/oQO1HJAIDdA
Right, of course. That makes sense, thanks!
How does this method of interacting with Clang/LLVM differ from what [bindgen](https://github.com/crabtw/rust-bindgen) does? (Obviously the plugins serve two different purposes.) Also, I really prefer the way you use attributes and make modules explicit, a lot of other syntax extensions should do this instead of expanding into modules implicitly.
The term for this would be [subtyping](http://doc.rust-lang.org/nightly/reference.html#subtyping). I didn't see any documentation for that syntax when I ran the regex `/'[a-z]+\s*:\s+'/` over the Rust repo. There are several tests for it though. It's discussed in [this RFC](https://github.com/rust-lang/rfcs/blob/master/text/0738-variance.md).
I wouldn’t really call it subtyping, as a lifetime isn’t really a type. But it’s certainly related: if `'a: 'b`, then `&amp;'a T` is a subtype of `&amp;'b T`. The RFC where the `'a: 'b` syntax was introduced is [RFC 192](https://github.com/rust-lang/rfcs/blob/master/text/0192-bounds-on-object-and-generic-types.md).
&gt; I wouldn’t really call it subtyping, as a lifetime isn’t really a type. I disagree, it makes perfect sense to consider lifetimes to be (void) types, with `&amp;` and `&amp;mut` as type-constructors that take two types. They're certain a special *kind* of type. However if we introduce HKT, you would certainly want to be higher-kinded over type constructors which take a lifetime. You also *only* need to introduce subtyping of lifetimes to derive the rest of subtyping in Rust via variance. See http://cglab.ca/~abeinges/blah/turpl/_book/subtyping.html for a more detailed attempt at doing exactly this.
While it’s true that lifetimes are just another kind of type from a type theory point of view, in Rust-land the word ‘type’ generally refers to something that can go after a `:` in a variable definition. Since it’s a matter of terminology, there’s no real right or wrong answer, but I still think that referring to lifetimes as types, while not technically incorrect, could possibly be a little misleading.
Well, that's very in depth! I've read those, actually, but maybe they've been updated. I'll take another look at some point. I don't know what owns the transaction. Originally, I was going to create one of these services for each entity type, which meant it needed to be borrowed for sure, but that isn't necessary if I go this route, so that's irrelevant. What may be relevant is that the transaction may be owned by some handler for a web request. I don't know because I haven't written that part yet. It may be best if I let this service thing wrap an owned transaction instead, but that's kind of down the road. I *do* want this function to consume the data struct, because I don't want the caller to still have the bare data after insertion (it should by that point be an entity), which means, basically, that the `'d` lifetime annotation is totally wrong... But I'm still not clear on why `'d` refers to some scope *external* to the function when, in theory, the thing `'d` refers to is owned by the function?
`impl&lt;T&gt; FancyTrait&lt;T&gt; for Person` should just work.
Two weeks ago: https://www.reddit.com/r/rust/comments/3axeu8/whats_the_story_for_binding_to_c_libraries/ (nothing much)
[Previous discussion on exactly this](https://internals.rust-lang.org/t/interfacing-d-to-legacy-c-code-a-summary-of-a-competing-languages-capabilities/1406). You're asking that as though no one has any interest in it. That's not the problem; the problem is that it's **ridiculously hard** to do. I also reject the premise that partial support would "allow smooth transition". C++ is *way* too complicated for that to work. Rust isn't as well-off as D when it comes to this, either. D has function overloading, Rust doesn't. D has specialisation, Rust doesn't. D has unconstrained templates, Rust doesn't. D supports inheritance, Rust doesn't. At least for the near- to mid-term future, you're likely to get more mileage out of wrapping a C++ API in a C one, then binding *that*.
Not even C has C++ interop :p Besides, D has a much better mapping to C++ things, both have classes and structs. Rust on the other hand does not have classes or inheritance.
This article has even met some rejection from /r/golang because it is a mixture of speculation, attention grabbing and misinformation. It reads like a defense of Go in the face of an impending Rust threat and as such is bad language evangelism. I'm unsure if it passes Rule #4.
&gt; One of the most successful games in history (Minecraft) was written in Java - a language not exactly known for its simplicity or small memory footprint. With each new version Minecraft becomes slower and slower. It chews through memory and the GC does collection pauses at the most inopportune times. Minecraft is an example of how popular something can be in spite of being programmed terribly. It is **not** a reason for why you should disregard optimization.
Really? The post *directly* below this one is the exact same blogpost, and has the exact same title. Why repost?
Well the chief difference is is bindgen uses Clang to generate AST and inspects AST, my plugin compiles a piece of code into LLVM IR and inspects that.
My goto-tool would be Valgrind in this case, but it doesn't support windows afaik. Maybe try [drmemory](https://github.com/dynamorio/drmemory). I've heard good things about it.
&gt; At least for the near- to mid-term future, you're likely to get more mileage out of wrapping a C++ API in a C one, then binding that. Exactly. Also, _"There's an app for that"_ , SWIG and rust-bindgen. I haven't used these two in injuction yet but the combo might be a timesaver...
Maybe full code example repo? I couldn't get the macro stuff for events compiling.. Might have missed something. Also I found jumping straight into macros a bit hardcore :)
The title is misleading here, this article is a thinly-veiled piece of "pro-go" propaganda and the bias it exudes makes me feel queasy.
Lifetimes are types? That's weird. I'd rather say lifetimes have their own kind, separate from the kind of types. For example, `&amp;` has kind `Lifetime -&gt; Type -&gt; Type`.
&gt; Is the usage of 32-bit floating-point number to speed up the calculation? Is the precision enough for this kind of operations? Or, are 32-bit floating point numbers generally preferable to 64-bit ones for scientific calculation in general? single-precision floats are half the memory and often &gt;twice the speed (per operation) of double-precision, so yeah scientific calculations often use single-precision when that's acceptable (or [mixed-precision algorithms](http://www.sciencedirect.com/science/article/pii/S0010465508003846) to get the speed of single-precision while keeping double precision). Games do the same.
You can hardly call it an optimization, I think exponential capacity growth is a requirement for the data structure to be “in spec”.
Good-enough: there no such thing. Publish now! Seriously, I'd like to help/review. And thank you for your work!
The MPI standard defines a lot of "variables" like MPI_INT, which implementations specify as C macros, which can be of different type: implementation A does `#define MPI_INT 5` while implementation B does `#define MPI_INT &amp;mpi_int_static_var` which might be of any type. Could this be used to automatically generate bindings for different MPI implementations? See also: https://www.reddit.com/r/rust/comments/2zpp3g/rust_ffi_mpi_linker/ And some current bindings: - https://github.com/gordon1992/rust_mpi (targets the MPICH implementation) - https://github.com/hestela/ompi-rust (targets the OpenMPI implementation) The problem is that in a cluster one generally has IntelMPI, CrayMPI, IBMMPI, ... these are based on MPICH, but there are a lot of MPI implementations tuned for particular cluster and hardware, so it would be nice to have a way to generate the bindings automatically during compilation.
&gt; The trade-off for this ease of use is loss of control. At the moment there is some cost for that trade-off in terms of performance - a cost that seems entirely worth it to me. In the long run a combination of Moore's law and compiler and runtime improvements will see that cost diminish. Perhaps one day the control deemed so necessary by C++ developers will seem as quaint as the techniques needed to write software for early consoles. Actually, I think the reverse effect might come up in the next decades. Single-thread performance is not increasing anymore. Multi-thread performance is probably about to hit its limits. Energy-efficiency becomes increasingly important (battery is scarce, energy in the data center is expensive). Wherever performance matters it will be increasingly burdened on the developers.
I suspect multithreaded performance will continue to increase for the foreseeable future, but there is less urgency there because most applications right now perform most of their work on a single thread. I wonder if the strong x64 memory model will become a bottleneck.
Not even C++ has full C++ interop. For example, strings are implemented differently in different STLs.
&gt;I think exponential capacity growth is a requirement for the data structure to be “in spec”. It isn't, unless rust has some specific thing. Vectors can have different growth rates - some people (Facebook) have found that certain allocators can do better with 1.5x as a growth rate, improving heap fragmentation iirc. I always just preallocate vectors optimistically, in both C++ and Rust.
&gt; And now Go works great in Windows. In fact Go is one of the most capable cross-platform platforms - as easy to use as Java without the need for an installed runtime. But Go is compiled to a non-vm? So that seems like an odd claim. &gt;The truth is that the battle Rust is fighting was lost a long time ago. Intelligence is deeply embedded throughout the whole stack. That arguement is a tad weak imho
still the same results. It's a loop upgrade pkg itself problem.
Growing by 50% each time is still exponential. 1.5 * 1.5 * 1.5 * .... = 1.5^n
I love how the Go community handled that. In many communities people go at lengths to defend &lt;whatever&gt;, including zealotry and misinformation. Go and Rust don't do that. Both communities talk respectfully about the other, despite the languages having been pit against each other by the world. I'm only a dabbler with Go (written exactly one project in it, have used it in place of python for scripting here and there, have read some Go codebases for various reasons), but I often lurk on their IRC and reddit. They're very respectful and overall quite awesome. When they discuss Rust they are quick to admit the places where Rust comes on top, and admitting Go's shortcomings. Rust's community is similar when discussing Go or other languages. Both sometimes get things wrong about the other language, misrepresenting things, but those are generally honest mistakes. I think if someone placed the Rust and Go community in a room and asked them to fight, we'd probably just all order pizza and geek out over languages.
Then: https://github.com/jadpole/rust-book-fr
I'll look into that. Thanks for the feedback.
I said we'd fight C++ – the language, not the community. All C++ devs I've met so far are pretty swell folks. The *language* however is a different beast...
Can SWIG generate C bindings for a C++ class? I'm sure there's something out there which would do that. Of course using a C++ class in a "native" way is really hard, but wrapping it like Boost.Python or Luabind seems quite feasible. You'd need to generate the C interface, then a Rust struct/impl to emulate the C++ class. And convert types where necessary. That's not super efficient, but it should be good enough for most situations.
I'm not familiar with the `shape type` vs `void type` terminology--it sounds more like you're referring to inhabited and uninhabited types, respectively. The only literature I found defining a notion of "shape types" has to do with syntactic shapes in macros, which isn't relevant here. Is "shape types" a folk name, or is it a technical term whose referent I'm missing?
Will this be used for `assert_eq!` in the future? :)
&gt; And I'm pretty sure the folks at intel take concurrent performance very seriously. I'm sure that they do, but relaxing their memory model will surely break a lot of programs because it's just so very forgiving, so I doubt they wish to take that risk. Then again as long as different cores are working on different parts of memory (and I suspect that is the common case) it shouldn't be a problem.
Most definitely a folk name. I call it that because a lot of the literature on JS JITing is about "shape guards" that when they fail, cause a deoptimization. `void type` is just a `shape` that cannot be instantiated. I *think* the equivalent for lifetimes would be the lifetime of data in an unreachable code block.
&gt;&gt;The truth is that the battle Rust is fighting was lost a long time ago. Intelligence is deeply embedded throughout the whole stack. &gt;That arguement is a tad weak imho Not only weak, but wrong. It amounts to saying that we shouldn't strive for performance because the systems we write on are too complex to reason about. While the claim that system complexity has grown is undisputed, this doesn't take away our capability to measure and model.
&gt; Like many programmers I try to view the world in clear, black and white rationality. Hm, I've found for me that trying to view the world as more gray has been a better strategy... but do what you gotta do.
hahaha unintentional joke 
[We're fixing BufReader](https://github.com/rust-lang/rust/pull/26849) not be so slow in zeroing memory. That should decrease the zeroing overhead down to maybe 10% of what it was.
Awesome, thanks for the link. I'm just halfway through it and I can't help wondering, how did the rust community get/stay so *polite*?
We are not using libuv anymore, are we? Changing the default buffer size should be backwards compatible, right?
&gt; Can SWIG generate C bindings for a C++ class? Yes, SWIG can specifially do that and it's smart enough to deal with some of the more complicated things such as overloaded functions/operators, namespaces and the like. Adding Rust support to SWIG might actually be the most viable option of them all since SWIG already deals with variety of C/C++ complexities. **EDIT**: There is a drawback to SWIG: you have to write an interface file. **EDIT2**: Sorry, SWIG can actually infer the interface file from a header file, if possible. Sorry for the edits, it's been a while since I've last used SWIG.
&gt; Could this be used to automatically generate bindings for different MPI implementations? It depends. c_import will import macro constant of any type as long as it knows how to declare it in Rust. So far, only integers are supported, but I'd like to add support for other types as well, obviously. In your example, Rust generated for implementation A would be: const MPI_INT: i32 = 5; and for impl. B it could be, for example: const mpi_int_static_var: u64 = 5; const MPI_INT: *const u64 = &amp;mpi_int_static_var as *const u64; when support is added for pointers/references. The question is how you would deal with this in the code using this - the constant has a different type in each implementation. I don't know if that's a problem or not. **EDIT**: I don't know MPI at all, but from a quick google search, it seems that `MPI_INT` is a type rather than a constant. Currently, my plugin won't help with that... 
I think that if you want a Rust the Game server written in Rust the Programming Language, you have to write one by yourself.
My use case is somewhat complex but I can give a stripped-down version here. I have a Backend trait used for generic DB interactions: trait Backend&lt;K, V&gt; { fn get(&amp;self, key: &amp;K) -&gt; Option&lt;V&gt;; fn put(&amp;self, key: &amp;K, value: &amp;V) -&gt; (); } I want to implement this trait for a specific backend (in this case RocksDB from the [RocksDB library](https://github.com/spacejam/rust-rocksdb)): struct RocksDBBackend { db: RocksDB } I want to be able to instantiate typed RocksDBBackend structs, like this: let backend: RocksDBBackend&lt;Vec&lt;u8&gt;, Vec&lt;u8&gt;&gt; = RocksDBBackend::new(); All of the work of converting K and V into bytes that can be stored in RocksDB would be accomplished when the trait is implemented. But you'll notice in the snippet above that I would need to specify types for K and V when I instantiate RocksDBBackend. But I can't do that because this is not allowed: struct RocksDBBackend&lt;K, V&gt; { db: RocksDB } Does that make sense? The PhantomData solution "works" just fine but I was hoping for something more elegant. If I'm asking for too much, that's okay!
Ah, so you basically have an unparameterized `RocksDB` that works on raw (byte) data, and a `Backend&lt;K, V&gt;` that implements a parameterized API, and you use the `RocksDBBackend&lt;K, V&gt;` type as a shim to attach the parameterization to. Now, I'm still more of a beginner in both low-level programming, and Rust's style of generic typing, but that does seem like perfectly fine use-case for `PhantomData&lt;T&gt;`. You're basically using the shim type to dispatch to the trait. Maybe someone else has a more elegant alternative, but the only advice I can give for that is to add a comment to the `PhantomData&lt;T&gt;` member(s). That would certainly be enough for me to understand the code and intention.
You completely *nailed* what I'm trying to accomplish, and your justification for `PhantomData` strikes me as pretty compelling. Thanks for your advice and time.
&gt; Because when it comes to Servo, it's really more about the parallel rendering than about Rust. &gt; &gt; Wouldn't building a parallel web rendering engine on "any language" get you similar results? There are two reasons Servo uses Rust. We want parallel layout, and memory safety (memory safety bugs are the majority of sec bugs in browsers these days). We could have gotten both with a mature language like Java or Python, or even Go. Not exactly, but close enough. Rust is the only language that gives these at a low/zero cost. That's why Rust is used. Otherwise there are plenty of languages with far less headache (given the yet-developing ecosystem of Rust) that could have been used. Heck, Javascript could have been an option. Writing the same layout algorithms in C++ would probably give us the same results, just that we'd all be less confident of the safety of it. &gt; Given the topic at hand and that Go is built for concurrency, wouldn't building a Servo-like (i.e. parallel rendering web platform) in Go, despite the GC, be suitably effective? &gt; &gt;Wouldn't it still be vastly better than today's legacy C++ engines? Layout is but one part of what a layout engine does. Would writing our layout passes in parallelized Go be faster? I don't know; it's possible. The whole browser? I doubt it. The GC isn't the only thing that makes Go slower here. In general the Go language is designed to allow for runtime costs if they improve ergonomics, i.e. the prevalence of dynamic dispatch, the GC, etc. Rust puts no such restrictions; and lets you make choices on costs and ergonomics. nterop nterop I'm actually not sure if Go's parallelism would be a great win over regular threads here. I'm not sure I recall correctly, but the scheduling is centered around I/O. Our parallel layout doesn't do I/O as far as I can tell. Though goroutines would be great for our network stack. &gt; Given what Go is about, its syntax might be better suited for the job. I don't see how, care to elaborate? Go's syntax is simpler and easier, but that doesn't make it better (or worse) suited IMO. &gt; Also may be more performant when it comes to scheduling through the rendering task queue. A Go Servo-like may even scale better, if its able to adequately sandbox tabs in goroutines instead of one process per tab. However, "goroutines" can also be built out in Rust and (if possible) achieve a similar optimization. Agreed. We actually used to use green threads (Rust's version of goroutines) when Rust had them in the past. And as you said it's possible to get goroutines in Rust as a library, and though they may not be as awesome as Go, they should be close enough in performance. Which is why making Servo in Rust makes so much more sense. Go's advantage over Rust isn't due to a limitation in Rust -- Rust can get there too, while Rust's advantages from Servo's POV are not something Go can match due to the fundamental design. &gt; 1. Are there other flaws with Go that would make such a Servo-like difficult? Nah, mostly just the performance. Though I think writing a browser in a language with a weaker type system would also be harder. Not sure, though. &gt; 2. What about Rust (i.e. The lower levels of abstraction, borrowing on the stack, better type system) makes it a better tool for the job? and, most important, is it a better tool for the job or is it a much better tool, and if so why? Better tool for the job. Go has advantages as a tool too, its simplicity for one is a great boon. I think borrowing on the stack would make a huge difference since otherwise we get GC madness. Lower levels of abstraction, too. Also, Rust being low level lets us interop with C well. For the Rust structs associated with Javascript DOM objects, we just tie them to the JS object by interacting with the Spidermonkey GC. If our Rust-side (i.e. Go-side) objects were themselves GCd by Go, this would lead to a slowdown in the JS GC because stuff is getting cleaned up late. I'm not even sure how this could be designed to avoid the two GCs to work without jamming each other. This is just one example of the C/C++ interop that we do. (Most of this is because we don't want to write a whole library in Rust when one exists in C++, it's not any shortcoming of Rust itself) I can't really comment on the type system. _I_ find it easier to write better code when I can structure things as compile-time-checked types, but this doesn't immediately make the type system a plus point. &gt; I get a little annoyed when people use Servo as an argument for Rust's success. FWIW I've only mostly seen folks saying this when others challenge Rust "why are there no big apps in Rust yet?" (which is a ridiculous question when you look at the timeline, Rust has only been 1.0 for a short while and most big apps wouldn't be open source anyway, so you need to wait for the product to reach release)
This sounds like an optimization for large binary files, doesn't it? Maybe we should mention that in the docs. Or even better: Provide two constructors with different buffer sizes (adding one more suited for text files that are read line by line).
&gt; The decrypting &amp; searching (of ~200 files) took about a minute in Python, whereas Rust did it in ~0.4 seconds! Congratz!
Thanks. BTW, I demonstrated that *speed* to some of my friends, and now they're eager in learning Rust :)
To determine what "too complicated" even means, you have to ask "for what"? Rust's core mission is something that has inherent complexity. I think we've done a good job of not adding additional complexity beyond that, personally. I would argue Rust is simpler than C++, which is widely used. &gt; is rust is planned to be used by average developer I believe in people, personally.
If you want simple languages, there is plenty of them. The aim of rust is to be as correct as reasonably possible, then simple, while many other languages have opposite priorities. Also lot of complexity comes to lack of good documentation with examples. Having said that, what's exactly seem to be complicated for you?
`cargo new` doesn't like having a dash in the name of the project and will remove it. I guess it doesn't change much, but on the moment, that's just how I did it. Cargo renaming my project also confused me at first, and I guess that I wanted to explain that behaviour, but didn't. I'm fixing that.
The initial learning experience can be rough because Rust introduces concepts that are new to many while the compiler rejects incorrect programs. What are you finding too complicated?
Nice post! But I have a one nitpick: mem::forget(c_string); // bzzzz... leak, leak, leak... c_string This `forget` here is totally a no-op. If it really had forgotten something, you wouldn't be able to use the forgotten value afterwards (it would be moved out). The `c_string` is just a raw pointer, which is just plain data type, just as `uint`, it has no additional behavior on the destruction and it's `Copy`-able. In your code, the act of forgetting is done by `.into_ptr()` call on `CString`.
/me mumbles 'black and white are socially constructed'
/u/pcwalton has said a few times that there's no way Servo could be written in Go. Maybe he can chime in.
I've been programming extensively in C++ recently for a major tech firm, and I think Rust could significantly improve our code quality and reusability. Rust isn't really more complicated than C++. Both have dynamic dispatch and compile-time generics, both support move semantics, both support functional programming, interfacing with C, have generally zero-cost abstractions, etc, etc. The biggest extra burden Rust introduces is understanding what the compiler can automatically verify is safe vs. what is safe but cannot be proved so by the compiler vs. what is actually incorrect. Maybe type coherence also introduces some burden, but usually only in edge cases. This isn't so much of a burden, though, and is actually quite useful. I've been bitten by use-after-move in C++, which is prevented by static analysis in Rust. At my company we use a crazy template hack to get sum types in C++, while Rust natively supports algebraic types. Our version of "Result", built in top of the hacky sum types, is not nearly as pleasant to use as Rust's system (though it is much better than output pointers). We have our own string slice type, but without lifetimes I end up making unnecessary copies to avoid carefully walking through my code to ensure the original string is not moved. Same with pointing into a vector. Interoperability with the standard library suffers, too. Want to index an unordered_map&lt;std::string, foo&gt; with a slice of a string, without allocating? Well, that's unfortunate. Adding a dependency on a new library with a symbol that shows up in a public API? You'll need to update the source file, the header, and the file that instructs our build system to link the dependency. In Rust or other modern languages like Go, you'll only have to make one change (or maybe two, if the extern crate is somewhere else). And of course people forget to remove things, and since you don't explicitly import symbols and #include doesn't create a new namespace, good luck if you'd like to pare down the dependencies to only what is needed, iwyu notwithstanding. All in all, I think Rust would make my job way easier. Rust is complicated in ways that improve productivity and reliability. C++, for all the nice code I've written in it, is complicated in ways required for backwards compatibility.
Great work so far! I'm a gamedev veteran who's been eyeing Rust, and a tutorial like this is perfect for getting me and my colleagues on the hype train.
what hard for me the most is the concept of lifetime, also the syntax isn't too friendly.. even for simple thing like struct with reference require lifetime syntax. now i do understand the necessity of such thing(for secure unmanaged language) , my question is still valid. how the average developer can start to use language that even the simple thing required advance stuff.. i "afraid" to invest in a language that won't succeed..
I was just recently wrestling with some Linux headers ([this one, mainly](https://github.com/torvalds/linux/blob/master/include/uapi/linux/dvb/frontend.h)) that `#define` a whole bunch of values, including some that get their values from macro-generated expressions (the ioctls). I only need some integer values from a few relatively unchanging headers, so I did a shell script that essentially outputs a modified version of the header, with the preprocessor constants baked into enum variants. It can then be fed to rust-bindgen. [Here's what](https://github.com/irauta/linuxdvb/blob/master/codegen/bindgen/frontend.sh) I came up with. This compiler plugin approach looks like a cool approach to extract the macro-defined constant info out of C headers. Definitely appears more suitable for general use than naive text manipulation. I don't know much about working with clang/llvm this way, nor about Rust compiler plugins, but the use of `auto` to make clang do more of the work is a particularly nice looking detail.
I'm not sure, but I would guess C/C++ programmers outnumber Go programmers for now.
I'm guessing we work for the same major tech firm, judging by your mention of iwyu (which I suspect is not used outside of it). Anyway, agree entirely with your post!
Good article. FFI is always hard because you have to break down languages to languages to a basic generic element which is very often nothing more than a void pointer. You should not rely on manual memory cleanup because you are basically using Cs memory model in your python code and even worse than that every raised exception may easily skip the cleanup and the programmer will never find the problem because nobody thinks about possible exceptions when looking at code. For this simple program it is ok but if you want to use it in a large project or distribute it in a library you should provide `__enter__` and `__exit__` so that everybody its ressources are automatically freed when using it in a with statement and even provide `__del__` as a fallback strategy that frees the object and protests.
/me starts singing michael jackson
Whoops, my baad
There are many comments here, and no one is really addressing the question: is Rust too complicated to _be adopted by big companies_? Even if we grant that Rust is less complicated than C++, which is a widely used language, the question remains unanswered. How long does it take an average programmer to _really_ learn a language—to learn it well enough that the programmer is _reliably_ productive with it when solving messy, ill-defined corporate problems? It depends on the language, but for systems languages such as C++ and Rust, I think one year is a minimum answer. Could be closer to two or three years. How much does one programmer-year cost? Let's keep it simple and say $100,000. How many programmers does a language need to have to be considered “adopted by big companies?” Let's use a round number and say 1 million programmers. So… 1 million programmers * 1 programmer-year to learn * $100,000 per programmer-year _equals_ $100,000,000,000, or $100 billion—just to catch up to a language that's already entrenched. If we double the amount of time it takes for a programmer to become _reliably_ productive with the new language then we double the dollar amount, adding another $100 billion. C++, as flawed as it is, has already paid its ramp-up cost—literally, hundreds of billions of dollars over several decades—of getting millions of programmers good enough with the language to be productive with it. The question remains for Rust: will its complications pay for its adoption? Not in some ideal world, but in _our_ world, where companies' short-term goals often trump long-term goals. It's something to think about.
The flags are exposed as the `flags()` method, but this is—distressingly—a `u32`, rather than a bitflags or similar. `f.flags() &amp; (1 &lt;&lt; (FlagV1::SignPlus as u32)) != 0` would be the correct way of checking it, with `FlagV1` private. Its definition, however, is `enum FlagV1 { SignPlus, SignMinus, Alternate, SignAwareZeroPad, }`, so `f.flags() &amp; 4 != 0` would be whether `#` is set.
If you want C++ interop, you need to put your C++ code inside `extern "C" { }`.
Well, Swift needs Objective-C compatibility, which uses reference counting pervasively. Yes, it is closer to Rust in the sense that it's an AOT process, and Python's is at runtime, in the intepreter.
You can reference-count everything in Rust if you want; you just have to say it explicitly. Go's threading model basically ruled out reference counting, which would require a plethora of locks that slow everything down. It couldn't go with Rust-style compile-time safety because that conflicts with its goal of remaining friendly to fast prototyping. Objective-C used to be manually memory managed like C, but with reference counting available (that you had to do manually). It changed this to add the refcounting code automatically and do some static analysis to elide it where it's unnecessary. Swift preserves compatibility with that.
I agree with the gist of your argument, but note that you're overlooking the fact that each successive iteration of C++ must pay a ramp-up cost as well. Most of the professional C++ programmers I know first learned C++98, and are still writing C++98 at their jobs to this day. Consider that the cost to retrain these programmers to become reliably productive writing idiomatic C++14 could actually be *higher* than training those developers to learn Rust, because C++14 can only discourage and never outright forbid the idioms found in former versions of the language. This is not to argue that Rust will be adopted by big companies, only to point out that "C++" is not a homogenous entity. Retraining costs aside, it would still probably be cheaper to port a C++98 program to C++14 than to rewrite that program from scratch in Rust (especially considering that porting to newer versions of C++ can be achieved gradually, minimizing disruption). For me, Rust's appeal is in brand-new codebases, or projects where a from-scratch rewrite has somehow become entirely unavoidable.
Depends, some places want to be able to handle tenth's of a cent, or hundredths (since in operations these numbers may add up later on). But yeah this is probably the best way to store money. Either way, choose what precision you want to cut off, then choose what is reasonable. Think of the largest number that is reasonable, something that is the craziest you can think of, multiply it by at least 100000 dollars (you'll have to make it bigger if you used cents or parts of cents) choose a number that has it within that range, either a i32, i64 or BigInt if needed. Notice that I did not choose unsigned integers, that's because you want a buffer against underflow (overflow should be very very very hard). Basically you can then have a constant check that a number is not negative, while a really large number might be possible, a negative is clearly something that shouldn't happen if you planned on using unsigned. Rust has some overflow protection, but it might remove this on release on the sake of optimization, you may not know what could happen there. You want to add your own checks at all times. Rational numbers might seem nice, but in reality it's better to avoid them. First of all fractions aren't always that friendly, Having 4/3 or a dollar is a weird way of thinking about it, and conversions to and fro fractional to decimal representation may lead to unpredictable loss of precision (which is not what you want when dealing with money). Also you might be surprised at how easily you can end up with $√2 at some point.
Also, I've tried to contribute to C++ projects before (I think grep (can you get simpler than grep?)) for experience but failed miserably because it's way too complicated (I think they used extensions and Gtk variables which weren't well documented. Building things were complicated back then.). Someone tried a Rust class last year (I think last year) and I'm pretty sure students in the class were contributing to Servo before the semester was out. Surely Servo is more complicated than grep. Seems Rust was easier to get into than C++.
I have done my fair share of adding `.clone()` and `'a`s until the compiler says I don't need to anymore, but I am starting to get a feel for it. Lifetimes are different than things in other languages, and it might take some time, but I promise it does get better. Do you have any particular things that the docs don't address or places the docs are confusing?
Manual memory management will always entail some level of complexity. In C and C++ the complexity is dealt with by simply letting things blow up or go wrong -- in Rust they introduced rules/syntax and made a really smart compiler that can prove a lot about correctness. If you only relied on the adage that catching an error at compile time is far less expensive than catching it at runtime -- that alone would justify a bit of extra effort.
What do you want to work on?? 
It's an entire book: https://en.m.wikipedia.org/wiki/Crossing_the_Chasm
I was going to say that libc is a pile of heavily processor dependent and OS dependent asm code in addition to just C and there's not much sense in copy-pasting it even if C parts could be reimplemented in Rust.. ..but then I realized that Rust doesn't use *all* that code and it would be interesting to estimate how much of libc has to be actually copied/ported to use Rust without libc, here's the preliminary list: - wrappers around raw syscalls (asm) (edit: this is not needed on Windows) - mathematical functions, floating point support (asm, C) - utility functions like `memcmp` (asm, C) - anything else? This still looks huge and hardly can be maintained by the Rust team, but more tractable than whole libc.
We need at least something providing `libm` symbols and it's considered right now that `std` not going through `libc`as being a very bad idea. It reduces portability and Go has had vulnerabilities in its syscall interface that glibc had fixed (can't find the bug report right now). Rust melds with the system as closely as possible. Why do you want libc independence?
I don't think Go is performance-oriented. IMO, it's productivity &amp; maintenance-ability oriented. (I mean in context of C++ &amp; python) Swift can be fairly performant if you give up safety. Yes, just like C. And I think that's shitty. They are using atomic reference counting everywhere to take minimum level of safety with multithreading. And I believe that synchronisation should also have some performance penalty in extreme situations. 
This seems a bit like saying that a straight-razor is simpler to use than an safety razor because you can just pick up the razor and start cutting *things*. While true in a very limited sense, it's really not true unless your goal is to make a program that happens to compile. If you want a program that does what you wanted, on the other hand...
&gt; Some people enjoy being on the bleeding edge I find the Rust subreddit a lot less interesting now that there aren't sweeping changes being made every week. *shakes fist at stability*
I'd argue the same about Rust. You could legitimately use Rust as C. Only use plain-old-data structs and plain functions. Make all the fields public. Just copy data around all over the place. It won't even be *that* slow because Rust automatically passes-by-pointer to functions for larger-than-pointer aggregates. Almost no one *actually* understands how lifetimes work (I've been doing it for a year and literally this week did how it *really* works finally just click). Almost no one cares about Higher Rank Trait Bounds or Variance or all the other really crazy type system stuff that makes Rust *really* complicated. Ownership and Lifetimes are relatively straight forward; especially if you're writing Safe code.
And interoperation with C code.
Any idea about when the next meetup will happen? Also, do you accept 17 years old Rustaceans? Thanks.
&gt; Almost no one actually understands how lifetimes work dammit why did you say that *now* when there's already that great pizza quote for TWiR
I swear there was recently a blog post applying this idea to programming languages specifically, and I swear you tweeted something about it too but I'll be damned if I can find it
I don't understand for certain what you're proposing, and I'm hoping you could clarify. I've ordered my best guesses from least complex to most complex. I'm not a professional in this particular subject, so any corrections are welcome. 1. [Making it easier to build a distributable binary through rustc that uses the rust std library on a system that does not have libc installed as discussed here](https://internals.rust-lang.org/t/static-binary-support-in-rust/2011/32). In this scenario you would be proposing that rustc be more easily usable to compile binaries that statically include libc, and thus are not dependent on the system having it. 2. A rust native "implementation" of libc (as specified by 1 or more posix spec) - same FFI extern headers (through impls), but the inside's all rust: The only conceivable benefit I can imagine for this is the maintainers of this libc implementation having an easier time maintaining their implementation and reducing certain classes of bugs. But from a strict port of say, MUSL, to rust there's no immediate, obvious benefit to end users of this library. 3. A hypothetical alternate libc which took and returned rust types for the interface but unlike the libc crate now, nowhere between calling it and the system calls is there a required FFI layer of translation (e.g. i32 is not cast as a c_int, it's just passed and used as-is) - i.e. there is no torque of typecasting, this will be most noticable in the super small number of cases for which that is a bottleneck / limitation. The challenges with that (not necessarily unsurpassable) is that the libc refers a very specific thing, and very large thing (MUSL, which is a relatively small implementation, is like 100k+ lines of code) * it's a very large thing; as mentioned above, even for a limited subset there probably isn't an institution easily interested in maintaining a separate version solely for the purpose of these slight optimizations (when 99.9% of the time there would be a "cheaper" way for that company to achieve the same performance). OTOH if some team perhaps one day writes something like item #2 of this comment (rust implementation of a libc spec(s)) for the easier longterm maintenance, they might (or they might not) also choose to expose a rust-native interface in addition to the ffi interface. * It is a very specific thing: libcs are specified by the various iterations of the POSIX spec - it's meant to accomodate a lot of different legacy systems and approaches to programming and obviously doesn't really have rust in mind. For a dual purpose rust libc implementation which merely exposed a "rust native interface" this would pose no problem, for one which was solely rust the question becomes to what extent does leaving the stability of being anchored to a libc spec reduce confidence in the library's behavior stability itself.
At work I have been contributing to a Java library that deals with custom classloaders, reflection, Spring, threading+locks, and java 8 functional features. So compared to my current experiences with a mainstream language, Rust is clean and simple.
I'm planning on either the last week of July or the first week of August. I'm totally happy to have 17 year old Rustaceans!
This one? :-) http://ericsink.com/entries/fsharp_chasm.html
It probably makes sense not to re-implement bits of all the different libc's on all the different platforms Rust should run on. What benefit are you hoping to get from not depending on libc? I guess it would slightly reduce virtual address space usage, at the expense of using a little more memory on the system (since it won't be shared with C programs). There might also be some room for optimization if `rustc` isn't constrained to using the C calling convention. Or are you just going for language/religious purity? Is there some big advantage I'm not thinking of? 
The book doesn't explicitly explain *when* to use lifetimes. [This page](https://doc.rust-lang.org/book/lifetimes.html) explains in broad strokes why Rust needs lifetimes and then jumps straight into syntax and special cases, without giving an example of when lifetimes are needed. Rust-by-example *does* list the cases, but they are below the fold on a page titled [the borrow checker](http://rustbyexample.com/scope/lifetime/borrow.html). The page on lifetimes in **the book** needs to explicitly state (in bullet points) to use lifetimes when: * A function returns a reference * A structure holds a reference and maybe give an example of each. Also, the `function&lt;blah&gt;`syntax hadn't come up previously, so it would be nice to expand on that.
&gt; Rust features reference-counting, raw unsafe memory management, RAII, a novel automatic region-based system for passing around references to data, and in the future it will also feature garbage collection. Didn't know Rust supported all of them, I thought it was enforcing RAII. I guess I'll discover this as I muddle through the doc, thanks!
I too agree with your arguments but I think you missed 2 major variables - lot of developers are not "paid" to *learn* a language. They did it at school or at home. This is true for all languages. Rust I think **is a very good language to be taught** because of all the compiler checks match functional root questions - a company is spending a *huge* amount of its time/money to the support. In Rust I guess this amount may be greatly reduced compared to C++ I plan to have Rust installed in one of our virtual machines. To be tested for our entire team. I am also launching a kind of internal contest on a not so small tool we need to see and compare the development time, performance and support (after a given period of time). To be honest even if at the end Rust doesn't win the contest, the process to learn it will I think pay out because my developers will learn new concepts in a funny way. I expect them to be better not only in Rust but in any language they may have to write.
tip: if you need the value iterated over and can't use a for loop, this is almost as nice: while let Some(chr) = chars.next() { let token = ... }
&gt; what techs are new graduates learning in university Do you think it likely that universities will use Rust for courses, because of its solid theoretical foundations which probably appeal to many CS people?
rust-openssl seems to be a relatively thin wrapper around openssl. You can find the openssl documentation here: http://www.openssl.org/docs/ Once you find out how to do it in any other language, translation to rust should be simple.
It has been considered a lot, and should be more-or-less possible, but its not a goal.
Spring is misused in a lot of places, contributing to a lot of incidential complexity (that's also why it has a bad reputation with most Java devs like me). Also, it's easy to go overboard with reflection and functional idioms (just today, I replaced a .filter(…).map(…).flatMap(…).… stream with a nested loop in my code. It may take a few more lines of code, but the code in question has to be sequential anyway and with loops it's much more readable). So now you have my background. I agree that Rust is (somewhat) *clean*. Not squeaky clean as Scheme, but clean enough not to negatively stand out on NCC-1701. For a low-level-language that's quite an achievement (Especially when compared to C++, which feels much more at home in a vulcan mine shaft) – it's actually quite surprising (then again, behold the power of taste, time, and successive iteration). However, *simple* is not quite the right attribute when it comes to Rust. A *simple systems language*, that would be quite the deal. Because, see, systems may be a lot of things, but they certainly aren't *simple*. So can we agree that Rust isn't exactly simple, but at least does not involve too much incidential complexity and goes out of the way to let you deal with the complexity of the problem at hand (that is, writing a program to control a system without all that plushy abstractions like GC getting in the way)?
You're just in time to watch this very functionality appear! https://internals.rust-lang.org/t/pre-rfc-simd-groundwork/2343/14
Even when you're just talking about math and syscalls it already doesn't make sense to reimplement things. That sounds like such an enormous amount of work just for the two.
&gt; iwyu http://include-what-you-use.org/ 
With libc, all of the system calls are just foreign functions in C. Without libc, the system calls will vary quite a bit between platforms, vary between architectures, and generally won't conform to the C ABI. Some of the syscalls don't even exist on some platforms and would have to be implemented in terms of other syscalls. Can you guess which ones? (`fork()` on Linux, for example. Betcha didn't see that one coming.) Then you'd end up reimplementing the math functions, which is just a royal pain in the neck. All for what... so you can save a few K on your 5MB binary, maybe?
&gt; technically, you're invoking undefined behaviors. Perhaps not exactly undefined (as in C's lingo) but certainly un*der*defined. ;-)
And I'm temporarily across the country. -_-"
&gt; I thought it's a matter of ownership It is for owned objects e.g. a `String`, in that case ownership is transferred to `mem::forget` which doesn't do anything and the bytes array on the heap is thus leaked. Raw pointers are not owned though (hence the [`Unique`](http://doc.rust-lang.org/std/ptr/struct.Unique.html) wrapper, which wouldn't do a deallocation anyway it just ensures there's only one user of the underlying raw pointer). A use case is when you've created a String or Vec or Box from a raw pointer you got somewhere else (e.g. via FFI) and you don't want to deallocate the original pointer (because you don't own it) so you'd `mem::forget` the owned object.
The optimizer can vectorize some loops by itself too already.
There is `mem*` family functions implementation in Rust named [`rlibc`](https://github.com/rust-lang/rlibc). Rust `libc` implementation could have fallback to `libSystem.dylib` and `kernel32.dll` as Rust support conditional compiling. It would be strange, but should work.
I was thinking mostly about Linux and BSD environments with defined `syscall`s. In Windows it could be left as call to `kernel32.dll` as it is **always** present in system, unlike `libc` in Linux (i.e. minimal lxc/rkt containers).
It was rather wandering about creating static binaries with no runtime deps.
On Windows, Rust's std only uses the following functions from the CRT (aka Microsoft's C standard library). * The entry point. Easily swappable for your own custom entry point if you don't link to any C code since we don't rely on the CRT runtime to be initialized. * `memcmp`,`memmove`, `memset`, `memcpy`, and `strlen`, of which `strlen` is only used for `CStr` and can be easily replaced (I'll write a PR to fix it soon). * All the various math functions that would require us to link in some sort of `libm` ourselves. Everything else that std uses is from regular system DLLs and not the CRT.
`kernel32.dll` doesn't really have an equivalent in Linux - `libc` is more like the msvcrt-s etc.
Yeah, if only other languages made you think about resource management like Rust...
But calling malloc is simple too, if you need that?
It could be a matter of missing system dependencies. Try running without `cargo clean` and with a single thread (`--jobs 1`). The compilation log will be cleaner and there won't be "other jobs" to wait for. Also, it's weird that the error message isn't printed. Maybe wait a while or Ctrl-C it (although I'm not sure if cargo prints a message after Ctrl-C). 
&gt;strlen is only used for CStr and can be easily replaced (I'll write a PR to fix it soon) Are you going to go full asm with alignment trickery, like CRT does? :D I guess naive implementation will be several times slower.
I've just realized that I thought you've used `into_ptr()` from the start! Anyway, the `as_ptr()` version is creating dangling pointer, because the `CString`'s destructor runs and deallocates memory on heap, but you still return the pointer (it probably leads to double-free too). You're using `forget` on raw pointer, but you probably want to run it on `CString`: let c_string = CString::new(count_string).unwrap(); let raw_ptr = c_string.as_ptr(); forget(c_string); raw_ptr 
A system call is an interface to another runtime dependency, which is the kernel. I honestly don't think that this is a pedantic distinction here: the kernel provides a lot of the functionality for your program at runtime. So your choice here for interacting with the kernel is between using a C ABI or using the syscall ABI: neither choice allows you to eliminate all runtime dependencies. Since Libc also provides floating point routines, and the Libc syscall interface is more convenient, what exactly are you gaining here by ditching it? To eliminate *all* runtime dependencies, you need to go "bare metal". I would really love it if someone could explain what the goal is of eliminating a Libc dependency. I mean, what is the actual purpose of doing that. And saying "to create a static binary" is a non-answer, because that's just restating almost the same thing in a different way.
Hm, interesting. I didn't know that Linux had a `fork()` syscall. Is it just legacy cruft around for compatibility?
Thanks, jobs 1 got it to print a (for some reason supressed) libssl missing message. Cheers :)
I think the trait system is simpler than classes.
complexity for professionals is not about the learning curve, it's about the actual usefulness of an instrument in the real world. There're so many languages arround you can easily write some "hello, world" tutorial with and it will look simple and good until you try it in a real task and find out that you actually have to write many supporting code by hand (for example null-value checks in at first neatly looking chains like a.do_smth().do_smth_else().do_another_thing()), because it hadn't been included in the standard library or language itself not to frighten newbies
&gt;In release mode it actually just inserts a simple inc cmp jne loop directly inline. It's `/Oi` in action (it's turned on by default in Release, but can be turned off) - some function calls are replaced with compiler intrinsics. Maybe naive loops are really executed better on modern processors or they just save code size because intrinsics are inlined, who knows.
Oh I know what it is, I'm just saying I don't think it's used outside of this major tech firm (despite being OSS).
Alright! I'll be looking for it, then. I can't join the meetup group, though, because: 18+ only and stuff :P So, if you see a weird teen who you didn't expect, it'll probably be me!
&gt; I believe in people This, *a million times* this. I've seen enough instances (some during school) where treating people like worker bees and "training them" resulted in a lack of ability to go outside the boundaries of that "training". You don't "train" someone to program, you don "teach" someone how to program, maybe you teach them some CS history and theory, but ultimately, you need to **empower** them. To me, programming is all about having more power at your fingertips than the entirety of humanity has had for millennia. Why waste all that power and creative potential by giving people mediocre tools (or worse, tools that have "being dumbed down" as a design goal) and by telling them *what* to think instead of showing them that they *can* think? I'm not a big fan of conspiracy theories so I'd go with "education is not a solved problem" (it doesn't help that the people who teach could be using methods which were perfect... decades ago). Treating humans as dumb or incapable is the best way to ensure they become exactly that. Everyone needs to stop doing it. I also believe that virtually every programmer can be a 10x programmer with some (more) dedication, but they first need to get out of the 0.3x box society has oh-so-thoughtfully provided. If you're reading this and you're unsure about diving into an apparently complex subject, just do it. If you have a cortex, you're very likely to succeed at grasping the relevant knowledge, and if you fail, only time can stop you from trying again. Anyway, this rant was a bit of a knee-jerk reaction, remembering all of the situations in which young people were not trusted with mastering complex topics. That's it, I'm done. Don't be afraid to better yourself and do awesome things.
[They already have!](http://www.rust-class.org/)
Swift's model is more: don't worry too much about memory management while writing code – you can fix leaks during testing. Yes, you get leaks in testing but memory is otherwise safe. And fixing a leak is as simple as adding an attribute to a property or closure (almost never rewriting code). I disagree that it's hard to track; Apple's Instruments tooling is well written in this regard. The problem with Swift's model is more that you might forget to look for leaks in the first place.
And here I am, thankful lifetime elision freed me from worrying to much, only to learn that it's out to bite me in the ass. Now I'm becoming paranoid. ;-P Ignorance really *is* bliss.
Hahaha, it's definitely wonderful when you're actually writing code. But I think the criticisms the parents posters made are valid, in that more attention needs to be paid to them in the learning materials. It would be interesting to see what some of the tutorials would look like if they introduced and used explicit lifetime notation from the start, and only mentioned elision after the reader would be comfortable with the notation. I think being exposed to the notation (even in simple cases) would also help people get a grasp on the basic idea of lifetimes.
I'll take the opportunity to promote [*The Rust FFI Omnibus*](http://jakegoulding.com/rust-ffi-omnibus/) again, especially the [section on objects](http://jakegoulding.com/rust-ffi-omnibus/objects/).
Nice. That's quite informative :)
I could think of an IDE feature that shows elided lifetimes (e.g. on hover or via toggle). That'd be great. Just after writing the above, I happened upon a source line in clippy I wasn't quite happy with, because it cloned a string (with `.to_string()`! blasphemy!) just to use it in a `&amp;format(…)`. Now this was in a `Result&lt;String, …&gt;::unpack_or(…)`, so I first tried to wrangle the lifetimes of an owned string against a given string, to no avail. Luckily I remembered that handy `std::borrow::Cow`, and rewriting the snippet with it (and std::convert::From) was a piece of cake. I even created a `fn` to make it reusable, thus I now have officially written my first lifetime annotation on a method. [*ACHIEVEMENT UNLOCKED*] For the interested, the function is: pub fn snippet&lt;'a&gt;(cx: &amp;Context, span: Span, default: &amp;'a str) -&gt; Cow&lt;'a, str&gt; { cx.sess().codemap().span_to_snippet(span).map(From::from).unwrap_or(Cow::Borrowed(default)) }
Ok, I think I was just thinking of these two tweets of @searls': https://twitter.com/searls/statuses/570612807244357633 https://twitter.com/searls/statuses/570612947455754241
Yes, that's correct.
Awesome! Worked very well. Thank you.
You still get crazy debugging output if you use said template library slightly wrong...
&gt; Most of the professional C++ programmers I know first learned C++98, and are still writing C++98 at their jobs to this day Lucky you! Most of the professional C++ I know are writing C with classes :(
Full ack! An IDE could: * Show types + lifetimes of everything - including inferred types + elided lifetimes * Convert fn to closure or (without upvals) vice versa * Create method stubs from call site * (Semi-)Auto-import * quick fix for common pitfalls (think clippy, but with automatic fix) * show rustdoc for anything * ... and more
You have to put in balance the cost of using C++ though: - what is the cost of security vulnerabilities? - what is the cost of tracking and fixing those software crashes? - what is the cost of tracking and fixing those data races? C++ has an initial cost of use throughout the lifecycle of software. Many newcomers struggle with very hard to diagnose errors right on their development machines, which they fix by nilly-willy poking at the software until it appears to work... only to break in the test environment. They don't know all those rules C++ has (who does), they are not capable to immediately identify any violation of the rules they do know by looking at the code (who can), and their company has not invested in the necessary tooling to help them out (static analyzers, ASan, MemSan, TSan, valgrind, gdb, ...) or has not trained them into efficiently using those tools. I've personally met 5 years old C++ veterans who had never used gdb before at my company and were just despairing because the software had crashed in production and all they get is that massive memory-dump file which they do not know what to do with... Thus, given the cost of using C++, and given that switching from C++ to Rust is relatively easy (as Rust only formalizes the already existing ownership/borrowing requirements that C++ developers need to intuit), I think that C++ companies have a vested interest in switching... ... however this switch will NOT occur overnight. Most likely, some small tasks will start in Rust to evaluate the language, producing some tools or non-critical components, and depending on the feedback things will move on or not, with a gradual transition. 
Hell, I just want proper aligned_alloc in rust stable.
I was just about to write such a blog post (titled Holy std::borrow::Cow!) then decided it would be too short and deleted the few lines I had so far. If there is real interest, I may decide to write it anew if I find the time.
I usually `const` all the things unless I really need to modify something. I even sometimes write `const * const int` in my function arguments just to be sure, though it seems somewhat excessive. ;-)
&gt; [1] Though last I heard the Rust compiler is still lagging behind Go slightly. rustc is lagging behind Go in compile-time performance, but the run-time performance of Rust programs should be on par with C programs, if identical algorithms are used, while Go will lag behind.
Rust aims at supporting Garbage Collections through libraries, like it supports Reference-Counting through `Rc&lt;T&gt;` and `Arc&lt;T&gt;` today (and you could create your own). That being said, it is likely that Swift's Reference Counting beats Rust's performance wise if the compiler manages to elide increment/decrement. From what I've heard of Swift performance though, the slight gain on Reference Counting might be lost in other parts.
Can't you avoid that in rust (in many but not all cases) with `impl&lt;T, U&gt; Trait&lt;T, U&gt; for FnMut(T) -&gt; U { fn method(&amp;mut self, t: T) -&gt; U { self.apply(t) } }` Edit: Also, it's unavoidable in java if the interface has more than one method that needs to be implemented.
I *do* use it just about everywhere (and have a save action to insert it anywhere applicable activated in my IDE). My code may feel a bit more verbose at times, but is very easy to read.
True, but before lambdas, we used it for single methods, too, and those tend to crop up quite a lot.
Right... Sorry, I didn't mean to launch you into a rant! I was just baffled at `grep` using C++ and GTK. :-) (I don't really disagree with anything you said. But then again, I really don't have many strong opinions about the topic *in general*.)
[Done](https://llogiq.github.io/2015/07/09/cow.html).
I don't see it being more complicated than C++, especially its latest iterations. And C++ is not a stranger in big companies.
Here's the thing that touches me about "I believe in people": It's not about 'dumbing down' or not, it's about how you respect the learner. I have been programming again for a year now. Before that, I hadn't touched anything more than a few trivial BASIC and Python programs in nearly a decade and a half. I've essentially had to relearn programming all over again, and because of the way my brain works, I've dabbled in a *lot* of languages and dealt with a lot of different approaches to learning a language in order to do that. I've done everything from interactive tutorials, to koans, to books, to good old-fashioned "type in the code from the guide" stuff, in no less than half a dozen languages. Something I think Rust gets right, the community, the book, even the compiler, is that it does believe in you, but it also doesn't expect you to just "get it" overnight. There is a tone to the literature, to the compiler messages, to the IRC channel, which is this: "we understand." Rust is a language that has taken some of the best ideas of programming, and added on top of them some very clever ideas of its own, ideas that take some thinking to get your head around. And it feels to me, like everyone knows this, and *hasn't forgotten that.* The single hardest thing about teaching anything is understanding just exactly how much you've internalized and forgotten that you even know. Perhaps this is simply because it's a young language yet, and to some extent even the team developing Rust itself is still sort of learning what Rust is and is to be. But I don't think it's just that. I've yet to feel stupid programming in Rust. I've yet to feel like "oh, dummy, you should've known that." I've yet to feel embarrassed for not knowing something or not puzzling it out on my own. When I read the books, the tone that comes across is "Yeah, this is a little tricky, but don't worry if you don't get it the first time. You'll get it in time." When I ask in the channel, I get prompt and helpful answers, not just to my question, but often to what it means and why. As if there's an acknowledgement that every such question is an opportunity to teach, an expected event, instead of a mutual nuisance to be exchanged in the hopes someone else will return the favor later. There's a collective impression that programming is challenging, the ideas Rust presents are something a bit new, and everyone's gone through their share of puzzling through how it works. No one's going to judge you in the slightest for not getting it the first time, but everyone believes you can do it, and wants to help you get to that moment that is by now already probably familiar to many Rust programmers where the little light bulb goes on and you go "oh! That makes perfect sense. Thanks." Part of truly believing in someone is helping them see through their failures to the successes they're capable of. I hope this community never loses that, because it's something special.
&gt; D has function overloading, Rust doesn't. Why not by the way? Is there some discussion about it? Function overloading is a very useful thing in general.
Any language with deterministic memory management (e.g. ref-counting, unique pointers) can opt into RAII. So both Rust, Swift and C++ can do it. Languages which don't have deterministic memory management (e.g. Go, Java, C#) where memory is freed long after a variable exits scope, need a mechanism to close resources, particularly after an error. Hence Java's try-finally construct and Go's defer statement. Swift supports both RAII (since it's ref-counted) and also has a defer statement. So you could do either, but the latter is definitely the "normal way". C++ and Rust provide no alternative to RAII. So Rust doesn't enforce it so much as offer no (straightforward) alternative.
What would the C++ equivalent of the snippet look like?
Explicit SIMD support is awesome as well, although I would like something like ispc in the form of a macro or plugin. Based on the response here it looks like I should start trying this out, so are their any recommended resources for macros and compiler plugins? I'm curious if this would even really work as a macro or if it should really be a compiler plugin.
Which main allocation function do you mean? Box (which is the main standard-library equivalent to malloc) doesn't let you ask for an arbitrary alignment. And heap is unstable and thus unusable on stable.
&gt; I happened upon a source line in clippy I wasn’t quite happy with, because it cloned a string. The problem I often have with `Cow` is that I don't know how to make a borrowed `String` from `&amp;'static str`. Is it at all possible? let zero_copy: Cow&lt;String&gt; = Cow::Borrowed (something? ("foo")); P.S. An expanded use case: http://is.gd/nzjPsH. I'm talking about `as_string`-like functionality of turning a `&amp;'static str` into a `Cow&lt;String&gt;` without allocation.
yes, heap::allocate.
Yes, true.
I'm not sure Rust is a good choice as a first programming language.
 pub enum Cow&lt;'a, B&gt; where B: 'a + ToOwned + ?Sized { Borrowed(&amp;'a B), Owned(B::Owned), } You're using the wrong type parameter.
That's the one I was thinking of :P
Running directly on the kernel is really nice when working with docker containers, but I think the ability to statically link musl provides 98% of the benefits with 2% of the effort.
Yes, if your rust is configured with jemalloc, you can just declare `je_mallocx` as an external fn and call it.
Please don't do this... :( It's possible, but the names of symbols within the compiler/stdlib internals definitely aren't stable, so doing this will likely cause upgrading pain down the road when they change.
I really hope so, because I think Rust is a fantastic language for learning systems programming. It may not subsume C, but (with the exception of game development courses) I see no reason why any course currently using C++ wouldn't benefit from using Rust instead (because let's face it, if a university cares about teaching students languages that will get them jobs, they'll just be using Java).
Ah, I see! So you need `both` the type and the value. I'll start from the easy ones: If `MPI_INT` is a macro that evaluates to a literal or an enum variant, that's easy, that can be imported, provided the type is supported by `c_import`. Importing types is not supported at the moment but could be done as well relatively easily as long as the type can be matched with a type in Rust. Importing structures as well as variables of struct types is more complicated, I can't promise anything here. The plugin would probably need to do something similar to what bindgen already does. I'd have to see what the LLVM IR representation of such a variable looks like. 
So it's the implementation that makes ne a provided method? And this implementation is an overridable default? (In other words, a 'Provided Method' is a trait method with a default implementation?) A search for "rust provided method" turned up nothing, so I felt the question needed to be asked. :)
Thanks so much :)
I can assure you that the cognitive overhead in writing it was much smaller than on writing the blog post. `Cow` is awesome, *because* it lets me do this stuff without thinking too much about the lifetime – if it's `Owned`, it's owned, otherwise it has the lifetime of the `Borrowed` argument.
I don't think the difference is that big with modern implementations of reference countings; see for example http://users.cecs.anu.edu.au/~steveb/downloads/pdf/rcix-oopsla-2013.pdf
In everyday talk they're always called default methods (i.e. trait methods with provided default implementations). And it is overridable yes.
Yeah, this is a bit of a jargon from the "trait" concept as it's been developed in multiple languages over the years. To quote https://en.wikipedia.org/wiki/Trait_(computer_programming) : **Traits have the following properties** * Provides a set of methods that implement behaviour. * Requires a set of methods that parameterize the provided behaviour. The provides/requires terms just refer to the methods that have (default) implementations in-the-trait vs. the methods that a trait implementor has to write themselves.
I expected to learn a bit about what Cow is doing there but I’m left wondering after reading. Sure I could check out the docs but I’d expect the post to detail a bit the API it’s centered on.
Thanks for the feedback, I will add a bit on this.
They're often called 'default methods.' The 'provided methods' terminology can be useful sometimes to avoid ambiguity, because Rust also has 'default impls', which are a very different concept.
I don't know, I don't think it's that much worse than other languages. It's not the easiest to get into, but for someone that wants to go deep into programming it makes sense. The thing is that there aren't any real beginner (as in knows nothing of programming) guides for Rust.
&gt; And a game not exactly known for its performance. Granted. Games today also use a ton of memory. It doesn't matter because you have a ton of memory. &gt; Rust had green threads, and Rust's design doesn't preclude getting them again (or having them created in a separate library). Go's API is simplified by only needing to support one use case. &gt; (I'm not sure he understants what memory management is about, this section seems totally random and doesn't make sense at all) The basic idea is an O(1) operation on top of an abstraction which is O(n) is O(n). (or whatever, this is an example) Since we write software on top of complex systems, those complex systems may involve intelligence we don't account for. In particular an operating system both manages memory and has a task scheduler. &gt; What? If anything, Go is an "additional smart layer", Rust is close to the metal. Agreed. I was not claiming Go is close to the metal. I'm claiming that "close to the metal" is often something of an illusion. Sometimes writing stupid code is better than smart code, because the layer below you is already smart and makes stupid code fast. When you try to work around that you actually get slower code. I am a web developer by trade. Years ago I spent a lot of time optimizing sites for IE6. All of that knowledge is now useless as V8 magically transforms crappy Javascript code and makes it fast. If I try to use those tricks I actually end up with slower code. &gt; There's some merit to the point that Rust and Go don't compete, but this article doesn't make the case well at all. I am claiming they do compete. I think a lot of people read into this article things that weren't there and a zealotry that wasn't intended. I think Rust is a great language and hope to take a deeper look at it in the future. I just think Go has more potential than people realize for areas it hasn't targeted and like it's focus on simplicity over control. But I completely understand that other people see things differently.
Look into downloading the entirety of stackoverflow to an external drive. Edit: ok I was joking but apparently [this](https://archive.org/details/stackexchange) exists.
Throw a fully working repl in there while you're at it. All this should be based on compiler-as-a-service features done right. Rust could leapfrog a lot of languages with that.
Strings are mutable, so they need to be constructed at runtime to give owners their own copy. Of course, there could be syntax for this, but it would just be syntax that obscures what is going on.
Because it's also a painful thing. It introduces uncertainty as to what any given function *actually does* when you see it (there can always be *another* overload hiding somewhere). It's also a huge pain for things like functional use; if you refer to a function `foo`, *which* `foo` do you mean? The one with one argument? Two arguments? Three arguments? Three arguments where the second is a string? Three where the second isn't a string but can be converted to one? Three where it does something totally unrelated, but happens to have the same name? I mean, yeah, it's nice, but I can totally understand why someone would decide to just avoid it in a language. :)
There is no restriction on borrowing a field based on named or annonymous. What is the exact error message? :)
&gt; Compiling project_zed v0.1.0 (file:///C:/Users/***/workspace/ProjectZedGFX) &gt; src\main.rs:203:22: 203:41 error: cannot borrow immutable anonymous field as mutable &gt; src\main.rs:203 Some(ref mut render_data) =&gt; { &gt; ^~~~~~~~~~~~~~~~~~~ &gt; note: in expansion of for loop expansion &gt; src\main.rs:201:9: 215:10 note: expansion site &gt; error: aborting due to previous error &gt; Could not compile `project_zed`. &gt; &gt; To learn more, run the command again with --verbose.
Ah, the important part here is "can not borrow immutable ... as mutable" You can only create a mutable reference into things you have mutable access to. Here it seems `render_data` is not mutable.
entity_manager is mutable. If I'm correct, that means its fields should be mutable. (you can't see it in the example I gave you, but in the code, entity_manager is mutable. Any idea what I'm doing wrong?
If anyone is interested in seeing what I've done in the proper context, here is a repository: https://github.com/Plasticcaz/ProjectZed Please note that this is a personal project, and I am a bit of a noob both in Rust and Graphics programming. Expect mistakes, and inefficiencies. Because this is a personal project, I will probably not accept Pull requests... Feel free to give me feedback though.
This is working fine: http://is.gd/o2yO0V With a `Vec&lt;Option&gt; instead: http://is.gd/C9Wtrv EDIT: Shorter version: http://is.gd/7vaBSt 
Thanks! I had no idea the get_mut() method existed! I'm going to try and fix this now...
IIRC there used to be a syntax for that, namely `@"Text"`. It was removed because everybody "overused" it, because typing `@` was so easy, diminishing the advantage of Rust - minimizing the number of allocations. Owned strings should be used as few as possible. As a side note, there are several ways to construct an owned string, and in my opinion, `.to_string()` is not ideal. - "Text".into() - The shortest! However if the targeted type cannot be inferred this cannot be used, so... - "Text".to_owned() - The second shortest. And... - "Text".to_string() - This is a full character longer than `.to_owned()`! Never use it! :P - String::from("Text") - Some people like this form because it feels more "natural". - From::from("Text") - This is two letters shorter, but has the same problem with `.into()`. Actually, they are the same method internally. I personally use `.into()` primarily, and only when the compiler complains about it, use `.to_owned()` instead.
It was `~""text`, though I think technically `@"text"` existed as well. `~str` is `String` nowadays.
&gt; A better solution: Actually stabilize the low level APIs. This is what we're doing, but it takes time.
Ah, didn't realize its a field of entity_manager (code is layed out weird on mobile). The issue in that case is that you are accidentially shared-borrowing it by iterating over a field of it. Try creating the borrows to the two fields explicitly
*deletes entire response* Oh, "What", not "Why". Oops. I'm not entirely certain in terms of "diff from current" because I just gave up trying to work out under what circumstances Cargo would or would not hit the network (I assume there's some combination of invalidation conditions and freshness). But off the top of my head (and if any of these are already in, then prefix that entry with "more documentation about how"): * I should be able to work 100% offline without fear of a package becoming uncompilable until internet access is available. If the internet's down, I can't really *do* much with Rust aside from gently poking at existing code and hope I don't trigger whatever causes Cargo to decide it has to hit the network. * I should be able to cache packages so that if I create a completely new package that depends on something in cache, I won't need internet access. If I split an existing crate into smaller crates, everything should already be available. * I'd *really like* to have binary outputs cached to cut compile times down. 
&gt; Granted. Games today also use a ton of memory. It doesn't matter because you have a ton of memory. Except that games like Assassin's Creed, GTA V, etc take up most of your memory; writing them in a less performant language will make you run out. (And it's not just memory that's the issue, CPU is too -- I said performance, not memory) There are plenty of applications where memory is an issue. It's disingenuous to dismiss memory inefficiency "because we have a ton of memory" when we have people complaining left and right about the amount of memory Chrome eats up, or the fact that many games need high-end PCs to be played well. Minecraft is a comparatively simple game which still doesn't perform well. If anything, it makes the statement that pure Java is not always right for games. Sure, many indie games and other small games could be written in such languages, but that doesn't make it possible for all. &gt; Go's API is simplified by only needing to support one use case. So? Rust could get green threads pretty easily if you put forth a single use case. I think the main issue with green threads in Rust is that folks haven't decided what kind of library they want. The API was pretty simple when we had it, the API should still be pretty simple if we get it again. &gt; The basic idea is an O(1) operation on top of an abstraction which is O(n) is O(n). (or whatever, this is an example) Since we write software on top of complex systems, those complex systems may involve intelligence we don't account for. In particular an operating system both manages memory and has a task scheduler. The memory management of an operating system is not the same as the memory management Rust talks about. Totally different goals, and they manage totally different things. &gt; I'm claiming that "close to the metal" is often something of an illusion. Sometimes writing stupid code is better than smart code, because the layer below you is already smart and makes stupid code fast. One major feature of systems programming is that generally you understand these underlying systems to some degree and know when they can trip you up. Not everyone can just dismiss it as "it's too complicated, let's ignore it". You're also making a false equivalence here. The issues caused by optimizations are not the same as the gains achieved from using _a language that is inherently more performant_ for its domain. Rust is indeed closer to the metal. It may not always be as close to the metal as you think, but there's no denying that it's close to the metal. This is not about stupid code vs smart code. This is about the stuff going on behind your code that is generated by the compiler, and how different languages/compilers do it differently. Big difference. &gt; I am claiming they do compete. Sorry, typo :p &gt; I think a lot of people read into this article things that weren't there and a zealotry that wasn't intended. There's a lot of (possibly unintentional) misrepresentation of Rust, misrepresentation of systems programming, and dismissal of the needs of systems programmers / browser developers / game developers. That's probably why. 
Please learn to use Rust first, before creating any practical projects. Then http://doc.rust-lang.org/std/net/ is maybe enough to migrate your winsock2 based project.
1. I just dropped into a Cargo package I haven't touched in a few days and did `cargo build`. It updated the registry and then proceeded to re-download and build the dependencies. I have *no* idea why. This is kind of what I mean: I don't know if it was *supposed* to do that or not. Having already triggered it *before* thinking to remove the network cable, though, I now can't work out how to trigger it again *without* the network cable plugged in, so I can't double check what happens if it fails. 2. I suppose, in my head, I'd *like* to think of the package system as a big, immutable filesystem that Cargo will do its best to keep up-to-date, but if it isn't, big deal: it's not like any published package can *change*, so the worst thing that happens is I use a slightly out-of-date version for a bit. 3. I mean, if I depend on regex in one package, compile it, check out a totally different package that *also* depends on regex, and compile *that*, it shouldn't need to recompile regex (subject to flags, etc. being the same, obviously). Incidentally, I'm not unaware that this is not exactly trivial... doesn't make me want it any less, though. :)
I keep repeating that the compiler usually lets us borrow stuff pretty freely, and it's mostly tied to the `Context` and its numerous side tables. In a way, this is to be expected. The different passes all have different access patterns, so the context and side tables are equipped to allow any of them. In some places, e.g. the `CodeMap::with_expn_info(…)` method, closures are used to tie some function into a shorter argument lifetime, and I suspect we'd encounter the 'lifetime 'madness 'soon should we try to borrow something to return from the closure.
Rust has no stable ABI, so technically, you cannot be sure that it would actually work. You have to compile with the exact same compiler.
filter_map (and every(? pretty sure) Iterator adaptor) is lazy and non-allocating. 
It just ignores the `None`s. `Iterator`s are great because they don't have to allocate a new collection at each step. If you want a `Vec` or some other collection at some point, you can call `collect` on the `Iterator`, but since you're using it in a for loop here, you don't have to.
Ahhh right, it's late. Yeah, it shouldn't have to hit the network, it should just re-build those things. Do you use multirust? I wonder if it possibly does something to accidentally invalidate the cache.
Nope, I'm a filthy Windows plebian; no multirust for me. :P
A better approach might be to use traits. I don't have time to compile this code right now, and I'm at a similar level to you, so forgive any grammar issues [so long as the general idea works!] :) trait Renderable { fn render(&amp;self, surface: &amp;RenderTarget) -&gt; Result&lt;_, RenderError&gt;; } struct X; struct Y; struct Z; impl Renderable for X { ... } impl Renderable for Y { ... } let all_renderables = vec&lt;&amp;Renderable&gt;::new(); all_renderables.push(some_X); // ok all_renderables.push(some_Y); // ok all_renderables.push(some_Z); // error fn render(renderables: vec&lt;&amp;Renderable&gt;) -&gt; Result&lt;_, RenderError&gt; { ... } render(all_renderables); // OK Also, note the existence of Typeable: https://crates.io/crates/typeable 
Someday we'll get you one too :)
Despite what people might say learning Haskell has been a valuable experience for me as a primarily C/Python/Ruby and now Rust programmer. Rust makes you modularise your program in a certain why by way of the borrow checker and ownership semantics making certain bad behaviors quite painful. Haskell does the same thing with it's purity. It forces you to write better seperated code by forcing IO out of the main code of your program and into the IO monad where it's clearly visible. Learning Haskell will make you a better programmer, just don't complain when the Kool-Aid sets in and you start wishing more languages were like Haskell. This gets much worse when you discover really powerful abstractions in Haskell not available anywhere else and awesome libraries like pipes, lens etc.
Eh? `Cow::from` doesn't help: http://is.gd/nzjPsH What I need is something like http://doc.rust-lang.org/std/string/fn.as_string.html, only stable. I used `as_string`, but it was deprecated a few weeks back. So how can I do that without `as_string`?
I was going to suggest Haskell until i read your final paragraph. Haskell it's actually rather practical, as it has the biggest ecosystem of any language of the ML heritage, which matters a lot for practicality. And there's tons of stuff in Haskell that's applicable to Rust. They're a lot more closely related than people realize.
If you look at my code, I just use `Borrowed("foo")`. Note that there is a `Cow&lt;'a str&gt;` impl for `&amp;'static str`, so it should work regardless the lifetime. In general, look into std::from::From implementations, they usually have what you need.
What I missed and am missing still is that let bs: Cow&lt;str&gt; = Cow::from (format! ("{}", 123)); and let bs: Cow&lt;str&gt; = Cow::Owned (format! ("{}", 123)); would work. How does it work? There seem to be some magic here. I mean, how `String` becomes `str`? P.S. Looks like there's `IntoCow` implementation in `String` which turns it into `str`, but looking at the implementation - it's still the same magic. Is `String` secretly a `str`?
Assembler. 
Thanks, `Cow&lt;str&gt;` works!
&gt; Is `String` secretly a `str`? No. A `str` is a ~~borrowed~~ (Edit: No, Artemciy is right) *slice of* a `String`. And `IntoCow` will convert a `String` to a `Cow::Borrowed(String)` – the `Cow` just wraps the (owned) String, thus taking ownership (and from then on being responsible for dropping the String when going out of scope, btw.). An `IntoCow` of a string slice will just take a borrowed pointer, whose lifetime ends with the `Cow`s lifetime. I may add this to the blog post, because it seems a good question. Are you ok with me quoting you?
I still don't get how `Cow&lt;str&gt;` owns a `String`. (Well, I have a theory, but I'd like to hear it from the pros. Also, it should be docummented somewhere). &gt; No. A str is a borrowed slice of a String. No, I think you're wrong. ; ) `&amp;str` is a borrowed slice. `str` is just a UTF-8 `[u8]`. cf. http://doc.rust-lang.org/std/primitive.str.html#representation &gt; And `IntoCow` will convert a `String` to a `Cow::Borrowed(String)` You sure? That doesn't seem obvious to me from impl IntoCow&lt;'static, str&gt; for String fn into_cow(self) -&gt; Cow&lt;'static, str&gt; IntoCow is supposed to *move* stuff, it doesn't *borrow*. I think it somehow *moves* `String` into `str`. How? That's the question. &gt; I may add this to the blog post, because it seems a good question. Are you ok with me quoting you? Yeah, but try to adress the real question then! This code: let bs: Cow&lt;str&gt; = Cow::Owned (String::from ("123")); doesn't *borrow* a string slice! It *owns* a `String`! And it's doing it with just a `str`. What gives?
I didn't so much missed it. I looked at it closely, but that associated type trick went way over my head. Also, I remember actually looking for implementation in order to see what the associated type in the `ToOwned` would be (it isn't in the docs!), but haven't found it in the immediate vicinity. (Now I have finally found it in rust/src/libcollections/str.rs) And... I'm still baffled! impl ToOwned for str { type Owned = String; fn to_owned(&amp;self) -&gt; String { unsafe { String::from_utf8_unchecked(self.as_bytes().to_owned()) } } } This allows `str` to be converted to `String`, which is nice. But that doesn't explain to me *how* `String` *became* `str`! let bs: Cow&lt;str&gt; = Cow::Owned (String::from ("123")); `String` and `str` are two different types with different memory representation. There's noting in the docs that tells me as a programmer that I can simple squeeze one into another. Yet this seems to be what's happening, because `Cow&lt;str&gt;` is allocated in the stack memory to keep a `str`. A `str` which *was*, just a moment ago, a `String`! P.S. You see now why I wouldn't normally consider storing `String` as `Cow&lt;str&gt;`? --- P.S. Ah, okay, I think I've got it. So `Cow&lt;str&gt;` is actually storing a `String` under the hood, because that's what `ToOwned::Owned` points to. Yeah, makes total sense now. P.S. Please blog, because the documentation is sorely lacking there (`Cow`'s documentation says nothing about `str` tricks. `str` documentation says nothing about `Cow`. We have a blind spot there).
Ok, I agree that this is a bit more complex than even I anticipated. Will blog, but not immediately – I need some time to think up a readable format. This will probably be a Holy std::borrow::Cow! Redux. Btw. you made me wonder what str actually is: According to the code, it is a real *primitive* type (which means the compiler just knows what it is). Under the hood (which can be seen by the `mem::transmute` call in `String::from_utf8_unchecked`), it's just a sequence of bytes that is guaranteed to be valid UTF-8. A `String` then is an *owned* sequence of bytes that is guaranteed to be valid UTF-8.
: ) P.S. The cow has two horns, a piece of cheese and a ticket.
Oh right, sorry!
Or more accurately: A "String" is a smart pointer that points to sequence of bytes (in heap) that is guaranteed to be valid UTF-8, and owns the memory it points.
It is not about making binary absolutely without deps (even kernel), because it is stupid and impossible. It is about making binary static in sense that it has no additional runtime libs on given kernel. I.e. I am building deployment tool and I want it to be "installable" via `wget` and nothing else. I could build it against musl, but I was wandering if was there any work/ideas to make Rust libc independent - like Go.
There is none: &gt; Rust: the final frontier. These are the voyages of the browser Servo. Its five-year mission: to explore strange new worlds, to seek out new life and new civilizations, to boldly go where no man has gone before.
My personal opinion would be asm is an immensely important language to learn, just to get actual machine abstraction exposed by the hardware vendors. Then, you start to see the links between what you write and the likely better, faster program the compiler writes for you. Basically, fiedzia's suggestion fulfils all criteria.
Try [ATS](http://www.ats-lang.org/). It's a systems programming language with dependent types and linear types. Like Rust it has no GC. Your experience in Rust will make parts of it faster to pick up (linear types) but you'll see how adding dependent types can allow making some things safer. You can do safe pointer arithmetic in ATS for example. I [have some articles](http://bluishcoder.co.nz/tags/ats/index.html) on how these features can be used for safer interfacing to C programs if you want to get a taste. Note that those articles are ATS1 and the latest version is ATS2. They are still relevant though - most changes are in the library which I don't make a lot of use of. If you want to try something a little different, maybe [Pony](http://ponylang.org/). It has an actor based concurrency system and has some interesting use of [capabilities](http://tutorial.ponylang.org/capabilities/object-capabilities/). If you want to go for Haskell I recommend getting and working through [Parallel and Concurrent Programming in Haskell](http://chimera.labs.oreilly.com/books/1230000000929). It's available online too. It's a great book on dealing with concurrent programming even if you eventually decide not to continue with Haskell.
You are officially my favorite redditor of the week :-)
Assembler is a solid choice. The one thing it did for me was to push me into seeing computers as just numbers. If you write assembly for long enough (as in, maybe 3 minutes) you'll start to notice a lot of redundancies and say to yourself, "I could write a programming language that just gets rid of this redundancy but keeps the power of assembly!" and then you realize that it's already been done, it's a language called C. Plus it helps you learn how the machine works and blah blah blah
Like you and everyone else said, Haskell is the obvious choice. Haskell is by-far the best non-systems language around. Besides that, Racket is also worth learning. Lisp is still the only language where you can use metaprogramming without friction. Lua, bash, and x86 are also very worthwhile.
`memcmp` isn't that much different http://git.musl-libc.org/cgit/musl/tree/src/string/memcmp.c porting other 3 functions shouldn't be hard.
It's easy to provide the basic functionality, but the complexity built up in `glibc` around these functions is there for a reason. So you lose something if you switch to simpler implementations. I'm sure it's fine for many applications, but not all.
When I read "let foo = vec![1, 2, 3]; let bar = variable2;", I don't understand why author thinks "`foo` can't be accessed further". How does variable2 have a relationship with foo 
&gt; ween str and String is that an str could conceivably reside on the stack, whereas String is always heap-allocated, right? I think that might need still some more clarification: "str" could reside anywhere, it's just an unsized sequence of UTF-8-encoded bytes. However, because str as itself is unsized, it's (currently) hard to place anywhere else but in heap. String literals are str's too, and they are placed in static area by compiler as far as I know, and that's why you can access them as &amp;str's only, i.e. you're not allowed to own them, because they reside in static memory and you can't own static memory. "String" is a smart pointer that often, but not necessarily, resides (the pointer itself resides) in stack and always points to a str that resides in heap. Basically it's a fat pointer, consisting of the address of the str it points, its size (the size of the actual UTF-8 data) and its capacity (the size of the allocated piece of memory that contains the data), so the str and its memory that String owns, is always heap-allocated. "&amp;str", too, is a pointer that points to str. It contains the pointer to str and the size of the pointed str, but like references in general, it doesn't own the memory it points to. This is basically just a problem of language and communication, but I'm not sure if we should say that "String is always heap-allocated.", because a "String object" consists of two parts, the String fat pointer, that can be allocated anywhere, and the str it owns and points to, which is always heap-allocated.
If the error output hyperlinked to 'a' relevant portion of the manual (or even a stand alone document specifying the pitfalls and examples in-depth) on supported terminals... well, it would be pretty nice for the new user.
Never mind. I'm having a hard time following your line of reasoning, and don't have the energy to work it out further. It was fun discussing with you, thanks.
So, I almost wrote a little Cow class to do just this. It would be a `string*` and a `bool` telling us if we have to call `delete` on it in the Cow destructor. Not that bad, but I decided against it. Also this doesn't help protect against borrowing short lived values.
Wow thanks! I'll keep an eye on this, it would simplify this a lot. The consequences of how things are at the moment is that we need a wrapper per implementation, and this has to be considered by any "higher level" abstraction over MPI, making these painful to write. Anything that makes this easier would be greatly welcomed!
Would be cool to see this as a library for others to reuse
Sure, that's planned. Wrapping the code as a library will be covered in the final part of the series (I guess it would be part 3).
Nah, this is not a real problem, just a thought experiment. I've never even coded rust yet. Or will until there is a sophisticated cross platform GUI for it ;)
I've heard good things about http://rayseyfarth.com/asm/index.html, and http://asmtutor.com/ looks useful. Lots of people still recommend http://www.ic.unicamp.br/~pannain/mc404/aulas/pdfs/Art%20Of%20Intel%20x86%20Assembly.pdf despite its age, too.
`@F` was supposed to mean `Garbage Collected`, but of course, since Rust never had a proper GC, it was implemented as reference counting. `~F`was basically `Box&lt;F&gt;`. But yes, being sigils, they were overused and noisy.
I'd recommend Clojure. It's fairly practical, can run both in the browser and on the server, and it's fairly "small", you can get a handle on the data structures and syntax quickly. And as a "learning" language, historically nothing has a bigger impact on your brain and the way you think about code and data than LISP. No, it's not going to show you the wonders of a serious type system (you'd want Scala for that, and I'm not sure that's a wise choice), but LISP-style macros are a transcendental experience.
I'm just gonna reply to myself with further comments. &gt; we’ve started the event loop successfully, although at the moment it does nothing useful for us. Let’s fix that. You might want to explicitly mention control-c or something here, to stop it from running. So far, this tutorial has been _very_ explicit, so calling that out here would seem like the right thing to do.
 let address = std::str::FromStr::from_str("0.0.0.0:10000").unwrap(); This should be let address = "0.0.0.0.0:10000".from(); but I'm not sure the right `impl`s exist. `FromStr` is an older trait from before `From` existed, the more general form is preferred now. EDIT: this comment https://www.reddit.com/r/programming/comments/3csjza/rust_in_detail_writing_scalable_chat_service_from/csynv5p has a better way that works for sure. EDIT2: This is this way because `From` is for things that always succeed, and this might not!
So you're saying that only the John Carmack's of the world have managed to avoid being intellectually crippled by their education and lack of motivation? I realize you're attempting to be upbeat but I don't think the message you're spreading is as heartwarming as you think it is. Is education the silver bullet that, by itself, will give us an order of magnitude improvement as an industry?
&gt; Does this mean that the Weak pointer is just a borrow of the original data with the same lifetime as the owner of said data? The `Weak&lt;T&gt;` type has no lifetime parameter (ignoring `T` here). It's not a "borrow" in the sense that its lifetime is restricted. A `Weak&lt;T&gt;` is just another owner of the memory block and reference counters and keeps the memory allocated. But it does not keep the wrapped `T` value valid. The difference between an `Rc` (or `Arc`) and a `Weak` is that holding on to a `Weak` does not prevent dropping of the internal `T` value. And when the `T` is dropped while there is at least one `Weak` the reference counters would still be there. That's how a `Weak` can check at runtime whether there is still a valid `T` or not. &gt; how does the 'deallocation while in use' thing is prevented? You can't do a lot with a `Weak`. You have to turn it into an `Rc` (or `Arc`) first using `upgrade`. And that's either going to succeed (if the `T` is still there) or not. So, if the `T` has already been dropped, you won't be able to access its former memory location by mistake because in that case `upgrade` will give you a `None` instead of an `Rc`. &gt; And does it even try to address things like iterator invalidation like the rest of the language does? Since `Rc`, `Arc`, `Weak` means sharing (you can clone these handles), you only get to access the internal `T` via a shared reference `&amp;T`. And creating such a reference means borrowing one of the Rc/Arc handles, so such a `&amp;T` won't ever dangle. You can't really provoke iterator invalidation if you only got shared references to some container. If you want *mutation*, you would have to use Rc&lt;RefCell&lt;T&gt;&gt; // instead of Rc&lt;T&gt; Arc&lt;Mutex&lt;T&gt;&gt; // instead of Arc&lt;T&gt; `RefCell` and `Mutex` will allow you to get a `&amp;mut T` given a shared reference to the `RefCell` or `Mutex`. But these wrappers make sure that if you have such a `&amp;mut T` to access/mutate the `T` it will be the *only* reference to that `T`. So, all is fine. HTH
(you accidentally linked to your local docs)
I would love to know about a set of resources for learning assember, from scratch, today, if you, /u/kortez84 or /u/charliefg have any thoughts. /u/joshmatthews posted a few below...
This time, I investigate further what kind of beef our specific `Cow` is actually made of.
But Strings are mutable in Rust, right? They have the push/pop APIs. If you're probably don't want to mutate it or take ownership of it, then you're better off taking a &amp;str.
&gt; They have the push/pop APIs. Let's take a look at those signatures: * http://doc.rust-lang.org/stable/collections/string/struct.String.html#method.push * http://doc.rust-lang.org/stable/collections/string/struct.String.html#method.pop inline: fn push(&amp;mut self, ch: char) fn pop(&amp;mut self) -&gt; Option&lt;char&gt; These both take `&amp;mut self`. In other words, you need an `&amp;mut String` to call these methods. Put another way: let s = "foo".to_string(); s.push('a') // error: cannot borrow immutable local // variable `s` as mutable but let mut s = "foo".to_string(); s.push('a'); // works just fine! Remember: mutability is a property of a binding in Rust, not a type.
I would think so (well, that and copies followed by destruction that Rust would do as a move); however passing by reference and moving instead of cloning are human driven, so some potential instances may be missed.
I don't really know if that's a great analogy. How about child-safe medication bottles. They are safe(r) but a pain in the ass to use.
I'd recommend learning Scala and the amazing Akka/Play libraries. Haskell and Rust type systems are amazing, but at the moment I'd argue the Scala ecosystem is more powerful particularly for cloud/web development. Though I say this without trying Cloud Haskell first hand, but I get the impression they are trying to catch up to where Akka is.
Haskell is what you're looking for, but in the meantime I suggest looking at F# or Ocaml, AKA Haskell with training wheels. Haskell is an amazing vehicle for advanced CS concepts. Much of my imperative coding skill I owe to my Haskell experience. 
Not to mention that it lets you get some basic debugging without relying on source or symbols, which is handy- especially when debugging horrible memory corruption, etc. where the machine may not be executing your code as written anymore!
Thank you!
Oops, I will change this! Thank you for pointing out my mistake. I always appear to miss some subtlety. It's a wonder I am able to write working Rust code at all. :-) Edit: Fixed. I also linked your comment.
[PR #1191](https://github.com/rust-lang/rfcs/pull/1191) seems really interesting, as stabilizing procedural macros would be a major step forward in stabilizing the language and having external tools being able to rely on the parser would maybe allow Rust to avoid C++ fate where *everyone* tries to parse the language (and mostly fail...)
Sure, but that doesn't change that `String` itself can be in a mutable or non-mutable binding. :)
Safe rust doesn't and cannot prevent you all sorts of bugs, you can still make lots of errors. Logic errors (infinite loops), dead-locks, writing to /proc/$pid/mem, not to mention turning off "access control" via the unsafe keyword.
C++ is scary. I tried coding the same way I did in Rust and for some reason it copied my value so when I tried to modify it I ended up modifying the copy instead of the actual value. You don't need to new() things either, for some reason you can put things into containers that just got declared (???) Everything is implicitly done in a way that I don't understand because I am not a professional C++ dev.
Try Julia, its basically a lisp with nice syntax and superb performance
I have it on good authority that we stole a fair amount of impl details from Boost ;)
There's a small typo: It's Atomic Object, not Atmoic Object (near the end). Otherwise, cool slides.
I used to write a lot of assembly in my youth. I still code a bit of it sometimes to see if I can still do it. Funny thing is, everyone is so certain that assembly is super-hard: It isn't. Actually it's rather simple and most things are like programming Basic, only more vertical. Apart from this, it's always good to be able to *read* assembly, e.g. to check the validity of microbenchmarks. Forth is another great language – and a good way to learn both Forth and Assembly is [jonesforth](https://rwmj.wordpress.com/2010/08/07/jonesforth-git-repository/). It's a great read!
I'll go against the grain here and say that Haskell is not the best next language. I've dabbled in Haskell, and before playing with Rust, I thought it was absolutely the future. The "if it compiles it works" aspect was awesome, but I actually found rust was like that for me, too. Now I'm thinking it was the ADTs and pattern matching that are 95% responsible, and the gains from the more sophisticated type system (HKTs like Monad) are more dubious. I think your next language should be *Erlang* (or, even better, it's more beginner-friendly cousin [Elixir](http://elixir-lang.org/getting-started/introduction.html)). It's mind expanding like Rust, but its concepts and philosophy are much more orthogonal. Haskell's approach to correctness and robustness is "rust's, but moreso" (even stronger typing), while erlang takes it in a completely different direction, and I think it's valuable to understand both approaches. The Learn You Some Erlang book [puts it nicely](http://learnyousomeerlang.com/types-or-lack-thereof): &gt; One classic friction point between proponents of static and dynamic typing has to do with the safety of the software being written. A frequently suggested idea is that good static type systems with compilers enforcing them with fervor will catch most errors waiting to happen before you can even execute the code. As such, statically typed languages are to be seen as safer than their dynamic counterparts. While this might be true when comparing with many dynamic languages, Erlang begs to differ and certainly has a track record to prove it. The best example is the often reported nine nines (99.9999999%) of availability offered on the Ericsson AXD 301 ATM switches, consisting of over 1 million lines of Erlang code. Please note that this is not an indication that none of the components in an Erlang-based system failed, but that a general switch system was available 99.9999999% of the time, planned outages included. This is partially because Erlang is built on the notion that a failure in one of the components should not affect the whole system. Errors coming from the programmer, hardware failures or [some] network failures are accounted for: the language includes features which will allow you to distribute a program over to different nodes, handle unexpected errors, and never stop running. Admittedly, after haskell converted me to the notion of strongly (edit: statically) typed languages (and made me despise my day-to-day language ruby), I cringed at the idea of Erlang's dynamic types. And yet, there's the existence proof that it can work. And not only _can_ it work but it often does; erlang and its philsophy/framework OTP is renowned for robustness. Now I'm of two minds: programming in the small, sure, sophisticated static types are extremely beneficial, but in the large things _will_ break (even just at the hardware level) so you need robustness baked in. And if you're baking in robustness, maybe that can take care of type issues, too. But all that aside, erlang _does_ have static typing via Dialyzer. Erlang/Elixir are functional (immutable, lots of recursion) and have pattern matching. I'd say haskell is like rust (with a more sophisticated type system) combined with erlang. They also are fundamentally based on a concept I'm guessing will be increasingly important for programming: concurrency and distributed computing. I see Elixir+Rust as the combo of the next decade. They're complementary and cover many use cases from low level to high level. Haskell is neat, but I'm not _quite_ sure where it fits in the toolbelt. At some level, it can replace rust, maybe, but not quite so well for very low level stuff, or maybe it could replace Elixir, but again not quite so well for distributed server stuff. OTOH, my love of Haskell waxes and wanes. Sometimes I think it can replace both Elixir and Rust, but better in both cases! But then I get discouraged when I try to use it, so ¯\_(ツ)_/¯.
I'd second this suggestion. Scala is awesome. I'm actually considering going the opposite way; I already know Scala and family and am considering dabbling with Rust a bit.
* LGPL has no implications for statically linked binaries which aren't distributed, which is the case when you're deploying binaries. If you're distributing binaries, you probably want dynamic linking anyway. * Static linking only incorporates the parts of a library which are used by the program. No matter how bloated glibc is, it won't affect your binary. Only the size of the parts which you actually use affects your binary. * "buggy" is relative. I would never call glibc "buggy". And people keep talking about how you cannot be sure of the safety of things written in C. We're talking about *syscall* interfaces here... they are going to have to get wrapped in `unsafe` blocks in Rust anyway, so what's the point? We're not talking about using crap from `glibc` like `qsort()`, where the lack of safety guarantees actually matters. Now, if you wanted to do something that actually made sense, you might consider avoiding things like `gethostbyname()` or `getpwent()` from `glibc` because those things would probably be better implemented in Rust all the way down.
Well, it would at least require people to think a second about what they're doing and the interface that they're exposing for their module. A better option might be to restrict lifetime elision to cases where it makes the most general choice.
That first bullet point should probably be in the documentation of String and str. Deref coercion should probably be in the documentation of the Deref trait. Overall, this is quality information here eddyb and llogiq.
Here's a patch you can apply to make it run on stable. https://gist.github.com/tomjakubowski/05a67d83f8912a08e811 To apply, save that to a file inside your local clone of the git repository, then run `git am --ignore-space-change --ignore-whitespace &lt;filename&gt;`
&gt; Deref coercion should probably be in the documentation of the Deref trait. There's an open ticket for it. I wrote a whole chapter in the book, just gotta connect them up.
This is pretty good. Good job!
How many of F#'s libraries are pure though, vs needing to be wrapped first? Scala clearly has a lot of libraries to draw on via its Java interop (maybe even more than the .net ecosystem), but in practice there are few that are suitable out of the box for pure FP-style Scala. 
You can actually run http://rustbyexample.com/ locally via https://github.com/rust-lang/rust-by-example , if you have `nodejs` and `rust` installed even the dynamic compiler output will function just like the live site. Maybe even make some more explanations / problems and push them back up! BTW, I recommend [multirust](https://github.com/brson/multirust) for installing the rust compiler. Even better than homebrew. 
I'm getting a patch does not apply error. :(
Lets understand a bit of how this works. The idea is that in Rust things have an owner and those resources are guaranteed to be kept alive as long as the owner. In Rust you can represent shared references through various things such as `Rc` and `Arc`. Since a resource is shared by all the `Rc` the resource will not be freed until there's no reference left to it. What happens when you want a reference to a shared object, but don't want to own it. That is you don't want it to keep the object alive? You create a `Weak` reference to it. This reference doesn't give you access to the resource, it doesn't own it! Instead what it can do is try to [`upgrade`](https://doc.rust-lang.org/std/rc/struct.Weak.html#method.upgrade) itself into a full `Rc` or such which does own it. Notice that `upgrade` doesn't return the `Rc&lt;T&gt;`, instead it returns `Option&lt;Rc&lt;T&gt;&gt;`. This is because there's a chance that when you want to upgrade the weak reference it's already gone! Notice that all of this is done *at runtime*. `Rc` is not "smart" enough to decide when it can release the resource at compile time. If a `Gc` reference were added (as a library) it would use a similar method, were it would ignore weak references, but it'd allow weak references to try to upgrade to full references if the resource is still alive. Now iterator invalidation can't happen in the language. Because an iterator always borrows what you're iterating, you can't alter the contents of a thing while something is iterating it's containers. In the same way a mutable iterator would contain a mutable reference to the container it's iterating, so you couldn't read it while another iterator to the same container alters the resources.
Anything you can do to help reproduce would be great.
I'm very interested in this, but you don't have a readme or really much for me to have a look at other than code and some pretty appropriately named folders, which have admittedly given me a good idea of what these libraries do so kudos on that. Can you tell me more about the future plans for this project, what it currently is capable of, etc?
&gt; I tried coding the same way I did in Rust and for some reason it copied my value so when I tried to modify it I ended up modifying the copy instead of the actual value. I'm curious what you did here, because the two have basically the same always-copy semantics (except C++ has some by-reference semantics in there for fun, too).
Great explanation! You should write a blog. :-) Edit: Meanwhile I'll go and link this as bonus material.
There is no need to wrap. F# has full support for imperative programming, so you can just use the stuff normally. 
I can't compile it with nightly (`rustc 1.3.0-nightly (e6a9be10b 2015-07-07)`) either. Lots of `error: missing documentation for a constant `..,
You can easily do the same in any other language and actually Steam does the same on windows too. It just does not clear you home-directory when the path does not exist. But sometimes a 3 lines shell script can replace a 100 lines script, especially when you have to automatically find, process and rename a bunch of files.
There are so much more "shady doings" in the Rust standard library that it'll take an army of master investigators to find them all, and they'll still need months, if not years to uncover them. Perhaps we should make a ring blog series "RSI: Rust Special Investigators" where we uncover one "secret" each time. It's a lot of fun to write that stuff, and whoever wants to join is hereby cordially invited.
Very nice write-up! One quick question, at the end of the "Visibility" section: &gt; If you have a function like foo::bar::baz::rad() in your project and want to make it usable as foo::rad() add pub use foo::rad; to your foo module. This is called re-exporting. Is this correct? I thought you would want to add `pub use bar::baz::rad` to the `foo` module, but maybe I'm remembering wrong (I'm on my phone so I can't easily check).
There is a [rust_gamedev](https://www.reddit.com/r/rust_gamedev/) subreddit, you can cross-post there too.
To add to that, it's an unstable feature, written `impl Trait for .. {}` which impls the trait for every type *unless* that type has a negative impl - i.e. `impl !Trait for T {}` - or its a struct/enum which has a field which is `!Trait`. It's used for some marker traits, specifically `Send` and `Sync`. And it has nothing to do with default methods, in fact traits with methods - provided or otherwise - can't have default impls currently.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/rust_gamedev] [Rocket: a toy game using Piston \[x post from r/rust\]](https://np.reddit.com/r/rust_gamedev/comments/3cwban/rocket_a_toy_game_using_piston_x_post_from_rrust/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger/wiki/) ^/ ^[Contact](/message/compose/?to=\/r\/TotesMessenger))* [](#bot)
&gt;No Windows support exists yet. Bah humbug. If you need any help with the Windows implementation, just poke me on IRC, and don't forget to use my library.
I don't think this is a fair characterization of the current situation. The semantics of `with_capacity`, `reserve_exact` and `shrink_to_fit` are relatively set. Since they are provided *only* to precisely control allocation behavior, it would be quite weird if they silently did something else entirely.
I'm not sure about that code either (what does "it works" mean? what's the goal?), but... Basically, the way parameter passing in C++ works is: * If the parameter type is `T&amp;` or `const T&amp;`, it's like `&amp;T` in Rust, except the compiler automatically inserts both the referencing (`&amp;`) operation into the calling code, and the dereferencing (`*`) operations into the function body, to make it look the same as if it were by-value. (`T&amp;` in C++ isn't a first-class type like `&amp;T` in Rust, you can't store it in containers and such, rather `&amp;` is a "type qualifier".) * If the parameter type is *not* a reference (by-value), then normally, it'll insert a call the copy constructor, which is the equivalent of `clone()`. Starting from C++11, if the argument is a temporary and the type defines one, it'll instead insert a call to the move constructor, which is the equivalent of `swap()`/`take()`, so you can pass things like `vector` without incurring an allocation. There is no equivalent to Rust's by-move semantics which statically move an argument into the called function with just a bit-copy (except for types that are essentially `Copy`). In addition: * If you declare a variable of a type `T` without passing any arguments and `T` defines a default constructor, which is the equivalent of Rust's `Default::default()`, then it is invoked. This is all oversimplified and not technically precise, but it should give you the general idea. 
Try `let h = g / 2.0;` `2` is a genetic integer while `2.0` is a generic float. Don't ask me why they're two different things, I couldn't tell you. There was an RFC to make generic integers work with floats but for some reason it wasn't accepted. 
Easy: scope `loan` so it stops existing before you try to call `share`: struct Foo; impl Foo { fn mutate_and_share(&amp;mut self) -&gt; &amp;Self { &amp;*self } fn share(&amp;self) {} } fn main() { let mut foo = Foo; { let loan = foo.mutate_and_share(); } foo.share(); } Also, "even though this program doesn't strictly break reference rules" is a bad attitude to have. All that matters is what Rust can *prove* about your code, and in this case, it *can't* prove that the mutable borrow isn't still active. Whether that's *actually* the case or not is sort-of irrelevant.
Borrows last for the lexical scope in which they're located. This *is* how you constrain them.
Fair enough. Is there a way to something C++ish like make mutate...() take an immutable reference and cast away its immutability within an unsafe block? I know this really isn't a good idea, just curious if it's possible. 
Righty - based on that, I think I'll be staying away from those little tricks. Thank you for all the answers. 
I actually wonder how a `Rc` box discovers the weak references to it, in order to set them to null.
Ah, yes, you're right! My bad.
Ha... Woops. :( Thank you!
&gt; Note that both IpcSender&lt;T&gt; and IpcReceiver&lt;T&gt; implement Serialize and Deserialize, so you can send IPC channels over IPC channels freely, just as you can with Rust channels. If process `A` is communicating with `B` and sends its channel to `C`, can `C` communicate with `B`? What's the security implications of that, could `B` implement a security policy?
&gt; As we know from last time, our type was `Cow&lt;'a str&gt;` (for a given lifetime `'a`, which we took from our default argument). Now this doesn’t say anything about str or String at all? Where do we get the implementation from? I was confused how `str` had a lifetime parameter, but I think you meant `Cow&lt;'a, str&gt;`.
Just... No. If you want to use C++, use C++. Rust will not pretend to be something it isn't for you.
Thanks for the explanation. Probably a good thing I found out about this here instead of by trying to actually do it in a project.
:) It can take a bit of a shift in perspective, but at the same time, you spend _way_ less time in a debugger, or at least, compared to when I wrote a lot of C, which was admittedly a long time ago.
I mean, just put the missing comma.
Ah, now I see it. I even made the mistake twice. Will update.
Could even have some graphical representation of inferred lifetimes within a single function. You could select a borrow and see the region of the CFG that is inferred for the borrow's scope.
As I understand it, the guarantees Rust and C++ make in terms of destructors being called is very similar. Neither Rust nor C++ guarantee an object's destructor will be called. In C++ you could allocate a pointer and never free it. In Rust you could leak an RC pointer with a cycle. Rust does give you one additional guarantee that C++ doesn't. Even if a Rust destructor doesn't run, your program will still be memory safe. This does mean that unsafe code needs to uphold this invariant, which is what caused the problem with thread::scoped.
 fn walk_pet&lt;W: Walk&gt;(pet: &amp;mut W) { // Try setting `pet.walked` here! // You can't! pet.walk(); } You should also show the dynamic version, ` fn walk_pet (pet: &amp;mut Walk)`
Don't try and think about this from a C++ point of view. The entire situation is not something that can be expressed in C++ terms. Rust's destructor guarantees are better than C++. It's always been possible in both languages to "leak" data by popping it into an Rc cycle, or a global hashmap, or sending it to a blocked thread. The difference is that in Rust it's not supposed to be possible to do this with data that isn't `'static` (`'static` means that the type contains no borrowed data and is not attached to any scope). In the past, in Rust, one would have (or think they had) the guarantee that when the scoped tied to data with a lifetime is up, the object must be destroyed. After all, Rust will not let you access borrowed data when its scope is over. Turns out that the guarantee isn't so strict. It is possible for an object with a lifetime parameter to outlive that lifetime, _provided that_ the object is no longer accessible after this point. In other words, the object can be leaked out of its scope, skipping destructors, as long as it's no longer accessible. This requires some icky gymnastics with `Rc` and is very hard to trigger unintentionally, but it exists. So RAII works as usual. Just that _scoped_ guards which are supposed to not escape a scope are not guaranteed to have their destructors run within that scope. No other mainstream language has the concept of scoped objects; so this is not a problem that you can express in C++ terms.
filter_map will run the function on each element and discard it if the function returns `None`. Since we are filtering an `Option` this will filter out any option that is None outright. You can get even fancier and get rid of all the loops and use only iterators. Here's [a version that will also do error handling with iterators](http://is.gd/MPiBig) because your rendering function will probably fail sometimes. I've added documentation explaining most of it. Notice how the lines that handle getting the data (the first two of the grand iterator, and I'd probably merge those lines) are separated from the lines that actually do something (the `.map` line) and then those are separate of the lines handling the error cases (the last lines of the grand iterator). The nice feature of this is that you can alter those lines independent of one another (as long as the types match) so your `render` function just doesn't even care that some entities can be `None`, and your error handling is separate (though close) of what you actually do. That's the main benefit of doing things this way with loops you might have to call `breaks` and such that mix what you do, with how you handle when there's an error. You keep separate concerns, nicely separated. Also pay attention to the lines that handle errors in different manners. They have different consequences. The default will stop rendering if there's an error, while the other two will keep rendering even when an error occurs, but give you different results.
It would get shit because system-wide dependencies are fragile. In python, virtualenv's are ubiquitous for this reason.
What is the motivation for the move from inter-thread communication to IPC for servo?
Yes, this is unsafe, because while the code is executing, it could do something bad: access uninitilized memory. There's no way to do this directly without `unsafe`, the closest thing would be to write some function that uses `unsafe` internally to do the initialization, but presents a safe interface. It would end up looking like this. If you're using something like a `Vec`, then you can initilize it in safe code. But arrays don't work that way.
A safe in-place constructor would be allowed to write but not read uninitialized memory. Isn't Rust's compile time analysis already capable of enforcing such a restriction?
Also bad: What if you throw while constructing? You might try to drop uninitialized structs.
Cool. How do IPC and 'normal' channels compare performance wise? Is the overhead from (de)serialization manageable? 
More ways of integrating Python and Rust is definitely welcomed. How is this better/different from generating a C compatible interface and calling via [cffi](https://cffi.readthedocs.org/en/latest/) ? Here is a short demo of calling Rust from Python using `cffi` https://gist.github.com/seanjensengrey/f5d73bbdf22cfa1ad463 What I would actually like to see is for a Python implementation to layout memory in something like `capnproto` _or_ have the layout strategy be a runtime specified behavior. I could see PyPy implementing this.
Sure, but maybe it's something that can be worked on. Eh?
Currently `panic!` works like an uncatchable exception. A `drop` is when a variable goes out of scope and its destructor runs.
&gt; Software Foundations This electronic book is a course on Software Foundations, the mathematical underpinnings of reliable software. Topics include basic concepts of logic, computer-assisted theorem proving, the Coq proof assistant, functional programming, operational semantics, Hoare logic, and static type systems. The exposition is intended for a broad range of readers, from advanced undergraduates to PhD students and researchers. No specific background in logic or programming languages is assumed, though a degree of mathematical maturity will be helpful. free download http://www.cis.upenn.edu/~bcpierce/sf/current/index.html
&gt; I really like the idea of a curated list of crates, but I see no reason for using stdx or a similiar collection as a dependency. Adding dependencies is so simple with cargo. Packages depending on stdx should not be allowed on crates.io since they pull many unneeded dependencies. I think this is a good point. Packages that depend on stdx don't disclose to potential users what are the actual dependencies. Also, until (and if) Cargo and/or rustc improves its story for lazy compilation, packages that depend on stdx will have their build time increased needlessly. Cargo can help here if it could automatically drop unneeded dependencies. For example, I begin using stdx, but at some time I decide that I want to document properly what are my dependencies. Then a Cargo command detects it (and perhaps remove some `stdx::` prefix or something - but that would be bad if I wanted to move back to stdx..). Generally speaking, stdx would be more useful for end users (that won't be uploading code to crates.io but will be developing applications local). Broadly useful crates should be held in a special place, and be considered for inclusion in the rust std. Not sure if being re-exported by a single crate is the right place.
This is the shit I knew I was going to get :) I also haven't had any problem with Python's dependencies yet. Haskell's dependencies are a whole 'nother story...
When I try to derive Serialize and Deserialize for a struct with a container of Trait Objects that use the method you mentioned I get: error: the trait `core::marker::Sized` is not implemented for the type `Trait` [E0277] [derive(Serialize, Deserialize)] trait Trait: serde::Serialize + serde::Deserialize { // ... } #[derive(Serialize, Deserialize)] pub struct Structure { field:BTreeMap&lt;String,Box&lt;Trait&gt;&gt; } 
&gt;A trait object can only include one trait bound for a non built in trait So does that mean I can't do: pub trait MyTrait : Serialize + Deserialize ? It compiles.
That actually kind of makes sense, though I had completely forgotten about it :/. Because `Serialize` and `Deserialize` have generic methods, they won't work at all in trait objects. The `serialize` and `deserialize` methods both take generic parameters, and are thus not "object safe". I don't think there's really a way to fix this at the moment.
&gt; What if B wanted to guarantee that its channel is always directly communicating with A, could it reject to establish a new connection to C? This doesn't make sense. If you forbid it, A could still work as a proxy and resend messages from C to B and vice verse, this would be the same as if B was connected directly to C.
If you do do that, and the `serializedthing` implements `Serialize` itself, it would be possible to then `impl Serialize for Box&lt;MyTrait&gt;` and work with that, if you want to them derive for the struct holding the trait objects. Might not be worth it, just another idea if you want to go down that path!
I think the benefit of something like stdx is just as much aimed at the package ecosystem as it is for new users. stdx represents a particular approach to new users interacting with a package ecosystem: Try to bring about a super-popular, one-crate-per-purpose list of packages for super common purposes, with the above search as a fallback. Compare this to the current way: Search cargo for your functionality, pick a result that has been downloaded a lot. Or go to one of many "awesome" lists on github, hope it isn't missing anything important, and then pick from multiple good choices. The current way is a lot like the NPM way and will probably lead to the Node Ecosystem problem. E.g. for every problem, you have a couple super-popular, and a dozen merely popular packages implementing a slightly different version of the same solution (e.g. grunt, gulp, broccoli, brunch) (requireJS, browserify, webpack) * each package with not enough developers working on them, eyeing them over for security problems, etc.. * new users face a high wall of "research costs" to learn which packages are barely used anymore, etc. Stdx would help avoid this costly fragmentation. I would prefer the slightly different approach of blessed crates, which [have been discussed](https://www.reddit.com/r/rust/comments/2mo0zb/the_race_towards_10_and_the_standard_library/) here and there over the last couple years but if there is a list of blessed crates somewhere, it's nothing you'd see in your workflow of getting started with rust, (e.g. homepage, install page, cargo homepage.) Either would lead to: * more focused community development makes one crate work really well , be documented really well and be really secure rather than three or four crates work kinda well, be poorly documented. * Help new users avoid frustration by being able to take advantage of insight beyond just past popularity, as to which ones have lots of ongoing development, more complete, etc., rather than each user having to research that themselves.
I'm not on the libs team. The criteria _used_ to be: 1. Anything that is used in the vast majority of Rust programs 2. Data structures which require lots of unsafe 3. Common traits, for easy interfacing That's in the standard library, at least. Most crates under rust-Lang are extractions of things that USED to be in the standard library but aren't anymore.
I doubt you need anything close to a full integer theorem prover like a Presburger arithmetic decision procedure. You could probably get away with something close to the affine recurrence solver that LLVM uses for loop optimizations.
How do you skip destructors in C++? Are you referring to forgetting to call `delete` after a `new`? If so, what if I am disciplined to use `std::unique_ptr`?
I'd say we've been moving strongly towards *just* common traits as the only thing that *fundamentally* needs to be in the std lib. If a common trait isn't in std then you need to "randomly" depend on a crate just to implement traits that people *might* want, and then downstream needs to agree on that trait and consume it. Other things that I think should be in std: * Basic types that libraries would want to have in their interfaces (so things like Duration, Vec, String, Cow, Rc, Box, etc...). Again this is a matter of establishing a lingua-franca so you don't need to figure out wtf MemoryBuf/ArrayStack/GrowArray are that this library is exposing or try to convert between MemoryBuf and ArrayStack because you're trying to glue two libraries together. It's Vec. I know Vec. * Low-level utilities that help you work at the "right" abstraction level, and that we can tightly control to do interesting analysis/optimization with. Things that make unsafe code less unsafe. For instance many thing in `ptr` and `mem` are actually compiler intrinsics. Unique is an interesting example of something that lets you avoid understanding variance/dropck and lets you basically "say what you mean". In principle we can do more interesting analysis with Unique in the future because it's provides a vague semantic, and not actually "do this thing". * Magic things that we can stabilize now but don't want to commit to stabilizing the underlying mechanism of. Send and Sync have the `impl for ..` syntax. Fn traits are sugar for the "real" trait. Box has DerefMove. etc
Cool! Still, I'm worried that it would be fairly brittle and hard to grok why the "array initialization checker" is rejecting your code. I suppose we could restrict ourselves to only linear for-loops that explicitly go from `0..N` or something.
&gt; How do you skip destructors in C++? Are you referring to forgetting to call `delete` after a `new`? Or you could put something in a reference cycle with `shared_ptr`, or have it owned by a thread that blocks until the program is finished. &gt; If so, what if I am disciplined to use `std::unique_ptr`? Doesn't solve the thread issue. Of course it's possible to write a program in which destructors are *not* skipped, but that's beside the point entirely. The problem (shared by Rust and C++) is that it's possible *at all*. At least, it's a problem when unsafe code *believed* it was providing guarantees based on it *not* being possible.
Hi. Noticed a stray f in the middle of lib.rs, and unit!(Inches, Feet, f64, 1.0); needs to be 12.0?
Things I never want to see in std: * Parsers or handlers for common data formats (html/json/xml/yaml/toml) * High-level networking protocols (HTTP/SSH/FTP) * Crypto * Linear Algebra / Calculus * Graphics * GUIs * Databases * Bindings to *any* particular app/library (other than libc) * All The Collections (persistent, concurrent, niche) * Ropes or other exotic (non-utf8) string formats * All The OS APIs (J/K we need DirectDraw support ASAP) Things I'm iffy on: * Numeric traits * BigNum * More hashers (having only one for Hashmap -- and the slowest at that -- is weird) * "green threads" Things I irrationally want in std: * rand. Even *javascript* has rand! But it doesn't actually fit any of my criteria... And yet, &gt; But how do you create a fucking random number? &gt; &gt; \- brson
I hope not. Rust is already frail and fails hard with basically anything that links to a 3rd party library, because crate authors can't be bothered to correctly setup a local static build of the dependency and just assume 'library is installed on system path'. ...which is great and simple and the 'python way', but it sucks badly for cross compiling, and generates terrible issues when the system version of the library upgrades, and then breaks your code until you update a dependency... that requires out dependencies to be updated and so on. Pillow/PIP is a particularly 'big name' example of how this screwed up basically every python application on linux a year or two ago. When you introduce the additional concept of 'system crates', it gets even worse with multiple versions of different crates you have to install externally before your application will build. 'Repeatable builds?' who needs em right? I mean, C++ gets by just fine without them right? (Just fork and copy the code straight into your project~ :P)
You may want to implement std::convert::From for your unit types.
Cool! There's a great example of this in action in Mozilla's Servo -- Matt Brubeck's post about it was a similar lightbulb moment for me. https://blog.mozilla.org/research/2014/06/23/static-checking-of-units-in-servo/
Are you sure you want it to be `&amp;'r RowBlock&lt;'r&gt;`? Perhaps those two lifetimes should be different?
Clever! F# actually does something like this now, so you're in good company. Not so goofy at all, it's a smart use of the type system. :)
I'd write: fn check_pin(&amp;mut self, pin: u16) -&gt; bool { self.pin == pin } Also seeing that the `match`block is the only thing in the loop, the `continue` is superfluous. Edit: Note that [rust-clippy](https://github.com/Manishearth/rust-clippy) would suggest the first thing, and there's an open issue to catch the second.
The bar has to be high for this. It’s easy to confuse "me and everyone around me" with "everyone". Programs have more diversity than we sometimes think.
Thank you for your interesting reply! Surely it could be analyzed if there is potential access to a &amp;mut from a different thread? But anyway, I was mostly curious if this is something that has been discussed and decided against. One of my predictions for rust is that the borrow checker will evolve to prove more situations are safe. Of course there is some limit where analysis gets so complicated and the benefit so small that there is no point in trying to prove the safety. I'm just hoping we are not already at that point! Non-lexical scope is the well known and discussed one.. and I don't think I've heard of any other improvments being planned. Are there any more safety proofs being planned?
If you can’t or don’t want to add the additional trait bounds to `Trait` you can also just define a new trait, provide a default implementation trait SerializableTrait: Trait + serde::ser::Serialize + serde::de::Deserialize {} impl&lt;T&gt; SerializableTrait for T where T: Trait + serde::ser::Serialize + serde::de::Deserialize {} and use it in the trait object: BTreeMap&lt;String,Box&lt;SerializableTrait&gt;&gt; 
Thank you for the comment. Yes, you are right. I don't need &lt;'r&gt;. I recognized that from you and the below comments. 
Thank you for your advices and the fixed source codes. From your comment, I recognized what I didn't understand about lifetime. I have to learn Rust lifetime again.
&gt; No other mainstream language has the concept of scoped objects; so this is not a problem that you can express in C++ terms. Okay, so what do you men exactly by a "scoped object", besides a stack-allocated object with a destructor/Drop impl? Because other mainstream languages have those, so I think something went over my head.
This can be seen as the more expensive and general variant of eddyb's `&amp;'a mut T =&gt; &amp;'a Cell&lt;T&gt; where T: Copy` idea.
I think if you use this locally in a function, much of the overhead is stripped away
Say more about this? By thin handles do you mean borrowing the separate slices into a vector of tuples? I don't see a way to avoid that, since mapping over chunks of two separate slices in a thread seems…complicated, but please feel free to sketch it out – I'd love to simplify this and eliminate the overhead. 
Thanks, I did!
Not that I have a good handle on rust as a whole, but couldn't you take immutable references to all of the stuff you need (including the one you want to mutate), then do your calculation, let all the other immutable references go out of scope, then finally promote your immutable reference to a mutable one?
You start with two vectors, one for longitudes and one for latitudes. All the allocations come from: - computing a single vector containing tuples (longitude, latitude) - computing single (smaller) vectors of those tuples Instead, you could: - keep passing two slices of integers, to avoid that single vector - pass slices instead of independently allocated vectors The `thread::scoped` API is sufficient here (if unstable), since you are anyway joining with the threads before the end of the function, and contrary to `thread::spawn` it allows you to pass variables with a non-`'static` lifetime, such as slices created from the slices you receive.
Worth noting that the current plan is to make SIMD support *entirely* an external library: https://github.com/rust-lang/rfcs/pull/1199 This is because it involves a large amount of conditional compilation, which the standard library can't really support.
&gt; The Minecraft example demonstrates that you can create a popular video game (probably the most popular video game of all time) with a garbage collected programming language. You think Minecraft is more popular than Tetris, Pac-Man, Mario Bros, Snake, Candy Crush, and Angry Birds (just off the top of my head)?
I think cargo solves the technical pain of adding dependencies, but not the social pain. To add a dependency, I have to do the following: - Get approval from PM/legal, and make sure licenses and so on are updated to reflect new dependency. - If the dependency has previously been approved for global use then I'm good. - If not, I need extra approvals. I also need to have the code scanned for copyright violations. Repeat this process every time I want to update to the most recent version of the package. I don't believe this is too unusual for large corporations. This is a pain in the behind, but it's not too bad when the stdlib is non-tiny and I can reasonably assume that I'm mostly only pulling in a few libraries for big deal functionality - Lucene, for example. If I need to do this for lots of bits of bog-standard functionality, it'll dramatically reduce my desire to use Rust in that setting - particularly as an early adopter, when libs are less likely to already be approved. Added to this is trust - the less I have to pull in unblessed crates owned by joe random, the happier I am that (a) I can reasonably trust the code, and (b) that my application will be able to be kept up to date with security updates and so on. I realise that maintaining superseded libraries is super unpleasant, but the ability to write code with a reasonable expectation that it will still work safely a few years after most active development on the application has ceased is immensely valuable. I think this is probably particularly true for a lot of systems projects, which can often have a very long lifespan. Obviously there's a tradeoff to be struck here, and I understand that completely. Nobody wants the project to get crushed under the weight of maintaining outmoded std libraries. To me, the decision of whether to include functionality in std comes down to three questions: * How many people are likely to want the functionality? * How likely is this implementation to get superseded (particularly in a non-API-compatible way)? * How complex is the functionality to implement? In this case, I'd say that a PRNG is going to be pretty widely desired, and $Mersenne_twister_variant is not going to be a large drain to implement/maintain. If something drastically better comes along, well, the interface for random numbers can be stabilised pretty easily, and the behind-the-scenes code improved.
Let's just say is a reason `numpy` isn't a part of Python's standard library (we are of course not opposed to a `numrust`).
Out of curiosity: would this process applied to the entire standard library whenever you upgrade Rust?
That's a neat idea, but I'm not clear on how to do that. If I put it in the library, I get a notification about conflicting implementations. I'll admit I kind of expected that one. What surprises me is that I get the same thing if I put it into that macro. In that case, I'm not even sure what it's conflicting with. It's an implementation on a concrete type. I guess it simply can't confirm it isn't converting Self into Self?
&gt; The most fundamental problem with keeping an &amp;mut "just around" is a multi-threading scenario. The fact that you've called a function that can't reach the &amp;mut doesn't mean the &amp;mut isn't being used. Do you have an explicit example of how this would be unsafe? If you have `a.b` and `a` with `a.b` being a sub-LP of `a`, it feels intuitively OK to me to pass an immutable `a` around with an `&amp;mut` borrow on `a.b`. After all, `&amp;mut a.b` takes out an implicit mutable borrow on `a`, you could convert that implicit mutable borrow to an immutable one, and that implicit mutable borrow locks out all other mutable borrows, so there should be no mutation. (I feel like I must be missing something here, as it seems too easy.)
I'm not sure what you're picturing exactly, this seems fairly contrary to the current lifetime system. Basically you're thinking of temporarily freezing an internal mutable reference while other "shared" work gets done? That *might* be sound? I'm not sure how it would interact with wrapper types that mutably borrow and interior mutability, though. edit: It sounds like a pain for unsafe code. e.g. let x = &amp;mut a.b; *x = unsafe { partially_initialized() }; println!("{}", a); // ok, we'll just make x inert x.finish_initialization();
After looking into it a bit more, I think this is a good point in that it is not "just as easy" to work with the interfaces. That said, implementing support for the interfaces would not have to be "from scratch" either. The interfaces are declared in all of the server classes that already exist. The real trick is working around the networking things that are built in. The good news is what this is trying to do is actually much much simpler than what Cap'n Proto was made for and thus should not be very difficult to adapt. Interestingly you also get the added bonus of an interface that is asyncronous by default and always returns promises.
I think you could implement it for a pair of concrete types, but then each type would need that concrete implementation for each other type, which would get pretty cray, and which you couldn't do in a macro (at least not the way I have it made right now), and... Yeah. An easier use case for From would be converting to and from `Self::Data`, I think, but even that couldn't be done at the trait level; you'd have to do it as a macro. Unless it's possible to implement a trait inside a trait? I thought of trying that. Like, nest the implementation of From inside the definition of Unit so that it has access to Self::Data. I wanted that on several occasions but never tried it. I wonder if that's a thing you can do. Edit: took two minutes and tried it. Nope. :P
It was hyperbole. That said I went to Target the other day and there was basically an entire aisle devoted to Minecraft toys. I don't think we realize the full impact this game has had on the next generation. Kids today don't watch TV, they play Minecraft. And increasingly they don't even do that - they watch other kids play Minecraft on Youtube. It's bizarre.
Yup. Right now, there's not a great way to make a type alias for compound units (for use in function signatures and such), but it's quite easy to make variables with them (e.g. `let speed = 7.0 * m/s;`). Conveniently doing so with type aliases is waiting on a Rust bug fix and allowing macros in types.