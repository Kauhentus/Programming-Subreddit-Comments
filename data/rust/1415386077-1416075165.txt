It was very easy for me to grok both pattern matching and the optional/result types from learning Haskell. Functional programming is almost a must for anyone to look into if they want to be a well-rounded programmer. Having learned multiple paradigms, I feel like I can look at a problem at more angles than if I had just stuck with OOP. That said, Haskell is not a language to take lightly. It really does require commitment. For a new programmer, it may or may not be a good fit. It was easier to understand Haskell with a blank mind than trying to match its concepts to OOP ones, because the latter just ended up confusing me and creating more questions. Having spent enough time with C-family languages, namely Java and PHP, I can read C code easily enough. Actually I have spent a lot of time sifting through the source for the Java Virtual Machine to nail down some strange or generally undocumented behaviours. I just haven't really had a reason to write any C code, and I don't really want one. 
Actually, gcc is not *always* faster than LLVM; if I remember correctly LLVM, owing to its appeal in the academic world, is faster on numerical work but slower on "regular" code.
I think one of the prerequisite to get that bitmask is having the lowest value/highest value range fit into a 64 bits integer.
It's like mind-reading!
Processing is taking a while
Wait, so how would this work with a trait? e.g., would it be something like: trait WhiteList { fn is_whitelisted(&amp;self, name : &amp;str) -&gt; bool; } impl WhiteList for HashSet&lt;String&gt; { fn is_whitelisted(&amp;self, name : &amp;str) -&gt; bool { ... } } fn main() { let wl : WhiteList = HashSet::new(); } This doesn't work. Sorry, I'm still kind of a newbie. 
The bindings are now up at https://github.com/eyolfson/rust-wayland. The raw module includes everything you'd get from wayland-client.h and wayland-client-protocol.h. 
Sure, but you ignore the real issue: why did Unix &amp; C become successful? We aren't talking about how C or Unix originated, but how it became successful.
Haskell also offers trait-based parametric polymorphism.
Hey, they rebased it! PR here if you want to help review! https://github.com/rust-lang/rust/pull/18747
When you pass an object to a function and give it ownership: fn take_whitelist(wl: WhiteList) The compiler has to make a byte-for-byte copy of the object into the function. This is because the object is on the stack, where it can't leave the function otherwise. Whereas in Java, it just makes a copy of the reference, which is on the stack and points to the object on the heap. I don't have much knowledge of the underlying pointer handling, as that's almost too low-level for me to understand right now, but I'm pretty sure it's to do with how the object is laid out in memory. So you pass a pointer to a `WhiteList`, and that pointer points to the part of the object that implements `WhiteList`, and then the implementation has the information about the rest of the object which it would need to get at its data or inherent (non-trait) methods. Or it might be a fat pointer, with one word that points to the start of the object, and another that points to the implementation of the trait. Since a pointer is constant-size, it's trivial to copy into another function, but an object that implements `WhiteList` could be anywhere from a unit (empty) to several words in size, or even arbitrarily large if it contains a fixed-length array. That's because a struct is always at least as large as the sum of its fields, so you have to know exactly how big it is to correctly copy it into a function.
Yeah, knowing Rust helped me (unfortunately) write C at university because I had the mindset of lifetimes and ownership in my head. Without that I think things would have been more opaque and confusing.
I agree with pretty much everything you just said. &gt; Actually I have spent a lot of time sifting through the source for the Java Virtual Machine to nail down some strange or generally undocumented behaviours. Bingo.
(I assume that) the reason for this is that if you had multiple receivers, it would allow for a race condition "which thread gets the message first" kinda thing.
Haskell and C are pretty painful languages for newbies. If you're most familiar with Python and not much else, Rust is probably a better choice than C or Haskell. However, you might want to try out a language slightly more forgiving or better documented than Rust is. I would recommend Go, or maybe even Java. Rust is a better investment for your time in the long run, IMHO, but it's probably pretty difficult to learn if you only know Python. If you're ambitious and industrious, I would recommend it! Happy Hacking! :)
Interesting. I would rather use the index when skipping numbers. You are not wrong. This is just different and it avoids the time spent on cloning and moving elements in the list. fn get_products(input: Vec&lt;uint&gt;) -&gt; Vec&lt;uint&gt; { let mut product_results = Vec::new(); if input.len() &lt; 2 { return product_results }; //Just iterate over the indices for to_skip in range(0, input.len()) { let mut prod = 1; //enumerate() will give the index for each element for (index, &amp;x) in input.iter().enumerate() { //Only multiply if it's not the skipped element if index != to_skip { prod *= x; } } product_results.push(prod); } return product_results } One could also go nuts with the iterators: fn get_products(input: Vec&lt;uint&gt;) -&gt; Vec&lt;uint&gt; { if input.len() &lt; 2 { return Vec::new() }; //Just iterate over the indices and map them to products range(0, input.len()).map(|to_skip| { input.iter() .enumerate() .filter(|&amp;(index, _)| index != to_skip) //Only accept the other indices .fold(1, |prod, (_, &amp;x)| prod * x) //Begin with 1 and multiply with the accepted elements }).collect() //Collect the products in a Vec&lt;uint&gt; }
Some notes: * In general, try to be weary of using .clone(), it's usually a fairly expensive operation, in this case, copying all items from a vector. * Functions that don't need ownership should take a reference, or a view. In this case, functions that take Vec&lt;uint&gt; should take &amp;[uint]. This prevents unnecessarily consuming the value, the caller will be able to re-use the vector. * When creating a vector, you should see if you know the capacity at creation time. If you do (and in this case, you do), use Vec::with_capacity(input.len()), over Vec::new(). * Instead of cloning, and removing the item in question, you can iterate over indexes, and take the product of all values not at the iterated index. Here's how I'd do it: use std::iter::MultiplicativeIterator; fn get_products(input: &amp;[uint]) -&gt; Vec&lt;uint&gt; { if input.len() &lt; 2 { Vec::new() } else { let mut v = Vec::with_capacity(input.len() - 1); for i in range(0, input.len()) { let (left, right) = input.split_at(i); let right = right.slice_from(1); v.push(left.iter().map(|&amp;x| x).product() * right.iter().map(|&amp;x| x).product()); } v } } If you're willing to relax the rule against division, the function becomes: use std::iter::MultiplicativeIterator; fn products_with_division(input: &amp;[uint]) -&gt; Vec&lt;uint&gt; { if input.len() &lt; 2 { Vec::new() } else { let product = input.iter().map(|&amp;x| x).product(); input.iter().map(|&amp;x| product / x).collect() } }
If you could clone Receiver, the implementation would be forced to use locks and other synchronization primitives so multiple concurrent calls to `.recv()` don't interact with each other. Since it doesn't, it's not Sync. However, you can get around this using the concurrency primitives in the stdlib: You can lift the Receiver into a Sync type by using `Mutex&lt;Receiver&lt;T&gt;&gt;`, and then lift it to a Clone type by using `Arc&lt;Mutex&lt;Receiver&lt;T&gt;&gt;`.
I think /u/GBGamer117 gave the right answrt to your first question. I think that a solution for the second problem is to have a middle hand which receives the message, clones it and broadcasts the clones over multiple channels. It would indroduce some overhead, but it would most likely work.
Rust doens't consider 'race conditions' to be unsafe, just 'data races', which are a form of race condition. I don't know if some form of what you've said factors in, just wanted to point out the difference.
&gt; This was due to the fail!(..) macro being renamed to panic!(..) Whoops! My bad. I wonder how I missed it with my semi-automated pull request script... &gt; but there isn’t a collection of what is available, a-la NPM, Rubygems, or CPAN. Very soon.
Thanks, TIL.
Full list of unsafe is here: http://doc.rust-lang.org/reference.html#behavior-considered-unsafe
I would never touch Java and javascript with 10 foot pole.
Is the optimization done on string pattern matching too ?
You already have `std::sync::deque` which is the opposite: single producer/multiple consumer. It probably wouldn't be a huge amount of work to glue the two together. The difficult part is making it lock-free or at least low-contention. There's not much point to having threads communicate if they all have to take turns using the same bus. A while ago I saw a neat article about a mpmc (multiple producer/multiple consumer) lock-free queue written in Java. I wish I could find it again because a Rust implementation would be a really neat project. It was based around the abstract concept of a "train" stopping in each thread to drop off and pick up messages. It was lock-free and *crazy* fast. I found [an mpmc queue written in C++][1] during my searches but it's not the one I was thinking of. Edit: I found the article I was talking about! http://www.infoq.com/articles/High-Performance-Java-Inter-Thread-Communications Second edit: there's already `std::sync::mpmc_queue`, but it's not documented at all. [1]: https://github.com/cameron314/concurrentqueue
What semantics would you like to have? Every receiver gets a copy? Or should a message only appear in exactly one receiver's inbox? And if the latter: Which one? Random? First-Come-First-Serve? If you want multiplt receivers to get a copy each, we would have to restrict channels to clonable types. So far, it also accepts move-only things. I think First-Come-First-Serve would make some sense. I don't know how much overhead this would cost compared to our "normal" channels, but it's probably some. So, maybe there should be another kind of channel which supports cloning the receiver. This way, the overhead can be avoided for those cases, where you don't need to clone the Receiver.
For things that require performance like encoding or compression, I'll generally prefer the FFI version over the pure Rust version, until the Rust version proves that it is just as fast. However for libraries that don't do a lot of heavy compute, I'll often prefer the pure Rust version.
As usual, please keep in mind Rule #4 from the sidebar.
I imagine when you have two libraries, one in Rust and one in another language (with Rust bindings), and everything else is equal, the Rust one would be referred (except for crypto, perhaps?). However, most libraries are not in Rust and rewriting everything is unrealistic and a huge waste of time, so in practice using bindings to C (or other language) libraries will be common, and doubt anyone thinks that is somehow wrong or undesirable. Better to quickly create bindings for existing libraries and implement new and exciting stuff than to rewrite existing libraries for the sake of it.
it entirely depends on how easy it is to use. I usually dislike FFI libraries because they don't integrate well when building/deploying the application. If cargo makes this pretty painless I would have little objection. (Also, rust has less reason to need FFI than other languages, given it can reach the same performance and control as C/C++).
Here we go: fn get_products(nums: &amp;[uint]) -&gt; Vec&lt;uint&gt; { let mut result = Vec::from_elem(nums.len(), 1); let mut prod = 1; for (&amp;a, b) in nums.iter().zip(result.iter_mut()) { *b *= prod; prod *= a; } prod = 1; for (&amp;a, b) in nums.iter().rev().zip(result.iter_mut().rev()) { *b *= prod; prod *= a; } result } It's O(n) runtime and, if we do not count the output buffer, O(1) space.
Interestingly I came up with pretty much the same iterator based solution (though my language background might show in the variable naming…). Thought I'd post it, since I personally had some trouble making it generic: use std::num::One; fn prod&lt;T: Mul&lt;T, T&gt; + One&gt;(x: &amp;[T]) -&gt; Vec&lt;T&gt; { range(0, x.len()).map(|i| { x.iter() .enumerate() .filter(|&amp;(j, _)| i != j) .fold(One::one(), |x: T, (_, y)| x * *y) }).collect() }
I suggest learning C, its a lot of fun, and Clang's -S parameter is amazing for figuring out exactly what is happening on a CPU level.
That's an interesting point, does anyone know the story of how Cargo interoperates with FFI and C/C++ libraries?
For extra insanity, but less efficiency because of a single extra allocation: fn get_products(nums: &amp;[uint]) -&gt; Vec&lt;uint&gt; { fn f(state: &amp;mut uint, n: &amp;uint) -&gt; Option&lt;uint&gt; { let tmp = *state; *state *= *n; Some(tmp) } nums.iter().scan(1, f).zip(nums.iter().rev().scan(1, f).collect::&lt;Vec&lt;uint&gt;&gt;().into_iter().rev()).map(|(a,b)|a*b).collect() }
Best guide I've read yet! I'd love to read the last three sections as well, have they not been written yet, or is the link https://github.com/steveklabnik/rust/blob/ownership_guide/src/doc/guide-ownership.md outdated?
They just haven't been written yet. I follow the 'post early, update often' philosophy. (oh, and glad you liked the beginning, thank you)
I really like the "Option" syntax. I re-wrote an ancient java server in go earlier this year. and just this week I got another bug (in production) which boiled down to being so used to writing if err != nil { // do some error handling return // implicitly returning err } That when faced with doing a trivial one liner at the end of a function it was more terse to write if err == nil { // a simple one liner on relevant data. } return // implicit data and err But having written "if err != nil" so many times, I ended up writing "if err != nil" instead of "if err == nil"... so in the rare edge case where the code was hit.. bang I got a bug. I have yet to write anything significant in Rust, but the Option syntax would seem to make the above error less likely. Bonus: Today I mentioned rust and go in a demo, and the CEO approved me using either when re-writing another service. :))))
That's not what I was thinking of but it's definitely an mpmc queue. Not sure how fast it is, though the use of 64 bytes of padding between fields is a curious thing. Doesn't say why they did that though. I do see a potential issue in its implementation, though: there's no way to query its current size, so if it's full then the object will fail to push, and if the object fails to push then it's gone forever. It should return `Result&lt;(), T&gt;` to allow the pusher to recover the rejected object, and/or a `.len()` or a `.is_full()` method so the pusher can see if it's full or near-capacity. The code isn't well documented even on the source page linked in the comments.
Cool. I like this approach.
Your comment got me interested because I work with problems like this in C/assmebler, so I googled "mpmc rust" and have found this. I haven't checked the code, but the 64 padding is typical for making all the access aligned to cacheline, to prevent cachelines bouncing back and forth between non-shared caches. I think it should be within my reach to implement things like this in Rust, and it would be fun learning experience. If you will find some interesting links to better models for mpmc implementations please link them so I can take a look. Sounds like a great idea for a library. :)
There was a [recent RFC just passed](https://github.com/rust-lang/rfcs/blob/master/text/0403-cargo-build-command.md) that greatly improves linking with external libraries inside Cargo. Amazingly, I think [some or most of it is implemented](http://doc.crates.io/build-script.html). I think if you skim through that stuff, you'll come away with a pretty good idea of how Cargo is going to handle C libraries. (C++ seems like a different story, because of ABI issues.)
There's been a lot of activity [here](https://github.com/conduit-rust), so I suspect it's going to be Rust backed. :-)
I wish I could find that article I remember reading because it was such a novel idea. But I searched Reddit and DuckDuckGo extensively and found nothing. And then I realized I'm an idiot. One Google search later: http://www.infoq.com/articles/High-Performance-Java-Inter-Thread-Communications DDG might be open-source and privacy-focused, but it just can't deliver results like Google can. 
Actually this link considers data races to be unsafe (but not deadlocks).
Right. Data races are unsafe. Race conditions are not.
This is beautiful.
I have access to the staging server and have been playing around with it. It basically works. I tweeted a screenshot the other day: https://twitter.com/steveklabnik/status/526895097179537408 I don't know how public the schedule is, but there's a schedule, and it's end is soon. It does in fact use Rust for the backend, and uses Ember for the front.
All of them. ;) Don't worry if something has already been done, multiple choices are healthy.
What does the workflow as a project owner look like? Does it just link through to GitHub, or will it integrate with CI to provide prebuilt packages?
You sign in with GitHub, and you get an API token. You can then run a command or edit your config file to add the token, and then `cargo package` and `cargo upload` (I think, it's 3am here and I'm too lazy to check) to send it to the site. You can use `semver =` constraits rather than `git =` constraints, and it'll do the right thing. There's no plans to distribute binary packages, source only. (I'm extra comfy talking about this because you can just read it in the source of Cargo, or at least, the client parts of it :wink: open source holds few secrets!)
But there are some essentials that could bring you users, fame , glory and tons of job opportunities. ;)
I was going to implement a binary tree sort of thing, but this is more clever.
TIL they are [different](http://blog.regehr.org/archives/490). Perhaps that document should list "race condition" as explicitly not unsafe.
I've seen this article before. After brief look, I share some concerns voiced in the comments. I'm not sure if this queue is useful enough other then some specific cases. But thanks for the link!
Yeah but that RFC is still run-time type checking. Just like how typehints on params work now in php.
I believe in this case `path_from_stream` will only relinquish the borrow after `path` is dropped. The `Option&lt;&amp;str&gt;` you return from `path_from_stream` is borrowed as well, and the compiler deduces that it is borrowed from the `&amp;mut BufferedReader` (as if the signature was `fn path_from_stream&lt;R: Reader&gt;(input: &amp;'a mut BufferedReader&lt;R&gt;) -&gt; Option&lt;&amp;'a str&gt;`). So until the return value is dropped (or goes out of scope), the argument remains (indirectly) borrowed. My rust is a bit rusty, I'm sure someone will offer a solution for multiple borrows from the same source.
I would like a library to do [HTML scraping](https://www.reddit.com/r/rust/comments/2k4s0w/is_it_possible_to_use_html5ever_and_servos_css/), preferably by [extracting Servo's code](https://github.com/servo/servo/issues/3669) as a library. For me the important bit would be to write short code that compares favorably to `doc.css('h3.r a').each do |link| puts link.content end`.
I'd love to see crypto libraries in Rust, perhaps an implementation of SSL.
Eg. the fact that each "thread" needs to "push the train". What if one of the threads decides it needs to do some blocking io, etc? Just see comments by Michael Barker and Nitsan Wakart under the article itself, which basically destroy this idea. 
try /r/playrust. This is for the programming language rust.
I think binary packages should be allowed. Imagine if you wanted to create a game that interfaced with the Steam APIs and Valve ported their API to Rust (highly ridiculously unlikely, but just as an example). It would be very useful to be able to simply put the Steam API in Cargo.toml and have it automatically download during the build. Not everything is open source, and things that are closed source shouldn't be inconvenient to use.
Servo uses several Cargo-ified C libraries, including some C-only libraries with an empty `lib.rs`. The Cargo.toml invokes a Makefile, and you use it the same way as any other Cargo library. It's definitely less painful than other foreign-build stuff I've used. C++ works too, you just need `extern "C"` wrappers to call it from Rust, but the build process is the same.
I'd really like something like http://shoup.net/ntl/
It would be nice to get mature Rust libraries, with auto-reconnecting, connection pooling,etc for: * Amazon AWS (this is a huge body of work) * RabbitMQ * Elasticsearch * Mysql/MariaDB * PostgreSQL * Redis (I think I read about one that's coming along nicely) But I haven't looked for any of these in the wild yet. 
For what it's worth, [stomp-rs](https://github.com/zslayton/stomp-rs) makes it possible to interact with RabbitMQ from Rust. It doesn't yet have auto-reconnect yet, but that's [planned](https://github.com/zslayton/stomp-rs/issues/35). (I also need to catch up with all of this week's breaking language changes around `fail!` =&gt; `panic!` and the stdlib. That'll be done this weekend, though.)
With macros being stabilized for 1.0, we need to do something about the worst usability problems. I'm sure there will be much discussion and I'm sure that nobody will be 100% happy with where we end up -- I'm certainly not 100% happy with this proposal. But I'd like to hear what people think. See also [the RFC discussion](https://github.com/rust-lang/rfcs/pull/453), and more discussion [here](https://github.com/rust-lang/rfcs/issues/440), [here](https://github.com/rust-lang/rfcs/issues/416), and [here](https://gist.github.com/kmcallister/96668f02200cdb81e33e#comment-1330021).
Wow, nice little blog post. Makes me want to try out rust and go right now. . . 
&gt; the inbox should keep displaying the old message until the new one replaces it, and should facilitate this without locking. Could you handle this by having an array of 2 messages and a pointer for which one to read, and atomically swap the pointer over when the new one is ready in the other slot?
How does one find out what libraries are available via cargo? I'd be better able to answer the question of which libraries are would be nice if I knew which ones exist.
&gt; everyday code lmao For reference, Gecko's HTML parser is machine-translated from Java to C++, with special annotations in comments.
You can't really judge whether the use of macros is reasonable or gratuitous without understanding the problem I'm solving, and I'm going to wager (although I could be wrong!) that you've never written a full-featured HTML parser. Ultimately Rust is a language for people who can exercise judgement about language features. It is not meant to tie the hands of people who make poor design choices.
&gt; Code using lots of macros can still quickly become hard to figure out. It's really on the macro author to make sure that the semantics of the macro are clear from its use without understanding the expansion. I talk about this in the [full slides](http://kmcallister.github.io/talks/rust/2014-rust-macros/slides.html), in the context of error handling for procedural macros. Like any feature, you can use macros to make a mess, or you can use them to make code better and more readable. I'm pretty confident this is an instance of the latter, based on comparing html5ever to hubbub and Gecko's parser. Of course you are free to disagree, but you seem to be passing a lot of judgement without much study of the code or the problem at hand.
[Slides are up now!](http://kmcallister.github.io/talks/rust/2014-rust-macros/slides.html)
dbus client *and* server.
You don't, since cargo doesn't have a central repository yet. At most, you can look for Cargo.toml on github. 
That's because `|` is not a part of pattern. If it were, a pattern like `Some(1|2)` should have been accepted but it doesn't. In hindsight we need an additional non-terminal for `|`-separated patterns (`pats`? matching `arm`?) that uniformly apply to every left-hand-side values.
The compiler is correct and catches another subtle error for you :) Any `Buffer` (that `BufferedReader` implements) has an internal buffer returned by `fill_buf` and (indirectly) other methods like `read_until`. The problem here is that the `Buffer` has a right to reuse its buffer! You are effectively *forced* to return the buffer to the owner (by dropping the mutable reference) before doing any operation that can possibly modify the buffer. I do hope some alternatives to solve both the memory safety and efficiency (e.g. append-only buffer?), but you should convert it to `String` for now.
My life for Aiur! 
Learn all. I'd start with C, then go to Haskell, and then Rust. By that time, you'll be seeing languages in a different way and will understand why Rust really brings something new to the mainstream. You'll also be able to spot the missing things in Rust and luckily be pushing a more functional approach to the language... at least I hope. :)
I think Rust has a really good chance of increasing the safety of embedded systems, exactly the same spot where Ada increases safety. We just have to get those embedded C and C++ developers to try some Rust for their feedback and control code.
I'd start by making a Rust interface to http://www.openbsd.org/cgi-bin/man.cgi/OpenBSD-current/man3/tls_client.3?query=tls_init&amp;sec=3 
In Servo we usually prefer pure-Rust versions of things for security. However, because of the newness of the ecosystem it's common that we need the FFI to do various things, so we use C libraries frequently (though they're often replaced with Rust over time—this happened with CSS parsing and selector matching and HTML parsing and we're working on doing the same with windowing and images).
Er, how are custom flags for rustc relevant to building C/C++ libs? My understanding (from doing this in Servo) is that Cargo will invoke whatever makefile / script you like, which could read those things from the environment, or run a configure first
[postgres](https://github.com/sfackler/rust-postgres)
Rust APIs seem a great deal more principled in their design due to the type system, and trying to wrap C libraries to be safe and idiomatic is a real challenge (I know from experience - I've done it many times). And Rust is pretty much set to be competitive with C and C++ performance wise. It emits the same IR – sometimes with better annotations to further help the optimizer. I think the advantage of the FFI is more that it allows us to bootstrap projects faster than if we were always starting from scratch. For example, gl-rs used to use libxml2 bindings - now we use rust-xml. Servo used to use a HTML parser written in C, but now it uses /u/mozilla_kmc's library that provides a much safer, more expressive API. glfw-rs has been a staple in the Rust gamedev ecosystem, but now it seems like once glutin gets fleshed out it will take over as the leading OpenGL context creation lib.
It's still OpenSSL code underneath. It has 2 problems: - a lot of old C code is still a lot of old C code. - writing wrapper doesn't give enough experience as writing pure Rust library. Perhaps moving forward [Suruga](https://github.com/klutzy/suruga) is more interesting both for learning and for ecosystem in general.
Check [Awesome Rust](https://github.com/kud1ing/awesome-rust)
Agree. Interfaces are the first step in replacement. Once the interface is written the backend can be replaced with Rust code. Rust clients don't need to be modified. 
&gt; My recollection of Ada programming was that wrestling with compiler errors, very cryptic compiler errors Rust's error messages are actually quite good. Borrow checker issues can sometimes be a little tricky because its often a subtle error that you are making, but as you say, things are rock solid once you straiten them out.
 fn main() { let args = os::args(); let pattern = args[1].as_slice(); let mut lineno : uint = 0; for line in io::stdin().lines() { let theline = line.unwrap(); lineno += 1; if theline.contains(pattern) { println!("{}:{}", lineno, theline); } } } [`std::os::args()`](http://doc.rust-lang.org/nightly/std/os/fn.args.html) returns an owned `Vec`tor of owned `String`s, so you need to keep it throughout the loop. Similarily, `theline` is a `String` and you need to keep it inside the loop body. (You may wonder if `as_slice()` keeps the original string while the reference is alive. It is currently not the case---a [tracking issue](https://github.com/rust-lang/rust/issues/15023) is available though---and you need to keep a separate variable for such owning variables.)
libraries for library writers: - a library of algorithms and data structures for metaprogramming (boost.hana), - a library for EDSl (boost.proto) fundamental libraries: - parsers combinators (boost.spirit) - compile-time generation of regular expression engines (boost.xpressive/D's regex) - linear algebra (Eigen3) - interprocess communication (boost.interprocess, boost.mpi) 
Most of them that I've met appeared to be fairly conservative and they are frequently stuck with some (possibly quite old) version of gcc, so it may take a while for rust to acquire 'market share' there. Especially since gcc supports more obscure targets than llvm. 
Could you elaborate? What do you like about it? How does it address your use cases?
I do not do much work with macros, but why do macros need their own visibility system? Why cant we just say pub macro! vec { ... } And keep the languages visibility rules? Wasn't the point of making everything crate private by default and only having the `pub` keyword to reduce complexity in privacy issues? If so, why have to different visibility systems? Sorry, on mobile, and that is all I could muster with this puny keyboard. 
Because macro definition and expansion happens at an early phase, before name resolution etc.
Sure. But why can't it just be public or private? if we can't do ´pub´ why not ´#[public]´ or something that looks like the rest of the languages visibility rules?
should've made sure the link worked.. thanks for correcting me :)
Nice overview, as someone who likes both languages it was a really good read. The only other thing the author could've mentioned aswell are concurrency features in Go. Things like goroutines and select are really a core concept of the language, and the final statement that Go "performs well at everything, but doesn't excel at any particular thing" is not exactly true if you take concurrency into account as well. The same kind of applies to Rust, it would have also been interesting to read some about rust's concurrency system, which also seems to be a big selling point.
It took me far too long to find a Clearlooks theme for Tk so `git gui` so doesn't look like a refugee from 1989, even if it still doesn't match the rest of my desktop. From-scratch toolkits are a big no-no in this day and age unless you're going to defeat half the purpose by loading arbitrary theme engine code written for another toolkit. As an LXDE user who expects to follow their diagonal upgrade path from GTK+ 2.x to Qt 5 in the near future, I'd like to see Qt bindings.
The pure rust implementation of the [Tox](https://github.com/irungentoo/toxcore) library, with bindings to C and other languages. I wish to have it implemented, but I'm not sure how to deal with concurrency. 
Documentation for the library is [here](http://rust-ci.org/dnaq/sodiumoxide/doc/sodiumoxide/) (I really need to put that link in the README in the repo). 
How about implementing pure rust cassowary constraint-solving algorithm.? It would be very cool to have rust version of http://pybee.org/cassowary/
Nice one :)
I guess this solution is the intuitive "iteratorization" of the original algorithm.
Another good (and possibly simpler) example is with `Map`s. If you update a map, you're rebuilding parts of it, and keeping some sub-trees the same. But you can have many references to various sub-trees elsewhere in the program, pointing at old versions, differently updated versions, etc. It gets complex quick. Especially with lazy evaluation.
I see. It seems like a tricky problem to make it work smoothly. I'll see what I make of it when I get there. Tanks :)
http://libcinder.org/ bindings or a similar library. Everything on this list http://libcinder.org/features/ This is similar in a lot of ways to a game library, but with a bit of a different tilt to a lot of things. Mixed 3d/2d graphics, leaning on simpler effects and images.
Here's a question I've been wondering about recently. I keep reading that the recently landed multiple dispatch will allow things like `Complex + f32` (previously okay) and `f32 + Complex` (previously not okay). But the latter seems like it would be implemented as impl Add&lt;Complex, Complex&gt; for f32 { ... } My first though is that this shouldn't be allowed, because I own neither `Add` nor `f32`. My next thought is, maybe it's okay because you only need to own one of the types being used, and I own `Complex`? However, if I actually try and use this instance I get an error: mismatched types: expected `f32`, found `Complex` Of course, I could be wildly misinterpreting what multi-dispatching allows.
&gt; except for crypto, perhaps? Am I correct in thinking that the main reason for this is that a large C crypto library is much more likely to be properly audited than a Rust one? Are there any other reasons you're thinking of?
I would think something like [this](http://is.gd/qzVvbr), but it causes a compiler panic. Edit: Made a [bug report](https://github.com/rust-lang/rust/issues/18767).
Destructure using `box`, not `Box&lt;_&gt;`. let foo = Some(box 5); match foo { Some(box x) =&gt; println!("{}", x), None =&gt; {} }
Agreed with this. I understand that macro expansion happens at a different phase, but I'm not sure why that means it should use a totally incohesive visibility/namespace system. It'd be cool to see macro modules defined and used like pub mod! module { ... } use! module::my_macro;
&gt; My recollection of Ada programming was that wrestling with compiler errors, very cryptic compiler errors... GNAT has some of the clearest error messages I have ever seen. 3. funcion X (Q : Integer) | &gt;&gt;&gt; Incorrect spelling of keyword "function" 4. return Integer; | &gt;&gt;&gt; ";" should be "is" When you’re used to other languages: prgm.adb:17:27: != should be /= 
I don't see anything wrong with `AtomicOption`, just seems like the devs deprecated it in anticipation of a better alternative but never got around to implementing one.
Servo could be a really awesome replacement for WebDriver if it exposed a coherent, public API. It'd be much more responsive, since I believe WebDriver relies on RPC.
[Here's (play.rust)](http://is.gd/WIfaaS) the code I'm trying. It's accepting the `impl`, but not the addition itself.
I'm waiting for a basic machine learning library.
It's a great philosophy :) The old guide http://doc.rust-lang.org/guide-lifetimes.html confused me quite a lot. Talking about &amp;'s as pointers to stuff on the stack and Box's as pointers to stuff on the heap, what does &amp;*on_the_heap then mean? That everything is copied to the stack and referenced again? Your 'handle' metaphor seems much better.
From my impressions (very limited) I think Rust isn't expressive enough to do that kind of generic meta programming aside from macro hacks.
For values of embedded involving a real operating system, absolutely. For micro controllers where you tend to allocate all your memory statically I'd tend to prefer something with better compile time evaluation like Nim, but hopefully those features will be added to Rust at some point. And Nim compiles to C, which is important for the reasons thiez mentions.
How about a solid core of immutable datastructures, fully integrated with the normal collection APIs along the lines of Clojure or Scala. 
I thought `Copy` was going to be made opt-in for POD. Has that just not landed yet?
Yeah, this made me think we need a C89/C99 backend to LLVM.
I think that unboxed means that it is on the stack, instead of the heap, but i'd wait to confirm
The RFC that introduces unboxed closures: [RFC 114](https://github.com/rust-lang/rfcs/blob/master/text/0114-closures.md). Short version: current closures in Rust are "boxed" in that they consist of a function pointer, and a pointer to the closure environment (which contains captured variables). LLVM has trouble inlining and optimising around these indirections. Unboxed closures work by having each closure expand to an anonymous type which is effectively a `struct` that contains the captured variables, and implements one of the three kinds of "call" operator. Because there's no indirection involved, LLVM can much more easily inline and optimise these calls. Eventually, it won't be unboxed vs. boxed closures, there will just be unboxed closures and everything will be puppies and unicorns.
Dump the AST from the compiler, use an ASTPath Expression to do the rename using RQuery and dump it back to source? Output will be clean since the file and line number info is now in the AST. edit: I made this all up, someone please make it true.
&gt; &gt; Code using lots of macros can still quickly become hard to figure out. &gt; &gt; It's really on the macro author to make sure that the semantics of the macro are clear from its use without understanding the expansion. But someone reading code usually doesn't know how far he can trust some macro author to not do surprising things inside the macro. I might have no idea who wrote the macro or what *exactly* he wanted to accomplish with it. I have to expect a lot more things that could happen compared to a normal function call. &gt; I'm pretty confident this is an instance of the latter Might very well be, such a parser is a complicated thing and it's expected that it isn't trivial to understand. But generally, as a reader of some piece of code, I still have to worry if a macro really (only) does what it looks like. Can I just trust that the macro doesn't have some unexpected side effect?
Everything which consumes data from the internet and is currently consumed by a C library. This includes JSON, XML, images, videos, and more. Why? Finally get rid of buffer overflow exploits and other C problems. I really like the idea to get a Rust image decoder into Firefox. :)
Just want to add that we can't (yet) use unboxed closures as return values because they have anonymous types. For example: // what's the return type? fn adder(y: int) -&gt; ??? { move |&amp;: x| y + x } For the time being, we'll have to return a "boxed unboxed closure", which is effectively just a trait object: fn adder(y: int) -&gt; Box&lt;Fn(int) -&gt; int + 'static&gt; { box move |&amp;: x| y + x } If/when we get ["unboxed, abstract return types"](https://github.com/rust-lang/rfcs/pull/105), we'll be able to express the first case without the heap allocation/trait object indirection: fn adder(y: int) -&gt; impl Fn(int) -&gt; int { move |&amp;: x| y + x }
this makes a lot of sense. thank you! i wouldn't have thought of instantiating the result with the one's already there. now i'm wrapping my head around the .zip() and rev().zip() pattern
thanks a lot. this is very readable and the comments are really helpful. the first example makes the second example much clearer too
What does it do that GMP doesn't, besides the matrices and polynomials?
Rust's regular expressions are already compile-time, and for Eigen there is alge-bloat.
[Still hammering out the details](https://github.com/rust-lang/rust/pull/17864)
&gt; is "macro!" a macro itself? Yes. As cute as it is, I always thought this was a bit silly, and I remember finding it confusing and distracting when I first learned Rust. Why implement macro definitions as a syntax extension, rather than implementing them directly in the compiler? When newbies think of the macro syntax, they think of `function_like_syntax!()`, rather than `sort_of_struct_like_syntax! ( &lt;several lines&gt; )`. If it were in the compiler, it would also be possible for it to use the more natural `{}` braces to enclose its patterns, rather than the weird-looking `()` parentheses, right?
There is a big difficulty with cryptographic libraries: in order to prevent timing attacks, a number of the building blocks must execute in constant time regardless of the input (and its validity). This is extremely difficult to get without dropping to assembly level (and preventing compiler optimizations), and therefore I wonder how much of the library could actually be written in Rust.
You can dump the AST with `-Z ast-json` but it has no span information (for dumb reasons no less).
Pretty much what BurntSushi said. Usually you just change what you want and the compiler will force you to fix the rest. A couple of gotchas that have cropped up in my experience: `Copy` types and autoderef. **Autoderef** is when a pointer (any type that implements the `Deref` trait) is auto-dereferenced. For example, `let p = gimme_pointer(); p.foo()` will call `foo()` on `*p` if the method is not on `p` itself. With autoderef, renaming methods can have funky consequences if there already is a method of that name on the smart pointer. An example of this is [here](https://github.com/Manishearth/servo/commit/707a2870fa5a7c10dad49a8ab0db39c7e568f1c6#diff-7fa3011afc74ee7695cb180db1da7791L168) (The explicit `deref()` call is the same as dereferencing it once). `self` here is an `&amp;Rc&lt;T&gt;`, calling `self.deref().trace()` is a recursive call -- there should be two dereferences. Normally autoderef will handle that for you, but in this case all of the pointer types have a `trace()` method. ------ **Copy types** are those which get implicitly copied. For example. `let x = 1u; let y = x;` will just copy the integer to `y`. However, `let x = "foo".to_string(); let y = x;` will *move* the string, making `x` unusable after that point at compile time. Owned pointers and other things get moved, primitive types and types implementing `Copy` (this happens to structs containing `Copy`-only data) get copied. References get copied too, but they are copied so that they refer to the same thing. Sometimes refactoring can lead to references turning into the data itself. An example of this is [here](https://github.com/Manishearth/servo/commit/707a2870fa5a7c10dad49a8ab0db39c7e568f1c6#diff-68415f2d790c8fc2d7db64e00a71bb6dL361). `last_reflow_id` is a `Cell`, giving internal mutability (`Cell` is `Copy`) . In this refactor, I was removing uses of the wrapper `Traceable`, which used `Deref`/autoderef to yield its inner value -- a _reference_ to its inner value. Which would always be moved, never copied. Most of the transition involved removing uses of `Traceable` and any lingering `deref()`s used to dereference the `Traceable`. Obviously, I wrote some `sed` scripts to make it easy. In most cases it worked. Uses of `Traceable&lt;Cell&lt;...&gt;&gt;` were limited to stuff like `self.width.get()` (which is just getting the value) and `self.width.set(...)` (which modifies the value in place without copying). However, in this case we do the following: // Before sed script let last_reflow_id = self.last_reflow_id.deref(); // &amp;Cell&lt;uint&gt; last_reflow_id.set(last_reflow_id.get() + 1); // reference to a Cell, no copying // After sed script let last_reflow_id = self.last_reflow_id; // Cell&lt;uint&gt;, the assignment copies the Cell last_reflow_id.set(last_reflow_id.get() + 1); // modifies the copied cell Which means that the actual inner value of `self.last_reflow_id` is not changed after the refactor. This led to a rather hard to track down bug where reftests would hang since the counter was never incremented (but not when Servo was run normally, because Servo doesn't exit after layout is complete -- that's done only when running reftests). This was fixed by changing it to let last_reflow_id = &amp;self.last_reflow_id; // &amp;Cell&lt;uint&gt; again last_reflow_id.set(last_reflow_id.get() + 1); // modifies the contents of the reference, which is fine ---- There probably are other gotchas, just that these are the two that have hit me hard while refactoring (autoderef has tripped me up more than once). It's additionally useful to split up your refactoring into smaller commits which individually compile (as I did with the [PR for the changes above](https://github.com/servo/servo/pull/3518/commits)). Any test failures can be pinpointed easily with `git bisect`. If this had been done as a single huge commit, I would have been chasing red herrings for days :) 
This. Nokogiri I think presents a very good model to build upon.
I'm not sure this would be possible in general, since you could end up with a situation like this: https://gist.github.com/stevenblenkinsop/945de41353b05b7e53f0 On the other hand, I can't think of a scenario where this would work where the type isn't lifetime-parameterized in some way, since &amp;'a T is the only Copy reference type that I'm aware of. So perhaps it's possible in the limited case where there are no lifetimes involved.
Is `move` a keyword? Somehow I missed it so far.
Most of the Haskell things, but one of the most interesting and not-just-abstract-nonsense ones is [diagrams](http://projects.haskell.org/diagrams).
I personally think it would be fine to allow refutable patterns in `for` loops, with the meaning it skips over the elements which don't match the pattern, but others have disagreed (e.g. /u/dbaupp).
algebloat is far _far_ from eigen. Eigen performs non-trivial optimization passes on the AST of the EDSL. the regex crate is indeed pretty good. 
A Rust-y Akka
Yes. http://doc.rust-lang.org/reference.html#keywords
&gt; Why do you need to specify the return type? Writing out the full type of a function signature is generally considered best practice in languages that do full inference, and so we've made the decision to require it. It's usually really nice. What C++14 does is a partial solution, but the one we'll have is slightly different, and a bit better. Some variant of https://github.com/rust-lang/rfcs/pull/105
Thnx. Do you have a link explaining what `move` does?
Currently Rust intentionally does not infer return types or argument types, because it makes the interfaces clear and does not magically change the public signatures of your functions if you change some private code.
Could you also try a recent version of clang on that C code and also Rust with `-C no-stack-check`? I believe GCC manages to "unroll" the recursion two or four times. There's also a nice optimization potential here that a superoptimizer could exploit, I'll try to find my notes on it, or remember the general idea. Also, [this](http://play.rust-lang.org/?code=use%20std%3A%3Aos%3B%0A%0A%2F%2F%20fib%20returns%20a%20function%20that%20returns%0A%2F%2F%20successive%20Fibonacci%20numbers.%0Afn%20fib%28n%3A%20u64%29%20-%3E%20u64%20%7B%0A%09if%20n%20%3C%202%20%7B%0A%09%09return%20n%3B%0A%09%7D%0A%09fib%28n-1%29%20%2B%20fib%28n-2%29%0A%7D%0A%0Afn%20main%28%29%20%7B%0A%09let%20n%20%3D%20from_str%3A%3A%3Cu64%3E%28%26*os%3A%3Aargs%28%29%5B1%5D%29.unwrap%28%29%3B%0A%0A%20%20%20%20let%20result%20%3D%20fib%28n%29%3B%0A%09println!%28%22LANGUAGE%20Rust%3A%20%7B%7D%22%2C%20result%29%3B%0A%7D) is closer to idiomatic and uses one line for parsing the argument.
I know that it is not nearly as mature. But as a project, it serves the same purpose (providing a matrix template library).
It'd be a change from standard policy and it's really premature since I haven't actually written anything yet. But if it's neat I'll see if I can talk about it.
One correction, &amp; references are actually Copy as well, so they will be copied rather than moved. Same goes for &amp;str. This means, that if you do `let x = "foo"; let y = x;` and then use x, it will work just fine.
find . -name '*.rs' -print0|xargs -0 perl -i -p -e 's/ABC/DEF/g' git ls-files -z '*.rs'|xargs -0 sed -i -r -e 's/ABC/DEF/g'
This is a benchmark of function calling. If the now useless stack check that Rust performed hasn't been removed yet, that's the problem. Alternatively, a different inlining algorithm might be the cause. BTW, this is a HORRENDOUS benchmark, since performance all depends on the compiler's inlining heuristics and code size/performance tradeoff, and also the compiler could in principle replace the computation with an efficient algorithm. 
Quickly ran this: rustc --opt-level=3: 94.98s rustc -O -C no-stack-check: 78.28 go build: 106.48s
If you find something in practice that contradicts this, file a bug.
I'm just going by what rust-lang.org says. ;)
Yes, results are much better. I assumed opt-level 3 is maximum optimization. What does -C no-stack-check means? /usr/local/bin/rustc -C no-stack-check --opt-level 3 fib.rs /usr/bin/time -lp ./fib 50 LANGUAGE Rust: 12586269025 real 71.56 user 71.50 sys 0.01 1044480 maximum resident set size 0 average shared memory size 0 average unshared data size 0 average unshared stack size 276 page reclaims 0 page faults 0 swaps 0 block input operations 0 block output operations 0 messages sent 0 messages received 0 signals received 0 voluntary context switches 451 involuntary context switches 
I took your statement as claiming that it was wrong. :)
Is the large number of page faults recorded due to stack size?
thanks lol 
&gt; If it were in the compiler, it would also be possible for it to use the more natural `{}` braces to enclose its patterns, rather than the weird-looking `()` parentheses, right? You can already [do this today](http://is.gd/JH7nSy). The macro system mostly doesn't care *which* delimiters you use to enclose token trees. &gt; As cute as it is, I always thought this was a bit silly, and I remember finding it confusing and distracting when I first learned Rust. Yeah, it is confusing. It's a macro that expands to nothing, and modifies the expansion context as a side effect. On the other hand it allows the implementation of the macro system itself to be less tightly coupled to the compiler, which is important if we want to evolve that system post-1.0 (even in third party libraries). It's not a huge thing though; we could just reserve `macro` as a keyword and have it invoke the syntax ext by the name `macro!`.
Good compilers must optimize this to O(1).
We could potentially, but neither of the `visible(...)` attributes corresponds nicely to `pub`, so it would be confusing to suggest a similarity where there is none.
That is a more complete list than I thought there would be for rust pre 1.0.
It disables stack checks, which we've been trying to replace with OS support (guard pages + SIGSEGV or the equivalent) + stack probes (for functions with very large stacks), but AFAIK patches providing LLVM support for the latter have been waiting for months now :(. 
[I've already given a nicer form here](https://www.reddit.com/r/rust/comments/2lolci/is_rust_slower_than_go/clwozba). let n = from_str::&lt;u64&gt;(&amp;*os::args()[1]).unwrap(); The `&amp;*` combo coerces `String` to `&amp;str`, just like `.as_slice()` or the `[]` operator (the latter might disappear soon). If a certain (quite popular) proposal is accepted, the following will also be valid: let n = from_str::&lt;u64&gt;(&amp;os::args()[1]).unwrap(); (where `&amp;` produces a `&amp;String` which then coerces to `&amp;str`)
This is wonderful. Not sure how I didn't see this, I *really* need to improve my google-fu.
Actually, the author argues in the Rust forums that with the current language it is extremely hard to even maintain the library in its current state. It is a nice prototype, but if it shows something, is that Rust is not on par with C++ in this respect. One thing about Eigen that most people don't know, is that its code is actually very simple and clean (considering what it does). 
As long as the toolkit's per app themeability is strong enough I don't need it to match my desktop. Things like [Tox's](https://tox.im/assets/ss.png) front-page GUI are a great example of something I wouldn't be bothered by.
thanks for the link!
I wrote the guide, or at least, provided most of the text. Glad you liked it. :) I'm _just_ about to jump on a plane, but a few things: We eventually talk about this in the section on patterns, I think. And yes, they're always exhaustive, but `_` is a 'everything' case, so I can see why it might not feel that way. And you're right that it can't tell in your case, and that this is a good place for `panic!`. It might worth adding a tiny note about it, sure :) Yes, iterators do no allocation. They're more like Ruby's lazy enumerables than the regular ones. They do have internal state, but I think the behavior you're observing is that `take()` doesn't actually modify it, it makes a new adapter. &gt; Pedantry alert I love pedantry! :) But I'm not so sure about this one. Can you cite using 'parenthesis' as an uncountable noun? The few dictionaries I checked agree with me :) (I have a feeling this is a British vs. American English thing...)
Beside that, what eddyb already said, you can just leave out the first two lines: let n: u64 = from_str(os::args()[1].as_slice()).unwrap(); Especially the conversion of the vector to a slice is unnecessary. 
Compare the assembly that gcc and clang generate: http://goo.gl/tD53J3 and http://goo.gl/LvVgnq It appears that gcc is eliminating one of the recursive calls per recursion while llvm is not. I don't believe it is reasonable to depend on your compiler to perform this kind of optimization and you should instead write more reasonable code.
# Mutability &gt; I see now that (1) will let y change... Your intuition about mutability is correct! # Pattern Matching Pattern matching must be exhaustive, however there are limits to what the compiler can prove. The `_` pattern will match anything, so any match with `_` is exhaustive. # Iterators * Rust's iterators are lazy, so they will only be iterated through once. * If you take a look at the docs for `take(_)`, you'll see that `take` passes in `self` as an owned parameter. This means that the iterator returned by `range` is actually being copied!. Your code could also be written like this: let r1 = range(10i, 20i); let r2 = r1; // IMPLICIT COPY let x = r1.take(5).collect::&lt;Vec&lt;int&gt;&gt;(); let y = r2.take(5).collect::&lt;Vec&lt;int&gt;&gt;(); If you wanted to have them both pull off of the same range, I'd use the `by_ref` method. let mut r = range(10i, 20i); let x = r.by_ref().take(5).collect::&lt;Vec&lt;int&gt;&gt;(); let y = r.by_ref().take(5).collect::&lt;Vec&lt;int&gt;&gt;();
&gt; I think the behavior you're observing is that take() doesn't actually modify it, it makes a new adapter. I think it's because the struct returned by `range` is `Copy`, so it gets copied.
Thanks for explanation !! Does that mean with no-stack-check flag, it will not fulfill "prevents almost all crashes*" promise.
Thanks, fixed :)
Thanks!! Hope proposal will get accepted because I feel as_slice() and unwrap() is overhead as from_str::&lt;u64&gt; should be sufficient enough for compiler to figure that out :)
re: Iterators. I wonder if your example is exactly the same as: range(1i, 1000i) .filter(|&amp;x| x % 2 == 0 &amp;&amp; x % 3 == 0) .take(5) .collect::&lt;Vec&lt;int&gt;&gt;()
How is unwrap() overhead? You can't know that a string you get from the user (I'm assuming that's what `os::args()` does) is on the form that you can parse it as an integer.
With unwrap(), when I pass non-integer (` ./fib abc `), I get following error: task '&lt;main&gt;' panicked at 'called `Option::unwrap()` on a `None` value', /Users/rustbuild/src/rust- buildbot/slave/nightly-mac/build/src/libcore/option.rs:347 Error message is not helpful to troubleshoot the issue is with non-integer number.
Did you check if `x / (1 &lt;&lt; k)` might be optimized into signed right shift?
You could use `.expect` which would make the error more apparant.
It's amusing how almost incompetent programmers start focusing on performance so quickly without bothering to learn how to code more efficiently.
You can use the asm! macro http://doc.rust-lang.org/guide-unsafe.html#inline-assembly
A library of persistent data-structures. A port of Sage's awesome libraries: http://www.sagemath.org/doc/reference/combinat/index.html
&gt; However, let x = "foo"; let y = x; will move the string, making x unusable after that point at compile time. Still inaccurate! Maybe if you change the example to `"foo".into_string()`?
I started working on a tool that could rename variables/methods/types intelligently earlier this year: https://github.com/jdm/rsed
&gt; the compiler's inlining heuristics and code size/performance tradeoff They are indeed very important in writing a performant system software, aren't they?
Oh right. *facepalm*. Fixed :)
 match x { Value(i) if i &gt; 5 =&gt; println!("Got an int bigger than five!"), Value(i) if i &lt;= 5 =&gt; println!("Got an int smaller than five!"), Missing =&gt; println!("No such luck."), } The trouble here is that exhaustiveness checking doesn't apply to the "if" part, so both matches on `Value` are filed under "might not match". Consider the general case, which is that *anything* can be done in `if`, so rustc would have to evaluate arbitrary expressions during compile time. If you want to do such things, try agda or idris or coq or whatever, but you probably don't want to. Try this: match x { Value(i) if i &gt; 5 =&gt; println!("Got an int bigger than five!"), Value(i) =&gt; println!("Got an int smaller than or equal to five!"), Missing =&gt; println!("No such luck."), }
Haven't looked closely, but my first impression is that Rust provides rather similar features. In addition, the checker would seem to provide significant help with avoiding certain kinds of races. Can anyone else speak to this?
&gt; `&amp;*os::args()[1]` I have to say this piece of code is enigmatic... I hope `slicing_syntax` is stabilized soon so that we can write `os::args()[1][]`
I would be more interested in multiple drivers for the browsers that are in use today by my actual users: Firefox, Chrome, IE, Safari, etc. Servo would certainly be good to have as an *additional* driver someday once it's mature and fully functional.
Please tell me those cheap ad-looking buttons are temporary...
&gt; Can you cite using 'parenthesis' as an uncountable noun? I don’t think OP was saying that ‘parenthesis’ is an uncountable noun. It’s just that in a couple of places in the guide (section 6, ‘some parenthesis’ and section 8.1, ‘the parenthesis and commas’), ‘parenthesis’ is used as a plural instead of [‘parentheses’](https://en.wiktionary.org/wiki/parentheses). That’s only done a couple of times in the guide, so it’s probably just a typo.
While I agree with Gankro's comment, I disagree with this further point. Analyzing the complexity of programs and having an idea of asymptotic complexity is really important, but so is actually measuring performance if you are at all concerned with performance measurements. A computer science professor of mine put it this way (in class the other day): "I've stopped trying to write particularly efficient algorithms when I prototype a piece of software, because all of the times I've actually profiled my code, I have literally always been wrong about where my program spent most of its time." Now, obviously, this professor hasn't been writing intractable, horribly complex algorithms - when he says not particularly efficient, he means "I went for the simple O(n^2) algorithm when I knew there's probably a O(n log n) version if I did something clever". But nonetheless, for a beginner interested in performance, it seems entirely worthwhile to do things like measure performance in terms of runtime (or use a more sophisticated profiling technique). Anyway. You have to get competent at coding somehow. One way to learn to code more efficiently is to actually write programs and see how well they perform. 
I think you can make a pull request to the docs here: https://github.com/rust-lang/rust/blob/master/src/doc/reference.md
I assume LLVM has knowledge about malloc, so it can see that you allocate memory and free it without using it, so it can be optimized out.
The reference is at https://github.com/rust-lang/rust/blob/master/src/doc/reference.md just edit the file (you can do so inline via github's interface if you want) and create a pull request. You can probably edit the `&lt;&lt;` doc as well, since they have the same error of specifying a *logical* shift when it's just a shift (and can be implemented to not do a shift at all, really). Rust devs will review the PR. See https://github.com/rust-lang/rust/blob/master/CONTRIBUTING.md for the full contribution guideline, but since it's just a small documentation fix most of it isn't going to apply.
&gt; With unwrap(), when I pass non-integer (./fib abc), I get following error: Well yes that's what unwrap does, if called on a `None` (~ a null) it faults (kills the thread) and provides a generic (and mostly useless as it gives a line number in option.rs rather than in your code) complaint: http://doc.rust-lang.org/src/core/home/rustbuild/src/rust-buildbot/slave/nightly-linux/build/src/libcore/option.rs.html#344 You can provide a custom error message using .expect: http://doc.rust-lang.org/src/core/home/rustbuild/src/rust-buildbot/slave/nightly-linux/build/src/libcore/option.rs.html#312 
Well, I'm on the side were we hope to remove `[]` because Deref is more useful in general, and `&amp;` is one char less than `[]` (or `[..]`).
I believe it's a crash either way.
&gt; Is it in C? Yes. It's not part of ANSI/ISO C. You can consider it a popular "compiler extension" but the behaviour is not guaranteed by the standard. Though, I'm not up-to-date. In the 2011 version of C this might have changed.
Conceptually, I don't know why you would want a "safe" and "unsafe" interface. "Safe" in this context just means the compiler can rule out certain classes of memory bugs, whereas "unsafe" means you (the library writer) have to verify it yourself. If you have already done the work to write an unsafe implementation that you trust, also having a "safe" implementation doesn't help you any. Either your algorithm is sound under the conditions you use it, or it isn't. Unless what you mean ins't "safe" in the Rust "unsafe" keyword sense, but in the sense that you'll skip certain bounds checks and sanity checks.
No, that's not what I mean. I mean, C is not the only language in the world, and it's not even a very good language. Judging Rust by C is not a strategy for building a good language. Plenty of languages have well-defined behavior for right-shifting signed 2's-complement values. It's not some "language extension", for those languages.
I'm just curious, was it an official statement? I mean, a statement like "If your program runs slower on Rust than other languages, file a bug" I've seen [PathScale EKOPath C++ compiler](http://www.pathscale.com/ekopath.html) officially talked about performance in such a way.
Ah! How silly of me!
Wow, the slice operator is going away already? What about subslicing vectors and strings? Or did you mean "former"?
I was trying to make a libwayland wrapper for Rust for a while. I kinda hit a wall when it came to `get/set_user_data` and I haven't been able to motivate myself to get past it. I want it to be safe (i.e. not relying on the user to know the type when it came to `get_user_data`), but every time I think of an idea it's been shot down by some other thing. I have no idea how the server-side is supposed to work either ([the documentation is pretty poor](http://wayland.freedesktop.org/docs/html/sect-Library-Server.html)) so it was all client side, at least for the moment.
It's worth noting that the C and Go versions don't work. That is, they don't check for an error and handle it in some basic way (as unwrap does). Rather, IIUC, they both ignore any error. 
Oh no worries if you can't. Still cool to hear that your boss was ok with it. :)
Just to be clear, gcc is performing this optimization: http://stackoverflow.com/a/10058784
&gt; I like that rust is actually a systems language (go was supposed to be, but then threw in a GC and then denied that they were supposed to be a systems language). Rob Pike just has (had?) another conceptualization of what "systems" mean than the people from Rust. In his case, I think it means something like "non-application software development", ie. servers and software that run underneath the 'application layer' ("I work on lots of things, mostly the things under the things you work on."). But as you know, as soon as people in general started to talk about both languages in the same breath, the term became ambiguous. 
No need for a lookup table: http://mathworld.wolfram.com/BinetsFibonacciNumberFormula.html
I feel silly now, I played with that formula once back in high school, and had forgotten all about it.
In that case, any idea why the convention for `macro_rules!` is to use `()` at its top-level delimiter, rather than `{}`? Seems like a strange choice.
Hmm, it really seems to be harder than I initially thought. I only found an algorithm which can do it in O(log(n)). Of course it is only usefull if you care about values after overflow. If you don't then you can actually use a lookup table because it overflows pretty fast.
It's worth a notice that panic! does not kill the thread in the traditional sence, like pthread_kill. It raises an exception instead (on the IR level), which unwinds the thread stack. If it would've really killed a thread then you could as well kill the process next, because you'd have memory leaks, file descriptor leaks, lock leaks etc. The task is not killed, it gracefully terminates, there's a whole load of difference between these concepts. Unlike being killed, the termination is kind of voluntary, the task still lives when it performs the termination's cleanups: "If a value with a destructor is freed during unwinding, the code for the destructor is run, also on the task's control stack. Running the destructor code causes a temporary transition to a running state".
He probably refers to the idea that mutable state should be avoided where possible, and if 'var' could be used instead of 'let mut', people, especially novices, may reach out to 'var' as the default choice.
It might be so, but I despise when someone creates deliberate roadblocks in the language to prevent what they think is misuse. I agree that mutable state should be used sparsely, but it is my choice, dammit!
&gt; 2- libc::malloc seems to be the fastest allocator. is there any downside to using it instead of heap::allocate ? * heap::allocate [forces aligned allocations](http://doc.rust-lang.org/src/alloc/home/rustbuild/src/rust-buildbot/slave/nightly-linux/build/src/liballoc/heap.rs.html#202). Most mallocs do aligned allocations anyway (that's explicitly mentioned in OSX's malloc(3), Linux's malloc(3) and openbsd's malloc(3)) so I wouldn't expect much difference (some platforms will crash on unaligned access, x86 will slow down, if you're looking for speed you're probably looking for aligned allocations) * from what I can see, heap::allocate might use jemalloc, libc::malloc always uses the system's malloc. On my machine (2010 MBP on OSX), heap::allocate takes half the time of libc::malloc so this may or may not be a downside &gt; Isn't that 0 ns/iter a bit stange ? Your use of `black_box` is incorrect: you're using it on the result of the loop which is unit. Apparently with `libc::malloc` the optimiser realises the loop body is a noop (it's just allocating and deallocating) so it can remove the whole thing, then remove the loop (since it does not do anything) and pass a constant `()` to the `black_box`. `black_box` does not disable optimisations, it just makes the optimiser believe the value you're giving it is used, and avoids *that value* being optimised away. The optimiser will still do as much as it can to remove stuff. `black_box` should be called on the result of the operation you're measuring, so that the optimiser does not go "unused variable -&gt; remove -&gt; useless call -&gt; remove" and optimise away the very thing you're trying to measure. In this case, it should be called on `buff` and `vec`. And indeed, adding a `test::black_box(buff);` in the malloc bench (right before `libc::free`) fixes it. Also according to the guide you're not supposed to do runloops within the benchmark, the bench harness does test runs and finds out a bench loop for itself: &gt; Make the inner iter loop short and fast so benchmark runs are fast and the calibrator can adjust the run-length at fine resolution Without `process`, with `black_box` on the allocation's result and without the manual inner loop I get: test alloc_test ... bench: 31 ns/iter (+/- 20) test malloc_test ... bench: 66 ns/iter (+/- 23) test vec_alloc_test ... bench: 33 ns/iter (+/- 12) with the manual inner loop I get: test alloc_test ... bench: 354275 ns/iter (+/- 144235) test malloc_test ... bench: 626320 ns/iter (+/- 117160) test vec_alloc_test ... bench: 354132 ns/iter (+/- 134917) and the bench runs *much faster* so I'll assume the second one is significantly less precise because it goes above some internal time-per-iter limit This is the bench after my modifications, in case I broke something: extern crate alloc; extern crate test; extern crate libc; use test::Bencher; use alloc::heap; use std::mem; static BUF_SIZE: uint = 100u * 100u; #[bench] fn malloc_test(b: &amp;mut Bencher) { b.iter(|| { unsafe { let buff = libc::malloc(BUF_SIZE as libc::size_t) as *mut u8; if buff.is_null() { panic!("failed to allocate memory"); } test::black_box(buff); libc::free(buff as *mut libc::c_void); } }); } #[bench] fn alloc_test(b: &amp;mut Bencher) { let align = mem::min_align_of::&lt;u8&gt;(); b.iter(|| { unsafe { let buff = heap::allocate(BUF_SIZE, align); if buff.is_null() { panic!("failed to allocate memory"); } test::black_box(buff); heap::deallocate(buff, BUF_SIZE, align); } }); } #[bench] fn vec_alloc_test(b: &amp;mut Bencher) { b.iter(|| { let vec: Vec&lt;u8&gt; = Vec::with_capacity(BUF_SIZE); test::black_box(vec); }); } 
This just seems like a bug. It was not meant to be a breaking change in this respect.
Then choose to type a few extra letters.
It is a benchmark of function calling.
the syntax for `let` is not `let [mut] ident` it is `let pattern`. It allows for code like this: let (mut a, b): (int, int) = returns_2_tuple(); a = 12; [playpen-link] It is usually not a good idea to suggest features for a language you do not know :) [playpen-link]: http://play.rust-lang.org/?code=use%20std%3A%3Adefault%3A%3ADefault%3B%0D%0A%0D%0Afn%20returns_2_tuple%3CT%3A%20Default%3E()%20-%3E%20(T%2C%20T)%20%7B%0D%0A%20%20%20%20(Default%3A%3Adefault()%2C%20Default%3A%3Adefault())%0D%0A%7D%0D%0A%0D%0Afn%20main()%20%7B%0D%0A%20%20%20%20let%20(mut%20a%2C%20b)%3A%20(int%2C%20int)%20%3D%20returns_2_tuple()%3B%0D%0A%20%20%20%20println!(%22old%20a%3A%20%7B%7D%2C%20b%3A%20%7B%7D%22%2C%20a%2C%20b)%3B%0D%0A%20%20%20%20a%20%3D%2012%3B%0D%0A%20%20%20%20println!(%22a%3A%20%7B%7D%2C%20b%3A%20%7B%7D%22%2C%20a%2C%20b)%3B%0D%0A%7D
Not possible if I really need mutable variable. The point is that I don't believe you should push people into doing the right thing with syntax. It has a whiff of Java-like deliberate bureaucracy.
I think the few extra letters were meant to be " mut", and that is very possible. Please note that both your suggestions are not workable in the general case. `let` accepts any irrefutable pattern. E.g. `let (x, mut y) = (1i8, 3i8);` which makes `y` mutable, while `x` isn't.
Branching recursion seems like a form of function calling that will be explicitly quite poor, though, right?
&gt; a cheap way for lazy programmers to get the value It's useful in one-shot programs or test scripts where you don't really care for proper error handling.
Rust provides pretty much the same concurrency features in channels, as well as many other concurrency constructs, but unlike Go it can guarantee that they are data race safe. While this isn't exposed by the standard library right now, Rust also has language level capabilities for pointing directly into other thread's stacks. The major advantage Go has is that its Goroutines (green threads) are very cheap, while Rust's tasks are full OS threads. I would agree that green threading is something Go excels at (not necessarily concurrency in general). Whether this is important in practice depends on your application.
Why would it be inherently worse than just calling a function a lot of times?
Haha, yeah, sorry that's what I was trying to say. Re-reading my post I can definitely see how it can be interpreted in the exact opposite manner from what I meant!
Shortening it will lead to: - More confusing keywords. `let mut` is quite obvious. `letm` is not. `var` is _definitely_ not. I've been programming Javascript since I was 9, and I _still_ sometimes forget which one of `a=5` / `var a = 5` sets the global variable. - More people using it, especially if it's `var`. We don't want that. You'll be surprised as to how little we really need mutability while programming. For a language like Rust, where more mutability means more issues with the borrow checker, this isn't too good. Mutability should be opt in, not what people use as the _default_. `var` would lead to many newbies naively using it, and then having worse problems with the borrow checker. Rust is hard enough to get used to as it is. - `let` destructures, as Sinistersnare mentioned. `let (mut a, b) = (1,2)` works, in your scheme it won't
Oh, is there a way to submit a PR to fix that? I honestly did a brief search to see if I could just fix it myself rather than bring it up here, but I didn't see the rust guide on github [here](https://github.com/rust-lang).
If you really need a mutable variable, use `mut`. There's nothing stopping you. 
It's not a deliberate roadblock. But yes, changing it to `var` will lead to novices misusing it. And `mut` isn't really blocking you, just making you type a few extra characters. Nobody's denying you your choice to make variables mutable. And yes, I called it misuse. In Rust mutability is important. Not from a philosophical point of view, but from the point of view of your code compiling. Making everything mutable would lead to more trouble for newbies. 
I agree that it's not a good argument for not providing ASR in Rust especially since almost every CPUs nowadays provides such an operation. Just wanted to confirm the C situation.
See also [an old mailing list thread](https://mail.mozilla.org/pipermail/rust-dev/2014-January/thread.html#8319) and [issue #2643](https://github.com/rust-lang/rust/issues/2643).
&gt; However, the compiler doesn't look at match guards. [Confirmed](http://play.rust-lang.org/?code=fn%20neg1%28b%3A%20bool%29%20-%3E%20bool%20{%0A%20%20%20%20match%20b%20{%0A%20%20%20%20%20%20%20%20true%20%3D%3E%20false%2C%0A%20%20%20%20%20%20%20%20false%20%3D%3E%20true%2C%0A%20%20%20%20}%0A}%0A%0Afn%20neg2%28b%3A%20bool%29%20-%3E%20bool%20{%0A%20%20%20%20match%20b%20{%0A%20%20%20%20%20%20%20%20val%20if%20val%20%3D%3E%20false%2C%0A%20%20%20%20%20%20%20%20val%20if%20!val%20%3D%3E%20true%2C%0A%20%20%20%20}%0A}%0A%0Afn%20main%28%29%20{%0A}). The first function doesn't give a compiler error, but the second does.
Hi and thanks you very much ! Yes I didn't understand how that balck box worked, but now I do thanks to you. After using your version of the benchmark, I have similar result on : Windows 7 - 64 : running 3 tests test alloc_test ... bench: 28 ns/iter (+/- 0) test malloc_test ... bench: 52 ns/iter (+/- 1) test vec_alloc_test ... bench: 27 ns/iter (+/- 0) And on Linux (ubuntu 64): running 3 tests test alloc_test ... bench: 14 ns/iter (+/- 1) test malloc_test ... bench: 49 ns/iter (+/- 0) test vec_alloc_test ... bench: 14 ns/iter (+/- 0) So I think i'll stick with using vec to store data and manage memory allocation. Thanks agan ! 
Well, for one, the types are different. Each successive filtering "wraps" the iterator in a new object that also implements `Iterator`. Of course, this happens on the type system; I have no idea if those wrappings actually trickle down to the actual compiled code.
My main critique is that you appear to be using `Box` way more than should be necessary.
Ah, you're right, the connection structs don't need to be boxed. Good catch.
How exactly do abstract return types work? In a C-style stack, you need to know how big the return value is so you can reserve the space in the caller's stack frame for it. How do you get around that restriction? Does the compiler statically determine the max possible size of the return type?
Ignorance is bliss :) Here idea is to write similar implementation in different language and see the performance. See the rust-http performance results where Go outperforming Rust. =========== ===== ===== ===== Concurrency Node Go Rust =========== ===== ===== ===== 1 10400 8600 10100 2 11400 21000 12300 3 11900 22000 13300 8 12400 21000 15000 =========== ===== ===== ===== https://github.com/chris-morgan/rust-http/tree/master/comparisons 
I don't understand the difference between lock and lockfree, both appear to spawn a proc per connection
In chatserver.rs each connection proc shares the list of all active connections used for broadcasting, so it has to be explicit put behind a Mutex object. In lockfree_chatserver.rs only the manage_connections proc has the list of active connections, so no explicit locking is needed (the channel synchronizes it internally).
I'm sorry, but those numbers are old and from an obsolete library. Nobody has bothered optimizing it for a long time.
Does this mean that (if the proposal gets passed) soon we will be able to write `from_str(os::args()[1])` (without any `&amp;` or `[]`'s), and the compiler will automatically read it as `from_str(os::args()[1].as_slice())`?
Yes. A sensible discussion about the downsides/failings is fine, but abuse is not.
Was under the impression that Rust would let you choose green threads (M:N threading) by simply adding an option, though native threads (1:1 threading) are the default.
They're in the process of being removed from the standard library (I would just say "being removed" but every time I do so someone points out that they will still be available in an external library). In any case, Rust's green threads were never really lightweight.
I wouldn't expect you could benefit from inlining or any kind of TCO (not that we have the latter). Although I suppose one could "flatten" the recursion by making a version with like 3 layers inlined and recurse with that? I'm mostly talking out of my ass at this point, though. :P
You can't beat O(n) because that's the size of the output in bits. And if you don't care about supporting arbitrarily large outputs, then there's only like 100 fibonacci numbers that fit into 64-bits (I assume no one cares about whatever fib(10000) overflows into), so you might as well use the naive iterative algorithm to build a lookup table.
I mean, that largely *is* what it does that GMP doesn't. It also provides a really nice interface to lots of useful functions. GMP is just bignums.
So we have multiple types of `self` arguments. There is `self`, which is of type `T`, and conversely moves or copies the object into the method (a move makes the original variable useless after the method is called). Then there is `&amp;self`, which is of type `&amp;T` and immutably borrows the object. Finally we have `&amp;mut self` (`&amp;mut T`), which mutably borrows the arguments. Firstly, your first impl an also be done as struct A5 (u64, u64); impl Mul&lt;A5, A5&gt; for A5 { fn mul(&amp;self, &amp;A5 (c, d) : &amp;A5) -&gt; A5 { let A5 (ref a, ref b) = *self; A5 ((a*c + 5*b*d)/2, (b*c + a*d)/2) } } (this is the common style in the wild) Unfortunately you can't destructure the `self` argument in the argument list itself. `self` is both the type and name of the variable, it's special (the internal compiler representation is special too). You have to do it in the next line. The keyword `Self` is when you want to declare traits that refer to the type they are implemented on in their definition — it doesn't have much of a place outside of trait definitions.
It's not, see http://www.reddit.com/r/rust/comments/2lolci/is_rust_slower_than_go/clwrr98 and my reply to my own comment.
&gt; This is a benchmark of function calling. See http://www.reddit.com/r/rust/comments/2lolci/is_rust_slower_than_go/clwrr98 gcc does some optimizations to turn one of the (non-tail) recursions into a loop, so its not fair to say it is a benchmark of function calling.
I came up with [a Gist showing how this *might* be done](https://gist.github.com/DanielKeep/18022271ef399c063177). I should stress: this is *not* idiomatic Rust. But without knowing more about how these objects are built and kept, I don't believe I could give a meaningful demonstration with lifetimes. `Rc&lt;RefCell&lt;_&gt;&gt;` should usually be your option of last resort. It's probably the most *general* solution, but also the *least efficient*.
That implementation is at least O(log n), not O(1). (I don't think an O(1) implementation is actually possible, since an exp implementation that is sufficiently accurate presumably has some dependendence on `n`, e.g. using big-ints/big-floats.) One can also do exponentiation by squaring [on the matrix form](http://en.wikipedia.org/wiki/Fibonacci_number#Matrix_form), which doesn't require higher-precision arithmetic for intermediate results.
Check out the Iron benchmarks, which were updated recently, we hit 85k where go only hits 70k and node is left in the dust with 17k.
I think `self` is the value while `Self` is the type, `self` in the argument list is sugar for `self: Self`, likewise `&amp;self` is `&amp;self: &amp;Self`, etc. EDIT: `&amp;self` in arglists should mean `self: &amp;Self`. 
What do you want to do when the parameter is not an integer? You can either pattern-match to have two branches; use `if let` to do nothing when it's not an integer; or use `.unwrap()` to panic the task when it's not an integer. The error message looks okay to me, if you consider what unwrap does: `Some(1).unwrap()` returns `1`, while `None.unwrap()` panics. (Perhaps the "\`" quotes makes the message a bit unreadable though)
Well, if the callback only happens once, you can take a `proc()` which can escape the stack frame.
Is `Fn(int) -&gt; int` a type? This looks a bit confusing, I would expect `Fn(int -&gt; int)` to be a type. Edit: [I'm looking here](https://github.com/rust-lang/rfcs/blob/master/text/0231-upvar-capture-inference.md), apparently this syntax is because `Fn(int, int) -&gt; int` looks better than `Fn((int, int) -&gt; int)` or something like this.
Just a typo, I think you mean `&amp;mut self` near the end of your second paragraph :)
PHP? Excuse me? PHP wasn't intelligently designed, it evolved.
Oh, I know things are better now. :) Just forgot to mention it. I just wanted to point out that those numbers wouldn't hold as an argument.
Uhm, no, I've used `&amp;`, not `*` in my post-coercion-proposal example. Also see the comments on [this RFC PR](https://github.com/rust-lang/rfcs/pull/439#issuecomment-61889589).
I'm a bit late, but here's my take: The language-level distinction between ASR and LSR is *necessary* in languages without a proper distinction between signed and unsigned integers. The two languages I know with a `&gt;&gt;&gt;` operator for this purpose are Java and JavaScript. Virtually every language out there with unsigned integer types and `&gt;&gt;` will choose between ASR and LSR depending on signedness, even C#, which is quite close to Java in the grand scheme of things (but better in a few respects).
No, `&amp;self` would be a sugar for `self: &amp;Self` - note the lack of ampersand. That's why it may be confusing somewhat.
Well, `&amp;self` would more accurately desugar to `self: &amp;Self` since the type of `self` becomes an &amp;ptr. Note that while these "sugars" explain it nicely, desugaring by hand won't work.
You should probably use OpenCL instead of coding parallel algorithms in C++ or Rust. 
Yeah, my mistake. :)
Thanks for the correction.
I don't have constructive criticism, only a question. What does this mean? "//this also breaks the main task due to chat chan going out of scope" 
&gt; You can either pattern-match to have two branches; use if let to do nothing when it's not an integer; or use .unwrap() to panic the task when it's not an integer. There are plenty other possibilities provided by `Option` without having to use a full-blown match. For instance you could use [unwrap_or](http://doc.rust-lang.org/std/option/enum.Option.html#method.unwrap_or) and provide a default value if none was provided or the provided one was incorrect.
think everybody!
Indeed! The equivalent to the C code would be to use `.unwrap_or(0)`. And I think this is a bad default for C's `atoi` - it doesn't make it any easier to distinguish a legit 0 from "not an integer" error. `strtol` is better but unreasonably complex. I think Rust delivers a better string-to-integer conversion API than what C provides. It even treats overflow as an error by default (`atoi` [silently overflows](http://rus.har.mn/blog/2014-05-19/strtol-error-checking/))
Also note that Rust has benchmark support in its test facilities, think [timeit](https://docs.python.org/2/library/timeit.html), except it generally turns out to be easier and shorter than doing it by hand: extern crate test; use test::Bencher; static SIZE: uint = 1_000_000; #[bench] fn vec_from_const(b: &amp;mut Bencher) { b.iter(|| { Vec::from_fn(SIZE, |_| { true }); }); } &gt; rustc --test test.rs &gt; ./test --bench running 1 test test vec_from_const ... bench: 60667125 ns/iter (+/- 1829700 test result: ok. 0 passed; 0 failed; 0 ignored; 1 measured &gt; rustc --test -O test.rs &gt; ./test --bench running 1 test test vec_from_const ... bench: 162830 ns/iter (+/- 56962) test result: ok. 0 passed; 0 failed; 0 ignored; 1 measured &gt; rustc --test --opt-level=3 test.rs &gt; ./test --bench running 1 test test vec_from_const ... bench: 157566 ns/iter (+/- 22178) test result: ok. 0 passed; 0 failed; 0 ignored; 1 measured 
OpenCL is not available on all platformes, specialy iOs which is one of our main production platforms, and OpenCL is most useful for the image processing part of an algorithm, which is a small part of a computer vision algorithm (the segmentation and machine learning part is generally the main part).and using OpnCL is not always faster then C++ when working on small parts of images, like when doing OCR.
By the way just to make this clear: unboxed closures (which the example uses) are a way for a closure to escape stack frame (because with the `move` keyword, they own the values they close over). Unboxed closure is an object which implements the Fn (or FnMut) trait. When you use `#![feature(unboxed_closures)]` the closure syntax just generates an struct that implements the `Fn` trait on the fly. The reason why you can't do: pub fn on_set_call(&amp;mut self, callback: FnMut&lt;(int,), ()&gt; + 'static) { is the same reason you can't do this for any other trait: Rust can't statically tell how large the object implementing the trait is. So you either have to use trait objects (i.e. you put the trait behind a pointer: `&amp;Trait` or `Box&lt;Trait&gt;`) or you use generics: fn stuff&lt;T: Trait&gt;(a: T) { ... } Which will create a separate implementation for every object that you pass in (see the section about monomorphisation [in the Guide](http://doc.rust-lang.org/guide.html#traits)). Because these separate implementations accept the a value of a concrete type (as opposed to a trait), they are `Sized` (because the compiler can figure out the size of any struct, enum and a primitive type). Oh, and in case you're not aware of this, the `where` syntax here is just a different way to write generic functions. This should be equivalent to the function from the example: pub fn on_set_call&lt;F: FnMut&lt;(int,), ()&gt; + 'static&gt;(&amp;mut self, callback: F) {
I haven't done anything with callbacks, so I can't really give any more advice on them. As to your other question: my code declares F as a generic type that implements FnMut. FnMut is a trait, not a type. When the compiler says that something is unsized, in this context, it means it doesn't know which type the argument will be. It could be any type that implements FnMut, so it could be any size. You can't have unsized types in arguments, variables, or return types; only via indirection like a reference or a box. In general, you pass a trait value either by a reference, boxing it, or by parameterising the type and constrain it to the trait. The last is the only one that doesn't impose a runtime overhead. So, I took a generic type to allow any unboxed FnMut closure to be passed, then boxed it internally so I could store it.
No idea.
I would try not to restrict the design too much and keep things simple. How about this: struct Obj2&lt;'c&gt; { … callback: Box&lt;SomethingHandler+'c&gt; … } This should be pretty general. You should be able to wrap all sorts of things into such a box including obj1 itself, a borrowed reference to your actual obj1 (because only `'c` instead of `'static` is required), some kind of owning pointer to it like an Rc, Weak etc and/or something involving `RefCell` if you really need it. And instead of `SomethingHandler` one of the `Fn` traits is probably a better idea. I expect it to require less boiler plate for glueing things together because you can use an unboxed closure for this. If the callback is optional, add `Option&lt;&gt;` around the type.
In rust `-8 / 4 == -2`. I believe this is what the OP wants by signed shifting.
Whoops. That's probably a relic from an earlier version that did use lifetimes, before I decided to simplify it.
&gt; it would **round** incorrectly for negative numbers. The computation -8 / 4 does not involve rounding. Try the following program: fn main() { println!("-1 / 2 = {}", (-1i) / 2); println!("-1 &gt;&gt; 1 = {}", (-1i) &gt;&gt; 1); } 
Well, good to know. And in some tightly knit kernel code (e.g. no general purpose libraries or something) disabling the unwinding might be useful. But if Rust were to plan abandoning the unwinding in its "normal" mode of operation then it should get rid of destructors before 1.0, or something like that, wouldn't you think?
This should be fixed with UFCS (unified function call syntax), where the `self` is just a short hand for a plain-old argument. Your example would be written like: impl Mul&lt;A5, A5&gt; for A5 { fn mul(&amp;A5 (a, b) : &amp;A5, &amp;A5 (c, d) : &amp;A5) -&gt; A5 { A5 ((a*c + 5*b*d)/2, (b*c + a*d)/2) } }
Ah, that's a very good point that I hadn't thought of. I'd imagine part of that could be avoided by some sort of no-optimize code section, but I guess the only way to be 100% certain would be to hand-write the assembly (or audit the compiler output).
Nice article! &gt; However, they are hoping to have a 1.0 beta release by the end of this year (2014), so we might start seeing some stability soon. Not "some" stability, _stability_. [Rust 1.0 will be released with a set of completely backward compatible features](http://blog.rust-lang.org/2014/10/30/Stability.html). This means many features just won't be there (even behind a flag), but it will be stable. Of course, _new_ backwards compatible features will continue to come out. :) ---- Nice that you introduced iterators a bit. They're what _should_ be used for almost everything, and usually for loops are more verbose or make it worse. (The exception is when passing closures to the iterator methods requires variable capturing) Also gaah you mixed 1TBS and Allman brace styles. MY EYES.
I opened one issue about your `Cargo.lock`, but I have a question for you: Why are you calling `drop()`?
So the type `fn(int) -&gt; int` implements the trait `Fn(int) -&gt; int`? Is `fn(int) -&gt; int` sugar for something else?
Your task can exit without destructors running anyway – you have to deal with it.
So what you are saying is that there is a special case, when the division returns 0, but signed shift should return -1.
Rather trivial RfC, but I feel it's useful for us to easily stay backwards compatible later.
It's not a special case. Try `-10 / 4` vs `-10 &gt;&gt; 2`, or basically any division that leaves a remainder.
thank to mcpherrinm and masklin. Optimizations really helped and i discovered #[bench] for me)
While doing a 'first-pass' of writing something in go (writing the happy path only) I found myself writing if err != nil { panic(err) } so much that I wrote this tool: https://github.com/placeybordeaux/panic-attack It is currently fairly buggy, but I have found it helpful. For right now there are no gartuntees that it won't vomit all over your code, so make sure you can revert whatever it does or check it before you use it. At some point I would like to see it recognize when it is in a function that can return an error and replace it with if err != nil { zerovalue, zerovalue, err }
Yeah in my 5 minutes on this subreddit I have seen every mention of go simply throwing away the errors. Kind of strange.
I haven't used Rust much yet, so someone please correct me if I'm wrong, but this part: let mut stream = match TcpStream::connect("irc.freenode.com", 6667) { Err(e) =&gt; { println!("error connecting: {}",e); return //kill main }, Ok(s) =&gt; s }; Shouldn't you use `panic!` rather than `println!` + `return`? Or am I misunderstanding `panic!`? Otherwise it all looks good to me! Thanks for sharing!
The `move` keyword is a modifier for the closure literal that causes it to capture variables by value rather than by reference. let a = 5; let fa = || a + 10; // Capture a reference to a let b = 6; let fb = move || b + 20; // Capture b by value
Just noticed that this wasn't actually posted this week.
It's in the rust repo, under src/doc/guide.md
But there is other type that implements `Fn(int) -&gt; int`, right? Is it only one type (the closure that gets an int and returns an int), two types, infinite types..? I don't understand why closures can't have named types (like other languages with typed closures) edit: also, lol, only `Fn`s with 15 arguments or less work?
Yes, a panic would be more appropriate, but putting it into an unwrap would be better, and an ok + expect would be best, which basically just wraps all of this, and looks nicer. let mut stream = ....ok().expect("error connecting"); Same thing!
I believe it's similar to Rust pretty much. It's just that that code example discards the error and shouldn't.
yes, great point. I'll make the change, panic is more appropriate, I think I may have written this when it was fail and that didn't seem appropriate for some reason. Thanks
Thanks, I'll include it. I didn't realize that it should have been for a binary but that makes sense. I don't need to, and I'll remove it to clean it up. The official rust example calls drop so I thought it prudent to be tidy with the connection stream. 
But that's quite different from having docs which clearly say one thing ("&gt;&gt; is always logical shift") and behavior which is quite obviously different (Rust picks ASR/LSR based on type of argument). 
Ah ha! We shouldn't in the example. Have a link handy? I'll fix it (or send a PR if you'd like!)
Rust does eliminate data races, but not race conditions in general. LVars can eliminate critical race conditions (that is, those that affect the final output and not just intermediate program states), but making a static guarantee is probably beyond Rust. Before you could even get into the lattice laws you'd need a way to control side effects. If you had that, proving the lattice laws hold for user-supplied types would likely require a full dependent type system. [Idris](http://www.idris-lang.org/) might be a better fit for this.
Although in this case the closure is probably optimized away and `Vec::from_fn` ends up doing the same, you can also use [`Vec::from_elem(n, true)`](http://doc.rust-lang.org/std/vec/struct.Vec.html#method.from_elem) in this case. Also have a look at [`std::collections::bitv::Bitv`](http://doc.rust-lang.org/std/collections/bitv/struct.Bitv.html). It’s roughly equivalent to `Vec&lt;bool&gt;`, but is more compact in memory.
&gt; And "static" CSS selectors (those you'd use in scraping) can fairly easily be compiled to XPath I’ve been there. (I maintain [cssselect](https://github.com/SimonSapin/cssselect/).) There be dragons. You do not want to go there. It might be reasonable for a small subset of selectors, but I don’t know how useful that subset would be. See https://github.com/SimonSapin/cssselect/issues/12 for an example of selectors that might not even have an equivalent XPath translation. (At least in XPath 1.0, which is what libxml2 implements.)
If something is public, you can't change it unless you bump major versions. If it's not, then you can change it however much you want. That's basically it. Making things public or private isn't a function of use, it's a function of interface. Make a private heirarchy of whatever makes sense for your own organization, and then map that to a public interface that makes sense from your users' point of view.
https://github.com/rust-lang/rust/pull/18818 :D
:) you rock
Would this be like the ["internal" visibility in C#](http://msdn.microsoft.com/en-us/library/7c5ka91b.aspx)? Because I really love that about C#.
&gt; Making things public or private isn't a function of use, it's a function of interface. Exactly, and interface only matters *between* crates, then why do visibility rules apply *within the same* crate? I've always found this limits how I can organize my code, and/or actually forces me to mark as public some functions that I don't want to expose. For example: pub mod strided { pub struct Slice&lt;'a, T&gt; { /* */ } impl&lt;'a, T&gt; Slice&lt;'a, T&gt; { // Not marked as `pub` because I don't want to expose this to another crate // nor want to let users to use this method unsafe fn from_parts(ptr: *const T, len: uint, stride: uint) -&gt; Slice&lt;'a, T&gt; { unimplemented!(); } } } pub struct Mat&lt;T&gt; { /* */ } impl&lt;T&gt; Mat&lt;T&gt; { pub fn col&lt;'a&gt;(&amp;'a self, col: uint) -&gt; strided::Slice&lt;'a, T&gt; { // more code here strided::Slice::from_parts(ptr, len, stride) //~^ error: static method `from_parts` is private } } Doesn't compile because of the visibility rules (but looks perfectly reasonable IMO), what are my options? - Move `strided::Slice` into the root of the crate (losing modularity), or - Make `strided::Slice::from_parts` public, and mark it as `#[doc(hidden)]` (hiding the method, does not impede an user from using it!) Whenever I encounter this issue, my knee-jerk reaction is: "visibility restriction shouldn't apply within the same crate!", but I haven't really thought about the implications of actually doing that. Isn't anyone else bothered by this?
`filter` only gives to the closure a reference to the value because it want to keep ownership of the value so that it can return it. `all` however only returns a boolean, so it can give away to the closure ownership of the value. Passing a function should work. Please copy (maybe in a pastbin or gist or something) some sample code and the error message you’re getting.
The error message was confusing because it said that it expected `|&amp;type| -&gt; bool` and it found `fn(type) -&gt; bool`. Before getting to look at the &amp;type vs type thing, I noticed the || vs fn, so that's why I was confused. I now realize that because of the difference in what filter and all pass to their closures, I cannot use the same function as a predicate in both. Is there a pattern for this? In Clojure and Haskell for example I can pass the same thing. [I understand why I can't, in Rust, I'm just thinking if there might be something that makes this interaction easier.]
&gt; why do visibility rules apply within the same crate? Well, a crate is just a particular anonymous module at the root of the module tree, it's not inherently special in some way. At least, as far as I know. But having it respect visibility is still useful, you are your own user, after all. Keeping breaking changes local is nice. I don't have experience with your specific situation, so I'll think about it and get back to you :)
I hope I understood the problem correctly. Well, anyway, here goes. Crate level visibility is possible. See this a little contrived example: mod utils { // Note that this is not pub! pub mod some_mod { pub fn do_it() { println!("Doing it") } } } pub mod visible { pub mod im_visible_too { pub fn public_do_it() { ::utils::some_mod::do_it(); } } } In it the mod `utils` is not public, but the function `public_do_it` still sees it despite being behind a different path. This is because they are in the same module - at least for visibility purposes you can consider a crate a module too - and items in the same module can see each other. You can see `public_do_it` is in a sub-module itself, but it's still also in the "crate-module" too, so it still can see `utils`. Notice that the sub-module `some_mod` still has to be public, because the owning module still controls what of its contents it wants to show to others. Because of the parent's visibility the sub-module is not public to the world, so you end up with effectively crate-level visibility. Of course there's the limitation that the crate-public stuff has to be in its own module in the crate root, but on the other hand the same can be applied for any module. You could have a non-public module in `visible` for example, and put stuff in there that you only want the other items in `visible` to see.
I see. The "implicit copy" vs. "by_ref" really helped me understand here. Thanks!
Awesome! I had actually planned to get around to writing an ID3 library as my next project to learn Rust. But instead it looks like my next project will be moving my family a few states away, so no time for hobby programming any time soon. Glad to see someone is doing it, and can't wait to find the time to look through your code!
Keep in mind that you will often be given an iterator of references. so with `vec.iter().all(SOME FUNCITON)` your SOME FUNCTION will take the values in vec by reference, not by value.
Glad to hear!
safe crypto!
Ok, thanks. But there's still the type difference. For example, this code: https://www.refheap.com/93003 yields: ``` hello.rs:7:36: 7:43 error: mismatched types: expected `|&amp;&amp;uint| -&gt; bool`, found `fn(&amp;uint) -&gt; bool` (expected &amp;-ptr, found uint) hello.rs:7 println!("{}", v.iter().filter(is_even).count()); ```
Of course! The more formats the better! If it could conform to the [AudioTag](https://github.com/jamesrhurst/rust-audiotag) trait that rust-id3 and rust-metaflac both use that would be great. If you'd like some help working on it I'd be happy to help out.
&gt; Well, a crate is just a particular anonymous module at the root of the module tree, it's not inherently special in some way. I think that's how crates are currently implemented, but crates have different semantics than modules. Crates are compilation units, they define a "linkage" boundary, all the public items inside the crate gets exposed as symbols to the linker. Modules are just how you logically split your crate, they are namespaces. Or at least, that's how I understand the semantics. &gt; But having it respect visibility is still useful, you are your own user, after all. I agree it's useful sometimes, but the library author cannot always be restricted to use the same interface that the library user uses. As you've seen in my previous example, I have chosen to only expose a safe interface, but to *implement* that safe interface I have to use a *unsafe* function which is not part of the safe interface. &gt; Keeping breaking changes local is nice. I'm not sure what you meant here. &gt; I don't have experience with your specific situation, so I'll think about it and get back to you :) Thanks in advance.
Yeah, the reason was explained elsewhere. `all()` consumes the iterator, `filter()` does not.
https://upside-down-research.kilnhg.com/Code/FreeOpen-Source-Repositories/Rust-Libraries/rust_wav Here's an old changeset of the library (such as it is) - I'll push the next update out later tonight. I don't have a problem providing AudioTag conformance, but I'll have to review the API further. It'll be LGPL licensed unless someone can provide a very compelling reason otherwise. :)
Thanks for posting. The numeric breaking change is coming...
If you find any usability issues with the AudioTag trait feel free to submit a pull request or log a github issue. As for the license LGPL works fine.
`expect` always lead me to think we are "expecting" the panic to happen, which isn't what the function means. (It panics when "unexpected" happens.) But I cannot think of a better name that is as short. :( 
Rust seems like a perfect substrate for high performance distributed computing. In that vein, I'd love to see: * HDFS client * code mobility (serialize a closure, run it remotely) * MapReduce implementation * distributed actor library * distributed collections library (like Spark but without the endless fetch failures and GC stalls)
It seems that currently Rust can only support tuples with up to 15 elements so `Fn`s are also limited in how many arguments they can accept. In the future this limitation can be lifted. 
Can anyone give me some reasons as to why this shouldn't completely replace the standard library TaskPool?
Interestingly `cargo doc` generates documentation for private modules too. Does it have an option to include/exclude private modules from documentation? 
[Slides from my talk](http://limpet.net/mbrubeck/robinson-talk/). They aren't much use on their own, but might be useful to complement the video. (If I have time later, I'll put them together with my notes to make a stand-alone version.)
Ouch, that sucks. If you don't mind waiting five hours to pull down 900MB, there is a "download" button below the video player.
Here are [my slides](http://kmcallister.github.io/talks/rust/2014-rust-macros/slides.html).
I've always been rather on-the-fence about this, myself. On the one hand, auto referencing and auto de-referencing are really convenient! On the other hand, auto referencing and auto de-referencing mean the code is invisibly not doing what it looks like it's doing. To be honest, with the borrow checker being pretty good about telling you where conflicting borrows are coming from, and `&amp;mut` being a distinct thing, I'm not sure I have any "solid" objections. *Then again*, adding such sugar should be a backward-compatible change (since it would allow code that would previously not compile). On that basis, I'd rather go with an initially conservative choice and see how it goes. Also, I haven't really seen this as being particularly onerous in practice. Also, also, "[that's] how it is done in C++" is a *terrible* argument. :P
I really like this because it makes it obvious when a function takes arguments by value or by reference. 
the same happened to us [on rust-rosetta](https://github.com/Hoverbear/rust-rosetta/commit/adc4d14097c6fe6db93f4ebaf44312fc21e04658) (blanket impl conflicted with our impl) but I thought it was by design. 
Yeah I agree this can be added after 1.0. It does make things simpler (without adding too much surprises) so may be worthwhile to do (and seems to be rather trivial in terms of implementation). I was looking at [this](http://www.reddit.com/r/rust/comments/2lotx4/is_there_a_better_way_to_read_command_line/) where one of the comments suggested that a certain rfs would make it possible to deref a string to get a slice, thus making it possible to write `from_str(&amp;os::args()[1])` instead of `from_str(os::args()[1].as_slice())`. If the conversion from `T` to `&amp;T` is done this can be further simplified into `from_str(os::args()[1])`, which is most natural to write in my opinion (and doesn't come with any surprises or gotchas really. It is safe because from_str cannot modify any internal state of os::args()[1]). Yeah I agree citing C++ is a terrible argument but I think in this particular example C++ actually got it right (well might not be absolutely right but at least not *that* wrong).
Yeah I agree with this but does it really make much difference? Especially in the case of const references `T` and `&amp;T` would differ very little (except for possible performance difference when `T` is large. Shouldn't be a concern anyway as you can almost be sure that `&amp;T` will be preferred when `T` is large.).
I think that's why it's called anonymous and not abstract.
In that example it was coercing `mut T` to a `&amp;T` right? (I'm not very sure as I feel very sleepy at the moment :) ) I was actually talking about coercing `T` into `&amp;T`, which seems to be fairly benign. Or am I misunderstanding your example?
I'm generally against inspecting version numbers for conditional compilation. Testing the availability of features is much more robust.
So, this is an ID3v2 parser, right ? That's great ! Did you work based on the spec or based on an existing code base, in another language ? I read somewhere that the spec was often ambiguous, and when I looked at it it was a bit intimidating... I have some [Rust code for parsing ID3v1 and ID3v1.1](https://bitbucket.org/olivren/rust-id3v1). I know, it's a trivial format compared to ID3v2, but maybe it could be integrated to rust-id3 ?
I can't see a licence on it for a start. Edit: scratch that, MIT in the source file. Cool.
In Rust, channels are just library types, like any other. If you're asking whether you can send a Sender or Receiver *over* a Sender/Receiver pair... you can send anything that satisfies the `Send` trait. I think both Sender and Receiver are Send, but if they aren't, you can ~~always~~ sometimes wrap them in a type that *is*.
From looking at the source, it needs a factory fn it can call multiple times (thus, it cannot be a proc), but it has to call it *inside* another task (so it *has* to be proc). So it has a closure that returns a proc. :D Presumably, with unboxed closures, this could just be an owning FnMut.
Sure, the reason makes sense, but from an expressivity standpoint, how do you overcome it? Is it the "idiomatic" way to just wrap the predicate in a closure?
So, a couple of thoughts: 1. Copy will eventually be opt in, but the author may well have chosen to opt in, so that doesn't necessarily help. 2. I think we could extend the unused-value lint to cover this sort of case. 
Can you give more context? It should be possible to manually implement the `Fn` traits, but it should not be possible to manually implement *more than one*. Regarding the [breaking-change] annotation, you're right, I forgot to add it, sorry about that.
If you ever do see any of my code that isn't licensed, please let me know and there's a 99% chance I will license it under some open source license immediately.
Details aside I'm in favor of solving this problem now. Specifying a version is not much work, but can save a lot of trouble later on. It would also improve error messages.
Unfortunately, this is incorrect: it is duplicating an arbitrary value, causing e.g. destructors to run twice, or aliasing `&amp;mut`s. One way to fix this problem would be restricting `T` to `T: Copy`, since `Copy` types are types for which the compiler can tell a `memcpy` is a valid method of duplication. // [... code from the gist ...] fn main() { let mut i: u32 = 0; let slice = [&amp;mut i]; let mut v = vec![]; v.prepend(slice); // prints 0 0 println!("{} {}", *slice[0], *v[0]); *slice[0] = 1; *v[0] = 2; // *should* print 1 2, actually prints 2 2 println!("{} {}", *slice[0], *v[0]); } 
&gt; you can always wrap them in a type that is. This isn't *always* true, e.g. no amount of wrappers will make an `Rc` satisfy `Send`, and similarly with `&amp;'a T` where `'a != 'static`. (In this case though, both types are `Send`.)
I sit corrected.
What's missing in the slice/vec API is some equivalent to memmove inside a vector, at least for &amp;[u8].
Are you sure it does? It doesn't for me on cargo 8cc600a. However, if I change visibility by adding/removing pub keyword, it doesn't regenerate the documentation properly, leaving the old one. Try removing the target/ directory and run cargo doc again. It shouldn't generate documentation for private modules. It would be nice to have an attribute for documenting selected private items. For example, I would like to document the internal APIs of my binary crate, but I have to declare the top level modules public for the documentation to be generated, even though my crate doesn't really have a public API.
&gt; The `unreachable!()` macro is a good way to annotate match arms that are superfluous I don't think your code passes the exhaustiveness check either. It should have been match x { Value(i) if i &gt; 5 =&gt; println!("Got an int bigger than five!"), Value(i) if i &lt;= 5 =&gt; println!("Got an int smaller than five!"), Value(..) =&gt; unreachable!(), Missing =&gt; println!("No such luck."), } &gt; However, the compiler doesn't look at match guards. I would say "the compiler doesn't look at match arms with guards (perhaps because the guards can contain arbitrarily complex logic)" (correct me if I'm wrong.)
&gt; I haven't done anything with callbacks, so I can't really give any more advice on them. From another angle; the problem is that objects cannot communicate efficiently both ways: If you have two "button" object and one "controller" object, the controller can't both efficiently control the buttons (e g, to disable/enable them) *and* efficiently get notified by the button that someone clicked it. It seems that Rust will be less efficient than C in this case, both in terms of memory and CPU: You'll end up with many small objects which are just gateways to the big object, with a runtime cost of reference counting and refcell accounting. And if you do a callback from one object to another and then back again, you'll risk an RefCell panic. That the small object could be written as an unboxed closure doesn't really change the CPU and memory cost - it just makes the code look a bit cleaner. Correct?
Well, keep in mind that you're totally allowed to use raw pointers in Rust via `unsafe` blocks. If you can do it in C, you should be able to do it in Rust, though the compiler will obviously be unable to help. Having cyclic mutable borrows probably will cause a panic. But then, Rust is *designed* to prevent those. That's a major motivation for the borrow checker. Off the top of my head, assuming I wanted to stay in safe code, I'd just make it so that changes have to be posted through a message channel. This all sounds somewhat messy, which makes me think that this might simply be a case of trying to use an inherently unsafe design. A bit like trying to pan fry something in zero gravity: you can do it, but it's probably not a good idea. Again, this sort of problem space isn't something I've tried to approach in Rust yet. If I did, I'd probably go poking around how functional languages handle this. Finally, I'm a bit confused about this being fundamentally less efficient than C. I mean, you *are* tracking the lifetime of objects in C. I dunno how you're doing it, but you are. As for the "many small objects"... if it's such a huge concern, then just store a function pointer and untyped context argument like you would in C! No, it's not as clean, but it's no less clean than what you'd do in C. Heck, once drop flags are gone, and placement boxing is in, you could probably set up a system that lets you put the boxed closure in a fixed amount of storage. Sorry if this is rambly; it's late here. :) I guess this all comes down to: sometimes Rust is a hostile language to try and implement an established pattern in... but usually, that's because the pattern itself is inherently unsafe. So you can either go `unsafe` and take responsibility for that unsafety, or change the design. :P
Consider this code: let mut g = Foo::new(); let mut v = Vec::new(); v.push(Bar::new(&amp;mut g)); v.push(Bar::new(&amp;mut g)); Will you get an error on the last line, or not? You can't tell from just reading the above, because it depends on whether Bar::new looks like this: pub fn new(g: &amp;mut Foo) -&gt; Bar or like this: pub fn new&lt;'a&gt;(g: &amp;'a mut Foo) -&gt; Bar&lt;'a&gt; So even with current semantics, you can't really see if you can use the object afterwards or not.
Given that `int(sys.argv[1])` is just OK with Python, you don't have any preferences to `from_str::&lt;u64&gt;(*os::args()[1])` over `from_str::&lt;u64&gt;(os::args()[1][])`, do you?
Doing it similar to how OpenGL handles core extensions? if (context.GetVersion() &gt; 43 || context.GetExtension("KHR_debug")) { enableDebugCallback(); } But then, I suppose that is redundant. You only need to check for KHR_debug...
I'm not the expert. I'm just speculating on the comments. But why the double [][]? That's quite nasty. Unless are you trying double slice? If you want be more specific on splicing, in python, it's [0], [0:1], or [0:1:1]. [begin,end,step]. if that's what you mean by that...
Also, remember the Windows 9 debacle...
Even C's original `getopt(3)` doesn't help the OP "to read command line argument and convert into integer" I'm dreaming of `getopts` implemented as a compiler plugin which can return a data structure such as `Options { n: Some&lt;int&gt;, m: int, f: Some&lt;String&gt; }` according to the list of acceptable options (with their types.)
That's not true, you *do* need to check for the version number, because OS X won't report any extensions, only the profile version.
Let me rephrase my previous comment: why do you think `*...[1]` is acceptable but `...[1][]` is not? Both of them are almost equally longer than Python equivalent (because Rust is not Python.) Rust requires you to add either `[]` or `*` roughly because `os::args()[1]` gives you a `String` object that is incompatible with `from_str` (which expects `&amp;str`.) Rust doesn't provide a variant of `from_str` which can accept `String` because (1) Rust doesn't allow function overloading (multiple functions with the same name but different signatures) and (2) `String` is very popular but only one of many types which can be converted to `&amp;str` in a single step.
I'm just suggesting that if you're doing command-line parsing, there are a few libraries to help, rather than rolling your own solution. (and I'm pretty sure Docopts basically does what you want)
Wow, I didn't know my dream had already come true! Thanks very much. This should be one of Rust's showcase projects. - https://github.com/docopt/docopt.rs
I had the same idea about a year ago and proposed it on the mailing list because I also found the current method (which forces to put a lot of things into single files) really inconvenient. Internal visibility like in C# is definitly a great thing to have. Missing it in Java too. 
I'm on the fence aswell. Would it be preferable to give the tool of auto coerce , and the reader can just assume the least noisy invocation does the most helpful, efficient thing? (i.e... deep-copying a big,complex object should be rare and explicit, but passing complex objects by reference should be the default) 
All three. Fields particularly, since when writing unsafe code in Rust it is often necessary to rely on a field just not being accessed improperly (since you can't mark a field as unsafe). This is also the case with statics--you can't prevent them from being accessed except with visibility rules. Unsafe fields might lessen the need for this. Some traits are also unsafe to implement (if they are relied on for memory safety), so keeping them private is important (this will be solved with unsafe traits). Note that it's *particularly* important to do this stuff for unsafe, because failing to preserve the invariants there leads to memory unsafety and security glitches. However, while the unsafe protections are most important, the same problems exist for any invariant you need preserved, and unsafe can't be overloaded for every such invariant. So I don't think the introduction of unsafe fields and unsafe traits will lessen the necessity for visibility-preserved interfaces between modules, just make them unnecessary for memory safety.
Please state which versions of ID3 tags your library supports (and, ideally, have an API and internal representations that don't conflate the different versions!). There's actually a lack of a good library which supports both reading and writing all ID3v2 versions, even in C, so this could be a very important project for Rust if you get it right. ;) The versions of ID3 tags out there are: ID3v1 (128-byte footer, 30-byte genre, no track number), ID3v1.1 (128-byte footer, 28-byte genre, track number), ID3v1[.1] extended (227 bytes prepended to the 128-byte block), ID3v2.2 (3-byte frame names), ID3v2.3 (4-byte frame names), ID3v2.4 (4-byte frame names, UTF-8 support). If you only support writing ID3v2.4 tags, Windows won't understand them (though most individual programs should); if you only support writing another version, you'll miss out on things like UTF-8 support. And if you don't support reading *every* weird variation out there, people will pass their old music through tools using your lib and lose metadata, which is the worst possible outcome. ID3 is a daunting technology to handle properly (partially because in lieu of a formal spec it was often hacked-on-top-of in strange ways), but there are memory safety bugs in a lot of the C software for it, and Rust has a clear advantage in that respect. If the rest of the library is well-designed, this could end up being the best option for ID3 tag manipulation bar none. I might see if I can contribute some code.
Right, I can certainly do that. 
You might want to read RFC 0369, specifically the [section on the new heirarchy](https://github.com/rust-lang/rfcs/blob/master/text/0369-num-reform.md#overview-the-new-hierarchy).
Yep right now it supports reading id3v2.2, id3v2.3, and id3v2.4 and supports writing id3v2.3 and id3v2.4. I actually implemented it based on the spec. I didn't really find the spec to be ambiguous though. I could definitely take a look at your id3v1 code and try to integrate it into rust-id3. If you've got any files that use id3v1 would you be able to send them my way so I could test with those? 
The README could definitely use more work explaining what the library does and doesn't support. Right now it supports reading all types of id3v2 but only supports writing id3v2.3 and id3v2.4. If there's any interest in writing id3v2.2 I could look into implementing that. Someone else commented with some of their rust code for reading and writing id3v1 tags which I can hopefully integrate into rust-id3 to have it reading all versions of the tag. 
I gave up, and just downloaded msys2. I think someone tried getting it working way back when, and couldnt make any progress
There's really no special context. I have a struct pub struct Chunk&lt;T&gt;(Box&lt;FnOnce&lt;(), T&gt;+'static&gt;); (yeah, I probably should have put it in the original post) and I have a trivial implementation of `FnOnce` for it, which is displayed in the error message in the post. That's it. I'll add this example to the issue.
I don't see why macros would be affected at all, this seems like something that hygiene would resolve. Currently, as far as I know, you can't reference a lifetime defined outside a macro unless it's passed in as a parameter (other than ```'static```, of course).
As an example, libcollections is a single crate. Inside of it is a btree module. Inside of that is a map, node, and set module. Node is consumed by Map in a non-priviledged manner, and Map is consumed by Set a non-priviledged manner. Both Map and Set are exported, but Node isn't. We don't want Set using any internal details of Map, and we don't want Map using any internal details of Node. We don't want *anyone* using Node outside of this module. Both maintain public and private APIs, so that we can freely refactor the collections' internals without worrying about breaking any other. And in practice that's worked great. [Here's a PR](https://github.com/rust-lang/rust/pull/18028/files) of *a completely new person* completely rewritting Node's internals. Note that Map received a couple trivial changes as the signature of a couple public Node methods were changed, and Set didn't change at all. If we didn't maintain a public-private API between Map and Node, I am certain this change wouldn't have been so clean.
Command prompt or PowerShell is usable and the documentation is out of date from my experience. When you use the command prompt it works perfectly fine. ;) Specifically the language FAQ [question] (http://doc.rust-lang.org/complement-lang-faq.html#does-it-run-on-windows?) is out of date. All the bugs that have links to have been resolved. edited
I just quickly perused the APIs. May I make a suggestion? Rather than panic! when an incorrect version is raised, create an enum of allowable versions. That way it's impossible to pass in an incorrect version and people don't have to worry about a panic.
FWIW, I figured out how to solve my problem: making `from_parts` private but retaining the module hierarchy. The solution was using a private trait for the `strided::Slice` constructor: pub mod strided { pub struct Slice&lt;'a, T&gt; { /* */ } impl&lt;'a, T&gt; ::Strided&lt;T&gt; for Slice&lt;'a, T&gt; { unsafe fn from_parts(ptr: *const T, len: uint, stride: uint) -&gt; Slice&lt;'a, T&gt; { unimplemented!(); } } } pub struct Mat&lt;T&gt; { /* */ } impl&lt;T&gt; Mat&lt;T&gt; { pub fn col&lt;'a&gt;(&amp;'a self, col: uint) -&gt; strided::Slice&lt;'a, T&gt; { // more code here unsafe { ::Strided::from_parts(ptr, len, stride) } } } // private &lt;3 trait Strided&lt;T&gt; { unsafe fn from_parts(ptr: *const T, len: uint, stride: uint) -&gt; Self; } Although not elegant, it gets the job done.
It looks like a significant change and simplification of the num API. 
Require that all source files specify version at the top. It's the only way. The point isn't to enable ifdefs, the point is to allow the compiler to have resiliency against old but broken behavior that has to be retained for backwards compatibility but that shouldn't be respected in newer sources.
Note that the RFC mentions that the traits are not intended for generic programming. You are not supposed to implement them on custom types, they are merely the API the primitive types have in common.
Woo! It would be "RFCs" not "RFC's," by the way.
I'll send you some files, yes.
RsFC?
Ah I've never had to try and support OpenGL on OSX.
Yay! TWiR is back! Also I'm going to steal that theme for TWiS ;P
True, though IIRC at the moment you can't pass lifetime idents to macros :)
No, but you'll get the right error message for the code you're looking at.
Okay, so restricting this to `where T: Copy` would work, and then a safer implementation for `where T: Clone`? Otherwise, is there any utility in this trait?
In Rust, mutability depends on the owner of a value rather than on the type (ignoring interior mutability). So either it's all mutable or not mutable at all. You can solve the issue by ~~making the relevant fields private with `priv` and~~ providing accessor functions. 
A PR's out. https://github.com/cmr/this-week-in-rust/pull/2
Yessss the system woooorkssss
Well, the convention seems to be that when doing an operation that could result in error to always return `Result`, and the error value could contain the original input (but to store the original input by default isn't desirable in a systems language). A function that returns `Option` is more like a partial function - it could have an output, or it is `None` when there is no output.
Fair enough if it doesn't work. Its just that I'm used to Cygwin since I use it with C already :) 
Seems to me like it creates unnecessary asymmetry by having some "constructors" on the type, and some as free functions. let s = String(); let t = String::with_capacity(30); vs let s = String::new(); let t = String::with_capacity(30);
Correct me if I'm wrong but I thought struct fields were private by default because you can only declare them `pub`, not `priv` as you suggest. 
You are correct.
You can contribute to the standard libraries! We have tons of work to do ranging from complex to simple! In particular we're doing a *massive* cleanup of libcollections. Some collections are basically getting re-written, some are just missing "standard" methods, and some just have meh documentation or tests. Here's the issue where we're tracking standardization work: https://github.com/rust-lang/rust/issues/18424 And here's the one where we're tracking misc cleanup: https://github.com/rust-lang/rust/issues/18009 I am willing to provide mentorship on any modifications you might be interested in. Edit: there's of course work to do in other parts of the standard libraries, but I'm not as tuned into that work.
&gt; It makes code very unmaintainable because a new introduced variable might require modifications elsewhere With the current rules, introducing a new lifetime parameter will silently obfuscate your code instead. *Especially* since lifetime parameter names are fundamentally meaningless, I'd much prefer to blow up here. If you introduce a new parameter and something goes wrong, pick a different meaningless letter and stop worrying. 8)
Deep copy is always explicit as you have to call clone(). The only thing that bother me is that this auto borrow is done for self, but not for the other arguments.
btw, instead of String::from_str("literal") you can also write "literal".to_string()
You can help with Servo, the experimental browser engine in Rust! Josh's talk [here](https://air.mozilla.org/bay-area-rust-meetup-november-2014/) (the last talk in the video) explains how to get involved (or look [here](https://github.com/servo/servo/blob/master/CONTRIBUTING.md)) Servo has some complicated components in the infrastructure, but you can usually black-box them when getting started. It's quite fun! (#servo on IRC for any questions) As Gankro mentioned there's a lot of stuff in the standard libraries. There's also [Piston](https://github.com/PistonDevelopers/piston/blob/master/CONTRIBUTING.md), the game engine. [Iron](https://github.com/iron/iron/blob/master/CONTRIBUTING.md) (web framework) is another great project. All of these have helpful communities to get you involved.
But seriously: if anyone wants to help get these out the door in future weeks, that would be great. Maybe we should stub out next weeks now so people can submit PRs throughout the week?
It doesn't make a difference? How exactly does `to_string` allocate more in this case?
What's the difference?
There is no sugar, you can implement all of this in the current language... actually, pretty sure this has been possible... since forever? This convention used to be in place a long time ago.
[Stub'd](https://github.com/cmr/this-week-in-rust/pull/3)
Isn't that the definition of [sugar](http://en.wikipedia.org/wiki/Syntactic_sugar)? Given a feature of a language, syntactic sugar is when that feature is expressed more clearly, concisely, or in an alternative style. There's no such thing as sugar that simply cannot be expressed in a more verbose way, because otherwise it wouldn't be sugar. 
Yes, I wanted this feature too. Seems like it should be straightforward to implement, but it's "not the Rust way". Maybe it could be bolted on as an annotation checked by a lint-style tool.
`to_string` uses the fmt architecture which is a bit less efficient; most notably, at present it overallocates, while `into_string`, bypassing fmt, doesn’t.
Due to some issues with specialization, `to_string` comes from a blanket impl which and causes a 128 byte allocation at minimum no matter the length of the input `str`. `into_string` is more specialized and will allocate exactly the length of the `str` it is called on.
Not the "Rust way"? This sounds strange to me because reducing data mutability to the bare minimum seems to be one of Rust's best selling points. Moreover, this is not only a matter of convenience to the programmer. I get the feeling that there are missed opportunities for thread-safety and optimizations by not providing this feature (though I'm not a language design expert).
FWIW, you make fields constant by not giving access to them, and instead have functions that return references or copies, usually called "getters". Then you just have to trust yourself not to mutate them.
I'm more than unhappy still with how error handling works and started a small project on github to experiment with how it could be improved. I think at this point it does not actually need syntax support but a general concept about how errors are represented, converted and tested against. I am currently not sure how I can improve on what exists but I know it's not good enough because presently you cannot easily figure out which error was caused. Part of the problem is that I want errors to be small so they are internally boxed in another struct to keep it word-size. This means that to actually pattern match on the kind you need to unwrap the internal box which I would like to hide. There are many more issues with this whole thing though :( If someone else is playing around with the same thing I really would love to hear about it. I think this is the biggest area where Rust is currently lacking for making good APIs.
Feedback is welcome.
Strong -1 on this. We had `Foo()` some time ago and it was changed to `::new()` so that multiple constructors can exist and look similar.
Have you seen https://github.com/reem/rust-error? I've been using it as the Error trait for Iron for a while now and there's a lot of similar ideas in here and rust-welder.
Bigger projects like Servo look a bit intimidating, but the fact that you suggest it is encouraging!
Sounds cool. I find data structures interesting conceptually and it would be nice to get some practical experience with a collections library. Also nice of you to offer some mentorship! 
Just leave it the way it is so we can grep for new. I like that it is a regular old static function.
We're happy to help new contributers get used to it. Servo was my first Rust contribution, and I found the community quite helpful back then, and I think we still are :) Starting with easy issues and then expanding a bit, all the while learning more about the codebase isn't too hard. Fun, even :)
No, have not seen, but a quick check tells me that it does not fundamentally solve any of the issues I'm encountering at the moment :(
Is this fixable?
The game is quite amusing :). I would suggest changing `Position` to use `u32` instead of `i32`, because it never makes sense for either of the coordinates to be less than 0. Why does `random_position` take its arguments by reference? The primitives are implicitly copyable so there is no reason to pass them by reference rather than by value. The `get_snake_positions` method returns a `&amp;Vec&lt;Position&gt;`, I would suggest just returning a slice there.
toying with same idea, thx for inspiration! [rust-taskman](https://github.com/viperscape/rust-taskman)
Maybe it gets optimized out?
[Here is a previous thread on it](https://www.reddit.com/r/rust/comments/26jzm5/immutable_struct_members_in_rust/), which has some ideas about getting the same effect and also some reasons why it might not work (e.g. you can still overwrite a structure with a copy, which Java for example doesn't let you do).
[Playpen](http://is.gd/igU3Q1) how that could look for the book example: use books::Book; mod books { #[deriving(Show)] pub struct Book{ _isbn: String, _reviews: Vec&lt;String&gt;, } impl Book { pub fn new(isbn: String) -&gt; Book { Book{_isbn: isbn, _reviews: Vec::new() } } pub fn isbn(&amp;self) -&gt; String { self._isbn.clone() } pub fn add_review(&amp;mut self, review: String) { self._reviews.push(review); } } } fn main() { println!("Hello, world!"); let mut book = Book::new("978-0321751041".into_string()); // this throws a compiler error: field `_isbn` of struct `books::Book` is private //book._isbn = "hahaha".into_string(); book.add_review("Thats a good book!".into_string()); let isbn = book.isbn(); println!("created the book {} with unmodifiable isbn {}", book, isbn); } 
You can't pass them directly, but they can be embedded in other things, e.g. `&amp;'a int` is a valid `ty`.
Just have constructor overloading. 
They explain how here: http://blog.rust-lang.org/2014/10/30/Stability.html
&gt;The game is quite amusing :). I would suggest changing Position to use u32 instead of i32, because it never makes sense for either of the coordinates to be less than 0. It would be good if Rust supported restricting the range of integers. There's no reason for the position to be 51231824, but that's possible as well. 
If the [cmp and ops reform RFC](https://github.com/rust-lang/rfcs/pull/439) goes through, you could probably reduce it to |n|(2..n).all(|o|n%o&gt;0)
That would require _function_ overloading, which isn't even in the language yet, and as far as I know has been actively avoided up until this point.
&gt; Does this include library stability? If so, does it include all of the standard library? Or will there still be parts of the standard library marked as unstable/WIP? The linked blog post actually explicitly covers this in the section "What parts of the standard library will be stable for 1.0?".
Things that aren't in the standard library due to instability or not being important enough will be in Cargo, filling much the same role as boost. You can see the current stability of the standard library here: http://doc.rust-lang.org/std/index.html#modules IIRC they're aiming for the most important of that to be green at 1.0 and most (all?) soon after. Somebody correct me if that's wrong.
What's wrong with function overloading? Does it make type inference a bit harder?
Nice work, replying to the single least important point I made.
It seems to me that the rest of your comment is dependent on the policy of stability in the Rust stdlib, or is just general description of how boost handles it; i.e. I don't see anything else to respond to other than: thanks for the info about C++.
No, it can make APIs more confusing and clumsy, and Rust prefers to be more explicit.
Because a concrete goal is a great motivator towards finishing all those breaking changes you've been putting off for months. :)
u8 or u16 gets you closer. 
Does Rust still support the old box destructuring syntax? That would at least make it relatively painless to unwrap the box, I would think.
How are you meant to do operator overloading without overloading?
Good to know. I assume this also holds true for non-literals? In my XML parser this gives me an execution time improvement of ~10% on a large-ish file. At that point this behaviour seems like a rather big foot gun.
Thanks for volunteering to fix it! ;)
excuse me? We have operator overloading, but it is much more understandable in my opinion; generally at least.
You can't overload operators without being able to overload functions, because operators are just functions in any sensible language. The mechanism by which you overload operators should be usable by non-operator functions. If overloading is considered confusing, I personally think that is because type inference is too common and it's not clear what type things that you are passing to functions are. 
The Rust team has explicitly chosen to not include non-operator overloading. It is because they would rather be explicit in function/method calling. They decided to allow operator overloading, however, because they thought it would improve readability. There has been multiple proposals to add overloaded functions and methods into the Rust language, but most (all?) of them have been rejected. Global type inference is disallowed in Rust because it could hurt readability. if you find some variable names confusing than you could add annotations. However, I have found that I do not see that in Rust code, as the function declaration can tell what the variables in the body are marked as.
You can use the `glob` crate: https://github.com/rust-lang/glob extern crate glob; let pat = std::os::args()[1] + "/*"; for p in glob::glob(&amp;*pat) { println!("{}", p.display()) } **I haven't actually tested this code myself**
This might be suitable for what you want: [walk_dir](http://doc.rust-lang.org/std/io/fs/fn.walk_dir.html). Something like this (possibly with some better error handling) should work: use std::io::fs::walk_dir; fn main() { let directory = Path::new("."); for path in walk_dir(&amp;directory).unwrap() { println!("{}", path.display()); } } Which produces something like: level1_folder level1_folder\level2_folder level1_folder\level2_folder\level2_file.txt level1_folder\level1_file.txt level0_file.txt directory_test.exe 
I think that this should eventually be the case (perhaps for Rust 2.0?). Many "safety-first systems language" have range-restricted integers (like [ATS](http://bluishcoder.co.nz/2013/05/07/ranged-integer-types-and-bounds-checking.html) and [Ada](http://en.wikibooks.org/wiki/Ada_Programming/Types/range)) and out-of-range is a major cause of bugs, because integer types normally represent values that should be impossible in the program logic.
I'm also new to Rust. I'm looking at your lock free chatserver and wondering if it would be simpler (and more idiomatic?) to have a single channel along which you send an enum, e.g. enum ChanMsg { Register(SocketAddr, TcpStream), Broadcast(SocketAddr, String), Terminate(SocketAddr) } So in your "startup" function, you'd do: let (send, recv) = channel::&lt;ChanMsg&gt;(); In your "server" procedure you'd do: for message in recv.iter() { match message { Register(addr, stream) =&gt; add_connection(...), Broadcast(addr, msg) =&gt; broadcast(...), Terminate(addr) =&gt; remove_connection(...) } } And in your "socket" procedures you'd do: send.send(Register(addr.clone(), tcp_stream.clone())); send.send(Broadcast(addr, "Hello, world".into_string())); send.send(Terminate(addr)); You could simplify a bunch of your function signatures this way.
Please have the readme show some sample code, eg solve a simple problem / parse something basic.
&gt; When importing the crate, be sure to use the #[phase(plugin)] attribute, eg &gt; #[phase(plugin)] extern crate peruse; I guess this needs to be `#[phase(plugin, link)]` or else only macros are imported (no functions or types), and there will need to be some `use` statements to bring the function &amp; struct names into scope since they are [used unqualified](https://github.com/DanSimon/peruse/blob/fff7c4f0ffa5ec271756b2c28d4e1ebad59901e4/src/peruse/macros.rs#L8) in the macros. You can improve this by using global paths inside the macros, e.g. `::peruse::parsers::coerce` and `::peruse::parsers::OrParser`. However, this will require a slightly peculiar inner module in `lib.rs` to allow them to be used in that crate itself: mod peruse { pub use parsers; } This ensures that the path `::peruse::parsers::...` is valid inside that crate itself (the normal global path to `parsers` would `::parsers`). The inner module is a temporary hack that is improved/removed with the proposed `$crate` syntax of [RFC #453](https://github.com/rust-lang/rfcs/pull/453). 
You certainly can do it, much like you can have memory-safe pointers in an "unsafe" (stupid word, but it seems to have stuck) language, but then that safety is still restricted to only certain portions of code. Similarly, "numerical semantic safety", if you want to call it that, is something that should probably exist throughout the language. 
The warnings are gone using `#[cfg(test)]` on those modules. So that closes the case. Thanks. 
You should be able to write the following once [#17320](https://github.com/rust-lang/rust/issues/17320) is done: let mut map = collections::TreeMap::&lt;&amp;str, uint&gt;::new(); let (key, value) = ("a", 123u); match map.entry(key) { collections::tree_map::Occupied(mut ent) =&gt; *ent.get_mut() += 1, collections::tree_map::Vacant(ent) =&gt; { ent.set(value); }, } This `entry()` API uniformly provides a way to get the handle to the slot associated to given key, no matter that key is in the map or not. This doesn't work at the moment, but if you don't have a strong reason to use `TreeMap`, then you can use `BTreeMap` or `HashMap` instead.
The README does provide a link to the experimental language which [lexer](https://github.com/DanSimon/coki/blob/master/src/lexer.rs) and [parser](https://github.com/DanSimon/coki/blob/master/src/parser.rs) make use of this library.
It should be in the language, but it won't be in 1.0. I'm just hoping that something like this lands after 1.0.
 fn hits_itself (&amp;self) -&gt; bool { self.segments.iter().skip(1).any(|s| *s == self.segments[0] ) } This is nice, but O(N), as you iterate over the entire snake. It might be more efficient to implement the board as a 2D array of booleans, then testing for impact is O(1). Or it might not, depends on the size of the board/snake, I guess.
I would imagine the same is true for &amp;Direction, since Direction is just an Enum with four possibilities.
That's great. Map operation is so common, and providing `entry()` API to `TreeMap` would make things easier. (From the `BTreeMap` documentation, I learned the difference between `BTreeMap` and `TreeMap` usage.) If it's not specific to map operation, in general, is there a better way to deal with the nested mutable borrows other than moving one out? 
I would think the snake will always be smaller than the board, so O(snake_length) will always beat a large constant (the board size). Or to be honest, it's more likely that either will be so fast that it's moot.
I don’t know of any other simple way of avoiding that issue. Your example is a classic example of the problems caused by the lack of [non-lexical borrows](https://github.com/rust-lang/rust/issues/6393). There’s actually [a relevant RFC](https://github.com/rust-lang/rfcs/pull/396) up at the moment that should allow for some work on that issue: it even gives an example almost identical to yours in the Motivation section.
\o/
Thanks for the tips, it hadn't dawned on me to use full paths in the macros. I actually read through the RFC the other day and wasn't sure what it meant by the inner module hack, but now I see. 
I don't think so. Checking a 2D board is just `board[x][y]`. Assuming both the snake vector and the board can fit in the same memory cache, checking for a single location should be always faster than iterating over the entire snake.
The compiler knows that your struct `Bob` is safe to send across tasks (and therefore it implements `Send`), but it can't know that, in general, a type implementing `Trait` is `Send`. Thus you'll need to change the type in your vector from `Box&lt;Trait&gt;` to `Box&lt;Trait + Send&gt;`. 
 let mut bobs: Vec&lt;Box&lt;Trait+Send&gt;&gt; = Vec::new(); This should fix the error. [`Send`](http://doc.rust-lang.org/nightly/std/kinds/trait.Send.html) is a magical trait that represents "types able to be transferred across task boundaries". [`spawn`](http://doc.rust-lang.org/nightly/std/task/fn.spawn.html) needs every variable captured by the procedure to be `Send`. [`Box`](http://doc.rust-lang.org/nightly/std/boxed/struct.Box.html) can contain virtually *anything*, from plain integers or structs to trait objects or some aggregates with a borrowed reference. Since the compiler don't know if `Box&lt;Trait&gt;` may contain non-`Send` values, it complains. `Box&lt;Trait+Send&gt;` indicates that it contains any type which implements `Trait` *and* `Send`. As long as `Bob` remains `Send`, it's fine. Otherwise you'll need something like [`Arc`](http://doc.rust-lang.org/nightly/std/sync/struct.Arc.html) and other sync primitives.
I guess it's a closure, too. You can remove the {} from a closure if the body of the closure has just one statement (which is returned). `()` is just a shorthand for writing `range()`. I like that a lot ;-) Thank you Quxxy!
Thanks! That really helped simplify it, especially the messy function signatures.
&gt;I guess it's a closure, too. You can remove the {} from a closure if the body of the closure has just one statement (which is returned). Ah, so its the same as `|n|{ (2..n).all(|o|n%o&gt;0) }`? &gt;`()` is just a shorthand for writing range(). I like that a lot ;-) So we will be able to do `for i in (0..5) {}` in the future?
I believe the intent is to allow `for i in 0..5 {}`.
Thanks so much.
Thanks, that makes a lot of sense. I had looked into using Arc, but as I don't really need to communicate with the process, it seemed excessive.
Slight clarification: it's not that you can remove the `{}`s from a closure, it's that a closure's body is an expression. Braces are how you group multiple expressions together. [1] I've always found it useful to consider Rust as not having statements at all; instead, there's a binary `;` operator that takes two expressions, evaluates the LHS, discards the result, then evaluates the RHS. [1] Actually, in the compiler, they *are* referred to as statements, but I tend to just mentally gloss over that :P
It's just sugar for `(let (lhs rhs) expr)`, you mean... :P
It's this "trust myself" part I find difficult to accept. I think it would be much better if struct fields were immutable by default and had to be declared mutable. But apparently that's just me.
Seems like none of these options are available for Windows. 
I know nothing about native development on Windows, but any profiler that works without needing compiler instrumentation should work (I have no idea if such a thing exists for windows).
The Rust nightlies, which include unstable features, will serve as the 'development branch' for stable Rust 1.x. See the above link.
I would write let mut map = collections::TreeMap::&lt;&amp;str, uint&gt;::new(); let (key, value) = ("a", 123u); let need_insert = match map.find_mut(&amp;key) { Some(x) =&gt; { *x += 1; false } None =&gt; true, } if need_insert { map.insert(key, value); }
Thanks for pointing the interesting RFC. I encountered all the 3 cases in the Motivation section, and am glad to learn that my solution/workaround is not peculiar. If the non-lexical borrows are not done in Rust 1.0, I'd suggest (/u/steveklabnik1? :-) ) to include those common borrow checker pitfalls/workarounds in the documentation, so newcomers hit borrowck's wall less often. 
That's better. Thanks! (Through that I found my imperative-programming mindset persists.)
That seems like rather pointless sugar to me :-/ what's wrong with range? 
&gt; That is to ask, do you know exactly what you want Rust's error handling story to be and how you would achieve that if you had complete control over the language itself? Or alternatively are you just trying to get a feel for this space and determine what other people think is the most usable solution given the language's existing constraints? It's a bit of both but I think at the moment I want to stay within the constraints and explore there. Introducing new concepts into the language just for error handling might be a bit extreme. Ideally the only thing that is missing is syntax support but currently that's not actually the problems I have. :)
While I completely agree that providing accessors to Book's internals is the right thing to do from an API design standpoint, I deliberately wanted to keep my original example simple. My issue with not having immutable struct fields is that, as the `struct Book` author, I don't want to inadvertently mutate some of its fields. There should be a better way to express this intent to the compiler because, for a larger code base, keeping track of what is mutable/immutable in your head is not a trivial thing to do.
Slicing syntax made `a..b` inside square brackets special, and required a new set of traits. By introducing convenient syntax for ranges, slicing can be redefined as a multidispatch overload of the standard indexing traits (which simplifies mutable slicing and, I believe, even allows slicing assignment!). That it gives us shorthand for range is, I believe, just a pleasant side-effect. So now, instead of slicing sugar, we have range sugar which works for other things in *addition* to slicing.
It's not just you. And I agree that declaring mutable fields `mut` would also be better than declaring immutable fields `const`.
I know you're not OP, reddit OP, but if you don't put a semicolon at the end of a block, the blocks value is the final expression. I guess OP didn't read the guide in his 2 hours.
Is there any reason why *map* should still be burrowed after None has been matched? This could also be extended to matching all atomic types that do not share memory with the original match expression. 
&gt; I guess OP didn't read the guide in his 2 hours. I don't know if you're being snarky or not (if you are, it's not welcome), but from the article: &gt; Here is a quote from the Rust guide: Just glancing over it now, [the section that describes semicolons](http://doc.rust-lang.org/nightly/guide.html#expressions-vs.-statements) isn't super-clear about the times when semicolons are actually necessary or not (e.g. it doesn't have an example like `foo() bar()` failing to compile).
To be fair it took C++ until 2011 to add user-defined literals and until 2014 to actually add some to the standard library. Doesn't matter, though, because the ones that are there are glorious. 
I guess that's because the Rust lifetime currently models lexical scope, as /u/unclosed_paren pointed out in the [comment](http://www.reddit.com/r/rust/comments/2lxoby/whats_the_idiomatic_way_for_nested_mutable_borrows/clz6cjb) above.
+1 I have always found Foo::new() looks jaring, and was recently pleasantly surprised to find that Foo() actually works already (and if I use rust i'll do this myself anyway). Having distinct names for special case constructors is fine, but ::new() over Foo() adds no extra information. Its unambiguous at the call site `Foo{` vs `Foo(`, and still unambiguous to grep for (grep `fn Foo(` vs `struct Foo{`vs `struct Foo(` ... Rusts' clearer syntax eliminates many of the problems in C++. The current convention also repurposes a word that has common meaning in other languages. creating cross language extern C interfaces it might be more logical for `extern "C" fn Foo_new()-&gt;Box&lt;Foo&gt;` to allocate &amp; init an object. (and perhaps Foo_init(Foo&amp;) ?) Rust has a bigger chance of wider adoption if it is friendlier in any way it can be to introduction to existing source bases. Or if you had per-type allocator overloading, you could have Foo::new() fill that role. Foo() creates symmetry with tuple-struct initialisation. You could start out with something being a simple tuple-struct and move to being a more complex struct with named fields later. I personally went with tuple structs for vector maths purely for the prettier constructor. if it was down to me, I'd add arity overload or default arguments too, so you could have String(), String("foo") etc, (with default args you can still have one definition in one place that is more versatile) but just one Foo() over Foo::new() itself would be a nice step forward. With default args, you could get more behaviour out of one definition which reduces navigation/discovery effort. (e.g. Vec(capacity=0){}? ) The only downside seems to be that the word is a function rather than a type in this invocation, but does `default` cover generic initialisation? (e.g., in some future HKT scenario, creating a generic container of a generic type, etc )
But can I easily switch between them as I want? Can I choose to say that 'every numeric in this program should be checked for overflow' while I'm debugging and "stress-testing" the app, and then say 'none, or just a few particular, numerics should be checked for overflow' since I find out that I need/want that extra performance, and I have made an informed decision that overflow is unlikely to happen?
Why are you using boxed trait objects instead of adding subparsers by value? The current implementation seems disastrous for performance. Also, naive parsing is exponential, so you should support using some sort of memoization. Finally, languages are generally static, so storing parser data as fields in a struct seems worse than just generating code that hardcodes everything. To achieve that, you should remove the boxed traits objects, provide some way to generate hardcoded literal parsers and change one-of to take just two parsers, so that all structs will be 0-sized, and monomorphization and inlining will generate a fully hardcoded parser. In other words, it looks like that a lot of fundamental changes are needed to be able to provide optimal performance. 
Still struggling. Why can I box a trait but not Rc it? Like this: trait OuterT { fn innerchanged(&amp;mut self); } struct Inner&lt;'i&gt; { outer_box: Box&lt;OuterT+'i&gt;, // This works outer_weak: Weak&lt;RefCell&lt;OuterT+'i&gt;&gt; // This fails outer_rc: Rc&lt;OuterT+'i&gt; // This fails too } The latter fails with "the trait 'core::kinds:sized' is not implemented for type 'OuterT+'i" (and it fails on *both* the Rc and the RefCell). But Box and Rc/Weak should both be pointers, right? So why can't I put a trait inside an Rc when I can put it inside a Box? What I'm looking for is a way to put the struct and its traits under the same Rc, e g, if you have two references to the object, one directly to the struct, and one to the trait, the struct should deallocate once both pointers are out of scope.
The displacement can only be 1 or -1 in one of the directions. There is no need to use a vector of 2 `i32`s to represent it.
My rule of thumb is to use signed integers for everything for which subtraction makes sense. If, say, you want to know if some position is to the left from the other on a distance of at least five cells you can use `if a.x + 5 &lt; b.x ...`. Then you might assume that it is safe to rewrite it as b.x - a.x &gt; 5 but this can easily overflow if `b.x &lt; a.x`. I don't know how is it in Rust but I had some troubles with such issues in C++.
Can you expand a bit on what you mean? The doc is awkward, or the function is awkward?
An easy project for beginners is (rust-rosetta)[https://github.com/Hoverbear/rust-rosetta]. I would be glad to review your code!
What is the output of `echo $PATH`? The rust bin folder needs to be on your path so you can access rustc. Here's how mine looks: $ echo $PATH /usr/local/bin:/usr/bin:/c/HashiCorp/Vagrant/bin:/c/Program Files/Java/jdk1.8.0_20/bin:/c/Program Files (x86)/Rust/bin $ rustc -v rustc 0.12.0-nightly (d64b4103d 2014-09-26 21:47:47 +0000) 
All good points. I'm boxing everything because I was originally taking a different approach (and I didn't really understand what I was doing in the beginning), but I suspect I can do it now without the boxing. I have a much stronger grasp of Rust now from doing this project, and my original goal was to just get it to work. Now that I've done that I think doing some of the things you mentioned, especially memoization, make a lot of sense.
In this case underflow would be fine, because when you're at coordinates `(0,0)` and you go up or left, you end up with `(u32::MAX_VALUE,0)` or `(0,u32::MAX_VALUE)`. Either way, you will fail the test that checks if you are still within the bounds, and the collision with the wall is detected.
On the other hand, Rust is arguably both more explicit, and more flexible. In this case the trade-off may not be worth it, but I think it would be in more complex cases.
I heavily disagree on many parts, but I upvoted you, because I don't see anything in your post that warrants downvotes.
Operator overloading is basically misnamed. Rusts operators are just generic functions, which might behave like overloaded functions on first glance but are fundamentally different as there is on only one function definition with a fixed behavior on how functions are dispatched to trait impls.
Well both kinda. How am i supposed to discover that function in the doc? I was looking all around the pages for `Path` and `File`. Now that I know about the `std::io::fs` module, i'll know to look there for filesystem stuff, but how many other convenient modules do I just not know about cause they're not linked anywhere? ( this really isn't a big deal, its just a little awkward, aha. ) The function is a little weird cause i would have expected `Path` or `File` to have it. It's also a little weird to me that there's no Directory or Folder or whatever class ( maybe `Dir`? ). You can do `Path.is_dir()`, and `.join()`, and `.exists()`, but not `.list()`?
I saw that, but I'm not sure its what I'm looking for. I only really want a single level of times. In your example it'd be something like: level1_folder level0_file.txt directory_test.exe
&gt; Seems to me like it creates unnecessary asymmetry by having some "constructors" on the type, and some as free functions. &gt; Imagine if there were default/named arguments. you could have one definition `fn String(value="",capacity=0)` ... One thing to search for &amp; document. `String(capacity=30)`, `String("foo")` `String()`. Similarly imagine fn Vec(size=0,init_fn=||T::default(),capacity=size) .
Np :)
Hey awesome! I also wrote a version of cgol for my very first rust program. Its old at this point, but may have some good ideas worth sharing.https://github.com/bjadamson/ConwayGameOfLife-RUST
There is also this one: https://github.com/Arcterus/game-of-life
I don't think the clone behaviour is set in stone.
Which misconception? It's true, Rust does not currently have a REPL.
Ideally you'd be able to index `HashMap&lt;String, u64&gt;` with a `&amp;str`, aka the Equiv problem. So that's being worked on. And if you want you could write wrapper function using `Default::default()` that works like C++'s `operator []`.
Somewhat off-topic: please use `https://` or at least `//` instead of `http://`, the styling breaks under HTTPSEverywhere :(.
I believe he is interpreting the quote to mean that there is a misconception of a REPL not being possible to exist?
Auditing and certification, yes. I know of at least one major company where you're simply not permitted to use any implementation of crypto routines except for the anointed, audited, certified, reviewed, maintained copy of the crypto routines. You're not even allowed to compile the source code yourself -- you have to use lab-built binaries, because you can screw things up even during compilation. I think Rust makes perfect sense for crypto algorithms, given its safety guarantees. However, crypto is a very sensitive, touchy subject, and being super-cautious about crypto always makes sense. Crypto is one of those things that one should never *casually* work on -- either you take the time to learn how dangerous and important crypto work is, or you leave it to someone who has taken that time. So I think *eventually*, once Rust as a language is well-understood and tested, we'll see crypto librares in it that hav reached the same level of maturity as C/C++ equivalents. But this is one area where caution is justified.
Neat. I'm trying to keep track of all the Rust benchmarks out there. Here's my list so far: * [fbench](http://www.fourmilab.ch/fourmilog/archives/2014-10/001537.html) floating-point benchmarks * [euler criterion](https://github.com/japaric/euler_criterion.rs) * [pnoise](https://github.com/nsf/pnoise) perlin noise benchmark * [benchmarks game](http://benchmarksgame.alioth.debian.org/u32/rust.php) * [rust-serialization-benchmarks](https://github.com/erickt/rust-serialization-benchmarks)
Maybe he meant the playpen?
It can be understood that Rust lacks a REPL because it is a compiled language. At least that is how I get the sentence. 
As amk9000 said. Looking back in time I wish i have started programming career with FP and SICP ( http://mitpress.mit.edu/sicp/full-text/book/book.html ) - as opposite to K&amp;R. Rust might be great as a *second* language to learn. 
I do wonder how go managed to get such fast results on ` go-capnproto`.
cool
These benchmarks measure the cost of translating between in-memory and on-wire representations of data. For Cap'n Proto, the two representations are exactly the same, so the translation is either a no-op or a memcpy. My hypothesis as to why go-capnproto appears faster is that its benchmark case avoids an allocation on its inner loop. Here's the [Go version](https://github.com/cloudflare/goser/blob/master/src/goser/bench_test.go): for i := 0; i &lt; b.N; i++ { buf.Reset() _, err := segment.WriteTo(&amp;buf) if err != nil { b.Fatalf("WriteTo: %v", err) } } and here's the [Rust version](https://github.com/erickt/rust-serialization-benchmarks/blob/master/rust/src/goser.rs), which allocates a new MemWriter each time: b.iter(|| { let mut wr = MemWriter::new(); capnp::serialize::write_message(&amp;mut wr, &amp;msg).unwrap(); }); 
I think someone needs to say, here, that `unsafe` isn't a mode, one doesn't "run Rust in `unsafe`", and the presence of a human-audited `unsafe` block in your code doesn't make anything "dangerous". What sort of memetic engineering do we need to do to invert people's thinking about this? 
Changing the name from unsafe to something more representative like trusted. after all, it's not inherently unsafe, you just have to trust to works. 
Nice catch. Looks like our `MemWriter` implementation is missing a `.clear()` method, so I submitted https://github.com/rust-lang/rust/pull/18882 to add support for it.
&gt; For Cap'n Proto, the two representations are exactly the same How does Cap'n Proto handle endianess conversions etc.? What about alignment? Does it only work between the same architecture?
I'm not sure if I'm going to have time to do this for a day or so. Anyone else got time to do it between then and now? I'd be happy to merge it in.
Huh? Perhaps an example would be fruitful...
&gt; My hypothesis as to why go-capnproto appears faster is that its benchmark case avoids an allocation on its inner loop. Supporting this hypothesis, replacing the MemWriter by a NullWriter I get 6588 MB/s, adding a `Vec::with_capacity(128)` to the loop (which is what `MemWriter::new()` uses) lowers the throughput to 3862 MB/s.
Or .unwrap() it, clear the Vec, and build a new MemWriter. I'd be interested to see what the bench looks like with that.
It does nothing but allow you to use code that is considered unsafe, for some definition of unsafe. Like, there's more functions you can call. That's it.
Everything is already documentable in a uniform way that the compiler understands with `///`. You can also document "a level up" using `//!` (useful when you want to document a module from within that module). All of [Rust's docs](http://doc.rust-lang.org/std/index.html) are automatically generated from the source code by this mechanism. We're missing pieces, though. There's no good way to make our "guides" using rustdoc. They're just loose markdown files we manually deal with. There's also no good way to refer to a struct/module/fn from some other docs. You just have to know what URL it will be at, which is awful. But I dunno what on earth you're actually suggesting.
If you only want one comment, then yeah.
I have one, too: https://github.com/crazymykl/rust-life
Thanks for doing this, much appreciated. Have you noticed downsides to capnproto both in general and with the rust library? After reading up on it, I really feel like many things should be rethought about programming because it makes senseand is obviously very fast.
Is the discussion about evaluating functions for which all the arguments are known at compile-time? Or also "evaluating" (specializing) functions for which only some of the arguments are known at compile-time? I would be interested in some links to Rust-specifc discussions on this topic.
So, now that I've got a little bit further, here's what the current configuration file looks like: https://github.com/Kintaro/wtftw/blob/6a696e3cd1c9e387c23f17d267ea705ef04bccf8/src/local_config.rs At the moment it's built into the WM at compile time, but I'm planning to move it into a standalone file that will be compiled as a dylib. See here https://github.com/Kintaro/wtftw/issues/2
Would probably be easier to read if broken down into tables for each format
I don't think C++ maps' behavior of `[]` being "find or insert default" is actually a good idea in common cases - it feels more like a decision forced by the way C++ references work. There's a reason most other languages instead return null or throw an exception. In client code "get" and "set" are very often logically distinct operations, and the STL behavior makes it easy to write code where you expected an element to be there but accidentally inserted a illogical default instead. Even when "find or insert" is desired, it's common to want to insert something other than the default; you can rely on the default being constructed and then replacing it, but then you might need to write an unnecessary default constructor for some class and perhaps undergo some overhead, while it would be more convenient to just specify the default. That said, it certainly makes sense to have some version of find_or_insert as a helper method.
The way it was worded, it made me think that the author thought there would *never be* a REPL, omitted by design or as a consequence of being a compiled language. That is the misconception, IMO. The author could have said more accurately, "Rust doesn't have a REPL right now as that is a tremendous undertaking for a compiled language and there's still much to be done in the language itself and its stdlib. But there is a clear intention to implement a REPL which I think would benefit the language greatly."
I love when a feature (multidispatch in this case) simplify the language elsewhere.
(This is slightly off-topic because it is not related to Rust, but if you are interested in the game of life you should know about it: [Golly](http://golly.sourceforge.net/) is an ridiculously sophisticated game of life implementation, with lots of crazy examples.) 
It looks like that is done with macros, which I think is different from what is being referred to here. CTFE (in the context of Rust) seems to be about evaluating *function calls* which can be evaluated at compile-time; i.e. the arguments are static, the function call terminates, etc. There might also be more to it.
You can get arity overloading by using macros (see `println!`).
We do it, but it's by saying "make a link to like ../../std/collections/treemap.struct.html". Not "make a link to std::collections::treemap".
I dislike how it moves the implementation of `Foo` out of the `impl` block. Compare impl Foo { fn new() -&gt; Foo {...} fn alternative_new() -&gt; Foo {...} } to impl Foo { fn alternative_new() -&gt; Foo {...} } fn Foo() -&gt; Foo {...}
I was looking for that! Thanks! I couldn't find it so I thought I must have misremembered. I'll add it. 
Good idea. I'll update in the next version. 
Would the code be available?
https://gist.github.com/ca2899134311f1bf919d
That's.... not strictly true. You only get arity overloading if you do *something with that macro* to pass it into the actually function, perhaps shoving it into a slice. You get it syntactically, but not at the type level.
I hope to have Cargo-findable target specs, when I get around to writing an RFC. I consider it low priority though.
This is untrue. You're forgetting the other two things: http://doc.rust-lang.org/reference.html#unsafety
Deref (as an operator) is logically a function call in Rust. At least, in my mental model of Rust. Didn't know about the statics stuff, though! (Is that at all changing/changed from all the const-vs-static stuff?)
I don't think there are any, either. You'll have to do [regex matching](http://doc.rust-lang.org/regex/) on each `p.display()`.
Why not store ISBN as a `&amp;str`? Then its already immutable 
Yes, `regex!` is a macro, but it's implemented as a syntax extension, which definitely has the ability to call arbitrary functions during compile time. You can see that, for example, it's calling `Regex::new` during the compilation phase: https://github.com/rust-lang/rust/blob/master/src/libregex_macros/lib.rs#L86-L92 When it calls `Regex::new`, it's invoking arbitrary code at compile time. There's nothing special you had to do to make `Regex::new` available to the plugin. The *catch* is that you had to write the plugin in the first place. I think the spirit of CTFE is that you can write a normal Rust function, call it during compile time and all without having to mess around with the plugin system. I think the answer is that Rust can solve problems that CTFEs solve in other languages, it's just less convenient (right now). But of course, convenience is kind of the point. :-)
This is really awesome, I was thinking of parsing some YAML-like files we use internally at work, maybe this would help.
What do you mean by "at the type level"? I can implement a macro `range!` that works like in python, i.e. range!(a, b, c) -&gt; range_step(a, b, c) range!(a, b) -&gt; range(a, b) range!(a) -&gt; range(0, a) This is what I would call overloading for all practical purposes.
No clue about the changes :)
That's one possibility, yeah. iframes and input aren't working that well in Servo at the moment, and they're not obviously required for an alpha-quality tech demo. Firefox OS works the way you describe; every app runs in an `&lt;iframe mozbrowser&gt;`. We discussed this a bit in the ["Boot2Servo" session](https://github.com/servo/servo/wiki/Workweek-boot-2-servo). If we decide to prioritize Boot2Servo then it could make sense to do the desktop UI that way as well. For a desktop tech demo the alternative is basically a simple native UI for OS X and another for Linux; they don't especially need to look the same at this early stage. Actually, right after that session, Patrick spent like a half hour in Xcode and put together a nice and simple UI for Servo. On Android we can re-use the native UI from [Firefox for Android](https://developer.mozilla.org/en-US/Firefox_for_Android), aka Fennec. This has a very narrow interface to the rendering engine compared to desktop Firefox, where the UI is specified in XUL and drawn by Gecko itself.
To the Rust core team: please consult the Rust community if you are planning to remove jemalloc-specific APIs from the language by default. It's not clear at all to me that this is well-motivated (as was pointed out in the issue, fixing dlopen does not require this), and I very much doubt any such RFC would be approved.
So is this just using the LLVM MIPS backend? Do you have any links to documentation about the PSP ABI? It can't be as bad as the PS3's with 32-bit pointers in 64-bit GPRs!
Maybe if negated trait restriction were added to the language it might be easier: impl&lt;T:Show+!Str&gt; ToString for T {...} // for non-strings impl&lt;T:Show+Str&gt; ToString for T {...} // for strings
I think it's talking about a mechanism like C++'s [`constexpr`](http://en.cppreference.com/w/cpp/language/constexpr), which allows you to do things like constexpr std::array&lt;char, 256&gt; lookup_table = generate_lookup_table(); and have it evaluate the function at compile time and store the result in a static location, instead of having it evaluate it when the program starts up.
What would I do if I'd want my function to be generic over all kinds of integers, big or not?
I don't really see why a generic `+` is good and a generic `add(a, b)` is supposed be bad.
&gt; Arity overloading based on traits, on the other hand, would probably be nice If you have of different implementations that are called based on the number and type of the function arguments, based on traits, how is that much different from function overloading?
Ok, this is awesome. I did a lot of PSP hacking more than 5 years ago. I still have it in the drawer back home, I guess I should get it out again :)
The proposed solution in the PR only works in the `dlopen` case because we happen to only use the nonstandard jemalloc API (`mallocx`, etc.) However, jemalloc is not designed to be used in a scenario in which `malloc` goes to one allocator and `mallocx` goes to another. This violates the jemalloc documentation, which states that it is OK for you to call, for example, `dallocx` on a pointer returned by `malloc`. The submitter of the PR believes that this isn't a problem because we tell people not to do that; I disagree for the reasons stated above. It's unfortunate that this disagreement once again devolved into hostility; there was a significant amount of confusion stemming from the fact that the jemalloc author did not realize that our use case just so happened to work because we were *only* using the nonstandard API, and so (quite rightly) told us that using jemalloc in the way we are using it is broken for the dlopen use case (because, if you use both the standard and nonstandard API as the documentation states you can, it is). As Niko has also pointed out to me, it would be a good thing if embedders were able to call `free` on the object returned by `box T`. This is what many embedders will expect. On a broader point, I would like to mention that the RFC process is designed to take community feedback into account, but the final decisions are up to the team. There is no "approval" process. When it comes to judgment calls like this, like whether upholding jemalloc's documented API and having embedders being able to free boxed Rust allocations outweighs potential performance improvements from jemalloc, then the team has to decide.
&gt; and I very much doubt any such RFC would be approved. The decision to approve, reject or postpone an RFC is made by the core team. As shown by these meeting notes, there is a tendency to misrepresent the issues involved and parrot thoroughly refuted talking points. It's not a legitimate form of community development. It's just a way to legitimize their control over the project.
&gt; The decision to approve, reject or postpone an RFC is made by the core team. It's actually made by the entire Rust team—the meeting attendees—but you are correct that it is not a democracy. However, that doesn't mean that community feedback is not taken into account. On the contrary, pretty much every RFC in recent memory has been introduced at the meeting with "community feedback is positive/negative/mixed". That doesn't mean we have to go with the feedback, of course, because it is not a democracy and we have to make technical decisions with the long-term success of the project in mind; however, in the vast majority of cases, we do go with the community consensus. (For example, namespaced `enum` variants were accepted despite the *entire* team being lukewarm about it, purely on the basis of the clear community consensus.) &gt; As shown by these meeting notes, there is a tendency to misrepresent the issues involved and parrot thoroughly refuted talking points. No, there was confusion in this case based on what the author of jemalloc told us. As was explained earlier, the author of jemalloc had told us that the embedding use case via dlopen would be broken with this patch. He was correct in that mixing of the standard and nonstandard jemalloc APIs *is* broken with that PR; because that something that the jemalloc API explicitly allows you to do, the author of jemalloc told us that jemalloc would be broken. However—and this is what none of us realized at the time—it just so happens that in Rust we do not mix the standard and nonstandard jemalloc APIs, so it happens to work. This subtlety—that our dlopen-safety depends on our *sole* use of the nonstandard APIs—was nowhere made clear in any of the threads and was only discovered via testing. The meeting notes therefore reflect what we understood at the time. Now, knowing what we do now, would we have made a different decision? I would argue no, although I can't speak for the core team. Because it just so happens to work *only* because we do not mix the nonstandard and standard APIs, I consider that fragile and something worth fixing. As Niko has pointed out to me, it would allow embedders to call `free` on objects allocated via `box T`, which would be a good thing all around. So, I believe we made the right decision, even though some of the reasons were not correct as stated (and I freely admit as such!) Implication of malice is unwarranted.
&gt; The proposed solution in the PR only works in the dlopen case because we happen to only use the nonstandard jemalloc API (mallocx, etc.) It doesn't *only work* because of that. It works because it was designed to deal with the problems that way. You're already off to a bad start by framing the issue in such a biased way. &gt; However, jemalloc is not designed to be used in a scenario in which malloc goes to one allocator and mallocx goes to another. There is nothing in the design counter to using it this way. This is a blatant lie. &gt; This violates the jemalloc documentation, which states that it is OK for you to call, for example, dallocx on a pointer returned by malloc. It doesn't "violate" the API. Rust is a user of jemalloc and is free to use only a subset of the API. &gt; The submitter of the PR believes that this isn't a problem because we tell people not to do that There isn't a use case for it, and the documentation tells people not to do it. AFAIK, no one has ever tried to do it and I don't think it's likely that people will. &gt; It's unfortunate that this disagreement once again devolved into hostility It hasn't devolved into hostility. It started off as a yet another attack on my work and a desire to belittle my contributions as useless and roll them back if possible. This was quite obvious to people who observed the conversation on IRC. &gt; like whether upholding jemalloc's documented API This doesn't make any sense. Rust uses jemalloc to implement the `alloc::heap` API and doesn't have to 'uphold' anything. &gt; having embedders being able to free boxed Rust allocations Embedders can free boxed Rust allocations... I have no idea what you're talking about. Rust doesn't define the implementation of the global allocator because it would hamstring future improvements and there isn't any benefit in doing it. &gt; outweighs potential performance improvements from jemalloc Performance improvements that you've chosen to misrepresent as being much smaller than they are. You've also left out how important these APIs are to custom allocators or alternate global allocator implementations. It took quite a lot of effort to get sized deallocation fully working, and even with an existing general purpose allocator like jemalloc not designed around it, the benefits are substantial.
&gt; Implication of malice is unwarranted. Mistreatment of long-time contributors to the project by dangling fake opportunities in front of them, repeatedly misrepresenting their statements, spamming all of their pull requests with FUD, belittling their contributions, striving to roll back their work and more is unwarranted. If you don't want to be called out on the fact that you let your petty grudge get in the way of technical decisions, then stop doing these things.
&gt; It works because it was designed to deal with the problems that way. You're already off to a bad start by framing the issue in such a biased way. I disagree with this design. I thought I made that clear. &gt; There is nothing in the design counter to using it this way. This is a blatant lie. Judging by the facts that (a) the author of jemalloc considered jemalloc broken if the API did not function this way, as evidenced by our communication with him and (b) there is nothing in the jemalloc documentation to suggest that the nonstandard API and the standard API cannot be mixed, or that the nonstandard API is designed to be used as a form of namespacing, I think it is counter to the design. Regarding the accusations of lying, there's nothing I can say that will be productive if we're assuming malice. &gt; It started off as a yet another attack on my work and a desire to belittle my contributions as useless and roll them back if possible. This was quite obvious to people who observed the conversation on IRC. No, there was no malice involved. I was extremely careful to, in the meeting, not name any personal names and speak only in technical terms, precisely because it was the right thing to do, and I did not want allegations of personal malice to surface. &gt; It doesn't "violate" the API. Rust is a user of jemalloc and is free to use only a subset of the API. &gt; There isn't a use case for it, and the documentation tells people not to do it. AFAIK, no one has ever tried to do it and I don't think it's likely that people will. &gt; This doesn't make any sense. Rust uses jemalloc to implement the alloc::heap API and doesn't have to 'uphold' anything. That is your opinion. I believe that the proper functioning of the low-level jemalloc API is something people may want to rely on in C code or unsafe code, and we should expose it properly according to the documentation (so, for example, `dallocx` should be able to free objects allocated via `malloc`). &gt; Embedders can free boxed Rust allocations... I have no idea what you're talking about. I have no idea what you're talking about. Rust doesn't define the implementation of the global allocator because it would hamstring future improvements and there isn't any benefit in doing it. Freeing boxed Rust allocations via `free` is what I'm talking about. There is a benefit to doing it: namely that embedders can use more natural APIs. &gt; Performance improvements that you've chosen to misrepresent as being much smaller than they are. You've also left out how important these APIs are to custom allocators or alternate global allocator implementations. It took quite a lot of effort to get sized deallocation fully working, and even with an existing general purpose allocator like jemalloc not designed around it, the benefits are substantial. Again, we consulted with the jemalloc author, and his opinion was that the performance advantages of using the nonstandard jemalloc API are relatively minor, and that we should "think long and hard" about using the nonstandard API. In any case, I'm not opposed to using the nonstandard API where we can get performance advantages out of it, and that was clear in the meeting notes.
&gt; You're lying again. This was not discovered via testing. I explicitly stated it several times, and then finally I suggested making test cases to prove that what you were saying is nonsense. You did not state that the reason that our usage of jemalloc would continue to work in the dlopen case is that we were relying on the nonstandard API. That is what none of us understood and it took testing to figure out precisely what was going on. On a meta-point, I apologize, but unfortunately I don't think this conversation can productively continue in a way that would be healthy for the project when we're at the level of accusing each other of dishonesty. :(
why do you think 32bit pointers in 64bit GPRs is bad? If a machine has 512mb ram, why would you want a 64bit pointer? (but you still might want big registers for other reasons)
grep fn ....( still works and is unambiguous. I've just got emacs setup to grep for f`n|struct|enum|.. &lt;thing at point&gt;` - infact i'd argue gripping for "fn Foo" is more useful than gripping for "fn new" - far fewer 'false posatives'
&gt; You did not state that the reason that our usage of jemalloc would continue to work in the dlopen case is that we were relying on the nonstandard API. That is what none of us understood and it took testing to figure out precisely what was going on. I stated many times that it worked fine and offered to demonstrate it with test cases. It's not my fault that no one was paying attention to the past design discussions or pull requests (or reading my comments on this issue / IRC). I find it hard to understand how the same people who reviewed those pull requests can suddenly claim that they have no understanding of how it fits together. The failure of the first integration of jemalloc led to a second attempt using the non-standard API both to leverage great support for alignment and avoid C symbol conflicts in cases like plugins. Now it's as if none of those conversations on IRC and elsewhere *never happened at all* and people have completely reversed their opinions on many of the issues involved. There's one week where I hear that jemalloc's performance relative to a faster custom allocator is a major problem for Servo, and then when it's convenient the allocator performance is suddenly not at all important. &gt; On a meta-point, I apologize, but unfortunately I don't think this conversation can productively continue in a way that would be healthy for the project when we're at the level of accusing each other of dishonesty. :( The project isn't healthy. The issues that led Rust down the road of investing years of work into an I/O and concurrency implementation that is going to be entirely thrown out months away from 1.0 are worse than ever. The development models favours those who churn out lots of proposals and misrepresent the other side in a meeting rather than being based around technical merit. I want Rust to be a resounding success, but I don't think the project deserves success. I contribute to a number of open-source projects, and Rust is the only one where I'm left feeling absolutely awful whenever I attempt to contribute to it or get involved in the design discussions. It astounds me that the development process revolves so much around misinformation and manipulative tactics. It's fun watching people strongly argue for whatever choices Rust happens to make that week on /r/programming. The ability for a group of developers to completely shift from one opinion to another and then portray it as always having been that way is incredible.
&gt; Or .unwrap() it, clear the Vec, and build a new MemWriter. That doesn't work, you can't bind the vec outside the closure because the MemWriter owns it, I found no way to write this (save unsafe shenanigans maybe)
From C++ to D to Rust, all solve the CTFE issue to a certain extent. The issue is that users want to do stuff at compile time, the problem is that unless you can do anything at compile time, some users will not be satisfied, and the feature is going to grow with time (some times in not so clean ways). The only consistent design for this that I have seen is in a language discussed in /r/programming last week, which offered a full VM at compile-time that you could use to run the game of life, do file I/O, network I/O, ... everything. It solved the problem once and for all, anything else looks crappy now. 
when posting this I had in mind a very vague idea of a DSL for function, ffi, trait, etc ... apis, and thought maybe somebody else has spent more thoughts on this. But certainly the surprise that a simple solution like docopt didn't exist until recently made me post this. Certainly more brainstorming than a concrete idea for a language modification.
Are the comments from jemalloc's author publicly available? I would like to read them and presume that I am not alone.
Cheers. I'm trying to do something with rust on a MIPS. This could be useful.
&gt; As Niko has also pointed out to me, it would be a good thing if embedders were able to call free on the object returned by box T. This is what many embedders will expect. 1. What about types which implement `Drop`? 2. What about custom allocators? 
CTFE would be awesome for Rust and it would expose less compiler internals (which are very unstable) than the current plugin infrastructure. The main problem is I think that nobody works on it and that it's not trivial to figure out how to make it work.
IMO the inability to ever switch away from the C allocator would be awful... things also aren't this simple on Windows where AFAIK `malloc` and `free` are not necessarily the same thing in every library.
&gt; It would be a good thing if embedders were able to call free on the object returned by box T. Really? I strongly disagree with this and I don't think it would be a good thing. Even outside of Rust, when shared library A and B are linked, one shouldn't allocate in A and free in B, because you can't know whether malloc/free is same in A and B. Given that, assuming Rust and C, two different programming languages, to use the same allocator seems to be a mistake to me. In other words, would it be a good thing if one can free (in C) the object returned by new (in C++)? I think no one expects to be able to free the object returned by new, and the situation in Rust is analogous to the situation in C++.
Could CTFE be prototyped with a compiler plugin and an annotation? 
I too had to do a double take when I read that. We use a couple of non-standard allocators in our (C and C++) code base at work (yay legacy), allocating in one and freeing in another is a massive nono. You just don't do that, it's completely wrong, regardless of how convenient it may appear, it's about as undefined as behaviour can be. Rust should _not_ go down the root of making this possible, it should be explicitly unsupported behaviour. If someone wants to do it then they're doing something that's patently unsafe. 
See also "supercompilation" and Neil Mitchell's supercompiler for a dialect of haskell. http://stackoverflow.com/questions/9067545/what-is-supercompilation. 
&gt; Freeing boxed Rust allocations via free is what I'm talking about. There is a benefit to doing it: namely that embedders can use more natural APIs This is bad behaviour and anyone implementing such an API is doing it wrong. Having a "natural" api in this case is exposing a bunch of your internals. In C or C++ the api should provide some sort of destroy function that allows you to clean up whatever it's given to you. You should NEVER assume that the library is using the system malloc/file manipulation etc unless it explicitly guarantees it in the docs. Rust should not be supporting a use case that C itself leaves undefined. 
[A plausible diagnosis found in this thread.](https://www.reddit.com/r/rust/comments/2lzc9n/rust_serialization_part_21_now_with_more/clzm4y9) For the lazy: the Rust version allocates stuff in the inner loop, the Go version doesn't.
I don't know. It might be possible if compiler plugins could expand the AST but even in that case, it might be quite tricky because all crates would have to be available at plugin time which they are not.
&gt; there is a tendency to misrepresent the issues involved &gt; ... &gt; It's just a way to legitimize their control over the project. You are fully capable of winning this debate via technical arguments alone, without resorting to conspiracy theories. Please don't bring that here, because I'm the one who has to bother to deal with it. Won't someone please think of the kibwen?
To be fair there does seem to be a large amount of debunked claims repeated ad nauseum in a lot of strncat's PRs, especially the ones dealing with low level performance. I'm not sure if it's lack of knowledge/ability on the part of the commenters, people just hating to be wrong or active dislike, but there is definitely something going on there. I'm inclined to think it's a combination of the first 2, but I tend to see the best in people It would be good if people would sit down and actually read what was being said. I don't want to see things go the way of the debian systemd process/leadership fiasco. 
Couple of problems: - At compile time, other functions from the same crate are not available. - You'd need to evaluate the arguments at compile time too, not sure if this is possible (literals are fine).
This just seems like science fiction to me. How do you even begin to do this? I couldn't even figure out how to hack my PSP, despite the million googleable guides.
&gt; To be fair there does seem to be a large amount of debunked claims repeated ad nauseum in a lot of strncat's PRs, especially the ones dealing with low level performance. I agree, though I subscribe to a more mundane interpretation of this. As we can see from this very issue, strncat has been given so much freedom to do as he pleases with Rust's innards that none of the core team has been able to keep up with what's going on down there. Combine this with strncat's long-held reticence to lay out his grand visions publicly (far, far predating any qualms he has with the RFC process) and the mongoose/cobra love affair that has always existed between strncat and pcwalton, and the result is to foster a complete breakdown of communication. If there exists any hidden motive on pcwalton's part here, it is almost certainly rooted in the desire to increase the [bus factor](http://en.wikipedia.org/wiki/Bus_factor) on Rust's allocator story as we inch ever closer towards strncat's loudly-touted and oft-threatened fork of the language come 1.0.
Could you add on some `#cfg(something)` flags that make it do checked arithmetic in debug/stress test builds and unchecked in release ones?
I don't have the stomach and psychic energy to confront people about it like he does, but for what it's worth, I happen to share /u/strncat's dim opinion of the weekly meetings and the counterproductively short-sighted tendencies of Rust's development process. (Again for what it's worth, my experience has been that the developers are nice and friendly people, so I suspect that the causes are more institutional than personal.)
&gt; You cannot write sizeof with a compiler plugin. Why not? Assuming you don't have the use of `size_of()` form intrinsics, you could still match on the type and recursively calculate the size. Also, usually preprocessors have their own syntax. Normal rust code can be placed in a compiler plugin, with Rust code that uses libsyntax to wrap around it. Hopefully we can get rid of the need for that glue code later.
If, as you suggest, we are already at the point where the core team has not kept up with Rust's innards, we would be better served by encouraging documentation rather than bullying away the person most familiar with those innards. /u/strncat is obviously prepared to write a lot of text, I imagine he/she could have documented all the allocation-related machinery in detail in half the time that has now been spent debunking bogus arguments. Now I no longer see that happening.
&gt; the counterproductively short-sighted tendencies of Rust's development process FWIW, ever since the beginning I have viewed Rust's development process as an application of simulated annealing, which makes them willing to sometimes try less optimal solutions in pursuit of a global optimum. If anything, their sin is that they didn't decrease their temperature parameter fast enough. :P
Do you have examples? As I said, we try very hard to take community feedback into account.
&gt; Why not? Assuming you don't have the use of size_of() form intrinsics, you could still match on the type and recursively calculate the size. My understanding is that no (or not all) type information is available when compiler plugins are run, is this wrong?
&gt; Even outside of Rust, when shared library A and B are linked, one shouldn't allocate in A and free in B, because you can't know whether malloc/free is same in A and B. That's not always true. For example, you are expected to call `free` on the result of `stbi_load`. &gt; Given that, assuming Rust and C, two different programming languages, to use the same allocator seems to be a mistake to me. In general, you don't want multiple *default, global* allocators in the same process.
Of talking around and spinning on side-issues? There's been a couple of interesting mailing list threads I've had to stop reading, the whole unified io abstraction over readiness and completion based api's got, some of the discussions about segmented stacks. During the whole mutpocalypse drama when it was suggested early on that unboxed closures would fix a lot of the problems. Then instead of working out what effect that would have and then reevaluating the situation people stuck to their guns and caused unnecessary worry and drama. more recently (ie the last couple of months) one of the io subsytem rfcs, which admittedly was eventually completely redone, but only after a lot of fairly pointless discussion that kept happening even though the problems with the suggestion were pointed out quite high up in the thread. There have been a bunch of other times when people on the core time have stuck to their guns way longer than they should have, despite technical problems being made obvious in the thread. It feels like they've had the discussion out of band and made up their minds, then gone to the community for consent, instead of starting the discussion in front of the community and only discussing it out of band after everything has been hashed out. I don't read the mailing list and only barely follow these days because they seem so much like second class channels as far as decision making goes. 
I have a very positive impression of Rust community and developers, however, this particular case (with /u/strncat's PR) struck me as very inappropriate and just plain wrong. As I said in that issue, I don't really know the details of how allocators work and how exactly they interfere with shared library loading and how symbol resolution operate, but I do understand the general concepts here. The main concern from the core team, as far as I understand, can be expressed by this phrase: &gt; since it slams the door on dlopen, which we can't do. However, /u/strncat, the author of the PR, stated *a lot of times* ([one], [two], [three], [four], [five], [six], and probably there are others) that this PR **does not affect dlopen at all**, and gave technical arguments why it is so. Again, I'm not really qualified in this stuff, but they look *very plausible* to me. After all, what if a shared library contains some name which *your* program uses? There *has* to be some protection for that, so shared library just couldn't affect existing programs. I would understand why the PR was turned down if these claims were in fact wrong. After all, everyone could be mistaken. However, **no one** has provided **not even tiny little bit** of evidence which falsifies these claims. No one said something like: "no, this PR *will* in fact break dlopening because of *this* and because of *this*" in response to one of the comments I linked to. The entire thread looks as if just *everyone* took some phrase out of the context and based their decisions on it. This is just wrong and inappropriate thing to do. This all really makes me feel *very* uneasy. I don't know anything about other similar "events", but I do believe that this particular case should be evaluated again, much more thoroughly. Turning down contributions because of *such* reasons is just plain nonsense. **Update** Clarified some points [one]: https://github.com/rust-lang/rust/pull/18678#issuecomment-62247689 [two]: https://github.com/rust-lang/rust/pull/18678#issuecomment-62371803 [three]: https://github.com/rust-lang/rust/pull/18678#issuecomment-62638580 [four]: https://github.com/rust-lang/rust/pull/18678#issuecomment-62653429 [five]: https://github.com/rust-lang/rust/pull/18678#issuecomment-62692205 [six]: https://github.com/rust-lang/rust/pull/18678#issuecomment-62694908
You can force early type inference with `expr_ty`,though I'm not sure if it works in all cases.
As I explained upthread, the PR was eventually turned down due to not wanting `malloc` and `mallocx` to be able to redirect to different allocators. We disagreed as to the importance of this.
So what happens if a library s statically linked against one allocator and loaded by an executable which is using a different allocator. How does that executable free something created by the library? This is actually something I've run into in afore mentioned work code. Are you not going to be able to swap out the default allocator on embedded platforms? 
Still, you also can't `free()` a pointer created by `new` in C++, why should you be able to `free()` a `Box` from C?
You would have to define your own trait and implement it for the integers you want to use.
If the library documents that, it's fine, however, it's binding itself to one specific allocator by this and this is in general viewed as bad design of an API.
&gt; Unless there's some reason to think this will be an issue that affects more than a tiny handful of people (if any), I would argue that yes, it should be outweighed by performance improvements to jemalloc. There are ways to use jemalloc's nonstandard API that don't involve bifurcating the API so that `malloc` can do one thing while `mallocx` can do another. That is the road I would like to go down (as I mentioned on both the RFC and the meeting notes).
You can't—but the design of `malloc`/`free` via weak symbols tries to discourage this sort of thing from happening.
Having shared Rust libraries binding themselves to one specific allocator is precisely what we would like to fix, though.
I don't think there's a good reason for making that undefined behavior just because we can. You don't want multiple allocators in the same process, and if you're working with one allocator you might as well have the ability to free any pointer allocated by that allocator.
Sticking to your guns isn't bad per se, if it's a subjective issue that's fine. The ones that grate are the occassions when a clear technical impass is encountered, no high performance abstraction for blocking io existing over readiness and completion based api's comes to mind. Sticking to your guns in such a situation is just draining to everyone involved, it's nearly as bad as just disregarding feedback. 
&gt; The ones that grate are the occassions when a clear technical impass is encountered, no high performance abstraction for blocking io existing over readiness and completion based api's comes to mind. Sorry, I don't actually know what RFC you're referring to here.
That wasn't an rfc, I think that was a mailing list thread somewhere around 0.4 or so.
I'm sorry about the flippant characterization of the comment thread; it was unfair. Thanks for calling me out on that. To be clear, the "commitment" to default type params is from [our stability plan](http://blog.rust-lang.org/2014/10/30/Stability.html), not tied to this particular proposal. I will bring the HKT issue back up with the core team, and make sure both they and you are satisfied by the eventual story there.
If it had been explained that the only reason why we avoid dlopen-related problems is that we only use the nonstandard API, then a lot of this could have been avoided. (Note that this would not have affected the outcome of that PR.) I'm not holding anyone responsible for this, by the way; misunderstandings happen in ways that neither party is responsible for.
It also doesn't work out nicely when trying to produce output that isn't for a browser.
Thanks.
Sorry, but it wasn't clear neither from meeting notes nor from the PR thread, especially from the latter one. And this is a very different problem which may be worth more open discussion. Not everyone reads very specific PRs, because it is very hard to follow the very quick pace of Rust repo changes, however, a lot of people, I believe, read summaries like meeting notes. I believe it is beneficial for the language and libraries to discuss advantages and drawbacks of design decisions like this one, and in the vast majority of cases it is done properly, but cases like this are really unsettling. This PR claims that it brings a significant speedup of almost any Rust program. I think it is sufficiently important to discuss the drawbacks it brings more openly. Again, I'm not qualified in this particular question, but I see (in this thread, in particular) that other people don't think that mixing allocators is a good idea. Probably there are valid reasons why they claim so.
The performance benefits of that PR are from having LLVM (more broadly: C code linked with Rust) and Rust using the same allocator. Nobody disagrees that this is a desirable goal, and there has been no argument as to whether we should do that. The question is *how* it should be done, and that's where the disagreement came about. It was believed to break the dlopen use case; it turned out that it didn't, but only because we avoid mixing the nonstandard and standard jemalloc APIs in Rust. Eventually we all came to the conclusion that the better way to avoid this is to use jemalloc the way it was designed: Rust executables should link against jemalloc, and shared Rust libraries should use whatever allocator that their host executable was linked with. The submitter of that PR has stated that he'll be working on that quite soon, actually, and I'd be quite happy to take that revised PR that, as far as I can tell, pleases everyone involved.
You might however might want to use different API to the same allocator from different parts of the same program, maybe just for optimizing the part you can in Rust (sized deallocation) and not doing it in the C part where you can't do it. The thing is, the C++ standard doesn't mandate this and probably has a very good reason for it. You just leave more options for the future. This is *not* because "we can". (You see almost a whole thread arguing that it's probably a bad idea to tie Box&lt;&gt; so closely to one allocator.) 
&gt; Being able to override the system allocator and get the resulting performance improvement seems to be a pretty good reason. If someone has tied themselves to an allocator they know what they're doing, or will soon... This isn't what we're talking about. Nobody has disagreed with the fact that being able to override the system allocator is a good thing for Rust executables. &gt; I can't see any issue with that PR it's a harmless performance boost. The problem is that in the dlopen case it bifurcates the API so that malloc/free do not work on pointers allocated with mallocx/dallocx. But malloc/free/mallocx/dallocx are all part of jemalloc's API.
&gt; By that standard every language has CTFE because you can just write a preprocessor that parses and evaluates properly marked functions. This is the heart of the matter. Rust's syntax extensions are a bit more than a simple preprocessor because you can call arbitrary functions from other crates seamlessly. Note that the GP did not say "Rust has CTFE." The GP said, "Rust can solve the some but not all problems solved by CTFE, but in a less convenient way. *But*, it's a better situation than a simple preprocessor."
I'm not about to say that the RFC process necessarily avoids this, but the mailing list has been essentially abandoned precisely for the reason that having a coherent and linear conversation among many participants is too hard. "Sticking to your guns" in that medium may very well just be a misinterpretation of an attempt to reply to people who have forked the conversation into dozens of microthreads, as well as an attempt to ensure that your arguments are laid out clearly to people when you cannot guarantee they have read your prior arguments.
I'd like to point out that the assertion that "this PR does not affect dlopen at all" is itself untrue. I think that the point that strncat was trying to get at is that the typical usage of dlopen will not be affected. I created an example which works today but does not work with this patch applied: https://gist.github.com/alexcrichton/781eb2f958150a890dd3. This example was brought up on IRC, but it never made its way to the github thread. Note that this only breaks if the flag `RTLD_DEEPBIND` is specified, which strncat has claimed is a generally unused and otherwise quite dangerous flag. I just want to factually state that it *does* break *some* behavior. Whether we're ok with this because it should be a rare occurrence is another story altogether, and the much of the debate is over whether cases like this are ok to accept or not. 
Author here! Some notes: * The code, especially the "parser", is a real piece of work and will be ripped out and replaced soon enough. Caveat lector. * Some kinds of numeric literals are broken but can be worked around. * Error messages might be kind of bad. * This was originally done as a sort of joke, but I was surprised to find it somewhat handy!
No, if for example from unsafe code you called `let ptr = malloc(10); dallocx(ptr, 0);` then it would crash in the dlopen case. But this is a way you're explicitly allowed to use jemalloc.
Nice! It's encouraging to see that stuff like this is possible. I've thought for a while that it'd be cool to have something similar for [capnproto-rust](https://github.com/dwrensha/capnproto-rust). It'd be awesome if I could write something like this: let address_book = capnp!(address_book, message, ( people = [ ( id = 123, name = "Alice", email = "alice@example.com", phones = [ (number = "555-1212", type = mobile) ], employment = (school = "MIT") ), ( id = 456, name = "Bob", email = "bob@example.com", phones = [ (number = "555-4567", type = home), (number = "555-7654", type = work) ], employment = (unemployed = void) ) ] )); and have it be equivalent to this: let address_book = message.init_root::&lt;address_book::Builder&gt;(); let people = address_book.init_people(2); let alice = people.get(0); alice.set_id(123); alice.set_name("Alice"); alice.set_email("alice@example.com"); let alice_phones = alice.init_phones(1); alice_phones.get(0).set_number("555-1212"); alice_phones.get(0).set_type(person::phone_number::type_::Mobile); alice.get_employment().set_school("MIT"); let bob = people.get(1); bob.set_id(456); bob.set_name("Bob"); bob.set_email("bob@example.com"); let bob_phones = bob.init_phones(2); bob_phones.get(0).set_number("555-4567"); bob_phones.get(0).set_type(person::phone_number::type_::Home); bob_phones.get(1).set_number("555-7654"); bob_phones.get(1).set_type(person::phone_number::type_::Work); bob.get_employment().set_unemployed(()); Implementing this would probably be a fun little self-contained project. I'd be happy to provide guidance to anyone who wants to take it on. :)
That's very helpful. Reading the response from jemalloc's author, combined with the meeting notes, it seems a right long-term decision, i.e. using jemalloc for Rust executable while leaving the allocator decision to the host for dynamic loaded Rust library, and avoiding tight dependency on jemalloc.
They're not the worst idea ever - I'm just bitter because I had to port a compiler backend from SysV ABI to Sony's. =) If PPC let you treat 64-bit GPRs as 32-bit registers, like amd64, it would be a piece of cake. Without it, you have to be very careful about masking the registers (which adds extra instructions as well). I absolutely agree that it was the right choice for the PS3 and its paltry 256 MB of RAM.
Huh, I didn't see that mentioned anywhere on the jemalloc doc page or the reply from the author above. I'd have assumed they needed to be matched. If it's the case that it's defined and correct for you to have mismatched malloc/dallocx then yeh, some code may have broken. I can go and remove one of my cppcheck rules.
I played around with a micro-language of my own and wondered which of the two following strategies was better: - lower down to IR, JIT, "inspect" output - write an interpreter The immediate advantage of the interpreter is not only that you do not have to inspect the output; it's also that you are in sandbox since you decide which built-ins to provide. In the case of Rust, I suppose an obvious downside would be the inability to correctly interpret the `asm!` macro. And of course, the interpreter speed could be an issue (by an order of magnitude). In terms of work, though, it's unclear to the untrained me which would be more taxing.
This makes a macro definition look more like any other item: macro early_return { ($inp:expr $sp:ident) =&gt; ( match $inp { $sp(x) =&gt; { return x; } _ =&gt; {} } ) } It can be qualified by `pub` (similar to `#[macro_escape]`) and/or `extern` (similar to `#[macro_export]`). The switch to using curly braces for the outer delimeter is purely a matter of convention; `macro_rules!` already allows this. I think if we're making everyone change all of their macro definitions anyway, we should switch to curly braces for consistency with the rest of the language. The syntax for importing macros becomes use macro std::vec; use macro std::panic as fail; also supporting `pub` and `extern`. The `macro` keyword is important here; it signals that something other than usual name resolution is going on. `use macro` provides a memorable and searchable name for the feature. We would lose that with something like `use std::vec!;` Setting aside syntax, the mechanics of the proposal (crate scope, `$crate`, etc.) are [unchanged from before](http://www.reddit.com/r/rust/comments/2ln1hg/rfc_macro_reform/).
Wow, I really need to update that...
And it is worth pointing out that C++'s very semantics were crafted so that for an object allocated by `A`, a deallocation call in `B` would result in delegation the deallocation to `A`; this is supported in the Itanium ABI by having `deleting destructors`. *Note: of course, unless said destructors are inlined...*
And the snake. :-)
We have `allocate`, `reallocate`, `reallocate_inplace` and `deallocate` in liballoc, and the jemalloc stuff is private. The only way a developer could get in the situation you suggest would be if they wrote their own bindings to jemalloc (which is not even guaranteed to be used in Rust!), and anyone who is able to do that will be able to figure out how to make that work.
Would it be possible to indicate that any `Trait` is also `Send`, like `trait Trait: Send {}` ?
The idea of `use macro` is twice great: - great to immediately identify that it's a macro being imported, not a regular symbol - great to search for it either in the documentation, tutorial or Internet Thank you for polishing the macros before 1.0!
The jemalloc API isn't truly private; it consists of publicly exported, documented symbols. I mean, we can *say* it's private, but that doesn't stop people from using it. It's certainly imaginable to me that people writing low-level unsafe code might want to use the jemalloc API directly; after all, it has some features that we don't expose through our API (like arenas and so forth) and C code statically linked to Rust that wants to take advantage of those features will find it easier to use the jemalloc API than liballoc. I feel we shouldn't be adding subtleties that people who want to call jemalloc directly (which not only unsafe code but also C code statically linked to Rust code could do) have to pay attention to when the alternative approach, outlined in the meeting notes and suggested on the PR, would not have this drawback.
This isn't subtleties. It's just what you had to do before, deallocate with the same allocator API that you allocated with.
No, as I said earlier, `let ptr = malloc(10); dallocx(ptr, 0);` was allowed before, because the jemalloc API intends it to be allowed, but would be broken if we removed the jemalloc prefix. `malloc` and `dallocx` are part of the *same* API; they aren't intended to be different APIs.
Are they JSON literals or JSON-like literals? Was worried they'd have weird syntax but it looks like normal JSON to me.
The inability to use asm! is fine with me. I would ban all unsafe code from compile-time execution. The efficiency is not really a problem, either. In the toolchain I worked with, we used the interpreter strategy and it worked brilliantly. We did all sorts of crazy stuff at compile time, like optimizing hash table coefficients to eliminate collisions, and the part of the build time that went to interpreting these functions was negligible. It's beautiful. The other nice thing about the interpreter strategy is that it enables cross-compiling. With JIT, unfortunately, you entangle the architecture of the compiler with the architecture of the target code. Think 32-bit vs. 64-bit conflicts, or differing alignment requirements for different platforms, etc. 
They aren't quite JSON because of the syntax for interpolating expressions and because trailing commas are allowed (and also the bug with negative numbers). I mostly say JSON-like as a convenient excuse in case somebody's pathological JSON snippet is rejected by the macro :)
That Rust uses jmalloc internally is a private implementation detail. If someone relies on that and stuff breaks, that is *their* problem. And since Rust can already be built without jemallloc any code that relies on jemalloc being present is already wrong. I really can't imagine that anyone would expect that when they `dlopen` a Rust library, that suddenly `malloc` and `free` are replaced by jemalloc.
&gt; That Rust uses jmalloc internally is a private implementation detail. If someone relies on that and stuff breaks, that is their problem. And since Rust can already be built without jemallloc any code that relies on jemalloc being present is already wrong. The issue is that it can break people who conditionally opt into the use of the extended jemalloc API. Code doesn't have to *rely* on jemalloc being present in order to use its extended API; it can provide a `cfg` option. And if that code uses jemalloc's extended API and mixes it with jemalloc's standard API, then it will break under the dlopen use case. &gt; I really can't imagine that anyone would expect that when they dlopen a Rust library, that suddenly malloc and free are replaced by jemalloc. That's not what's being proposed.
huge +1! I am not sure if you got the `pub` and `extern` ideas from the last thread, but i love it. It really unifies the macro system and makes it look more regular. I do not think breaking code is a downside, its still pre 1.0, and this is some pretty looking code :D
&gt;macro_rules! has many rough edges Are you implying that macros, do not, in fact, rule?!?
Alllll for this...especially the procedural macro sugar +1
What do you mean by 'interpolating' expressions?
Inside an invocation of the `json!` macro, you can include expressions wrapped in a set of parentheses. The expression inside the `()` will be included directly in the macro expansion expansion accompanied by a `.to_json()` call. For example this snippet: let (x, y, z) = (123i, -475i, 670i); let json = json!({ "sum": (x + y + z)); Will expand to something like: let (x, y, z) = (123i, -475i, 670i); let json = { let mut ob = TreeMap::new(); ob.insert("sum".into_string(), (x + y + z).to_json()); ::serialize::json::JsonObject(ob) }; 
Maybe I'm misunderstanding, but this `pub` seems to be doing something completely different than `pub` in other places in the language. The proposed `extern` seems to be closer to the effect that you normally get from `pub` (it can be accessed by everybody). So wouldn't it be more logical to use `pub` for "can be used by other crates" and some different keyword for "magically places itself in the crate root"? Maybe `global` would be a good name for that. It's not clear to me why something like `global` would be needed for macros if it's not needed for functions or anything else, but I'm sure there are technical details I'm missing. With the `$crate` variable, my fear would be that people will simply forget to use it consistently. The macros will work for the original author, but later, when someone tries to use them from a different crate, they break. Would it be possible to enforce usage of absolute names and `$crate`, if the macro is declared to be accessible from outside of the module/crate? 
No need to apologize! I very much appreciate it when people take the time to question our assumptions :)
Having maintained [Zinc's](http://www.zinc.rs/) `ioregs!` syntax extension for the last few months I can say that it's hardly any worse than any other Rust code. Admittedly the language as a whole is still in a state of flux so perhaps this isn't the best standard but 90% of the relatively small number of changes I've had to make were renamings and adding new constructors to pattern matches. That being said, CTFE would be incredible to have if for no other reason than the current disaster surrounding static initializers.
It might be good to use more explicit splice syntax: let json = json!({ "foo": 3, "sum": $(x + y + z)+, }); I believe this will parse today as a [`TtSequence`](http://doc.rust-lang.org/syntax/ast/enum.TokenTree.html), although the `+` sign after the expression is entirely meaningless. We could change the parsing of macro invocations slightly to better accommodate use cases like this.
The easiest thing to do here is to use `Box&lt;Reader+'a&gt;`. Dynamically sized types cannot be used on the stack, which is what you're doing here with the `let r = ...` line. Then you can simply do `box MemReader::new(...) as Box&lt;Reader+'a&gt;` to get the field. IIRC there's a way to make the struct `MyReader` itself dynamically sized (in which case you have to do `let r = box ...`), though I'm not sure how to cast the `MemReader` in that case)
Was there any issue about posting them in the past? It seems to me that, besides from the miscommunication, the absence of those comments made them much harder to verify for spectators following the thread. Like /u/strncat *you* also had enough power to avoid the unproductive discussion, but I think you didn't.
`regex!` is part of the standard Rust distribution, so whenever a breaking change is made to the internals, the person who made the change fixes it. :-) ... however, I do maintain other libraries that use syntax extensions (quickcheck and docopt), and keeping up to date with the latest Rust has been as easy as fixing any other breaking changes. (QuickCheck has a pretty small syntax extension. Docopt's extension is a bit more involved.) The difficulty in writing extensions (IMO) is 1) learning how to manipulate the AST and 2) learning how to parse token trees. Neither of these tasks is intrinsically hard, but they are made hard because there's really no documentation *and* the API surface is ***large***.
In general I hesitate to post private conversations.
Can you provide some explanation of what is happening there? I'm rather surprised that a rename from `je_*x` to `*x` would make calls to the C allocator resolve to jemalloc (assuming that is what is happening here). Edit: Nevermind I guess. This apparently includes a rename from `je_malloc` to `malloc` too, which is less obvious from the diff.
As far as I can tell, this gist actually depends on *undefined behavior* in Rust today - that it happens to work is basically inconsequential. I have often seen it said on IRC, and as a result have also said myself many times, that interchanging C allocations and Rust allocations is undefined, since the implementation of the Rust allocator *is also* undefined. This gist (which enumerates basically the only reason this PR was rejected) feels like an extremely weak reason to reject the PR, since it has *multiple* things that are considered mistakes, like using `RTLD_DEEPBIND` and `free`ing an allocation from another library that has no guarantees about its allocator. If this guarantee is changing, it should really go through an RFC and it should not block a PR that reduces rustc memory usage by 20%.
I'm still sort of wondering whether this a general enough solution. Specifically I have at least one use-case where I'd want `borrow()` to return a new object instead of a reference. As is the trait can be applied for `str`, `[T]` and sub-fields of a struct (this sort of includes `Vec&lt;T&gt;`). Personally I still need a solution for doing `find()` on a `HashMap&lt;(String, String), V&gt;`, which is not workable right now.
This looks great. I'm excited about how much thought you've put into future-proofing it.
What do you want to actually query with? `(&amp;str, &amp;str)`? It might (should?) be possible to `impl Borrow&lt;(Owned, Owned)&gt; for (Borrow&lt;Owned&gt;, Borrow&lt;Owned&gt;)`.
Yes, I'd like to query with `(&amp;str, &amp;str)`. I don't think the current definition of `Borrow` allows for `impl Borrow&lt;(Owned, Owned)&gt; for (Borrow&lt;Owned&gt;, Borrow&lt;Owned&gt;)`, because you'd need to implement `fn borrow(&amp;(Owned, Owned)) -&gt; &amp;(Borrow&lt;Owned&gt;, Borrow&lt;Owned&gt;)`. I don't think there is a way to return that type, you can't in general get a `(Borrow&lt;Owned&gt;, Borrow&lt;Owned&gt;)` that lives long enough.
Talked to aturon about it, you can't do it right now anyway because of coherence issues with other impls. With negative where clauses (which we should get eventually, but not for 1.0), *that* issue should go away. You make a fair point about the reference thing. Will look into it.
Big +1 from me! Looks like a much saner system!
It would be super nice if identifiers in the macro system could expand to be in the same lexical scope as the module which they are defined in. AFAIK what is stopping us from doing that is the fact that the scope isn't known at definition time? It would be nice if I could have the constraints of the current macro definition system explained to me :)
There's a [new draft available](https://github.com/kmcallister/rfcs/blob/9bbb13fb686efed4ef46c654d82711238326081f/text/0000-macro-reform.md). The most significant change is to add item macro sugar, so that a macro in item position can have qualifiers like `pub`, `mut`, etc. In particular this allows macro-defining macros to support `pub` and `extern`. The rest of it is cleaning up the exposition thanks to many helpful suggestions from you all :)
No, it does not. You could however create a new task that evaluates a function and then passes the result back through a channel, and wrap that channel in a `Future` using `Future::from_receiver`. Sadly this is not as convenient as the C# stuff.
There aren't high-level abstractions for asynchronous code at the moment. There are third party libraries like [mio](https://github.com/carllerche/mio) with safe wrappers around the platform event loop functionality, but the standard library doesn't actually expose any of it.
Just wanted to say, thanks for all the work you're putting into this. Even though it's breaking *everything* I do appreciate it and the new APIs are very nice.
That too, but it can still work quite well when the function to evaluate is heavy enuogh. E.g. in Rustray (not sure if that still compiles these days) something like my suggestion was done for the raytracing, which worked very well. But if evaluating the async code is very cheap the overhead will probably negate any parallelism benefits.
The RFC he links to 404s for me, can someone explain this to me what this changes for rust end-users as gently as possible?
Love the Cow!
Thanks. I wish the mio library has better example programs other than unit tests to document how to use the library. Does the abstraction incur performance penalties?
https://github.com/rust-lang/rfcs/pull/235
That's really great to hear! My somewhat dropped faith is restored again)
That it was thrown away. I'd prefer to sacrifice performance and get expressive compiled language with (actor|CSP)-like[0] support baked in [1]. I believe in a long term it is much more important direction than just dropping everything for performance. And it is exactly what happens because of Servo[2]. In general my disappointment might be re-framed as Rust started as a language which made 3 steps forward and transformed into a language which made 1 step forward. It is still better than nothing but I can't get rid of sad feeling of missed opportunity. [0] focus here is on "like" because it is just a direction for research without any preference other than direction to safe multi-core development. [1] so far I can't believe green threads as a library gonna work primarily because there always will be a difference between native and green and keeping it sane without unification might cost too much to be practically usable. When it is baked into language - language developers solve the problem once instead every developer bikeshedding it from scratch. Also... When you write Lisp - you can't avoid lists, they are natural and you just use them. Same applies to concurrency and communication - the only way to get it correct is to get it to be available as a natural part of the language. [2] to be fair - it is just an assumption that Servo birth started a killing spree - segmented stacks, green tasks, GC and so on, I don't have a lot of insider information and haven't tracked Rust development long and thoroughly enough to be sure. 
If this stuff isn't exposed in the standard library, I can imagine different higher level libraries using different and incompatible lower level libraries, but I don't really know enough about mio to know if this is a big concern. This article about Node.js comes to mind, noting how by providing a standard way to do nonblocking IO, various libraries by different authors can interoperate: https://blog.nelhage.com/2012/03/why-node-js-is-cool/
What happened to this?
&gt;I'd want borrow() to return a new object instead of a reference. Then, I suppose, it would be a full fledged implicit user defined conversion and would have a little reason to keep the name "borrow" :)
Both naked functions and special function registers (memory addresses) are important in the embedded device world. Would love some feedback on how best to handle these in Rust.
I remember that I read somewhere discussion about adding such features, although it was more about defining iterators (generators) using yield. It was decided that it is too complicated (because you will need to make sure it interoperates well with borrow checker and type inference) and can be implemented in the future, sometime after 1.0. There are simply many more important things that should be done before 1.0. I also want to see this in Rust very much. 
You are basically saying that Future is missing some is_ready() method? I agree. For IO it would be very helpful to have async/await supprot in Rust. 
&gt; For IO it would be very helpful to have async/await supprot in Rust. If you're going to build that kind of infrastructures and parse transforms, provide limited coroutines similar to Python's [extended generators](http://legacy.python.org/dev/peps/pep-0342/) which can be used as lightweight syntax for both iterators and async I/O (and more). Don't build something specific to I/O.
You can mark `get_reg()` with `#[inline]`, which will basically keep the pointer in place since transmute and casts don't add any runtime load.
Not quite. It could be used as a user-defined conversion, as far as the method signature goes, but that's separate from the actual implementation. If you just look at the method signature one could also argue that `BorrowFrom` is (quite unintuitively) exactly the same as `Deref`. Edit: That last sentence is obviously wrong. The types are switched, but it's still reference-to-reference. I should wake up before anwsering comments.
It has been doing this already (becomes a `load lower 16-bits`, `load upper 16-bits` pair), but I can add it to be explicit.
I've found some limited use for it in scenarios where I'd like to spawn a small set of stuff in the background so that it benefits from parallelism, and then "join" the results of each future. [Example](http://benchmarksgame.alioth.debian.org/u32/program.php?test=regexdna&amp;lang=rust&amp;id=1). Certainly, there are other ways to do this, but future makes it easy to do in an unprincipled way. I'm not necessarily disagreeing with you, but I just wanted to point out at least one useful invocation.
Shouldn't Future be a trait? Alternatively, there should be a trait which Future implements? Currently, std::sync:Future is for computations. However, many things can probably generalized to IO, Communications, etc instead of only computation.
Funnily enough, the way I use futures in java I almost never need an "Is it done" sort of message. Usually I spin off a couple of futures doing some expensive things and then at the end of whatever it is I'm doing (usually loading data) I simply wait on each future and join the results together. Do rusts futures have the ability to do call backs? Because that is the other thing I use on java futures a lot.
You can still use it to launch several things in parallel and then check them all at once. The missing thing is making it a monad so that you can chain computations on it. It might be a good idea to remove it for now until a proper implementation is done. 
&gt; Shouldn't Future be a trait? I'm not sure it'd make much difference. &gt; Currently, std::sync:Future is for computations In that context, I/O is a form of computation. In theory, sync::Future is for data which is not available yet (but may/will be in the future). In practice though, the current version of future mostly does "memoized computation", it will resolve (run its callback) by blocking the first time the value is requested, memoize the computed value and yield it immediately the next time.
&gt; Do rusts futures have the ability to do call backs? Nope: http://doc.rust-lang.org/std/sync/struct.Future.html
A function that small is pretty much guaranteed to be inlined, performing the function call is more instructions than the function body contains.
If anyone here thinks that std::sync::Future is too useless at the moment to exist in the stdlib, I would recommend they file a bug to have it removed before 1.0.
Using a channel and `rx.try_recv()` is preferable to polling on a `Future`, in my opinion. Creating a `Future` and doing the rest of your main thread work, and then waiting on the future is a very common pattern. EDIT: also see http://doc.rust-lang.org/std/macro.select!.html 
This would be a very useful addition.
You can always create a new future that waits on the original one, and methods could be provided that allow this pattern to be optimised by running the new closure in the thread of the original closure.
It isn't useless, I have [used](https://github.com/Thiez/rustray/blob/master/src/concurrent.rs) `Future` to add concurrency to Rustray. Perhaps `Future` could be expanded, but complete removal seems rather useless, especially since libraries don't freeze at 1.0; there is nothing holding us back from improving it later, and it is already useful today.
The RFC mentions this in "Alternatives": &gt; We could try to implement proper hygienic capture of crate names in macros This is suggesting the same thing you are suggesting, I think. Unfortunately the thinking seems to be that this would be too hard to implement :(
Well, a big problem (IMO) in C++ is that `future` is not a `concept`, but a particular implementation of a particular type of future that is not good for a lot of applications (see `boost::future`, `hpx::future`, ...). This is fixed in the ASIO proposal, which can work with multiple future types and solves the `std::async` problem. TL,DR; Future should be a trait.
An `is_ready` function or similar would be nice, but I strongly disagree with the claim that the lack of it makes them almost useless. The typical programming pattern with futures - and this isn't Rust-specific - is to do something asynchronously, and then get its result only when you need it. Being unable to check if a future is ready does mean that you can't, for example, do some work item A in a loop until another work item B is ready, but that's not a typical use case for futures.
You'll need to do a *lot* of JNI to integrate as a normal Android app, and you're going to lose any advantage of using Rust. Just use Java. See also http://developer.android.com/reference/android/app/NativeActivity.html for what they have in their NDK for C++. But, https://github.com/tomaka/glutin/blob/master/examples/window.rs works on Android. You can draw to the screen with OpenGL ES. You just don't get access to the rest of the Android SDK.
This is a question along very similar lines that's a bit too simple to deserve it's own post, so I hope it's okay to pop it in here. Is there a reason why `sync::Mutex` does not have a `try_lock` method, while the underlying `sync::mutex::Mutex` does? It looks like it just hasn't been implemented, but there could be a good reason for that I can't see.
Really? I didn't know you can patent programming concepts.
It took me a while to get into different mindset (coming from Java-like languages) that traits are not classes, and not even interfaces. Traits are more like pattern matchers for structs and other traits, unusable without having a concrete struct behind them or hiding them in a Box.
To clarify, isn't it pretty much the same situation as with C (or any non-Java language for that matter)? 
The motivations are incorrect. Servo was brought up as one of the main use cases for keeping libgreen, until it became clear that Servo wouldn't really benefit from libgreen staying around. In fact, /u/strncat was one of the main people, perhaps the most vocal person, leading the charge against libgreen and runtime I/O, and his complaints in the thread above are about the fact that we didn't remove it sooner.
I somehow doubt it, given that there are at least two ES6 -&gt; ES5 implementations of generators via state machines (one of them backed by Google, and the other one by Facebook, IIRC - those are the ones I remember right now, anyways).
http://www.google.com.ar/patents/US20100313184
Yup, it's a bummer.
Ohh, it's a bit unexpected turnaround for me. My apology in this case for false blaming, although it still doesn't solve disappointment.
Note: https://twitter.com/posco is the author of this, not me.
Not to mention Clojure also does this via it's macro system in core.async.
I am unsure how much you could do without `unsafe` though, since most `std` containers contain `unsafe` code.
This is basically what a Future is, in most environments that provide them. Each unit of work spawns off up to N potential pieces of work, then does its own work until it encounters a data dependency. What you want is probably a Promise, not a Future. Promises are a pretty well-understood approach to handling this. Promises are basically Futures, but you can't block on them. In most environments, you provide some kind of continuation block that executes when the Promise becomes resolved. In some, you can do Promise chaining, so that you can issue "speculative" requests against a Promise object, even before it is resolved. There's a lot of good literature out there on Promise vs. Future. I think Futures have their uses, although I'm more fond of Promises, having used both extensively.
In the language I worked with, all containers except the most basic heap-array type (with length fixed at allocation time) were 100% safe. If Rust's containers contain unsafe code, then it would be necessary for the interpreter to be "in on the joke", and for the interpreter to directly implement the semantics of the container. Hopefully the set of container types that actually need unsafe code can be minimized. I hope I can build an effective hash table, or AVL tree, or whatever, without resorting to unsafe code. 
Hey, I'm the author of [bincode](https://github.com/TyOverby/bincode); thanks for including my library in your benchmarks! I'm somewhat surprised that rust-msgpack is beating bincode when it comes to deserialization; I would imagine that both libraries have to do fundamentally the same operations and I can't think of anything that I'm doing that would account for a 60mb/s performance difference. If anyone has ideas or would like to help, please shoot me a message or reply here!
I thought `#[inline]` meant `#[inline(always)]`?
I agree with what you're saying here. I would love a monadic bind operation on any optional, future, promise, etc, and really enjoy functional programming. That being said, i'm not sure i would conflate monadic binding with the need for a public `is_complete()` operation, which is what I think the OP is actually asking for.
The Cap'n'Proto results are just incredible: 5 GB/s to serialize &amp; 2 GB/s to deserialize? Faster than most disks/network cards can provide.
Why was it removed? Is there no way to use it now?
[Google is implementing and standardizing async/await in dart](https://www.dartlang.org/docs/spec/Asyncdraft-TC52.pdf). Even with how ridiculous patent laws are, I doubt you could patent operator names or the general concept of async/await. Perhaps the implementation. Either way, Google has kinda set a precedent here.
If I have to chose between a significantly more efficient IO library and slightly better support for futures I'll have to go with the former.
Because it was quite awful :p This is the RFC that led to its removal: https://github.com/rust-lang/rfcs/pull/230
I mean hey, Google can also afford the lawyers if something goes wrong. I don't know if Mozilla can or wants to.
According to local laws, but then trade agreements make them respect US IP law anyway :/
Take a look at [read_be_uint_n](https://github.com/rust-lang/rust/blob/16d80de231abb2b1756f3951ffd4776d681035eb/src/libstd/io/mod.rs#L669). Which is the core of `read_be_u64`. The LLVM might be a good compiler, but I don't think it is smart enough to turn that into a buffer read with an endian swap. Spending some time optimising that module will yield some large improvements in `bincode`'s decoding performance.
Do you know for how long such a patent remains valid?
Perhaps in theory, but has it actually been tested in court, with software patents?
My interpretation is somewhat less generous. Which is that we tend to always go for the easier-seeming worse-is-better solutions first (`while` one is available), and only grudgingly get backed into doing things the "right way" after experience shows us that it actually is worse. This (among other reasons) is why, almost invariably, if you take some aspect of the current language, some earlier iteration of it made the diametrically opposite choice. Of course, part of this is due to the completely natural fact that we are not omniscient, and sometimes only discover the better solution after some time. But not all of it. And as I mentioned above, I don't think that it's due to anyone having anything other than the best of motives; but just because the problem isn't a personal one, that doesn't mean a problem doesn't exist. I think a large portion of it is attributable to the fact that we have been in perpetual imminent release mode for (at least) over a year now. We thought we were going to release at the end of 2013! We thought 0.4 was going to be one of the last preview releases! This naturally leads to people wanting to patch up what already exists rather than commit to replacing it, so that it can be released. But while this is understandable from a human perspective, it is (as originally noted) *counterproductively* short-sighted: it is what leads to the kind of the situations where we invest half a year of effort into polishing the built-in closures, before (correctly) yanking them out entirely, or likewise the concurrency and IO design. If we had admitted that replacing the built-in closures with trait-based ones was the way to go around last winter, then they would probably already be stable as of half a year ago by now. The decision to dogfood and write the compiler itself in Rust, alongside Servo, is the wisest decision Rust as a project ever made, essentially single-handedly enough to compensate for every other mistake and wrong turn taken along the way - albeit not without a significant cost in terms of wasted effort and lost time. It's hard to imagine where Rust would be today otherwise. 
Or maybe they did a bit of research first to make sure everything is kosher? You certainly seem set on dismissing the idea entirely based on some litigious boogeyman, so forget I even said anything. 
Fairly recently, rust added lifetime inference for a few select cases where there is only really one reasonable way to add explicit lifetimes eg: fn(&amp;self) -&gt; &amp;bar You almost always want to annotate that like fn&lt;'a&gt;(&amp;'a self) -&gt; &amp;'a bar (The only other thing you could write is &amp;'static bar (or if in an impl&lt;'b&gt; block or something 'b but I think we might turn off inference in this case))
I think that's a false choice. I think there are other unfortunate design decisions that led to this being inefficient, not that it was mandatory if you're willing to make some changes to make it work. Also, I don't think this is "slightly" better - I think it's a huge improvement. A much bigger deal than most "high level" language features are. 
I don't see what being compiled has to do with not having a REPL. Clojure is compiled, yet it has one. Not supporting reflection and dynamic evaluation would be a bigger issue preventing a REPL from being made.
Unsure about one through trade partnerships specifically.
Huge tech companies sue each other over IP all the time, and especially over software patents. Here's just one recent example I remember where Microsoft and Apple sued Google: http://arstechnica.com/tech-policy/2013/10/patent-war-goes-nuclear-microsoft-apple-owned-rockstar-sues-google/ And it's not so much a dismissal as it is considering the risks involved.
It's incredibly complicated because the laws keep changing. Also, you can file a PTA with the USPTO, which can lengthen the time. Most software patents have a 20 year term, assuming that the patent-or pays all the associated maintenance fees.
Hey, are you the original author of Rustray?
That's so cool, I've kept it somewhat up-to-date since over a year ago (although I haven't updated it the last two months).
this proc thing is plain ugly, when are we getting the new lambdas?
As far as I know, operators do not have special rules for overloading. They're just sugar for trait method calls.
I dont think anyone thinks generic methods are bad. Anyway, + is just sugar for `Add::add`.
There'd need to be special rules for the snake case lint. So, technically it's got a liiiittle bit of sugar.
Then you have to wait on the one future waiting on the other. :)
Any language can have a REPL. I was just complaining about how the OP phrased it.
They're implemented, [but there's still some bugs to iron out][1]. For the most common use cases, they work great. They're just feature-gated right now. But the plan is to dump the current closure syntax and go all-unboxed by 1.0. I can't wait, either. [1]: https://github.com/rust-lang/rust/issues/18101
I naturally can't speak for the linked thread's OP (and don't have anything but superficial knowledge of GPU computing), but I believe single-source GPU support refers to having an entire program written in a single language, while still being able to leverage the GPU. So in practice, the entire program would be written in Rust, and parts of it would run on the GPU with minimal hassle for the programmer, instead of having to write separate OpenCL programs that are called from the Rust code. The intermediate representation of OpenCL, [SPIR](https://www.khronos.org/spir) seems to be based on LLVM IR, so maybe there is a way to make this happen by using the LLVM behind Rust's compiler?
Good point. Do you have a better suggestion to measure the bandwidth? There are a couple enhancements I'd like to add to our benchmark suite: * extra metadata so we can display the size of the metadata or other arbitrary stuff. * min, max, and standard deviation * optionally output results in a csv * optionally use a fixed iteration size rather than an adaptive size. Is there anything else that would help?
There's a [use in the test harness](https://github.com/rust-lang/rust/blob/3a8f4ec32a80d372db2d02c76acba0276c4effd0/src/libtest/lib.rs#L1054), and there's quite a few externally e.g. searching github for `try_future` gives [a](https://github.com/servo/servo/blob/82f314d2c475cf712b42a7817f98e6fe422e76f6/components/util/task.rs#L40-L42), [b](https://github.com/zonyitoo/shadowsocks-rust/blob/bdc543862b7e63ea21dd6ccb642fa33c17119817/src/relay/server.rs#L92), [c](https://github.com/nikomatsakis/rayon/blob/a64adb6e43b84e265d49f5e21d6e497bcf35a4b8/src/lib.rs#L41), [d](https://github.com/bjz/glfw-rs/blob/b76cd8bb9ecd063de25cf380a1ad6bac8e264390/examples/render_task.rs#L42) as a somewhat random subset.
http://blog.theincredibleholk.org/blog/2012/12/05/compiling-rust-for-gpus/ was from 2012, so lolol, but I see no reason why this couldn't work out somehow.
I implemented a procedural macro to generate bitfield-like struct. Feedback is very welcome.
Thanks for posting this. I'd love to see more articles around embedded dev with Rust.
Yes, and for many use-cases this is perfectly OK: either you're doing something with the result right now and so are forced to wait under any strategy (or, if not *right now*, very soon, and waiting now is not noticably different to waiting then), or you can just spawn yet another future to run in the background. Of course, this doesn't cover every use-case of futures, but it is quite a lot.
Primarily the big changes is that I found out that it's actually more than possible to have a `fail!(...)` macro that can both "throw" errors that already exist as well as creating new ones. The way it works is that it accepts an arbitrary number of arguments and then invokes `ConstructError::construct_error(args_as_tuple, location_info)` where the location info is the current file/lineno and the args are a tuple of the arguments provided to `fail!`. For it there is a generic implementation of a single argument tuple that invokes `FromError::from_error`. This means you can now do this: fail!(an_io_error); As well as fail!(MyErrorKind, "some message here"); Or something similar. The example linked has an implementation of `ConstructError` that supports the latter case. Would love to get more feedback about it :)
Measure decoded data instead....
You could also wait and see how Go is going to tackle this. Perhaps you would be able to leverage some of their tooling/approach and make it work once they have something ready. Their approach involves JNI as well so you are not going to work around that aspect. https://docs.google.com/document/d/1N3XyVkAP8nmWjASz8L_OjjnjVKxgeVBjIsTr5qIUcA4/edit?pli=1#
It might be, but I know Microsoft has submitted it for inclusion in the C++ 17 standard and now that C# is open source, it is licensed under the Apache License 2.0, which has patent related clauses. So I'm not 100% sure what the deal is. I could see it being a *potential* issue for languages other than C# and C++.
Cool, nice to hear that!
That would be fantastic, although yes, my understanding is that the patent grant is just for C#.
Looking nice. It would be nice to require less boilerplate though (maybe some macros to rescue?). Are you looking to contribute it back to stdlib or proceeding as a separate crate? edit: I would shorten CommonErrorData, ex: ErrorInfo, ErrorData, CommonError
&gt; But the fact that Future hasn't been touched in months besides fixing it so the module compiles only demonstrates how little utility it has in the wild in its current state. That really isn't a good indicator at all. Look at the history for the entire `sync` crate. The whole crate hasn't received much attention in the last few months.
Ok, so I went ahead and tried to reimplement the code with traits, but it didn't work out so well. The problem is that I use several methods from `HashSet` in the same scope that I call `is_whitelisted()`, including: `get()`, `contains()`, and `insert()`. But the only way I can use them is if I add every method I need from `HashSet` to the `WhiteList` trait and add wrappers around them in the `impl WhiteList for HashSet&lt;String&gt;`. This is essentially no different than having to using a struct with a single field, and adding wrappers for every needed method.
The bugtracker has [some information](https://github.com/servo/servo/issues/1908) on that.
Actually, it's licensed under the MIT license, not Apache. Microsoft just licensed the patents to the world separately.
All of the synchronization support is going to be thrown out and rewritten. It will have features like try_lock / timed_lock the second time around.
Doesn't that mean it's under MIT? The compiler isn't the important part of C#, .NET is.
An `#[inline]` marker does not tell the compiler to inline the function. It only raises the cost threshold and this function doesn't need that. In Rust, it's also required for cross-crate inlining because the ability to enable *automatic* cross-crate inlining as used in languages like Go is missing. The only time cost-based inlining gets used is at `--opt-level=2` and above, so `#[inline]` will not accomplish anything at `--opt-level=0` / `--opt-level=1`. A function marked `#[inline(always)]` will be inlined whenever possible at all optimization levels, rather than it being based on a cost threshold for `--opt-level=2` and above.
&gt; #[inline] is a strong hint to the compiler. It's not really a hint. It just raises the cost threshold so it will be willing to inline a larger amount of code. There isn't cost-based inlining at -O0/-O1 so it won't have any positive impact. It's used extensively in Rust because we don't have automatic cross-crate inlining - `#[inline]` results in serialization / duplication of the AST across every crate.
That's pretty neat! Do you think it would be able to support packed arrays of items that aren't 8 bit aligned?
I guess it depends on your use case, really. If you have a function that only needs the methods implemented on trait `WhiteList`, then you can have it be polymorphic over *all* implementations of `WhiteList`, and not just `HashSet&lt;String&gt;`. You could implement `WhiteList` for `TreeSet&lt;String&gt;` or `BTreeSet&lt;String&gt;` or any struct you want, and you'd only have to write a single function if all you needed was `is_whitelisted()`. It's also necessary if you want to monkey-patch structs that you don't own, like `HashSet`, since the compiler won't let you add inherent (i.e. non-trait) methods to an external struct, for privacy reasons. You can't do either in Java, IIRC, so +1 for Rust! It's unfortunate for your case that all the collections traits were recently removed, because then your function could have been polymorphic over, e.g. `WhiteList + Set`, or have `trait WhiteList: Set`, meaning that any struct implementing `WhiteList` must also implement `Set`, equivalent to `interface WhiteList extends Set` in Java. This is assuming the `Set` trait had the methods you need. But I guess the devs decided that polymorphism in the collections crate should only apply to the contents of the collections and not the collections themselves. It didn't make much sense to me either. I think the reason is that only a few structs covered 90% of use cases, and the others were reserved for when one needs specific behavior or performance characteristics, such as choosing a tree-based set implementation (better cache locality and automatic sorting) over a hash-based one (constant-time insert/index/remove), so it made sense to push just those core structs and specialize the rest for edge cases instead of making them all conform to common behaviours. Edit: While monkey-patching with a trait is the recommended pattern here, there's actually a better way to create a wrapper if that's what you need. You create your wrapper struct with the inherent methods that you need, and then implement `Deref` and `DerefMut`: struct WhiteList { set: HashSet&lt;String&gt;, } impl WhiteList { pub fn is_whitelisted(s: &amp;String) -&gt; bool { self.set.contains(s) } } impl Deref&lt;HashSet&lt;String&gt;&gt; for WhiteList { fn deref&lt;'a&gt;(&amp;'a self) -&gt; &amp;'a HashSet&lt;String&gt; { &amp;self.set } } impl DerefMut&lt;HashSet&lt;String&gt;&gt; for WhiteList { fn deref_mut&lt;'a&gt;(&amp;'a mut self) -&gt; &amp;'a mut HashSet&lt;String&gt; { &amp;mut self.set } } This is how containers like `std::rc::Rc` and `Box` work, since they obviously can't magically implement every method on any arbitrary struct they might contain. `Box` doesn't actually show as implementing these traits in the documentation for some reason, but I know that it does, possibly via compiler magic. What happens here is the compiler first checks the container for the method you're calling, and if it doesn't find it, then it will check the type parameter on the `Deref` and `DerefMut` impls, if they exist. Not sure what happens if there's a name collision. I think it just calls the outermost method it finds.
Awesome, thanks. Do you know if there's a vague timeline on that, or if it's just something that will happen in the future?
I'm glad you're suspicious of this design! Please question everything we do! This is the whole purpose of the RFC and PR process. To make sure we haven't worked ourselves into the wrong solution. I look forward to your notes. (I, too, am becoming suspicious of this design)
I'm sure it could be written a lot nicer now. IIRC it was originally written on v0.1. Tons of stuff just didn't work. 
Servo has [bitfield!](https://github.com/servo/servo/blob/master/components/plugins/macros.rs). Not entirely sure how it works though.
Did you by any chance also benchmark Cap'n'Proto C++?
It's a less nice version of bitflags macro.
Ack, sorry about that. I've fixed the submodules, added a README, and if you do a `git pull &amp;&amp; git submodule init &amp;&amp; git submodule update` everything should work for you.
Not yet, sadly.
If the default value is ‘actually an expression’, does that mean that it is just copied into the use site? So if I set `println!("hello world")` as a default value for a field, would it print ‘hello world’ every time I initialised the struct without specifying that field’s value? If not, how would it work? /u/chris-morgan has [a related proposal](http://discuss.rust-lang.org/t/allow-a-trailing-in-a-struct-literal-meaning-default-default/885) that just proposes making `Default` a built-in trait that fills in any omitted values in a struct literal (if `..` is used at the end of the struct initialiser). (Also, I don’t think that this would be any more flexible than `Default`. You can manually implement `Default` for any type, so it doesn’t have to use the `Default` implementations for its fields’ types.)
How would e.g. int *x; int foo() { printf("%d", *x); } turn to Rust? It wouldn't. Therefore perhaps only the most trivial C programs could be translated to Rust without problems. Sure, a translator could comment problematic parts of the code and require a programmer to manually handle such situations. Perhaps the biggest problem would be that such a translator wouldn't be able to infer required information on e.g. lifetimes or ownership from the C dode.
I would love to contribute this to std::error. The boilerplate is a problem and i hope i can resolve this somehow. Right now the biggest issue is still handling errors in a good way.
This is not really what futures/promises are supposed to be. The problem here is that chained computations must start as soon as the parent computation ends, not when the final future is forced. This requires to store all chained computations in the future they are chained on, run them eagerly, and so on. There are plenty of implementations that do this, for instance in Scala, C# and JavaScript. 
Why not just make it so that writing initialisers causes the compiler to emit an implementation of Default for the struct? You'd probably have to require that if any field has an initialiser, all of them have to.
What about `X::is_signed::&lt;T&gt;()`?
I think if you really want a "bandwidth" measure rather than a time measure, then what you want to be considering is the in-memory, ready-to-use size of the message, not the on-the-wire size. Think of it as "I have X MB of data in my memory space that I want to transmit to someone else's memory space. What's the fastest way?" The X here is a measure of your data independent of the encoding you use. The size on the wire is a function of both X and the compression ratio achieved by your encoding. As a shortcut, you could simply use the Cap'n Proto message size as your X since Cap'n Proto encoding is very similar to in-memory data structures. In any case, as long as the value you choose for X is consistent across all encodings (regardless of whether it's accurate), it should be fair for comparison purposes. (Of course, the semantic data structure being encoded had better be the same across all the benchmarks.) (Disclosure: I say all this as the author of Cap'n Proto, knowing that this recommendation will likely reduce Cap'n Proto's lead over protobufs, though the lead over JSON may actually increase.)
Yeah, I have been using the &amp;self approach but in sometimes, creating an extra temporary object just for invoking such methods looks like an overhead. I guess will have to wait till a suitable implementation is available and use the work around till then. 
The trouble is that sometimes a field doesn't have a reasonable default.
&gt; write your whole app in C/Rust/C++/etc. using OpenGL to draw to a full-screen window, without using any of the Android UI framework. Sounds like writing an android game in rust is easier than writing a non-game application! :P
Well, I've just started using Rust, and I wouldn't call myself an experienced Rustacean by any stretch of the imagination; however, I am a fairly experienced C hacker, and am learning Assembly (and fairly good at it). I think it's a real trade off. With C, you don't have to fight with any borrow checker, and it's easy to use, but, as you said, it's also easy to abuse. It has some memory management, which is nice, but, on the whole, it's fairly easy to screw up and have a Segmentation Fault. On the other hand, Rust you do have to fight with a borrow checker, which is sometimes annoying. However, once that code compiles, there’s a 99% chance it'll work on the first time. No segmentation faults, no use after frees, nothing like that. And, after a while, you realize where you're being non-Rusty, and you change your style to work with Rust better. 
I have been writing C code for years, and would consider myself proficient, and have been playing with a lot of the new C++11 stuff. Given that background, I think Rust feels quite natural now, though the language still has rough edges. We recently had an email thread at work (C++) that went on for a few hours about the semantics of APIs and how long pointers returned could be used for. That is the sort of thing the borrow checker could have answered just by trying, but instead we had to go read through code, consult coworkers, and carefully examine how data structures lived. In that way, I think it isn't cumbersome at all: The compiler is doing the hard work most of the time. That doesn't mean Rust doesn't have bugs, too harsh restrictions, and rough edges. It does. And after almost two years of using it, I will use it over C whenever I can.
Note python 3.4+ has c# style async/await
GCs are problematic, even as highly advanced as Azul's. First, they don't really scale (to 100GB heaps). Second, they have an impact to runtime efficiency. Finally, they only have a limited throughput - you have to test your application and see whether it's producing too much garbage for the GC to handle. Yes, the technologies are improving, and there are probably many more improvements possible (with the advent of hardware transactional memory and CPU transactions, or better kernel support), but AFAIK they are not there yet. I think that the last point is the most damning - in GC'd languages such as Python and Java, most programmers are not really mindful of how much they're allocating. Also, the languages themselves offer little facilities for preventing allocation (Go and C# are better). Using lifetimes/regions/unique pointers, you can write code that is much more flexible with regards to where the object is coming from (a reference to a heap- or stack-allocated object), or use patterns such as memory pool allocation (which is *way* faster than both malloc/free and any form of GC) - allocate a pool at "the beginning" (of e.g. an event handler), allocate all temporary objects in the pool (using bump-pointer allocation), and release the pool at "the end".
As others have already said, the ownership and lifetime rules in _correct_ C are basically the same as in Rust. The difference is just that C also allows for many incorrect or at least subly broken programms to pass the compiler, while Rust rejects them outright. Of course, in Rust this is achieved through types and annotations that need to be reasoned about up-front, so there is definitly a treadeof going on: Directly finding a correct design to please the compiler vs writing down something quick that might contain memory managment bugs. My experince after quite some time using Rust is that it's often very obvious what types and ownership structures to use, there isn't really much variation for day to day code. Mostly you just stack allocate and work with refetences to or into those values.
I believe all GC-managed objects have to be allocated on the heap, whereas structs in Rust are allocated on the stack. You only start looking at heap allocations when you get into vectors and boxes and owned `String`s. Heap deallocations in Rust are also deterministic, which gives much more consistent memory usage and performance. Even a low-pause GC might not be suitable for timing-sensitive applications, and definitely not embedded systems, where memory and CPU cycles are both very precious. You can use a refcounted container like `std::rc::Rc` or `std::sync::Arc` if you need more flexibility. Since recounted containers aren't usually pervasive in Rust code, cycles aren't really an issue.
&gt; Defining variadic functions... It might be interesting to try to compile a more exhaustive list of problematic constructs. Others I can think of: * `goto` * `switch` with fallthrough or `break` in unusual places, Duff's Device, etc. And, er... what else?
I agree. For example, if you have `int *x, y;` then x is a pointer and y isn't. It would be misleading to have the `*` next to the `int`.
I'm not entirely sure, but a [recent RFC](https://github.com/rust-lang/rfcs/blob/master/text/0401-coercions.md) describes type coercions in more detail. The section that is pertinent to you, I think, is: &gt; the box expression, if the expression has type Box&lt;U&gt;, the sub-expression is a coercion site to U (I expect this to be generalised when box expressions are); Which to me implies "yes, `Box&lt;T&gt;` is special cased, but hopefully will be generalized." (I'm curious what generalization means here though. Does it mean coercions will work only with things that work with `box`?) (Note that the RFC isn't implemented yet, I think.)
I just want to add that tt is a myth older more than twenty years that hardware is getting faster so the software doesn't matter. In reality it does even more than before - on mobiles, large datacenters, games and many others. At any particular point in time during those yeas there were some "huge improvements comming" yet it never made any real difference. So I do not believe that this time it is any different.
I don't have much to say because this is at best a personal anecdote, but I'll just use some reference points. From the best that I can tell, I can write Rust more quickly than I can write Haskell, but more slowly than I can write Go. I write C slower than all of them. (Well, I guess the *writing* part is pretty quick, but it's the stuff that comes after that slows me down. e.g., Running valgrind to get a stack trace on sigsegv...) And I probably write Python more quickly than any, although refactoring is by far the slowest. (And I love to refactor.) I will also acknowledge that I went through some serious flailing (fun) pain when I first started with Rust in earnest (back in March?). It definitely took a few days/weeks to get really comfortable with the language. I think that's perfectly acceptable, particularly when faced with lifetimes that have been formalized in the language (which was a first for me).
&gt; I'm often unsure about what certain constructs are *actually doing* despite having a tenuous understanding of what they mean in terms of ownership. What do you mean by "actually doing" here? The machine code that it actually compiles to? Rust is *much* better than C++ in this area, and closer to (though not *quite* as explicit as) C. Thinking about when to pass things by what kind of reference or value in C++11 still makes my head hurt, especially when templates are involved, even though I have been doing C++ for a decade. (Admittedly, this is in part because the codebase at work is C++03, and because I have near-completely forsworn dealing with C++11 in hobby projects now that there is Rust.) Rust is *much* simpler: &gt; In particular here I'm thinking in comparison to C, where there is one pointer type and everything is pass-by-value Everything in Rust is pass-by-value as well. References are first-class values just like pointers in C, only with restrictions on what you may do with them (e.g. you can't return them outside the scope in which they're guaranteed to be valid). When you "pass a thing by reference" in Rust, that just means you're passing a reference by value. Copies/moves of values, parameter passing, assignment etc. are always shallow memory copies of the given object just like in C.
The question mark was just to disambiguate "can we have single-source GPU support?" from "we have single-source GPU support". :) I know (roughly) what it means.
Technically you are correct, but in many cases they are legitimately unsafe (that is, there are ways to use the API which would cause memory unsafety or undefined behaviour), and rely on the programmer to do the right thing.
The main selling point of Azul's GC is that it *does* in fact scale to enormous heaps. They consider 100GB to be a "small" heap. GCs can be done right, but it is hard.
I'd probably just use if let, because it's more general: let value = Some(42u); if let Some(t) = value { println!("{}", t); }
That can be written `.as_ref().map(call)`, or just `.map(call)` if you’re OK consuming the variable.
But with 'map' I have to return something, which I don't want, because I've nothing to return and then I even have to handle the returned 'Option' of 'map'. 
Ok, thanks, I haven't been aware of 'if let ...'. At the end it's the difference between: if let Some(t) = Some(42u) { println!("{}", t); } Some(42u).on_some(|t| println!("{}", t)); 
Thanks! Yes, that RFC makes a lot of sense to me. Ultimately I want an Rc&lt;Trait&gt; too (or even Rc&lt;RefCell&lt;Trait&gt;&gt;), that's why I started these experiments in the first place. In short, things do not work now, but hopefully before 1.0.
I don't think you do. E.g. this works: Some(35u).map(|n| println!("n is {}",n)); 
Thanks, this series of benchmarks has been really interesting :-)
The Rust playground's compiler doesn't seem to complain about unhandled return values for me. Is there an example of it doing this?
 rustc --version rustc 0.13.0-nightly (e82f60eb4 2014-11-11 22:37:05 +0000) For a line like this: writeln!(stderr, "err: {}", err); I'm getting: &lt;std macros&gt;:2:34: 4:6 warning: unused result which must be used, #[warn(unused_must_use)] on by default &lt;std macros&gt;:2 ($dst:expr, $($arg:tt)*) =&gt; ({ &lt;std macros&gt;:3 format_args_method!($dst, write_fmt, $($arg)*) &lt;std macros&gt;:4 }) &lt;std macros&gt;:1:1: 5:2 note: in expansion of write! &lt;std macros&gt;:3:9: 3:51 note: expansion site &lt;std macros&gt;:1:1: 5:2 note: in expansion of writeln! 
Ok, thanks, but I'm not quite sure what I'm thinking about this different behaviour of Result/Option. This might explain funtions I saw returning 'Result&lt;(), Error&gt;' instead of 'Option&lt;Error&gt;'. 
Ah. I suppose the problem is that you need to fully resolve any identifiers and the infrastructure for that is inaccessible. I wonder if there is some kind of horrific hack that would work for that :p
&gt; `goto` Did labelled blocks and break/continue arrive yet? Won't suffice for things like Duff's Device, but then such things should probably be left to the compiler, anyway. `goto` in `unsafe` blocks might be a good idea, though, also first-class labels, so you can do jump tables: `let x = [:start, :foo, :bar, :end]; goto x[baz()]`. But for the sake of sanity, don't put such things in the default language, just have them there so people don't drop to assembly.
Oh, right. Didn't realize that.
AFAIK, the Oracle JVM haven't implemented the stack allocation yet (I've been waiting for the thing for years and periodically checking the state of affairs). They've argued that it's too much work and GC is good enough. They use escape analysis for other things, like locking optimizations.
It works for `from_str()` because `FromStr::from_str()` *returns* `Self`. Here, however, `is_signed()` neither consumes nor returns `Self`, so standalone function won't work. Phantom type parameters is the only solution in such cases, AFAIK.
&gt; I know that there are many concepts that are hard to grasp at first, but once you have understood them they feel very natural to use. This is basically how I feel about Rust. There is a curve at the beginning, but then you end up fighting with the checker less and less. I also don't mind fighting with Rust's checks, because that means I would have had a bug if it had compiled. Maybe I've just accepted my fate ;)
I write Scala (previously Java) for a living, so I don't have much experience with C or C++. I used them only in university studies briefly, that was some time ago. However, I do want to write lower-level programs which don't need JVM, and Rust turned out to be the language I was looking for. Higher-level languages usually make the programmer "forget" about memory management. That's how they usually are advertised, at least. When you are not careful, you can get nasty memory management-related bugs even in garbage-collected languages. Rust, however, provides *more* safety than GC, *without* runtime costs of garbage collection. Some bugs like iterator invalidation can happen in Java very easily, but they are impossible in Rust in principle. Also Java is supposed to be concurrent language, and threading support in JVM is really great, but how much pain it is to write multithreaded code with bare threads, with lowest-level synchronization primitives! Yes, complex libraries like Akka are very pleasant to work with, but you still can blow your foot off if you are not careful (e.g. if you use mutable state in actor messages or incidentally capture over actor state in async callbacks in futures). Rust, on the other hand, *enforces* good concurrency practices and eliminates a lot of possible bugs which could happen in concurrent code. Rust also changes your mindset, very much like functional languages do, just in another direction. Even if you work with a high-level language, after Rust you do think more about who owns what and how the data should be passed around. However, Rust does have some learning curve. It may take some time to get accustomed to compiler errors and to understand *why* it complains on your code. The more you work with it, the better you understand the way you should write Rust code and how to work with ownership and borrowing rules. Personally I can say that after some time working with Rust you *will* see in advance where the compiler would complain, so you will be able to write the code which passes borrow checker right away. You won't be fighting the compiler anymore; its more like the compiler gently points you to your misspellings, and you instantly recognize what's wrong there and fix it immediately because you do understand what's wrong there. I guess it took me somewhere around 6-12 months of hacking with Rust in my free time to reach this level. I did have problems understanding some basic concepts in the beginning, but after some time they just went away. I can't say how much time I would need to write the same program in C and Rust because I'm not fluent in C (so it is likely that I will write Rust program much faster), but I definitely can say that I don't get more compiler complains with Rust than I do with Scala. And yes, Rust feels very easy and natural to use. Granted, sometimes borrow checker is too strict and rejects safe programs. Mostly this happens because of purely technical reasons (e.g. non-lexical borrows are not implemented yet, but they will be, and it will allow a lot of valid programs which were rejected previously to compile). There *are* situations when borrow checker is too restrictive (though I can't think of one now, except non-lexical lifetimes), but almost always the compiler complains for a reason, and after some practice this reason will almost always be apparent to you.
I would argue that the compiler is the important part of C#. It's an implementation of the specification. The specification is maintained by the team that develops the compiler. You could make the compiler target some other runtime, but it would still be C#. The framework and CLR are definitely going to both be MIT and the compiler will be targeting an IL that I would say is also licensed under MIT, but the compiler itself, which is the implementation of the C# spec is Apache. So I would argue by virtue of the fact that the compiler team is not only in charge of implementing the spec, but maintaining and updating that spec... that C# as language is Apache.
+1, Haskell is the GC'd dual to Rust.
&gt; Rustacean Is this a thing? Please be a thing. It sounds cool. Cooler than *Ruster*, I guess.
Its a thing. I know, I thought it was cool too :D
Or write your own automated Java binding generator for Rust! 
It allows "automatic and correct mutability" in the sense that things that have a single reference are mutable, while those that are shared are not mutable. Also it works better with swap and can use less memory (although moving GC avoids fragmentation, so it's not necessarily clear cut). 
Its actually what it does. I added one example that try to illustrate that. Yon can find it [here]( https://github.com/dzamlo/rust-bitfield/blob/master/examples/bits_position.rs) and you can run it with: cargo run --example bits_position
If you comment out the test.assign_ref() line, then it compiles. Which is instructive -- it means that what is happening (or could happen) in assign_ref() could interact with what happens in later statements. I think you're running afoul of the rules concerning single mutable access to data. After assign_ref(), it is possible to have two different access paths to the same data -- to 'test'. One of them as the binding 'test'. The other is the borrowed reference that is currently stored at 'test'.reference. The compiler rightly rejects any access to 'test' after the assign_ref() statement, because the self.value.as_slice() reference is still live. The confusing fact is that it is held live by a reference within itself. So you've kind of locked a box, and then pushed the key for the box into a slot on the side of the box. You can't unlock it. I think this is correct behavior. Can you describe a bit more what you're trying to do, and maybe we can suggest a different pattern for accomplishing the goal?
/r/playrust
I prefer Rustafarian.
That's all of C/C++ in a nutshell.
In general, returning a `Result` for errors is recommended, yeah. Option is for a value that may or may not exist, not for error handling.
Are the names mutable, or immutable? You might consider using Rc&lt;String&gt; for the names, since you want your data structure to be effectively a dag instead of a tree. The language is pointing out this potential hazard: If you store &amp;str in a field, which points to one of your current Vec&lt;Box&lt;Foo&gt;&gt;, what happens when someone modifies the vector? The reference now points to invalid memory, and then boom, safety can be trivially violated. You can avoid this either by copying the string (yeck), or by using Rc&lt;String&gt; (I *think*) and then borrowing &amp;str from Rc&lt;String&gt; when you need. I haven't actually done this yet, but hey, first time for everything.
This is why I am so excited for Rust. I'll be honest, I don't have the patience for what you just described in C. I don't care for Java or Scala. Go feels cumbersome in a lot of ways. Rust is the first statically typed language that I've used in almost 20 years that has made sense both conceptually and in practice.
Personally I'm not sure I like it. Filling missing items on instantiation (Struct{a: 1, ..} w/ default::Defaut seems like a good idea though.
I don't know if it's possible, but somehow it would be nice to be able to do something like: #[must_use] type Error = Option Because it feels a bit pointless to have these '()' inside of Result.
It just isn't implemented, so manually putting inlining annotations on *everything* has become the norm. It teaches people that LLVM isn't able to reliably make the inlining choices itself, when that's really not the problem at all. It is a simple iterative / deterministic optimization leaving no static call to a function below the inlining threshold (other than a few cases where it *can't* inline, regardless of cost / marked for "always" inlining).
In many other cases they would be completely safe, the borrow checker just doesn't know that. Just because I want, for example, multiple mutable references to the same struct it doesn't mean that I'm going to do problematic things with them. I can implement a completely reasonable and safe algorithm/data structure, and it simply would be convenient to have multiple references for doing so. But the borrow checker isn't smart enough to know if I'm trying to do something problematic. It is afraid someone might misuse the references, even if nobody actually does. I would assume this will make lots of C implementations uncompilable if literally translated to Rust, even though nothing dangerous was actually happening in the C implementation.
I think looking at microbenchmarks in which the entire point is to optimize them as much as humanly possible, even (especially) at the expense of the "simple"/"obvious" algorithm, is a poor way to compare code size for typical problems encountered in the language. Take SIMD, for example. It usually takes more code to write an algorithm that uses SIMD effectively than to write the naïve/obvious algorithm. But you're encouraged to use SIMD in the benchmarks game, because C and C++ do. However, doing this regresses your code size. In effect this kind of article would ding a language like Rust for *supporting SIMD*.
It's not pointless though: they're semantically two different things.
I have a good deal of experience in high-performance GC-based systems. GC can be very efficient, when used correctly. I've worked with a database system that easily saturates multiple 10 Gb/s links, with virtually all of the code running in a safe, GC-based environment. It *can* be done, but when you're aiming for this level of performance, you have to code to the GC. Meaning, you have to develop data structures that exploit the strengths of GC, and avoid the weaknesses. And that also means exploiting the particular GC implementation you're using -- in our case, it was a generational, non-concurrent relocating GC. My main takeaway from all this work in a GC environment is this: GC is a fine, fine tool to have in your toolbox, but it is only a tool. You have to master it to use it effectively, and you have to understand what is happening, and when it is happening, in order to master it. We had solid debugging / tracing facilities in our GC, and this was really crucial to understanding the real dynamic behavior of a complex system. I believe lifetimes have the potential to simplify *some* aspects of designing high-performance "systems" code, for these reasons: 1) Costs are exposed, rather than being hidden behind abstractions like GC, 2) Costs are usually more diffuse (pay-as-you-go), rather than being semi-periodic GC scans, 3) Lifetimes clarify "ownership", and so many techniques that are required in GC environments become unnecessary, simpler, or more efficient, 4) Lifetimes are a more natural fit for interop with foreign code, and in most systems, interop is really important. Dealing with the mismatch between GC and RC memory management is costly and complex. I'm still a GC true believer, because I've seen it done so well. However, GC is not appropriate for all systems. I'm still ramping up on Rust, but so far I'm very impressed with the language, its constraints, goals, status, etc. I'm now working on exploring whether I can apply Rust in the same situations where my team has used GC-based languages. Also, I think about the "learning curve" or "complexity curve" in languages. GC systems allow you to become productive in a language *very* quickly. You can quickly build very useful, correct, and efficient programs. Rust is much more restrictive for small apps. However, as you scale up the complexity, the strong static constraints of Rust become a powerful tool for managing complexity and efficiency. The ownership / access rules give near total clarity on what is going on, at nearly any point, in a program. That's not true in GC systems -- in GC systems you can look at any particular piece of code and know what is potentially reachable, but you have no constraints on read/write hazards (in most languages). Mix in concurrency with read/write hazards and you have the classic unmanageable soup of complexity. This is where I have the greatest hope for Rust -- to make concurrency safety possible and efficient in large-scale software systems.
Well, at the end you're just defining what kind of return values are considered as errors and have to be handled. Why shouldn't there be a return value with an optional error that never returns a valid value? The Result containing a '()' value seems more like a workaround, it just adds noise without containing any meaningful information. 
My comment has nothing to do with Rust or Go specifically, I just want to point out this line: &gt; Given the computers are getting faster and cheaper but software becoming more complex and maintenance is expensive I think there's a fallacy hiding here. It implies that in a decade your hardware will be faster, but the productivity of your programming team will be the same as today. This presumes that the the price:performance ratio of hardware will improve with time, but that programmer productivity does *not* improve with time, and is only affected by choice of language. I think this is false: over time we continue to produce language-independent tools that increase the productivity of programming teams in general (e.g. DVCS, continuous integration), while also refining best practices (e.g. the modern proliferation of test suites) and widely-disseminating ever more knowledge (e.g. checking SO rather than calling up your IBM support contact). One more thing: the current demand for programmers will eventually result in a *tremendous* glut of people taking up programming, which will likely drive down average programming salaries immensely in the coming years (my contacts in universities tell me that this is already happening, with enrollment in CS classes (*and yes I know CS is not programming, but it takes a degree to realize that ;_;*) have more than doubled in the past few years). It may very well be that ten years from now, your typical programming team will be twice the size and twice as productive via improved tools and practices, while your decade-old application remains running on legacy hardware that your execs are too stingy to upgrade. This is also ignoring the fact that the line "hardware is getting faster and cheaper" glosses over the reality of the matter: single-threaded performance is dead, Jim. And honestly, I'm going to go out on a limb and say that your typical enterprise app is going to be too bound by Amdahl's law to see much benefit from concurrency (which both Go and Rust are certainly focused on). In conclusion, I'd probably still use Java for entirely unrelated reasons. :) But wait, C# is open source now... Hmm....
One reason is that with Result you can use [try!](http://lucumr.pocoo.org/2014/10/16/on-error-handling/). There's also the idea that you could have multiple error types (not only return () but some other custom type) but still [interoperate](https://github.com/aturon/rfcs/blob/error-chaining/active/0000-error-chaining.md) by having all of them implement an Error trait. Exceptions in Java or C++ actually interoperate with each other and that's nice.
&gt; Cooler than Ruster, I guess. Or Rusties...
This has very weird/inappropriate cultural and religious connotations, so many of us stay away from it.
&gt; One reason is that with Result you can use try!. Ok, that's a good reason. Would it make sense to have a 'type' for reducing the noise? type Error&lt;T&gt; = Result&lt;(), T&gt;; 
Be advised that there are costs / benefits to both approaches. Making your algorithm generic over the reader type will cause your code to be polyinstantiated for every distinct reader type. That cost can be significant, *especially* if you apply this pattern many times. You can end up with a polynomial explosion of template specializations. Storing a boxed trait, or a trait reference, is probably the right trade-off for most situations. 
What happens if someone creates MinimalTest manually and puts something else in the reference, instead of ref to self.value?
I think of traits as naked vtables. Rather than having a vtable pointer inside an object, as in Java or C++ or C#, you carry around a vtable pointer and an object (struct), together. 
I don't think code size is a good proxy for productivity. I find that it takes way more time to write Rust code than a similar amount of Go code, even though I have been playing with Rust since 0.4 and barely ever touched Go.
That's encouraging. I'm also from the GC languages world and there's an extra difficulty going to Rust's level, but I like the language a lot. I look forward to being the 6-12 months experienced Rust developer. (but right now, my head hurts)
I recall a discussion about the Java email API not so long ago; it was argued that the fact that the `email` object was both the e-mail itself (from/to/body/attachments) AND the interface to send the e-mail was awkward.
Same here, after years of C++, I just welcome Rust. In C or C++, you reason about ownership, lifetime and aliasing on your own; which is both time consuming, mentally draining and error-prone. In Rust, everything is laid out clearly in the type system so you don't have to think about it (and re-discover it each time), and if you make mistakes the compiler spots them for you. So, coming for an existing "memory manually managed" language, Rust is just natural: I just re-invest my previous efforts.
&amp;self would not create an extra temporary object, afaik
Well, lifetimes are useful on their own. Specifically, lifetimes are very good at *bounding*, allowing you a clear picture of which object will survive the current scope (and which will not) without even delving into the functions you call! This means being able to spot potential memory leaks *locally*, and memory leaks contrary to popular belief are not an issue solely faced by C or C++ programmers: - ever heard of the ever growing cache? It affects all languages. - ever heard of the lazy blow-up? Languages (such as Haskell) with lazy evaluation have ways to force evaluation, but it's on the developer to invoke them as appropriate. - ...
In general, if you had a struct with interior pointers to itself, then it couldn't be moved. That's why Rust doesn't let you do this.
The other issue, of course, is library support. Languages like Python which tend to delegate heavy computations to C libraries will require very lines of codes for solving a problem: just call the right library function(s)! Furthermore, equating productivity with number of lines of codes is quite a bold assumption.
I didn't mean to target you specifically by the reply, just got the impression from the original thread that it can be unclear what the post was about, and thought to add (a potential) clarification before getting one from someone more into the field. I don't have an account at discuss.rust-lang.org, so ended up putting the reply here.
Just to point out that, while I do not think you are wrong, performance/price of hardware (in hard but moderately-inaccurate metrics) have historically increased exponentially, but programmer productivity (by even fuzzier metrics, I admit) increased asymptotically to a constant ceiling. 
&gt; (my contacts in universities tell me that this is already happening, with enrollment in CS classes (and yes I know CS is not programming, but it takes a degree to realize that ;_;) have more than doubled in the past few years). The question is, how many of those enrolled will actually graduate with a CS degree, and continue working in the field? When I was in school, there were a lot of students in introductory CS courses, but then they end up switching majors once they find out it is not for them. I think (hope) that there will always be demand for excellent programmers, with salaries to match.
What about maintenance? Does that fall under productivity as well? Sure, maybe you spend more time to get a Rust program to pass the compiler, but you get the advantage that certain entire classes of errors are eliminated, with a likely decent performance boost compared to writing the same code in the same style in a "managed" language (not talking about post development profiling and optimization here).
&gt; rely on the programmer to do the right thing. Exactly. I'm constantly telling my co-workers that C/C++ (without smart pointers like shared_ptr) is like driving without a seat-belt: unrestrictive, liberating, and utterly disastrous in the case of an accident.
Oh yeah, the dreaded "we had no idea for 20 years" vulnerability...
More likely Shenandoah will progress to the point where it will be production ready and shipped in some future Oracle JDK. I don't know how good it is compared to C4 though.
Note that as far as representation goes, Result&lt;(), T&gt; and Option&lt;T&gt; have exactly the same size and performance characteristics. This is because () is a zero-sized type. If T is a reference type, you'll get the enum null-pointer optimisation too.
Do you think that something like [this](http://is.gd/Thk0Z3) could make things faster?
Would wrapping reference in `Rc&lt;RefCell&lt;...&gt;&gt;` work for you?
There are and will always be situations like embedded programming, plugins to other software, and systems like interrupt handlers, device drivers, and garbage collectors themselves, where the only way to do GC "right" is not to do it at all.
Ahhh, I just wrote a 1000 word reply that just flew away right with my browser. Anyways, it was just a lot of rambling about the perceived flaws. I've been manually managing memory for over a decade, I love C++ and Haskell and I am generally good at picking up programming languages. Rust ought to be a match made in heaven for me. Yet I'm struggling with it and it makes me frustrated :(.
&gt; reference: *mut str,// should be private MinimalTest should live in it's own module. and the field reference should be private
On a related note, it'd be nice if I didn't have to initialize unit struct fields, like when using `std::kinds::marker` structs: use std::kinds::marker::NoCopy; struct Struct { nocopy: NoCopy, } let mystruct = Struct { nocopy: NoCopy, // Is this really necessary? }; It's pretty much just noise, and extra technical debt if you have several constructors or widespread use of the initializer. You don't need to be constantly reminded that the struct is marked, do you?
I haven't got it to work either. I'm not a very experienced user yet, but I think it's not possible yet. My best guess is that this is the same thing as I was pointed to [here](http://www.reddit.com/r/rust/comments/2m9ahc/foomystruct_as_foomytrait/) about type coercion. At least if this line: let control = btn.label as Rc&lt;RefCell&lt;Box&lt;Control&gt;&gt;&gt;; ...fails with a "non-scalar cast" error.
I've been working with a similar problem. From what I've read, the full implementation of UFCS will allow you to call the `is_signed` method in a fashion similar to `&lt;int as X&gt;::is_signed()`. I've developed my own solution, as seen here: http://is.gd/f1BkTN
Safe /resource/ management is the main use case for lifetimes. Memory is but one of many resources. It is largely the only one dealt with by garbage collectors. Management of other resources is often punted to "finalizers" but given the constraints of garbage collectors, those are run on a "best effort, if we get around to it" basis.
The advantage of implementing `SlicePrelude` for `[T]` is that it combines all the methods for `&amp;mut [T]` and `&amp;[T]` into a single trait. Instead of a `self` method implemented on `&amp;[T]`, we’ve now got a `&amp;self` method implemented on `[T]`. It’s the same idea, but instead of two traits for the same underlying type (just behind different pointers), we’ve got one trait. Saying that it should be split up is like saying that a trait like `std::io::Seek` that contains both `&amp;self` and `&amp;mut self` methods should be split into two traits, `Seek` and `SeekMut` that both contain `self` methods, and then implement `Seek` for `&amp;T` and `SeekMut` for `&amp;mut T`. `SlicePrelude` is the same, but hasn’t been like this forever because the ability to do this with `[T]` was gained only recently through the implementation of dynamically sized types (DST). Regarding terminology, I prefer to refer to `[T]` as an unsized slice, `&amp;[T]` as a borrowed slice, and `Box&lt;[T]&gt;` as an owned slice. The latter two types are still a pointer and a length, so they’re still slices. This terminology is the same as with normal types: `&amp;int` is a borrowed integer, and `Box&lt;int&gt;` is an owned integer. Treating slices specially and giving them their own terminology would just be confusing and inconsistent. It should be noted that `[T]` is not a normal type, and in fact has no representation and cannot be used as the type of a value unless it is behind a pointer. For more information about this, see the original (well, really the fifth) [DST proposal](http://smallcultfollowing.com/babysteps/blog/2014/01/05/dst-take-5/). As Huon said on IRC, `shift_ref_mut` and so on could possibly be replaced by a way of turning `Items&lt;'a, T&gt;` (the iterator for slices) back into its original slice. So you could use `.next` on the iterator to extract the first item and move the iterator forward, and then turn the iterator back into the slice. I miss `shift_ref_mut` too :’( Regarding the name of `SlicePrelude`, it’s called that because we currently have no way of adding methods to primitive types other than implementing a trait on them. Because traits must be imported for their methods to be available, the `-Prelude` traits are all imported by default in the prelude (the imports that are implicit at the start of (almost) every Rust file).
Thanks for your answer! :) Oh ok! So, “Prelude” in `SlicePrelude` refers to the prelude module whose members are magically imported. As for terminology, I sort of internalized “slice” = pointer+length already which makes me cringe when people use that word for the DST `[T]`. I used to say `array` to that as well as `[T,..N]`. Sure the first is runtime-sized and the second is not. But that is kind of orthogonal to whether we're talking about the elements or about something that just *refers* to the elements via indirection (`&amp;`). If we decide to use the word `slice` generally for zero or more elements (the “pointees”) as opposed to things of type `&amp;[T]` or `&amp;mut[T]` (the “pointers”) we should probably stop spreading the idea slice=pointer+length. I really don't like blurring the line between pointer and pointee. They are seperate things -- at least in case of borrowed pointers.
That section I believe would mean allowing `let _: Box&lt;MyTrait&gt; = box MyStruct;`, which is closer to what we're looking for here I think. The language of the RFC isn't very clear on "generalized box expressions", unfortunately. 
With Rust to C translation, you could also try compiling the translated Rust code on GCC and compare it apples-to-apples with LLVM.
Java *very occasionally* moves allocations to the stack. It's by no means a robust optimization that you can count on, and more importantly there's no way of knowing. 
I just made a logical assumption on the behavior based on what I've seen, e.g. `Box&lt;T: Clone&gt;::clone()` producing a new `Box&lt;T&gt;` instead of a new `T` (which is cloned in the process). It really isn't documented at all. &gt; Was there actually a Set trait before? You can actually go to the [docs for the 0.12 release][1], before they removed the collections traits, from the main website. [Here's the page for `std::collections::Set`.][2] It only has a small subset of the methods all the sets implement, probably not everything you needed. Again, it's pretty common to stick to one implementation of a collection type instead of trying to be polymorphic over a bunch of different ones. It's relatively painless to convert between them, though, so it's not like you're up a creek without a paddle if you have the wrong one. use std::collections::{HashSet, TreeSet}; let set: HashSet&lt;String&gt; = HashSet::new(); let set: TreeSet&lt;String&gt; = set.into_iter().collect(); If you just used a `HashSet&lt;String&gt;`, no one would fault you. I learned pretty quickly that it was easier to use Rust once I stopped trying to make my code conform to OO concepts, because Rust isn't actually OO. It's more like a safer, cleaner C. Once I realized that, everything made a whole lot more sense. [1]: http://doc.rust-lang.org/0.12.0/std/index.html [2]: http://doc.rust-lang.org/0.12.0/std/collections/trait.Set.html 
So here's the problem - Shenandoah has a 10ms target pause time goal. That's *at least* 10x higher than an acceptable bare minimum for a 60Hz video game. GC developers just don't have the needs of high performance interactive applications at heart. They presumably think that 10ms is an impressive target, but it's *way* too conservative for it to be useful in e.g. games. IMO, there needs to be 3 GCs available for inclusions to cover the various scenarios, and apps pick the one they want (per thread and therefor heap in Rust?): 1. A fast semi-space collector that maximizes throughput but stops the world. Parallel compaction etc. Just maximize throughput. 1. An incremental collector that is fast, has no read barriers, and allows the application to occasionally call "incrementalCollect" with a deadline. This will allow things that have a natural cadence on which to collect to just do a little bit of work at a time. E.g. a game could spend 0.5ms at the end of the frame to do some GC work. That's not enough to collect the whole heap, but as long as that time is, on average, enough to collect faster than the allocation rate we're in business. 1. A *fully* concurrent collector. Read barriers and all that jazz. Makes the whole program slower, but has zero unpredictable pause times in the mutator. Useful for cases where there isn't a natural cadence to do (predictable) invocations of the GC. This is where Azul fits in. 
I was just looking at this - $3500 to $7500 per server per year! The only entities using this are going to be huge institutions, not your average chump trying to play Minecraft without massive GC pauses... Until there is a *free* GC of similar performance, I still count GC out of the game for performance.
I think/hope people are moving away from using "owned `T`" for `Box&lt;T&gt;`. A plain `T` is just as owned as a `Box&lt;T&gt;`. I prefer "boxed `T`" for that reason.
I hope all of these will have tutorials by the time 1.0 comes out. I can't keep up XD
Does that mean the only kind of threads supported will be native threads?
And are they still called *tasks*? That would seem like an incredibly bad name when they're really just threads. The kind of thing you deeply regret later when you want to add real tasks (e.g. for parallelism and asynchrony).
&gt; do what C++ and good C libraries do and have an extra allocator generic on everything that needs allocation. Per-container allocator schemes should end up faster, less fragmented and less wasted than one-size-fits-all things. If this wreaks havoc on binary sizes, we could adopt an approach like Haskell's SPECIALIZE pragma (i.e. compile generic code by default and use heuristics for extras, which might be a good idea anyway). This isn't an alternative: it is an additional feature, there still needs to be a default allocator of some sort. See e.g. [#244](https://github.com/rust-lang/rfcs/pull/244) for work that has been done on how the Rust standard library might support this. To be clear, this blog post is just discussing the allocator one gets by default in a Rust program, it is not trying to prescriptive ("you should use this allocator always") or otherwise restrict scope for awesome custom allocators in future. &gt; roll our own malloc: we already have mmap. I imagine we're willing to use a separate algorithm for non-Send allocation, whereas C mallocs need to be multi-threaded with all the associated costs like locking and running all the frees at panic time. This still has the exact same problem of multiple allocators in a process, and still needs to be multi-threaded since `Send` allocations exist. (I do not think this is fundamentally different from using jemalloc or any other non-libc allocator.)
Yes, the only threads supported will be native threads, and they will soon be renamed from "tasks" to "threads" to reflect this.
Woooooooooooooo option 3! Get your secondary keyboards and 4th and 5th monitors, boys! You can sleep when you're dead!
Funny enough, I've [just posted a PR](https://github.com/rust-lang/rust/pull/18967) to remove libgreen entirely. In the [long run](https://github.com/rust-lang/rfcs/pull/230), we expect Rust to provide library support for much lighter-weight tasklets; Servo already has its own support for a custom tasklet abstraction. We're also hoping for some great async IO abstractions to crop up; see [mio](http://github.com/carllerche/mio) for some solid preliminary work. Keep watching this space -- lots of exciting stuff to come!
Yes, the 'owned pointer' terminology hasn't been in favor for a few months, due to this reason. All the official stuff (except if we missed something of course) calls them boxes now.
Comparing GC to allocation is missing the point of Rust a bit, I think. Allocation is slow. Lifetimes provide tools for safely *avoiding* allocations, which is so much faster than allocation that it makes the point moot IMO. The other benefits you mentioned are good as well, particularly thread safety. Efficient guaranteed safe shared mutable references like Rust has flat out cannot exist without a garbage collection free subset of a language.
Most obviously safe cases where you want single-threaded, shared mutable references work with Cell. If it doesn't work with Cell, odds are pretty good that it is not *obviously* safe. I also think you are severely underestimating how much C and C++ out there is buggy.
It's perfectly reasonable for a struct to borrow itself, that's not the problem here (it's how you make a type nonmovable, which opens up lots of good stuff like non-Rc cycles and intrusive data structures).
You can do this easily with some very small tweaks to the code and without using any unsafe code at all. use std::cell::Cell; struct MinimalTest&lt;'w&gt; { value: String, reference: Cell&lt;&amp;'w str&gt;, innocent: int, } impl&lt;'w&gt; MinimalTest&lt;'w&gt; { fn new() -&gt; MinimalTest&lt;'w&gt; { MinimalTest { value: "test".to_string(), reference: Cell::new(""), innocent: 5 } } fn assign_ref(&amp;'w self) { self.reference.set(self.value.as_slice()); } } fn main() { let mut test = MinimalTest::new(); test.assign_ref(); let ref _a = test.value; // no error let _b = test.reference; // also no error let _c = test.innocent; // third nonerror } We really need to highlight the existence of `Cell` more.
These symbols are from the statically linked standard libraries. Pass -C prefer-dynamic to link them dynamically. On my machine a dynamically linked hello.rs takes only 8k (a C version takes 6k). My guess is that the reason that static linking is preferred is that rust libraries are in constant flux these days, and static linking makes the generated executables less likely to break if you upgraded the rust toolchain in your system.
The only possible representation of a `slice` as zero or more elements is through a pointer+length pair. There's really no possibility for confusion- if you have a slice (as opposed to a `Vec&lt;T&gt;` or `[T,..N]`) then you always have a pointer+length.
You can also use `#![no_std]` to not link against libstd at all.
Well, here's wrapping the `main` function for you, from some of my code: #[link_args = "-L.. -lc_main"] extern "C" { fn c_main (argc: c_int, argv: *const *const u8) -&gt; c_int; } // The `main` is wrapped in Rust in order for all the threads to have its runtime. fn main() { let args = std::os::args_as_bytes(); let zargs = Vec::from_fn (args.len(), |idx| args[idx][].to_c_str()); // Null-terminated. let argv = Vec::from_fn (args.len(), |idx| zargs[idx].as_ptr()); // Array of pointers. unsafe {c_main (args.len() as c_int, argv.as_ptr() as *const *const u8);} } Might give you some ideas. Just my two cents.
That would likely require it to be baked into the language, and that seems rather un-Rusty to me. I'd be happy with: format!("{p.First} {p.Last} is {p.Age} years old.") The problem there is that (I believe) hygiene makes this impossible right now. So what I *really* want is an escape hatch from macro hygiene. :D But then, it would likely be extremely difficult, if not impossible, to make this localisable, so it's probably not *that* important.
I mostly agree, but I think some of the other suggestions can be done in conjunction. Such as improving the glibc allocator and having greater granularity for attaching allocators a la C++.
A thing that seems to have been missed is that if Rust code always uses jemalloc, it is possible to compile jemalloc to LLVM bitcode and use LTO to inline the allocator into the program, which might make small allocations much faster. And also there is the issue that if you use the system malloc, code will start relying on the ability to interchange malloc/free and Rust boxes, and it won't be possible to ever change this choice. 
Pretty sure it would require allocation to do this, and it's less flexible than a variant that writes to a buffer like we currently do, so I don't think it makes much sense to bake into the language.
Can you actually safely provide libgreen in the library once it's been removed from the standard distribution? It seems that accessing std's TLS would become unsafe once you don't know which task maps onto which OS thread...
The escape hatch from hygiene is actually trivial here, but *still* enforcing hygiene is preferable IMO, albeit (much) harder.
It is? I was under the impression that any identifier emitted by a macro that *wasn't* passed in would get hygiene'd; hence why you can't refer to `self` in a macro without passing it in via the invocation. And, in general, I like hygiene a lot. But then there are cases like this where working around it just introduces noise. Like with... was it one of Mitsuhiko's macros... where he had to pass `self` in as the first token of *every* invocation explicitly, because he couldn't refer to it otherwise.
In Windows, allocating memory in one DLL and freeing it in another is very much illegitimate. Different compiler versions have different C runtimes and therefore different allocators. Even with the same compiler version, if the EXE or DLLs have the C runtime statically linked, they'll have different copies of the allocator. So, it would probably be best to link `rust_alloc` to jemalloc unconditionally on Windows.
Oh god no. Please let green threads die :)
You have to keep the extern crate definition in the crate root, in this case main.rs.
Ah, great. Thank you so much. **main.rs** #![feature(phase)] #[phase(plugin)] extern crate regex_macros; extern crate regex; fn main() { stuff::stuff(); } mod stuff; **stuff.rs** pub fn stuff() { let re = regex!(r"^\d{4}-\d{2}-\d{2}$"); assert_eq!(re.is_match("2014-01-01"), true); }
Hmm, you mean `escape!(self)` in the RHS of a MBE would get modified by hygiene *after* the expansion? We don't have that macro, but I'm suggesting we could, by setting the syntax context of the `Indent` to `0` or whatever the default value is.
A few things. Given that this is a Win32 api call, you'll need to use `extern "stdcall" fn ...` in order to get the correct calling convention. Secondly, closures and functions are *not* the same; closures are effectively a `(fn_ptr, env_ptr)` pair, so you can't pass them to an API that expects a bare function pointer. Third, if you look at the docs for `EnumWindows`, you'll see that the `LPARAM` argument is application defined. What's more, `LPARAM` is always *at least* pointer-sized. As such, you should [1] be able to put the array in a structure, then pass a reference that that struct (appropriately cast) as the `LPARAM`; then, in the callback, cast it back to a pointer. The reason I say to put the array in the struct is A. you might want more than one value passed into the callback in another context, and B. pointers to references might get confusing. Also, just in case you don't realise it, a `&amp;mut [T]` is *not* pointer-sized; it's actually a `(ptr, length)` pair, so don't try casting that to an `LPARAM` directly. Personally, I'd do all this by wrapping `EnumWindows` in a safe, Rusty function that accepts a normal Rust callback, so it's easier to use in future. Hope that helps :) ----- [1]: *Should* because I haven't actually done it and verified that it works. **Edit**: I tried it, and got the following to *apparently* work. It just prints out the raw HWNDs of all the windows that get enumerated. #![feature(macro_rules)] mod win32 { #![allow(non_camel_case_types)] #![link(name="User32")] extern crate libc; macro_rules! DECLARE_HANDLE { ($name:ident) =&gt; { #[deriving(Show)] #[repr(C)] pub struct $name(pub HANDLE); impl $name { #[allow(dead_code)] pub fn null() -&gt; $name { $name(::std::ptr::null_mut()) } } }; } // basetsd.h pub type LONG_PTR = self::libc::c_int; // minwindef.h pub type WINBOOL = self::libc::c_int; pub type LPARAM = LONG_PTR; pub static FALSE: WINBOOL = 0; pub static TRUE: WINBOOL = 1; // windef.h pub type HANDLE = *mut self::libc::c_void; DECLARE_HANDLE!(HWND) // winuser.h pub type WNDENUMPROC = extern "stdcall" fn(HWND, LPARAM) -&gt; WINBOOL; extern "stdcall" { pub fn EnumWindows(lpEnumFunc: WNDENUMPROC, lParam: LPARAM) -&gt; WINBOOL; } } enum EnumResult { ContinueEnum, StopEnum, } fn enum_windows(mut enum_func: |win32::HWND| -&gt; EnumResult) -&gt; win32::WINBOOL { use win32::{HWND, LPARAM, WINBOOL}; extern "stdcall" fn callback(hwnd: HWND, l_param: LPARAM) -&gt; WINBOOL { let enum_func_ptr = l_param as *mut |HWND| -&gt; EnumResult; match unsafe { (*enum_func_ptr)(hwnd) } { ContinueEnum =&gt; win32::TRUE, StopEnum =&gt; win32::FALSE, } } let enum_func_ptr = &amp;mut enum_func as *mut |HWND| -&gt; EnumResult; unsafe { win32::EnumWindows(callback, enum_func_ptr as LPARAM) } } fn main() { enum_windows(|hwnd| { println!(" - {}", hwnd); ContinueEnum }); } 
I saw that strcat mentioned this on IRC as well. Good point, I'll update the post. One of the trickiest aspects of this whole business is keeping up with the fact that each platform handles it differently.
All I can recall from poking around the macro expansion code a while back (which I didn't entirely understand) was that `SyntaxContext`s undergo sanitisation both before (on the input) and after expansion (on the output). I should note that I was trying to solve the *opposite* problem: how to match an ident in an expression with one generated from a string literal (*i.e.* generated with a 0 context). The only two solutions were to accept an ident token as part of the macro syntax instead of a string literal *or* search the parsed expression AST for the ident and steal its context (eww).
I'm thinking that could always be circumvented by using Cell if the field you want to mutate is Copy. While the language semantics say having multiple aliasing &amp;mut references is undefined, there are other ways to get multiple mutability.
Would it be possible to make this work using trickery with weak symbols? For shared/static libraries you would define weak versions of the jemalloc functions that redirect to equivalent libc functions. For executables you would use jemalloc directly. When linking the library with a C program (that doesn't use jemalloc) there would be no other version of the jemalloc symbols so the weak versions would be used, effectively causing the library to use the libc allocator. When linking to a rust program (or a C program that uses jemalloc) the jemalloc functions would override the weak symbols, which will make all libraries use jemalloc. I've only thought this through for Linux, I'm not sure how well this would work on Windows/Mac. EDIT: It seems that libc exports malloc &amp; friends as weak symbols, which makes things a bit more complicated. In that case we could instead make rust_alloc a weak symbol that uses libc malloc unless it is overrides by a strong rust_alloc defined by the executable which uses jemalloc.
From someone with no experience in this, I just want to say that was a very well-written piece and made things far clearer for me. Thanks!
&gt; write our own safe libc and port the entire userland to Rust. You know you want to. YESSSSSS
also the reason is the ease in deployment of your apps, go has the same trend, that's not a bad decision in my opinion.
It's an artifact of how macro work in regard to resolving external libraries - namely they don't, you have to provide the paths they use manually.
I think LTO here should be left as a choice to the client. When absolute performance is necessary, it seems reasonable, however let us remember the OpenSSL fiasco: people were very disappointed to not be able to switch the allocator behavior dynamically (to wipeout/inject poisoning on `free`).
&gt; This still has the exact same problem of multiple allocators in a process, and still needs to be multi-threaded since Send allocations exist. I think the point was that anything that is not `Send` could be allocated in strictly thread-local pool; taking advantage of the semantic knowledge that it cannot ever leave the current thread to do so. Thus, only `Send` objects, which may be created on a thread and free'd on another, would have to be allocated in an "exchange heap" which would have to be multi-threaded (with all the associated costs). 
Thanks. I actually did what you said, and made it work, but I like your solution much more, I wanted to wrap it too, but wasn't entirely sure how to do it right. 
Don't already modern allocators do something similar with thread-local caches? I'm not sure how much could be gained by the "will never have to be freed in another thread" assumption. Any ideas on what (if any) optimizations the might enable? And if you want to manage free space efficiently, there would still have to be some synchronization between the thread-local heaps and the "exchange heap".
Because in Scala, symbols in the form of &lt;letter&gt;"&lt;string&gt;" are considered [implicit conversions](http://docs.scala-lang.org/overviews/core/string-interpolation.html#advanced-usage). Rust does not have implicits except for borrowing and derefs (AFAICT).
moreover, how big are we talking about? 500k or so? unless you're doing embedded work, does it really matter?
Haskell has nearly no design goals in common with Rust and works fundamentally different in most respects. Rust is *much* closer to C++ than to Haskell.
The old plan was that within each task, you could have many "jobs", as /u/nikomatsakis called them. Given that both of these things are threads under the hood, couldn't this also lead to confusion, like "within each thread, you can have many threads"? What I'm trying to say is that our notion of a "task" had some semantic content (isolation), which a "thread", as an underlying "physical" thing, doesn't necessarily imply.
"Given the computers are getting faster and cheaper but software becoming more complex and maintenance is expensive, I would use Go for an enterprise application." Really?
You may find conradk's work interesting: http://redd.it/2eyd36
I'm still digesting this, but I think there's some really good ideas in here. I had completely blanked comparators from my mind since I had to punt on them [4 months ago](https://github.com/rust-lang/rust/pull/16047) while unboxed closures baked (still baking). If nothing else, this is something we should be keeping in mind with the design. One concern that we've been trying to work out is futureproofing for an eventual uniform Map trait. Borrow excels at this by just assuming "If it's a borrow, it should be equivalent". Since all your bounds are hoisted up to the impl level, I *think* this should work, but I need to think about this some more. I think having a "comparator" for HashMap is a bit odd (especially if multidispatch operators land). HashCompatible is also a frustrating reality. 
It would be great if you could cite some references on performance measurement of jemalloc with 100 GB heap.
I hear they're using scalar replacement (http://www.stefankrause.net/wp/?p=64; http://cgo.org/cgo2014/wp-content/uploads/2013/05/Partial_Escape_Analysis.pdf) - a technique the IBM JDK pioneered some time ago. As far as I know, this only works when coupled with inlining. It's not the same as stack allocation, but thanks for the pointer.
&gt;I think having a "comparator" for HashMap is a bit odd Well, yes, but HashMap still has to compare elements with something in presence of collisions. My inspiration was actually C++14 and it does have comparators for hash maps ([see KeyEqual](http://en.cppreference.com/w/cpp/container/unordered_map))
Define “not working” and “new version”. The most recent nightly from crates.io works.