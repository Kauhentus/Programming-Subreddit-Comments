Nope. Lifetimes are just a way to kind of describe a 'relation' between two types, but they can't prevent you from moving stuff. I believe Pins are the best way to do this.
&gt;When we need to store a mutable reference-counted object we have to put an object inside a (Ref)Cell and then inside an Rc Thank you and @l[hedgehog1024](https://www.reddit.com/user/hedgehog1024/) very much:)
Thanks, but the question was about the amount of pointers in a chain, not the memory used:) I feared that this construction yields two levels of indirection: Rc (points to) -&gt; RefCell (points to) -&gt; Object &amp;#x200B; And other comments say that RefCell doesn't point, it simply contains the whole object, so there's only one level of indirection: Rc -&gt; RefCell + Object
Oh, that makes sense! I was not thinking about the right thing. In my understanding they're related in that `yield` is used to implement `await` - python's original syntax for `await` was even `yield from`. But still, I wasn't really understanding your post until now.
Would it really? I don't think I've ever had a reason to, but it's not at all obvious to me how you would do this in Java (or any other language I know for that matter).
Do you have benchmarks comparing your library to others?
I appreciate having the book and haven't found it to be outdated.
Now how safe is this when hackers know the dictionary?
Added Debug as well and now I can use it in a generic function. Thank you very much.
Also most services have password policies that require at least one number and one symbol. You could insert the same symbol between words instead of spaces.
Yes, listed below (or run cargo bench. There are some \`slog\` benchmarks too). &amp;#x200B; Unfortunately \`cargo bench\` can't silence output from other threads so you'll have to pipe it to grep for ns/iter to find the results. &amp;#x200B; Another caveat about the benchmarks is that they may exceed the queue size, so loads of discarded messages (which aren't realistic, in fast-logger that's an atomic compare and thus very fast). To prevent this, there are \`#\[test\]\`s that "benchmark" within the queue size. &amp;#x200B; test tests::custom_writer_sending_a_complex_format_message_info ... bench: 325 ns/iter (+/- 17) test tests::custom_writer_sending_a_complex_message_info ... bench: 44 ns/iter (+/- 2) test tests::custom_writer_sending_a_complex_message_trace ... bench: 7 ns/iter (+/- 0) test tests::custom_writer_sending_a_message_to_debug_default ... bench: 12 ns/iter (+/- 0) test tests::custom_writer_sending_a_message_to_info_default ... bench: 47 ns/iter (+/- 1) test tests::custom_writer_sending_a_message_to_trace_default ... bench: 9 ns/iter (+/- 1) test tests::message_slog_disabled_by_compiler ... bench: 0 ns/iter (+/- 0) test tests::message_slog_disabled_dynamically_on_async_side ... bench: 531 ns/iter (+/- 24) test tests::message_slog_disabled_dynamically_on_caller_side ... bench: 5 ns/iter (+/- 1) test tests::message_slog_enabled ... bench: 473 ns/iter (+/- 44) test tests::sending_a_complex_message_info ... bench: 43 ns/iter (+/- 1) test tests::sending_a_complex_message_trace ... bench: 7 ns/iter (+/- 1) test tests::sending_a_message_to_debug_default ... bench: 12 ns/iter (+/- 0) test tests::sending_a_message_to_info_default ... bench: 45 ns/iter (+/- 3) test tests::sending_a_message_to_trace_default ... bench: 9 ns/iter (+/- 0) test tests::using_macros_to_send_message ... bench: 63 ns/iter (+/- 2) Here are the results from the tests that do not overflow the internal queue: (`cargo test speed -- --nocapture`) void logger speed debug: 72ns void logger speed trace: 74ns void logger speed info: 209ns void logger speed info with macros: 350ns This doesn't use statistical estimation, so may be off. I only ran it once to post this result.
Looks awesome - congrats on a 0.1 release!
I've got a similar request for Reddit from other users, I think it'd be a great idea to make something like /r/rustaudio.
[Made a PR.](https://github.com/rust-lang/rust/pull/61399)
Look up java reflection, it let's you get the access, name of, or call anything really.. That's how minecraft mods are made
On the same machine or across different machines?
&gt; If that were true, then it seems like that would impact every page in the std docs. It does affect every page, I gave you the servo page as an example to how it affects page load times. If you want statistical data - [the search_index.js file alone is responsible for delaying the page load by 8 out of 10 seconds](https://i.imgur.com/vfN6eLk.png). It *is* (one of) the main reason(s) that the site is slow and it definitely is an area where rustdoc could be improved. Of course I can think of more reasons why the page loads slow, but that would just be me hunting for excuses. &gt; If you adopt a more charitable interpretation, I'm sure you can too. Yes, simplicity of generating the search index. Just look at the contents search_index.js file: var N=null,E="",T="t",U="u",searchIndex={}; var R=["alloc","layout","nonnull","allocerr","dealloc","realloc", ... 10000 more entries here ... ]; searchIndex["core"]={"doc":"The Rust Core Library","i":[[0,"isize","core",R[3182],N,N]}; searchIndex["std"]={"doc":"The Rust Standard Library","i":[[14,"assert_eq","std",R[2646],N,N]}; // [... 1000 more entries here ... ] initSearch(searchIndex); addSearchOptions(searchIndex); ... and then the script is included with a `&lt;script defer src="search_index.js" /&gt;`, which doesn't have the "async" property set, so it will block the rendering of the page. The search index is just one huge hashmap, of course it's going to be slow. There is no reason to write a search index this way, besides the fact that it's the simplest way, just dump it all in one file. Since JS is required anyways for the search to work, I would suggest lazy loading the index in parts or using service workers to not block the UI thread. Any other solution would be better than this simplistic approach. Why is this community so extremely defensive towards any criticism of Rust. I say "rustdoc generates a huge search index which isn't efficient for large projects like servo", instead of saying "alright, let's fix it" I get told "noo actually having to wait ten seconds for the page to be interactive is a good thing, you're just too negative". Of course I can try and find excuses for the shitty output of rustdoc, but that doesn't change the fact that the page load time of rustdoc is a problem. I'm just sick of this attitude of constantly trying to bend over backwards to justify the existence of bugs - instead of just acknowledging and fixing them.
This code works fine: ```#[derive(Deserialize)] struct RawCommonCrawlIndex { id: String, name: String, } #[derive(Debug)] struct CommonCrawlIndex { url: String, name: String, } fn get_warc_indexes() -&gt; Result&lt;Vec&lt;CommonCrawlIndex&gt;, ()&gt; { let indexes = reqwest::get("https://index.commoncrawl.org/collinfo.json") .map_err(|_| ())? .json::&lt;Vec&lt;RawCommonCrawlIndex&gt;&gt;() .map_err(|_| ())?; Ok(indexes .into_iter() .map(|index| CommonCrawlIndex { name: index.name, url: format!( "https://commoncrawl.s3.amazonaws.com/crawl-data/{}/cc-index.paths.gz", index.id ), }) .collect()) }``` I am now trying to remove the intermediate vector. It isn't clear to me how to get serde to give me an iterator of RawCommonCrawlIndex rather than a vector. Am I missing something simple?
I guess I should have stated my credentials a bit more clearly. I am a professional software engineer and much of my work over the last couple of years has been in Java. I know what reflection is and how to use it. I still don't see how it would help with the problem posted above. In java this would look something like ``` int value_name = 10; Integer arg = value_name; System.out.println(???) ``` Usually when reflecting in Java you would have to fill in something like `this.getClass().getDeclaredField("arg")`, which defeats the point entirely, never mind getting the thing that was used to initialize `arg`. I have no idea who reflection would help you convert the value back to the name. C# has the very recently introduced `nameof` pseudo function that will get you `"arg"` back from `arg`, but that isn't going to tell you anything about `value_name` either.
It's kind of because bzip2 is stuck with inappropriate defaults - like using a whopping 4 megabytes at the highest setting. I can't find where (so I may be wrong), but I recall Matt Mahoney said that BWT and LZ compressors exploit the same kind of redundancy, and BWT can do it better in some sense. Certainly modern BWT compressors (like [BCM](https://github.com/encode84/bcm)) are competitive.
Still nothing. openblas-src crate does not still work, but netlib first gave me an "No CMAKE\_Fortran\_COMPILER could be found." even though I installed gfortran and had it on path. When I tried to use "set(CMAKE\_Fortran\_COMPILER "C:/MinGW/bin/gfortran.exe")" in CMakeLists.txt it still did not work, saying that compiler is broken.
First non-zero number is compatibility indicator. It's not about "under 1.0".
I haven't found idle time on my Xeons, but I collected some data on a POWER9 with 2 sockets, 16 cores per socket, 4 threads per core = 128 logical threads. https://gist.github.com/cuviper/700f10182e484790a4b8b84e3f00f586 Using all CPUs, I got about half the throughput with Rayon. Using `taskset` to restrict CPUs to the first NUMA node only, OpenMP and Rayon had very similar performance. So it definitely seems like an issue with NUMA memory-locality. But seeing as you have "dissertation" in your repository name, I'm going to leave further research to you. ;) I encourage you to hack on Rayon yourself, and please let us know if you find ways to improve!
&gt;it's impossible to delete packages That doesn't address the copyright issue. What if I take a proprietary library from work and publish it as open-source and my work decides to take it down? The library will have to be removed one way or the other, or cargo.io will be open for a bunch of lawsuits. Which arguably is even worse.
Looks like a good language to me, especially the short keywords and partially the syntax. Would appreciate the addition of macros like `setq`. Also the reason for &lt;code&gt;`[&lt;/code&gt; is not very clear.
You're definitely right, although you could go back and check if the value associated with the field "arg" is equal to any other fields in the object, since in what I'm making it's structs not Integers. I used integers hypothetically The only reason I mentioned Java was because the creator of this thread mentioned reflection (and I understand he didn't mention specifically Java's reflection, but that's what appeared in my head) Thanks for the input!
If you're sorting, you fixing yourself to an order. And that's fine - that's probably a better solution if you can do it (which is usually the case). Also, it doesn't have to do perfect shuffling. Swapping two random elements is enough to make users not depend on the order.
This would have to be dealt with manually by the crates.io team, following a copyright complaint. I should have been more specific! It's not part of the general interface to remove packages. It is possible to, but it would have to be in exceptional circumstances like a valid copyright complaint, or distributing malware. To quote the [official policy page](https://crates.io/policies): &gt; Removal &gt; &gt; Many questions are specialized instances of a more general form: “Under what circumstances can a package be removed from Crates.io?” &gt; &gt; The short version is that packages are first-come, first-served, and we won’t attempt to get into policing what exactly makes a legitimate package. We will do what the law requires us to do, and address flagrant violations of the Rust Code of Conduct.
&gt; instead of just acknowledging and fixing them. Oh I'm fine with this. That's a good thing. But that ain't what you're "just" doing. You're being an asshole about it. Hence the reason I suggested taking a more charitable interpretation. But no, apparently this is equivalent to being "defensive." What a bunch of bullshit.
&gt; I say "rustdoc generates a huge search index which isn't efficient for large projects like servo" But that's not the discussion at hand, is it? People are talking about Iterator being slow, specifically, which doesn't have to do with the search index, because the search index is the same all over the stdlib docs whether you're opening Iterator or `mem::forget`. It's larger on Servo, of course, but again, that's not what's being discussed. The search index slowing things down is _also_ an issue, but it's not relevant to the issue being discussed _here_. Ignoring the search index is not good, but ignoring _other_ issues and just claiming that the search index is to blame is not good either. Your first comment strongly reads as using this opportunity to rag on the rustdoc devs about a pet issue of yours. It's a _real_ issue (hell, i've complained about it before too), but it wasn't constructive at all. &gt; "noo actually having to wait ten seconds for the page to be interactive is a good thing, you're just too negative" Saying that a proposed solution may not work is not the same as saying a problem is not worth solving. I think that `script defer async` may even work, you'll have to file an issue for it though. I don't think it's been brought up before. There aren't many people working on rustdoc, and just maintaining it is a pretty large task, it's quite unfair to rag on them for not doing things your way. &gt; instead of just acknowledging and fixing them. There even was a giant attempt to rewrite rustdoc into a web server that was spurred in large part due to this issue. It didn't go anywhere because it turned out to be more work than anticipated and people didn't have time, but people _have_ been trying to fix things.
I started on something similar a while back, but kinda ran out of steam for the other project this was to support (trying to talk to a DVB USB stick from userspace), so it's abandoned for now: [https://github.com/dholroyd/usbfs-device](https://github.com/dholroyd/usbfs-device)
Would something like this work for you? https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=ef1f33bd1a81836355ab18adb88db9e0
Across different machines.
Ah yes, that's the word I couldn't recall, thank you!
Those entropy calculations are based on the assumption that the attacker knows everything about the password except for the exact random numbers that the program used. Naïve brute-force attacks have no hope at all against passwords with more than a few dozen characters.
Awesome! I'm certainly going to try this out. The demos (particularly the ragdoll one!) remind me of my experiments with the [ODE](https://www.ode.org/) C++ rigid body dynamics library ages ago, when ragdoll physics in games were a new and exciting thing.
Overall the Rust code coverage situation leaves much to be desired. I agree that Tarpaulin is the best in the Rust ecosystem. It was common to see it miss lines that should have been covered, and I eventually reached a point in my project where Tarpaulin would segfault. At that point it was, obviously, of no use. I reported some bugs, and the authors intend to make improvements I believe. In fairness, I haven't contributed any fixes myself, so I'm not complaining, just reporting my experience.
To my understanding, making a class Copy and using it as such doesn't make your code appreciably less efficient. The compiler may still pass something by reference if it would be more efficient to do so. As I understand it, if you type can be Copy, then you may as well make it so.
This doesn't look bad, though I'd rather avoid TypeId (because it doesn't feel right, I don't exactly have any logical reasons for that ;p). After embarrassingly reminding myself that match is an expression, not a statement, I think I'm simply gonna stick with that.
Not sure if you’ve seen Banana yet?
Piston is mostly stable, that's why there is low activity on the game engine part. Most of the work goes into modular libraries. The design is much more modular than the other engines, to maximize the amount of libraries that can be used independently. E.g. image library was originally developed for Piston, but has now moved to its own image-rs organization.
https://github.com/pistondevelopers/piston-tutorials https://github.com/pistondevelopers/piston-examples
The core difference is that you can be careful to ensure that your thread doesn't panic from within, but you can't control if the scheduler decides to cancel you from outside.
Well, I have good news for you. Rust is 100% free! Just download it from the official website: https://www.rust-lang.org/tools/install
I know the bzip2 author well. I don't think the "nominally unmaintained for years" description is accurate; I think it just hasn't needed an update for years.
A few reasons: * A panic always implies a catastrophic scenario, some assumption broken that means the code just doesn't have any way to do things defined. * A panic comes from within, basically when you call a function you define the code that it runs, therefore you can, being very careful, avoid panics entirely. A cancellation comes from outside, when you call await you cannot, by definition, know what code will run which means you can't know if a cancellation will happen or not. You could have code that works for years, until you bring a library that somehow gets your task cancelled. * Because of the above it's also much harder to know what triggered your task to not be ran. Panics are loud and result in a full collapse of a stack. A cancellation is far more quiet and how things are cancelled are not as obvious, which makes it that much harder to debug. * Panicking functions have a `-&gt;!` type which makes it clear that the program ends there. Because cancellation cancels another task, it clearly returns, there's no reference that await can do this.
aw yesss
https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=c6ae51fbde86dac4dc32dcec072dba47 `request::Response` implements `std::io::Read`, and `serde_json` has a `Deserializer::from_reader` method that accepts anything implementing the `Read` trait.
Awesome!!
Tarpaulin is broken with segfaults atm and has been for months :(
This looks like an interesting idea along the lines of what I have at the moment, but would you mind putting a license on it? I'd prefer not to reinvent the wheel, given that my current efforts have a device enumeration and hotplug API, but also not commit copyright infringement without realising. :P
Thanks for putting that into my radar. They talked about it at Rust Asia. I asked for details to the speaker so I knew they ported lucene to rust... But I didn't know if and when they would opensource it. After a superficial look at it, it looks like they did a very clean job!
Hi. `slog` author here. Judging by the numbers, you should be able to go even faster, especially in the static case. https://github.com/BourgondAries/fast-logger/blob/51d6608ffda2e226351e8e192e292058802f3cf8/lib.rs#L614 looks like you're sending 3 things. You could probably just send one pointer to a `static` struct. Usually logging level is not dynamic. This is how it's done in `slog` to combine a lot of statically known data into one `'static` pointer: https://github.com/slog-rs/slog/blob/1bf30b1c57ab51d2533c74983141bc17c1dc00a5/src/lib.rs#L474 This is also a way to cheapily pass module, function, line number etc. The only cost is increased binary size, but it's typically not a problem. I wonder why do you use atomic for setting log level. https://github.com/BourgondAries/fast-logger/blob/51d6608ffda2e226351e8e192e292058802f3cf8/lib.rs#L590 I don't think it can be faster than a local variable, can it? Quite the opposite - it might prevent compiler from doing some clever optimizations in real-life code that does not change level. Even that deference of `Arc` in ` level: Arc&lt;AtomicU8&gt;,` seems like a waste. Maybe you have some specific use case, but I don't really see why would you want a global log level like this anyway. Usually logging levels are quite static, so noone needs to change it after setting the first time. And even if they do, you could just give them a method that gives them a new instance of `Logger*` with a different `self.level`. Anyway, just my 2c after doing quick check. Good luck!
Syntactic sugar for a (body ...) clause maybe?
The reason there's a "global" log level is to exit the log function as fast as possible. Checking the atomic is quite a bit faster than sending the message over crossbeam. This logger is used from multiple threads, hence the need for it to be atomic, and since the atomic doesn't change too often, cache invalidation shouldn't happen so the check should be fairly fast. If I understand your suggestion correctly, I should be able to send in a `&amp;'static LvlAndContext` together with the actual log message right? Not sure how crossbeam works exactly but I imagine there's a `memcpy` internally, and if that can be reduced to `64 bits` + whatever the message is then that's better than `u8 + alignment + 64 bit context + msg`. If you're interested, do you think the slog benchmarks (in the fast-logger crate) can be improved? If slog can be at least as fast that'd be great.
Hey thank you for taking a look! &gt; Rust has a difficult time optimising tail recursion, so you should avoid it where unnecessary Ah that's unfortunate :( I took a look at your code, but your implementation is in O(n) whereas the optimal one would be O(logn), which is the one I'm trying to implement.
Perhaps you should add it to your r/rust flair...
Would it have helped you if the `rustc` output had been the following? ``` 42 | let mut words = con.unwrap().split_whitespace().peekable(); | ^^^^^^^^^^^^ - temporary value is freed at the | | end of this statement | | | creates a temporary which is freed while still in use | | while words.peek().is_some() { | ----- borrow later used here | help: consider using a `let` binding to create a longer lived value | 42 | let x = con.unwrap(); 43 | let mut words = x.split_whitespace().peekable(); | ``` Filed https://github.com/rust-lang/rust/issues/61405 to do so.
Thanks for your suggestion. I think it's too complicated in *intuition. (One special cases: if the machine is remote connect via ssh. Are there anyway to check the tty and TERN environment?)* But I will give it a try :)
I guess that would be a quoted array for the body. Then that would be back to my original question: why does it not have a macro system so that we don't need to quote the symbols, values manually?
Looks like every other failed Lisp people have been churning out for decades.
Typo: quatum instead of Quantum. This almost kind of feels like something aimed at non programmers I think.
The password entropy is a direct measure of the amount of attempts it would take to brute force the password when the method used to generate the password is known. A random selection of four words from the long list of words has 51.70 bits of entropy because there are 7776 words in the long list, meaning that with four randomly selected words there are 7776 \^ 4 = 3,656,158,440,062,976 possible combinations. This is what yields the password entropy log2(7776\^4) ~= 51.70 bits of entropy. For comparison, randomly selecting an 8 characters long combination of a random selection among any of the full set of printable characters in ASCII -- that is, randomly selecting eight symbols among all of the following: space and !"#$%&amp;\'()\*+,-./0123456789:;&lt;=&gt;?@ABCDEFGHIJKLMNOPQRSTUVWXYZ\[\\\]\^_\`abcdefghijklmnopqrstuvwxyz{|}~ -- of which there are 95, would give you 8\^95 = 6,634,204,312,890,625 possible combinations, or log2(8\^95) ~= 52.56 bits of entropy, so would take about twice as long to brute force as the example given above. And would also be really hard to remember. For example: =o\\Jtm!\^ And if we only add one more word -- so we pick five instead of four words from the long list, we increase the total number of possible combinations by a factor of 7776. Password entropy 64.62 bits. And five words are still really easy to remember. For example: clique widow thus ruse obstinate And this is why my tool includes the functionality for calculating password entropy. You generate passwords with sufficient entropy as per the use-case. In cases where you only care about online attacks, just four words is already massively good enough (51.70 bits of entropy vs recommendation of at least 29 bits of entropy). And for the most extreme case, you would need 8 words from the long list (103.40 bits of entropy) to meet the requirement (96 bits of entropy) which is not quite as easy to remember but still totally reasonable. Typically this would be the password for the full-disk encryption of your computer or the encryption key for your password manager. So you only need one or two long passphrases like that. One of the points of using this tool is that you get passwords that are possible to remember. Another point is that you know exactly how strong they are. We can't trust our intuition about these things. And that is why it is better to use a method that you can safely announce the exact workings of, including revealing the word list that you use, and to calculate the strength of the password, than it is to pick an arbitrary way of picking your passwords but which you don't know how to express mathematically and so you can't calculate the password entropy for. I had the displeasure in the past of having forgetting an overly complicated password for the full-disk encryption of one of my computers, resulting in the data on that computer becoming inaccessible forever. Loss of data because of dumb beliefs about password security on my part in the past is what prompted me to re-evaluate how I pick my passwords. And xkcd sold me on the idea of using passphrases.
And?
r/bicycling This is not the place you're looking for. Look at the sub description please.
This is not the forum you are looking for however I hope you find this article helpful! [rust removal bike chain](https://www.livestrong.com/article/94578-remove-rust-bike-chain/)
Nah, they're just a shitty troll, they keep posting to this subreddit feigning different styles of confusion.
try using `--release`
🤣
you're welcome. I'd be happy to review your code too, or just your reviews of that project.
I don't use words; my script filters `/dev/urandom`: #!/usr/bin/env bash set -e # USAGE: genrand [length [charset]] # e.g. genrand 12 'a-zA-Z0-9_!@#$%' length=${1:-12} charset=${2:-a-zA-Z0-9} tr -dc "$charset" \ &lt; /dev/urandom \ | head -c "$length" \ | sed 's/$/\n/'
Wrong subreddit. This is for the Rust programming language, not the game.
Is it just me, or do I see this exact post a *lot* (for r/playrust, complaining about framerate, with the same comment about `--release`
People like their fun.
Hi all, I'm still trying to get my head around the borrow checker and I am having problems with the "no two mutable references at once" rule when it comes to the `&amp;mut self` parameter in struct methods. For example: #[derive(Debug)] struct Struct { x: u64 } impl Struct { fn method(&amp;mut self) { println!("struct is {:?}", self); } } fn lol2() { let mut x = Struct{x: 42}; let y = &amp;mut x; y.method(); println!("y is {:?}", y); } In the function `test()`, when calling `y.method()`, does there exist *two* mutable references to `x`? The first is `y` in scope of `test()`, the second is `self` in the `Struct::method()` scope? I'm really interested as to the semantics of why this works? Thanks!
I had forgotten about ODE, good times.
With IntelliJ Rust, is it possible to see the type for the term under the cursor? There is an option to see inferred types everywhere but that's not what I want, as it creates too much clutter.
Downloaded! Will this be an ongoing project or just a little side thing? Would love to see more features implemented :)
&gt; The reason there's a "global" log level is to exit the log function as fast as possible. Checking the atomic is quite a bit faster than sending the message over crossbeam. I get that. But why would it have to be global in `Arc` - I don't. :) &gt; This logger is used from multiple threads Why? All that you have in that `Logger` is trivially `Clone`able, and each thread can use their own copy. The queue you're using is `mpmc` (or `mpsc`, doesn't matter). So each thread can have their own copy. Then the `level` can be just `self.level`. In real code if you have multiple logging statements: logger.info("foo"); do_something(); logger.info("bar"); do_something(); logger.info("baz"); compiler can easily prove that `logger.level` does not change anywhere, and can do the `if logger_level &lt; message_level` check once. With `Arc&lt;Atomic&gt;` it has to do it every time because it might have been changed by another thread. Unless you really need that ability to change it mid-way from any other thread, it seems like a waste. &gt; and if that can be reduced to 64 bits + whatever the message It could be all reduced to just 64 bits: one pointer, to statically allocated `struct` that contains all other information &gt; If you're interested, do you think the slog benchmarks (in the fast-logger crate) can be improved? If slog can be at least as fast that'd be great. I don't think there's much to improve there. The corresponding code is here: https://github.com/slog-rs/async/blob/e33823ceb8e7af9e0874262e1e57845bd5f30249/lib.rs#L458 If anyone has any ideas how to optimize it, I would be happy to hear them. :) `slog` is meant to be used for cases where there might be a lot of metadata - enterprise settings, production systems etc. There is fair amount of logging there, but it is not a massive volume. It also optimizes for the case where the logging is actually not happening: you can put *a lot* of logging statements all over your code, but enable most of it, only if you're really debugging production issues. That is why it takes all the stuff as borrowed data, and has to `clone` it before sending to another thread. The idea is that you'd do most of the filtering on sender site, and that should be fast. And then whatever you actually want to log is not going to massive amount of data, and can take a bit of hit. The main benefit of async is here is that serialization and IO which are veeery slow can happen on another thread, which is still a big win. With `async` there's a problem, that it can't even sustain a prelong period of massing logging traffic. In `slog-async` we implement either dropping, or just blocking. https://github.com/slog-rs/async/blob/e33823ceb8e7af9e0874262e1e57845bd5f30249/lib.rs#L574 . One thing that could be done is putting the static messages in an owned value, associated with the logger. That would save one allocation. It won't change timing much, I think. If you can narrow logging to just static messages, than as I explained before, you can be just sending one pointer, which makes your logging benchmarks effectively benchmarking the queue itself. If you'd like to take it to an extreme, I think you could eg. experiment with multiple logging receiver threads, to lower the contention on the queue itself. Maybe you could use `spsc` queue and so on.
[Yep.](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=289d206c78d53f0d568f674825937834) I had the same assumption. I think that it would make more sense to always do overflow checks, or at least to not allow primitive casts between types that might overflow.
Out of curiosity, what do you mean by "failed"?
They're the same reference. If you want to look at it like two different references, one in `test` and one in `method`, you'll see that there's no way to use them both at the same time.
couldn't get `from_utf8(&amp;[ch as u8])` to work myself, as compiler complained about dropping the temporary reference inside the call to from_utf8. However, transmuting char to `&amp;[u8; 4]` then converting from_utf8 on the `&amp;bytes[..1]` slice does it. The case for this compared to the stack allocated array of length 1 is it refers to the same memory, but it's not important. [playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=2bd608402338889131dc1a0e8c9ec467)
That's effectively what I did originally, except using `from_parts` instead of `transmute`. I also took care of endianness, which I think your thing does not. I think the right answer is really let b = [ch as u8]; let s = std::str::from_utf8(&amp;b); which is like what was suggested above. Yes, it uses extra memory, but only on the stack. Can use `from_utf8_unchecked` for bonus speed points. ([playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=7b35e803cb1abb2134b1493bb85e1c8b))
Yup that’s it. I’ve seen the panics while adding/subtracting.
OHHH
"8^95" should read "95^8". The former has 285-bits of entropy, while the latter has 52.56 as you calculated.
Or worse maximum lengths.
&gt; `World&lt;Fixed32&gt;` Is there a reason `f32` wouldn't work here? IIUC floating-point errors are about loss of precision, but in a deterministic way.
You got the wrong kind of Rust here. Check out /r/playrust instead, unless you're planning to switch videos about programming.
 Seems to me it'd be easier to have 2 enums: One with the Tokien identifier and one containing the exact values and use a struct for both? #[derive(Debug, PartialEq)] enum TokenKind { OpeningBrace } #[derive(Debug)] struct Token { kind: TokenKind, // ... } fn peek_next() -&gt; Option&lt;&amp;'static Token&gt; { Some(&amp;Token {kind: TokenKind::OpeningBrace}) } fn peek_is_next(kind: TokenKind) -&gt; bool { match peek_next() { Some(ref token) if token.kind == kind =&gt; true, _ =&gt; false } } fn main() { dbg!(peek_is_next(TokenKind::OpeningBrace)); }
["RPCS3 and Dolphin on macOS using gfx-portability"](https://gfx-rs.github.io/2018/09/03/rpcs3-dolphin.html) \- 03 Sep 2018
I guess I’m still not having a good mental model for what happens when you pass data into a function call. I suppose I’m trying to think of it as though references are an “object” that get moved into the function’s scope, the same way that normal (non-reference) objects are moved into the function when called and deleted at the end of the function. So in this mental model the mutable reference to should get moved into the function and thus no longer be accessible from the caller. Clearly this isn’t what happens as the mutable reference is still usable from the caller, so I guess references are “special” intrinsic in some way and can’t semantically be considered as just a `struct` instance containing a pointer (like a `Box`)?
If `#[warn(elided_lifetimes_in_paths)]` or `#![warn(rust_2018_idioms)]` is set, rustc will warn about `fn funcname`: &gt;error: hidden lifetime parameters in types are deprecated
Ah, that's a good point. The rust 2018 solution still kind of elides things, though, right? It's still `fn funcname(x: Foo&lt;'_&gt;) -&gt; Bar&lt;'_&gt;`, as opposed to fully explicit `fn funcname&lt;'a&gt;(x: Foo&lt;'a&gt;) -&gt; Bar&lt;'a&gt;`.
Only on the same machine, though.
Index would be preferred solution even in C++, because a push on vector could invalidate memory contents.
( '\_&gt;) 👍 For anyone interested in a motivating example, [what is the elided\_lifetimes\_in\_paths lint for](https://users.rust-lang.org/t/what-is-the-elided-lifetimes-in-paths-lint-for/28005/2)?
To test, set `RUSTFLAGS=-Zsymbol-mangling-version=v0` (note that this can only affect code compiled with the flag, but not e.g. `libstd` from `rustup`). You should see a lot more detail from panic backtraces (via `RUST_BACKTRACE=1`) right away, as `rustc-demangle` has been updated to support the new ("`v0`") mangling. However, `binutils`/`gdb` (`libiberty`), `valgrind` and Linux `perf` still require [the C port](https://gist.github.com/eddyb/c41a69378750a433767cf53fe2316768) of the mangler to be upstreamed, and that's what I'll focus on next. AFAIK `lldb`'s Rust support must be kept in our fork and not upstreamed, so we might be able to land it there first, we'll see.
[Rendered RFC](https://github.com/rust-lang/rfcs/blob/master/text/2603-symbol-name-mangling-v2.md)
Similar name: http://libharu.org
Ctrl-Q (Quick Documentation) works for me usually.
I wrote a small testcase to show this off: ```rust fn foo&lt;T&gt;() { panic!() } fn main() { foo::&lt;Vec&lt;(String, &amp;[u8; 123])&gt;&gt;(); } ``` This is the relevant part of the `RUST_BACKTRACE=1` output: ```rust 4: std::panicking::default_hook::{{closure}} at src/libstd/panicking.rs:197 5: std::panicking::default_hook at src/libstd/panicking.rs:211 6: std::panicking::rust_panic_with_hook at src/libstd/panicking.rs:474 7: std::panicking::begin_panic::&lt;&amp;str&gt; 8: foo::foo::&lt;alloc::vec::Vec&lt;(alloc::string::String, &amp;[u8; 123])&gt;&gt; 9: foo::main 10: std::rt::lang_start::&lt;()&gt;::{closure#0} 11: std::rt::lang_start_internal::{{closure}} at src/libstd/rt.rs:49 ``` Frames #4 and #11 are clearly compiled with the legacy mangling, because of the `{{closure}}` part in them (which is a compiler implementation detail leaking through) - probably all the frames with location information are pre-built (since I didn't enable debuginfo). But in frames #7, #8 and even #10 you can see the new mangling at work.
Beats me. Ask someone in /r/playrust which is the subreddit for the game. This one is about the Rust programming language.
Nice way to solve it, thank you!
kk thx
&gt; I get that. But why would it have to be global in Arc - I don't. :) Just to be sure: it's not global, just shared among cloned loggers cloned from each other. I've called it "global" but should probably use "shared" or something. &gt;Unless you really need that ability to change it mid-way from any other thread, it seems like a waste. You're right, micro-benchmarks indicate a 2 ns waste, but that's with the atomic likely in cache so in reality it might be bigger. Slog-async with caller-side level filtering takes 5 ns while the atomic takes 7 ns. I have a CLI that runs in its own thread with a cloned logger, it needs to be able to control the log level for the shared loggers. Alternatively, I could send a message from CLI to main to change the log level in main locally, but this seems not very scaleable. Another alternative is to have the atomic, but cache it locally and manually do a `poll_shared_log_level` every once in a while for every thread having a logger clone they wish to keep in sync &gt;It could be all reduced to just 64 bits: one pointer, to statically allocated struct that contains all other information That'd be great, but would that work for non-static messages? &gt;If you'd like to take it to an extreme, I think you could eg. experiment with multiple logging receiver threads, to lower the contention on the queue itself. Maybe you could use spsc queue and so on. Good point!
Can you please compare this with the old mangling?
Sure, this is what the same part of the output looks like, without the `-Zsymbol-mangling-version=v0` flag: ```rust 4: std::panicking::default_hook::{{closure}} at src/libstd/panicking.rs:197 5: std::panicking::default_hook at src/libstd/panicking.rs:211 6: std::panicking::rust_panic_with_hook at src/libstd/panicking.rs:474 7: std::panicking::begin_panic 8: foo::foo 9: foo::main 10: std::rt::lang_start::{{closure}} 11: std::rt::lang_start_internal::{{closure}} at src/libstd/rt.rs:49 ```
That is actually pretty cool, good work!
&gt;Any cleanup **has to** rely on Drop RAII is the only sensible way to manage resource in any case, so what's the harm? If the resource is memory, the compiler won't allow the code you posted. If it's something else, why would you not write an RAII wrapper for this? Await is not really special in your case, any control flow (a carelessly added \`?\` somewhere) could leak the resource.
Whoops. Yeah made a little typo when typing up my comment. Thanks for catching that :)
I have a few more features in mind that I plan on implementing in the future. I am open to feature requests as well, should you have any.
A big usecase I recently discovered for `slog-scope` is `std::panic::set_hook`. By setting the scoped logger at key points and using it in a panic hook you will get at least _some_ context with your panic logger, e.g. in my case the request id of the currently processing request. I definitely advocate passing loggers around manually where possible, but using a combination of that with scoped logging can be very powerful.
Sort of, in a very hacky way, to support passing in resume arguments from`yield` (because it uses the unstable generator syntax as a part of its implementation). I can’t think of a futures model that would support passing in a value as the result of an `await`, since you are already getting the ouput of the future back from the yield, where does this new value go?
That is correct, it re-borrows. So mutable references are still special to the compiler in that regard. I wish there was a `Copy` like trait for values that can be re-borrowed temporarily, because although this behavior is needed for ergonomic reasons, it can definitely be surprising. For instance, tuples of mutable references aren't re-borrowed implicitly, even though you can re-borrow the individual elements just fine. Although I'm not sure how such a trait would work exactly.
That’s a bit different in that you are taking in a `Stream` to the function, whereas the example is closer to defining a function that returns a `Sink`. Generally taking in streams is going to be easier, but there are some usecases for using sinks instead. If we get both generator resume arguments, and async generators, then those two features combined could give a very succinct way to write something similar to the OP example.
Is there a way to distinguish them? Does \`rustc-demangle\` support both of them? &amp;#x200B; I'm asking, because I'm not sure how this should be handled in e.g. \`cargo-bloat\`.
&gt; Does `rustc-demangle` support both of them? Yes, as you can see from [my example](https://www.reddit.com/r/rust/comments/bvifst/psa_new_rust_symbol_mangling_is_now_in_nightly/eppf23q/?context=2), some of the entries in the backtrace use the new mangling but the rest use the legacy one (which is still the default). &gt; Is there a way to distinguish them? [As per the RFC](https://github.com/rust-lang/rfcs/blob/master/text/2603-symbol-name-mangling-v2.md) the new ones start with `_R`.
You could use `fold` here: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=40c66fa8293d791a0e502ac4a27687cd
You may find the design of the Haskell fast-logger library interesting. It achieves its performance by having one buffer per *capability* (roughly core) to avoid performing IO as frequently. When the buffer is full the thread takes a lock on the handle and writes the buffer in one go. The design could be extended to also fork a thread per capability but at some point if the producer is faster than the writing thread you either need to block or lose data. I’m not convinced that minimising latency at all costs is really worth it, my app want to know its logs have been logged so I know what broke.
Thanks. To there is no need to modify `cargo-bloat`.
Well looks like I messed up then. It IS what I get as an output. What happened is that the "temporary value freed at the end of the statement" overflowed the code block to a new line so I did what I could for the diagnostic to make sense. I'm sorry if I caused you trouble.
Well looks like I messed up then. It IS what I get as an output. What happened is that the "temporary value freed at the end of the statement" overflowed the code block to a new line so I did what I could for the diagnostic to make sense. I'm sorry if I caused you trouble.
Looks like I messed up then. I do get that output from the compiler. What happened is that the "temporary value is freed at the end of the statement" overflowed to the next line and I tried my best to format the output so it makes sense. I'm sorry if I caused you trouble.
Don't take Big-O as a Bible; it's only relevant for huge arrays. Binary search is more efficient, sure, but it's also painful for caches and the branch predictor, because it can't tell in advance the size of the array or where this is in relation to an array. Equally, the next jump goes to wildly unrelated places in memory, so it probably won't hit cache. Linear search is less theoretically optimal, but also a lot friendlier to the caches and branch predictor; the branch is only ever taken zero (nothing found) or one (found once) times, so even basic predictors can guess this. Additionally, the loop always looks at the next element in the array, and CPUs can guess that, too, fetching memory in advance. So, benchmark. I'm *reasonably sure* my simple linear search will be faster for small and medium arrays, but there will be a point where yours is faster by being more efficient. I just think it's not worth the complexity.
Looks like I messed up then. I do get that output from the compiler. What happened is that the "temporary value is freed at the end of the statement" overflowed to the next line and I tried my best to format the output so it makes sense. I'm sorry if I caused you trouble.
I think the problem with buffering is that we don't immediately get the output, which can be revealing in some cases especially terminal logging. You're right that your app probably shouldn't use lowest-latency async logging. I'm writing latency-sensitive code so I need a latency-sensitive logger, not so much the throughput being important, but rather the effect of 500 ns vs 15 ns.
Iirc fast-logger also forks a thread which flushes buffers periodically (iirc it was something like 10 times a second) so you also get timeliness too.
We've been using a hand-rolled, seriously basic lib for our error management (it just adds the .context family of trait methods to errors). So far we are happy. As for the backtrace story, I've added panic mode depending on sneaky env var, so we let Rust give us its backtrace. Obviously useful in development, not so good at working out what happens in production :) As an author of a basic introduction (The Gentle Intro) I feel quite torn about how to present error management. I'm doing a revision and I'm going to leave out any helper crates; just present the machinery as it is (which people need to get anyway).
There is nothing wrong just using a loop and variables for counting the variants. There is no need to force functional programming on every use case.
Excellent, this is a tremendous effort by all involved. What I'm curious about is the potential compiler performance benefit. The old mangling scheme has no notion of compression, right? For something with a significant number of nested types (let's say an impl Future function, for topicality), what sort of compression ratio might we expect? (Also, is it just me or is there less compression defined in the current version of the RFC than in the original?)
Perhaps loop just once: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=cfd2290a1a4d895ce1438787c534bdb1
Often I find myself using OmniDiskSweeper to free some disk space at the end of an assignment, and even more often I use \`sn sort\` to learn about directory sizes. The former is quite slow and uses a lot of memory, while providing a point-and-click user interface that could be more efficient. The latter requires quite some typing, and is using a single core by default. \`dua\` aims to become my tool of choice when querying disk-usage and clearing out unnecessary data. Thanks to [jwalk](https://crates.io/crates/jwalk) all the hard work is handled elsewhere, so v1.0 was ready for release in just a few hours. The real fun starts with v2.0, when I will get the opportunity to build my very first more interesting terminal user interface, using `termion` and `tui`.
For a rust noob, what does this actually mean for me ?
That sounds interesting and a bit icky. I mean, I'm thankful for compiler optimization but just hoping that the compiler will optimize something away that my code explicitly states doesn't seem quite right. Not because I doubt the compiler would create correct results in those cases but because the optimization might go away at some point in the future. To be fair I'm not even sure how much of a difference a copy would even make for my simple struct that's just two f32s. So far the biggest lesson I think I should take from this is that it helps to think of references as their own types not just aliases.
Yeah, it only has line coverage. Branch coverage is WIP as far as I remember.
rename Rust to Rustlang for easy web search results
Very cool indeed. I find this a fascinating project and am curious to know how it works. Well done!
Better cross-platform support (since characters like `.` are not used in the new mangling scheme), and better debug messages especially for generic functions.
Oh don't get me wrong, I completely agree with you, I was only curious if it was possible to write this in O(logN) without resorting to unsafe code and still get the best possible optimisation from the compiler!
I started started a similar crate called [metrix](https://crates.io/crates/metrix) about a year ago that also uses crossbeam channels under the hood. On first sight it seems to be very similar to what you have started. My focus was on enterprise application monitoring so there are a lot of "widgets" in there like for alerting etc .. Are you also planning to cover something like this? There is no need for 2 crates doing the same.
People have already tried doing graphics toolkits in WebGL (just not in Rust), and it didn't really work well. It also entirely sidesteps the traditional browser behaviors (interacting with links? images? accessibility? extensions?), which is usually not a good thing. Also, Vulkan is definitely _not_ a better choice for a GUI - OpenGL is simpler conceptually and compatible with a lot more things.
I'm curious: isn't the disk always the bottleneck? (even with an SSD) Have you tried to compare parallel vs serial?
What's the point in using WebGL over OpenGL? Using a web renderer just to get a WebGL context seems like a lot of wasteful overhead just to draw something. I think OpenGL is probably the right thing to use if you want to make a cross-platform UI toolkit.
First of all, I'm a fan of fast logging in general, so I am glad to see another attempt at the issue :) **Question** All data-to-be-formatted must be `Send + 'static`, right? **Remark** &gt; Actually logging pushes the data over an asynchronous channel with size limits. It is possible to further limit contention by using Single-Producer Single-Consumer queues instead of a generic Multi-Producer Single-Consumer queue. If ordering matters, you can simply capture the number of cycles since start as a global order and have the (single) consumer of all those SPSC queues "merge" together the various streams based on this counter, although I personally find it easier to just not bother and run the log file itself through `sort` before reading it.
I like the sneakiness! Here's my take on "basic" error handling: https://docs.rs/csv/1.0.7/csv/tutorial/index.html#basic-error-handling
I think threads bring an improvement because drives are faster when you submit more IO at once.
On my system, scanning \~68GB of files, \`-t 1\` takes about 3x as much time as default, which is an 8 HT core system.
Good points. One could get super creative and draw HTML/CSS content with a z-layer that is on top of the webgl context canvas element - but getting those to work, so it looks nice is very likely not possible. &amp;#x200B; So solution could a transpiler that takes a DSL \([GtkBuilder XML?[(https://en.wikipedia.org/wiki/Glade_Interface_Designer#GtkBuilder)\) for the GUI + Rust code as input and generates WASM (via rustc), HTML, and CSS. &amp;#x200B; DSL could be something like that GtkBuilder are using, but I don't like XML.
For some reason, the `crates.io` version doesn't seem to support multiple paths: $ dua target src error: Found argument 'src' which wasn't expected, or isn't valid in this context USAGE: dua [FLAGS] [OPTIONS] [DIR] For more information try --help
If you're going for that, why not just write HTML directly with WebComponents or Vue or whatever?
I think you are looking for [`wgpu`](https://github.com/gfx-rs/wgpu/) project. Eventually it should work over native and web targets. (though it will use future [WebGPU](https://en.wikipedia.org/wiki/WebGPU) standard and not WebGL)
I think OP's original idea was to allow the same app to run in WebGL on the web and raw OpenGL on the desktop, but it's much harder than they seem to think it is.
Is there an equivalent command I can replicate the behavior against, such as using df or du?
I noticed something like this as well, but with a different folder name. It's the issue that clap also tries to parse sub-commands, and sometimes gets confused. One way to fix it is to use the long command invocation: \`dua a \*\`, or \`dua aggregate \*\`. I am still seeing if this is annoying enough to attempt to fix it - in the worst case I have to remove sub-commands and provide multiple binaries with one crate.
I believe that's why multi-threading helps here. Please note that \[jwalk\]([https://github.com/jessegrosjean/jwalk/blob/master/benches/benchmarks.md](https://github.com/jessegrosjean/jwalk/blob/master/benches/benchmarks.md)) also has benchmarks, and it's the core of \`dua\`.
\[Here's a solution with newly-stabilised \`std::mem::discriminant\` and a hashmap\]([https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=1317c3a518c6e7456413be0cef308f11](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=1317c3a518c6e7456413be0cef308f11)). This will be probably slower than a couple of handrolled for loops though. &amp;#x200B; Btw, I spend some time questioning my sanity when trying to understand what's wrong \[with this code\]([https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=af8037c7a7a6a882200553ff88073681](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=af8037c7a7a6a882200553ff88073681)) :)
does it have rust macros ?
This is indeed the solution if you actually want to work with references, but I would just make that type `Copy` regardless. I understand your concerns, but 16 bytes is well below the size where Rust developers typically start to wonder whether deriving `Copy` is a good idea. I *really* doubt you'll be able to come up with benchmarks where adding references is faster than copying the values.
Ugh, I installed the wrong `dua`. No wonder the UI looked differently.
When you know how many things there are in the list, you know that all the remaining indices are not things, so why not; `let not_things_count = vec.len() - things_count;`?
Just create a HTML and JS combo transpiler , that will let everyone use HTML as the UI for rust , and that can help us ignore wasm too.
I’m gonna straight shoot down this idea. Web browsers are the best at creating UI’s. It’s been built up for decades. It is the easiest with the most resources to learn. Recreating that in Webgl is pointless. You will end up with something slower and worst. Webgl is for like games and really complicated animations.
I have read that since Haswell the branch predictor got good enough that it made computed gotos obsolete; but i'm using sandy bridge so that doesn't count :P Does LLVM generate dynamic dispatch tables as well? Shouldn't that be `map()`?
&gt;All data-to-be-formatted must be `Send + 'static`, right? I don't think so. The data needs to be `Display + Send`, and correct me if I'm wrong but `'static` I think comes automatically, and is just needed internally to ensure that a thread that outlives the producer does not access invalidated memory. &gt;It is possible to further limit contention by using Single-Producer Single-Consumer queues instead of a generic Multi-Producer Single-Consumer queue. This sounds like a good experiment. I'm split on this since more threads is more scheduling overhead, more context switches, which can slow down the application. Do you know of any benchmarks on this using crossbeam? I'd love to see some stats. Also, is contention an issue when latency is the only important factor? We may log 1-10 times every second but that latency is important here, not general throughput, and I'm not sure contention is going to play a significant role here. &gt;If ordering matters, you can simply capture the number of cycles since start as a global order and have the (single) consumer of all those SPSC queues "merge" together the various streams based on this counter, although I personally find it easier to just not bother and run the log file itself through sort before reading it. That's a good idea if it works, have you got any benchmarks on this solution?
I"m doing an interview exercise and I need to implement a very simple web API which should kick off background processes in response to call A and call B should get status updates of those processes. I was going to do it with Hyper and spawning threads but... are there better options?
Awesome, [just used it](https://github.com/Nemo157/tide/commit/2955542ddb328718fe37b6570112008558b46c99) to replace my own similar implementation. One suggestion would be to change the examples to have `use slog_scope_futures::FutureExt as _;`, otherwise users are very likely to run into conflicting imports with `futures::future::FutureExt`.
Shoutout to \`ncdu\` too. Interactive \`du\`, very useful.
&gt;Does LLVM generate dynamic dispatch tables as well? Haven't checked. Anyway consider the fact that build a language take time, so when you get ready your sandy bridge will be even more old :) &gt;Shouldn't that be map()? Ups, yes. (or: you can do map/filter/... on top of fold as a desugar pass!)
That is correct
Indeed. This is clear in any benchmark that compares e.g. QD1 to QD32; the latter is way, way faster for small reads and writes. Copying small files (e.g. boost, which is hundreds of thousands of files) is also way faster with many threads. I use "robocopy" to do this on Windows now and then.
Then you need to understand that OpenGL only handles graphics. You will still have to write separate code for the event loop, inputs, windowing and all that other stuff.
Ah, nice to see such explanation in library documentation! Yes, that's the basics - more people need to know the Box&lt;Error&gt; trick. The linked blog post also explains some conveniences and the trade-offs made for the basic box-everything approach.
I guess you're right on this particular example but for me it's more about understanding than building something for production right now and while I'd probably cringe at operator overloading for massive objects I still learned a (I hope) valuable lesson: References are first class citizens and there's nothing stopping me from implementing Copy and both versions: `impl ops::Add&lt;&amp;Vec2&gt; for &amp;Vec2` `impl ops::Add&lt;Vec2&gt; for Vec2` And let the caller decide which is best. Not that this is useful for a 2D vector but I think it helps me with understanding Rust a little better - which this whole excercise is about.
I think the closest thing is: [https://docs.rs/criterion/0.2.11/criterion/struct.Bencher.html#method.iter\_batched](https://docs.rs/criterion/0.2.11/criterion/struct.Bencher.html#method.iter_batched)
For sure, that’s a great takeaway!
I don't think O(log N) is actually possible. I can't think of a way to prove a peak does not exist without examining all elements at least once, because by definition you'll need to compare each element to its neighbours.
Thank you! :D
&gt; (Also, is it just me or is there less compression defined in the current version of the RFC than in the original?) It's simpler (byte backreferences rather than AST positions), but it should be just as powerful. &gt; what sort of compression ratio might we expect I just updated [my stats](https://gist.github.com/eddyb/786598131525ef5adc9189a30e31c2fc) (I needed new dumps so I can test the C port of the demangler, with all of the last-minute changes), and there's a breakdown of what that table means at the end of [my end-of-January comment in the RFC thread](https://github.com/rust-lang/rfcs/pull/2603#issuecomment-458410463) ("old" is now "legacy" and "new" is "v0", but everything else should be unchanged). On average, the compressed symbol is 1.5x smaller, but in extreme cases, it can be close to 16x smaller (note that this is compared to the uncompressed new mangling, not to the old one). &gt; What I'm curious about is the potential compiler performance benefit. There isn't any: the old mangling had less information. Your example of `impl Future` isn't relevant because that type wouldn't be found in the output the old mangling. In other words: this new mangling makes symbols larger (2-3x on average AFAICT), and may slow down compilation a bit, but actually encodes generic parameters for once.
Indeed, this version of \`dua\` is installed with \`cargo install dua-cli\`. I have just published v1.1, which allows to just run \`dua\` for disk usage on all entries in the current working directory. This might additionally help people to avoid using '\*' and run into parsing problems.
I keep hearing about const generics on this sub and some of the rust team meetings on youtube. What are they and why do they matter to me as an average rust user?
That is correct! I am currently researching how a macro system would fit into the underlying infrastructure.
Thanks for the hint! This tool is great! The site also links to a bunch of other, similar tools which might inspire me! In any case, it's amazing to see how close ncdu is to what I imagine! Can't wait to implement my own :).
&gt; This sounds like a good experiment. I'm split on this since more threads is more scheduling overhead, more context switches, which can slow down the application. Do you know of any benchmarks on this using crossbeam? I'd love to see some stats. I think there was a miscommunication; the solution I propose does NOT add any thread, it only adds more *queues*. One queue per producer thread, all consumed by a single consumer thread. &gt; Also, is contention an issue when latency is the only important factor? We may log 1-10 times every second but that latency is important here, not general throughput, and I'm not sure contention is going to play a significant role here. It depends whether you target median-latency or tail-latency. For median-latency it probably does not really matter for 1-10 times every second as the chance of contention is low, for tail-latency it matters as that low chance of contention will still happen from time to time. &gt; That's a good idea if it works, have you got any benchmarks on this solution? Reading the [TSC](https://en.wikipedia.org/wiki/Time_Stamp_Counter) with the [RTDSC instruction](https://www.agner.org/optimize/instruction_tables.pdf) is about 20 cycles, assuming that timing does not matter so much that you do not use any serialization instruction. You can use a timestamp instead, if you already have one; it's just that a full timestamp costs slightly more to obtain than the TSC.
That's a good idea, thanks. I actually have a `dus` function in my `.profile`: `du -had1 "$@" | sort -hk1`. It gives almost identical output: $ dus 4.0K ./bors.toml 4.0K ./Cargo.toml 4.0K ./.gitattributes 4.0K ./.gitignore 4.0K ./LICENSE-MIT 4.0K ./README.md 4.0K ./rustfmt.toml 4.0K ./.travis.yml 8.0K ./.cargo 12K ./LICENSE-APACHE 24K ./.vscode 96K ./docs 100K ./Cargo.lock 4.9M ./crates 61M ./.git 84M ./editors 4.0G ./target 4.2G . $ dua 37.00 B .gitattributes 77.00 B rustfmt.toml 86.00 B bors.toml 99.00 B .gitignore 109.00 B Cargo.toml 1.02 KB .cargo 1.02 KB LICENSE-MIT 1.44 KB .travis.yml 2.48 KB README.md 10.05 KB .vscode 10.85 KB LICENSE-APACHE 68.88 KB docs 98.42 KB Cargo.lock 2.37 MB crates 61.14 MB .git 73.95 MB editors 4.89 GB target 5.03 GB total They do report different sizes, but I'm not sure which I prefer.
Thank you very much! This was my first serious Rust project and I had a lot of fun implementing it! LambdaCore works by using [Pest](https://pest.rs/) to parse a given `.lcore` file. It then goes through the entire tree and pushes LambdaCore Value types onto a stack and evaluates arrays. Initially, I created the interpreter to recursively-evaluate forms but I ran into some performance problems. One of the problems was a stack overflow of all things! This was due to Rust allocating a new stack frame per function call. It took about a week but I changed the underlying algorithm to be iterative rather than recursive to eliminate the space and time limitations. Once the stack has been populated, the interpreter goes through one Value at a time and performs logic in a big `match` block. One cool aspect of the language was how easy it was to implement module importing. All I had to do was evaluate the module to import and extend the original symbol table with the symbol table of the imported module! Lastly, every built-in function (or "keyword") is literally just a Rust function underneath so they run at the speed of Rust! User-defined functions use the same system that evaluates normal code.
Hey sorry for taking so long to respond, here's the specific function implementation I had: ```Rust // Matrix is just an Array&lt;f32&gt; type alias. #[no_mangle] pub extern "C" fn copy_matrix(hmatrix: MatrixHandle, to: *mut f32, size: usize) { use std::mem; use std::slice; // handle2mat(hmatrix:MatrixHandle) -&gt; unsafe { Box::from_raw(hmatrix as *mut Matrix) } let m: Box&lt;Matrix&gt; = handle2mat(hmatrix); let slc = unsafe { slice::from_raw_parts_mut(to, size) }; m.host(slc); mem::forget(m); println!("Array copied successfuly"); } ``` I tested from visual C++ passing a ``float* x = (float*)malloc(sizeof(float)*4);`` and ``float x[4] = {0};``
&gt; du -had1 "$@" | sort -hk1 This will show bytes allocated for files (your partition uses 4k allocations). Use du -b to show byte usage instead, like this: `du -bhad1 "$@" | sort -hk1`
&gt; du -bhad1 "$@" | sort -hk1 It shows the same result. I also tried `-b` and `--apparent-size`. The difference doesn't bother me, but I found it interesting that while `du` shows larger sizes for directories, the total comes out less. I'm on `ext4`.
The scary part is that the standard library itself is dependent on several crates from crates.io. Rust compiler, cargo, rustup and crates.io itself have many third-party dependencies. [This is a list of crates](https://github.com/rust-lang/rust/blob/cd3f21bc7dc3d2d1a44367618940701ab308f7d2/Cargo.lock#L4090) used in Rust compiler, so the question is - when dependencies are updated is anyone thoroughly reviewing these updates to not contain malware?
That's odd. I get different results on ext4, Debian Linux. Maybe you are looking at sizes for directory entries? Look for a small file &gt; du -had1 "$@" | sort -hk1 &gt; 4.0K ./.xinitrc &gt; du -bhad1 "$@" | sort -hk1 &gt; 198 ./.xinitrc
What about context-sensitive aspects of font rendering such as hinting? Are the used and, if so, would they move to the GPU? Would font-kit convert them from the font file into to a shader or just a set of parameters?
not at my computer right now, but funnily enough I built roughly the same thing a few months ago, and wondering how it compares perf wise https://crates.io/crates/dirstat-rs
Couldn't you do that with only async io?
&gt; In any case, it's amazing to see how close ncdu is to what I imagine! Can't wait to implement my own :) Why not contribute to ncdu then? Instead of rewriting it.
Depends on the platform, but Linux isn't big on async IO for disk accesses.
Your [crates.io](https://crates.io) site says you're x4 faster than dua (?)
can't be the same one as this one was just released.. edit: https://crates.io/crates/dua
I found actix to be quite good for this sort of thing, though the API is somewhat complex. The examples are helpful.
You probably mean this one I guess: [https://github.com/Pistahh/dua](https://github.com/Pistahh/dua)
Const generics let values be used in the type system: fn foo&lt;const X: usize&gt;() -&gt; usize { X } The most common uses I think would be to create generic functions which work with any sized array, or to create vector/matrix types of flexible size without needing to generate many different types. E.g. this struct Matrix3x2 { ... } struct Matrix3x3 { ... } ... Could be struct Matrix&lt;const W: usize, const H: usize&gt;{ ... } let x = Matrix::&lt;3,2&gt;::new();
I am listening to older New Rustacean shows and one mentioned that the Rust team is working on adding classes / classical inheritance. Is this still true? Side note: I hope not.
Hey aesamattki, I hope you have a wonderful day.
`time ds` real 0m0.415s So it's well within a margin of error imo for dua. This is *not* a good benchmark, I'm running what I believe to be equivalent commands and just relyin gon 'time', so I wouldn't take 50-100ms too seriously.
thanks! and yeah, I noticed just switching terminals (iterm to alacritty) has a bigger effect.
You code is already excellent. You can check out the optimization barskern mentioned, but it won't matter unless you are operating on a very large list.
Check out some of the exploration [makepad](https://github.com/makepad/makepad) has done. &gt; Making a live-code editor with all sorts of visual manipulation components just doesn't work in HTML. We tried. It also doesn't work in JS+WebGL. We tried. But now with Rust we are getting right performance figures (up to 200x HTML). And even with Wasm, its plenty fast. This new UI stack has a new way of building UI called 'dual immediate mode' and uses multi-platform shaders for styling that are compiled from Rust source via a proc macro. Dual immediate mode has the code simplicity of an immediate mode API with the scalability/componentisation of a retained mode API. The animations in their [demo](https://makepad.github.io/makepad/) are neat, e.g. when highlighting text and when pressing "alternate" in the keyboard tab.
I spent a moment trying it, and when I wanted to check memory consumption when indexing the whole disk, `/` it just ‘stopped’, as in being gone without a word. It’s probably not intentional. With Rust even a mere mortal can write great software, and it’s very reasonable to assume it will ‘just work’, even with multi-threading. Besides that, C or Rust, it’s just easier to work on a new codebase and implement the features you use yourself. Something I would like to clarify is that I am not going out with the goal to rewrite or replace ncdu, but it’s definitely something to learn from when trying to be (even) better.
LLVM's sanitizer-coverage works with Rust. You can use any tool that uses sancov as a backend.
Curious how io_uring would impact that.
Actually the easy way to do it should be quite simple. Macros would be stored in the same scope, and you can just call `lcore_interpret` again on the returned value instead of returning it directly. You'll just need to do some type checks when you write the macros.
It can always be done for the last variant.
That's true, but then I would rather use one of the other proposed methods, like `fold`.
Ah, that's probably a good idea. Are there any best practices/guidelines written up somewhere for extension traits? The convention seems to be to name them `ThingYoureExtendingExt`, which obviously comes with the potential for name conflicts if multiple crates are extending the same type. Forcing all downstream users to import them with `as _` seems a bit cumbersome and less-than-ergonomic to me. One thought I had was to always put extension traits in a `prelude` or `ext` module, and then expect users to always use glob imports for them. Though an entire prelude seems a bit overkill if your crate (like mine) only has one extension trait.
properly formatted stacktrace 4: std::panicking::default_hook::{{closure}} at src/libstd/panicking.rs:197 5: std::panicking::default_hook at src/libstd/panicking.rs:211 6: std::panicking::rust_panic_with_hook at src/libstd/panicking.rs:474 7: std::panicking::begin_panic::&lt;&amp;str&gt; 8: foo::foo::&lt;alloc::vec::Vec&lt;(alloc::string::String, &amp;[u8; 123])&gt;&gt; 9: foo::main 10: std::rt::lang_start::&lt;()&gt;::{closure#0} 11: std::rt::lang_start_internal::{{closure}} at src/libstd/rt.rs:49
Great share!
There was a use case (I think DOM related) where inheritance would have simplified some code, but the lang team resisted the proposed change IIRC.
Great! Thank you!
Heh, that's where you're wrong, lifetimes *can* prevent you from moving stuff [playground proof](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=22f101c79d27bb1c0e844d552133e233) Even more reason not to use lifetimes to do self-referential types.
On `debug`, I think you can remove `target/debug/incremental` and `target/debug/app`.
`cargo clean -p &lt;your_crate_name&gt;`
A DSL like the GtkBuilder XML file format are special made for solving the problem of creating an UI. Using only HTML + CSS written by hand is including tons of noise.
You’ll get much better answers if you give more context.
Have you seen how Web Components or Vue.js work? They basically allow you to define your own custom "component" tags instead of using the base HTML elements.
I have a workspace with multiple subcrates; can I clean them all with one command?
Hmm, why would avoiding characters like `.` gain you better cross-platform support? Are there linkers/debuggers which don't support non-alphanumeric characters in symbol names?
I'm curious about this line &gt; As opposed to C++ and other languages that support function overloading, we don't need to include function parameter types in the symbol name. Rust does not allow two functions of the same name but different arguments. Is function overloading never going to arrive in rust? And if it ever were to , would you just have another identifier letter plus the function arguments?
`cargo clean -p crate1 crate2 crate3`?
Are your fields constant? That pattern is then probably as good as any. Do you have a C pattern you'd like to emulate?
Come to think of it, this may not hold.
First of all: I like work around fast logging. It's important - there are a lot of applications out there whose performance gets severely impacted by logging speed. &amp;#x200B; Some feedback around this: As far as I understand the general idea is to just send references/instructions to the logging thread - and to perform all serialization as well as actual writing there. This has the benefit of requiring the least work on the actual application thread, since it moves lots of actions to the logging thread. &amp;#x200B; I see the benefits of this, but I'm not sure whether this makes up for a good general-purpose logging solution: \- People usually don't want to be concerned with whether their data can easily be moved to another thread in order to get serialized there. It's just too hard to understand why the scope of a logging call doesn't immediately end after the call. In the worst case they will work around this by already serializing data on the application thread, \`Arc\`ing it, etc. Then the performance will degrade to the worst of both worlds. \- Moving all serialization from the application thread to the logging thread actually leads to less parallelism in the system, since the logging thread now has to perform more work. If the application thread blocks on the logging thread (e.g. in order to submit a new log item) then under high load the performance can get worse. This obviously doesn't apply if the queues are lossy - but lossy logging is also not what a lot of people expect. Therefore I think the approach is mostly useful for very specialized applications. &amp;#x200B; For general-purpose logging I think I would look into how one can keep serializing messages on the application thread, but make that as fast as possible. E.g. by using buffer pools for logged items instead of heap allocating everything.
I dont think `metrics` will cover that stuff as a core concern, no. I can sort of see where you're coming from, with terminology with "cockpit" and all that, but it really feels like that should be a concern of the visualization layer. Scopes already let you inherently group metrics -- and eventual tag support will supercharge that -- so defining the groups in code using terms reserved for the actual visual component feels... weird. I could see some sort of exporter/recorder where you maybe define some of that cockpit stuff, define alerts around different, specific metrics... and expose an API of sorts, kind of like Dropwizard's metrics/healthcheck API.
Thanks! That seems to only work when Vim emulation is off though :-/
FYI there is a bunch of k8s related stuff under "Weave" umbrella — https://www.weave.works/
Maybe you can do with an enum and a method that serializes it to a buffer?
Also the many duplicated crates in there seems less than great...
It took me a while to find that sn referred to tin summer. Any idea why sn is used? I'm having a hard time seeing the connection.
Very nice - this might come in handy. But I'm afraid I immediatly found a bug. I repeatedly managed to crash the ragdoll3d example by just dragging the ragdolls into each other for a while (maybe a minute at most). `'Matrix index out of bounds.', [...]nalgebra-0.18.0/src/base/ops.rs:69:9` I wanted to provide a backtrace but the non-release build is just too slow.
\&gt;People usually don't want to be concerned with whether their data can easily be moved to another thread in order to get serialized there. It's just too hard to understand why the scope of a logging call doesn't immediately end after the call. The problem with the interface at the moment is that it requires specialized types that are moved into the logger call. This is not ergonomic for logging nested types by `info![lgr, "ctx", "Hello"; "item" =&gt; a.b.c.item];` as it would attempt to move `a` itself. Unfortunately we need to dynamically allocate for the generic case in this logger because of `Sized` constraints (the lambdas capture context which has arbitrary size). If it weren't for that the logger could completely avoid dynamic allocation. &amp;#x200B; \&gt;Therefore I think the approach is mostly useful for very specialized applications. &amp;#x200B; I completely agree. That's why I'm not against using multiple loggers in an application, as long as they provide adapter interfaces so we can easily plug loggers into different libraries and so on.
Man I love cargo
Licenses added to both the `usbfs-device` and `usbfs-sys` repos. Feel free to use the code as you wish; including forking / incorporating into another codebase, or just taking some inspiration :)
This is done by callbacks that are initialized by Rust code. In the java world, the `Native` class that you mentioned should extend the `NativeCallbackToRustChannelSupport`. The `callNativeCode()` should call the `doCallback` method that is inherited and the Rust world will be called via a Rust Channel. You may see an example where a JavaFX button caĺls Rust whenever it gets pressed here:https://github.com/astonbitecode/j4rs-showcase/blob/master/java/src/main/java/io/github/astonbitecode/ButtonEventHandler.java
FWIW: sites.google.com is very inaccessible when signed into my G Suite account ("We are sorry, but you do not have access to this service. Please log in to your Admin Console to enable this service.")
Responding partly to myself: the IO layer can merge requests (rrqm/s and wrqm/s in iostat), there's also Native Command Queuing at the physical (hdd) layer. I'll do some tests.
This comes up all the time in this subreddit, so I'm sorry for bringing it up again, but if you use triple backticks to format code blocks, it doesn't render correctly in the old reddit UI. Instead you should indent each line with four spaces. Yes it's dumb.
&gt; in particular to have cargo print all the warnings on my files again You may want to watch the in progres `-Z cache-messages` feature [https://github.com/rust-lang/cargo/issues/6986](https://github.com/rust-lang/cargo/issues/6986)
I ran some benchmarks on these two solutions ([here's the code](https://gist.github.com/jDomantas/3b1fe1b7a0b47876ac7b3eeeca436b77)) - and on arrays of 20 elements binary search already performs better if the peak is at the middle of the array. The fact that the optimizer managed to delete the function is not anything interesting - it's a small function will be pretty much always inlined.
Peak exists in any non-empty array. Here's a proof: pick the smallest index `i` such that `array[i] &gt;= array[i + 1]`. * If such index does not exist, then the array is sorted in strictly increasing order, and the last element is a peak. * If `i = 0`, then first element is a peak. * If `i &gt; 0`, then `array[i - 1] &lt; array[i]` (because otherwise index `i' = i - 1` would have been picked instead). This means that element `i` is a peak.
It seems to be a reference to ag, the silver searcher.
A bit OT, but why does LLDB for Rust remain relegated to a fork?
&gt; Another alternative is to have the atomic, but cache it locally and manually do a poll_shared_log_level every once in a while for every thread having a logger clone they wish to keep in sync That's what I'm thinking too. Your threads are probably running in some form of a loop, and could probably just periodically check current shared log level. &gt; That'd be great, but would that work for non-static messages? For non-static things, you'll need to allocate at least once, so you can just put everything in that allocation. So it will still be 64bits. You might even use two different channels so that you can send `&amp;'static _` through one and `Box` through the other. Or you can use a bit of unsafety, and share the channel, and encode the `&amp;'static _` vs `Box&lt;_&gt;` in one of the unused bytes of a pointer (typically at least two lower ones, due to alignment, and sometimes couple of highest ones - eg. ARM64).
Maybe there's a clippy lint that alerts when using &amp;T where T isn't an enum with mem::variant. That would prevent the problem you had with the 2nd one.
Okay, I misunderstood the original problem of being greater than **or equal** to its neighbours.
Can't find it right now but e.g. [Apple themselves are doing it for Swift](https://github.com/apple/swift-lldb). I could be wrong, but I have a vague recollection of learning of a new(?) LLDB policy that they will only support C and C++ and everything else must be in a fork.
You have two main choices: use a trait (specs uses this method) or an enum. A trait allow other people to implement their own storage while enum don't.
Thank you! I might take a somewhat simpler approach than this, due to cross-platform considerations, but this is going to be an excellent cross-reference for my own code.
Still way over my head. Can you elaborate?
I don't know about "never", but I'd say that generics and traits (including but not limited to `Into`/`From`) are intended to cover the use cases of function overloading, with the additional advantage of consolidating code paths for functions that are "overloaded" in such a way. I don't know of any examples of things that people have wanted to do in Rust that have been stymied by lack of C++-style function overloading (and Rust actually is a bit more expressive here, since bounded generics allow you to "overload" a return type (e.g. `Iterator::collect`) even when the input types are identical).
It's their names in the periodic table of elements. Sn for tin, Ag for silver.
I know, I just was hoping nobody was still using the old UI...
Mine was fine too, just not compatible with the old reddit UI (see sibling comment), I've changed it now.
Overloading already exists in Rust, sort of (or rather, only on nightly) - that is, you can have a type that implements `Fn(A) -&gt; X` and `Fn(A, B) -&gt; Y` at the same time. That works just fine with the new mangling because the trait's parameters are encoded into the symbol.
Fwiw, we are working on both WebGPU and WebGL targets. The latter will likely come first, since it's already supported by thd browsers.
I'm confused then because isn't that in contrast to what the part I quoted from the RFC?
Sure it doesn't stop any development, but in a few other languages, it is arguably nice to be able to do `foo()` or `foo(x)` rather than needing to do a separate `foo_with(x)` or `foo_with_default()`. Of course it's debatable which reads better, and you can spoof some of it with enums/optionals as input parameters.
Specifically "function overloading" in the RFC refers to C++'s very ad-hoc multiple definitions of the same name, that are only distinguished by their signature. In Rust, you can achieve similar static dispatch with `trait`s and `impl`s, and the nightly feature I mentioned lets you use `f(a, b)` instead of `f.my_call(a, b)` - although using your own trait means you're stuck with fixed arities (for now). Either way, methods in an `impl Trait&lt;X&gt; for T`s have properly encoded symbols, where their identity depends on `T` and `X`, instead of their signatures, like in C++.
a quick glance at your code makes me think you're using a lot of `HashMap`s when what you actually want to write are pattern-matching functions. for example, in `src/pieces.rs`, i would replace fn get_piece_moves() -&gt; HashMap&lt;Piece, Vec&lt;i32&gt;&gt; { ... } with impl Piece { pub fn moves(self) -&gt; &amp;'static [i32] { match self { Piece::Pawn =&gt; &amp;[ ... ], Piece::Knight =&gt; &amp;[ ... ], ... } } } also, in that same file, you have written: pub struct Direction {} impl Direction { pub const NORTH: i32 = -(BOARD_SIDE as i32); pub const EAST: i32 = 1; pub const SOUTH: i32 = BOARD_SIDE as i32; pub const WEST: i32 = -1; } i don't know what you're trying to do there, but that's a bad way of doing it. i might write: enum Direction { North, East, South, West, } impl From&lt;Direction&gt; for i32 { fn from(dir: Direction) -&gt; i32 { match dir { Direction::North =&gt; -(BOARD_SIZE as i32), Direction::East =&gt; 1, Direction::South =&gt; BOARD_SIZE as i32, Direction::West =&gt; -1, } } } i only looked at that one file. i assume that, if you look around the rest of your code, you'll find plenty of other places where you could do things in a more obvious way, thus letting the compiler better reason about your code.
Fair enough. I was only thinking of the c++ case of multiple defines with signature variants, but this is definitely interesting.
Takes about 6 seconds to load for me
very nice tips here
Derive a to_str for the Enum and then collect all into a HashMap, with the keys as Enum variants and values as count of variants
I've made a MWE on my machine and I'm getting a segfault. I'll do some more investigating when I get a chance.
Why can't you do structA = structB if manual_mode { structA = sourceB.structB; }
Neat! I made a [really terrible chess engine](https://github.com/SicariusNoctis/rs-chess) for my first Rust project too. It can only move non-sliding pieces and appears to be a bit buggy. XD
Depending on the specific types of the structs and their contents, I think he might need to clone \`sourceB.structB\` there.
Maybe, but there isn't much info to go off of so I gave the simple solution.
Thank you so much for the feedback, great tips, even just using slices instead of Vecs makes so much sense! Is calling Piece::Pawn.moves() going to be as fast as accessing PIECE_MOVES[Piece::Pawn]? (I don't really care as it's much cleaner anyway, just curious) &gt; I don't know what you're trying to do there I just want to be able to use North, South, East and West as constant values around the codebase, and make explicit that they are directions, why is that a bad way of doing it? (e.g. using `end_position + Direction::SOUTH` to refer to the square below `end_position`, and using `Direction::NORTH + Direction::NORTH + Direction::EAST` to indicate how a knight moves) Thanks again for the tips, will try to implement them asap
I really hope sunfish_rs can be readable enough to help others write their own engine I see you're using bitboards, though, so move generation and board representation are going to be completely different :/ But when you manage to implement them it's going to be much stronger than my engine!
Ah yes, varying the number of function arguments is a good use case, although I think Rust people tend to look toward default argument values as the potential answer to that use case (which, returning the context of this thread, wouldn't have any impact on symbol mangling).
That makes sense - I hadn't thought of returning a sink since I've got no idea how to do that in current code. It'll be interesting to see where this goes.
You may also want to have a look at `sccache`. It can speed up compilation quite a bit when you are doing a clean build.
I can't find more recent numbers, but this [8 month old survey](https://www.reddit.com/r/dataisbeautiful/comments/9e4gvn/reddits_opinion_on_the_redesign_who_loves_it_and) indicated at least ~75% avoid the new UI (at the time, of the people who responded, etc.).
Generally if the language supports pattern matching natively (e.g. Rust, OCaml, Haskell), then pattern matching is almost always faster due to the amount of optimisation the compiler can do compared to hashmap/hashtable.
With no other context, a macro could make this slightly nicer: macro_rules! copy_commands { ($($name:ident),* $(,)?) =&gt; ( $( structA.$name = structB.$name; )* if manual_mode { $( structA.$name = structB.structB.$name; )* } ); } copy_commands!(command1, command2, command3); This would expand to your exact code above. You could of course generalize it too, so you could reuse it in multiple places. If you only need to do this once, though, you can define the macro_rules!() locally in the function and it will have access to local variables. This is simply cleaning up the literal code, though. If you give more context into what you're doing, maybe we could help clean up the organization so this isn't necessary?
well because they're not the same type
this is awesome. Thanks!
Can you please give more context?
Thanks! &gt; pattern matching is almost always faster due to ... In hindsight it makes complete sense, I really don't know why I didn't think about it (I guess I'm way too used to python) &gt; variant types give you proper membership and exhaustiveness check ... I guess in general you're right, instead of giving a numeric value to the directions I should define the operations with them. But in this case they kind of are just numeric values I want to give a name to, as I use a position as an index in an array (so integers are a natural choice) and moving from a position maps exactly to adding the integer associated with the direction Maybe I should just use `pub const SOUTH = BOARD_SIDE` as it's already clear from the name that it represents a direction I think I need to think about it
Personally I think that serialization code is in general tightly coupled to the actual underlying data, hence it makes a lot of sense that it's implemented directly next to the data. A quite normal pattern I've seen done by many crates is to make a feature for serde and then only derive the implementation if the feature is enabled. This means that all crates which need serde can enable it, but all crates which don't can just leave the feature off.
calling `Piece::Pawn.moves()` will be much, much faster than indexing into a `HashMap`. in many contexts, i would expect the compiler to optimize the function call, compare, and match away entirely, replacing the expression `Piece::Pawn.moves()` with a static reference to the vector.
Cool! I wonder if a program variant for mathematicians (or just true nerds haha), where it uses prime numbers as the dictionary, would be any more secure?
why would you use names to describe this, when you could instead use types? variable, constant, and function names are completely opaque to the compiler. types communicate the same intent to the reader, but can also be understood, checked, and optimized by the compiler. rustc is a powerful tool, but if you don't tell it what you're trying to do, it won't be able to help you. use language constructs to encode your intent, not opaque souce-level-only names.
I got it to work fine. It wasn't working at first because I was creating an `Array&lt;f64&gt;` before casting it to a `*mut c_void` because the default float type in Rust is `f64` and there was no inference to `f32` at that point. Check if you are doing something similar, the easiest way to check might be explicitly marking the type of anything containing a `f32`/`f64`. Otherwise, I'd have to see the rest of your code that touches the array to know where the issue is.
It is coupled in a way, that's true. At the same time though, the serialization may be contexually dependent. Serializing for HTTP may be different from serializing for the database for example. Currently, you have to define either separete types or traits for that. Both are not ideal IMO. Regarding the feature flag, yes that works kind of well for libraries but also only if the data structure has a unambigous serialization, i.e. there is no way the user would want a different one in any case. In general, my point was more about how you deal with it in an application and not in a library.
Thanks, it really is intuitive, now I don't even know why I asked it in the first place
Having serde's attributes (especially ones that adjust names or flattens) also invides adding `structopt`'s or some database-related attributes, because of why serde is allowed into the core and others not? Maybe there needs to be a Rust-adjusted Hexagonal Architecture, where the core knows that there is _some_ logging, _some_ database and _some_ user interfaces, but does not choose specific technology? Like depending on `log`, `serde_derive` and some UI abstraction crate implementable by `clap`; but not on `env_logger`, `serde_json` or `clap`/`structopt`.
I'd like to see what the effect of a preloaded substitution dictionary (maybe using negative backrefs) of common tokens would be.
&gt; I guess in general you're right, instead of giving a numeric value to the directions I should define the operations with them. But in this case they kind of are just numeric values I want to give a name to, as I use a position as an index in an array (so integers are a natural choice) and moving from a position maps exactly to adding the integer associated with the direction If you don't intend to do any bound checking and so on, then sure I guess. Relying on integer as your base representation of direction yields less flexibility for later code development - what if you want to use direction for other purposes than just calculating coordinates now? This is more of a bad design issue IMO as it restricts expansion of code base. &gt; Maybe I should just use pub const SOUTH = BOARD_SIDE as it's already clear from the name that it represents a direction Clear to you doesn't mean much to code correctness - are you going to go through the entire code base to check each individual use case whenever you compile? Probably not, but compiler does. Using names mean you put more burden of static checking onto yourself which is generally bad for code quality and security - automated checking by compiler or static analyzers always perform consistently while manual checking doesn't. Moreover, trusting yourself too much is always a bad idea - humans are not great with handling mechanical and repetitive tasks, especially if you need to do that multiple times. As said by the other commenter, if there is a language construct (in this case, variant type) that offers the mechanism you need, then you should use it and allow compiler to double check your work.
Thanks for the suggestion! This is very interesting, could you elaborate further on this?
A switch statement might be a better example... it's often literally a branch table or at worst case a binary search that jumps to the needed code ("duff's device" in C/C++ is a pretty crazy example sometimes used in embedded software).
You are raising some interesting points. I'd say having some knowledge about other parts of the system in the core is totally ok. In fact, I'd say it is quite common for the core to define certain interfaces (persistence, logging,etc) that allow to cause side-effects without knowing about the actual implementation that is going to be used at runtime (Dependency Inversion Principle). However, for my taste, deriving \`Serialize\` adds too much knowledge to the core. I'd prefer it if the derive would just emit a description similar to a reflection-based API that describes the symbols (number of fields, names, data-types, etc) and then implementing \`Serialize\` based on this description in a different module. I may actually try and write such a custom-derive now :D
I use the old UI exclusively (mod tools were lacking in the new UI last I checked), but eyeballing the metrics Reddit gives us, 60% of our uniques for May were on the new design, ~&lt;20% were on mobile web, ~&gt;10% were on the old design, and the rest were on apps.
I've only used Tarpaulin, but this looks like it could be good. Can anyone report some personal experiences with this library?
Here is the [first benchmark](https://github.com/rust-lang/rust/pull/60466#issuecomment-497911806).
The memory measurements look interesting.
&gt; variable, constant, and function names are completely opaque to the compiler. types communicate the same intent to the reader, but can also be understood, checked, and optimized by the compiler. To emphasize just how much it can be optimized, /u/Recursing, I've made a simplified example ([playground link](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=5cc9c7f71267966032ace6de57aae923)) of looking up given moves as in `pieces.rs` and run a benchmark on it. This is obviously a simplified scenario, but really shows the cost of a `HashMap` lookup: hashmap time: [30.877 ns 31.241 ns 31.623 ns] enum time: [94.847 ps 98.132 ps 101.49 ps] YMMV given that the compiler may be able to perform more optimizations when the piece is known ahead of time, but a simple branch (when `match`ing on an `enum`) is massively faster than hashing the enum and then performing the lookup. The compiler cannot know for certain, for example, that a provided "move" (really an arbitrary value) is present in the HashMap until it performs that lookup.
There was "Shar", an FPS. It was on Steam for a while (or was close to reaching that milestone, not sure). Can't find it now, though, so I don't know what happened.
I've been waiting 4 years for this...
I believe [UniverCity](https://store.steampowered.com/app/808160/UniverCity/) was implemented in Rust.
Looks faster, can you explain why to a beginner?
What is a thread?
[Robo Instructus] (https://store.steampowered.com/app/1032170/Robo_Instructus/) releasing fairly soon, uses gfx-rs pre-ll (opengl) + glutin/winit.
It's when you have multiple "paths" or "flows" of execution happening in your program at the same time, which share the same memory space, rather than being separate processes with separate memory. Here's the Wikipedia article: https://en.wikipedia.org/wiki/Multithreading_(computer_architecture) Here's the standard library documentation for threads in Rust: https://doc.rust-lang.org/std/thread/
&gt; I'd prefer it if the derive would just emit a description similar to a reflection-based API that describes the symbols (number of fields, names, data-types, etc) and then implementing `Serialize` based on this description in a different module. Isn't that what `Serialize` already does? It doesn't emit a data structure describing the type, but derives visiting code for it. You could write a `Serializer` implementation that did exactly that. There is indeed some overlap between serialization with `serde`, relational mappers (`serde_rusqlite`) and structured logging (apparent in `slog`'s `SerdeValue` and `log`'s [RFC](https://github.com/rust-lang-nursery/log/blob/master/rfcs/0296-structured-logging.md)). But maybe that's not such a bad thing.
I have worked with C/C++, Java, C# and I'd say they're about 80% similar syntax. Given that Rust is supposed to supersede C/C++, why is the syntax so different?
So, this issue ([#61415](https://github.com/rust-lang/rust/issues/61415) should also be linked to. There is some contention as to when this switch should be done so as to not accidentally stabilize const generics too soon. TL;DR: Get excited, but not too excited.
[A Snake's Tail](https://store.steampowered.com/app/654810/A_Snakes_Tale/) is in Rust AIUI.
&gt;Multi, multi, multi, multi multi multi, multi multi multi multi. Multi. Ah got it! Thanks!
I mean yeah basically. That plus a terrifying about of potentially undefined behavior :)
You are probably looking for /r/playrust
You actually do not have to implement Serialize manually for the wrapper struct, you can tag the wrapper with [`#[serde(transparent)]`](https://serde.rs/container-attrs.html#transparent) to make serde handle it like it was the inner type.
How do you feel about adding a single unit mode? I like the unit auto-detection, but if all entries in the output table are the same unit the output becomes implicitly graphical; the scale of each item relative to each other is clear from the number of characters it occupies in the size column. &amp;#x200B; Another option to help with glancability: to colour the different unit markers differently so that "GB" and "MB" are clearly different.
As far as lexical trait implementations go, if I recall they have not been considered because they only avoid half the problem that the orphan rules are designed to prevent. Specifically, while there is an unambiguous way to select the implementation in any one location, that implementation is not constant throughout the entire program, which can lead to seriously surprising/broken behavior when trait objects/trait generics cross crate boundaries. For instance, imagine the following situation involving three crates, A, B1, and B2: Both B1 and B2 depend on A, and they both involve performing operations on hashmaps keyed by the struct `A::Foo`. However, A does not implement `Hash` for `Foo`. So, both B1 and B2 implement `Hash for A::Foo` privately, but end up using different strategies to generate the hash. Now these two crates can never interoperate: passing a hashmap generated by B1 into B2 or vice-versa will cause it to completely break, because the definition of `Hash` depends on which crate's code we are currently executing^[note]. There are most likely certain traits where such a mismatch could even cause memory unsafety. Now, you might argue that the above doesn't apply for derives, since they're not handcoded, but based only on the definition of the struct in question, and therefore should never differ between crates. However we can't guarantee this because the procedural macros used to implement custom derives have full access to various elements of state that can change/be changed across invocations (and this is necessary for various features like error reporting). &amp;nbsp; **[note]**: I believe I read about this specific situation occurring and causing very confusing bugs in Haskell, a language whose equivalent to traits do not enforce any sort of orphan rules. I could be wrong about this, though, so anyone who uses Haskell should feel free to correct me.
I am loving both ideas! The first one, single unit mode, will definitely be done! For the second, I will see what I can do - it just depends on finding the 'right' colors, which doesn't even seem easy when using more powerful canvases than the terminal 😅.
Syntax Error: Unmatched Parentheses &gt; So, this issue (#61415 should also... ^------------------------here =help: add a closing parenthesis
Read subreddit description before posting (do this on all subreddits).
As far as I understand it's just because it has less code to parse, compile, optimize, etc. Previously they had 32 impls (I think generated with a macro, but still), one for each array size. Now they have one. But in the end they should give more or less the same code.
'mem::disciminant' starts counting from 0, a Vec or mut slice is enough.
Closures are just struct in disguise. The closure f takes ownership of v, it doesn't get dropped at the end of the scope and is valid for the lifetime of f. &amp;#x200B; This is perfectly valid.
Ohhh, I forgot that `dbg!` moves its argument. So `f` is taking ownership of `v`, not borrowing it like it normally would if I had called `println!("{:?}", v)`, which does cause a compiler error. https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=e07b8de8483e8bb91f0f13ac22b0d3e4
One thing you can try is to use [De Bruijn indices](https://en.wikipedia.org/wiki/De_Bruijn_index). The main idea is that variable names don't matter -- you only need to number them. When you have no nested `let`s, you can refer to a variable by its index, e.g. in `(let ((x 10) (y 20)) x)`, `x` will have index `0`, and `y` will have index `1`. This no longer works when you nest them, so you need to add an extra value that indicates the nesting level of your binding. Note that while I may have implemented such a scheme in another life, I don't know if it's considered to be the standard thing to do in your case. Someone with more experience in compiler front-ends might advise you otherwise.
As OP points out, I think it would be more precise to say that `dbg!` takes ownership of `v`; if the closure were to take ownership, it should be marked as `move`
Correct. F takes a reference to the closure produced by dbg!, which has ownership on v
&gt;Is calling Piece::Pawn.moves() going to be as fast as accessing PIECE\_MOVES\[Piece::Pawn\]? (I don't really care as it's much cleaner anyway, just curious) Benchmark it.
That's not entirely true. Closures can take ownership without needing the move keyword.
Would anyone ELI5 this?
Thanks for the info, but I think you meat to tag /u/rerecursing...
I *think* hinting works by adjusting the vector coordinates of bézier control points. So it could be run on the CPU before giving hinted outlines to Pathfinder. I don’t know if anyone tried running the hinting algorithm on GPU.
You're right! The closure f takes ownership of v, and then the call to dbg!(v) takes ownership of v from the closure. This is the critical thing that I didn't realize, or forgot about: even though it's not a `move` closure, it still takes ownership of the value if the value is moved in the closure. The `move` keyword makes the closure take ownership of the value unconditionally, and the only cases where that makes a difference are when the value is not moved within the closure body.
I'll be honest, I would simply not use serialization in this instance. For background, my experience lies in implementing distributed systems that run 24/7. For example, I used to belong to a team when our service, itself part of a larger system, was composed of 8 partitions (communicating together), and each partition was taken fully off-line at most 15 minutes per year for database upgrades^1 . No two partitions were off-line at the same time. The application did not serialize its core data-model. Ever. Doing so is suicide. A distributed application which is upgraded piece-meal requires the ability to support multiple versions of the messages that are exchanged, and sometimes to radically switch the protocol to support new functionality, while supporting previous versions so that clients can migrate incrementally. Not surprisingly, the application used dependency injection (or an hexagonal architecture, if you wish to name it so). The core was built around the business model, which could evolve at leisure, be sliced and diced, or fused, and was entirely unaware of the messages/protocols involved in the outer I/O layers. Some of these messages were mostly a big blob of text requiring parsing to extra the meta-data (TTY Type-A/B), others were more structured with extra meta-data, others were compound messages, ... the core didn't care. The external layer was in charge of ingesting the unruly data, convert it into core-speak, invoke core-usecases, and translate core-results back of course. Remember, when you serialize your core business model, you lose encapsulation, which seriously affects your ability to evolve communications. A single layer of indirection restores encapsulation, and you can simply start it shallow: 1-to-1 mapping. ^1 *I really wish the database could have been upgraded online too... but 99.997% availability is not too shabby I suppose.*
Also, actually having those traits implemented for arbitrarily long slices!
I did my waiting! 4 years of it!
Not sure about ELI5, can I aim for ELI15? ###State of the Art At the moment, Rust supports the idea of Generic types such as `Option&lt;T&gt;` in which `T` can be any type. It does not yet support parameterizing by *values*, though, so `Array&lt;T; N&gt;` where `N` is the size is not yet feasible. This limitation means that the native arrays of Rust `[T; N]` are not first-class citizens: you cannot create a method, or implement a trait, for any array. Instead the standard library has been using macros to implement said traits on every array size from 0 to 32, as well as for power-of-2 sizes afterwards. Regularly, a user tries to use `[T; 48]` and is surprised that the traits are not implemented; it's an unfortunate situation. ###Ongoing Work The ability to use `[T; 48]` and `[T; 1023]`, etc... has been long awaited. There were proposals before Rust was ever released as 1.0 over 4 years ago! @varkor has been spear-heading the implementation of `const_generics` in the compiler, which aims at supporting `Array&lt;T; N&gt;`. This is still quite experimental, and there may be unresolved issues, but it's progressing steadily. ###This PR Here this PR comes, which switches the implementation of traits on arrays from using macros for a certain subset of sizes to using `const_generics`, and everyone gets excited because: - It demonstrates the viability and progress of the work on `const_generics`. - It would solve a long-standing limitation of arrays.
I won't work if the inner type doesn't implement `Serialize` which is entire point of solutions in the post.
The standard library contains a lot of impl&lt;T&gt; SomeTrait for [T;1] {...} impl&lt;T&gt; SomeTrait for [T;2] {...} impl&lt;T&gt; SomeTrait for [T;3] {...} impl&lt;T&gt; SomeTrait for [T;4] {...} impl&lt;T&gt; SomeTrait for [T;5] {...} ... that are typically generated using macros to cut down the redundancy in the source code. But the compiler has to process all of those nontheless. It would be much more convenient if the compiler supported generic non-type parameters like integer constants, see impl&lt;T, const N: usize&gt; SomeTrait for [T;N] {...} This would be less of a hassle for programmers when implementing such a trait for a lot of arrays and it would be a lot less to process for the compiler, thus, boosting compilation speed.
I have this code that reads a directory and reads in the files to memory: struct Storage { field_1 : String } fn read_directory(dir_to_read: &amp;str) { let dir : std::fs::ReadDir = std::fs::read_dir(dir_to_read).unwrap(); for file in dir { let text_path =file.unwrap().path().to_str().unwrap(); Storage{field_1: text_path.to_string()}; } } And i get the error: error[E0716]: temporary value dropped while borrowed --&gt; src/main.rs:9:24 | 9 | let text_path =file.unwrap().path().to_str().unwrap(); | ^^^^^^^^^^^^^^^^^^^^ - temporary value is freed at the end of this statement | | | creates a temporary which is freed while still in use 10 | 11 | Storage{field_1: text_path.to_string()}; | --------- borrow later used here | [playgound link](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=79de648e7311e038df2645dd3feb90a9) How can i go about making this a non-temporary value ?
[A Snakes Tale](https://store.steampowered.com/app/654810/A_Snakes_Tale/) was one of the first rust games on steam afaik
Related question: Is buying the printed book the best to support your excellent work?
All the proceeds go to Black Girls Code, so in some sense yes, but not to me personally. That’s fine though!
[playground link](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=83211ee7326c6a2640ff9738b67b5315)
huh, right I somehow forgot about that part again.
Given that steam doesn't do any curation of games whatsoever, "made it to steam" isn't a very good benchmark for success.
Ah, right. Thanks.
Lol sorry, nice username btw
Eli5 from the same subreddit says its to be interpreted as form of speech and not literally.
Sorry if I stole the one you wanted...
You could be waiting some time yet. Certainly when I used them the other day, I couldn't do so without repeatedly crashing the compiler even in simple cases.
This was a super nice and clear explanation. Thank you!!
I read about this language / proc macro on hackernews last night and believe this is a very very interesting endeavor, because I am interested in GPU programming for scientific purposes and have not had a lot of success with using CUDA/OpenCL in conjunction with Rust.
Yep finally, there is no reason for lazy me anymore to convert larger slices to Vec just to use those traits :)
wait someone managed to land power of two size impossible?! that still was strongly opposed last i looked (~6 months ago)
I thought so, must have misremembered from the discussion :/
The wording makes me think that a single kernel can be written (EMU global). Or is it a single kernel per module?
Link to repository: https://github.com/calebwin/emu Link to crate: https://crates.io/crates/em (not sure why the "u" is missing)
`dbg!` produces no closure.
An easy way to check whether a move happened or not is to simply insert `drop(v);` after `let f = { ... };`.
Oops, thank you for that. I had a hunch I forgot to add something crucial to my comment.
Indeed! Sorry to rope you this haha
https://github.com/rustsim/nphysics/issues Would be a good place to let the developer know you found a problem.
This is a link to the introduction section of the “book” the author is providing: [https://github.com/calebwin/emu/blob/master/book/introduction.md](https://github.com/calebwin/emu/blob/master/book/introduction.md). It states the following: &gt; All of the code for your functions gets automatically translated into OpenCL code at compile time and the code is stored in a `const &amp;'static str` global constant called EMU. This global constant is a generated string containing all the kernels, which are then loaded (at runtime, I believe) into a OpenCL context, see the example here: https://github.com/calebwin/emu/blob/master/examples/multiply_by_scalar/src/main.rs#L38 The constant itself is defined here in the proc macro: https://github.com/calebwin/emu/blob/master/src/lib.rs#L558 I am not exactly sure how this interacts with several potential invocations of the `emu!` macro in different modules. Will those “global” constants then have a path?
You are looking for /r/playrust. This subreddit is about the programming language Rust, not the game.
Lexical trait implementations - yes, please, the orphan rule has given me so much grief ever since I started rust. Derive macros separate from struct definitions you could already sort of achieve with some reflection macro, but it would probably be a bit slower than compile-time rustc-provided reflection, if such a thing existed.
Except that's a very clunky way of calling native code as it requires threading and inheritance of a specific class - inheritance itself being something that should be avoided whenever possible. Additionally, since the host environment has to initialize these callbacks and then wait for their response, it's really hard to provide an API for Java code to interact with. Look at game engines such as Unity or Godot, for example: there is no way that their natively-implemented functions could be done with this system of callbacks, at least not without greatly hindering development. Such an API would require writing code to redirect data where it needs to go, which would require a great deal of channels to be kept open and a "middleman" function to redirect data to the correct function based on an id that gets passed representing the correct function, none of which can possibly be healthy for performance. Essentially, existing facilities provide a way to get from A to B but using callbacks for this kind of API instead imposes a point C that requires a costly long-way-around trip. Additionally, being thread-based means that callbacks are inherently non-blocking, which is not always desired, especially when you're doing operations that are not thread safe such as modifying the object/instance directly. The fact of the matter is that this callbacks system can be good for some things - and I do plan on making use of them - but the JNI facilities for native methods are better tools for the job in most cases.
It looks like there is support for it, but i am not sure if the WiFi will work. https://github.com/atsamd-rs/atsamd
But shouldn't closure be declared with \`move\` keyword to take ownership of \`v\`?
That actually makes a lot of sense, I'm going to test it and I'll get right back to you.
In fact, I'd like to do some experimentation with the base-N encoded compressed binary technique (probably not zstd, though). You wouldn't happen to already have a corpus of mangled symbols, would you?
I'm mostly curious what engine it was using, piston/amethyst etc?
certainly wouldn't provide anything substantial, but success to me is actually getting a game out there and on the steam store. I can look at scores, number of downloads etc. I am mostly curious about which game engines were used. Making a game in one engine, and if that engine is used in almost all games that have made it to steam, would indicate that the engine did not too significant of barriers to entry.
yea, it looked cool. do you know what engine they used?
[Here's a blog post about it.](https://michaelfairley.com/blog/i-made-a-game-in-rust/) Looks like he built his own little engine for it.
Still having the same problem... here [https://github.com/fica-ps/fica](https://github.com/fica-ps/fica) this is my repository, all the casts happen on the ffi module.
okay. so it shouldnt be too bad if i use piston, the piston tutorials are using glutin and opengl.
Very interesting project, does the macro do type checking/inference before translation?
Why does core care about the in-memory data representation? That seems like an unnecessary implementation detail leaking into the business logic. An alternative would be to define a trait, and define the core logic to interface with that trait instead of any particular implementation.
No problem!
My hope is that this generates the constant inside the module, but... :p
I've got your back ;)
Just wanted to say this was a really useful writeup. Thank you
`move` forces moving, but moving may happen implicitly if required by the expressions in the lambda.
I created the [usb-rs](https://github.com/usb-rs) organisation, and uploaded my current code, and also forked your code there. I can invite you if you wish to work on the code, otherwise I'll continue tinkering by myself.
I think this is something I ran into earlier as well... what's the best way to use an Arena, and keep track of references into that Arena elsewhere (e.g. in a vector)? I have something like struct Foo&lt;'m&gt; { x: Arena&lt;u64&gt;, // From typed-arena v: Vec&lt;&amp;'m u64&gt; } but this is giving me all sorts of headaches when I try to write a function like impl&lt;'m&gt; Foo&lt;'m&gt; { // To be stored in self.v later, depending on something else... fn process(&amp;mut self, n: u64) -&gt; Option&lt;&amp;'m u64&gt; { if n == 0 { None } else { Some(self.x.alloc(n)) } } } I understand why it is problematic, but I don't really get how to solve this...
Also, for Serialize specifically, there's https://serde.rs/remote-derive.html
You're right. I'm gonna open report there.
when is it explicitly necessary then?
I think one way to do this is to pass the listening socket to the new instance, then shut down.
If you don't do anything in the closure that requires moving, but the closure lives longer than the value. So, this example, but if something other than `dbg!` were called.
Interesting! I've skimmed through the types supported and just wanted to know, is there at least theoretical possibility of structures support?
You can reuse the same connection for things that are supposed to be handled by different processes, so that won't work. I'm also thinking of handling the static resources in the listening instance, so as not to introduce unnecessary load when serving big files.
The way that lambdas capture or borrow their environment normally (without `move`) depends on how expressions in the lambda body use names from the lambda's environment. For example: ``` let foo = String::from("bar"); let lamb = || something(&amp;foo); ``` Produces a lambda whose memory representation (effectively an anonymous struct) contains a reference to `foo`. This sets a lifetime constraint on the lambda such that `foo: 'lamb`, i.e. the lambda cannot outlive the scope of `foo`. If instead i want the lambda's representation to contain `foo` itself, such that its lifetime is no longer constrained and e.g. may be returned from the enclosing function, i can add the `move` keyword which forces all used locals from the environment to be moved into the lambda's representation (and become unavailable in their original scope) You can granularly control moving of names by e.g.: ``` let foo = String::new(); let bar = String::new(); let lamb = || { let bar = bar; // only `bar` moved something(&amp;foo, &amp;bar) // `foo` is borrowed } // imagine `struct Lamb { foo: &amp;String, bar: String }` ```
Reading books without practice has never worked for me. Try writing some code.
I have been waiting 5 years.
There's not much content, but I'm surprised that people still use Gopher.
I stuck the two C examples from [a blog post on computed goto vs. switch](https://eli.thegreenplace.net/2012/07/12/computed-goto-for-efficient-dispatch-tables) and a Rust translation of the `switch` version using `match` into the [Compiler Explorer](https://godbolt.org/z/b9-Ftd) to compare. The Rust version comes out equivalent to the C `switch` version; it has an extra branch for checking that the value is within range, and it has a single `jmp` instruction for the dispatch table rather than one per opcode. Actually, the safe Rust version has an additional extra branch for bounds-checking the `code` slice, which the C version really should have, but if you make it unsafe and replace `code[pc]` with `code.get_unchecked(pc)`, that will get rid of the bounds-checking branch for `code` but not the bounds-checking branch for the `match`. I'm actually somewhat surprised that you get a bounds-checking branch for the `match`; given that this uses an `Instruction` type rather than a raw `u8`, it shouldn't have to do that check, as it should be able to rely on the type to determine that it's impossible for the value to be out of range. Anyhow, it looks like for now, the Rust `match` approach should be pretty much equivalent to the C `switch` approach; whether that meets your performance needs is up to you. It will probably be possible for the Rust `match` approach to do better in the future by relying on the stronger type-safety guarantees that Rust provides to eliminate a branch, but I'm not sure if that branch or the better possibility for branch prediction that having separate `jmp` instructions for each opcode is what provides the bigger speed up.
According to this log, it was built on top of the SDL2 bindings: https://thinkof.name/2017/05/24/may-change-log.html
Yup. Rewrite some of your code or try to contribute to a project. Or, once you learn a little, try to contribute to the projects of other beginners.
The most popular framework in Rust is SDL2, which was also used by several games making it to steam.
My first thought was to just write something, so I grabbed the html for the play Othello from the MIT website, so I could parse through it and create a csv with all of the words in the play, stored in lines which also had the speaker, scene, act, etc to try some data analysis on each player's vocabulary. &amp;#x200B; It was going well until I hit a wall involving strings, which I tried to get around by cloning, but had a lot of trouble trying to make it work, so I went to the book resources. I still haven't figured it out! I'm rewriting it today to see if I can get around it, but I'm afraid I'll run into some difficulties which will lead me to search out a more fundamental understanding of types and structures in Rust and I'll end up in a similar place! &amp;#x200B; What projects did you start with?
I've been following along with [Brooks Builds](https://www.youtube.com/playlist?list=PLrmY5pVcnuE_dyWibakRuGJcuiwAkhGZB) on YouTube where he goes through the Rust book and does all the examples using Code-OSS on Linux. It's interesting to listen to him trying to reason out `Strings` vs. `str` vs. `&amp;str`, ownership &amp; lifetimes, etc. He also has good production value with clear audio, code zoomed in enough that you can actually see it, pushes everything up to GitHub so you can see later, etc. It's not for everyone, but would be worth a look if you're new to Rust.
I think the first "serious" thing I wrote in Rust was a small piece of code that did some weird crypto on a bunch of data. I basically had a huge amount of data encrypted with a homebrew cipher that was implemented in C by a university professor, and I needed to get the data into Python, so after a few attempts of getting his C89 code to work via cffi, I just gave up and wrote a small Rust program that would read encrypted garbage from stdin and dump decrypted hopefully not garbage to stdout, and then piped the data through it. It was not a good design by any stretch, but it worked, and thankfully I never needed it after that.
Not exactly a code review, but I also wrote a compiler for a compiler course a year-ish ago and used a uniquify implementation with hashmaps. If you pass a "count" along with it, you can elegantly recurse through all of your types that derive expressions until you hit a base level expression that can be derived. It's written in Kotlin, but the idea can be translated pretty well. In theory you shouldn't need more than one copy of the string which is when you put it in the hashmap. (uniquify here) [https://github.com/DarrienG/CompilerConstruction/blob/master/main/src/main/kotlin/Lang.kt](https://github.com/DarrienG/CompilerConstruction/blob/master/main/src/main/kotlin/Lang.kt)
&gt; not much content _sadness noise_
Um, sorry, please don't take it like that. You have a nice example of the `gophermap` crate (you didn't even mention it was yours). But as someone who never used a Gopher client, I think I would have liked to read more about the protocol, and maybe a client recommendation for those who want to follow along.
Awesome use of macros
I read some of the book and some blog posts, started writing some lints (my first code soon became part of clippy) and did a lot of curiosity-driven programming (for example, with bytecount, I was nerdsniped by a comment in xi-editor's rope crate left by Raph Levien, and mutagen is itself an experiment of how far one can push procedural macros). I also contribute to rust, because I want to use the changes I made. If you, like me, are a person who needs to do something to understand it, join a project as soon as you can. Many projects have easy issues to get into ([this week in Rust](https://this-week-in-rust.org) has a weekly list) and most have really awesome mentors.
You're right actually. I should write about the Gopher ecosystem once I'm a little more familiar with it. The client I'm using is called [cgo], but [Lynx] works well too. [cgo]: https://github.com/kieselsteini/cgo/ [Lynx]: https://lynx.browser.org/
Forget Gopher, what about a feed reader?
I'm still learning and working through a fun project on my own right now, but I have to say that [Learning Rust With Entirely Too Many Linked Lists](https://rust-unofficial.github.io/too-many-lists/) was extremely helpful. It's a nice walkthrough of a few designs, that gradually works you through the process of writing a working Rust program. Make sure you write out the code while doing it. &amp;#x200B; After that I started working on a project, and though I was certainly lost at times, I had enough of a point of reference to figure problems out. Good luck!
Disclaimer: Not sure what exactly you’re trying to achieve so I’ll make some educated guesses along the way. It is possible to make runtime library loading to work in conjunction with hot-reloading parts of a HTTP server. However how well it will work depends entirely on how you will structure your application and what parts of the server you’re planning to reload. For example hot-reloading "handlers" is fairly simple and will most likely not impact the overall implementation much – the only complication is having to keep the old library version loaded while there are outstanding requests running old handlers. As soon as you attempt to make hot-reloading more widely applicable it will also become less feasible. Consider that for this to work you should restrict your dynamic libraries to only expose (and use) defined ABI conventions (i.e. `repr(C)`). Just spawning a new server, moving the ownership of the listening socket to the new server and terminating the old one after it completes execution of the old requests will be very significantly more straightforward to implement, understand and maintain.
Have you checked „awesome-embedded-rust“ on github?
I learned C and some C++ in the past, and learned Haskell from time to time (but it didn't go well). When it came to Rust, I felt so familiar that I only needed to learn the syntax and library API to be able to write something :-)
&gt; Reading books without practice has never worked for me. Try writing some code. Just to provide a counterpoint (is that a word?): Works for me. I learned Rust by just reading the book and blog posts and reddit stuff, until at some point I felt like "Ok, now I want a project", and then I started writing.
/r/playrust
&gt; 4 Sea animals [We have](https://rustacean.net/) that!
[Allow me to pre-empt the gradual escalation of this subthread with how long *I've* been waiting for it](https://mail.mozilla.org/pipermail/rust-dev/2012-March/001467.html) :)
This blog post contains a lot of misconceptions and misleading information about Piston. I can't respond to all of them, there were too many, but I wrote up some that I thought were most important. Link to my comment on /r/rust_gamedev about Piston: https://old.reddit.com/r/rust_gamedev/comments/bvrkmi/a_guide_to_rust_2d_game_frameworks_2019/ept7hc0/ Notice that this is not the first time OP is spreading misleading information about Piston. I believe this is on purpose. For example, the word "useless" is mentioned 4 times in the blog post and all used about Piston. There has also been a previous episode where OP used inappropriate words on Piston's issue tracker: https://github.com/PistonDevelopers/piston/issues/1185 I believe that competition among graphics APIs is healthy, but not like this. I believe OP could be more careful and respectful. I have been done this community a service by contributing to several important libraries for years, and I would like to have a working environment which I thrive in. Professional competition is different from harassing other people. When this happens, I will speak up and be honest and open about it. If you have problem with that, then you are not welcome in the PistonDevelopers organization.
Wow, lots of good info here! Love the pointers on using SVG files with Rust. Thanks for sharing!
I mentioned I was biased in the article and was trying to work past it because my biases are based on old information. I've said dumb and bad things in the past, and I'm sorry. This was my 100% attempt to be as unbiased as I could be and honestly try to make something useful, treating Piston as exactly the same as I treated everything else I tried. You can dispute my conclusions but my method was honest and my intentions pure. That's all.
I started with The Book, and then got a copy of Blandy and Orendorff's *Programming Rust* when it came out. That book is fantastic. Just wrote code along the way like I always do: chose Rust as the implementation language for small projects I was doing anyway. The learning curve is reasonably tough: I'd say it took me a year of occasional practice to get pretty comfortable, and I'm still learning things.
Bit by bit. Lots of small projects. Submitted one to GitHub and was astounded by the community's help to fix it up. https://github.com/timClicks/cool_faces
- "Supposed to supersede" is too strong I think. Rust can solve many of the same problems, but that doesn't mean that C or C++ are going away. - The biggest syntax difference is lifetimes, which don't exist in other languages. The second biggest is putting the types on the right, which I think makes it more natural to leave types out when they can be inferred, and Rust does more type inference than C/C++/Java. What other differences jump out at you?
Rewriting something you've written in another language is a great exercise as you already know the domain, so you don't try to learn the both the problem domain and a new language at the same time.
If I remember well, from old "What's everyone working on this week?", at a moment in time it used WebRenderer for the graphics. But that's just some old memories, with no fact backing them up.
I read through parts of the Rust book, and looked at the samples on Rust By Example. Beyond that, it was a bunch of playing around, rewriting little things I'd done before in other languages. The first non-trivial project I did from scratch was writing a web-based Assetto Corsa server configuration generator with Rocket, which used configuration data stored in JSON files to generate the CSV-based configuration files. That was before I decently learned how the borrow checker worked, so there were a bunch of unnecessary clones all over the place, but it worked. That one ended up going into use, because I fixed all the little annoyances with what we already had, making it easier and quicker to use.
that actually makes so much sense. Thank you!
 . Tu nie u tucy
You should be able to depend on the xml-schema crate by using it as a git dependency in your Cargo.toml: https://doc.rust-lang.org/cargo/reference/specifying-dependencies.html#specifying-dependencies-from-git-repositories
Rust is definitely a language you need to read a lot about and especially read other people's revelations about it and tuck those articles under your pillow for when you feel like starting your first program in it. You'll definitely need to reread those. But as an advice, I find that writing something that you actually need, and would find tremendously useful for yourself, is the best way to start. I actually haven't learned a lot of Rust but I feel like I'm heading in the right direction. Lately I'm more concerned about where the language is heading especially with async/await, though.
I'm writing a chess engine targeting wasm :)
This isn't harassment. It's an honest testimonial. I've had effectively the same experience trying to use Piston myself. It's not off base at all. If you think it's too harsh to say, maybe you should improve the documentation until Piston makes more sense to others.
Yes you can but you have to "erase" the concrete type because all functions have unique types even though they have similar signatures. What we do is create a trait object of a function. You can read about this in the book [here](https://doc.rust-lang.org/book/ch17-02-trait-objects.html). [playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=a96163b2dda0809abd21ff7751846e04)
&gt; Gopher, `cgo` My lol no generics sense is tingling!
I have about a million symbols, in roughly 1GB of `csv`s. There is a commit in the PR that lets you automatically dump symbols, with several manglings (but that logic is removed in a later commit in the PR because I didn't want to actually expose the hacky dumper). I build Rust and Cargo with the appropriate env var set and get a bunch of csv's. Now, regarding the compression: I would rather us have something more extreme, like fully opaque symbols with the names in separate debuginfo, or interning at the whole binary level, etc., as an option. I don't think hardcoded dictionaries are worth it, as it stands, but feel free to bring it up on the tracking issue.
What are Rust classes?
There's pretty good support for the nRF51 and 52 series: https://github.com/nrf-rs/
As u/barskern said you have to erase the type of the function but you don't have to use a trait object, you can use a function pointer like [this](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=88065222b95e503eedd71c9475d044a7). This way it's just a thin pointer.
TIL. Is this a special case for static functions?
You don't even have to cast. [Playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=0d73fd2b8280ccebf2200a6c360bb5df)
I like the interned symbols idea, would 3rd party tools like gdb be able to be make to work with that without too much fuss? I still think independently-decodable symbols are probably of value; I've been on a data compression research kick lately, I'd like to try to take advantage of just how structured the symbols are. A hardcoded dict definitely wouldn't be worth much currently, too little context is utilized. Multiple dictionaries selected by a partial matching context model should be more interesting and 'trainable.' If I come up with something substantial, would the tracking issue be the place to get feedback on its potential utility?
Probably meant `struct`s+`impl`s. When you squint, they look a lot like how idiomatic C++ separates classes into the `.h` portion and the `.c` portion.
I simultaneously read *The Rust Book* and listened to the [The New Rustacean](https://newrustacean.com/) podcast. Can't say enough about The New Rustacean, great content and a wonderful way to learn stuff on my walk to and from work.
Yeah - the "fn() {test1}" type is actually zero-sized and implements `Fn()`, so that if you pass it something generic over a closure, no actual function pointer has to be used, and it can instead be resolved during monomorphization.
If the code works, then you are doing fine. The desire to do the right thing won't go away, but the worry that you are doing the wrong thing fades with experience.
I am by no means experienced, but here is my 2c: I think of the borrow checker and ownership as proving that my program works to the compiler. If I can't prove that my program works to the compiler, have I really proven to myself that it works? More often than not, the borrow checker shows me a mistake that I've made that I would never have even considered. Once I realise this, then the code I wrote in some loosey goosey dynamic language starts feeling a whole lot less correct. Imagine if the borrow checker was there to point out all of your mistakes. It turns out writing code like I did before was actually just plain wrong.
Thank you!
Good point, I've never looked at `Serialize` that way. As a followup question then: How would I go for having two different serializations of the same type in different contexts? It seems like the customization of how to actually serialize an instance of a type should be passed into the `Serializer` instead of modifying the code generated by `derive(Serialize)`. Something like: ```rust serde_json::to_string(&amp;instance, &amp;config) ``` where `&amp;config` is the a type which describes which fields should be renamed, omitted, etc.
Haha its just that easy huh? Thank you !
Interesting insight, thanks for pointing that out. If hadn't fully thought it through but I expected some kind of problems with what I suggested. It looks like it would only work in very specific cases and it is probably impossible to pin that down to rules you can implement in the compiler.
Wrong sub, I think you want /r/playrust
I tried ggez a few times and I think it's actually cost me more time than saved. At this point, I'll usually jump straight into glium. It's worth mentioning that most of my usage for these high-level rust graphics libraries has been for doing quick visualization, mostly for large quantities of repetitive geometry (i.e. draw 100,000 dots or 10,000 bounding boxes) ggez lacks any sort of general-purpose interface for instanced rendering, which is essential for me. Additionally, it's difficult to do any sort of low-level mesh generation. It appears everything goes through lyon by default, with no option to opt-out. A recent example of this issue: 1. I pass a small line segment (tiny, even, but still a valid segment) to MeshBuilder::line or poly_line 2. Lyon looks at my line segment and decides it's too small to matter, and emits no vertices 3. GGEZ looks at the (empty) data returned by lyon and tells me I've made a huge mistake by submitting no vertices (even though I did) and returns a generic error type (i.e. one that I cannot specifically ignore). Additionally, I've encountered situations where I've been able to destabilize the main loop; i.e. where the update method runs too slow and ggez keeps running update, failing to draw or handle inputs in a timely manner. I understand a high-level interface involves trade-offs, but they aren't the right ones for me.
&gt; Not surprisingly, the application used dependency injection (or an hexagonal architecture, if you wish to name it so). Nit: I think dependency injection done right will lead to a hexagonal architecture but just because you are using DI doesn't mean your modules are designed to support a hexagonal architecture. You could even use DI without any modules. &gt; The external layer was in charge of ingesting the unruly data, convert it into core-speak, invoke core-usecases, and translate core-results back of course. That is the dream! The question is, how well does the ecosystem support building such an application? It appears like, if you wanna go this way, you have to hand-roll many things that you could automatically generate in other ways. However, maybe it is also more of a feature and if the serialization format is largely different from the core data model, they should be different types.
I generally just start by writing "sloppy" code, eg. some clone()s, owned types all over the place (String, Vec) etc. Once I got it somewhat working I go to fix it. Eg. does this function modifies the Vec from arguments? No? I can probably just use slice here etc.
&gt; Why does core care about the in-memory data representation? Did I suggest that somewhere? You are right, it should not care at all! &gt; An alternative would be to define a trait, and define the core logic to interface with that trait instead of any particular implementation. I think, that is actually the only way to do that. If that is not the case, the `core` would have to depend on the specific implementation, which is actually what we want to avoid. The downside of this is that you have to duplicate the struct on the outside, even though in some cases, it is exactly the same.
Made it to Steam implies finished. That can help to demonstrate that Rust can enable productive programming
&gt; Remember, when you serialize your core business model, you lose encapsulation, which seriously affects your ability to evolve communications. I agree, it would still be nice to have an automatically generated implementation for all the cases where it can be generated and just start to handroll it once it evolves (i.e. the core model changes but you don't want to break the API). In the Java world, I used `Mixins` for exactly this purpose: https://github.com/FasterXML/jackson-docs/wiki/JacksonMixInAnnotations That requires some serious reflection magic though :)
Awesome!!
Well, yes and no. If I have to re-define the struct anyway, I might aswell just serialize or deserialize into that directly and then convert at the module boundary.
Is that a problem you feel when writing Rust? In that case, I'd say that if your code works and you have no problem maintaining it, you're good. (Of course, maintainability issues may crop up later; in that case, take it as a learning experience.) I have the opposing case: nowadays writing Ruby, Python and JavaScript feels very awkward to me. I feel like I should restrict myself to stricter semantics around mutability, ownership and state. I try to do that, only to find that I'm coming up with code that isn't quite fluent and natural in the idioms of those languages (or at least, what I perceived those idioms to be, and what some of my coworkers perceive them to be). During the last five years, I've become gradually stronger proponent of minimizing and isolating side-effects and mutability, keeping state explicit and thinking hard about ownership and references. I think this has made me a better programmer overall, but it has also made me more awkward with things that I used to be fine with.
This is pretty cool since one of the biggest hassles with OpenCL is dealing with the runtime setup. Performance-wise, I think it tends to lose out to CUDA? I wonder how difficult it would be to add a CUDA backend. May need-arch specific extensions unfortunately.
As someone who's getting into learning rust and is also interested in game development. This is going to be a really fun project to break down. Looks really cool!
For me, it did go away, but it took a long time. I can now write decent amounts of code without ridiculous amounts of `.clone()` or `Arc&lt;RwLock&lt;T&gt;&gt;` and without running into borrowing errors. I'd suggest not worrying about it too much for now. Your Rust will likely get more idiomatic over time as your understanding of things solidifies. It will probably never look as clean as, say, Python, but that's to be expected. Rust is actually quite verbose and low-level when it comes to memory, even more so than, for example, C++. In C++, you can hide a `.clone()` behind a simple assignment, an initialization behind the declaration of a variable or an `.into()` behind just passing something as an argument to a function. Rust makes your write all these out, which is both a good thing and a bad thing. After using Python, C++ and Rust for a few years each, I feel like Rust is the only one where I *know* what's going on in my code at all times.
&gt; Would it be safe to use transmute *something something* No.
You lose performance but gain portability. Can't win every battle.
He was using webrender for the GUI a while back, but that’s since been removed.
Because?
Apparently Actix Web type is bytes::Bytes (from Bytes crate), I just need to convert that into the BSON (the same as it has been before converting into Binary)
Does Ruma intend on directly providing a client, or providing a crate with a nice api for building clients?
Love these videos, thanks.
From a quick look through the source, it looks like it replaces the function definition with the constant. So you can use it at in any scope, but at most once per scope to avoid name conflicts.
What's the opposite of "async"? I want to have two methods, one that makes a blocking call (for non async use cases) and one that returns a future (for async use cases). What should I call them? `call_async` and `call_block`? Maybe just `call` and `call_async`? Since `Sync` in Rust means something else (thread safety related).
`Serializer` has a method [`is_human_readable`](https://docs.rs/serde/1.0.92/serde/trait.Serializer.html#method.is_human_readable) that lets you change the behavior based on whether you're using a text or binary format. When I had a need to use it, I made the smallest part of the struct that depends on it, implemented `Serialize` on that small struct, and then derived for its container. Otherwise, pretty much all customizations come from the `Serializer` itself.
\`bytes::Bytes\` provides a \`into\_buf\` method (from \`IntoBuf\` trait), which is a reader you can pass to \`bson::decode\_document\`.
Actually the easiest way was just &amp;input\[..\] :) Thanks for the help! PS: By any way, do you know how to show a BSON as slice (u8)? So I can send it later on to the network
You're welcome! I'm glad it was helpful! :)
There is ruma client api which is what I assume you are after: https://github.com/ruma/ruma-client-api
Ah, yes, in that case, and if you can't customize the `Serializer` enough, I'd go for the old DTO approach (you solution 1). You say it's awkward because you end up with two identical types. But in the cases I've encountered, there were more differences than the names of the fields. I wanted to group multiple objects together (to avoid chatty interfaces), skip a lot of their fields, add in fields from related objects, avoid other quirks like lazy loading. So in the end I had completely different objects. But of course, you might not find yourself in the same situation.
&gt; inheritance itself being something that should be avoided whenever possible Inheritance can be always avoided if needed, but IMHO, this is too general to say. I agree that inheritance can hurt and I too prefer composition generally. However, in this case it is more clean to use inheritance because j4rs internals are hidden by the user; it is not just about code reuse. Moreover, projects like JNA uses inheritance [as well](https://github.com/seanjensengrey/rust-jna-example/blob/master/src/main/java/rustjna/Treble.java). Regarding host environment initialization in Rust, j4rs is implemented having Rust code calling Java rather than the other way round. Rust is the main "world" here and Java is getting called __by__ Rust. This is why Java-&gt; Rust direction is considered like callback. Using Rust channels instead of invoking Rust functions for callbacks felt more flexible generally. The early versions of j4rs were using [functions for callbaks](https://github.com/astonbitecode/j4rs/tree/v0.1.5#callback-support). If you believe that it would be helpful for you to have such callbacks as well (or something else/better), please tell me in order to design something for it. And of course, your help and ideas are greatly appreciated.
`bson::encode_document`? Or you wanted something else?
Because that's not how lifetimes work. `'m` is some arbitrary lifetime that the creator of the struct chooses, not the lifetime of the struct itself. Transmuting a reference into the Arena (and keep in mind that references into the Arena may not be valid as long as the struct itself, because the Arena could be mem::replace'd and then dropped while the struct lives on, for example) to `'m` is unsound. But I didn't actually need to figure out any of that until just now because *reaching for transmute is always, always, the wrong answer, always*. If you need to ask, doubly so.
Try reading Programming Rust in addition to the book. They complement each other pretty well. Start with very small and easy projects first, and slowly increase the difficulty. Good luck!
Also you can work better with larger arrays as the impls are there for any size.
Literally the worst mentality to have when you're software engineer, but ok... If you used it on anything else, you'd be busy recovering from a hospital ceiling falling onto you while you were barely recovered from the bridge that collapsed while you were on it, not spouting bullshit on here. And that's just the best case.
I suppose, yeah. I don't want to be a downer, but right now I want to focus on getting the first version of this upstreamed into all the tools. You can see on the tracking issue that we considered a change but it wasn't a big enough win to bother. If you really need it, I can clean up that commit I mentioned, and take out the unnecessary parts, so you can compare the current mangling and your modifications.
&gt; Transmuting a reference into the Arena (and keep in mind that references into the Arena may not be valid as long as the struct itself, because the Arena could be mem::replace'd and then dropped while the struct lives on, for example) to `'m` is unsound. So it's fine as long as I guarantee no one calls mem::replace on the Arena? &gt;But I didn't actually need to figure out any of that until just now because *reaching for transmute is always, always, the wrong answer, always*. If you need to ask, doubly so. How is one supposed to learn if not by asking questions? &gt;Anyways, what you've got is a self-referential struct if I understand your goal correctly. My goal is to use an arena for allocating a bunch of things that I need to store somewhere. If that involves creating a self-referential struct, that's ok. Creating a self-referential struct is not the goal.
encode\_document I think it doesn't return a slice actually
I don't have quite a much experience as you, but i can really relate with that last part. The more i learn, the easier it is for me to spot problems in other people's code, and the harder it is for me to trust my own code, from fear of having missed something or not knowing about some edge case. Rust tries to protect you, but in doing so it made the ever-innocent Python suddenly look like a minefield. Filled with cobras.
The TWIR explain wat is Rust. Each time.
Yeah, I wasn't expecting to change the first version; throwing out the whole spec outlined in the rfc is not exactly a 'last-minute change!' I have the commit before you removed the dumper checked out, now I'm just waiting for my pitiful internet connection to allow git submodules to do its thing, so I think I have it from here.
&gt; I feel like Rust is the only one where I *know* what's going on in my code at all times. I think this is the main advantage of having things explicitily stated. In C++, for instance, it is easy to make wrong assumptions (ie. is that operator= doing a shallow copy, or a deep one, or a mixture of both). Especially when dealing with piece of code written differently. Knowing what is going on by reading the source without looking deep at the impl outweights the verbosity cons, IMHO.
&gt; But as an advice, I find that writing something that you actually need, and would find tremendously useful for yourself, is the best way to start. Just keep in mind that there are different types of learners, there's no one-size-fits-all. Even beginners (well, everyone starts as one, right?) need to figure out their preferences some time, and no one can really do it for them. So I kinda find it important to bring over the message that this is an individual thing :)
https://steamcommunity.com/sharedfiles/filedetails/?id=868228143
Let me ask: 1. Are you familiar with functional programming concepts? 2. How many other languages do you know? 3. How familiar are you with strongly typed languages? 4. What exactly, besides problems with str and String are you struggling with? I mean, it's not for the faint of heart. I read the book front to back before I really wrote much code. Then when I did I wrote a web app, because that's what I'm most familiar with. I wouldn't say it was easy, and I'm probably only an intermediate level Rustacean. And I had to ask for help a lot in the IRC channel. But bit by bit I've learned well enough to be comfortable and relatively proficient in the language. It's probably taken me about 6 months to get here. I think past experience with functional programming really helped me, because I didn't have to struggle with those concepts on top of Rust specific stuff. Same for having worked with strongly typed languages. And I like to learn at least one new language each year. So, I know I'm going to go in, read all the recommended stuff up front to set the stage, then work through doing something familiar, making sure I'm doing things as "by the book" as possible, based on what the recommended literature said. For a while, you're just struggling through and making things work. But after a while, it all gets easier. Biggest thing is that you try to write idiomatic code, even very early, so that when things are getting clearer, you're not having to go back and undo any bad habits you developed along the way. No really good way to learn a new language beyond that. Just gotta get the foundation in there then, in the case of Rust, bash it out with the compiler for a while. And ask specific questions.
You may also be interested in https://itch.io/c/449652/rustlang-games and https://itch.io/games/made-with-rust lists.
The unceasing feeling that there ought to be a more elegant and memory-efficient way? Check. But I often have that with Ruby as well. In fact, starting with Rust increased my consciousness about design when working with Ruby. However, I have lately come to think of the borrow checker in terms of maintainability. In this view, the ownership system's greatest contribution is not in the initial version, but in catching the well-meaning refactoring that would have introduced a memory leak.
I ended up with `blocking_call` and `async_call`.
Can someone explain how this does not generate an infinite amount of machine code? Since N can be any value?
Any program with UI requires some kind of async support. This UI may be graphical, pseudo graphics or even stdin/stdout. Also any user of network API should be async, to not waste memory and cpu resources: bluetooth file transfer/rs232 communication/HTTP REST API and so on. &gt;While standard file access is generally Fast EnoughTM to work &gt; syncronously, there are other cases where this is not the case. It is not true for any good GUI program. Your program can hangup for several seconds even if you write file with several kilobytes. For example on windows virus scanner can catch `CloseHandler` call (file close) and start investigate file. So good GUI program should do any potential blocking operation in async way.
That's correct, thank you. The word "class" is baked into my mind from using other languages.
I really liked [Rust by Example](https://doc.rust-lang.org/rust-by-example/) when learning. I found it to be a very terse and efficient way to get familiar with the language. (This was when TRPL first edition was not yet finished.)
[Here](https://github.com/daa84/neovim-lib/pull/23) is a real-world problem I'm facing. It's a communication loop (editor &lt;-&gt; plugin) that blocks, but communication really needs to be nested to function fully. I've solved this locally by spawning a new thread for the most relevant part of the communication, but that's actually not a full solution (for that, one would need to spawn a new thread for each message). Prime material for async/await, I'd say, although I don't yet understand how to do it :)
The last few days I've experimented with profiling [orion](https://github.com/brycx/orion) using Valgrind + KCachegrind, trying to reduce memory footprint and doing general refactoring with some performance improvements. I'll be continuing this and hopefully get to improve some fuzzing targets as well. I also want to add support and testing for `wasm32-unknown-unknown` in CI. By looking at the [rustwasm docs](https://rustwasm.github.io/docs/book/reference/add-wasm-support-to-crate.html#maintaining-ongoing-support-for-webassembly) this should be a trivial task. Yesterday I finally got to release `0.3` of [checkpwn](https://github.com/brycx/checkpwn) after refactoring, fixing a bug and updating its dependencies.
Somehow the small things ate the whole previous week and will cut a good piece from this one: `battop` apparently does not handle edge cases properly and can even do a [stack overflow](https://github.com/svartalf/rust-battop/issues/8) trick; my small crate for the actix-web HTTP authorization stuff needs attention too, since `actix-web` is going to bump up to the `1.0` version soon; there is a lot of stuff to fix, update and publish. In addition this `winapi` [PR](https://github.com/retep998/winapi-rs/pull/772) is blocking my work, so there are no excuses now not to dig the pile of technical debt.
I tested it in my computer: Fedora with AMD CPU and Nvidia GPU, it works very well. I only miss a way to change the controls since I don't use a US keyboard. I think that's an important feature if you want people for many countries to test your game.
Didn't know about implicit moves into closure. Shame on me :)
This started to go away for me as I started to understand more of the design decisions behind each restriction. I still write some code more awkwardly, but since I understand a lot of what I gain by writing that awkward code, I feel much more in control, and it isn't awkward to write.
Ah right, thanks for the update. Good luck with your next project!
I wasn't involved in Shar's development, it's /u/not_fl3's project :)
Rocket and wasm-bindgen are exceptions to the rule - the former is a DSL and the latter is specifically a codegen tool. Most libraries don't use procedural macros.
Except Rust is one of the few languages where if the code compiles and works, it's probably correct as well.
Keyword is probably. Brainlet.
If you are building something for a customer, then rigor is important. If you're learning a programming language, you don't yet have the intuition to apply rigor.
I've got a related feeling sometimes: the 'Attack of the Clones' syndrome, when I *have* to make a piece of code work under some too tight deadline and I have to resort to .clone() instead of nice lifetimes to get out quickly. In this case I know I'm leaning to the dark side of the force.
Apology accepted.
Is it perhaps `cargo time` for another plugin?
Sure. Macros are just like functions in lisp, with the only difference that the inputs and outputs are code instead of normal data. In your language, the structure/type of code information is pretty close to that of normal data information with the exception of function calls. When evaluating a macro, you would parse the inner expressions back to code ASTs, i.e. lists of list or identifiers or values, so that the macro can process it like normal data. After the macro returns, you would just unwrap the AST into your intermediate representation and interpret it, i.e. calling `lcore_interpret` on the resulting `Values`. So basically for macros you have to process the incoming arguments and the return values before you use them. Macro definitions can be essentially the same as function definitions, except a macro would only take one argument (like [this](https://software-lab.de/doc/tut.html#fun)) or however you want to do it. Sorry to reply this late. Tell me if you have further questions!
Perhaps asking on https://github.com/Genymobile/gnirehtet/ would be a better idea? If you do, you might want to give a better description of your issue, though.
&gt; The previous version of Pushrod ran on an average of about 30% CPU at idle. The new release with 3D textures and current optimizations has dropped that down to 6% at idle. Is ~0% at idle a realistically achievable goal? I realise that it might not be as simple as that, but my understanding is that the "native" toolkits (win32, cocoa, QT, etc) come pretty close to that. And once you get up to something like a constant 6%, it seems like you're not much better off than something like electron (other than memory usage - but memory is cheap, personally it's the battery life effect that I hate about electron). In any case: good work! I've been enjoying following your progress on this, and impressed at how well you've been doing at actually working through your roadmap :)
Same reason why Vec&lt;T&gt; doesn't generate an infinite amount of machine code: instances are only created if they're used.
I have the same thing - don't think it will honestly. However, I wouldn't look at this as a bad thing necessarily. As developers, it is important to work with memory in an efficient/appropriate manner. It is quite possible (and is my opinion) that this could have been entirely intentional as a mechanism of the language design (is there a way we can check?)
We actually have that, although it's not in the settings yet. You can change the keyboard inputs in the configuration file that gets stored in your local configuration directory (`~/.config/veloren.ron` if you use Linux).
At the moment, the are no way to get such information, so we can't write a plugin. `-Z time-passes` is unstable and it returns global time and not crate specific.
art, 76pm as
(Sorry for the late respone. The last days were a bit stressful.) &amp;#x200B; I ran your code and also rewrote the C/C++ examples to use DWT and measured the times. @optimizations in STM32CubeIDE I used the default options. On the "compile"-icon there is a dropdown for release mode. I used that and then when starting the program the IDE asked me if I wanted to used that as run config. I have to admit that I was confused that there is no button like "Flash the device" or just "start", so I used the "Debug"-button to start the program. The main.c file then notified me that this file is only available in ASM, so I think this should do the trick. &amp;#x200B; The odd thing was Rust defaulted to 2 wait states. I added the following function to set it to 1. The files are [here](https://github.com/DevelopingHydra/filesharing/raw/master/primes_examples.zip). pub fn set_flash_ws(){ unsafe{ let flash=&amp;(*stm32f30x::FLASH::ptr()); flash.acr.write(|w|w.latency().bits(1)); } } &amp;#x200B; Here are my measurements: &amp;#x200B; ||Cycles DEBUG|Cycles RELEASE| |:-|:-|:-| |Rust|188898479|3218370| |C|7295974|3346441| |C++|7563968|3896466| &amp;#x200B; That C++ is slower than C does not supprise me, but why is Rust faster? If I'm correct the STM32CubeIDE uses GCC and Rust uses the LLVM. Maybe the optimisations by the LLVM are better? But this also seems odd: Why is the compiler provided by the vendor slower than the more or less generic LLVM? Because, I'd say the vendor should optimize their products as much as possible.
You can certainly write a `RUSTC_WRAPPER` script which times the invocation of `rustc` for each crate and shoves the results somewhere.
As far as I know De Bruijn indices are a bit different from your description (and more like rustc's): You don't number the definitions, just mark the places where a variable is defined. Then index 0 refers to last visible variable, 1 - to the second closest, and so on. So for example, this: let a = 1; let b = { let c = a; let d = a + c; c + d + a }; let d = 1 + b; println!("{}", d); using De Bruijn indices would look like this: let &lt;var&gt; = 1; let &lt;var&gt; = { let &lt;var&gt; = var(0); let &lt;var&gt; = var(1) + var(0); var(1) + var(0) + var(2) }; let &lt;var&gt; = 1 + var(0); println!("{}", var(0));
Nice project, but I'm puzzled by the syntax choices for types in function declarations and their arguments. Why not match Rust's syntax there?
Yep, same for me. The first version of the Rust book was already very good for that. I just read it while commuting and was ready to start coding once I finished it after a few days.
Sooo is this a memory leak?
We can intercept rustc calls in cargo plugins, time them, and I think it should be enough to collect the total build time for each crate by using the dependency graph?
It's been there a while, so clearly this is due to lack of garbage collection.
You might be looking for /r/playrust
Would you mind saying which errors are you getting ? Also your cargo.toml would be helpful, as well as a list of extern crate... you use in your project.
I put [my asynchronous USB API](https://github.com/usb-rs/usb-async) on GitHub. I also have a fork of the USBFS wrapper made by /u/99pctofeverything in the same organisation. I'd appreciate people at least throwing ideas of things they need so that I know what to prioritise.
Look at TiDB over at ping cap. They use mysql client + rust + rocksDB.
This looks interesting. Is there are any docs on how to use it? I've tried writing a basic wrapper, but it didn't work: ``` #!/usr/bin/env python3 import sys import subprocess subprocess.run(sys.argv[1:], check=True) ``` ``` &gt; env RUSTC_WRAPPER='./wrap.py' cargo build --verbose error: unexpected character in cfg `/`, expected parens, a comma, an identifier, or a string ```
Can you make the recordlog an independent crate? I think I might have usage for that.
Thanks. For $10 ea I grabbed a couple.
This does not address the issue but it is related. If you want to know which crate (in a multi-crate project) is taking most of the compile time, you may want to look at the size of the rlib file. I find in my case that it is a good indicator.
More likely https://github.com/ruma/ruma-client, `ruma-client-api` is more or less a part of it.
Yeah, it seems like this is just a thing being discovered now, I don't recall any previous discussion about similar naming conflicts in the past (which is surprising, I thought the naming convention was pretty old). One upside of a prelude is that you can have ```rust pub mod prelude { pub use crate::FutureExt as _; } ``` and when a user has `use slog_scope_futures::prelude::*;` that will act appropriately and just bring the methods into scope without bringing the trait name in. But I'm anti-glob imports in general, so I'll just keep hiding extension trait names on import myself...
[v0.23.1](https://github.com/iliekturtles/uom/releases/tag/v0.23.1) of [`uom`](https://github.com/iliekturtles/uom) (type-safe zero-cost dimensional analysis) was released last week to fix a regression. I also worked on a [PR](https://github.com/iliekturtles/uom/pull/144) to fix an issue where `uom` was not zero-cost!
This. I would \_love\_ to see an "async-first" UI framework in Rust. It could be glorious :)
Every time you offload a computation futures can be very helpful. In the end a simple Http request is also just offloading the computation to a http server. With futures you can precisely define which computations have to be finished to continue while being to compute all of those in parallel. Imagine you have two tasks t1 which loads some data from disk 1 and t2 which loads some data from disk 2 If you both task can be executes in parallel you can just define t3 = wait(t1, t2). With futures you can easily use t3 as task because the wait operation does not block.
 [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=387669e507944f819a7b3665052815f6](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=387669e507944f819a7b3665052815f6) One more question. I added a bit to println the "data" part of the grp struct. When I declare it as &lt;Pos as Test&gt; in main it works works, but if I use a generic function &lt;T as Test&gt; it says data doesn't exist. I'm assuming that the generic function is just creating a trait object and not the object itself? If thats the case, is there a way around this or a different way to go about it and still use the object as itself and not a trait object?
Is my proc-macro crate considered no-std-compatible if the code it generates does not rely on `std` but it itself depends on `std`? In other words, only the compile-time system requires `std`, not the run-time system. Am right to believe that the binary of a user of my library can then run in various no-std environments? Am I allowed to assign my proc-macro crate to the crates.io category `no-std`?
Sorry if this has been asked before, but does anyone know when the new `await` syntax will be merged into stable? Also, is there a general place where people can track the migration of features from nightly to stable?
I just published my first CLI tool on crates.io, called [project-cleanup](https://crates.io/crates/project-cleanup). I started working on it about a year ago, but haven't done much with it since. Now I had some free time and decided to update it to the latest stable Rust version (it used to require nightly), and so I published it as well. The tool looks in whatever directories you give it and discovers all Rust, Java and Javascript/Node projects. Then it deletes all unnecessary files and directories (generated build files, modules) for all projects that haven't been touched in more than a month. I ran it on my own dev directory and it removed almost 4GB of old node\_modules and target folders. The repository is on Github at [woubuc/project-cleanup](https://github.com/woubuc/project-cleanup) and since it's my first published Rust project I'd love some feedback on it if anyone wants to check it out!
Continuing to chip away at a better interface for making multiple borrows with the hashmap from [ccl](https://gitlab.nebulanet.cc/xacrimon/ccl). Also working on some major performance improvements.
The Rust compiler doesn't try and figure out what concrete type is behind a generic type parameter. So everything you need to interact with the type must be available from the generic specification. In this case, you could add a `trait Data` that must be implemented by `Test::Container` similar to how you did with `Default`. That `Data` trait then would have a `fn data(&amp;self).` accessor. Basically, when you have a type parameter like `fn foo&lt;T:Test&gt;` you tell Rust that you're interacting with the `T` as specified by the trait bounds, in this case `Test`. Rust doesn't look at what types are actually used and simply substitute them. You need to use the trait bounds to specify the interface you expect the type to have. Trait objects are different. They are for dynamic dispatching. In your example, if you add a: pub trait Data { type Item: Debug; // I used &amp;[...] instead of &amp;Vec&lt;...&gt; because its a better practice, // but you can use &amp;Vec&lt;Self::Item&gt; as well if you like fn data(&amp;self) -&gt; &amp;[Self::Item]; } impl&lt;T:Debug&gt; Data for Grp&lt;T&gt; { type Item = T; fn data(&amp;self) -&gt; &amp;[Self::Item] { &amp;self.data } } and then change your `Test` trait to require this: pub trait Test { type Container : Default + Debug + Data; } then you can access the data through the `data` getter: println!("Data {:?}", g.data()); The crucial point is the same as in the `Default` issue before. When a function deals with a generic type instead of a concrete one, the *only* things it knows about the type are specified in the trait bounds. It will not look at the types it is actually used with. If it did, you would have invisible dependencies that aren't specified in the function signature. This allows the generic function to be verified without being used. If the expected interface is specified via the trait bounds, Rust can tell you right then and there if your code works. It doesn't need to wait for the trait to be used to verify if it can compile it with any given `T`.
&gt; I'm not really sure what the actual expected alternative to `#![macro_use]` is supposed to be You don't need it anymore - macros use the same visibility tools as everything else now. As you observed, you can do `use crate::macro;` and use the macro as normal, or you can use the macro directly with `crate::macro! { ... }`.
In case you have seen sled already, similar goals, different route. https://github.com/spacejam/sled
On the subject of errors, I was told the consensus is moving towards implementing and exposing `std::error::Error` types. The book does mention the specifics of how \`?\` works here: https://doc.rust-lang.org/book/ch09-02-recoverable-errors-with-result.html#a-shortcut-for-propagating-errors-the--operator (paragraph beginning "There is a difference between") but it doesn't exactly explain at a higher level. This is what my understanding of "good practice" looks like: https://gist.github.com/rust-play/e8fc005c393b357f8b75af5a03ddd725 Additional note: in this case, someone using this fictional library could have their own `Error` enum that could either implement `From&lt;library::Error&gt;` and make use of `get_foo()?` or `get_bar()?` but they could also implement `From&lt;library::GetFooError&gt;` to do something different when calling `get_foo()?`, it won't be used when calling `get_foo_and_bar()?` though. I personally make use of https://crates.io/crates/err-derive and https://crates.io/crates/derive-more to achieve "good practice" while reducing boilerplate significantly, the snippet above would looks like this: https://gist.github.com/rust-play/d7f650cb35abbe1124e54512b687082a
You can use `#[cfg(debug_assertions)]` to conditionally compile a section of code if you are compiling a debug build. See [this SO post](https://stackoverflow.com/questions/39204908/how-to-check-release-debug-builds-using-cfg-in-rust) for more. &amp;#x200B; If this is only for logging information and not behavior, then look into [log](https://crates.io/crates/log) and one of its implementers, which you can configure to only output logs of specific levels.
It's not for logging, the debug assertions were exactly what I was looking for. This is very helpful. Thank you so much!
As I understand it, futures could be useful for doing file I/O, too, if all the file async APIs weren't so convoluted and disparate.
I've just added my DWT code to a 72MHz example that I've posted you earlier to save time. So it's not strange to see 2WS flash latency there, it's by design. The datasheet on the chip [specifies that when you run on 72MHz](https://i.imgur.com/s0HonM2.png) you must have 2WS, or else you may get issues. Looking at this numbers I don't see any significant difference. It's probably just this specific task has much room for optimization. I think gcc on embedded targets mostly optimizes for code size, while LLVM tries to get more performance by doing more aggressive inlining and loop unrolling, disregarding increase in code size. Also llvm backend for arm is very high quality, so I'm not surprised that it works well. You can try to compile your C code with clang and see if it gets any better.
Very nice to see work on XML Schema for rust. The project has an example XML that gets parsed to rust structures. They look very different. &lt;?xml version="1.0"?&gt; &lt;purchaseOrder orderDate="1999-10-20"&gt; &lt;shipTo country="US"&gt; &lt;name&gt;Alice Smith&lt;/name&gt; &lt;street&gt;123 Maple Street&lt;/street&gt; &lt;city&gt;Mill Valley&lt;/city&gt; &lt;state&gt;CA&lt;/state&gt; &lt;zip&gt;90952&lt;/zip&gt; &lt;/shipTo&gt; &lt;billTo country="US"&gt; &lt;name&gt;Robert Smith&lt;/name&gt; &lt;street&gt;8 Oak Avenue&lt;/street&gt; &lt;city&gt;Old Town&lt;/city&gt; &lt;state&gt;PA&lt;/state&gt; &lt;zip&gt;95819&lt;/zip&gt; &lt;/billTo&gt; &lt;comment&gt;Hurry, my lawn is going wild!&lt;/comment&gt; &lt;items&gt; &lt;item partNum="872-AA"&gt; &lt;productName&gt;Lawnmower&lt;/productName&gt; &lt;quantity&gt;1&lt;/quantity&gt; &lt;USPrice&gt;148.95&lt;/USPrice&gt; &lt;comment&gt;Confirm this is electric&lt;/comment&gt; &lt;/item&gt; &lt;item partNum="926-AA"&gt; &lt;productName&gt;Baby Monitor&lt;/productName&gt; &lt;quantity&gt;1&lt;/quantity&gt; &lt;USPrice&gt;39.98&lt;/USPrice&gt; &lt;shipDate&gt;1999-05-21&lt;/shipDate&gt; &lt;/item&gt; &lt;/items&gt; &lt;/purchaseOrder&gt; PurchaseOrder { attrs: { FullName( None, "orderDate", ): "1999-10-20", }, attr_order_date: Some( Token( "1999-10-20", ), ), ship_to_us_address: ShipToUsAddress { attrs: { FullName( None, "country", ): "US", }, attr_country: Some( RestrictToken2( Token( "US", ), ), ), name_string: NameString { attrs: {}, xml_string: XmlString( "Alice Smith", ), }, street_string: StreetString { attrs: {}, xml_string: XmlString( "123 Maple Street", ), }, city_string: CityString { attrs: {}, xml_string: XmlString( "Mill Valley", ), }, state_string: StateString { attrs: {}, xml_string: XmlString( "CA", ), }, zip_decimal: ZipDecimal { attrs: {}, decimal: Decimal( BigDecimal { int_val: BigInt { sign: Plus, data: BigUint { data: [ 90952, ], }, }, scale: 0, }, PhantomData, ), }, }, bill_to_us_address: BillToUsAddress { attrs: { FullName( None, "country", ): "US", }, attr_country: Some( RestrictToken2( Token( "US", ), ), ), name_string: NameString { attrs: {}, xml_string: XmlString( "Robert Smith", ), }, street_string: StreetString { attrs: {}, xml_string: XmlString( "8 Oak Avenue", ), }, city_string: CityString { attrs: {}, xml_string: XmlString( "Old Town", ), }, state_string: StateString { attrs: {}, xml_string: XmlString( "PA", ), }, zip_decimal: ZipDecimal {
It looks like the `emu` macro is determining the `__global` address space qualifier from the `global_` prefix of the identifier, is that right? I don't have much experience w/macros but I wonder if there's another way to decorate these names in order to make that distinction.
If it is still needed, you can feed a `Vec` as the first argument to `encode_document`, and then borrow `Vec` as slice.
I will finish [basic GUI support for `coffee`](https://github.com/hecrj/coffee/pull/35). It is mostly a matter of writing nice docs, updating the examples to use the brand new UI, and debugging some weird cases (potentially contrubuting to [`stretch`] and [`glyph_brush`] in the process). [`stretch`]: https://github.com/vislyhq/stretch [`glyph_brush`]: https://github.com/alexheretic/glyph-brush
I've written a very simple graphics application/GUI with winit that had close to 0% idle usage, so it's definitely possible. It's just a matter of only running code on user interaction. Using winit's eventsloop helps with that.
Not as of yet but I am working on coming up with a solution on how to add them.
https://areweasyncyet.rs
I think you may need to run something like `['rustc'] + sys.argv[1:]`. Or even `[os.environ.get('RUSTC', 'rustc')] + sys.argv[1:]`.
This is a useful write-up, thanks! For coverage, I've found [tarpaulin](https://github.com/xd009642/tarpaulin) to work really well, and also be easy to integrate with CI and such. See for example the [coverage CI for inferno](https://github.com/jonhoo/inferno/blob/07fe3b78e26a5180c143124c9fed300220c89835/.travis.yml#L52-L61).
Say that I have something like: let a = &amp;String::from("Hello World"); What would be the owner of the value that *a* references? My guess is that the compiler would insert some variable that could be the owner, but it would be nice to get that confirmed, or even some pointer to some documentation about it! Thanks!
[stackoverflow](https://stackoverflow.com/questions/47662253/why-is-it-legal-to-borrow-a-temporary)
Thanks!
Adding on to this, any kind of IPC would greatly benefit from the addition of async.
Cloning the DOS game [Drug Wars in Rust](https://gitlab.com/spesk1/dwars/blob/master/src/main.rs). I have almost no Computer Science background and the only language I've worked extensively with is Perl, so the learning curve has been a bit steep, but enjoying learning both the language and more about programming in general.
Not the author but at least in my case that usually works. Maybe I was doing something wrong but I have run into problems doing this with derive-macros after migrating to 2018 and still had to have some `macro_use`s.
Do you poll http connections?
Not Rust related, but I bought a new laptop and installed Trisquel on it, then installed Gentoo over that, then installed OpenBSD. Had some fun playing around with those, but I think I'll stick to OpenBSD for now. Now I'm back to hacking away on some challenges on codewars to get up to speed in Rust. (Still a relative newbie)
This talk was well received - many people at [Libre Graphics Meeting](https://libregraphicsmeeting.org/2019/) had heard of Rust, but didn't know enough about it to seriously consider using it. There will be video, but I'm not sure when those will be posted.
I've been working on a framework for programming GPUs from Rust. It provides a language called Emu for writing high-level code that operates on vectors with features such as built-in mathematical and physical constants, unit annotation, and implicit conversion. It also provides 2 procedural macros - `emu!` for compiling Emu code to intermediate code and storing it in a global constant and `build!` (this is the new thing I've been working on and the reason why I wasn't able to answer people's questions over the weekend) for taking functions from the intermediate code stored in the constant and auto-generating Rust functions that will run the code for the Emu functions. Here's an example- emu! { multiply(global_buffer [f32], scalar f32) { global_buffer[get_global_id(0)] *= scalar; } } build! { multiply [f32] f32 } fn main() { // a vector with elements of type f32 let initial_data = vec![3.7, 4.5, 9.0, 1.2, 8.9]; // call the multiply function written in Emu on the vector of data let final_data = multiply(initial_data, 3.0).unwrap(); // print the results to the console println!("{:?}", final_data); } You can check out the book, examples, and the crate from the repository at - [github.com/calebwin/emu](https://github.com/calebwin/emu).
Yes, that is correct. The code generated by the \`emu!\` macro is literally just a \`const EMU : &amp;'static str = #generated\_code;\` where \`generated\_code\` is a multiline string containing the compiled intermediate code.
&gt; Coming from Node, this feels like a sharp corner: I have to manually edit Cargo.toml, change the version, run cargo build to update the Cargo.lock so I don't end up with a random diff, then I have to manually git ci -a -m 'vX.Y.Z', git tag -a vX.Y.Z, fill in a message, git push --follow-tags, and then cargo publish. Just for a basic release. I'm used to at least having a baseline of npm version patch &amp;&amp; git push --follow-tags &amp;&amp; npm pub. Checkout [`cargo-release`](https://github.com/sunng87/cargo-release). It handles `Cargo.toml`, `Cargo.lock`, `README`, `lib.rs`, tagging, pushing, publishing, etc. In the next release we have workspace support lined up, including updating dependencies.
This looks great 👍 TIL
*filled with pythons :)
nice, I just started learning Amethyst so I appreciate having more examples to study!
Yes—it's mentioned in the post. https://github.com/ruma/ruma-client is what you want.
Yes. Though it's important to realize that you often don't need asynchronous file I/O, even when you think you do. Many parts of a normal OS rely on filesystem operations always succeeding and being very fast, to the point where attaching asynchronous I/O on top of it could even decrease throughput significantly. But then on the other hand, many things these days run on networked filesystems (basically everything in "the cloud"), where it could probably yield some benefit for particular workloads.
The only things Emu functions can really accept right now are vectors (technically arrays/pointers) and primitive data (`f32` or `i32`). Simple structures could be accepted with 2 changes. - A change to the language so you can declare what kind of structs you accept and how to unpackage primitive data from them in the declaration of the Emu function - A change to the `build!` macro to generate a function that can accept structs of a certain type and unpackage them into primitive data to send to the Emu function.
Looking forward to the talk!
My goal is to have a `&amp;mut Write` which I can use later on, buffered or not, depending on a flag. The following code shows a simplified version to test it. I think I understand why the error happens, but I don't know what would be the best way to solve it. Some kind of Box? An Option? use std::io::{self, BufWriter, Write}; fn main() { let buffered_io = std::env::var("BUF") == Ok("1".to_string()); let mut stdout = io::stdout(); let writer: &amp;mut Write = if buffered_io { &amp;mut stdout } else { let mut buf_writer = BufWriter::new(&amp;mut stdout); &amp;mut buf_writer // &lt;- error[E0597]: `buf_writer` does not live long enough }; for x in 1..10 { writer.write(&amp;format!("{}\n", x).as_bytes()).unwrap(); } }
Right now the intermediate code the `emu!` macro generates is really just OpenCL. But ultimately I want it to generate some sort of bytecode or some other sort of IR. Then it should be easier for the `build!` macro to generate functions that use either OpenCL, CUDA, or other tools for running things on GPUs.
I like this convention, because one of the footguns of supporting both versions is that calling the sync version from async code might appear to work but lead to poor performance. Does anyone know if clippy or something similar is able to detect this kind of mistake? (I guess "this kind of mistake" would be anything that transitively calls blocking IO functions from the stdlib?)
For much of the development of Emu, I designed it as if you would run the intermediate code generated by `emu!` yourself using a binding to OpenCL. But now I just want `emu!` to generate some sort of IR that is general. This is still OpenCL as of now but running it is easier using the `build!` macro. So the `global_` prefix is something that will only make sense if you are familiar with OpenCL. Its removal will be part of a bunch of changes that change the language to make it more general.
Thanks! The fact that it's a constant 6% (give or take) just depends on the usage of the CPU when other events are generated by the run loop. I can put the run loop into what's called "lazy mode" and it will probably drop to near-zero. It will just make the management of the display buffers a little more tricky is all, and will make timer invalidation a little harder to manage.
It's targeted for 1.37, which goes into beta on July 4, and stable August 15. There's not a good general place to track every single unstable feature in one place, no.
Thank you so much for posting, I guess I'll adapt that to gxi and if that works try to get it on [gtk-rs.org](https://gtk-rs.org) :)
Yeah, Sled is really awesome . LSM-tree is much easier to understand for me.
Easy wholesome upvote of the day :)
Temporarily not in the plan, maybe you can just port it ?
Haven't stumbled over piet, but I think it's a good idea to have an abstraction over different APIs, and it looks quite usable from a quick glance at the docs. A library you call "green" is usually much more polished than what I usually produce. ;-)
I accidentally the borrow checker.
Documentation *when it exists* is great, but it is pretty incomplete and generally involves a lot of source diving or looking for articles like [this one](https://gill.net.in/posts/auth-microservice-rust-actix-web-diesel-complete-tutorial-part-1/). Others are right when they say you don't need to understand (or know about) actors to get started though, even for database IO. The handler/extractor/responder pattern is absolutely beautifully abstracted and really sets one up for a clean, organized API.
One solution I do have is using an enum, but this looks a bit ugly, since both match arms do the same, more or less: enum Writer&lt;'a&gt; { Unbuffered(&amp;'a mut Write), Buffered(BufWriter&lt;&amp;'a mut Write&gt;) } impl &lt;'a&gt; Write for Writer&lt;'a&gt; { fn write(&amp;mut self, buf: &amp;[u8]) -&gt; io::Result&lt;usize&gt; { match self { Writer::Unbuffered(x) =&gt; x.write(buf), Writer::Buffered(x) =&gt; x.write(buf), } } fn flush(&amp;mut self) -&gt; io::Result&lt;()&gt; { match self { Writer::Buffered(x) =&gt; x.flush(), Writer::Unbuffered(x) =&gt; x.flush(), } } } Now I can use it: let mut writer: Writer = if buffered_io { Writer::Unbuffered(&amp;mut stdout) } else { Writer::Buffered(BufWriter::new(&amp;mut stdout)) }; Is there a better solution?
Please feel free to file issues if there's stuff missing or could be improved.
Is there some way to make `#![windows_subsystem = "windows']` configurable by build? I tried `#[cfg(debug_assertions)]` but it fails because "an inner attribute is not permitted following an outer attribute" which makes sense. Specifically I'd like to turn it on the the release build but leave the debug build as a console app so I can log to stdout.
My guess is that that's because you're running into issues like [this one](https://github.com/clap-rs/clap/pull/1397)?
Adding more to my SDL2 wrappers. Just need sound and library loading and it'll be enough for basic usages.
Second this. I come from javascript, mostly writing client applications. Just reading the rust documentation (great doc btw) has helped me to understand lot more of how computers work but to really get a grip of the language I've had to get it into muscle-memory as well. A tip is to try rewriting some small program you've already got in another language in ideomatic rust. Also the compiler and clippy are both really helpful guiding you :)
You can try something like `#![cfg_attr(not(debug_assertions), windows_subsystem = "windows")]` Or use Cargo features to toggle with command-line flags.
The `emu!` macro currently does nothing to report errors that occur at compile time or at run time. &amp;#x200B; But I do believe this is possible to add with the current design. The only job the `emu!` macro has is to generate an IR from the code for Emu functions. It does this by first paring the Emu code to an AST and then traversing it and generating code. So checking for most errors can happen while parsing the code to an AST and then also while traversing.
[That seems to be a wide-spread experience](https://twitter.com/bodil/status/1135343799671164928).
When working with no_std, I've heard the exact opposite, that the team doesn't want to make it too fundamental, hence why it's only in std and not core.
Thank you as well, I had a hunch that my design would cause that kind of issue
It feels silly that application developers are jumping through hoops to make file access asynchronous when the OS goes to do much trouble to turn the asynchronous interface into a blocking one.
`Box`ing your trait object would do the trick use std::io::{self, BufWriter, Write}; fn main() { let buffered_io = std::env::var("BUF") == Ok("1".to_string()); let mut stdout = io::stdout(); let mut writer: Box&lt;dyn Write&gt; = if buffered_io { Box::new(stdout) } else { Box::new(BufWriter::new(&amp;mut stdout)) }; for x in 1..10 { writer.write(&amp;format!("{}\n", x).as_bytes()).unwrap(); } }
Here is an example that I use to avoid having `proptest` cause problems while running `cargo-tarpaulin`: #!/usr/bin/env bash set -euo pipefail ARGS=$@ CARGO_HOME=${CARGO_HOME-} CARGO_PKG_NAME=${CARGO_PKG_NAME-} if [[ "$CARGO_PKG_NAME" = "proptest" ]] then ARGS=() for i in $* do if [[ "$i" = "link-dead-code" ]] then ARGS+=" lto=no" else ARGS+=" $i" fi done fi if [[ -f "$CARGO_HOME/bin/sccache" ]] then exec $CARGO_HOME/bin/sccache $ARGS else exec $ARGS fi
1. Not familiar with functional programming languages. Someday I would like to learn Haskell. 2. I've had a little experience with C, rewriting some parts of the standard library. Enough to have run into the problems of buffer overflows, having pointers attached to the same memory, double frees, maybe some of the other things Rust fixes. Also a little python for Data Analysis projects a few years ago. 3. Strongly typed languages: A little familiar with C. 4. Problems other than String vs str vs &amp;str? Just fighting with the borrow check quite often. I keep trying to do things you can do in C and then running into surprising walls. &amp;#x200B; What you are saying about trying to write in the Rust idiom seems right to me. I want to learn this language because it seems to do what C and C++ do, but in a safe way that prioritizes concurrency. I want to learn the things that make Rust different so I can think like a Rust programmer. &amp;#x200B; Honestly I am really almost a total newcomer to programming, with only a few basic concepts under my belt. Some of the first projects people are listing here makes me realize that a lot of people are coming to Rust with a lot of experience programming. &amp;#x200B; I don't even know what async is, or what all goes into writing a web server. I guess it is going to be a lot of work to try picking up all these different domains at once - programming generally, functional programming, concurrency, and Rust-specific stuff. &amp;#x200B; Where would you suggest going to learn more regarding idioms?
Looks like there are two videos in the playlist still marked as Private. Intentional?
Regarding the `extern crate` issue, wouldn't it be possible to make clippy complain about that? That would probably help the whole rust ecosystem to move away from it.
&gt; @rybot666 started working on porting the 16-bit assembly of the bootloader to Rust. ***WHAT‽*** That's madness, and also incredible!
&gt; The question is, how well does the ecosystem support building such an application? It appears like, if you wanna go this way, you have to hand-roll many things that you could automatically generate in other ways. The application was written in C++. Even the internal messaging protocol was hand-rolled...
You could try something like [enum_dispatch](https://crates.io/crates/enum_dispatch) if you want to go down the enum route but reduce the boilerplate.
I think I had a similar situation which I solved like this: let writer_store; let writer: &amp;mut Write = if buffered { writer_store = Either::Left(BufWriter::new(&amp;mut stdout)); writer_store.as_mut.left().unwrap() } else { writer_store = Either::Right(&amp;mut stdout); writer_store.as_mut.right().unwrap() }; So, `writer_store` is used to store the `Write` object and `writer` refers to it. Compared to the "Box approach" (see /u/FenrirW0lf's answer) it saves one dynamic memory allocation. But it's probably not that big of a deal. So, go with what seems most convenient to you.
RemindMe! 1 week "Did this ever get solved?"
I will be messaging you on [**2019-06-10 16:54:13 UTC**](http://www.wolframalpha.com/input/?i=2019-06-10 16:54:13 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/rust/comments/bw7qkq/per_crate_build_time_measurement/epwoahv/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/rust/comments/bw7qkq/per_crate_build_time_measurement/epwoahv/]%0A%0ARemindMe! 1 week ) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
Wow, thanks! I have reread your post about 15 times trying to come up with a way to make it work lol. Unfortunately, the way that LambdaCore evaluates a block of parsed code is: 1. Start with stack of Values. 2. One at a time, the stack is popped and evaluated. 3. Arrays are recursively-evaluated. 4. Identifiers are looked up. &lt;- Problem 5. Ending Parens trigger a function call. So, when a function gets called (Rust or user-defined), the arguments passed to that function have already been evaluated. Even if I create a new `Value::Macro` variant, it won't be able to receive it's arguments un-evaluated. For example: ```clojure (defm setq [key value] '[ (set (quote key) (eval value)) ]) :: `name` would be looked up (and trigger an error) before it could get to `setq` (setq name "Pebaz") ``` Unless I am overlooking something obvious I don't think I could implement macros without a significant redesign (i.e. have each and every function choose to evaluate their own arguments). In which case, functions would actually *be* macros at that point. Thank you for taking the time to explain, I really appreciate it!
You could use [`Either`](https://docs.rs/either/1.5.2/either/enum.Either.html) for this as it already implements the `Write` trait: let mut writer = if buffered_io { Either::Left(&amp;mut stdout) } else { Either::Right(BufWriter::new(&amp;mut stdout)) }; :-)
You could use [`Either`](https://docs.rs/either/1.5.2/either/enum.Either.html) for this. See my response to /u/georgm3010.
I'm working on [a daisy-chainable keyboard](https://gitlab.com/polymer-kb/polymer) with its firmware written in Rust. Each segment of the keyboard operates independently, scanning its matrix and sending just the key up/down events asynchronously over a USART connection to the "parent" with the USB connection. The "chainable" aspect comes in when you have more than two segments. Each board has two USARTs, so it needs to listen on one of them for events coming from other child boards while also scanning its own matrix at a timed interval, and send all of the events along to either the next child in the chain, or the parent if it's next. The parent owns the USB connection for the system and listens for events on both of its USARTs. Since the actual matrix scanning only needs to happen at a 1KHz frequency, and the translation of physical keypresses to actual keycodes to send isn't a terribly cpu-intensive process, it spends most of its time either waiting for the scan timer or in IO to/from other segments. The core logic is built using `Future` and `Stream`/`Sink` abstractions over the low-level interrupts for peripherals and timers, with `async`/`await` support provided by [embrio-rs](https://github.com/Nemo157/embrio-rs) and executed using my [embedded-executor](https://gitlab.com/polymer-kb/firmware/embedded-executor). A lot of it relies on the availability of an allocator, and it's almost certainly less optimal than a similar [RTFM](https://github.com/japaric/cortex-m-rtfm) app, but I really like the idea of using the same async/concurrency abstractions on embedded devices as are used for network services. I've only just recently gotten the firmware to a working state for an MVP, and I ordered PCBs and some other missing hardware bits last night, so I'm hoping to have a V1 complete by the end of this month, along with a more in-depth blog post :)
It is :D. See also https://www.reddit.com/r/rust/comments/ask2v5/dos_the_final_frontier/
I have been vaguely following your embrio project and you have my full support! I actually didn't know that embrio was the same guy as `embedded-executor`, but it's not too surprising. :P Can't wait for async/await on no_std natively rather than with your proc-macros!
Yeah, I didn´t think of that. Then it makes sense that it executed with 2 WS. Now that you say it I remember that I read somewhere that the LLVM optimizes more for performance than binary size. I also think that this slight increase is the result of this particular example. More benchmarks would be needed to say if LLVM/Rust is really always faster than GCC/C. Good idea. Trying Clang and comparing it sounds interesting. Maybe I´ll try that :)
I don't have a good mental model for when `RefCell`'s `borrow_mut` method can panic. Can someone give a code example? Since the type isn't Sync, seems like it shouldn't be possible to create multiple shared references...
No one ever said that asking is wrong, just that the answer is always "no, it's not okay" for the question you're asking. You've asked it again so it's not so clear any more whether your goal is learning or proving yourself clever enough to misuse the language. The arena reference lifetimes have already been manually tuned to be as long as they possibly can by people who understand the rules. Any "clever trick" you think of to extend them then by definition must be unsound. You aren't "tricking the compiler", you're only tricking yourself.
I'm not sure I understand completely, I meant that library developers would generally decide to expose \`std::error::Error\` types if they were to make new crates, you've said that The Core Team doesn't want to make it fundamental, are those statements exclusive? I still see libraries being published using \`error\_chain\` and \`failure\` I'm uninformed when it comes to \`no\_std\`, is anything akin to \`std::error::Error\` (or \`failure\`, etc..) used in those codebases? If so, what prevents \`no\_std\`/\`core\` from getting an \`error::Error\`? Thanks
Strange. This doesn't work either.
[Simple example](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=a62d855d4b0bd23b0d9c4af9cc9e0310)
Counterpoint is a word. And your counterpoint is a good one to consider
Yeah, /u/nemo157 is the real mastermind behind `embrio` - I've just been contributing :)
gtk-rs binds Gtk+ and Gio which are async-friendly. However, I predict that marrying Glib event loop infrastructure with Rust future API will be a struggle.
`core::error::Error` does not exist and neither does `core::io::Error`. `core::fmt::Error` *does* exist, as do a few other specialized error types, but as far as I know, there do not exist any general purpose error types in `core`. There's nothing preventing `core::error::Error` from existing beyond beaurocracy. `failure` does support `no_std`.
&gt; You've asked it again so it's not so clear any more whether your goal is learning or proving yourself clever enough to misuse the language. I'm trying to dig deeper and understand nuance. This is not about me "being clever", it is about solving a problem I have with the code. If I can get what I want by maintaining some invariants manually, that's still useful information to have. &gt; The arena reference lifetimes have already been manually tuned to be as long as they possibly can by people who understand the rules. Any "clever trick" you think of to extend them then by definition must be unsound. You aren't "tricking the compiler", you're only tricking yourself. I'm not trying to "trick the compiler". I don't even want to use unsafe! I just want to solve a problem with using the Arena properly. The whole question is about "how do I express what I want to the compiler while exposing a safe API".
I have a `$CARGO_HOME` variable in my environment when using this particular script, so it's not a bug for me. If you don't use `sccache`, then you can reduce the entire if statement to `exec $ARGS` in addition to getting rid of `$CARGO_HOME`.
&gt;Drug Wars in Rust I think your repo is private.
The last slides are wonderful, there there is so few of them: most of the deck appears to be dedicated to Rust itself.
I think this is already being explored; it's certainly the sort of thing that rust futures are supposed to be able to accomplish gracefully, due to the absence of any built-in executor or runtime.
Regarding errors and the `failure` crate: my recommendation is to define an enum for your errors and `#[derive(Fail)]` on that enum. All the options for deriving `Fail` on structs also work on enum variants. The reason to prefer this over simply returning a `failure::Error` is twofold: 1) `failure::Error` can hold any kind of error, so the kinds of errors which can occur are not self-documented. 2) If you use an enum, the caller can match on the error type to cleanly handle different kinds of error. Deriving `Fail` is very powerful: you can include backtraces and error causes in your enum variants, so no information is lost about the underlying issue. My guiding principle with errors is to make sure that the level of detail in my errors matches the level of detail in the API which returns those errors: you want to provide enough information to the caller that they can handle the error and recover, but any more than that should only be visible in the Display implementation.
Ooh. I've just started rust myself and I'm like 2/3 of the way through the book. Really enjoying it so far and learning a whole lot more about managing memory and stuff. Now I know how complicated strings really are 😂😂 Good to see more people as enthusiastic about it too. Will definitely check out you guys' channel.
I also came here to mention this! :)
Yes, so there should be a Glib runtime/executor that all asyncronous gtk-rs code should run in. But it may not be achievable without a significant redesign.
The EDIT solution is what I was looking for. Thanks. I didn't realize before that just declaring the variable is fine.
I thought this would be of interest to the Rust crowd, given how many of us are interested in Rust GUIs, ad how similar Swift is to Rust.
Turns out that I've managed to brake the `.rustc_info.json` somehow. `cargo clean` fixed my issue.
Awesome! We've been really enthused to find that the Rust community seems to be so welcoming to mere mortals coming from high-level scripting languages. It's great to know there are others like us out there!!! 🤓👏👏👏
How do I make a hyper service where I have a request whose Response doesn't fit in memory and it runs on its own thread?
Thanks again for an idea. I've posted a solution in the opening post.
You still need `macro_use` for some (pre-2018) macros, if they expect their sibling macros to be in global scope.
I think that there is no real idiomatic UI framework for Rust yet, so this might be quite interesting. Or am I mistaken?
Oh 💯. Just from browsing this subreddit it seems to be a pretty good community. That's definitely something that can make/break it and the rust team themselves seem to be pretty good with this too. oh and I finally understand why people love the rust compiler it's been soo good 😍. The errors are so nice 😂
What you want is not safe. You cannot express what you want to the compiler because the compiler does not control the lifetime of references, you do. The compiler only stops you from introducing memory safety violations which is what you're attempting to do here. You cannot store references into that arena in that struct (unless you use `rental` or copy exactly what it does for some reason).
you probably want r/playrust
Yes, that's the opening an closing aka. "here's the toilets" and "here's the restaurants around the venue". I actually need to remove them from the playlist.
Ah whoops, changed it to public. Hopefully my code's not too embarrassing :D
Sampling synthesizer. Hoping to finish something demoable today, as I'm supposed to speak about it Wednesday evening. :-)
Why would we want to move away from it?
derive_builder is super nice! Thank you!
GitHub: [https://github.com/Gymmasssorla/anevicon](https://github.com/Gymmasssorla/anevicon)
HOLY SHITE... OS in rust would be too OP
Defining ones own tags **is** interesting, but it's too low level for a DSL. I think that this DSL has to be a high-level **declarative** (describing what you want, not how) language, which means that the actions for event handlers is the only ones that has to look like code. ... and BTW , I had read that Web Components is a mistake (to work with and use it), but i can't - of course - find the source of that, when i need it.
Thanks so much for making these, I don't have time to watch them all but when I do get to watch one I learn so much.
I glanced over your code and I saw some things that you could improve, I hope it helps. You should definitely try Rusts enums instead of matching on strings. You can implement the `FromStr` trait for an enum with the locations for example, and then call `LocationEnum::parse(&amp;input)`. This gives you a `Result`, which removes the need for a "null" value. Also, instead of having `dump` functions, you could implement the `Display` trait for your structures and just do a `println!("{}",x)`.
I thought Cloud9 was just an esports team! lol
[Serde](serde.rs) Serializer and Deserializer traits for [rustler](https://github.com/rusterlium/rustler) types, so you can easily serialize and deserialize native Rust types directly to and from native Elixir terms within your NIFs. Hope to have something to show by the end of the week!
My learning-rust project that is also useful for work is making a faster version of exonerate, a tool for protein to DNA alignment across (and within) species. It's used for gene annotation / prediction but is very slow and not updated anymore. There's some alternatives but none have the performance. Making it multi-threaded and supplying some more modern concepts to speed it up. Still very much a WIP but thanks to r/rust I've gotten over many of the learning speed bumps and found faster ways to do things.
I'm a simple man: I see Phobos and I upvote. Seriously, it's great to see that this fantastic project is actively maintained. Thanks so much for it.
Awesome! &amp;#x200B; For each of you, what was the main motivation for choosing to learn Rust? Was it to contribute to deno? Or are there more reasons?
Compile in release mode :P
&gt; Weird File API I don't think so, you didn't highlight what's trouble you &gt; Literally no chown? https://doc.rust-lang.org/std/fs/struct.File.html#method.set_permissions https://doc.rust-lang.org/std/fs/struct.Metadata.html Didn't this suit you ?
As metal is somewhat similar to Vulkan, would it make sense to Port Piet-metal to Piet-vulkan for Windows &amp; Linux systems?
Then you'll love [Redox](https://www.redox-os.org/)
Hey! I noticed you guys are in the DC area—if you're ever able make it to the Rust DC meetup (https://www.meetup.com/RustDC/), would be cool to say hey in person! (Our next meetup is a week from Thursday.) We're a pretty informal group. But if you can't make it to Ballston Thursday nights, def still join us on Zulip (https://rust-dc.zulipchat.com)!
&gt; We're stoked to learn more about systems programming and WebAssembly, and we're both potentially interested in trying to try to land some PRs into [Ryan Dahl's Deno](https://github.com/denoland/deno) \&gt; newly spawned webshits \&gt; wanna work on big webshit project \&gt; haven't even looked at what some of the current PRs are \&gt; 🆗️
Checkout [Redox OS](https://www.redox-os.org/), subreddit is r/redox
https://areweguiyet.com/ Everything has severe limitations right now.
You're on the rust thread, not the pcj one
`Wrapping&lt;T&gt;` type has a `#[repr(transparent)]` attribute meaning it will directly compile into `T`. IMO for use in hashing function I would just Wrapping. I wouldn't depend debug/release overflow checking.
I have another lifetime question :( The goal is I'm making a new `Iterator` called `EntityIterMut`. The iterator moves along a `Vec&lt;Option&lt;...&gt;&gt;` list, skipping any index that contains `None` and returns the next `Some(...)` value. Inside the options is a `&amp;'a mut Box&lt;Entity&lt;T&gt;&gt;` where `Entity` is a trait, that detail is probably not important, but maybe you're wondering why I'm using a Box. Code: impl&lt;'a, T&gt; Iterator for EntityIterMut&lt;'a, T&gt; { type Item = &amp;'a mut Box&lt;Entity&lt;T&gt;&gt;; fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; { loop { if self.curr &gt;= self.vec.len() { break None; } let result : Option&lt;&amp;'a mut Box&lt;Entity&lt;T&gt;&gt;&gt; = self.vec[self.curr].as_mut(); self.curr += 1; if result.is_some() { break result; } } } } Error: error[E0495]: cannot infer an appropriate lifetime for lifetime parameter in function call due to conflicting requirements --&gt; src\core\iter.rs:119:59 | 119 | let result : Option&lt;&amp;'a mut Box&lt;Entity&lt;T&gt;&gt;&gt; = self.vec[self.curr].as_mut(); | ^^^^^^^^^^^^^^^^^^^ | note: first, the lifetime cannot outlive the anonymous lifetime #1 defined on the method body at 114:5... --&gt; src\core\iter.rs:114:5 | 114 | / fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; { 115 | | loop { 116 | | if self.curr &gt;= self.vec.len() { 117 | | break None; ... | 124 | | } 125 | | } | |_____^ note: ...so that reference does not outlive borrowed content --&gt; src\core\iter.rs:119:59 | 119 | let result : Option&lt;&amp;'a mut Box&lt;Entity&lt;T&gt;&gt;&gt; = self.vec[self.curr].as_mut(); | ^^^^^^^^ note: but, the lifetime must be valid for the lifetime 'a as defined on the impl at 111:6... --&gt; src\core\iter.rs:111:6 | 111 | impl&lt;'a, T&gt; Iterator for EntityIterMut&lt;'a, T&gt; { | ^^ = note: ...so that the expression is assignable: expected std::option::Option&lt;&amp;'a mut std::boxed::Box&lt;dyn entity::Entity&lt;T&gt;&gt;&gt; found std::option::Option&lt;&amp;mut std::boxed::Box&lt;(dyn entity::Entity&lt;T&gt; + 'static)&gt;&gt; Sorry if this is a mess, let me know if I could explain something more. I need to watch a video on rust lifetimes again lol. Thank you!
More formally. When I get a `Request` from hyper, how do I create a `Response` whose body is gradually fulfilled by another thread?
&gt; Multi-type errors `derive-more` to derive more, especially error traits. Some people considers `failure` to be a bad practice. Why using boxed types instead of raw structs? (Look at `failre::Error` inners). &gt; Returning iterators It's a generic error for returning traits instead of structs. Maybe they should make error message better and/or create a special case for iterators (and futures, of course. And maybe other types). &gt; WTF is AsRef? Just a type that can give specific reference. Since it's a reference it doesn't alloc anything so it's cheap. I didn't look at source, I just analyse types, considering borrowing rules. &gt; Weird File API Look completely fine to me. Some examples? Also, I'm not sure about node development, but in backend languages world you just type things and IDE adds all imports. It doesn't really matter how devs place their structs, you just should know what you want to do. &gt; Literally no chown? Why you think it should exist? How should it work on Windows? on `wasm-unknown-unknown` target? MIPS? &gt; Everyone uses extern crate Not be more &gt; No cargo version built-in Type `cargo install cargo-edit`, then just `cargo upgrade`/`cargo add serde_json`/... &gt; No nyc equivalent for coverage In most cases you don't need code coverage for static analyzed languages. If your code compiles then it's just like 100% coverage, otherwise it won't :) I played with code coverage tools in static analyzed languages and never found them useful. Merely a numbers, funny to see them but completely useless.
See "path clarification" part of rust 2018. It's just more clear and consistent with the rest. Of course it's better to move here from the more obscure and complex approach.
We are both in DC! 😁 Kait and I were actually discussing your meetup today, and we're interested if we can get the logistics to work. Thank you so much for reaching out! 🦀💖🙌
Why does the following code not compile... #![feature(never_type)] struct S&lt;'a&gt; { v: Vec&lt;&amp;'a u64&gt;, } fn g() -&gt; Result&lt;!, ()&gt; { Err(()) } impl&lt;'a&gt; S&lt;'a&gt; { fn f(&amp;self) -&gt; Result&lt;&amp;'a u64, ()&gt; { if self.v.len() == 0 { g() } else { Ok(self.v[0]) } } } The error message says - = note: expected type `std::result::Result&lt;&amp;'a u64, _&gt;` found type `std::result::Result&lt;!, _&gt;` which makes no sense to me. Is `!` not a proper bottom type?
From what I can tell from the `#[repr(transparent)]` [docs](https://doc.rust-lang.org/1.25.0/unstable-book/language-features/repr-transparent.html), its main purpose is allowing the use of the inner types in FFI function calls while encoding additional type information on the Rust side. Even just a regular struct should have no additional overhead.
Ok. I'm trying to use `Wrapping` within the code. So far it seems ok, but I have a followup question, do you know how to make an array literal where all the elements are wrapped? This is a truncated array: &amp;#x200B; const K: [Wrapping&lt;u32&gt;; 64] = [ 0x428a2f98, 0x71374491, 0xb5c0fbcf, 0xe9b5dba5, 0x3956c25b....] I don't want to have to put `Wrapping` around each element.
I think Druid or Azul are the most compelling in terms of a viable long term future. However I'm most keen on Druid. I've been doing GTK programs in Rust and tracking global state is incredibly painful. I have some bugs due to shared state and data races, and I dont know if it's solvable with GTK. Instead I'm more interested in a different framework that makes it easy to write correct GUI code.
Sadly, the best you can do is alias Wrapping to W. - `use std::num::Wrapping as W;` or - `type W&lt;T&gt; = Wrapping&lt;T&gt;;`
Tee hee wow such quirky h4x0rs!! 👏👏👏😜🤪
I just checked and according to [this](https://github.com/rust-lang/unsafe-code-guidelines/issues/34) you are right.
Or have fun with macro!
You could write a simple macro that wraps a list of things each with Wrapping.
I have an internal library that I need to cross-compile to wasm for one of its use cases. It has an indirect dependency on sodiumoxide, which is an ffi wrapper around libsodium. I have managed to get wasm32-unknown-unknown working manually for libsodium and hook it up with stubs in place of sodiumoxide, but that's a one-off solution, and I need this to work as a stable target specification. I'm starting to look at how surmountable creating a "linkage" for bindgen and running a special build step would be...
It's useful for NFS, and not very useful anywhere else.
You could make a [simple macro](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=03c71761ac846005de15a2b2517dccb0)
I"ll give it a shot
My understanding is that piet-metal uses Vulkan 1.1 feature (subgroup) and can't be ported to Vulkan 1.0.
A lot of people have been saying that Rust is not well-suited to programming competitions, where you need to hack together a working solution under time pressure. Having tested this hypothesis on Codeforces, I'm inclined to disagree. After all, in contests, correctness is absolute, and we can't afford to waste time on mysterious bugs! &amp;#x200B; I think the issue here is there weren't enough examples available to help people climb the learning curve. So, I took it upon myself to create those examples :D
Been using SwiftUI internally for a little while and it's certainly a very nice API to write with. I would definitely like to see a Rust Gui library that was similar in style. Not sure how viable it would be, but so far I haven't seen anything that couldn't be done with more rust idiomatic syntax. Of course the actual Gui aspect also needs to be done.
Not _really_. However, you can get away with replacing `!` with a type parameter: fn g&lt;T&gt;() -&gt; Result&lt;T, ()&gt; { Err(()) }
The problems that you have are recognized in general as the "leaking abstraction problem". No matter how many abstractions you put on top of each other, you need to be able to work with the levels below. This is another argument in favor of modular architecture. However, the most important thing is to focus on what makes you productive, which is sometimes not the same as what API designers think it is. I believe you did the right choice of jumping straight into Glium. You learn to "do it yourself" and you get more control. This is often significant for productivity. I think there are ways the points you mentioned can be dealt with in some way in Piston, however I am unsure if it would save you time, depending on the complexity of your needs. For example, re-balancing the event loop can be done using this settings: https://docs.rs/pistoncore-event_loop/0.44.0/event_loop/struct.EventSettings.html#structfield.ups_reset. Piston also has a Glium backend for 2D, but it is less maintained than the others.
Looks interesting, but what advantages does Persy offer over Sqlite?
I found code coverage extremely useful for Rust. Rust prevents type errors, but can't prevent logic errors.
Can't offer much advice as I am also in the same boat as you, but just wanted to say don't be afraid to ask questions in the easy questions thread (it recycles every week but [this week](https://www.reddit.com/r/rust/comments/bw7t49/hey_rustaceans_got_an_easy_question_ask_here/)). They are super friendly and patient, and willing to type out multiple-paragraph answers to my most banally idiotic questions.
But pythons are harmless! (Except Terry Gilliam, he'd kick your ass)
Hmm, do you know why it doesn't work? I thought the whole point of special casing `!` was to enable these kinds of functions.
One thing to note is that some of these competitive programming sites have outdated versions of the Rust compiler! Timus OJ, for example, rejects about half my solutions due to the fact that it's running Rust 1.25. (That's an improvement, BTW - a week ago it was running 1.14!)
You unfortunately can't return mutable references to `self` from `Iterator::next()` without unsafe code. The function signature just doesn't hook the lifetimes up how you need them to, to make this work. The `IterMut` iterator for slices (which `Vec::iter_mut()` uses) uses unsafe code internally because the individual elements are independent of each other so all of them can be returned at once without conflicting borrows, so you want to build your abstraction on top of it instead of reimplementing it yourself. You can do this easily with iterator chains: pub fn entities_mut&lt;'a&gt;(&amp;'a mut self) -&gt; impl Iterator&lt;&amp;'a mut Box&lt;Entity&lt;T&gt;&gt;&gt; + 'a { // `.flatten()` turns an iterator of iterables into a single iterator by chaining them // `&amp;mut Option` actually implements `IntoIterator` (giving an iterator which yields 0 or 1 elements) entities.iter_mut().flatten() } However `impl Iterator` isn't a type you can place in a struct so if you want a nameable type you have to create your own wrapper: use std::{iter, slice}; pub struct EntityIterMut&lt;'a, T: 'a&gt; { inner: iter::Flatten&lt;slice::IterMut&lt;'a, Option&lt;Box&lt;Entity&lt;T&gt;&gt;&gt;&gt;&gt;, } impl&lt;'a, T&gt; Iterator for EntityIterMut&lt;'a, T&gt; { type Item = &amp;mut Box&lt;Entity&lt;T&gt;&gt;; fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; { self.inner.next() } }
Oh, perfect. Thanks!
This comment made me feel old
Avoid the global state. Use channels to make your widgets stateless. Manage application state in a single location. Share a single Rc&lt;State&gt; structure with the signals that need it.
Because you thought it was just a [pokemon ability](https://bulbapedia.bulbagarden.net/wiki/Cloud_Nine_(Ability\)), right?
Great question! For me, I would say the biggest attraction is to find a systems programming language with modern conventions and nice tooling, including package management and WebAssembly. I'm using my GI Bill to go back to CS grad school (which I started this past January), and I had to self-teach C for some class projects using K&amp;R. I appreciate C, but working in the language feels like Segfault Minesweeper. I also did a WebAssembly project with Emscripten, which was quite painful. My hope is that Rust offers a kinder onramp and makes me a bit more productive in my graduate coursework. I perceive that the Rust ecosystem is much more standardized and approachable versus C++ for example. Finally, I help and mentor other career changers through Operation Code, which helps military vets and spouses learn to code, so I'm also hoping to find a systems programming language that offers a reasonable onramp for folks that haven't had traditional CS training yet have perhaps a coding bootcamp education and a year or two of experience with React/Angular + Node.js. Many career changers land in fullstack JavaScript solely because of bootcamp curriculum and the availability of junior dev roles, and some of those folks really would prefer to transition into something lower level. I'm hoping Rust is a reasonable path for folks like that to follow their bliss without getting crushed. In terms of Deno, I'm attracted to that particular project mostly because I'm still plugged into the fullstack JavaScript ecosystem, I enjoy TypeScript, and I am interested in learning more about compilers and programming languages. It seems like a reasonable place where I might be able to make a difference. That said, my mind is open, and I'm looking forward to exploring the Rust ecosystem for other opportunities. By biggest goal is to grow as a journeyman developer and meet other interesting devs. All the rest is gravy. Kait isn't on Reddit currently, but I'll paraphrase her answer from our first video. She views learning Rust as a good way to broaden her horizons by learning a language that greatly differs from JavaScript (AOT versus JIT, strong versus weak types, manual memory management versus GC). She's attracted to Deno basically for the reasons I outlined above: It's a chance to leverage some of her experience with Node.js and TypeScript while authoring commits in a new language at a lower level of abstraction. Plus, she and I are friends that don't get to work together now that I'm back in grad school, so this gives us a chance to keep collaborating.
I have been following the project and think you are doing some great work. I would like to have an API that depends on the Piston core instead of Piston-Window. So, I made a PR: https://github.com/KenSuenobu/rust-pushrod/pull/138 I also think the event loop problem is interesting and maybe there should be a new mode added to Piston's event loop to reduce the overhead.
I did not get all the slides up in the browser, so I downloaded to view the rest.
This is interesting! Is it based on the previous work described in the blog post https://raphlinus.github.io/rust/graphics/gpu/2019/05/08/modern-2d.html?
Thanks so much for the feedback, I really appreciate it. I didn't quite understand what Enum's are/what to do with them and this explanation has helped. Will look into Display as well, thank you again for the feedback!
Still working on my ant colony optimization, which will hopefully be useful for GTOC in less than two days. Then I'll working on reference frame transformation for my astrodynamics toolkit. &amp;#x200B; I'm also considering asking Seb Crozet whether there would be any need for an nalgebra based propagator/integrator library and whether he would like to have that filed under \`rsim\` on Github. I'd be stoked!
&gt; every number in my array needs to be a Wrapping struct? That seems like a lot of overhead. What makes you say this? The other comment mentions `repr(transparent)` but even without that why would it cause overhead?
Yes it is.
With Tokio, can you have multiple Futures poll a single Stream? Or should it just be one Future polling it at a time?
I don't know swift, but it doesn't have ownership like rust does right? My understanding is that it's garbage collected and I'd assume that means it also doesn't have the same strict rules about shared data. I'm assuming ownership and borrowing in rust is what makes GUI frameworks hard and so swift patterns might not apply.
What is your userChrome.css for putting the firefox tabs on the bottom of the screen?
Hey thanks I will use that! &amp;#x200B; I just discovered the rust IRC channel: [https://chat.mibbit.com/?server=irc.mozilla.org&amp;channel=#rust](https://chat.mibbit.com/?server=irc.mozilla.org&amp;channel=%23rust) &amp;#x200B; I keep that open in my browser and pop in to ask questions, and people seem willing to look at code in the rust playground and offer advice, I would certainly recommend it to someone else just starting out.
Nice to see work on Ruma continue! Looking forward to trying it out.
&gt;I don't know swift, but it doesn't have ownership like rust does right? Correct. There are plans to add an ownership model based on Rust's that can you can opt-in to, but it's not here yet (although the Swift standard library is already full of `__consuming` annotations). &gt;My understanding is that it's garbage collected It uses reference counting, which technically is a form of garbage collection. &gt;I'd assume that means it also doesn't have the same strict rules about shared data. Indeed.
I'm working on something relatively complicated in Ruby right now, and I just have this fear of the bugs that I know I'm going to write that I'll have to track down and fix later, stuff that would be caught in Rust or any other statically-typed language. On the other hand, what I find quite liberating when I'm writing Ruby is the feeling that is ok if my code is inefficient. There's this sense that the whole ecosystem prioritises getting a simple solution that works as painlessly as possible, and only worrying about performance when you really need to.
Happy to see others new to the language sharing their experiences! &amp;#x200B; I've just started getting into Rust, working on my first project now, and am interested in following the progress of others learning the language to see what other folks find helpful. Do either of you have a blog where you might be able to post shorter updates or the resources you found to help you the most? I would be super stoked to even see a section in your videos where you provide links to helpful resources you use on your journey to learning more Rust. &amp;#x200B; Probably the most helpful resource for me right now is the Rust IRC channel: [https://chat.mibbit.com/?server=irc.mozilla.org&amp;channel=#rust](https://chat.mibbit.com/?server=irc.mozilla.org&amp;channel=%23rust) I often get on when I get stuck, and some more experienced Rust programmer points me in the right direction or even looks at my code to tell me what's wrong. &amp;#x200B; Good luck learning Rust!
I'm 40 I'm just also 12
What you're describing is exactly what Vue code looks like if you're using a prebuilt widget library.
but code coverage doesn't prove the check every condition. But that give some hint I agree.
Actually no chown, this could be implemented for \*nix target compile. [https://internals.rust-lang.org/t/no-chown-or-chmod-in-std-lib-still/10295/4](https://internals.rust-lang.org/t/no-chown-or-chmod-in-std-lib-still/10295/4), funny how someone ask for it recently.
Looking for /r/playrust
Are there any how-to guides that explain how to take a project and split it into multiple crates? For instance, I'd like to take my Pushrod project and split it into two crates: `pushrod-core` and `pushrod-widgets`. This way, I can manage core and widgets separately, and optimize each one separately. Thanks in advance!!
Makes me wonder what Linus Torvalds thinks about Rust for OS writing...
It might have meant that five years ago. Since then they've explicitly taken a hands-off approach and do no curation of games. At all. Not even to make sure they're finished.
Considering his hatred of C++ and his refusal to use it in the kernel, probably not much.
I'm saying that the hardware exposes an asynchronous interface to the disk and the kernel then blocks / switches to other processes to present a synchronous interface.
[removed]
This feels like how a correctly built Rust GUI would handle this. I just don't like that it's done through convention, I want a GUI framework where this is \*the\* way it's done and it's the \*obvious\* way to do it.
That is so fucking dope thank you so much, u/tacokingyo and u/CrazyKilla15. 10/10 will try.
That's what I've done, however it doesn't seem to have been done correctly. I've tracked it in [this issue](https://gitlab.com/susurrus/gattii/issues/27). An additional tricky thing with application state is that widgets generally store state in addition to the application, which makes it harder to keep state in sync. I have this in my app where I need to populate a dropdown dynamically, and it's pretty unergonomic to manage it.
I'm going to adapt the code to use threadgroups as the main interthread communication mechanism, with subgroups as a performance tweak. That should help a lot with portability to much larger classes of existing graphics cards. And yes, /u/bzm3r is working as a Google Summer of Code student with me to port the prototype to Vulkan.
The GPU rendering is, yes. There's a lot of other stuff too, like the abstraction that allows the use of platform rendering engines, which I think is a valuable feature, as it gets you on the screen with much less code than having to ship a rendering stack, not to mention stuff like text matching appearance with other apps on the system.
Thanks! In particular, I'd love to see experimentation of a React-style layer in Rust on top of a retained widget hierarchy (that's basically how all these things are built, and many signs point to SwiftUI being similar). I'm not planning on doing any of that experimentation myself, as I've got my hands more than full building out the lower levels of the stack.
looks like it's basically React for Swift.
I'm not sure a language dedicated to GUI is the way to go. Sun made a specific scripting language for JavaFX a first, but they came back to regular Java.
IIRC, he's been asked before and he basically said "Rust may be great but it doesn't matter to the Linux kernel because we're never gonna use anything but C."
Competitive programming is not an area of activity I have any familiarity with, but I think your points are extremely valid and articulated beautifully. Go you!
Essentially everything in swift is in an `Rc` . That's one of the harder things to make a UI library like this in rust, but not impossible either
What do you do when you need a 2d array, write one from scratch every time? or always use Vec&lt;Vec&lt;?. Also casting slice indexes to usize is not nice.
SwiftUI is not a language, it’s a library.
Trying to make a reverb? Fair warning, I believe partitioned convolution is patented (iirc one of the patents ran out recently). For partitioned convolution (today, 6/2019) your best bet is probably to use the [ipp-sys](https://crates.io/crates/ipp-sys) crate (must install the IPP static libs for your system, it's liberally licensed if still closed source) and use the FFT implementation there as your base. [Here's a decent paper on implementing it](https://pdfs.semanticscholar.org/2e4c/2797267dd50850eb38fd9d33b9ad7f8d407c.pdf) (pdf warning). If you're not doing reverb, my personal rule of thumb is that &lt; 128 length convolutions are faster using linear convolution in time. Benchmark to be sure. It's pretty trivial to do by hand, and straightforward to vectorize using SIMD. [Here's a C implementation of linear convolution, couldn't find pseudo code](https://stackoverflow.com/questions/8424170/1d-linear-convolution-in-ansi-c-code). If you _need_ more than 128 sample convolutions then resort to an FFT. Since the audio DSP story in Rust is nascent I'd recommend going with something simpler than partitioned convolution, like for example [feedback delay networks](https://ccrma.stanford.edu/~jos/pasp/History_FDNs_Artificial_Reverberation.html), the [Dattorro paper](https://ccrma.stanford.edu/~dattorro/EffectDesignPart1.pdf) has a classic implementation, and last year [Ian Hobson from Ableton gave a talk where he implemented the Freeverb algorithm in Rust](https://www.youtube.com/watch?v=Yom9E-67bdI). Sorry I can't point you to something that "just works." But if you do roll your own, please publish a crate and post about it on the [rust-audio forum!](https://rust-audio.discourse.group/)
C++ is/was a convoluted mess, so no big surprise there.
Agreed, a fully async first stack would be better. Anyone up for writing a new non-POSIX OS kernel?
I am aware of the 2018 changes. But if many people continue to voluntarily use `extern crate`, perhaps *that* part of the original module/path system was not as complex or obscure as claimed/expected. Or perhaps the `extern crate` declarations are combined with `macro_use` to support older compiler versions (the macro exports weren't always in the 2018 edition, as I recall). Regardless, I don't see the harm. This confusion amongst newbies is surely primarily a consequence of having multiple editions (which means all education material before the 2018 edition is suddenly 'wrong) and an evolving language? Forcing people to stop using `extern crate` through lints is not solving that root cause, and will probably piss off people who use it for backwards compatibility reasons.
If it's for matrix operations, just use nalgebra?
So I have been sprinting and now when I sprint it’s really slow why is thay
Note that ‘crate::macro’ imports all items named ‘macro’.
With some efforts I can sustain myself with writing C++ (in CP), these are: - No or less global variables - Use vector instead of raw array - Use address sanitizer and UB sanitizer - -Wall -Wextra - Slow down and do some manual borrow checking As much as I am a fan of Rust, the benefit isn't worth switching IMO.
Meanwhile I found some piece of Linus interview where he says a word about Rust https://www.infoworld.com/article/3109150/linux-at-25-linus-torvalds-on-the-evolution-and-future-of-linux.html &gt; *What do you think of the projects currently underway to develop OS kernels in languages like Rust (touted for having built-in safeties that C does not)?* &gt;That's not a new phenomenon at all. We've had the system people who used Modula-2 or Ada, and I have to say Rust looks a lot better than either of those two disasters. &gt;I'm not convinced about Rust for an OS kernel (there's a lot more to system programming than the kernel, though), but at the same time there is no question that C has a lot of limitations. &gt;To anyone who wants to build their own kernel from scratch, I can just wish them luck. It's a huge project, and I don't think you actually solve any of the really hard kernel problems with your choice of programming language. The big problems tend to be about hardware support (all those drivers, all the odd details about different platforms, all the subtleties in memory management and resource accounting), and anybody who thinks that the choice of language simplifies those things a lot is likely to be very disappointed.
I don't disagree, but the nature of his objection to including it didn't seem to end there
Interesting. Thanks. &gt; The big problems tend to be about hardware support (all those drivers, all the odd details about different platforms, all the subtleties in memory management and resource accounting), and anybody who thinks that the choice of language simplifies those things a lot is likely to be very disappointed. This is probably true in general but seems like it misses the point a bit. The things Rust outright prevents aren't things that are particularly *hard* to do right in C; it's just that it's also easy to do them wrong, so lots of trivial issues come up. I don't think Rust is about doing hard things more easily; I think it's about doing them with fewer mistakes.
These are, unless I'm mistaken, identical programs in C and Rust. C version works while Rust version does not...? C: #include &lt;ifaddrs.h&gt; #include &lt;stdio.h&gt; #include &lt;errno.h&gt; #include &lt;string.h&gt; int main(int argc, char *argv[]) { struct ifaddrs *ifaddr; int ret = getifaddrs(&amp;ifaddr); printf("ret = %d\n", ret); if (ret != -1) { freeifaddrs(ifaddr); } else { printf("err = %s\n", strerror(errno)); return 1; } return 0; } C-Result: /tmp $ gcc test.c /tmp $ ./a.out ret = 0 Rust: use libc::{ifaddrs, getifaddrs, freeifaddrs}; fn main() { let mut addrs: *mut ifaddrs = unsafe { std::mem::uninitialized() }; let ret = unsafe { getifaddrs(&amp;mut addrs) }; println!("ret = {}", ret); if ret != -1 { unsafe { freeifaddrs(addrs); } } else { println!("err = {}", std::io::Error::last_os_error()); } } Rust-Result: $ cargo run Compiling foo v0.1.0 (/tmp/foo) Finished dev [unoptimized + debuginfo] target(s) in 0.22s Running `target/debug/foo` ret = -1 err = Operation not supported (os error 95) This is the root cause of some networking code example not working for me. Does anyone have idea why `getifaddrs` does not work in Rust? Thanks.
Use ndarray: https://docs.rs/ndarray/0.12.1/ndarray/
Competitive programming platforms typically only accept solutions as a single source file and don’t provide you with a way to depend on libraries outside the standard library.
Could be worse. CodeChef is running Rust 1.14 [in debug mode](https://discuss.codechef.com/t/bug-report-rust-code-is-being-compiled-in-debug-mode-without-optimization/24530). Lack of compiler optimizations often makes it hard or impossible to write solutions that run within the time limit. :-/
In competitive programming you usually not allowed to use third party libraries. So you can't even generate a random number, unless you write your own LCG.
Ah, I haven't tried competitive programming and wasn't aware of that constraint. Thank you.
Just published the first version of [libss](https://github.com/amousa11/libss), a secret sharing library. I'm working on digital signing the shares to guarantee integrity. I also plan to learn more about lifetimes and borrowing this week.
Far more nuanced than how people requote him. And he's very much right, but that said there's been so many vulns that have sat in the kernel for years undiscovered, for a lot of people it's becoming untenable in $currentyear. When the most looked over C codebase on Earth has use after free bugs that take a decade to be found it speaks volumes about how hard it is to write correct C code. I've never tried redox but the world could very much use an alternative kernel.
I was pretty serious about competitive programming (I was in the team that won ACM in 2000), and I still sometimes participate in contests. I think Rust is a very good option for marathon contests like those on CodinGame, but for shorter contests I would definitely prefer Python. It has a lot of idioms that simplify algorithmic code, and allows very consise and clear implmentations. If you are spending half of the contest time in debugging your code, than you are probably doing something wrong.
You're allowed to use reference code. I have prepared code for linear algebra, which is smaller than (albeit not as nice or fully-featured as) a linear algebra crate: [https://github.com/EbTech/rust-algorithms/blob/master/src/math/num.rs](https://github.com/EbTech/rust-algorithms/blob/master/src/math/num.rs) &amp;#x200B; And if you're not doing linear algebra, it's easy. Here's a bit of 2D dynamic programming: [https://codeforces.com/contest/1168/submission/54903799](https://codeforces.com/contest/1168/submission/54903799)
SPIR-V?
&gt;I thought this would be of interest to the Rust crowd, given how many of us are interested in Rust GUIs, ad how similar Swift is to Rust. Maybe I am misjudging the community, but I'd have thought: not many and not very!
Judging by the project structure, none. It's a virtual filesystem, not database... Nowhere do I see it being advertised as superior to databases either.
Isn't SPIR-V designed more specifically for shaders? I want something that is compute focused, something like Java bytecode but with built-in instructions for working with work-items.
&gt;Assuming ownership and borrowing in rust is what makes GUI frameworks hard I guess if instead of references you used IDs, it could work.
Apple's answer to Flutter? Has the same hot-reloading behavior, and from a bird's eye view, the code samples look very similar.
This might be one of the nicest things Torvalds has said about any language other than C.
Interesting. Having worked with UDP a lot in various roles, I think this would have been useful in various testing scenarios that I have been through. The feature I'm missing is to be able to have the tool send more than one message. For example, since UDP has no re-transmission logic, messages will often contain a sequence number. Similarly, if I want to test lookup or logic inside the recipient, I will want to vary the messages so as to defeat any caching in the recipient. &amp;#x200B; Also, when reading the source, I was confused that the folder containing most of the implementation was called "testing". &amp;#x200B; Good work!
I started parsing binary [Blender](https://www.blender.org) files to feed the data to my [renderer](https://www.rs-pbrt.org/about). See issue on [codeberg.org](https://codeberg.org) for details: https://codeberg.org/wahn/rs_pbrt/issues/2
Where can people help with druid/piet/skribo/kurbo and where do people need to wait?
I know, and that's true, but the synchronous interface isn't too bad given how fast modern disks are, is it?
To make it easy to add extra commands in the pipeline and to keep the same order as with the cat example.
It wastes performance by having to copy everything an extra time and you have more context switches. All for no reason at all. You can just as easily add more parts to the pipeline when writing in the redirect format (that's why I used it instead of the normal argument format). You also have to write four more characters when using cat. :-P
Short answer: best way for contributors to get on board is to stop by the [zulip](https://xi.zulipchat.com). There are a few issues in the github tracker that might be starting points. Also, we're specifically looking for an owner for the X/Linux port, which is currently in a fork. A great starter issue on that is scroll wheel support. I don't have a roadmap written down yet but there's some thinking in [this zulip thread](https://xi.zulipchat.com/#narrow/stream/197829-runebender/topic/The.20runebender.20repo.20begins) (login required).
Is there a way to detect memory allocation errors when I'm pushing to `Vec`, making a `Box`, etc?
Nice project! Although the website color is quite disastrous. It is a file system, but looks like sqlite, not sure it provided any POSIX like fs API? I made a similar fs, also support transaction but more focused on privacy. ZboxFS: https://github.com/zboxfs/zbox
yea, this did end up focusing more on the rust stuff since it was for a decidedly non-rust audience. Another issue is that these slides are missing a lot of the context from the presentation itself; most of the (to me) most interesting stuff was discussed, but hard to get from the slides alone.
I've always wanted to do something like this, so I'm writing a bunch of pathfinding algos and a program that shows them working: [graphical-pathfinding](https://github.com/raybritton/graphical-pathfinding) . I've got a bunch of plans for it but I've run into an issue where I can't find any graphics libraries that support resizing so currently it's fixed at 1920x1080
Sure, Hyper even provides `Body::channel()` which is designed for this; its API assumes asynchronous usage but if you import `futures::Sink` you can call `.wait()` which turns it into a blocking sender: use hyper::{Body, Response}; use futures::Sink; let (sender, body) = Body::channel(); let response = Response::new(body); // return `response` from your service function // on the other thread let chunk = Chunk::from(some_bytes); if let Err(e) = sender.wait().try_send(chunk) { // connection was closed, return from thread }
It would be nice, if there was a cargo tool, which just packed all of the Rust crates into a single file for this usecase.
This is a storage, is the bottom part of the database,if you add on top of this a SQL query engine and a record format like tuples, you can have a database like sqlite, this is just one of the underlying blocks that give you transactions, transactions isolation, and crash resilience using logs and copy on write.
&gt; So you can't even generate a random number [You just have to get creative](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=9336b768ab01f03d08a7861143a0cca8) ;)
Yes you are right it looks like a file system but my scope is to give an API to build a database on top, so has no file concepts
My instant reaction to reading "Cool guy looking for ..." was "wrong subreddit".
LSMT is great, I've also implemented one in ZboxFS, it is used as index manager for data blocks. &amp;#x200B; [https://github.com/zboxfs/zbox/blob/master/src/volume/storage/index\_mgr.rs](https://github.com/zboxfs/zbox/blob/master/src/volume/storage/index_mgr.rs)
My use case has been to use tokio-process and futures to create prepared shell statements for asynchronous sys admining. Basically, all my shell tools now work in async and it works equally well across many servers or many network requests wherever stdin and stdout are involved.
Not really. You might be able to do something with a custom allocator, but I don't think anyone tried that before. Most likely, you would have to implement your own `Vec` that supports allocation errors.
Confusion comes from requiring `extern crate` in main.rs when you need to use `use` everywhere else. I personally know several peoples not getting it until explained rigorously. The only part of the new system I don't like is `mod.rs` -&gt; `mymodname.rs` on the same level as directory (instead of being nested). The other stuff is completely fine.
That makes sense. I have a good quote for this situation: &gt; My usual response to “why is feature X not implemented?” is that of course all features are unimplemented until someone designs, implements, tests, documents and ships the feature, and no one has yet spent the money to do so.
You just express everything in types an that's all. Encoding missing value in `Option` instead of some kind of "negative one means something went wrong, otherwise it's an index in the array" stuff. You can go crasy with `OddNumberBiggerThanFive(u32)` and typecheck everything.
There are no advanced subtyping rules in Rust (except lifetimes). `!` is an uninhabited type which can be cast to any other type. Read more on this in this [rendered RFC](https://github.com/rust-lang/rfcs/blob/master/text/1216-bang-type.md). In your case, write `g().map(|x| x)` to make your code compile.
Shoutout to [Kattis](https://open.kattis.com/help/rust) for keeping Rust somewhat up to date, currently at 1.32!
It's amazing that the implementation seems really clean.
I heard that [crossbeam-epoch](https://docs.rs/crossbeam/0.7.1/crossbeam/epoch/index.html) is doing similar stuff.
You are probably looking for /r/playrust
r/playrust is what you're looking for.
I guess there have been proposal to add Ada-style mod types to Rust? Any change of that ever happening? The approaches in this thread don't sound very ergonomic.
Multiple version tree C-function comparison between older software releases and current master branch.
Not really behause it would mean that your id can become invalid and youll end up with a whole lot of unwrap() and the negatives associated with those.
I’ve tried Cloud9 in the past and it’s okayish. I used it for C/C++ mostly and although it did have autocompletion, it is was more like an online sublime text than an IDE or even VSCode with some pluggins. I honestly wouldn’t really recommend it, it’s perfect if you want to have a dev env on the go but it’s simply not an IDE. I read somewhere that Microsoft was going to make an online version of VSCode. I think either that or Eclipse Che is a better bet for a decent online dev environment.
This is not possible for a lot of crates because of proc macros. &amp;#x200B; You just can't have proc macros in the same source file as the solution.
The issue is just the lack of subtyping rules, but also a layout problem. `g` returns a value whose layout is zero sized (because only one case is possible, and that case has zero sized payload). `f` returns a value whose layout is that of a pointer - because `Err` case is zero sized, and `Ok` case is not but has one niche value. These two layouts are very much incompatible. The compiler could insert a conversion automatically, but then what conversions are valid? For `Result` it is straightforward, because it is a plain rust type that no code assigns any special meaning to. But for example, you can't convert `Vec&lt;!&gt;` to `Vec&lt;u64&gt;` - because it actually has different code for managing allocations depending on condition `size_of::&lt;T&gt;() == 0` - so a naive automatic conversion would break it!
Actually, in this case it is possible to implement `Iterator` without unsafe: convert the vector into `VecDeque` (which should be done without reallocating or copying), and then pop elements from the front. [Playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=2db079295f0b36c7334d259ffdb2d771)
Kattis has support for Rust nowadays as well!
Wrong sub. r/playrust
&gt; PS: turns out that proc-macros are hurting build times a lot. I always noticed `syn` taking a long time to compile.
Sounds like a stamina mechanic. Wrong sub though
Try this: ```rust trait Server { fn is_chinese(&amp;self) -&gt; bool; } fn fix_china_invasion&lt;'a&gt;( items: impl Iterator&lt;Item = Box&lt;dyn Server + 'a&gt;&gt;, ) -&gt; impl Iterator&lt;Item = Box&lt;dyn Server + 'a&gt;&gt; { items.filter(|s| !s.is_chinese()) } ```
 fn foo&lt;T: Trait1&gt;(val: T) -&gt; impl Trait1 { // ... } Is it possible to spell "If `T` implements `Trait2` in addition to `Trait1`, the return type also implements `Trait2`"?
ty mg
If I'm implementing it in three or more places where it could be derived, I think the time saved using them instead of writing them by hand would outweigh the time to compile the dependencies.
Hey, thank you for taking the time to respond! :) I'm curious, would your solution with `iter::Flatten` `move` and/or consume the `Vec` indices? I would like to be able to iterate over this several times. (Maybe I have a misconception about iterators moving data that I need cleared up...)
Thank you for taking the time to respond! :) Would calling pop `move` the data? I would like to be able to iterate over this `Vec` several times.
Possibly. But I would avoid dragging e.g. `derive-builder` (just an example, I don't have anything in specific against that crate) in a library crate that I want others to use. It's not just the time to build it once: - there's the CI time to consider - it can lower build parallelism (e.g. if `syn` blocks other crates from being built) - sometimes I end up with multiple versions of transitive dependencies, because not everyone updates and releases their crates often enough I think I've seen crates that could take a couple of seconds to build, but need a lot more in practice because they depend on `failure` or another custom derive or procedural macro crate.
Yes, this is `Type info` (Ctrl+Shift+P). Also, if you want to invoke some action but don't know its hotkey, you can open `Find Action` dialog (Ctrl/Cmd+Shift+A) and just write something related (e.g. "type" in your case).
My first Rust project is some sort of TUI for reading foreign languages. I made my own weird word wrapping function. [Playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=4f71dd795e018d52b00c305927db5398) Any pointers on idiomaticness would be very appreciated
It would be great if cargo-bloat could analyse not only `.text` section, but also `.data` and `.rodata`. I've seen some crates put huge amount of static data in the binary, but its hard to pinpoint why.
Yes, it certainly does not free yourself of tracking and managing data properly. What are better approaches in your view to projects like the following? * [https://rust-leipzig.github.io/architecture/2016/12/20/idiomatic-trees-in-rust/](https://rust-leipzig.github.io/architecture/2016/12/20/idiomatic-trees-in-rust/) * [https://raphlinus.github.io/personal/2018/05/08/ecs-ui.html](https://raphlinus.github.io/personal/2018/05/08/ecs-ui.html)
I've planning to implement this when I will have free time.
I believe that the idea is that integer overflow checking would also be enabled in release mode if it didn't have a performance hit, e.g. if some future CPU supported that.
Same. That's why I wrote this feature to pinpoint such crates. My current pain point is `euclid` (-j1): ``` Time Crate 20.89s syn 8.41s euclid_macros 5.30s proc_macro2 2.05s num_traits 1.54s quote 1.47s euclid 1.46s autocfg 0.12s unicode_xid ``` Or 17 second on 4 cores just to build a simple geom library?! There are no point optimizing rustc when we have such monstrosities. Also, even the simplest `build.rs` takes like a second to build on my machine.
I might be holding it wrong, but I tried running `cargo bloat --release --crates --time -j1` on one of my apps, and it doesn't seem to show `syn` and other custom derive crates. On the other hand, it shows some of my transitive dependencies.
You should remove the `--crates` flag. `syn` doesn't impact the binary, so it's no in the table.
If you just need meat behind the compiler maybe try an sshfs/docker combo?
Regarding the implementation, I saw you are doing several things manually and wondered if some crates would help clean up the code and to help harden the crates: - cargo-metadata seems well enough known that I'm assuming there is a reason you aren't using it. - escargot is like cargo-metadata but for builds. - clap-cargo is trying to make it easier to write cargo plugins with consistent arguments. So far I assume only `manifest-path` is helpful but I assume you could contribute back some more ;) If you have reasons not to or just not worth the effort, understandable; just trying to raise there visibility.
&gt;I've never tried redox but the world could very much use an alternative kernel. Wouldn't rustifying security critical features of the linux kernel (and demonstrating safety) be a much more practical goal?
Thanks, that's more like it: 41.55s tower_web_macros 22.56s syn 19.60s serde_derive 17.26s clap 16.64s diesel_derives 11.54s regex 7.44s regex_syntax 7.00s tower_web 6.18s hyper 5.64s diesel 5.30s serde 5.22s structopt_derive 5.11s h2 3.72s cc 3.51s idna 3.17s http 2.94s unicode_normalization 2.65s serde_json 2.58s tar 2.54s tokio 2.49s url 2.37s mime_guess 2.37s proc_macro2 2.33s rand 2.28s futures 2.25s headers_derive 2.01s tokio_threadpool 1.95s rayon 1.90s headers 1.70s chrono 1.66s mio 1.57s env_logger 1.48s rand 1.45s rayon_core 1.45s pkg_config 1.22s tokio_reactor 1.20s unicode_bidi 1.17s sha2 1.09s memchr 1.05s unicase 1.03s httparse 1.02s num_traits 1.00s checked 0.97s time 0.96s cookie 0.91s http_serve 0.83s libc 0.83s byteorder 0.79s parking_lot_core 0.79s tokio_uds 0.79s futures_cpupool 0.78s hex 0.75s build_script_main 0.75s pretty_env_logger 0.74s libsqlite3_sys 0.74s walkdir 0.70s tokio_timer 0.69s tokio_tcp 0.68s bytes 0.67s aho_corasick 0.67s quote 0.67s crossbeam_epoch 0.66s tokio_sync 0.66s semver 0.64s strsim 0.64s arrayvec 0.64s num_cpus 0.63s tokio_io 0.62s tokio_fs 0.62s termcolor 0.62s semver_parser 0.61s flate2 0.61s version_check 0.61s scrypt 0.60s autocfg 0.59s num_integer 0.57s tokio_trace_core 0.57s crossbeam_utils 0.56s net2 0.56s ryu 0.56s heck 0.54s tokio_current_thread 0.54s parking_lot 0.54s crossbeam_epoch 0.53s crc32fast 0.52s scheduled_thread_pool 0.49s textwrap 0.48s typenum 0.48s base64 0.47s rustc_version 0.47s unicode_segmentation 0.46s tokio_udp 0.46s mime 0.44s mime 0.43s miniz_sys 0.43s unicase 0.42s proc_macro_hack_impl 0.39s indexmap 0.38s sha1 0.36s filetime 0.35s humantime 0.33s serde_urlencoded 0.33s base64 0.30s ansi_term 0.29s rand_isaac 0.28s generic_array 0.28s serde_plain 0.28s rand_chacha 0.27s horrorshow 0.27s httpdate 0.27s chrono_humanize 0.26s mio_uds 0.26s rand_os 0.26s tokio_codec 0.25s tokio_executor 0.25s thread_local 0.24s r2d2 0.24s rand_hc 0.23s smallvec 0.23s log 0.23s tokio_buf 0.22s percent_encoding 0.22s log 0.21s ucd_util 0.21s utf8_ranges 0.21s rand_core 0.20s rand_pcg 0.20s lock_api 0.19s rand_jitter 0.18s siphasher 0.17s crossbeam_deque 0.17s crossbeam_utils 0.16s vec_map 0.16s want 0.14s dtoa 0.13s digest 0.13s either 0.12s same_file 0.12s crossbeam_deque 0.12s atoi 0.11s phf 0.11s crossbeam_queue 0.11s bytesize 0.11s rand_xorshift 0.10s owning_ref 0.10s itoa 0.10s phf_codegen 0.09s slab 0.09s iovec 0.09s block_padding 0.09s string 0.09s phf_shared 0.09s block_buffer 0.09s subtle 0.08s structopt 0.08s hmac 0.08s phf_generator 0.08s unicode_xid 0.08s pbkdf2 0.07s antidote 0.07s tower_service 0.06s atty 0.06s crypto_mac 0.06s unicode_width 0.06s try_lock 0.06s safemem 0.06s headers_core 0.05s void 0.05s lazy_static 0.05s quick_error 0.05s http_body 0.05s byte_tools 0.05s fnv 0.05s stable_deref_trait 0.05s fake_simd 0.05s scopeguard 0.04s nodrop 0.04s rand_core 0.03s matches 0.03s bitflags 0.03s proc_macro_hack 0.03s cfg_if 0.03s memoffset 0.03s opaque_debug
Thanks!
&gt; The builder pattern is super common in Rust. Almost obnoxiously so. Why all this hate? The builder pattern is super cool IMO. That's a matter of taste, sure, but it allows to easily configure everything.
What's exactly the [unknown] ? ``` &gt; cargo bloat --release --crates -n 12 Compiling ... Analyzing target/release/broot File .text Size Crate 5.3% 19.4% 415.7KiB std 4.2% 15.6% 334.0KiB [Unknown] 3.9% 14.3% 306.1KiB clap 3.6% 13.4% 286.2KiB regex ```
or... we need a framework like QT 🙂
You are right. I need to work on my titles. Thank you.
I suspect this requires [specialization](https://github.com/rust-lang/rust/issues/31844) (since you'd be further constraining `T` to provide specific behavior).
The code is garbage. It was a weekend project originally and I don't have enough time to rewrite it. - Last time I've checked (a year ago?), `cargo-metadata` didn't support enough for `cargo-bloat` needs. Looks like they've added a lot of useful stuff lately. - Not sure what `escargot` does. It's just an interface for cargo? `cargo` invocation is very simple in the `cargo-bloat`, so I'm not sure that this can be improved. - `clap-cargo` looks nice, but rather limited. I need more flags. I've wanted to write something like `cargo-plugin-base`, but time is the factor as usual.
Yes, [`pop_front` takes `&amp;mut self`](https://doc.rust-lang.org/std/collections/struct.VecDeque.html#method.pop_front), because it: &gt; Removes the first element and returns it
The kernel by definition runs in ring 0 so it's *all* security critical.
True on one hand, but there's obviously bits (eg networking) which need a lot more attention than others which could definitely benefit more
Im not 100% on the return type but you can do `fn foo&lt;T: Trait1 + Trait2&gt;(val: T)` .
&gt; Not sure what escargot does. It's just an interface for cargo? cargo invocation is very simple in the cargo-bloat, so I'm not sure that this can be improved. Yup. For existing code, it doesn't offer much to change. It is helpful for writing new code to make sure you are calling things correctly and not having to worry about manually deserializing the output. &gt; clap-cargo looks nice, but rather limited. I need more flags. I know. I've only written support for the flags that I need for cargo-release. You using it wouldn't offer you much benefit since you'd need to add support for the flags you need but it could be nice for the next person who follows along. &gt; I've wanted to write something like cargo-plugin-base, but time is the factor as usual. Understand. I have too many project ideas and not enough time :) I am curious what else you would find useful in a `cargo-plugin-base`?
I'm using futures in my [Erlang VM port](https://github.com/archseer/enigma) as a way to schedule erlang processes (actors/green threads). I started with my own threadpool, but realised that tokio's work-stealing executor and time-wheel based timers perfectly match up with what I needed to build anyway (and they outperformed my implementations).
The goal is to port all cargo plugins to it. So they would not depend on cargo directly. So, a lot.
Working on a slew of minor tweaks, and writing docs for API changes for the [wasm framework I'm working on(https://github.com/David-OConnor/seed). Sometimes it seems futile since it's so against the community std of JS/ES6/TS/React etc, and there are a number of similar Rust projects, but keeping at it.
Who posts to a sub they've never even looked at? You're looking for /r/playrust
Is this something that will compile to GLSL? Is it for numerical/scientific work on the GPU, or 3d as well, eg games.
 extern crate csv; #[macro_use] extern crate serde_derive; use std::error::Error; use std::io; use std::process; // By default, struct field names are deserialized based on the position of // a corresponding field in the CSV data's header record. #[derive(Debug,Deserialize)] struct Record { column_1: String, } fn example() -&gt; Result&lt;(), Box&lt;Error&gt;&gt; { let mut rdr = csv::Reader::from_reader(io::stdin()); for result in rdr.deserialize() { // Notice that we need to provide a type hint for automatic // deserialization. let record: Record = result?; println!("{:?}", record); } Ok(()) } fn main() { if let Err(err) = example() { println!("error running example: {}", err); process::exit(1); } }
You can deserialize each record into a hash map. Data: column_1,column_2,unknown foo1,bar1,baz1 foo2,bar2,baz2 `Cargo.toml`: [package] name = "csv-reddit" version = "0.1.0" authors = ["Andrew Gallant"] edition = "2018" [dependencies] csv = "1" `src/main.rs`: use std::collections::HashMap; fn main() -&gt; Result&lt;(), csv::Error&gt; { let mut rdr = csv::Reader::from_path("data.csv")?; for result in rdr.deserialize() { let record: HashMap&lt;String, String&gt; = result?; println!( "column_1: {:?}, column_2: {:?}", record["column_1"], record["column_2"], ); } Ok(()) } Output: $ cargo run column_1: "foo1", column_2: "bar1" column_1: "foo2", column_2: "bar2" The tutorial also covers this: https://docs.rs/csv/1.0.7/csv/tutorial/index.html#reading-with-serde
The `build!` macro generates Rust functions that use a binding to OpenCL for sending kernels (chunks of computing instructions) and data to the GPU and retrieving results. It generates Rust functions using intermediate code that is generated by `emu!` and stored in the `EMU` global constant. Currently, this intermediate code is OpenCL but in the future it could be something else. So to answer your question, this is for numerical/scientific work on the GPU (using OpenCL behind the scenes). Graphics is not a focus; GPGPU - general-purpose GPU computing - is what the focus is on.
Nice, very useful. Just wish it supported windows..
You compile the code far more often than you write it.
/u/Jonhoo by any chance was this video recorded?
Lots of mobile Reddit apps only do 4 space indent code blocks. I know it's kinda stupid but could you use that format?
You might be able to do something like this, depending on your use case. trait Trait1 {} trait Trait2 {} struct Struct&lt;T&gt; { x: T } impl&lt;T: Trait1&gt; Trait1 for Struct&lt;T&gt; {} impl&lt;T: Trait2&gt; Trait2 for Struct&lt;T&gt; {} fn foo&lt;T: Trait1&gt;(x: T) -&gt; Struct&lt;T&gt; { Struct { x } }
No, sadly not. I'm hoping I might get to run the workshop again though (feel free to reach out if you read this and are interested), and then I'll definitely make sure to have it recorded!
&gt;the world could very much use an alternative kernel. For embedded systems I am certain that a kernel in Rust will be a very goof choice, because no one wants a system crash - especially if that system is in a very harsh, desolate, remote area.
&gt;Interesting. Having worked with UDP a lot in various roles, I think this would have been useful in various testing scenarios that I have been through. The feature I'm missing is to be able to have the tool send more than one message. For example, since UDP has no re-transmission logic, messages will often contain a sequence number. Similarly, if I want to test lookup or logic inside the recipient, I will want to vary the messages so as to defeat any caching in the recipient. So you want to specify several packets to be sent by Anevicon? For example, I can implement the `--send-file firstfile.txt --send-file secondfile.txt --send-message "Text message"` syntax which means that Anevicon will send `firstfile.txt`, `secondfile.txt`, and `Text message` at the same time. Is it what you mean?
Could the new name-mangling scheme help? I think it encodes information on the source crate of each function. Of course that doesn't always help, you can always tag functions with no_mangle....
That's one alternative. In practice, I think it would make more sense to have files with multiple lines or records. For example, I'm currently contemplating a setup with an AWS NLB and a set of nodes which receive and track messages by a sender id in the message. I now want to pour in, say 1M messages in 10 seconds, across a few sender ids with message ids unique within each sender. A quick shell command would generate a file with all the messages and I could use your tool to pour in lots of messages with specified timings while examining the node logs for missed deadlines, out-of-sequence messages and other weirdness. Today, I would use nc, but that has limited options to pace the output. This is speculative at this time, but it shows one reasonable use case.
... without anything that just remotely look likes inheritance! One very good reason to avoid inheritance is: &gt; The fragile base class problem is a fundamental &gt; architectural problem of object-oriented programming &gt; systems where base classes (superclasses) are &gt; considered "fragile" because seemingly safe &gt; modifications to a base class, when inherited by the &gt; derived classes, may cause the derived classes to &gt; malfunction. &gt; &gt; The programmer cannot determine whether &gt; a base class change is safe simply by examining in &gt; isolation the methods of the base class. via The [Fragile Base class, Wikipedia](https://en.wikipedia.org/wiki/Fragile_base_class) Using [Webrender](https://hacks.mozilla.org/2017/10/the-whole-web-at-maximum-fps-how-webrender-gets-rid-of-jank/) would be a good start. BTW, If you are interested in stuff like Webrender,I would recommend to read Lin Clark's blog posts. She is amazing at making complex stuff very easy to understand.
Rust does eventually want to provide an API for falliable allocations, but the story for that is still evolving. That being said, chances are you won't even run into an allocation error if you're running on a typical desktop environment. The OS will just pretend that every allocation is successful until you try using way too much memory, at which point your process will be killed with no prior warning.
Hey RalfJ, sorry about the late reply... ## Booleans In my particular case booleans are problematic as they pop up in some 'FFI' cases such as reading memory from another process or interpreting files by casting their memmapped contents to a structure. In such cases it is technically illegal to use bool as I cannot guarantee that these bools do not have invalid contents. I've already accepted my fate here and use u8 instead of bool in these cases. (the same issue is present in C/C++ code but some people like myself in the past have generally ignored this and just used bools because it's more convenient) ## Leaking memory When dealing with certain 'should never happen' cases I sometimes simply prefer to silently ignore errors. These aren't always _big_ errors but something stupid like a configuration was specified twice. [Here's an example](https://github.com/CasualX/cvar/blob/master/src/console.rs#L25-L35). It uses a helper function to walk a configuration tree and invokes a callback when it matches the name. I wrap it to return the value of the configuration node. What happens if two nodes have the same name? Idc, don't do that but it isn't catastrophic. You could argue that this is an error and should panic but sigh, idk. Panics are very heavy handed and in this case I decided not to panic. Should the original value be mem::forgotten? Here I didn't, but depending on my mood I may do it. It's not a memory safety issue and correctness outside of that is not always the most important thing. ## MaybeUninit symmetry I would argue that the symmetry between MaybeUninit, Option and hypothetical &amp;uninit references is extremely important! In a sense MaybeUninit and Option implement a kind of uninit references but with different choices in semantics in the edge cases: MaybeUninit implements it by going through raw pointers and unsafe code to let the developer decide when something has been initialized. The semantics when a panic occurs is then defined to forget already initialized MaybeUninit but not yet assume_inited. Thus the 'manually drop' is explicitly chosen as the fallback case when a panic occurs! This isn't an 'implementation detail' but an explicit choice in semantics. Option implements it by doing a runtime check, if an they're used to emulate uninit references then the value is dropped correctly when the code panics but before the developer has asserted that the out value has been initialized. That's what I meant when I said that MaybeUninit does not represent an initialized value. I was trying to figure out how out parameters could be built into the Rust language and what their semantics could be. Both MaybeUninit and Option can be used to emulate out parameters, each with different choices for what happens if the code panics. MaybeUninit forgets the value, Option drops it.
Here are the code blocks formatted for old desktop/app layouts: int rlu_list_add(rlu_thread_data_t *self, list_t *list, val_t val) { int result; node_t *prev, *next, *node ; val_t v; restart: rlu_reader_lock(); prev = rlu_dereference(list−&gt;head) ; next = rlu_dereference(prev−&gt;next) ; while (next−&gt;val &lt; val) { prev = next; next = rlu_dereference(prev−&gt;next); } result = (next−&gt;val != val) ; if (result) { if (!rlu_try_lock(self , &amp;prev) || !rlu_try_lock(self, &amp;next)) { rlu_abort(self); goto restart; } node = rlu_new_node(); node−&gt;val = val; rlu_assign_ptr(&amp;(node−&gt;next), next) ; rlu_assign_ptr(&amp;(prev−&gt;next), node) ; } rlu_reader_unlock (); return; } &amp; let rlu: Arc&lt;Rlu&lt;u64&gt;&gt; = Arc::new(Rlu::new()); // global memory arena let obj: RluObject&lt;u64&gt; = rlu.alloc(1); let reader = { let rlu = rlu.clone(); thread::spawn(move || { let thread = rlu.thread(); let mut session = thread.session(); let n: *const u64 = session.read_lock(obj); let n2 = unsafe { *n }; thread::sleep(time::Duration::from_millis(100)); assert_eq!(unsafe { *n }, n2); }) }; let writer = { let rlu = rlu.clone(); thread::spawn(move || { let thread = rlu.thread(); loop { let mut session = thread.session(); match session.write_lock(obj) { Some(n) =&gt; { unsafe { *n += 1; } break; }, None =&gt; { session.abort(); } } } }) }; reader.join().unwrap(); writer.join().unwrap();
Can't wait for the CCD to be released! You guys rock
&gt; Caution; The next command delete the .git folder, if there is found in the specified directory. check first if this is what you want. Maybe it would be better to have it quit with a warning message if a .git folder is present, and require a command-line flag to make it ignore that check and delete it.
I think a lot of this is from proc macro crates needlessly being built in release mode. There is typically a 3x difference between release and debug compile time for proc macros, and there is typically no difference in runtime for the proc macro (= compile time of downstream crates) because they take almost no time to run anyway and that time is already bottlenecked by talking to the proc macro bridge so optimization doesn't help. On my computer serde\_derive is 25.2 vs 7.8 seconds for release vs debug (3.2x) and syn is 15.3 vs 5.4 seconds (2.8x) [RFC 2282 - Cargo profile dependencies](https://github.com/rust-lang/rust/issues/48683) should help get that back with a *.cargo/config* that you place in your home directory, but I haven't gotten a chance to try yet.
Not sure about the context of the "Digital Currency Initiative" but there's a cryptocoin with a rust implementation (at least one AFAIK). It's called "[Grin](https://grin-tech.org/)" and you can find more info over at /r/grincoin.
Also, even though checking for overflow is not enabled for release builds, it is still undefined behavior. Overflowing an integer is considered to be an error by the compiler and, theoretically, anything could happen when an integer overflows (although in practice it will wrap around). Use the Wrapping type instead (for correctness and to indicate your intent for future programmers).
Oh ok, good idea. I will implement it. :)
Fixed in the post. Sorry for the trouble.
Oh yes that looks super helpful, thanks for the link.
Have you considered PRing this into the official Rust book? Would be a great help.
Thanks for the reference, amazing project!
Thank you for the suggestion. &amp;#x200B; I'll think about implementing the functionality that allows you send multiple messages with ease. For example, we can use a JSON file to specify which messages to send: ```json { } ```
Working on a `phpMyAdmin`/`Django Admin` proc macro to generate an admin front end for `diesel`. Currently struggling to figure out how to generate code for a row to be updated while also having read only fields.
Thank you for the suggestion. I'll think about implementing the functionality that allows you to send multiple messages with ease. For example, we can use JSON to specify which messages to send: ``` { "send-file": "my_file.txt", "send-file": "your_file.txt", "send-message": "hello, world!", "send-random-packet": 18374 # 18374 random bytes } ``` But we also can extend the existing functionality (the syntax of command-line arguments discussed above).
I don't know that they link to third-party sources from the book? If so, I'd be happy to have my material included! My video on how futures work would probably also be a good addition.
Probably. I have to test it when it will land into stable.
It requires a good PDB crate and some understanding of it. ELF is times simpler, imo.
Regarding async, this (not Rust specific, and only about I/O) is maybe interesting to read [https://en.wikipedia.org/wiki/Asynchronous\_I/O](https://en.wikipedia.org/wiki/Asynchronous_I/O)
From their code example it appears the asker wants this to not consume the vector.
No, this won't consume data from the underlying vector. Unlike the `VecDeque` solution the other person proposed, this is a repeatable iterator. This yields mutable references like you wanted.
Have you tried running with RUST_BACKTRACE=1? That should help you debug a bit.
Wait how do i do that like where should i type it
It seems, the items in headline do not match the columns \&gt; File .text Size Time Crate
They do.
If you're talking about the game Rust, you're in the wrong sub...you want /r/playrust
How would I go about creating a set of piano keys that are highlighted to indicate the keys pressed? I have a piano synthesizer that upon pressing keys on a midi piano, will return the key(s) pressed. Such as keys [69,71,73] (3 keys pressed at once). I want these keys to highlight on the piano visual. I am a rust beginner. Thank you.
Might want to check out https://github.com/polyfractal/bounded-spsc-queue which is probably as fast as you can go for a spsc situation in rust. Slightly different design goals, but similar concepts and very fast.
But im in that
r/playrust
FYI: this post is an Aprils fools. Not sure why this was posted again.
Also you should adopt 'fail fast' function structures. instead of writing this: function() { if condition { do thing } else { do other thing } } you should rearrange things like this: function() { if condition { do thing; return; } do other thing } where 'condition' is testing for some kind of error case or exclusionary test and 'do thing' is about handling this case. They are logically equivelent, but doing it this way has some advantages. 1) make it a standard to handle the error cases or prerequisites on things, and you make it a habit to \*actually\* handle them. 2) if you find yourself needing to do prerequisites / action / prerequisites / action...well that's actually \*two\* functions there isn't it? 3) reduced indenting. Indenting is a new context to hold in your head, avoid that at all costs. Your issues now are not programmatic ones, they are organizational ones. The trick at this stage of your programming development is to learn how to organize your code for a specific goal: readability, demonstration of a feature, future modifications, maintenance, etc. Any one of these could be the way you decide to organize things, but the trick is learning which ways of organizing meets which goals.
I think you meant to post this to /r/playrust. Maybe you should try looking at a sub before posting to it?
&gt; This means that you essentially have to spawn a new thread just to get a simple timer future You don't have to spawn a new thread _for each timer_ - you can handle millions of timers with just *one* thread, which you can additionally get for free, if you integrate it directly with the futures' executor. &gt; I have to say this is a bit unsatisfying as a 'timer future', all of this initialization/synchronization cost sounds much more expensive than just repeatedly polling a Timer future *Even if* a pure timer future could be coded in a more efficient manner with the `push` model, there are tons of other types of futures that would be handicapped - i.e. I/O-related; and keeping both models would most likely be just way too complicated.
Thanks you! I did not notice the date even though it's right under the post's the title and I was really confused. It did read like an April's Fools kind of a post, but the timing didn't match.
This is a weird April Fool's post that isn't funny or insightful. It's just... weird. If this were HN I'd flag it off the site.
If only it was written in a performant yet memory-safe language.
Wdm
Can the parser recover from any corruption of the MFT, or is it meant only for known-good tables?
You're in the wrong subreddit. They're making a joke.
/r/rustcryptofin
If you are alright with this timer-future implementation to be polled repeatedly - just notify the \`Waker\` in \`else\` branch. But solution with one thread to handle all timer instances sounds better to me.
IIRC SPIR-V was designed with purely computing applications in mind. (e.g. [here](https://www.khronos.org/spir/) Khronos explicitly lists OpenCL and SYCL as front-end languages for SPIR-V) To be honest at first I expected `emu` to be translated to SPIR-V, and now I think it will be a really beneficial way to move forward.
/r/playrust ?
Yes, it reads entries one by one (by simple seeking to entry\_id \* entry size), so as long as the entry in question is intact, it should parse correctly. &amp;#x200B; The parser will yield error values on entries where the header is corrupted.
you should give this a go, I found it very helpful! https://github.com/rust-lang/rustlings/
Actually, the pull model proves to be quite problematic for IO on Windows and recent Linux systems.
It's fine if widgets have local state. You can communicate that state through channels as their signals are activated. In the glib main context, you can centrally handle all UI events triggered throughout the application.
Great work! Can I suggest that you don't reserve the three-letter name "mft" on crates.io? It is completely obscure to the uninitiated reader, and could refer to a gazillion different acronyms, while analyzing NTFS Master File Tables is a highly specialized use case. If it's not too late, that is. `ntfs-mft` might be an idea?
It's a perfectly fine idiom in my opinion, but I believe it doesn't matter in the long run. You can convert your project to it or not, it won't make much, if any, difference in performance, readability or ease of maintenance. You aren't going to forget what it means when you come back to the language after a year, and it won't prevent people from contributing to your code.
But with Flutter-like syntax
How can I pass `std::cmp::PartialOrd::le` as an argument to a function to use with floats ? Something like this: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=018bb9b6b0926bc79f6165caa89d0366
Actually it doesn’t. You might be thinking of the specifics of the AsyncRead trait, but that does not mean you can’t model a completeness api efficiently using the pull model: you pull the completion status.
A pratt parser is more of a concept of a parser that you can use as reference. I recommend the sqlparser-rs crates' tokenizer as an example of some nice code to refer to. Also, I have experience with parsers, so if you just message me, I could give some advice.
The general bloat of `euclid` is one of the reasons for `pathfinder_geometry::basic` (which I want to split out into its own crate once I find a good name for it).
Same here regarding a Visual Studio Online (not VSTS/DevOps) that is VS Code online. I've seen a few options for self-hosting a web version of VS Code as well. I have a friend that just fires up a bigger cloud server for a few hours here and there when he needs more horsepower and uses RDP.
Seconded. `ntfs-mft` is much clearer.
IIRC MS made an abstracted kernel+fs for use with SQL server to make the SQL interfaces consistent while abstracting different underlying OSes. Wish they'd open those bits up. It seems that libuv binding might be the best near term approach.
Is there a reason why you're going through all the trouble of calling the git binary instead of using git2-rs ?
I don't know if anybody is going to be able to tell you concretely good/bad idea. To me, it seems like a 'just because you can, does not mean you should' thing. I would err on the side of using the simplest tool for the job -- return. But I like the 'maximally relaxed' code that gets the job done.
Thanks so much for the detailed feedback! Most of my programming experience is writing small utilities for other teams at work, and as such am very green in regards to sane code quality practices such as above. I really appreciate the feedback and will keep this in mind as I continue learning.
As another commenter pointed out, the simplest tool is `return` . It also has the convenience of being easily searchable/greppable, so with search highlighting you can very easily see exit points of the function, with `?`, especially mixed with `Result` extraction this won't be as easy.
its not clear to me what problem you're trying to solve, so futures might not be the right solution. if you want a real timer, the Tokio version does a good job. If you want to call something in a loop in the context of other futures, then you have a lot of scheduling decisions to make. The two simplest i can think of are either * Manually switching between polling your thing and polling the reactor ( not sure if thats still a thing ). * Build a custom Stream&lt;Option&lt;Value&gt;&gt; which will be polled continuously if you return a result IIRC.
`descartes`? =)
really great slides.
Thanks!
I want to say, "Thank you," for the wonderful essay that explains the problem space so well. What seems like a detour to microcontrollers and DMA actually becomes quite relevant to asynchronous IO processing with multi-core commodity server processors.
Mind sharing how you make them? Is there a beamer template or something?
While it makes total sense that it does work, I wouldn't really ever use it like that. As I was writing this, I didn't really like the idea of using it, but now I'm not so sure, because it would really only confuse beginners. Still, I personally like return keyword more, because it's quite explicit. As long as you don't mix both approaches, either is fine.
No worries at all. Your code looks fine for the stage you are at. The next steps like I said is rewriting for clarity and organization which is what separates pro's from amateurs. It just takes practice, and you obviously are willing to put in the work! This is where programming starts to come closer to writing rather than math and logic. Questions like 'Is this easier to read then this?' or 'Should I pull this out into another file or leave it along with this other code since these two things are logically grouped?' etc. Most of the writing advice you see in Strunk &amp; White actually apply to programming at this point! Weird but true.
libuv doesn't get you anything more than tokio or anything in Rust can get you. It's still limited to the syscalls offered by the kernel. Problem is, the fs limitations are in the kernel layer.
To me, the advantage would be to restrict the use of `return` to the "happy path", so that (non-panicking) error paths are *always* marked by `?`.
hi man... I did not know about that package ... Thanks for the information :)
I made them directly in Google Slides :)
I kinda prefer the return version, but I could go either way. To me it depends on how the code "feels" about errors. If this is a function that can fail in a million different ways, and any failure is basically just gonna go in the logs and retry or something like that, then I don't see any downside to using `?`. But if the function expects the caller to understand exactly what it's error conditions are, then it might make more sense to use return and make it obvious in the code.
I’m a backend developer by profession, not in rust. In situations like this the goal would be to decouple the logic that performs the manipulations. The handler should call something that performs the action rather than performing it itself. The idea being that the controller is simply responsible for transforming the user request into something that your model can process. Everything gets tricky when you’re actually going to store the data. At this point you should use a dummy instance of the persistence layer that takes the place of the ORM/connection you’re using. I imagine that in low level languages that would be a considerable amount of work though.
There’s a crate that I like which adds a macro that, while arguably unnecessary syntactic sugar, I find eases the awkward boilerplate here. Check out `try_guard`: `guard!(response.status.is_success())` https://github.com/phaazon/try-guard
I remember a recent thread talking about how drop cancellation leads to overhead when you try to use IOCP with Rust futures. But I think one of the takeaways was that it's more than just drop cancellation that leads to overhead: the IOCP model kind of demands that you give it ownership of some buffer, and you can't do that without heap allocation. Something like that?
`PartialOrd::le` doesn't take trait objects as arguments; this works, however: fn sort_special(floats: Vec&lt;f32&gt;, cmp: fn(&amp;f32, &amp;f32) -&gt; bool) { // use cmp here } fn main() { let a = std::cmp::PartialOrd::le; sort_special(vec![0.1 as f32, 0.2 as f32], a); }
I'm creating a web app where both the client and server are written in rust. The client portion uses dodrio and typed-html to render HTML. I'm having trouble figuring out how to update application state following an HTTP request. I can make the request (I believe) and this results in a js\_sys::Promise. I can't seem to see how to handle that promise, though. The .then() function takes a closure, but the closure must be 'static lifetime. &amp;#x200B; I'm relatively new to rust, so I'm probably just missing something. &amp;#x200B; If someone is interested in seeing the project it's at [https://github.com/jeffw387/sched/tree/dodrio-typed-html](https://github.com/jeffw387/sched/tree/dodrio-typed-html) specifically in the test-client directory for this code.
Does this do what you wanted: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=db0debda557e9d506d178896d0e87af3
Oh wow, duly noted and thanks for replying!
Is there an example of the network accessible hashmap? Or is this the same as the in-memory kv store listed in the live-stream voting queue?
Thank you so much for providing a pull request! I have a few ideas for how the event loop could be used to reduce overhead. For instance, the Atari ST has an event loop system that takes in a mask of desired events. Once the event loop runs, it checks the mask to see if an event matches the mask. If it does, it fires off an event, else it quietly ignores it. I'll make some notes in the PR when I look it over. But all in all, thanks for providing a pull request. Quite honestly, I wasn't expecting one from you, and I'm honored to have received one!
Like the other response(s), its a good idea to abstract away your database calls behind a trait. So define a `Dao` trait (Data access object), and put all the function signatures that your app needs inside of it. Then implement `Dao` for `Connection`. Then create a `FakeDatabase` struct, and implement `Dao` for it as well (you may need to implement `Dao` for `Arc&lt;Mutex&lt;FakeDatabase&gt;&gt;` to make this sharable via Rocket's State). Now you can install a `DaoProvider` enum in your State that returns `&amp;dyn Dao`s depending on if it was set up with a connection (pool) or a fake variant. This enables unit testing of your code, which should be very fast, but offers no guarantees that your db calls are correct, only that your testing-only fake implementation works. &amp;#x200B; You have 2 options for integration testing using your database code: Run your queries inside test-transactions, and then never commit them to the database, or create a new test database for every test. Using test transactions is fast, but you run into problems when you need to test two or more effectual requests within one test, because the effect from the first request is rolled back before subsequent verification requests can be made. Diesel has [test\_transaction()](https://docs.diesel.rs/diesel/connection/trait.Connection.html#method.test_transaction), and [here](https://github.com/diesel-rs/diesel/issues/2058#issuecomment-493575433) is how to configure pooled connections to run using test transactions. I have a diesel-dependent crate ([source](https://github.com/hgzimmerman/diesel_test_setup)) that I'm close to releasing that creates a new database, runs migrations on it, and then tears it down when the test is finished. &amp;#x200B; Overall, it is a lot of work to get properly working unit and integration tests working, and likely more so if your chosen database interface doesn't support wrapping calls in transactions or creating temporary test databases.
I'm honestly not sure what would happen if you used LLVM 7.0.0, given that 1.35.0 is currently set to compile with 8.0.0. It looks like the commit upgrading to 8.0.0 [was just a submodule update, though](https://github.com/rust-lang/rust/pull/59285), so it might work? (if there's nothing else depending on 8.0.0) --- In general, there are definitely problems which come up when changing LLVM versions, at least when upgrading. See the "Changes required" sections of the tracking issues for [LLVM 5.0](https://github.com/rust-lang/rust/issues/43370) and [LLVM 7.0](https://github.com/rust-lang/rust/issues/50543)). Most of these seem to be changes required in rust, but there were also patches to LLVM for bugs that need to be fixed before rustc could use the newer version.
Yeah absolutely. The last little project I worked on was my first time trying to break things out into separate Perl modules, as the line count for a single file was starting to get out of control, and I definitely found myself considering the layout of the modules and the functions therein as much as writing the actual code. I certainly still have a lot to learn though, and am finding the just repeatably iterating and working on it has been much more valuable than trying to read too much about it. Thanks again so much for the pointers!
I'm going to guess, that the worse that would happen if it built would be features that require components from the newer version of llvm wouldn't work.
&gt;I certainly still have a lot to learn though, and am finding the just repeatably iterating and working on it has been much more valuable than trying to read too much about it. OMG, you are \*so\* far a head of where I was when I started. That line right there is programming epiphany gold.
Since you read around a bit I'm curious if you came across any resources that were about learning programming through Rust. &amp;#x200B; What I mean by that is - most Rust learning resources try to leverage the readers experience with other programming languages (most notably C/C++) to explain concepts, or otherwise rely on the reader having a background in CS/programming concepts. Have you found any resources that use Rust to explain larger programming concepts?
I've done that before, mostly because it felt like the natural solution at the time. I don't have strong opinions about it, though. Use it if you want. It's fine code.
I haven't worked with those frameworks much, but if you want a closure with a `'static` lifetime, you can often get there using the `move` keyword (so that the closure automatically takes ownership of any local variables it references) and possible a few `.clone()`s (in the case that some of your local variables are themselves references with non-static lifetimes).
Where's the mid 2019 rundown on the various popular non-json serialization targets? Thrift, Argo, Capnproto, FlatButter, ProtocolBuffers, what is actually being used these days?
I meant something like this, but narrative; it's a useful part of Rust that's had no guides until recently. Would be perfect in the book. I remember trying a few months back, and found no tutorials on the topic.
Dubbo, hessian, reddit is so deaf ti the China eco system.
&gt; Shouldn't this be done just when it goes out of scope at the end of the function? This would be functionally equivalent, but might not be as clear. As I understand it, `drop()` is being used to make explicit that this value is dropped. &gt; Why not just call drop first and then move into inner? This would not be equivalent - think of what happens if dropping the object panics. In the current code, when `drop()` results in a panic, the new value will already be stored in the cell. Thus if someone catches the panic and observes the cell, the cell has a valid value in it. If we dropped the object before setting it, and the drop panicked, then the cell would no longer contain a valid value. If someone catches the panic and observes the cell, they'll be reading effectively uninitialized / freed memory.
I'm not sure how you intend to have multiple futures poll the same stream since only one of them should have mutable access to it (wrapping it in `RefCell` maybe?). Even if you did get that to work the order in which the futures wake and poll the stream when it is ready is going to be arbitrary and depend on implementation details of the runtime and probably declaration order in your code, which sounds like a great way to shoot yourself in the foot.
Wow! You're awesome! Thank you :D Here's a playground of what you described: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=9ab3ed528203d5656d7a7d58a7abe564 If I may ask, how is it that this works? fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; { self.iter_mut.next() } How does it skip all the `None` inside of my `Vec`? Does flatten remove them?
This is incorrect in Rust. Rust math panics in debug and wraps in release. It's defined. (Addition is safe, so cannot possibly be allowed to be UB.)
The kinds of things needed for that (literals that can be multiple things, for example) would also make the wrapping types more ergonomic.
Thanks!
A such event filter might be added as a library, I guess. Otherwise it's up to the window backend to choose which events to emit. I believe you can disable or enable events in SDL2 and GLFW.
`flatten()` turns an iterator of iterators (or items that implement `IntoIterator` which includes iterators) into just a single iterator that yields all the collective iterators' items in turn: let values = // Vec&lt;Vec&lt;i32&gt;&gt; vec![ vec![0, 1, 2, 3, 4], vec![5, 6, 7, 8, 9] ] // Iterator&lt;Item = Vec&lt;i32&gt;&gt; .into_iter() // Iterator&lt;Item = i32&gt; .flatten() .collect::&lt;Vec&lt;i32&gt;&gt;(); assert_eq!(values, vec![0, 1, 2, 3, 4, 5, 6, 7, 8, 9]); `&amp;mut Option&lt;T&gt;` implements `IntoIterator` which `flatten()` can work with; let mut options = vec![Some(0i32), None, Some(2i32)]; let mut values = options.iter_mut().flatten().collect::&lt;Vec&lt;&amp;mut i32&gt;&gt;(); assert_eq!(values.len(), 2); assert_eq!(*values[0], 0); assert_eq!(*values[1], 2);
I wouldn't say it's a "trick" at all. If you're familiar with \`?\` it reads pretty naturally. Practically speaking, it's more likely that in situations where I'm building the \`Err\` manually, it's because I need the same types on my \`match\` arms. This is valid and puts more clarity around the happy-path: let my_table = match response.status() { true =&gt; Ok(response.body()), false =&gt; Err(InvalidResponseError), }?;
Oh, I see what you mean. I think something like that could be a really good addition, though sadly I don't really have the spare time to _both_ make these _and_ transcribe them into textual format. Maybe one day!
You're basically implementing something like memcached, or a simple version of redis. Though the focus should be mostly on implementation, and for the protocol for example I would just go with something simple like HTTP :)
People on hacker news were saying that this implementation has problems, though I don't remember what they are.
I just finished up what I think is the first Rust application that runs on calculators. I wrote [n-flashcards](https://www.reddit.com/r/nspire/comments/bww71a/news_nflashcards_for_ndless_study_from_csv_files/), which is a GUI app for studying from flashcards in CSV format. It runs on TI Nspire calculators using [ndless](http://ndless.me/), and is registered as a `no_std` target in Rust.
Not intentionally. Do you know how an English-only speaker can be better informed about things in the Chinese OSS ecosystem?
I'm trying to figure out the gfx-hal crate, but this seems like a general rust issue. In one of the samples it has code like this to compile and load a shader: let vs_module = { let glsl = fs::read_to_string("quad/data/quad.vert").unwrap(); let spirv: Vec&lt;u8&gt; = glsl_to_spirv::compile(&amp;glsl, glsl_to_spirv::ShaderType::Vertex) .unwrap() .bytes() .map(|b| b.unwrap()) .collect(); unsafe { device.create_shader_module(&amp;spirv) }.unwrap() }; This is done for the vertex and fragment shader, seems like I should be able to write a helper function like this and call it twice: fn compile_shader(device:&amp;back::Device, path:&amp;str, ty:glsl_to_spirv::ShaderType) -&gt; back::resource::ShaderModule { let glsl = fs::read_to_string(path).unwrap_or_else(|_| panic!("Unable to read {}", path)); let spirv: Vec&lt;u8&gt; = glsl_to_spirv::compile(&amp;glsl, ty) .unwrap() .bytes() .map(|b| b.unwrap()) .collect(); unsafe { device.create_shader_module(&amp;spirv) }.unwrap() } And then call it like this: let vs_module = compile_shader(&amp;device, "data/quad.vert", glsl_to_spirv::ShaderType::Vertex); let fs_module = compile_shader(&amp;device, "data/quad.frag", glsl_to_spirv::ShaderType::Fragment); ... but this seems to not be able to work. I need to declare the return type, but it won't let me due to syntax issues: error[E0603]: module `resource` is private Is there some way for me to do this? How is it even working in the inlined version when the type is inlined if it's a private type?
Downgrading can surely also cause issues, though exactly what those are would depend on the versions involved. Each new release is going to include features and bug fixes which you'll miss out on if you use an old version. Rust is also known to cherry pick specific LLVM patches that it needs so even using the same version of LLVM without those patches might cause issues. My understanding is that tracking older/newer versions is generally doable but requires a bunch more effort so it is only worthwhile if you have a particular reason that you need to. The vast majority of users should be just fine with prebuilt binaries via rustup.
[Here is a simple example with piston](https://gist.github.com/780ae2f9a13664cfb8b10f5add6825e2), using [this](https://i.imgur.com/s2Xi1WH.png) piano image I made in Inkscape ([here's what it looks like when you run it](https://imgur.com/9ef3b087-49aa-4e73-b1a9-368e96d2b0e6)). There are many ways to do this, this is just the way I chose to approach the problem.