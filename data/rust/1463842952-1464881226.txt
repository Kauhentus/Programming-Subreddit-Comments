I agree with @Manishearth about the frequency of this error but I'd like to repeat here what was also mentioned in the original thread - use of fixed sized arrays is uncommon mainly because they are limited in Rust - for oexample, there is no way to write generics with integers. Once these limitation are fixed the usage is likely to increse and therefore this usecase will not be so uncommon as is today. 
my own
&gt; So I'd have to return the entire object? You could also return an owned String.
Maybe interesting in that context: https://internals.rust-lang.org/t/should-there-be-a-rust-planet/3434/9
I get what the error is, its usually because of me forgetting to count from 0, like if you have a 4 element array and address element 4 (which doesn't exist). Alternatively, I sometimes get mixed up with 2D arrays and address the wrong dimension.
This is a really good point.
If its just 4 a struct or tuple might be better.
I just read whatever's posted here/ the rust twitter/ TWIR
That was just an example.
I'd bet that it is possible, with suitably strong type machinery. You just need a language with dependent + linear typing, and the insane desire to write code in it. At that point any run-time bug is a bug in the spec (i.e. the type of your program).
I have the email alert set up for TWiR and I always get it a day or 2 after its posted here, is that the same for everyone or just me?
I can't point to any example code outside of Redox. It's usable outside Redox because it uses SDL2 on Linux. I've been able to compile and run a simple SDL window with a few widgets. It's lower level than established toolkits like GTK though.
Rust's had a CoC since pretty much the beginning, has neither been used as a weapon by trolls nor killed the language. Hasn't really had to have been enforced very often, either. Really the only downside of having it has been the concern trolling comments like yours that pop up every so often.
I'm sorry. I expressed an opinion different from yours, therefore I'm a troll and should be banished from your community. I felt so welcome up until this point. Gee.
You can have a type checker which always terminates, or you can have a type checker which encodes arbitrary program invariants in the type. You can't have it both ways, unfortunately, and either option precludes the possibility of actually running fully verified programs.
You're thinking that you'd want to catch functions like: fn get_second&lt;T, N: usize&gt;(vals: &amp;[T; N]) -&gt; &amp;T { &amp;vals[1] } and flag that there needs to be a constraint that N &gt; 1, the same way as if you were missing a trait bound?
[Niko's blog](http://smallcultfollowing.com/babysteps/) is really great for reading about the history of Rust. To take a random example, [here's a post from 3 years ago](http://smallcultfollowing.com/babysteps/blog/2013/06/11/on-the-connection-between-memory-management-and-data-race-freedom/) in which he explains the (then) recent realization that Rust's ownership model helped eliminate data races.
You should probably return the Error; your function will then return the same error type as all of the io functions, which can make it easier to compose with them if you want to do that later.
&gt; As was pointed out, how is that enforced? Erm, trivially? Blogs that don't follow CoC are removed from the site?
Thank you. That did the trick. 
Thanks - that did it
Never mind, solved it.
That also happens to me.
It might terminate with "please assist me, programmer". Every time, forever, if you're not smart enough to prove the invariant you want. What has actually happened is that your brain has been embroiled in a non-halting verification algorithm ;). Sure, you might have better heuristics for deciding when to give up, but you haven't changed the fundamentals of the proble.
[removed]
You would need to solve the halting problem and that is impossible.
While I agree with you in that this error is not very common I also think that it should result in at least a warning.
For a language created expressly with the intention of catching all errors at compile-time, the Halting Problem isn't really the core issue. It's easy to imagine a language where we can trivially solve the halting problem. Just exclude control-flow operations (while, for, goto, function call) from the language, and it's obvious that every program will terminate. The real problem is creating a language with control-flow operations and other data structures that can be statically analyzed. For instance, if you include a while loop in the language, you must also provide compile-time machinery for ensuring that the loop condition will eventually be false. In cases where it is statically verifiable that the loop condition becomes false, the program can be permitted to compile and every program will still terminate. However, if the compiler is unable to determine whether or not the loop condition will eventually become false, it must refuse to compile again. By refusing to compile more programs that *might* be valid, you can create stronger conditions on the types of programs that can run. Of course, the more of these constraints you have, the smaller the set of valid programs is, and so after a certain point it becomes harder for programmers to do their jobs (as they're constantly having to write induction-style proofs and other compiler-verifiable bits of logic).
&gt; Of course, the more of these constraints you have, the smaller the set of valid programs is, and so after a certain point it becomes harder for programmers to do their jobs You *must* give up Turing completeness before you solve the halting problem. I suspect - haven't proven it but it feels like a reasonable conjecture - that this super-safe language is not capable of compiling itself.
One exists in clippy. I'm open to it being uplifted into rustc itself.
This is marching towards a dependent type system. Nothing wrong with those, but I don't see Rust getting one.
I know, just pointing out that in a large chunk of these cases a strict works better anyway.
It's not built-in, but you can do it with a library like `lazy_static`.
It either leaks to the signature or to the runtime behavior (additional panic). I think representing it in the signature is better because it forces the client decide what to do on error, instead of implictly panicking. If the client wants to panic on error it's easy to just unwrap.
Aren't most of the C catch errors examples going to look like this? extern fn my_callback(a) -&gt; c_int { if let Some(err) = catch_unwind(|| /* ... */) { -1 } else { 0 } } I don't think it can panic.
As /u/PM_ME_UR_OBSIDIAN mentioned, Turing-incomplete languages (like Agda and Coq) exist and are useful for some types of problems. I didn't understand that your claim of "that is logically impossible" meant "that is logically impossible for Turing-complete languages." Thanks for the clarification.
Agda and Coq can compile themselves, they just can't prove themselves consistent.
to add to the parent comment: Returning the `Error` directly needlessly exposes an implementation detail as part of the API. You'll be locked into using OpenOption unless you're willing to break the API.
Started on bindings here: https://github.com/pcwalton/libui-rs Very incomplete WIP.
~~No, this is incorrect. You can read [Huon Wilson's blog post](http://huonw.github.io/blog/2016/04/memory-leaks-are-memory-safe/) for some more information, especially the section about `std::mem::forget` (which is not unsafe, and throws away a value without running the destructor).~~ Edit Ah, now I get it. 
How do I go about providing global access to a variable that can't be shared between threads? I'm trying out an MDL renderer built with Glium, and want to provide global access to the display window, since this contains the context and is therefore necessary for creating textures and other OpenGL objects. The window can't be declared as static because it requires initialization. The natural solution would be to use `lazy_static!`, but OpenGL throws a wrench in the works: &lt;lazy_static macros&gt;:35:22: 35:34 note: `alloc::rc::Rc&lt;glium::context::Context&gt;` cannot be sent between threads safely My understanding of the synchronization primitives in Rust is not great. I've tried putting the window in a `Mutex` and an `Arc` but neither works. Any good solutions to this problem?
Many of them might look like that, but I also believe there will be many that want to report the type of error - as a string, an error object or whatever - and even the slightest amount of calculations could panic for some unforeseen reason. Edit: Btw, what if the `err` object in your example above has a destructor? The destructor will run outside `catch_unwind`, and it might panic.
Yes, that is why unsafe code that relies on destructors being run should never give away ownership of the type whose destructor needs to be run, so it can be certain that nobody will `forget` it, or put it in a `Rc` cycle, or whatever.
I'm not aware of any in the stdlib, but pretty much any implementation of "scoped threads" requires that destructors are invoked during unwinding. Here's crossbeams: https://aturon.github.io/crossbeam-doc/src/crossbeam/scoped.rs.html#59-64 Ownership of `Scope` doesn't leave `fn scope`, so it is clear that the value cannot be passed to `std::mem::forget` or put into an `Rc` cycle. Thus the code can rely on `impl Drop for Scope` joining the threads started in the scope, which is necessary for memory safety.
Ah, this is a good point. If you rely on destructors being run (or process abort) for stuff you don't give up ownership for, then "the third mode" won't work for that, it needs to be rewritten to use `catch_unwind` instead.
I doubt Mutex helps here, Mutex doesn't make a type Sync or Send.
Agh, that's so cool. I'm very envious of this sort of environment - one where languages are treated as specialized tools. I don't get the attitude that lends itself to "We are an X or Y shop" - why? Different languages provide different benefits, it seems so limiting to choose just one or two and stick to them. Also, yeah, going from languages with pattern matching to languages without it sucks. Especially for error handling.
Destructors panicking is UB anywhere in Rust code, not just in FFI, I believe.
Nope.
I don't think this is UB. Destructors can panic.
For anyone who doesn't know, unwrap needs the type to be Debug so that it can put it in the error message.
&gt; Ideally, there shouldn't be a difference between looping with an index and looping with an iterator. Taken to an extreme this is saying something like "ideally, there shouldn't be a difference between performance of Python code and performance of Rust code". While both are true, the unfortunate reality is they're not: compilers have time budgets and so can't do things (inferences, alias analysis, etc.) that are too expensive, and they have to be fairly conservative about what optimisations do to not break even the curliest defined code. Iterators have the performance advantage of packaging all the information the iteration needs in one place rather than separating the "progress" from the use, a use which requires checks. Having everything encapsulated allows leaning on `unsafe` code and undefined behaviour for performance. Additionally, ignoring the performance question, iterators compose better than manual loops, allowing one to build on existing generic algorithms like `let all_empty = foo.iter().all(|x| x.is_empty())`, which is nicer and less error-prone than let all_empty = false; for i in 0..foo.len() { if !foo[i].is_empty() { all_empty = false; break } } 
Yep, double-panicking results in a process abort, but is not UB.
I feel the pain, but share the same conclusions of /u/matthieum. When people discuss languages, they seem only to think about the programming language, forgetting that it brings along: - maybe new paradigms - features not available in other languages on X shop - IDEs and editor support - Available toolchainspossible) - Libraries (and maturity) for all required use cases - Testing frameworks - Communities - Deployment options So the development costs usually increases exponentially with each new language that gets introduced.
I guess I shouldn't say that I don't understand the attitude of not adding new languages, there are, of course, real costs. I think what I dislike is the attitude that new languages won't provide benefits.
That one's a doddle, but I regularly run into things like: are all elements equal, which can't be written with a .all(), I think, and I also remember running into some borrowing problem which was easily solved by using a counter. &gt; While both are true, the unfortunate reality is they're not: But we also don't want to write assembler code, do we? &lt;&lt;insert smiley of choice&gt;&gt;
I see now, thanks. I should have read more carefully. In this case, I don't think there's anything you can do - the glium/glutin API is not designed to let a thread other than the creator access the Window. Looking at the glium docs, it says so explicitly on the front page: &gt; The display object is the most important object of this library and is used when you build buffers, textures, etc. and when you draw. You can clone it and pass it around. However it doesn't implement the Send and Sync traits, meaning that you can't pass it to another thread. So I don't think this is something that /u/tomaka can or wants to change.
[Here you go](https://crates.io/crates/ruby-sys).
But lazy_static requires the type to be Sync.
"Crates" are Rust's compilation units. They're compiled independently from one another, and linked together when you build the final executable. Crates are provided by downloading a chunk of Rust code and compiling it into a library, and then telling `rustc` where that library is located when you build something that links to it. You can see exactly what is happening if you run `cargo build --verbose` to have it print out the exact commands that it's using to run the compiler.
You have no way of creating an owned texture with a lifetime of `'s` inside that function. rust-sfml's documentation shows no way of making a sprite own a texture. I see one way of avoiding the problem, which is creating the texture outside Obstacle::new: struct Obstacle&lt;'s&gt; { sprite: Sprite&lt;'s&gt;, } impl&lt;'s&gt; Obstacle&lt;'s&gt; { fn new_with_texture(texture: &amp;'s Texture) -&gt; Obstacle&lt;'s&gt; { Obstacle { sprite: Sprite::new_with_texture(texture) .expect("Could not create sprite."), } } } fn main() { let texture = Texture::new_from_file("../resources/treeSmall.png") .expect("Could not create texture."); let obstacle = Obstacle::new_with_texture(&amp;texture); } Edit: fixed formatting. Perhaps your problem could be blamed on rust-sfml's API design, but I'm not familiar with it.
Of course, and you need no to use any value coming from outside the program itself. 
Whoops, I probably should have read the docs more carefully. Thanks for clearing that up though, I'll just have to pass the window around.
I'm working on a BitTorrent client. I'm new to Rust and haven't done much low-level network programming before, so I'm learning a ton. I've got a working Bencode encoder/decoder and I'm using the `hyper` crate for HTTP communication with the trackers. Next step is to implement the hard part, communication with peers. I'm loving Rust so far (coming from a Python/Go background).
I thought about it, but a comprehensive example is pretty verbose for a README. If you look at the test suite, you can see a more complete example. I also considered just making the entire test suite doc tests, but then you lose nice test names and have to edit the source cod all within a comment. Any idea on what would be a better presentation? Just duplicate the code from the tests into the docs?
What is the bigger picture (assuming there is one)? I have a hunch that you've probably stumbled upon the [XY problem](http://xyproblem.info/) and composition vs inheritance may not be the question you actually want to ask about.
Essentially, how to build good program architectures in Rust. How do I replace inheritance? 
You don't *replace* inheritance. You just build stuff differently. It's gonna depend entirely on the use case how that's done.
Which it becomes (if Send) when in a Mutex.
Here is an example of using composition in rust struct Shared { x: u32} struct A {shared: Shared, a: String} struct B {shared: Shared, b: i32} trait GetShared { fn get_shared(&amp;mut self) -&gt; &amp;mut Shared; } impl GetShared for A { fn get_shared(&amp;mut self) -&gt; &amp;mut Shared{ &amp;mut self.shared } } impl GetShared for B { fn get_shared(&amp;mut self) -&gt; &amp;mut Shared{ &amp;mut self.shared } } But in many situations it is easier to use a struct with an enum field enum Kind { A(String), B(i32) } struct Foo { x: u32, kind: Kind } fn main () { let _x = Foo { x: 42, kind: Kind::B(7) }; } 
I have already read the related pages in the rust book, but they don't describe how to apply stuff for real, thanks for your third link, that seems perfect 
I also built this example just now, in case it helps at all: [example](https://is.gd/BYjzqx)
Very good point. In this case the crate's error types might by useful on their own without the trait.
Class inheritance is usually a bad idea, sometimes it's handy but you don't really need it. Maybe you should try to think, "how would I solve this problem in Java without using Class inheritance?". I found [this](http://www.artima.com/designtechniques/compoinh.html) article, which explains it in Java. 
The reason I gave the good ol' explanation with ASTs is because even though you can understand that you can keep two parts of a sufficiently long string in sync wrt repetition with a PDA because you have a stack, it's much harder to understand why that's the case. For example, what's the minimum string length for a PDA in the pumping lemma? In a DFA that's the number of nodes, because at the end of the day it all boils down to finding a cycle in a directed graph. In a PDA, OTOH, it becomes way more complicated because you now also have to take into consideration that you also have access to a stack. Then you also need to figure out what it means to be able to replicate parts of the string. Does it mean that you can find 2 cycles in the path, where the first precedes the second and the first cycle pushes N elements to the stack, whereas the second cycle pops N elements from it? Or is it something else? It may be possible to give an intuitive explanation of the pumping lemma with PDA ([For instance, there's this proof](http://cstheory.stackexchange.com/q/6954), but I haven't really bothered to look into it in detail), but IMHO it's much easier to just stick with ASTs at the end of the day. **TL;DR**: There's nothing wrong with your explanation. It's just that if someone wants to delve into why the pumping lemma holds by using PDA, they're most likely going to hit a brick wall, whereas they'd probably have to only climb a fence if they used ASTs.
Haha, it's cool. Doesn't detract from the actual content.
I guess [Zalando](http://www.zalando.com) should be added to the [Rust's friends] (https://www.rust-lang.org/friends.html) list :)
Minor nitpick: you refer to the "`Option&lt;T&gt;` trait", but `Option&lt;T&gt;` is an enum.
I have one too! https://twitter.com/jimmycuadra/status/730277464706945024
New error now, I swapped in ruby from `require` to `load` specifying the `.dylib` -- this _appears_ to me (being inexperienced here) in achieving what I want. I get a different error this time around. test.rb:2:in `load': /Users/brandonbuck/Dev/rust/ruby_test/ruby_things/target/debug/libruby_things.dylib:1: invalid multibyte char (UTF-8) (SyntaxError) from test.rb:2:in `&lt;main&gt;' I'm not sure what the problem here is, I'm doing a conversion (which according to my understanding of the docs should be valid) from Rust `&amp;'static str` (I think that's the type) to `*const c_char`/`*const i8` with `CString::new("RubyThings").unwrap().as_ptr()`.
agreed with the statement about iterators being more efficient. For example, here I document performance improvements w/ iterators as opposed to indexing http://www.suchin.co/2016/04/25/Matrix-Multiplication-In-Rust-Pt-1/
You don't need to use core intrinsics to use [TypeId](http://doc.rust-lang.org/std/any/struct.TypeId.html) - they are stable, and usable (but might change from one compilation to another, so don't store them on disk or so). Your `T.id_static()` could be replaced with `TypeId::of::&lt;T&gt;()` and the `T.id()` method could be replaced with [Any.get_type_id()](http://doc.rust-lang.org/std/any/trait.Any.html), provided your traits inherit from `Any`. 
Why not? Are you aiming to create an extensible library? Perhaps you can reveal a little more information :-) EDIT: Found it (I guess) https://github.com/Limeth/euclider EDIT 2: One idea is to use an enum to wrap boxes with traits. Now you can add new variants by implementing trait 'B' or trait 'C', but the enum ensures that you always know the relevant traits implemented by a given variant. struct A {} trait B {} trait C {} enum Object { A(A), B(Box&lt;B&gt;), C(Box&lt;C&gt;) } 
I think people here are sometimes unable to comfortably answer this question because they jump right to short-selling inheritance as a very pointlessly inflexible interface. That is not the only appeal of inheritance. If the only reason people used inheritance was because 1. they didnt have interfaces yet or 2. they were not very smart, people would have stopped using them long ago. A major appeal of inheritance is rough behavioral guarantees, it allows you a certain level of invariance/design-by-contract within a class. Composition is said to replace inheritance (rather than interfaces or roles/traits alone) because it is a pattern which can provide those behavioral guarantees. Here's a concrete example of using composition in a way to provide those guarantees (in some composition examples the "base" class is the one encapsulated, this is the opposite). In this example, you are guaranteed that the result of talkTwice will always be the same result as if you had called talk() twice. You would never have an animal where calling talk() twice and calling talkTwice() in the same context would produce different results. This is not a guarantee that appears very useful, but it is selected for simplicity of example. In an inheritance model I might do something like (forgive the mishmash of c-like languages I am using below): abstract class Animal { public Animal() {} public abstract String talk(); public final String talkTwice() { this.talk() + this.talk(); } } class Dog extends Animal { public String talk() { return 'bark!'; } } class Human extends Animal { public String talk() { return 'hello'; } } In a composition model I would do this Interface AnimalSpecies { String talk(); } class Animal { private species; public Animal(AnimalSpecies species) { this.species = species; } public abstract String talk(); public final String talkTwice() { this.species.talk() + this.species.talk(); } } class Dog implements AnimalSpecies { public String talk() { return 'bark!'; } } class Human implements AnimalSpecies { public String talk() { return 'hello'; } } 
As part of learning Rust more in depth I'm trying to figure out a good way to implement flexible compile-time expression templates, maybe even with lazy evaluation of more complex expressions if I get that far. 
Working on thread-local (well, coroutine local) garbage collectors in [embed_lang](https://github.com/Marwes/embed_lang) sort of like Erlang. The idea is that not only will the smaller heaps mean less runtime overhead for each collection but the garbage collector may not even have to run at all for really short lived threads as all the data owned by the thread could just be dropped by the thread as it goes out of scope. Having separate gcs also means less or no synchronization between each collector which should pave the way for running embed_lang programs in parallel.
Slowly but surely making progress on my GBA emulator [Pyrite](https://github.com/ExPixel/pyrite). Working on sound right now and currently have 2 out of 6 channels working (The Pokemon Fire Red intro music is starting to sound right). I would appreciate it if anyone could point me in the direction of a library like [rustbox](https://github.com/gchp/rustbox) that also works on Windows since that's currently the only thing stopping the emulator from building on the platform.
I released [netsnmp-sys](https://crates.io/crates/netsnmp-sys/), and am working on a [rusty wrapper](https://github.com/hroi/netsnmp/blob/master/src/lib.rs) around it.
Implemented these pull requests for new features to the [IntelliJ Rust Plugin](https://github.com/intellij-rust/intellij-rust) * [#388: Quick documentation support for resolved items](https://github.com/intellij-rust/intellij-rust/pull/388), shows formatted documentation on Ctrl+Q/Ctrl+J when caret is over resolved references with attributes. Fully supports both doc and comment, inner and outer documentation as emitted by RustDoc, but the formatting is currently incomplete. It also cannot handle cfg-gated doc attributes (though this is rarely used). * [#393: Add "remove parentheses from expression" intention and annotators for unnecessary parentheses](https://github.com/intellij-rust/intellij-rust/pull/393) * [#408: Add documenting for trait members](https://github.com/intellij-rust/intellij-rust/pull/408) * [#409: Add more annotations for unnecessary parens](https://github.com/intellij-rust/intellij-rust/pull/409) * [#410: Add more icon getters.](https://github.com/intellij-rust/intellij-rust/pull/410) Shows more icons in completions/file structure views. There is a WIP branch for type inference and resolution that I have also worked on a bit. [My branch of that branch](https://github.com/Furyhunter/intellij-rust/tree/types) supports some modicum of type inference and completions for many common expression forms. It's very incomplete, but can already provide completions close to Racer in functionality.
Thanks /u/CryZe92 and /u/nsan1129 for your helpful comments. I've implemented this function in particular, and am continuing to muddle my way through wrapping the fits library. [I've put the project up on github][1]. Unfortunately I'm dealing with segfaults at the moment, but that's to be expected from calling unsafe code! [1]: https://github.com/mindriot101/rust-cfitsio
I'm really looking forward to meet some new people at the GPN in Karlsruhe, Germany this weekend and talk about Rust, crates and especially maybe finding some contributors for me project "imag", the [commandline personal information management suite](https://github.com/matthiasbeyer/imag). Besides this I really want to get another crate started for writing scrobble applications in Rust, though I havn't started it yet, so maybe GPN will lead to something useful. I also want to get some more things implemented for imag, for example the bookmarking module, the diary module (which is pending for weeks now).
Parallel Array Processing TL/DR: * **q1)** What's the rustic way to process a big array in place with parallelism? * **q2)** Is `crossbeam::scope` safer than `std::thread::scoped_thread`? * **q3)** How do you tell a thread to write its result into a slice? Longwinded Question: This must be a common Rust pattern, but I can't see how to write it. We have a big array. We want to do some calculation on every element, and we want to get all the CPU cores involved. **q1) What is the rustic way to do this?** For example, I'd like to translate this: let mut array = [0; 100]; for chunk in array.chunks_mut(8) { for elem in chunk { *elem += 1; } } Into something like this: let mut array = [0; 100]; // error: `array` does not live long enough let threads = array.chunks_mut(8) .map(|chunk| thread::spawn(|| { for elem in chunk { *elem += 1; } })); threads.map(|t| t.join().unwrap()); But that does not compile. I read about scoped threads and about why they were removed. I also found that `crossbeam` still has scoped threads. And they work great for this example: extern crate crossbeam; let mut array = [0; 100]; crossbeam::scope(|scope| { for chunk in array.chunks_mut(8) { scope.spawn(move || { for elem in chunk { *elem += 1; } }); } }); **q2) Is `crossbeam::scope` safer than the deprecated `std::thread::scoped_thread`?** I tried some other non-crossbeam things like `Arc&lt;[i32]&gt;`, `Arc&lt;RefCell&lt;[i32]&gt;&gt;`, and Arc&lt;RefCell&lt;Vec&lt;i32&gt;&gt;&gt;`, but could not get past `error: \[something\] does not live long enough`. **q3) What is the best way to tell a thread to put its result into a slice without copying?** Thank you.
Unfortunately, I won't be able to make it to GPN. :-( Have fun!
I'm working on making an n-dimensional/n-order bezier curve library. 
Hi, I'll be on GPN and give a talk about Rust. Any way I can contact you once I'm at the place?
I'll PM you.
Please publish whatever benchmarks you did, your numbers sound impressive!
The benchmarks are in the benchmark subdirectory. Run these commands to try them yourself: cd benchmark cargo run --release --bin mutex 0:8 1 0 3 cargo run --release --bin rwlock 0:4 0:4 1 0 3 EDIT: Add `--features nightly` if you are on nightly, it enables a few extra features that can help performance.
For now, I stopped coding [Panini](https://github.com/pczarn/panini/), a half-finished general-purpose parser generator. I'm going to write some documentation for it and its engine, gearley.
What does CFP mean?
That article also recommends using an entire cache line per lock to improve cache behaviour when multiple CPUs are present. Is that relevant here? Is it actually optimal to reduce the size of a lock as much as possible?
I have std::sync::mpsc ported to this library at https://github.com/polachok/mpsc
It is touched upon in the issue [29701](https://github.com/rust-lang/rust/issues/29701), but no thorough discussion yet.
God I love seeing how the Rust Community thrives. Maybe I can be there.
I have a question: Do you have a location for sleeping already in mind / a sponsorship or similiar? That would be pretty awesome. You could make different tickets depending on whether people want to be with hotel or arrange their stay themselves.
Is there any article summing up the strengths/weaknesses of all those different type of parsers? I have seen so many different (nom, lalrpop,pest, etc) libraries and no idea what are the differences between them. Ie I might want to replace my handwritten parser for https://github.com/Keats/tera/ if using one of the library above is better but I don't know which one would be the best fit and no time to try all of them.
This is actually a pretty important thing to consider when writing unsafe code. But in the end, its always the end-user who has full access and if he wants he can destroy anything.
I guess that the idiomatic rust approach would be to add talk_twice as a default method to the trait named Talk. trait Talk { fn talk(&amp;self, s: &amp;mut String); fn talk_twice(&amp;self, s: &amp;mut String) { self.talk(s); self.talk(s); } } https://play.rust-lang.org/?gist=23b00affadcd492a1e44a24d328e2c10&amp;version=stable&amp;backtrace=0
From [the docs](https://amanieu.github.io/parking_lot/parking_lot/struct.Mutex.html): &gt; # Differences from the standard library `Mutex` &gt; &gt; - No poisoning, the lock is released normally on panic. &gt; - Only requires 1 byte of space, whereas the standard library boxes the &gt; `Mutex` due to platform limitations. &gt; - A `MutexGuard` can be sent to another thread and unlocked there. &gt; - Can be statically constructed (requires the `const_fn` nightly feature). &gt; - Does not require any drop glue when dropped. &gt; - Inline fast path for the uncontended case. &gt; - Efficient handling of micro-contention using adaptive spinning.
Is `&amp;*string` the same as `string.as_str()`?
Could your implementation be merged into `std` without breaking backwards compatibility? I think that your implementation of `Condvar` and `RwLock` works on Windows XP is a win for the presence of Rust code in Firefox, because Firefox still wants to support this platform (IIRC Windows XP has still more users than Linux).
Answer to Q2: Yes, `crossbeam::scope` does not have the soundness issues that `std::thread::scoped_thread` had.
Interesting. This works. use rayon::prelude::*; let mut array = [0; 100]; array.par_chunks_mut(8) .for_each(|chunk| for elem in chunk { *elem += 1; }); I wonder why `rayon::par_chunks_mut` does not implement the `Iterator` trait. Nonetheless, this works. **EDIT:** `for_each` is what spawns and joins the threads. Of course that can't be done in a for loop, so there's no reason to implement `Iterator`. Now I have to understand whether rayon threads can `recv`. I think they can't. 
I'm trying to learn Rust by implementing a stack and (later) a small RPN calculator on top of it. For *reasons* (really for the exercise) I decided my stack would take i32s, f32s and Strings. Which meant I needed to make the stack generic to a Stackv enum, and the push method (and others) generic, bound to a ToStackv trait. Now I worry that I'm not doing this the properly Rustic way. I'm especially concerned that I had to monkey-patch the ToStackv trait into the i32, f32 and String base types, something the book says is a bad idea. I'd like to hear if I'm on the right track doing things this way, and how to do this if I'm not. Here's the abridged code: enum Stackv { I(i32), F(f32), S(String), } trait ToStackv { fn to_stackval(self) -&gt; Stackv; } impl ToStackv for i32 { fn to_stackval(self) -&gt; Stackv { Stackv::I(self) } } impl ToStackv for f32 { [...] struct Stack { s: Vec&lt;Stackv&gt;, } impl Stack { fn new() -&gt; Stack { Stack {s:Vec::&lt;Stackv&gt;::new()} } fn push&lt;T&gt;(&amp;mut self, x: T) where T: ToStackv { self.s.push(x.to_stackval()); } fn pop(&amp;mut self) -&gt; Option&lt;Stackv&gt; { self.s.pop() } }
Answer to Q1: It Depends. :) I would reach for crossbeam first when doing this, or possibly rayon, depending. Crossbeam is a bit older so I have a habit of reaching for it.
It's error-prone, but not any more unsafe than destructors, which can just as easily see objects in an inconsistent state due to panics. The standard library mutexes allow you to bypass the poisoning, anyway, and those methods are not unsafe.
It seems a bit like the [Expression Problem](http://c2.com/cgi/wiki?ExpressionProblem). Rust is a language with trait so the solution would be to use them. Choose if you want to make traits of shapes or materials. For example with shapes trait make each shape type a trait with a single method and implement them on each material. It will be monomorphized and easily extendable in each dimension.
Oh, right. Forgot that. Thanks. :) 
I have poisoning support implemented in a [branch](https://github.com/Amanieu/parking_lot/tree/poison), however I have not merged it in for two reasons: - Poisoning makes the API ugly and hard to use. - The benchmarks become around 10% to 15% slower with poisoning support. If I'm going to end up replacing the standard library primitives with this then of course I'll have to include poisoning support for compatibility reasons.
Would it be possible to add lock poisoning to the mutex to make it a drop-in replacement?
The compiler just needs to get better at eliding bounds checks (and copies/moves, and dead stores to zero memory right before the memory is overwritten). C compilers are actually pretty good at optimizing away these things. Apparently a lot of that happens in the front-end (before LLVM). This gives hope that we'll get better optimizations for safe coding idioms shortly after MIR is "done."
Not having to trust the compiler to figure out how to make slow code fast is half the reason to use Rust. "Sufficiently smart" compilers are notoriously unreliable, and there's a *long* way to go before these sorts of things are close to automatable.
Well, here are a few of the ideas behind pest: - everything works with macros, so you won't have an intermediary step or need to use nightly - it uses https://en.m.wikipedia.org/wiki/Parsing_expression_grammar to express grammar which I would argue is pretty intuitive - I'm trying to maximise performance over here and generate a parser that's as efficient as possible - works with UTF-8 - generates errors based on the deepest rules (compile master; errors would be 'expected int, string or object'-like) - I'm planning to implement Packrat in order to speed it up a little bit more by using memoisation Now, this comes with a few reality checks: - its current state is a bit experimental; I'm planning to write a Lua parser and put it to the test - I'm going to measure its performance vs. ANTLR 4 today and probably post the results on the repo - the resulting output from the parser is a stream of indexed tokens that you can use to index the input - it's currently rough around the edges to do this separately, so what I'm going to do is to design a macro to be able to handle output easily AND on a separate thread; so parsing will work on one thread, and matching and handling the input on another - the current state of the documentation is decent, but it doesn't offer a clear or obvious way for new users to get started, so I'll have to work on that really soon too This is pretty early, so I will probably do a few more updates on Reddit. I'm definitely going to make a few comparisons so it's easier for people to choose, but right now I would definitely appreciate a bit of testing and feedback since the crate is really new.
Excellent article. I'm working hard to get Rust used in my company. I need to synchronize Rust code with C++/gcc code. Is your parking_lot crate compatible with the WTF HandoffLock/ParkingLot ? 
:) Something like this is already feasible in Haskell by the way, using something called "generalized algebraic data types". And if you're willing to go further down the rabbit hole, "dependent types" as in Coq and Idris. Hit me up if you need reading material.
Gah, I knew I forgot something :) I am not 100% sure what "put its result into a slice without copying" means. Moving something from one place to another results in at least some kinds of copy, and putting things _into_ a slice only makes sense with a mutable one... but, re-reading it now that I've had some coffee, I understand what you mean, I think. That last code, yes, that's what I'd write.
&gt; It's also exactly equivalent to a C++ std::unique_ptr. This is really the crux of the post. It's not exactly equivalent; Rust's guarantees around these things are stronger. `unique_ptr` can cause a segfault, `Box&lt;T&gt;` cannot. But to someone who's already deeply entrenched in C++, it is a significant improvement, even if it's not a complete, foolproof one, and depending on their attitude, may not be enough to justify switching. There's also tons of other things as well, but this post seems concerned with the core language semantics, not stuff like, for example, Cargo.
We have no location for sleeping in mind. Berlin is full of Hotels and Hostels and the probable venues is easily reachable from any. Once we commited to a venue, we will give a list of good locations nearby.
I get the impression that the author has not looked very closely at Rust. `unique_ptr` is nowhere near exactly equivalent to Rust's built-in ownership and move semantics, Rust does work out of the box with GDB, and I don't see references or the borrow checker mentioned at all. It's possible that the author doesn't care about Rust's memory-safety guarantees, in which case they should indeed likely stick with C++, but overall this article is a poor comparison.
You bring up a good point. This situation that, memory safety depends on functional correctness, occurs whenever `unsafe` code relies on (calls) safe code. True, this is a wider dependence when the safe code called is a parameter, but it can change without explicitly being a parameter, e.g., if the version of a dependency changes. This might be something we need to get used to, wary of, and document carefully. True _guarantees_ of safety around unsafe code will probably _require_ actual use of formal methods.
LALRPOP and nom are definitely more feature full than pest will probably be. They both have tremendous work behind them, and I would totally recommend you used one of them (probably LALRPOP would fit the job nicely) instead of pest mostly because pest needs some time to settle and cement. I still need to add a CI and way more testing too. The idea behind pest is to have a very simple grammar (almost pure PEG) that is smart enough to generate a super-fast parser that knows its error reporting. I'm willing to work on it until it shines and there's still much work to do, but I want to keep it simple. Still, before hopping onto LALRPOP, do take a look over the [json example](https://github.com/dragostis/pest/blob/master/tests/json.rs#L18) and tell me what you think. :D
&gt;It's possible that the author doesn't care about Rust's memory-safety guarantees But Rust is not only about memory safety. I find it sad that the concurrency aspect is never mentioned in comparisons with C++. 
&gt; Try calling fork(2), a Unix system call coming up on 50 years of age, on Go, Rust, or Java. None of these languages support it because forking a process requires careful handling of file descriptors that survive the fork. Rust supports fork, we even use it in ipc-channel. &gt; because they internally use evented I/O loops Not Rust. &gt; It's also exactly equivalent to a C++ std::unique_ptr. Without the compile-time uniqueness check, yes. It's still very easy to mess up with unique_ptr because you might end up having a pointer to a unique_ptr escape scope or something. In Rust ownership goes hand in hand with borrowing to give you a usable system that prevents segfaults. On the other hand, to use unique_ptr in C++ you have to be very careful (the compiler isn't going to help), and often break out shared_ptr, which has a cost.
&gt; I get the impression that the author has not looked very closely at Rust Agreed. This is another hiccup: &gt; Try calling fork(2), a Unix system call coming up on 50 years of age, on Go, Rust, or Java. None of these languages support it because forking a process requires careful handling of file descriptors that survive the fork. As far as I know none of these language support fork specifically because they internally use evented I/O loops and no one knows how to expose this in a sane way to application developers. It's understandable to have not heard that Rust got off of evented I/O loops, but it seems odd for that confusion to make its way into a critique such as this one.
You can specify what the RHS is. For example: https://is.gd/sJkB1l Taking the RHS by value is more flexible because you can take an object by value, or you can take a reference to an object also by value, which may be cheaper.
Sure, but this article in particular, IMO at least, doesn't convince the reader that a proper analysis of the trade offs has actually been applied. I'm totally on board with "Go's data race detector is good enough," but I don't think it's fair to draw that conclusion when the extent of the trade offs described is "C++ has `unique_ptr` which is *exactly equivalent* to Rust's pointer ownership with borrow semantics." This article has other issues unrelated to Rust. Its treatment of Go is incomplete, particularly around the comments about polymorphism and assembly. The article doesn't mention that Go *does* have type safe polymorphism (even if it doesn't strictly meet the author's requirements, which are imprecisely stated) nor does it mention that one can pretty easily integrate Assembly into your Go program, even if it's not exactly "inline." (It's restricted to the function level, which does put it at a disadvantage, but I find it odd that it would be omitted.) Personally, I don't think there's a lot of value in language comparisons where the author isn't an expert in all of the languages being compared.
I want to use `grammar!` inside of `impl_packrat!` as well. The structure is currently not so cool. Maybe there's a better way of organising everything, but right now, I see none.
Posts like this make me think that people who like C++, and are C++ experts, and are productive in producing human-verified safe code, are not really the core of Rust's target audience. The people who are, are those who want to write high-performance code without having to learn all the things that you have to know about writing safe C++ code that the Rust compiler will enforce for you (in safe Rust), and who don't have the "sunk cost" of having learned C++.
Thanks! This is exactly what I needed to know. I didn't know that you could assign the RHS type. I guess I also just learned about the syntax of the trait definition. pub trait AddAssign&lt;Rhs=Self&gt; I assumed that this was a type assignment that could not be overridden. But I guess now I know that's just a default assignment for the type parameter.
I can see that `ws` is hardcoded, what's the purpose of the `_` then? Maybe rename `ws` to `whitespace` to be clearer as well, that would be clearer (if `ws` actually means whitespace).
Plenty of room for multiple APIs ;) Nice to know you are close by though!
I maintain that *nobody* is capable of producing high-performance safe code in large C++ codebases. They just think they are because nobody is actively trying to attack the code.
And just a heads-up, it's Crate of the Week! :-)
I would be happy to argue that, but I rarely ever see anybody make the argument that directly. Instead, like this article, they falsely claim that C++ offers complete memory safety (or memory safety sufficient to prevent UAF in practice), which flies in the face of all the real-world data we have.
`_` means that token will not be generated by the rules nor errors reported. As I said, the documentation is a bit lacking. :D
Found https://github.com/ptal/oak while looking around, is this what you're going for (with rust stable compilation)? A calculator like in that project (http://hyc.io/rust-lib/oak/full-calc-grammar.html) could be a nice example 
&gt; Is that relevant here? Maybe. Putting all your one-byte barriers in a array would be the worst way to do it. Each will potentially fight with 63 others. If the sync variable is embedded in a struct that's at least one cache line long this isn't an issue. 
Also `std::vector` cannot use `realloc()` under the hood, because C++ objects cannot be assumed to be relocatable (except for standard layout type objects) and `std::allocator_traits` doesn't expose a `reallocate` or equivalent function.
ages ago I suggested that in the case where 'C' is an application, why bother worrying about any breakage; 'C' could just fix it if A &amp; B later cross-implement. Why not just make it a warning, or an error by default which you can optionally disable. So much code is written that other people will never re-use, so it's not worth worrying about. You still benefit the ecosystem and community by increasing mindshare. Making it harder to extend just reduces the chance I'll ever look at it. 
The only missing feature is poisoning, which I have implemented in a branch. See [this comment](https://www.reddit.com/r/rust/comments/4kng0h/parking_lot_highly_optimized_synchronization/d3gh3gt).
For this exact example, I'd just pass `"/"` in, instead of making a path out of it first.
&gt; EDIT: I meant to add that I'd be interested to hear more (or get a pointer to) the exact scenario where this is arising, just for my edification. Happens for me when wrapping around the Vulkan API. The user can manipulate buffers that are located in video memory thanks to the `Buffer` struct. A `Buffer` holds a reference to a `Device`, which represents the video card where the buffer is located. This reference is in fact a template parameter that must implement `Deref&lt;Target = Device&gt;`, so that the user can choose between using `&amp;'a Device`, `Arc&lt;Device&gt;`, or something else. When the user for example wants to ask the GPU or DMA to copy from a buffer to another, the wrapper has to check that the two buffers belong to the same device. This is done by accessing the `Device` objects referenced by the `Buffer`s. By using a weird implementation of `Deref`, a user can trick the wrapper into asking the GPU/DMA to copy from a buffer or to a buffer which it can't access, which is obviously unsafe (and may even crash the video driver in the kernel, since Vulkan is so low level that it doesn't even perform basic checks). In reality it would be possible to make this code safe (by holding an identifier for the device in the buffer, and panicking if dereferencing the user-provided pointer did not yield the expected device). In fact this is what I initially went for. In practice however it makes the code slower and almost unreadable, and I switched to an unsafe trait. I don't think there's a situation where this limitation of Deref/Hash/Ord/etc. results in code that's impossible to make safe, but it definitely makes some code much more difficult to make safe. 
What's the appropriate way to name a binding if I want the name to be a Rust key word? Should I prefix it with an underscore? let use: i32; // No let _use: i32; // Maybe? 
You can just use `.foo(::std::path::Path::new("/"))` to skip the `use` statement.
 As a C++ dev, I think Rust is great, and clearly a massive improvement over C++. I haven't jumped on the Rust train for the simple reason that the tooling is so far behind C++. 
I really do wonder if this is really such a concern that Rust has to make people go the long way around. Has this been a big issue for other languages? Wouldn't it be sufficient to lock the impls away into a namespace or something?
`&amp;str` implements `AsRef&lt;Path&gt;` so you can [just pass `"/"`](https://is.gd/JMDDUe).
Plus library support; there's still a lot of gaps, or a lot of places that are filled by barely-maintained nascent libraries that could disappear from existence in a matter of weeks. I'd love to replace C++ with Rust, but as long as I have to write more Rust code to get something done than with C++, I'm going to continue to stick with writing C++.
I prefer to postfix the underscore as a prefixed underscore in an identifier will implicitly silence the unused-variable(/field/function) warning for that identifier (you see this used most often with `PhantomData` fields): // Silences unused variable warning let _use = 0; // Does not silence let use_ = 1;
Ah, I may have copied the wrong error, I was tweaking that code a lot. Ultimately, I want a struct field that's a generic function. struct Query&lt;F, A, B&gt; where F: FnMut(A) -&gt; B { filter: F, } This gives me the message: "parameter `A` is never used". Is it possible?
I didn't know that. Thank you!
I think swift comes close to that hypothetical language.
Yep, your looking for `PhantomData`! Though this gets trickier if you want your type to implement marker traits that A/B don't implement, e.g. `Send/Sync`.
&gt; and may even crash the video driver in the kernel, since Vulkan is so low level that it doesn't even perform basic checks This sounds.. absurd, and a serious regression on the stability of desktop systems. I want to rely on kernel drivers that avoid crashing when receiving bad input from userland programs, of all things.
How to implement the same methods that are added via an external **traits**? ABuilder::new() .os::linux::ABuilderExt::abc() .build(); https://is.gd/bmmC0S
It's important for performance that any atomic variables (and generally data accessed from different CPUs) occupy different cachelines (usually 64 bytes), as that is a unit of granularity for CPU coherency (read about MESI protocol for details). Otherwise it is possible that two different spinlocks/mutex would be sharing same cacheline and needlessly bouncing between CPUs, interfering with each other. Accessing other data, that shares cacheline with a spinlock, will also negatively affect the lock. Making spin-lock byte-size makes it possible for naive user to make very much worse, as one can have 64 bytes spinlocks that are all serialized as one cacheline. But that would only happen if one created `mutexes : mutex[64]`, which is not a typical pattern. I wonder if byte-sized mutex is any faster than eg. word-sized. Also if all architectures have support for non-register size atomic instructions. Maybe they do.If not, some of them might need to do some tricks to support byte-sized atomics. CC: /u/Amanieu Also, accessing any unlocked data sharing cacheline with a spinlock, will interfere with other threads. 
You don't even need to, because you get an `impl Into&lt;Stackv&gt; for i32` for free because there's a blanket impl for `T where T: From&lt;Self&gt;`
Suggestion: Post to /r/playrust instead of /r/rust. This subreddit is for the programming language called rust.
This just made everything click, thank you. 
The kernel driver crashing is always considered as a bug in the driver. In theory it's not supposed to crash, it's just that in practice it does.
Something like [this](https://crates.io/crates/unreachable)?
I believe you need to convert it to a Duration since the unix epoch first: `let std_duration = your_systemtime.duration_since(UNIX_EPOCH).unwrap()` [0] . [1] Then you'll probably want to use chrono [2] to interact with times displayed to the user (since advanced time handling was deliberately excluded from the standard library): Convert your duration to a chrono duration [3]: `let chrono_duration = ::chrono::Duration::from_std(std_duration).unwrap()` Then create a chrono `NaiveDateTime` [4] for the unix epoch: `let unix = NaiveDateTime::from_timestamp(0, 0)` and add your duration to it: `let naive = unix + chrono_duration` Depending on your use case you may want to convert the `NaiveDateTime` to a `DateTime` with timezone information using [5]. *Warning:* None of this was tested, this is wholly based on the linked documentation: [0]: https://doc.rust-lang.org/std/time/struct.SystemTime.html#method.duration_since [1]: https://doc.rust-lang.org/std/time/constant.UNIX_EPOCH.html [2]: https://github.com/lifthrasiir/rust-chrono [3]: https://lifthrasiir.github.io/rust-chrono/chrono/struct.Duration.html [4]: https://lifthrasiir.github.io/rust-chrono/chrono/naive/datetime/struct.NaiveDateTime.html [5]: https://lifthrasiir.github.io/rust-chrono/chrono/datetime/struct.DateTime.html#method.from_utc
Learning a new language is a cost to pay, especially when it is a teamwork (every programmer working with the codebase needs to learn it). Many have acknowledged that Rust has a non-trivial cost to learn, and unless there is a sufficient motivation (e.g. cost of software failure, killer library etc.) it will take a lot of time to see Rust widely used at work. For one data point, I *am* using Rust at work, but mainly because I'm probably the only person to work on this codebase (a language implementation) no matter the language is.
I do a lot of numerical and vision processing stuff and the library ecosystem isn't there yet. I'd need things such as OpenCV, GSL, ATLAS and so on. I have no doubt it'll be there in time; meanwhile I learn the language on my own. 
I don't know Rust well enough yet to use it in a work project. 
Well, stockfighter-sdk-rs is still quite in-progress, so don't assume everything is ready to actually use for playing stockfighter. I recently submitted a pull request to implement "new order", and I had to modify some of the structs quite a lot before I could massage it into shape to actually be able to create a correct order message to send. I then stumbled into the same problem you mention, and I think the solution is to just make the fields public, and submit a pull request. Or at least, that was what I was planning to do once I get a bit of time to play with the API so I can see exactly what I need to change. But if somebody gets there before me, all the better! Disclaimer: I'm quite a Rust n00b, so maybe I'm also missing something blindingly obvious! :)
gRPC would be a big one for me, too. The longer answer is that there are a lot of components that any new service I write for work is going to need to communicate with. Only a small number of those are currently supported well by the Rust ecosystem. I anticipate this gap closing to a tractable size over the next 12 months or so.
&gt; For instance, one of the major language features touted in Rust is pointer ownership with move semantics which is a powerful way to avoid memory leaks via static verification that memory is freed, and with no runtime overhead. This is indeed a cool feature. It's also exactly equivalent to a C++ std::unique_ptr. Well let me get aboard the nope train too. C++ has move semantics. Rust has move semantics. But what C++ calls "move semantics" has very little in common with what Rust calls "move semantics". [C++'s move semantics correspond to Rust's `Option::take`, and their `&amp;&amp;` "move references" correspond to a watered-down `&amp;mut`.](https://www.reddit.com/r/rust/comments/2oes6s/cs_rvalue_references_correspond_to_rusts_mut/)
ArcadeRS (https://jadpole.github.io/arcaders/arcaders-1-0) is a good introduction in real-world Rust. I disagree with some of their design decisions, but you can just change anything you don't like and follow the rest.
Swift isn't as ambitious as Rust IMO; it uses a form of GC (refcounting) as such it isn't so remarkable - it seems the world has been able to diversify GC'd languages easily. Swift also builds directly on apple's frameworks - it's designed explicitely for those existing "objective-C" interfaces, whilst Rust has to build it's own idiomatic libraries *from the ground up*. My explanation doesn't quite cover Go, but it's even simpler than swift. For the niche of gamedev, Rust is potentially interesting, but the priorities don't quite align ("performance&gt;productivity&gt;safety" vs "safety&gt;performance&gt;productivity"). The gamedev community continues to use underlying C++ engines for performance(where rust would be inferior), complemented with scripting languages/C# for productivity (where Rust would be inferior). What gamedev needs is something as performant as C++ but more productive, to provide an improvement worth switching for. For my own personal use: Rust was the only language other than C++ I found myself wanting to put serious time into, but I found in the end that tools (IDEs especially dot-autocomplete) counted for more than anything in the language. I gather there's moves toward making that easier, 'RLS'?. The majority of my time goes on looking things up and getting things working correctly, not segfaults. I have to write all sorts of other debug code anyway, so productivity (for the debug code) helps me more than safety. Thats why I want a performant language with productivity features built in. Its logic &amp; maths problems which the type-system can't help with."was this the right encoding" (-&gt;check it by comparing with the decoding.)."are these values making sense" (-&gt;check it by writing a visualizer). But there's a lot in Rust that would help.. the elegant expression syntax, the macro system etc.
Related blog post: https://xsznix.wordpress.com/2016/05/16/introducing-the-rsthd-layout/
I'm embarrassed to say I googled "whatnot" looking for a programming challenge.
I guess the biggest reason is that most of the existing code is already written in a language where it's hard to create Rust binding for - e. g. C++.
Apart from other defects in the article which others already wrote about, I would like to add what I see as the most important feature of Rust. From my experience in programming I can say that I make mistakes. Not because I'm stupid or bad programmer or uneducated but simply because I'm human. Every human makes mistakes. So what I really love about Rust is that it prevents many, many possible mistakes. And because debugging always takes more time than coding, it saves time.
&gt; pepper my code with `.0` You can solve that with [deref](https://is.gd/AYYf9t), and if you're doing it a lot [consider a macro](https://is.gd/uTTMe5). 
Thanks! It's fixed.
Use Ctags. To be precise use Universal Ctags not Exuberant. 
"I wonder why those are not put into std, but there's probably some reason." The first push to the parking_lot repo was May 13th. Afaict the author *has* made multiple PRs related to Rust's concurrency story, and a lot of them are getting merged. It would be way more constructive to phrase your sentiment as something like "maybe these can make it into std", rather than throwing shade and suggesting it should have somehow already magically happened in the 11 days that code's been public.
With the below I can add keys to my etcd instance, but I don't get any data with them. It seems I somehow lose my POST data with "value=tacos&amp;ttl=900" on the way. extern crate hyper; extern crate mime; extern crate url; use hyper::Client; use hyper::header::ContentType; use url::form_urlencoded; fnt main() { let etcd_base_url = "http://localhost:2379/v2/keys/queue/"; let etcd_client = Client::new(); let formcontent: mime::Mime = "application/x-www-form-encoded;charset=utf-8".parse().unwrap(); let mut serializer = form_urlencoded::Serializer::new(String::new()); serializer.append_pair("value", "tacos"); serializer.append_pair("ttl", "900"); let payload = serializer.finish(); let builder = etcd_client.post(etcd_base_url).header(ContentType(formcontent)); let r = builder.body(&amp;payload).send().unwrap(); }
Having tried Dvorak and Colemak, I find it unfortunate that these keyboard layout designers are so focused on the home row, and consider 'stretching' to the top row to be stressful for the hands. Maybe it's just my hands, but my 2 middle fingers are much longer than my outer fingers, so they naturally fall on the top row when laying my hands flat on a keyboard. (Qwerty's e is nice in this regard). I also think (but I haven't found evidence on this, as is common in these discussions) that having a flatter hand while typing is better for hands than curling them up to have all fingers in the home row. I wish there was a layout that prioritizes the top row for ring and middle finger and home row for pinky and index. I could make my own, but I prefer it have some form of adoption.
Inheritance means structuring your code in a hierarchy. People used hierarchies to represents lists of interesting webpages (see DMOZ) in the early 2000s, but then they realised it quickly got overcomplicated, and instead moved to flattened lists augmented with tagging (e.g. Del.icio.us) where every link had one or more tags, and you could identify related pages by selecting appropriate tags. OOP languages encourage structuring your code according to a hierarchical model of the _data_. ML-derivatives (like Rust) encourage structuring your code according to a tagging model (using traits) of the _actions_ you want to perform on the data. In short, skip inheritance completely, and "tag" types with traits when you want them to exhibit common behaviour. In some cases, it may be easier to use a enum. This is handy when the number of possible variants of your parent "class" is known, and known to be small; it won't grow outside the code-base; all variants have the same API; and you will never want to expose any interior state. Typically favour enums over tagging in cases where in Java you'd have a bunch of singletons implement an interface / extend an abstract class This does mean repeating yourself in some situations, but Rust is a fairly terse language, particularly if you don't bother with getters / setters. As to why you'd stop bothering with getters / setters, the reason is experience has shown that it's very rare for a getters and setters to do something unusual, and experience has equally shown that refactoring around an API change isn't that hard.
I have looked at it a bit. Last I checked, it was still in active development. I am going through the parts of the ecosystem that are relatively stable first. Hopefully that gives time for Diesel to get closer to a 1.0 and I can go into that a bit as well.
&gt; In searching for case studies all I've found There's also https://www.rust-lang.org/friends.html &gt; mozilla only feels comfortable using a few lines of rust in firefox so far. The way that this is phrased implies some sort of lack of confidence in Rust, but it's not that: it's that changing things in a codebase that's millions of lines long takes time. &gt; While I'd like to ask why Rust has yet to see serious adoption despite Go and Swift seeing significant uptake, that's the wrong question. Go has been stable for four years, Swift is pretty much the native language of a platform. Rust has only been stable for a year. 
Sorry for not being more clear. I found the implementation more readable compared to `std`s, so I wondered aloud why they didn't choose a similar approach. Then again, the project is young, the technique only recently battle-tested, so I guess I shouldn't wonder too much. Edit: Vikrant removed the sentence on my behalf.
I want to like diesel but the documentation is nonexistent for anything beyond the simplest example. How do you join a table to itself for tree-like structures? For that matter, how do you control joins at all? I'd rather have an abstracted sql-&gt;struct lib over an ORM any day. Also I like that diesel includes migrations, but why can't that be a separate project independent of diesel? They're separate concerns.
Team knowledge i'd say. I'm still pretty new here, happening to use a project with Go. I like rust more as a language, but for getting web properties done, i don't know if rust is a good choice. Often companies we hand it off to will also need to either hire us again to do work on it, or will have their own team. So rust hire-ability is something to consider. Still once I get to know rust more, try somethings like Iron with it, I would love to find ways to apply it at work.
Great, thanks for explaining! Seems like a perfect project scope to hack on in a meetup. I see you have several other issues open (ex: websockets for the data stream) for people to work on. Here is one follow-up question, although don't feel obligated to reply: Why are the struct fields all wrapped in Option? I assume that's because the json decoder cannot guarantee that the fields are found, and truly some of them are optional (or at least nullable) in the web API. For example, Orderbook may have null values for the bids/asks lists, rather than just a 0-length list. I struggled with decoding that to a rust representation until I read through the rust json decoder code and found that it had some limitations, like not handling enums (due to lack of lookahead, it can't speculate/backtrack). I got stuck until I found this sdk and started learning things from it. But as someone writing a stockfighter client bot, I'd prefer to just assume those fields exist. Is this something where clients should just adapt the sdk structs into their own bot fields w/ another translation layer, or is it something the sdk can de-Optionalize at some point?
Well, it does the job, doesn't it? :)
In this cause it wouldn't matter because the arrays won't be big (10-20 elements) to do this I guess, but it does create an intermediate array which can be avoided. I first tried to write a simple insertion sort and that worked but then I figured I could avoid creating 2 parallel arrays and instead create a single array of a struct type that can be sorted with the .sort_by method in the standard library... :|
:(. It's the usual. If its old and working, you are not hipster enough. If it's new, you are reckless.
I am not sure if `realloc` itself is such a big deal, with modern slab allocators it only comes into play for "big" arrays (&gt;4KB?) and most allocations are generally small... ... however the impossibility to use `memcpy` for most user types (`std::string` included) IS crippling, certainly. There are a few traits for container writers to take advantage of, but any type that allocates is generally off the table.
The [Glium book](https://tomaka.github.io/glium/book/index.html) is interesting. Also of course [Learning Rust With Entirely Too Many Linked Lists](http://cglab.ca/~abeinges/blah/too-many-lists/book/).
This is exactly the sort of stuff I was looking for.
Ah sorry by systems I meant technologies like what CMS , and/or libraries they use/can be used to achieve a similar purpose. It's going to be a website or web application so I'd figure we'd need to use some sort of CMS or maybe build from scratch. 
I simply haven't started any new projects lately that use Rust for, e.g. ones that I would normally write in C or C++.
(2) really cinches it for me. The company I work at has developed a *significant* ecosystem around its software/datacenter: introducing a new language requires integrating with all the existing infrastructure, that's a non-trivial cost. When you add the "and nobody's really proficient in it yet", management looks at you with raised eyebrows.
&gt; months old Is this a bad thing? Are they feature complete? Did docker change their API? IIRC, docker uses JSON over TCP/UNIX sockets, so a crate 'just' needs to represent the used schemas as rust types :)
Is there any reason that simulated annealing was used, as opposed to, say, random restart?
Coworkers don't know rust and don't really want to learn it either. They don't really care about programming, they just want to get shit done and go home. I still rewrote a tool in rust and everyone was surprised that it is at least 30 times faster than the previous incarnation. To be fair, the architecture of the previous tool was insanely bad and it was written in java so it was basicly a very complicated GC stress test engine, so it was easy to beat it. 
It might be *in theory* possible to expose a C API from a Go library. See: http://blog.ralch.com/tutorial/golang-sharing-libraries/ --- There's likely some hidden costs involved there though, so it's not ideal. There's probably also some serious explicit costs too, such as actually exposing particular Go functions as C functions. This might work well if you only need a small slice of the Docker API. I don't know of any other way to directly use Go code from Rust. Personally, I'd seriously investigate native support for a couple days to see if I could get that working for my needs.
They're unstable for a reason -- rustc reserves the right to add more attributes. This will never stabilize. Plugins can register attributes to silence the warning, though. Or you can `#![feature(custom_attribute)]`
I think this should be a last resort, if Rust ever gets a stable ABI. As mentioned, the check would have an impact on start-up performance. As long as you depend on a known version of a library though, it's pretty easy to get the impls it knows of (just has to be listed in the crate metadata), and from then on it should be possible to ensure no two crate have conflicting definitions. Therefore it should be possible to error out at compilation time. On the other hand, it would be more brittle, as you could find yourself in a dependency hell where no two versions of dependencies you need are compatible because they both define a given `impl`; and giving a crutch, even specifying it is a crutch, might still result in it becoming the "default" way to do things (because easy often does). It's a bit like Pandora's box: easy to open, but untold woes might fall onto the community and there'd no getting them back in.
In general, you implement `From`, rather than `Into`, because then you get `Into` for free. Beyond that, `From`/`Into` are for conversions that cannot fail. `to_foo()` functions that return `Result` can still be useful.
If you mean compiler plugins/procedural macros I don't believe so. But I also don't follow the RFCs super closely. I *do* however think this is one of the big items people would like stabilized, but since it would mean stabilizing compiler internals which are in constant churn it'll probably be some time yet before it happens.
Your question gives the impression ^^sorry ^^if ^^I'm ^^assuming ^^too ^^much that random restart's results are always as good or better than those of simulated annealing. Is this right? If so, what are your sources?
It'd be nice for the compiler to have a defined convention for reserved names, like C has with __whatever. Probably too late to make that distinction now, unless we go the other way and make double underscore names safe to use.
I think for the biggest part, we are using Github Pages and rarely build interactive stuff ourselves. We mostly use Github issues for communication and plain old mailing-lists for internal communication. The forums are run using Discourse. 
So, poisoning is the... technique for enabling `lock()` and 'try_lock()` to return a `LockResult&lt;MutexGuard&lt;T&gt;&gt;` and `TryLockResult&lt;MutexGuard&lt;T&gt;&gt;` respectively? Why give the means of creating these `Result` types a special name? 
I had to fight to get us to use Go, and starting that up again seems silly for little benefit. If we run into memory safety issues, we'll probably give it a look.
Would love to use it for embedded but it comes down to toolchain and library support. Even if I did have tooling for my current hardware platform, am I still going to have it in a year when the client wants to move to a different chip for cost reasons? Also, I'd have to wrap the vendor HAL with an extra layer of indirection which adds some maintenance. 
Ha, someone else recommend those two as well. I have to admit I almost didn't bother with *Learning Rust With Entirely Too Many Linked Lists* because linked lists are a terrible idea 90% of the time. Good to see the author actually agrees with me.
First my opinion: If I worked on mega-buck and lives-at-stake projects, I would consider switching to Rust before my competitors do an awesome opportunity to gain competitive advantage. That said, I wouldn't introduce it alone, missing one small piece or another, can be a large delay in a small project. If you can make the case to someone senior enough that Rust is a strategic investment, and have 3-5 people working in it for a few projects, however... then you can afford the necessary investments. My experience: I am using it for research, and here are things that have hindered me. Two things to note: 1. I came with my eyes open, used rust for numerical work where speed matters, and am very happy that I did. 2. Lots of improvement over the 9 months I've been at it. - Profiling integration sucked. Interoperates with perf and oprof, but those have bad UI, and you need to add some particular parameters. There was no easy way to get a manual profile, now there is flamer. - At the time, scirust was the best "numerical array", now there is the pretty wonderful ndarray, which is also constantly improving. Integrates with BLAS libraries. - Integration with Python/R was annoying. I had one segfault from rust itself, about 20 from cross language communication. Now there is rustinr for R, which is competitive with the state of the art (Rcpp for cpp integration) hope someone does something similar for Python. - Learning the language takes some effort, but: it is accessible to many different kinds of programmers (Python to C++'ers), and asymptotically you get 95% control and correctness at a price in productivity that is much lower (in my estimate) than the alternatives.
A wrapper that puts whatever type it stores in a non-shared cache line would be generally useful since basically any data that has to be shared between threads should be on its own cache line to prevent false sharing [*]. [*] unless you have two use two objects together (e.g. a thread has to take two locks to do work), in which case it makes sense to have them in the same cache line but one can achieve this by putting them into a struct. 
It sounds like you could benefit from a different physical layout, something like [this](http://ergodox.org/) where the keys are higher up for the longer fingers. Note: I have not actually used that keyboard, although I recently joined a massdrop for one and am excited to check it out.
The corpus should probably be programs, and not books, by default. I'm sure there are a lot more brackets and special characters if nothing else.
IMO the iterator invalidation example (e.g. pushing elements at the end of a vector) is enough to understand the difference between ownership and `unique_ptr`. Seem like the author didn't make it that far.
I don't work in software engineering as such, I'm a scientist but I do write software for my day job. We have a considerable amount of C/C++ code but most of my day to day work is in Python. Generally for processing data and visualisations, python is more than fast enough - thanks to fast C-based algorithms, but for some of the mission-critical stuff I'd like to use Rust. My two limiting factors are: there is no crate for the main data format used (.fits files), and team adoption. I am in fact trying to wrap some C code for the data format issue, [here is my git repo, comments are more than welcome it's currently segfaulting for some unknown reason][1]. The other I've not encountered as such, but I do predict in the long term. Currently my employer is keen to stick to technologies that _he_ understands, reasonably as his involvement will outlast mine. [1]: https://github.com/mindriot101/rust-cfitsio
Right now if it compiles it works, what you are suggesting breaks this. There is value in both approaches but this is one of the reasons `cargo` "just works" which is a "Rust feature" that I like alot.
I don't understand this. What's the problem with floats and array indexing? I have never seen array bounds checks be a problem in real-world code (in Servo). That's because Rust iterators are both easier to write and easier to optimize.
I'm not OP, but I actually think you are both making the same point: why simulated annealing as opposed to something else?
We've been getting better at this kind of messaging over the years. The tough part is that to PLT nerds, the memory safety is the interesting part. But to many developers, the rest matters as much if not more.
Are there any gotchas with FFI calling c? Perhaps some compiler optimizations are not possible? Is this any different when calling rust from c?
&gt;What's the problem with floats and array indexing? If I had to guess. It's that floats don't auto-cast to int's like in C/C++ when you index an array. 
what I'm suggesting is it wont compile if theres something that would break it. It just lets you write impl's, which a future version of a library may conflict with. In that future, it would cease to compile. However the risk is low: I only had to write the impl for 'A's traits with B's types because these two libraries don't know or care about each other; and yet there is the clear community solution of making a separate "AB" library for the overlap .... which is precluded by the current rules.
I finally have a counter example to this! I, like you, had a hard time finding code that I could meaningfully optimize by removing bounds checks. This is the PR: https://github.com/rust-lang-nursery/regex/pull/202 To be clear: the first set of benchmarks explicitly measure the difference between `unsafe` elided bounds checks. The second set in a subsequent comment measure the overall improvement of the PR. Perhaps there is a way to safely elide bounds checks here, but it feels hard and I wasn't smart enough to do it.
Same here. We're a Haskell shop and any utility scripts are in Python. Trying to get people to use Rust when we don't have a need too and where all of our custom build stuff is in Haskell, it's hard to get others to learn a new language and have a good reason to use it.
The motivation for that tutorial is solid. With the way ownership and memory management works, data structures can get complicated in Rust, and linked lists are at just the right level of difficulty to be approachable but allow you to give the borrow checker a bit of a workout.
Here is an example: https://github.com/frankmcsherry/pagerank/blob/master/src/main.rs#L97-L98 The code is doing pagerank as fast as it can, which means reading a sequence of locations in a relatively compact (usually L3 resident) hunk of memory. Flipping between safe and unsafe have a 30%-ish impact on runtime. I mention here for high perf union-find https://github.com/frankmcsherry/blog/blob/master/posts/2015-01-15.md#important-things that doing unsafe indexing takes the running time from 16.0s to 11.8s, which is a fairly big deal. I think if you are iterating over a `Vec`, the bounds checks are basically no cost (redundant with loop termination tests; optimized out), but if you are probing in a `Vec` using indices from somewhere else, and not doing a lot of additional work, you can see the cost.
I've poked around with that one, but a few things held me back: 1. Stable Rust wasn't a requirement, but a very desireable goal (I used serde_macros during faster iteration, and now that I'm bugfixing I use a buildscript). 2. The up-to-date version of the simd crate wasn't (isn't?) available on crates.io. 3. In practical terms, the software benefits from AVX/AVX2, which means feature detection is needed, but the cpuid crate (last I checked) relies on inline assembly, which is even less stable than SIMD :(. 4. Halfway through finding out 1-3 I found a freshly published library that already did the complex algorithm in C, ultimately saving time. That said, I am excited to see where that crate goes -- I think that while Rust is making some nice inroads with GPGPU computing, many HPC applications also need fine-grained SIMD (due to high memory xfer) and I think it would be phenomenal to see Rust pull up a chair alongside Fortran and C/C++ for high performance cluster work.
There are no wrappers for SAP and I don't have enough knowledge to use the C library.
Is something like this what you mean? https://github.com/rust-lang-nursery/regex/blob/71cd186f19cdce6fdb1f748146ac4d10119308ee/src/simd_accel/teddy128.rs#L797-L823 --- It elides the bounds checks and it seems to compile down to efficient code.
Wouldn't it be "shh-keygen"?
Same. There are plenty of features of Rust that I'd like and several would have prevented some of the bugs we've run into (like race conditions), but it's not enough to justify migrating the entire codebase.
&gt; Indexing is very important for moving code over to a vectorizable form. Indexed meshes are one of the most important data structures for me. Can you explain in detail what the specific code is in which bounds checks are removing the ability to vectorize? Be specific. Usually bounds checks happen with scattered indices, which is not amenable to vectorization anyway. I still don't understand what the claimed performance problem with floats is. C++ floats are IEEE 754 compliant too, and therefore admit NaNs.
Copy/Clone is derived instead of explicitly calling `loadpu` (wrong opcode I know). I haven't dug into this to see if the ASM is different (also splat isn't load, it's a fill operation reg-&gt;reg not heap-&gt;reg). `sfense` I believe is rustc LLVM intrinsic files but it doesn't exist `mfense` has the incorrect args. If I remember correctly one is the wrong args, and one just flattly doesn't exist. IDK I remember Alex commented on my commit concerning these and wrote a long comment explaining how to use gitblame so it could be fixed? I have more trouble reading the json files that define LLVM SIMD IR. I'll try to throw together a new PR about these this weekend.
It's tough in part because the memory safety story *is* a really big deal, and quite an achievement. I'll be interested to see how large Rust's use-cases end up reaching. I've seen the argument several times now that if you don't need Rust's performance/memory usage, lack-of-GC, etc. then you should use a 'more productive' language^*. While I haven't written a ton of Rust, what I have written so far has made me increasingly skeptical of this claim. If the application is big enough that I have to think about architecture and want static types, Rust is a contender in my mind, modulo the usual concerns about libraries and ecosystem and such. Given the rate at which these are improving, I'm optimistic we'll see an increasingly broad variety of uses for Rust. I'm barely competent at using Iron, but writing a little web service with just Iron and the standard library was lovely. ^* This takes the form of both 'just use Java/C#(/Go)' and 'if you can afford a GC and want something with more assurances/more functional than Java, then you should use Scala or Haskell'. I'm not convinced of a *language* level improvement in productivity here, so much as better infrastructure support, library availability, and easier hiring - and this falls apart pretty quickly for Scala and Haskell.
Since a month and a half is *kind of* long, I'd say many crates are spare time projects especially in a relatively young language like Rust. So I wouldn't want people thinking only a few commits over the course of a few months necessarily *dead*.
I don't because I'm pretty sure it won't compile on my work machines. But even if I could, I'd still need things like allocation arenas and better interop with C unions and whatnot. (And 128-bit pointer support would probably be cool.)
again, great; but that's a hell of a lot of effort to do something thats been as simple as writing ```vertices[i]=...``` for the past 20+ years. if you're writing code that centres on dealing with meshes, you want indexing to be easy to write. It's a case of just picking the right tool for the job. With the addition of a simple --unsafe flag (which wont damage any of it's safe projects), Rust could be the right tool for a whole extra set of use cases, broadening it's adoption &amp; mindshare. The unsafe code is still going to exist, you wont rid the world of it; it will just continue to be written in C, or C++. As it stands.. Rust is for provably safe code; so it doesn't replace C++ in every possible niche. Check out jonathan blows videos where he talks about his language for games (the first one), he presents a good explanation of the tradeoffs. His language is an interesting possibility. I've also had a bash at writing my own, but would prefer to use something with more than one user. My problem with C++ is general clunkiness (header files etc), not safety. I like so much what Rust does .. decent immutability by default, nice lambda syntax, match... but unfortunately it comes with some downsides so it's not an unambiguous 'win' .. which it has to be to justify the cost of switching (ditching all your familiar tools libraries etc). (there were other frustrations too that added up) I'd probably be happy with something that literally was just C++ with cleaned up syntax. Rusts syntax is great. 
It's not as nice as the field attributes you use here, but perhaps my new [validations](https://github.com/jimmycuadra/validations) library combined with some macros could get you a somewhat close approximation of that code.
Cool! Setting the flag seems doable, but then you still have to detect the current CPU's vector features at build time, instead of run time, and you get back to the stable vs. nightly issue, and you also can't reuse a binary compiled on an AVX2 CPU on older CPUs. For this particular project there's a goal of being able to have a biologist download rustup and `cargo install`, which means that they can't really be expected to know what CPU they have. As an aside, I love Rust to death, but its my opinion that the lack of stable CPU feature detection/inline assembly and manual SIMD calls is one of the last things standing in the way of being able to replace C everywhere (but I'm biased since I write code for scientists right now, CPUs with SIMD are much more plentiful in research labs than GPUs, at least around here).
&gt;&gt; "The orphan rules also prevent another third party D from implementing A's traits for B's types. " ok but I figured this is the same hazard as 'A' or 'B' retroactively doing it; (in this scenario 'C' is the 'user' deciding to implement the crossover support; 'A','B' are library vendors) &gt;&gt;"I don't know if what you've suggested handles this or not." the suggestion basically splits implementations &amp; declarations, and would want a separate implementation crate for every combination of declarations. So you'd end up with A_decl, B_decl, A_impl, B_impl, AB_impl, etc. I think you'd assume the original 'A_impl' would handle all the impls for standard library traits, but maybe you'd want even more fidelity than that
 These are attributes, and its mostly useless to define new ones unless you are rustc, writing a plugin, or writing a rust tool that does things with the attribute. So un-reserving everything but a namespace doesn't feel right, it should be the other way around. Double underscore names in rust work fine, but they have a different connotation. I'd suggest going the other way and un-reserving `#[custom_attr(foo)]` as a way of namespacing.
Saw this one, [Termion](https://github.com/ticki/termion). It's pure Rust, but it hasn't been tested on Windows. Not sure how much work it would be to get it working on Windows.
I'm aware of that, that's why I was asking.
The most obvious problem with your proposal is that there's no upgrade path from current Rust. Another problem is that having to separate your impls into different crates from your decls is extremely unergonomic, and that many impls are provided based on private members of the types they're for, so they need to be in the same module as the decl. But the biggest problem is that this does not have the same guarantees that Rust has today. You've decided anyone is allowed to define the "AB_impl" crate, which means there can be more than one "AB_impl" crate. What do I do when C depends on one "AB_impl," D depends on another, and I want to depend on both C and D? "C" and "D" are incompatible; the whole point of Rust's orphan rules is that crates can't be incompatible to compile together.
You may or may not be aware of this: [https://doc.rust-lang.org/book/ffi.html](https://doc.rust-lang.org/book/ffi.html) It seems like C++ isn't supported very well, but you may be able to call Rust functions from C++.
Now if only Qt developers would hurry up and finish bringing QML's support for native widgets up to parity with the C++ API.
I'm trying to figure that out.
My coworker and I tried using Rust for a project, when our team primarily uses Python. We got yelled at because no one else in the company knows Rust, so the project could not be maintained or debugged if we were gone. The highest cost of new programming languages is the debt associated with teaching people new paradigms. It's the reason old languages like Java and Python are so ubiquitous even though there are clear advantages to using more modern ones.
Keep up the good work!
That's more or less what I was thinking: moving parts of the code in C++ to Rust, but I'm not sure how it would work -- make a static lib, perhaps?
&gt; I am trying to collect example of using Rust in research, to justify my own usage of it =) FWIW, I've been using Rust for research full-time since March, as a PhD student. I do work somewhere between compilers and computer architecture, and I'm currently developing a system with a runtime to do dynamic analysis on instrumented code. The runtime is written in 98% Rust (with a C++ wrapper around the `extern "C"` Rust functions for a few use-cases). I've found myself to be *way* more productive than I had been in C++, not just because of memory safety but because of expressivity too (often a Haskell-style "it works the first time it compiles" experience). I only wish I could rewrite the LLVM pass in Rust. One big factor here is that I'm working mostly by myself, and where I might collaborate with someone for one piece soon, we have a clean interface (a trace file format using Protobuf). And there's no legacy code to link with. I'd say go for it if you think you won't hit compatibility or collaborator issues!
Figure what out?
An array will be directly embedded, whereas a Vec keeps its data in a separate heap allocation.
Yes, Numpy, Scipy, Matplotlib and friends really has become the go-to toolbox for a lot of analysis. And it manages to be pretty fast too, as long as you let the libraries do all the heavy lifting. 
&gt; I don't think it's intended to be the real solution It's not. (Also, incidentally, `RUSTFLAGS` lands in the Cargo distributed with stable on Thursday!) See: https://github.com/rust-lang/cargo/issues/2112 &gt; As with the Rust build itself, when distros build and package binaries of Cargo applications, they want to be able to customize all command lines to all compilers. This is truly what `RUSTFLAGS` is for.
Which platforms do your work machines use?
EDIT: Solved it. tl;dr: you need to actually call the conversion in the method as well. So: pub fn push&lt;T: Into&lt;Stackv&gt;&gt;(&amp;mut self, x: T) { self.s.push(x.into()); } Implementing trait From&lt;t&gt; for Stackv also implicitly (and magically?) implements Into&lt;Stackv&gt; for t. But we need to explicitly call it in our code. I have no idea what to do if you want to use two different custom types like this in the same struct, by the way. Combine them into a third custom type I guess. -------- Trying to use convert::From for type conversion (along a suggestion for an earlier question): #[derive (Clone)] pub enum Stackv { I(i32), F(f32), } impl From&lt;i32&gt; for Stackv { fn from(x: i32)-&gt;Stackv { Stackv::I(x) } } impl From&lt;f32&gt; for Stackv { fn from(x: f32)-&gt;Stackv { Stackv::F(x) } } I have a method that uses the Stackv: pub struct Stack { s: Vec&lt;Stackv&gt;, } impl Stack { pub fn push(&amp;mut self, x: Stackv) { self.s.push(x); } } And I try to use it: stack.push(3); But it fails: main.rs:265:16: 265:17 error: mismatched types: expected `second::Stackv`, found `_` (expected enum `second::Stackv`, found integral variable) [E0308] main.rs:265 stack.push(3); ^ main.rs:265:16: 265:17 help: run `rustc --explain E0308` to see a detailed explanation Edit: Also tried a few variations of this: pub fn push&lt;T: Into&lt;Stackv&gt;&gt;(&amp;mut self, x: T) { self.s.push(x); } So did I misunderstand what the conversion::From trait is supposed to accomplish? I thought it would let me transparently use any supported type, but that doesn't seem to happen. If I have to explicitly convert values to my enum type, then what is the From trait meant to do? 
I see Option::unwrap() and Option::expect("panictext") in almost all rust code. Why is that deemed good practise?. I mean, why even bother with wrapping results in Option if we break out of it first thing anyway? APIs could just aswell give the result directly (or panic). I suppose I expected to see more pattern matches or mapping over Options, in order to keep the result wrapped.
In practice I think you always need kind of a leader in a team to push in a direction. If he manages to prove he is right (by a wide margin) then the others will start investing their time. I don't really believe in waiting for a consensus before any real change happens.
&gt;&gt; statically typed object oriented languages is that most are more restrictive about implementing interfaces than Rust is. C++ is restrictive with class-methods: but there's the world of free-functions which are completely open. The C++ standards committee did accept half of UFCS (foo(a,b) falls back to a.foo(b), so what we have there now is a choice between writing 'a.foo(b)' if you want an ultra-restrictive world, or foo(a,b) if you want the fully open world. I came to realise rusts designers actually consider C++ 'too open' overall. &gt;&gt; "Many types have fields which are private to uphold memory safety (e.g. if Vec's len and cap were public, I could create a wild deref in safe code)" I hope the standard library would be more likely to contain these - i.e. the complete set of types that prevent most users needing to fallback to 'unsafe'. &gt;&gt; "I would rather abandon orphan rules flat out with no other changes than this proposal, because at least that wouldn't have the ergonomic disadvantages." well the impl/decl split would be an opt-in; a library author could still deliver one crate (if he's confident people will accept him as a 'benevolent dictator') or go for open (appealing to community collaboration). (I think the proposal might work only needing the 'decl' crate to be special; an 'impl' create could still make new traits) My other suggestion ages ago was to relax the orphan rules for 'application' code vs 'crate code' , but the answer I got was 'this will stop people contributing crates'. But surely, even if someone doesn't contribute re-useable code, you still benefit more from growing the language mindshare? And in practice I see the reverse problem - "there's *too much* source out there, and its extremely unlikely anyone will be interested.."
Re: testing, I think we agree -- testing isn't a differentiator between Rust and C++ here. I meant that anecdote to undercut my concerns about the lack of formal verification or study of Rust's type system and the (pending) lack of a memory model. As if to say, "yeah, there's always going to be some concern about a young language with a newer compiler, but there's always tests, and hey, they're good enough for NASA!" Re: funding, I would also agree with you there. However, having a clear business sponsor (or foundation with multiple sponsors) is one good sign of health and probable longevity of a project. I don't personally have concerns about Rust here (I'm a hobbyist in my free time and I love it), but it was a question I needed to raise and consider before bringing it in to work. Re: zero-copy, the program itself is architecturally simple. The analysis is a little convoluted, but it's basically just a funnel. You have X million queries, and you need to identify where in a reference text they match within Y edits (Levenshtein distance). To get good performance, there are a series of steps that are increasingly expensive which filter out unlikely locations without having to calculate the full edit distance for each of them. Each of those steps in C++ was a place where I accidentally triggered deep copies instead of references, or accidentally modified shared data which I had intended to treat as immutable (again, not super experienced with C++). I'm sure with C++ experience the spurious deep copies could be reduced. But, Rust's move semantics, slice types, enforced immutability via the borrow-checker, and strong type system mean that you decide when to allow copies and modification, as opposed to needing to chase them down. I suppose I should have phrased my original post as "Rust ended up allowing me to easily and seamlessly do completely zero-copy work." It's not that I *couldn't* have done so in the C++ version, but it was a really awesome force multiplier for a one-man band to have it so straightforward.
&gt; Why is that deemed good practise? The general rule of thumb is that you never panic in a library, but feel free to panic in a binary as long as you would have exited anyway. For example let foo = match opt { Some(v) =&gt; v, None =&gt; { println!("x went wrong"); std::process::exit(101); }, }; is semantically no different to let foo = opt.expect("x went wrong"); &gt; why even bother with wrapping results in Option if we break out of it first thing anyway? Often `Option`s and `Result`s are used when the function writer can't statically verify the inputs, but it is possible that the caller can so it's perfectly acceptable to call `unwrap`. A perfect example of this is [`parse`](https://doc.rust-lang.org/stable/std/primitive.str.html#method.parse), `"4".parse::&lt;u32&gt;()` will always succeed (so it's fine to call `unwrap`) but `recieve_input_from_user().parse::&lt;u32&gt;()` could very easily fail so you have to account for the error condition. &gt; I see Option::unwrap() and Option::expect("panictext") in almost all rust code. &gt; &gt; ... &gt; &gt; I suppose I expected to see more pattern matches or mapping over Options, in order to keep the result wrapped. I'm guessing you're mostly seeing this in examples and tutorials? It's common to use `unwrap` excessively in those because proper error handling can distract from the concepts being shown.
&gt;&gt; "Rust also has free functions. The whole point of traits is that you can use them as bounds in parameteric polymorphism, not just because they contain methods." sure. With overloading, free functions in C++ are effectively 'compile-time multi methods', just using the prefix syntax. &gt;&gt;"Application code should very rarely need unsafe, but there will always be " so there is a sliding scale; lets say 70% of the crates will be 'all pub , no unsafe', 30% will be 'priv details, unsafe' &gt;&gt; "Allowing binaries to ignore the orphan rules means binaries can't upgrade their dependencies without breaking their builds" But thats ok if it's an opt-in surely? I accept that risk for my code. I'm only risking it because A &amp; B weren't interested in my needs already (but I can plug the gap, instead of needing to waste time arguing with people). I don't impose that risk on the community by releasing it as a crate. In practice, it's hard to get contributors or interest in re-use. The world is overflowing with source code.
I think the point is to name it confusingly. No one would imagine `keygen` to be an "optimized keyboard layout generator" tool.
&gt; I'm guessing you're mostly seeing this in examples and tutorials? Correct, just started reading the rust book. Very good answer btw :)
&gt; Panic-on-abort doesn't really help since there's no way to make code conditional on which abort runtime is going to be linked in. But, this seems like a bug, for the reason mentioned. Also, I think if it were integrated into libstd, libstd should be able to use some magic to tell which semantics to use, as abort-on-panic should enable lots of parts of libstd to be optimized (away).
That's going to take a long time. It's not simply "make an RfC and we're done", since the current APIs need to evolve with the language. That said, there are people working on a new API that will not have these issues.
I do not get compatibility issues (writing a simulation code from scratch), but might have collaboration issues:: the goal is to create something that can be shared and modified by other groups.
How do you solve the Expression Problem in Rust with traits? Say I have two Traits: Material and Shape, and I want to write a function that works for all Materials and Shapes: `fn intersect(Material, Shape);` but that dispatches to a specific function for an specific combination of Material and Shape, e.g., `fn intersect(Metal, Plane)`, or for a polymorphic function for a single Shape: `fn intersect(Material, Sphere)`. The only "clean" solution I know to this problem is multi-methods as implemented in Lisp, Scheme, Racket and Clojure, but AFAIK Rust doesn't have them. In Rust it would be like implementing a Trait for a pair of types, instead of implementing it for only one type.
No, it's not possible because you need something to keep alive the `Ref` value returned by `RefCell::borrow()`.
FFI is definitely a boundary for optimizations, because all you have when compiling is a function signature that's going to be fulfilled by the linker. There's optimizations that can be done with static linking, but they're limited in scope. Dynamic linking is basically a black-box that can't be optimized into or out of. The biggest gotcha I can think of with calling C from Rust is ensuring that the ABI of the function prototype in your Rust code matches the ABI of the compiled C function, which may not always be the C ABI. Calling a function with the wrong ABI is undefined behavior. One big gotcha with calling Rust from C is that unwinding into C from Rust is undefined behavior; the stack unwinder will gladly go into stack frames owned by C code and assume they're Rust, potentially wreaking havoc. If you have code that can panic (or if you're calling into code that you don't have control over), you need to wrap it in a call to [`std::panic::catch_unwind`](http://doc.rust-lang.org/nightly/std/panic/fn.catch_unwind.html); if you have a stack that goes from Rust to C to Rust (e.g. passing callback functions to a C API from Rust), you should return out of all the C stack frames and then resume the panic from the Rust side with [`std::panic::resmue_unwind`](http://doc.rust-lang.org/nightly/std/panic/fn.resume_unwind.html) so that everything gets cleaned up correctly. You can continue after a panic but you have to be very, *very* careful. The docs for [`std::panic::UnwindSafe`](http://doc.rust-lang.org/nightly/std/panic/trait.UnwindSafe.html) kind-of explain what kind of stuff might break if you catch a panic and resume naively. 
yes. the crates you use, your code, and the standard library.
I'm using Rust at work to prototype something. However, I'm unwilling to push it further until tab completion improves. I'm too worried Rust will get rejected when my peers try it and find out that a lot of the time (nearly every function) they have to exit their flow and google around or wade through a git repo just to get a function signature. I've previously kept up with the racer master branch, and have recently switched to YouCompleteMe. Neither of these work very well. Has to be said: I'm truly thankful for the brilliant work done on tab completion so far. You guys rock and I know you'll get there. 
You want /r/playrust
Wrong subreddit. This is for rust the systems programming language, not rust the video game. 
It would: if only binaries are allowed to avoid orphan rules, then since you cannot depend on binaries, no dependency can introduce an issue.
Published my first tiny crate: [rs-release](https://github.com/JIghtuse/rs-release/)! I would appreciate any feedback.
Rust uses `memcpy` on a `Vec&lt;T&gt;`. In Rust, moving is implemented by a bitwise copy, and the source is then known to contain an invalid value and cannot be used any longer (it is overwritten today, but won't in the future). And therefore moving a range of items is as simple as a single large `memcpy` (or `memmov`).
I tested that on windows a month or so ago and its not working. To make it work at least the `libc::termios` would need to be ported to Windows though there may be more as I did not explore it any further.
You didn't mention that restriction above, but it would still mean that adding a dependency and providing impls for types/traits in that dependency would be a breaking change.
The link in your README.md is wrong. Also why are you using a HashMap? Do you think that the keys could differ?
I don't think the "single libstd" approach works, for this reason. In particular, it doesn't make sense to have a bunch of machinery for dealing with unwinding in libstd (including, in your case, the `std::thread::panicing()` call) when unwinding will never happen. I think the libstd people are aware of the problem as it came up during the Firefox Rust integration discussion. I know if/how they're planning to solve it.
it seems Gui libraries are very language dependant. As such it's really hard to move a complex framework from it's native language. Swift works for apple because they designed the language around their existing objC frameworks. is there any interest in the Qt community of making bindings easier, e.g. alternate un-overloaded function names and so on; or do they already do this for other languages (I gather qt has various bindings already) ? i guess Rust is a language for a rather popular gui, "the web browser".
Which part?
No more TIOBE! This happens all the time! :)
The TIOBE index is pretty bad. It really needs to just go away. 
&gt; On the firmware side, Rust can't target the microcontrollers I care about because of its ties to LLVM Which microcontrollers do you care about? 8051? PIC? Not that I've successfully used Rust for microcontrollers yet, but I definitely want to start using Rust for embedded dev as soon as I can.
One of the things discussed in the interview was how the "VM" part of LLVM is very different than the "VM" in something like the "JVM".
Is there some better method to quantify programming language popularity?
How can I implement `From` for a simple struct? I've done it "manually", like so: fn from_vec&lt;T&gt;(mut vec: Vec&lt;T&gt;) -&gt; Array { vec.shrink_to_fit(); let array = Array { data: vec.as_ptr() as *const libc::c_void, len: vec.len() as libc::size_t, }; mem::forget(vec); array } But this: impl From &lt;Vec&lt;T&gt;&gt; for Array { fn from(mut vec: Vec&lt;T&gt;) -&gt; Array { Array { data: vec.as_ptr() as *const libc::c_void, len: vec.len() as libc::size_t, } } } Tells me that `T` is undefined. So where should I define it?
&gt;Getting the flag seems doable, but then you still have to detect the current CPU's vector features at build time, instead of run time, and you get back to the stable vs. nightly issue, and you also can't reuse a binary compiled on an AVX2 CPU on older CPUs. For this particular project there's a goal of being able to have a biologist download rustup and cargo install, which means that they can't really be expected to know what CPU they have. That's a problem with all statically compiled binaries. If you want to detect and emit the appropriate machine code at run time you will need to link in code at runtime or use a jit compiler.
It does? I use it behind YCM and noticed it wouldn't pick names from other crates outside the current opened buffers except for the std, I was pretty OK with that "limitation". But now I want the full experience.
I'm not sure I understand -- my impression is that you can compile a binary with SSE/SSE2/MMX/AVX/AVX2 instructions, but only execute code paths compatible with the current CPU (thus the need for runtime feature detection). That said, I haven't written a project which does this, but I've used ones which purport to.
I agree that fixed sized arrays are a pain to work with. In my case it's mostly because any size over 32(?) doesn't have any trait impls in the stdlib. I'm hoping once Type-level integers are implemented they'll get more love in the stdlib and elsewhere.
If I accidentally use try!() in a function that does not return a result type (say I forgot to change the signature), I get an error message that mentions the try macro but doesn't say where in my code the mistake is. Examples: https://gist.github.com/brinchj/7e291c91979980c5fd572b1bbedae3a6 Is there some way to get the actual line of code that called try (apart from manually deducing it)? :-)
What is the issue with multirust? I thought it used rustup.sh?
Thanks; that clears some of it up. I did go back and change the signatures to use an explicit lifetime. `'static` enforcing that the boxed data didn't have any internal borrows wasn't something I'd considered, and it makes sense (now). 
The [RedMonk rankings](http://redmonk.com/sogrady/2016/02/19/language-rankings-1-16/) seem to be more representative (in my subjective, not at all substantiated opinion). They aren't updated very often though.
You're not being very constructive.
[removed]
I am, but it's in one of the parallel universes.
RedMonk is bad at languages that are under-represented on GitHub, such as LabVIEW.
If I wanted to build out the recommendation microservice work has me on in Rust instead of Python I'd have to build machine learning tools they'd end up owning the rights to--so I couldn't open source them later as my own project (or fork of another project) without significant ethical gray area.
Are you on beta or nightly? I think the macro diagnostics got worse recently and then were fixed.
Wow, that looks great. In my case, I'm not using HTTP, but I am using the openssl crate for TLS connections. This looks like it'll probably work well. I'll keep reading through the docs, but I was wondering - one of my favorite parts about Asio is I can create an io_service, spawn some threads to run it, and just post work to it - completely unrelated to network I/O. It's basically an easy-to-use work queue. I see that mioco has some support for channels, which look like mpsc queues. Is there anything that you could use as a general mpmc queue (with multiple "work" threads)? It's okay if it can't, but would be sweet if it could.
Ah, no, sorry, there are multiple variants, but I know that in some particular part of my code only one of those variants can appear. For example, say that I have some other struct somewhere that will only ever embed a particular variant of this enum, and I'm writing an impl for that struct.
IBM POWER6 (and up), with AIX or i Series.
Hmm... it may be worthwhile to add an answer to the Rust FAQ for this, as it seems to come up fairly often. I'll add it to my to-do list.
With such robust threading, does one still need async I/O in Rust?
I was basically looking for a thread pool. It looks like there's a fairly popular crate named... threadpool. Which seems to work. Between mio, threadpool. and eventual (for futures), I think I can tie those libraries together to get a decent async I/O thing going. I'm probably going to simplify it a bit by always having one thread dedicated to I/O in mio's event loop, plus a threadpool that generally handles the rest of the async stuff. In my case, I want to be able to asynchronously read data off a network stream and deserialize it (but not do the deserialization on the single event loop thread).
The example in the "Expressivity and productivity" section seems badly written, because almost every second line is a comment.
Why not? Even with async IO, you'll still need to allocate memory for each simultaneous connection and block the event loop when doing CPU intensive tasks. With multi-threading you let the kernel schedule which threads are running on which CPU cores, allowing you to process data almost simultaneously. I believe on Linux native POSIX threads are treated like processes, except they share the parent processes virtual memory space; as opposed to forking off and doing copy-on-write. You can set a maximum number of simultaneous connections, and let the TCP socket store pending connections in the backlog. There are a bunch of StackOverflow posts on calculating the maximum number of threads based on memory/CPU cores.
What you say about Linux threads is true - to Linux, they're all "tasks". But I think the main point is this: &gt; block the event loop when doing CPU intensive tasks. I think the main idea is to have an event loop that simply notifies a thread pool when you're able to read/write/accept data from/to sockets. You definitely don't do anything like parsing or deserialization or computation on the event thread, you pass it off to some other threads that are around just for that purpose. I'm actually not sure how Boost Asio does it, maybe I should try reading through their source (although I'm not a C++ expert by any means and it's quite... intricate). I'm pretty certain you can just have a single thread on an io_service that can service regular work requests *and* deal with asynchronous operations.
And it's broken, please ignore this link
Mioco author here. I'm very happy to help with mioco related questions. Ping me on gitter any time. I'll respond whenever I can. It should be fairly easy to implement a work threads with mpsc queues. All you need is to spawn one broker, that has a receiving end of the queue, and broker spawns N workers that have two channels, one each way: one receiving (work to do), and one sending (I'm ready). Broker `select`s on work queue, and "i'm ready" receivers, keeps track of which worker is ready, and sends any new job to first idle worker.
....how have I missed this? I've been looking for more audio content related to Rust and I'm amazed I haven't come across this. Thanks! (PS: I am a developer on a podcasting platform.. if you ever decide SoundCloud isn't for you :D http://omnystudio.com/. That also makes it even stranger I haven't come across your podcast yet.)
&gt; The idea behind async I/O is that you have one thread that waits for any of the sockets to have any data to read (but doesn't necessarily read the data). But isn't the kernel already reading and buffering that data for you?
Ah, perhaps I don't really understand how mioco works. In the example in the readme, there's a loop where you call listener.accept(). The "normal" std::net TcpListener would block in this case. Digging through the mioco source (which I don't fully understand, as I'm still a bit new to Rust), it looks like calling `accept()` on a MioAdapter actually ends up calling `try_accept()` on the "adapted" type, and if you can't accept something immediately, it waits on being able to "read" from the acceptor. Is this correct? My specific situation has a couple layer on top of TCP which seem like they might make this a bit more difficult. First, I'm using OpenSSL. That library works by "wrapping" a TcpStream in an SslStream. Calling the normal read() and write() operations actually just fall through to reading and writing (encrypted) data to the underlying TcpStream. As a result, there's no Mio type for SslStream. Now, SslStream *does* implement AsRawFd, which I'm pretty certain returns the fd of the underlying stream. I think Mio has a way to wrap this (EventedFd?) to turn it into an Evented type. I think I probably need to implement some extra glue to get that EventedFd all the way to Mioco, and I'm not sure of how I'd do that, but I can spend some decent time trying this weekend. I'm bound to learn something either way. On top of the SSL/TLS is (for my project) going to be an object serialization layer. I don't plan on doing anything fancy - so the stream won't be "self-describing". For example, something like: struct Point { x: int, y: int, } would be written out simply as two ints, and read in as such. The catch here is that I can't just deserialize objects out of nowhere - I need to have some kind of ObjectStream where you call read() with the type of the object, which would implement some trait that lets you construct it from a Stream. Imagine you had a big object you were reading and it fell across TCP segments. You'd want to asynchronously read/parse those objects. I'm very uncertain with how to accomplish this with coroutines, although perhaps it'll make more sense once I finish getting TLS working. Basically, I'd like the code to deserialize to look like this: impl Deserialize for Point { fn deserialize(stream: Stream) -&gt; Point { let x = stream.read_int(); let y = stream.read_int(); return Point { x = x, y = y }; } } but if you only have enough data to read the `x`, it should asynchronously await receiving more data so it can read `y` and return the constructed point. Thanks for your help so far, by the way. I've never actually used a coroutine system before.
But the kernel doesn't keep a kernel thread for every socket. And you have no other way to get it from the kernel buffers to your program with synchronous I/O. Ok, technically you could repeatedly poll all the sockets in a loop to ask which ones have data - but that requires spinning your thread at 100% all the time, or close to it.
All this really depends on your application; threads can have all sorts of hidden costs. They are slow to spin up, although you can get around that by keeping a thread pool. Every thread has to allocate the first page of its stack which means they can use a bit more memory (probably not a big deal on modern systems unless you're dealing with an embedded system). The context swaps again add a bit more overhead. By far the biggest issue though is the potential for cache thrashing, threads tend to compete for cache as they are being swapped out so again it's a good idea to limit their number.
In that case you should have that variant contain a struct and just pre-destructure to that struct.
Sort of an unrelated question but I was curious if rustup only supports architectures for which it can download a binary or can it it also manage building your own rustc binary/libs from source with user provided custom architecture definitions?
It's exactly the "pre-destructuring" that ends up looking like the code I give in the post.
Could you give a more concrete example of such a situation?
What I was getting at is the kernel is essentially your event loop.
The way OS thread scheduling works makes context switches for having hundreds of concurrent receiver threads incredibly inefficient. It's better when there's just a handful of threads that can maximize their timeslice usage as much as possible, hence why we have non-blocking IO.
&gt; The context swaps again add a bit more overhead. Isn't this solved by assigning threads to specific CPU cores? Each thread will usually be rather small, so the virtual memory it accesses will also be rather small. I tend to lean towards the kernel and CPU's MMU being far more aware than me and my program.
Coroutine is like a thread. All mioco APIs emulate normal threading APIs. It's just when some IO is not ready at particular moment, mioco will block given coroutine, and switch to the different one that is ready. Because of that you can have millions of coroutines. The OpenSSL part is more difficult one. You might need to implement `mio::Evented`. Mioco can work with any `mio::Evented` using an adapter. See eg. `src/tcp.rs`. If I were you I'd first start with unencrypted version of what you're doing, see if it works, and then add the OpenSSL one. Let's discuss this on mioco's gitter. You can start with pointing me to the OpenSSL wrappers you're using.
By writing a program I made in various other languages, for me that is the board game othello/reversi, just learning by trial and error.
Oh, sure it's possible to write this in a lot of different styles. It kind of depends on what you want to show off to the readers, or what background you expect the readers to be more familiar with. (In this case, it's explicitly contrasted with the linked piece of C++ code.) I was just puzzled at "comments = badly written".
Yeah, that was the intention (to contrast it with the C++ code). There's a [bunch](https://news.ycombinator.com/item?id=11775753) [of](https://news.ycombinator.com/item?id=11775883) [discussion](https://news.ycombinator.com/item?id=11775860) over this example in the Hacker News thread too, much of it complaining about its inscrutability.
Good old Hacker News :)
Good stuff for the commute! Thanks! 
A good scheduler will already prefer to run threads on the same core over and over, moving them to a different core only when doing so appears to make sense. Pinning manually may sometimes make sense but in general I'm not sure it's actually helpful. 
It greatly depends on your background! Pick your favorite here: 1. Rust is probably the only language that can ever replace C++ completely. It gives a relatively familiar tool to the modern C++ developers, but in the much more consistent and reliable ways. (This is, by the way, the reason that I've looked into Rust at first.) 2. Rust benefits from a modern-enough type system, which appeals to functional programmers looking for a *lower-level* language with a better type system. 3. Rust is designed for programming at large, and it is much less prone to problems naturally arising from projects' scale (Servo seems to be a good example). Not today, but given a mature ecosystem it can be eventually a good language for doing large things(tm). 4. Rust is low-level enough that you take account of most resources. It is a good property for systems programming (which doesn't necessarily need such accounting by itself, but it's good to have).
This seems like unnecessary overhead to me. why not change it to this? ... struct Bar { a: usize } ... impl Bar { fn func(&amp;self) { // do something with self.a } } If you want to `a` have the same type as content of `Foo::A`, so you can refactor easily, you can do this: type MyType = usize; enum Foo { A(MyType), B(usize) } // or enum Foo { A(MyType), B(MyType) } struct Bar { a: MyType } ...
I want to ban myself, too! :-P I didn't tell you from what, though.
I've been using Coio: https://github.com/zonyitoo/coio-rs and I really love it. Simple and well-maintained.
Don't remove them. It is a bit of a mess but it makes *a lot* more understandable for people who aren't very proficient with rust, like moi.
First, you'll have to find the answer that suits you yourself – we don't know what kind of projects you want to start nor what constitutes 'fun' for you. However, adding to the bunch of reasons /u/lifthrasiir gave, * Rust is new enough that you can write useful stuff that would have already existed in other languages (if that's up your alley) * If you want to learn systems programming, having rustc as your coach will keep your head free to worry about other things than memory safety. As one famous programmer once said: Pascal is like wearing a straightjacket, C is like playing with knives, and C++ is juggling flaming chainsaws. In that metaphor, Rust is like doing parkour while suspended on strings &amp; wearing protective gear. Yes, it will sometimes look a little ridiculous, but you'll be able to do all sorts of cool moves without hurting yourself. * cargo is awesome. Managing crates just works as intended, which makes a whole lot of troubles you may have in other languages just vanish with a satisfying *poof*. * the [community is awesome](http://llogiq.github.io/2016/04/23/awesome.html). So awesome I felt compelled to blog about it.
Has the feed changed? Episode 4 is the last I see in my pocketcasts. Time to go resub and see.
I really liked this article and its inspired me to spend more time with rust. Thanks for writing it! A sort of small point about the example indexing program: the C++ implementation uses a prefix tree (a.k.a. trie) where yours uses a hash set (right?). The trie will be much more space efficient and would have better performance than a hash set, I believe. (My experience in this matter is implementing a Boggle game in a Python irc bot, where a naïve set was unusably slow and a simple trie saved the day). So unless I've misunderstood the example, it's not a straight apples-to-applies comparison.
Yes, I guess that's a good solution for the context swaps if you have no more threads than CPU cores.
I think I've even hardly used Google at all, as all the information I need is in the Book, Standard Library Reference, Documentation on crates.io or the corresponding repository.
The rust-threadpool crate actually [dropped support](https://github.com/frewsxcv/rust-threadpool/commit/459c9892f923df569a337023f9b8a23ab9fd9ef1) for scoped threads. Crossbeam still supports them though
The latter came from the former. When the choice was made to stabilize or not stabilize certain things in the standard library, `rand` was one of them that was not ready, so it's distributed as an external crate. But the compiler also needs it to function, so it has its own internal copy.
I needed a lockless hashmap for my work project, couldn't find any on crates.io, so I [made one](https://github.com/polachok/intmap) following [this guide](http://preshing.com/20130605/the-worlds-simplest-lock-free-hash-table/). I'm new to lockless alghorithms, so I'd appreciate a review.
Done.
Well, there is quite a lot advantages. I'll try to outline some of them here. **Advantages over C:** 1. Perhaps most importantly is _safety_, which, in this case, is defined as the absence of undefined behavior. Particularly, the protection against pointer related undefined behavior. This is achieved through the type system, which can express 'regions' through 'lifetimes'. Lifetimes can be used as parameters to type constructors. Furthermore, lifetimes have a transitive relation defined on them: the outlive relation, `'a: 'b`, this specifically means that for a type constructor, `T`, `T 'b` is a subtype of `T 'a`. This way you effectively avoid aliasing issues (shared mutability), use after free, dangling pointers, and more. The second important construct Rust uses is linear types. Every type in Rust is linear by default. It is worth noting that Rust has no move constructors, instead it uses drop flags, which denote if a value "belongs" to the scope (i.e. that it has not been moved or dropped). This is a way to avoid calling the destructor of moved values. Linear types is a way to make sure that you do not alias a NOALIAS value. Examples includes, `Box`, the owned heap allocating container. 2. High-level abstrations, these are extremely important to the nature of Rust. The standard library of Rust is in many ways more extensive, and contains more high-level primitives, which makes it possible to write programs without having to reimplement common primitives. 3. A proper type system, Rust uses a Hindley–Milner like type system with type inference (it should be noted that Rust _does not_ use Algorithm W, despite being commonly falsely stated so). This type system makes it possible to generalize many primitives without a lot of pain. To express classes of types, you use _traits_, essentially a set of functions signatures, which you can implement for types (either specifically or generically). Traits are used as bounds for generic parameters, making you able to parameterize over types satisfying a specific property. **Advantages over C++:** 1. Type inference, C++11 added the `auto` keyword for inferring the type of a variable, but it isn't quite there yet: In fact, C++'s type inference is a very primitive one. Rust has a custom type inference system (called the "Diamond rule"), where the compiler tries to find a set of types satisfying the bounds requested. Furthermore, Rust is also able to infer lifetimes: it tries to narrow the lifetime down as much as possible, while still satisfying the bounds. All this combined makes it very pleasant to work with types, type constructors, and functions with advanced signatures in Rust. 2. No exceptions, while Rust _does_ have unwinding by default, it does not have exceptions, in the normal sense. Rust does error handling through algebraic data types: commonly the tagged union, `Result&lt;T, U&gt;`. This is a type with two variations, `Err` and `Ok`. These are used for expressing the result of a function, in a less ambiguous form of exit codes. This do obviously have a performance advantages, but it does also have a disadvantage: Dismissing the errors. Rust is not a pure language, so dismissing an error is completely fine (since you can downlift ADTs), but it is not wanted. To deal with this issue, Rust has a way of making tagged unions as `must_use`. This spans a warning if the result isn't handled by the caller. In fact this works quite well. You don't have all those nasty bugs of exceptions. But you still preserve a sophisticated error handling system. 3. Automatic frees, Rust automatically calls the destructors of all the values, which are not moved, when they go out of scope. Destructors are specified through the `Drop` trait, which defines the custom routine for destructing some value (for example, release lock, deallocate data, etc.). This does not only remove virtually all memory leaks, but it also avoids classes of deadlocks. A common mistake in C/C++ is to forget to free the lock of e.g. a Mutex. In Rust the freeing of the lock is included in its destructor. 4. No race conditions. Race conditions is a major pain point in most languages. Rust solves this smartly on compile time: namely that it constraints certain things to Send/Sync bound. These are a way to express, through the type system, that a type is thread safe. Particularly, Send means that you are able to "transfer" a value of the given type over a thread safely, whereas Sync is the opposite: the ability to synchronize a type collaboratively across threads. **Advantages over Go:** 1. No garbage collection. Rust does not have garbage collection, nor does it has the unsafe conditions of languages without garbage collection. Instead, Rust has a suffisticated model of scopes and aliasing. This is not only a performance gain, but it is also a correctness gain: namely that your program is completely deterministic in its memory management. There is no 'stop the world' situations, or things alike. 2. Rust has generics. For many programmers, Go is almost impossible to use due to the lack of generics. Rust has generics (though it is not as expressive as Haskell's, for example there is no higher-kinded polymorphism). Types, traits, and functions can all carry generic parameters and bounds to these. Generics is of major importance in modern programming. It is a possible to specify a generalized function, and avoid rewriting code over and over again. However, Go has inheritance, which Rust does not. Rust is like to get some form of inheritance in the future (probably through so called 'virtual structs'). 3. Operator overloading. Go is famous for the lack of operator overloading, something Rust has without magic: Rust defines the operators through traits implemented on the various types. This makes it possible to, for example, define custom integer types while still preserving the awesomeness of infix operators. 4. No runtime. Rust has a runtime about the same size of C's. This makes it possible to make operating systems like [Redox](https://redox-os.org) without third-party languages. The standard library of Rust specifies a bare minimum runtime: namely unwinding and panicking strategies. But Rust still works well without this bare minimum. This is something Go does not. Go is highly dependent on its runtime, something which carries numerous downsides. Whew, that was quite some text. 
Rust can replace Java or C# or C/C++ or any other similar language in certain situations, but not all. It's designed for systems programming with certain needs (safety and speed, in that order) but can be used in other areas. Rust can replace C/C++ if you desire you and your customers to be "safer from hackers" by making your code be memory safe. Second benefit of Rust when compared to C/C++ follows from how this safety is implemented and will cause your programs to be generally less buggy (because compiler statically analyzes access patterns to data) so writing in Rust is an investment that can pay off in shorter and less painful debug sessions. Memory safety used to be in domain of languages with garbage collectors, GC can come with certain drawbacks (pauses, efficiency) that are in edge cases hard to tolerate, most times not though. Additionally languages often times come with large runtimes that are written in C/C++ - many vulnerabilities in Java programs will stem from this part, so having Rust cover some of this part of functionality will add some security. In short, you can continue using your favorite language, but you can keep Rust around for when you need to replace C/C++ or if you just like to invest a small amount of effort that could potentially save you when GC starts acting up. 
Emily Dunham literally gave a talk at OSCON last week entitled "How to learn Rust". :P Sadly the video isn't online yet, but her slides are packed with ample links (mostly near the end): http://talks.edunham.net/oscon2016/how-to-learn-rust.pdf
Alternative solutions I've seen with several libraries is to either branch at a higher level (duplicate code for longer functions that are closer to 1 microsecond or millisecond), or to store a function pointer to the correct fine-grained call. That said, my experience is as a relative newcomer to HPC, so perhaps the examples I've seen are the exception, not the norm. However, for the applications I've looked at, branching on the SIMD feature is a low cost compared to the amount of work done in the dynamically dispatched function.
You seem to know something about the topic, I have a vaguely related question. The index trait looks like this: pub trait Index&lt;Idx: ?Sized&gt; { type Output: ?Sized; fn index(&amp;self, index: Idx) -&gt; &amp;Self::Output; } How does the `Idx: ?Sized` bound make any sense? You can't pass an unsized type by value, `index` takes an `Idx` by value, so how can there exist an implementation where not `Idx: Sized`?
I got to see part of this presentation, it was good! :D
https://github.com/ctjhoa/rust-learning
Thanks! :) That's very kind of you.
I am more concern about the vector's *content*. In C++, moving items in a vector generally requires moving them one at a time. In Rust, you can move a bulk of items with a single `memcpy`, and use `realloc` when growing. All because moving is generally not trivial in C++.
Hurrray!
Typo: `volitile` =&gt; `volatile`
Specialization hype! Big thanks to the contributors, via code, general discussion, documentation, or any other way I'm forgetting to mention! On the topic of specialization, is there a chapter in the book on it yet?
\o/
&gt;Raw pointers gained as_ref() and as_mut(), which returns an Option&lt;&amp;T&gt;, translating null pointers into None. Lifesaver, working with pointers and I am making tons of mistakes like: if ret.is_null(){ let error = unsafe{CStr::from_ptr(ret)} .to_string_lossy() .into_owned(); println!("error:{}",error); } and getting segfaults around my FFI code... Now I can write the more legible: match ret.as_ref(){ Some(ref c_err)=&gt; println!(...), _=&gt;() }
thread '&lt;main&gt;' has overflowed its stack fatal runtime error: stack overflow
The documentation on [bindings in match statements](https://doc.rust-lang.org/book/patterns.html#bindings) specifies: &gt; If you use `@` with `|`, you need to make sure the name is bound in each part of the pattern: &gt; &gt; let x = 5; &gt; &gt; match x { &gt; e @ 1 ... 5 | e @ 8 ... 10 =&gt; println!("got a range element {}", e), &gt; _ =&gt; println!("anything"), &gt; } Why is that? Why not just have one `e @` on the left and make it implicitly apply to all the alternatives within the pattern? I could imagine trying to have some alternatives bound with `@` and others with the usual `EnumVariant(name)` syntax, but I don't see how you could ever get that to type check within a single match.
Don't these do completely different things? Seems to me the first one creates a String, the second one an &amp;c_char
So about stabilization. Does this mean that as of 1.9 to_string is efficient? or only if we use 1.11 nightly? 
Typo in the docs on the page [UdpSocket](https://doc.rust-lang.org/std/net/struct.UdpSocket.html#method.set_nonblocking). TCP should be changed to UDP.
Quick note: Rust has *Affine Types* not *Linear Types*. A value of a Linear Type **must** be consumed once, a value of an Affine Type **may** be consumed at most once (but there is no guarantee it will ever be). Using `#[must_use]` is not sufficient (a use does not necessarily consume the value). It would be great to have Linear Types in Rust, however it's an open problem.
This is effectively free at runtime, correct? Since reference types are `NonZero`?
I like to see D coming all the way up. If i wouldn't be so much in love with Rust – D would be my best friend, maybe ;) 
As of 1.9, `to_string` is efficient. The release notes only discuss the stable release, nothing nightly-specific. _You_ can only use specialization on nightly, but the compiler can use unstable things when building the standard library.
Can someone clarify what the purpose of the `default` syntax is in the context of specialization? It looks really out of place, why is it required in the first place?
Awesome that is good to know. I look forward to new specializations in the standard library 
Looks like it's nightly only at this point.
Those seem like pretty complicated solutions. And the one using function pointers will add a level of indirection to the function call; like using a virtual function in C++. It also destroys all hope for link time optimizations (lto). It will be far simpler to compile a statically linked executable for each architecture and wrap the executable in a script that dispatches based on the detected architecture. e.g. using `sysctl machdep.cpu.family` `sysctl machdep.cpu.model` or `sysctl hw.optional`. Or when you have users submitting to a specific cluster, they can load up the environment for that cluster which points them to `/opt/clusters/&lt;clustername&gt;/super-genome-stuff/v1.2.3/bin/super-genome-lookup` which is a symlink to `/opt/arch/haswell/super-genome-stuff/v1.2.3/bin/super-genome-lookup` or something similar.
This. 100% - I don't see the difference between the high level implementation of an event loop and a task scheduler. Except that the latter can use multiple cores and takes advantage of optimizations. Switching variable contexts in callbacks/closures has to be at least as expensive performance wise as context-switching registers (and I assume if true that thread switching wins by a huge magnitude). I have high-level understanding of low-level OS primitives, but I haven't yet read anything that truly convinces me that one is better than the other. At some point I don't care. Threads "feel right", async seems hacky, and event loops aren't hard to implement since they are just a queue. It feels like a violation of SRP if I try to force my apps to be responsible for concurrency, which is very clearly the role of the kernel.
`Option&lt;&amp;T&gt;` is guaranteed to be represented by a single word at runtime, yes.
Glad to hear it. :) You're welcome!
The consequence is, you're asserting that the unsafe code is safe. But if you mean codegen wise, yeah, a safe fn that just calls an unsafe fn should get optimized, I'd imagine.
After the Boston Rust meetup last night a few us were talking with /u/steveklabnik1 about what Rust needs to get bigger. One of the things he mentioned was just people writing blog posts like "How do I do X in Rust?" More posts gets more exposure allowing us to seem more legitimate, creating a network effect, and well maybe even have jobs writing Rust. To that end I've started a series of articles answering the most basic of questions for the new user to help them out. edit: Hey all, I've updated the post. If you made a suggestion I incorporated I made sure to link to it and gave credit to you. Thanks for the help. You're all a wonderful community to be part of and get help and suggestions from. 
yes! :D :D :D
Sadly Rust type inference isn't flow insensitive, e.g. [this issue](https://github.com/rust-lang/rust/issues/28943).
The venue, symbol, and other things going into the url should always be ascii for stock fighter. Verifying this on parsing should be good enough. Is there a data validation lib?
As a new user, I appreciate this immensely. 
These are very emphatically _not_ exceptions, though they are implemented in a similar way. Rust will pretty much never get real exceptions.
That's impressive! Since the quality did not seem to downgrade even though the release cycle is so tight I am even more impressed by the Rust team. Hats up for you!
it's on stable 
Yup! This is very interesting, and good to see C++ improving here. It still doesn't give you nearly the same safety as Rust does; data race prevention is a non-goal, and concurrency is marked as "TODO".
Sure! Thanks for the suggestion! I'll update the post later with some of this stuff.
Opinions about this differ, but as far as I'm concerned what matters is whether the value is consumed, not whether the consuming is explicit. So if `mem::forget` (and its equivalents) were `unsafe` I think Rust would have a somewhat-restricted linear type system. As it is it's pretty clearly affine.
You should not try to use this like exceptions. You should use `Result` and `Option` instead.
As far as I understand from the Rust 1.9 docs the only difference between panics and exceptions is that panics do not contain stack trace information? Is this correct? (The docs even mention that this can be used as "a general try/catch mechanism")
It would be nice if someone outlined practical reasons for this.
It's not just about the implementation, it's about what they should be used for, and how it fits into the language. You _could_ use these to sorta-kinda emulate exceptions, but you shouldn't. This isn't a general error-handling mechanism.
That doesn't answer the question though. Also, what are the practical reasons why I shouldn't use this like exceptions, and what is a general error-handling mechanism in your mind? I am assuming you don't consider `Result` type to be error-handling mechanism?
Exceptions introduce control paths which are untyped and of limited visibility to the programmer. `Result` and `Option` are fully typed and highly visible, forcing programmers to handle error cases at the boundaries to other programmers' systems. By placing limits on the use of unwinding, we eliminate the responsibility for most programmers to write transactional "exception safe" code. The RFC discussion around catch_unwind contains a lot of discussion of the downsides of using exceptions for control flow: https://github.com/rust-lang/rfcs/pull/1236
&gt; What if I don't want to consider webpage being down a reasonable error? Because if I do, I'd need to write code to handle that .01% case in the same code that does business logic, which is so bad for code quality. You're welcome to ignore a webpage being down or panic when a webpage is down/ assert that it won't be down. &gt; However, if that ever happens, I want my error-handling code to log the failed communication session, show my user an error message, and move on. I also do not want to employ any error handling means that involve multiple threads or processes etc. So what is the idiomatic way of solving this in Rust? match get_obj() { Ok(obj) =&gt; // do a thing with it Err(e) =&gt; //log and move on } If you want to handle a certain variant of the error, like getting an e, you can simply match 'e' and ignore the cases you don't care about.
Is `format!(...)` really all that idiomatic. It feels like if this is a blog for new users it might be okay to sacrifice some level of coverage for focusing on idiomatic code. But maybe not. :D
Sure, but that means that I have to write 20 lines of error handling code in the very same place of those 2 lines of business logic code.
I don't think it's 20 lines at all. Why would it be? if let Err(e) = WebError::WrongType {// handle the error}
Okay, thanks for the link and for the discussion. The `?` operator certainly does look much better. That might actually be a solution. Still rather ugly in my opinion, but `foo?` is so much better than `try!(foo)`.
See also `!crates regex` which takes you to https://crates.io/search?q=regex.
No problem. :)
`catch_unwind` is not a general-purpose error-handling mechanism. A library that tries to pretend as such is going to make its users miserable with the deliberate lack of ergonomic support from the language. Furthermore, once support for turning panics into aborts lands, it will be impossible for library authors to assume that panics are catchable in any capacity whatsoever. `Result` remains the mechanism for handling recoverable errors.
I think nulls are about interface design, not language capabilities. It might very well be that in some cases there should be nulls/Option somewhere, just not in this particular call location. Thanks for the index stuff, that's 1 less `?`. Edit: Anyway, thanks a lot for the discussion, with `?` things aren't looking so grim. Looking forward for this feature to release and to start using Rust.
Usually, "on stable" means "I can use this on stable". Everyone who is not the standard library cannot use specialization, and therefore, it's not really "on stable".
? Only applies to Result, to be clear. Option does not have anything like ?. Happy to have a discussion.
It's for all new users but I think we should give them all the tools they need yet hand them to them with care. I'd rather have good coverage, methods explained well and when to use what. I totally understand what you're going for here though
I don't know anything about aborts, but that sounds like the only practical reason against using `catch_unwind` for handling recoverable errors mentioned in this thread. Thank you.
Ah, that makes sense. Thanks.
I don't hack on the compiler much, but `make tips` has a number of recommendations including: # Rust recipes for build system success // Modifying libstd? Use this command to run unit tests just on your change make check-stage1-std NO_REBUILD=1 NO_BENCH=1 // Added a run-pass test? Use this to test running your test make check-stage1-rpass TESTNAME=my-shiny-new-test // Having trouble figuring out which test is failing? Turn off parallel tests make check-stage1-std RUST_TEST_THREADS=1 // To make debug!() and other logging calls visible, reconfigure: ./configure --enable-debug-assertions make .... Hope that helps.
That makes sense. It might be good then to add a small paragraph about "the idioms of rust" as they appear for each concept.
LabVIEW is a good example that has a surprising amount of usage that is difficult to track.
It definitely would be easier if Rust had monadic bind and do notation like Scala / Haskell Closest I can find https://github.com/TeXitoi/rust-mdo This way the result of a series of chained Option or Either/Result computations is the final result, or the first failure encountered. It is really freaking nice, and rust needs to offer this in some fashion.
I think `to_string` is better than `to_owned` because it signals to the reader that the result is `String` rather than `Borrow&lt;T&gt;` for some `T`. That way, eventual type errors become simpler too.
Also, `!rust Option` which takes you to http://doc.rust-lang.org/std/?search=Option
They are the same thing for `&amp;str`, but may be different for some other type (eg. a custom type you create). Well except the `.push_str` method that [works only for &amp;str](https://doc.rust-lang.org/std/string/struct.String.html#method.push_str).
I'm so happy that we finally can have non-blocking reads and writes in the standard library! It would be nice to have a cleaner API though. It doesn't make sense to make a socket non-blocking and then set a read or write timeout, for example. The documentation is also very sparse.
&gt; &gt; The RFC discussion around catch_unwind contains a lot of discussion of the downsides of using exceptions for control flow &gt; Please see this answer. I am certainly not trying to use exceptions for control flow. I've never understood this attitude. Exceptions are a control-flow mechanism. They can be used [to implement other patterns](http://okmij.org/ftp/ML/resumable.ml). Smarter folks than I have even argued that their untypability is [essential and useful](https://existentialtype.wordpress.com/2012/12/03/exceptions-are-shared-secrets/).
Nice job Rust team! Thanks for making Cargo behave well when run concurrently, Alex.
That's the best part of this release for people who wrap C libraries in a safe Rusty API. 
Not sure how lightweight you want to keep your Docker image, but I think [racer](https://github.com/phildawes/racer) be an awesome addition to it. Although that would also involve requiring the [rustlang/rust](https://github.com/rust-lang/rust) repository be cloned somewhere on the system and setting the proper environment variable to point to it. Is this something that would be an appropriate part of the docker image? I know very little about docker. Or maybe since racer is a development tool, it would be more appropriate in a rust-dev image of some sort.
The difference isn't in syntax. Its all the things you don't have to write when using exceptions: 1. You can just decide not to catch exceptions, causing your program to crash. 2. Intermediate code is not required to be explicit about the fact that exceptions are passing through it (this can lead to _unintentional_ failures to catch). I know that languages like Java have "checked exceptions" which don't have these attributes. They _do_ still lack explicit identification of which call throws an exception within a function, which is important information to be lose, and otherwise are just a big special case for what Result is, without all of Result's expressive combinatory methods. Even if you do catch the exceptions, its much easier to leave your program in an incorrect state when recovering from an exception. You could fail to consider the implications of catching a _particular_ call, but still have the catch which you wrote with a different call in mind. If they throw the same exception (or related ones, in inheritance based systems), even checked exceptions will not help with this.
Honestly? I have no idea. It was right before bed when I wrote that, and it didn't twig at the time.
FWIW I've always been on board with the "errors are values" style error handling that both Rust and Go have decided to take on, but this explanation really drove it home for me. Nice one.
Having a look right now. Stay tuned for a pull request. edit: https://github.com/fmenozzi/dinky/pull/1 The two real things to watch out for are: * Taking &amp;Vec&lt;T&gt; when you can take &amp;[T] - slices are cheap * For loop over the length of an array and indexing into the items - this is better expressed by iterators of different types. I chose the Windows iterator.
But really, that's why String::from is still the best.
Thanks for taking a look! I merged in everything except for the Windows iterator (it doesn't work for polygon triangulation). I noticed that you removed a lot of the refs (&amp;) when passing arguments to functions. Why is that?
Our reasons : - We have to interface with tons of 3rd party software, most of them are using c++ apis, so it would be too much work to create wrappers for all of them. If anybody knows tools to automatize this, I'm all ears. - I'm the only one who knows rust, or follows the news, at the company (well, somewhat know), and the others don't feel the need to move aways from c++. - Tooling doesn't feel as mature as for c++. Racer + VSCode works acceptable, but not as fluid as CLion or Visual Studio. - Not many of the apis we use have rust bindings. (alembic, openimageio, openvdb, openexr, etc) 
Something that helps a lot is setting `RUSTFLAGS=-Ccodegen-units=&lt;number of your cpu cores&gt;` as it will allow LLVM to do it's optimizations in parallel.
&gt; Option does not have anything like ?. [Yet.](https://github.com/rust-lang/rust/pull/33389)
Someone on twitter suggested that an improvement would be to motivate things a bit more; include a few sentences on _why_ you'd be doing this conversion in the first place. Thought I'd pass it along. :)
Checked exceptions still don't show up at the call site, only in the surrounding block or function definition.
The `e @` is not required to be at the top level of destructuring. For example: fn main() { let x = (1, 5); match x { (e @ 1...2, _) | (_, e @ 1...2) =&gt; println!("yes: {}", e), _ =&gt; println!("no") } } 
Still not seeing how this is different from Result.
There's also [into](https://is.gd/CsB5lI), which is nice and short. It is less clear than `to_string` or `String::from`, but I think it would make more sense to a newcomer than `to_owned`.
Nice! I'd love having a `!rust cratename fnname` and be redirected to the documentation of that particular crate, if mentioned in crates.io
Yes, but it gives a sanity path for those of us that cannot move away from C++ for system programming on Windows. Rust still needs to handle COM, WinRT and mixed debugging with .NET to be a viable alternative.
I think there is something missing at &gt; Next we will install Nickel.rs if you haven’t already: Because it only tells you how to install rust (not nickel!) but never seems to mention that you have to add nickel as a dependency.
Actually, learning materials for RUST exist right on the web site @www.rust-lang.org in the documentation. It also provides a playground to write and check out your code. Why not check it out and see if you understand it? The same goes for Dlang, another excellent newer language. As far as Python goes, it's been heavily promoted as easy to learn and easy to produce code quickly, but it's also very slow compared to D or RUST or C, C++ and other better languages including the Pascal derivatives. Personally, I think Python sucks. Lua is extremely quick. If you understand HTML and CSS, there is no reason not to understand Rust or D. It all takes practice.
Note that the main reason it's better to ask for slices in function arguments is it makes the function more reusable. References to slices aren't really "cheaper" than references to Vecs.
I assume because it might be possible in the future. https://github.com/rust-lang/rfcs/issues/990
Can someone explain what are the "crazy amounts of machinery" mentioned in the article? Seems like both to_string and to_owned are trait methods, so both do dynamic dispatch. What's the difference?
I've implemented the `From` trait for this struct, which I use for FFI interaction: #[repr(C)] pub struct Array { pub data: *const c_void, pub len: libc::size_t, } impl&lt;'a, T&gt; From&lt;&amp;'a mut [T]&gt; for Array { fn from(sl: &amp;mut [T]) -&gt; Self { let array = Array { data: sl.as_ptr() as *const libc::c_void, len: sl.len() as libc::size_t, }; mem::forget(sl); array } } This works for functions like pub fn func&lt;'a&gt;(lons: &amp;'a mut [f64], lats: &amp;'a mut [f64]) -&gt; (&amp;'a mut [f64], &amp;'a mut [f64]) { // some stuff requiring lifetime annotations bc of a generic } But when I try to use `Into` I get an error: src/ffi.rs:129 let mut longitudes_v = unsafe { longitudes.into() }; ^~~~~~~~~~~~~~~~ src/ffi.rs:129:9: 129:25 help: run `rustc --explain E0277` to see a detailed explanation src/ffi.rs:129:9: 129:25 note: `[f64]` does not have a constant size known at compile-time src/ffi.rs:129:9: 129:25 note: all local variables must have a statically known size src/ffi.rs:129:48: 129:52 error: the trait bound `[f64]: std::marker::Sized` is not satisfied [E0277] src/ffi.rs:129 let mut longitudes_v = unsafe { longitudes.into() }; I've written my own version: impl Array { pub unsafe fn as_f64_slice(&amp;self) -&gt; &amp;mut [f64] { assert!(!self.data.is_null()); slice::from_raw_parts_mut(self.data as *mut f64, self.len as usize) } } Which works, but I'm wondering whether there's a way to get `Into` working.
I personally am somewhat optimistic on this point. Go has a similarish split between idiomatic error handling (using return values) and a panic/recover mechanism. The details are of course very different than Rust, but the split exists in both languages. Arguably, using panic/recover in Go is more convenient than doing so in Rust and error handling in Go is probably less convenient than Rust. Nevertheless, folks seem to have stuck with using return values for error handling, so it gives me hope! (It is, however, true that some libraries use panic/recover as an error handling mechanism *internally*, but it's reasonably rare and typically because writing `if err != nil { ... }` out can get a bit onerous in some circumstances.)
Very cool! Any chance of splitting Teddy into a separate crate? That would make it easier to play around with...
If g() can result in the same error type as f() then the first example will compile despite the error from f() not being properly handled, while the second example wont compile. try { g(f()); } catch (SpecializedException e) { // handle error from g() } match g(f()) { Ok(_) =&gt; // continue SpecializedError(e) =&gt; // handle error from g() } 
Who bans the banners?
To be fair Stackoverflow is not as useful as for languages that did not change as much or have been around for a long time.
I don't have any first hand experience (yet), but you might want to check out the [sharedlib](https://tyleo.github.io/sharedlib/doc/sharedlib/index.html) crate.
I see this as being a *good* property. It lets you write more concise code in the case where you don't care which of the operations fails, and if you *do* care, you can just use two separate `try`/`catch` statements.
why is it called "default"?
And it is indeed. Not quite fluent in rust just yet....
I am not an expert, and I look forward to hearing from one. :-) From what I have heard: Incremental compilation within a crate is actively being worked on. I think It is waiting on MIR. Don't know not an expert. Splitting up the rustc/std into sevral crates can be done. Then only the crate that changed needs to be compiled. It can be done when rustic switches from make to cargo, which is actively being worked on. While I was typing some compiler hackers chimed in so I will stop putting my foot in my mouth.
First, there is a standard location for source files, i.e. in `src/`, because Cargo assumes that unless you tell it otherwise. I don't think there's many "standard" ways of organizing your crate's modules. There's a few conventions I have seen, e.g. FFI function definitions are typically in a `ffi` submodule, and low-level bindings in a `sys` module or an extra crate with that name. Other than that, just structure the modules so that you find it logical, and so that privacy/visibility makes sense. In a library, the API that the library user sees is often much less hierarchical than the modules, because API items can be re-exported (`pub use`). The std library is an extreme example for that; much of its content is reexported and either located in OS-specific submodules, or even from different crates altogether (`core` and `collections`). But in most small libraries you'll have a `lib.rs` that contains a lot of private `mod`s, and then `pub use`s those parts from the modules that are public API. For larger libraries, this is shifted to public submodules containing the `pub use`.
Absolutely. When this was first announced, many people said "oh cool now C++ will be as safe as Rust", which isn't true. That's all :)
&gt; compile-time automatic differentiation, I'm not even certain if such a library is possible to write in Rust yet Should be. I didn't know about [the technique](http://blog.sigfpe.com/2005/07/automatic-differentiation.html) until today, but it mostly involves substituting different types for floats. Super simple proof-of-concept: Define your functions with a `DualF64` type then wrap them with an appropriate macro to compute a the (f(x), f'(x)) pair. Clean generic abstractions probably do need natural-number type parameters and higher-kinded typing. Those aren't supported yet, but could be worked around with a syntax extension.
The flow we use for Go (since we dont use Rust in prod... yet) is one image to build the binary and then we place it in another for deployment. Its kinda silly to have compilers in production images.
I assume you don’t mean just changing the syntax. What would `|&gt;` do, and why would it be preferable to `?` ?
"String slice" and "String"
&gt; Seems like both to_string and to_owned are trait methods, so both do dynamic dispatch To be clear, you only get dynamic dispatch if you have a trait object. fn foo&lt;T: ToString&gt;(x: T) -&gt; String { x.to_string() } is statically dispatched.
I like C++ a lot, but I am also aware of its painpoints. Ironically, the way some in the C++ community criticize Swift, Go, Rust, D among others, is similar to the way we used to be criticized by the C guys when C++ was the new kid in town. Now 20 something years later, their beloved C compilers are being written in C++. You just need to keep pushing it forward. Also congratulations on the new release.
yes; I see it must do ```*foo.index(..)``` to match the behaviour you get with C++ references. but in future would it be possible to return some intermediate object with 'deref' implemented for access the the underlying object (a smart-reference?)
You don't need an unsafe block to call `into`, but that doesn't affect the type error. But `Array` is a sized type, so if it's really using the `From` impl that you posted then it doesn't make any sense. Perhaps there is another impl floating around? What are the types of `longitudes` and `longitudes_v`?
Those are all valuable benefits. I guess I should say "most people" rather than "anyone."
looks like a spamming post ..
As long as the language allows leaking types, it cannot support linear typing. The idea of Linear Types is to guarantee that at some point in the future the "on Drop" action will be executed. Of course, you might accidentally making it a bit further in the future than you'd like by storing it for longer than necessary, but it'll still be executed. I think a framework support Linear Types would require: - some kind of `Linear` trait - that anything owning a `Linear` value become `Linear` too (impossible to express today in Rust) - that `std::mem::forget` be bounded by `!Linear` ... but in the meantime, affine typing is already great!
If a value can be leaked (`std::mem::forget` is safe), then it cannot be of a Linear Type. That being said, Affine Typing is already more than most languages propose :D
We'd have to lose `Rc` and/or `RefCell` for that to happen. Since both of those are pretty heavily used, making Rust a truly linearly-typed system like you describe would be a pretty large change to the type system. fn safe_forget&lt;T&gt;(data: T) { use std::rc::Rc; use std::cell::RefCell; struct Leak&lt;T&gt; { cycle: RefCell&lt;Option&lt;Rc&lt;Rc&lt;Leak&lt;T&gt;&gt;&gt;&gt;&gt;, data: T, } let e = Rc::new(Leak { cycle: RefCell::new(None), data: data, }); *e.cycle.borrow_mut() = Some(Rc::new(e.clone())); // Create a cycle } [[source]](https://github.com/rust-lang/rust/issues/24292#issuecomment-93513451)
Yes.
Out of curiosity why would `format!()` no be considered idiomatic?
The situation is actually kinda subtle, since there are separate notions of "is it memory-safe to `forget` this" and "can it be disposed of without requiring any additional input (i.e. a nullary function)". Notes-dump: `trait Forget` for types where not running any destructor is memory safe `trait Drop` for types which have a unique, obvious, nullary way to consume them, corresponds to Clone what about the by-value Drop, Inner&lt;T&gt;, etc. stuff? `trait DropIsForget` where drop() is just forget(), corresponds to Copy DropIsForget: Drop, Forget basically the same types which are `Copy`, plus `&amp;mut`? (any exceptions in the reverse direction?) both Drop and Forget: all existing Rust types Drop but not Forget: the destructor *must* be run, can't be passed to `mem::forget()`, `Rc::new()`, etc. the type is truly linear, but has the call to consume it inserted automatically Forget but not Drop: should be consumed manually in some specialized way, but "forgetting" to do so doesn't implicate memory safety example: File where you need to handle errors from `close()` explicitly neither Drop nor Forget: the type is linear and must be consumed manually example: &amp;out being assignable requires `Drop` `&amp;mut &amp;out T` can be `replace()`d and `swap()`ped only
Wow! Nice notes!
At least to me the use of `.to_string()` makes it more obvious what you want to do. Also (I am not entirely sure) but I think the computational complexity of `format!()` is greater than the use of `.to_string()`.
I'm still having some trouble with memory management, such as understanding when it's appropriate to use the * symbol to dereference. I imagine people coming from C/C++ don't have as many issues with this, but my background is in Java so memory management in general is something I need to spend a some time on. 
You've switched the direction of the conversion and these examples don't quite compile together -- can you show a full self-contained example that compiles, and your desired generic code that doesn't typecheck? You can't have generic `#[no_mangle]` functions, so the question may be moot.
Genode is a server-based userspace on top of a L4-like microkernel. So it goes like this: it's relatively easy to write a microkernel. You provide couple of syscalls for spawning new processes and IPC between them. But now the problem: how the heck do you organize hundreds of services that normally would be part of big monolitical kernel like Linux? It's very difficult because you're now writing a distributed architecture on top of your tiny micorkernel. This is part of [Linus Torvald microkernel approach critique](http://yarchive.net/comp/microkernels.html) ctrl+f "it's a hell of a lot more difficult to write". This is exactly what Genode did. It's really impressive and visionary work. So the announcement is: " The current Genode release introduces basic support for executing Rust programs as Genode components. This support includes the build-system integration, the configuration of the LLVM-based Rust compiler, and the port of the low-level language runtime. A simple example is provided via the libports/run/rust.run script and the accompanied code at libports/src/test/rust/. The example runs on the x86 (32 and 64 bit) and ARM architectures. " So Genode is flock of processes talking to each other, implementing different services etc. And now you can write them in Rust. I guess the syscall APIs were exposed as a Rust library too. Personally I find it really nice, since one of the things that I found off-putting in Genode was C++... :D 
Awesome I'll add it to the list! Thanks for your input.
Except you can "not care" by mistake. In Rust, once the `?` has fully landed fully, you can write something fundamentally the same as the try/catch, except if you didn't realize one of your functions threw an exception, Rust would bother you about it: if let Err(error) = catch { g(f()?)? } { // handle error } And there are proposals to create sugar that looks more like try / catch in the future, as in catch { g(f()?)?; } match error { Error::Variant1 =&gt; { } Error::Variant2 =&gt; { } } The important thing is that you have to add the `?` to all your "throwing" calls so you can't have one without realizing it.
That makes sense. I'm using Rust in prod for periodic data processing tasks that are started externally and run on the build server anyway, so I didn't bother pruning the image. It also has some non-Rust dependencies so it's based on ubuntu.
That response is informative in a way that a complete non sequitur is informative, as far as I can tell. (Or do you think that SML isn't a strongly statically typed language, or something?)
Typo: &gt; a module that unsafe code somewhere inside of it contains? It turns out I have a lot of questions. Some of them are things I don't completely understand, some are nitpicks, and in the others I just want to hear your opinion since you seem to have given it a lot of thought to these examples. I agree with the general tone of the blogpost and love to see progress on "formalizing unsafe". Nice Job! - I don't know about the module glanularity. `Vec` itself is a module and contains unsafe code, does that mean that within `Vec`'s implementation the compiler looses all aliasing guarantees of safe Rust? - &gt; except they sometimes employ type-based alias analysis; we would not) why not? LLVM alias analysis is being revamped in a GSOC project this summer, shouldn't we at least try? AFAIK C and C++ let you access an address in memory either by a pointer to the type of the object it stores, or by a `char*`, do we really want to allow unsafe code to access an address in memory with a pointer of any type? The example about `RefCell` below is not strictly about aliasing (but reordering `drops`/RAII in a module that contains unsafe). - &gt; let copy: &amp;mut [T] = unsafe { mem::transmute_copy(&amp;self) }; If we decide to make this illegal in unsafe code, can we lint against it and suggest using pointers instead? - &gt; (Note that this raises another interesting question, though, about what the legal aliasing is between (say) a &amp;mut and a *mut that are actively in use – after all, an &amp;mut is supposed to be unique, but does that uniqueness cover raw pointers?) Could we "move" a `&amp;mut` into a `*mut`? That is, `&amp;mut` gives you a guarantee that it is the only way to access that memory, but in unsafe code one could "move it" into a `mut*` such that one cannot use `&amp;mut` anymore until one reassigns something to it (restore the invariant). `*mut` don't offer the same guarantee as `&amp;mut` and can be copied and such. - Here: // interestingly, this cast is currently legal in safe code, // which is a mite unfortunate, but doesn't really affect // the example x as *const _ as usize Why is this unfortunate? Encoding a memory address in an integer is fine, it cannot cause memory unsafety. Decoding an integer into an address is something different, and that can cause unsafety, hence why it is unsafe. - Isn't it possible for `escape_as_usize` to say that the `usize` can only live as long as the integer behind `&amp;i32` lives? That would make it a bit safer (and the same for the consume function).
I think the CoerceUnsized stuff is unnecessary there; the same code compiles without it: https://is.gd/iZSVF7 Doing this without a generic type, like in the original example, would require some way to write custom impls of the [Unsize trait](http://doc.rust-lang.org/core/marker/trait.Unsize.html), which isn't possible even in nightly as far as I know.
This sort of reminds me of inline ASM and how it gets (not) optimized. Come to think of it, unsafe Rust really is the "strongly-typed inline asm" of rust, which I guess is what /u/Gankro has been telling us all along :) I personally feel that the examples shouldn't work. I'd prefer something where safe rust behaves exactly the same in an unsafe block, rather things are optimized based on their type. Aliasing two &amp;muts in an unsafe block is not okay, and &amp;muts get reordered freely. On the other hand, the compiler is careful about reordering raw pointer accesses. Open question whether the presence of a raw pointer should inhibit nearby &amp;muts from being considered unique (same as the one posed in the post about aliasing *mut with &amp;mut) This still leaves us to define what "careful" and "nearby" mean; both involving some form of boundary ; though I think making the boundary the unsafe keyword may work. This would forbid private actually-unsafe safe functions, though. The reason I prefer the above model over the proposed optimization boundary one is because I feel like the boundary might be too fragile, and hard to grok. On the other hand, "don't break core rust invariants, ANYWHERE" is a pretty straightforward thing to follow. Of course you have to know those invariants :), but most of them are simple. There's also the "put unsafe-critical code within the unsafe block" requirement for the boundary to work -- not hard to grok but easy to mess up. It's ... a tough call.
Could you speak more on some specific examples of Rust being more readable than C? Would this advantage apply to C++ as well? (My inclination is that it would not if the features that improved reability were generics, iterators, and maybe some forms of overloading).
`unsafe` is more than okay. &gt; You can instantiate such a structure by creating it with a known length, Thanks. A [tiny bit more experimentation](https://is.gd/MCphI8) shows that as of current Nightly, the additional data in the fat pointer is the length of the array. (Makes sense. It's the most obvious place to put it.) With `size_of_val`, `RawVec`, and a willingness to venture beyond "unstable" into "undocumented," I'll see if I can handle the dynamic-allocation case too.
Since we're mentioning the many ways that exist, there's also `String::new() + string`, which is the worst way because it may allocate twice if the optimizer doesn't figure it out. I'll also say my own opinion (since this is just a style discussion at this point) is to use `String::from` if you have a literal or a variable, and if you are chaining methods, use `to_string()`, eg: String::from("foobar") String::from(string) value.as_str().to_string()
Oops, I see I miscounted the number of dereferences. (This has nothing to do with what you just posted since your reply wasn't a counterpoint.) A Vec is not a "stack allocated structure". It's possible to allocate it on the stack, but that isn't required. This argument is silly because the difference between 2 and 3 dereferences is so tiny that it really doesn't matter. The reason people advocate the use of slices in function arguments is just because more things can turn into a slice than into a Vec. It's not because it's faster.
&gt; How do you define an unsafe boundary? This really depends on what happens inside the unsafe block. If the unsafe code relies on a value to uphold a language invariant, anywhere that value can be mutated is inside the boundary. Unfortunately that rule is not very easy to analyze. &gt; Naturally alternative models might consider this code illegal. They would require that one use raw pointers, as the current implementation does, for any pointer that does not necessarily obey Rust’s memory model. I personally find it more intuitive that Rust would universally uphold the aliasing rules around references, even in unsafe code, but have absolutely no guarantees about raw pointers. :-\
You still need to reimplement basic operations and functions somehow. Introducing a new type for reals looks like the cleanest way to do so.
Is there an easy way to poll the keyboard and see which keys are currently being pressed?
I feel in agreement about disallowing aliasing `&amp;mut`. In general invariants of Rust should be upheld inside unsafe and raw pointers should be used to do the illegal bits. I most certainly feel that stuff returned out of unsafe **must** under all circumstances be legal under safe Rust rules. I haven't put much thought in this (and I use unsafe pretty often. Too often sometimes...) though...
`unwrap()` in tests is one of the best places for it, `unwrap()` can be thought of as another way of writing `assert!()` (in normal or test code). You may want to use `expect()` with a message string for more context though.
For the first assert I would personally use assert_eq!( tokenizer.next(), Some( Token::ParenOpen ) ); The panic message will be more explicit I think. I am not sure what others use, but would be interested to see :)
Also, it seems as though the use of `write!` has increased the running time of the image-generation code from ~6.5s to ~12s. `write!` is a lot cleaner, but this twofold increase in running time won't hold up when I start adding more images. Any idea why this is happening?
If by "easy way" you mean "crossplatform, batteries-included solution", no. Not that I can think of at least. If you're working in a terminal, you can set the no-echo attribute and poll for a character. The `pancurses` crate appears to make this pretty easy: https://github.com/ihalila/pancurses#example-pattern-matching-with-getch However, it does require third-party libraries on both Windows and *nix. On Windows, the library will be downloaded and compiled as part of the build process; on *nix, you have to have [ncurses](https://www.gnu.org/software/ncurses/) installed (just search it in the package manager for your distro). What this will *not* do is tell you all the keys being pressed on the keyboard at once. Each keypress will emit a character, and holding keys will emit streams of the same character (I assume). Multiple keys pressed will emit mixed streams of characters (also an assumption), and modifier keys will change what character is emitted (assumption). On Windows, you can poll keyboard state with [GetKeyboardState()](https://msdn.microsoft.com/en-us/library/windows/desktop/ms646299.aspx), which is wrapped for you [by winapi](https://retep998.github.io/doc/user32/fn.GetKeyboardState.html) via the `user32-sys` crate. Just instantiate a `[u8; 256]` as mutable and call `GetKeyboardState(bytes.as_mut_ptr())`, then index into the array with the virtual key codes listed [here](https://msdn.microsoft.com/en-us/library/windows/desktop/dd375731.aspx); the `GetKeyboardState()` docs tell you how to interpret the values in the array. On Linux (but Linux only, I think), you can open the keyboard as an input device and read it like a file: http://stackoverflow.com/questions/20943322/accessing-keys-from-linux-input-device Unfortunately, I can't find a crate that wraps the `linux/input.h` header, which you want for the `input_event` struct definition. `nix` doesn't have it, neither does `linux-api` nor `libc`. You can write your own wrapper, of course, if you can find the source of the header (I always had trouble navigating Linux sources online). Additionally, it only gives you key down/up events in the stream and may not give you events from before you opened the input device, so there's still a lot of manual work there. BSDs and OS X presumably have their own APIs for this, but I wouldn't even know where to start looking.
WOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOAAAAAAAAAAH. I had NO IDEA catch-unwind was in the works. This is huge for me! Thank you, thank you, thank you!
(edit because subreddit style makes my bold too emphatic. Don't need to yell at anyone &gt;&lt;) `unsafe` does not always mean mean "trust me while I perform questionable pointer manipulation." I've actually used it just to read the CPU's cycle counter. (RNG seeding for DSP.) Forcing that kind of thing into its own submodule just-because is a very real cost. It makes the language more just-because, which is both harder to learn and harder to live with. ---- pub fn split_at_mut(&amp;mut self, mid: usize) -&gt; (&amp;mut [T], &amp;mut [T]) { let copy: &amp;mut [T] = unsafe { &amp;mut *(self as *mut _) }; let left = &amp;mut self[0..mid]; let right = &amp;mut copy[mid..]; (left, right) } &gt; **My belief is that this program ought to be legal.** One reason is just that, when I first implemented split_at_mut, it’s the most natural thing that I thought to write. And hence I suspect that many others would write unsafe code of this kind. I believe this places a little too much value on beginner's-mind. Rust is aggressively optimized. Thus programs are expected to follow strict aliasing rules. This is inherently hard, which is why the safe type system is fairly restrictive. Using `unsafe` is okay, but it requires understanding the optimizer's assumptions. This operation should ring alarm bells: `&amp;mut *(...)` . If it doesn't there's a failure of education and understanding about the concept of "uniqueness." Likewise this is also mistaken: &gt; what the legal aliasing is between (say) a &amp;mut and a *mut that are actively in use – after all, an &amp;mut is supposed to be unique, but does that uniqueness cover raw pointers? Yes `&amp;mut` should be unique; that's the promise we're making to the optimizer! "Dear optimizer, you're the only game in town here." No other pointers touch that memory. No other threads. No memory-mapped devices. No cosmic radiation flipping bits. All of those things interact with optimization in undefined ways. Hope for crashes because crashing is the best thing that can happen. ---- The second example is a bit more subtle. let t = *self.value; // copy contents of `self.value` into `t` mem::drop(self.borrow); // release the lock t // return what we read before If Rust follows the C/C++ model for sequential execution, the steps are as follows with their defining sequence points: 1. load from `self.value` ((end of initializer)) 2. load from `&amp;self.borrow` ((before function call)) 3. call `mem::drop` and drop the return value of `mem::drop` ((end of statement)) 4. return `t` ((return from function)) Actions can only be moved between these steps if the optimizer can prove that they are side-effect free. Since `self.value` has the `&amp;'b` type discussed, the optimizer has its proof. This is indeed subtle. But before saying the optimizer needs to be reined in, let's look at optimizer-friendly changes. There is a problem that `struct Ref` *lies* to the optimizer. The type should be `*const T` *not* `&amp;'b T`. Then access is forced into either `unsafe`: unsafe { let t = read_volatile(self.value); // read before release mem::drop(self.borrow); // release the lock t // return what we read before } or, using the public interface that needs to be correct anyway: let t = *self; // use Deref&lt;Target=u32&gt; mem::drop(self.borrow); // release the lock t // return what we read before or, honestly, this works too: unsafe { let t = *self.value; mem::drop(self.borrow); // release the lock t // return what we read before } because the only reordering that's possible in this case is completely benign. As a slightly more extreme example, if the optimizer sees this: 1. Check borrow state of `RefCell` 2. Increase shared borrow count 3. copy from inside of `RefCell` 4. Decrease shared borrow count the second and fourth step can be and should be completely omitted. In more complex cases, the optimizer should be more careful. Using `*const` makes it a bit more careful. `*mut` makes it a lot more careful. `&amp;` and `&amp;mut` - the programmer assumes responsibility. *This should be more clearly taught* since it's an area where unsafe Rust is more subtle (and potentially faster) than C or C++. *If "unsafe Rust is more dangerous than C" were well understood,* this wouldn't be a problem. In the same vein, more work should be done to nail down the memory model. I'm happy to cheer that effort on, also a rewrite of the 'Nomicon, but I'm not a fan of even the basic idea here.
Don't worry, I only learned this trick in the past week :-D
Just curious, if you can tell me, what kind of software is this? It's an interesting case that you're not primarily a software company, but you need such fine-grained control over the memory.
True, all of this is unsafer-than-unsafe. Not really useable until it at least reaches "unstable." The standard library would need more functions along the lines of `slice::from_raw_parts`. The synthetic reference passed to `size_of_val` is indeed invalid. It points to the special address of zero-size values and hopes that `size_of_val` doesn't dereference it. That shouldn't happen because the size should only be calculated from the `len` field and magic numbers associated with the type. But, you know, "shouldn't happen." Your example won't work correctly if the fields are zero-sized.
To handle `Result`, there is only one thing to check: what error this function returns? To handle exceptions, you need to look all the way down (DFS?). For example, when coding in Python, I often struggle with: 1. do I mis-catch anything? 2. do I catch too much? (typing on iPhone, would like to show some code later)
Yes. There might have been a delay before the binaries were uploaded. Have you tried again recently?
Expect is definitely useful for stdlib errors such as file not found though, to specify things that your program has access to like file name but aren't included in the error.
Ok, thanks . As an aside, i do use #[test] on stable, but perhaps I'm not supposed to be able to? :)
I do it. Maybe not in an `assert` itself (there I'd just use `assert_eq!(..., Some(x))`, but otherwise all over the test code. Interestingly enough, I got into nasty argument on `#go-nuts` IRC, looking for a way to do something similar in Go, as I got tired of writing `if err != nil { t.Fatal(...) }` every other line.
Clippy uses a lot of compiler internals and will probably never be able to use stable rust. This isn't an indictment of stable rust, you should be using stable and writing programs that work on stable. Clippy is a developer tool, though, and will not be distributed with your library, so its fine for it to require nightly locally. Just use rustup.rs if you want to switch. That said, we may at some point make clippy something that you can fetch with rustup itself, along with rustfmt. If this happens, it will be able to be used with a stable compiler the same way libstd can be used with a stable compiler even though it uses nightly only features internally.
Same for me, I tried it before but struggled. I just started it again and I'm loving it! I especially love how much the book has improved. 
Indeed, I wasn't paying attention about how many writes there will be. Buffering will be necessary. This is a case for BufWriter. `use std::io::BufWriter` and then adding `let mut file = BufWriter::new(file)` after opening the file should do the trick.
Not really. Cargo doesn't have this functionality. Clippys readme does explain this though.
I use `cargo clippy` to lint-check my local code, this does not require nightly compiler. Does that not serve your need? Edit: I was wrong.
&gt; Isn't this only true when f is defined in a module that contains `unsafe`? No. It just requires the implementation of slice iterators to be in a module that contains `unsafe`. If you are pessimistic about what happens inside of `unsafe` blocks, then the only thing you can assume inside of the loop is that you have a pair of an `&amp;mut u32` and a `&amp;u32`, where the scope of the borrows is bounded by the loop body. In particular, the guarantee given by the `&amp;mut u32` only implies non-aliasing for the body of the loop. Given the function signatures involved, you can't really assume anything stronger than this across loop iteration, as I mention in my second point about `alias.scope`. In C, the equivalent use of `restrict` would be this: void f(int* a, const int* b, size_t size) { for (size_t i = 0; i &lt; size; i++) { a[i] += b[i]; } } This loop can't be vectorized unconditionally because the arrays `a` and `b` might overlap. However, if I add `restrict` it can be vectorized: void f(int* restrict a, const int* b, size_t size) { for (size_t i = 0; i &lt; size; i++) { a[i] += b[i]; } } This is because `restrict` states that only `a` and derived expressions will be used to access (modulo the slightly odd rules in C for what constitutes an access, which doesn't affect this example) the memory object pointed to by `a`. In a natural interpretation of Rust, one would hope that a `&amp;mut [u32]` comes with the guarantee that all accesses to that same object in the scope of that borrow are derived from that same `&amp;mut [u32]`. However, in the body of `f`, the only thing that is known is that the result of calling `next` on the iterator returned by `a.iter_mut().zip(b.iter())` gives a `Some((&amp;mut u32, &amp;u32))`. In particular, there is no knowledge that the memory access to `p` in the body of the loop is derived from `a`. And in fact, with `unsafe` code, you could implement a function with an identical signature to `iter_mut` in a way that violates this assumption. For example, you could leak a heap allocation of a `u32` and return a pointer to it, or you could take a mutex (that is never unlocked) to return another `&amp;mut u32`. This example seems really silly, because it is so obvious from looking at the implementation of `iter_mut` that the pointers returned by its `next` implementation are derived from the original `&amp;mut [u32]`, but Rust's type system isn't strong enough to express this in a composable way. And you certainly don't want a rule along the lines of "well, look in the implementation of the function to see if it returns a pointer derived from its input".
I tried installing cargo-clippy 0.2.2 with the same results. For reference, the errors are the same; variants on: clippy_lints/src/lib.rs:3:1: 3:25 error: #[feature] may not be used on the stable release channel 
I'm getting the same issue with `gdb` but `lldb` is working fine for me, obviously it's not a solution to the problem but at least you can debug in the meantime.
That might not be the right main function, try `info functions main` and break on the appropriate one. I think rust has a generated wrapper main function.
Yes much better, if the Rhs was None, the unwrap would panic, now the assert_eq yields it instead
You don't need locks if it's just a queue, there are implementations of lock-free queues and stacks out there.
&gt; but I'm just getting 1.10 nightly. There may be a delay of a few days until the version number is changed. But it's not like you'll get anything different: since nightly is a daily release, there's no big step from the last 1.10 nightly to the first 1.11 nightly.
Is it possible to remove the run-time dependency so that people can build it using a nightly, and run with a stable version?
On the matter of C's illegal to have a pointer more than one element past the end of an array, it's to do with "A pointer should not overflow one past the end of the array" arbitrary past-the-end guarantees are impossible. With how alignment works tho it effectively means "A void* may point anywhere within the phantom 1-past element" except I'm not sure about larger structs
Here's a playground link which compiles: https://is.gd/B5SJib Sorry, I was being unclear: I'd like to be generic in `impl&lt;'a&gt; From&lt;Array&gt; for &amp;'a mut [f64]` in the same way that I am for `impl&lt;'a, T&gt; From&lt;&amp;'a mut [T]&gt; for Array`, not in my external function. At this point, everything works, but I think I've made it a bit too complicated.
We technically could statically link to the rustc stuff, but that would mean a huge link step when creating cargo clippy and a large binary.
Running time has dropped to ~8.5s, even if I preallocate the buffer with `with_capacity()`. Is there something fundamentally slower about using the `write!` macro compared with other methods?
Link fixed. The rough idea is to expect that `unsafe` blocks will contain pointer-aliasing bugs, thus to not optimize as aggressively the modules they contain.
Wait, is transmuting from `*const T` to `&amp;mut T` correct? I thought `&amp;mut T` could only be formed via a `*mut T` pointer. (Just looked into the nomicon but couldn't find anything about that...)
It is correct. [Unique&lt;T&gt;](https://github.com/rust-lang/rust/blob/master/src/libcore/ptr.rs#L677) in libcore (which Vec uses internally) converts the *mut T it's given to a *const T and later in [Deref](https://github.com/rust-lang/rust/blob/master/src/libcore/ptr.rs#L731) it transmutes it back to a *mut T again. This is important for type variance: &gt; *const has the exact same semantics as &amp;, so variance follows. *mut on the other hand can dereference to an &amp;mut whether shared or not, so it is marked as invariant just like cells. https://doc.rust-lang.org/nomicon/subtyping.html
ok I remember drawing some table, whats the status in todays rust? (i'm unsure) type |reg cache?|C/C++ |rust,safe |rust unsafe ===================================================================== aliased,readonly |no |const T* | | aliased,r/w |no |T* | | no_rw_alias, ro |yes|const T* restrict |&amp;T ?? | no_alias r/w |yes|T* restrict |&amp;mut T ?? | I remember some discussion about making mut ‘only’ at one point;and I’m not sure whats actually in that middle column now. Coming from C++ the hope was that: Rust would fix the C++ issue where we often need to write ‘const T* restrict’ for immutable data that we want to keep in registers.. by the assumption that ‘const’ (or lack of mut) really *is* immutable (i.e not aliased by any &amp;mut), and of course Freezing. ’no_alias readonly’ really means no *mutable alias*. alias,readonly really means potentially alias, hence not really 'immutable' . &gt; The rough idea is to expect that unsafe blocks will contain pointer-aliasing bugs, &gt; thus to not optimize as aggressively the modules they contain. my preference is to assume the programmer knows whats going on in unsafe, and maintains invariants - so fully optimise everything; I think one wants to assume that &amp;T is non-aliased, really immutable (the default) and as such it will stay in registers. Actually going through memory for tricks like reading a floats bit pattern as an int would be what takes extra annotations (I suppose you can do these with intrinsics) C++ has annotations to cover all the cases, the problem is the assumption you most frequently want is the most verbose.. Rust inverting 'const' to 'lack of mut' was appealing ; and my assumption would be to treat 'restrict' the same. (i.e. it's going to stay in registers, unless you say otherwise with another anti-restrict annotation).. 
How are you doing these timings? I can only go by `cargo run --release`, which dropped from ~0.4s to ~0.3s from revision 4cd68cc to current master. Preallocation does not make a difference. `write!` uses the same formatting machinery that `format!` does, so especially with a preallocated buffer I don't see how it could be slower.
Thanks, I am gonna finish this course, courtesy my company provided Pluralsight account:-)
That's a fantastic side effect of this proposal. The module boundary has always been the boundary between safe and unsafe code, which means that it's not just unsafe blocks, but *the entire module that contains even a single unsafe block* that must be audited if you want to ensure that your code contains no memory errors. This means that, ideally, functions that use unsafe blocks should be shunted off into their own leaf submodules in order to reduce the auditing surface area and improve the theoretical guarantees that Rust is able to make about your code. However, this has so far been only a social incentive. This proposal makes it into a practical incentive, and purposefully sabotages the sort of people who seek to blithely use unsafe "because it makes my code faster". I'm all for this.
just do `cargo new your-library` and add it to your other project's Cargo.toml like so: your-library = { path = "path/to/your-library" }
Ahhh I was running in debug mode. Running with the `--release` flag gives me results similar to yours. Thanks for your help! That `write!` macro looks like it will be really useful in the future.
There's a typo/bug in the `format!` example, variable is declared as `world_var` but used as `world`: fn main() { let world_var= "world"; //Becomes "Hello World!" let new_string = format!("Hello {}!", world); } 
You want to break on `your_crate::main` instead.
&gt; Thus, we deliberately introduce a data race on the VGA buffer. For this reason, the function is marked as unsafe and should only be used if absolutely necessary. &gt; &gt; However, the situation is not that bad. The VGA buffer only stores characters (no pointers) and we never rely on the buffer’s values. So the function might cause mangled output, but should never be able to violate memory safety. [Any data race is undefined behavior](https://doc.rust-lang.org/reference.html#behavior-considered-undefined), so this is a concerning statement. An approach I've used is to break the lock so the interrupt code can take it, but then you must not return to the original lock holder.
Thansk for the help. How would the project tree be organized? Should the crates be created within the src directory? **edit** Additionally, what about if one of the crates depends on another crate?
&gt; Personally, I just want native widgets, which in my case means learning enough low-level stuff to interface with Windows. (Which uses truly crufty 90s technologies like null-terminated UCS2 strings, like, seriously...) I'm [working on that](https://github.com/pcwalton/libui-rs)!
Also we'd need to pin to a specific nightly (and risk either being left behind or even more painful rustups every now and then) and require people to get that specific nightly. Both are more trouble than it's worth.
Yep this was it. Its weird to me that I've never had this issue before but I guess I was always running from windows and maybe the c library start functions there are called __libc_start or something that doesn't have 'main' in it.
Yes, that's the general approach. Breaking a lock is only useful if you're doing something like printing a fatal error which will terminate normal operation.
Has gdb not worked well enough?
Crate are usually complete packages. You may be interested in modules. When it means that a crate can only have a lib and a bin, that means the output of the actual crate. I.E. If you make a game server, you can have a single executable to run, and/or a single library for people to link to. --- Crates are independent of each other. So unless you wanted the code to be used for a different game server, you shouldn’t make a new crate. If you did, you would make a `game-server-lib` crate for people developing game servers, and then you would link to in your `awesome-game-server` in your game server file. If its just a function that you'll use, like `approximate_square_root(i: f32)`, you should just put it in a `math.rs` file. (it becomes a module) In your case, if the math and number crunching are not a complete package, and you're just organizing them, that’s what modules are for. Think of Crates as finished products which others can use, while modules are components. The standard library is one crate, made of many different modules. You can also have sub-modules, and have as many as you like. You can also include as many crates in a package as you like, but your crate only usually results in one executable (most cases) or is just a lib to link to. the standard Library is just a single lib (its linked automatically, no need to import), even if you just use `arc`, you std::sync::Arc::new(); or use ::std::sync:Arc; fn main() { ... Arc::new(x); } Cargo is an interface so you don't have to manually link anything, just `extern crate cratename;` in the main.rs and `mod module;` in files you use it in to use a function/struct. --- Modules can be declared by either a new file, a new folder with a mod.rs file in it, or a `mod modname { }` in any other file. Modules can be inside other modules. --- If you were making a program, it'd look something like this: math-program/ ├── Cargo.lock ├── Cargo.toml └── src/ ├── main.rs ├── math/ │ └── mod.rs └── numbers.rs src/main.rs: mod math; mod numbers; fn main() { let x = math::sqrt(9); println!("square root of nine to the power of two {}", numbers::pow(x,2) ); } src/math/mod.rs: pub fn sqrt(_: i32) -&gt; i32 { 3 } src/numbers.rs : pub fn pow(_: i32,_: i32) -&gt; i32 { 9 } just compile with `cargo build` or `cargo run`. `cargo build --release` optimizes it.
woops, thanks
A lot of people like to talk about how "undefined behavior" means *literally anything* can happen, but the reality here is that it just means anything could wind up in the output buffer. That's basically fine. And if you're really concerned that Rust might explode, you could always write the acknowledged data race in assembly instead.
Wow, I've always wanted to see a series on OS development from scratch, I'm definitely going to go through this entire blog from the beginning! Thanks!
I still don't see the problem here. It is fully possible to use unstable libraries on stable Rust, just as long as they don't use any nightly features. Getting off of nightly is the point of this thread, no?
That's true of x86, but you can't make the same assumption with the compiler, largely because it's portable to systems without the same memory ordering guarantees. Suffice to say, the [LLVM memory ordering rules](http://llvm.org/docs/Atomics.html) (which are inherited by rustc) are a complex beast.
Another approach is to just use [volatile writes](https://doc.rust-lang.org/std/ptr/fn.write_volatile.html).
It's still a data race, but LLVM gives it limited semantics (e.g. it doesn't crash). This is similar to how Java gives semantics for all data races. Rust could adopt the LLVM memory model but it would probably weaken race-freedom guarantees and make the rules for `unsafe` even more confusing than they actually are.
Here's the first (modified) example I found in the codebase: // sequence is Vec&lt;u8&gt; // sub_seq_len is usize let subs = (0..(sequence.len() + 1 - sub_seq_len)) .map(|i| (i, &amp;sequence[i..i + sub_seq_len])); for (start_pos, subsequence) in subs { // do things without having to track iteration variable or pointers // ... } The goal here is to enumerate tuples of all length `sub_seq_len` substrings and their starting positions in the original string. This gets saved to a binding which can be reused later in a for loop to cleanly iterate over all `(starting_position, substring_slice)` pairs. It's certainly denser than C can be, but I think it's nice declarative code which is written separately from the loop control constructs. This way, instead of having a for loop where I manually track the current iteration position and the pointer to the middle of `sequence`, there's just a nice entity which is "all of the position/subsequence pairs" that I can reference later. While this is an iterator example, I'm not sure this is do-able in C++, but again, I'm not super familiar with modern C++ constructs. Also, the `&amp;[u8]` slices I generate here are zero-copy, guaranteed to be outlived by `sequence` (for memory safety), and also carry their length with them so I can call `subsequence.len()` in the loop without needing to check which length variable was used to create the reference. EDIT: The one thing that's too bad here is that the range `0..(sequence.len() + 1 - sub_seq_len)` will produce starting positions that will produce all valid substrings of length `sub_seq_len`, but the index operator does bounds checking, and there's no way to pass a range to `Vec::get_unchecked()` and get a slice back, so we pay a tiny runtime cost for this. That said, bounds checking this is very inexpensive and is O(num substrings), whereas the other calculations in that function have much worse average-/worst-case behavior.
&gt; Crate are usually complete packages. You may be interested in modules. I was looking for the equivalent of a static library, a component which can be developed independently from a project, which could be compiled and built independently, and which will be a dependency of other components. As the rust book refers to crates as libraries, I assume that that was what I was looking for. How are rust's modules compares with C's static libraries?
The simplest way to explain generics is that they let you write a data type where you leave a blank to be filled later, such as "List of &lt;X&gt;". Then, you can say "List of &lt;integers&gt;" and "List of &lt;strings&gt;" and "List of &lt;images&gt;" without repeating yourself and still get benefits as if you'd written a custom list type for each one. (In "dynamically typed" languages like Python, it "just works" because everything has to go the slow path of asking "Ok, what is this?" for every variable lookup. In "statically typed" languages like Rust, it's made explicit at compile time, so it can skip that step while running to get more speed.)
Unfortunately, libui is unacceptable for my GUI projects because Linux is my primary target platform and I have a strong difference of opinion with the direction the GTK+ 3.x ecosystem has been going. (I may re-evaluate that decision if their "GTK+ 3.x is a development series" claim proves true and GTK+ 4.x returns to performance parity with Qt in addition to stabilizing the theming APIs.) ...and I'm trying to not start any new projects using GTK+ 2.x. Given my insistence on consistent appearance and how much KDE applications have segfaulted on me over the years, that basically leaves Python with PyQt as my last remaining option.
Yeah, I think volatile writes are really needed here, even without the `print_error` function. Thanks for the pointer! I've opened [an issue](https://github.com/phil-opp/blog_os/issues/168) for this.
Rust has no form of protection around IO (it can happen unrestricted), but that's actually just a standard library thing. I think Rust's type system could actually work really well for a object-capability system, with an alternate stdlib.
Keep me informed on how this proceeds. :)
&gt; purposefully sabotages the sort of people who seek to blithely use unsafe "because it makes my code faster" That's downright hostile. People who want code to be fast aren't satanists, and Rust is frequently advertised to those very people. Do you really think [this line](https://github.com/rust-lang-nursery/rustc-serialize/blob/master/src/base64.rs#L190) is so horrific that unexpected side effects to [totally unrelated code](https://github.com/rust-lang-nursery/rustc-serialize/blob/master/src/base64.rs#L269) are just punishment for using it? You're right that unsafe is infectious, but *everything* is infectious. If your code is wrong, it's wrong. The only reason memory safety is privileged is because it's on average more difficult to debug and more prone to escalating. It's not like the code I linked to would be less dependent on the previous line's correctness if it used [`String::from_utf8`](https://doc.rust-lang.org/std/string/struct.String.html#method.from_utf8) - it would just break more reliably. Significant nonlocal performance impacts sound absolutely terrifying to me, and I *really* hope Rust never goes that way.
That's only true if you implement the unsafe code *correctly*. This proposal make this more performant than the more correct version, for completely artificial reasons: mod fast { fn deref&lt;T&gt;(ptr: *const T) -&gt; &amp;T { unsafe { &amp;*prt } } } fn my_cool_fn(s: &amp;MyFfiStruct) { let k = fast::deref(s.field); do_stuff(k); } Put simply, Rust is intended for fallible programmers, not stupid ones. That's why it has unsafe features *at all*. If you really want a language that imposes completely arbitrary constraints on your code in an attempt at making bad programmers write good code, use [self-censored in deference to rule four].
&gt; Strong rules have *negative* value when it's the programmer's burden to obey them, without receiving any feedback from the compiler, because it means *more* opportunity to make mistakes. But it also means the programmer can rely on more. For instance, I don't like the idea that a function that takes two `&amp;mut`s should have to *ever* be concerned about whether they alias. Only the caller who touches `unsafe` should need to care.
I have posted the results to the [survey two weeks ago](https://www.reddit.com/r/rust/comments/4ir2ss/redox_survey/). The raw data [can be viewed here](https://docs.google.com/spreadsheets/d/1V8DdiAfxgpM7HogtmNsnluwPdfuYLotSuC6-aLBHWPc/edit#gid=743853416). I am working on a news post to detail our progress, and how we intend to respond to the survey results. Thanks to everyone who participated!
Redox is a Unix-like Operating System written in Rust, aiming to bring the innovations of Rust to a modern microkernel and full set of applications. The [website is here](http://www.redox-os.org), and the [git repository can be found here](https://github.com/redox-os/redox).
Thanks. Fixed.
Obligatory "you're in the wrong subreddit" response. I think you want to post here: https://www.reddit.com/r/playrust/ Cheers!
 You claim here: &gt; Unlike with promises, we don’t have to be constantly allocating closures on the heap. How else would you implement `yield`?
`String::new` just calls `Vec::new`, and `Vec::new` looks like this: pub fn new() -&gt; Vec&lt;T&gt; { Vec { buf: RawVec::new(), len: 0, } } Finally, RawVec contains a pointer and capacity, and starts out with the pointer value `alloc::heap::EMPTY` (which happens to be `1`).
I sorta figured it's something like this. It's not really a big deal for me since I mostly just poke around.
Tried today, May 28 ~2030 PST, still old numbers. Not a big deal though.
The state of a generator could be represented as a code pointer plus the state of the local variables of the generator function. I don't see any reason why the state couldn't live on the stack. I found erickt's posts about Stateful to be helpful in getting a rough understanding about how this all might work: http://erickt.github.io/blog/2016/01/27/stateful-in-progress-generators/ http://erickt.github.io/blog/2016/01/28/stateful/
I actually do construct a control flow graph in stateful. I need it in order build the state machine, and manage the flow of variables across yield points. It's no where near as fancy as MIR of course but it gets the job done. I even implemented some simple [optimizations](https://github.com/erickt/stateful/tree/master/src/mar/transform), like squashing goto chains and optimizing away empty, unreachable states. Boxed iterators suck, but it seems like we're getting super close to getting [impl trait](https://github.com/rust-lang/rfcs/pull/1522). Fingers crossed! MIR is definitely the right place for all this though. If only it too could be stabilized :)
&gt; could be represented as a code pointer plus the state of the local variables of the generator This is true, however it would require an unsafe `union` and generally makes me uncomfortable. Also has to problem of not being immediately optimizable, compared to an `enum`-based state machine, which has a bounded integer discriminant. OTOH, once it comes down to codegen, a function pointer could be faster. Would be interesting to compare the two approaches once we have a working implementation. It could very well be the case that we want to perform high-level optimizations on the generator MIR and the lower it to function pointers instead of an `enum`.
You really only need compiler help if you want to have unboxed generators, which needs a concrete type. We'll hopefully get that with [impl trait](https://github.com/rust-lang/rfcs/pull/1522).
It was really easy to add optimizations. I just copy pasted the MIR optimizations and they worked :) I'm trying quite hard to keep MAR as close as MIR for these kinds of benefits. 
There was a survey? Damn, I missed it :/
It's downright hostile to people who believe that unsafe Rust must be inherently faster than safe Rust, when ideally I'd expect the opposite to be true. Restrictions are what allow optimizers to optimize, and lifting restrictions as unsafe Rust does then naturally cuts off potential avenues for optimization. That safe Rust has thus far done relatively little to exploit its natural advantage wrt pointer aliasing guarantees is an artifact of history. It's even misleading to describe this as unsafe code being penalized, when in practice unsafe code will be as fast as it's ever been and pure safe code will get *faster*, which is exactly the sort of situation that Rust needs to encourage. The alternative, which would be to apply aliasing optimizations to unsafe pointers that we cannot guarantee are distinct, would be even more deplorable than what C does.
This is an actual subreddit lol
I'm not working on an OS (and probably will never be), but this article is still a really interesting read, thanks! 
The use of `to_bytes_with_nul()` seems fishy to me, that gives you a slice including the null, which you then print to stderr.
&gt; I don't see any reason why the state couldn't live on the stack. How would you store references to local data? This would probably require introduction of unmoveable values or store references as offsets (is this even possible?).
More specifically, the volatile is only required for code running outside an interrupt handler. This is because the memory you are writing to may get modified by an interrupt at any point, and therefore the compiler should not make any assumptions about the content of that memory even if it has just written to it. However this is different when running in an interrupt handler or if you are running in a context when you cannot be interrupted. In that case you can safely use normal stores since the memory won't suddenly change while you aren't looking.
Check you % for CPU, Intel has &gt;90% and amd &gt;20%
Did you compile clippy with the same rustc you run it with?
If I recall correctly you could select multiple items, and many people have multiple computers. My laptop has an Intel processor, and my desktop has one from AMD.
Multiple choices. You could've chosen both Intel and AMD if you had CPUs from both. 
&gt; &gt; &gt; Most rust developers really dont use libraries in that sense of a prebuilt static library . It sounds like you're trying to develop a build strategy, but you don't need to, cargo handles everything. You may be right. As I'm coming out of C and C++, I supposed that I'm guiding myself based on the mental model I've built based on my experience with C and C++. Perhaps with time things will become a bit clearer.
Yes, apparently it's broken on Windows with rustup atm. It seems to be a known issue.
I believe the current progress is tracked at [rust-lang/rfcs#323](https://github.com/rust-lang/rfcs/issues/323).
Got it!
Yeah, it might not be easily representable as safe Rust code. I was thinking instead about what the low-level code emitted by the compiler might look like. Thinking in terms of enum-based state machines, as [Stateful](https://github.com/erickt/stateful) does, is probably a cleaner way to go.
Indeed, there are a lot of different opinions and ideas but in general people agree that it would be nice to have. There are also a couple of interesting concerns and limitations raised. Overall it is more considered a convenience feature and so it doesn't have a high priority. I think that, for it to really move forward, someone has to draft a solid RFC with a good design proposal that addresses all (or most) of the raised issues. But that is a huge amount of work and requires a decent amount of knowledge about the language. 
Personally, I started re-writing my RFC from [rust-lang/rfcs#257](https://github.com/rust-lang/rfcs/pull/257) by integrating all the question's issues, for example by separating the "default args" and the "keyword args" features in two RFCs (as they have some different implementations/design issues) As I now use [hackmd.io](https://hackmd.io) to write these, I could possibly share the links so people could contribute, or maybe even re-starting the mozpad I created before in order to gather a new "common view" of all this.
Just start and try to program something. I'm coming to Rust from mainly Java and Scala amateur coding, borrow checker is what you'll fight in the meanwhile. Remember it is (almost) always right and it can help you reason about data ownership. And in the end, it's easier for me than c (never wanted to really learn even a bit of c++) because when your program starts becoming not so small and you segfault you'll be happy to have a compiler telling you "are you serious about this? This is wrong for this and this" :) Read the whole book and maybe start digging a little bit in the Rustonomicon book, which gives the gory details of safe and unsafe code!
I'm curious how you would distinguish Coroutines and Fibers. Personally, I group them together, along with Green Threads, as "user-space threads that use `getcontext()` magic to manages stacks at runtime".
Composition over inheritence is a pretty big change too.
You're using `into_iter()`, which (for `Vec`) means it will move the values out of the source vector (the `layers` parameter). Then, you transform each value by `s.as_ptr()`, after which the value is discarded and `drop`ped - destroying the string you now hold a pointer to. `CStrings` are not garbage collected, so holding a pointer does not keep it alive, and pointer types do not have lifetimes, so the borrow checker can not warn you about the dangling pointers you are creating. What you want is probably `iter()` from [Iterator](https://doc.rust-lang.org/std/iter/trait.Iterator.html), which keeps the original vector intact.
And in cases where you need to act on the shared component in a uniform way between structs, there's a little more boilerplate. Either: fn tune_up(e: &amp;mut Engine) { // what do I look like, a mechanic? } ... tune_up(&amp;mut my_motorcycle.engine); // or tune_up(my_motorcycle.get_mut_engine()); or, with traits: trait HasEngine { fn get_mut_engine(&amp;mut self) -&gt; &amp;mut Engine; } impl HasEngine for Car { fn get_mut_engine(&amp;mut self) -&gt; &amp;mut Engine { &amp;mut self.engine } } // repeat // static dispatch fn tune_up&lt;T: HasEngine&gt;(vehicle: &amp;mut T) { let engine = vehicle.get_mut_engine(); // mechanic stuff } // or dynamic dispatch fn tune_up(vehicle: &amp;mut HasEngine) { let engine = vehicle.get_mut_engine(); // stuff }
:-( yup you did confuse me ... This is actually the thing that caused me to ask this questions. Inheritance is a big part of oops. Composition is something new , and unlearning something is always hard :-( still for the awesomeness that follows :-)
That first thing is exactly possible in Cargo as well. It's only the ones after the first that aren't. `cmake` does not have a central repository of dependencies that you download stuff from, that I'm aware of.
That's a bit of an understatement. Complexity of the code can explode when you convert from a generator to a class, because you have to desugar/flatten lots of other things and explicitly add a lot of state to make that work. It's not just equivalent to a find/replace, which is usually what's meant by syntactic sugar. Have a `yield` inside of a `for` loop? Now you have to explicitly keep track of that iterable and iterator. Have a `yield` inside of a few nested control flow statements (`while`, `if`, `try`)? Good luck with that.
In the generator paradigm (e.g. Twisted's inlineCallbacks is what I'm most familiar with), you just use `yield` in place of `await` and have some external machinery (the `inlineCallbacks` decorator) that does the scheduling.
The main problem with this is that you can't use regular data-types like `u32` or `String` almost anywhere in the compiler anymore, which I had ruled out for ergonomic reasons. In theory it would be possible to use a non-cloneable wrapper around these types but I suspect that having none of the compiler's core datastructures cloneable would make writing regular code rather cumbersome. But it would be interesting to pull this code into a standalone library and start experimenting to see the actual impact on usability.
Who is DJB?
Rust has fat pointers instead of using thin pointers everywhere. That means it passes around strings (`&amp;str`s, to be precise) as ptr + length instead of as ptr with delimiter, and passes trait objects as ptr + vptr instead of as ptr to struct containing vptr, like C++ does with virtual objects.
Please correct me if I'am wrong, but from my current understanding Rust's borrows are not totally restrict (i.e. not in C++ sense) - you can create have still multiple references to the same data - just with only one active - thus, when its goes from the scope one time reg cache reload will still be required...
We take inspiration from where ever we can find it. We are not locked to a single source of inspiration, but we do especially take inspiration from Plan 9, from which many core concepts in Redox comes from.
&gt; you can create have still multiple references to the same data In C++ restrict means 'no aliases *that you have to worry about*': so the compiler can cache in registers. And that's fine for immutable data, even with multiple references. It's needed because C++ 'const' on it's own only means 'write-only through this reference', not *actual* immutability. const-restrict tells the compiler you think something is *truly* immutable. The correspondence between C++ and rust seems hard to pin down because of these differences &gt; " thus, when its goes from the scope one time reg cache reload will still be required..." i think in rust it should be better at figuring out when it needs to reload, because it knows when a mutable borrow was taken - and it can assume true immutability otherwise ? (due to freezing/lifetimes/borrow-checker)
Before `retain` can automatically opt-in to aliasing optimizations we need a way to allow the compiler to determine that `retain` does not leverage privacy, either directly or indirectly, to violate aliasing guarantees. If you can think of such a way to automatically restrict unsafe boundaries, then I welcome proposals. In the interim I would expect all the bits of the Vec module that actually need to leverage unsafe to be shunted off into a submodule and be imported/reexported as necessary. In the medium term I would expect some mechanism to opt back into aliasing guarantees in unsafe code, as Niko proposes at the end of his post.
There's something about the "what color is your function" post that rubs me the wrong way, especially in the context of a systems language that can't afford one-size-fits-all abstractions, and can't justify intercepting and special-casing all I/O operations at the language/runtime level as Go does. It reminds me of discussions about solving the expression problem, where the accepted solution is "hey, just make every call virtual!", which is also something that systems languages can't afford.
Nope, this was [my mistake](https://github.com/rust-lang/rfcs/issues/1081#issuecomment-221403518) - the correct way to do this is not to care at all! I know it sounds strange, but @eternaleye's proposal to have a simple "suspend" result from the generator seems the most optimal, as long as it's not too expensive to keep retrying all choices all the time. **EDIT**: Although, your model is slightly better in that it doesn't necessarily need to understand "generator paths", as there is no data feeding back into the generators. It's not that different from /u/pcwalton's description of mio+generators.
I always tell people the same things: * Try to join a project that interests you – the Rust community is full of great mentors. Don't fear that you'll be over your head – as long as you pick easy issues first, you'll be OK. * Use a good development environment. Some people swear by vim (or neovim) and I heard one can configure it to do all the things other editors/IDEs do, but I have not yet done that (I'm looking into it, though). I hear good things about Atom+Tokamak, VSCode+RustyCode and Eclipse+RustDT. * Go to https://www.rustup.rs and install rustup. This will allow you to keep multiple toolchains which will come very handy later. * Get [clippy](https://github.com/Manishearth/rust-clippy) – this requires a nightly rust (just type `rustup update nightly` on the command line. See? I said it would come handy) * Whatever code you write, write tests for it, too. Things like [quickcheck](https://crates.io/crates/quickcheck) can make those very powerful * Ask questions a lot. Whether on IRC, StackOverflow, on our weekly Q&amp;A thread or in person, you'll usually get a useful answer (perhaps also search for clues on the internet first)
Even in Java, and Python, it's useful to think about whether a given relationship between classes should be modeled by composition or inheritance. Inheritance and composition are both used for code reuse. However, inheritance is very specifically an is-a relationship, (`DieselEngine` "is-a" `Engine`), whereas composition is a has-a relationship (`Car`, `Motorcycle`, `Lift`, etc. "has-a" `Engine`). A lot of code abuses inheritance because it's less boilerplate-y than composition (for example, with regards to forwarding behavior). In Rust, composition works just like in any other language. "Is-a" relationships, on the other hand, are modeled by traits rather than by inheritance. The chief difference between it and inheritance in other languages is that implementing a trait is just as boilerplate-y as using composition. While boilerplate isn't great, it does put "is-a" and "has-a" relationships on the same level, meaning people are more likely to choose the one that's really appropriate.
https://en.wikipedia.org/wiki/Daniel_J._Bernstein
It seems unusual to me (also coming from a Java background) that the tune_up method is not part of the HasEngine trait. Is there a specific reason why it's done like the way you have shown in rust?
Hello Rust community! This is my first open source library, and I wanted to share it with you. I am a noob in a lot of senses, so any feedback and advice are welcome. About bytevec, I am aware there are many libraries that already have the same functionality as this one, but I wanted to roll my own one. This started as a design I stumbled on while playing around with the Linux API, I wanted to get some bytes out of my dummy data, so I wrote some simple routines to get it, when I finished, I liked it, and proceeded to make them a "full library". You can think of this as an experimental project of someone who didn't have experience before, so as I said previously, any advice about my coding style or anything is pretty much welcome.
Neither, if I understand you right. The (half-baked) way I picture it: 1. A `Generator` would be a struct that has (by value or reference, I don't know yet) an activation frame for the coroutine. Possibly allocated in a fresh stack, like a lightweight thread. 2. The `yield` and `next` operations are lightweight context switches—they are stack context switches within a single OS thread. 3. The closure also gets a special return address that transfers control back to the appropriate location in the `Generator`'s owner's coroutine. The idea would be to implement this in `unsafe` code as much as possible without touching the compiler. And to heed /u/glaebhoerl's point about the relationships between all these concurrency concepts, and not bake any one mechanism into the language unless there's a good argument that it's not too committal.
I've actually been implementing byte serialization by hand using byteorder, which had started to become annoyingly verbose. I'll take a look at using this for my projects, which would be awesome if it worked. A question though, how well does this handle extremely large sets of data? (Larger than the 4gb range) I saw that you used u32, would it be possible to provide a feature that allows size to be stored as a u64 instead?
Rust is *absolutely* trying to kill C/C++ -- doing so is a moral imperative. However there are certainly plenty of reasons to keep using C and C++, even today. And yeah whatever write stuff in Ruby/JS/D/Python/VBScript. I do take issue with some languages when they do something poorly, and really drag down the practice. Static typing is a big example of that -- C and C++ are notable for frequently make a mockery of static typing, but even Java does static typing in such a way as to make people think it's incredibly cumbersome with few benefits.
I think it's not such a big change if you're using a modern java style. I work in a java shop and our newer projects, especially in java 8, could be translated into rust with very few changes. 
What you're describing are "stackful coroutines", which, while a valid approach for some things, have much larger overhead than stackless coroutines because of the need to have a separate stack. They're also not semantically equivalent, because stackful coroutines let you implicitly transfer control from inside a function call.
Sure, nobody's suggesting we should stick with callbacks or futures over async/await. But we should at least have a look around to see if there isn't an abstraction that's even better.
I'm not sure about that. Even in object oriented programming languages it's best practice to prefer composition over inheritance.
There is, however, a difference in tone between "killing" and "supplanting." You could say one is vinegar, the other honey (as in, "you catch more flies with ___"). And I strongly suspect that a positive approach to selling Rust based on advocating for its strengths, rather than decrying the deficiencies of C/C++, will be more successful in the long run.
Unfortunately, many try to dismiss the merits of Rust by erroneously declaring the merits of C/C++. As in, "what's the point of Rust, when Modern C++ is already safe?" or "correct code in C is actually easy since the language is very simple". It therefore becomes necessary to step in and say "no, C(++) is dangerous and error-prone".
Quickcheck is great. OP you can use it for property based testing. For example if I reverse a string twice it should be the same string. Quickcheck then makes a ton of random cases to find errors in your reasoning. It's a great tool that Haskell has had for awhile that Rust has now as well.
Finally got myself to resume my work on wayland. Things are slowly coming into shape, recently fixed a big soundness isue in my crate, and leveled-up in the "understanding of wayland internals" skill. :) I feel it'd be nice to write an actual program using `wayland-client` and friends, and blog about it step by step, to make a clear example about how it works. Just need to find an idea, I guess.
Obviously, perceptions of the community will vary from person to person, and I do absolutely think the Rust community is 90% great in the ways I laid out in the post. I've definitely seen examples of people, in discussions of Rust, and from the apparent perspective of the Rust community (whether that identification is warranted or not), do the things I describe. The problem may not be endemic, but it's a problem nonetheless (inside and outside of the Rust community) that is worth talking about. Edit: Also, tedu is an OpenBSD team member, and an avid C programmer. [His blog](http://www.tedunangst.com/flak/) is quite entertaining.
You're running into two issues at once: - `graph` is indeed mutably borrowed outside of the block you created there. That's because you've bound it to the `neighbors` variable, which outlives that block. What a block can do for you is limit the lifetime of variables that are *declared* inside the block, but because you declared `neighbors` before the block, the block has no effect here. - The `note: borrow occurs due to use of self in closure` is important. Because you're using the variable `self` inside that closure, the closure is borrowing _all_ of `self`, and that includes `self.graph`. This conflicts with `neighbors`. Luckily the second problem is easy to fix. You can create a local variable like `let myref = &amp;self.i2v` right before the call to `map`, and then use that variable instead of `self` inside the closure.
Holy crap I had no idea you could do that magic thing. Is that the same as doing this more boring thing? let graph = &amp;mut self.graph; let i2v = &amp;self.i2v;
&gt; A lot of code abuses inheritance because it's less boilerplate-y than composition (for example, with regards to forwarding behavior). If you're interested in reducing boilerplate and willing to sort of shit all over the intended semantics of the language, you can also have your `Car` struct `impl Deref&lt;Target=Engine&gt;`, which allows you to pass a `&amp;Car` where an `&amp;Engine` is expected and call any `Engine` methods on a `Car` struct. But really, the `Deref` trait is primarily for smart pointer =&gt; value and container =&gt; view conversions, so you'd be violating [this](https://en.wikipedia.org/wiki/Principle_of_least_astonishment) little rule, so I'd recommend against it. Maybe if you're crazy enough to use Rust in a speed-programming competition.
Yes, except that by doing it in a single statement you can convince rustc that doing so is actually fine. If you do it in two lines it starts whining about `self` being mutably borrowed. Here's a oneliner version of your method: pub fn neighbors(&amp;self, v: &amp;'a T) -&gt; HashSet&lt;&amp;'a T&gt; { self.v2i.get(v) .map(|i|self.graph.get(i).iter().cloned().flat_map(HashSet::iter) .map(|i|self.i2v.get(i)) .map(|v|*v.expect("Corrupted graph, missing neighbor")) .collect()) .expect("Attempt made to get neighbors of a missing vertex") }
I'm not even sure how to formulate what the *right* attitude consists of precisely, so I'll have to settle for talking about what *isn't* it instead: (a) I definitely agree with the blog post that the attitude that using (or being knowledgeable about) Rust makes one a Superior Human Being is not helpful, and is more likely to be counterproductive by making people feel bad ("feel bad" and "Rust" is not the association we want to cultivate); (b) I *also* completely agree with Gankro that in our noble striving to avoid hurting anyone's feelings, we should also avoid devolving into meaningless relativism where every language is equally good and whether to use C or Rust is really just a matter of personal taste, like what kind of music you like, and it doesn't really matter which one you choose in the end. Programming languages aren't people; we shouldn't have to be afraid of hurting *their* feelings. (But people are often emotionally attached to a programming language, even though they probably shouldn't be -- see (a) -- so that doesn't mean it's a good idea to insult programming languages as if they were people!) Basically feelings are Important and facts are also Important and we should try to keep both of these things in front of us at the same time.
Yes used make long ago when I first taught myself C. Makes perfect sense. 
Thanks make sense. Guess I might see what I can do by reading the book 
That [second article](http://blog.paralleluniverse.co/2015/08/07/scoped-continuations/) you link has long bugged me: "Gee, monads are HAAAARD! So let's use first-class scoped continuations instead!" I'm reminded of the old joke: "Programming without continuations is like commuting without time travel." The [associated talk](https://www.youtube.com/watch?v=449j7oKQVkc) is better than the article, though, and the basic concept I think can be distilled to this: * A thread of execution is, fundamentally, a stack + a scheduler (like the talk says). * We really would like to have operations that synchronously transfer control from one (lightweight) thread of execution to another, passing arguments and returning values along the way. I.e., a form of lightweight, cooperative multithreading. * If you're in an imperative context already where you already have the power to launch heavyweight OS threads, it's unergonomic and a bit silly to have to interface with a first-class monadic API to launch fibers (see e.g., the `CompletableFuture` examples in the talk; also the "red" vs. "blue" functions from your first link). Instead, you want to enrich what some of us call the "ambient monad" (the thing at the heart of the language that *is* a monad, but not a *first-class* monad). This is very sensible. But otherwise beware every single time he says the word "monad," because it comes with a litany of inaccuracies (e.g., 34:45: no, monads don't force you to implement `map` and `filter` in terms of `flatmap`) or irrelevant accuracies (e.g., "monads don't solve concurrency"—well, of course not, an *interface* doesn't solve implementation problems!). (If I'm not mistaken, the article and talk are by /u/pron98 (*ping!*), or at least a coworker or acquaintance of his.)
"The core abstraction of a pure functional language is the function while the core abstraction of an imperative language is the continuation" is a point that really stuck with me, even before I really grokked much of the rest of it. I dunno if scoped continuations are easier than monads, but *if* you want that level of power, they sure seem to fit in better inside of a language that's *already* monadic. :) (Edit: I think we're violently agreeing, perhaps with different points of emphasis.)
Meta: is 'prosletize' a typo? I'm not a native speaker, I've never seen this word and I'm only able to find 'proselytize' on `dict.cc`... If it's not a typo: what is its meaning? :)
Great timing! I tried to poke at `wayland-client` this weekend to learn about it and maybe to write a command-line tool to fetch and set the clipboard contents (like xclip on X11, or pbcopy/pbpaste on Mac OS X). Some feedback about the library: - It surprised me that the `wayland_env!()` macro doesn't allow a trailing comma in the list of fields. - It's confusing that `WlDataOffer` and `WlDataSource` are both described as "offer to transfer data". I guess Source is "an offer to send data to other clients" while Offer is "an offer from another client to send data", but I had to read between the lines to figure that out. - Sometimes your sample code unpacks a global object with something like: let compositor = env.compositor.as_ref().map(|o| &amp;o.0).unwrap(); ...which I had to stare at a bit. I was expecting something more like: let (compositor, _) = env.compositor.unwrap(); Generally though, where I'm stuck is that I don't understand the Wayland programming model. That's not really a problem with `wayland-client`, I think that's just a problem with the Wayland documentation I've been able to find in general. For example, Windows programming is based around the message loop: you create a (possibly hidden) window so you can receive messages, then you loop infinitely waiting for messages and turning them into method calls on your business logic, which might make API calls that cause later messages to arrive. GTK+ is based around event callbacks: each object has a number of events you can add callbacks to, so you set them up and call `gtk_main_loop()` and off it runs. Wayland apparently gives you an event iterator out of the box, no window required, but I'm not sure what to do with it. I guess all the method calls on all the objects turn into messages to the server, then I have to crank the event loop for a bit to make sure they're all sent and all the replies received? What about asynchronous notifications (like, say, "something has been copied to the clipboard, so enable your Paste menu")? Do they just show up in the event stream? What if one shows up while you're handling keyboard input or something, how do you stop it getting dropped on the floor? I guess I need to re-read the Wayland protocol documentation in more detail... or if you know of better, more appropriate docs, I'd love to hear about them.
I think that storing in Big-Endian does have the advantage of making it so that data can be transferred across the network with minimal conversion, as Big-Endian is what is used for networking.
It looks like it does, I suppose I should have checked that before I started asking complicated questions!
One perhaps basic question that nonetheless isn't obvious to me: It seems likely that the "99% solution" of cheap user-mode threads isn't viable for Rust, because it would entail burdening every program with an I/O and threading runtime, which is in conflict with Rust's goal of being able to compete with C/C++ for "bare metal" applications. But is the same thing true for scoped continuations? Could scoped continuations be implemented without having to centrally coordinate on an alternative to OS threads, without a garbage collector, without having to allocate things on the heap and use lots of virtual calls, and so on? In other words, is it a strictly more powerful alternative to async/await in some sense, or are there real drawbacks as well (leaving UX to the side)?
Slides are actually [here](https://github.com/GorNishanov/await/blob/master/2015_CppCon/C%2B%2B%20Coroutines%20-%20Gor%20Nishanov%20-%20CppCon%202015.pdf).
Oops! Yup, I misspelled it. I guess that's what happens when I write blog posts in the middle of the night. Thanks for pointing it out!
"Convenient for humans" isn't a dismissal.
You wanted /r/playrust
How does this compare with [bincode](https://crates.io/crates/bincode)?
[removed]
20 lines? You're letting this fine type system go to waste. If you don't encode a proof of the correctness of your program in its types, you are woefully underusing the type system.
I kinda agree. I see a lot of random comments that just remark "they should rewrite in rust", but nobody is fighting for it, and its often not from the rust community. What I do see more often is "Rust could have prevented that"-smugness, which really is somewhat warranted (when actually correct, which it sometimes isn't) :)
Nice to see a properly written debugger! My own is not of the "keep the CPU code free of debugging code" variety, so for one it's only compiled in debug mode. Will be interesting to see how you do stuff like breakpoints on memory access. I might make a PR to use rustyline and save a history file, that will be very helpful and I assume not above the PR threshold?
The suckless approach would include not using C++ at all. They consider it bloated and part of the problem. Not sure what they think of Rust.
Couldn't there be a helper fn that returns an arbitrary Addr that matches the input?
Wrote a little `map_in_place` function for `Vec`s, since it's my first non-completely-trivial unsafe code, I thought I'd ask for some review: ~~https://is.gd/3fKekq~~ ~~https://is.gd/PitOUR~~ ~~https://is.gd/jCFd9R~~ I decided to simply move it to github: https://github.com/BurntPizza/map-in-place Now with tests!
Was it a conscious choice to offer a serialization interface around `Vec&lt;u8&gt;` instead of `io::Read`/`io::Write`? If so: Why? `Read`/`Write` seems more general as a source/sink of bytes.
You can match trailing comma(s) by adding $(,)* after matching list. [source](https://danielkeep.github.io/tlborm/book/pat-trailing-separators.html)
Maybe one comment on style (since you asked for that, not related links :)): You have a macro `collection_encode_impl` which seems like it is just abstracting over the ability to call `iter`. If that is right (maybe I'm wrong) you should be able to provide an implementation for `I: Iterator ... where I::Item: ByteEncodable`. If possible, I think this would be more "rustic" than a macro that happens to find the right calls for each type (and more general, if I write a new collection). Also, some perf observations from having implemented Abomonation: 1. It looks like you'll do an allocation for each element of each vec, just to make the signature work out correctly. It is probably expensive, I'd think. I skipped that, and you probably can too if you tweak the trait methods (note: some private methods you wouldn't want people using, like "write your data to this `&amp;mut [u8]`"). 2. There is a difference between your `decode` signature and Abomonation's `decode` signature that allows it to decode `&amp;` types. Maybe it is interesting to you, but if you'd like to head that direction we should just team up and figure out whether Abomonation is missing something you need.
thank you, that helped a bit!
my problem is that it recompiles the whole stdlib, even if I change just some rustc component
I'll try to summon /u/steveklabnik1 :) (I don't know who is responsible for the buildsystem, sorry!) tl;dr rustc's build system recompiles libstd even though I change just some rustc-internal bits, say librustc_mir. Is it necessary to rebuild std everytime the compiler gets built, or is this a shortcoming of the build system in it's current form? If the latter is the case, would it make sense to add a flag like RUSTC_NO_STD_REBUILD to the build script that suppresses a rebuild of std? I can imagine you'd have to rebuild std if you, eg., changed the way structs are laid out in memory, but sometimes the rebuild seems unnecessary. Eg. if you add an explanation for some error. Thanks in advance!
Oh, neat trick, thanks!
Dude NICE, I didn't know about rustyline! This would be very helpful indeed. PR away :)
Thanks! :)
Hm, it's pretty sad to see such a big project going the all-nightly road. I see why they want it (mostly because of serde and diesel) and I think both of these libraries are a major cause of rippling effects to keep projects on nightly and further dig in on that route.
Why not use `usize`? (maybe the exact size type needs to be known for serialization purposes?)
can anybody give a practical example of Option &amp; Result enums?
Well, a GC has nothing to do with continuations, and neither does a specific threading runtime. Interop with libraries that just compute and don't block isn't a problem, either. But the benefit of continuations is, of course, blocking (suspending) them, and every function that calls a continuation-blocking function may need to be aware of the specific continuation implementation. What is easy is providing a general solution that transforms async IO (no matter how that library is implemented) to continuation-blocking: you suspend the current continuation, and as a callback to the async operation, provide a function that unblocks the continuation.
Thanks so much! Feel free to ask if something's unclear.
Is is a practical solution indeed, and sometimes gives some very fun discussions where I ask about strange edge cases that rust type-system forces me to consider (especially regarding thread-safety and lifetimes), and the first answer I get is "Why would you do such a thing?!". ^ ^
C++ Coroutines have standard wording and existing implementations but they did not make it into C++17, among other things, to have more time to try and find such a cost-free abstraction with better ergonomics than async/await (without the "what color is your function" problem). Some people seem to believe that such a better abstraction exist and that it is worth spending 3 more years in trying to find it. We'll see how that goes but it is an effort that definetly those interested in coroutines in Rust should track since the tradeoffs and design decisions they are facing are similar.
http://programmers.stackexchange.com/questions/254140/is-there-a-difference-between-fibers-coroutines-and-green-threads-and-if-that-i TIL. 
&gt; let mut str_buf: Vec&lt;u8&gt; = Vec::new(); You're creating this string buffer in a function, and then attempting to store a reference to that buffer. However, when this member function ends, that reference is no longer valid. It looks like you want to own that buffer.
And what should I do instead?
What's your background? If you know C/C++/Java/etc., you could liken Option to a nullable pointer/reference, where None is akin to the null variant. Option is used to indicate an optional value, where an object may or may not be present. struct Task { title: String, assignee: Option&lt;Person&gt;, // a task might may or may not have a person responsible for it } Option also allows you to signal a failed computation in very simple scenarios, where you have no need to return additional information about the error, just indicate that no value has been produced. For example, the `f64::sqrt()` method in the standard library returns `NaN` (not a number) if you try to take a square root of a negative number, but we could instead [implement it with Option](https://is.gd/ecWlif). Result is similarly used to indicate the outcome of a computation that could either produce a value, or return an error. For example, writing to a file either returns `Ok(n)` where n is the amount of bytes successfully written, or `Err(e)` where e is an error type. IMO the Rust documentation is really comprehensive on the semantics and use cases of both [Option](https://doc.rust-lang.org/std/option/index.html) and [Result](https://doc.rust-lang.org/std/result/), but if it does not answer your questions, I'd be glad to provide further help.
[So very true...](https://youtu.be/aPvbxuOBQ70?t=30m27s)
Your struct should not hold a reference, but a String. Then you can call to_owned() on the buffer.
Yep, this is the other solution, allocate the buffer outside of the function so that it lives long enough. This would be faster I suppose.
The standard location for these posts is /r/playrust.
The performance-sensitive way of doing this sort of thing is to use the [`std::borrow::Cow`](https://doc.rust-lang.org/stable/std/borrow/enum.Cow.html) ("Copy on write") type. It allows you to store either an owned value or a view into another owned value. [Demo](https://play.rust-lang.org/?gist=46bb5e5acb98acc62622706332c22a80&amp;version=stable&amp;backtrace=0)
Oh thanks man i got lost new to reddit :P
Ah this is really helpful, thank you. I hadn't realized that since `graph` is being modified, and contains a reference to `neighbors` that I was borrowing `graph` for as long as I had a reference to `neighbors`. 
Thanks :)
Preparing to go to Rust Cologne Save the Date meetup. While keeping up my day job and pushing random PRs here and there...
Can someone give me any example of breaking optimisation with explanation? I can't see any...
That's a good way to think about it. I was hoping there was a way make it explicit that I've taken a non-`mut` reference to `neighbors` using the Entry API, since the only mutation happens in `.or_insert`. After that, `neighbors` is an immutable reference to something `graph` owns, and shouldn't conflict with other immutable borrows of `self`.
I disagree for several reasons. The most important thing for a project is (IMO) ease of development, nightly is almost purely better for this than stable, since it can do everything stable can do and then some, use every library stable can and then some, etc. Unless people use unstable features in all types of projects, their is no way to get good feedback on if they are good features. For libraries sticking to stable features provides the advantage of upgrading the library/rust at the wrong time can break a lot of peoples build. For a binary however it only breaks for the developer of the binary and there is always a working rustc one can roll back to. The rust compiler also does a great job of making this not to painful. The one issue it has that I see is the optics are bad for outsiders, it makes rust as a language look a lot more unstable than it is. I don't think that matters enough to cancel out the positives.
I have half a mind to just disallow ZSTs.
`(start..stop).step_by(step)` is unstable. What is the best stable Rust equivalent?
I like the constraint because 1) it allows for nice front-to-back ordering, 2) if the constraint holds, then there is guaranteed to be enough space, without having to assert on capacity, which isn't known at compile time. I'll look into seeing if I can have the alignment constraint be more flexible.
(start..stop).map(|x| x * step) Would be my guess Edit: Though the [implementation](https://github.com/rust-lang/rust/blob/887e9471783ff3f5edc920a85b6110486dc063c0/src/libcore/iter/range.rs) is a bit more complicated than that. It swaps n with n + step every next(). This seems like it would have the same result? There's probably a good reason for it to be implemented that way though. 
Yeah, but it's messier than that. (start..start+(stop-start+step-1)/step).map(|x| start + (x - start) * step) And that mess doesn't reveal anything about the original intent at all.
Looks like there's a crate: [`stepper`](https://crates.io/crates/stepper) although it looks limited.
The other examples look really great, but if you're trying to learn to unwrap or try! these enums, you might want to go look at std::io::result::Result in the language reference. I use try! when I'm doing something like a read with no immediate way out of the error within my read function because try! nicely just returns an Err() for you to match out of upstream. I haven't used option very much, but I imagine it would be useful when you have more than an Ok() and Err() as possibilities to return. Hope this helps!
Yeah, `Cow&lt;'a, str&gt;` is strictly more general than `Cow&lt;'static, str&gt;`. I just picked `'static` because I'd expect a class like that to mostly be used with static strings when it doesn't want an owned value.
How should bindings be exposed? E.g. the raw functions/methods which the user can use in unsafe blocks or safe rust-style functions/methods? Thanks!
&gt;Personally, I find the two proposals rather similar. What's the big difference between: on one hand `async int function()`, on the other hand `std::future&lt;int&gt; function()` ? Hm... I think there might be a big difference for an interface like my proposed `AsyncWrite` / `AsyncRead` where methods take borrowed references. See my comment [here](https://www.reddit.com/r/rust/comments/4li9v2/generators_are_the_missing_piece_for_async/d3pcl52). If we try to rewrite`AsyncRead::try_read()` so that it just returns a `Future`, we get something like this: pub trait AsyncRead { fn try_read&lt;'a&gt;(&amp;'a mut self, buf: &amp;'a mut [u8], min_bytes: usize) -&gt; Future&lt;Result&lt;usize&gt;&gt;; } When we call `try_read()`, the ongoing computation that wants to write into `buf` will want to hold on to `buf`. But how do we tell that to the borrow checker? let buf = vec![0; 100]; let future_n = stream.try_read(&amp;mut buf[..], 100); // can we use `buf` here? 
It amuses me that long after the uphill battle to get `int`/`uint` renamed so people wouldn't use them as default integer types, we've got people advising it's okay to use them as default integer types.
You should use usize whenever you deal with something related to container size, and u32 and u64 for everything else. See [here](http://stackoverflow.com/questions/29592256/whats-the-difference-between-usize-and-u32) Just a warning, u32 does not always mean int. u64 does not always mean long too. It all depends of your computer architecture, but for example on x86_64 both int and long are 64bit. And on i386 both int and long are 32bit (long long are 64bit though).
`make_string` needs to be `unsafe` because it allows dereferencing a raw pointer. Using `turn_into_null_string` leaks memory. There should be a way to free these strings but the `ffi` gem doesn't seem to provide a straightforward way to do that. :(
&gt; The one big difference is if you split the world of functions and declare that only an async function may call an async function. This I think should be avoided at all cost, because it makes async viral. In a world where `async` functions are just generator functions, they could in fact be called from any context. The restriction would be that `yield` or `await` would only be allowed in an async/generator context. And this a lot of sense to me; you can only yield or await if you know that there is there is something like an event loop above you in the callchain driving your execution.
oh wow, I'm coding in Rust for over half a year now and I never understood why they called it usize, and now it totally makes sense :O Btw, I always interpreted it like a "generic" type. u&lt;size&gt;, where usually it's u8, u16, u32 and u64 and now with the "generic u&lt;size&gt;" type, it automatically chooses a suiting size based on your architecture. I've never intepreted it as Rust's way of saying "size_t" so far :O
Except that without volatile writes, the compiler is free to realize that you never actually read screen memory and delete the writes entirely.
The pattern where a compile-time known state machine is encoded in types is called session types. Hyper uses it too in its connection api, to distinguish between the stages of a connection. Nice post!
This is good, but I think it would benefit from an explanation of *why* affine types are useful for modelling state machines (A more expansive version of "some states are not used more than once"?)
That's a good point, thanks. I'll see if I can expand upon that. edit: I've expanded on how the affine type system makes it easy to implement session types.
Probably a stupid question, but how do the two implementations compare performance wise? I mean, is there some sort of cost (or gain) in using those compile time safeties through the type system? I have no clue how this would get optimized and represented 
About your second paragraph: I don't see how the correspondence with C types is relevant here. If that's what someone was looking for, they'd just use libc::c_int or libc::c_long, yeah?
That sounds really interesting. Anything to show yet? :)
Generally the unsafe wrapper is still published as `&lt;crate name&gt;-sys` so it can be versioned, downloaded, and built concurrently. 
Oh, now it makes sense to me, thank you! I'm thinking maybe the compiler could detect some dangerous cases but I'm not sure...
Alex is more responsible for this stuff than I am; I rarely compile `rustc`. I believe that this is why people use the `stage1` based targets, since Rust is written in Rust, the bootstrapping process means that yeah, it all needs to be reocmpiled when something changes. By only building `stage1`, you skip the later stages. I believe the new rustbuild system will make it easier, IIRC?
Thanks. I've changed quite a bit so I just put it on github. Right now I'm just handling the integer division by `assert!`ing that there is no remainder after division as a stopgap. I haven't put much thought into panic safety. Hmm.
Do you have prior experience with a lower-level programming language? I find those sorts of "is it faster to..." questions are a lot easier to answer if you know what sorts of things they compile down to.
Even with C++ knowledge it can be a bit of a black box when you start to take compiler optimizations into account. I've done some basic benchmarks using a similar pattern as the one in the blog post, compared to mutable references. My hope was that RVO would optimize the move into a reference. However, this didn't appear to be the case. I haven't looked into it in depth yet, my initial assumption is it has to do with the Result return value maybe breaking RVO, but I have nothing to back that up.
It could be that [raw pointers](https://doc.rust-lang.org/book/raw-pointers.html) + unsafe blocks would let you write the code you want to write instead of going through hoops with the borrow checker.
That's awesome! Could it be used in the future (with Rust supporting asm.js) for a "React.rs" for generating front-end html/css/js?
I recently had a pair of threads collecting `ChildStdout` and `ChildStderr` for a similar non-blocking `get_stdout_or_else(...)` piece of functionality. I eventually removed the threads using the `O_NONBLOCK` flag on their file descriptors. I abstracted it into a simple crate, [nonblock](https://crates.io/crates/nonblock), but I'm kinda surprised that `Stdin` doesn't implement `AsRawFd`, so it's not really going to help you here.
The real way to do what you want is with a terminal control library. In C or C++ this would be ncurses; there's probably more Rustic libraries available. Unfortunately, that means you can't use readline to read input, you'll need to handle all the line-editing yourself. But, as you've discovered, readline is pretty specialised, and writing your own thing might be less work than hacking up readline to work the way you want it to.
Thank you! Maybe! It'd be fun to try. 
Actually, what I did was this. let mut i = start; while i &lt; stop { // loop body i += step; } That only works for positive steps, but it's good enough.
I'm not sure if anyone else has mentioned this below, but as Rust is a systems programming language, it's definitely beneficial to have a good understanding of the basics of computer systems (if you don't already)! I suppose that knowledge is not mandatory, but I think it's very helpful, especially if you want to write something using unsafe. Computer Systems: A Programmer's Perspective is a great introduction, and after that an operating systems textbook would be good. Systems knowledge will benefit more than just your Rust programs too!
As far as I know, little endian is limited to Intel, but it should be indeed more used than big endian these days, so I might just change it as well.
Huh? This is just a video of some guy playing a video game.
How can I get an Iterator over a any collection that doesn't consume it and lets me iterate over references of the elements of the collection? AFAIK, there is IntoIterator, but it consumes the collection.
Worth adding: OP comes from Java, where int and long are 32 and 64 bits.
Do you know a good way to learn Rust? I believe I had some official documentation bookmarked, but do you have any specific recommendations?
I see now why there's likely need for compiler support for *something*, but I still don't understand whether this complex code rewrite/state machine idea is *necessary* or just one *implementation*. Again, the semantics seem to be a variant of threads + channels, but with some extra restrictions that allow the stack less coroutine to share the same stack as the parent thread. So can we bake the ability to enforce such restrictions in some manner, and the move more of the work to libraries?
I see now why there's likely need for compiler support for *something*, but I still don't understand whether this complex code rewrite/state machine idea is *necessary* or just one *implementation*. Again, the semantics seem to be a variant of threads + channels, but with some extra restrictions that allow the stackless coroutine to share the same stack as the parent thread. So can we bake the ability to enforce such restrictions in some manner, and the move more of the work to libraries?
I just published another side project [here](https://github.com/nelsoncarrillo/n) This is a toy programming language (at the moment it looks a lot like Rust because I like the syntax :-)) with a primitive type checking / inference and compilation to LLVM IR. It was a cool learning experience. I'd never done any compiler work before, its super fascinating. It tokenizes the input, builds an AST, runs a series of passes (3 for type checking (gather, infer, verify), 1 for code gen). I also support scopes by representing them as a stack of lookup tables which map to various points in the AST (Ident to NodeId, NodeId to TyId, TyId to Ty) and it can look up the stack of scopes until it reaches a function boundary. I don't know if any of this is best practice from a compiler dev. standpoint, but its certainly cool to see it all come together. 
I can also say that learning Rust after just a little bit of C++ definitely helps reinforce those C++ good practices!
C++ is a very big language. It's been around for many years, so there's just a lot of stuff in it, and good practices has changed over time. There's many ways to do things, and there's a lot of things to learn. On the other hand, Rust is still very new. There's less to learn, and no baggage from earlier versions. But there's also a lot less stuff available. Whatever you want to do with C++ you can find libraries, blog posts and books on how to do it. Rust still doesn't have all that. C is the granddaddy of them all. You probably want to learn C at some point no matter what. Not just for the language itself (which is small and quite elegant for what it does), but because you learn more about how things actually work in the background as well. I wouldn't recommend starting with C though, unless you are interested in robotics, microcontrollers and that sort of thing. 
Here are my suggestions based on my own experience. IMO, learning is a process that is most effective when it comes with intrinsic rewards. With that said: 1. Find a small to medium project that excites you. Maybe it solves a specific problem you have or maybe it teaches you a new algorithm or data structure that you've always wondered about. 2. Briefly read introductory tutorials on Rust, C and C++. Which language satisfies your own sensibilities best? Which language makes (1) easier or more enjoyable? Include other criteria that seem important to you and choose the language based on that. I say these things because your post seems to suggest that your goal is to learn, and if you do something that's fun, you're probably more likely to better absorb the material. Learning any one of these languages will provide a good stepping stone to learning the other two IMO.
The convention is to use the dash.
Nice to see lifetimes management emerging in other languages.
Most (all?) collections define an `iter()` method which does that, in addition to `into_iter()`. E.g: [`Vec::iter`](http://doc.rust-lang.org/stable/std/vec/struct.Vec.html#method.iter) (via deref to slice)
http://rustbyexample.com/ https://doc.rust-lang.org/book/ And when you feel like you've got a solid handle on things the Nomicon for unsafe rust https://doc.rust-lang.org/nomicon/
I recommend starting with Rust just because it has better documentation and it's easier to get started developing from scratch. There's a huge mental burden to programming with C or C++ with header files, makefiles, missing critical pieces in the standard library and even difficulty just to get a development environment set up on platforms like Windows. Rust provides all in one solutions for all the above, with better documentation to boot. I would learn C after Rust though. I can't at all recommend C++ due to it's unstable ABI.
I'm just another beginner, but I believe usize and isize are architecture dependent. Good for indexing, unsafe memory operations and so on, but not good for a general numeric type. Use u32 and friends instead, so you know what you get. 
It might be arguably better to just learn C. You still learn pointers without the complexity of C++. Plus C is still very useful since it's used by most languages as the lowest common denominator for an FFI (including Rust).
The problem with Idris' uniqueness typing is exactly that you can't write arbitrary lifetimes in function signatures, so I think it's less expressive than Rust.
I disagree. The borrow checker is there to make the ownership handling part *easier*, and judging based on how often this gets messed up in other low-level languages, it does a stellar job.
Given that a vector of zero-sized types is semantically equivalent to a number, go right ahead. It's not like anybody would want to use it for that.
I'm really glad to hear that the site is still alive. I was worried when the SSL certificate lapsed before. Thanks for making it!
C is good to know, as much of your systems and libraries are written in it. In the most idealistic scenario where Rust/Go/$some_future_language displaces C, you'll still need to know C in order to port legacy C libraries or write FFI bindings. Also learning C will help you appreciate all of the safety mechanisms that Rust provide; as C is a memory mapped language where pretty much everything is mutable; also no `Option` or `Result` types. C++ is also good to know and still widely used, although it's OOP model is kind of, how do you say "prickly" (ex: virtual private functions and friend functions). If you just want to focus on learning good OOP design principles, I'd recommend Java or even Python/Ruby.
My thoughts: * I love Rust and think it's a great language to learn, because it enforces safe memory management. The existence of unsafe blocks in Rust make it largely as powerful as the other two languages, while still being safe by default. I've also come to love it's amazing lazy iterator system. I'd love to switch to Rust at work, but I suspect C++ is going to remain dominant for many more years, simply due to library support and the existing infrastructure. * I use C++ in my day job doing computer vision and would say I'm fairly familiar with it. It's an extremely powerful language, with lots of libraries and tooling available, and a huge number of support resources. On the flip side, it can be an incredible pain to use and debug, largely due to memory/concurrency bugs that can be nearly impossible to trace. Guard pages are your friend. Cross-platform support is also...inconsistent. I target 5-8 different platforms with the same code base, and most are missing at least some portion of the C++11 standard. I generally don't recommend learning C++ as a *first* language, because inscrutable memory and linker errors can prevent you from getting the basics down (loops, control flow, functions, etc.). I learned Java first, than switched to C++. * C is the common-denominator language spoken by many other languages for interoperability, including Rust (via FFI). I'd recommend learning it for that alone, but not until you're familiar with another higher-level language. It feels a bit too primitive for everyday use (e.g. no formal String class), unless you want to do embedded systems development. Summary: I'd recommend learning Rust first, with the understanding that you'll likely *have* to learn the other two at some point.
ruma, haha
As a side effect to this, I've came to experience what I can only describe as the post-Rust depression syndrome, where I become very aware of the unsafe stuff I write in C++ and C and shake my head at the fact that the compiler will just take it. (But do it nevertheless, because I ain't getting paid for correctness).
cleaning up my pure rust port of [stb-vorbis](https://github.com/bungcip/rust-stb-vorbis). Need to cleanup a lot of pointer usage and make a more rustic api interface.
My bad, someone else here mentioned it would stabilize for 1.10 so I didn't check further... :/
Yeah. The way I see it, with both Rust and C/++ you have to learn this stuff. The thing is, Rust forces you to learn it on the outset, whereas you can learn basic C/++ and write simple programs without this. But eventually you have to learn it, and I'd rather have the language help you and have explicit constructs in these terms than something like C++ where this has to be learned with experience in a very nebulous fashion.
Yeah. The way I see it, with both Rust and C/++ you have to learn this stuff. The thing is, Rust forces you to learn it on the outset, whereas you can learn basic C/++ and write simple programs without this. But eventually you have to learn it, and I'd rather have the language help you and have explicit constructs in these terms than something like C++ where this has to be learned with experience in a very nebulous fashion.
In short, `&amp;static str` &gt; `&amp;str` &gt; `String` `String` is the simplest to work with because you *own* it: you can modify it, extend it etc ... The drawback is its performance: it is very slow (allocated on the heap). You can make an analogy with `Vec`. `&amp;str` (on the stack) is much faster than `String`. But you cannot modify it. This is only a borrowed version of a `String`, which means you cannot create new `&amp;str` out of nothing. You can make an analogy with `&amp;[]` `&amp;'static str` is the fastest one but also the less flexible: not only you cannot modify it but you need to know its value at compile time! You can make an analogy with ~~[;N]~~ EDIT: `&amp;'static []`
Well, it's actually to use a hyphen right? Technically a dash is a different character/grammatical construct than a hyphen.
You can use `Cow` if you want to have the option of having an owned `help_text` as well: https://gist.github.com/anonymous/e32e5d58b5316989d55219aab68eba8f#gistcomment-1790491
Yes, I'm pretty sure that's right. It seems backwards at first because in most type systems all types inherit from a single base type like Object (ie. more specific objects are at the bottom of the tree) whereas with lifetimes the root lifetime ('static) inherits from all others, so the smallest lifetimes are at the top of the tree. Honestly lifetimes aren't too bad once you work with them in real world scenarios. It's usually just a matter of making sure the data structure/scope you get references from stays around for long enough. They only really get complicated if you're writing a custom data structure.
If you're starting your own project, use Rust. Much more fun. C++ is a powerful monster, and if you're working with existing projects you may need it. Learning the basics of C at some point is a must. Ultimately, jumping between languages isn't all that scary.
Actually, you can do it as long as the shortest one lives longer than the struct does. Lifetimes have proper subtyping.
Right, most of the time in real code it wouldn't be a problem. But technically it is a constraint that can be triggered.
I'm not sure why you're downvoting me since I'm contributing to the discussion? The code you posted is not an example where this would be an issue. In your example text and up have compatible lifetimes.
Most ARM chips out there are little endian as well.
&gt; The one issue it has that I see is the optics are bad for outsiders, it makes rust as a language look a lot more unstable than it is. I don't think that matters enough to cancel out the positives. I disagree on precisely that point. Industrial consumers of the language want stability and the only thing stable is stable. I also disagree that nightly is strictly more easy to develop. It's always full of upcoming changes, badly documented and there is no guarantee that those features will ever see the light of day in that form, meaning that you are potentially buying into a lot of technical debt.
Yes.
It's ok, makes it stand out. And since it has been done as far as I have been reading TWIR it just feels like "tradition" at this point :P
I like it! It's not overdone, and even if it is a bit silly, I don't think it matters. Programming doesn't have to be serious business all the time.
Ceci n'est pas une link.
I would like to program the [BBC micro:bit](http://www.bbc.co.uk/mediacentre/mediapacks/microbit/thebbcmicrobit) in Rust. It's a programmable microcomputer for teaching programming in UK schools. It has an ARM processor, does that mean it's possible?
... how did this happen.
V2 of the book? Is it currently available somewhere? EDIT: I guess that's it: http://rust-lang.github.io/book/ownership.html Thanks a lot for the documentation. I bet it's a huge task and I'm having lots of fun learning Rust thanks to your work.
The truest form of [bike-shedding](https://en.wikipedia.org/wiki/Law_of_triviality).
Yes, that's it :) Hopefully getting some chapters merged today... You're welcome. It is, but it's also how I learned Rust as well. I used to joke that I wanted to be the documentation person because then everyone else _had_ to explain to me how things worked, and they couldn't give up on me, because it would then be my job to explain it to everyone else.
That sounds like an interesting issue you have there! Yeah the repl needs work, it was a quick addition for quick tests. Not sure what the best way to extend it would be though, it annoys me that I can't even use arrow keys properly in it!
The second version does make better sense to me, but I don't think I would have gotten it before reading the 'Nomicon. A beginner's tutorial would ideally be accessible to someone who only knows Javascript or Lua or Python (or Java or C#). They need extreme basics, like "values are made of zero or more bytes" and "a value has exactly the same size and type as the location that holds it." Maybe even "this is a byte." The RAII pattern, with examples, Rust ownership is RAII. I shouldn't just complain, so I've started outlining this, something like "A Beginner's Guide to Rust's Object Model: Where We're Going We Don't Need Runtimes."
Yes, it _is_ a tradition started here - https://users.rust-lang.org/t/this-week-in-rust-editors-thread/1806/22
&gt; Even weirder, gdb shows the segfaults coming from random places in the standard library, and not the same one every time. That makes sense though. If something goes off the rails in JITted code it won't be trapped until it hits a segfault, illegal opcode, or assertion. The standard library is full of assertions.
Its tough in the sense its something you've never had to fight a compiler about before. I agree its good, but it is different.
Is there a stable rust equivalent of std::intrinsics::discriminant_value() for comparing two enum instance types regardless of any value they may contain?
Unfortunately it wasn't recorded. :-(
Thanks so much! I actually meant to email you to say I was sorry we couldn't actually talk that much, given time constraints and all. I really enjoyed your talk too.
It wasn't but I gave a version of it here: https://www.youtube.com/watch?v=iTSx-8qK4Hw The one in Boston was half as long and a bit more polished, this recording was the first time I gave it.
Yes I realized that:)
For future editions, it might make sense to move all the "This Week In" links down under the "Project Updates" section, since those are quite project-specific.
Sure, makes sense.
Probably `Future` would need a lifetime marker. Much like a lambda function, the `Future` embeds state, so you would need a `Future`, `FutureMut` and `FutureOnce`, with the appropriate lifetime markers... maybe?
Thank you! Looks great so far, I will take a closer look tomorrow
Thanks! I'll be at the All Hands in a couple weeks, so if you're there we can catch up then!
That's more of a house than a shed. Can we get a more accurate bikeshed emoji? Preferably in yellow.
Sorry but I have to disagree with the "getting the development environment running". To develop C++ you have to do nothing but download an IDE, like CodeBlocks on Windows or XCode on OSX, which already includes the compiler with everything set up from the get-go. You haven't had to fumble with makefiles for your own projects since over a decade. With Rust however you have to install everything manually. "Better documentation" is arguable too, http://en.cppreference.com/w/ is really good and almost every library part has an elaborate description and a usage example. I do not disagree that Rust is a better starting point today, but there are other arguments for that.
That sounds like a really interesting bug:) If I were in your shoes, I would focus on finding a test series that reliably reproduces the bug. I think that's an important step. Try to repeat the tests in the same process, without exiting, if a single run does nothing. Also try to remove everything that uses random values.
Awesome! Glad you figured it out.
`usize` is Rust's version of `size_t` as in it is supposed to be the integer representation of a pointer. Though it is not defined in terms of `size_t` so when using ffi you should use `libc::size_t` because they may differ on your platform.
You can use `#[repr]` and `unsafe` to set the discriminant type (otherwise it's arbitrary) and view it, respectively: https://is.gd/UxFg3K AFAICT, this is 99% stable. The compiler is free to reorder struct fields as it sees fit but an enum discriminant would always have to be first in the memory representation. N.B. enums utilizing null-pointer optimization such as `Option` won't have a discriminant when said optimization applies. This only applies if one variant is empty and the other has a value type that starts with a pointer such as `&amp;/&amp;mut _`, `Vec&lt;_&gt;`, `String`, any type that has any of the previous as its first field, etc.
Do you just want to compare them for equality [like this](https://is.gd/vlzq3L)?
&gt; It seems backwards at first because in most type systems all types inherit from a single base type like Object (ie. more specific objects are at the bottom of the tree) whereas with lifetimes the root lifetime ('static) inherits from all others, so the smallest lifetimes are at the top of the tree. I personally find it easier to think of it like a trait bound, i.e. `'a: 'b` means that `'a` must satisfy (or implement) the lifetime `'b` and it can only do that by being a superset of it.
Abstracts, bio, schedule, and ticket sales coming soon!!! Thank you to everyone who submitted, it was tough to choose!!!
Thanks, this looks like it should do the trick more nicely than how I was doing it!
[This thread](https://www.reddit.com/r/rust/comments/4grfi3/argument_evaluation_order/d2k35fi) mentions that Rust's evaluation order is left-to-right. But it looks like in assignments, the right side is evaluated before the left ([playground link](https://is.gd/M2tLq2)): // The write to v happens at the after-bump index. v[i] = bump_int_and_return_5(&amp;mut i); Are assignments an exception to the evaluation order rule, or am I misunderstanding the rule?
It could at least be a Github issue tag. `I-🚲🏠`
This talk was really excellent. Highly recommend watching it.
I'm working on a (A)Rc that will incrementally delete children RC'd pointers instead of performing a giant RC destruction tree, and see if it makes sense to integrate into stdlib at all. Also starting a sort of work-stealing/sharing job scheduler that should have high throughput than normal chase-lev based schedulers and also support basic concurrency primitives and asynchronous operations.
This is a [dup](https://www.reddit.com/r/rust/comments/4lqoyg/wrote_my_first_blog_post_about_rust/).
Aha, I see. :) I'll lock this one then.
Workshop tracks that take half the day are followed by a different track that takes the other half of the day.
Gankros have a GIL. Rust can handle parallel tracks!
It is of vital importance that emotes are used for Rust releases.
Can anyone here compile clippy? Tried with nightly rust, fails for versions 0.0.70 all the way down to 0.0.66... Then I tried to compile the latest commit that Travis shows as passing... That didn't work either, tried the next two commits and gave up 
Hah - that UB is just what I commented on in my edit. I don't have a good answer yet. I'm still searching. What I said about the discussion in the other thread still stands. I'm just not 100% on whether `i` is specified to be read after the RHS evaluation or if that happens to be an artifact of the implementation.
The only problem Rust has with emojis is that they aren't allowed in idents.
Thanks for your talk as well! Learned quite a bit about the regex crate which is pretty cool!
If only I could see it :(
Interesting, but skimming the article it doesn't seem something which I can only do in rust, does it? In principle it could be done even in C or there's some rust feature which I'm overlooking?
I tried installing and got an error leading me to [this PR](https://github.com/rust-lang/rust/pull/33929) merged two days ago. I think you'll have to use a nightly older than that or wait until clippy is updated.
Yours was really good as well. I brought a coworker who wasn't familiar with rust and they were impressed both by the regex project and your presentation of it.
Yes right, I was concentrated on the structures without remembering about the ownership features of rust. Thank you
I'm working on http://rusty-dash.com this week. If you have contributed to Rust before, I'd appreciate any feedback. Ongoing discussion is at: https://internals.rust-lang.org/t/the-rust-project-needs-much-better-visibility-into-important-metrics/
I would definitely try to make this more clear on the site if possible.
Can you give out *-pointers so we can alias you?
First and the only question: what are you trying to do? Like, Rust guarantees memory safe doesn't mean it's good for everyone, since the overhead or you "might" need to know what behinds the scene. For this reason, C or C++ would be a nice place to "make mistakes", learning how to manually manipulate memory and why Rust exists. So, before choosing the tool, decide what you want to do first.
I'm having difficulty understanding lifetime of newly created structs as return values from methods. I couldn't find an answer in The Book - checked under Variable Bindings, Functions, Lifetimes, Structs, Traits, Trait Objects, Method Syntax, others, as well as The Stack and the Heap. Simple example: struct TestData { data: [u8;4], } impl TestData { pub fn new() -&gt; TestData { TestData { data: [0;4] } } } I thought the constructor method `new` creates the anonymous instance of `TestData` on the Stack, which leads me to believe that it goes out of scope at the end of the method. My next thought is that Rust recognizes it's returning the value so it's placed on the Heap instead - however I thought that would require it being wrapped in a Box. Could someone clarify this for me? Here is one of the examples from my code which I'm still not sure why it works, and what confuses me even more is that I'm assigning a field in the object to a method-local variable. pub struct DicomStream&lt;StreamType&gt; { stream: StreamType, } impl DicomStream&lt;File&gt; { pub fn new_from_path(path: &amp;Path) -&gt; Result&lt;DicomStream&lt;File&gt;, Error&gt; { if !path.is_file() { return Result::Err(Error::new(ErrorKind::InvalidData, format!("Invalid file: {:?}", path))); } let mut fstream: DicomStream&lt;File&gt; = DicomStream::new(try!(File::open(path))); let is_dcm: bool = try!(fstream.is_standard_dicom()); if is_dcm { return Result::Ok(fstream); } return Result::Err(Error::new(ErrorKind::InvalidData, format!("File is not DICOM: {:?}", path))); } } impl&lt;StreamType: Read + Seek&gt; DicomStream&lt;StreamType&gt; { pub fn new(stream: StreamType) -&gt; DicomStream&lt;StreamType&gt; { DicomStream { stream: stream } } }
Rust newbie here with what's probably a dumb question: I have a mutable variable and an array full of values. I want to iterate through the array and change the value of the mutable variable to that of a member of the array, then use that mutable variable later on in my code. Right now I'm getting the dreaded "somevar does not live long enough" error. How do I go about pulling the member value without running into a wall?
I can't help, but feel free to make a new post here or on users.rust-lang.org (or even StackOverflow). To get the best results, I recommend you be as specific as possible, e.g. with the code you've written, its Cargo.toml and any error messages you're seeing.
No tool like this exists. You may want to investigate one of the catch panic solutions. With this you can guard your main event loop/handler etc. 
Since you never need to instantiate State1, wouldn't it be a better idea to make it an empty enum?
Oh actually found out that the author intended that the crate be consumed from the source. So I did that I got much different results.
Will some of the talks be filmed?
The problem is that I can't fix problems when getting panics in this way. I need to fix problems eagerly.
Yeah, we're working on that.
That's kinda different from hunting for `panic`s at compile time. I'm writing code for an embedded arm board. Yes, I could set something up to reboot on panic, but I'd much rather see what could cause one in the first place, to avoid a possible reboot loop.
I more or less agree with what the article says, but for me the state of the Rust ecosystem is abysmal compared to the state of the Rust compiler. The Rust compiler has 1407 contributors. Compare that for example to 97 contributors for hyper and glutin, 48 for serde, 71 for image, 46 for nom. It's another order of magnitude. I often wish more people worked on libraries and less on the compiler in order to balance things out a little bit. 
&gt; Special shout out to mbrubeck for completing the ongoing triage of every unlabeled issue in Servo’s issue tracker! Hooray dedicated issues management. :)
&gt; unlike map it is hard to imagine what Option::flat_map would be useful for in my opinion. The exact same thing `and_then` does since it's semantically the same operation (the monadic `bind :: m a -&gt; (a -&gt; m b) -&gt; m b`). *However* the stdlib picked the more readable (and expected in a sense) version rather than the theoretical one, just as it did for Result (instead of Either).
&gt; I can imagine that conciseness is a reason for example. It is one, especially when you consider that Rust uses snake_case for methods and members, so you'd have `to_option().get_or_panic("")` which isn't really clearer than the existing once you know what the existing does (which basically means know how `ok` and `expect` are used in the language) but is way longer. Not only that, but in the stdlib `get` generally returns an `Option`, so `Option.get` doesn't really make sense (it's just an identity), and `get_or_panic` makes none whatsoever`. &gt; I also noticed that there is and_then in some cases and flat_map in others. which is inconsistent. It's theoretically inconsistent, but practically more convenient, in the same way the stdlib uses `Result { Ok, Err }` rather than `Either { Left, Right }`: for a single-valued container, `a.and_then(b)` better expresses the operation and avoids delving into monadic concerns. I expect if/when Rust gets HKT there will be a Monad trait which both implement with a `flat_map` or `bind` method, but that's not the case yet.
As someone who would like to start helping in the Rust space - where do I start? Looking at some of the issues on Hyper for example.. I'm not sure what I should actually tackle. Only a couple of them seem marked but there doesn't seem to be anywhere that says what I could try picking up? The READMEs are just a link to documentation or a huge tutorial. I'm not sure whether to pick up issues submitted by anyone or only the labelled ones. Some of the open issues have PR's already merged to fix them. Some of the open issues are random people asking questions, etc. I haven't checked the others you've listed.. but I've generally found this to be the case in most Rust repositories. I don't know how or what to help with and the repositories don't make it clear either. (I might be asking for too much? I just don't know where to begin when I look at these repositories and I would really like to help any way I can)
For me as a Rust advocate, the biggest issue are the libraries that keep on using nightly features. It is hard to sell a language whose libraries cannot work with a stable release. 
Add `expect` and most cases are covered, thing is that some library calls may panic. I'd like to know which so the library can either be replaced or improved to return Result instead.
I am looking forward to "War Stories from Debugging Rust". I always find debugging stories fascinating.
wow, please share recordings after the conference
&gt; hashed point Using [raw bits](https://is.gd/jSokVr) from f64 using [std::mem::transmute](https://doc.rust-lang.org/std/mem/fn.transmute.html) 
Are there any in specific you're thinking of? I found this to be a problem a year ago, but lately just about everything I use (~100 transitive deps) works fine on stable. I agree it should be a priority but I believe that this specific problem has already been mostly addressed.
&gt; With Either, both sides are treated the same, and you have to pick left or right before you can map. That's not true, in Haskell there's a Functor instance for `Either a`, so you can directly `fmap` on the "Right" type (which corresponds to `Ok`): http://hackage.haskell.org/package/base-4.9.0.0/docs/src/Data.Either.html#Either You do have to extract Left somehow to map it. &gt; Now that I write this, it strikes me that the there may be a connection between the enum-value OK and the method name ok() as well :) Indeed, hence there also being an `Result::err` which returns an `Option` to the `Result::Err` case.
Adding on to that, libraries that just link to their generated docs with no example usages or high level explanations are quite frustrating. I don't know about others, but example code is pretty much the main way I learn a new library.
The author appears to have little experience with Java. Byte code wrangling tools (edit: yes, and code generation) afford us Java devs many of the same freedoms that Ruby developers have. JOOQ for example uses this to generate their query DSL. Also Java does have simple web frameworks (especially since we have lambdas). However, despite the author's assertion to the contrary, the market forces for Java web development are different than with Rails, favoring more complex applications (and thus architecture). Furthermore herd thinking and job security considerations lead to the currently predominant frameworks. Apart from the Java bashing, the article raises a valid point that is made all the more important by the recent trend in language design to move functionality out of the language and into libraries (perl 6 being the canonical counterexample).
Is there a list of something like "most requested" libraries somewhere for Rust? I think that would be helpful personally as I have no real idea what most people are lacking in terms of library coverage.
Clippy is one example that comes to mind as all APIs that are still listed as unstable on the documentation.
No, because Clippy is a plugin for the compiler and the API for compiler plugins is not stable. The compiler can change at any point and break Clippy, as you have had the pleasure to experience ;) But now I'm wondering myself, since Rust stable can be used compile the next version of the compiler by temporarily enabling unstable features. Wouldn't it be possible (hypothetically) to abuse that behavior to make Clippy run with stable Rust? /cc /u/llogiq &amp; /u/manishearth
Clippy is a compiler plugin. It *can't* be stabilized yet.
Clippy is for development only, no lib/binary depends on it. You use it just to check your code, not to build it!
The 2nd day talks will be recorded, the first day workshops won't be since they're likely to be more interactive :)
I come from Scala and `and_then` does not make any sense for me. I'd understand it in context of function composition; but I cannot imagine temporal composition of *values*.
An uninitiated programmer may be confused at first, true. But if one is familiar with monads, he cannot *unlearn* them. It seems like making an effort to deter savvy users. Personally, I wouldn't want my language to be noob-oriented.
What's taking so long with that? I saw something about changing the Macro system? But that was also months ago.
To be fair, there's only one compiler but hundreds of libraries. What's the total number of library contributors?
On the enterprise level many IT departments control what stack goes in the developer workstation, so being stable matters.
As I answered on a sibling post, on the enterprise level many IT departments control what stack goes in the developer workstation, so being stable matters. As for the libraries not much else from what I mentioned, my goal is just language advocacy, I hardly use Rust as it doesn't fit the type of projects I get paid to work on (JVM, .NET and C++, customer specifies what).
`multirust` (and now `rustup.rs`) make this super simple. Basic CI can easily ensure everything builds on the Rust versions you care about.
We hook directly into the compiler, which changes a lot. Not directly. There are plans to do it a different way.
That's actually part of the plan, to get it distributed as an addon (along with rustfmt) with rustup.rs.
How much would you pay for watching the speaker walk around the room answering individual questions? ;) ETA: This is not to imply that we're planning on charging for access to the recordings, we're definitely not!!! But there is a cost to recording more, which if the internet wants to put together a kickstarter type thing, would definitely help ;) When I run events, I'm biased towards optimizing the experience for people physically there, which does mean allocating funds towards things other than recordings :)
how about .ok().with_default("")
Should be fixed now!!!
I use multirust, with travis testing code on stable/beta/nightly: https://github.com/Keats/tera/blob/master/.travis.yml Locally i have a feature that uses clippy (https://github.com/Keats/tera/blob/master/Cargo.toml#L18-L20) that I run before pushing to a PR. You can also use a fixed nightly version for development if you want, no need to be on stable. It makes things like Serde easier to use
So then your error is something like `panic: a valid number`? That seems less than ideal.
&gt; I often wish more people worked on libraries and **less** on the compiler (Emphasis mine.) That’s not how this work. People and pieces of work are not interchangeable arbitrarily, especially with volunteers/hobbyists doing unpaid work. If you add barriers for new compiler contributors, that won’t make them go work on your favorite library instead.
So, if developer tools are available only on nightly, then who is stable for?
clippy is the only developer tool I'm aware of that is nightly only. You can use stable for everything else. And if you REALLY need clippy and want to ensure the builds are the same and cannot use multirust you can checkout the stable branch and compile with `./configure --release-channel=nightly`. This will give you the stable compiler with nightly features unlocked.
*Some* developer tools are only available on nightly *at the moment*. &gt; then who is stable for? It's not like nightly is *intended* to be "for developers." The intention of nightly/beta/stable is to run a train model of releases that supports the steady stabilization of various language and library features. At this very moment in time, there are a few very convenient tools that only work on nightly. Therefore, it makes sense to do development on nightly if you want access to those tools. Because of that, tools like multirust and rustup.rs have been built to make it very painless to switch between nightly/beta/stable. The point is that if you want to use nightly-only development tools, then there is a very low barrier to entry. Stable is what people should be using. Nightly is what you use if you need it. The goal is to lessen those needs over time, but it isn't going to happen all at once.
 thread '&lt;main&gt;' panicked at 'a valid number', ../src/libcore/option.rs:700 Yeah, it's not perfect.
[Clippy](https://github.com/Manishearth/rust-clippy/) has some optional lints to catch usage of `unwind` and `[]` (which panics on overflow). It's not perfect, but it'll catch most issues.
We're having trouble in recent issues getting nominations for crate/quote of the week. Can we 🚲🏠 some strategies for reviving those features? Maybe sticky some threads somewhere, or take nominations on IRC?
I agree with most of this, except for this part: &gt; No mainstream language today allows you to write a library to extend its type system. This strikes me as not true at all. There are all kinds of type systems that can be bolted onto Lisps, and even languages such as Python. Even in Rust I would say that things like typenum, and session types, are examples of libraries extending the type system.
非常棒，已加入MVC测试例子，希望新增sqlite, MySQL的示例，赞一个 我是枪炮:)
&gt; I asked, and I'm supposed to use stable for development. Nightly is for the compiler developers. I don't know who said that, but I think the compiler developers would be unhappy about it. All new features are put as unstable features into nightly, and they depend on enough people trying them and thinking about them to get them ready for stabilization. You certainly shouldn't use *only* nightly for development, but as others have said, use rustup/multirust make that very easy.
They inherited that from Haskell
Maybe the string can be prefixed with `expect: `?
I'm having troubles with lifetimes right now. I wanted to create a struct that holds some data in memory and stores slices to various parts of this data in a vector. I have a parse function that reads this data from a reader and basically initializes everything pub struct Foo&lt;'a&gt; { data: [u8; 123], groups: Vec&lt;&amp;'a [u8]&gt;, } impl&lt;'a&gt; Foo&lt;'a&gt; { pub fn parse&lt;R&gt;(&amp;'a mut self, source: &amp;mut R) where R: Read { // ... } } Once I call parse() I can't get out of the mutable borrow. let mut foo = Foo::new(); foo.parse(&amp;mut source); // can't use foo anymore Is there a workaround? Should I use a RefCell? Or is my brain missing a simpler alternative? Playground link of what I tried to do: https://is.gd/hna6Dc 
&gt; Mainly just looking for thoughts or tips on this issue. You could always use a `vec![0; size]`. Note that it's not the most convenient either since the vec won't get "resized", so you'll be using it as a boxed slice (that's actually what BufReader does internally). I don't think there's any good API in the stdlib to use vectors as Read targets at the moment.
Ok. It's just hard to find updates about progress on compiler plugins reaching stable.
The `read_to_end` method takes a mutable vector, which it will grow on its own. For a fixed number of bytes this is no different to `read_exact`, but if you want to read a potentially large amount that you don't want to preallocate if unneeded, use it together with the `take` adapter: reader.take(max_bytes).read_to_end(&amp;mut vec)
But, how can the api change if you base it off a stable (1.9) version? 
Nice, I like that. Thanks!
My point is that a lot of people think the Rust type system is expressive enough that linking to the generated docs is sufficient. And that's fine if you're already a knowledgeable Rust programmer, but for someone like me who is still getting into the language, it hinders adoption.
I understand... but the topics are interesting and I am most likely unable to attend physically. Hold a remote conference? Then I'll pay to participate.
If possible I would look at /u/birkenfeld 's solution to see if that works for you as well. The use of `unsafe` code here seems unnecessary, if you can get the job done with safe code instead.
For me the issue are compiler plugins. A lot of them are super useful and there isn't even a concrete proposal for stabilizing them. It might not happen in 2016.
Developers shipping code that needs a guarantee of compatibility into the future. AFAIK the tools in question are eventually going to work for stable, but they (or, more accurately, compiler features they depend on) aren't yet in a state where people are ready to make promises about them for the rest of 1.0-ternity. Not rushing things into stable before they're ready for that long-term promise is a *feature*. 
Whoever answered that way was giving you the safe answer. The real answer is that stable is for people who need long-term forward-compatibility guarantees (for example, you're shipping widely-used libraries), while nightly is fine for people who want the latest-and-greatest irrespective of whether it might break in the future (i.e. you're just screwing around on the weekend with things nobody else will ever touch). 
&gt; Wouldn't it be possible (hypothetically) to abuse that behavior to make Clippy run with stable Rust? In general yes, but this is a dangerous game...
One issue is https://github.com/rust-lang/cargo/pull/2583 which is a feature that would help a lot, but we haven't been able to have someone get over the finish line.
We used to use the rfcs repo for this, the 'wishlist' tag, but it hasn't seen as much use lately.
&gt; prevent your process from ever aborting due to panic `panic` can abort itself, so this doesn't always work.
Stable compiler plugins are waiting on the compiler's new intermediate representations to be implemented, to provide an interface to AST transformations that can feasibly be stabilized. It's a very high priority, but also an enormously large project.
It's going to be even weirder when we get HKTs and both Iterator, Option, and Result all wind up implementing `fmap`, since the `flat_map` and `and_then` definitions have been stable for ages.
Of course, I would never do this for a library. But Clippy is not used as a dependency, so it should be relatively safe, unless I am missing something? And it would considerably improve user friendliness to be able to run clippy on "stable" :)
My advise: commit and be patient. You rarely can just jump into any significant project and make contributions. First pick a project that you find interesting. It will take some time to become a contributor, but the reward is going to be that you'll learn a lot about the given project and it's domain, so pick the project you especially want to learn about. Then find an interactive communication channel for a given project (IRC/gitter) and hang around there to get a feel of it. Start going through the code and asking people for help whenever there's something you can't understand (just don't be too lazy about it - try to understand everything yourself first, read docs etc. but if you fail for longer than 15 minutes, unblock yourself by asking for explanation). After a while you will know how things work, you will learn what are other devs working on, what are the project pain points, and then you will know where can you help. 
&gt; Is there a workaround? It's more likely clearer to express `parse` as a factory function or conversion method that produces `Foo` structures. A conventional pattern in the standard library is to put the implementation in `from` and `from_$other_type` methods and automatically derive methods like `into`, `str::parse`, and `collect` from those. 
Structs with references into themselves are very hard to use in Rust, because if you move the struct the references might be invalidated and that makes the borrow checker very nervous. An easy workaround is just to store indices instead of actual slices into the data (i.e. `groups: Vec&lt;Range&lt;usize&gt;&gt;`). Also the owning-ref crate might have something for you.
... may I ask why?
Wow is it just me or does the code look really elegant? I was using MVC in PHP and NodeJS so I have some comparisments. I dont really think MVC works too well for Rust, it a good idea but in reality very hard. When I was coding on a PHP Framework I kept having issues between Controllers and Models. Sometimes one has to do dirty hacks (proxy controller in Model) and alike. I dont completely remember the use-case, just that there was no workaround and it was a pain in the eye to see in business apps you have a hard time really respecting the MVC rules. Back to the topic, I love seeing people do this stuff with Rust now and I love to read Rust code. I hope it will succeed. Maybe I will follow this topic closer and throw in some of my experience.
More accurately, ensure your libraries work on stable. Feel free to use nightly locally to get early access to performance improvements and things like clippy. Nightly is _not_ just for compiler devs. Just don't publish nightly-only libraries (except for dev-dependencies)
It panics with "a valid number", which isn't nearly as clear. You can manually prefix your string with "expected ", but the code remains pretty cryptic, in my opinion. I suspect that if you asked unfamiliar users to explain what a piece of code does, the average correct answer would be a lot quicker for `optional_thingy.or_fail(msg)` than for `optional_thingy.expect(msg)`.
Google translate (or rather Google Randomize): Great, we have joined the MVC test cases, hope the new sqlite, MySQL example, I was like a gun :)
Yes, I'm doing something in a similar fashion now. I moved the parse function out of the struct implementation: pub fn parse&lt;'a&gt;(data: &amp;'a [u8]) -&gt; Vec&lt;&amp;'a [u8]&gt; It would have been nice to let a struct do the file reading and parsing as one, but it seems to be too much trouble for too little reward. Thanks for your answer! 
Are you using the musl or glibc versions of the rust compiler? If you're trying to use musl, you should cross compile to it. Check out http://blog.rust-lang.org/2016/05/13/rustup.html
Interesting. Reflection has gotten a bad rep with Java devs because it's usually slow as molasses.
No need to jump to conclusions - 99% of the time when you see "1 comment" with no comments it's simply that the comment has been caught in the spam filter. Unfortunately us moderators are only human (as far as I am aware), so it takes us a little while to actually do moderation unlike our trusty bots.
Two things seem clear to me: - this is bad form - but it still should be defined as something safe, if not completely expected. In C and C++, `=` associates right-to-left, allowing you to chain `a = b = c;` Rust doesn't because it takes a cue from the ML tradition and assignment evaluates to `()`. The order of association is still RtL. Thus, since evaluation walks the parse tree, the evaluation is also RtL, leading to the esoteric gotcha that using `=` is different from [spelling it out explicitly](https://is.gd/TT3ROc). I seem to remember some discussion of this, but it definitely depends on MIR being hammered out so I'm sure it was tabled.
Linux has a filename limit of [255 bytes](https://unix.stackexchange.com/questions/32795/what-is-the-maximum-allowed-filename-and-folder-size-with-ecryptfs/182632#182632) and a path limit of 4096 bytes. Your filename has 280 bytes: echo -n fn.cv_calib3d_cv_solvePnPRansac_InputArray_objectPoints_InputArray_imagePoints_InputArray_cameraMatrix_InputArray_distCoeffs_OutputArray_rvec_OutputArray_tvec_bool_useExtrinsicGuess_int_iterationsCount_float_reprojectionError_int_minInliersCount_OutputArray_inliers_int_flags.html|wc -c 280 Your path has 333 bytes: echo -n /home/benjamin/dev/opencv-rust/target/doc/opencv/sys/fn.cv_calib3d_cv_solvePnPRansac_InputArray_objectPoints_InputArray_imagePoints_InputArray_cameraMatrix_InputArray_distCoeffs_OutputArray_rvec_OutputArray_tvec_bool_useExtrinsicGuess_int_iterationsCount_float_reprojectionError_int_minInliersCount_OutputArray_inliers_int_flags.html|wc -c 333 That's why changing the directory wouldn't fix it. This seems like a design problem in rustdoc. It should abbreviate long filenames, and keep track of each abbreviated filename when linking. I suggest you file a bug on Github.
I doubt that will happen. Iterator::flat_map has a different signature from Option::and_then in non-parameteric ways. In general, there's no way that is obvious to me to define Monad::bind that works with the external iterators that we have. There could be a definition of Monad for types which are both IntoIterator and FromIterator, which will eagerly allocate.
Thanks. That's unfortunate, perhaps I can get doc to skip the internal wrapper methods for now. Edit: Issue has been opened: https://github.com/rust-lang/rust/issues/34023
Would it be possible to perform the two derefs first, yielding two normal references, do the check using those references, and then perform the buffer copy before returning to the caller? I guess that wouldn't work if the API call that does the validation is different from the one that performs the copy.
I think it depends on the comparison. If you compare the performance impact of reflection with static method invocation or dynamically dispatched methods, you are right. (I'm not even going to compare with static dispatch of Rust). I my two main issues with reflection are: - The API is not very enjoyable to use. It is filled with lots of different checked exceptions - It is really easy to abuse and implement a magical feature in an awful way with bad performance In jOOQ's case, the `fetchInto` family of methods get a class as a parameter and through reflection, find a way to create an instance of that class (either a constructor annotated with `@ConstructorProperties` or an empty constructor with JPA annotations on fields) and load the fields with data. Since this information can easily be cached, I assume it won't cause any performance issues after the first discovery of the class since it will just be constructor and method invocation.
How does sapper performance compare to popular frameworks? The docs have [benchmarks results](https://github.com/sappworks/sapper#basic-benchmark) but doesn't compare them to anything.
rough translation: Wonderful, added into MVC test examples. I hope sqlite, MySQL support and examples will be added in the future. Liked it, I am gun (probably username or twitter handle) 
&gt; And that's fine if you're already a knowledgeable Rust programmer As a knowledgeable Rust programmer, I'm just going to chime in and disagree with that! Missing documentation, whether its API docs, an overview or just simple examples, really makes using a library much harder. With experience, I can usually flail around a bit to figure things out, but it takes longer and usually leaves me wondering whether I'm using the library correctly or not. I'm chiming in because I really disagree with the idea that docs are somehow only for beginners. They definitely aren't! 
Was [this thread](https://www.reddit.com/r/rust/comments/4grfi3/argument_evaluation_order/d2k35fi) wrong when it concluded that evaluation was LtR?
Nice to see Tera getting some use! I think lots of people (including me, already started actually) are going to work on their web framework in Rust now that hyper has async. The req/sec seems a bit low though, on hyper master I get 90k/s while the benchmark for sapper show 35k/s
&gt; both Iterator, Option, and Result all wind up implementing fmap Hopefully it won't be `fmap`, in Haskell that's the Functor-originated version of `map` not the monadic flatmap (that'd be bind)
Can someone explain to me the overwhelming preference for the MIT license? I can certainly understand how people feel strongly about the GPL. But as far as non-copyleft licenses go, why does this one stand out? 
Ok, I tried having all dependencies compiled with musl. That didn't work.
Cheeky
This can do short read, so do not use this when you require exactly a fixed number of bytes, unless you also check the amount actually read.
Does the cruft collect as more code is added? Like, every time I add a crate, am I going to balloon up a megabyte or two?
Ah, that makes sense. I was under the impression that only shadow banned users still added to the comment count. Thanks for the clarification.
I don't necessarily need a circular dependency like above. Honestly I only really care if one of the two methods for creating a `Second` struct would work. Could you explain how using a `trait` would help?
Depends on Crate. If it is crate that statically links big external library then yes, it will "balloon up", but in most cases it will not as there will be only one instance of jemalloc. Of course you could link jemalloc dynamically and then save on binary size in exchange for need to have additional external library file or use system allocator.
Repeating what /u/zzyzzyxx said just in a different way: You're right that nothing goes on the heap unless you use a `Box`/`Vec`/etc. The way things work here is very similar to how they work in C and C++. When you pass something into a function, or return something back out, Rust will copy the bits that make up that value. The compiler might optimize away that copy if it can, but the meaning remains the same. So the really interesting question becomes, why is it _safe_ for Rust to do that? For example, passing a `Vec` into a function means that I'm duplicating the pointer/capacity/length triplet inside of it. Does that mean that I'm eventually going to double free the memory behind that pointer? (Or could I maybe create two `&amp;mut` references and cause a data race?) This is where we get into Rust's "move semantics". In general when you make a copy of a value by moving it, the original just *disappears*. Code that comes after isn't allowed to use the original value at all, and equally important, its destructor is never called. This is what we mean when we say that "ownership" has moved. The exception here is `Copy` types, like `i32`. That trait means "this is plain old data, so it's safe to use the original even after you move it." Without this, using integers would be a huge pain. But a type can't be `Copy` if it has a destructor.
Some repositories have issues tagged "Mentored" or something to that effect. These are generally very easy first time issues, where the repository collaborators are willing to spend some time helping you along the way by giving hints or assistance. I think [clippy](http://github.com/manishearth/rust-clippy) does this, I also do it for [clap](http://github.com/kbknapp/clap-rs) (all Easy tagged issues I'm willing to mentor as well), and there are others out there too. If all else fails simply ask the repo maintainers what to work on and how.
Ah, I see. It would be interesting to see what hello world in C would be with static linking. I can live with a half megabyte if that's all it is and it makes the binary more portable. I'd rather not depend on an external library that is who knows what version with who knows what bugs or back doors included.
That's my reasoning as well - coming from Java background where we usually declare our test methods to `throws Exception` for a similar reason.
That's the beauty of abstractions, though: it doesn't matter what you call them. If I can't recognize a monad when it's called something else, then perhaps I'm not as savvy as I'd like.
thank you for attention. my plan is also to make sapper as easy as possible, from design and api.
latter let me make a bench comparation.
sapper hasn't release 0.1 now. there are many places to optimize. in the early stage, we focus on designs. thanks.
Ruby programmers tend to like small files, and hate IDEs. A single counter example might not invalidate a rule, but still...
One of the best things to do when you get complex, hard to debug errors like this, is to try and pare it down into the smallest possible example that reproduces the problem, eliminating all of your own code that is irrelevant to reproducing the problem. If you do that, you will frequently either remove one piece and it will start working, which makes it obvious where the problem was. Or you'll get down to a small enough piece that you can figure out what the issue is. Or, you will have removed all of your code other than that which reproduces the problem, and you will have a small, self-contained example that you can post and ask questions about, as now it doesn't contain any of your private code. Once you've posted that, other people can try it out on their systems, debug, or just have everything in front of them to be able to help out. As it is, it's a bit hard to say as you just have a few snippets of your toml and linker errors.
Ok, I'll do that. Thanks, I didn't know that's what people were thinking.
This is the second in my "How do I X in Rust?" Criticism and feedback are always welcome. As an fyi I may or may not be able to post over the next two weeks due to National Guard commitments, but if I can find time I will definitely create a new post!
So here is yet another question about lifetimes. I'm learning Rust by making a small Discord bot. For this, I also needed a database, and figured I could take it as an opportunity to learn sqlite as well, using [rust-sqlite3](https://github.com/dckc/rust-sqlite3). [Here is the file in its entirety](https://gist.github.com/Vakz/5bb4ef9dbded1fefb2010658c450beee). The issue I am having is on row 58. The commented code: match self.conn.prepare("SELECT * FROM Files WHERE user=$1 AND name=$2") { Ok(mut s) =&gt; match s.query(&amp;[&amp;user, &amp;file], to_file) { Ok(mut results) =&gt; { match results.nth(0) { Some(r) =&gt; Some(r.unwrap()), None =&gt; None } }, Err(_) =&gt; None }, Err(_) =&gt; return None } compiles just fine. The uncommented code, let mut stmt = match self.conn.prepare("SELECT * FROM Files WHERE user=$1 AND name=$2") { Ok(mut s) =&gt; s, Err(_) =&gt; return None }; match stmt.query(&amp;[&amp;user, &amp;file], to_file) { Ok(mut results) =&gt; { match results.nth(0) { Some(r) =&gt; Some(r.unwrap()), None =&gt; None } }, Err(_) =&gt; None } results in this error: src/database.rs:76:15: 76:19 error: `stmt` does not live long enough ... src/database.rs:47:79: 85:6 note: reference must be valid for the destruction scope surrounding block at 47:78... ... src/database.rs:75:11: 85:6 note: ...but borrowed value is only valid for the block suffix following statement 1 at 75:10 I cannot for the life of me figure out why this is..
It's `ok_or` (on `Option`) and it either gives you `Ok(val_in_option)` or `Err(failure_here)`. I've never counted this among the confusingly-named methods.
Can you point some out? Does the ORM not handle the sanitation? (I only skimmed the code)?
I don't believe so, no. Aside from the usual stdlib API stabilizations, I think most of the language-level work has gone into MIR and its various surrounding concerns. I wouldn't expect many new language-level features (especially big ones, like type-level numerics) until the fallout from MIR has been sorted out.
It would be worth mentioning that you can provide a string argument to `assert!` that will print out if the assertion fails. 
It would be cool to use your router! macro (or something like it) with other frameworks. Much better syntax than passing string literals.
&gt; . Notice how you can pass any two types there? The macro is able to take a variety of different inputs unlike a function in Rust which has to be given a type signature. This allows us to test for all kinds of different types of equalities using one macro! Can you imagine having to test for equality with a whole bunch of functions designed for specific types? This isn't the reason it's a macro, you could just as easily have `fn assert_eq&lt;T: Eq&gt;(T,T)` I think the reason it's a macro is so that it can internally apply deref coercions, which assert_eq can't do without needing extra `&amp;`s in the arguments.
In the `assert_eq!` example, there are typo : you write `assert eq!`
Regarding other podcasts that are out there, rustyrad.io is the other one that comes to mind. It's not nearly as regular, but it does have a nice format of in depth interview with a member of the community that is working on an interesting project. Reminds me of the episode OP did with Sean Griffen of diesel fame.
Looks like a bug related to `Drop` implementations involving references and implicitly returning a value at the end of a method. See [here](https://github.com/rust-lang/rust/issues/22252) and [here](https://github.com/rust-lang/rust/issues/31439) (both of which seem erroneously closed to me). Apparently the workaround is to either use an explicit return or assign the result to a variable and return the variable at the end of the method. Separately, if you're going to discard errors I think you may gain some readability if you use `if let` or `map` or `and_then`. Something like self.conn.prepare(...) .and_then(|mut stmt| stmt.query(...)) .ok() .and_then(|mut results| results.nth(0)) .and_then(|r| r.ok())
Lifetimes, yet again. How would one do something like [this](https://is.gd/wpxOf8) without compiler errors? I would like to add references to the `Vec` in `A` after the instance of `A` has been created. I know *why* it's not possible, but how can I work around the lifetime issue without resorting to `Rc`? I suppose [this](https://is.gd/LSziHb) is an option, but it requires a separate struct and indirection. Is there something nicer?
I am almost 100% that the answer would still be: depends. I think (not tested, so I can be wrong) that: - binaries by default are linked with jemalloc (obviously) - `rlib`s aren't as they do not need to be, they use whatever `alloc` end-user would choose - `dylib`s should be (by default), as they need to use some allocator - static libs are the same story as `rlib`s (which are static libs with metadata AFAIK)
&gt; ...trying to support generics... See [`parse-generics-shim`](https://github.com/DanielKeep/rust-parse-generics). That will parse a subset of normal generics syntax in stable, and give you everything after the generic list for further processing.
Hmm I find the explanations not so satisfying. I'm looking for the 'why or when would I use this macro' not just 'how does this macro work'. Eg. `assert_eq!(a, b)` you don't explain why you want to use this over `assert!(a == b)` (I believe it is because it can show pretty error messages with the actual values that weren't equal, not just 'these values weren't equal'). `panic!()` isn't about "deallocating all resources gracefully and exiting your program." but ties in the error handling model Rust uses, with `panic!()` being the primary entry point for invoking panics (duh). But why is it a macro? Is it so it can pretend to be like `format!()` when generating the panic message? That's the part that's interesting. This should link to an in depth explanation of Rust's error handling model. It's not `print!()` that's special, it's `println!()` which wraps `print!()` and [appends a `"\n"`](https://doc.rust-lang.org/src/std/up/src/libstd/macros.rs.html#118-121). They also use buffered output which flushes on newline. Nothing magical about it. You can make `print!()` flush by simply adding a `"\n"`. This can lead to annoying situations where you print and read input from stdin. Imho it should use C semantics where reading from tty stdin automatically flushes stdout. Here's an [issue](https://github.com/rust-lang/rust/issues/23818) I found. `try!()` is essentially, unwrap a `Result` and return it if it contains an `Err` value (performing error conversions as needed), otherwise use the unwrapped `Ok` value. See the Rust book's chapter on this topic. The point of `unimplemented!()` is that it allows your program to typeck while you're working on it. Eg. in places where you'd otherwise be forced to return a valid value. As others have mentioned `unreachable!()` doesn't hint the optimizer in release builds, it just panics. Its purpose is to indicate that you (the programmer) know a certain part of the code is unreachable but the compiler cannot infer this on its own. Without it your program might not typeck (eg you handle a certain match arm in another place so you know these cases are not reachable here. Without `unreachable!()` you'd be forced to handle these cases meaningfully). Your example shows this pretty well. For `vec!` you can use `vec!(..)`, `vec![..]` or `vec!{..}`. In fact [you can choose any brackets type](https://is.gd/jrYe7w) you want for any macro call. `println![..]` works just fine! _Which one you use_ should follow convention for that specific macro. About `write!()` I'd find it more useful to talk about its relationship to `std::io::Write` trait. How it allows you to format bytes to any type which implements it, including `Vec&lt;u8&gt;`. ## TL;DR In conclusion, I find it more interesting to talk about the _why_ than the _how_.
The short answer is no. If you're holding a `&amp;B` somewhere, no one else can mutate that `B`. You either have (exclusive) mutable access, or you have an (shared) immutable view, never both at the same time. If you're viewing the data, no one else can change it. This is normally referred to as Rust's *aliasing rules*. If you want to track changes that someone else makes to a piece of data, you need to use `Rc` or `Weak` or `Arc`, or reinvent one of those constructs yourself using `unsafe`. 
&gt; Is there any production ready finished project? No. When compiled with proper options, rust programs can be debugged with gdb (just like C programs). However, the little understanding that gdb has of the rust language makes some features... uncomfortable. [rust-gdb](https://michaelwoerister.github.io/2015/03/27/rust-xxdb.html) is a smart printer to at least show variables in a more readable way. Apart from that, you still get all of gdb's features, albeit in a non-super-intuitive UI. You may also be interested in [rusti](https://github.com/murarth/rusti), a REPL for rust.
Would be awesome if Rust was shipping with "rustdb" along with rustc, rustdoc and cargo, because debugging with print statements, albeit pretty, can only get you so far. A "rustdb" would be my second most requested feature after of course faster compile times. 
You can use pub_restricted which currently only works on nightly behind a feature gate: //lib.rs #![feature(pub_restricted)] mod first; mod second; //first.rs pub struct First { pub(super::second) id: i32 } //second.rs pub struct Second { id_from: i32 } impl Second { pub fn from_first(f: &amp;super::first::First) -&gt; Second { Second { id_from: f.id } } }
You thought correct, thanks!
Submodules can access private members of their parents, so if you had the `First` definition in mymod/mod.rs and `Second` in mymod/second.rs it would be able to access `First`'s private members. This is what tests do. One note: `use some_mod::*` will not import private members, even if the importing module has access to them.
Ah, TIL!
Visual Studio Code's gdb extension works really well with Rust. It can do pretty mich everything you might want it to be able to do. The local variables panel is slightly broken when it comes to Rust code though.
Only a reddit lurker, but I had to comment since the author's Java knowledge appears to be outdated by several years. Java has first class functions (aka lambdas) now. Also, if you want performant, _scalable_ web-apps, frameworks like spring web and the play framework kick Ruby on Rail's backside any day. If you want top notch performance, use vertx. Besides, you can use languages like Ruby and Clojure on the JVM if you _really_ want the polyglot experience in a single project (*shudders*).
If I remember correctly the RustDT project (plugin to support for Rust on Eclipse) do that too.
Ahhhh awesome it'll be included in the update!
This was [cross-posted to Stack Overflow](http://stackoverflow.com/q/37586216/155423).
First public release. Works at a basic level, but I'm still learning Rust so progress is slow. Any constructive criticism or suggestions are welcome!
Ah okay. My understanding was wrong. Thanks for the input I'll make sure to update accordingly!
Ahhhh really? Good to know thank you!
Noted thanks for pointing that out!
Alright, I made an example that exactly replicates the issue.