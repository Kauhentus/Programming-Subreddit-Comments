I'm still not that experienced so I'm not sure if I have a concrete answer, but I've been thinking of it as "robust" libraries that can perform a reasonably comprehensive range of tasks effectively e.g. compared to C++?
I'm pretty sure the rate of job openings is much higher -- I hear about Rust job openings all the time, both in SF and outside. They're just not posting on reddit, which makes sense because it's not immediately clear if it's allowed. A job thread would be nice to have and might make that clearer. But there are definitely more Rust job openings than what you see on reddit.
Anchored paths seem cleaner to me as well. Uniform paths seem harder to understand when reading code.
If you enable `#![feature(external_doc)]` you can keep the docs in a separate Markdown file, but I don't know when that's going to stabilize.
I don't live in the U.S either. That's the point - I had to search hard, but I did realise that even where I live, there were Rust jobs, but we had to find out through word of mouth, trawling forums, inquiring with places which didn't list it explicitly, etc. The whole idea is to have a pinned recurring thread for such opportunities, which may also incentivize companies to start posting opportunities here.
Thanks for the clarification. 
What I did was use a #[no_std] environment, emit the LLVM IR with `-Clto`, then compile it to C using `llvm-cbe` (doesn't work for too complex objects), then compile it to 65816 using `tcc-816`. The last step required quite a bit of pre-processing and so I scrapped the idea; in the future, I'll likely reuse this approach for the Genesis, if there won't be native support for the m68k.
Also a beginner but I think this is where you need to heap allocate vs stack allocate? So either move to an owned string or something like Box&lt;str&gt;?
I have to admit I still prefer the way things currently are, except for the `crate::` prefix. I wish there were some kind of "unambiuous mode" or lint you could enable that would mean * every import is absolute, or explicitly relative via `self` or `super`. * every namespace that is used is actually `use`d and doesn't just appear in the prelude.
Record::name should either be `&amp;'[some lifetime] str` or `String`
If you limit yourself to only using mature *libraries*, how many things can you just not do because no one's written the library for it? Rust is a new language and there's plenty of gaps (as is to be expected for a new language).
This tileset on Open Game Art: https://opengameart.org/content/zelda-like-tilesets-and-sprites It's a wonderful tileset that's in the public domain!
This tileset on Open Game Art: https://opengameart.org/content/zelda-like-tilesets-and-sprites It's a wonderful tileset that's in the public domain!
So this does compile -- but now I have these annotated things all over. Not pretty -- :). I'm not really clear why this works though -- how does this solve the compile time issue of not knowing the size before runtime? struct Record &lt;'a&gt; {name: &amp;'a str} struct Example &lt;'a&gt;{ records : Vec&lt;Record&lt;'a&gt;&gt; } impl &lt;'a&gt; Example&lt;'a&gt; { fn new(self, records: Vec&lt;Record&gt;) -&gt; Example { Example{records} } }
That happens, it's just a matter of familiarity. :-)
Which libraries? It's a very broad question, I've never had any problems with that. What exactly you want to do? Tools have purpose, no Rust isn't ready for everything, but it's getting there and to most things it already is.
You should rephrase the question, what is your goal/scenario that you want to run? Is it an some sort of embedded system, web server, other? 
`str` is a string slice of indeterminate length the same way `[T]` is an array of indeterminate length. `&amp;'a str` is a fat pointer, so it carries the length with it, and it itself has a defined length.
You can use rust modules with ``` /*! inner block commoents */ ``` to emulate markdown files. In one of my crates I write the documentation entirely in Rust modules so that it can be read from docs.rs or cargo doc.
&gt; C++ in terms of easily findable, high quality, beginner documentation. There are *far* more good books on C++ than Rust. Which is natural, given how new Rust is. &gt; C++ you really need a mentor to sift through all the legacy and point you in the right direction. Why? &gt; C++ doesn't need to be hard or painful, but a lot of the old stuff is. If OP knows C, that covers most of the "old" and "painful" stuff.
Since we are talking about maturity of libraries. &amp;#x200B; Would you guys be interested in paying for access to a "mature ecosystem". &amp;#x200B; I am thinking about a scheme where to provide: 1) Documentation, low and high level documentation behind a paywall 2) Support for libraries. 3) Releases on best practise &amp;#x200B; So I am thinking about an entity that will take charge of some libraries and write documentation for it and provide it under a paywall. Then it will fix bugs, makes libraries "unsafe free" and "panic free" and provide "rolling releases" under paywall. i.e.: Free everything but the last minor releases, suppose we manage libfoo-v1.2.3 and libfoo-v2.0.1 the release of libfoo-v1.3.0 will be paywalled while making available the previous libfoo-v1.2.3 and keeping libfoo-v2.0.1 under paywall (different major release). After 6 months also libfoo-v1.3.0 will become free no matter what. (Patch releases will follow the same policy of the minor release.) People with access to the paywall content will have access also to a github/gitlab repo of the code where to explore the code, open issues, etc &amp;#x200B; Then I am immagining a plan for single-developer/hacker that would be quite affordable 5EUR/month (?) and one for companies that will be more expensive. &amp;#x200B; Is this something interesting? What libraries you will like to include in such fictional program? &amp;#x200B;
Most people would just use String, why do you want str and not String?
If :: is analogous to / in filesystem paths, then ::foo looks a lot like /foo. .:: would be better in this regard, though I'm sure the aesthetics of that would upset some people.
&gt; Why is the following statement resulting in a move of my PriorityQueue (pq)? It's because `front()` is missing the &amp; from self: ``` fn front(self) -&gt; Option&lt;i32&gt; { ``` Taking the `self` argument as a value (without the ampersand) means the value will be moved into the function. Same applies to "normal" arguments too. This effectively consumes the value when the method is called and will destroy your priority queue during the first iteration. So the solution is to change the definition of `front` to include `&amp;`.
I got a solution working, although it's clunky, and I think very inefficient for large CSV's. &amp;#x200B; I sample up to 100 fields of a CSV, and apply a regex to check whether the fields match certain conditions, in a specific order (boolean, timestamp, double, integer, string). I'll share the code once I put it on Github.
&gt; Even when compared to the C# world, which has been around for a long time and heavily pushed by Microsoft, i think the Rust ecosystem is better. I doubt it, not by a long shot. The C# ecosystem is *huge*, like the Java and C ones. Any other languages pale in comparison, including Rust, C++, D, Nim, Go... I would say two that are getting very close (for some domains at least) are Python and JS, but that's about it.
As I mentioned in another reply, I think Rust is getting there, but it'll be a little while longer. One issue I always turn to is the lack of a standard way, or at least generally agreed upon way, to propagate errors. There are things in the pipeline to improve that, but I'm not aware if there's more consensus then there was a few months ago when I last looked. It's by far not a perfect analogy, but I kindof feel that Rust today is where python was at v1.5 or so. That's my opinion from someone who has only dabbled in it though. 
It just needs CI and ongoing maintenance effort - what the LLVM developers care about is that the code gets fixed quickly when something in the core LLVM code is changed and the build breaks in these backends. Just keeping up with what LLVM is up to doesn't take that much effort.
I don't see why this is an argument against the language of Lua, but why has it bitten you?
&gt; C++ you really need a mentor to sift through all the legacy and point you in the right direction. Just get Stroustrup's books. After that there is the C++ book list for more in depth. I cant wait for Rust to have the amount of books C++ has.
&gt; Rust is intricate, but at least it has extraordinary learning resources and tooling. I support learning Rust before C++. This is simply not true. The Rust book is good, but for experienced programmer's. Same with Programming Rust and Rust In Action. An actual beginner will have a much easier time with C++ thanks to books like those written by Stroustrup, which so far have no match in the Rust world. 
Main question is whether Record actually owns the string (String) or borrows it from somewhere else (&amp;str). Because borrow will require explicit lifetime annotation *everywhere* (`&lt;'s&gt;`) it might be easier to just use String.
Because the system allocator is a better default, and if you want to use `jemalloc`, you can do so by just adding `jemallocator` as a dependency.
It was the simplest breakdown of a problem I encountered I could think of. Didn’t want to confuse my issue w additional code that likely has its own isolated issues. :). Im doing the ‘write a raytracer in a weekend’ to help learn rust and encountered this issue. 
This is explained in issue [#36963](https://github.com/rust-lang/rust/issues/36963), in the second set of bullet points (near the top).
Thanks for reminding. I've written that comment because I haven't found how to do this until now. You have to define a feature in `Cargo.toml` (e.g. `doc`) and put this in the crate root: ```rust #![cfg_attr(feature = "doc", feature(external_doc))] ``` And then you can use the `doc` attribute like this: ```rust #![cfg_attr(feature = "doc", doc(...))] ```
Wouldn’t this cause a performance regression across the whole rust-ecosystem? 
Gratz! Will this have an effect compilation speed?
I would look at it from comprehensiveness, reliability, documentation, and stability. On comprehensiveness, Rust seems to weirdly have most things. For instance, when I wanted to load up ply files and las files, I found amazing libraries for this compared to most languages. You can also find random web API crates for even some obscure things. It also has all the building blocks to do most tasks. One of the areas Rust is weak in currently is machine learning, and I hope const generics being introduced into the language will allow us to create better abstractions for machine learning. Rust is also weak at photogrammetry currently (pretty specific, but something I work with). Reliability is very high in Rust. I expect libraries to work correctly and expose typed and documented errors for problems that can go wrong. Even in a small library made by one person I usually find high reliability, and code people write is usually forced by the API constraints of each new library to do things correctly. I have encountered some bugs in libraries, but they are significantly more rare than what I am used to in C++. Documentation is one of the things Rust excells at. I regularly open up documentation for python modules and node packages and get absolutely no help and am forced to read through the codebase. In Rust I almost always see documentation even in the smallest and underdeveloped of libraries. Even if somebody doesn't document, docs.rs will provide you with a nice interface to browse their types and methods and you can usually figure out how to use a simple API with no documentation without looking at the code because the names, types, and trait bounds usually make it obvious. It's also important to note that example code in documentation is also kept up to date because cargo tries to build it. Stability is where Rust is weaker than many other languages. While many libraries are "1.0" and have stable APIs, many are not. For instance the `rand` crate that many rely on just bumped its version. Every time they update their API they make great changes and better their abstractions, which I appreciate, but this does mean we haven't settled as a community on a standard. Rust's situation is much better than it was just a year ago. Currently, one of the major problems is that async coding is not present in stable Rust. This means that crates cant yet standardize on the standard async infrastructure. Within the next few months it should be available in stable, but it may also take crate maintainers another month or so before everything is up to date. It is important to note that many of the important crates in terms of abstractions have been stabilized (like `serde`), so this alleviates the problem to some degree because so long as you interact with the abstractions and not the backends (like serialization formats for `serde`), then you will not encounter significant issues with instability (though some of your crate's users might). Hopefully that gives you a good idea about the ecosystem. I am vadix on the Rust discord if you have any questions about crates in the Rust ecosystem (and I am sure others there would be super happy to answer questions about that as well). 
That's only if you assume that jemalloc is universally faster than the system allocator for every possible program, which isn't necessarily the case. Plus a C hello world is 1) not going to be doing much allocation and 2) is also going to be using the system allocator, so idk why you'd expect a rust version to regress by comparison.
C also uses the system allocator by default.
If your hello world is dependent on memory allocator performance, you're doing it wrong. It does, on the other hand, cut the size of hello world in half, which is a more common beginner's question.
My code had UB that only crashed with jemmaloc, with the system allocator it happened silently. I only found it when I disabled the system-allocator. So this may make finding UB harder.
As a FreeBSD user I welcome this, because my system allocator is already jemalloc :P
I once saw someone confused about why their rust hello world was taking up way more memory than the C one too, and the answer was because jemalloc reserved a bunch of memory up front for its allocation arenas or whatever. Which makes complete sense once you know what's going on, but was initially puzzling to a casual observer who wasn't aware that their rust binary was carrying around a custom allocator. 
Is there any way to make things like this work on docs.rs? Or do you have to self-host your own docs?
I wrongly assumed rust uses jemalloc __on all platforms__ by default for performance reasons. After reading through issue the change makes more sense.
&gt; Why my hello world is so slow compared to C? C doesn't use the system allocator. What hello world programs are you writing where memory allocation is a substantial bottleneck?
`valgrind` is the best way to catch UB. jemmaloc probably just *happened* to crash your program.
Of course, it's not a way to catch UB, it's just that malloc tends to hide it because otherwise a shitton of software would endup crashing, so being quiet is a solution when you are the standard in the C environment.
I don't think anyone should have to pay for access to libraries. This is nonsense. Setting up donations for crate authors, on the other hand, is a better idea.
Out of curiosity what was the causing the UB in your application?
I'm really confused by all this. Does this mean I'll need to reinstall it completely it or does Stream take care of it with automatic updates?
Total noob question: what effect will this have on programs compiled for Windows? 
Ok, I look at it and is a bit old and not maintained? I found another implementation: https://saulius.github.io/croaring-rs/croaring/index.html 
This is the rust programming language and has nothing to do with the game. That's r/playrust I believe. This doesn't affect the game at all.
Lol this subreddit is talking about the Rust programming language, not the game. You're looking for /r/playrust.
If I'm not mistaken, jemalloc was not used on windows so it should't have any impact.
Again, thanks for the detailed reply. I think you're right, but to detail my thoughts: &gt; If the insertion happened in a group of size 15, we now made a full group so the average probe length has increased. This means means we should have grown the hashtable. It's not obvious that it's true that this means we want to grow the hash table. Growing a hash table means every access (*sans linear traversals) become [a constant amount](https://arxiv.org/abs/1212.0703) slower. So we want to put it off as much as possible, unless the costs are too much. In this case the costs seem pretty small from a naïve point of view. 1. You only take a speed penalty on a failed lookup. 2. This only ever directly extends this chain from length-1 to length-2. What I realized after your post is that it can also merge chains, so it's possible at this stage, where you'd expect a significant number of full chains, that you're bloating the chain length by significantly more on average. Which I think kills the idea. 
This has nothing to do with the the video game Rust. This entire post is about a change in the programming language Rust. You’re probably looking for /r/play_rust
This ain't it chief.
I think ya may be lost, bud. this subreddit is for the rust programming language. 
On the other hand such systems would provide better libraries to all and at the same time it would advance the ecosystem much faster than donation to authors. Also companies would have many incentives to buy in...
Name checks out
As a matter of principle, I do everything I can to *avoid* endorsing paywall schemes, implicitly or explicitly, to the point of making sure to never mention or link to parties engaged in such activities. I also have no interest in libraries with GPL-incompatible licenses... and the GPL is pretty much specifically designed to undermine paywalling with its protections against what the [Debian Free Software Guidelines](https://en.wikipedia.org/wiki/Tentacles_of_evil_test) refer to as the "Tentacles of Evil test". Now, I don't inherently consider "early access" to be a paywall scheme, but I only consider it acceptable as a "freemium" model in cases like Patreon-supported webcomics, where the early access is on the order of a week or less and there are no security implications to the patches being held back. (ie. Where it's less a paywall and more "paying for DVD extras".)
/r/lostredditors
No, the compiler itself still uses jemalloc.
What do you mean it doesn’t? On Unix platforms, what Rust calls the system allocator is `malloc`.
Companies would have no incentive to buy in. They would use whatever is open and free at the time. Companies in general care less about the quality of the software they use, and more about how much they have to pay to get a minimum viable product.
I concur. However, I do hope that things speed up a bit!
Can you back this up? I'd quite surprised to see an allocator \_deliberately\_hiding broken usage of its API, and hence covering up UB which probably has security implications .
Background: I haven't really worked in embedded or "systems" programming and am working my way through v2 of the rust book. The chapter on concurrency mentions that rust eschews green threads because keeping the runtime very small is a hard requirement (presumably so it can be used in embedded or resource-constrained applications?). My question is: are there platforms or applications where you'd want to spawn heavy OS threads, but where you also need a very small binary? It seems like these are two separate domains, but that may just reflect my background and experience. The book also mentions green threads are available as a library: what libraries should I check out? And if you have experience in Haskell or Go, does rust lose anything by not having green threads in the runtime? 
Absolutely not. Software development is moving _away_ from such models; I would have no interest in using or contributing to such a scheme.
Seemed like a very good suggestion [I got timings down -16% for the header and -20% on the bulk](https://www.rubdos.be/embroidery/embroidery-formats-rs/criterion/dst/fold-many0/report/). The second one needed a `fold_many0_till`, which I had to "hand craft", but was easy enough.
I don't know what company you work for but we will pay a lot of money for software (and libraries) as well with support. Especially if it is something that will be completing open after a small time frame and with a dedicated team of support...
Also there lacks a good channel for non-English speaking companies to post job openings. I've heard there are some Chinese companies having been looking for Rust developers for long, while their recruiters might not be comfortable with contacting in English or so.
And running valgrind on Rust or Rust/C code just became much easier with this change.
The big difference between the old green threads runtime and the current runtime is that, with the current runtime, you explicitly opt-in to every cost. You don't *have* to spawn threads whereas I think the old runtime would automatically spawn as many threads as there were cores in the machine (or start with one and scale with the number of coroutines). The current design philosophy of Rust is that every cost is explicit. You manually spawn threads, you explicitly allocate objects on the heap, etc. The green threads runtime was antithetical to this. The current *de facto* green threads runtime is [Tokio](https://tokio.rs/) building on [the `futures` API](https://crates.io/crates/futures). It's a bit lower level than you might be used to coming from Haskell or Go but you have much more control over the runtime as well.
Comment out the GLOBAL_ALLOC variable and it will segfault. With it it doesn't https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=91734996c40fb208e2e35e6a67401593
At least with `glibc` you can opt in to stricter heap checking at runtime with an environment variable. 
&gt;I am thinking about an entity that will take charge of some libraries and write documentation for it and provide it ~~under a paywall.~~ for free, openly, with community involvement That's a reasonable description of a subset of the Rust team's duties. As far as more general libraries, many popular libraries are run by very dedicated and hard working teams with excellent documentation and support. &gt;makes libraries "unsafe free" and "panic free" There have been myriad discussions recently on /r/rust around what "unsafe free" and "panic free" mean, but to sum it up: * There are many problems which must be implemented with (well tested, highly scrutinized) unsafe code in order to be performant, including many parts of the standard library * It is impossible to guarantee panic-free code. This goes well beyond eliminating `unwrap` and array indexing. Many libraries will and should panic when an invariant is broken or some other unrecoverable error is encountered This is all kind of besides the point, however, because I do not think that an Oracle-esque cabal in control of key aspects of the Rust ecosystem can mean anything positive in the long term; a pay-for-play system feels downright antithetical to an open source project run by a company deeply rooted in open source.
Well actually the make Vs buy is a common dilemma in all fields. Ford is not making all the mechanical pieces it needs, why our field should be different?
Great, but whats the effect on linking? Rust is normally statically linked AFAIK, is that the same for the system allocator?
Yes most software development today is sponsored by big corp like Mozilla, Google, Microsoft, etc... Are we sure we want to relie on them in the future? Some months of delay to access the open new version of a great libraries that otherwise wouldn't exist is so much scarier than leaving the open source development to big corps that use it only for their mere bottom line? Please answer and let me know your thoughts. But please consider who is developing any system you are using. Who is developing Linux? Who android? Firefox? Rust itself? How many great and useful libraries have never received contribution from people working in big corporations? Should we repeat the openssl fiasco?
&gt; One problem I see with it is that it becomes another thing you need to remember while creating new crate if you want it to be performant. Only for leaf binary crates. Library crates don't need to care about this at all since its up to the final binary to choose the allocator. 
This field does not involve tangible components like mechanical pieces. There are no material or labor costs to fetching dependencies from the Internet, and then compiling everything into a binary. If Ford could magically obtain free components to replace higher quality non-free ones, you bet they'd do exactly that. Companies aren't going to be enthused about their programmers requesting money for a subscription model to access Rust libraries.
I'm not sure I understand your point. Your argument seems to both assert that a "mature ecosystem" can only be created by companies which are paid for the software, but also that we should be scared of the Big Corporations who are backing Rust. You can't have it both ways. I'm not interested in playing 20 questions with you, especially as I don't think you've successfully outlined any tangible benefits from this walled garden idea. 1. The Rust ecosystem already has a heap of [awesome](https://github.com/rust-unofficial/awesome-rust) libraries and tools. I'm not convinced that `hyper`, `ripgrep`/`regex`, `diesel`, `actix`, `rustc`, or many others would be better if they were behind a paywall. 2. Big Corporations are already using Rust, and even [releasing their work](https://github.com/dropbox/divans) back to the community 3. What you're describing is not libraries that "have ever received contribution from people working in big corporations", you're positing libraries which are solely controlled by Big Corporations; where the latest major version is gated behind a paywall 4. I fail to see how keeping source in-house behind a paywall would reduce the chance of security vulnerability a-la Debian/openssl. The last thing I want is having to trust a closed source crypto library where the same "solution" could be implemented but never discovered due to only a handful of eyes working on the code. It's clear that there's a fundamental, philosophical difference here. If there's this vast, untapped market for paid closed source libraries out there, by all means whip up a startup and collect your winnings! However, I think it's pretty clear from the replies on here that without a hefty amount of forethought you an expect a healthy amount of pushback from the community.
I'm working my way through the rust koans at https://github.com/crazymykl/rust-koans I've been programming in C++ for 27 years; maybe this is more of a liability than an asset in some cases. In particular, my experience with references in C++ is apparently interfering with my ability to understand references in Rust. I have read the O'Reilly Rust book twice, and I've particularly read the chapters on ownership and references a total of four times, but I'm still missing something. In the array koans, I am presented with these two problems, which both lead me to the same question: // Arrays can be iterated over. #[test] fn array_iteration() { let arr: [u8; 3] = [3, 2, 1]; let mut iterator = arr.iter(); assert!(iterator.next().unwrap() == &amp;__); assert!(iterator.next().unwrap() == &amp;__); assert!(iterator.next().unwrap() == &amp;__); } and // You can filter an array for results that match a given condition #[test] fn array_filter() { let arr: [u16; 5] = [1, 2, 3, 4, 5]; let mut iterator = arr.iter().filter(__); assert!(iterator.next().unwrap() == &amp;2); assert!(iterator.next().unwrap() == &amp;4); assert!(iterator.next().is_none()); } I'm trying to limit myself to filling in the blanks of the problems as posed instead of making any other changes that make the tests pass. Note that at this point in the koans, references, ownership, iterators, Option, and unwrap haven't even been introduced. So I only know that iterator().next().unwrap() actually returns &amp;u8 (in the first problem) thanks to my IDE and my reading outside of the koans. I know that I need to compare a &amp;u8 to something. In both cases, I don't understand why it would be necessary to take a reference to anything on the right hand side of the comparisons, rather than *de*-reference something on the left hand side. I was able to get the first test case to pass by filling in the blanks like this: assert!(iterator.next().unwrap() == &amp;3); assert!(iterator.next().unwrap() == &amp;2); assert!(iterator.next().unwrap() == &amp;1); But from my C++ background I don't understand why in the world I would want to take a reference to the number 3 (what does that even mean?) just for the sake of matching the "reference-ness" of the left-hand-side with the right-hand-side. I often warn junior C++ programmers not to throw * and &amp; at things until it compiles if they don't really know what they're doing. I want to follow my own advice here. I never would have even considered solving the problem this way if it weren't for the example set by the (later) problem "You can filter an array...", whose pre-written code already contained the same construct. What makes sense to me for lines like this would be: assert!(*(iterator.next().unwrap()) == 3); But that would entail changing the koan code as given to me, so I guess I still have something to learn. Can you help me understand how taking a reference to an integer literal works in this case? Thank you. 
I'm pretty sure the windows-gnu toolchain uses jemalloc
Somewhat unrelated question, but now something that's enabled by this change. I'm writing rust primarily for AWS Lambdas. I've been thinking about how I can change the allocator to optimize for this. I don't really ever need to 'free' memory, so long as I stay under the limit, and I can probably just preallocate the max memory amount right off the bat and not worry about resizing. Anyone have thoughts on a good allocator approach to this? I don't care too much about it because it's almost certainly not a bottleneck, just something I've been thinking about.
honestly at this point you might even want to write your own allocator.
Re-reading Chapter 5 of *Programming Rust* one more time, I see "Comparing References" on page 99, which gives this example: let x = 10; let y = 10; let rx = &amp;x; let ry = &amp;y; let rrx = &amp;rx; let rry = &amp;ry; assert!(x == y); assert!(rx == ry); assert!(rrx == rry); and says that the comparison operators "see through" the dereferences, as long as both operands "have the same type". I incorrectly assumed that the "same type" requirement was imposed after, not before, the dereference collapsing. I guess I should have noticed that assert!(rx == 10); does not appear in their examples, nor do any other comparisons between variables with different levels of indirection. Then on the next page, in the section "Borrowing References to Arbitrary Expressions", I see that an expression like "&amp;10" creates a reference to an implicit temporary variable whose lifetime is that of the expression it appears in. Putting these two together, that explains why I can compare assert!(iterator.next().unwrap() == &amp;3); The "&amp;3" creates a reference to a temporary variable, with the same degree of indirection as the left hand side (&amp;u8), so the comparison compiles. Is that idiomatic Rust? Or would idiomatic Rust de-reference on the left hand side and simply compare to an integer literal?
Browsing termion's docs I stumbled upon this peculiar enum: ``` /// A key. #[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)] pub enum Key { /// Backspace. Backspace, /// Left arrow. Left, /// Right arrow. Right, /// Up arrow. Up, /// Down arrow. Down, /// Home key. Home, /// End key. End, /// Page Up key. PageUp, /// Page Down key. PageDown, /// Delete key. Delete, /// Insert key. Insert, /// Function keys. /// /// Only function keys 1 through 12 are supported. F(u8), /// Normal character. Char(char), /// Alt modified character. Alt(char), /// Ctrl modified character. /// /// Note that certain keys may not be modifiable with `ctrl`, due to limitations of terminals. Ctrl(char), /// Null byte. Null, /// Esc key. Esc, #[doc(hidden)] __IsNotComplete, } ``` The part I find confusing is the `__IsNotComplete` thing. What is the purpose of such a construct? From the name seems it is not intended to be used, but why on Earth would I want an unusable variant of an enum? Doesn't it kind of robs you of ability to rely on the compiler to check pattern matches for exaustiveness, forcing you into adding a `_ =&gt; ...` arm? The only thing I can think of is that adding new variants won't break library user's code, but isn't it what SemVer is for?
libc-provided malloc() and friends. 
Rust links against libc? Huh, i thought it was independent.
'Amazing' but the bloat is kind of awful. Between cargo and maven i'll take cargo anytime. Those libraries and artifacts which pull 30.000 'dependencies' which are optional and you then have to blacklist in the ivysettings.xml (ofc) reminds me in a very bad way of badly done debian packages. In a way it's a blessing in disguise that rust prefers static linking.... pulling log4j and some bridge libraries should *not* pull 3 different kinds of databases and eclipse libraries, but here we are.
It does. There's no real reason not to, honestly.
don't know which bridge log4j do you use, but in 10 years I have never had to do something like that with mature libraries. Optional dependencies are quite rare. A more common problem (mainly on less mature libraries) is when you need to specify the specific version you want to import (because two libraries require the same library with different versions), but it is quite rare with mature and mantained libraries. Even more, the problem is not related to the package system, but with the JVM itself (which cannot import the different versions of the same library). 
The C runtime is pretty much the everything runtime on Linux. I highly doubt you can build anything resembling a functional Linux system without a libc. 
&gt; Perhaps, it's just really weird to think of the C language runtime as part of the operating system Historically it’s very much the opposite. For most OS, and very much so most Unices, the « base system » includes the standard library and that is the only supported way of interacting with the kernel. Linux stands out as shipping the kernel and the rest of the system completely separately and thus supporting raw syscalks as a proper interface, but that’s infeasible on windows (literally, syscall numbers move around all the time), risky on OSX (as Go was reminded several times when their raw syscalls blew up because Apple altered the abi, it’s also why you *can not* statically link libSystem) and unsupported on BSDs, or Solaris/Illumos or what have you. 
The last functional version of alpha port for llvm is 5-6 years old, and even 5-6 years ago it was unmaintained for few years already.
They're normally dynamically linked, as part of libc.
I wish a llvm backend for MIPS (not micromips, the original, psp and ps2) existed. Plenty of projects go 'c89' for 'portability' to terrible platforms like that that are stuck on gcc 3 or some barbarity like that. TBH, this is probably because MIPS is kind of barbaric (i has a 'delay slot' for registers or somesuch) and the hardware is hard to make a functional api of.
Thank you! I had not realized that a self argument by value was possible - and managed to repeat the error twice! I can see how this would prove a useful capability for e.g. terminally closing a resource like a smart pointer or file descriptor.
I did my very best to be the second accepted submission of the aliencodebreaker challenge, but so far to no avail: [https://github.com/Byron/kattis-aliencodebreaker](https://github.com/Byron/kattis-aliencodebreaker) . On my computer and with the latest Rust compiler, I seem to be coming in in the required time, unfortunately the judges computer can't reproduce it yet. For now I am hitting a dead-end, as I see no other opportunity for parallelization.
I did my very best to be the second accepted submission of the aliencodebreaker challenge, but so far to no avail: [https://github.com/Byron/kattis-aliencodebreaker](https://github.com/Byron/kattis-aliencodebreaker) . On my computer and with the latest Rust compiler, I seem to be coming in in the required time, unfortunately the judges computer can't reproduce it yet. For now I am hitting a dead-end, as I see no other opportunity for parallelization.
Your comment implies that the cost of components is mostly material. I would argue that your assumption is at least questionable in all the industries but the commodize one. Otherwise R&amp;D department simply wouldn't exists in any field... Then I would like you to re-read either my proposal or your comment. I never suggest to make libraries accessible under a subscription model. I suggest to make documentation available under a subscription model (and I would bet that any company would very much prefer to pay a negligible amount of money that waste employee time on reading bad blog post on the internet) and to make the very last minor version of a libraries available under the same subscription model of the documentation (the one that any company would buy anyway). And again I am pretty sure that any rational company would prefer this deal than badly re-implement all those libraries.
At very least it is a drawback. 64bit integers are everywhere, in our case it was being used for timestamps and general ids. We ended up working around it but it was always a nuisance.
Your comment implies that the cost of components is mostly material. I would argue that your assumption is at least questionable in all the industries but the commodize one. Otherwise R&amp;D department simply wouldn't exists in any field... Then I would like you to re-read either my proposal or your comment. I never suggest to make libraries accessible under a subscription model. I suggest to make documentation available under a subscription model (and I would bet that any company would very much prefer to pay a negligible amount of money that waste employee time on reading bad blog post on the internet) and to make the very last minor version of a libraries available under the same subscription model of the documentation (the one that any company would buy anyway). And again I am pretty sure that any rational company would prefer this deal than badly re-implement all those libraries.
It is never been tried?
&gt; The only thing I can think of is that adding new variants won't break library user's code, but isn't it what SemVer is for? That's exactly what this is for. It allows the author to add variants in some sort of backwards-compatible fashion because otherwise it would be a major version bump every time. It's purely a matter of opinion which is preferable; in this case the author appears to have chosen the former. There's also the unstable [`#[non_exhaustive]`](https://github.com/rust-lang/rfcs/pull/2008) which will do the same thing but a bit more elegantly. I wouldn't get too hung up on it. It's not used everywhere. You weren't planning on handling every variant of that enum anyway, right?
Do you start a new Rust process on every Lambda invocation? If not, you do need to free memory. If so, maybe creating a single Rust process on first invocation and re-using it is a better optimisation.
On `x86_64-unknown-linux-gnu` for example, glibc was and is dynamically linked (regardless of which allocator is used, since `std` also uses it for other things) whereas the version jemalloc shipped with the standard library’s `alloc_jemalloc` was statically linked. Since there is now strictly less work for the linker to do, I suppose there must be small improvement in linking time. However this is likely to small to make a practical difference. The reduction is probably much important in code size, for small programs.
Reviewing [AWS Lambda pricing](https://aws.amazon.com/lambda/pricing/), it seems like this could be a pretty interesting optimization. Profile against different allocators, including one that preallocates based on an environment variable or just never frees, whichever mode delivers faster performance.
I agree that it anonymous sum and product types should be designed together. I'd go further and say you should be able to take a sum type and invert it to a product type, concatenate lists of types, delete a type from a list of types and so forth. &amp;#x200B; I have the feeling Rust doesn't really have the tools to do the type-level logic that you are likely to want but I'm new to Rust so I don't know (associated types?). &amp;#x200B; Disclaimer - I spent a fair bit of time on writing a programming language in which the only way to construct types was with anonymous sums and products plus a newtype-like wrapper thing. I worked out the type-system stuff but lost interest in making it an actual functional programming language :)
No, there’s no sign of `exe_allocation_crate` in https://github.com/rust-lang/rust/blob/1.30.0/src/librustc_target/spec/x86_64_pc_windows_gnu.rs or https://github.com/rust-lang/rust/blob/1.30.0/src/librustc_target/spec/windows_base.rs. (Compare with https://github.com/rust-lang/rust/blob/1.30.0/src/librustc_target/spec/linux_base.rs#L39. It says `maybe` because jemalloc could be force-disabled when compiling rustc.)
&gt; If the C++ library uses new, does it use the same allocator Rust does? If you build it as a dynamic library it will just use the normal `malloc` symbols and those will be resolved dynamically. You can set the global allocator in Rust to the system allocator in stable using `#[global_alloc]` and then it will be used for Rust as well. Otherwise, Rust might be compiled with jemalloc, in which case depending on which symbols are generated might result in Rust using a different allocator than the dynamic C++ library. In any case, the only situation in which this would matter is if you were allocating memory in one language and freeing it in the other which is something that you probably want to avoid anyways.
`env!("CARGO_MANIFEST_DIR")` is the directory containing your `Cargo.toml`.
I agree that it anonymous sum and product types should be designed together. I'd go further and say you should be able to take a sum type and invert it to a product type, concatenate lists of types, delete a type from a list of types and so forth. &amp;#x200B; I have the feeling Rust doesn't really have the tools to do the type-level logic that you are likely to want but I'm new to Rust so I don't know (associated types?). &amp;#x200B; Disclaimer - I spent a fair bit of time on writing a programming language in which the only way to construct types was with anonymous sums and products plus a newtype-like wrapper thing. I worked out the type-system stuff but lost interest in making it an actual functional programming language :)
&gt;I'm also curious about mobile support, is that planned? &amp;#x200B; Me too
Awesome! Thanks for the explanation!
Yes we could have a network of developer paid by the user for their libraries, it is not a crazy idea... Like people being paid for their work... 1. Of the one you mentioned only ripgrep (which is not commonly used as a library from what I know) and diesel are not backed by a big corporations paying developers. I would love to hear the thoughts of the diesel maintainer on the possibility to work full time on its work while being paid... 2. And 3 I believe are just off points or I haven't understood them. 4. Again paying people from their work will actually allow them to work on it full time. OpenSSL was developed by a single guy with basically it's own money... We were just criminals on putting the safety of people in such model. Again, I am not suggesting to develop closed source software. I am suggesting to make access to that software delayed or behind a payment. How this would be worse than what is now? Basically all the mainteners of any successful open source project complain about the same thing. It is start to be a work and a chore and they are not even paid for it. It is quite easy to just follow the mass and downvote, actually it is a little harder to raise a family on wishful thinking and fake internet points. Moreover I believe that the atmosphere is such that nobody will proclaim itself pro this initiative. Finally time is my scarse resource, I want to be the one paying for it, not the one building it...
Well, I suppose it depends on which strand of history you follow. I understand that the Unix world has always been very C-oriented, but that certainly was not the case for other systems. The Linux approach has always seemed much more normal to me than the "every language pretends to be C" strategy used on other unices.
I just mean that everything will work fine because c and rust are using the same allocator - but you'll suddenly start getting strange errors if you change the allocator. Just needs stressing that allocation and deallocation need to be in the same context to newbies like I was 😁.
You can certainly build a functional Linux *executable* without a libc if you have some other way of invoking syscalls. This is the same reason you have your choice of multiple libc implementations when building programs written in C or C derivatives.
Thanks! I was more worried about the performance implications of two allocators running at the same time but I suppose for most cases the impact is negligible.
Exactly. It's also what you should (usually) use when your type implements Copy - small values are faster to copy than pass by reference.
No fix on the "humans not reading the sidebar" bug either, I see. This subreddit has nothing to do with the video game.
Database connections (for SQL databases at least) are stateful, it's not possible to share one between multiple concurrent requests. Normally you'd draw connections from a pool when you need them and put them back as soon as you're done.
They were asking for proof of the claim that malloc deliberately goes out of its way to permit undefined behavior.
Several hundred KB, according to https://lifthrasiir.github.io/rustlog/why-is-a-rust-executable-large.html
As a quick note, I believe [pikkr](https://crates.io/crates/pikkr) is the currently the fastest Rust JSON parser. And yes, a lot of people are still interested in fast JSON.
So now how can I enable it in a crate so that I will use jemallocator, but also support old rust version at the same time?
The thing is, UB means anything can happen, so anything is indeed acceptable. The point is that the system allocator silently ignores the UB instead of crashing. So it can be not noticed.
Thank you for explaning this. Could u share a simple example how to draw connection from the pool and put back ? Thank you :) 
But why? Why don’t they just switch to a binary format that lends itself to faster parsing?
I'm really not attacking anyone and I'm pretty detached from the issue personally, but wouldn't it be better to continue making jemalloc the default and add global_allocator? It makes sense to switch eventually, but especially seeing as Rust 2018 is so close and that would be a pretty clean time to do it, it seems worth allowing `global_allocator` but still defaulting to jemalloc, right?
It just happens that your UB was ignored (or had no *apparent* effect) with the system allocator, while causing a crash with jemalloc. It could have easily gone the other way. You'd have to dig into the allocator to actually figure out how it failed in each case. That might be interesting, but of course the real solution is to just fix your UB.
Of course it could, but this is a extremely trivial example, freeing a null pointer. It smells like intentional. Because it's a fairly trivial situation. But yes we need more evidence of that. I was just showing a case in wich things would be different.
Because binary format changes break things, whereas changes in JSON tends to not.
Yes, for every invocation the Rust process is restarted, or started from a snapshot. This is all handled on the AWS end with only a bit of control on my own.
Yeah, I think I can find some fairly representative benchmarks to demonstrate if this will have a positive impact on my code.
Because they have ot interface to some existing json protocoled thing.
&gt; basic level of maturity We're writing kernels, system services, web services, desktop applications, and entire operating systems in Rust. I think we're beyond a basic level at this point. In some ways, it is more mature than C and C++, thanks due to the comprehensive standard library, the Cargo build system, and `cargo doc` / Docs.rs. Much less re-inventing the wheel here.
Self-describing, self-explanatory, human-readable, human-editable, debuggable, widely understood, good enough, fast enough
I confess, I hadn't even clicked on your example -- being lazy on my phone. But `free(NULL)` is specified to do nothing, even for jemalloc. However, `Box` will have `nonnull` attributes on its LLVM IR, so codegen may still do weird things with that. It could just be that the support code wrapping the two allocators is different enough for that UB to have different effects. But I'm hand-waving now...
Are there any allocators written entirely in Rust? I'd like to write an extremely naive allocator just for the fun of it.
Neat. Do report back on this sub!
RemindMe! 10 days
I will be messaging you on [**2018-11-14 00:50:37 UTC**](http://www.wolframalpha.com/input/?i=2018-11-14 00:50:37 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/rust/comments/9qd9cc/writing_an_os_in_rust_hardware_interrupts/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/rust/comments/9qd9cc/writing_an_os_in_rust_hardware_interrupts/]%0A%0ARemindMe! 10 days) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
This would work better if you just licensed your software such that it could only be used royalty free for non-commercial purposes. I don't personally like such libraries, but that is much more acceptable and better for contributors in the community and still makes money. Don't restrict access to the individual hacker and the student, just take royalties from commercial products being sold with it.
You're probably looking for "r2d2" for connection pooling, which can be used with Diesel: https://docs.diesel.rs/diesel/r2d2/index.html If you follow the links there, you'll find plenty of examples. :)
oh sorry
FYI your formatting is broken. Only on the new reddit redesign are code blocks delimited by ``` supported. You have to indent it with 4 spaces to ensure compatibility with the old reddit (which is very popular). Example: ![cfg_attr(feature = "doc", feature(external_doc))]
Wow, I just started working on this exact thing about a week ago. I'll definitely check out your project. https://github.com/JoshMcguigan/cargo-run-script
You might get a little extra heap fragmentation, at a guess. But as you said, it usually isn't a big deal.
What do wasm targets use as an allocate memory at runtime?
Yes, exactly that - a bump allocator was what I had in mind, actually, but was curious about any other approaches (or confirmation that bump allocation was what I'm after). It seems like this may work well for my use case.
If you want stable, you can use the new procedural macros in 1.30.0 for this if you're willing to write the code that embeds the doc yourself. I've written a POC and it does work. My only concern is that there is no stable way to get the path of the current file in procedural macros yet (i.e. `Span`), so everything must be done relative to the current working directory of the compiler (which seems to be the root of the crate it's compiling). I also haven't managed to find a statement in the official docs stated that the current working directory of rustc is the root of the crate, which makes depending on the cwd unstable on stable rust. But it's doable, and it works.
&gt; Why don’t they just switch to a binary format Because you don't own both sides of the pipeline? Or because the data being sent has a very unstable format, and so anything that isn't self-describing is unusable. At least the applications I've worked with and seen don't spend most of their time parsing JSON, but I'd use a faster parser if it doesn't cost me much compared to serde. ---- You made a post suggesting you have a faster JSON parser then made two comments that sound like you look down on anyone who uses JSON or don't understand why anyone would. I'm just confused honestly.
Isnt stuff like that why you're never supposed to free memory that someone else allocated, though?
Your post doubled up due to Reddit derps. Just to let you know. 
I’m just playing devil’s advocate. I’m still surprised that it’s that common for it in performance critical situations, but based on the replies I guess it is
Isn't this the definition of boilerplate tho? You just chose another way to boilerplate it?
Fair, message pack is basically the same structures as JSON but binary.
By default, a port of dlmalloc. You can also use wee_alloc to save a few kb
Thank you very much :) &amp;#x200B;
Thank you :) &amp;#x200B;
Yeah, to make any progress on vulkano I've had to dig through the examples exhaustively, and then straight up go to the source.
https://www.reddit.com/r/rust/comments/9twam5/jemalloc_was_just_removed_from_the_standard/ With the next nightly (which will become 1.32, I think?) it no longer uses jemalloc by default on any platform.
Principle of least surprise. You shouldn't be pushed into a 3rd party allocator by default just by choosing Rust as your language. Choosing the right allocator depends on your workload. This is probably a boon to smaller programs that don't do much allocation, and bigger programs that already minimize their allocations using object pools. Also, the system allocator is easier to debug, as there's less "magic" going on underneath. A custom allocator isn't quite a runtime, but jemalloc may hide your allocator abuse (and that may be a reason to turn it on)!
And, perhaps you’re saying this already, but way faster to parse
I know people here generally have a huge bias against Windows, but... The Windows heap allocator intentionally avoids certain correct optimizations, because there are so many broken apps that have been on the marketplace for years, if not decades. This is well-documented; see Raymond Chen's blog, for example. As a purist, I hate it, because Microsoft cannot improve certain aspects of their products, and not because of something that is their fault, without going through a lot of pain. 
You're making a huge assumption that this is a performance regression. Unless you've measured the difference for your specific workload, you just don't know that this is a significant effect. And if doing perf analysis had taught me anything, it's that even the best engineers cannot guess the right result more than 10% of the time.
&gt; Rust is also weak at photogrammetry currently (pretty specific, but something I work with). Hey, me too! :) The ply support crates have been alright, but in my use-case I had to write a custom parser as I was processing ply files ranging from 40-150GB in size(binary format) and even with 128GB of RAM, that was easy to blow through, so I wrote a streaming parser for the processing needs I had(splitting the ply into chunks for baking which used 8x RAM vs filesize), that way only a small amount of RAM was needed at any given moment :) Might get around to releasing a crate for it some day, I also want to write an oct-tree crate as I didn't see any decent ones for my needs in Rust yet, which again need some special/niche features/handling for some extra optimizations I want to build(much better approach for the baking then the simple splitting approach, as well as being a good foundation for decimation/remeshing and favouring the CPU cache for speed).
That's interesting, are you sure? I always assumed they just put the process to sleep between requests. Wouldn't that mean you'd have to reconnect to databases, etc for every request?
Yep. You do have to reconnect to databases for every request.
By tomorrow's nightly you mean \`1.32.0-nightly (04fdb44f5 2018-11-03)\` or the one that is going to be marked as \`2018-11-03\`? &amp;#x200B;
Rather than using `Iterator&lt;Item = T&gt;` as a bound, I think it's more flexible to use `IntoIterator&lt;Item = T&gt;`. That way you can accept a `Vec` directly, rather than calling one of the iteration methods on it first. Also, probably left over from an older draft, the first code snippet has `struct Thing` where I think it's meant to have `struct Person`.
Awesome! I will be excited to give it a watch after its uploaded 😊.
Hi! I’m trying to do some memoization in Rust and I’m running afoul of the borrow checker. ``` fn calc\_deps(cache: &amp;mut HashMap&lt;String,String&gt;, key: &amp;String) —&gt; String { if let Some(v) = cache.get(key) { v } else { let r = /* computations */ cache.insert(key.clone(),r.clone()); r } } ``` I’m told that cache is borrowed twice. Why is it a problem if I’m done with get by the time I get to insert? Is there a way for me to encode that information? 
&gt; If you're in Auckland and don't want to be spoiled, don't look! Scrolling through the posts on r/rust, saw that in bold. Is that Auckland, NZ? I think I remember there being another Auckland somewhere in the world, I'd be a bit surprised if it were my city!
Yep. The process is snapshot'd and restarted before the handler executes, I believe.
&gt; But in programming, you usually only have one to work with. Not necessarily, there are many different categories that are relevant for programming. For example, categories for substructural arrows in addition to the typical unrestricted function arrows are quite useful. Having all sorts of substructural categories allows various optimizations, increased correctness guarantees, more free theorems, and make some code feasible to write that would have been a night mare to maintain before. Using various forms of partial isomorphisms and other bidirectional categories can also be quite useful. Even for endofunctors, the functor laws are still useful reasonable principles for manipulating data under data structures. The functor identity law provides the useful guarantee that mapping only does not touch anything but the As (in set/dcpo-like categories). The functor category compose law provides a useful fusion optimization. &gt; (If you can even call it that..... Hask is not a category, after all. Haskellers like to pretend it is, but it's all loose reasoning by analogy). Hask not being a category is a huge wort in Haskell don’t disagree there, but I don’t see how this can be a valid argument against the idea of using categories as an abstraction in programming. One can imagine a Haskell-like language where `seq` is constrained such that it cannot be called on functions and functions in such a language would follow the category laws.
&gt; But for programming and writing software which requires any kind of maintenance, fancy types have issues with instability under change in requirements. The reason for the endless stream of obnoxious dynamic languages and "stringly typed" libraries is partly due to the fact the program doesn't need to be re-written when you have to fudge things. (It's just likely half-broken instead!) And when you don't know the formal requirements for a piece of software, or you are uncertain about how permanent the requirements are, you don't want to commit too much thought and effort to formalizing it. Doesn’t gradual typing address this problem? One can imagine a language with a full spectrum gradual type system going all the way from fully dynamic types to full dependent types.
Yeap it's Auckland NZ, here's the meetup link: [https://www.meetup.com/rust-akl/](https://www.meetup.com/rust-akl/) and slack channel: [https://rust-akl.slack.com/messages/CCC7KUXMY/](https://rust-akl.slack.com/messages/CCC7KUXMY/) &amp;#x200B;
A stripped hello world with stable rust is 414.5KiB large The nightly that just landed produces a stripped hello world of 194.3KiB We got 220.2 KiB less on stripped hello world. Non-stripped stable rust hello-world is about 3.8 MiB Non-stripped nightly rust hello-world is about 2.3 MiB NOTE: in release mode
People who learned rust without a c background know none of these rules. 
That API function escaped me, thanks! I like that option better than switching to 2018 since I'm not the boss of the project I'd like to contribute to.
&gt;First of all, if you're having trouble with code, post the exact code you're having trouble with, not an approximation. That code has an invalid character in it, and an unrelated type error. &amp;#x200B; That's a fair point and I apologize for the oversight. For the record, here is the code I was actually looking at: use std::collections::HashMap; fn calc\_deps (cache: &amp;mut HashMap&lt;String,String&gt;, key: &amp;String) -&gt; String { if let Some(v) = cache.get(key).clone() { v.clone() } else { let r = String::from("foo bar"); cache.insert(key.clone(),r.clone()); r } } &gt;As an aside: code in triple backticks doesn't render correctly on old Reddit. If you want everyone to be able to read your code, use four space indents instead. I will keep that in mind. Thanks!
I tried a variation on the above code which I thought would pass the lexical borrowing rules but which doesn't. Any light shed on it would be very much appreciated. fn calc_deps (cache: &amp;mut HashMap&lt;String,String&gt;, key: &amp;String) -&gt; String { let x; { x = cache.get(key).clone() }; if let Some(v) = x { v.clone() } else { let r = String::from("foo bar"); cache.insert(key.clone(),r.clone()); r } } 
I'm interested in the question: If you're not using something like `cargo-cmd`, `cargo-script`, `cargo-run-script`, or just plain `cargo build`, how are you packaging build tools along with your crate? e.g. `make`, `build.sh`, `./x.py`.
&gt; (are you from NZ?) Yep! I live in Auckland, but have trouble getting into the programming industry with a lack of bachelor degree in comp sci. Only managed to work for a couple startups over the years via personal network, but each time went south(I did well but quit due to treatment)... sooo no good references either :\ Wow 100+ members! I'm not sure if I'll be able to make the next meetup as I'll be off to China for a few months soon, so I will probably have to wait until a meetup when I'm back next year. 
I anticipate that anyone who learned Rust without a C background is not in a position where they are interoperating so closely with C that they find themselves in need of manually managing memory that spans FFI boundaries, and if they are doing such a thing, that would seem to already imply that they should acquire familiarity with C before proceeding for a host of other reasons.
It is not enabled by default with the Windows GNU toolchain, but I can attest to the fact that manually enabling it is objectively a performance booster in essentially all cases.
One thing I'd note is that the slides by themselves are very light on any information. Will the talk be recorded? 
In the majority of cases, yes. Even on Windows, when using the GNU toolchain. It's as highly regarded a library as it is for a very good reason...
Shouldnt it be using AtomicUsize instead of UnsafeCell?
Here there is a struct for serialize/deserialize with Serde: [https://github.com/softprops/openapi/blob/master/src/v2/schema.rs#L170](https://github.com/softprops/openapi/blob/master/src/v2/schema.rs#L170) And here there is an enum where the struct is defined again: [https://github.com/softprops/openapi/blob/master/src/v2/schema.rs#L202](https://github.com/softprops/openapi/blob/master/src/v2/schema.rs#L202) I've tried to reference the struct outside the enum inside the enum instead of copy/paste it but I dont know how to do that. Is it necessary to copy paste it?
The best strategy is probably to put something nasty by default, such as signalling NaN in float fields, if possible. In Rust, specifically, though, one generally avoids mut variables and it's statically impossible to forget to initialize a variable, and idiomatic Rust avoids mut values, which precludes initialization as default.
Variants are not types. The usual way of handling this is to write something like this in the enum: Parameter(Parameter), 
Also, ecosystem includes tooling to my mind. And so, it includes Visual Studio. Sorry, but VS Code with RLS + Racer is *nothing* like as good as the C# VS experience. And I'm a Rust fan.
That's exactly what it is. The biggest upside in the npm world is that it provides you with some convention, which is good good for newcommers to a project: - You always know where to look for those commands instead for searching for a build script: in your `package.json` - There are some commonly used names such as `build` and `test`, so you can almost always assume that you can run a npm project with `npm run build` It's probably not as needed with cargo, since cargo already has a builtin `cargo build` and `cargo test` (as the scope of cargo is a bit bigger than the one of npm).
I'm personally not convinced Rust (or even C++, for that matter) is a good beginners' language, or that it should be advertised as such. Both languages make certain decisions for the user (e.g. execution speed vs development speed, flexibility vs. simplicity, catching errors early vs. getting something half-working quickly) that frankly don't matter to beginners. To start off, they're better off with languages like Python or Haskell.
In practice that's only really true for a small subset of changes to JSON.
Maturity has nothing to do with how much people are paying for things.
You can't get the column as a slice, because a slice guarantees that all of its elements are contiguous. You're going to need to build it manually.
Can you put it on GitHub? There's not that much of a difference between GitLab and GitHub. It'd just help a lot for discoverability and cross-pollination of issues referencing eachother. Cheers and thanks for sharing.
If you don't have the luxury of working against a fixed schema, then the json crate is about twice as fast as json_serde. Also 100% safe Rust.
That is what happened; global allocators were added back in August, this will land in stable in January. We’re just farther along in the timeline than you think.
Depends on how old; the allocator API landed in 1.21. You could do it via a Cargo feature, I’d bet.
wee_alloc is
What should be excluded from crates using [the `exclude` fields in `Cargo.toml`](https://doc.rust-lang.org/cargo/reference/manifest.html#the-exclude-and-include-fields-optional)? I think anything that's related to CI should be excluded, but what about the `examples` folder? Or any documentation like a `README` or a `docs` folder of supplementary documentation? [The Manifest Format page for `cargo`](https://doc.rust-lang.org/cargo/reference/manifest.html#the-exclude-and-include-fields-optional) doesn't have any guidance on Best Practices here, so I wanted to see what the community take on this was.
&gt;So I wouldn't even pay the cost of the big allocation, in the majority case. Saving the time of a *single* allocation.. didn't think I'd hear this, ..ever:)
You might want to look at my spatial library im working on `space`. An early version is on crates.io currently. I had similar issues with the ply library, but I did appreciate its quality. It is easier to just use las files, but im currently more concerned about more spatially oriented formats like potree for obvious reasons regarding the memory limitations.
I think I'd generally recommend using `include` rather than `exclude` and including: - any rust source managed under the project (src, integration test, examples) - simple documentation (README.md, CONTRIBUTING.md, CHANGELOG.md, etc.) - LICENSE - Cargo.toml Definitely don't include: - images or binary data not used by cargo I've got no opinion on including documentation books nor on including full example projects. I mean I wouldn't do it, but it wouldn't harm anything either? At the end of the day though, the only thing you really don't want to include is extra images or binary data which could make the crate's packaged form significantly bigger. CI and stuff is best excluded, sure, but it doesn't really hurt. I think `exclude` makes it harder to remember to exclude new files added to the repository, but each to their own. For reference, I typically add include = [ "Cargo.toml", "src/**/*", "tests/**/*", "examples/**/*", "README.md", "CHANGELOG.md", "LICENSE", ] 
Great! Now go on and finally add classes so I can use it
Hmm, yes indeed! And dealing with alignment correctly becomes tricky, too.
This is awesome. Natural next step would be a walkthrough of a best practice specs game made in Amethyst ;)
[muon](https://crates.io/crates/muon) Is the only WSGI server I can find from a cursory search of crates.io, so you might investigate that.
The way to double check is to copy-paste that commit hash and make a URL like https://github.com/rust-lang/rust/commit/04fdb44f5. This shows the date *and time* of the merge commit for a given Nightly. As of this writing, GitHub says “12 hours ago”. Then go to near the bottom of the page for the relevant PR and find when it was merged. https://github.com/rust-lang/rust/pull/55238#issuecomment-435583059 shows “20 hours ago”. So yes, it looks like this change is included in Nigthly 04fdb44f5. Note however that there’s (usually) an off-by-one error between the date of the merge commit (shows in `rustc -V`) and the date of the Nigthly known to rustup. In this case, `nigthly-2018-11-04` is the toolchain name. More background: https://github.com/rust-lang/rust-central-station/pull/27 The output of running `rustup update` today shows both dates: info: latest update on 2018-11-04, rust version 1.32.0-nightly (04fdb44f5 2018-11-03) 
This is a real risk, but it’s not a sufficient reason to ship a different allocator with each Rust program by default.
Historically, I've used [just](https://github.com/casey/just) for build automation beyond `cargo build` but, recently, I've been considering a move to [cargo-make](https://sagiegurari.github.io/cargo-make/).
You best bet is probably [version detection](https://crates.io/crates/rustc-version) in a build script. The `#[global_allocator]` attribute was stabilized in 1.28.
&gt; Well, I suppose it depends on which strand of history you follow. I understand that the Unix world has always been very C-oriented Windows is not a unix. And shipping a kernel rather than an entire system is rather rare historically. If you’re shipping a *system*, there is very little reason or benefit to making the low-level syscalls an official interface. 
nice. one of the more frustrating experiences I've had in the rust community was a few overly arrogant people telling me, in response to my request for customisable allocation behaviour, that "jemalloc is all you'll ever need, AND our testing confirms this, so you're wrong.." .. as if their testing is going to apply to all use cases that have ever been, or ever will be in future. (I'm not doubting that jemalloc is a good default, I was just shocked at the arrogance for one person to assume their experience over-rides that of everyone else)
This is why context is always important. Software development is like a series of 'we will never need that' followed years later with 'oh, we need that' combined with 'obviously this will always be true' followed years later with 'but...now this isn't true'.
&gt; Why is it so important to you that a general purpose programming language be designed around the alleged needs of small closed source chinese banks? Because this isn't just one tiny company - closed source software in countries where English isn't the primary language is a huge market. People write proprietary software, and this removes a barrier for those people using rust. Given how many developers work on rust, adding this feature is very little effort. On the other hand, it opens up use of rust to a large number of people! Even if that usage isn't reflected in the open source realm, it's still legitimate usage, and beginners who use non-english languages being able to use variable named in their native language isn't too bad either.
I'm not suggesting you should - I'm just flagging it as something that affected me.
A common pattern to avoid the cost of spawning and destroying many threads, or the load of having many simultaneous threads, is to have a "thread pool" with a fixed number of long-running threads that take jobs from a queue. See https://crates.io/crates/threadpool for example for doing exactly this, or https://github.com/rayon-rs/rayon for a high-level library in this area. But synchronization across threads has a cost, which can be significant for such small jobs as a single step of this Fibonacci algorithm. Servo’s parallel style system batches the styling of multiple nodes into a single job, to reduce that overhead. 
Because your binaries are going to be smaller?
I didn't say that Rust doesn't auto vectorize code, it is just not that good at it for floats. This applies to the ECS and non ECS code. I know that there are fast floats, but they are only on nightly. I also haven't had much success with them. Unity's Burst compiler is able to generate extremely efficient code and it uses llvm behind the scenes. *Although the source code is not public and I am not sure if they have modified llvm themselves* That being said, if the ECS uses SoA and if there are no holes then you could technically generate very efficient code. I am not sure how useful vectorization is if you don't have vectorized loads. Maybe in Rust it would make more sense to have an explicit API like faster. 
&gt; I'm really not attacking anyone and I'm pretty detached from the issue personally, but wouldn't it be better to continue making jemalloc the default and add global_allocator? That happened 4 months ago.
I find it easier to click on the &lt;3 horizontal lines menu button&gt; at bot left, then click the &lt;four horizontal lines button with dots&gt; at the top left, then you can see the index of slides ...I understand that this is the common language of UI desgin for buttons these days...but man it feels wierd describing it like that...
*ASan is the best way
&gt; but I have heard that the library support in Rust isn't that mature yet since it's a fairly new language. I do use both Rust and C++ every day and consider myself proficient in both. In my personal experience, it is much much easier to use libraries in Rust than in C++. This makes such a big different that it feels at least like there are more Rust libraries that you can actually use than C++ ones, even if the C++ ones might have more features, which is not always the case. The Rust libraries also feel to have, in general, better documentation, and are better tested, such that the chances of you encountering issues while using them is much smaller. I've had to debug and fix segfaults and memory errors in pretty much every C++ library that I use, and I've almost never had to do that in Rust even though I regularly use more Rust libraries than C++ ones.
Update: I've made a list of dependencies available, description of [what they are used for](https://github.com/brycx/orion/wiki/Dependencies) and [how much unsafe code there is](https://github.com/brycx/orion/wiki/Security).
&gt; If you're making a distinction between primitives and high level interfaces, it's a little weird to provide unauthenticated encryption as a high level interface. This is fixed in the upcoming 0.9 release.
&gt; But with all the use of binary formats out there, is having an even faster JSON parser important to anyone? I'm trying to decide if it would be worth pursuing. JSON is not dead and it seems unlikely that it will die in the foreseeable future. As long as JSON is alive, people will need to parse it, and some people will have to parse it as fast as possible. 
pikkr is only fastest on data that's structured in a certain way; it's not the fastests in the general case. Still, it's a nice benchmark, and is immensely useful if your data fits the mold.
I think i'm in the same boat - i'm just used to dealing with horrible documentation from other things, so most of the rust crates i've used are amazing in comparison...
It's needed enough that the Cargo team wants to add it as a first-class feature. "it" being the feature itself, not this implementation.
I think you have more luck over on [r/playrust](https://reddit.com/r/playrust)
Wrapping it in {} didn't work - it looks like the problem is the `::`, no matter what they're wrapped in. [`Attribute::parse_meta`](https://docs.rs/syn/0.15/syn/struct.Attribute.html#method.parse_meta) is parsing it into [`Meta`](https://docs.rs/syn/0.15/syn/enum.Meta.html), which is - as you have said - rather restrictive. What I did was to parse the `.tts` myself (OK - not entirely myself. I did use `syn::parse`), and then I'm not limited by the attributes' restrictions.
Yes! :) I already get rid of `jemalloc` when compiling down 64k with `no_std` anyway. :)
&gt; You might want to look at my spatial library im working on space. An early version is on crates.io currently. It looks interesting :) Will keep it in mind! &gt; spatially oriented formats like potree for obvious reasons regarding the memory limitations. For viewing point clouds, I think sketchfab has been working on something for their viewer to scale really well. I'm not sure what your experience is with rendering 3D data, but there are some common techniques for optimizing performance/memory. View frustrum or was it occlusion culling I think is one, where you only need visible data being rendered in the scene(and to an extent you can keep some nearby segments/octants in a LRU cache keeping the rest on disk). Another is Level of Detail(LoD). Not sure how applicable it is for point clouds, I guess you'd have an equivalent way to filter out points like decimation on a mesh, to retain ones that contribute the most useful information. Then you can have an octant in the distance contain fewer points, loading in the others as you get closer. I remember an online viewer for point clouds did this(can't remember it's name or link unfortunately), the demo was a fairly decent aerial scan, you could see all the houses, but zoom down to a certain part which would then be more sparse but load in more points for that area/zoom level, seeing the house in more detail, ground, kids playground etc. Was pretty cool. It ran in the browser I think with webgl. Still, I don't quite get the interest in point clouds for viewing as an end product at least. You can retain plenty of detail information with textures on detailed meshes in real-time and efficiently, UE4 is really optimized well for it. Granted, that can involve a lot more work to process the point cloud to that point.
You’re right on both statements! I’ll update it, thanks!
Another, less verbose, more readable way.
That's very nice to hear. Great!
IMHO, these are for special situations. Best practice is to use the defaults.
ASan does not detect the majority of memory errors (like reads of uninitialized memory, returning references to the stack frame, etc.). 
The `jemalloc-sys` crate does not build on windows-gnu yet, so this just can't be the case (patches welcome though). 
I'm honestly surprised that this even comes up. JSON is, if anything, rapidly growing in use across industries, especially with the wholesale adoption of the REST API concept.
Don't think anyone's actively working on it. You can definitely build kernel modules with LLVM though.
https://github.com/thepowersgang/mrustc generates C code that AFAIK you can compile with GCC
There's [mrustc](https://github.com/thepowersgang/mrustc).
Did not know about this. Thank you. Might turn out to be useful in embedded projects where the only option is sometimes a GCC toolchain.
I would expect that the most common usage of JSON today is to power web-services. In web-services, the performance is not "critical" in the sense that the service is still viable even if slightly slower, yet performance is appreciated because: - higher performance means lower latency, especially important in chains, when it compounds. - higher performance means higher throughput, which means reduced hardware costs. There was a neat little post the other day about taking advantage of free hosting: a number of services such as AWS Lambda or Cloud Workers offer a free tier, if your service is fast enough you can host it without paying anything.
I've also faced this sort of opposition from rust enthusiasts before. I don't know what lends the rust community to exhibit this particular nonconstructive behavior. If they can't see how something is useful to them, they tend to treat it as an attack. Other communities tend to at least treat it as an intellectual curiosity. At least, this is the impression I get from my personal experiences. Good news is that things appear to be changing as the OP shows. Rust the language, its tooling, and the community that surrounds it are all maturing (though a bit slowly), so don't loose heart!
Ah never mind, we have `AtomicUsize::fetch_update` which makes this easy!
I'm a Rust noob. I thought a 2D array was a matrix. Are they different?
Nice article! I think this is good advice. I might add a bit more discussion on downsides though. In particular, one might consider introducing polymorphic APIs only if their complexity cost is worth saving the allocation. It many cases it is worth it, but not always, especially if you're already going to be doing work that will dwarf the cost of the extra copy and allocation.
GCC only recently released a D front-end, despite the age of the language. The main issue is that GCC's very architecture is adversary. GCC's architecture is driven by political goals, rather than technical ones: it was conceived in part by R. Stallman with the explicit goal of forcing the GPL on any code that would integrate with GCC. To this end, the IR layer of GCC is purposefully incomplete: you cannot, like in LLVM, have a front-end emit a textual representation of the IR and feed that into GCC and call it a day. Instead, there are explicit "callback" points that MUST be implemented for each language, which the GCC toolchain will use to further translate the IR down the road, requiring a front-end implementer to provide GPL-licensed sources. This is, of course, the very reason that most new languages would rather: - transpile to C, at the cost of losing source correspondance. - OR target LLVM, rather than GCC. --- This is a particular problem for Rust because the whole rustc compiler is dual-licensed MIT/Apache, and not GPL. On top of being in Rust. This means that a GCC front-end would require rewriting the Rust compiler in C (and some C++), and forever maintaining the C compiler, without any opportunity to reuse the existing parts of rustc. While it may be interesting, at some point, to have multiple competing compilers, this is a massive endeavor. --- Another possibility, therefore, is to transpile to C. There are some difficulties there, though it is technically feasible. rustc itself is already considering going the multi-backend roads, with a Cratelift backend, which should decouple it from LLVM IR, so that afterward adding a 3rd backend (targeting C) should be a smaller effort. Of course, as mentioned, you lose the assembly-to-source mapping. Or more specifically, debugging instructions will map to the emitted C source rather than the Rust source. --- A last possibility is to simply forget about GCC altogether until it cleans up its act (unlikely as it is) and go with either LLVM or Cratelift. A naive backend for either may not produce optimized assembly, but it should be simple enough to get you going, and can always be refined on the go. It's also interesting to note that interest in Systems Programming has been rekindled in the last years, and this renewed interest has led to LLVM sprouting new backends, with progress being made on AVR for example. --- From a cost point of view, I would rate the effort of those alternatives: - Cheap: C backend, at the cost of debugging experience. - Moderate: LLVM/Cratelife backend. - Very Expensive: GCC front-end. *Note: the C backend being the cheapest because emitting ANSI C means portability to a whole lot of architectures at once, so cost is amortized.*
I was confused by this too. I haven't seen most APIs agreeing with taking `&amp;str` and then allocating, actually the reverse, so it seemed like a strange recommendation. I'm not sure how best the section should be reworded though :(
A 2D array is an array of arrays. That means that you have an array where each element is also an array. In Rust you declare those with the syntax `[[T; M]; N]`. Where T is the type of item in your 2D array. If you decide that each inner array is a row, then N becomes the number of rows and M becomes the number of columns. You can also decide that each inner array is a column in which case it would be the opposite. In memory, this is represented as N*M spots large enough to fit T. So if T was 1 byte, the array would take up 1*N*M bytes. The choice of whether the inner array represents columns or rows is important because it changes which kinds of calculations are easy and which are hard. In OP's case, if you chose each inner array to represent a column, they could get back a given column by indexing once: `a[m]` would return the column at index m. However, storing 2D arrays like that can cause other problems. If you want to look it up, search for "row-major vs column-major order" To go back and answer your main question, now that you know this stuff, a matrix is a mathematical concept, and a 2D array is one way to represent it concretely in a program. There are trade offs in every implementation and that is exactly what OP is running into now. Note that if you want to just work with matrices and forget these details, there are lots of linear algebra crates that make it possible to do that. 
Good article! Thanks for clearly showing the nuance between similar but distinct mechanisms!
Can't wait to deprecate this 🙏
I read the book first. Still rust-101 also doesn't speak about cloning, so I came up with that. Still, I'm confused about the error. I understand that there were a type mismatch, and it suggest to remove the borrow: but it works as long as I don't use the variable first, if I do, I get the expected error: error[E0382]: use of moved value: `vec` --&gt; src/part01.rs:121:15 | 120 | println!("vec_sum(): {}", vec_sum(vec)); | --- value moved here 121 | vec_print(vec); | ^^^ value used here after move | = note: move occurs because `vec` has type `std::vec::Vec&lt;i32&gt;`, which does not implement the `Copy` trait Is there a more simpler way to solve it, and also, why the reference doesn't work? 
The reason you can't use `vec` more than once is because it's a non-`Copy` type. When you pass it into `vec_sum` the first time, that moves the value, meaning you can no longer use it from there. Cloning first makes a duplicate of the value before you give it away. The reason you can't just add `&amp;` is because Rust is strongly typed. A reference to a `Vec` is a different type to just a `Vec` and they're not interchangeable. If you want to pass a reference, you have to redefine the function to accept a reference. You might need to read through the book again. Borrowing and ownership are *fundamental* concepts in Rust, and you appear to have trouble with both of them. If you have trouble understanding a specific part of the book, you should ask for help with that. 
Honestly, I completely doubt that somebody that doesn't know why parsing JSON fast might be interesting is able to write a JSON parser that beats modern parsers, most of which have required many man years to become as fast as they are now while still being useful (being able to deserialize into appropriate data-structures) and fully correct.
How would I do API calls in Rust, as opposed to say, Python. Where I would just import a package into my program. Do I have to note a dependency in my cargo file, and if so, would this be done differently from non web content?
I think you also would lose optimization opportunities by going via C, because less information is available to the C compiler. Whether this is a serious hit or not, I have no idea. Definitely, something (anything!) working reliably through C translation and GCC compilation would be fantastic for some targets, even with a performance hit and debugging issues. So a C backend to rustc would seem worthwhile, if anyone has the time and inclination to work on it. Maybe this would depend on more optimization being done in rustc (e.g. MIR), in order to get reasonable results. (Perhaps the same applies to Cranelife, though?)
Could you link to that post? I sounds fascinating. 
Neat! https://github.com/rustwasm/wee_alloc
To be most general, it should take an `Into&lt;String&gt;`. That covers `&amp;str` as well.
wee_alloc might be [a solution](http://reddit.com/r/rust/comments/9twam5/jemalloc_was_just_removed_from_the_standard/e90rrp0) worth testing, too.
If you're doing a lot of matrix/array stuff, consider using the `ndarray` crate, it is meant to provide these kinds of operations.
N is the number of items in a row, aka the number of columns. M is the number of rows.
[Playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=7e63eb76db028a2ba9676f9e5df428da) struct Foo&lt;'a&gt; { r: &amp;'a mut i32, } impl&lt;'a&gt; Foo&lt;'a&gt; { fn make_func&lt;'b&gt;(&amp;'b mut self) -&gt; impl FnMut() + 'b { || *self.r += 1 } } This does not compile, and as I understand the error it is because the actual return type is something like `GeneratedClosure&lt;'a, 'b&gt;`, and the compiler does not like that there's that hidden `'a` which is not visible in the actual signature. What are my options to making something like this work?
Don't forget to mention that mrustc does not have a borrow checker.
I watched the 2018 keynote. And am now tinkering in specs on a game. It’s nice here. 
Protobuf has *many* problems of its own, and it's rather annoying to work with in Rust too. Capn proto is somewhat better but at the point where I'd have to start caring about binary size I'd just start writing my own binary format. It's not that hard once you get down to it.
Not in my example. I have `[[T; M]; N]`, not `[[T; N]; M]`. It also depends on if you consider it as row-major or column-major which I was very explicit about. 
Amazing.
ummm... have you heard of https://github.com/thepowersgang/mrustc
&gt; I have found that in debug mode, function calls even 3 or 4 layers deep can cause a stack overflow. That's a little surprising to me. Have you looked at the stack trace from these cases? Maybe there's some recursive algorithm you're using inside the AST, beyond just traversing it, that's taking up a ton of stack frames? If every funtion call in the script is 100 stack frames in your interpreter, that can probably be cleaned up somehow.
those rules are *from* a C background.. because other libraries may allocate memory differently than you, or use different versions of allocators, you should always handle your own memory.
In *Programming Rust* (Blandy, Orendorff) Chapter 15, "Iterators", I find a similar usage: let v = vec![4, 20, 12, 8, 6]; let mut iterator = v.iter(); assert_eq!(iterator.next(), Some(&amp;4)); assert_eq!(iterator.next(), Some(&amp;20)); assert_eq!(iterator.next(), Some(&amp;12)); assert_eq!(iterator.next(), Some(&amp;8)); assert_eq!(iterator.next(), Some(&amp;6)); assert_eq!(iterator.next(), None); But in this case, the calls to iterator.next() are not unwrap()ped. So there is no way to write the comparisons without the references to numeric literals that I found so unnatural. It seems to me there is something different about the way Rust is doing this than other languages that I've seen implement an algebraic Option type and iterators that yield a stream of Options. I don't recall ever running into this problem before. Maybe it's because all of those other languages use reference semantics for *all* values, so the distinction between reference and not-reference types doesn't exist.
Is .cargo/config not first class? You can set aliases there, including project local.
I have a fully commented Tic-Tac-Toe example that I often give to people learning Rust: https://github.com/sunjay/tic-tac-toe This may be helpful as you learn. Writing your own small project is also very helpful so I definitely recommend trying that too. :)
&gt; Cheap: C backend, at the cost of debugging experience. You are assuming that this is both possible and cheap. While I suspect that this is possible, I don't think it will be cheaper than a C LLVM backend, which turned out to be much more harder than expected :/
Thanks! While I agree that writing is the best way to learn, making a program functional and properly writing a program ,I feel, are completely different things. Is it using the proper data structures, language constructs...etc? The problem is that there are multiple ways to achieve an objective and without a deep understanding of the language you won't be able to weigh in the tradeoffs. 
Rust panicking, io library, fmt library, etc. This is a binary which prints hello world, so it needs a few things. 
Yes. For me the real question is around Serde. If it’s built on top of Serde then it’s a win-win. From my point of view you just have to prove it’s safe and battle tested, and then it’s a replacement to the JSON Serde. End of. If you cannot build on top of Serde then the question is why not? Is there something about Serde that makes it inefficient?
libbacktrace, also the standard library itself is not very efficient
&gt; Instead, there are explicit "callback" points that MUST be implemented for each language, which the GCC toolchain will use to further translate the IR down the road, requiring a front-end implementer to provide GPL-licensed sources. I don't think this is correct. Both [MIT](https://www.gnu.org/licenses/license-list.html#Expat) and [Apache-2](https://www.gnu.org/licenses/license-list.html#apache2) are compatible with the GPL v3, so writing a front-end in MIT/Apache-2 should be fine.
Useful article! I’m still struggle with good api design in rust and this shed some light on it. More articles like there are much appreciated:)
Thanks for clarifying.
I don't think you need HKT for that? Can you maybe do it with associated types? The currently accepted RFC for something "HTK-like" is [Generic Associated Types](https://rust-lang.github.io/rfcs/1598-generic_associated_types.html), and you can follow its progress [here](https://github.com/rust-lang/rust/issues/44265).
Never done anything parallel in C++ (thank god). [Rayon](https://github.com/rayon-rs/rayon) is amazing, and if you feel like it, you can probably also use something like [faster](https://docs.rs/faster) to exmplictly make use of SIMD.
Just to be on the same page - by `space` you mean `context`, right? So you have context from inside the class, like here: ``` class MyClass { function someMethod() { // this = context } } ``` ... and context from outside the class, like here: ``` obj = new MyClass() obj-&gt;someMethod() // obj = context ``` And you have an `invoke` opcode that takes a pointer (like `this` or `obj`) and calls a method on it, right?
Right. That was how the class structure looks: ```rust pub struct Class { pub sig: ClassSig pub methods: HashMap&lt;String,Method&gt;, pub superclass: Option&lt;Rc&lt;Class&gt;&gt;, pub pool: Pool, pub fields: RefCell&lt;HashMap&lt;String,Field&gt;&gt;, } ``` and Pool: ```rust pub struct Pool { pub classes: RefCell&lt;Vec&lt;Option&lt;Class&gt;&gt;&gt;, pub idx_pool: RefCell&lt;Vec&lt;usize&gt;&gt;, pub allocated: RefCell&lt;usize&gt;, pub globals: RefCell&lt;HashMap&lt;String,usize&gt;&gt;, } ``` And i need to know from what context invoke method
Maybe the quote crate helps?
Can you elaborate on how you compiled and ran the C and Rust programs? Did you make sure to include all the optimization switches for gcc?
Lol my bad I was wondering because I was getting rust posts but forgot where to post
'Too complex' might as well be the achilles heel of rust, so i agree with the advice above. K.I.S.S. until you really need it.
I would also make sure to add `crossbeam` into the mix for algorithms that you can't use `rayon` for. It lets you access stack data without the `Arc` (atomic reference counted wrapper like shared_ptr) by making guarantees about the lifetime of the data. It also just looks cleaner. `rayon` is just the best though if you can use it, which you usually can.
Wrong subreddit. You want r/playrust.
Imma tell you this is the wrong sub. But it's nice to see you are having fun
That how Invoke opcode defined: `Invoke(usize) // Invoke arg count` when frame executes that opcode, frame pop from stack: target(Class), Method Name(String) and then arguments.I think one more idea how to check from where to call it to put a dollar sign at the beginning of the line, if it is there, then the method is to be in the pool in the globals field
Awesome, will have to dive into this. I'm trying to parallelize a sorting algorithm by splitting the workload of the for loop
That's a piece of cake with rayon. One of the examples in the documentation is actually an implementation of quicksort, if I recall correctly.
Awesome! I'm unfortunately not working with quick sort but I have a good feeling about trying out rayon
I don't think there is a right subreddit for this.
Yeah, you’re right. We tend to use polymorphic functions everywhere and some recent changes will make it very hard people to actually see a polymorphic function (I’m looking at you, universal `impl Trait`). Thanks for the feedback!
I’m glad you like it! At first I almost cancelled releasing the article because it’s too different from what we usually see (it’s more about opinion and decision, which is always hard to present to others).
Your intuition is correct, there is no implicit borrowing like that. The argument should be `&amp;stat` AFAICT. A second minor point is that ordering is typically done through [`std::cmp::Ordering`](https://doc.rust-lang.org/std/cmp/enum.Ordering.html), which is reversed by calling [`reverse()`](https://doc.rust-lang.org/std/cmp/enum.Ordering.html#method.reverse). This is what I would expect it to look like: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=7e2144cc47bac4ca64df11f06552f014
Thanks for the input. I'm new to reddit. My buddy told me to post this
Another thing is that in that VM everything is a class, value has class pointers too: ``` pub struct Value {` pub class: ClassSig,` pub value: EnumValue } ``` And operations like binary add, sub, etc... Is a method of a class. That was an example of a "Hello world!" program(pseudo code) ``` LdChar(72) LdChar(101) LdChar(108) ... // other characters loading anew(12) // array initializing with length 12,args will be reversed LdString("&lt;initializer"&gt;) // LdString doesn't load string_class, it loads name of field or method LdClass(string_class) Invoke(1) // 1 argument, array&lt;char&gt;, String uses ASCII symbols for encoding LdString("println") LdClass(io_class) Invoke(1) ``` That what i want to see in future. 
/r/playrust 
Count sort. It's a pretty interesting sort that isn't all to efficient but I find its method cool
This is a Use case I tackled from the beginning with [nom](https://crates.io/crates/nom). For a lot of formats, we won't get the entire data. It is using parser combinators instead of regexps, but it should be enough to get you started. There's another funny way that you could try: mmap the file in memory and let the OS handle loading data into your process. Your code will see a very long slice, but only the necessary pages will be loaded
This looks pretty good. Although I'm wondering if the piece enum should implement `std::ops::Not` instead of creating the `.other` function you had. For reference: https://doc.rust-lang.org/beta/std/ops/trait.Not.html
OK thanks for the pointers /u/killercup. Apparently I should have titled this post GAT ETA - and it's not ready soon.
Yes, I understand borrowing/ownership are fundamental. I'll go again into those chapters because it's kind of hard to get my head around them. Nevertheless, I did understand why the error happened, so thanks anyway :). The idea that a `Vec` is different to a reference to a `Vec`, still seems strange, but it just I need more time to grok it.
This is great to target embedded like avr while llvm fixes its bugs.
Thanks!
I can see where category theory might have some relevance to compiler writers, for the exact same reasons they are useful for logicians. But I still feel for general programming, it's borderline useless. (And with no shame, either, as I would say the same thing about integral calculus). Programmers just don't need to think in terms of objects and morphisms. Things like "Hask" come up because, as far as I can tell, people have tried to find the relevance. But my biggest problem with Hask isn't it's failure to model `seq`. It's that it doesn't capture polymorphism. I am not an expert, but I looked very hard back when I was still interested in Haskell to figure out how you handle polymorphism in categories. The seemingly proper way to do it is using fibered categories. But the thing I found interesting was that morphisms were *not* Haskell function arrows. Instead, morphisms of the relevant categories are always modeled as going from contexts to terms judged against those contexts. And so it's not like I don't think you could, in principle, do proper formalization. So let me refine my issue into two: First, I think, while very interesting, I don't think category theory is an appropriate tool for general programmers. (Although I admit in some niche cases, it might still have some value). And second, I very much dislike the sloppy way the Haskell community works with the mathematics. This is certainly a personal issue, so I expect disagreement. If a community becomes such an evangelical advocate of an obscure branch of mathematics, I would hope that it pays proper reverence by maintaining a higher level of rigor in its use. But I'm sure at least someone else in history had felt the same way of the engineer's use of integral calculus.
A word of warning, this is a particularly challenging exercise in Rust. Particularly for developers who have done this before in other languages, their initial approach will likely have them fighting the borrow checker in Rust. 
Hrm, interesting. I'll take a look. 
Ah, apparently I still suffer from the learner's "I understood this" when it's not what the teacher said. Shall get that amended, and try to make accurate statements. Thanks!
The [Hello Rust YouTube channel](https://www.youtube.com/channel/UCZ_EWaQZCZuGGfnuqUoHujw) might interest you. It goes through tackling small programming projects with a healthy amount of explanation along the way.
Nom has already helped me get started :) Unfortunately the regex part is unrelated to the parsing part. I've written a parser for the format with nom, which definitely does what it needs to do and does it well, but it's a bit overkill for my current use case. In this more limited case, the parsing code itself is pretty much trivial. The regex is only needed to decide whether or not (a part of) the entity should be written to stdout.
Finally, I make it work, this code works perfectly! ``` Instruction::LdInt(2), Instruction::LdStr("test".into()), Instruction::LdClass(test_class), Instruction::Invoke(1), Instruction::Ret, // "test" method code Instruction::LdInt(2), Instruction::LdLoc("a".into()), Instruction::Add, Instruction::Ret, ``` and test method definition:`Method::new(args: vec![("a".to_string(),Type::Int)], ret_type: Type::Int);` But this is only the beginning of all the work, much more remains to be done.
With grpc in JavaScript available it makes less sense as a data format. And with large data dumps a little ETL goes a long way. Converting to avro, orc, or parquet can get you monstrous performance boosts.
&gt; env!("CARGO_MANIFEST_DIR") Thanks, this do the trick.
Wrong sub.
You want /r/playrust.
I would say the Java ecosystem is larger (and generally better) than C#. Python and JS' ecosystems are huge, but being scripted languages, there obviously domains they are not very well suited to.
Have you got a docs link for that?
Yeah, I was talking about job posts on reddit, not job openings in general.
I will definitely try mio/tokio, but for now they're too complicated, so I wanted to create straightforward and simplest POC (like I've done in other languages), but it looks like I need to forget everything I know about programming :) P.S. I thought that it will stop me from trying Rust, but it's not, as I understand that Rust is trying to prevent future errors like race condition, and it's great!
I can understand tokio being too complicated, but as far as I understand the mio api is almost directly 1:1 with the epoll api. What stuff is it making you do that a simple poc wouldn’t involve?
It will need more time that I thought :D But it looks like a challenge, let's see who will win :) For your case, am I right that you're trying to send some data between VM and host (server is on host and clients inside VM)? If so, have you tried e.g. virtio-serial? It should work without network and there are drivers for guest Linux and Windows, but unfortunately I'm not sure about it's bandwidth and latency.
The IO happens in the Read / Write impls of mio::net::*. As for the complexity, it’s pretty much the same stuff in the api as epoll, just with strong typing and bitfield types instead of integer fds and integer flags. 
https://doc.rust-lang.org/cargo/reference/config.html
Even if you could register a raw FD with mio, which I think comes under https://docs.rs/mio/0.6.16/mio/unix/struct.EventedFd.html now, on disk files aren’t able to be read in a non blocking fashion. Attempts to register file descriptors that correspond to files with an epoll / kqueue/ ... handle would either error or just always return readable events (but block when you attempt to read, anyway). 
Your guide is both detailed and easy to read. Nice work.
&gt; For your case, am I right that you're trying to send some data between VM and host (server is on host and clients inside VM)? Server runs on guest(s) actuall,y with client on the host or other guests, and if I support networks like UDP, then I guess separate physical systems could be supported too. &gt; If so, have you tried e.g. virtio-serial? I'll give virtio-serial a look :) though I'm hoping for macOS guest support as well. &gt; unfortunately I'm not sure about it's bandwidth and latency. My program is capturing the display output(s) of the guest and sending it to a client on the host or another guest. Generally you want 30-60FPS and minimize resource usage. With shared memory high bandwidth and low latency is great, but it requires special driver support in the guest OS that I wouldn't know how to implement properly for macOS. These VMs have direct/exclusive access to a GPU each.
It sounds like you defined the variant as `Parameter(Parameter)`, but tried to match it as `Parameter { name, location, etc }`. There's no magic involved; if the variant is `Parameter(Payload)`, then you need to match it as `Parameter(payload)` (which binds `param` to a value of type `Payload`). That or `Parameter(Payload { field_a, field_b })` which destructures the payload. That the two have the same name makes no difference. But this looks like you're just using types defined in a crate you don't control. In *that* case, there's nothing you can do. You're dealing with two things that have the same name and fields, but which are incompatible.
Ah, thanks. I see you can alias cargo command but not arbitrary shell commands. 👍
I've successfully implemented POC in C with poll, and fd from device worked properly. I've never tried epoll, maybe this is my problem with trying to re-implement same with Rust. And yes, when I'm handling POLLIN in poll for device's fd, I'm using blocking read, but it's not a problem, as events are coming from the device with frequency of \~250-1000 Hz, so there are plenty of time to send them to UDP socket and handle events from UDP socket, in case if there are any.
Cargo is not that small, but it's code is really great source of knowledge. Definitely worth reading.
My attempt. https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=9e09ebcfc989f0e00b3a638c579c9905 No `Either` or `once`. `std::iter` things only. One extra `let` binding needed. 
thank god i thought it was just me 😅
I want tab searching to be the default. I know Firefox can search tabs.
Going [wd] very quickly results in death when it shouldn't.
IANAL, but as far as I know that compatibility mean that you can use MIT/Apache code within GPL code, but you cannot use GPL code in MIT/Apache. And as you are **required** to use callbacks and other things from within GCC you are bound by the GPL license to it. 
Happy to! There's lots of built in traits for things like this that people don't know about. I think code discovery is one of the current productivity issues with rust. With C++ you can arbitrarily change the functionality of operators and you can basically do the same thing in rust but nobody seems to know.
Cool. Coincidently I also recently built a rust cli tool for AWS based on rusoto.
Yes necessarily, for the most part. Rust wouldn't have been using Jemalloc the way it was in the first place if Jemalloc wasn't well known to outperform most other malloc implementations in the majority of situations. This change will lead to smaller binaries to be sure, but also *definitely, unquestionably* slower binaries in many, many cases. I'm unsure why you'd want to sugarcoat something like this.
I think sharing an “opinionated “ approach explaining the trade offs and how you made these decisions is especially useful. We all want to write “the best way” and we can use some guidance what that best way might look like. Happy you didn’t cancel it. Thank you.
But when I try to sign up for email alerts, I always get a 500 internal server error :/
I am aware there are better programs to do this and websites which do this, but those can be complicated at times and some may not trust the websites. Plus, a CLI password generator could be a good thing to have. And hey, RIIR is a good thing to live by!
Are there plans to add functionality to support pass**phrases**? There is a growing trend to have people use passphrases because they can have sufficient entropy and are much easier to remember. 
And I'm up! https://github.com/rust-lang/rfcs/issues/2586
Looks like you've implemented a custom iterator, though?
Looks like you're using tuples to replace the `Either` type, though?
This is a nice solution! It looks like even the `let` binding can be removed: iter_outer.flat_map(|iter_inner_result| { match iter_inner_result { Ok(v) =&gt; Some(v).into_iter().flatten().chain(None), Err(e) =&gt; None.into_iter().flatten().chain(Some(Err(e))), } })
Looks pretty close to YAML, and YAML has more features (like better support for [multiline strings](https://en.wikipedia.org/wiki/YAML#Indented_delimiting)). It is important to have a parser for JSON5, since people may need to write a Rust program integrated with another tool using it, but for a greenfield I prefer something like YAML or even Toml.
Some things don't allow spaces in their passwords. I guess I could add an option for that. Yeah, I can do that, sure thing. I don't know how I'd make it coherent, but I could try. Another fun challenge!
&gt;one that preallocates based on an environment variable or just never frees, whichever mode delivers faster performance. Maybe I'm wrong, but I think that AWS can keep the process running if the lambda function is called frequently, so if you never release memory, it will fail from time to time
http://aturon.github.io/2018/04/05/workflows/ was one of the earlier posts, and Ashley has told me personally that she’s wanted this for a while. They wanted to do templates first and then tasks second, and it’s going slowly. I don’t think there’s been a real RFC yet.
The end result is that the rust gcc compiler, as a whole, would be licensed as GPLv3 (even though it would have MIT/Apache components - the Rust bits). But the current rust llvm compiler would continue to be MIT/Apache just fine. That is, you don't need to license the current llvm compiler as GPL just because you created a derivative work that integrates with GCC. I think this situation is okay.
Also worth noting that you apparently can use this release with `#![forbid(unsafe_code)]` (from (here)[https://github.com/rust-lang-nursery/lazy-static.rs/issues/125])! Nice change, makes some of my projects usable with lazy_static again. 
Stumbled on this project recently and had to share :) - [Encrusted's github](https://github.com/demille/encrusted) - Runs in a web interface or directly in a terminal. - Built with Rust and WebAssembly (``wasm32-unknown-unknown``). - Wiki/Background - [Interactive fiction](https://en.wikipedia.org/wiki/Interactive_fiction) - [Z-machine](https://en.wikipedia.org/wiki/Z-machine) - [Infocom](https://en.wikipedia.org/wiki/Infocom) - [Zork](https://en.wikipedia.org/wiki/Zork)
Interactive fiction, often abbreviated IF, is software simulating environments in which players use text commands to control characters and influence the environment. Works in this form can be understood as literary narratives, either in the form of Interactive narratives or Interactive narrations. These works can also be understood as a form of video game, either in the form of an adventure game or role-playing game. In common usage, the term refers to text adventures, a type of adventure game where the entire interface can be "text-only", however, graphical text adventure games, where the text is accompanied by graphics (still images, animations or video) still fall under the text adventure category if the main way to interact with the game is by typing text. Some users of the term distinguish between interactive fiction, known as "Puzzle-free", that focuses on narrative, and "text adventures" that focus on puzzles.
Thanks a lot! My fingers were too slow to uncover this awesome bug. Fixed: https://github.com/yiransheng/rust-snake-wasm/pull/12
Any contexts outside of games you'd recommend to dabble in ECS? Currently use OOP a lot at work but would love to experiment
I'd be happy to write an RFC for this, but it would be my first RFC. Apart from reading the `rust-lang/rfcs` repo README, do you have any other guidance for this? My biggest question at the moment would be whether the NPM decision to use shell scripts for this would be sufficient (it solves my use case, but users may write scripts which aren't cross platform and it wouldn't solve all use cases), or if the RFC should include some mechanism to run Rust code as a "script" (either in place of the shell script option or alongside it). My concern is allowing (or worse, requiring) scripts to be written in Rust rather than using the shell would add significant complexity.
Just out of curiosity, as I'm fairly new to Rust, why does `generator.resume()` require unsafe?
As a Windows user, “just using the shell” has complexity for me that Rust code doesn’t :) it’s not likely that the shell code is portable, whereas rust code is. You can also post a draft to internals to get feedback before making a real RFC too, which can be useful.
HashSet iteration order isn't defined and could change in the future. I think that's why. I think one of the reasons not to interpret less than as the subset relationship, is that it strongly implies that greater than should be the superset relationship. But that would violate the standard rule for &lt; and &gt;, which is that not &lt; and not = implies &gt;.
Just FYI I decided to [implement](https://github.com/blt/bh_alloc/pull/1) a thread-safe, aligned bump allocator in the same crate, now at version 0.2.0. The fuzz focused allocator is under `bh_alloc::fuzz`. 
You can separate with hyphens as an alternative. Per another user’s comment about selecting a password from a corpus of valid characters, this approach extends nicely to passphrases as well. For a password, your corpus is the list of all valid characters, you select the correct number of them (perhaps with initial requirements upfront, like one upper case, one digit, and one symbol, then the rest of the password padded with any valid character), randomly sorted and joined together. For a passphrase, your corpus is just a set of common words, select the correct number of them, then sort randomly and join by spaces or hyphens.
Ah, this is going to strongly depend on what the input directory was. /u/Amanieu how are you running the test?
From what I’ve seen, it’s the best option around. They have a fast ECS called specs running under the hood and they use gfx-rs for graphics, which is probably the best Rust graphics crate at the moment. Examples are definitely the biggest downfall of Amethyst. They seem to prioritize documentation for their API and their “The Book” and updating those texts to always be current with their quickly-evolving API. (not necessarily a wrong decision). And, of course, their actual development work seems to be fast, and effective. I also think that they are integrating the nalgebra crate for linear mathematics. All in all, it seems like (coming from someone who has not seriously used their crate) the Amethyst team is hard at work creating a library that uses some of the Rust ecosystem’s most impressive crates as giants to stand on. Definitely an exciting project. 
This broke things for our crate because we specified "1" as the version for `lazy_static` we rely on and our minimum supported Rust version is 1.22. Was there ever a consensus on how changing the minimum supported Rust version requirement interacts with semver? I assume it should definitely not be done as a patch version increase, but is it reasonable for a minor one?
One of the key advantages of JSON is that it's trivially consumable by anything or any*one*. Sure, it can take a little longer to machine parse (but even this is often negligible), but the fact that Random Joe can take a JSON payload and figure out what it's doing with no tool more complicated than a simple text editor is *massively* valuable in roughly every application.
Well, I know what I'm doing next with this. Expect an update soon! Hopefully by next week adding in all of these features.
https://users.rust-lang.org/t/rust-version-requirement-change-as-semver-breaking-or-not/20980/2
There isn't really any strong consensus out there around how bumping minimum supported versions should be handled, or what a reasonable minimum supported version looks like. For `lazy_static`, we had some discussion about the minimum version [in this issue](https://github.com/rust-lang-nursery/lazy-static.rs/issues/114) which resulted in adding a [note to the readme](https://github.com/rust-lang-nursery/lazy-static.rs#minimum-supported-rustc) that details what the minimum is and how we'll change it. We do try to be mindful when bumping things for `lazy_static` because it's so heavily relied upon.
I've been playing with this - I set up an aws free instance running nixos, like my laptop, and then I scp the executable to the instance and run it with tmux. ctrl-b-d and exit and there you go! Oh and I do 'setcap 'cap\_net\_bind\_service=+ep' server' to enable port 80 access for the program 'server'. 
You're right, I need to think about that more.
And this is the link to third lesson of the Rust crash course which covers Iterators: https://www.snoyman.com/blog/2018/11/rust-crash-course-03-iterators-and-errors
I have an example in the stack-vm acceptance tests of how I did it. There’s lots of ways to define a calling convention though, so it’s kind of up to you. https://gitlab.com/huia-lang/stack-vm/blob/master/src/acceptance/functions.rs If you’re new to this sort of thing I highly recommend Language Implementation Patterns from pragprog. https://pragprog.com/book/tpdsl/language-implementation-patterns
Yes, it still happens that a new rustc version will break clippy and we fail to fix it before the nightly is built. In that case, rustup will not download the newer version if you have the clippy-preview component. If you really want the newest version and can live without clippy for a day, you can remove the component and install clippy manually from github.
Wouldn't you need to free the allocated memory, then?
Flexbox style layout is pretty much perfect for at least 90% of the sorts of things I'd want to do, but would there be any possibility of getting [cassowary](https://github.com/dylanede/cassowary-rs) in there somehow for the remainder?
rejection-based sampling is much easier to make theoretically sound. There's a good chance that other approaches have biases.
The point is that you cannot share a lot of code between these two implementations. Instead you need to rewrite everything and maintain separate codebase, which is hell lot of work. 
That is absolutely true, unfortunately I noticed it way too late. At least it was a fun experience to get a 10x speedup on the number generator. Still, it puzzles me how anyone can solve this task in under 13s, 27s is the best I got, 13 of which for the number generation alone. Probably it’s all about the work *not* being done...
&gt; as it obviously should, because Jemalloc supports Windows I know, I work on jemalloc too, and CI is set up to test it on the 4 tier-1 windows toolchains (it breaks on the `gnu` toolchains on CI though). &gt; once your build.rs script is corrected to not make the various overly-narrow assumptions and adjustments it currently does that break things on Windows. That build script has been inherited from rust-lang/rust, and its kind of normal that it doesn't work on windows since Rust never used jemalloc there. &gt; I might take you up on that patch request in the near future, once I've made some changes to my version to make it work fully "generically." Thank you! Maybe once that works you might be able to help with migrating the crate from appveyor.ci to travis-ci for the windows bots?
That seems like a pretty simple setup. I'd probably try to run the executable in a way so it gets restarted when it crashes or when the server reboots (a `service` file for systemd, or whatever sinmilar thing nixos uses).
So I wrote a password generator for my password manager, and I like the interface, perhaps you might like to adopt it. It's: generate 30,11aaAAA. This will generate a 30 character password with a minimum of two numbers, two lowercase letters, three uppercase letters and 1 symbol. All remaining characters will be random characters from any of the specified types. So "generate 8,A." Would generate a password with a minimum of 1 uppercase letter, and 1 symbol, and 6 random uppercase letters or symbols, but absolutely no lowercase letters or numbers. I've found it to be sufficient to satisfy my password generation needs and I find it intuitive to use.
There are several crates out there doing that.
I’ve CTRF+F’ed through this thread again and I don’t think anyone quite claimed that. Either way, you’re correct: it’s **only Rust toolchains for Windows** (and some other targets) that disabled jemalloc because of Rust-specific breakage that no one ever finished to investigate.
I noticed you were using `rand::prelude::*;`. If I were you, I'd would use the `OsRng` to ensure that the randomness is actually suitable for use as a CSPRNG, which is required when generating passwords/passphrases. You can have a look at [passgenr](https://github.com/defuse/passgenr) for inspiration on this.
I feel like if it's a breaking change, to align strictly to semver the major version should be incremented. I don't see many downsides in this approach, but I am assuming there is a good reason why crate maintainers are shy about bumping major versions?
Sure, there are not a lot of examples, but the team is on Discord, and they profide a lot of help for the newcomers.
You do not need to rewrite anything because of the license, just use MIT/Apache-2 for your code and it is usable in both. The only duplication is that you would need to support two compiler architectures, but that has nothing to do with the license. In fact, the [multiple compilers for the D language](https://wiki.dlang.org/Compilers): Digital Mars D Compiler (DMD), GCC D Compiler (GDC) and LLVM D Compiler (LDC) all share the DMD compiler front end.
I don't know about "recommend", but perhaps pick something non-trivial as a design exercise. e.g. Design the components and systems for an automated distributed system deployment tool. I actually don't know a good way to lay out the data. * Have a component per software to install * Another component for variables to feed to the installer (e.g. configuration, environment vars) * `InstallerSystem` joins on those components, and kicks off the installers * `InstallPollingSystem` joins on `InstallationStatus`, sends progress updates. * System to display status * ... Maybe
If you distribute the program which includes GPL code (as in GCC), yes you are bound by the terms of the GPL. But if you distribute another program for LLVM which reuses your code but includes no GPL code by anyone else, you are *not* bound by the GPL for that.
Cool thanks, I currently use digital ocean it just works fine with systemd. AWS having thousands of options I was confused where to start and whats best for this kind of thing.
What you are looking for is /r/playrust
Yes, I do, thanks
It doesn't work this way.
It looks like you've forgot to include the `Iterator::flatten` code -- you pasted the `flat_map` implementation instead.
There are some [valid reasons](https://arp242.net/weblog/yaml_probably_not_so_great_after_all.html) to avoid YAML.
Hmmm, might be fun trying to get involved. 
Just wanted to confirm that I encounter this exact problem quite often when programming Rust. I will freely admit that this may be related to my Java/Kotlin background (basically trying to use structs as makeshift classes). However, I do feel I got better at writing idiomatic Rust code and I still encounter this borrowing issue sometimes. The 'view struct' trick is new to me, but I used some of the other workarounds. For me personally this issue would be a high priority to fix in the language itself, because it feels bad to employ these workarounds to satisfy the borrow checker.
Thanks for the kind words. &lt;3
Agreed. Java seems to be also gaining more and more ground against C# in the enterprise world lately, for some reason. Anyway, both of them won't die for a couple of decades at least, so we will see who remains king/queen of the enterprise.
I like the point of still doing your interviews in Python. I also would still do my interviews in Ruby, doing them in Rust would be a really ballsy move. I wonder if it's as ballsy as doing them in C++ or C would be. Your C++ might compile alright, but you might have a bug that your interviewer spots, but if you did it in Rust the bug you didn't spot wouldn't even compile, that's a pretty harsh trade off. Of course in a perfect world your interviewer would be suitably impressed by your attempt of doing Rust from heart. 
I'd recommend ggez if you're still learning - it's much easier. Amethyst is great but hard.
Doesn't `#![forbid(unsafe_code)]` apply only to your crate, and not to the dependencies? And it's [not like](https://github.com/rust-lang-nursery/lazy-static.rs/blob/master/src/inline_lazy.rs) the new release doesn't use `unsafe` anymore, or like you could really avoid `unsafe` in a no-`no_std` application.
Phew, for a moment I thought this belonged to *playrust*
I thought so too but it refuses to compile previous versions of `lazy_static` for me. 
&gt; Doesn't #![forbid(unsafe_code)] apply only to your crate, and not to the dependencies? Doesn't matter if the dependency comes in form of a macro that inserts code into *your* crate :)
Probably `lazy_static!` macro expanded to unsafe code and now doesn't.
OK, thank you. I submitted an erratum. 
Yes, this is what it used to do
Check out "Anchorhead" , one of the best z machine games I've played. If you enjoy Lovecraftian fiction you'll love this one.
It would be interesting to try and see what they think about the intricacies of our typesystem w.r.t API design, I guess we'll just *never know*.
llvm can and is used in the embedded field as well. It just does not support as many CPUs as gcc, but most of the common ones.
I am working on such a library :) Currently, it's very experimental, lacks docs and API is basically what I need to implement a language server. It builds on stable and is (as of one minute ago) published on crates.io: https://crates.io/crates/ra_syntax.
There is a heroku buildpack that works well, or atleast it did when I used it a few months ago. A google search will take you to it. I've also setup one manually on a linode vps. I used a nginx reverse proxy to the app which is a pretty solid way to set things up (niginx can handle all your ssl cert stuff). You can figure out how to do that with a few searches as well.
We should have a challenge of the most playrusty title that would still be a perfectly valid r/rust post
Thanks! I've looked at the threads that /u/the___duke posted above, and the biggest blocker was figuring out how exactly to try it out. I'll give this a shot! 
You can use `impl Fn(...) -&gt; ...` instead of `Box&lt;Fn(...) -&gt; ...&gt;` to avoid the allocation https://rust-lang-nursery.github.io/edition-guide/rust-2018/trait-system/impl-trait-for-returning-complex-types-with-ease.html 
Finish and release [graphql_client_web](https://github.com/graphql-rust/graphql-client/pull/174) - so it becomes trivial to work with GraphQL APIs (with precise types) from within the browser in webassembly code.
I added the possibility to do difference set operations for polymorphic types to my [sdset](https://docs.rs/sdset/0.3.0/sdset/duo/struct.DifferenceByKey.html) library. It means that I can do a difference on two slices based on a key, for example: [{a: 1}, {a: 2}, {a: 3}, {a: 4}] - [2, 3] = [{a: 1}, {a: 4}] This will be useful for a search engine that we will release my team and I in a few weeks !
Thanks! Is there a way around it in 2015 edition?
&gt; You can separate with hyphens as an alternative. Another option is to use words that you can unambiguously concatenate even without separators, like in [these wordlists](https://www.eff.org/sv/deeplinks/2016/07/new-wordlists-random-passphrases).
It works on stable Rust already, I just couldn't find a better guide than in the edition preview. Here's an example running on stable https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=2efd375316dae52ac7540204b9e4ee76
I misread, sorry.
&gt; There is a growing trend to have people use passphrases because they can have sufficient entropy and are much easier to remember. That was my thinking! Now, I regret it a bit, because the downside is that they become so long! There is so much to type, and I often make a mistake and have to try several times. :-/
Just leave it to the Rustaceans to create the most sound programming language in which as little behavior as possible is a surprise, only to start encouraging others to create the most confusing subreddit filled with underhanded Rust submissions... ;)
Are you a newbie to programming in general, or just new to Rust? &amp;#x200B; Amethyst is very exciting but depending on your overall experience level it might be better to start with something simpler, and make a few very simple games first. Something like ggez or the sdl2 bindings, make some very simple 2d games to get some basic understanding, before you hop into a super complex engine. &amp;#x200B;
Ah, beautiful! Thanks for clarifying, this is clearly superior.
Do explicit lifetimes have any effect on the outcome of the compiled code? In other words, could I write correct code with lifetime annotations, make sure it compiles, and then remove the lifetimes, disable the borrow checker, and end up with the same output?
Hi! Author of the post (and of the css) here. Thanks for pointing this out, it seems I didn't test this properly on mobile.
Hi all! Author here, good to see this sparked some interest here as well, let me know if you have questions/comments! (I'll link this thread from the post)
I'm continuing to extend the feature set of my [MegaZeux client](https://github.com/jdm/mzxplay). I'm adding support for executing the global robot every cycle, and I've found some ways in which NLL significantly improves the expressiveness of my code. I've also hit a snag related to mutable borrows that I need to figure out how to resolve.
Haha. \[Author here\] I totally didn't realise the risk for, uhm, confusion here till I saw this.
[Yeah it's not great pal](https://i.imgur.com/hsvEVUC.png)
Well, you *can't* remove all lifetimes, and you *can't* disable the borrow checker, so no, you can't. But if you *could*, then **yes.** This is how something like `mrustc` works: it doesn't bother checking lifetimes at all, it just compiles code by assuming they're correct.
that, and currently it just logs to stdout I believe, so the logs aren't persistent beyond the tmux session.
But do annotated lifetimes have any impact on the output of the compiler at all or is it just a verification?
I've had good experience with ggez, but I'd note that it's a primarily 2d API, if you want to do 3d games, it probably can be used, but I wouldn't recommend it. It's great for simple 2d games though.
This is true sorry - i use it for 2d
I believe that impl trait works in rust 2015.
So I'm really only familiar with using Linode for this (I work there), but I highly recommend [taking a look at LinuxGSM](https://linuxgsm.com/lgsm/rustserver/) to get this installed for you. It takes a LOT of guesswork out, and I use it for all my game servers.
They have no impact on the output of the compiler other than allowing it to reject invalid programs.
Indeed it does, thank you
It's the law of conservation of confusion in action.
Perhaps at some point the `Fn` traits will finally be stabilized and it will be possible to return a real named type that implements `Fn`, instead of being forced to use either boxing or closures + impl trait.
Looks like you broke Gitlab's syntax highlighting :) I wrote [mkpass](https://github.com/Freaky/mkpass) for making passphrases. Probably the most notable bit is instead of just offering a `--length`, I use a bit strength target and calculate the length to achieve the desired strength. A 10 character alphabetic password is a much different beast to a 10 character mixed-case alphanumeric, but 2^72 is 2^(72). I wrote a [blog post](https://hur.st/blog/2018/08-25-password-generation-in-ruby-and-rust/) based on the experience.
The situation I have in mind is when you use the password several times per day, and have it "in your fingers". Reading and typing a random string would indeed be cumbersome.
Why? It'd start off from an unused arena every time, essentially.
Much obliged. I'll use that and get back to you. 
Sadly I won't make it to this meeting. I hope there are some more to come…
 But I did not used them myself, but there are implementation on crate.io of `gettext` a tool commonly used on other languages for translation.
There already is a [/r/rust vs /r/playrust classifier](https://www.youtube.com/watch?v=lY10kTcM8ek). Then all one would have to do is to find the /r/rust post with the highest /r/playrust score (excluding posts in the wrong sub).
Nothing even close to Qt integration, sadly.
Finishing up the stabilization PR for RFC-2086
I'm working on a simple kitchen timer using embedded rust. Things are coming along nicely and everything is working in a prototype configuration on a bread board. The 4 buttons from left to right are: +1 min, +5 min, +15 min &amp; reset. The little speaker is actually pretty loud when using it's resonant frequency of 2048hz. One of the things I've done recently is separated the timer logic from the hardware. The logic controls the hardware through use of a Trait. I put the logic into its own crate so now it can be unit tested. This seemed like a pretty big win. Another neat thing is this program is all interrupt based, meaning the main program loop just sleeps, waiting for an interrupt. If anyone is interested in current source, it's here: [https://gitlab.com/freiguy1/timer](https://gitlab.com/freiguy1/timer)
I wrote a little [route53 dyndns daemon](https://github.com/mattico/route53-dyndns/blob/master/src/main.rs) because it was easier/more fun than figuring out python dependencies.
I posted below about using apis, and unfortunately the http api i was going to use is broken, so I now have to do it myself
I don't know how the snapshots work, but if they're snapshots of a running app, I assume they'd also contain the memory contents, including the allocator's arena.
No, you can. Code that is licensed as MIT/Apache can be incorporated into a GPLv3 codebase and still be licensed separately as MIT/Apache. The copyright holder can license their code under any licenses they want. The GPL doesn't change that.
[it looks fine here.](https://i.imgur.com/pskoVCU.jpg)
Are there any good options for (de)serializing ePub files in Rust? Got a project I'd like to attempt, but so far not having much luck finding anything that would allow for creating, reading, and editing ePub files...
Right, so why clear it? Essentially you'd start every run with the just-allocated arena, ready to be used.
I'm adding support for the Lean prover to the sccache compiler tool. It's a nice way to learn Rust it seems.
That depends on when the snapshot is taken; is it at the start of the process?
In your first example, you've called `blocking`, which executes immediately, and then simply moved the return value into the spawned future. That's likely not what you want, but rather want to call `blocking` multiple times until it succeeds, which is what the second example does. The lifetime problem is that the spawned future could outline `self`, which you are accessing inside the closure. You'll need to clone self, or something similar, to not make the closure's lifetime too short.
You say you're experienced. Did you already defeat the borrow monster?
Not 100% sure. My understanding, which could be incorrect, is that it takes place directly before the handler is called. I'll see if I can't find out.
I think it depends which level of support, and performance, you are looking for. For example, if you restrict the panic mode to `abort`, then suddenly unwinding is no longer necessary, which drastically simplifies the conversion to C code. On the other hand, if you do want unwinding, then the easiest translation is to return a flag on whether a panic is ongoing or not, to be checked by the caller. This costs a little bit in performance, though remains relatively straightforward. If you want full blown Zero-Cost Exception, then I'd expect it to be very difficult. I think that the just using `panic=abort` (or infinite loop) would be sufficient to start things off.
&gt; I think it only requires license to be GPL-compatible and MIT is. There's a debate in another of the answers; and people arguing in both directions... personally, it's just very unclear to me.
No, the `+` allocates a new String and moves it into `this`, replacing the old one.
Thanks for posting this, this is really neat, especially the web support. Modern inform games tend to use .gblorb not z-machine, but this is useful for the older Infocom ones!
I am not sure you'd miss that much, as long as the C code was rigorously annotated. For example, while human beings may shirk from using `restrict` to the disastrous effects misuse can have, a compiler should be able to relentlessly apply it wherever it makes sense. I definitely expect Cranelift to produce less optimized code; for example to be lacking smart vectorization heuristics. On the other hand, most exotic targets are small embedded processors where I don't expect to find much vector instructions, so it seems a good match.
You can either try the fluent-rs or gettext crates.
Anchorhead is beautiful, it uses z-machine 8 though, so won't be compatible. If you want a Lovecraftian horror game that uses z3, there's Infocom's Lurking Horror.
I see, thanks for replying! I do get the same thing when using push_str instead of +, is it because both of these functions make some kind of clone of &amp;test? 
'for making a maze..' I'm also a fan of high order functions for tasks like this (processing images.. 'do this for each row' etc , or scanning octtrees, whatever).
Learning a lot about AWS Lambda, or rather, learning that I'm not so sure of the implementation :) https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html https://github.com/aws/aws-lambda-go/blob/master/lambda/entry.go I think the virtual address space may be reused between execs, with processes being frozen before/ after. This means that I'd have to clear my bump allocator before every execution but the allocation itself would already be reserved. Clearing is just adjusting a pointer though so that should be extremely cheap.
... I'm a dummy ignore me. I saw Rust web server and didn't even check which subreddit I was responding to, haha. I thought he meant Rust the game.
In the documentation, `push_str` takes a `&amp;mut self` which tells me that it modifies the existing String. That string doesn't get it's lifetime changed.
Robotics seem interesting, but I have not actually explored that field much myself. I'm in the game industry, and mostly have worked to develop our engine's proprietary physics system (mostly based on soft body positional dynamics). Just starting to explore rust over the past couple of months and it's awesome to see all the numeric work that's been done in the language so far.
[EntropyRng](https://docs.rs/rand/0.6.0-pre.1/rand/rngs/struct.EntropyRng.html) offers an infallible(ish) wrapper for OsRng which will fall back to JitterRng on failure, the slow performance of which shouldn't matter to something like this (not that it should ever really be used). Whether there's any real benefit over ThreadRng is debatable. Keeping RNG state in the kernel helps defend against state leakage a bit more than keeping it in-process, but most uses aren't going to be in situations where an attacker's going to have much, if any, opportunity to perform such an attack. Ultimately I chose it because nobody ever got fired depending on the OS for random numbers. It's strictly one less thing to break, since it's also the source for seeing the in-process CSPRNG, and if you're making passwords and encryption keys and things like that, and aren't performance-limited, you might as well go for the most conservative option available.
&gt; In effect, the service freezes the execution context after a Lambda function completes, and thaws the context for reuse, if AWS Lambda chooses to reuse the context when the Lambda function is invoked again. Yeah, sounds like you need to clear it. You might also need to touch the memory after the initial allocation, in case it's lazily committed. I'm not sure how Linux works, but that's the default behaviour on Windows, AFAIK. Anyway, I suppose this might be a fun project, but do you really think it would have any measurable benefits?
So I just started a client side load balancing + sharding library today, so far I already finished Round Robin, Basic P2C, and a jmp hash based chash ring, and you could provide any type to the balancers as long as it satisfies Clone + PartialEq. The algorithms I plan to support besides what I already have, are P2C + PeakEWMA, P2C PKG, Chash + P(N)C, and Tiered Replication. The crate organization is one for the actual lb-algorithms,the main one which exports the algorithms and adds a layer around tokio, and an optional one that uses a FRP signal for service discovery. The caveats are that, you have to handle connection management and failure detection on your own, since the way it's designed is to be as modular as possible. I'm gonna push it to github soon, this is probably going to be the first crate I publish, so any reviews, or api suggestions would be helpful. I'm also working on a clustering library that uses Hierarchical overlapping ring monitoring protocol, that's found in tipc, the main attraction of it over say Hyparview or SWIM, is the fact that it's delivery properties are more deterministic than gossip (all events are usually propagated within the gossip period which defaults to 300 millis), the fact that any peer in the cluster is at most 2 hops away, and that every node only has 2 * sqrt(N - 1) connections, which doesn't seem amazing but with gossip you really have no clue, with full mesh networks it's (n ^ 2) per node, so like in a cluster of 800 only 56 connections are needed. 
But the GPL code in question is GCC's own code. Code on the Rust side is only forced into GPL if it's legally a derivative of GCC. This doesn't affect any code from rustc.
I think so. You can use `cargo check` to run type- and borrowck. Then use `mrustc` on the crate.
As I've said, there's next to zero docs at the moment, so the best thing wold be to browse the code itself or ping me on GitHub/Reddit/Discord :( I think I'll make writing at least initial docs my next goal. For traversal of the tree, you'd want to use descendants method: https://github.com/rust-analyzer/rust-analyzer/blob/d685a9b564fe524865cda5713c527aaeb1ca6b1d/crates/ra_syntax/src/yellow/mod.rs#L73-L78 To get function definitions, you can use ast::FnDef::cast. Take a look at this code, which walks the file to find all `#[test]` functions: https://github.com/rust-analyzer/rust-analyzer/blob/d685a9b564fe524865cda5713c527aaeb1ca6b1d/crates/ra_editor/src/lib.rs#L116-L137 
Not sure. It would mostly be for fun. "Measurable" is hard to say haha. There's way more impactful optimizations I can make for my project, certainly. I might shave off... a few milliseconds? Maybe?
What is your main focus? What is the thing you need the most? Given that you're new to programming I would first try to divide your problem into smaller ones and order them depending on your needs. Now to your problem. I assume you know the specification. For creating you can create a struct that contains all the important data and methods to add them. You can implement then the Display trait (auto implements ToString) from the standard library on your struct and then write your struct as String with the to_string method to a file (https://stackoverflow.com/questions/31192956/whats-the-de-facto-way-of-reading-and-writing-files-in-rust-1-x). I just made a crate that creates iCalendar files, so you can take a look at [components.rs](https://github.com/hummingly/ics) to see how I implement the Display trait and then the example shows how to write it to the file. For reading you can use parser combinator libraries like nom or pest. Depending on the token you get, you create and add data to your previous created struct for writing. Then to edit it you must add editing methods. That is the fastest solution I came up and that I use in my library.
Thanks for taking a look at the code! Nice to have some confirmation that I'm not doing anything glaringly wrong. :)
Just started working on a sparse octree structure, hoping it will be released as my first crate! I worked on a voxel engine in C++ a couple years ago before getting into Rust more, and got stuck dual contouring. Now that I've got a job and more experience under my belt, I'm ready to give it another shot.
My main focus is a web app that can be used to easily read, store, and create web/light novels. I want to save them to disk and index/tag them in a separate database, NOT store the text inside said database with the index and tags. The thing I need most is support for inserted images, formatting (bold, italic, etc), and *maybe* fonts. Light reading lead me to ePub as a format that is both widely used and seems to tick my need boxes. Sure, I can likely make my own file format (or stash it in the DB), but that defeats the point of making the neatly formatted stories easier to share and read. No, I don't really know the specification. I presume what I'll have to do there is find and read it so my code crates and reflects the patterns the standards say it should. Just had no idea if I would need to do so, so I haven't yet. Sounds like there is no easy path for this since I'll basically need to write my own parser for the format...
Mostly I was just looking for a project to do in rust. Pest looked cool so I wanted to take it for a spin, likewise with serde. Also I think JSON5 is a neat idea and there wasn't a JSON5 parser yet as far as I could find. So I guess it's mostly a learning exercise, but it would be nice if people found it useful too!
Are your binaries fully statically built or how are you getting away with dropping binaries on a nixos system like that?
Rust has affine type system, not linear.
graphs
Why do you want to take `i32` by reference?
This seems to work for me: https://play.integer32.com/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=8639706958a3b51389474b328331d9d8
Polishing my type\_level\_values crate for the 0.1 release. &amp;#x200B; After polish is finished I'll be writing issues for features that I'm leaving for after 0.1.0: * Adding examples to all the items that don't have them (the more confusing items already have examples). * An improved type\_fn using a procedural macro,with descriptions of the grammar and how it translates to the generated code.
I did my interview in Rust for the company I work at now. It didn't compile, but it doesn't matter - it's just demonstrating algorithms, and I program faster in Rust than any other language.
GC can definitely be faster than Rust's approach. A GC like Java's will have a bump allocator, and can defer free'ing memory, or compact the memory, or other optimizations.
I have a question regarding Rust coding practices: Do people actually use something like a Cacher from the TRPL: https://doc.rust-lang.org/book/2018-edition/ch13-01-closures.html#storing-closures-using-generic-parameters-and-the-fn-traits I am just a noob, but still: Good god. Are you serious? All of this extra code and generics and impls just to save a simple return variable? 
I don't - my code is using a type that isn't `Copy` and I did a bad job thinking up a contrived example
How does one handle conflicting trait implementations? They are a problem I've ran into repeatedly, and it seems that there's often no good way to resolve them. &amp;#x200B; Examples: An `impl&lt;'a, T: MyTrait&gt; MyTrait for &amp;t { ... }` exists, and I want to add an `impl&lt;T: Eq&gt; MyTrait for T { ... }`. Or, I would like to do `impl&lt;T: Eq&gt; MyTrait for T { ... }` as well as `impl MyTrait for fxx`. &amp;#x200B; I've been recommended to use newtypes, but when writing a derive crate (as I am doing now), this obviously doesn't work.
Here's [the first installment](https://blog.debiania.in.ua/posts/2018-11-05-how-not-to-start-a-rust-rewrite.html) of what will hopefully become a series. I'm writing these in the "lessons learned" format, so I can reflect and think, and readers can understand what's going on even if they don't know much about Newsboat.
sibling comment is exactly what I had when I played with it.
I have written a rebuttal to that article [on another site](https://lobste.rs/s/dwjgn1/yaml_probably_not_so_great_after_all#c_havijf) a while ago, I find it rather bad and showing poor understanding of what comes from where.
As an example of emulating linearity is my [example 10](https://github.com/rodrimati1992/type_level/blob/master/type_level_examples/src/_10_state_machine/mod.rs#L46) where I use a type to describe the sequence of operations the user of the bi-directional channel has to write before the code compiles. In here I emulate linearity by creating a closure which returns a \`Channel&lt;\_, tlist!\[Closed\]&gt;&gt;\`,meaning that if the closure is not fully written out it will not compile.Of course,this does not prevent the user from panicking,but at least it provides the guarantee that they can't accidentally drop the channel.
Well lucky for me I'm running x64 nixos on both. I had a bit of an incident when my laptop got ahead of my server in nixos versions and then I got the dreaded 'file not found' error trying to run my executable. Once I updated the server then it worked again.
So: &amp;#x200B; \&gt; just to save a simple return variable? &amp;#x200B; It's not clear to me what \*specific\* beef you have with this code, as this example is not about "saving a return variable." Could you maybe say a bit more?
write a systemd unit file to start your rust program. If you are hitting it from more than a handful of clients, run nginx in front of it as a reverse proxy. Would highly recommend not just running it in tmux 
I would disagree that it shows a poor understanding of the language. YAML is what it is. You find its bad parts minor, the author of the article doesn't. &gt; The problem there was the lack of knowledge of the danger of accepting arbitrary class names from the outside on the side of library authors, not the format authors. The same could be said about C's pointers or string handling. &gt; For 24000 words, the YAML spec comes with: a motivation section, a comparison section, an index and a list of standard schemata. So that leaves what, 18k words? --- YAML is extremely complex, and I'd argue it's, more often than not, a poor choice compared to the alternatives. So... "probably not so great after all".
I would suspect it only faster when it's gobbling more memory (size-speed tradeoff), but I have no data to confirm/deny. I am skeptical: it seems deferring whilst having to trace an object graph is harder than allocating (and we can still have pooling to accelerate 'many-small-allocations'). Thats probably it .... the medium size allocations with a bump-allocator will chew up tonnes of memory with deferred frees, and the 'many-small-objects' case will have more to trace to figure out what can be freed.
OK, maybe I - and this is quite likely - have misunderstood why I need to learn about closures. So let me share my thought process. All this related to the example in the TRPL. This is my brain talking: 1. Hmmm OK so the problem is that this very compute intensive function should run as least often as possible. 2. With normal functions this isn't possible. In the best case I need to run it at least once so that I can save the returned data from the compute intensive function in a variable and then just use that variable instead of rerunning the function. But even then I need to run it at least once, even if I dont need it computed. Yes that's not optimal. 3. Ahh ok so there are closures and apparently they solve this problem. 4. Hmmm seems like closures cannot really solve this problem either. But closures enable me to use something called a "Cacher". And with the help of a Cacher I can run the compute intensive function once if I need it so i can store/return the return value into a variable. And once thats been done it saves that variable for using it again and again in the future without needing to rerun the func 1000 times. Got it. 5. Holy shit that is a lot of complicated code with all sorts of advanced topics like Generics, Traits, Impls, etc. Lots of lines of code. Just the Cacher has more LOCs than the actual code we want to run. This looks horrible. There must be something to make this easier. Is there an equivalent of "unwrap()" for this maybe? 6. Hmmm no, doesn't seem like there is. Hmmm will I ever even need this? Is this used a lot? Seems to be a pretty specific problem to have. I think I'll just skip it and continue with the "move" chapter. So my actual question: Are my thoughts understandable? Is there no easier way to implement a "only-run-once-and-then-reuse-the-returned-value-from-then-on" scenario than using a Cacher?
I don't disagree about the systemd aspect - but why nginx? Can't actix-web handle web requests by itself?
&gt; The same could be said about C's pointers or string handling. C pointers are part of the language standard. Using tags for class names literally isn't (and I've more then once seen a "type" field in JSON that does just the same and parsers supporting that). It's a flaw on another layer. &gt; So that leaves what, 18k words? Please don't cherry-pick, I wrote quite a bit on how its competitors are often underspecified, leading to serious problems. I'm not really willing to go into much more discussion around this, just saying that the linked article - IMHO - barks up the wrong tree.
No worries! I appreciate that you care about the contents of the book. Some answers: &amp;#x200B; &gt; Are my thoughts understandable? &amp;#x200B; Yes. &amp;#x200B; \&gt; Is there no easier way to implement a "only-run-once-and-then-reuse-the-returned-value-from-then-on" scenario than using a Cacher? &amp;#x200B; There probably is. I don't do this specific thing in my code often enough to have a go-to solution, personally. My first place to look would be [https://crates.io/search?q=memoization](https://crates.io/search?q=memoization) &amp;#x200B; \&gt; Am I just too much of a noob and Cacher implementations like this are totally normal for pro-Rust-programmers? &amp;#x200B; Not at all! This is supposed to be an example, and it's reasonably contrived, I've never used this specific thing in this exact code. &amp;#x200B; I think what's happened is, this is supposed to be a "hey let's write a thing that happens to use closures" and not a suggestion that this is something you'd write and use all of the time. Maybe I should come up with a new example...
Is it *really* machine learning or just providing suggestions that deal with semantics and not only syntax? Don't get me wrong it looks like it does a few things that I think all suggestion engines should have but I'm not seeing machine learning here. I could be missing the obvious though...
Could you please tell me how to submit an erratum for a book? I wanted to do it many times when I saw mistakes but I didn't know how to do it... :-(
Awesome, really cool! This is pretty much along the lines of what I was thinking of. You should consider submitting this to r/rust, so that more people can see it :) Or post on https://users.rust-lang.org/ I guess. Thanks for the update! :)
&gt; If Newton lived long enough to make laws of allocating and freeing memory, he’d have quipped that in a given scope, number of mallocs and frees are equal but opposite. I think some of the most interesting cases of move semantics are precisely where this rule is violated. For example, if I have a Vec and I move it into a function, the function is going to deallocate that Vec at the end of its scope, even though it didn't allocate it.
It does describe the approach used on the site, it looks like there are two parts: 1) A tokenizer, replacing repeated symbols with abstract identifiers. This is used both during indexing and during auto-completion. 2) A simple machine learning algorithm is trained on the tokenized index, and then used to search for and rank completion results after tokenizing the query in the same way. Finally the actual identifiers are substituted back into the result.
It uses a softmax regression model to rank candidate completions. This helps determine the relative importance of various factors such as: does the token occur in the file, how often does the token occur in the project, does the token often occur in the context for the completion (eg. `unwrap` often follows a dot).
First example in "Enforce order" example brings back the borrowed Letter: pub fn wrap(&amp;mut self, letter: &amp;Letter) { It was removed in the example preceding it, so I assumed we were moving forward. PS: enjoyed the letter delivery analogy.
Is there anyone that knows Rust and Golang? I'm working through the TRPL and if I understood it correctly, this: thread::spawn(|| { // some code }); is the same as this in Golang: go func() { // some code }() The empty(?) closure here is simply used as an anonymous function of sorts? Is that correct/similar? &amp;#x200B;
Great!
Sure. I won't tell you that GC is better or faster. Just that it can be.
Ah, good point, that really reduces the code duplication!
This looks super promising and inspiring! Will use and test it out. How was your experience building TabNine with Rust? Just asking for general feelings, nothing too specific.
This has actually been an observation of mine while learning Rust. There are cases where I find myself having to introduce \`Rc\` wrappers or other workarounds to satisfy the borrow checker in cases where I \_know\_ the type of sharing I want to do is perfectly memory-safe.
I am not sure if Amethyst would be good for you. Read for example this https://www.reddit.com/r/rust/comments/9o5m3z/amethyst_is_growing_news_on_ongoing_projects/e7sjty2/
&gt; Suppose i have a Future which is running on Tokio runtime, how often should i call task.notify() if i don't know the time (or exact time) this future will be resolved. What are the existing ways of doing that properly? `task.notify` is a very low-level concept, which you normally (as an end user) don't mess with. But here are some examples: - Future that reads bytes from a file: `task.notify` will be called when the OS signals that bytes are available to read on that file. - Future that wraps a CPU-intensive function that runs on a threadpool. `task.notify` will be called when the function completes. - Future that's a one-shot timer: `task.notify` will be called when the time runs down to zero. In all cases `task` is an internal implementation detail of how the future works. Most futures don't need to call notify themselves, as they're combinators on top of others: `Future.map` never calls `task.notify` itself, because the inner future will do that (and then the mapping function will complete). The rule of thumb is you should only call `task.notify` if you sometimes return an Async::Pending that you made yourself (not one you got from some other task).
Um, are you sure? Your "Failed attempt 1" and 2 sections have the same code, and the former actually seems to mention `flatten`.
I needed a break from university so I created a [small tool](https://github.com/sondr3/git-ignore) for listing and fetching `.gitignore` templates from [gitignore.io](https://www.gitignore.io). It was quick and relatively painless, I always stumble around for a while with StructOpt before things finally work and then it's mostly smooth sailing (besides some really ugly code, but hey).
I appreciate your patient explanation. Yes, I totally pasted the code for failed attempt 2 into failed attempt 1. Thanks for pointing this out!
Totally. I think that notion of authority is roughly the same as what a lot of Rust writing calls "ownership".
Initial impressions are quite good! Really exciting stuff. My big feature request at this point is that this interferes pretty severely with automatic brace completion; I'm used to my editors automatically inserting )]} as relevant. The specific feature request is that, in cases where a TabNine completion inserts one or more half-open braces, it inserts all the necessary closing braces as well, but sets the point to the correct place *inside* the brace, and uses the snippet system so that the next time you hit tab (outside of a completion context) it jumps outside of the brace.
Very neat! Have you considered making an LSP compatible version of this? Would cut down on the need to implement one for every editor.
This is so cool! Two Questions: 1. Does it automatically prune suggestions that would result in invalid syntax? 2. Can I train it on an arbitrary list of projects (even ones that I am not currently working on?)
I'm currently finishing my TWiR contribution and work on rewriting [flamer](https://github.com/llogiq/flamer) to the new `proc_macro_attribute` interface. When I'm done, I might also convert [overflower](https://github.com/llogiq/overflower) and [mutagen](https://github.com/llogiq/mutagen), but especially the latter is a bit more involved.
How do I deal with a SystemTime before the UNIX_EPOCH? It seems you can't inspect a SystemTime directly and the only "anchor" you compare against is the UNIX_EPOCH. But if I have a file and its modification date is before the UNIX_EPOCH, there doesn't seem to be any way to get that information out? let metadata = fs::metadata(&amp;some_file)?; let modtime = match metadata.modified().duration_since(std::time::UNIX_EPOCH)?.as_secs(); - the metadata exposes the modification time of the file - Can't actually extract it because duration_since expects it to be after the UNIX_EPOCH - except linux supports times well prior to the epoch - Doesn't seem to be any other anchors?
In fact, yes, looking at String's `impl&lt;'a&gt; Add&lt;&amp;'a str&gt; for String`, it's defined as: `fn add(self, other: &amp;str) -&gt; String` which means that it consumes the left-hand String and thus can do whatever it wants with it.
Is there something like the C++ nano Coroutines planned? This just popped up on my mastodon feed: https://www.youtube.com/watch?v=j9tlJAqMV7U and it's pretty impressive
Not a great solution, but: you can reverse the order of the arguments to `duration_since` to get the number of seconds the epoch is after the file modified timestamp.
That's a good idea, I'll add it to the to-do list. Which editor do you use? 
[Here](https://github.com/moshohayeb/tictac) would be my take on it If anyone got a few minutes to kill I would very much appreciate any feedback on my attempt
The danger of misusing a hammer doesn't typically come from holding the wrong end.... In fact a hammer held by the wrong end is probably safer. Just saying.
AWS gives you lots of options. For beginners, I'd suggest [Fargate](https://aws.amazon.com/fargate/) and exploring how to containerize Rust. I suggest [this blog post](https://www.fpcomplete.com/blog/2018/07/deploying-rust-with-docker-and-kubernetes) as a starting point. While that post focuses on deploying to Kubernetes, everything up to the section titled “Deploying our Docker image with Kubernetes” is relevant. For deploying a docker container to Fargate, I suggest looking at [this documentation](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-cli-tutorial-fargate.html).
Yes.
1. It's worth considering. My main concern is that sometimes the correct completion will temporarily cause a syntax error. 2. Ok, I'll add that in a future release.
I know that the core code is closed source, but do you have a place where we can file bugs / feature requests that are not specific to a particular editor?
I've been meaning to do this for a while. I'll make one tonight (and put the config files in it).
You might need to understand what swart pointers are, but you won't need to know all the ones rust provides. You'll know when you need to use one, or to use a `Box`, because some types just can't be made without them. Besides that I don't see anything too important for a starting app in your list. Even skipping `Box` and smart pointers won't be so bad - it might be easier to learn anyways after finding something unrepresentable without them. I'm not too familiar with GTK in rust, though, so I don't know what concepts gtk libraries will need you to know. If you do run into something that seems impossible or completely foreign, though, you can always go back! Get your hands dirty, build something, and you can always learn the rest of the concepts when you run into the in the wild.
&gt;So just coding until you hit up against compile errors isn't a bad strategy for learning. This is exactly what I'm doing!! :-) Thanks!
&gt;Get your hands dirty, build something, and you can always learn the rest of the concepts when you run into the in the wild. Glad to hear it. Thanks for your input!
You mentioned you wanted to learn a "difficult" language. Well, to understand Rust you need to know some CS basics like memory, pointers, heap, stack, ownership etc., which can be quite a big hurdle if you have no CS background. If you want a simpler language for building applications that will still perform quite well use a GC language like C# or Java (or Kotlin or Scala or Swift). Those will be much easier to learn and also help you build your GUI application in less time.
You have the important parts down. Use Rust on future projects - when it's time to use the features you don't understand (smart pointers, traits, RCs etc), it will be evident by a problem you have that points you in their direction. You compared to Python. Think about these features as (somewhat) analogous to Python's generators, async, decorators, inherritance etc: You can do a lot without them, but they'll make sense once you find a use for them.
Why not train it on github projects incrementally (crawling) so that it learns the syntaxes? And the next level would be that it learns the coding style from a given set of repos.. (This could yield a richer context for better predictions.) 
You'd have to check the [spec](https://microsoft.github.io/language-server-protocol/specification), but i assume it allows specifying order. Wouldnt really be much use otherwise, would it?
This is going to be a bit stream of consciousness, so forgive me for that. I have basically two responses to your points about all the weird data types in Rust: - First, I don't use any of them about 95% of the time. Generally I try to program in a style where Traits, generics, and references are sufficient to get everything done. That being said, the 5% of the time I do need them, it's because: - Rust very much tries of emphasize modularity and composition with its type system. This means that, similar to Unix, each data type does one thing and one thing well: - Cell and co. allows you to "bypass" Rust's mutability checking, in a way that is checked at runtime to not violate of Rust's data access guarantees, especially related to threading. - it sounds like you're familiar with lifetimes and ownership. All the smart pointer types allow for *dynamic* ownership, which means that the ownership heirarchy can be preserved, it's just managed at Runtime. Each smart pointer has different guarantees and behaviors; the more powerful ones tend to be more complex and slightly slower (Box provides unique ownership; Rc provides shared ownership; Arc provides thread safe shared ownership). - Though you didn't mention it, Mutex operates under a similar model. - Trait objects are similar to the interfaces of virtually all other languages. They allow use of an object whose type is not known until Runtime. You'll notice that the common thread between all of these features is "runtime" or "dynamic". Because Rust tries to emphasize 0-overhead abstractions, each of these things provides a small runtime penalty, and therefore rust allows you to "opt in" to only the abstraction costs you need to pay. I agree that it's an intimidating state of affairs. My own concerns tend to be more related to places where rust duplicates functionality for no obvious reason: there are many traits that abstract "get a reference" (Borrow, AsRef, Deref), and many others that abstract type conversion (Into, ToString, From, (arguably) Borrow). `str` has no less than 3 built in methods to convert it to a `String`, plus you can do `String::from_str(str)`. Understanding the subtle differences between all these things has been one of my most persistent roadblocks to Rust mastery.
I now post my weekly "what I'm working on" in a thread on our [zulip](https://xi.zulipchat.com). However, since then, and possibly of interest to a segment of the Rust community, I've been dusting off [pulldown-cmark](https://github.com/raphlinus/pulldown-cmark), as there's been interest in a new release and concern over its maintenance. Take a look at recent issues if you're interested.
&gt;You mentioned you wanted to learn a "difficult" language. Well, to understand Rust you need to know some CS basics like memory, pointers, heap, stack, ownership etc., which can be quite a big hurdle if you have no CS background. I understand everything listed here. The problem is that the advanced features of Rust just don't make any sense to me. I get what pointer is. I get what a Box is. I get that a String is a Box. But I cannot think of a single example where I would ever need a reference counted smart pointer. Or a RefCel&lt;&gt;. I'll certainly "lol" and think of this thread/post when I will say to myself "Ahhh I should use a RefCel here" in the (distant) future :-)
&gt; You have the important parts down. Use Rust on future projects That's what I wanted to hear. Thanks! :-)
I'm using Sublime Text as my main editor. How does TabNine interact with RustEnhanced? Can both be enabled at the same time? And how about rls?
&gt;and many others that abstract type conversion (Into, ToString, From, (arguably) Borrow). str has no less than 3 built in methods to convert it to a String, plus you can do String::from_str(str). Understanding the subtle differences between all these things has been one of my most persistent roadblocks to Rust mastery. I fully agree. I have (had) the same confusion here. Also thanks for you other points. It's always nice and helpful to get an insight and background info from more experienced people.
&gt; excluding posts in the wrong sub But you can't use the same classifier for that.. 
Thank you!
Thanks! 
(Small note, cell does not do runtime checking; refcell does though)
“Generators” are the rust equivalent, as far as I know. Well, rough equivalent, there are some differences, for example, rust’s allocate even less, in my understanding.
That’s accurate.
Hahaha, I agree. A hammer you can only pick up by the handle is no safer, and arguably less useful. Maybe a knife ir a saw that you could only pick up by the handle could be a better metaphor? Out perhaps it's intentional playful joking about rust making things hard unnecessarily.
I read a few snake rust codebases, I believe I saw yours. It's interesting to see how `stdweb` vs. `wasm-bindgen` differs in their approach. Added a section in my README for other snake games, including yours: https://github.com/yiransheng/rust-snake-wasm#similar-projects
We just don't update the minimum required version unless we need to for some reason. The official policy for nix is stable-2, but we dont upgrade every releade. It's just easier for our documentation and CI to set it to a fixed version and only update as-needed.
Thanks for sharing. I don't quite understand what you've just explained. (Due to my lack of knowledge) But since there's a concrete Python vs Rust example, I'll look into it tomorrow! 
Tested on vim on around ~2000 lines for code base, sometimes it is sooo slow in my machine
How can I sign extend the bottom x bits of a u32 into a i32? I'm kinda stuck on it for a project I'm working on.
I'm not sure I've ever had to use an `Rc` before, but you'd want to use one when you have multiple objects sharing a reference to a value, but there's no clear single owner. E.g. you're implementing a language like Python, where strings are immutable. Rather than copy a string every time you pass it to a function, bind it to a new name, etc. you just pass around the `Rc`. (If you do need to change the value of the string, you can call `make_mut` which will give you a mutable reference to the enclosed value (if this `Rc` has a refcount of 1) or clone the value for you and return a mutable reference to that.) An example of a use case for `RefCell` is if you want to memoize a method on a type. The method might take `&amp;self` but having the cache wrapped in a `RefCell` lets you update it despite things being nominally immutable. I have yet to use a RefCell in my code either
IIRC, sign extending is converting the value of a narrower type (e.g. i32) into a wider type (e.g. i64). That is the default behavior when casting in Rust, so `a as i64`. But maybe you meant something else?
That's sort of what Microsoft have been doing with the new generation of autocompletion for visual studio. It's a pretty interesting approach to it. Lots of data to train on and code files are fairly will categorized by extension which makes training per language an easier task.
I have also created issue on Hyper repo (was not able to load video there so decided to upload it to reddit) [Hyper issue](https://github.com/hyperium/hyper/issues/1701)
I have a simple fractal program and want to use Rayon. To use an iterator, I put all the X values in a vector. iter() works, but when I switch to par\_iter, it doesn't work. Error: rayon::slice::Iter&lt;'\_, u32&gt;\` is not an iterator &amp;#x200B; Or should I look into a different approach. Worker threads for a given X,Y? `const width: u32 = 640;` `const height: u32 = 480;` `let wvec: Vec&lt;u32&gt; = (0..width).map(|x: u32| x).collect();` `...` `for y in 0..height {` `for x in wvec.par_iter() {` `// lots of maths` `}` `}` Thanks!
This is pretty straight forward when using mutable state: [my implementation](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=88c82d61d4ae129800f1112a80461e78).
Awesome! I wonder if it works as a additional source of completions in Vim (so it could run side-by-side with existin ones), or do I have to try it out after disabling all existing completers.
What is the Rust idiom for "this function accepts an iterator of \`T\` and I don't particularly care whether T is passed by reference or by value"? Context: I have a "constructor" function that needs to take in a list of \`T\` and I want to be generic over any concrete input iterator type. (FWIW, \`T\` in this case is \`\[u8; 32\]\`) Initially, I tried something like fn foo&lt;I&gt;(iter: I) where I: IntoIterator&lt;Item=T&gt; { ... } This typechecks but when I attempted to write a test using a constant array as an input slice, it fails to typecheck because the \`Item\` associated type of an iterator from an array is a \`&amp;T\`, not a \`T\`. OK, fair enough, there is a difference between moving \`T\` and borrowing it. I can make this typecheck by changing \`IntoIterator&lt;Item=T&gt;\` above to \`IntoIterator&lt;Item=&amp;T&gt;\` (and also adding in an extra \`.clone()\`, which works and typechecks with the array above, but now I'm requiring the caller to pass in references to \`T\` when they might not actually have that, so I'd like to be generic over both values and references arbitrarily. I also tried something with \`AsRef\`: fn foo&lt;I&gt;(iter: I) where I: IntoIterator, I::Item: AsRef&lt;T&gt; { ... } ... but this doesn't even typecheck because \`T\` does not implement \`AsRef&lt;T&gt;\` (which also apparently makes the \`AsRef\` trait totally useless because it doesn't do what the documentation says it does). For now the \`Item=&amp;T\` solution seems to work, but what is the preferred way to deal with this? Seems like this is much more difficult than it needs to be.
Random guess: maybe by serializing threads via console I/O you are removing waste of time the threads spend competing for some lock/mutex somewhere, hence increased throughput. DB access is eventually serial and it's common to see multiple threads thrown at non-parallelizable job to end up taking more time. If all the work is done by a single thread anyway, extra mutexes and context switches can only make things go slower. Try repeating the test with 1 thread?
The comment you've linked raises important (and known) issues with Specs. I intend to fix them, but for such a framework, careful design work is needed to come up with a robust and future-proof solution.
We certainly still have a lot of work to do :)
Sorry, my bad. This is addressed in the FAQ.
Optimizing a complicated codebase is a great topic for study. Great to see such an example for Rust; it should allow for deep and meaningful comparisons C and C++ and great discussions about the relevant ease-of-use and safety of the optimizations.
Pretty cool, looks very useful!
&gt; Will the do_thing macro just see this as an identifier of some kind? Yes &gt; Would it be possible to go find the full function definition and bring in its TokenStream so that it can be operated on inside the macro? No. Macros operate purely on the tokens passed to them, and have no way of looking anything up by name or type, or accessing anything defined elsewhere. &gt; Essentially what I'm looking to do here is produce a modified version of a function, without losing the original. It could look like this: let new_fn = transform!(original_fn) where new_fn now has a new line at the top of the function body, like logger::log("In new_fn"), and every subroutine of new_fn similarly has this transformation made. The closest you can get with Rust's current macro system is to make this an attribute macro: ``` #[log_transform(new_fn)] fn original_fn() { fn sub_fn() { ... } } ``` which receives the tokens of the function definition (including children). It could expand to both the original token tree, along with a definition of `new_fn` with the logging inserted. Syn could help you parse these tokens and find the function definitions, but it's just a library for token manipulation -- it can't do anything that you couldn't do yourself in any procedural macro with a lot of code. 
Hello! I agree with the comments made here, Amethyst can be a bit harder to grasp if you don't know Rust very well, but nothing stops you from trying! In case you want to give Amethyst a try, please join us [on Discord](https://discord.gg/meqMUFV). We are happy to help you there 😉
There are long term plans for integration with semantic completers. But at the moment you will need to disable your existing completers, sorry.
I have tried running it in the same poll function with out thread\_pool, acts the same but much slower. impl Future for PutPost { type Item = (); type Error = (); fn poll(&amp;mut self) -&gt; Poll&lt;Self::Item, Self::Error&gt; { let db_connection = &amp;*get_conn(&amp;self.pool).unwrap(); diesel::insert_into(posts::table) .values(&amp;self.new_post) .execute(db_connection) .unwrap(); println!("Hello"); Ok(Async::Ready(())) } } Also simplified version of that pool function wich perform like that is : impl Future for PutPost { type Item = (); type Error = (); fn poll(&amp;mut self) -&gt; Poll&lt;Self::Item, Self::Error&gt; { let db_connection = &amp;*get_conn(&amp;self.pool).unwrap(); tokio_threadpool::blocking(|| { diesel::insert_into(posts::table) .values(&amp;self.new_post) .execute(db_connection) .unwrap(); println!("Hello"); Ok(Async::Ready(())) }).map_err(|e| { println!("Got error in thread {:?}", e); }) }).poll() } } &amp;#x200B;
Thank you for the response. Taking a step back, generally what I would like to be able to do is get the AST of a function, as well as those of any subroutines called, and modify them. Is there anywhere else I should look for this kind of functionality, perhaps `rustc`? 
For a quick and dirty comparison: I (the author of the post) have a lot of experience with these kinds of optimizations now. In short, Rust is great for them. They tend to work, often at the first attempt. They rarely cause correctness or issues, and if they do, it's due to logic problems above the language level, rather than language-level problems such as memory or thread safety. Off the top of my head, I don't recall a single optimization that landed and then caused problems. In short, in my experience the standard claims about Rust's safety and performance hold up well.
I should add: when I say "at the first attempt", I mean "attempt at running", i.e. once they get pass the compiler! Getting past the compiler sometimes takes several attempts, but the compiler's eventual acceptance bestows a high level of confidence.
I have a situation where I either want to increment a value in a hashmap, or if it doesn't exist, then just put in a default value. For example: if let Some(x) = hashmap.get_mut(foo) { *x += 1; } else { hashmap.insert(foo, 1); } However, this doesn't work because "hashmap" is borrowed mutably twice. What's the right way to do this to make the borrow-checker happy?
`*HashMap.entry(key).or_insert_default() += 1;` might be what you want.
And Arc might be fast enough, after all...
[Added](https://github.com/zxqfl/TabNine).
as said in the github issues, changing the default font for macos to "arial" instead of "san-serif" helped a lot. again, a very exciting library, excited to see it stabilize on mac
Did you try `I::Item: Borrow&lt;T&gt;`? There is a reflexive `impl&lt;T&gt; Borrow&lt;T&gt; for T`. From the `AsRef` docs: &gt; The key difference between the two traits is the intention: &gt; Use AsRef when the goal is to simply convert into a reference &gt; Use Borrow when the goal is related to writing code that is agnostic to the type of borrow and whether it is a reference or value You say you don't care whether `T` is passed by reference or value so it sounds like `Borrow` is what you're after to me.
Nope, I excluded both ~/.cargo and ~/.rustup from the backup.
Nope, I excluded both ~/.cargo and ~/.rustup from the backup.
This looks fantastic. I find I'm often using IndexMap and IndexSet (See https://github.com/bluss/indexmap) so that I can have ordered hashmaps. Would it be possible to implement something similar using hashbrown? 
See this thread for justification https://www.reddit.com/r/rust/comments/9i6jv6/comment/e6hb8me?st=JO5F0S7Z&amp;sh=9a08b26d
Yes and the other 90% are `target/` directories
The tricky part is doing this at the right stage of compilation. In my understanding, macros are evaluated before the type system is understood and function name resolution is done. I don't believe there is any stage of compilation that can both access function name resolution and produce new function definitions. With that said, you could build this as an external tool to be used in `build.rs` to generate code - or read the whole source code of the project yourself from the macro. If you used `syn` to parse the whole crate and walk the entire tree of the source, you could probably figure out a way to tell what functions are being called and get their bodies. You'd have to watch out for recursive functions, and wouldn't be able to get function bodies of external crates, but otherwise you might be able to accomplish this? --- Really though, there isn't an easy way to do this. What's your end goal of this macro? Maybe there's some other method to accomplish it that would fit more with how the macro system works?
This will eventually be solved by specialization / "default impl" (https://github.com/rust-lang/rust/issues/31844), but there isn't really a solution right now. Implementations can't conflict, so if you want a blanket implementation, then none of the other ones can fit under it. For `impl&lt;'a, T: MyTrait&gt; MyTrait for &amp;t { ... }` and `impl&lt;T: Eq&gt; MyTrait for T { ... }`, the best I know to do would be to leave the second, and manually implement (or derive) `impl MyTrait for &amp;MyConcreteType` for every `MyConcreteType` which implements the trait. For the second, it probably depends on what exactly the trait does. One solution I've had, though, is to make a newtype for the "blanket" implementation. You could have `struct EqMyTrait&lt;T&gt;(pub T)` and `impl&lt;T: Eq&gt; MyTrait for EqMyTrait&lt;T&gt; { ... }`. This way it would not conflict with other implementations, and anyone wanting to opt into the blanket "eq" implementation instead of a custom one can wrap their struct in the newtype. Those are mitigations, but they can be quite effective. Until we have `default impl` to allow one trait implementation to override another conflicting one, it's all we've got.
Thanks, I was using the flags that benchmarksgame uses (shown at the bottom of the pages), using core2 as target architecture. I'm not sure what it does in the Rust compiler, but as far as I saw in Wikipedia SSE4.1 and AVX are optional for Core2 processors. Still, the fact that Rust compiler beats C easily means that it may be using AVX (we can check the asm output of course). 
Even though TabNine is language-agnostic, can it tell when you are working in different languages from the file type and use different indexes?
I was about to say! One does not simply compile a rust program first try
I feel like, with uniform paths, the proper style will be to just always prefix extern crates with :: (::chrono::NaiveDate). This feels like a unix absolute path, is unambiguous and gives us hassle-free relative paths. Basically the way I see it the difference is do we want the default namespace to be the relative, or the absolute. And when the question is posed like that, relative as default seems more conventional, because that's how file system paths work.
I feel like, with uniform paths, the proper style will be to just always prefix extern crates with :: (::chrono::NaiveDate). This feels like a unix absolute path, is unambiguous and gives us hassle-free relative paths. Basically the way I see it the difference is do we want the default namespace to be the relative, or the absolute. And when the question is posed like that, relative as default seems more conventional, because that's how file system paths work.
Rust is very good at the whole "if it compiles, it works" thing, in part because the compiler yelling at you several times will make you re-check your program more thoroughly and notice any logic errors you've made.
Yes.
&gt; Smart Pointers, Boxes, RefCels, Some more complicated closures stuff, Trait Objects for different Value types, Reference Counted Smart Pointers, etc. Are you f* kidding me? When will this ever stop? So I decided to skip all these topics I just mentioned in the last sentence, because I have absolutely no idea when I would ever ever ever need Boxes, some weird RefCounter type or any of the other stuff. This is way over my head. I'm getting sick of learning all these cryptic concepts. The further I get in the TRPL the more often I ask myself: "What the hell is all this sh*t? Who needs this?" Maybe in 2-3 years when I want to help do a super-performance critical operating system driver. But for a simple GTK App? So I skipped those topics. Yes, perhaps at the moment Rust really is a bit over your head. I might make sense to not directly jump from Python to Rust, especially if you don't know a lot of the CS basics. The Go language might be the right next language for you. It's more low level than Python, you get to use static types and the performance improvements should be more than enough for your use case. I think quite a lot of people have moved from Python/Ruby to Go for their performance critical applications, so the transition might be a lot easier. 
&gt; My question: For building what I want to build. Is there any concept that I am now skipping where you professional Rust developers would say: "Ohhhh if you're building a minimal GTK based IMAP client you DEFINITELY have to look at topic XX. I know this is hard AF to understand, but it will make your final app significantly better! It's definitely worth looking into this!!" I highly recommend reading Clean Architecture by "Uncle Bob". This isn't specific to Rust, but the book explains how to structure your application in such way, that "huge rewrite" is least likely - at worst you'll have to rewrite chunks of your code. Alternatively, you can read Uncle Bobs blog posts or watch his talks, but they aren't as detailed as the book. Since the book doesn't cover Rust specifically, I'll try to summarize how these concepts apply to Rust: * Traits are interfaces. * Interface segregation principle: Try to write functions that accept traits (via `impl Trait` or as generics) instead of types. Also a function accepting `&amp;str` instead of `String` has more generic interface. However, if your function would require to convert `&amp;str` to `String`, you're doing too much in that function and losing performance in cases the caller can provide `String`. Of course, if your function will only `Display` the string, you should take by `Display` trait. * Accepting `IntoIterator` instead of `&amp;[]` is also a great idea. * Each layer of architecture as described by Uncle Bob (circle in his circle diagram) should be *at least* one crate. `http` crate is a nice example here - it contains only entities and itself depends only on three small crates (ideally it wouldn't depend on anything else). Then `hyper` crate depends on it. Of course, HTTP is IO as described by Uncle Bob, so it might look kind of weird, but in my view, good architecture looks more like a fractal, so it makes perfect sense. * The binary crate should only connect everything together - load and parse (using libraries!) inputs, inject dependencies and then hand over the control to the Interactor. * `futures` crate provides low-level primitives for dealing with async code. If you need to define async traits, you should use `futures` crate. Unfortunately, this is currently pain because Rust can't return `impl Trait` from trait methods.
&gt; But I cannot think of a single example where I would ever need a reference counted smart pointer. The most straightforward example is several threads sharing a state. In that case `Arc&lt;Mutex&lt;T&gt;&gt;` or `Arc&lt;RwLock&lt;T&gt;&gt;` are common because both threads must have pointers to the same data. Another common thing is, if you have several threads that spawn and end during the lifetime of the program and need access to same configuration, use of `Arc&lt;T&gt;` can save cloning the data every time.
I responded to someone else, so just making sure you see this: https://www.reddit.com/r/rust/comments/9ulbj3/weird_rust_hyper_async_behaviour_why/e95esc3/ The task.notify() call may be causing that thread to spin at max CPU, which would mean that anything you do to slow that thread down would *increase* throughput because it leaves more CPU for useful work. I'm not sure about this and don't have time to debug though, this is just an idea for something to look at. 
Note that with non-lexical lifetimes, your original example will probably work! The Entry API is still more efficient, though, as it performs one lookup instead of two.
I think only `derive` macros are stable, aren't they? I was considering derive on the struct, but I was unsure how man page generation and shell completion would work. Also I was worried about generating new types (`ArgParseError` etc) from proc macros and the config struct being littered with attributes. Finally, my solution has one nice feature: you can inspect the generated code very easily just by looking into a file in `OUT_DIR`. Of course I agree that proc macros would have nicer interface and, in case of derive, it'd be possible to see the resulting config struct, which would be very helpful when using it. The great thing is that the architecture of codegen crate allows me to implement this approach just by replacing toml deserialization with manual parsing of Rust structs.
Super exciting to see ML put to good use in coding! Based on your experience, how big of an improvement do you think it's possible to make to auto-completion given more data? Also, do you think auto-completion will get to a point where subtle overseeable bugs will be introduced *because* people will put too much faith in it thus overseeing?
I see your reasons! About the proc macro attributes, they are stable now. All proc macros are stable, now; I use them in stable Rust.
Why does the compiler have issues with big constants? I've read that `include_bytes!()` also slows it down, so it must be more than evaluating and validating the expression that's slow. At least initially most code won't require non-lexical lifetimes, so I was thinking that the overhead of NLL could be avoided by only using the new borrow checker for functions that the old borrow checker rejects. But then I remembered that the old borrow checker has soundness holes which I understand not wanting to keep around. With only a 5% average instruction count increase, this would only help if less than 1 in 22 functions require NLL too.
Proc macros stabilized in [1.30](https://blog.rust-lang.org/2018/10/25/Rust-1.30.0.html), released Oct 25 (almost 2 weeks ago)
The compiler does lots of things on a per-basic-block basis. Large constants -- e.g. constant arrays with 20,000 elements -- are the most common way to produce very large basic blocks, which then result in very large per-basic-block data structures.
\&gt; The new Zcash node will be written in the Rust programming language and will be the first alternative client after Zcash Company's [zcashd](https://z.cash/download/), the only full node software available for Zcash. \[…\] Developers with experience in Rust will soon be able to have a greater impact in the space of applicable zero-knowledge proofs and privacy software.
I copied the command line from the web page.
Yes i have removed \` task.notify() \` from there check out git hub repo (still the same issue).
Please, create a new reddit post when you're releasing the first version of the crate. I'm interested in this kind of stuff, and I'll be happy to see such a crate. If I can give you a little piece of advice, don't wait to have a perfect crate to release it. You can release a very basic tool as a `0.1` version, and then enhance it incrementally.
TBQH, I'd _love_ a post on that.
With the print you delay some select calls a bit. With that you make more time for connections to come in and a single select can give you a larger list of ready file descriptors. And you can work faster with more fd in a single call than few in multiple. But that's just my guess. 
You can use something like this: fn sign_extend(n: u32, x: u32) -&gt; i32 { let msb = 1 &lt;&lt; (x - 1); (n ^ msb).wrapping_sub(msb) as i32 } This assumes that `x` &gt; 0 and that the most significant `32 - x` bits are 0. For example for an `x` of 4: n msb n ^ msb wrapping_sub --------------------------------------------- ..000yyy ..001000 ..001yyy ..000yyy ..001zzz ..001000 ..000zzz ..111zzz 
Is there any plan to expose a Rust/C interface so that it can be a dynamic module instead of a subprocess? I imagine it would help with performance. I’m pretty familiar with dynamic modules in Emacs, so I can help with that, if needed.
So what would be the best way to optimise that so i don't have to call print and still have good performance ? 
Something like: let extension = 32 - x; (n as i32) &lt;&lt; extension &gt;&gt; extension
Thanks for all your suggestions. I have a follow-up question about this: &gt;Try to write functions that accept traits (via impl Trait or as generics) instead of types. Currently I'm doing the exact opposite. :-) Why is it better to focus on traits compared to types? What's the benefit? And what would be a specific example for an IMAP client? I'm not quite sure that I understand the advantages of doing so. 
I started learning Go and Rust at the same time. Go is much easier. But for my use case Rust is better. Then GTK support is MUCH better and there's also no overhead for Rust (compared to cgo). And leaving out all the "advanced" stuff, Rust isn't even that hard. It looks like all the things I'm skipping (for now) are not at all needed for my project. And once I'm more familiar with Rust, these concepts will be easier. The problem isn't that it's so complicated. The problem is that there's just way to much and for now I won't need it anyway. I just wanted to double check with you guys just to make sure.
Try replacing the print with various sleep times to test whether or not it is because the print is keeping the thread busy for a brief time.
[removed]
First and foremost - don't get scared or frustrated (too much); Rust has a quite advanced type system, but you don't have to grasp and utilize every concept at once. I myself have no formal education in the CS field (at least yet), never understood what a `monad` is (although I've tried really hard!), gave up after a few days of learning Haskell, but it didn't stop me from trying to learn how things work The Rust Way (TM). I'll say what virtually everyone agrees at, which is: the [official Rust's documentation](https://doc.rust-lang.org/stable/book/2018-edition/) is a really great source of help when it comes to Rust - did you try reading it? It does assume you have a basic knowledge of how programming works, but nothing more - I haven't really ever felt like `was this even written for humans?` (well, maybe except when it comes to describing macros - you'll see for yourself sooner or later). So, going back to your question: &gt; where would be a good place to start learning about type systems? Take a deep dive into the Rust's documentation and start writing programs, it's the best you can do - you'll get to know each concept &amp; idiom one by one, and you'll start writing idiomatic Rust in no time. No guarantees included, but hey - 45k people subscribe this sub, so it cannot be _that_ hard after all! Of course there's nothing wrong in taking a lambda calculus course - quite the contrary! But still - you'd learn how calculus works, not necessarily how Rust does it. By the way: &gt; I have no idea what a sum type is Actually you might have stumbled upon them in the TS - although they are named differently there: https://www.typescriptlang.org/docs/handbook/advanced-types.html
Which version of Rust stable / beta has this change? 
Hmm but do you also think that this would be the case for my current project? All it does is handle the GTK GUI layout, receive/send emails, and read the config/settings from a toml file. Could you give me an example of where I would need something like Smart Pointer or RefCel for this use case?
I'm working on an intermediary proxy for [InfluxDB](https://www.influxdata.com/time-series-platform/influxdb/) to apply intelligence to metrics before passing them on to the actual database. Version 0.1 goals include routing metrics to different InfluxDB servers based on measurement names or tag values, and "massaging" the data itself, initally just providing the ability to strip tags from the metrics. I'm using Actix as the web framework and Nom to parse the metrics. This is my first real project in Rust and it's challenging, but I'm enjoying myself immensely.
&gt; But I cannot think of a single example where I would ever need a reference counted smart pointer. Since you're writing a GTK application, there are examples at hand :-) First thing to consider is that GTK widgets are already reference-counted in the GTK C implementation. Rust GTK bindings provide a `Clone` implementation that will use the refcounting under the hood, and so, in effect, "cloning" a GTK widgets in Rust is equivalent to cloning an `Rc`. Therefore, you won't need an `Rc` for a gtk widget since it already is an `Rc`. _But_ your custom types _won't_ be reference counted and you might need `Rc` for those. One concrete example of usage of `Rc` when writing GTK programs in Rust is event handling. Event handling is done in the GTK binding by passing closures. For example, this is a handler for GTK window resize event taken from an actual Rust GTK prorgam: let somewidget_ = somewidget.clone(); somewidget.draw_area.connect_configure_event(move |_, _| { somewidget_.on_resize(); false }); `on_resize()` is a method on `SomeWidget`. And this is what `SomeWidget` constructor looks like: pub fn new() -&gt; Rc&lt;SomeWidget&gt; { ... } This is a case where the `Rc` is needed. In order to call the `on_resize()` method, the event handler closure needs a reference to the `somewidget`. This, however, _cannot_ be an ordinary reference, because the closure is passed into the GTK binding and GTK internals and needs to be independent. And so in effect, both the code that created the `somewidget` originaly as well as the closure need to "own" the widget, otherwise there would be a risk that a resize event could be delievered to a handler which would reference an object that was already freed - an you don't want that. This is where the `Rc` comes in, it ensures that `somewidget` lives long enough to be accesible by both the code that created it _as well as_ the event handler. &gt; Or a RefCel&lt;&gt;. Again, there is a peculiar quality to the Rust GTK binding in that the widget objects are already kind of like `RefCell`s just like they are already kind of like `Rc`s as explained above. That is, the binding is - for better or worse - written such that you can change properties of widgets through immutable references, which is unusual for Rust. And so, again, you're probably not going to need a `RefCell` for widgets, but you might need them for you own types. For a conrete example, consider again the event handler above. Since `Rc` gives you immutable access to the object it holds, the `on_resize()` method takes an immutable reference. Meaning you can _read_ data from `self` inside this method, but cannot _write_ to them. This is where a `RefCell` comes in (or `Mutex` / `RwLock` in a multi-threaded case, which is typically _not_ the case with GTK event handlers). The specific way you'd use the `RefCell` in this case depends on how you structure your code, how much data you need mutable access to, etc. In the code that I copied the snippet from, the `SomeWidget` struct is actually declared like this: struct SomeWidgetData { // Custom / private data that I need mutable access to go here } pub struct SomeWidget { // GTK objects go here ... data: RefCell&lt;SomeWidgetData&gt;, } And the `on_resize()` handler looks like this: fn on_resize(&amp;self) { let mut data = self.data.borrow_mut(); // ... code that writes to data goes here ... } So there you have it - practical usage of `Rc` and `RefCell` in a GTK application :-) I'm pretty sure you're going to run into similar issues if you're going to write a GTK app in Rust... 
Thanks for the explanation. I want to automatically derive MyTrait for arbitrary structs, based on Eq and MyTrait implementations. This is the reason, that this wrapping won't help me. Thanks regardless. I'll keep an eye on the issue, but I'm sad to see that it's open for two and a half years already.
But does it work for [brainfuck](https://en.wikipedia.org/wiki/Brain fuck) or [whitespace](https://en.wikipedia.org/wiki/Whitespace_(programming_language))? 
speaking of GTK specifically, I needed to use things like RefCell for my Gui program for things like callback functions, which need to own/modify a value which I had stored in my GUI struct. Because i had implemented the callback functions as closures, I needed two owners to the gui elements. (note: there are probably better ways to do this)
yea it's the other side of the coin—on one hand it's \*very\* convenient to be able to install rust as a user, maintain it separately from the system, on the other hand installing a toolchain into home directories complicates backups, and potentially pollutes heavily redundant storage with non-essential data
&gt; Hmm but do you also think that this would be the case for my current project? All it does is handle the GTK GUI layout, receive/send emails, and read the config/settings from a toml file. It's less about what problem you solve and more about how you solve it. &gt; Could you give me an example of where I would need something like Smart Pointer or RefCel for this use case? I'm not saying that you need these for your use case, but that your approach solving the problem might need them, which might be an indication that you're using the wrong approach for Rust, which is hard to tell as a Beginner. What I'm after is that the difference between Python and Rust isn't only on the language syntax level, the typing, how low level it is, but also on the software design level. All of this at once might be too much for you, or not, and you're fine with it. If the pain just gets too much, you don't feel motivated because you stumble into one issue after another, then this might be a sign that you're trying too much at once. Hey, it's your call! Have fun! :) 
I disagree that it's a big hurdle without a CS background. None of these things are complicated.
chattr +d .cargo .rustup
NLL is enabled in Rust 2018 in Rust 1.31, and will be enabled in Rust 2015 early next year.
 // Wether this gets sent via IMAP or gets saved to disk // depends on which provider we created the app with! // Or maybe you end up wanting that POP3 support? // Just write a new client struct that implements the EmailProvider trait! // No need to change a single line of code in here! Ahhhhhh! This is actually makes sense! Thanks so much! See the problem I always face is: Why all this complication when you can have it simpler, easier to read, etc. Quick and dirty example. Code will obviously not be correct: struct IMAPaccount() { // host, port, username, etc. } impl IMAPaccount { fn get_mail(&amp;self, other, vars) -&gt; whatever { mail_data_code } fn send_mail(&amp;self, other, vars) -&gt; whatever { send_mail_code } fn delete_mail(&amp;self, other, vars) -&gt; whatever { delete_mail_code } ...and so on } This is so much more beautiful. No traits, no generics, no complicated stuff, more verbose, easier to read, probably not slower. Downsides. More code (Still way less than Go for example). And obviously being less flexible (refer to the text I quoted from your earlier post). But besides that. Are there any MAJOR downsides to implementing it in a much simpler way like I tried to show in the example?
Anchorhead is a work of art. 
Most simple structs are either all-mutable or all-immutable, and which one they are depends only on the binding. `let v = Vec::new();` is immutable. `let mut v = Vec::new();` is mutable. Things get a little more complicated in the following cases: - Interior mutability: `RefCell&lt;T&gt;`, `Mutex&lt;T&gt;` and friends allow "hiding" mutable data inside immutable data. - References: a struct `Wrapper&lt;'a, T&gt;` can have a `&amp;'a mut T` as a field, and can then mutate T regardless of whether the binding for the `Wrapper` is mutable or immutable. Conversely `Wrapper&lt;'a, T&gt;` with a field `&amp; T` means all access to T is immutable, no matter the binding for `Wrapper`. In both these cases mutability lets you change *which* reference you have, but has no effect on what you can do to the referred object.
Awesome! It seems I missed it. I will re-evaluate my decision and possibly provide proc macro as well. :)
Yeah, see my other comment. :)
Hmmm "your ops people are more familiar with nginx". Unfortunately I'm the ops people, and I already got the TLS thing working. The attack argument is more compelling, something I'll worry about a little down the line though. I don't have a problem of unwelcome numbers or types of visitors as yet. 
I was thinking about doing exactly that. But since the boilerplate was greatly reduced and I'm seriously considering proc macros, it probably won't happen anytime soon.
I'm unclear on what you're asking. I encourage you to write a small example of what you mean. Methods that can only be called on mutable bindings can simply take `&amp;mut self`, while methods that can be called on either one can take `&amp;self`.
Interestingly, the `impl IntoIterator ...` seems to be somewhat tangential to the problem. You can reproduce it more minimally as follows: ```rust trait Trait {} struct S1; impl Trait for S1 {} struct S2; impl Trait for S2 {} fn main() { // Fails let _ = vec![ Box::new(S1), Box::new(S2), ]; // Also fails let _: Vec&lt;Box&lt;_&gt;&gt; = vec![ Box::new(S1), Box::new(S2), ]; // Works let _: Vec&lt;Box&lt;Trait&gt;&gt; = vec![ Box::new(S1), Box::new(S2), ]; } ``` I'd actually think that the two failures make sence: the `vec!` macro sees `Box::new(S1)`, and concludes that's a `Box&lt;S1&gt;`. It then sees `Box::new(S2)` and finds those constraints to be inconsistent. (It's cute that there's a shared trait `Trait`, but it's not the compiler's business to then conclude it must have been a `Vec&lt;Box&lt;OneOfTheTraitsThatHappenToBeCommonInBoth&gt;&gt;`. I mean, the compiler could conclude it's a `Vec&lt;Box&lt;Sized&gt;&gt;` too, but that'd be rather unhelpful. Clearly, though, the infrastructure to do this is in place, since the type annotation `: Vec&lt;Box&lt;Trait&gt;&gt;` makes it all work.
Sorry, I mean that I implement all inmutable and mutable methods for &amp;#x200B; pub struct Array { rows: usize, cols: usize, data: Vec&lt;Scalar&gt;, } And not need to worry for rust, because mutability is applied in binding time. 
&gt; the `vec!` macro sees `Box::new(S1)`, and concludes that's a `Box&lt;S1&gt;` Aside: macros *do not* have access to type information. No macro can do this.
The main benefit of traits + generics is that it eliminates a lot of code duplication and makes the code much more flexible. An extreme example would be a function generic over an iterator vs creating a function for every single kind of iterator. Also, in the case of libraries, generics allow the users of the library to provide their own types. You can keep going the way you do now but I'd say try to keep all IMAP-specific stuff isolated to that struct and avoid leaking anything about IMAP. When the time comes, you should be able to refactor your code quite easily to use a trait instead. So to answer your question, there are no major downsides besides less code duplication and flexibillity that I'm aware of. (Although I consider those to be some very major downsides) Also, note: the IMAPAccount struct itself wouldn't need any generics, just instead of `impl IMAPaccount` you would just do `impl EmailProvider for IMAPaccount`. The generics show up in whatever that uses the provider. (or I guess you can nowadays use `impl Trait`? Looks nicer for sure)
Right, I was using that informally, but indeed, it's not the `vec!` macro that does this, but the typechecker when checking either the `box` syntax or the `&lt;[_]&gt;::into_vec`.
Push down control to the smallest thing that needs it. Learn to split borrows. Regidly enforce separation of concerns. Then the borrow checker will hardly complain.
I found two resources particularly helpful in understanding type systems: - Types and Programming Languages by Benjamin Pierce, which someone else in the thread recommended. TAPL is considered a foundational text in type systems. - [Flow’s type system documentation](https://flow.org/en/docs/lang/). You mentioned familiarity with TypeScript; Flow is Facebook’s static type checker for JavaScript, and it differs from TypeScript in a number of (IMO) more principled ways. I learned a lot about type system features (including variance, nominal/structural typing, depth/width subtyping, flow-sensitive typing, etc.) from using and understanding the Flow docs. Rust specifically uses an _affine_ type system, which stipulates (roughly) that each occurrence of a variable can be used at most once (corresponding to the idea of moving a variable when it’s used without a reference). Affine types are related to _linear_ types, in which each occurrence of a variable must be used _exactly_ once. 
Thanks for answering
So i checked out your repo, and ran it locally: With print commented out: christophe@___&gt; wrk -t4 -c500 -d120s --latency http://localhost:3000 Running 2m test @ http://localhost:3000 4 threads and 500 connections Thread Stats Avg Stdev Max +/- Stdev Latency 169.71ms 105.78ms 830.78ms 72.07% Req/Sec 772.57 112.52 1.07k 72.27% Latency Distribution 50% 152.48ms 75% 224.84ms 90% 303.10ms 99% 505.68ms 368314 requests in 2.00m, 40.04MB read Requests/sec: 3067.56 Transfer/sec: 341.51KB With print uncommented: christophe@___&gt; wrk -t4 -c500 -d120s --latency http://localhost:3000 Running 2m test @ http://localhost:3000 4 threads and 500 connections Thread Stats Avg Stdev Max +/- Stdev Latency 170.31ms 107.50ms 830.40ms 72.07% Req/Sec 771.40 113.08 1.34k 74.39% Latency Distribution 50% 152.73ms 75% 225.50ms 90% 308.85ms 99% 519.17ms 367911 requests in 2.00m, 40.00MB read Requests/sec: 3063.97 Transfer/sec: 341.11KB Which seems reasonable: `print` has a minuscule slowing effect on the throughput.
I think what you said is a perfect example of why I should finally start coding. When I read it it mostly makes sense, but the real world implications/advantages are not that apparent to me. Time to get coding! :-)
I use a pretty obscure editor called [Kakoune](http://kakoune.org/), I don't expect you to spend the time to implement a plugin for it but it'd be great if you released standalone binaries and documented the interface so I could write one myself. I tried to send requests on the command line using the sublime plugin as a guide but every request returns `null`.
Yes, you have no reason to perform runtime checks. Provided you don't use `unsafe`, the compiler will ensure that the aliasing rules are respected.
I recommend checking out https://licensezero.com for a license scheme that’s more open source friendly but still captures the same value as your current model.
&gt; Anyway, my question is: where would be a good place to start learning about type systems? Like what rust has. I don't know that Rust's type system is actually documented in full anywhere. There are resources for learning about type systems in general, and I assume some papers on Cyclone for affine type systems, but the algorithms really aren't in standard texts anywhere, I don't think.
&gt; Very nice. I totally get the point about code duplication. But Rust is complicated enough for me as it is. So I need to start at a basic level. And once I'm better I can still "upgrade" my code. After all this is supposed to be a learning experience. Not a submission to a contest for "best programmer". Yup, this is also the way I learn most things. As long as you just remember that these more advanced features exist and have a vague idea of what problems they solve, you can go back and look them up when you need them. There's no need to learn how to use them right away. :)
As with C++, a lot of the complexity is there for library authors. For example, what's the difference between `Iterator` and `IntoIterator`, and what the heck is `where T: ?Sized`? Honestly, until you start designing your own fancy container types, you can mostly ignore those questions. Some more complexity is there for multithreading. Like what does it mean that `Arc` is `Send` and `Sync` but `Rc` isn't? Until you start using threads, it doesn't really matter. Some especially gnarly bits are there for interfacing with C code, like `CStr` and the `#[repr(C)]` attribute. When you need things like that, _you'll know_. Of course all that complexity is there for a reason. But it sounds like your instincts are pretty good about what you need and what you don't. I think Rust is better than most languages at pushing you into good patterns. Spaghetti code usually won't compile in the first place, so if you have a working app, you probably have decent code. Keep it up, and send us some screenshots when you get some :)
Rust needs to know in compile time if there is a possibility that borrowed variable will be mutated. For example: fn potentially_increment(num: &amp;mut i32, should_increment: bool) { use rand::prelude::*; if should_increment { num += 1; } } When you call `potentially_increment` you already know if the number will be changed or not - but that only happens at runtime. At compiletime, `potentially_increment` can potentially change the borrowed value - and thus it has to be `mut`. In your case, you are making an interpreter - meaning it'll be decided at runtime if the values will be changed or not, based on the interpreted code. Even if you are using a `let a`, you don't know at compile time that the variable will not change - and thus it has to be `mut`.
So if I grab the "Beta (1.31)" right now from https://www.rust-lang.org/en-US/other-installers.html and write "edition=2018" in my cargo.toml I can test NLL and my code? Any other things to watch out for when switching from stable to this beat? But as always, impressive work again and again how Rust is developed!
Yep! https://blog.rust-lang.org/2018/10/30/help-test-rust-2018.html
I just benchmarked the overhead of subprocess communication on my machine. It looks like it ranges from 2 ms (small file) to 4 ms (file large enough to hit the 200KB limit on the amount of context sent). Unless other systems are substantially slower, it probably won't be added.
Awesome, this is exactly what I wanted to hear. It sounds like you know what you're talking about so it is also what I needed to hear. For reasons of not blocking the GUI, I need so spawn threads and I already looked into it. If I have a specific use case it's not that bad to be honest. But if you have chapter after chapter with cryptic advanced topics where you don't even know why you'd ever need them, it can get frustrating! Thanks for your input!!
For a type of `Vec&lt;Box&lt;_&gt;&gt;`, there is incomplete type information, so I would indeed expect the typechecker to fail to unify the types. But in my original code, `breaks()` takes a `IntoIterator&lt;Item = Box&lt;Trait&gt;&gt;`, so there is only one valid typing for the expression. I would expect the `Box&lt;Trait&gt;` constraint to flow into the calling code and constrain the type of the vec items to `Box&lt;Trait&gt;`, as was done for the call to `works()`, but that isn't happening.
There is another big complaint about Java: it is difficult and expensive to call (and being called from) native code. For example I would love to introduce Rust on my organization, but almost anything is written in Java, so I will have problems if I want to reuse a Java library or use a Rust library from Java. I hope Graal and Panama will fix it.
Sorry about that. I'll write a guide sometime within the next few weeks. `null` is returned when the query is invalid. You can get error messages telling you why it was invalid by passing `--log-file-path` to the binary.
Oh you're right: http://gtk-rs.org/tuto/closures I thought this was just some weird beginner problem I'm having. Looks like it seems to come up a lot. But that means that whenever I need to use something like this I have to clone it first. Meaning if I want 10 different buttons to change the headerbar, I need to clone headerbar 10 times? That seems pretty primitive. Does Rust really not have a "nicer" way of handling this?
Thanks, this clears things up quite a bit.
I have 3 files: - main.rs - file1.rs - file2.rs In my main.rs I have "mod file1;" so I can call any "pub fn" from files1.rs via: file1::some_func(); Easy. However: Now I also have file2.rs and I want to use the same function, but not called from main.rs, but form file2.rs. But if I put "mod file1" into my file2.rs file I just get an error, because the compiler is looking in "file1/file2.rs" instead of "file2.rs". All three files are in the same folder! So why is it not working? And why is it not working? Seems like a bug?
I would also like to see writeln! and println! as a proc macro too. The current way of using format_args! to construct Arguments struct doesn't optimize very well, especially on small microcontrollers.
Could you add a link to Niko's original blog post that you mentioned? I could venture a guess at what the phrase 'View Structs' refers to, but I'm not certain.
http://smallcultfollowing.com/babysteps/blog/2018/11/01/after-nll-interprocedural-conflicts/
RUSTFLAGS="-C linker=/use/bin/cc"
RUSTFLAGS is a envelope variable
Have you seen this thread on internals? https://internals.rust-lang.org/t/changing-core-fmt-for-speed/5154 
Thanks, y’all! Now I can strip out the custom .cargo/config from remy!
No problem 😀
Thanks! You can instantiate a view struct by [using destructuring](https://stackoverflow.com/a/32404611/109549) to get references to the individual components of the original struct. Let's look at the example struct and view from the blog post: ```rust struct MyStruct { widgets: Vec&lt;MyWidget&gt;, counter: usize, listener: Sender&lt;()&gt; } struct CheckWidgetsView&lt;'me&gt; { widgets: &amp;'me Vec&lt;MyWidget&gt;, counter: &amp;'me mut usize, listener: &amp;'me mut Sender&lt;()&gt;, } ``` You could create an instance of CheckWidgetsView by doing something like this: ```rust impl MyStruct { fn as_view&lt;'me&gt;(&amp;'me self) -&gt; CheckWidgetsView&lt;'me&gt; { let MyStruct { // Destructure `self` to get the necessary references ref widgets, ref mut counter, ref mut listener } = *self; CheckWidgetsView { // Use those references to create the view struct widgets, counter, listener } } } ``` Here's [a working example of the above](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=e57243a7a90fb452413e50b0bef8fe6c) in the Rust Playground.
To build a gtk app, I suggest you to use [relm](https://docs.rs/relm/). Take a look at [examples on github] (https://github.com/antoyo/relm/tree/master/examples) to take a look. It does not need to be very good in rust to write your first program. You will need to learn how component communicate first. Then you will need to do the networking part. Good luck.
I hadn't. I knew it was shower, but didn't realise it was that much slower.
You can just put the `mod file2` in your `main.rs`. Putting it in the `file1` module would make it a child module (accessed like `file1::file2`), which is why it is trying to look for `file1/file2.rs`.
I'm in a similar situation, but I get the impression I lean more on the "try to find your own solution before bothering others" end of the spectrum. Since you may find it useful, here's a quick overview of why the term "sum type" comes into play. It all comes down to the possible states the value can take. A struct is a product type because, if you have a struct with values `(a, b)` of types `A` and `B`, respectively, then the set of possible values is the cartesian product `(A × B)` of the two fields. (That is, every possibly combination of valid `a` and valid `b` values.) [This Wikipedia illustration](https://en.wikipedia.org/wiki/File:Cartesian_Product_qtl1.svg) makes the concept clear. An enum is a sum type, because, if you have an enum that can be of types `A` or `B`, then the set of valid values is `A + B`. (ie. If they're both booleans, then each one can be `true` or `false` and the domain of `A + B`is `(A(true), A(false), B(true), B(false))`... `A + B`)
Wdym? Sorry
Thank you. So Niko said that these were an "extreme" way to solve some lifetime issues. But he doesn't (in the above) go into a lot of reasons why. &amp;#x200B; Is it because non-overlapping views cannot be used together because of the view still "consumes" the whole struct? Could this be fixed by making views (or something like it) first-class constructs?
&gt; because I have absolutely no idea when I would ever ever ever need Boxes, some weird RefCounter type or any of the other stuff Just for a little bit of context: Every object you ever used in python sits behind something equivalent to Rust's Arc (an atomically reference-counted smart pointer) - so it's not like you never used those things, you just didn't know.
Thank you. This helped a lot. So is lambda calculus sort of a more general subject in relation to type systems? I'm not entirely sure the two subjects are even related 😅 I have been through about half of the rust book, but I found it was a little too focused on the specifics of Rust (for the purpose I'm looking for, ideal for leaning Rust though). Maybe it is just that I can learn about type systems via simply exposure over time. I guess that's how I've picked up the various JavaScript type systems. I might just try on focus on that next. Union types!! Of course! That makes a lot of sense. Alright, baby steps. Thank you again for taking the time.
Haha yes that might be true. What I mean was: I'm not planning on writing my own OS, my own programming language or some other super complex software. Just a simple GTk tool. I hope I will never need RCSPs for that! :-)
Thank you. Those look like some really good suggestions. I'm going to dive into these for sure. Yes, I'm not too worried about being able to be product with Rust, I'm more worried about not being able to enter into a dialogue about the intricacies of the language. But, thank you again. I appreciate the resources.
&gt; TabNine could not exist without the Rust ecosystem Was there anything in particular that makes you say Rust makes TabNine _possible_ as opposed to writing it in another language?
&gt; Java I actually talked about that before on a lang comparison post on this sub, it's one of my biggest pet peeves about Java. https://www.reddit.com/r/rust/comments/8dzzzx/newb_question_whats_a_good_usecase_for_rust/dxrwq76 Yeah JNI is god awful but in terms of being expensive in terms of resources and performance not necessarily there are a lot of optimizations you can use that are just not worth the effort 99.9999% of the time, if you're that in love with rust check out the mechanical sympathy mailing list shit's full of masochistic motherfuckers using FFI in Java and fine tuning it, JNA doesn't suck hard usability wise but incurs a much higher performance cost. But I actually think this will be fixed in a few years, the packaging problem is a mix of legacy issues and the mentality of users, which are the two biggest road blocks you could have. 
Why VM uses isize instead of i64 for Int value?
From my amateur understanding, the high complexity of lifetimes and type system tricks to avoid copies, the weirdness that are associated items in generics etc, are mostly *library tricks to make the user life more pleasant at the cost of complicating signatures*. A simple example is: plain type in argument forces user to 1. use a Copy type to clone the value 2. decide he can afford the move 3. manually clone the value or the library author can use &amp;T if he or she doesn't care about eating a clone or doesn't need to keep the value. Nearly everything else to do with ownership are cases of mostly premature optimization or cases of too much pride imo. As for lifetimes... well, they're pretty necessary to assure correctness of unsafe code, so they're pretty fundamental to create bindings to C libraries from what i've seen, but in pure rust, if they're not implicit, i'm starting to think it's a code smell. 
Very helpful reply! Thanks! wvec.par\_iter().for\_each(|x| { math; }) seems what I'm looking for.
Yeah, that's true -- perhaps a whitelisted set of crates for Kattis would be more appropriate?
&gt; It doesn't feel like it's being taken away when the rules prohibit it. I laughed out loud reading this. :) I use C++ at my job, and we have an excellent way of structuring vendored dependencies, but getting to the point where they're fully vendored is a P-A-I-N.
Of course, as a beginner you might want to start with types and try out traits when you're more comfortable with the language. The main benefit is that when you accept a trait, you can pass in different types. For example, if your function accepts `T: Display` you can pass in `&amp;str`, all number types, many error types, `Arguments&lt;'a&gt;` - the result of `format_args!()` macro, etc. When you compose your program of logical pieces that aren't hard-wired together, you can easily change part of the program without affecting everything else. For instance, I guess IMAP client will store the e-mails locally somehow. So if you define `Storage` trait specifying all the functions necessary for storing the e-mails, you can implement multiple backends (sqlite, mysql, simple file, directory structure, whatever else). Further, traits allow you to implement "fake" backends operating only in RAM that are useful for testing. I try to do this as much as I can and it simplifies testing greatly. I hope this helps. These concepts are explained by Uncle Bob as well.
Thank you
That is strange, have run it many times on arch linux and it does not work like that at all. 
Author here, AMA.
What OS are you on (May be it is OS related)
&gt; If you’re not sure when to use them, just don’t at first until situations pop up where you find them necessary. I also gave this advice, but from reading his responses, I think his point is that he is at that stage... from wanting to do something 'real simple(tm)'. Namely because GTK (or whatever windowing library he's using), like 99% of them wants you to use callbacks and move your closures to another (GUI) thread because of a event loop.
Thanks for your reply. \&gt;So if you define Storage trait specifying all the functions necessary for storing the e-mails, you can implement multiple backends (sqlite, mysql, simple file, directory structure, whatever else). I hear things like this quite a lot, but I just don't understand them. Let's take your \`Storage\` example. I will still have to write the code for sqlite, postgres, etc. And then I just do something like: if user_setting_storage_method = "postgres" { postgres_func(user_data); } else { mysql_func(user_data); So help me understand this: Even if I used traits I would still need to have a postgres\_func and a mysql\_func. Correct? So then what do traits really help with? Having a bit fewer lines of code and maybe therefore a bit more flexibility? I think I'll just have to look into this more at some point. Everybody seems to think traits are the best feature ever. I don't dislike them I just don't see why they're so great. But I'm sure in the distant future I will as well. And now it's time to google who Uncle Bob is. Sounds like a meme!! ;-)
I am also on arch linux (but this is a desktop). I used `wrk` from the AUR to benchmark. Do note that I'm using a longer time period for measurements, and I've decreased the number of connections because 2000 resulted in connection issues, which I didn't have time to debug.
Can it do without any toml files (especially containing Rust code), like in structopt?
The Code terminal is probably slower.
Great piece of work! Congrats!
Well yeah, they all use `format_args!`, ideally you'd just rewrite that as a proc macro.
Ah, automatically derive, that makes sense. Yeah, not sure there's any way to have both an implemention for everything Eq and allow specific structs to override that blanket impl and still be `Eq`.
&gt; However I don't want to regret this decision in 6 months from now, having to re-implement 80% of my code. I disagree heavily-- this is exactly what you should be doing! Write what works, learn what could be better, and incrementally improve both the codebase and your skillset. Besides, I don't think I've ever written something significant and *not* had something 6 months later that I regret / wished to improve.
The default font size is quite small for a 1440p fullscreen browser window. I had to zoom in to 175% to make it legible.
Besides TaPL and other good resources already mentioned, I would recommend: - http://pauillac.inria.fr/~fpottier/slides/fpottier-2007-05-linear-bestiary.pdf ( - http://www.cse.chalmers.se/edu/year/2017/course/DAT140/ One way to learn type systems well is to implement your own parser + type checker + interpreter, so you can make your own mini language; it's also quite fun. :)
Ha! Kudos to you, great implementation of a smart safe mutex. Even if Rust was not used it would already changed the PL landscape for the better since it inspired so many improvements in other languages.
I have been trying to do [rust-koans](https://github.com/crazymykl/rust-koans), I'm stuck with hashmap, specifically with [this test](https://github.com/crazymykl/rust-koans/blob/2e31e3cce84cbed69c8e96c42e5bcf37966bba57/src/koans/hash_map.rs#L56): fn just_the_keys() { let mut map = HashMap::new(); map.insert("Episode IV", "A New Hope"); map.insert("Episode V", "Empire Strikes Back"); map.insert("Episode VI", "Return of the Jedi"); let episodes = vec![__]; for episode in map.keys() { assert!(episodes.contains(episode)); } } First, I don't get what's the idea of what specifically what's doing the assert. I guess it's trying to see if "Episode XX" is contained in the episodes vector. I get that `episode` inside the for loop is "Episode IV", "Episode V" and "Episode VI" (iterating over the keys). I don't get what that `episodes` variable should be.
As I understand your question a vector of Foo structs in Rust is just created as `Vec::new()`. If type inference needs some help you use `Vec::&lt;Foo&gt;::new()`
You can do like this `pub struct foo {}` `let bar: Vec&lt;foo&gt; = Vec::new();`
Sorry, I have to disagree. Just reimplementing format_args! as proc macro will not help with my use case, unfortunately. I would rather not use the Arguments struct at all (because it requires dynamic dispatch and is hard for the optimizer), And instead just have writeln! macro expand directly to calls to specific formatters. But I'm not sure if it's possible to do so.
It seems you're using `rustc` directly instead of `cargo`. Try running `cargo build`.
Apparently VS Code was doing that. With `cargo run` it worked. Thanks.
That's what I wanted to hear! Thanks! :-) (And I actually decided to move along and start coding. So far it seems like it was definitely the right choice!)
Yeah, maybe it is that way. Thanks again!
Glad that worked, by the way: https://www.reddit.com/r/rust/comments/98gqsg/announcing_the_rls_10_release_candidate/e4gkhwq
Interesting. _Takes 30 minutes to read._ Neither of the License Zero licenses appear to be "open source" licenses, in the sense of the OSI definition. The "prosperity" license bans all commercial use of any sort after 32 days, and the "parity" license was rejected by The Open Source Initiative and is almost certainly not GPL-compatible. According to their site, the OSI approval failed because "parity" had stronger code-release provisions than the AGPL: &gt; Extensive debate eventually focused on the fact that L0‑R goes further than existing licenses in when and what code it requires users to release. Eben Moglen, a professor of law at Columbia University, and Director-Counsel of Software Freedom Law Center (formerly of the Free Software Foundation, and before that a clerk to Supreme Court justice Justice Thurgood Marshall) has argued that licenses "stronger" than the AGPL have a slight risk of running into issues with [copyright misuse](https://en.wikipedia.org/wiki/Copyright_misuse) in several US federal court districts. And their "relicense as open source" option uses something called the "Charity License" instead of a better known open source license. Personally, this seems like unnecessary license proliferation. Frankly, all three of these licenses look like something a programmer created on their own, with minimal legal advice. And as somebody who's written a lot of open source (both on my own and at work), I personally wouldn't contribute to projects using the "prosperity" or "parity" licenses, because I'd basically be helping out somebody else's commercial business without getting paid. (I _could_ apparently create something called a new license "scope" and try to run my _own_ business that existed "on top" of the existing business. But it looks like this will inevitably cause the price to go up over time?) So it's certainly an interesting experiment, but I don't think it's particularly accurate to call it "open source friendly", unless you'd say the same thing about the ancient [Microsoft Shared Source Initiative "restricted" licenses](https://en.wikipedia.org/wiki/Shared_Source_Initiative). These licenses were certainly well-known in the early years of the "open source" movement, and nobody considered them "open source" at the time. (Personally, I've written a lot of open source over the years. Most of it is paid for by my employers in recent years. I've also supported open source tools directly, and I'd love to see some kind of Patreon-like model become more popular. For libraries, there's also the option of a GPL/commercial dual license, which gets you something like the "parity" license but uses time-tested licenses.)
Very interesting read!
&gt;The API forces the initialisation of the user-defined type to happen by passing parameters to the constructor of that type. This is because passing an extant object isn’t safe – other references to it may exist in the program and data races could occur. I like how this shows the advantage of having borrowck in rust, but also how you can come up with ways to make up for not having it in other languages.
What is the reason for this delay in 2015?
I'm curious how "pass by value" compares as a solution to this problem in D. Does it implicitly invoke a copy constructor or something like that?
Can you just mask off the bits you don't want and cast it directly? Any positive i32 should be bitwise identical to the same value as a u32. I _think_ this representation is guaranteed.
Niko said “we’re holding off until we have some more experience with people using it in the wild.”
The point of a view struct is that it doesn't borrow the entirety of `self`. So you can't create it via `fn as view&lt;'me&gt;(&amp;'me self) -&gt; View&lt;'me&gt;`, because then you can't use fields of `self` unused by `View`. You have to manually initialize the view every time, or use a macro.
Yep, that’s the one. It’s been super helpful to me since I can glean some of the intent of the abstractions by seeing the underlying representations.
It is currently unstable to use procedural macros to output statements, because of macro hygiene (see [issue 54727](https://github.com/rust-lang/rust/issues/54727)). But other than that, it should be possible.
You are looking for r/playrust
Sorry
That's awesome. I'm not quite sure how to read it though. Let's take the Vec. So according to the CheatSheet a Vec is a Pointer towards a heap allocated memory area. Every element has its own share of memory. But what is with cap and len? They are green and green means "size". But what is "size"? Is that a Pointer as well? Is it just stack allocated memory that holds cap an len? I'm confused.
Parabéns e bem-vindo à comunidade da Rust. :)
&gt; So is lambda calculus sort of a more general subject in relation to type systems? I'm not entirely sure the two subjects are even related 😅 Lambda calculus is a formalised model of computation based on function application. The lambda calculus can be typed or untyped, but the concept itself is highly relevant to understanding the theory behind type systems; programming languages courses that cover type systems will often start by introducing the [simply-typed](http://www.cs.cornell.edu/courses/cs6110/2013sp/lectures/lec25-sp13.pdf) lambda calculus and polymorphic lambda calculus (also known as [System F](https://crypto.stanford.edu/~blynn/lambda/systemf.html)). That said, I believe what /u/Patryk27 is getting at (and please correct me if I'm wrong) is that learning the theory behind type systems is not the most direct route to programming more productively in Rust. Of course, learning for the sake of learning is valuable, and understanding/contributing to RFC discussions can be very empowering! But many fantastic Rust programmers have little to no knowledge of type system internals, so don't let confusion there discourage you from writing more Rust. &gt; Union types!! Of course! In Rust, sum/union types appear as enums. :)
You make many valid points. The only point I wanted to get across is that a License Zero license is definitely a step up from completely closed software, OSI-compliant or not.
Awesome! You've motivated me now. That's very kind of you. I'll be sure to reach out. That is very much appreciated.
A few people have been wondering whether or not this submission satisfies the on-topic rule. The answer is that it does: not only does it meet the criterion of explicitly mentioning Rust, its content is also conceivably useful to Rust users who don't realize what sets Rust's mutex API apart, either because they've never used `std::sync::Mutex` or because they've used mutexes in Rust but not in other languages. To wit, just because the submission focuses on a language other than Rust does not immediately make it off-topic; at least not to the extent that we as moderators feel the need to act. I recommend that people who do believe that this is off-topic to use their capacity to downvote submissions to signal as such (yes, I realize what the on-hover popup says, that's a CSS error and is only supposed to apply to comments :P ).
/u/fgilcher is indeed one of the founders of Ferrous Systems.
And what of Asquera? Is he just running two consultancies now? :P
This is James from Ferrous, by the way. Ferrous has taken over Asquera's Rust engagements, and Florian splits his time between Asquera and Ferrous. Currently Ferrous has some full time people of it's own, while also working together with folks from Asquera who have Rust experience.
A few years ago, I tried to implement this with `format_args!` (patching the compiler to emit direct calls to the formatting methods). It's doable, but my (limited) experiments showed that it lead to larger code and longer compile times.
A sequence of locking chambers where entrants are subjected to progressively worse white-board problems.
I just relearn the macros every time I have to do them and then hope I never have to do them (variadics and const generics, please come sooner 😳).
ok sorry new to app, thanks
[`clap`](https://github.com/clap-rs/clap) has a fantastic ecosystem around it for generating completions. There's even an issue that's planned to be completed for manpage generation, where /u/yoshuawuyts I believe already has an initial implementation: https://github.com/clap-rs/clap/issues/552#issuecomment-406903779 So, here's my question: why not let `clap` handle the heavy lifting for args parsing? :)
It’s not really a “backport.” It’s that there’s no reason to make it specific to a later edition.
i would say similar to this [u8qgehH3kEQ](https://www.youtube.com/watch?v=u8qgehH3kEQ)
Thanks for the suggestion. I just tried this asm!("mov %cr3, %rax" : "=rax"(cr3) :: "mem" : ); and asm!("mov %rax, %cr3" :: "rax"(self.0) : "mem" : ); but appears to make no difference in behavior. I've also previously tried "rax" where "mem" is now, and also "volatile" after the last ":", and had no luck.
This API can also be implemented in C++, and I've done it myself at work.
Then the question becomes why the rationale is benefiting absolute beginners if the primary argument is closed source corporate environments?! Clearly the rationale for this RFC was non existent, maybe you have a point about closed source, maybe you don't and they use english like the rest of the industry, But they were not the rationale for the RFC, teaching beginners was, so the constant need to fall back on "but closed source!" says this RFC was poorly thought out and shouldnt have been accepted, IMO.
 if user_setting_storage_method = "postgres" { postgres_func(user_data); } else { mysql_func(user_data); Well for one, you can't nicely pass backend specific data to the functions unless you use generics/traits or you pack the data for every possible backend into your user_data struct. In Python you could simply duck type it, but that won't work in Rust. It goes a lot further than "fewer lines of code" and "a bit more flexibility". Being able to generalize over methods is huge, and coming from Python, you will probably understand that. It's kind of like duck typing except that your application won't blow up if you try to call a function that doesn't exist, because it won't even compile if you try. I personally think it would've been a better idea to check out C++ or Java before starting Rust, so you can see what Rust's features do for you (and to get used to stricter type systems and interfaces.)
I'm trying to make a very small file server [source here](https://github.com/getynge/microserve/blob/master/src/main.rs) It works, but I'd like to make the base path configurable via clap. My first thought was to pass the base path to get_get file, and change the serve closure to look something like this serve(move || service_fn(|r| get_file(base_path, r))) But then the compiler complains that service_fn takes Fn but the closure is FnOnce because base_path is moved from its environment. Is there a way I can take command line arguments parsed by clap and pass them to get_file without running into issues like this?
As I understand Vec, the stack allocated part is a pointer, whose size can vary depending on the platform (i.e. usize, so u64 on 64-bit systems, u32 on 32-bit systems, etc) and two usize values indicating the size and capacity of the Vec. Size refers to the number of elements in the Vec and capacity refers to the number of elements that the Vec can contain without having to reallocate memory. Then the heap allocated memory is capacity * sizeof(element).
Why does my focusing on one rational invalidate another? The two main beneficiaries are novices and closed source companies. Just because closed source companies benefit doesn't mean novices don't benefit... I'm not sure I follow your argument for rationale being nonexistent. Why don't "helps novices be comfortable" and "helps closed source corporation" exist?
No offense but can you please elaborate? ('m willing to genuinely listen but I grew up with att syntax and seems I prefer it 
I prefer intel syntex as well. [Here](https://imgur.com/a/3yQMCTk) it is. I could use invlpg, and probably will in the future. At this point, I havne't implemented a rust wrapper for it yet, and invalidating the whole thing was easy enough since I already had Cr3 implemented. I had a bug in my original paging code, which led to finding this weirdness. After I found the workaround for this, I found out about my own bug. At this point, I'm just able to take the first 2M page (statically defined in boot.s) and remap it to however many 4K pages are needed for the kernel, invalidate the tlb, and continue running, which is pretty good in my book. I'm still thinking a lot about how I want to handle paging and memory allocation, and I'm also still learning a lot about rust, so what I have now could change drastically overnight at whatever point I realize it is inadequate.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/5DDp2JY.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20e97eila) 
&gt; Is there a way I can take command line arguments parsed by clap and pass them to get_file without running into issues like this? I'm guessing you're running into a lifetime issue because accessing arguments from clap (specifically `ArgMatches`) borrows the value. The `Builder::serve` requires the service to be `'static`, which for you means that the closure must take ownership of anything it uses. But it can't do that with `base_path` since it is borrowed. [Example of the problem](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=e6e42869c2f65e5cad13c361897ca0e1). You can solve this by [copying the variable](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=4fb452eb69e348fd8a4882762b5120cd) so that it is moved into the closure.
Sure. I can provide two reasons: - there is the practical reason: intel syntax is the dominant syntax for most security and reverse engineering tooling. I’ve seen both used in osdev, although att is predominant in existing open source tools - there is a pedantic reason: x86 memory references can contain a lot of things. Specifically: `base+size*index+displacement` (this is from memory, so might have the terms wrong). The general idea is imagine you have an array of structures. You want to be able to do something like `int x = array[i].field `. With intel syntax is is clear: `mov rax, [rcx+rdx*rdi+0x10]`(again from memory, might not be a valid encoding. You get the idea). I’m not actually sure how you’d express this in att. Tbh it was mostly a joke, but I was curious to look at the output in a format I could read without constantly second guessing myself. 
cargo-xbuild oddly enough gives the exact same error as rust-libcore: ``` error: couldn't read "/Users/earlz/.cargo/registry/src/github.com-1ecc6299db9ec823/rust-libcore-0.0.3/rust/src/libcore/../stdsimd/coresimd/mod.rs": No such file or directory (os error 2) --&gt; /Users/earlz/.cargo/registry/src/github.com-1ecc6299db9ec823/rust-libcore-0.0.3/rust/src/libcore/lib.rs:253:5 | 253 | mod coresimd; | ^^^^^^^^ ```
The easiest solution would be to use the Clone trait (but beware this has runtime costs). So for the struct `Bij` you need to do ``` #[derive(Clone)] pub struct Bij { mem: Vec&lt;bool&gt; } ``` then you can change the first part of your function to this; ``` fn add(self, other: Bij) -&gt; Bij { //note the removal of the borrow from "other" let mut out = self; let mut carry = 0u8; let bigger; let smaller; if out.mem.len() &gt;= other.mem.len() { bigger = out.clone(); //clone smaller = other; } else { bigger = other; smaller = out.clone(); //clone } ``` this will compile fine and produce the same outcome you are looking for. You have to remove the borrow from `other` because the type of `bigger` is now implicitly defined as `Bij` not `&amp;Bij`. You also want `smaller` to have the type `Bij`. `.clone()` copies the value over into `bigger` instead of giving a reference to it. The runtime cost of course is that now you are copying the entire `self` struct into `bigger`.
Looks like cargo-xbuild also can't build libcore using stable channel &gt; WARNING: the sysroot can't be built for the Stable channel. Switch to nightly.
&gt; Can you elaborate a bit on treating this as an external tool? Parsing the whole crate with syn and getting the relevant function ASTs sounds like what I want to do. Would I be able to use this kind of approach if I want to make this available as a library for use in other projects? I'm not too much of an expert on making tools like this, but I think you would be able to. The structure I've most often seen is exporting a crate which the user puts in their `[build-dependencies]` and then calls from `build.rs`. They'd then do something like `your_crate::generate_stuff("src/lib.rs").unwrap();`, you would generate the extra rust source, stick it in `OUT_DIR/generated_stuff/src/lib.rs`, then they would `include!(concat!(env!("OUT_DIR"), "/generated_stuff/src/lib.rs"));` and include your generated source code. The best example of this I have is how [bindgen](https://rust-lang-nursery.github.io/rust-bindgen/introduction.html) works - see the "Library Usage with build.rs" section. &gt; Where I pause compilation once name resolution has occurred, get the AST chunks I want (i.e. the AST of the function and any subroutines), and then create the modified functions. What I'm not clear about is how I might be able to "inject" these modified functions into the crate. This looks promising! I've got no more idea how to inject this, though. Maybe you could write out source code, then run the compiler from the beginning again? Not sure. &gt; And finally, assuming the above approach is even feasible, I'm not sure how I could make a library crate affect the compilation process of a user's program. Is that possible? It's possible if the end user modifies their environment, but I don't know if there's a good way to do that in a library crate, and it isn't standard. The main way two ways to affect compilation are crates intended to be used in `build.rs`, and proc macros. Neither of these will give you that intimate connection with the compiler, though. Maybe you could still depend on the compiler, though, and get it to process AST information within your crate called from `build.rs`? You would still have to write out a file to generate the code manually, but it might be possible to get rustc to do your name resolution first... I'm just speculating here, though, as I haven't ever hooked into or depended on `rustc` itself. &gt; edit: the more I think about this, the more appealing the "write out the modified functions to their own module" approach seems. Can I create a module programmatically? Would this kind of thing be suited to a custom cargo tool? `bindgen` (mentioned above) is a good example of doing this, I think. It'd be more suited to a crate required in `build.rs` than to a custom cargo tool, I think, unless you want to have users manually call the tool once and have it then just included in their source.
Um... This is not the sub for the Rust game, this is for the Rust programming language.
Yeah libcore cannot be built with stable, the standard libraries require nightly.
What's the point of no_std executables without cross-compiling though? Pretty sure 99% of the use of no_std is OS dev and embedded stuff, both of which require cross-compiling. Guess I'll just have to wait for things to progress before I can use stable. There's an outstanding issue to integrate sysroot building into cargo itself, but thus far looks like very little progress has been made on designing how it will even work, much less implementation
Everything is incremental steps.
I just got the type-checker for my "C" compiler working. I need to read a bit more about ELF, but I'm hoping to start on codegen this week.
I thought what is that about seems like wrong sub :)
The output constraint isn't in the correct format. `"=rax"` means "the output variable may be in any general purpose register (`r`), **or** an SSE register (`x`)" (and I'm not actually sure what `a` is interpreted as). Note that LLVM only has to match ONE of the constraints [per the documentation](http://llvm.org/docs/LangRef.html#constraint-codes). With --- a/src/cpu/registers/mod.rs +++ b/src/cpu/registers/mod.rs @@ -5,7 +5,7 @@ impl Cr3 { #[inline(always)] pub fn read_register() -&gt; Cr3 { let cr3: Cr3; - unsafe {asm!("mov %cr3, %rax" : "=rax"(cr3) ::: );} + unsafe {asm!("mov %cr3, %rax" : "={rax}"(cr3) ::: );} cr3 } it now generates 100a29: 0f 20 d8 mov %cr3,%rax 100a2c: 48 89 84 24 30 04 00 mov %rax,0x430(%rsp) 100a33: 00 100a34: 48 8b 84 24 30 04 00 mov 0x430(%rsp),%rax 100a3b: 00 100a3c: 48 89 84 24 50 01 00 mov %rax,0x150(%rsp) 100a43: 00 100a44: 48 8d 84 24 50 01 00 lea 0x150(%rsp),%rax 100a4b: 00 100a4c: 48 89 84 24 38 04 00 mov %rax,0x438(%rsp) 100a53: 00 100a54: 48 8b 84 24 38 04 00 mov 0x438(%rsp),%rax 100a5b: 00 100a5c: 48 8b 00 mov (%rax),%rax 100a5f: 0f 22 d8 mov %rax,%cr3 which seems to make the round-trip just fine. You could also be more general and do - unsafe {asm!("mov %cr3, %rax" : "=rax"(cr3) ::: );} + unsafe {asm!("mov %cr3, $0" : "=r"(cr3) ::: );} which just puts it directly in `%rdi` as you'd originally observed: 100a29: 0f 20 df mov %cr3,%rdi 100a2c: 48 89 bc 24 30 04 00 mov %rdi,0x430(%rsp) 100a33: 00 100a34: 48 8b bc 24 30 04 00 mov 0x430(%rsp),%rdi 100a3b: 00 100a3c: 48 89 bc 24 50 01 00 mov %rdi,0x150(%rsp) 100a43: 00 100a44: 48 8d 84 24 50 01 00 lea 0x150(%rsp),%rax 100a4b: 00 100a4c: 48 89 84 24 38 04 00 mov %rax,0x438(%rsp) 100a53: 00 100a54: 48 8b 84 24 38 04 00 mov 0x438(%rsp),%rax 100a5b: 00 100a5c: 48 8b 00 mov (%rax),%rax 100a5f: 0f 22 d8 mov %rax,%cr3 
Edit: Doing a quick search, https://github.com/danigm/epub-rs pops up as a possible crate to use. Perhaps it gives you the metadata you need? --- Besides that, though, you could implement a library interpreter yourself. Going off of https://en.wikipedia.org/wiki/EPUB, it seems like it uses an xml format internally, so something like https://crates.io/crates/xml-rs should be able to do parsing. Then you'd just have to create the data structures and get the right data out. Not tiny, but not too huge of a project to read metadata?
As someone who's been following the project since 0.8, I just want everything to be done already! Guess I need to put on my contributing hat if I'm really serious about that though
I hear you :)
By chance do you know if there is any guide or pointers available for compiling a rust-stable libcore from source (I guess with a nightly compiler?) and then packaging that binary so that other people can use rust-stable but compile for my custom target? 
jacob, it's really great, but could you add a purchase option like paypal or else for people without credit card?
The Go version: https://www.net.in.tum.de/fileadmin/bibtex/publications/theses/2018-ixy-go.pdf
Several of the companies on the [friends of rust page](https://www.rust-lang.org/en-US/friends.html) are hiring. I recommend sifting through that page, clicking on companies that you are interested in, and checking out their careers page. It's not a good way to watch for new opportunities, but if you're looking for jobs to apply to right now then it works pretty well.
For those looking for the source, it is [here](https://github.com/OISF/suricata/tree/master/rust/src). Looks like it is GPL. I wonder if they'd be willing to break out the reusable parts and license them with the standard crate licenses. I know I've been eyeing implementing SMB support and their code could help a lot.
Potentially a dumb question, but have you tried a rebuild? I can't recall if it was with pest or with another parser library I was using, but I learned the hard way that simply modifying the grammar file wasn't enough. The Rust source file deriving the parser needed to be recompiled and that would regenerate the grammar.
Lol. I was thinking the exact same thing about their SMB support. Then I saw who posted and realized - we were probably thinking about using it for the same thing.
It's the sort of puzzle I enjoy: "what the heck **was** the compiler thinking, anyway?" ... honestly, I found this because I've never used the `asm!` macro in Rust, even though I've used inline assembly quite a bit in gcc (but not clang/llvm). Thank **you** for posting a buildable Github project; I've spent the day throwing around `clang` build flags until some BPF bytecode comes out. It's pretty refreshing to spend less than two minutes getting cargo-xbuild and immediately have a testable reproducer.
I think you have two options: 1. Put your `bigger` and `smaller` references inside a scope with curly braces, so that the borrows are forced to end before you need to return `out`. 2. Try this with Rust 2018, which has non-lexical lifetimes enabled. It might just work. If neither of those helps, please post your compiler error.
I've fallen back to an old workaround, which you can see [here](https://github.com/blt/bughunt-rust/commit/d5c218b9a7703a8835716dce17ee8ed0b41ada90). I believed that the fuzz-specific bump allocator would cause very large reservations to gracefully fail. Clearly that's not correct. Anyway, something for me to look into. Should work for you now.
You may have to handle them differently anyway. for i in out.mem.len()..other.mem.len() { if carry == 0 { break; } match (other.mem[i], carry) { (false, 1) =&gt; {out.mem[i] = true; carry = 0;}, //1+1=2 ... out.mem[i] will panic since i is larger than out.mem.len().
&gt; Why does my focusing on one rational invalidate another? It doesnt, but one was the rationale and motivation for the RFC and the other wasn't. So far i havnt seen any arguments that it would *actually* help novices be "comfortable", the main arguments being it helps closed source Chinese shops, even though that wasnt the rationale. The fact that it wasnt shows, to me, that the RFC wasnt thought out enough. All well and good to have an RFC, but if you can't defend it based on it's own motivation.. &gt; Besides all this, I'm not really sure I understand why there's any argument against this. It won't change the open source sphere, and particular care was taken to avoid accidental confusing identifiers. How does this RFC hurt anything enough to warrant the opposition? * It's (imo) unnecessary complication * third party tooling, introduces the need for confusables linting * i don't see who this *actually* benefits, just who we *think* it will. Do closed source chinese shops actually want to switch between input modes for identifiers, and then everything else? The keywords and standard library are all ASCII. It's inconsistent. * I feel there should have been some study or survey or something on whether features like this are actually considered useful. There *should* be plenty of data on this, all the other major languages have it. Does anybody use them seriously? Why havnt we looked? * None of the standard docs or resources are translated, so it currently benefits no one who doesn't know english anyway. * Who says it won't effect open source? Theres no rule it won't. What if someone makes a good open source library, but all the identifiers are Chinese so no one else can use it unless they want to copy paste every identifier? * if the *actual* primary motivation and benefit is closed source, then why is it the default? It should be opt in to those closed source companies that want it.
No good reason, it should be changed to i64 I just haven't gotten around to fixing it https://github.com/gluon-lang/gluon/issues/403
/r/playrust is &lt;- that way
Somewhat related: [Writing parsers like it is 2017](http://spw17.langsec.org/papers/chifflier-parsing-in-2017.pdf) talks a bit about using `nom` in VLC and Suricata.
It definitely can be. There is even at least one for C: https://www.hboehm.info/gc/
I am trying to connect something, which uses rusqlite internally, to actix-web. The problem is that actix-web expects threadsafe state while rusqlite is not (completely) threadsafe. Is there by chance a magical wrapper struct which makes any access to the interior threadsafe? Arc + Mutex doesn’t seem to help.
&gt;References: a struct Wrapper&lt;'a, T&gt; can have a &amp;'a mut T as a field, and can then mutate T regardless of whether the binding for the Wrapper is mutable or immutable. Conversely Wrapper&lt;'a, T&gt; with a field &amp; T means all access to T is immutable, no matter the binding for Wrapper. In both these cases mutability lets you change which reference you have, but has no effect on what you can do to the referred object. This is not true,the binding of Wrapper&lt;'a,T&gt; has to be a let mut: [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=6ed94a61fca7150f02795e6108fbde69](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=6ed94a61fca7150f02795e6108fbde69) In that playground,when I try to compilet this code: struct WithMut&lt;'a&gt;{ mut_:&amp;'a mut u8, } impl&lt;'a&gt; WithMut&lt;'a&gt;{ fn increment(&amp;self)-&gt;u8{ *self.mut_+=1; *self.mut_ } } fn main(){ let mut number=0; let ref_=WithMut{ mut_:&amp;mut number }; ref_.increment(); } This is the error I get: error[E0389]: cannot assign to data in a `&amp;` reference --&gt; src/main.rs:11:9 | 10 | fn increment(&amp;self)-&gt;u8{ | ----- use `&amp;mut self` here to make mutable 11 | *self.mut_+=1; | ^^^^^^^^^^^^^ assignment into an immutable reference
How do you write to stdout without an allocation? `fn main() {` `putn(0);` `}` `// function mostly from Huonw/rust-malloc/blob/master/util.rs` `// he uses a different write statement with unsafe.` `pub fn putn(mut x: u64) {` `use std::io::{self, Write};` `let mut out = [' ' as u8; 20];` `let mut i: isize = 19;` `if x == 0 {` `out[i as usize] = '0' as u8;` `}` `while x != 0 {` `let digit = i % 10;` `out[i as usize] = '0' as u8 + digit as u8;` `i -= 1;` `x /= 10;` `}` `io::stdout().write(&amp;out).expect("Output not written to stdout?");` `}`
Those are some compelling points! I've been looking through the RFC thread to see if I can find more of the rationale than I know. &gt; So far i havnt seen any arguments that it would actually help novices be "comfortable", the main arguments being it helps closed source Chinese shops, even though that wasnt the rationale. The fact that it wasnt shows, to me, that the RFC wasnt thought out enough. All well and good to have an RFC, but if you can't defend it based on it's own motivation.. This comment somewhat addresses this, but unsatisfactorily imo: https://github.com/rust-lang/rfcs/pull/2457#issuecomment-394744141 This comment dismisses the reasoning I had for closed-source companies (at least in china), but does support the "novice" argument: https://github.com/rust-lang/rfcs/pull/2457#issuecomment-394747233 Some people are already writing code in foreign languages, though, using transliterated english: https://github.com/rust-lang/rfcs/pull/2457#issuecomment-394816373 Responding to individual points: &gt; third party tooling. Previously only strings could be unicode, and most third party tools wouldn't care about the contents of your string so can just ignore it, right? Now they have to deal with UTF8, which is also variable width. Do they have to repeat normalization too, but make sure it's the same kind rust does? Even rustfmt. how does it account for length? Thats an issue i saw brought up. I agree with this, partially at least. Tooling isn't perfect, and doesn't have perfect unicode support. But we already have unicode comments and strings, so we already have all the issues with displaying things, right? There are potential issues with autocompletion, but even rustfmt has to deal with multi-width stuff to wrap comments correctly and to treat strings as the right length for the sake of formatting the statements surrounding them. &gt; Speaking of normalization, thats confusing too? Two byte different identifiers can now be the same thing? I... don't really ever see myself looking byte-to-byte at my source code? Unicode unaware tooling would be affected by this, but again, we already have problems with unicode unaware tooling and they should be solveable. &gt; i don't see who this actually benefits, just who we think it will. Do closed source chinese shops actually want to switch between input modes for identifiers, and then everything else? The keywords and standard library are all ASCII. It's inconsistent. I have yet to see anyone refute this point especially. See linked comments above. I'm sure there are more in the RFC, but I'd really rather not look through all of them to find the specific anecdotes. &gt; None of the standard docs or resources are translated, so it currently benefits no one who doesn't know english anyway. Disagree with this - what if I just want to write a variable in my local language in my learning? Sure, someone will have to know english to read the docs, but that doesn't mean they _prefer working in english_. The primary person benefiting from this is someone who knows english enough to use rust, but prefers to write in their native language and thus does so as much as possible. Less mental translation is needed, then. &gt; Who says it won't effect open source? Theres no rule it won't. What if someone makes a good open source library, but all the identifiers are Chinese so no one else can use it unless they want to copy paste every identifier? I'm saying this mainly because open-source software tends to be self-selecting. The English open source community won't use crates exporting non-english identifiers, and thus it seems reasonable that crates which do choose to use non-english identifiers will be just not included in the community. If someone makes a great library with only Chinese identifiers, then it's not a great library. Plus, someone can already do that today, using romanization of characters! No one has made any popular libraries like that because they're a pain to work with, and won't be popular. &gt; if the actual primary motivation and benefit is closed source, then why is it the default? It should be opt in to those closed source companies that want it. That was a motivation I came up with mainly to see if I could shed light on a comment someone up the comment chain made. After looking at the RFC again, it seems like a pretty weak motivation compared to the novice one. &gt; In the end, i feel this RFC was hasty and couldnt properly defend itself, and was primarily merged because it's "trendy" to support and it "feels right", I mean who doesn't want to make it easier for everyone right? but does it? Really not sure how you got "hasty" or "trendy". There were 600+ comments on the RFC! It took over 5 months of intense discussion to decide whether or not this was needed. That seems like anything but hasty to me? &gt; I would've liked for us to look at whether other languages with this feature actually saw any benefit from it, and for a rationale that can properly defend itself. Look into the RFC thread then! There are a number of comments contrasting this to how similar features in python and other languages have been used, and lots of arguments about the rationale that end up being fully defended. I'm not siting them here only because I'd rather not look through 600+ comments. For a few, though, see the ones I linked above that were fairly near the top of the RFC thread. ---- Thank you for going over your reasons, though. I think I understand the rationale quite a bit better looking at your objections and seeing the thread, and I hope my replies aren't _too_ combative rather than informative. I didn't realize we had people who actually benefited either, until I looked and saw the (anecdotal) comments about different situations. 
You probably want to wrap them up \[the other way around\]([https://pest-parser.github.io/?bin=iyjva#editor](https://pest-parser.github.io/?bin=iyjva#editor)).
[I've also done this previously](https://docs.rs/katex-doc/0.1.0/katex_doc/), but I like your explanation better!
Oh i see. thanks i just missed the point that `Vec` was in a struct. 
What's the benefit of fargate over elastic beanstalk?
One little hack you can use to make the compiler tell you what type an expression is is to assign it to a variable with type `()` and see what the error message is. In this case: Adding `let _: () = self;` gives `expected (), found &amp;Wrapper`. So `self` has type `&amp;Wrapper`, not `Vec&lt;String&gt;`. Change that to `let _: () = self.0;` and it gives `expected (), found struct std::vec::Vec`.
I've done a few GTK apps in C before so it may cloud my view of how a structure a proper Rust app. Anyways, there are two ways you could do it, have the gui objects be global (with lazy static) but then you need to use `unsafe{}` to access them. The other way which is what I used is to pack them into a struct but then the problem I had with writing the callback closures is that you have multiple owners because the closure needs a copy of mutable elements. [here](https://github.com/cjschneider2/novluno/tree/master/experiments/rm_viewer) is my project where I used it. It's been a while since I've touched it so it could be wrong. Mea Culpa :)
Sorry for the code mess. I did kind of answer my own question... Except there is one problem, namely the code from Huonw doesn't compile as it is 5 years old. https://github.com/huonw/rust-malloc/blob/5943bc688650d55ac7cd3b875b89bbd820af53f1/util.rs#L48-L65 I was looking for the replacement of write that also doesn't use the memory allocator. Reason I think there is a bug (with memory alignment) in this code: https://github.com/blt/bh_alloc/blob/master/src/lib.rs But it is the allocator :D Any suggestions to what the new way to write to stdout is without allocation? 
Wow thats awesome and the best advise on rust i got so far. Thanks. I think this should has some place in the book for debugging too. 
Just wanted to say thanks for your post on this story, it's reassuring to see the thought that has gone into it by the mod team.
Asquera isn't going anywhere, indeed, splitting off the Rust business allows us to concentrate ASQ on its core business again: ops and distributed systems building, independent of technology. Yes, I'm CEO of both. Also, some employees chose to remain with asquera despite having the offer to switch over at some point, which I'm very happy about.
Hm, I do see your point, but the Friends page is also something we use as proof for actual use towards others. So people lending their name is already something and the exposure in the friends page is rather small.
Thank you for your suggestion regarding \`thread\_local\`. Yes, this might not be ideal with Sqlite. The background is, that i have a \`trait Store\`. The first store i've implemented uses \`rusqlite\`, but there many be others. I tried to connect this Sqlite store with actix-web which fails due to \`Send\`/\`Sync\`. I've never tried to make something theadsafe before. At first i tried to make it work just for \`actix-web\` but then i thought having a threadsafe \`trait Store\` would be nice in general. Some stores might be threadsafe naturally, e.g. by using a threadsafe database API. For Sqlite or in-memory store i hope there is general pattern, where threads can share a non-threadsafe core where every call is e.g. blocking or something. Is there such a primitve but generic struct/pattern?
Hi, thanks for the post and I like the policy. A couple of thoughts on that: * There's already a lot of that stuff here (e.g. Parity news), but it's rarely that these companies offer services to the people on this subreddit. It's complex to draw the line. * I do indeed plan to make the "contract us" part of these posts smaller soon, but it is a pressing issue for a bootstrapped company, so I'm feeling fine with that. * I think giving a glimpse into business development is a useful thing for this community, as the question of commercial viability and jobs is a frequent one. For example, the fact that we'd be so successful in _selling_ embedded Rust was surprising to all of us. I'm unexperienced with writing that down, though, so I hope that will improve and become more interesting. * We still have a lot of things to cover concerning FOSS involvement, especially structure and I'd like to present the thought process there. I'd be happy to take questions to answer!
Thank you for that! 
You forgot to mention that lazy_static can now be used without unsafe, so you can `no_unsafe` your code! :-)
Wow! Great analysis! I was staring at this code for so long, complaining about this borrow, and didn't realize this looming problem! Thank you for pointing this out.. Now the breakout makes more sense, since it will need to push to the vector instead of simply updating the value. 
&gt; Some people are already writing code in foreign languages, though, using transliterated english: Another thing, this RFC overwhelmingly benefits "kinda ASCII" languages like that, but it doesn't seem worth it to me because they seem to be able to get by just fine anyway. At least things are consistent, both with the rest of the character set and the rest of the professional industry. &gt; But we already have unicode comments and strings, so we already have all the issues with displaying things, right? Well, thats what i mean. Third party tooling wouldn't usually have to *care* about any of that, right? It can just pass those bytes right through, no need to understand them, not unless it's doing anything *to* those strings or comments. &gt; I... don't really ever see myself looking byte-to-byte at my source code? Unicode unaware tooling would be affected by this, I meant about the third party tooling, yeah. If they don't normalize identifiers the same way, shenanigans could ensue for example. And what about confusables? rust may lint them, but will my IDE? If i don't use RLS? Viewing? What about github? &gt; &gt; i don't see who this actually benefits, just who we think it will. Do closed source chinese shops actually want to switch between input modes for identifiers, and then everything else? The keywords and standard library are all ASCII. It's inconsistent. I have yet to see anyone refute this point especially. &gt; See linked comments above. I'm sure there are more in the RFC, but I'd really rather not look through all of them to find the specific anecdotes. Yeah, good finds. The first comment you linked seems to support me, saying that languages like russian, chinese, or Japanese with completely different character sets tend to just use ascii for identifers, even in languages that support unicode like Python, C++, and javascript. Though for some reason the comment is in benefit of the RFC despite pointing out that accents arent used even in languages that support them, and that the non-english programmers they asked said they won't use native characters because tools(not the languages) don't support it, or they don't want to. Thats an.. interesting conclusion, to me. The stuff they linked and people they asked suggest unicode identifers aren't used, even when supported, or only sometimes in beginner tutorials(but if it's only beginners, and only sometimes, then that means they move past it pretty soon?) The second comment i *definitely* agree with, even in languages *with* unicode identifers they aren't used and are even *unprofessional* Maybe it helps absolute beginners, but to that i say: So? Should the language be made more complicated just so absolute *beginners to programming* and non-programmers have it maybe *slightly* easier?! Why? Rust is not a teaching language, it isnt a toy language, Rust is a general purpose systems language and shouldnt be designed for the lowest common denominator, and the way i see it unicode identifiers just unnecessarily complicate things for little if any benefit. I found [this](https://github.com/rust-lang/rfcs/pull/2457#issuecomment-394827110) comment which goes into some detail about some of my points on transliteration, switching input modes, poor RFC motivation, and whether non english programmers actually want/would use these and why we havnt looked at other languages. And even debugging and other tooling. &gt; Disagree with this - what if I just want to write a variable in my local language in my learning? &gt; &gt; Sure, someone will have to know english to read the docs, but that doesn't mean they prefer working in english. The primary person benefiting from this is someone who knows english enough to use rust, but prefers to write in their native language and thus does so as much as possible. Less mental translation is needed, then. Well, so what? You may find it slightly easier as a beginner, but everything else seems to suggest *only* as an absolute beginner. Variable name language seems like a small price to pay for keeping things simple. Is it really that important that the whole language and all tooling should be made more complicated? &gt; and thus it seems reasonable that crates which do choose to use non-english identifiers will be just not included in the community. Which is something i kind of see as a problem, islands of libraries. English is the lingua franca of programming, if you know english you have access to pretty much *all* programming resources. If things get increasingly subdivided by language then you'd have to become a polyglot to have access to everything! &gt; Plus, someone can already do that today, using romanization of characters! The way i see it, at least everyone can type those, and possibly remember them too! Languages have patterns after all, they aren't all random, so can be easy to remember. At the very least, autocomplete can help here. If the variable is named `ドア`(door in Japanese), only japenese people can comfortably use it, or sometimes even tell it apart from other characters! However, if it was in romanized to `doa`, everyone can hit `d` on their keyboard and autocomplete it! It may not be pleasnt for an english programmer to use, but i imagine i can remember that `doa` is door. (Japenese is especially an interesting case because [everyone whose been to elementary since ww2 has been taught to read and write romaji](https://en.wikipedia.org/wiki/Romanization_of_Japanese)) &gt; If someone makes a great library with only Chinese identifiers, then it's not a great library. &gt; [...] &gt; No one has made any popular libraries like that because they're a pain to work with, and won't be popular. I will admit the scenario is unlikely &gt; I'm not siting them here only because I'd rather not look through 600+ comments. &gt; For a few, though, see the ones I linked above that were fairly near the top of the RFC thread. Well, i don't want to sift through them again either! it's a long thread, and threads are hard to follow too. It's just one stream of many different conversations all interwoven together! Shame comment trees like reddit havnt caught on anywhere else.. make it so easy to track a conversation, and if you hate yourself and want classic style, it can be trivially sorted to display linearly by post date! &gt; I didn't realize we had people who actually benefited either, until I looked and saw the (anecdotal) comments about different situations. The comments you linked don't read, to me, as would benefit from this? Do you mean others? ---- &gt; Thank you for going over your reasons, though. No problem, happy to go over them and would be happy to being proven wrong too. Not like i don't want things to be easier for our non english friends or anything, i just don't see this change making it easier for them, or anyone else for that matter! Third party tooling will always lack, existing programmers already have their way of coding with stuff like transliteration, and because of that last point it can even be seen as unprofessional to use Unicode identifiers! At the very least i feel this should be an opt in feature for the people who want it, like absolute beginners, as the motivation for it as a default is pretty weak.
Pass by value in D usually moves. I'm not sure I understand the relevance though.
Rust doesn't have variadic templates, so this option wasn't even available anyway. I can't see how macros would make up for it but maybe that's just my lack of imagination.
Thanks, probably message passing is what i am looking for.
&gt; xss-probe on docs.rs: a Rust crate that injects JS and CSS into rendered docs.rs documentation by using build.rs to rewrite some files on the docs.rs build machine. holy shit
Short answer, you can't. The only way to extend the lifetime of a variable defined on the stack in a function is to change owners (by returning the variable, which is a move). &amp;#x200B; &amp;#x200B;
What target do you work with? Some embedded targets now have a prebuilt libcore in rustup, which you can use on stable.
Where did you learn all of this ? Is there a document explaining how `asm!` works anywhere ? Like, every time I use `asm!`, I have to refresh all this, and all of the existing documentation is woefully incomplete, or only touches one architecture (e.g. arm instead of x86/x86_64/aarch64/...), or is broken in nightly, or... I wish there was a rust book for this with examples that are maintained updated with nightly.
Edited the description with the code I was trying .
There is no way to make the `x` in `with_int` last long enough to be stored in the `x` in `main` because it is in an inner scope, but you can make the lifetime of `y` long enough to be stored in x like so: ```rust fn with_int&lt;'a, F&gt;(f: F) where F: FnOnce(&amp;'a isize), { static X: isize = 3; f(&amp;X); } fn main() { let mut x = None; with_int(|y| x = Some(y)); } ``` [Playground here](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=e0a3cf83d319ec152e8eea7d804c9718)
I've put the explanation at the end of the README. Short version: I don't like stringly typed interface of clap and want to handle files as well. Man page generation is WIP and completion is planned.
Not now, I'm considering supporting proc macros in the future.
Thanks for sharing, I'm looking forward to the video being uploaded.
Relicensing requests come in all shapes and sizes, and aren't just limited to requests for removing copyleft. I've been the recipient of many requests to relicense away from using licenses such as the UNLICENSE or the WTFPL. &gt; The only reason you are asking, is because you're building non-open source software yourself. This isn't necessarily true and speculating on the motivations of someone else isn't cool. (As a counter example, I don't permit the use of copyleft dependencies in my Rust programs, but my Rust programs are not proprietary. I do this for both ideological and practical reasons.) With that said, as the recipient of relicense requests, I do try to refrain from making unsolicited relicense requests. They are tiring and tend to lead to off topic discussions, such as this one. :-/
Can't you simply capture `state` by value? `program::run(move|ui: &amp;'ui Ui| {state.set_renderer(ui); do_stuff(&amp;state)});`?
 impl SomeStruct { fn new&lt;F: FnOnce() -&gt; SomeType&gt;(fn: F) -&gt; SomeStruct { do_something_with(fn()) } } macro_rules! deferred_call { ($fn:ident, $($arg:expr),*) =&gt; { SomeStruct::new(|| $fn($($arg))) } } let x: SomeStruct = deferred_call!(SomeType::new, some_arg1, some_arg2); No idea how you'd refer to the type of each parameter, so I think D's templates and your code are a cleaner solution.
Ah, ok. Not so in D - moving a value with indirections means the moved-from object still has the same indirections. Since D by default uses a GC this is not usually an issue. This is why I have a static if that only allows one to pass a value of a type with no indirections. All of this of course mitigated by just creating the object at the same time by passing in the relevant parameters and forwarding them.
I'm sorry I cannot be of help here, but: very good question! I'm in the exact same situation right now. And I agree that gtk-rs is awesome! 
I'm not experienced with GTK but in Cocoa apps you might do this by sending an NSNotification from the lower level parts that the GUI parts have registered a listener for. Not sure if there is anything analogous built into Gtk but a message passing or actor system (maybe the actix crate) might help.
Thanks. That worked 👍. And thanks for pest. I find it simple and easy to use and love the documentation.
&gt; But I cannot think of a single example where I would ever need a reference counted smart pointer. Or a RefCel&lt;&gt;. &gt; But I cannot think of a single example where I would ever need a reference counted smart pointer. Since you're writing a GTK application, there are examples at hand :-) First thing to consider is that GTK widgets are already reference-counted in the GTK C implementation. Rust GTK bindings provide a `Clone` implementation that will use the refcounting under the hood, and so, in effect, "cloning" a GTK widgets in Rust is equivalent to cloning an `Rc`. Therefore, you won't need an `Rc` for a gtk widget since it already is an `Rc`. _But_ your custom types _won't_ be reference counted and you might need `Rc` for those. One concrete example of usage of `Rc` when writing GTK programs in Rust is event handling. Event handling is done in the GTK binding by passing closures. For example, this is a handler for GTK window resize event taken from an actual Rust GTK prorgam: let somewidget2 = somewidget.clone(); somewidget.draw_area.connect_configure_event(move |_, _| { somewidget2.on_resize(); false }); `on_resize()` is a method on `SomeWidget`. And this is what `SomeWidget` constructor looks like: pub fn new() -&gt; Rc&lt;SomeWidget&gt; { ... } This is a case where the `Rc` is needed. In order to call the `on_resize()` method, the event handler closure needs a reference to the `somewidget`. This, however, _cannot_ be an ordinary reference, because the closure is passed into the GTK binding and GTK internals and needs to be independent. And so in effect, both the code that created the `somewidget` originaly as well as the closure need to "own" the widget, otherwise there would be a risk that a resize event could be delievered to a handler which would reference an object that was already freed - an you don't want that. This is where the `Rc` comes in, it ensures that `somewidget` lives long enough to be accesible by both the code that created it _as well as_ the event handler. &gt; Or a RefCel&lt;&gt;. Again, like with `Rc`, this is another quality that the many of the Rust GTK objects already have, that is, they already are like `RefCell`. That is, the binding is - for better or worse - written such that you can change properties of widgets through immutable references, which is unusual for Rust. And so, again, you're probably not going to need a `RefCell` for widgets, but you might need them for you own types. For a conrete example, consider again the event handler above. Since `Rc` gives you immutable access to the object it holds, the `on_resize()` method takes an immutable reference. Meaning you can _read_ data from `self` inside this method, but cannot _write_ to them. This is where a `RefCell` comes in (or `Mutex` / `RwLock` in a multi-threaded case, which is typically _not_ the case with GTK event handlers). The specific way you'd use the `RefCell` in this case depends on how you structure your code, how much data you need mutable access to, etc. In the code that I copied the snippet from, the `SomeWidget` struct is actually declared like this: struct SomeWidgetData { // Custom / private data that I need mutable access to go here } pub struct SomeWidget { // GTK objects go here ... data: RefCell&lt;SomeWidgetData&gt;, } And the `on_resize()` handler looks like this: fn on_resize(&amp;self) { let mut data = self.data.borrow_mut(); // ... code that writes to data goes here ... } So there you have it - practical usage of `Rc` and `RefCell` in a GTK application :-) I'm pretty sure you're going to run into similar issues if you're going to write a GTK app in Rust... 
&gt;I'd advise having the GUI part query the backend to obtain data from it rather than having the backend access GUI elements. I agree and that's how i do it in my app currently. But I'm just a beginner and I thought there might be a slick Rust-ian way to achieve this. If there isn't it won't be a major problem either.
I want to keep things as simple/minimal as possible, so an actor model is probably overkill. I just wanted to know if there's a simple Rust-ian way to do this. If there isn't it is fine and I'll just use a workaround. My app currently is setup so that my GUI code triggers other code and then just displays what is being returned. That will be about 99% of my final code. I just wanted to make sure there's not something I'm missing before going all in.
That's actually one good reason. Long ago when I used to program in assembly- I had the exact dame thought (that i recall now after seeing your comment) it objectively sucks but I don't like the macro shit. NASM I think was best of both worlds if I remember correctly 
Just in case: did you look at [gtk-rs/examples](https://github.com/gtk-rs/examples)?
Yes I have. They have a similar approach. They have a main function and then just call a build_ui function which does all the initializing similar to my example above. However their examples are more about showing how GTK elements work and there's only very little/no "actual" code.
If they don't want to relicense, I'd respect that. I was talking from the perspective of making crates out of their code for the rest of the ecosystem to be able to use. While a relicense isn't required for that, the Rust ecosystem has [standardized on dual-licensed MIT and Apache](https://rust-lang-nursery.github.io/api-guidelines/necessities.html#c-permissive) and I'm in the middle of the long process to [relicense to follow the ecosystem](https://github.com/cobalt-org/liquid-rust/issues/7). I personally have come to the point where I don't have a preference on open source licenses and the license doesn't matter for my use case for this code.
Also Redox OS?
I took a look at it and I have to admit. This is way over my head. RefCels all over. I could tell that they partly used structs, but as I said: Way over my head. I want to keep everything as simple as possible. If there are 2-3 instances where I need to have some workaround from within my GUI code, but thereby save myself from this RefCel complication mess, I'll happily use the workaround! :-) Thanks a lot for the link and input!
&gt;The main issue is that GCC's very architecture is adversary. GCC's architecture is driven by political goals, rather than technical ones: it was conceived in part by R. Stallman with the explicit goal of forcing distributing as GPL any code that would integrate with GCC. Why it would be an issue that Rust backend for GCC would carry GPL?
There's also bigger gtk-rs applications like [Fractal](https://wiki.gnome.org/Apps/Fractal) and the [GNOME Podcasts](https://wiki.gnome.org/Apps/Podcasts) application that can be useful for getting ideas for design patterns. Generally for your data you'll probably end up with some kind of `Rc&lt;RefCell&lt;_&gt;&gt;` wrapped struct for your application data, window data, etc. that is passed between the different callbacks. And probably also store references to various widgets that you need elsewhere inside those structs. You can also cheaply clone GTK widgets: it's all reference counting. There's no problem with having multiple owners.
I haven't used Elastic Beanstalk in any capacity, so I can't say for sure. I _have_ used Fargate and EKS, and I've enjoyed using them, hence my recommendation. I suggesting trying both out Elastic Beanstalk and Fargate/ECS/EKS!
&gt; C is rightfully called “portable assembly”: it is designed to closely match the abstraction of the machine itself It is designed to closely match the instruction set of the PDP-11. Modern machines? Less so. &gt; C features memory addressability at its core But you can't even address the first byte in memory :( Also I believe casting an integer to a pointer and using it is strictly implementation defined, so you you can't really interact with memory mapped things in pure C in a way that is guaranteed to work for all compilers (even on a particular platform), you have to get the pointer to that memory from "outside". &gt; With only a few exceptions (e.g., Haiku), serious attempts at C++-based kernels withered From what I've heard Microsoft Windows also uses some C++ in the kernel, although it's probably mostly C in there. &gt; Java-based operating systems like Sun’s JavaOS fared no better; hard to interact with hardware without unsigned types! Huh, what? I'm not a fan of Java not having unsigned types either, but is that really the reason a Java operating-system doesn't work? Java demands two-complements integer representation, so addition, subtraction, and multiplication already give you the exact same bits as a hypothetical unsigned integer would. &gt; Rust has a novel system of ownership, whereby it can statically determine when a memory object is no longer in use Well, sorta, kinda. You can still create reference cycles, and not running destructors is not considered `unsafe` (hence `forget` is safe).
&gt; Why does this project not have a main.rs ?? Isn't that mandatory? There are also other options, such as having a `src/bin` directory (to have multiple executables), or as in this case, specifying in `Cargo.toml` what the main entry point is: ``` [[bin]] name = "process_viewer" path = "src/process_viewer.rs" ```
Starts `cargo build` or `cargo test` in docket container depending on `--target`. The containers also contain some native libs like OpenSSL.
Not a single day goes by where I don't learn stuff about Rust that I didn't actually intend to learn about Rust. Nice. Thanks for the explanation! Actually one last question: WHY would you want to even do that? What's the benefit? 
&gt; you can setup a gtk timer and update the widget's state from within its callback Really only use timers for something like this if you have a state machine pattern properly implemented and a global way to register handlers within a specific namespace ... ... unless you like race-conditions. Best is to simple avoid them, Timers + UI-Input = Hell opens. 
Hey, they didn't mention nebulet :(
I don't know! with no further reasoning I also prefer one of the methods to make cargo automatically find executables. Makes it easier for contributors where to start looking.
&gt; I am not denying that within Rust there may be sufficient workarounds that you never need to do what I described. I am simply claiming that prima facie they are good reasons why a program (not specifically a Rust program) would hold two pointers to the same location in memory. &gt; ... &gt; Yes, it is terribly bad form, bad for longevity, resilience, maintainability, portability, etc. Then again, so are most other forms of extreme optimization. Yeah, this is kinda what I thought. Definite reasons for dual pointers but Rust makes it harder to use them without making it apparent you are doing something that needs extra care and attention paid to it (both when writing and reading). Thanks for the reply!
Just a note, the organizers cancelled the Utah Rust meetup this month due to babies. They're definitely planning on holding it next month though!
[removed]
[removed]
num-derive crate.
https://github.com/antoyo/relm As far as I understand it you'll have to think around the corner a bit anyway due to the nature of Rust - MVC patterns might help you with that and above Framework seems to have gone a long way for asyncronous UI state handling already. Maybe it'll help you.
It's because a long time ago, it started looking in the file which had the same name as the project, not in `lib.rs` or `main.rs`.
I don't consider these to be perfect analogues. Friends of Rust is a page whose only purpose is marketing, and while I think it would be ideal if companies there each had a blog post/whitepaper discussing their use of Rust, until such time as FoR is overflowing with requests to join there's little reason to deny anyone who claims to be using Rust. People can lie in a blog post with little more effort than lying in a GitHub issue. In the meantime, /r/rust is a community space that happens to tolerate marketing (for now), and I guarantee that the subreddit has more traffic than FoR. Maintaining the goodwill of the community requires keeping commercial submissions from getting out of hand, which is what this policy seeks to achieve.
Ah ok interesting. Some Rust history! :-)
I shared a struct that stored my widgets in [boxcar willie](https://github.com/bitemyapp/boxcar-willie)
I knew your username rang a bell. I've watched your Rust + GTK Youtube videos! :-) But back to topic: If you use a struct you have to mess around with Rc/RefCel, is that correct?
I have no questions, and I look forward to seeing what you come up with. I'd hoped the tone of my above comment had signalled my approval of the way you're managing this. :) As for other companies, I anticipate they will find ways to comply with the policy; Parity already open-sources their code to community benefit (one of their repos even gets mentioned in https://rustwasm.github.io/2018/10/24/multithreading-rust-and-wasm.html ), PingCap hires out of the Rust community, and so on. It may be a complex line to draw, but we will attempt to draw it nonetheless and see how that works out.
This looks really cool!, I'm a bit confused as to why you'd compare your crate with clap, instead of something like https://crates.io/crates/config which seems to be more in the same line of what you are doing, or am I misunderstanding the purpose of your crate?
I mean, it doesn’t really answer what I think your question is with regard to not “overloading +”, but I’d wager this is exactly why the `std::ops::Add` trait exists: so you can define how addition should work for your custom type. In your given example, implementing it is arguably as trivial as adding in `num-derive` and groking how it works.
This is a common pain point, as `concat_idents!` isn't very useful since macros are not allow in ident position (tracking issue for `concat_idents`...doesn't look promising: https://github.com/rust-lang/rust/issues/29599). There are a few crates that can help with this, including [this one](https://crates.io/crates/mashup), which happens to be my favorite.
Bangin! If only we could get more targets working, like Darwin and MSVC... I’ll give cross a try!
The closure is run in a loop unfortunately
This is a good solution but I don't have access to change the crate impl. Is there a way to transmute this lifetime
Perhaps you could provide a minimal example on the playground that demonstrates your problem?
I feel like this is a case of asking the wrong question. What do you actually want to do here? If you need to own a value that you only have a borrow of, consider cloning https://doc.rust-lang.org/std/clone/trait.Clone.html
The value is a reference to a singleton from ffi
&gt;But you can't even address the first byte in memory Yes, you can. The fact that \`NULL\` is usually an unmapped virtual address in modern operating system is not a limit of the language. Also, the first byte of memory does not need to be the first byte of virtual memory. One can map physical page 0 to any virtual page it desires. 
How do you propose to execute these parallel tasks? Launch a thread per node? Have a pool of workers that receive work tasks over a channel? If we use a thread per node, how do you plan to get the results back and join them? But wait, it takes a _long time_ to launch an OS thread. Probably somewhere between 100μs and a full millisecond, if I were to guess. That would be super expensive. If you just have a magical pool of workers, that's a heavy runtime component to put in the language, and that's the kind of thing Rust isn't interested in. But even if it were there, it's still relatively expensive to launch tasks and collect results. In my opinion, you're vastly underestimating how expensive it is to schedule and run operations in parallel. Parallel execution requires a real chunk of work that needs to be done, not just the ten or twenty instructions that need to be executed _once_ inside some random scope. What it sounds like you're suggesting would actually be more likely to slow the program down while consuming way more processor time, rather than actually benefit the program. [Rayon](https://github.com/rayon-rs/rayon#rayon) makes it incredibly easy to do a first attempt at parallelizing things, if you're interested, and there is some neat theory behind its _"potential parallelism"_ concept. It launches a threadpool and sends chunks of work to each node, and it can automatically execute iterators in parallel and collect the results for you. Even so, it requires benchmarking, because you can just as easily hurt performance with a library like that.
More concretely, you can use the `num-derive` crate to annotate your newtype such that implementations of the operator-related-traits are automatically provided. #[macro_use] extern crate num_derive; // NumOps is an alias for Add+Mul+Sub+Div+Rem #[derive(NumOps)] struct Height {i32} 
It certainly *could* be a feature built in to the language. Way back before Rust 1.0, [we did have green threads](https://stackoverflow.com/questions/29428318/why-did-rust-remove-the-green-threading-model-whats-the-disadvantage) (similar to goroutines) that worked similar to what you describe. Unfortunately, to make them go fast you effectively have to have a runtime (see the previously linked SO question) which was considered unacceptable for a "systems" programming language. However, it turns out that you can do a lot of that using existing syntax and machinery. For example, [rayon](https://crates.io/crates/rayon) allows you to explicitly express data-parallel computations. If you really want to go all-in on the asynchronous, DAG model of computation where you wait for lower levels of computation to resolve, you can build it on top of [tokio](https://crates.io/crates/tokio) and the futures ecosystem. IMHO having this sort of thing live in libraries rather than the compiler/language is a good thing. That way multiple approaches can take shape, and people can pick the one that works well for them. Instead of trying to build something that is fast in all situations (i.e. it efficiently handles both data-parallel and asyncio workloads) we can mix and match different libraries to use the right tool for the job. [It may even be possible to bring some of the magic of having a runtime back, as a library](https://boats.gitlab.io/blog/post/shifgrethor-i/). There's definitely room for more approaches than just the ones I outlined too: it sort of sounds like you're very interested in graphs of computation similar to those used by machine learning libraries like Torch or TensorFlow. I'm not aware of a Rust library that does all the evaluation and automatic differentiation magic, but at least from an initial glance it seems likely to be something that could be elegantly accomplished with low overhead in Rust. I'm sure the community ([especially those already using Rust for ML workloads](https://github.com/snipsco/snips-nlu-rs)) would be happy to help =).
I didn't think about any specific implementation. I'd say that the pool of workers, launched the first time a par function is met, may work. What I am saying is that, given the capabilities of the language, I'd want an easy way to parallelize a scope. In that way I would be encouraged to write parallel software and better exploit the characteristics of the language. The decision if it is good to parallelize or not should be on the programmer, maybe the word "automatic" was misleading. I meant automatic in the sense that it would not be necessary to explicitly define the threads. i.e. to launch parallel functions: par fn fun(){ let a = fun1(); //thread 1 let b = fun2(); //thread 2 let c = complex for loop; //thread3 return a + b + c } I looked shortly at Rayon, I'd say it accomplish another goal
&gt; that's a heavy runtime component to put in the language, and that's the kind of thing Rust isn't interested in It might be worth expanding on that. Rust doesn't have much a runtime, much like C. This is a big difference from Python or Java or even Go, and it's one of the things that makes Rust suitable for OS kernels and embedded systems. Baking a thread pool into the language -- even if everyone could agree on some perfect design and tuning for the pool -- would run counter to that major design goal of the language.
I think the derive more crate is a little more helpful.
I would say that I was tempted by an approach that makes parellelization transparent, thanks to the characteristics of the language. However, it is undoubtedly true that it does not exist an approach that fits all situations, and so it is not possible to generalize efficiently.
That’s pretty cool 😎 
Well, with the rls you can (usually) hover over a let binding and see the type as well. Intellij should also be able to tell you the type, but I haven't used it in a while and I don't know how good it is.
The first thing that comes to my mind is to use a [channel](https://doc.rust-lang.org/std/sync/mpsc/fn.channel.html). Set one up so that when your JSON API is updated (or whatever) it sends a message to the channel. Add an [idle callback](https://gtk-rs.org/docs/glib/source/fn.idle_add.html) to check the receiver end periodically and update the ui accordingly.
Could you be more specific? GC was really only mentioned once -- and that GC makes interacting with C more arduous is not at all controversial, I don't think...
Awesome write up. Great example of making it run correctly first, then measure and optimise. Must have been quite nerve-racking to see memory go up to over 20 gigs in some instances. Thanks for sharing!
&gt;Why would you want to even do that? What's the benefit? The benefit is that the various pieces of large projects can share a single source repository &amp; cargo lockfile. For instance I have a project (`aqua`) that, collectively, is a photo tagging program. It has many sub-projects though: there's a binary w/ a GUI for actually searching &amp; browsing the photos, there's a web server that lets you access the image library remotely, there's a binary that does routine maintenance on the database, there's a binary that migrates an older version of the database to a newer one, there's a binary that looks for orphaned files and cleans them up, etc. There's also various libraries: one for the database model, one that handles various image transformations, one that handles creating thumbnails for non-images (e.g: PDFs, videos, etc.), etc. They all share a lot of code &amp; dependencies, hence why it's useful for them to be in a single Cargo workspace, but all the programs &amp; libraries do very distinct things and are useful in isolation.
What you're describing in that code example looks exactly like the [async/await stuff](https://github.com/rust-lang/rfcs/blob/master/text/2394-async_await.md) that's being developed on top of futures. That document isn't the best explanation, but it's very similar to how C# does async/await, from what I understand.
I would recommend using my [`paste`](https://github.com/dtolnay/paste) crate to make this work. `[&lt;$n Trait&gt;]` turns into the concatenated identifier, for example `TheTrait` if $n is `The`. macro_rules! m { ($n:ident) =&gt; { paste::item! { pub trait [&lt;$n Trait&gt;] {} } }; } 
Rust coroutines are stackless, so when he cites a few bytes per coroutine, I think that should be possible for us as well, though I haven't used them yet so I can't comment on their space requirements in practice.
Totally agree. Also using the clippy linter is going to help improve the code quality as well.
The quote and question do not match. The quote's main point is that the architecture of GCC is adversary because it is driven by political goals instead of technical ones. There is nothing about the GPL being an issue; it's only mentioned as the reason why the architecture is purposefully convoluted.
Is `Iterator::flat_map` a strict super-set of `Iterator::filter_map`? `Iterator::filter_map` expects its function argument to return an `Option`, but `Option` implements `IntoIterator`, which is what `Iterator::flat_map` expects as its function argument's output. It seems to me that in all cases, uses of `Iterator::filter_map` could be replaced with `Iterator::flat_map` with no change to functionality. The only reasons I could see to use `Iterator::filter_map` instead is to preserve code clarity and to possibly take advantage of compiler optimizations to `Iterator::filter_map` that are not available for `Iterator::flat_map.` Is my intuition correct?
Hmmm interesting. I'm know how to use channels from the TRPL. I haven't heard of "idle callback" but from the description in the docs it sounds interesting. I'll have to look into it more on the weekend. Thanks a lot!!
This is most definitely not the correct reddit.
&gt; I looked shortly at Rayon, I'd say it accomplish another goal Rayon's `join` does *exactly* what you propose, i.e. launch two functions in parallel.
I started with a Google search for `rust asm`, which leads me to [one of the editions of the Book](https://doc.rust-lang.org/1.9.0/book/inline-assembly.html) — I can't actually get there by chasing links from docs.rs, so I don't know what's up with that. That links to the [LLVM documentation](http://llvm.org/docs/LangRef.html#inline-assembler-expressions), since Rust is just a thin wrapper around that. That's probably the most definitive documentation you'll get about LLVM's behavior there, but since it's built for compatibility with GCC's flavor of C, you may also have to drop to the GCC documentation every once in a while.
Object safety for arbitrary self types? Hallelujah!
Agreed. I have implemented a `Mutex` in C++ as well and there are two issues: 1. No guarantee that the argument is self-contained, for example I can put a `Mutex&lt;T*&gt;` where `T` points to an arbitrary point in memory. 2. No possibility to prevent "leaking" a reference or pointer to the internal. There are few enough `Mutex` in our codebase that (2) has not been a significant issue as inspecting the call sites is relatively straightforward... but (1) is a tad harder.
Yes, but from my conversations with the gtk-rs devs, that's not avoidable anyway. Cf. https://github.com/bitemyapp/boxcar-willie/blob/master/src/main.rs#L108 They gave this approach the thumbs up I think, but I have finished doing the changes they asked for to get it incorporated as an example in the gtk-rs repository yet. See here: https://github.com/gtk-rs/examples/pull/161
I'm a Rust novice, casually tinkering with the language, and haven't yet finished TRPL. I understand if the response to this question is "finish reading the book first". Is there a way to get back the underlying Iterator from an adapter like Skip or SkipWhile? Alternatively, does my approach here sound unnecessarily complicated: As a simplified context, I have a function `italicize(chars: &amp;mut Chars)` that takes this iterator and processes it, replacing pairs of `'_'` with pairs of (`"&lt;i&gt;"`, `"&lt;/i&gt;"`). The lines of text I'm processing are either ordered-list-like strings e.g. "2. The _dog_ jumped", or just more normal strings e.g. "The _cat_ jumped". These two types of lines are handled by two different callers, one of which directly passes the normal string `chars` to `italicize()`, and the other of which discards the "2. " prefix by instead trying to pass `chars.skip_while(|c| c.is_digit(10)).skip(2)`. Unfortunately this second call doesn't work because it's passing a `Skip&lt;SkipWhile&lt;Chars&gt;&gt;` instead of a `Chars`. `italicize()` needs to take a `Chars` because I am calling `chars.as_str()` in its body, which won't work if the trait bound is simply `Iterator`. I do intend for the underlying `Chars` itself to be advanced as a result of these adapters.
AFAIK there are no suitable abstractions over pin-driven counters and timers in embedded-hal yet (and I'd count those towards the more advanced peripherals, so might take some time until there are). I think that what's missing most here is some kind of system timer; these things are hard to get right with all the properties you might want from them (wall clock time, monotonic time etc). I'm confident those will be in or around embedded-hal at some point in time, but not yet - so you'll need to fall back to platform specific time. What I'd suggest from my own experience with such meters is to shove the "division" part that gives you frequency a bit down the application. Most of the time I found it way more convenient to work with counter values than with increase rates, and to define a particular interval in time in the application. Say you want to show average flow over the last day, the last hour, the last minute and - if imulses are frequent enough for this to make sense - the last 5 seconds: Then store counter values at some intervals, hand data like (start value, end value, start time, end time) around, and only at the very display do the division and derive your m^3/h (or whichever unit) from counter ticks and seconds. (When it comes to embedded-hal, that still leaves you with no abstracted GPIO interrupts so you have to poll, and no abstracted real-time clock, but it's generally easier).
The great Utahn baby invasion of 2018? Sounds terrifying :)
&gt;The point of a view struct is that it doesn't borrow the entirety of self. That's one (very helpful!) use case. However, Niko's example illustrates how view structs can be used to be explicit about which fields inside of `self` are being borrowed mutably and which are not. Notice that the `widgets` reference inside of `CheckWidgetsView` is immutable (`ref`) while the other references are mutable (`ref mut`). &gt; So you can't create it via fn as view&lt;'me&gt;(&amp;'me self) -&gt; View&lt;'me&gt;, because then you can't use fields of self unused by View. True, it only works because in Niko's example, all of the fields are used by the view. If you wished to use fields not used by the view, you would need to get references to them when you perform the destructuring. However, this effectively detaches them from `self`, which can be pretty un-ergonomic if you have lots of methods expecting to operate on the type `Self`. &gt; You have to manually initialize the view every time, or [use a macro](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=c2bb09ab5dfdd30cb63050a54834142b). Hey, neat! I hadn't considered using a macro for this, that's clever. Thanks for the example.
Refcells might seem complicated now, but they're a very important tool that you need to learn how to use. They're not even very complicated, and they're certainly not "way over your head". Nothing is.
&gt; The great Utahn baby invasion of 2018? Sounds ~~terrifying~~ exhausting Good luck to everyone involved X(.
The propose was to do it transparently
My recommendation: use an existing js&lt;-&gt;rust wrapper, make your own rust "crate" depending on this wrapper, and have the user depend on and call into this crate. For instance, you could have your crate just be `Cargo.toml` with: [package] name = "js-display" version = "0.1.0" [dependencies] stdweb = "0.4" and `lib.rs` as: #[macro_use] extern crate stdweb; use std::fmt::Display; pub fn output&lt;T: Display&gt;(thing: T) { let as_string = thing.to_string(); js!{ callMyJsFunction(@{as_string}); } } Doesn't have to be complicated. Then, provide the user with a template like: extern crate js_display; fn main() { let x = 2 + 2; js_display::output(x); } --- Finally, when compiling their code, _use cargo_, but don't let them edit `Cargo.toml`. You can create a single `Cargo.toml` to be used by all user crates, like this: [package] name = "user-code" version = "0.1.0" [dependencies] js_display = { git = "https://github.com/my-org/stub-js-display.git" } stdweb = "0.4" When compiling user code, you can make a new temp directory, stick a copy of `Cargo.toml` into it, stick user code into `src/lib.rs`, and call `cargo web build`. (`cargo web` being the utility for `stdweb`). You'll get your output in `target`, including a JS wrapper. ---- If you don't like `stdweb`, you can do something similar with `wasm-bindgen`. I don't know how to use `wasm-bindgen`, though, so I've made an illustration using `stdweb` instead. In any case, I would strongly recommend that you _do use cargo_. Rust code isn't meant to be built using just `rustc`, and you'll be severely limiting yourself if you don't allow yourself to use things from the crate ecosystem. TL;DR: do use cargo, and use one of the existing JS wrapper generating libraries: `stdweb` or `wasm-bindgen`.
I really appreciate tromey tirelessly working on [improving the debugging experience](https://github.com/rust-lang/rust/pull/54004)! Thanks!
Ok, and since that just won't fly, for reasons other people have already explained, `join` is the next best thing :)
Wrap state in an `Rc/Arc` and clone that?
You might be right. :-) But this will have to wait. I’ll proceed with the app in a minimal way first and then „upgrade“ my code later once I have a better grasp. Thanks!!
wasm-bindgen and wasm-pack will create those wrappers for you and put them in an npm package.
Try fn italicize(char: impl Iterator&lt;Item=char&gt;) That way you can take any iterator that produces characters. 
I long ago stopped feeling guilty for using `Arc&lt;&gt;` everywhere. I think Rust tutorials should start by wrapping everything with it and optimize later by stripping out where possible. It makes coding _much_ easier if coming from a high-level language and has a negligible performance impact in most uses. There are other reasons than speed to use Rust!
New to Rust, going through the book now (just got to structs/traits).. Currently translating a simple scraper I wrote in python &gt; rust to give some extra practice/context as I go through the book. I was wondering how to implement a trait/trait bound to the following struct. (The idea is that it there is a `Record` struct, the method goes to the page, put's it in `document`, creates a blank array, then loops through the page and populates the array for each `Record` found on the page. This array is then written out to .csv) ``` extern crate reqwest; extern crate select; extern crate csv; #[allow(unused_imports)] #[allow(unused_variables)] use csv::Writer; use std::error::Error; use std::process; use select::document::Document; use select::predicate::{Class, Name, Predicate}; #[derive(Debug)] struct Record&lt;T&gt; { name: T, address: T, phone: T, } fn scraper() -&gt; Result&lt;(), Box&lt;Error&gt;&gt; { let res = reqwest::get("https://www.com/").unwrap(); assert!(res.status().is_success()); let mut data_list = vec![]; let document = Document::from_read(res) .unwrap(); for node in document.find(Class("box")).take(3) { let mut _data = Record { name: node.find(Name("h2")).next().unwrap().text(), address: node.find(Name("p")).next().unwrap().text(), phone: node.find(Name("p").descendant(Name("a"))) .next() .unwrap() .text(), }; data_list.push(_data) } let mut wtr = Writer::from_path("data.csv")?; wtr.write_record(&amp;[data_list])?; wtr.flush()?; Ok(()) } fn main() { if let Err(err) = scraper() { println!("error running example: {}", err); process::exit(1); } } ``` This won't compile and results in: `the trait bound `std::vec::Vec&lt;Record&lt;std::string::String&gt;&gt;: std::convert::AsRef&lt;[u8]&gt;` is not satisfied` I'm wondering if this code is designed totally incorrectly, or is it alright and I just need to implement that trait to write to a csv? Thanks in advance for any pointers! 
Utah Rust meetup owner here! Thank you very much for the PSA! I appreciate it.
It's funny you mention that, because I was starting to think I could build a generic crate that could do what you were mentioning. I appreciate the more complete idea and the package recommendations
Good luck with the baby! My first was in the NICU for a couple days and it was terrifying. I hope it all works out!
Think of it as cloning a reference, which is what the compiler would do implicitly when passing a reference to somewhere else. The Gtk-rs objects seem to be somewhat like smart pointers (Rc&lt;T&gt;).
This is one of the things blocking `async` functions, right? As I understood it this was in the critical path for `Pin&lt;...&gt;`, which in turn was necessary for `async.
`csv::Writer::write_record` expects the type of its (2nd) parameter to implement `IntoIterator` ([docs.rs](https://docs.rs/csv/1.0.2/csv/struct.Writer.html#method.write_record)). You could impl that trait for `Record&lt;T&gt;` using the snippet `[self.name, self.address, self.phone].iter()`. Btw, why do you define `Record` polymorphic over any type `T`? I suspect you'll ever need `Record&lt;String&gt;` (even though you should avoid stringly-typed items).
And I'd be glad to help with that, regardless of the license selected for those broken out pieces! I'm working on a project where I want a user-mode smb client to replace the use of python (and pysmb). Feel free to point me to a mailing list, email address, or some other way I can help work on splitting the logic out.
You are not only correct, but correct on a fairly deep level. Fundamentally, the generic iterator type could be thought of as an affine list. Type calculus describes a (linked) list as L(t) = 1 + x * L(t), using sum and product type notation; in other words, it can be thought of as either no elements (1), or one element (x), or two elements (x * x), etc. The option is just no elements or one element, so it can be thought of as a kind of subtype of the list. Consequently, filter_map is effectively flat_map restricted to abstract lists of length 0 or 1. As for performance, I'm sure it's a factor. There are a ton of ways that rustc optimizes the snot out of enums. Calling next in a loop and destructuring is very fast; maintaining two layers of iterators less so.
Why use this instead of mashup?
Bingo. Future is defined as: trait Future { type Output; fn poll(self: Pin&lt;&amp;mut Self&gt;, waker: &amp;LocalWaker) -&gt; Poll&lt;Output&gt;; } The fact that `poll` takes a `Pin&lt;&amp;mut Self&gt;` receiver is what made it not object safe. `Wake` has a similar problem as its method is defined as `fn wake(arc_self: &amp;Arc&lt;Self&gt;);` which takes an `&amp;Arc&lt;Self&gt;` receiver. This change should allow `UnsafeWake` to be deprecated as it had a similar purpose as `FutureObj`.
Well you could declare a marker trait that requires all of them, and then make a blanket impl of it for any type that implements them, like [this](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=ce0f1db7cd9911ae8de18075474a55f3).
You can start with importing the required traits, it will shorten you were clause from 112 symbols to 49. In future we will get [trait aliases](https://github.com/rust-lang/rust/issues/55628) which will significantly improve the described problem.
Here's an example that wouldn't have compiled before: https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2015&amp;gist=4cc8a20b90dd2d1f18771a9051b2451a
You can create a custom trait that inherits from a list of traits and use it in your where clauses. https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=78bdd8cc7f43aa383df33bfad53ad040
I think that still might not work for me because I'm trying to call [`as_str()` as impl'd on the `Chars` struct](https://doc.rust-lang.org/std/str/struct.Chars.html#method.as_str) in my `italicize()`. `Iterator&lt;Item=char&gt;` on its own wouldn't have an `as_str()`, I believe.
OMG, that worked, and it's so much nicer! Thank you!
These wrappers should be easily optimized away. But if you have lots of such newtypes, you might see a noticeable increase in compile time.
When you pass the argument, you just need to ommit the `mut`, that way you can pass an originally mutable, non mutable reference, or even an originally owned variable. Someone more experience might add corrections, but the correct way would be to only declare the non mutable argument, and call it as `yourfunction(&amp;arg);` rather than `yourfunction(&amp;mut arg);`. Also, consider adding a https://play.rust-lang.org example to help us understand your problem.
But sign extension means that if the most significant bit in those bottom `x` bits is 1, then the number is negative and each of the top `32-x` bits have to be set to 1 as well. That is why it is called sign extension: the sign bit (that most significant bit in the bottom part) is extended such that all higher bits become equal to it.
I found this great guide a couple of weeks ago. It might be useful for you! https://blog.k3170makan.com/2018/09/introduction-to-elf-format-elf-header.html?m=1
It is, but it's not a performance problem. Also you can write a kernel in Go. It's slower than e.g. C but it's not impossible as people seem to think.
I'd imagine that you clone the handle to the widget that you need to a thread that does the polling. When you want to update the title, you call http://gtk-rs.org/docs/glib/source/fn.idle_add.html with a closure that updates the title.
passing the handle to the component might be tricky, in this example they use a thread local. maybe the handles are but Send? (that would make sense. how else to enforce using them only on the main thread.)
I think I got confused by both of the types in question being 32-bit.
Yeah, sometimes it seems to be unable to help. This is a case I would expect it to help, but no promises XD.
If they can be and are removed, no. That is, if they're unused you may need LTO to remove them. Or if they're inlined everywhere and/or get optimized to the type you wrap. All these conditions are _likely_ for a classic newtype. But if it matters test! If you really care about this sort of thing you are already testing for it. You can use `cargo bloat --release --crates` to get an estimate of what contributes to your binary size.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/u_unsignedint] [TabNine: an autocompleter for all languages, built with Rust](https://www.reddit.com/r/u_unsignedint/comments/9v40rx/tabnine_an_autocompleter_for_all_languages_built/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Unless you can *only* use these with exhaustive matches that don't include a catch-all, I am not in favor of *any* of these open Enum proposals. At least not until cargo has some sort of Backpack-like API versioning that can ensure things don't change underneath you.
It could just set RUSTC_BOOTSTRAP.
Ideally I would like to be able to infallibly convert `enum(u8, u16)` to `enum(u8, u16, u32)`, but unfortunately we either will have to create `From` impls for every possible combination (hello, combinatorial explosion), generate `From` impls on the fly (currently no precedents), or add language-level support for it. All variants are not that appealing. :/
So you always have to repeat the type whenever you use the type? I *guess* that prevents bugs, but it kinda makes me think it'd be better for everyone to just create an actual Enum.
I can see the reasoning for local situations, I think. My bleh-ness comes if this sort of thing appears in a public API. It just seems not great in a public API and maybe even in a private API.
That will never work then. Chars::as_str() only exists/makes sense because the underlying data is layed out sequentially (a [u8]). Without consuming the iterator and thus producing a new [u8] (e.g. a String) there is no way to call as_str() on the adapted iterator.
Looking at the definition of write\_record, you need to make Record implement AsRef&lt;\[u8\]&gt;. I've never used the csv crate, but I think you might want to just use [https://docs.rs/csv/1.0.2/csv/struct.Writer.html#method.serialize](https://docs.rs/csv/1.0.2/csv/struct.Writer.html#method.serialize) 
If it's a reference to a singleton then it should have the `'static` lifetime
Utah does have the highest fertility rate of any U.S. So I'm not at all surprised :D (Disclaimer: I used to live there, and I also belong to the strongest Christian religion there - The Church of Jesus Christ of latter day Saints)
What if there are two different traits that both provide a read method? By bringing the trait into scope, you know exactly which one of the two applies.
is it possible to `impl` two different traits with read method on the same struct? 
Yep.
how would it work and most importantly why would you want to implement 2 different methods under the same name? 
I think this could be disallowed all together and the programmer could write different functions for different functionalities. Its really adding a ton of confusion this way. It looks like some sort of overloading but you still need to annotate what you are using because compiler wont guess it. 
You *could* but that has other tradeoffs. Imagine these two traits were in two different crates. Now you can’t use these packages together, which splits the ecosystem. Now imagine that you’re not even using these two traits yourself; they’re dependencies of two different packages you want to use. It would get out of hand very quickly.