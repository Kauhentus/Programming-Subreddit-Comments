So much! I'd happily mark my old GBA emulator project as *"there's a better alternative"*. An other Rustacean was faster and doing better work at the same time.
I don't think so. The reason the coherence rules prevent you from doing this isn't just that other people might define their own impls, but mainly that the upstream library (libstd in this case) might someday add a blanket impl for all Option&lt;T&gt;. If you had your own impl, that would be a breaking change. That sort of breakage would make it very difficult for popular libraries to add impls to their types, in general. There was actually a thread today proposing some new ways of working around these limitations: https://internals.rust-lang.org/t/pre-rfc-forward-impls/4628/2
The best thing to do would be a kata, a project that you already know how to solve in languages you already use. The benefit is that you're not trying to figure out how to solve the problem, you get to focus on the language and how it is similar or different to what you already know. For what it's worth, the main things Rust specializes in is memory safety and concurrency without sacrificing stability. If you don't already have a kata, trying to make something that focuses on Rust's strengths will give you the most obvious benefits on display. Trying to reimplement them in your previous languages can then show you the strengths and weaknesses of both languages.
This seems like a nice idea
Huh thanks 
Huh thanks 
I'm getting a constant evaluation error from this code. (I don't really know what a constant evaluation error is.) The `return -1;` is what is highlighted. fn quadrant(radians: f64) -&gt; u32 { let x: f64 = radians.cos(); let y: f64 = radians.sin(); if x &gt;= 0.0 &amp;&amp; y &gt;= 0.0 { return 0; } if x &lt;= 0.0 &amp;&amp; y &gt;= 0.0 { return 1; } if x &lt;= 0.0 &amp;&amp; y &lt;= 0.0 { return 2; } if x &gt;= 0.0 &amp;&amp; y &lt;= 0.0 { return 3; } return -1; }
Ah right, that too! Forgot about that. :-) It's most useful on very large files where you want line numbers.
You've tried to return a signed value (-1) from a function declared to return an unsigned value (u32). You'll need to return a different value or change the return type.
in my case, optimism, mostly
- Reimplement common cli programs, (ls, grep, ...) - Game of Life - Brainfuck interpreter - CHIP-8 Emulator - Games like snake, space invaders, pacman, tetris; You can use piston for this.
well, i am not sure not linking to him is a good option either, he is probably the most influental german tech bloger after all.
Neat project. A few nits; I wrote a [small crate](https://github.com/scottlamb/http-entity) which does the server half of HTTP byte range serving, so I'm somewhat familiar with parts of the HTTP 1.1 RFC relating to range serving. In `main()`, I think you're getting the total length via a `GET` request (`hyper_client.get_http_response(&amp;url).unwrap()`). That seems wasteful; you'll download and discard some (I think 1 TCP window + one kernel receive buffer - the header bytes) of the HTTP entity before requesting it again in the first chunk. If you're not going to use that data, you can use a `HEAD` request to get just the headers without any body. (Alternatively, if you wanted to use fixed-size chunks rather than ones of size (total / threads), you could have this be a range `GET` request for the first chunk. But the requests would be asymmetric / code would be more complicated to save only a couple round trip times, so it's probably not worth it.) It looks like your chunk GET requests are not conditioned on an etag, or even on a `Last-Modified` header. So if the server doesn't serve the same file every time (such as if the file were replaced on the server halfway through), you'll blindly put together parts of different files without any warning. This checking shouldn't be hard to add; I think it'd be better to do it and warn the user in this case. Similarly, you're not checking that the HTTP response is actually for the requested range. Not all HTTP servers support byte range serving on all files; typically not on dynamically-generated content, for example. (Though my library helps you do that.) It'd be good to detect this by verifying the response has status `hyper::status::StatusCode::PartialContent` and the expected `ContentRange` header.
Fair enough. I guess lots of people speak irreverently about programming languages. He should know that people will take such criticism personally and be more tactful in his phrasing (and make sure his arguments are sound), but I think we have a responsibility not to try to avoid taking undue offense. But I know it's not easy. :/
Hi, I'm one of the authors of Tokio. Let me try to answer some of your questions. So, Netty was definitely inspired some of Tokio. Specifically, tokio-core. Tokio-core is the package that is closest to what Netty is. What Tokio calls a **transport** is kind of what Netty would call a channel handler (I believe). In the same way that you can build up your pipeline, you can build up your Tokio transport. `Codec` just happens to currently be the easiest way to do it today, but my guess is that as Tokio evolves, there will be more and more transport stages. Now, tokio-proto is specifically for request / response oriented protocols, like Redis, HTTP, etc... I looked very briefly at the Minecraft protocol and it didn't really look like it was request / response oriented (I may be wrong). It looked more like a streaming protocol. For these, it makes more sense to work with the transport directly and not use Tokio-proto at all. See here: https://tokio.rs/docs/going-deeper/transports/#using The section after that describes how to augment the transport: https://tokio.rs/docs/going-deeper/transports/#augmenting-transport which really means building up your transport with different stages. This is basically what netty does. So, in summary: 1) It doesn't really look like a pipelined protocol because it isn't request / response oriented. 2) Tokio is designed for such a protocol, but you would probably stay at the transport / tokio-core layer 3) You can work by building transport stages. However, there isn't *that* much documentation / examples on how to do it today, but it definitely is designed for it. Hope this makes sense :) 
Thank you for your answer, that really make sense to me. And further more, `loop_fn` is very nice api, I think I cloud build something like bluebird's `Promise.each` upon it. 
Let me ask you about the same concern from a different angle. I build/maintain distributed systems and server applications all day. Why should I (or any potential user) feel comfortable choosing Rust for building the next best distributed system or server framework/library/application, when the Rust core team doesn't want to prioritize/maintain the foundations (i.e. async io)? Asked a different (more personal) way: Strings are opinionated (UTF8, im/mutability, not platform agnostic, etc), and as long as Rust exposed byte arrays, slices, and optionally the char type, building a `String` abstraction is solvable in the crates ecosystem. If the Rust core team had decided Strings were complicated and opinionated, not worth getting right or maintaining in std, would you have felt comfortable(enthusiastic?) building your amazing ripgrep application (and its internal libraries) in Rust?
If it helps, [Exercism](http://www.exercism.io/) has a Rust track that I've been using as a source of katas so I can focus on finding and internalizing the best approaches to solving various problems in Rust.
This project excites me greatly, as an emacs user, because it's my chance to contribute to emacs. Upstream emacs is a labyrinth to contribute to, but Remacs, if it's like other Rust projects, will be easier to contribute to. 
&gt; I build/maintain distributed systems and server applications all day. Why should I (or any potential user) feel comfortable choosing Rust for building the next best distributed system or server framework/library/application, when the Rust core team doesn't want to prioritize/maintain the foundations (i.e. async io)? This is where our wires are getting crossed. I think it's pretty clear that the Rust teams consider async io important. Why do you think that since there aren't any concrete plans to put it into std, that means async io isn't being prioritized or maintained? Why is your litmus test "std or it doesn't count"? Why can't it be, "officially blessed and maintained but not in std"? The latter isn't true yet of course, but it's more likely to happen than direct support in std. As a library team member, I would personally like to see some of the core async io stuff officially blessed, but we have to decide that as a community (including the opinions of the current maintainers). &gt; Asked a different (more personal) way: Strings are opinionated (UTF8, im/mutability, not platform agnostic, etc), and as long as Rust exposed byte arrays, slices, and optionally the char type, building a String abstraction is solvable in the crates ecosystem. If the Rust core team had decided Strings were complicated and opinionated, not worth getting right or maintaining in std, would you have felt comfortable(enthusiastic?) building your amazing ripgrep application (and its internal libraries) in Rust? I find very little practical difference between using std and well maintained crates. (There is of course a drop off in quality of less maintained crates.) Cargo makes everything so painless that it just doesn't matter much in practice. Your hypothetical is hard to imagine though, given the amount of stuff in std that actually uses strings. Virtually every Rust program and library is going to use strings somewhere, and that's exactly the kind of broad applicability that std is shooting for. Async io is really nowhere near that. I mean, it's not like anyone is saying async io is never going into std. But tokio 0.1 and the futures library were literally just released. Talking about std is just way too early at this point. Even if it were std bound, it would probably be at least two RFCs away. *None* of this means we don't care about async io, which seems to be the message we are having trouble communicating to folks such as yourself. Perhaps the answer here is more patience: let's wait and see what people do with tokio. If there's significant buy-in, then we'll understand the solution space much better and perhaps the next step will be clearer.
Development Status 1.8: Complained about a problem while drunk and there was a partial implementation of a solution sitting around the next morning Development Status 2.25: Not as good an idea as I originally thought Development Status 3.125: Turns out something already exists that does it better Development Status 3.3: As good as it is going to get Development Status 4.5: Not done but likely will never be Development Status 4.8: No longer my problem, please stop asking me about it Development Status 5.0: Does the minimum of what we actually said it would but it's not actually useful yet. Development Status 6.667: Works fine for the most part but I'm not interested in it anymore, but I'll still accept pull requests Development Status 7': Author disappeared into the ether, people still use it and there's no real bugs but nobody knows how it actually works well enough to add features Development Status 8: Bought by Oracle
Very good article. I think you succeeded in showing the expressiveness of the type system. Your examples are relevant, clever, and demonstrate good practice. I also liked the idea of using diffs to emphasize what you changed in each step. The very first example is the only one that ends up being a little confusing. Now, I don't really know why, but reading about rust in Portuguese (my native language) feels strange. I guess I'm not used to seeing this kind of information in any language that is not English. Muito bacana cara, obrigado :)
No one stole Jack's thunder, he talked about Pathfinder (GPU font rasterizer) at linux.conf.au 2017, see [this video](https://www.youtube.com/watch?v=an5abNFba4Q).
 env!("CARGO_PKG_VERSION") This one is pretty useful, it generates a string that is set to your crates version number. Helpful in applications that need to define the version number. println!("application: {}", env!("CARGO_PKG_VERSION"));
Xargo's claim to fame is that it automatically recompiles the core crates on the fly, which Cargo on its own currently does not do. The latest versions also let you recompile the standard library or even compile a fully custom libstd if you want to go that far.
&gt; for the average case where performance isn't paramount Which is the case where using a threadpool is probably fine too.
Full ack. I tried to argue with him once, linked several independent replications that even implement open science like nobody else turned up. Turns out he already made up his mind and therefore I'm a nutjob and should never contact him until the sun literary "implodes". Funny side note: his opinion was then even busted by his own radio show guest that works in this kind of field ;)
I wonder how he bootstrapped his c compiler without a c compiler...
mrustc exists.
You can exchange `.extend([...].iter().cloned())` with `.extend(&amp;[...])` Also, remove-duplicate-edges algorithm is not optimal with respect to big-O notation. I do not know if it is actually meaningful as if I understand correctly there can be maximum 18 edges in any given case. But if this is true, you should have your edges constructed `with_capacity` outside the loop to prevent allocations you don't really need. 
How would you count down in a loop? As a concrete example: how would print 10 to 0 in a "count down" style?
You could maybe define rules for it to work that way, but I think it would cause confusing problems? The same method on the same type would mean different things depending on what crate it was called in. I think that's the sort of thing the designers wanted to avoid.
I remember that the Go situation was fairly anarchic when using a person's Github username as the namespace. So one had to look at maybe a dozen forks to find out the 'canonical' one - and it wasn't always the mother repo. And then, the handover. I have an old go project I want to move on, but it will be forever in the stevedonovan namespace.
`for i in (0..11).rev() { println!("{}", i); }`
More dimensions. This one-dimensional classification does not seem that linear. For example, Python's `7` looks a lot like `None`, and others range from `Some(1)` to `Some(6)`. Here is a number of important dimensions: - Completeness of implementation: if it's an image library, how much of the standard is implemented? - Development resources: can I count on future updates? - Number of users "in production", where "in production" must be clearly defined. And then, a discrete scale of values in each dimension seems fine. If a library wants to claim completeness / performance superiority over others for an important common tasks, why not ask the authors of all competing libraries solving a common task to agree on unit tests and benchmarks to rank them? 
That's ... not the problem?
Well, you could make a completely safe version by using `HashMap::iter_mut()` and iterate until you find the right key/values. Needless to say, this will be bad from a performance perspective :-) 
Would it be possible to get a green threading library in Rust akin to Go's model? This post brought back a lot of bad memories from Node.js with oodles of different opinions on how to make async suck the least, including, but not limited to: * callback as first parameter * callback as last parameter * promises * "streams", or promises with events you can listen to When I had an opportunity to suggest a better language (we had performance issues), I chose Go because it didn't have an async API but gave the same behavior essentially. I really like that model, but there's plenty I don't like about Go (lack of destructors and generics, lack of default implementations for interfaces, forced GC, etc). Tokio looks good, but I'm just curious if green threads could make a comeback in Rust, especially if they could be done without having to recreate the IO stack again.
I can go pretty far in GCed languages with nice async IO. If i'm giving that up, I want to go mind-boggling far.
Is he though? That's feels a bit like saying that Kim Dotcom is the most successful tech export of Germany, imho.. Germany also has a very popular news paper called BILD. Again something I wouldn't want to link to myself.
You had me at DI containers. DI containers that register all types from a random collection of assemblies with a particular namespace so when someone renames a path suddenly some dependencies can't be resolved!
Worse is better. The code has to appear to run (mostly), add some loud and confident marketing and FUD out competition. In case you were wondering about the mountain of vulnerabilities today's web is built upon.
&gt; This works for languages like python, ruby, javascript, java, which are all just different syntax around basically the same paradigm. It doesn't work and they all have different paradigms. The compiler/interpreter may not refuse to work with you if don't know that though.
Rust avoided this 'create a language that is only suited for writing compilers' problem by starting work on Servo before version 1.0 came out. Servo helped give feedback during Rusts development.
&gt; it's better to avoid directing reddit links to that blog. Can you explain a bit further? I don't see a problem in linking to any kind of (tech) blog.
Fair enough. I think I have found sufficient evidence. 1. Please see this talk by Alex Crichton: https://youtu.be/agzf6ftEsLU?t=39m13s . He says "Another question that I get a lot is 'Should I be using Rust right now?' and my answer is '*yes!*', i think *everyone shall be using Rust*. Some reasons i will give behind that are..." 2. And this talk (also happened to be by Alex Crichton): https://youtu.be/d1uraoHM8Gg?t=6m2s . He says "... zero-cost abstractions. What this basically means is something that at compile-time you can have this very nice interface, it's very easy to use, it's very fluent to use, but it all optimizes away to nothing." I think that's exactly what I was talking about. The first passage implies that Rust is as easy as the easiest languages out there. The second passage explicitly says that the language constructs are easy and fluent, but performant. And now we say that kinda sorry, but no, the language is complicated, sometimes unnatural, and you basically have to use it only when you need performance and safety and want to sacrifice additional development time to overcome the compiler restrictions (and may be also sacrifice performance). And some people get frustrated with this disparity and rightfully so. And one can put it even harsher: the advocates of the language provided misleading information about it (probably unknowingly).
It is a very popular blog. It is also very .. special. Opinionated. Controversial. Polarizing. I feel that linking to sources like these is not a Good Idea™. Just like I feel that links to Gruber's Apple commercials aren't tech related links worth discussing. But I kinda see that I stated that rather badly in my original post. I should've said that I, personally, would try hard to avoid that particular site - without actively trying to convince others to do the same.
Ruby had a similar problem when Github allowed easy cutting of gems from repositories as &lt;username&gt;-&lt;lib&gt;. There's reasons why stability-focused communities don't namespace and opt for new names for new things. In the TeX world it even used to be that you need to pick a new name if you want to fork a project and possibly break the interface.
How do I define a new target triple? I'd like to have a target 'armv7-unknown-linux-gnueabi', which would be very similar to 'arm-unknown-linux-gnueabi', same ABI, but compiling for a newer CPU-architecture. This is not supported by the default compiler, so I guess I have to compile a new compiler for that. But where are the target definitions, so I can edit them?
This would be true if people only downloaded (large) files. But imagine your neighbor talking to his relatives overseas by Skype video call. I don't think it would help if you tell them "Yes, I will make your Skype call freeze now, but that will allow me to free the bandwidth in half an hour". (And if you think Skype is not good example, think of watching a movie on Netflix, or anything else that heavily depends on *timely and continuous* delivery of traffic)
Path: you don't need to add Cargo's home dir to the path to make this work (aside from to run stuff installed via Cargo). If you do, and you also have rustc, cargo, etc. there you may have problems (depending on which `cargo` gets found in your path first).
They're pretty damn similar. Procedural languages with elements of OOP that people are consistently writing with more and more functional styles. You can go from one to the other far quicker than you can go from any of them to haskell or rust or prolog.
In case others are out of the loop: Stylo (a.k.a. Quantum CSS) will integrate Servo's CSS style system into Gecko, such that the style system code can be shared by Gecko and Servo. Stylo V1 will support Firefox on Windows, macOS, and Linux. Android support may ship in a later release. https://wiki.mozilla.org/Stylo
Actually, _Snatch_ is a (funny) _side project_ to become familiar with Rust. Currently, we code using Elixir, OCaml, JavaScript and Ruby - but I plan to familiarize my company with Rust to get some Rust projects :-)
Moving post thx 
&gt; Right now there is nothing to cause anyone to suspect it would be anything other than a walk in the park. I don't think that's fair. Right before diving into the ownership system, The Book makes it very clear that there is a learning curve. From the book: "However, this system does have a certain cost: learning curve. Many new users to Rust experience something we like to call ‘fighting with the borrow checker’, where the Rust compiler refuses to compile a program that the author thinks is valid. This often happens because the programmer’s mental model of how ownership should work doesn’t match the actual rules that Rust implements. You probably will experience similar things at first. There is good news, however: more experienced Rust developers report that once they work with the rules of the ownership system for a period of time, they fight the borrow checker less and less." https://doc.rust-lang.org/stable/book/ownership.html That being said, I agree that a few words about remodeling one's approach to structuring software would be helpful.
This is kinda confusing… **Remaining Properties:** Properties that are in Firefox but not in Stylo yet (See what's missing), should reach 100% 169/423 A reasonable guess might interpret that as "Stylo has 169 of Firefox's 423 properties", but I'm not sure. And it *"should reach 100%"* but it is not a percentage. :-) The reftest ones are a bit unclear too, because they seem to be referring to the same data. Am I correct in interpreting it to mean that 8366 of 14378 tests are failing, but 905 of those failures are disabled? Might be cool to chart all of these instead of reporting the raw numbers -- then progress is visible!
Maybe someone can enlighten me; I thought that Stylo was just taking the style code out of Servo and putting it into Firefox, so how are there differences between what's implemented in Stylo and what's implemented in Servo?
https://github.com/Wilfred/remacs#why-a-fork
i watched both videos, and can't find evidence of them conveying the message of rust being an easy to learn low compelxity language. 1. in the next slide he lists the reasons for why people should be using rust: large systems, fast, correct code, embedability. notably missing is "easy to learn". a few slides later he even answers the question if you should be using it _now_ with well, mayyybe. 2. the zero cost abstractions _are_ making things easier, but not easy, and that is exactly what is being presented. it is more comfortable to work with an Option then with something that could be NULL all of a sudden. having iterators is more comfortable then then not having them. it is just said that the language/compiler helps with some things that are a pain in other systems languages, notably C(++), and gives you an easier/less error prone abstraction that does not cost you any performance. notably missing is the claim of this making rust a low complexity language or easy to learn. Maybe the more relevant question here is: what gave you the expectation/hope that rust would be simple &amp;| easy to learn?
What about physiological challenges? My skin turned green after I started to learn Rust. This is normal, I suppose? Something to do with using lots of `mut`s?
This doesn't work on Safari. It just says "Loading..." [screencap](https://www.dropbox.com/s/w3h72agb09qyio1/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202017-01-20%2014.39.16.png?dl=0)
As a Java programmer (OK, really a polyglot, but mainly working in Java), I found learning Rust more empowering than humbling. Then again, I wrote my first lifetime annotation about 4 months into learning Rust (because I wrote lints where the compiler already owns all the stuff somewhere). Personally I feel we should advertise mentoring more – both to newcomers and project maintainers. For the latter it may appear as more work upfront, but I've seen it pay off handsomely a good number of times. Also perhaps we should form more learning groups, but I don't know enough of those that exist.
Have you used rustup? It's really excellent at handling everything around installation, current version, supported toolchains, etc.
A fraction and a decimal number and a percentage are different representations of the same thing. Nonetheless the wording is incorrect indeed. I think it should say "Stylo needs to reach property feature parity with Firefox. Current status: 169/423."
I'm really glad to see this. I switched from Firefox to Chrome a long time ago (FF 3.5ish?) and have just never had a compelling reason to switch back. However when I saw [Patrick demo WebRender](https://www.youtube.com/watch?v=erfnCaeLxSI) a while ago: I knew Mozilla was going to win me back. Project Quantum is a game changer to me, _I'm so excited!_
It's meant to illustrate the differences between Stylo (which was baked in Servo) and Firefox's existing style code.
Yes, and I'm sure it's a much better option for many platforms or using custom toolchains. But what's wrong with alternatives, especially when small and simple?
Rustacean, like the crab. And &lt;http://up-for-grabs.net/#/tags/rust&gt; is a site with OSS projects, some of them written in Rust, that have mentored issues to solve.
I think he even said it himself -- fefe is a yellow-press blog for nerds. Lots of strong opinion and often unbalanced reporting on things. When read carefully, it can be entertaining though.
Some of his complaints cannot be avoided. Others were blatantly mis-informed. A solid 60% of his blog post(s) were pure pre-opinionated ideological opinions on Rust which ANY language less then 10+ years old, or not backed by a massive corp (Mozilla isn't Google) would fail to address. So overall we need to ask ourselves why come to such harsh conclusions? I'd wager it's because he spent 4 days having the borrow checker slap his hands away from the keyboard. One can start developing rather *negative* and *totally unreasonable* opinions about the whole affair. This is in my experience how frustration classically presents. ESR's second blog post boils down to *too many stalls in the bazaar*. Which contradicts his own book. Which he always holds up as a *founding work of the OSS movement*. So he's lying this whole time? Or rationalizing his own personal failings in Rust into the failings of *the language* which is a common human psychological defense mechanism. 
Yup, the [new book](https://github.com/rust-lang/book) has [a new project chapter](http://rust-lang.github.io/book/ch12-00-an-io-project.html) about halfway through and we'll probably have at least one more. We've also got an upcoming chapter planned about design patterns that work well with Rust. It sounds like you know a lot about the kinds of examples that would be useful-- you should write them! [Rustlings](https://github.com/carols10cents/rustlings) attempts to get people used to fixing compiler error messages, but I haven't had much time to work on it lately and there aren't many advanced beginner/intermediate examples yet. I'd love PRs!
This is the point of my third paragraph (sorry if it's not clearly written), if you, individually, could contribute enough to the congestion of the network to make a appreciable direct impact you would be entirely right. But ISPs are multiplexing between so many people I don't see that as a realistic concern.
"teething problems" would be a good idiomatic translation, here.
I'm kinda surprised though that `self` is #1 instead of `let`. Is `self` used so much in "real-world"-code to beat `let` by x1.5?
Interesting that 4 (the, to, of, a) of the top 15 words in the C family of languages have more (or only) examples of license headers than actual code. It probably says a lot about the culture of the ecosystem. I wonder what their corpus is, though. EDIT: the README linked in the upper right corner explains it. It also says that Java had even more license words than the C family before they decided to filter it.
Probably because every use of `self`, on average, eliminates the many more `let` bindings that would have occurred if the corresponding function had been inlined everywhere. I'm more surprised to see `tcx` and `ast` so high. Makes me think the results are pretty severely biased toward the compiler's code?
I'm surprised that `ty` (regularly used when working on the AST) is _so_ popular.
Que bom que gostou! I hope there are more content in Portuguese to make it less strange in the future (: 
Yeah percentages should be used for something that isn't tangible like steps or really big numbers or sales bullshitting you. The fraction is very understandable given the context. /nitpick
Depends on whether the first part of the description is wrong or the second part. They could be counting properties in Stylo or properties that are not yet in stylo.
Will this be in the version of cargo that ships with 1.15 in a few days?
I don't think "ownership" is a thing? Everyone who has the descriptor has equal rights to it? Certainly multiple processes can be in a select-accept loop at the same time, that's how (g)unicorn works. To avoid *any* interruption, start the new version first, wait for it to start accepting (no automatic way I think. the app has to be programmed to like, send a signal. or just wait a fixed amount of seconds and hope it's fully started), THEN tell the old version to gracefully stop.
You can click on a word to see matches. I clicked self, and can see some code from Servo, anyway.
They definitely looked at other packages, but there's a lot of duplication so I don't know their exact methodology for how they analyzed the code. For example, the data claims that the following was present 1,368 times: ``` // If field is not initialized, it is initialized with default value first. ``` Which is a comment from `rust-protobuf`, which occurs once. Even taking forks into account, that's still only 96. Even taking all commits on the repo into account, there are only 525 commits. So I have no idea how the count of 1,368 was arrived at.
:(
QT even has its own String class... :(
Imagine a newcomer arriving on /r/rust trying to see if this is a welcoming community or not. Pokes around and 3 threads out of 6 have people slinging ad-hominen attacks, vehemently arguing with each other (with not much objective bases), ... Does it look like a nice community? The *normal* solution is mods going around and cleaning individual comments, warning their authors to keep cool and be nice. When the situation degenerates, however, this is not a scalable solution. Now, I would be sad if the threads were deleted because among all the drama there were very valid points raised and interesting discussions on those points. So, egoistically, I wish for the moderators to step up to the challenge and clean-up the threads without nuking them. I have enough empathy to understand that they may not consider it worth their time to muck in there trying to salvage the few gold nuggets.
I can't overstate how important setting expectations properly is. As a designer, 80% of my job is setting up proper expectations so that potential users put in the necessary time to get to love my products before abandoning them. These days I have a keen nose for when there is a misalignment between a product and the way it is talked about. Rust, as it exists right now, has that smell. My point is not that the language is too hard to learn - that's a separate problem, and one that many people are working hard to fix right now. It's that it is difficult to learn, but we talk about it like it's easy. I believe wholeheartedly in the principles of the Rust project. The vision is great, the community is great. But I have 20 years of programming experience, have been following every blog article and tutorial that has come out since September of last year, and have still not yet been able to write anything substantial in the language - not for lack of time or effort. For me I think the problem has been that the things that I've tried to implement in order to learn Rust seem to go against the grain: port some of my massive Objective-C project over to Rust (a graph of controller objects!), create a filter-based image processing library (graphs!), do anything with a persistent GUI (more graphs, this time with inheritance!), etc. The fact that making graphs and object hierarchies in Rust is difficult, inadvisable, or requires knowledge of many advanced concepts is markedly different from many other popular languages. But it's not just graphs and objects that require a different way of thinking - strings are different, closures are different, container classes are different, even calling a function or assigning a variable is different. Rust is fundamentally different from other popular programming languages, but I think that many of us will agree that this is actually its greatest strength. Unfortunately, this means it will take a lot of effort for new Rustaceans to get to the level where they will see the benefits from those differences, instead of just the frustrations. It's not a language that you can learn in a weekend, but it's worth it to put in the time in the end. What can we do as a community to present this more honestly, and to cut down on this misalignment of expectations?
As I understand it, [cargo rides the trains now](https://github.com/rust-lang/cargo/pull/3345), so I *think* it'll be in beta at the next release, so it'll come with Rust 1.16 stable?
Being able to have the CI badges pinned at the published version tag would probably be most useful. It doesn't matter if master broke, only that the version I'm downloading was working.
I don't think any of the CI services support that.
Then that badge should not be used imo.
Not all crates tag their releases.
You already can make blanket impls willy-nilly and make your crate hard to compose. You already can use `#[fundamental]` and later cause a breaking change by implementing a trait. You can already impl `Eq` and `Ord` to not do what they're supposed to. These are all disciplines that if not held lead to annoyances and breaking changes, not lack of safety. Most of the more niche features already do have a discipline of use.
There it's explicitly connected with the github repo and the status of the master branch (That's the version of the README that's shown too).
Perhaps. 2 requests checking both of those formats, and only using the default after 404s seems feasible, and a better default, no?
&gt; perhaps it could be as simple as making the default Cargo.toml file created by cargo init include a set of useful tags and a suggestion to pick one in a comment? Actually, I would propose to go ahead and have `cargo init` automatically use the least mature rating. After all, an `init` is to start a library, so in the vast majority of cases it's really immature. 
&gt; That's the version of the README that's shown too I think the point was more about the README at a tag: https://github.com/bluss/rust-itertools/tree/0.3.25 Note that the travis badge links to the master branch; it doesn't show if the code was passing at that point in time.
Maybe, but it would actually be 3x 10 requests; these badges are shown on list pages as well.
&gt; However, it doesn't show activity That's true by default. Travis [has a beta feature](https://github.com/travis-ci/beta-features/issues/1) to run your tests on a time schedule, and Gitlab CI has the same thing, IIRC. Not sure about AppVeyor. I've found it super useful for testing against rolling nightlies.
D'oh. I thought the server just read the toml and magicked it.
It might worth warning about design patterns that do NOT work well with Rust, and maybe alternatives? Patterns introducing cycles (I'm looking at `Observer`, notably) require the use of `Rc`/`Weak` and cells, which is prone to forming cycles (the `Observed` has a list of `Weak&lt;Observer&gt;`, the `Observer` has a list of `Weak&lt;Observed&gt;`). I expect that introducing a broker in the middle may cut the complexity, but there are still lifetime complexities.
Cargo.toml could allow specifying this with some sort of template... `travis-ci = { repository = "username/repo", branch = "v${version}" }`. Though I think just figuring this out at publish time is even nicer. I definitely love badges, but I'm a little hesitant about breaking master making my crate appear broken. Then again, maybe I'll be more careful about what makes it into master. ¯\_(ツ)_/¯
It could be that Stylo and Servo are operated as different repositories and have little dev-forks that are a few commits apart and therefore don't quite match up. Or, that some things in stylo and/or servo aren't completely wired up with boilerplate code and is as of yet unfinished in one of the projects
You wrote `where I::Item: (T, usize)`, but in rust this means "`I::Item` implements the *trait* `(T, usize)`", which doesn't make much sense. Instead, write the following - which denotes associated type *equality*: impl&lt;T&gt; PrefixTree&lt;T&gt; { pub fn create_tree&lt;I&gt;(counts: I) -&gt; Self where I: Iterator&lt;Item = (T, usize)&gt; { ... } } 
We count "implemented in stylo" as "works in stylo", and same for Servo. The parsing/cascading code is shared, but for a property to be considered implemented in Servo, there must be Servo layout code that handles displaying it. Similarly, for a property to be considered implemented in Stylo, Stylo must be able to pass that property on to the Gecko layout code (most of this glue is in https://dxr.mozilla.org/servo/source/components/style/properties/gecko.mako.rs). 
Working on Part 2 of my rust-on-teensy tutorial series. Right now it looks like Part 2 will be "making bit manipulation less terrible", "setting up the system clock", and "serial port"... but that's all still subject to change if I find I need to explain another concept first.
Do you have a rough date estimation? What experience with coding those people have?
It means that Stylo still needs to implement 169 of Firefox's 423 properties. I reworded it a bit in my PR here: https://github.com/shinglyu/are-we-stylo-yet/pull/5 . You can see the updated version in https://manishearth.github.io/are-we-stylo-yet/
I categorized my crates! Most of them are macro utilities, which I filed under "Rust patterns", hopefully that's appropriate.
Because its London, I hope someone from mozillla may show up - its certainly great opportunity for them to promote rust. If that would fail, than I suppose I could help.
No doubt I understand what she is talking about, but it really doesn't seem to be an interesting point to me.
Unfortunately, I don't have those results from the same machine. The development machine I can use is completely terrible, so benchmarks of anything look like from the 90s.
The expectation that one can simply just be productive in another language simply because it has semicolons and braces is utterly false. No amount of experience or skill replaces a learning curve completely, they just shave off parts of it. Suppose Rust didn't have a borrow checker in it. You'd still have to learn the build system, the syntax, the idioms, how to model things using ADTs, which libraries to use, get your editor setup, and read a bunch of manuals along the way. Oh, and write your program, too. Here's the thing. It can feel like you're the only one who gets stuck with this stuff, or that it isn't worth it. Both of these are _false_, and just your ego trying to justify itself. Everyone who writes Rust has fought the borrow checker at some point. There's nothing special about you that should exempt you from that. Rust doesn't need a disclaimer. Devs need to get over themselves and embrace a beginner's mind. I mean, everyone is somehow OK with re-learning a new JS framework every six months, how is this different? One could argue is proportionally more powerful for the increased time investment. 
What's the status of websockets and http2? Will tokio help with that?
I'm unsure of within hyper, but implementing RFC websockets on top of plain tokio-core was pretty easy in my own project. You just build a Frame Codec and you're good to go.
Yet I'm hopeful that Rust will carve out a niche where people can recognize that better is actually better.
Looks like a link to the examples is missing.
You can already use after free, too.
What is the difference between these two categories? https://crates.io/categories/api-bindings https://crates.io/categories/external-ffi-bindings
Do you mean for the Appveyor badge? If so, what would you have rather seen? Our understanding is that more Rust projects are hosted on GitHub than Bitbucket, so it seemed a reasonable default. The upshot is that it's only ~20 or so characters to specify Bitbucket, which only needs to be done once per project, so it shouldn't be an annoying problem for most people!
I googled the pathfinder project (for gpu font rasterization), and couldn't find anything (outside of transcripts of this talk). Are there any public links to this work?
You may want to ask /r/playrust
You can edit your post to copy the markdown, then submit a new text post in /r/playrust and paste it.
The circumstance that there is a large number of C compilers but only a single Rust compiler out there does make a difference, though: there's a technique (DDC) that [detects if any compiler in a set has a self-propagating backdoor](https://www.schneier.com/blog/archives/2006/01/countering_trus.html) as long as not all of them have the same backdoor. Self-propagating backdoors, on that matter, [have in fact been seen in the wild](https://nakedsecurity.sophos.com/2009/08/19/w32induca-spread-delphi-software-houses/) rather than being a purely academic concern. I'm pretty disappointed that so far, the point about C compilers seems to be the only serious counterpoint made to the accusations in here; everything else looks like wagon-circling and attacks on the blogger.
You're probably looking for /r/playrust
Are there slides for these talks available? I really find video hard to deal with because it's often relatively low-density in terms of time to consume.
&gt; http2 is better than websockets in every possible way I'm curious about that statement, could you expend on that or link a related article?
Great work, Carol! It would be really useful to have a Cargo command that validates that the categories you've listed are ones that actually exist on crates.io without having to run `cargo publish`. I've had this same annoyance before with keywords: only when I ran `cargo publish` did I get an error saying I had too many keywords, and by that point I'd already created a Git tag and pushed it to GitHub.
And wouldn't that just be websockets 2 instead of 1.1?
This is a great one https://samsaffron.com/archive/2015/12/29/websockets-caution-required
I remember some rust related backdoor experiment lately and yes i totally understand the problem. I hope at some point there will be some second rust compiler - it does not have to be able to optimize or do any fancy stuff, just enough to translate the rust compiler itself - best written in portable c. on the other hand, there is so much binary blobs running from firmware and drivers or even look at the management mode modern x86 implement and the management unit itself. Lots of modern hardware runs without binary blobs half decent if at all.
But what about multiplayer HTML5 video games?
&gt; are operated as different repositories They kind of are, Stylo refers to the codebase containing both Servo and Gecko (http://hg.mozilla.org/incubator/stylo). But this gets regularly synced with Servo master. I usually regenerate the property list only after these syncs (and I use the Servo repository to do that), so this discrepancy has nothing to do with the two-repository model. &gt; that some things in stylo and/or servo aren't completely wired up with boilerplate code Yep, that's the case. A different set of properties is parsed depending on the mode (servo/stylo) that you're in, because not all properties have layout support in servo and not all properties have glue code to be converted to gecko-form.
Eh, I still think websockets have a very important usecase. I work on a realtime chat site and we really can't afford to have all of the concurrent users constantly asking for messages from the regular backend, that's why we have a separate push server that uses websockets. (Which I recently migrated from node.js to Rust for performance/memory usage reasons, the old nodejs version had serious memory leaks). We still also serve regular HTTP requests from that server for clients that can't use websockets (e.g. certain mobile 3G networks), so I basically ended up writing the HTTP stack myself with httparse since there didn't seem to be any frameworks that supported websockets and HTTP on the same socket.
Had to look quite hard to find a reasonable voice here. People, get it together and listen to this guy.
If I add categories and badges to a crate, will existing users, with the current release version of Cargo or older, be able to download and build it? I thought Cargo threw an error on unknown keys.
Understood, usually tests are run multiple times on different instances in different regions.
Thanks!! &lt;3 You may be interested in [this github issue](https://github.com/rust-lang/cargo/issues/971), to allow authors to upload metadata without needing to publish a new version :)
 I actually think that chat applications could use HTTP2 at less cost. But still, as I mentioned, I'd be fine with letting some framework take an h1 socket from hyper and figure out websockets. Spending time on h2 has more of an impact to more people.
What about pushing messages/events from server to browser? [It doesn't seem](https://en.wikipedia.org/wiki/HTTP/2_Server_Push) like h2 server push can be used for that. The article above says "most won't need push" and while that is true, the games I implement will need push. The article also says &gt; HTTP/2 has the ability to stream data to clients by sending multiple DATA frames, meaning that streaming data from the server to the client is fully supported. Which sounds very interesting! But this seems like we'll co-opt a performance feature to implement something unintended on top of it, which may or may not work depending on how browsers implement things. Can I set up a connection with JS to listen on any incoming DATA packets and respond to each differently depending on its payload? Will I need to write filters to discard packets that are part of images / videos that are streamed? Hope you don't mind the questions. If you can point me to a good place to look for info regarding this I'd appreciate it, other than the very nice multiplexing perf gains from H2 I've seen people write about, I'm still naive when it comes to understanding the protocol.
Oh for sure, I'm not disputing that h2 should be a higher priority, it definitely is. And honestly for something like a push server writing the tokio code was far less work than the actual logic, httparse and Codec makes it all pretty straightforward, so I'm not sure it needs to be a huge priority within hyper.
That's a slightly different use case than what I'm describing, but thanks for the link. I subscribed to it.
I'm working towards [releasing v0.0.6 of my RethinkDB driver](https://github.com/rust-rethinkdb/reql/issues/3#issuecomment-274195446) that is set to coincide with Rust `v1.15.0`'s release. In `v0.0.5` I shipped the first functional version of the driver. This version is a much cleaner rewrite using Macros 1.1. Every command is just an attribute macro and documentation now. While the previous version only shipped with a few commands, this version will ship with all RethinkDB commands implemented and a much better API. After this release, I expect the driver to start stabilising towards `v1.0.0`.
That's not really the consistency I'd want. In general i expect a dedicated machine from benchmarks.
My guess is that API would be for connecting to web-based APIs like Reddit or Dropbox (not FFI).
AWS can also silently upgrade their hardware or fallback to faster hardware if they feel like it. At a startup I was at a couple years ago I babysat deployments across 10+ aws instances. The slowest machine would sometimes take twice the time of the fastest machine. And it wasn't random either - the slowest machine would often stay slowest for days. Benchmark on dedicated hardware. And preferably on a desktop machine. Thermal throttling on laptops can also mess with your benchmark runs.
https://crates.io/categories/external-ffi-bindings is for ffi-bindings, which usually come in the form of -sys crates. The code in those crates is often autogenerated from header files, and presents an unsafe interface where you have to deal raw pointers. https://crates.io/categories/api-bindings is for idiomatic Rust bindings. They are usually human written and provide a safe (as far as that's possible) interface based on the -sys crate by ensuring correct usage of their functions. They also optimally implement convenience traits like `Iterator`, so that the external library they wrap can be seamlessly used with Rust code.
Here are my slides: https://github.com/michaelsproul/aus_senate/blob/master/talks/rust-sf-jan-2017.pdf
Thanks for your input in this thread chain. Seems like my best bet is to use websockets and wait for them to be multiplexed over h2, since I need bidi and don't need insane perf for them just yet.
'Poor mans' may not be a bad thing. It might be missing a feature or 2, but it's 98% there, and actually does cost less (a connection less). - If you have bigger messages, yea. If not, it should work out. HTTP2 recommends turning off tcp delay, so small messages shouldn't be grouped together. - Yea, one way, but with HTTP2, shooting a client-to-server message is also near-free. - Safari: ¯\_(ツ)_/¯ Does a streaming fetch make mobile browsers keep spinning? It might not. You also have plenty of reported issues using websockets where intermediaries screw up everything. 
The chunks would just be data sent to a client, just like in HTTP1. The only difference here is the framing that the server and browser utilize, but what is exposed to the `fetch` response is the same. HTTP2 recommends disabling tcp delay, so it should send a DATA frame immediately upon writing the frame to the socket, and the browser would receive it right then (minus speed of light over a cable). A server could definitely pause. It's possible that someone in the middle of the pipeline could close the connection if it is quiet, but that issue also exists with websockets. HTTP2 does define a PING frame that it can send frequently, and the browser will receive and PONG back, without affecting your Fetch Stream. Good point about the web server and game server perhaps being different. But also true, if a user is running multiple games in multiple tabs, you stop websockets from creating a connection per tab.
Netflix tests for this before deployment: if the instance has noisy neighbors it terminates and tries again.
I'm sorry to say that but having the title and the link alone are not very helpful. First, what parts of the verification do you think rust should use? Is that even possible? Is that feasible? It looks like it also requires an execution environment and or an IDE to verify on the fly. Do you want Rust to have enforceable pre/postconditions / invariants? A static checker to enforce those? A dynamic checker to enforce them? Without those questions clarified this point is rather pointless. I would also assume that some points there were already discussed when developing the language, though I do not know for sure.
Thanks.
nov 11, 2014. sigh. someday I will have the time https://github.com/rust-lang/cargo/issues/841
Writing a faster and simpler [alternative](https://github.com/tafia/quick-protobuf) to rust-protobuf. The main reason is to have much simpler generated files so one can easily reason about them and eventually manually modify them for even better performance.
I will be present to answer any questions about Redox OS, which will be discussed in the first half.
Will be possible to complete a request using a Future?
Is it possible to have async functions but "invert" the flow so you manually *opt-in* async behavior rather than have to *opt-opt* (by virtue of invoking the callback, or using `await`, etc)? This way, you can start writing synchronous code, and it will remain working whether or not the function you are calling is async or not. If a function supports async, you can also (e.g. by using `defer` as the opposite of `await`) call it in "lazy" mode, meaning it won't block but then it's up to the programmer to manually poll on the promise or otherwise await completion. It also means developers can intelligently decide when to bother opting-in async, or internally a compiler can optimize switching to async if it detects work can be parallelized without changing externally visible behavior. 
So you either have to trust an existing binary representing a C compiler, or one representing a Rust compiler. And people will have different opinions about which of these options is better. It would be hard to argue that one of them is objectively the better one. I hope that we can at least agree that in the long run, humanity should get away from depending on a language like C, which was designed the way it is only so it is possible to execute the compiler on common hardware of the seventies. If we are forever only allowed to trust a C compiler binary, then there is no way to ever get away from C - even in a hundred years. And that would be a sad outlook.
I'm curious. How does this differ from Eiffel? Ada/Spark? ATS? I think with special formatted comments, or perhaps macros, a compiler plugin could extract a proof to be verified using a solver. I'm not sure if `debug_assert!` suffices, though.
The two mentions of `T` are first declaring it, and then using it. You can read the line as "Here is an implementation, given some type `T` which implements `PartialEq`, of the struct `Rectangle` applied to that type `T`."
That's what I figured, but couldn't you condense the declaration and usage like I have at the end of my question?
Note that it would certainly be possible to bootstrap with a reduced version of rustc that could be easier to audit.
I wish the Rust team would have said yes to a stdlib. Would make the languages way more approachable.
I'm sorry. I was on mobile. I was asking if we can use a Future as a response for a HTTP request. I checked the [documentation in master](http://hyper.rs/hyper/master/hyper/server/trait.Service.html) and it seems that it is possible :) impl Service for HelloWorld { // [...] type Future = Box&lt;Future&lt;Item = Self::Response, Error = Self::Error&gt;&gt;; fn call(&amp;self, req: http::Request) -&gt; Self::Future { let resp = http::Response::ok().with_body(b"hello world\n"); futures::finished(resp).boxed() } } 
I never was a huge Chrome fanboy but [this made me totally blacklist it](http://www.forbes.com/sites/marcwebertobias/2014/01/26/heres-how-easy-it-is-for-google-chrome-to-eavesdrop-on-your-pc-microphone/#198f28484b3e). I know most people don't see the point of privacy or using tools that respect the users but I think the debate over this is becomeing more mainstream.
Safari is the new IE :D
Isn't that something that should be encouraged?
I think `impl Trait` (unless my understand if it is wrong) should fix that signature issue whenever it gets implemented I believe. Relevant RFC for that is [here](https://github.com/rust-lang/rfcs/blob/master/text/1522-conservative-impl-trait.md)
I'm also looking at doing some work formalizing Rust, specifically extending the work in Jonatan Milewski's thesis. Do you have your models of the Rust type system available anywhere? It would be cool if I could cite it and bit duplicate work. I haven't watched the video yet, I'll do that soon, so I apologise if the answer is in there. Jonatan's thesis: https://open.library.ubc.ca/cIRcle/collections/ubctheses/24/items/1.0220521
&gt; Specify your Travis CI and Appveyor badges. I hope I'm not the only one that would never put the name of a company in my Cargo.toml. Adding support for specifically travis and appveyor contributes to killing competition by telling people that nothing but travis and appveyor matter anyway. I've had the same debate about the fact that cargo deeply integrates with git (by creating a git repo when you run `cargo new` and looking at your gitignore to decide what to publish). I don't have anything against git but I don't want a world where git is a standard and a better software wouldn't be able to break through. 
I think you are overstating the problem but I also think it's indeed a good idea to just be up front and say "Rust works differently than you might expect and until your brain shifts around you'll be getting incomprehenshionable errors, but don't worry, we were all there once." I went into Rust knowing that it applied substructural typing to guarantee memory safety. The biggest obstacle for me was correctly putting lifetime quantifications in traits but I'e not been fighting with the borrow checker for a couple of days now. Now you should try Idris for fun, I just gave up because the documentation was bad and I wasn't that motivated but there you got a compiler who tells you it wants a formal proof that your reversing algorithm does not alter the size of the vector you're reversing because the compiler can't prove it itself.
Sadly, looks like you can't add categories without pushing a new version of a crate. It is something to keep in mind the next time I do update my crates though.
Not really. You can't find this type of code with equality: ``` where I: Iterator&lt;Item = (T, usize)&gt; ```
That doesn't address the problems with virtualization itself, such as having no control over any other things running on the same machine, competing for resources like CPU time slices and cache.
On IRC #rust-embedded is the place, but I'm not getting in there as much as I would like lately. I'll be at Rust London on Feb 9 if you're UK based.
Non-lexical borrows is the #1 pending feature for Rust usability, hope it gets accepted in some form.
Is there a RFC for macros 2.0? Afaik just macros 1.1 is going stable soon 
I don't think it's quite that easy. `some_fn` would have to be pure (for some carefully chosen definition of 'pure') or it could not be used in static checking, and would result in a different program when using dynamic checking. In addition, you would have to be able to specify pre- and postconditions on trait methods (that must hold for all implementations of that trait), to be able to reason about generic things.
Acknowledging the fact that things have to be done differently is supporting the idea expressed here and there that it would be good to express algorithms in the Rust way. Maybe taking Knuths Art of programming and trying to express them in Rust will reduce the language barrier by offering Rust alternatives to common algorithms. And as stated, graphs are so commonly used in today's applications. Just saying you can't do that in Rust will certainly not bring more people on the contrary, it will push them away. Either Rust is a niche language that is very good at solving particular kinds of problems or Rust wants to be a general purpose language and needs to support all general usages easily. I've always been a big fan of Ada but the attitude of the Ada world, albeit having a nice community nice but sometimes condescending and other issues (compilers) have pushed people away and has now confined Ada into particular niches whereas the language was designed to be an excellent general purpose language.
Like I said in [last week's post](https://www.reddit.com/r/rust/comments/5mwl04/whats_everyone_working_on_this_week_22017/dc7e9lf/), I have been working on my Game Boy emulator ([repo](https://github.com/javierbg/RustyBoy)) (even though I souldn't, but whatever, exams went well :P). Just yesterday I managed to show the Nintendo logo from the boot rom!! I really didn't think I would be able to get this far, I'm very pumped up right now. I'm loving Rust so far (although I'm still not used to its module system). The way you can guide the coding process using the compiler errors/warnings is pretty nice, and the "if it compiles, it works" has been pretty much true so far. If anyone is interested, go take a look and maybe give me some advice. And of course, thanks again to /u/yupferris for the motivation!
&gt; Am I better off using a "sparse array" (say an array of [Option&lt;T&gt;; N]) that's larger than I'll ever need or should I use a vec and grow it as I need? Almost always use a vec. If you already have an idea of how many elements are going to be used in the common case, use `Vec::with_capacity`.
I wouldn't say it _leads to_ "weird looking code"; it just enables you to write code like `&amp;&amp;&amp;&amp;&amp;s` and have it work -- nobody will actually do that.
I just recently stumbled over [rustproof](https://github.com/Rust-Proof/rustproof) and just leave this here without a comment from my side as i've not really know Dafny nor rustproof but just felt somebody could be interested in by the given context // Proven to never overflow #[condition(pre="(x:i32 &lt;= i32::MAX - 5:i32)", post="return:i32 == (x:i32 + 5:i32)")] fn add_five(x:i32) -&gt; i32 { assert!(x &lt;= 2147483647-5); x+5 } 
Loving the consistent streaming, even though I pretty much only watch the VODs. It's nice having things to look forward to every week! Will be interesting to see you go back to Rustendo after presumably learning lots from this Virtual Boy project :) Also gotta thank you for introducing me to the patch mode in *git add* a couple of episodes back :D
Really happy to hear that! I'm stoked to get back on that project soon too :) Also yeah, patch mode rules! My mind was blown when my colleague showed me that :)
:D
Well, I think that niche is already established: systems programming. In general, as long as Rust manages to displace more and more C or C++ programs, the world should be a safer place for it. If people prefer to use more dynamic languages, fail to tame them, and their website is buggy, it's a pity for them but there's no life in the balance.
This looks quite close to naughty dogs fibers, as seen in their well-known presentation. There's been a few previous attempts, https://github.com/slide-rs/fibe-rs at least, and I did my own little experiment. I'm curious to see how this turns out, as it would be very nice to have for game development.
Hum... making sending an arbitrary large file could make it relatively easy to DOS the server no? (Of course you can still likely DDOS it anyway)
I am hoping that at some point cargo will support pijul, and will decide between git/pijul based on what's installed :)
What do you think about a `--dry-run` option for `publish` (and `update`) that would validate that everything looks fine without actually modifying anything?
I suppose every programming language has a meta-language, most of it composed of common terms ('function','block','scope',etc) and then special terms ('borrow','box' and so forth.) So part of learning to program is learning to talk about programs. And always paraphrasing special terms (like 'box' in your case) until they become natural. My current obsession is writing tutorials, and it is most interesting to become aware of _how much background_ a person needs to understand Rust effectively.
It's a bit strange that you'd do physics sequentially - it's usually the first thing that gets parallelized. If you are set on making an entire engine yourself, you could find inspiration on how to design your ECS by looking at existing implementations such at [specs](https://github.com/slide-rs/specs). There are some others floating around, try looking around at r/rust_gamedev. 
Well, the description says: &gt; Includes HTTP API wrappers as well. There are very few of those in the Rust ecosystem as far as I can tell, so even with people categorizing their existing crates now, it might take a while for some of those to show up.
Yes. we would need some notion of `constexpr` or similar. But most of these things could probably be enforced through compiler plugins with macros 2.0 and custom attributes.
&gt; There will be a lot of communication between systems. Systems *never* communicate directly, at least if you're going for a cache-efficient implementation. They are pure functions. (Unless the OS's graphics API insists on side effects). The solution is to slap events on the whole thing. One simple solution is this: Instead of saying "if component A, B and C exist run system Z on them, output shit to (temporary) component D" (i.e. "sql join with all keys equal, then map over the thing"), allow components to be optionally present and still run over them. That gives you message boxes, possibly even ones that can store more than one message per update (allow multiple id&lt;-&gt;component pairings per id). On the sender side, sending a message to an entity then is the same as attaching a component to it. If you have that, thinking about actors is a good idea: Entities are actors if and only if the sum of the systems operating on their components act like actors do. --- In general: Do what DB engineers would do :) You have a specific set of queries, optimise them. If in doubt, write down some (pseudo-)SQL, or maybe datalog if that doesn't suffice.
This is some timing; I actually just asked [a question on SO](http://stackoverflow.com/q/41780024/1443496) that seasoned Rustaceans might find cute – I'm 'fighting with the borrow checker' as they call it. Take pity on a Lisper!
PM if you want me to run some benchmarks for you on my company infrastructure (Xeon E5-2680v3, 10G ethernet).
Who needs constexpr when we could allow annotating functions as pure, with the exact same system that understands the preconditions and postconditions? In addition, it might be nice to consider memory allocation as "pure" even though it probably isn't?
this actually depends a lot on the internal structure of the ISPs network, it might be multi-tiered and maybe you are bottle-necking your apartment complex or something. or your flatmates. then again, usually ISPs do the traffic limitation/balancing per customer, not per tcp connection, so it would have no effect. (or a negative one, if the authors of the tool you are using did not know about HEAD requests) so yeah, it either does nothing, or hurts people that share the connection with you.
SSE are supported in Firefox and Chrome. I've used them. They work great and are pretty easy to implement.
Besides bug fixes and optimizations, what are the biggest things currently being worked on?
[Searching for REST API yields 56 results](https://crates.io/search?q=rest%20api), which I don't think covers all HTTP APIs.
I don't really understand how a `cargo update-metadata` command wouldn't solve the problem in your use case, but a dry run would, could you elaborate?
Reduce your scope to a core of the engine. Unless it's the ECS itself that is the most interest to you, think not about how to design an ECS but about how to make everything work with an ECS that you can potentially swap. You can check [ecs_bench](https://github.com/lschmierer/ecs_bench) for the list of candidates to start with and ignore the numbers for now. There is so much to do about an "engine": graphics subsystem, entity components, data loading, configuration, tooling... If you don't want to get stuck writing one of those things, just pick an existing library and focus on the core that glues everything together and exposes a nice API to the user. Might be worth having a look first at the other engines in development: https://github.com/amethyst/amethyst/wiki/Other-Game-Engines-in-Rust .
If you mutate in-place you don't get to run two systems over the same component in parallel. If you don't need to do that because only one system needs that component -- be my guest, mutate in-place. But that shouldn't be a choice of the system implementation itself, but the scaffolding behind. Separation of concerns. Another nice side-effect is that you can double-buffer your game state and can do funky things like have a transaction log, which is the same as a delta-compressed history of the game state. Ideal for debugging. &gt; What cache efficiency concerns do you have? All of them :) If it's not a linear walk down an array, then it's not cache efficient. Channels are notorious for causing cache misses, not to mention you're using concurrency primitives which cause stalls. There's already enough *necessary* cache misses in an engine when you actually need random memory access for algorithmic reasons, there's no excuse to do that when it's not necessary. --- *Conceptually*, or maybe "a naive implementation that can later be amended with fusion and/or specialised data layout": Have a bunch of sparse arrays representing id&lt;-&gt;component relations, one per component. Merge-join those arrays, that's O(n). Map the system function over them, also O(n). The system can have another of those arrays as output, which can then be merged with others and fed into another system ("internal outputs"), or be considered part of the next game state ("external output"), though more is necessary: So far that's a straight data processing pipeline, what needs to be added is systems being able to say "delete this id", "spawn a new id (e.g. from prototype", "add component to id". Collect those actions per-system, merge-sort them, that's O(n log n) but cache-optimal, apply those at the end of the update to the external outputs of the systems. --- Notably absent from that is "how to interact with bullet". You do that by biting the bullet and doing side-effects. But just because you need them there doesn't mean that you should use them all over the place, on the contrary, they're spaghettiing up your data flow and that kills the cache.
As part of my shapefile library, I've implemented a simple dbf file parser. I'm curious if people can test it out. [More details on users.rust-lang.org](https://users.rust-lang.org/t/help-me-test-a-dbf-reading-library/9032)
It's briefly mentioned on the [associated types](https://doc.rust-lang.org/stable/book/associated-types.html) chapter, but only for a very [specific use-case](https://doc.rust-lang.org/stable/book/associated-types.html#trait-objects-with-associated-types). In fact, I think I learned it from reading the std.
&gt; If you don't need to do that because only one system needs that component -- be my guest, mutate in-place. But that shouldn't be a choice of the system implementation itself, but the scaffolding behind. Separation of concerns. &gt; Another nice side-effect is that you can double-buffer your game state and can do funky things like have a transaction log, which is the same as a delta-compressed history of the game state. Ideal for debugging. So basically: systems generate a list of commands that are executed when all systems have finished executing?
Sorry for the late response. Life happened 😛. I'll check for sure.
I agree, using existing libraries would probably be the best approach. Unfortunately, my goal is to work out a Game Engine design and I think the ECS is one of the most important things, so I want to do it myself.
The RFC was accepted and there's an open PR for part of an implementation.
Not that i disagree with the decision, but why rust? Was it just to learn rust or do you intend to use it for this type of application? Wouldn't go be better? 
I'm sorry but I think I didn't get everything. If any system changes anything like a position, there are two possibilities: 1) Emit an event to change the position after all systems have finished or 2) There has to be some locking / sequential execution. The problem of 1) would be that there may be nasty bugs because the changes are applied after the update. So the only advantage of an immutable system over the mutable one would be the parallel emission of change events which are applied after that stage? Could you please explain this a bit more?
I think the story might be that the team was shooting for the minimal possible feature set for 1.0 that wouldn't prevent progress down the road. So the fact that `..` was missing from patterns and `...` was missing from expressions was "not a big deal", because they could always come back later and fill in support for them. Now _since then_ it seems like people have gotten interested in a more general mechanism that lets you make the range exclusive on the left, and that discussion has made it take longer for anything to stabilize.
/r/playrust
Mercurial has a similar feature through "hg commit - i". I slightly prefer it as it uses an nurses UI for hunk selection. 
I recommend using a nightly option to verify the target you're trying to create. I'm assuming you're using rustup here but you can modify the command line if you're not. rustc +nightly -Z unstable-options --target arm-unknown-linux-gnueabi --print target-spec-json rustc +nightly -Z unstable-options --target arm-unknown-linux-gnueabihf --print target-spec-json You'll see the difference here as: 15c15 &lt; "features": "+v6", --- &gt; "features": "+v6,+vfp2", 20c20 &lt; "llvm-target": "arm-unknown-linux-gnueabi", --- &gt; "llvm-target": "arm-unknown-linux-gnueabihf", Then if you look at the following: rustc +nightly -Z unstable-options --target arm-unknown-linux-gnueabihf --print target-spec-json rustc +nightly -Z unstable-options --target armv7-unknown-linux-gnueabihf --print target-spec-json You'll see the difference here as: 15c15 &lt; "features": "+v6,+vfp2", --- &gt; "features": "+v7,+vfp3,+d16,+thumb2,-neon", 20c20 &lt; "llvm-target": "arm-unknown-linux-gnueabihf", --- &gt; "llvm-target": "armv7-unknown-linux-gnueabihf", Hopefully that helps you craft your target. FWIW, I'm hoping to get that print option stable soon. https://github.com/rust-lang/rust/issues/38338
Sure, you can expose a C interface from Rust and call that from Go - [one example](https://github.com/medimatrix/rust-plus-golang). Do you mean http over the TCP server built in to tokio? If so, definitely, that's what [hyper is moving toward](http://seanmonstar.com/post/156128815358/a-hyper-update).
I never considered to do physics sequentially. Probably my post was not clear enough about that. I do want to call it after I have the input; the physics part itself however will split up its components and process them parallel.
What is the point of pointing to the repository for the badges? Isn't this info already exposed in the "package:;repository" value? Why is specifying it under the appveyor or travis CI section useful? It's not clear from the post what this extra information is used for.
My CPU is an [Intel Core i5-2430M CPU @ 2.40GHz](http://ark.intel.com/es-es/products/53450/Intel-Core-i5-2430M-Processor-3M-Cache-up-to-3_00-GHz), so that would make it 2nd generation. It comes with Intel® HD Graphics 3000, so that would support OpenGL up to 3.1. To be honest, I didn't remember it was so old! (I have upgraded it with an SSD and extra RAM, so it's having a long life). It's probably not worth your time trying to support hardware this old, so no worries. I'll try it on my (certainly more modern) desktop when I have the chance.
I'm trying to loop over an array of structs and I'm getting problems with this code: fn addBullet(x: f32, y: f32, dx: f32, bullets: [Bullet; 1000]) { let mut found: u32 = -1; for i in bullets.iter() { if bullets[i].is_None() { found = i as u32; break; } } if found &gt;= 0 { let i: u32 = found; bullets[i].x = x; bullets[i].y = y; bullets[i].dx = dx; } } The first issue is with `if bullets[i].is_None() {`. It says "the trait bound `[Bullet]: std::ops::Index&lt;&amp;Bullet&gt;` is not satisfied. The type `[Bullet]` cannot be indeed by `&amp;Bullet`." So that means the `i` created in the for loop is not an actual number. So how do I loop over the array? And how do I use the `i` as a number?
I love that the -p basically works with every command in git. It's definitely my most used flag.
The current issue is because i is a reference to a member of bullets not a position in the array. To get an index you would use for i in 0..bullets.len() { ... } But there are several other issues with this code. You're moving bullets, so the array is discarded at the end of the function. You want to pass as a mutable reference. fn addBullet(x: f32, y: f32, dx: f32, bullets: &amp;mut [Bullet; 1000]) Next since some bullets may not exist they need to be Options, which are None and Some(value) fn addBullet(x: f32, y: f32, dx: f32, bullets: &amp;mut [Option&lt;Bullet&gt;; 1000]) And the rust style guide uses snake case for function names. fn add_bullet(x: f32, y: f32, dx: f32, bullets: &amp;mut [Option&lt;Bullet&gt;; 1000]) I strongly recommend using Vec instead of arrays while learning. fn add_bullet(x: f32, y: f32, dx: f32, bullets: &amp;mut Vec&lt;Option&lt;Bullet&gt;&gt;) So a working example could be fn add_bullet(x: f32, y: f32, dx: f32, bullets: &amp;mut Vec&lt;Option&lt;Bullet&gt;&gt;) { //Use Options to indicate missing instead of magic values //Indices are usize, the type inference can figure this out let mut found = None; for i in 0..bullets.len() { if bullets[i].is_none() { found = Some(i); break; } } //If found has a Some value bind that value to i if let Some(i) = found { bullets[i] = Some(Bullet { x: x, y:y, dx:dx, dy: 0.0 }); } } There's a function that can do the find part fn add_bullet(x: f32, y: f32, dx: f32, bullets: &amp;mut Vec&lt;Option&lt;Bullet&gt;&gt;) { let mut found = bullets.iter().position(|b| b.is_none()); //If found has a Some value bind that value to i if let Some(i) = found { bullets[i] = Some(Bullet { x: x, y:y, dx:dx, dy:0.0 }); } } Most likely it would just be better to store only existing values in a Vec struct Bullet { x: f32, y: f32, dx: f32, dy: f32, } //Putting the bullets first will seem better once you learn how to add methods to types fn add_bullet(bullets: &amp;mut Vec&lt;Bullet&gt;, x: f32, y:f32, dx:f32){ bullets.push(Bullet {x:x, y:y, dx:dx, dy:0.0}) } fn main() { let mut bullets: Vec&lt;Bullet&gt; = vec![]; add_bullet(&amp;mut bullets, 1.0, 1.0, 1.0); }
So you don't need to do this for Travis or AppVeyor is how I'm reading that (my appveyor link is working fine without it). But it's there for future use evidently, which is interesting. But since Cargo is riding the release trains now you got get stuff in early. edit: Actually, the img link isn't generated correctly. The generated URL is `https://ci.appveyor.com/api/projects/status/github/Susurrus/serialport-rs?svg=true&amp;branch=master` but the link in the README `https://ci.appveyor.com/api/projects/status/gv9v4b78ywa4bn2g/branch/master?svg=true`
That won't work because `truncate` does not return the string. it just mutates it
Personally, I wouldn't get hung up on the fact that it's referred to as a systems language. You can use it for that: writing USB drivers or an OS, and really that's just because it's fast and gives you fine-grained control over memory, with no overhead. But it's also a very rich, expressive language, which means it is often a good choice for higher level applications: games, web applications, desktop apps...
tl;dr is 'they're pretty much tied'
&gt; Where is the title() defined? `title()` is a custom function returning a string. Your solution doesn't work (it returns `()`). The method signature of truncate is `fn truncate(&amp;mut self, new_len: usize)` - it does not return a string! That's why I use the above code, but wonder if there is a better way. 
Thanks.
That is a fair point. Looking at the large error bars in the 2nd graphs still doesn't paint a particularly good picture though.
Oh. That makes it more difficult, but here's a "work-around" that I found. ``` fn truncate_string(mut string: String, length: usize) -&gt; String { string.truncate(length); string } let t = truncate_string(title(url)); ``` Basically you just make a function that takes ownership of the string, truncates it, and then returns it. It makes the code a bit cleaner at the site, but it still involves about the same amount of code. If you know that the strings will always be longer than 50 characters, you can use the following, but it will panic on shorter strings. ``` let t = String::from(&amp;title[..50]); ``` It also allocates an entirely new string.
There is another side to this. Fefe is a person that deeply favors C and dismisses most things for a very good reason. He doesn't criticize stuff he finds bad to begin with, but either dismisses them or jokes about them. In older (months ago) blog posts he pointed that he really likes Rust and finds it interesting. And now he has been doing a bit of "today I didn't like this and that", which I guess is his way of saying "I think this could be enhanced" (the OCaml part). I think the right way to go about that, is accepting that a person pointed out stuff he didn't like. You can agree or disagree, but why waste your time essentially being like "Someone is wrong on the internet!". That's not in benefit of You, Rust or the author of that blog post. If you have some valid points and not just excuses then Fefe can even be mailed and will even respond. If his problems simply don't apply for you (you don't wanna use C to bootrap Rust) then that's not something that will be gained from letting him know. And saying "C needs to be bootstrapped too" is a variation of that. Fefe has a C compiler trusts. That's status quo. It's really pragmatic. Maybe Rust therefor simply isn't something for him (right now or never). It's not like he made an in-depth analysis on Rust in that one blog post or says that people shouldn't use Rust or whatever, but pretty much just stated that you can't bootstrap Rust with a C compiler. He also said you can't do that with (recent) Go. That's also true. I guess most people here have known that. And people that want to look into Rust will either also learn that or not. If they are basin their decision on one of Fefes blog they might look into it because of another or not look into it because of that, but even then. Do you really want to convince people making such decisions to become Rust programmers just for the sake of it and do you want to blame Fefe for making decisions based on some random post, when he didn't even say so? tl;dr: Don't make more of that post then it is. It really just boils down to the fact that Rust can't be bootstrapped with C at the time of this writing. And some stuff about OpenSSL. If that's not true then tell him. Not so hard. He has his email address up there.
I wouldn't really compare RMS with Fefe. While there might be some truth in similarity (eg. them being outside of the average) RMS is more like the fanatic idealist, while fefe is the extreme pragmatist and while both of their views might be extreme (not in the same direction all the time though) Fefe never says he is right or that others should follow his views. He even tells people not to and sometimes sprinkles in some wording and obviously false claims (it's more complex than that) into his articles to make people not forget that. But then again, I'd criticize what he says rather then him as a person, cause I think that would get the Rust community further the psychoanalyzing either Fefe or Richard Stallman. ;) (plus it probably just leads to drama, trolling, etc.)
This is a project I've been working on over the past few days to implement tokio codec support for the server side of the Language Server Protocol. It's my first major project in Rust and any feedback is welcome. This only implements Codec support with no plan for supporting the full tokio pipeline as there are many correct ways to implement IO and Request handling. Server to Client notifications are not currently supported, but should be soon.
How do large error bars paint a bad picture? That just means that there is little input data, in this case few people overall using Rust if I interpret it correctly.
/r/playrust is the sub you're looking for. Though if you're disillusioned with the game, feel free to stick around here and write some code!
did you mean /r/playrust?
I find that too much of a drain on rapid prototyping for something that has to support both SQLite and PostgreSQL, so I've been sticking to Python with either Django (web) or SQLAlchemy+Alembic (non-web) while I wait for the Rust ecosystem to develop something comparable.
...have I got the wrong subreddit? what's this one? I'm not 'playing' yet though
This sub is about the Rust programming language. http://www.rust-lang.org 🙂
...wwwwhhhaaaat
If you run `cargo build -v` you will see the exact rustc call that cargo makes. Should help you figure out what to do.
If you only avoid cargo because you don't want it to depend on the internet each time, you can try [cargo vendor](https://github.com/alexcrichton/cargo-vendor), which puts the entire source code into your local repo, using only path dependencies so that nothing from the internet is needed. Then you can continue to enjoy the comfort of cargo.
Awesome! Maybe contact the Chipmunk2d devs to include your crate in their [list](http://chipmunk-physics.net/bindingsAndPorts.php)?
We're using [this method that appveyor supports to generate badges](https://github.com/appveyor/website/blob/31845d611455db36ddf9b2c2ece76859dc5d9cb6/src/docs/status-badges.md#badges-for-projects-with-public-repositories-on-github-and-bitbucket).
Since LSP(Language Server Protocol) is generic and not specific to RLS(Rust Language Server), it would be better to avoid using "rls" in the project name.
I was speaking descriptively, not prescriptively. How it is/was, not how it should be, by any means. I think the rust community is doing a good job at moving beyond old social conventions, though. &gt;I assume you didn't mean this, but it sounds a heck of a lot like "Rust is suited to fields that women are not suited to". Rather, I was saying that Rust is suited to fields that women don't seem to be interested in, for reasons most likely mentioned above. You can bring a horse to water, but you can't make it drink. Every woman that I know that is interested in tech, has gotten a job incredibly easily. Tech companies are desperate to hire women. And yet, you can't force people to do jobs they aren't interested in. 
oh nice :)
Edit: Nevermind, I found out what the problem was, I'm leaving this here for reference because I assume it's a common beginners mistake: Basically, it's about using look_up rather than something like contain to check _if_ the symbol is in the table. look_up returns an id, in this case a non Copy object Pathbuf which is a reference which has the same lifetime as the borrowed self into the global scope of the function. So the borrow continues indefinitely. If you use contain, which returns a Bool, the borrowed scope is only for the function call as it does not return soemthing whose lifetime is tied to the actual table and you can freely mutate it in the global scope and any nesting thereof. This does not happen with `usize` of course as it's a Copy trait. The correct code is thus: if ! self.contains(symbol) { // borrow of self ends right here now, return value's lifetime is not tied to self self.push(symbol) } self.look_up(symbol).unwrap() The problem still is though, what if checking whether it contains the value/looking it up is a fundamentally expensive operation you would rather not do twice.
Here is a structure with one member: &gt;pub struct ModbusService { &gt; in_: futures::sync::mpsc::Sender&lt;( &gt; ModbusRequestPDU, &gt; std::sync::mpsc::Sender&lt;ModbusResponsePDU&gt;)&gt;, &gt;} It's ne kind of sender. You use it to send out a request, along with another Sender that the recipient uses to give you the response. Then comes this method call: &gt; fn call(&amp;self, req: Self::Request) -&gt; Self::Future { &gt; let (resp_in,out)=std::sync::mpsc::channel::&lt;ModbusResponsePDU&gt;(); &gt; self.in_.send((req.pdu, resp_in.clone())); &gt; let pdu = out.recv().unwrap(); Problem is this &gt;error[E0507]: cannot move out of borrowed content &gt; --&gt; src/main.rs:522:9 &gt; | &gt; 522 | self.in_.send((req.pdu, resp_in.clone())); &gt; | ^^^^ cannot move out of borrowed content What exactly is the problem here? I'm calling th Sender's send method, and self is borrowed here, but why is that an issue?
Rip
Thanks for the update! :)
`constexpr` is a purity annotation though. Rust used to have a `pure` keyword, but since nobody could agree on what `pure` should mean (no side effects? can be used at compile time? etc.), it was removed in favor of having more specific meanings. Rust also used to have pre and postcondition checking (called Typestate). It was also removed, mostly because it was arduous to actually use and didn't give clear benefits.
Side note, no ligature support yet. 
The [`Sender::send` method](https://docs.rs/futures/0.1.9/futures/sink/trait.Sink.html#method.send) takes `self`, not `&amp;self` or `&amp;mut self`. Since you are only borrowing `self` within this method, you cannot call a method that moves it or any of its members.
You should see how I solved this kind of problem using my Rust implementation of Parallel. It creates as many jobs as there are in CPU cores and performs work stealing using a mutexed custom Iterator. The outputs of each command need to be streamed in the correct order, and printed in real-time on demand. The outputs have to be in the same order if you had run them serially. Basically, the streaming solution is solved by writing outputs to a temporary location on the disk as the receiving end tails the next job's file, printing new text as soon as it's available. A channel is then used to send a completion signal, which tells the receiver to stop tailing the current job and to start tailing the next. The result is pretty fast. No rayon, no futures, no complications. Just the standard library. I did also make use of transmuting to make some values static though. It's perfectly safe for these types of applications. You can leak the value and return a static reference to it if you want to make it 100% safe. Leaked data will remain in the heap forever, until the OS reclaims that data after the program exits. Although it's typically recommended to use crossbeam for this. https://github.com/mmstick/parallel
Complete noob here (just like the idea of rust): I thought Rust had functions built in for creating and parallelizing processes and threads? Why would the author have to turn to libraries for the basics of the functionality?
&gt; *Partly this is because C UB includes untestable (and, in my opinion, overly aggressive) rules like “every loop should do I/O or terminate”* …the standard says this? where? and … why? &gt; *(and, in my opinion, overly aggressive)* Wait, that's overly aggressive? If the loop isn't doing I/O, and it isn't terminating … what *is* it doing? What else is there? This loop sounds like the proverbial tree falling in the forest.
That looks perfectly idiomatic to me!
[6.8.5 Iteration statements](http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1570.pdf): &gt; An iteration statement whose controlling expression is not a constant expression, that performs no input/output operations, does not access volatile objects, and performs no synchronization or atomic operations in its body, controlling expression, or (in the case of a for statement) its expression-3, may be assumed by the implementation to terminate. The reason is optimisations that remove/merge loops in ways that would change some aspects of the program's behaviour if they were infinite. There are a few examples in http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1528.htm . &gt; Wait, that's overly aggressive? If the loop isn't doing I/O, and it isn't terminating … what is it doing? What else is there? This loop sounds like the proverbial tree falling in the forest. It makes code "unstable": bugs/typos (e.g. infinite recursion) that should just result in the computer spinning forever result in arbitrary memory corruption, e.g. https://github.com/rust-lang/rust/issues/18785 and https://github.com/rust-lang/rust/issues/28728.
That would be awesome. My Rust knowledge is unfortunately not at the point that I feel like I'm able to write/understand macro code (haven't looked much into it).
Does tokio make this stuff a lot easier??
That's a great idea. I don't really understand why it isn't implemented that way in the std-library. Is there some advantage with the `&amp;mut self` approach to the Trait one? 
&gt; However, as far as I know, none of the C sanitizers is able to detect the full range of undefined behavior. While this is true, there are tools which detect much more undefined behavior compared to Valgrind and Google sanitizers. KCC C interpreter and Frama-C C static analyzer were used by [C-Reduce](http://embed.cs.utah.edu/creduce/). [CompCert C interpreter](http://compcert.inria.fr/man/manual004.html) is also available. In practice, combination of these tools detect all undefined behavior of interest. C interpreters in the list basically work just like miri. In addition to detecting memory errors, C interpreters can detect relying on undefined evaluation order. It doesn't apply to Rust because Rust defines evaluation order.
This matches my impression. I was nearly flamed when I said in 2015 (near Rust 1.0) that despite efforts, participation of women as ratio didn't seem to have increased at all compared to 2013 (when I first joined), and suggested that maybe we are doing it wrong. Many said my impression from the internet may not be representative of actual uses. Well, StackOverflow is also "from the internet", so I guess you can say the same. I still think maybe we are doing it wrong.
This is kind of a separate topic, but I think it would really help me and a lot of others learning rust if there was some kind of resource for how to rewrite code from a more widely understood programming language like Java. When someone first starts learning they want to do something with their knowledge, so it'd be nice to be trained on how to do everything I could do before in one place. I mean, I can look at the language and study it from the books or tutorials, but to do something higher level I'm gonna have to fight the borrow checker for a few hours... better just for someone to tell me what to do
I don't think this post really answers that question, instead it provides an alternate framework for reasoning about this. http://smallcultfollowing.com/babysteps/blog/2016/05/27/the-tootsie-pop-model-for-unsafe-code/ and the next two posts are more focused on exploring this space.
After learning Rust you have a way better understanding of ownership than you would have after learning C++. But C++ has many pitfalls. What in Rust is a compile time error, in C++ it may be only a warning from the compiler or linter(!). So, you should install a linter and never ignore warnings from the compiler. Besides: What you learn on universities about programming languages are only basics. So you shouldn't be concerned.
For example, crates.io [doesn't allow spaces in keywords and doesn't allow more than 5 keywords for a crate](https://github.com/rust-lang/cargo/issues/2098). A few times this has happened to me: 1. My crate is ready to publish. Yay! 1. `git tag 0.1.0` 1. `git push --tags` 1. `cargo publish` 1. api errors: invalid upload request: ApplicationError("invalid keyword specified: foo bar") 1. Crap! Now I have to change Cargo.toml, retag 0.1.0, and force push the tag. Being able to update the metadata separate from publishing doesn't help, because I still have metadata that crates.io considers invalid in the Cargo.toml committed to Git. With a `--dry-run` option, I could run `cargo publish --dry-run` before step 2 to make sure crates.io will accept my crate as is. If it doesn't, I have an opportunity to correct Cargo.toml before committing the tag. Of course, I could also publish to crates.io before tagging and pushing the commits, but I'd only think to do that because I've been burned by this before!
Off the to of my head - C++ has references, also using the &amp; sigil. You will probably use them less than in rust, except as inputs to functions. If you try to be fancy there are some surprisingly subtle rules around const-references. Inheritance from a class with "virtual" methods is similar to implementing traits, but any struct members come along for the ride. I think exceptions are probably going to be the biggest weirdness? They're very much like panics, but used for most error reporting. You're expected to catch them in at least some code. &gt; My professor also said that it'd be easy to learn C++ after learning Rust. Is that true? I knew C++ long before playing with rust, so I don't have direct experience, but this *feels* right to me. With the exception of... exceptions most of C++ is similar to a more-permissive, more mutable version of rust. My experience is that once you know one imperative language it's reasonably easy to pick up others. Rust is imperative enough to count.
rush -- parallelly execute shell commands. A GNU parallel like tool in Go https://github.com/shenwei356/rush . rush supports basic and practical features such as line-buffer, timeout, retry, continue, replacement strings. It also has good performance.
A lot of the things in C++ are similar. Inheritance and virtual dispatch are different. The whole deal with move/copy/assign/moveassign constructors/operators can be confusing too. Once used to an affine type model like Rust it's hard to go back to implicit copy ctors being called willy nilly. Pay close attention to these; if you have been programming too much Rust you may end up assuming that everything moves. You will have to build a discipline for writing safe code. Using ideas from Rust helps, since Rust _enforces_ such a discipline. It should be easier to learn C++ since you already understand many other concepts of low level programming from Rust.
That sounds good. Do you know if there are any plans to get the problem you describe refactored (The one about traversing down the whole tree)? If not... would you participate in an issue on github? I'd file one... but my voice might not have that might weight, as I never used Cursive by now...
For those of us whose background isn't in the Programming Language Theory side of computer science, would you mind elaborating on whether this has any effect on the generated machine code and, if so, what?
We are the ones building our culture, so we share responsibility for the outcome. Denial and blaming women for 'not being interested' will only serve to perpetuate the problem. We can do better. Step one is to accept there's a problem and step two to accept that we can do something about it.
How come I can do `String.starts_with` when I don't see it in the docs? fn main() { let s = String::from("Bob"); do_stuff(s); } fn do_stuff(x: String) { println!("{:?}", x.starts_with("B")); } I only see `str.starts_with`. I was thinking maybe when you send the String to the function you are really sending a `&amp;str` but my type parameter on `do_stuff` is enforcing `String`. What's going on here?
`String` *dereferences* to `str` – you can see there is a `Deref` implementation for `String` to that effect. This is called "deref coercion". For your method call, the compiler will insert an appropriate number of dereferences (and optionally one reference) to cut down the number of `&amp;` and `*` you have to write.
I'm a bit wary of our needless_lifetimes lint actually, because of the false positives (with `Box` defaulting to `'static`, `impl trait and `fn` references that specify the lifetime). That said, those are exotic cases and I like it very much in the majority of cases where it works. My personal favorite is `precedence` because while it's for readability, I actually caught two real bugs with it while testing clippy on various projects.
I think cppcheck is ok. Works fine with Qt Creator as plugin. You may also want to use clang static analyzer which is integrated in Qt Creator.
Don't know how I've missed cppcheck, it seems pretty good! Thanks! I'm using Emacs but I can probably integrate it somehow!
Can you put these three lines from your blog: rustup install nightly # +nightly not necessary if nightly is your default toolchain cargo +nightly install clippy # in your project folder cargo +nightly clippy in the clippy documentation? I've been using `rustup run nightly cargo clippy` because that's what was suggested there, and I didn't even know you could do `+nightly`!
I have a clippy lint which I can not really follow. fn read_c_string(process: &amp;memlib::process::Process, offset: usize, count: usize) -&gt; String { let mut buf = vec![0u8; count]; process.read_ptr(buf.as_mut_ptr(), offset, count); let mut cstr = String::from_utf8_lossy(&amp;buf).to_string(); let first_0 = cstr.find('\0').unwrap_or_else(|| cstr.len()); let clean = cstr.drain(..first_0).collect(); clean } clippy tells me to just return `cstr.drain(..first_0).collect()` directly, however this will not compile: error: `cstr` does not live long enough --&gt; src\csgo\netvars.rs:44:1 | 41 | cstr.drain(..first_0).collect() | ---- borrow occurs here ... 44 | } | ^ `cstr` dropped here while still borrowed | = note: values in a scope are dropped in the opposite order they are created How am I supposed to do this?
How did you compile the Rust version? MUSL? What is the setting of `transparent_hugepages`?
The per iteration overhead is negligible here because each iteration took an average of 48ms to begin with. One or two reference counts is nothing by comparison. Also note that the Go version has to allocate as well. In fact, it's much harder to avoid allocations in Go than in Rust.
I download the v0.11.0 [parallel_amd64_linux.tar.xz](https://github.com/mmstick/parallel/releases/download/0.11.0/parallel_amd64_linux.tar.xz) and directly run. I do not know Rust well, I learned it for few hours but gave up :smile:
If you're using Senders and Receivers, you shouldn't require to have them wrapped in a Mutex. You could use a VecDeque instead, but I'd need a closer look.
What's the status of `cat /sys/kernel/mm/transparent_hugepage/enabled`? 0.05 seconds is too long for that kind of task to execute.
Here's results from my AMD laptop: ## MIT/Rust Parallel `seq 1 10000 | time -v ./parallel 'echo {}' &gt; /dev/null` User time (seconds): 0.37 System time (seconds): 2.77 Percent of CPU this job got: 86% Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.63 Maximum resident set size (kbytes): 1768 ## Rush: `seq 1 10000 | time -v ./rush 'echo {}' &gt; /dev/null` User time (seconds): 181.33 System time (seconds): 55.40 Percent of CPU this job got: 281% Elapsed (wall clock) time (h:mm:ss or m:ss): 1:24.17 Maximum resident set size (kbytes): 20500 ## GNU Parallel: `seq 1 10000 | time -v /usr/bin/parallel 'echo {}' &gt; /dev/null` User time (seconds): 207.01 System time (seconds): 68.97 Percent of CPU this job got: 276% Elapsed (wall clock) time (h:mm:ss or m:ss): 1:39.95 Maximum resident set size (kbytes): 16280 Rust is by far the fastest, most efficient solution, using significantly less memory, CPU cycles, and time. The Go solution is only barely faster than the GNU Perl solution.
I'm not sure what's wrong with your system, but measurements aren't adding up. Even if I execute the same command as in your benchmark seq 1 10 | time -v ./parallel 'echo job:{}; seq 1 10' The times on my 2Ghz quad core AMD laptop are much smaller than yours. ## Rust User time (seconds): 0.01 System time (seconds): 0.01 Percent of CPU this job got: 76% Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.04 ## Go User time (seconds): 0.21 System time (seconds): 0.02 Percent of CPU this job got: 198% Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.12
Alright. 
Generally, Clippy would be correct here. Any other iterator, if used in the same manner, would be fine. `.drain()` looks to be a bit funky in the lifetime variance department (because it has a destructor, I think) and the compiler can't seem to correctly reason that having `cstr.drain().collect()` in the return position is perfectly valid. You can avoid this problem, save a redundant copy and clarify your intent by simply truncating the string to the desired length: // `into_owned()` saves us another redundant copy if one was // already made for the lossy conversion, compared to `to_string()` // which makes the copy regardless let mut cstr = String::from_utf8_lossy(&amp;buf).into_owned(); // `.unwrap_or_else()` is preferred for when the "or else" bit is expensive: // fetching the length of a string is not, so we'll just provide it directly. let first_0 = cstr.find('\0').unwrap_or(cstr.len()); // Sets the length of the string if it is equal to or less than the current length; very cheap. cstr.truncate(first_0); cstr
Something like lsp-rs seems more sensible to me, though I don't know if that would make any Lispers unhappy 🙂 Great project though!
Forgetting (leaking) a value doesn't cause memory unsafety though. Memory leaks are perfectly safe because they will never be destroyed. You just want to be wary to not overuse leaks by only leaking values you want to remain for the rest of the application's lifetime. Perfectly acceptable for `main()` level variables that are already living to the end of the application. As for a better workaround, I wouldn't necessarily call it better, but you can import the crossbeam crate which basically does the same thing, minus leaking, for you.
In addition to the garbage collector, which has much CPU/RAM overhead when running which harms multi-threaded performance, in addition to the stops. GC pauses aren't the only performance concern when it comes to GC languages. Something has to spend time tracking and sweeping resources, and it's not free.
But not in this context. If your destructor doesn't wait, the task system will write into memory that doesn't exist anymore. See https://doc.rust-lang.org/beta/nomicon/leaking.html#threadscopedjoinguard And crossbeams scoped thread is basically what I described above. 
I think this is a typo. In the draft that I saw, the function was called `evil`. Probably Niko forgot to update this occurrence when he changed the name to `innocent_looking_fn` (miri is not yet that smart :P).
The Mutex is for my custom "Receiver". Go channel allow multiple writer and reader, in contrast a Rust channel allows only multiple writer and a **single** reader.
Really nice work, especially the lack of macros. I am not sold on the overloaded operators though. Maybe it's because I still have PTSD from the C++ days, but I never like to use overloaded arithmetic operators in contexts that have nothing to do with arithmetic. I don't mind your use of `+`, but I don't find any of the others particularly intuitive.
I agree that GC causes overhead. In fact, GC was the single biggest cost when profiling the Go code (~25%, which is especially ridiculous when you consider that the Rust version avoids GC entirely with nearly the same code). But one of the complaints about the original benchmark on the r/golang side was that it didn't play to the strengths of Go's awesome concurrent GC, so there you go.
I don't think that sanxiyn is saying that "you shouldn't use RLS" as much as he's saying that "RLS is easy to misconstrue as meaning specific to the Language Server for Rust" when your library could, I expect, be used for any language. Using LSP would make it clearer that it supports any language. Plus, it's a crate, so it's obviously in Rust :P
I am not sure whether it matters. I see UB as a kind of miscommunication. For example in the code provided, is the issue that `innocent_looking` frees? Or is it that its type signature is lying? Either is plausible, so I would find it best to point at both places and note that there is an inconsistency that need be addressed without actually taking sides.
&gt; &gt; &gt; &gt; &gt; I did also make use of transmuting to make some values static though. It's perfectly safe for these types of applications. You can leak the value and return a static reference to it if you want to make it 100% safe. Leaked data will remain in the heap forever, until the OS reclaims that data after the program exits. Apparently, it's [not safe](https://github.com/rust-lang/rust/issues/27616).
Damn you kerning!
Seems that's not related to leaking specifically, but working with a mutable static reference. Not something I would do, personally.
In theory you could just use the method of binding for C - http://lua-users.org/wiki/BindingCodeToLua Then don't mangle the rust code so you can call it like C - http://zsiciarz.github.io/24daysofrust/book/vol1/day23.html
Worth noting, this isn't a clippy feature, it's a `rustup` feature.
[Hlua](https://github.com/tomaka/hlua/tree/master/rust-hl-lua-modules) used to provide this functionnality ([example](https://github.com/tomaka/hlua/blob/master/rust-hl-lua-modules/example/module_test.rs)), but since it uses procedural macros (which are unstable) it has been abandonned when Rust 1.0 was released.
Well, this wouldn't fit to my needs. I'm looking for something more abstract. e.g. the Repository pattern like this: pub trait Repo&lt;T&gt; { type Error; fn get(&amp;self, id: &amp;str) -&gt; Result&lt;T,Self::Error&gt;; fn all(&amp;self) -&gt; Result&lt;Vec&lt;T&gt;,Self::Error&gt;; fn save(&amp;self, &amp;T) -&gt; Result&lt;(), Self::Error&gt;; }
Could rust or miri do something akin to escape analysis on pointers from unsafe code? Presumably, a freed pointer should not leave a method of maybe even an unsafe block. That would move the error closer to the source which should make diagnostics easier.
a slightly tangential question -- are there tools (or can rust itself) get test coverage info? if i have to write tests to exercise my code, how do i know that i've covered everything?
The large error bar goes the other way as well. When you have a measurement of "it's bad" and the error bars go all the way from "an epidemic will kill everyone" to "you may stub your toe today", choosing to believe the latter because it is more in line with what you desire is just sticking your head in the sand.
We'll see when plugins are stable. However I have tons of codes waiting for plugins to be stable, so it's probably going to take some time before I tackle this.
That was fixed over a year ago, scroll down to the bottom.
I did an abstract DB layer in C++ at my previous job, used for ~4 years and it was quite a breeze. The thing is, a good unit test is *deterministic* and *reproducible*. Thus, I literally built my mock as a list of query -&gt; response. My cases were simple enough that I didn't even introduce any kind of regular expression or anything. Instantiating the mock required listing each query (query text + arguments serialized in pseudo-json format) and for each query indicating the response (error code OR result, with possibly results to be deserialized from pseudo-json format). It helped that my DB layer at the time was built to provide table and column name for each argument, and result, making the serialized formats understandable. A little parser sprinkled on top, and I specified my test with something like: let mock = Mock::expect( r"Inbound: { "query": "...", "arguments": { "x": 1, "y": 2 }}\n" "Outbound: { "nb": 2, "rows": { "a": 1, "b": 2, ... }\n" "Inbound: {...}\n" "Outbound: { "error": 404 }\n" ); Then the mock would take a query, serialize it, match in against the current inbound, if it matches, return the outbound which is deserialized in the bound variables, otherwise error out loudly (throw an exception, in C++, in Rust you would `assert_eq!`). Some people didn't like that "innocent" modifications to the queries would require updating a good number of tests sometime, but I personally find this kind of "impact tracking" important, as sometimes you didn't mean to affect those tests.
 &gt; I think this is a typo. I think too, just a funny one as it was so well aligned with the title of the post :-)
I'd paste that comment in your Readme if I were you.
Nice post! &gt; In C++, it is possible to accidentally use moved value. Therefore, the move operations usually set the original container size to zero. IIRC you have to do this for a more practical reason too: C++ doesn't have drop flags (and isn't allowed to elide drops, I *think*). So each owned type needs to be nullable, and when moved out the original owner needs to be nulled out so that the destructor can be written to ignore the null. So while you're not supposed to read moved-from objects, destructors will be reading them and you need to design your move ctors and dtors to work together and make the dtors noops in the moved-from case. IIRC it's not even UB to access a moved-from value, and destructors always get called. In Rust after a move the stack space may be reused by other variables, but this isn't the case in C++. 
There are two main things that are causing errors with this code: 1. As you see at the bottom of [this page](https://static.rust-lang.org/doc/master/std/ops/trait.FnMut.html), `FnMut` is implemented for `&amp;mut F where F: FnMut`, but not for `Box&lt;F&gt;`. You need to get the `&amp;mut` from the `Box`: items.iter().filter(&amp;mut *my_filter).collect(); That also requires marking `my_filter` as `mut`. 2. Iterator yields `&amp;Foo`, so the filter passes `&amp;&amp;Foo` to the closure. Changing `&amp;Foo` to `&amp;&amp;Foo` in `complex_filter`'s signature is enough to fix it. Another way (that also fixes #1) is to just call items.iter().filter(|x| my_filter(x)).collect(); Also, the `complex_filter` can just return `Box&lt;Fn(...) ...&gt;`, because the closure doesn't actually mutate its environment. And you can get rid of all the `mut`s then. It works because `&amp;Fn` automatically is also `FnMut`.
this is a possibility. it feels a little bit like repetition too, since the fields of the enum are listed in both the enum definition and the fn definition. i wanted `action: Action::PlayerAttack` to work in the fn definition so that i could then inside that fn say `action.target`, but perhaps that's not worth the trouble?
I think the 2016 Rust Survey had a question about main language. You missed Java, JavaScript and Ruby, IIRC. Apart from that, our outreach so far has been quite humble – other communities (e.g. Ruby) have simply been more active last year, so we may get some easy wins by stepping up our outreach. One should however note that the Rust community is still relatively small, so it hinges on someone taking the lead.
Updating cargo.toml and publishing a new version should do the trick?
Updating the metadata without publishing a version is a planned feature, but not available just yet.
There is no way to specify "the content a specific enum variant has" as a type, but you can put `target` and `damage` into its own struct: #derive(Copy, Clone, Debug) struct TargetDamage { target: u32, damage: u32, } enum Action { PlayerAttack(TargetDamage), .... } ...and then have `player_attack` take that struct as input. Could be an option, especially if you have many fields.
&gt; In most cases maintainability wins, and avoiding “premature optimization” is very much a necessity in C++. I agree with this conclusion. Quite often, when you start coding you don't know, where the performance bottlenecks hide, and you don't want to waste time, thinking about allocating memory for a routine which you could write equally well in any scripting language. Unfortunately plugable automatic garbage collection for less verbose performance uncritical scripting tasks within the language is not yet usable.
Je bent op zoek naar /r/playrust.
I've written/spoken about why I use Rust many times before, [here's one such place](http://www.integer32.com/2016/07/11/why-rust.html). I really can't speak to why other women aren't using Rust.
Nice post, thanks! Found a typo: "This is a problem is when" -&gt; "This is a problem when" Also, not sure about the move constructors? They are used when you call `std::move(foo)` where foo is a custom class? Also also, I never realized that if you didn't take care of that (or before it existed, I think `std::move` was added in C++11?), C++ made that many copies in your back. I feel less guilty about my unnecessary `clone()`s to appease the borrow checker :)
Ok, so not many haskell people, i just defaulted to Haskel/C due to design/intended field of usage. would it be possible to get the rust survey results, but with the diagrams sorted by value, not by alphabet?
really highlights the potential problems you have to be aware in C++ and how easier it is to rely on rust to help you.
The bit about return values is inaccurate, btw. See https://news.ycombinator.com/item?id=13457329
And `Send` and `Sync` can _almost_ completely be defined in an alternate libcore. The only reason the Rust compiler knows these traits is: - `static`s need to be constrained to not contain `Sync` - `Send`/`Sync` are allowed in `+` bounds in trait objects (unnecessary, but helpful, for a working concurrency model) - It caches impls Aside from the static thing you could design all of this out of tree.
Wow! Thank you for taking the time to do this!
Your example doesn't provide enough context for the type checker to infer the type, and the one in the repo does - the rest of the function has enough info for the concrete type of `image` to be determined.
How could I hard code the type then?
You can annotate it with `let mut imgbuf: &lt;concrete type goes here&gt; = ` etc., but I'm not entirely sure what the type should be as I haven't used that crate myself. The [docs](https://docs.rs/image/0.12.1/image/struct.ImageBuffer.html) say you need something like an `ImageBuffer&lt;P, C&gt;` where `P` implements `Pixel` and `C` implements `Deref&lt;Target=[P::Subpixel]&gt;` - so, `P` is one of the implementors [here](https://docs.rs/image/0.12.1/image/trait.Pixel.html) and `C` is a collection of type `P::Subpixel`, which seems to be `T` for all the `Pixel` implementors. So, something like `ImageBuffer&lt;Rgb&lt;u8&gt;, Vec&lt;u8&gt;&gt;` is what you want, I think.
Typo fixed, thanks!
Yep. Most of the stuff you can do in Rust can theoretically be done in other languages, but only Rust makes it safe enough to be practical. 
The nice thing about String conversion and unchecked indexing is that it is obvious what conditions you have to satisfy to avoid UB. For more interesting stuff, like anything involving references, you never really know what the compiler might assume when optimizing the code.
There's pros and cons. Procedural style is more explicit, but it is also more verbose and less semantically meaningful. If I see a call to filter, I know exactly what the intent of the code is. If I see the equivalent 6 line for loop, I have to check all the variables and loop conditions and so on and reverse engineer it into filter. Besides, you can use procedural style in Rust if you want to. Or mix the two, depending on which is clearer in any particular circumstance.
This is a soundness hole, please file an issue for it. While exploiting soundness holes is ok for the underhanded contest you should still file bugs for them. I see https://github.com/rust-lang/rust/issues/10389 , but that seems to paint this as an unlikely occurrence, whereas your example is more concrete. Explaining this particular use case there would work too.
Perhaps a build script would be enough. http://doc.crates.io/build-script.html
IMO this actually isn't abuse, it's exactly what the borrow checker is for -- preventing accidental mutations while other code expects that it doesn't change. In essence, preventing the (non-memory-safety-related) problems from http://manishearth.github.io/blog/2015/05/17/the-problem-with-shared-mutability/ However, you'll have an issue with valid moves from two games being mixed up. Using an invariant lifetime fixes this IIRC (and is a trick you can use to implement unchecked indexing with "safe indices")
hey - I think you meant this for an arkham variant, I am not a Rust guy, sorry
Oh, you are right. The title is a bit click-baity :/ I can't change it on reddit anymore, I guess. Sorry! &gt; However, you'll have an issue with valid moves from two games being mixed up. Using an invariant lifetime fixes this IIRC (and is a trick you can use to implement unchecked indexing with "safe indices") Oh right, this is indeed an issue :( What do you mean by "invariant lifetime"? Do you have a link where I can read about those? Are those the things making interior mutability containers special?
&gt;with Rust’s linear type system I always thought its affine not linear?
Yes, I think so. But (without knowing a lot about these things) I think quite a few people use "linear type system" as a broader term. Even [Wikipedia](https://en.wikipedia.org/wiki/Rust_(programming_language)) says "linear" in the section "typing disclipline". Is Wikipedia wrong or too vague?
For the initial proof of concept see [this post](https://www.reddit.com/r/rust/comments/3oo0oe/sound_unchecked_indexing_with_lifetimebased_value/), especially the linked [playpen](https://play.rust-lang.org/?gist=21a00b0e181a918f8ca4&amp;version=stable). The [indexing crate](https://github.com/bluss/indexing) seems to be a more polished version of that concept.
It doesn't say that it's "undefined behavior," but it is allowed to assume that the loop terminates, which means it can assume anything that must be true in order for the loop to terminate. You can trivially prove contradictions with that, and anything follows from a contradiction.
Uber-pedantry: thanks to `Drop` you can say the system is linear since values always get 'used' at ends of scopes by the implicit `Drop::drop`call. 
&gt; `set_cell` a method of `ValidMove` That's a great idea! :)
Although Rust is far more complex than go. I would avoid saying "Go is for beginners". There are a lot of low-level things which can be done with Go that could spell disaster for someone new to programming. I actually have a lot of friends who avoid Golang because to them, it's "too low level". 
Oh you are talking about that thing! I even linked the "indexing"-repository in my blog \^_^ (search for "sound unchecked indexing") But I guess I should try to really understand it instead of just skimming over it. "Invariant lifetime" sounds promising.
By the way: I am really not sure whether "blog spam" like this is wanted/OK in this subreddit. I figured that if it's unwanted it would be downvoted and ignored (therefore I posted it). Or should I rather not post small, unimportant things like this?
Well, since we agree that the standard doesn't call an infinite loop undefined behavior, please show an example program with such a trivial contradiction (whatever that means in piece of code?) which is undefined behavior.
`fn magic&lt;T&gt;() -&gt; T { magic() }` This *assumes* that the recursion never returns, *therefore* nothing can see the return value. You can do something similar with actual loops but UB effects are harder to show in LLVM optimizations, whereas that function will *invariably* become `mem::uninitialized` under optimizations atm.
Maybe shorter, but functional style maps directly to what I'm trying to accomplish. "Take the people last named 'Smith' [filter], find their age [map], and average them [reduce]". Those high order functions allow you to think about instances of the list rather than the list itself, and that's a lot easier for me.
Author here! I posted the first iteration of this here a few weeks ago and based on feedback have refactored it into a cargo plugin. The project has been renamed `cargo-safety` to match plugin naming conventions and is available through crates.io. Feedback welcome and thanks for taking a look!
Arc is built on top of `std::atomic`. These themselves are just wrapper types over integers with methods that link to atomic intrinsics. The atomic intrinsics are where the language gets involved again.
&gt; (and isn't allowed to elide drops, I think) When performing copy/move elision for return value optimization, the drop is elided, just because it moves directly into the result, so there's nothing to drop. Moved-from values are still safe to use but their contents are specified on a per-type basis, and often unspecified. So for example, it's safe to use a moved-from vector/string of you `clear` it first (and in practice, even if you don't). 
Here's a guide I used to get statement coverage working: http://sunjay.ca/2016/07/25/rust-code-coverage This guide claims to give you branch coverage, but it's more involved, so I haven't tested it out yet: https://users.rust-lang.org/t/howto-generating-a-branch-coverage-report/8524
Yeah, you're right, I poked at it more. How do you prevent this from happening with intrusive datastructures? Delete the move ctor? But then you lose move semantics .... Edit: oh, I'm an idiot, this will elide the copy too so there's no need to call any ctor. &gt; Moved-from values are still safe to use but their contents are specified on a per-type basis Yeah, I'm aware, this was more as a general rule
I would prefer not to see these types of comments in this subreddit. It's completely unstructured and unconstructive criticism.
It appears this is a [known bug](https://github.com/rust-lang/rust/issues/31723) but nobody really knows what is going on.
After the recent discussion in the subject I made a [major overhaul](https://users.rust-lang.org/t/stdx-the-missing-batteries-of-rust/2015/56) to stdx. I've tried to structure it as a teaching resource for new Rust programmers trying to get a grasp on the ecosystem. Let me know what you think.
Did piston change? My dependency is now on the latest and cargo is giving me this: error[E0432]: unresolved import `piston::event` --&gt; src/main.rs:11:5 | 11 | use piston::event; | ^^^^^^^^^^^^^ no `event` in `piston` How do I get around this now?
You need to use the permalink button on the playground for the link to be shareable.
What types were you using from `event`? They've probably been moved.
It might help just putting a `crates.io` link on `https://www.rust-lang.org/`, as this might be the page most new Rustaceans visit first. One might even add an *"easy package management"* example to this page, in addition to the playground example code.
Did you end up having any success with the non-allocating parser?
If the result is going to be unsigned, it sounds like you should cast the `i32` to a `u32` before the arithmetic (casting is done as in `a as u32`). Check for negative numbers at that time.
You might think this but it is the exact opposite. My implementation was originally using channels to pass messages back for handling, but writing to the disk proved to be significantly more efficient on CPU cycles, memory, and times -- especially memory. I thought about adding a feature flag to re-enable the old behavior, but I may not do that at all now because there's no benefit to it. Basically, the receiver that's handling messages is located in the main thread. When a new job file is created, it automatically starts reading from that file without being told to by a channel. It merely stops reading that file when it gets a signal from the channel to do so. The goal of handling messages is to keep outputs printed sequentially in the order that inputs were received, so if it receives a completion signal of a future job, it merely adds that to a buffer of soon-to-check job IDs once the current job ID is done. This allows for real-time printing of messages as soon as they are written to a tmpfile. Now, your OS / filesystem typically does not write data directly to the disk at once but keeps in cache for a while (which effectively means zero cost to access), and tmpfiles on UNIX systems are typically written to a tmpfs mount which is actually located in RAM, which means even less overhead. These tmpfiles are deleted as soon as a job completes, so they rarely live enough to make a difference in resource consumption. As for the costs of handling files in this way, there's pretty much no visible cost because the speed that the receiver prints has no effect on jobs being processed, and in the worst case scenario the cost would be the time to print the last N completed jobs (which can usually be buffered into one syscall to read a file and reading that entire file directly to standard out).
One of the distinctions between linear and affine types that popped up in a previous thread was that, if Rust had a linear type system, it'd be possible to implement a more thorough version of `#[must_use]` that doesn't get fooled by assigning the value to a variable and then never doing anything with it.
I'm wondering why you use exact version specifications in stdx' Cargo.toml, but not in README.md where it recommends copy/paste if you only want one or two of the crates. I figure the reasoning is that (1) stdx wants to guarantee that this exact combination works together, and (2) usually having upgradable dependencies in your Cargo.toml is the right choice. But there seems to be a conflict here -- like "hey, what does stdx know about semver that I don't?".
Have you considered [dependency injection (DI)](https://en.wikipedia.org/wiki/Dependency_injection)?
This experience has taught me a lot about macros. Now I finally understand how matching works and how to debug when a macro fails to match.
&gt; Why the hell would they be? This question was what motivated this thread; the answer is optimizations, as linked above.
As far as I can see the files `Cargo.toml`and `src/lib.rs` arent' really being used, so perhaps they should be deleted. IMO the best aproach is to teach users how to import libraries directly instead of teaching them to import them via stdx. Great work btw :-)
`std::sync::atomic` is just `core::sync::atomic`. Which is quite useful in `no_std` programs. The [spin crate](https://crates.io/crates/spin) is just implemented with these library types.
Those docs seem to be old (as is the case with too many crates' self-hosted docs, unfortunately). The most recent version looks very different: https://docs.rs/piston/0.27.0/piston/ It looks like that stuff is now spread between `piston::event_loop` and `piston::input`.
We can't pass a mutable reference to the board to the player: the player implementation could do everything with it, including changing more than one cell (I assume the player implementation is foreign, thus evil, code :P). It might be possible to enforce a "change only one cell" through the ~~linear~~ *ehem* affine type system, but I'd rather mutate the board outside of `Player::next_move()`. Additionally, it would be nice to perform the check "is this cell occupied" only once for a given cell-coordinate. If we wouldn't use strong typing *somehow*, the calling code and the player both would have to execute that check. Of course, the check isn't expensive at all and this all just a fun excercise.
No love for `slog` ?
No stale moves ever exist. Either a move is a success or a failure. On success the turn swaps, on failure the player needs to try another position.
Maybe this thread should be pinned?
We only get two announcement threads at once, so one of them would need to be un-announced to pin this. Just upvote it so it stays on top?
Serde-json is in the Cargo.toml, from what I can see.
An RDF library called [Rome](https://crates.io/crates/rome). It has a [set of traits](https://github.com/vandenoever/rome/blob/master/src/graph.rs) that can be implemented by data sources to make them accessible as RDF. Figuring out how to make the most beautiful yet low overhead set of Rust traits has taken quite some thinking so far and it is still ongoing: there will be API churn. So far the library can read and write Turtle and NTriples files. The parser is written in nom. There's an [additional repository](https://github.com/vandenoever/rome-hdt) for reading [HDT files](http://www.rdfhdt.org/). I pushed the code public yesterday and will spend some hours this week writing documentation. For application developers the interesting part is the code generation. RDF properties are translated to Rust traits and data can be accessed via these interfaces. In that sense it's an early days Diesel for RDF. 
Drawing parallels with C#, `Concat` (the same as `chain` if I am not mistaken) uses two explicit `foreach` loops. Maybe that structure is easier to optimize (chaining of two, generated, state machines). What enables that particular programming style there is the use of `yield` (or generators or coroutines, or semi coroutines). https://referencesource.microsoft.com/#System.Core/System/Linq/Enumerable.cs,692
I am surprised by this, too. I expect people will copy the exact version numbers to their Cargo.toml, and then never even get bugfix updates for the crates they use as they declared the full version number. Isn't that a problem?
`mem::forget` consumes the value, but regarding leak-by-Rc value I don't really know enough to say whether moving the value into an `Rc` constitutes consume/use, but it does sound intuitively like you're right.
If you need to defer the operation, you can just return a guard or the `&amp;mut Cell` where it may be mutated later. The borrow will be equal to what panda did.
We never pass the player a mutable game, only an immutable view to see the board. The player must already iterate over the whole board so there is nothing left to optimize here. The game would call `let move_attempt = player.tick(&amp;game)`, and then try to apply the move. If it is valid, we progress; else ask the player to try again. The game knows which player whose turn it is, and it asks that user. It can then use that knowledge to set the value, so the player doesn't need to know who they are. Your current solution doesn't prevent this additional check anyway. The player always inputs a number or `CellId`. Where did the player acquire the `CellId`? One option there would be to give the player a vector of possible moves. This is needlessly over complicated. I stick with my [original suggestion](https://www.reddit.com/r/rust/comments/5pk1pu/abusing_the_borrow_checker_to_make_tictactoe_safer/dcscogr/) The following provides a deferred interface. You could wrap this mutable pointer with a prepared guard where you would just call `guard.commit()` to apply it. I see little value in this method. https://play.rust-lang.org/?gist=1906743a814f1b15e9d1fa24edaaa35b
I don't think the question is about individual data structures and efficiency but rather about features that software must reimplement if they're not available in the standard library. For instance, an HTTP client/server implementation, json parsing, csv writing, shell syntax parsing, and so on.
A quick scan of the code shows a bunch of conversions from string -&gt; []byte -&gt; string, which is probably the source of the allocations. Sticking with one (likely []byte) would reduce the GC pressure and is one of the first things I'd try to optimize. It is true that the concurrent collector is currently tuned for latency at the expense of throughput. This means that parallel CPU-bound code will be slower due to the time stolen by the collector. Throughput will be addressed in upcoming versions.
wow, that's a great way to build a state machine over nom parsers :) Do you think it would be easy to support `Incomplete`? Like, a special version of `traverse` and `traverse_recursive` that will allow reparsing from the same state once we get more data?
Thanks!
You can indeed handle arbitrary types with `serde_json` using the built in `Value` struct (to DOM parsing). You can also use your own defined enums to manage cases where JSON types differ at times. The main difference between the crates is that `json` is built exclusively for DOM and makes some trade-offs for that purpose, which shows in benchmarks and the API design.
&gt; But my arrays are static, so what's the problem here? Actually, your arrays are `const`, not `static`. The two seem equivalent, but behave differently in this case. When you use a `const`, you're actually creating a copy of that `const` in the local scope. So here, you're making a copy of each array and referencing it, so it would be as if you did: if arg { let array = STATIC_ARR_2; &amp;array } else { let array = STATIC_ARR_1; &amp;array } Now you can probably see why this is invalid. There's a couple solutions, both work similarly in the end: Use `static` instead, which allows static references (usage remains the same): static STATIC_ARR_1: [usize; 32] = [...]; static STATIC_ARR_2: [usize; 64] = [...]; Or, use references in your `const` declarations, which is allowed (usage can drop the reference operator or keep it; deref coercions will take care of it): const STATIC_ARR_1: &amp;'static [usize; 32] = [...]; const STATIC_ARR_2: &amp;'static [usize; 64] = [...]; let arr: &amp;[usize] = if arg { STATIC_ARR_2 } else { STATIC_ARR_1 }; You would probably consider the latter more verbose, and I would agree. However, its one advantage is the ability to create static dynamically sized types (usage remains the same as above): const STATIC_ARR_1: &amp;'static [usize] = [...]; const STATIC_ARR_2: &amp;'static [usize] = [...]; Though with the `static_in_const` feature you can drop the `'static` from the declaration and it becomes more terse (usage remains the same as above): #![feature(static_in_const)] const STATIC_ARR_1: &amp;[usize] = [...]; const STATIC_ARR_2: &amp;[usize] = [...]; This feature is unstable and thus requires nightly, but it has recently [gone into final comment period for stabilization](https://github.com/rust-lang/rust/issues/35897#issuecomment-274425821) and doesn't appear to have any blocking issues, which means that it will likely be stable as of the next release. There was also [an RFC recently](https://github.com/rust-lang/rust/issues/38865) to allow rvalue references to be promoted to `'static`, which would eliminate this issue. However, it looks like it's still blocked on implementation.
While I enjoy reading it, I'm not sure "What's everyone working on this week" need to be pined.
I should have said "well you can't really do that **easily**". EDIT: I should stop commenting on reddit when I don't have time to write a long explanation. I always end up looking like an idiot that said something wrong and doesn't know what he's talking about. Let's close this discussion, do as if I didn't say anything. 
No, it should be in the sidebar instead. Edit: sidebar is getting really long, can it be condensed into a few points and a link to a longer Reddit FAQ/Rules?
Finishing up bindings for [rust jack](https://github.com/wmedrano/rust-jack), a real time audio and midi library.
Actually, it would be more like this: int func(int a, int len, int *p) { while(a &gt;= len); if (a &lt; len) return -1; return p[a]; } The compiler removes the bounds check, because the loop means it's unreachable, then it removes the loop.
I think I understand. I'll paraphrase: you have a HashMap which one thread just consumes and the other updates in bulk. A partial bulk update is not valid; you don't want the consumer to observe this. You're trying to decide how to avoid that if the update process fails halfway through. Sound right? One option is to create a `Transaction` object. You can build up a set of changes to apply to the HashMap which can be committed as one or rolled back. The Transaction would essentially be another hash map with the changes for each value: `enum Change { Delete, Upsert(ValueType) };` You could provide a function to do a lookup on the transaction which returns the change value if one is present, or the original hash map value if not. The disadvantage to this approach is that if a Transaction updates the entire database, you double your memory requirement. I have something kinda similar to this in some of my code [here](https://github.com/scottlamb/moonfire-nvr/blob/168cd743f431b572afe8214510e03eeeddffc4d3/src/db.rs); search for "struct Transaction". It's not quite the same because this is actually doing a transaction on a SQLite database, but it uses the same idea to transactionally update an in-memory cache if the SQLite commit succeeds.
Looks like just a proposed approach to getting generators in rust.
Many rustaceans want generators, but the design has to be right and all kinds of design details and their pros and cons have to be considered before it can be implemented and committed to. There has been quite a lot of discussion and two RFC's (design proposals), and this write-up summarizes the discussion thus far.
I _think_ I understand what you're looking for. The [cernan](https://github.com/postmates/cernan) project has a subsystem that's lua calling rust and rust calling lua. You can see its implementation [here](https://github.com/postmates/cernan/blob/master/src/filter/programmable_filter.rs). We ended up using the [lua](https://crates.io/crates/lua) crate. 
Nice talk. I think the most useful bit was about how to design a C library to be easily callable from Rust. If that's not in the Rust Book under the C interop section, it should be! Oh, hmm, it makes a brief mention in the [Representing opaque structs][link] section. [link]: https://doc.rust-lang.org/book/ffi.html#representing-opaque-structs &gt; Sometimes, a C library wants to provide a pointer to something, but not let you know the internal details of the thing it wants. The simplest way is to use a void * argument But it's got an additional pattern (hack?) that'll let you distinguish between different types of void pointers in Rust. Cool!
It's there: https://github.com/brson/stdx/blob/1ba644efbb56cb91c49eb275daba0970fe51d82b/Cargo.toml#L15
slog is mentioned as an alternative, fwiw.
My bad, you are right.
No, parameters are passed by value unless you have an `&amp;` or `&amp;mut` parameter (though you can get the same results with an Rc). The "reference" in "pass by reference" is different from the "reference" in "dereference". Really, it's more of a Java-esque "pass reference by value" thing even in the case Rc&lt;T&gt; or &amp;T are involved. In case of the string, the pointer and the length just get copied into the function. ---------- The pass by reference / pass by value duality from C doesn't map cleanly to Rust (or most other languages, really). You either pass by copy (similar to pass by value), or move an owning reference, or move/copy a non-owning reference. The last one is closet to typical pass-by-reference semantics. Moving an owning reference, like Box or String, is still passing a reference (a pointer), but it's a pointer to non-scoped heap data, and you can't observe the changes outside, so the duality doesn't make sense for these things. Instead, start thinking in terms of explicitly whether shared things are being passed. Passing &amp;/&amp;mut/Rc/Arc means that the data is shared with the caller.
To be fair, isn't it kind of a moot point to criticize a data structure like that because of a slight performance difference? I mean, Python doesn't even really have arrays by default either. It has lists which are quite a bit more heavy weight. Pretty much every primitive is a heavy weight derived tool to fulfill the purpose of something maybe a little bit more strictly used in a lower level language, of course sacrificing some performance. For the most part it's about having general purpose primitives that have the benefits of array access and linked list inserts all in one, etc. It's a very general purpose language and someone can come in and write good software without explicitly being trained with knowing the difference between these sorts of things. They can come in and say "I just need something to hold a column of integers and I might add to the end of it." and not dedicate their program to a specific implementation of a specific data structure. If you REALLY needed something like that to eek out as much performance as possible, you'd write a C or Rust extension which handles that bit of logic. That's what you have to fall back to. Otherwise, we assume that you can sacrifice some performance by using a more general tool. For something like web dev, if it's the difference between a request being handled in 300ms instead of 200ms or less, people aren't going to care as much. The people that can't make this sacrifice know it ahead of time generally.
Perfect sense to compscis. Normal people would say you transform lowercase to uppercase. You don't 'map' it. Fold is really bad, you are right. They could have gone with 'combine' or 'merge' or something instead.
Python does have OrderedDict apparently, so I may have been a bit pedantic. Still, I think Python is the only language I've used that doesn't have something like a TreeSet. 
If you put `bitflags = "0.7.0"` in your Cargo.toml then `cargo update` will still get you upgrades in the 0.7.x series, but not to 0.8.0. It's when you give the spec as `"=0.7.0"` that you don't get upgrades at all. ^(Sorry to be annoying if you already know this -- can't tell from your wording.)
Thanks. I was hoping there was something built into cargo, but this is probably the least bulky option.
The JSON situation is a little unclear to me. I use serde_json strictly for serialization, and json for handling arbitrary JSON. It looks to me though like serde_json has that same capability, so maybe it makes sense to use serde_json for all functionality (fwiw serde_json is discussed explicitly under the serde section).
After some discussion [in another thread](https://www.reddit.com/r/rust/comments/5ofuun/multi_mut_multiple_mutable_references_to_hashmap/), I improved my [splitmut](http://docs.rs/splitmut) crate a bit and released a new version. Also, since my [dbus](https://docs.rs/dbus) crate is what most people use to connect to D-Bus, the amount of questions and bug reports is slowly growing. I've been working on improving dynamicity with support for trait objects, and also an experimental code generator that generates some rust code from intrespection data. Maybe it's ready for a 0.5 release soon, I hope so.
Wouldn't the Cargo.lock be good enough for apps? It effectively constrains by strict equality (but with an easy way to upgrade) and we already recommend checking it in for apps but not libraries.
Yes, it may be that a stdx lockfile is sufficient for some validation purposes. In particular, if I could take one lockfile and apply the versions in it to another lockfile, that would be quite powerful.
Now that you say it, I think I may have read this before somewhere... still, thanks for explaining :) I have to say I find this slightly surprising though, usually I would interpret the equality symbol as expressing, well, equality. For `bitflags = "0.7"` to pick an arbitrary patch level would not be surprising, but when it is "0.7.13", it looks like a specific patchlevel was quite explicitly selected. Whatever.
Sure, I'd be happy to make stdx compatible.
I try to compile this: fn main() { let a: u64 = (1..101).sum().pow(2); println!("{}", a); } I get this error: error: the type of this value must be known in this context --&gt; src/bin/p6.rs:2:18 | 2 | let a: u64 = (1..101).sum().pow(2); | ^^^^^^^^^^^^^^^^^^^^^ I've heard good things about rusts type inference, so I was a little surprised that rust (if I interpret the error correctly?) needs some help to figure out what type I want `a` to be. Can someone explain what's the problem here? I ended up solving the issue by putting a `::&lt;u64&gt;` on the `sum` function.
This comment is closest. :) If you want generators now-ish, use Stateful! 
&gt; I actually discovered interesting information about this upgrade - that clap wasn't compatible with the latest libc. I'll look into this.
I'm far from an expert on this but I believe it needs to be &amp;Any because it turns it into a trait object which are unsized and can't be passed between functions. But to get it back you just use Any::downcast_ref
Yes, I do intend to publish a stdx crate. The story there is a bit unclear to me yet, though (e.g. I wouldn't do so with the equality version constraints).
Thanks!
There is a github repository that explains the similarities and differences between the two languaes (although it is from the perpective of a c++ programmer coming to rust), it might help: https://github.com/nrc/r4cppp
The `.sum()` function is actually implemented in the `Sum` trait, which is generic over its elements/result. Worse your range has no specified type. Adding a type suffix as in `0u64` to the start of the range should suffice. 
I just tried it out on a project I'm working on. It outputs a large JSON blob that's a bit difficult to parse by eye (or even via `| jq`). It would be nice if there were a `--pretty` flag to output a human-readable list of packages and unsafe code in each? (or maybe make pretty the default and add a `--json` flag?) It should also probably exit with a non-zero exit code if it finds any unsafe code. Then you could easily use it in your build scripts to assert that no unsafe code is present. Hmm, that might warrant a `--quiet` flag too. and `--help`. `&lt;/brainstorm&gt;` But overall, this is a nice tool! Thanks for publishing it!
Thank you 😊 yeah I also thought about a better support for `Incomplete`. It should be possible yes. The biggest "problem" which is not solved for now is the support of Result types referencing to the input slice. ☹️ [With this](https://github.com/rust-lang/rfcs/pull/1598) it could be possible...
Ah. The three most difficult things in computer science: naming things and off-by-one error.
&gt; usually I would interpret the equality symbol as expressing, well, equality. There's no equality symbol here, or rather, the `=` is part of toml, not part of the spec. You're right that this is easy to forget, though. The key is that the default does the right thing, by default.
For the record, that exact type is available as a `type` definition in `image`: https://docs.rs/image/0.12.1/image/type.RgbImage.html
Probably something that would be better with HKTs, maybe it'll spark up again if/when those land
Being familiar with npm/node/package.json, I was also surprised at this. The `semver` crate documentation refers to https://github.com/npm/node-semver; this page describes npm's behavior where `=` is the default, so leads to the wrong conclusion. Rust's default seems to only be documented here: http://doc.crates.io/specifying-dependencies.html#specifying-dependencies-from-cratesio I do agree that `^` is the better default though.
Thank you so much for trying it out! I'm following some convention here with cargo plugins outputting json, but I agree it can be quite a lot for unformatted output. I would recommend `cargo safety | python -m json.tool` for the pretty printing from the command line. Per your comment about signaling failure, I'm not quite sure it should do a non-zero exit code because `unsafe` code is more of a warning to the consumer that you may want to review that code yourself to see if you are comfortable with it eliding Rust's guarantees about safety. I'm definitely torn about this, but I think the pragmatic approach here is to surface the issues and withhold judgement? ¯\_(ツ)_/¯ Thanks again for the thoughtful feedback!
I've added a tag for 0.20.0, and will add more tags as we publish new versions (or bug others to do so ;))
I'm iterating on [quasar](https://github.com/anowell/quasar) - an experimental rust-to-wasm/asmjs framework. As a first web framework for me, lots of experiments have run into interesting walls when trying to build little demos, but some core concepts and goals are starting to become clearer. I'm hoping to get something close to [this To Do example](https://gist.github.com/anowell/531dda3ffe1146ad6553cfa896481c41) working in the next couple days, and start a demo site.
No you are not missing something, I replaced partition with something that is incorrect...
Mostly trying to forget how much I love Rust because it's just a hobby for me. ;) But really, been trying to make my Rust port of gearmand, Rustygear, multi-threaded: https://github.com/SpamapS/rustygear/tree/threading
I will be fixing the crate docs, thanks :)
Other than servo I think I got bindgen with [zmq](https://github.com/erickt/rust-zmq/commit/03fa03e6b999090948f4ae3c25af1d9237b0b21f). December 10, 2011! Check out those beautiful sigils!
That sounds like exactly the thing I'm looking for :) Do you have some code snippets? How does the API look like for DB actions with transactions? Like this: pub trait MyDbTrait { // generic calls fn get&lt;T&gt;(&amp;self, id: &amp;str) -&gt; Result&lt;T&gt;; fn all&lt;T&gt;(&amp;self) -&gt; Result&lt;Vec&lt;T&gt;&gt;; // transactions needed fn delete_foo_and_save_bar_and_vec_of_baz(foo_id: &amp;str, bar: &amp;Bar, baz: &amp;Vec&lt;&amp;Baz&gt;) -&gt; Result&lt;()&gt;; // ... }
I updated the repo with a version that addresses most of your points. Thanks again for reviewing! I haven't added spaces around the .. in iterators, though, as it seems that rustfmt just removes them. Should it not do that?
I really like this approach. Most updates will be partial. Thanks a lot I'm gonna give it a shot.
This sounds solid, thank you.
To me, it is because generators feel like anti-functional programming. Where functional stuff is all about side effect free functions, generators maintain their own internal function state (which could be a lot, especially if the generator is a composition of multiple generators.) I'm not a fan of a lot of mutating state, and generators feel like they are mutating state on steroids. With that said, they are super powerful, async &amp; await are useful constructs that are made possible by generators.
Ah wow I didn't know that, thanks!
I work on Redox at night and on the weekends now - I have some time during the day and will try to negotiate more
Why do `println!("")` calls get hidden when running `cargo test` but calls to `writeln!(&amp;mut std::io::stdout(), "")` do not?
Cool thanks.
it would help to know what platform you are on. beware that if you are on windows, there is some important behavior differences that depend on if you're running your executable through cargo or not. 
 fn id&lt;T&gt;(x:T) -&gt; T { x } fn main() { let id = id; println!("{}", id(4)); println!("{}", id(true)); println!("{}", id("hello")); } This code also cannot compile. Generic functions can't be values. We can't declare a variable with the type Vec&lt;T&gt;, unless we specify a concrete type for the type parameter T.
&gt; In my case it was an idea to remove a range of values from collection described by a pair of two iterators Isn't this just Vec::drain()?
&gt; You might think this Indeed I do. &gt; but it is the exact opposite. My implementation Yes. :-) &gt; using channels to pass messages back for handling Good idea! &gt;but writing to the disk proved to be significantly more efficient Which disk? How does it look in a Docker container? Mounted read-only for for ultimate security, of course. :-) &gt; on CPU cycles Interesting detail, but extraordinarily hard to believe. &gt; memory Ah, you must have been using buffered channels! &gt; and times Measured how? &gt; especially memory Big buffered channels! &gt; I thought about adding a feature flag to re-enable the old behavior, but I may not do that at all now because there's no benefit to it. https://12factor.net/ &gt; the receiver that's handling messages is located in the main thread Thread? Go doesn't have threads. &gt; When a new job file is created Please don't do that. You'll regret it later should you need to scale. &gt; The goal of handling messages is to keep outputs printed sequentially in the order that inputs were received An impossible goal should you need to scale &gt; This allows for real-time printing of messages as soon as they are written to a tmpfile. That's not a feature, it's a dependency you'd be better off without. &gt; Now, your OS / filesystem typically does not write data directly to the disk at once but keeps in cache for a while (which effectively means zero cost to access), and tmpfiles on UNIX systems are typically Engineering for typical brings down the application... &gt; As for the costs of handling files in this way, there's pretty much no visible cost because the speed that the receiver prints has no effect on jobs being processed, and in the worst case scenario the cost would be the time to print the last N completed jobs (which can usually be buffered into one syscall to read a file and reading that entire file directly to standard out As opposed to the cost of receiving the message on a channel and printing it to stdout. It just cannot be...
Whoa, C++ support? That is an accomplishment!
This is *very* application specific. Does your application have customers? Well you will have a customer ID, you will need to be able to look up a customer information by ID, and by god you should be able to hand such a customer over to the DB api and have it save it for you. The API will probably be larger than one single trait. Simple DB API's usually can be handled this way, but even moderately complex API's get broken up by usage. The key is that you are building the Program &lt;=&gt; API &lt;=&gt; Storage abstraction. Your mock for testing should just replace the storage side and look to your program just like your API. Fun additions which make life easier? pass through api's (ie, Program &lt;=&gt; API &lt;=&gt; Logging &lt;=&gt; API &lt;=&gt; Storage etc). My current application at work does this type of thing. I have an ILoggable. But I also have an 'echo to terminal' ILoggable, a 'send to port' ILoggable, a 'bundle everything that is to be logged into the ILoggables I have as a collection', etc etc. This lego connection type behavior makes things about a billion time easier to debug and reason about. *You* need to implement things at that level, it's very application dependent.
The wrong sub
libtest (which is what `cargo test` uses) [uses the standard library's unstable `io::set_print` function](https://github.com/rust-lang/rust/blob/ac5046cf67e51df286e7c8df02d67c302d4c4d09/src/libtest/lib.rs#L1304) to replace the reference to the real stdout with an IO sink. This is a thread-local change. `println!`, which calls `print!`, uses the standard library's unstable `io::_print` function, which will write to whatever IO object was passed to `set_print`. None of this prevents you from getting your own reference to the real stdout and writing to it, which is what is happening in your `writeln!` example.
I needed something like this in [Domafic](https://github.com/cramertj/domafic-rs) and [wrote it using generic functions.](https://github.com/cramertj/domafic-rs/blob/master/src/processors.rs)
But the same could be said for iterators, which have to manage internal state. If mutating state happens behind a safe facade, then what would be the problem? (cf. Builder pattern)
[imag](https://github.com/matthiasbeyer/imag/) finally got the [PR for ruby bindings](https://github.com/matthiasbeyer/imag/pull/847)! It is not merged yet, but I plan to merge it this week. So we can finally script the imag store with Ruby, which is also the goal for the 0.3.0 release. The other crates (imag-diary for example) were removed in the process and will be rewritten in Ruby. Why I started this shift-of-focus is [described here](https://beyermatthias.de/blog/2017/01/17/whats-coming-up-in-imag-22/) - TL;DR: Remove own integrations, build Ruby-Gem out of core libraries, because bootstrapping tools/integrations in Ruby is faster. So: Basically, I would love to keep everything in Rust. But as progress is (in my opinion) more important than a homogenuos codebase, I opted for writing Ruby bindings and then write the user-interfaces (notes, diary, mail, calendar, contacts, ...) in Ruby. Another background for this was that Ruby has awesome libraries for all the things - example: Mail parsing. Rust has none (at least not high-level libraries). Eventually, the whole stuff will be rewritten in Rust in the future - if more people get interested in imag, maybe... 
Maybe sidebar then?
&gt; this 'dependency' has no burden on the runtime. Choosing not to implement it would basically mean that I could no longer claim to be a drop-in replacement for GNU Parallel In that case, my apologies for misunderstanding!
Quick question: there are two popular crates dealing with time, 'time' and 'chrono'. The first is certainly simpler if I just need to parse dates and get broken-down time. But what is likely to become the go-to, 'canonical' crate in the future?
`println!()` is actually a macro that expands to quite a bit of code. The most expensive thing is does is probably formatting the input. With that aside, are you compiling in debug or release mode? If you're in debug mode, everything is going to be slow. Also, 500ms seems really slow to me, and I can't get anything close to that on my machine (i7 6700).
How to measure the time from calling println! until it is shown on screen? The best way i can think of is printing the time (with milliseconds) and letting another (faster) program doing the same on the screen. capturing this with a (highspeed) camera and skip to the frame the println! shows the time for the first frame. The difference in time from your println! and the other program (with near zero latency) on the screen is how long it takes for println! Either way ~750ms sound ridiculously slow. No matter how computational complex println! is, i can literally make trillions of operations in that timespan. I guess something in the i/o chain between your executable and the output of your monitor is very slow. Maybe your terminal is not very fast? EDIT: The camera thing is not very practical as i have no camera around atm. but i did the following use std::time::Instant; fn main() { loop { let mut start = Instant::now(); println!("i am a println"); let end = Instant::now(); let delta = end.duration_since(start); let delta_ms = delta.as_secs() as f32 * 1000_f32 + (delta.subsec_nanos() as f32)/1000000 as f32; println!("cost: {}", delta_ms); } } with the following result i am a println cost: 0.001349 i am a println cost: 0.001425 i am a println cost: 0.00167 i am a println cost: 0.001468 so to measure the delay to your terminal you need to use the camera method as i cannot see println! beeing slow from the "code side" 
I see. Thanks a lot! One more thing: you said "I wrote an article about this"; Where can I found this article?
And perhaps "which console are you using?".
Very important. In all my experience with various languages, I've never written a program where the bottleneck slowing down perceived time to output text *wasn't* I/O with the system at large. Terminal emulators are generally much slower than you might intuitively expect. (And, when it's not the terminal emulator, it's usually some kind of pathological "program loads slowly because some piece of it isn't in the disk cache and the backing store is on rotating media under contention" case.) 
Point taken, those are all language I try to avoid if I can. 
Can you try it in [Alacritty](https://github.com/jwilm/alacritty)?
We already have interfaces more polymorphic than the data they're applied to (generic impls and methods) - we can model named functions and closures like that, IMO, so the generic parameters are unapplied until each call.
Though even without doing that, you're still nowhere close to 500ms/println!. With ZSH in Terminal, same machine as above, the naive program I posted takes ~3000ms for a million println, or about 3µs/println, which is 5 orders of magnitude below /u/cars10k's value.
&gt; weird. I use Intellij-Rust for my daily programming and VS-Code for debugging. I have quite the opposite experience as stated in the blog post. Note for background that Ayende comes from the .Net world and is the lead of [RavenDB](https://ravendb.net), a .net nosql db.
Maybe something like this: https://is.gd/Xb7L5r To bad it does not run on stable. It will run on Beta and Nigthly in release mode and debug mode.
Ayende's struggle is real, these frustrations are perfectly valid. Sadly, they managed to stumble upon a handful of known pain points in Rust all at once: 1. Lexical borrowck: Niko talks about precisely this issue on [his blog](http://smallcultfollowing.com/babysteps/blog/2016/04/27/non-lexical-lifetimes-introduction/). 2. `entry` would be less efficient since it requires always cloning (which would be fixed by [RFC 1769](https://github.com/rust-lang/rfcs/pull/1769) 3. `entry` would require an explicit match on `VacantEntry` due to `lookup_host` returning a `Result` preventing its use in `or_insert_with` (this issue is common in the sense of `?` error-handling breaking in the face of closure-oriented APIs). 4. Needing unstable API-s to solve what seems like an easy problem (like DNS lookup). I think dismissing these complaints would be irresponsible. Furthermore, I think I'd also give up (at least temporarily) on learning a language if I got unlucky and managed to stumble on a lot of its issues in quick succession like that. So I have a lot of empathy for Ayende and I can't wait for us to solve all these issues this year.
So nice to hear news about the project
Pretty sure the creator said that he was sorry for be dishonest or smth like that.
Thanks for all the suggestions! I am not sure why, but while using zsh on my notebook the output is really slow. After trying with default bash it's super fast (in release build. still really slow in debug, wonder why the difference is so big because it's "just" printing) 
Ok, so, question: is giving up not a common experience for programmers? I put rust aside on three or four occasions before finally sticking with it for more than a few days. Did the same thing with C# when I was first learning to code--and I have done the same with every other language I have learned since then, too. Is that not a normal part of the learning process? Sorry for being completely off topic. &gt;.&gt;
This was last week, but I spent an evening trying out tokio_core by [writing an "advanced" echo server](https://github.com/justinas/tokio-dabblings/tree/master/shutdown) than can handle special "shutdown" messages by, well, shutting down. The one issue in my code I'm aware of is that I am throwing away lots of errors (turning them into `()`) to consolidate types of different futures, but introducing my own error type with `From::from` implementations should solve that. Still, I would be grateful for any feedback, as I am still wrapping my head around the tokio ecosystem.
Thanks - exactly what I needed to know. The maintainer is a noble person, keeping a deprecated crate alive like that. So the 4:1 ratio in time/chrono downloads probably comes from dependencies on older crates. Which shows that just looking at download numbers can be misleading. Edit: and the documentation is better!
Some people like to go to a new thing and be able to understand it after a just a little work. Right now Rust requires many people to put in a lot of work. Personally, I don't mind working at something till I get it if I see an advantage to knowing it, but some people don't have the time or patience.
That didn't happen. 
[Already posted](https://www.reddit.com/r/rust/comments/5pvh44/ayendes_struggle_with_rust/) three hours ago.
As with all software, it depends. In my benchmarks, it's the fastest one out there. There are certain issues with some graphics drivers that make it slower. Also, as it does not have scrolling, if you run tmux on top, it's slower than the more performant terminals like urxvt, but still faster than for eg Terminator. It has plenty of bugs though.
This whole blog post deserve a "quote of the week"
Whoops, I'll delete it then. The other submission used a different url.
Try locking stdout around your loop: https://doc.rust-lang.org/stable/std/io/fn.stdout.html println! locks implicitly every time you use it.
Avoiding fundamentals of Rust like aliasing mutable references there are a lot of things Rust just doesn't do without nightly feature flags * Let you specify the alignment of a buffer (non-trivially important when working with Atomics). * Doesn't allow platform specific extension features (RTM, SSE, AVX, etc). * Doesn't allow in line assembly. This has nothing to do with *a new paradigm of systems programming*. Those are fundamental tools systems devs are used to having access to. Nightly isn't a buggy messy the testing infrastructure is very nice so working on nightly isn't the end of the world. The things I pointed our aren't priorities so it is unlikely they'll change soon anyways so you aren't trying to *hit a moving target*. My point is not having these tools in *stable* carries a stigma nonetheless. 
I learned something from this. Thanks for sharing!
Isn't the story with C/C++ even worst? There's no crates.io to help you out there.
It seems when it comes to vendoring Rust is being judged by Ruby/Python standards. Maybe it's good :)
The comparison I'm making in inherently unfair I'll admit that. Either Rust must have the package management of Python/Ruby (which is what? 10 years mature?) OR Rust needs the long term stability and eco-system building C/C++ has had the past 50/30 (respective) years to build.
Good luck! Another nice thing about this approach is that it might improve your consumer's performance. Right now I imagine to prevent the consumer from seeing partially written stuff, your updater is holding a lock on the hash map the entire time it's doing its thing, including IO, which might take a while. With this approach, you don't need to do that. You need an exclusive lock only within commit; you can acquire and release shared locks freely otherwise.
When I was a young teenager, I wanted to learn C since the BASIC I'd been using was too slow and C was what "real" programmers used. I found the one book on C at my local Waldenbooks. This was before the web. I tried getting a simple "Hello world" working, but got tripped up by all the punctuation C required compared to BASIC. I think the problem was something like: printf("Hello, World!";) It may have been an actual misprint in the book. Either way, I had some compile error—compile errors were a pretty new concept to too, coming from BASIC—that I didn't understand and I couldn't even run the program to try to debug what was going wrong. I didn't know a single person who knew C, I had the only book my bookstore had already, and there wasn't anything like the web yet. So... I gave up for like two years.
Fwiw I think the fact that &amp;str isn't just &amp;String is a big mistake (or Box&lt;str&gt; instead of String)
That didn't [**exactly**](https://github.com/jwilm/alacritty/issues/289) happen. But the claim that Alacritty is the fastest in practical benchmarks (dumping unformatted text doesn't count), is disputed to say the least.
`std::string_view` is coming. I expect that to be fully equivalent to `&amp;str`.
Giving a function different name then the original one as declared in the C code. For example, Ruby FFI module has this: `attach_function :ffi_function, :function, [:string], :pointer` , where `function` is the original function name, and `ffi_function` an alias you can reference later in the code.
The first point is the most annoying to work with Rust I think. Would be nice some focus to solve this in 2017.
I believe `&amp;str` is more general; it can refer to things that looks like bits of a `String`, such as static strings in the program data segment, which aren't actually a `String` because they aren't backed by dynamic memory. It's the exact same as `&amp;[T]` vs. `Vec&lt;T&gt;` The first one points to some contiguous memory somewhere with a length known at runtime, the second points to a particular structure that is initialized and managed in a certain way (and can be resized, truncated, etc) that happens to *include* some contiguous memory somewhere with a length known at runtime.
If you just have a value I think it's simpler to `.or_insert(value)`?
It needs selling up front: Here are all the advantages of Rust, but here is the cost you need to pay in terms of the learning process. Then they make their own choice. Personally if the boss didn't give me a Rust project I was considering asking for 3 months off work to really get into it. Fortunately I now have a Rust project, but unfortunately still a stack of other work to get through.
I've given Rust a shot about 3 times now, one year apart. I've given up all 3 times now and I'll say the nail in that coffin has been these precise borrow checker deficiencies. This is very frustrating, mostly because I believe in the language and I like a lot of features of the language including the idea of the borrow checker. In my day-to-day in the mean time I've worked with various projects using C, C++ (98, 11, 14), Go, Python, Scala, Java and a few other ones occasionally. I know a fair amount of folks who are more capable software engineers / comp-sci people them myself, but I'm not entirely a dummy. So I've decided after 3 tries that I'm not going to picking up Rust again till I feel like this has been addressed (or been sufficient progress). I'm not very optimistic that this will be solved anytime soon... mostly since I'm not the only person who's been running into for 3+ years. There's lot of issues &amp; RFC in github over the years that get closed and reopened as new ones, but not really traction. 
Because they are two entirely different things. A `String` is basically a special structure that keeps a reference to a heap-allocated `Box&lt;str&gt;` internally. The structure has methods for modifying it's internal `str` data, and may completely replace this data when the `str` is too small and new `str` needs to be created to replace it. The `str` type, on the other hand, is basically a UTF-8 character array with characters of varying sizes, which may be allocated on the stack, as is the case with `&amp;'static str` or in the heap if it is being sourced from a `String`. It cannot grow because it's a fixed-size structure in memory. When you reference a `String`, you typically get a `&amp;str` back because that's all you need. You want direct access to the UTF-8 data, and not a reference to data (`&amp;String`) which contains a reference to the data (`Box&lt;str&gt;`) you actually want. The only time you borrow a `String` is when doing a mutable borrow (`&amp;mut String`) as you need to have access to modifying the string. You cannot, on the other hand, modify a `str` directly via a theoretical (`&amp;mut str`) because that would be entirely unsafe. If you tried to overwrite a character with another character that had multiple code points, you'd overwrite several other characters or run out of space. You need the `String` type in order to properly handle such actions safely.
I wonder if there is room to learn from the approach [Kotlin is taking](https://github.com/Kotlin/kotlin-coroutines/blob/master/kotlin-coroutines-informal.md). They implement a coroutine primitive rather than generators or async and then have libraries that add those on top. The coroutines are also built on top of Java futures so that would seem to be a good fit for where we're going already.
When you understand why it really makes sense though. borrock can't / really shouldn't make decisions based on implementation details of a function. It would really suck if changing the definition of a function without changing its signature could break code that uses it. So when you really need something like this, I'd suggest creating a separate accessor function like `split_at_mut` that returns mutable references to all the fields you need in a tuple.
&gt; VS Code could build and run (but not debug) The [LLDB Debugger](https://marketplace.visualstudio.com/items?itemName=vadimcn.vscode-lldb) package works really well for me. Maybe they didn't find it, or didn't set the `sourceLanguages` option as explained in the package description?
I'm not familiar with Kotlin, but it sounds like exactly the same approach /u/gornishanov is pursuing for C/C++. I haven't yet read the two RFCs, but I certainly hope this approach was (or will be) taken into consideration because: 1) there will be support in LLVM (based on the last presentation it seems pretty soon) 2) you could build whatever proposed safe abstraction on top of unsafe, but "relatively" simple, feature built into the language
You can certainly accomplish that in Rust, but you have to understand what you're actually asking. In a dynamic language like Ruby, you *need* a wrapper which acts as an adapter between how function-calling works in Ruby and how function-calling works in C. `ffi_function`just lets you pick the name of that wrapper. In a language like Rust or C++, the wrapper is optional because they can compile to C-compatible machine code. (ie. You don't need a wrapper for a C program to call a C library) However, you can certainly create a wrapper function if you want and the compiler should optimize it away. Given how concise Rust's function syntax is, it'll probably be about the same amount of typing as Ruby's approach.
There has been active work on this, the problem is that there is a long tail of compiler refractors that need to work up to it. A major piece landed last year, now it is just blocked on MIR-borrowck I think. Note that not all of these "deficiencies" are deficiencies, some are by design: http://manishearth.github.io/blog/2015/05/17/the-problem-with-shared-mutability/
To be fair, C++ doesn't have standardized/stable inline asm either. It just lets you use it on the stable compiler.
Try this? extern { #[link_name = "symbol_name_foo"] fn rust_name_foo(); } https://doc.rust-lang.org/reference.html#ffi-attributes
Indeed. I specified `or_insert_with` because “a `get_or_insert` method that took a closure” was what was asked for.
The new book's ownership and borrowing chapter goes into this in fairly extreme detail: http://rust-lang.github.io/book/ch04-00-understanding-ownership.html
Last time I checked the Visual Studio plugin required downloading the source code, having the VS Plugins SDK installed and compile it. Only Windows devs really interested into trying out Rust would bother to go that far, instead of just installing a plugin via the extensions gallery.
Yeah, that is indeed sad and I’ve been longing for RFC 1769 to be accepted and land, because then the whole lookup method would be four indubitably more efficient lines: fn lookup(&amp;mut self, host: &amp;str) -&gt; io::Result&lt;&amp;[SocketAddr]&gt; { match self.cache.entry(host) { Entry::Vacant(entry) =&gt; entry.insert(net::lookup_host(host)?.collect()), Entry::Occupied(entry) =&gt; Ok(&amp;entry.get()), } }
&gt; To bad it does not run on stable. There is nothing preventing it to run on stable. I could have crafted types for the current stable but then it wouldn't work on beta and nightly because something in type id calculation has changed recently. Your code will work on next stable.
&gt; downloading the source code Rustup handles this transparently for you these days. $ rustup component add rust-src one time and you're good.
Chris is a smart guy, but I read the bits relating to Rust and I just can't find myself in agreement. &gt; I think that Swift is more interesting than Rust in some of the higher-level application demands, but we'll see. Specifically, what are two ways that it is more interesting? &gt; Unlike Rust, we can't make that be a core part of the type system that everybody has to use. It has to be something that sufficiently smart programmers, when they're solving a specific performance problem, end up using, or an embedded-kernel programmer might want to use, but an application developer can completely ignore. That’s really the challenge: it’s deploying similar type-system mechanics and other low-level language geekery to the problem, but do so in a way that is tasteful and allows most normal people to just ignore it. So, Rust's memory management model is untasteful and only abnormal people would want to use it? Rust allows you to invoke ARC when desired, but for most things, it's just unnecessary. I would rather default to the faster, better option than the slower one. I just don't understand why anyone would willingly choose to use Swift over Rust. For iOS and macOS development, there's no competition, simply because of the first-party support in terms of libraries and tutorials, but why would you develop a server application or systems application in Swift, unless you were just reusing your existing Swift/ObjC core business logic? I think Swift is certainly one of the best languages on the market, but I wouldn't personally want to compare it to Rust if I were Chris Lattner, because the advantages Swift has over Rust are so hard to enumerate, where the reverse is relatively simple.
Thanks to nrc and jseyfried for reviewing this post :)
&gt; perf [...] can both do system-wide profiling which is useful when you’re optimizing around system calls. Another thing to keep in the back of one's mind is that perf can also profile a single program, with much lower overhead than valgrind.
You're making political and social arguments. I'm talking from a technical viewpoint. The reality is that developers are more likely to use Python than Swift for all of the same reasons. - Python has been around for decades, it is reasonable to expect it will never be abandoned or disappear. - Python has some amazing IDE support and tooling, developed over decades of real world experience. - Python is perhaps the most well-known language after C or Java - Python is immensely popular - Python is extremely productive while being very safe The reality is that the correlation between safety and productivity is not well defined. Many Rustaceans would claim that they are _more_ productive because a lot of their problems are found at compile time, rather than later on in production, rushing around like chickens with their heads cut off to try to fix them. Rust was arguably started in partnership between Samsung and Mozilla to create a platform for producing a high-performance next-generation browser engine that would better utilize the many-core architectures of Samsung's processors. They're both large sponsors. Mozilla's core product is Firefox, and it has existed continuously for a very long time. No one worries about that being abandoned, and Rust is being made integral to their core product through Stylo and others. You can _certainly_ make arguments like the ones you made, but it doesn't mean that I have to agree with them.
I understand why we have a different type, my point is that the language could have handled that sort of detail under the hood, similarly to how it handles the vtable and size for trait objects. The fact that it didn't leads to some very key types feeling like they operate under "different rules" than everything else. Another possible tradeoff could be not being able to assume that `String` and `Vec` are always growable, and panic if you try to grow a stack allocated or static value. I'm not saying that it's necessarily the right tradeoff to make, but am mostly just contesting the idea that they had to be different types. You could also flip it around, and have methods that might require reallocating only be available on `Box&lt;str&gt;` and `Box&lt;[T]&gt;`, which would have different tradeoffs (I think the language may have worked that way in the past if I'm not mistaken?)
So you can think of `&amp;str` as `&amp;[u8]` and `String` as `Vec&lt;u8&gt;`, the same differences apply to both. `Vec` is growable, and consists of a pointer, a length, and a capacity. Slice is just a pointer and length as it cannot grow.
Swift is easier to pick up - there is no borrow checker and it is more similar to popular languages (e.g. there are classes).
Ha! :) As far as I know, you're correct: Derive can't have any arguments.
That makes sense. What do you mean by: &gt; I think the fact that &amp;str isn't just &amp;String is a big mistake ? All the things that currently take `&amp;str` should instead take `&amp;String`s? I suppose that means to access the actual `u8` would be two pointer indirections...
&gt; Specifically, what are two ways that it is more interesting? Deals with strings in a more high-level form with explicit defaults relating to grapheme clusters. Built from the ground up to bind to OSX UI stuff. In general _being designed for a primary purpose_ will always have an advantage with that primary purpose. Swift can make choices biased towards that purpose that are harder to make in Rust as they would disadvantage other use cases. A lot of UI toolkits are based on heaps of shared references. This has always made them challenging in Rust. ARC helps. &gt; So, Rust's memory management model is untasteful and only abnormal people would want to use it? If I am programming an application which needs lots of shared references, yes, Rust is annoying to use. If I don't really care about lack of GC, yes, Rust is annoying to use. "normal" depends on the community you're dealing with, and thinking about memory is indeed "abnormal" in Swift, which is mostly an application development language. &gt; Rust allows you to invoke ARC when desired, No, it allows you to invoke Arc. ARC is "Automatic Reference Counting" (not Atomic RC), which means the compiler inserts the clone calls. Rust does not have this, nor will it. &gt; but why would you develop a server application or systems application in Swift, but that's not what clattner is talking about. "Application developer" is "user-facing application" dev in this context. ---------- If I had more experience with swift and actually had to write application-level code I'd probably choose it over Rust often. I like having a GC around.
I'm saying that I think the language would be more accessible if it were designed so that `str` and `String` were not separate types. Either by replacing `&amp;str` with `&amp;String` or replacing `String` with `Box&lt;str&gt;`. &gt; I suppose that means to access the actual u8 would be two pointer indirections... I'm not talking about how it's implemented today. I would expect the language to take care of that sort of detail in this hypothetical
&gt; So, Rust's memory management model is untasteful and only abnormal people would want to use it? I think this characterisation is needlessly confrontational. Swift is geared towards application development and choosing a memory model that is easier to learn is sensible. I really think that borrow checking with optional reference counting is better in many ways than throwing ARC at everything, but we pay with a loss in ergonomics and a steep learning curve.
I [wrote](https://llogiq.github.io/2015/07/15/profiling.html) about perf a while ago. /shameless plug
Except for the explicitly checked life times.
Yeah, a lot of older crates depend on it because it was originally part of the standard library but got split out.
Yes, but he wasn't comparing with Rust in that context, and that stuff is about _future_ Swift (he himself basically admits it's not there yet, which is true), so it's not fair to compare with current swift.
Before 1.0, `String` and `Vec&lt;u8&gt;` used to be `~str` and `~[u8]`, where the `~` sigil was a language built-in version of `Box`. This meant capacity and growth were built into the language rather than the standard library.
Could you link to the [`syn`](https://github.com/dtolnay/syn) and [`quote`](https://github.com/dtolnay/quote) github repos instead of crates.io? Those are a more useful landing page - explanation, usage examples etc. rather than what crates.io has which is version number and download count and list of dependencies.
Holy crap, that is a huge improvement since the last time I read through that chapter.
For me it was the same in the beginning as I tried writing programs the same way I write C or modeling stuff using the same heavily OOP design patterns I use when writing C++/Java. And, naturally, it was a struggle. I guess a lot of people (myself too) come to Rust expecting it to be a more modern C/C++ instead of being Rust. After a while it seemed to just "click" on me and now I find myself struggling less and less with each new line of code I write. This could, in part, be an unfortunate byproduct of Rust being hailed as a better C/C++ (even though it doesn't advertise itself that way in any of the official docs) on various blog posts and Reddit discussions. 
Sorry, haven't written much rust recently and having a bit of trouble visualizing how your suggested `split_at_mut` would look. Would you mind sharing a simple example?
This has been discussed
Definitely a good point. Profiling a program with Valgrind can be impractically slow in some cases. In particular, interactive programs like games may be better off with perf.
I did this not too long ago for hyper and found a few quick fixes to improve things pretty significantly at the time. Great and quick read on how to effectively use valgrind with rust :-)
I was talking about VS plugin. https://github.com/PistonDevelopers/VisualRust Last binary release was several months ago. https://marketplace.visualstudio.com/items?itemName=vosen.VisualRust 
Instead of `HashMap&lt;Box&lt;String&gt;, Vec&lt;std::net::SocketAddr&gt;&gt;` could you use `HashMap&lt;&amp;str, Vec&lt;std::net::SocketAddr&gt;&gt;`? 
I blogged about this some time ago: https://casualx.github.io/2016-10-18/abstract-ffi-callback-interfaces/#callbacks-without-context With value generics it could look like this (the same technique works in C++): fn foo&lt;f: fn()&gt;(...) { f(); } In hindsight it seems obvious that a trait could work (perhaps `StaticFn`) but this is kind of exactly the thing value generics (can) do. You can even hide the ergonomic loss in a macro (which can be argued has a similar ergonomic loss). 
In fact I would go so far as to say that I would never use valgrind profiler. In my experience it's results are inaccurate. In many cases it points at incorrect hotspots and in many cases it's to slow to be useful at profiling. For example, I was trying to profile a dataflow kind of workflow that ran for about 45 seconds, with valgrind profiler this took over 10 of minutes. You should just use perf. Not only is it pretty damn accurate where time is spent, but has a lots of options for tracking down an issue. With perf you can tell if you're spending a lot of time in your app or in the kernel (like a syscall, an fslock, or mmap sem) but also investigate further why (look at cache, branch prediction, kernel events). Don't waste your time with valgrind's profiler mode. 
See the @SimonSapin's answer.
Hmm, I usually manually downloaded the source but I discovered the rustup method was possible by way of a random comment to a post somewhere, but then it took quite some googling to find the actual command. After that copious amounts of googling failed to tell me where I could find the source on my hard drive (Linux yes, windows no). I finally found the src package with "attrib /s stage0.txt". It might be documented in more places, like racer install instructions, by now. It's quite new I believe.
It's a relatively recent addition.
The problem he ran into is annoying, I've been there and probably many of us have too so I really hope non-lexical lifetimes will solve some of these problems. Many times faced with this problem I've come up with alternative solutions which I liked better, but sometimes I don't at which point I work around the problem and hope the borrowck will get smarter in the future. Rust forces you to think different and when you can't come up with a good solution it's frustrating so then you can decide to swallow it and hope for better days or give up I guess.
done!
A lot of real issues, but I feel like they're being a little unfair on such a young languages. Go doesn't even have an official debugger yet. C++ has had *decades* for IDEs to be developed. C++'s stdlib only very recently has any kind of 'batteries' at all (e.g. regex), and finding third party packages is totally left up to Google. I think a problem with advertising Rust is that no-one appreciates before they start that you *can't write programs in the same way as C++*. I read about the borrow checker and thought "ok fine I understand", but it doesn't really hit you that you'll have to fundamentally structure your program in a different way than you might in C++ to make Rust happy. Also I think the docs should be much more up-front about the unexpected limitations of the borrow checker. You know, stuff where you think "but obviously it's fine?" but the borrow checker is just too stupid - non-lexical borrows, stuff like `foo.set(foo.get()+1)` and so on. Ask anyone who has *only* read the docs whether that code will work and I guarantee they'll say "yeess?? won't it?". Finally, I'd like to see his C++ code! Curious if it has any bugs.
You're conflating a few issues. `str` is just `[u8]` that's guaranteed to be valid utf-8, [`String` is just `Vec&lt;u8&gt;`](https://doc.rust-lang.org/src/collections/up/src/libcollections/string.rs.html#262-264) that's guaranteed to be valid utf-8. Everything else you're discussing is just what falls out from orthogonal language features. `&amp;`, `mut`, `Box`, etc. have nothing to do with Strings explicitly, they're just language features that apply with uniform semantics to any data type, and str/String are data types.
The `Box` bound to `y` is mutable, but the binding at `y` isn't. That does not however preclude moving the box to a new mutable binding. Note also that you can have an immutable binding to a mutable reference and mutate the deref, as in let bar = &amp;mut foo; *bar += 1;
`Vec&lt;u8&gt;` does not enforce valid UTF-8, but `String` does. The in-memory representation is the same, though.
By default, rustdoc produces documentation for users of your library, not internal stuff. So yeah, this isn't going to be documented. I believe that there's a way to do this but I always forget what it is. In general, rustdoc needs a lot of documentation...
Sorry Steve, I found this confusing. I get that macro_rules is what we have today. I get that syntax extensions are never going to stabilize. I get that derive only procedural macros is macros 1.1 which will be coming very soon. But I already knew these facts. Macros 2.0, declarative macros, macros by example, full procedural macros, and macros 1.2 are still confusing in terms of what they are supposed to be. I guess what I want to know most is what is the difference between the macros 2.0 effort (declarative and by example, I think? Is there a difference between these in anything besides name?) and full procedural macros (should this be considered macros 1.5?). And also: Are declarative macros intended to completely supercede procedural macros and macro rules or will there eventually be three (more?) subsystems for writing macros that the Rust language supports and encourages?
If you advertise a language as a safe, modern systems language how could it not be compared to it
Actually it hasn't. You have to distinguish between *values* (which are always mutable), *references* and *bindings* (which both come in mutable and immutable flavors). For another example, consider: let x = 0; let mut y = &amp;x; // y is mutable, *y not
I'm talking about String's memory representation b/c the original point that you were responding to was about String's memory representation - "reference to a heap-allocated Box&lt;str&gt;". So, I apologize, you weren't equivocating, you were slightly changing the subject by blurring together two layers of abstraction.
I believe that's all correct, yes! &gt; what are the three procedural macros? * custom derive `#[derive(Foo)]` (this is 1.1) * attribute-like `#[whatever]` and `#[whatever = foo]` etc * function-like `foo!` These are all procedural macros, but the way that you declare and use them are slightly different in each case. Fundamentally, they're all functions of `TokenStream -&gt; TokenStream`, basically. 1.2, if it happens, would be a not-fully-featured version of the other two.
Yes, but the reason it was is because there _was_ GC at one time, but Apple didn't find it to be good. https://news.ycombinator.com/item?id=9085563 is some discussion, that link seems to no longer exist though.
Not trying to start a flame war here, but C is neither safe (by any stretch of the imagination, it happily lets you shoot your feet any way you want), nor is it particularly modern (it's a 45-year old language). Edit: I guess the key word here is "compared". Comparing Rust to those languages doesn't implicitly mean it can/should be coded in the exact same way.
Heck one of the most popular x64 C++ compilers doesn't even *have* inline assembly ;)
They are, but I don't think they can all really be corrected. For example, Rust's weak spot is graph data structures. Trying to dive in an make a graph data structure (A trie) is just asking for trouble. You fight borrowck because this is exactly the sort of data structure that can very easily lead to use after free and double free bugs. I don't think rust can truly safely make those data structures safe. I don't know how rust resolves this with C programmers. They want all the unsafety that they are use to. But the language doesn't allow it. Perhaps improving borrowck will make the friction decrease, but you can't totally eliminate it without getting rid of some of the big guarantees of the language.
This is desired but much trickier than it sounds. npm is delegating just this to a microservice, for example, and GitHub Flavored Markdown is a giant pain.
There's a lot to be excited about in this upcoming release--the obvious one for most people is custom derive stabilizing. In addition, I'm very excited about [`std::process::Command::before_exec`](https://doc.rust-lang.org/std/os/unix/process/trait.CommandExt.html#tymethod.before_exec) landing. This resulted in a [significant cleanup](https://github.com/jwilm/alacritty/commit/6e51e9e27b384e68bda85f1a35f5c9219bb49400) for Alacritty's child process setup.
Yes. The support of that relies on your machines support for pref cache event. But since perf originally supported mostly hardware events (PMU events). They added other ones later (including tracepoints). But on a modern machine or VM (with PMU passthrough) you should be able to use it. So what you do instead of using the default event (which usually is usually a timer or a instruction count). Here's some resources: * http://stackoverflow.com/questions/12601474/what-are-perf-cache-events-meaning Good stackoverflow article with answer on different cache events * https://developers.redhat.com/blog/2014/03/10/determining-whether-an-application-has-poor-cache-performance-2/ Redhat article, that shows you how to get cache misses along with source annotation. If you built your application with debug symbols, you should be able to see the events by line number of our source langage (C, C++, Rust, whatever). * http://stackoverflow.com/questions/17223481/open-perf-data-in-kcachegrind If you've only ever used the cachegrind via the KDE UI, there's ways of loading the perf data into the tool too. * http://www.brendangregg.com/perf.html Just a great overview of perf with lots of example commands you can use to get going. The upside of this is that you'll be measuring real cache events not simulated ones. 
Still early stages but I'd love feedback. The idea is a "configurable" configuration system that can be used to easily consume configuration in any feasible arrangement. Features: - Local configuration files in TOML format - Loosely-typed: Configuration values are automatically converted to the requested type - Automatic overriding from (optionally prefixed) environment variables Setup as in node-config: https://github.com/lorenwest/node-config config::merge(config::File::with_name("default").path("config")).unwrap(); config::merge(config::File::with_name(env::var("RUN_MODE").unwrap_or("development")) .path("config") .required(false) ).unwrap(); Setup for Rocket: https://rocket.rs/ config::merge(config::File::with_name("Rocket").required(false)).unwrap(); ------ Roadmap: - Add `config::Etcd` / `config::Consul` to support merging configuration from remotes - Support JSON, YAML, libconfig, "dotenv" file formats (controlled via feature flags) - Support de-serializing chunks of the configuration into structs via serde 
I love all your posts Steve. Big fan! Thanks for this intro. I understand macros a lot better now.
Mostly same here. `perf` is excellent, and ever since it got support for Rust name mangling, it's a much nicer experience. I do occasionally still use valgrind if I can stand it, but only because I find `kcachegrind` to be a much nicer experience than the `perf report` tool. I've never had success viewing `perf` output in `kcachegrind`. (The SO link in your child comment isn't very promising. :P) The various abilities of perf to track, say, L3 cache misses were really critical for fixing some tight loops.
I think String was StrBuf or StringBuf for a very short period of time two years ago.
It only supports TOML right now. I guess I didn't make that clear. The Roadmap section is in addition to. I fixed my top comment; thank you.
it would be nice to have the current crate fully documented and all dependencies documented from a user's perspective. Maybe one day. 
yep, fixed. It is a bug if it doesn't get documented.
Not OP, but I think they're referring to this: https://youtu.be/pTQxHIzGqFI?t=994
Rust has a different design from the ground up. It's based on orthogonal features that are combined to give the same result as a C++ program. Otherwise, Rust will be as complex and unapproachable as C++ in the end. This means that Rust has a smaller amount of features than C++ and they're less complex. Unfortunately, there was very little done to make this approach user-friendly. That's because the focus has been to let programmers do things in the first place (if it takes some weird code to do, at least you did it). Then the focus shifted to let programmers do those things on the stable compiler, and that's where we are at today. When more things stabilize, the Rust team can put more effort into usability concerns (even though they're doing it in parallel now)
Yeah, I'd love to get rid of `.into_node()`-- that's one of my short-term ergonomics goals. However, it requires a nasty hack on top of the existing nasty hack that is `DomNodes`, and I'm worried it could screw up inference or at the very least make the generics in the API harder to read (not that they're not already a bit of a struggle to get through).
What kind of content are you looking to be able to paint? Do you need more than just text?
You can certainly get *some* of the same error checking, and I believe there are projects that already offer this, but I don't think it could catch everything that Rust does (I'm not qualified enough to give a definite answer). And some of Rust's useful zero-overhead features are dependent on the strict memory safety guarantees.
I'm trying to implement a form of caching and am having difficulties with the HashMap API and mutable borrows. I have two HashMaps, one which contains the owned byte vector, the other contains the owned value parsed into different types (starting with String). This is what I've been able to boil down to without mutable borrow issues, however it requires that I must insert a value into the strings map which I don't want to do. Any suggestions? // this caching structure pub struct ElementContainer { elements: HashMap&lt;u32, DicomElement&gt;, strings: HashMap&lt;u32, String&gt;, } // get value if already parsed, otherwise parse from bytes pub fn get_string(&amp;mut self, tag: u32, cs: EncodingRef) -&gt; Result&lt;&amp;String, Error&gt; { let elem: &amp;DicomElement = self.elements.get(&amp;tag) .ok_or(Error::new(ErrorKind::InvalidData, format!("Element does not exist: {}", tag)))?; Ok(self.strings.entry(tag).or_insert_with(|| { if let Ok(value) = elem.parse_string(cs) { value } else { String::new() } })) } 
Is `**y` mutable then?
This is one of the differences between 1.1 and 2.0; 1.1 only gives you the string. 2.0 will give you the tokens. Since this is only intended for custom derive, there's really no need for the exact spot, IMHO.
That matches up with my vague memories, yes!
Great! Alas! So little time.
Lack of move constructors makes more efficient code in some cases.
This is a great post! Thanks for breaking down the current state of the world, and what's to come!
Thanks, this is great.
Check the types. `y` here is a `&amp;i32`, `*y` is an `i32` and `**y` is an error because `i32` can't be dereferenced.
Well, you know, for me personally it is part of the process of learning anything--all g a break gives that information time to go from short term to long term memory, or something like that. Like you, I am baffled by people who are unwilling to get back on the horse, so to speak. But, then, I have been called a cowboy coder from time to time. Maybe that's related.
I was working on windows, but hope it can run in Linux. there is a launcher process which run as a daemon/service and another cli command which run as user process. when cli process start, it will send some message to launcher process and exit. the launcher process is expected to spawn a new process based on message from cli process, and this process need to run as same user as cli. that is my intent.
[Hjson](https://github.com/hjson/hjson-rust) is close to HOCON in syntax, though it has less features.
Nice. The features can be added on top and do not need to be in the same lib. Excuse me, now I have a project...
Cool stuff, I'll have to try it out this weekend! BTW, how good is the emscripten in terms of performance and size of generated code?
Absolutely. There appears to be a common pattern: smart, experienced people tackle something _hard_ as their first Rust project. Except they don't think about it as particularly hard, since they know how to do it with X (previous high-level language). Experienced people aren't afraid of 'hard' (that's how they got to be experienced) but they get frustrated when everything they used to do without thinking hits a wall. It's natural to want to have multiple mutable references, it's bothersome to have to hunt down a suitable crate for something that X has in its stdlib. And somehow they all start with tree-like data structures which are a little ... awkward to do, unless you know the Rc&lt;RefCell&gt; dance (and _why_). Even smart people have to respect that learning curve, and can't just jump straight to black belt because they knew another martial art. It's tough because smart people have lots of energy but little time.
A good example where Swift and Rust are well suited and work together: xi-editor is Swift, xi-core is Rust. :-)
Neat. I don't think I'll get to this for a bit as I'd like to solidify more standard features but I would accept a PR to add support for this format through feature flags (currently just TOML is supported but I'll soon be adding at least JSON).
Hooray! This is very pleasing. :-)
I haven't run into any performance issues until trying it on pages with many times more nodes than any reasonable website. See more info [here](https://www.reddit.com/r/rust/comments/5oilh2/introducing_domafic_safe_highperformance/dcjxexp/). Uncompressed debug builds are pretty big, but the release build for asm.js is 162KB after being minified and gzipped.
I guess this is like f# vs OCaml , Swift is like a corporate Rust , Some of us that's a non starter ,it's not even for political reasons ,not many people can afford apple hardware.
These recent "struggling with Rust" posts frustrate me. Most of the pain-points are IMO not valid, and the Rust language/ecosystem should not change based on them. It seems that it's just people trying to hack something in 2 hours without properly RTFM and then say Rust is unusable. Things are just done differently in Rust and it has a pronounced learning curve. I think we shouldn't fatten up the stdlib, make "Hello" + "world" work etc. just because of 5 people on the internet that won't read the Rust book and understand it.
/u/steveklabnik1's [answer](https://www.reddit.com/r/rust/comments/5q069i/how_would_you_make_an_array_containing_different/dcv80ft/?utm_content=permalink&amp;utm_medium=front&amp;utm_source=reddit&amp;utm_name=rust) is of course correct, but if you actually do know the exhaustive list of types that each cell could contain, then you might also want to consider using an `enum`. Rust's `enum`s implement a typesafe tagged union. There's more information about them here: https://doc.rust-lang.org/book/enums.html The difference in your specific use case would be: - With an array of Box&lt;Tile&gt;, the array contains pointers to trait objects, and the values themselves are stored in _N_ different locations on the heap. - With an array of some `enum` type, each element in the array takes up as much space as the largest type in your `enum`, and is stored inline. Whether or not using `enum`s here is a good idea will depend entirely on your use case. E.g. if all your Tile implementations are quite small types, and cache coherency matters a lot to you (e.g. you're doing a lot of tight loops over the grid) then it might be worth looking into. Sorry to dump this unsolicited information on you in response to a fairly simple question -- I just wanted to highlight the existence of an alternative approach in case it might actually be a better fit for what you're trying to do. :)
&gt; one example caused the concept of higher-order functions to snap into place in my head. Do you remember what that was?
This library is an alternative to [rust-protobuf](https://github.com/stepancheg/rust-protobuf). It is only few days old and many pieces are missing. But the performance is decent and I believe the code is much simpler to read (in particular the generated files).
Not really. Something about a database, or maybe sports? It's been too long. I *think* it was using `map` on a callable passed into a function... whatever it was, it was the first time I realised you could use functions as values.
Some of these features make normal code really easy, but if you wind up needing to write a library around it (and thus refer to types in it, use generics, *&amp;c.*). Quite seriously, you can end up with it being trivial to write a correct *application* using something, but very difficult to maintain a *library* with it. Think about things like serde: it’s delightfully easy to use the normal way, but if your needs are more complex it can be fairly hard to get it to do what you need, potentially requiring a fairly detailed understanding of traits, generics, associated types, and a few other things. Like most things, it’s a tradeoff. Also you have to be careful with respect to marketing. I firmly believe that possessing HKT and dependent types would have been catastrophic for publicity before Rust 1.0 and would substantially have hindered adoption; now, I *think* we could get away with either of them, and the longer we wait the more likely it is that their addition won’t blow up in our face (up to a point—if we wait too long inertia stops people from using the new features and so they blow up for a different reason).
If you want to leave the original name but add an alias, you can use `use as`. In fact you can use that to alias anything, FFI or not. For example: use self::original_name as alias; 
&gt; Can you suggest a way for us to emit a better message without actually committing to a path that brings async I/O into std? Have std++ with a set of officially supported libraries would be another way. But frankly async IO is so fundamental that I don't see any reason for not supporting it in std. &gt; That a standard library is relatively small really should not be a foreign concept to C programmers As a developers we longer write the same things that were written at the time C was created. We write more complex applications and need more sophisticated std.
I was using VSCode for dabbling in Rust, which has great Rust support, but decided to stop contributing to the statistics of Electron based apps.
From a thread either here or on HN, the author of hyper made it excruciatingly clear it would never support websockets.
It goes back to I want to use VS, but would like to have up to date binary plugins, instead of having to build them myself.
I'm still looking for a TOML reader/writer library that preserves the file's formatting and can be used by cargo-edit to add/remove dependencies without messing up your `Cargo.toml` :) ([tomllib](https://github.com/joelself/tomllib) was/is an effort to do that, but it doesn't support insertion yet.) _Update:_ [More information](https://github.com/killercup/cargo-edit/issues/69)
A feature that would play well with automated deployment systems (ansible, habitat, puppet) is being able to automatically merge all the files in a directory, optionally recursively. Nginx, apache, cron, etc all support this style of config, I think the [sensu docs](https://sensuapp.org/docs/latest/reference/configuration.html) in particular explain it well. Edit: I took "feedback" and gave a feature request. I'm amped to see this! I've rolled my own several times and every time I've thought "I should really take the time to make this nice and package it as a crate", but I never have. 
&gt; I didn't know that this is right place for it. It doesn't seem that there is any push from other places to be there. `regex` was just added. Crates intended to be moved into `rust-lang` are in the nursery: https://github.com/rust-lang-nursery
I want to call a `libc` function (`waitid`) that's defined for some Unixes but not others. In my `#[cfg(...)]` guards, do I need to hardcode the platforms where `waitid` exists, or is there something I can write that means "if this function exists"? I'm hoping not to hardcode the platforms, because I'd like to get support for them automatically when other Unixes add that function someday.
Ah, I've found rfc 1242 that documents this process. I think it doesn't have the visibility it should have.
I can agree with that. We tried to solve that problem recently by proposing a ["Rust platform"](https://aturon.github.io/blog/2016/07/27/rust-platform/), but it was not received well. (I happen to think it wasn't received well because it mentioned taking inspiration from the "Haskell platform," which is apparently hated with the burning fire of a thousand suns.)
I like the idea of CMSD server adapters
It seems to me that the audience was wrong, not the idea. People who are familiar with rust ecosystem are not the ones who need it.
One big issue was the limitations due to C semantics and mixing frameworks compiled with GC and not, usually leading to application crashes.
Is this not what you are looking for? https://github.com/alexcrichton/toml-rs
Pretty interesting recommendation to just use the good old chunked response and stream over HTTP2 instead of websockets. Furthermore, I don't think the author said that hyper won't support WS, just that hyper will return something low level if you ask for WS as .. ws is just a low-level stream, and after the connection upgrade there's not much there to support. (Okay, the websocket framing.)
`perf`takes a while to learn how to use (not unlike Rust). I would say that a big part of that is that is supports so many collections, use cases and work flows and the high level interface is not high level enough for people just starting to use it. I hope that somebody does build a simpler tool for others to use ontop of perf one day. One of the defaults of `perf report`is that it shows the callees with that had the most events. You can flip it over to caller (which is more like pprof) with the `-g` flag. Again, take a look at this comprehensive document: http://www.brendangregg.com/perf.html#CPUprofling , in the CPU Profiling section there's info on inverted call graph. Also, you might find the famegraph generation scripts that u/brendangregg put together. They are very helpful when visualizing where time was spent in top-down approach that pprof uses. 
It does not preserve the formatting, but rewrites the file on each save. cargo-edit is using this crate right now.
[Safari shipped Fetch yesterday](https://twitter.com/johnwilander/status/823962970807906304). I'd have no nervousness about using one of the Fetch polyfills that works atop XHR. There's shockingly little of importance in Fetch. A statement sure to attract ire, but I do feel it's introduction was merely a courtesy to not be unbelievably ugly &amp; a big turnoff to newcomers.
This is a great explanation, I wish I had this when I first started. I had a problem similar to OP's and it was suggested to me to use the following construct: pub const INSTR_TABLE: [&amp;'static Instruction; 256] = [ ... ]; where Instruction is a trait. I assume this works because the compiler realises that it can compute the size of the array since its contents are unmodifiable and are known at compile-time.
Enums sound like a good idea, but from what I've seen, you cant easily implement the same function for each variant, only on the enum as a whole. This is a problem for me because a monster's `update` function is a complicated function to attack/move depending on various factors, and then how to attack and where to move etc, while a barricade object's `update` function might simply do nothing, or a bomb might de increment a counter for when it explodes. Would you still recommend an enum?
A good parser generator that supports custom state/input objects, and can track line/column numbers. Existing projects such as rust-peg, Oak, and others don't support this as far as I could find. Position tracking would be necessary when you want to present errors based on input, custom state objects can be used to parse context sensitive input (e.g. Python style indentation).
I'd still recommend an enum. The pattern you want is probably something like: struct MonsterData { ... } enum Tile { Monster(MonsterData), ... } impl MonsterData { fn update(&amp;self) -&gt; MonsterData { ... } } Doing in-place update if that's to your taste and compatible with your sim :) You might investigate using an ECS like https://github.com/kvark/specs 
Pretty good!
Hm, considering the name, I wonder if rust logo design should not go in this direction: https://www.google.co.uk/search?q=steam+punk+crab&amp;espv=2&amp;biw=1920&amp;bih=953&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0ahUKEwidzMLG5t3RAhUhDcAKHWa2DvMQ_AUIBigB
I was actually sad to find no old-Rust sigils in there with that build-up. `alt` and `tag` are neat throwbacks though.
&gt; That's true but not everyone wants the same level of stablisation. Indeed, that's why nightly exists: people who don't need the strong stability of stable can opt-in to possible breakage in hard-to-stabilise or low-priority features (the stable features are still stable, in nightly). If stable was changed to serve these "weak stability" people, then people who *do* want strong stability have no option. At least, not without introducing an even stronger level that would probably have even an worse strong-stable vs. weak-stable divide, given they've both considered stable. &gt; Rust has tried to hold to the ideal that there's only one standard but the reality is is that most are running a nightly version of the compiler. The only concrete data I've ever seen, rather than just people giving anecdotes, is that more people use a stable version: https://blog.rust-lang.org/2016/06/30/State-of-Rust-Survey-2016.html#using-rust (And, if you add up the three columns with nightly, it wouldn't be much over half, if it is at all.)
Webrender renders web pages very quickly but webpages are mostly just viewport aligned rectangles. By the time web stuff gets to WebRender, it's been transformed from html/css/js into a list of things to render. A widget library could produce the same list format and get the rendering benefits without being web related at all.
Interesting. I've never seen that idea as environment variables generally are treated as a different class of configuration thing. Perhaps instead of the environment being implicit it could use the merge API (which would allow for priority). Would that work for you? I toyed with this idea already because I'd like to strongly recommend an environment prefix and that's hard with an implicit environment binding. config::merge(config::Environment::with_prefix("rust")); 
Well, sdl is still a C dependency. 
Lol, lyon uses my svg parsing library. They trying to implement an SVG render on GPU, but I'm working on the same task but for CPU. rusterize also uses GPU.
&gt; I assume this works because the compiler realises that it can compute the size of the array since its contents are unmodifiable and are known at compile-time. That's not quite correct. &gt; pub const INSTR_TABLE: [&amp;'static Instruction; 256] = [ ... ]; The part that's most relevant here is the &amp;, not the const (though the const is important too, as we'll see). &amp; means "a reference to", which is similar to a box, in that it's not the value itself, but a number which represents where to find the actual thing. Because a &amp;Instruction has a known size (it's just the number for where to find the Instruction), we can make an array of them! The rest of the stuff on this line is important, but not the core reason this works: You need to give all references a lifetime (unlike a box) so that rust knows for how long the reference is valid. that's the 'static part. We need the lifetime because while a Box "owns" the value it knows about, a reference does not. Rust needs to make sure that the reference can't stick around longer than the thing that it knows about! static' is a special lifetime that means "the lifetime of the entire program", which we can use because this declaration lives outside a function or a struct - so we know that the things we put inside the INSTR_TABLE must live for the lifetime of our entire program. So what this declaration actually does is very similar to [Box&lt;Instruction&gt;; 256], but it uses references instead of boxes!
&gt; They trying to implement an SVG render on GPU AFAIK, the tessellator is the core part of lyon, and it (as a library) doesn't use GPU. The examples do. /u/nicalsilva can correct me on that. &gt; rusterize also uses GPU It's a "A 3D Software Rasterization Library for rust.". It only uses GPU to display the rasterization result in the examples, not in the core. 
You have to hardcode it. Of course, you can be liberal about it and then your code will not compile on the platforms you have omitted, if that's your use case. I think the rust "scenarios" work may improve this.
Ah, that makes sense. Thanks for the informations. I don't feel comfortable enough with rust to make my own widget library but if someone start a project I'm ready to help :)
That comes with the caveat that it adds a lifetime to every generated struct/enum, that is more contagious than anything in the rust land.
What percentage of a transaction goes to Rust team? Who gets the money. I don't trust these types of sites at all !!
Nice! What's on the back of the t-shirts though? I don't see any photos of the back side.
What's wrong with https://github.com/aldanor/hdf5-rs ?
No, ObjC was refcounted before it got its GC. The GC was an attempt to get rid of the explicit refcounting (via retain &amp; release), but failed due to being pretty much a separate language and a conservative GC with rather poor performance that they didn't want to deploy on their mobile platforms. It created a split in the ObjC world that would probably have hurt Apple. So they got rid of it and implemented ARC instead.
I don't know the percentage offhand, but DevSwag [is a side project of Tilde](https://devswag.com/pages/about-us), which has a long history with Rust. I can assure you that they're on the up-and-up.
Thank you!
There are bindings to lmdb and rocksdb. The later is by all means a superior leveldb.
Right, `f.call()` is static dispatch when `f` is a generic type and not a `fn` ptr. I want to take _exclusively_ function items as an argument to another function, but have them still use static dispatch. What actually wound up working for my use case, but isn't quite what I asked for, is to be generic on `F: Fn(Input) -&gt; Output + Copy`. Really, though, it seems like there should be a trait for non-capturing closures and `fn` items. Can you think of any reason against it? If not, I might start putting together an RFC.
Looks great. I'm really having an overflow of t-shirts problem though :(
"is to be generic on `F: Fn(Input) -&gt; Output + Copy`" -- that's totally possible? I don't really see why you need to single out fns and non-capturing closures though.
&gt; quitting vim I don't understand this step ;)
Sorry, I think I confused the issue unnecessarily by adding that in. The `Fn... + Copy` requirement is the solution that I wound up using with _current_ Rust (that works). However, it's not what I was initially asking for. `fn` items and non-capturing closures have the nice property that they don't reference any non-static variables (other than their own input/output, obviously). Because of this, it would be possible to make `FnTrait::call` methods for them that don't take `self`.
Eh I'd prefer it if it were more minimalist. I love rust but the large crab makes it seem like a gamer t shirt. Something I am not embarrassed to wear to a bar or a casual gathering would be better
Love it!
This is amazing work, and well documented too. Thank you. I'm especially happy about the indexing API for serde_json values. Makes mocking something up quickly much easier. Also, just a quick question(s): why does serde_json deserialize to String instead of &amp;str? Also, how would deserialization work using no_std (which lacks the ability to allocate strings)?
Note that version 3 of Protocol buffers removes has field and just returns default on missing fields anyway.
If you don't want to store borrowed refs, you can just make all the lifetimes `'static`. Perhaps some type aliases could make it nicer and hide the lifetimes entirely.
From that github issue: &gt; I'd like to apologize to those of you who feel mislead about performance claims. You're right that 4 comparisons is not particularly thorough given the volume of terminal emulators on the market. FWIW, I've been doing these comparisons all through development and found Alacritty to be the fastest. &gt; &gt; Moving forward, I plan to continue marketing Alacritty as the fastest terminal emulator available and back that up with indisputable benchmarks. It seems like there's a bit of work to do on that front.
An interface/library for [Pin](https://software.intel.com/en-us/articles/pin-a-dynamic-binary-instrumentation-tool). It could be very very useful. 
Does catting a massive log file count as dumping unformatted text? If so, that's maybe the only case where I find myself wishing my terminal was faster. But yeah, saying that someone admitted to being dishonest, when they didn't (and might not've even been wrong at all), is way out of bounds. That's the kind of thing that people start repeating without checking the source. Putting "or something like that" on the end of it does not make it any different.
Thanks. Makes complete sense.
Would really love a cog or crustacean plushie for my desk at work, I remember seeing some homemade ones a while back and they were fantastic.
Coming from Java/Python (and a heap of others). Trust me, I would.
Hehe, I'm an "open vim for each session" kind of person.
I think you could return a Cow&lt;str&gt;, then only allocate if you have to convert characters in the string.
&gt; Currently serde_json does not support no_std Is this a goal for `serde`?
Exactly, that's what we would have it do if your data structure contains Cow\&lt;str&gt;. If your data structure contains &amp;str we can still deserialize it most of the time but error out if an allocation would be necessary.
This looks interesting! What are some use cases? Static dispatch with traits seems to solve performance concerns with dependency injection.
After some thinking I've come to the conclusion that what you want (a `FreeFn` or more appropriate `StaticFn`) cannot work. Let's try to work out how such a trait might be used: fn foo&lt;F: StaticFn&gt;(/* note, it CANNOT take a value of F since that is exactly what we're trying to avoid */) { F::call_static(); } Where `call_static()` is analogous to `Fn::call()`, `FnMut::call_mut()` and `FnOnce::call_once()`. But how could this be used? Something like this is nonsensical: fn callback() {} foo::&lt;callback&gt;(); `callback` is a value, not a type! Values are passed as arguments, not type parameters. The above syntax is more akin to value generics, where it could take the following syntax: fn foo&lt;f: fn()&gt;() { f(); } Instead of the constraint specifying a trait it specifies a type. EDIT: Actually `callback` can also be seen as a unique fn item type which implements the `StaticFn` trait which can then be dispatched on, invalidating my whole argument :D
Is rust officially dropping the "Fungus" branding? We seem to have gone full crustacean :D
I specifically didn't put 'shared mutability' in my list :P
The new array indexing syntax for json values looks amazing! Can't wait to try it out. 
see my update above
lol @ the edit Yes. You pass `callback` into a function that takes a parameter of type `F: StaticFn`. That function then has a type parameter (`F`) that it can use to refer to `callback` as `F::call`.
The serde_json "any value that implements Serialize" example is a bit ambiguous: &gt; Any expression can be included in the serde_json::Value that you build. This works as long as the type of the expression implements Serde's Serialize trait. ``` let value = json!({ "code": code, "success": code == 200, "payload": { features[0]: features[1] } }); ``` Obviously `features[0]` must be a string or the result cannot be valid JSON. So will this fail at run or compile time? If compile time, will it allow any `Borrow&lt;str&gt;` for keys, or only `String`?
It's useful when working with C FFI where you want to call a function in a callback but don't want to dynamically allocate. More generally, it allows you to avoid passing around, copying, and making space for an extra variable-- you can just use the type.
The language was named after the fungus, but that has never been a part of the branding.
&gt; You just pass it into a function as a parameter of type F: StaticFn, then you can just use F::call to call the function whenever you want. That's ... not what I mean. There is literally no way to name the type of a closure or function item. You can only refer to it in terms of the singleton value. You need something like `decltype(function_name)` for this to work. Assuming your StaticFn trait existed, could you give some example code that actually makes use of this? I'm pretty sure it's not possible, since without the ability to name the type as a type parameter input somewhere, you can only do things off of the value, in which case you can just call with receiver as normal.
Check out https://github.com/emoon/rust_minifb
Graydon said that that was one of the inspirations, but that it wasn't ever just one thing, basically. (and you're welcome!)
That's a spore? Say more? I thought it was a bike gear.
I have it in my OSes dependencies for some reason; I haven't figured out exactly why yet, it's a transitive one... EDIT: Okay, looks like https://crates.io/crates/x86 uses it for some reason, interesting. EDIT 2: looks like it's used in the build script https://github.com/gz/rust-x86/blob/03417d713789fe8931d51baafa0ff392569977bf/build.rs
I really thought it was supposed to be this: http://www.psmicrographs.co.uk/_assets/uploads/fungal-rust-spores--order-uredinales--80016096-l.jpg
One of the questions at the end mentions the anonymous struct rfc -- anonymous structs work like tuples so they don't cause any additional coherence issues. I think coherence wasn't as smooth in older Rust and structural types were more of a wild west in terms of being able to implement things.
ah, neat!
Sure. [Something like this.](https://is.gd/ms24sm) It seems kind of contrived in a miniature example like this, but I think it gives you an idea of why it might be beneficial. Edit: To clarify: yes, you have to have a function value _somewhere_, but you don't have to have the value available at the callsite. You only have to keep track of the value's type through `PhantomData` or similar.
Right, so why can't `Foo` carry around an instance of `F` directly? It is zero sized for fn items and closures without capture clauses. You have to pass the function's singleton value to Foo to make it work anyway, why not keep it around?
This is /r/rust, the programming language subreddit, for the game you should go at /r/playrust.
To add to this serde-xml could use some love
I'm still reading through this change set, and everything sounds really positive. There's one thing I wanted to call attention to that seems exceptionally well done. &gt; The new scheme is more concise and uses ownership to enforce a correct implementation. It is impossible to forget to call end() because you cannot get an S::Ok instance otherwise. Most people won't see this since it's for custom implementations of `Serialize`, but it's still great. This is a fantastic use of the type system.
FYI you don't even have to get it from twitter. You can download it for free [right from the home page](https://www.rust-lang.org/en-US/). ;)
lol . wrong subriddet hahaha 
A symbol has many meanings. They are spores if you want them to be.
I know we don't get along but &lt;3.
&gt; I think he's explaining why its important to have nominal types I know, I'm supplementing his answer :)
I'm looking for an XMPP crate!
Is there a way to encode at the type system that a function can only take a certain range of values? Like for example, I can specify that a function can only take a u8. But is there a way to encode at the type level that it can only take values 1,2 and 3 and nothing else? One way would be to create a custom type where the only inhabitants of the type are 1,2 and 3. Is there any other way?
Thanks for asking. I used rust-protobuf a lot at work and "it just work". This is truly a very good library. I have of course tried simplifying the generated files (small steps at a time like [here](https://github.com/stepancheg/rust-protobuf/pull/150) and [here](https://github.com/stepancheg/rust-protobuf/pull/181)). Then I entered more deeply in the code but I couldn't really understand the design decisions (the trait objects in particular, the need of MessageStatic) so I wanted to try by my own expecting to get to a point where I do not have a choice. This didn't happen so I pushed forward etc ... I really hope you do not take this personally, your library is great in many many ways. I don't really expect being able to compete with it but having done it this far, I figured out it could help some people. EDIT: to answer your question I'll definitely contribute in the future if I see some easy things to add without breaking to much things. I will continue with quick-protobuf for now as it is easier for me to try new paths (the `Cow` mentionned in this thread for instance)
I read that indeed. But then how do you know when a field must be empty (a-la NULL in databases)? I decided to keep `Option` and replace by a regular field only when the default value is explicitly mentioned, I prefer to keep things as explicit as possible (like most rust crates). I don't think oneof (which is not supported) addresses it fully either.
Well, as long as it is automatically generated, it should be doable. The performance benefit may be huge. On the other hand it may complicate things.I'll give it a try and probably end providing some arguments on codegen side.
have you looked into https://github.com/nikomatsakis/lalrpop? i'm using it for a toy project and I *think* it supports the things you want?
Not yet, no. We'll be making more announcements as things develop.
* Diffutils replacement, diff and patch (library and cli) I found some projects that seem far along, mostly limited by sparse community feedback and use. It seems like it only needs some gaps filled in and some finishing touches. https://github.com/messense/unidiff-rs https://github.com/uniphil/patch-rs https://github.com/johannhof/difference.rs https://github.com/dflemstr/sparkey-diff https://github.com/elwl/wzpatcher https://github.com/nadirs/ipster https://github.com/BurntSushi/fst/tree/master/src/levenshtein https://github.com/DimaKudosh/difflib https://github.com/ucarion/rust-lcsh https://github.com/joshuawarner32/libbsdiff https://github.com/KokaKiwi/rust-bsdiff
Natural language processing for time and dates anyone? :)
The only way to do this would be an enum.
I think that's just being excessive. I'd just use previous version. It'd be more familiar for others.
Last year I was fighting off a head cold (sorry if I spread it) and still learned a TON and had a great time. Very excited for this year!
Well I want Rust to be the best language ever, and I want nothing but success for the community, so can we start there? Partial common ground? I think my pcj posts are funny, and amusing, and that's why I posted it. Philosophical differences are healthy. I /am/ sorry I singled you out in that post. It was not about you but the CoC itself which I really don't want to start a flamewar over right now so yeah. Thanks for answering me, thanks for being part of the rust community and I'm happy we both exist.
[removed]
Woo! This is the only conference that I plan on going to every year. Are we expecting RustConf to always be in Portland?
&gt; Are we expecting RustConf to always be in Portland? We're not making any guarantees into the future. We investigated several options for this year, including elsewhere in the country, not just the west coast. Portland is both wonderful and cheap! Things can always change, though.
Don't forget the examples and the docs ;). I'm a fan of lean &amp; mean protobufs, since the full-blown Google variant is very big. For our embedded projects we implemented a lean &amp; mean C++ version with a much nicer API - smart fields not tons of generated accessor methods. Works fine on dinky devices with 256K RAM. Maybe think of quick-protobuf as something that could be positioned for the important 'small device' niche?
Not if you write macro_externs!
Damn, sucks being in aus :) I'd go if it were closer. No way to swing it for work since we don't use it,just my hobby
I still think it’s sad that macros can’t detect whether they’re used with `()`, `[]` or `{}`. Then `json!({})` and `json!([])` could have the parentheses dropped.
Only Europe?
I don't need any kind of windows or user interactions. Just a plain image buffer/canvas.
On the one hand it seems really convenient. Those longs lists do not look good. On the other hand this will be really cumbersome to resolve git conflicts when the list in the macro will be big (multiple lines). Happens to me from time to time (thanks to Rails 3 and long list in `attr_accessible`;)) and it's a nightmare to resolve that.
+1 to that. I could probably do it through work if it were Melbourne or something. Maybe next year :)
But folding is a great analogy for what you do.
Hey, I know I'm late, but ... any links? I've been searching a bit, and all I can find is https://zinc.rs/blog/ from 2014.
I think the whole `extern crate` is somehow unnecessary, because if you already define your dependencies inside of `Cargo.toml` why have you to list them again with `extern crate`?
Why not use Qt bindings?
Due to several (happy) reasons I cannot yet speak about, that probably means I cannot attend RustConf this time, too.
Well Cargo is not mandatory for rustc, and you need to know where these symbols you are using come from to validate the signatures and link. Cargo and IDEs could theoritaclly (might not be simple and require searching lots of places for potential symbol sources) add the externs automatically, but otherwise it's kind of needed. Use and extern could technically also be fusioned in one statement, but crate names and namespaces will still be two different things.
No the latest version of legacy, I'm not talking about zombies and the rest of the buggy garbage. I don't really get why people liked zombies that much, there's nothing fun about them.
Yes. I personally prefer to avoid anything that is LR(1)/LALR(1) in favour of LL(1)/PEG. I also wasn't too much of a fan of the syntax, but then again I'm quite picky.
Perfect companion for my "let me building something I know in Rust" project I wanted to start. I worked on search engines too and one of the things I had to do all the time were indexers. So, basically: Get all files from 'somewhere', convert them to text, put them into the indexing library. I thought about using a Solr instance, because interfacing Lucene from Rust sounds very ugly (It is written in Java), but this is far better. So: Thanks for sharing.
This is about the Rust programming language. The subreddit for the game is /r/playrust To spare you further embarrassment, I have removed your post. Feel free to repost in /r/playrust.
What lifetime should the returned array and strings have? Should they be owned by the caller? Use `Vec&lt;String&gt;`. Should they live throughout the program's lifetime? Use `&amp;'static [&amp;'static str]`. Should they live as long as some other object that you have been given as input? Use a generic lifetime like `'a` for both the input and output. In this case, it seems like you want them to be owned by the caller, so you can use `Vec&lt;String&gt;`. If you own an object yourself, you don't have to worry about lifetimes. Lifetimes only come into play when you're *borrowing* an object that someone else owns. *However*, returning a vector like this is typically not the easiest and most efficient thing to do. I'd actually go with returning an *iterator* instead. This is both faster, uses less memory, and because of Rust's excellent iterator system, *easier to use*. In fact, the function you're writing already exists in the standard library: [`std::fs::ReadDir`](https://doc.rust-lang.org/std/fs/fn.read_dir.html) does almost this, but returns a `ReadDir` iterator instead of a vector. 
Looks great, but seriously why not semver? If you don't want to push it to 1.0.0 for the breaking changes (because you think 1.0 is special or something), then just jump to 9.0.0 instead.
Cool 🙂 what version of the sort in std did you measure against? AFAIK, the std implementation has been significantly improved for the upcoming 1.15 release 
Good luck constructing a `&amp;static [&amp;static str]`. See my comment. Use an array in this case. (Or a vector if you own the strings)
The measurements are against the new sort in std (I wrote that one, too ;)). So I did my best at both unstable and stable sorts. Hopefully now it's a bit clearer how much faster can we get if we allow instability and forbid allocation. The most surprising thing to me is that pattern-defeating quicksort is often so fast it can even compete with radix sorts. When sorting random 64-bit integers, it's only tiny bit slower than [ska_sort](https://github.com/skarupke/ska_sort), which got a lot of attention last month.
Oh, I missed it was you! :D And very cool indeed :)
http://github.con/dragostis/pest has size and position data for every token.
Typo sorry. Corrected thanks. Please demonstrate this type. As for ownership. I meant this more specifically for strings. You never own a `str` while you can own a `String`.
As my_two_pence pointed out, you will likely want either a vector or an iterator output instead of an array or slice. Arrays are fixed-length.
I'm a huge fan or the R-in-a-gear. The crab? Not so much :(
I don't see the problem with that. Many lib.rs files are long lists of externs, features and reexports anyways, so I'd prefer the expanded version over the macro.
I stand corrected - thank you. Neat static trick. You could argue you don't own the heap (you own the unique pointer to it) but yes this is logically owned.
Is it worth adding unstable_sort* functions to std lib?
I would personally like to see `slice::sort_unstably` available in libcore. Then, for the sake of completness, we could also implement a slower, non-allocating version of `slice::sort` in libcore.
This is following the Cargo variant of SemVer, in which each `0.x` release is considered to be a backwards compatible series; so `0.9.1` should be backwards compatible with `0.9.0`, but not with `0.8.0`. The convention in the Rust community has been to use `0.x` versions while the API is still considered to be undergoing relatively frequent (couple times a year maybe) breaking changes, and only `1.0` once the maintainers feel like the API can be stable on the longer term (couple of years or so).
Cool idea. One suggestion, have a default name if the mock name isn't provided (maybe something like just appending or prepending `mock` to functions and structs). That way you could be slightly more concise. #[mock(read_records)] fn do_the_things(h: &amp;mut Handle) -&gt; Records { read_records(h) } That would call `read_records()` normally and would call `mock_read_records()` or `read_records_mock()` if `cfg(test)`.
Sorry -- any international orders.
The more I think about this, the more I like it. This is brilliant. In the past I have used generics for mocking, this approach removes the need for that and is simpler. Excellent work.
Impressive progress. This is coming together nicely.
Thank you for kind words. :)
Hard to see your code, could you indent 4 spaces? This version works a little bit faster. struct Fibonacci { curr: u32, next: u32, } impl Fibonacci { fn new() -&gt; Self { Fibonacci { curr: 1, next: 1 } } } impl Iterator for Fibonacci { type Item = u32; fn next(&amp;mut self) -&gt; Option&lt;u32&gt; { let new_next = self.curr + self.next; self.curr = self.next; self.next = new_next; Some(self.curr) } } fn main() { for i in Fibonacci::new().take(200) { println!("{}", i); } }
When comparing language implementations for speed, it's important to actually use the same algorithm. There is no way the website uses the algorithm you wrote above. Secondly, the program you posted above will not be able to generate 200 numbers in the fibonacci series, because it uses 32 bit numbers to represent the elements in the series. You'll run out of bits around number 50 or so.
&gt; `cargo run` Use `cargo run --release`
Yes, my server runs the same code. About the i32 part, yeah, tha may be another reason, it is not coping up with it.
Your little program has exponential behavior, and thus will not be fast in any programming language unless the compiler is extremely clever and memoizes the invocations. [This website](https://www.nayuki.io/page/fast-fibonacci-algorithms) has a lot of different solutions for Fibonacci computation, one even having O(log(n)) behavior.
I haven't, in a rust project, run into a case where I care about stability. I'll make sure to use this in the future.
On the gamedev side, something that would really help with adoption is direct support in cargo for Sony's SN-DBS distributed compile tool. Since this is not publicly available software I'm not sure how that would get accomplished. I've been meaning to look into whether or not cargo's plugin system can handle it.
For Fibonacci numbers, we can do without the loops: fn fib(n: u64) -&gt; u64 { assert!(n &lt;= 75); let golden = (1.0 + 5.0f64.sqrt()) / 2.0; let golden_n = golden.powi(n as i32); let approx = golden_n * 0.2f64.sqrt(); let exact = approx.round(); exact as u64 } 
&gt; I like that the fields are public, and the enums are visible! Even the Java world ([well, the OSGi world](http://enroute.osgi.org/services/osgi.enroute.dto.api.html)) has come around to this.
Very cool! Btw its https://thrift.apache.org/ not https://apache.thrift.org/ ;)
Agreed, this is a very important need. I have been thinking about ways to achieve this, I have an idea but I need to try to see if it will actually work.
An interesting, however I am unsure about passing locales to operations. I mean, just because I speak English doesn't mean I am not looking at a Greek menu; in this case, shouldn't the sort order be Greek (because that's what the menu is written in) rather than English (which is the language I usually speak)? And extending that, passing a locale doesn't seem to account for multi-lingual options; for example mixing the aforementioned English and Greek when discussing the menu options in English. (I am taking Greek because I had a sorting issue with it ;))
Oops! Thank you - updated :)
Yes, that is one use case. The other is the ability to set field to `&amp;str` for serialization. Think about a message called `Msg` with a field called `text`: // Creates some structure which owns a string and that string can't be taken out as owned. let x = magic(); // We access this string calling function `get_text(&amp;self) -&gt; &amp;str`. let text = x.get_text(); // Set `text` field of a message msg.set_text(text); // Serialize and write to stream msg.serialize(writer); In rust-protobuf, one needs to allocate to do this.
String is going full Collection-of-Grapheme-Clusters. Do we have a crate that does that? It could wrap a `String` reasonably easily while providing better notions of string length.
Yeah sure you can say "at the moment C++ is better because it has better IDEs" but it seems like the article is saying "I'm really disappointed in Rust; everyone is talking about it but it doesn't even have IDEs as good as C++". That's clearly unfair.
Turns out there's support in the [standard library](https://github.com/rust-lang/rust/pull/15619).
Nice! I agree there should be an alternative to `rustup show` doing exactly this.
This is big, one step closer to me getting to use Rust at work! Any plans for tokio support?
I'd be really curious about compiling something like Tantivy to wasm. It'd be cool to have good full-text search on my static site, or to have an addons that takes the content of pages from Readability, then pass it to Tantivy for indexing, then have my own simple client side search engine for all the webpages I've visited (or my bookmarks, or feeds, or whatever). [Lunr.js](http://lunrjs.com/) is cool, but pretty limited.
No, that was later removed. For graphemes, you will need to use the `unicode-segmentation` crate: https://crates.io/crates/unicode-segmentation
Are you **sure** you're using the same algorithm everywhere? Because if you open your python console and type: def fib(n): if n==0: return 0 elif n==1: return 1 return fib(n-1)+fib(n-2) fib(200) The console just hangs forever (it probably returns at *some* point, but after several minutes it was still frozen). This directly contradicts what you just said: &gt; The same code, I have written for C, Java, Python, JavaScript and Rust. The only language I got this much of time constraint is Rust. Can you perhaps share the code you used for the other languages so that we can compare?
Thank you! And, no idea yet, sorry. I'm not a thrift maintainer, so it depends on their schedule. And no, `rift` is not in sync anymore (same codebase though!) During the PR review process I was asked to change the crate name from `rift` to `thrift` (a very reasonable request). I'd named the initial crate `rift` to avoid owning the `thrift` name. When the next release is made they will push the `thrift` crate to crates.io.
&gt; only 1 cpu core is utilized by the hyper server. Right now, Tokio is one event loop per thread, so you end up with a single core doing work. The plan is to add multiple event loops with work stealing between them, in my understanding. &gt; the performance clearly isn't great Are you saying this based only on the use of one core? What are the actual numbers? Did you compare to other web frameworks on the same machine?
&gt; I think the rust "scenarios" work may improve this. This is now being changed to "a portability lint" https://github.com/rust-lang/rfcs/pull/1868
I remember some people suggesting that we could also use a specialised implementation of `sort` based on a `UnstableSortSafe` marker trait. I think that'd be pretty cool!
I wonder then what are the compilation unit boundaries for rustc?
Note that there's nothing stopping someone from snatching the `thrift` name in the interim. Might be wise to grab it. (It can be transferred.)
I use https://github.com/kbknapp/cargo-outdated It acts as a `cargo` subcommand, and lists crates with newer versions available on crates.io than those in your Cargo.toml.
Stability guarantees that things with equal sort keys are in the same relative order in the output as they are in the input, and so, with that guarantee, things with early track numbers will always be before things with late track numbers after the second sort (because the first sort put them in that order). Without this guarantee, the things with equal albums could end up in any order, completely mixing up the track number. Stability also guarantees that repeated sorts of the same list won't change the ordering, which is important in user interfaces, to avoid things jumping around whenever a sort is triggered.
I'm not 100% sure which allocation you want to avoid here but I believe you could use get_mut_ref in rust-protobuf. In quick-protobuf as all fields are public this is like any struct 
Actually, I don't know what I was thinking. Of course this needs stability. I've even used this method myself! Edit: "Of" -&gt; "Of course"
Isn't your notion of equality through normalization equivalent to Homotopy Type Theory's Univalence Axiom, which treats equality and equivalence as...equivalent?
Right, nothing about this impacts the code you get. This is the part of the compiler that spits errors at you, not the part that actually generates code.
I like that this could apply to someone who uses vim so much that they never leave it, and someone who has never used vim and ends up with a bunch of :q :wq :wq at the end of their file before they finally get out.
&gt; even more if they are duplicated for each files within a crate. Usually, you have one `extern crate` per dependency; it wouldn't be duplicated per file.
One problem is that you can't use a simple API like this for copies where the input and output slices may or may not overlap: fn copy&lt;T: Copy&gt;(src: &amp;[T], dest: &amp;mut [T]) { ... } because you can't have overlapping `&amp;` and `&amp;mut` borrows of the same memory. So you need to either use unsafe functions using raw pointers, or safe special-case APIs like the `shift` function in this blog post, or this hypothetical function that can only copy within a single slice: fn copy_within_slice&lt;T&gt;(slice: &amp;mut [T], from: Range&lt;usize&gt;, to: Range&lt;usize&gt;)
Looks like ATC is winning over HKT. Damn HKT seemed to be so easy from the first glimpse. Maybe I should get used to ATC and make friends with it. 
Given the similarity between Rust traits and Haskell type classes, maybe there's some useful ideas that could be gleaned from how GHC handles type constraints and type equalities? [`OutsideIn(X)` Modular type inference with local assumptions](http://research.microsoft.com/en-us/um/people/simonpj/papers/constraints/jfp-outsidein.pdf)
ATC is compatible with HKT given certain restrictions, but you're right that we believe there aren't that many compelling use cases for HKT that can't be solved with ATC. EDIT: To be more correct, everything HKT can express *can* be expressed with ATC because ATC just reifies the `type -&gt; type` constructor into a nominal type. We believe the most common use cases for HKT are ergonomically handled with ATC.
I only glanced at the implementation and saw something about median of medians while choosing the pivot so maybe this is irrelevant, but perhaps [Andrei Alexandrescu's improvements](https://arxiv.org/abs/1606.00484) are applicable here? He talks about them towards the end of [this video on sentinels](https://vimeo.com/171927600) (which is totally worth watching in its entirety). Actually maybe his sentinel approach is applicable too? I didn't spot those during my glance through.
It's surprising how different Rust seem to have been. I mean, garbage collection (no lifetime concept, I presume), struct types that are defined by what fields they contain, … Here I was, thinking that the current incarnation of Rust, with all its characteristic features, was like how it was conceived.
There's a [readme](https://github.com/rust-lang/rust/blob/master/src/librustc_borrowck/borrowck/README.md) in the Rust repository. It's pretty detailed.
Note that data races have to do with threading, and borrow checker does not deal with that directly. The thread safety is enforced using two traits: `Send` and `Sync`. Check out the book: https://doc.rust-lang.org/book/concurrency.html and Nomicon: https://doc.rust-lang.org/beta/nomicon/send-and-sync.html
This is really nice. If I may ask, what did you use as a reference while writing this? The only thing I can really find is the [protocol spec](https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v3.spec)
Yes this is a thing. Hasn't landed at our place yet but I think the programme is a few years ahead of schedule nationwide
I wonder if Rust Down Under could work. Could host it in Queenstown and ppl could go skiing etc as well
Yeah. So rustc will look for any `extern crate` statements anywhere in the tree and try to find them; cargo passes the paths directly so it doesn't have to do the path lookup. Putting them in the root module is the right thing to do IMHO; I even argued that it should be disallowed elsewhere, but that's a bit _too_ restrictive :)
I was in that directory and didn't see that that readme! D'oh, thanks.
&gt; Note that data races have to do with threading, and borrow checker does not deal with that directly. The borrow checker does help with `Send` values. It ensures that you can't move them unless there aren't any references left, so you don't need to worry about further access from the source thread after you send the value to another thread.
It appears to be a 3rd party hosting service through FastMail.
Thank you! Will take a look.
In general, while the borrow checker aids the data race mitigation system (Send and Sync), it exists for an independent reason, see http://manishearth.github.io/blog/2015/05/17/the-problem-with-shared-mutability/
&gt; As a proof of concept of that approach, I’ve implemented withoutboat’s associated type constructor RFC, which I will describe in a future post (preview: it’s very easy to do and works out beautifully; in fact, it doesn’t add anything at all to the logic, once you consider the fact that we already support generic methods). Exciting!
After having read Niko's four blog posts on the subject multiple times, I think I can truly see the appeal of ATCs and agree that ATCs are a great feature in and of themselves. There is something to be said for going after low-hanging fruit that catches the majority of the use cases. Especially if it means that such a feature would make it into the language faster. In addition, even someone like myself without much PL background could follow the theory and sections on ATCs pretty easily, though that might also be due to Niko's excellent pedagogy. C++ has some really amazing features but as a daily user, I often feel a gap between other users who really understand and know how to effectively wield those features vs. myself. Also the amount of attention and work being paid to the compiler recently, and all the accompanying blog posts and discussion is really interesting. EDIT: As a note, I find it rather remarkable that he was able to prototype the mentioned RFC with the new system. I had thought that implementing even ATCs was going to be a fair long ways out.
The whole blog post was super interesting, but that section there impressed me the most. I had assumed ATCs were going to be a lot more (?) work given what they add to the language.
&gt; All the ATC proposal really does is let you define a generic associated type that you can actually use for things. Its basically the intersection of two features that already exist. Ah, that makes sense when I think about it like that. Thanks for the edification. 
This post is amazing, and I think your division of newcomers to Rust into programmers of three backgrounds is a very useful tool! As a total aside, I'm curious about a fourth group as well - those for whom Rust is one of their first programming languages. I wonder what their expectations are going in, and how they were met?
I'm not sure! My general sense is that, at least for now, Rust is not a good first programming language. Although maybe I'm wrong. Anyone disagree?
The nice thing with C++ is that all this complexity is not heaped on you on day 1. You can write quite a few things without having to worry about this. They may be riddled with UB, they may not be production quality, but they have a semblance of working. This isn't so true with Rust, where you are forced to internalize the rules before you write code. I think that's a good thing, but I also think that that's an easy way to turn away someone used to learning languages with a smooth learning curve. So yeah, there's desensitizing going on, but folks also compare their experience getting started with C++ and see a difference. I think you're spot on with the problem of training yourself to the wrong mental model.
I think that's a reasonably fair characterization, but I also don't know how useful a starting point for conversation it is. The way to bring C++ programmers into the fold is by highlighting the safety that Rust provides that C++ can't. There hasn't been a lot written to illuminate those differences, and I think in providing those materials we can convince more C++ programmers to give Rust a try.
They need all kinds of things that we don't currently have the resources to support: for example, a totally different book. That doesn't mean it's impossible now, but it takes a very self-driven and determined kind of learner to make it with Rust as a first language today.
Thanks!
I am bit scared of the size of the resulting wasm bytecode. Still it could be useful for extensions like [worldbrain](http://www.worldbrain.io/) and [falcon](https://chrome.google.com/webstore/detail/falcon/mmifbbohghecjloeklpbinkjpbplfalb) . Also it might be useful for nodeJS based webapp or desktop app.
If you can't read it for some reason (still an issue for my site :( ) then it's rendered [here](https://github.com/mgattozzi/mgattozzi/blob/master/src/client/app/posts/hyper-async.md) on GitHub for reading!
~~Whoosh.~~ Could someone ELI5 the above thread? 
Looks good here! &gt; As of this writing I'm using hyper at the commit 5c89032 for this. It's subject to change, but it looks like a good am You might want to encode this in your TOML with rev. Not in the project, but in the post. That way anyone copy pasting it will get the right thing. Also, ^ and ~ are equivalent here, and ^ is the default. So 0.9.0 works too :)
&gt; The way to bring C++ programmers into the fold is by highlighting the safety that Rust provides that C++ can't. Speaking as a long time C++ programmer looking at Rust, I don't think you know your audience. I'm not here for the safety. I'm here because Rust could be a cleaner language with comparable performance. C++ is complicated and unsafe, but I'm only annoyed that it's complicated. 
Is there an efficient way to do an arbitrary shift without another allocation? Like if I wanted to swap the left and right halves of a big array. I assume you either go element-by-element, or you use memcpy with a big temporary buffer?
Insert Socrates joke here.
Right now I've got "fibre to the neighbour". Seems everywhere but my street is eligible :')
Nice comparison. My take-away is that both languages have their weaknesses. Ones that I think can be solved.
https://www.cs.washington.edu/tr/2015/03/UW-CSE-15-03-02.pdf
If you want efficient, treat it like a ring buffer. Write to `stats.last4[i &amp; 3]` on every iteration, move once at the end.
Cool, I made a plugin to display a green/yellow/red gear icon for my favorite prompt using this. https://github.com/frmendes/geometry/pull/98
When implementing generic traits shouldn't trait parameters always be implemented as associated types? I can't seem to think of a good use case for "pure" generics here over associated types, what would that be?
You're confusing stable with deterministic. Pattern-defeating quicksort is 100% deterministic.
You're confusing stable with deterministic. Pattern-defeating quicksort is 100% deterministic.
You are right, I was more looking into combining the two things to maxout the performance on a single server.
[removed]
Thank you very much for your reply! I think I understand the problem now. Arrays have a fixed lenght in Rust. Since I want the length to be variable, I should rather return a ```Vec```. Also I will look into ```std::fs::ReadDir``` and iterators.
Thanks for your reply! Could you explain where I used a slice? I added the function parameters because I will extend the functions to return something similar to ```["file1.avi", "file2.sub"]```. I will look into ```std::path``` and ```std::fs```, thanks!
You can put `extern crate` other than in top-level module of your crate. It can even be function-local. If externed crate is not used all over your crate, then there's a chance to move it from the long list in `lib.rs`/`main.rs`.
 fn get_all_files&lt;'a,'b,'c&gt;(files: &amp;'a [&amp; 'a str]) -&gt; &amp;'b [&amp;'c str] { Note the `-&gt; &amp;[&amp;str]`, here you returned a slice. But your output was really an array but `rustc` didn't get that far as to care about informing you about the type mismatch because it wanted you to fix other errors first. Note the type that `read_dir` returns. A `ReadDir` (how creative), which is an iterator that yields `DirEntry`s. for entry in ::std::fs::read_dir("folder") { println!("{}", entry.path()) }
If you want your function to return all values immediately, yes. Otherwise you want an Iterator (which is what `std::fs::ReadDir` provides). Note that the vector will allocate everything onto the heap and you probably just want to iterate over the results anyway. You can if you want to, collect an iterator into a vector. When it makes sense prefer to use iterators over vectors.
If you take an array (or slice) as an input argument, you can return a sub-slice of it. I.e. `output = &amp;input[3..6]`. This does not make sense for your use-case however. It looks like you just want to perform some filters - and so again, just use iterators! `std::iter::Iterator::filter`
I just reviewed your code. I am mobile right now, but if you want I could file a PR to fix things up a bit... Each individual commit would then fix one aspect. Is this OK for you?
That would be fantastic! 
I have done this for my project with Conrod. I created 2 Command structs (GraphicsCommand (actually an Enum) and GraphicsResponse). Pressing a Conrod button generates a GraphicsResponse which is sent to the logic thread. GraphicsCommand has a couple of different types. The ScreenState type tells the graphics thread what screen to show. If a new ScreenState is received, the module matches it and passes it off to the appropriate function to render the screen. I have multiple threads as well, the main thread which deals with system stuff. This starts the logic thread. The logic thread deals with all the I/O and logic. It also manages the graphics thread. This did take a while to set up, but wasn't that hard and it is pretty easy to add a new screen.
Huh! What's wrong with **E**xtended **S**earch &amp; **R**anking?
I think C++ is a vast community with a wide range of attitudes; I certainly know *many* C++ programmers for whom this is the primary win. It can be both.
Given the readme it's pretty obvious that that is a backronym. Please, don't :|
Can you post the version that's being run on the server? Maybe you're overlooking a little difference. &gt; But, if I generate the same there, it takes me only some seconds. The code you posted is O(2^n ) and won't ever be able to calculate 200 fibonacci numbers within a few seconds, even if you hand optimize the generated assembly. There are trivially implementable O(n) versions (both iterative and recursive).
… Now, what do we do? Edit: Oh, I read the question as "Did you link", rather than as "Did you mean to link".
&gt; The current trait system is a bit hacky and technically indebted, which means a) its slow, b) it has weird edge case bugs, c) its hard to add features to. Are we talking about the implementation or the actual language?
Does "interview question" mean "code it on a whiteboard while we're watching you, impatiently" or "take-home assignment"?
It matches all [ESRs](https://en.wikipedia.org/wiki/ESR).
Most of which are contextually irrelevant.
&gt; rewrite your code into a more imperative algorithm There's nothing really wrong with recursion though, it can be easily made O(n) as well. People are just used to seeing the awful O(2^n ) algorithm that is overused, purely to show recursion for the first time...
Before anyone attempts this, you should read the exchange I had with the author in the comments on the previous blog post: https://ayende.com/blog/176801/the-struggle-with-rust The challenge is outlined [here](https://ayende.com/blog/174049/the-low-level-interview-question). The key difficulty here really has nothing to do with graphs. The difficulty comes from these specific restrictions: &gt; * The trie will be represented by single 32 kilobytes byte array. &gt; * You cannot store any additional information about the trie outside of the array. &gt; * You cannot use any in memory structure other than the byte array. But it is fine to allocate memory during the processing of the operations (for example, to turn the string key into a byte array). These restrictions imply a particular implementation path, and the OP doesn't want to have to do any explicit deserialization or serialization. Instead, they want to cast [parts of the buffer to struct pointers](https://github.com/ayende/trie-cpp/blob/master/trie/trie.cpp#L240) and do reads/writes by reading/writing to the struct pointers directly. AFAIK, there are two major problems with this: 1. You better be damn sure you've got your alignment right. 2. I think that the C++ code violates the *strict aliasing* rule in *a lot* of places, which means there's undefined behavior. (Read the OP's comments in the "struggle" post for their thoughts on this.) In particular, it's not actually clear to me whether the strict aliasing rule applies to Rust when using raw pointers. In other words, this challenge is fraught with peril. The requirements essentially dictate a non-portable and a very unsafe implementation path as far as I can tell. I don't actually see why doing this task in Rust would be difficult. The difficult part is doing it safely in *any language*.
&gt; Before anyone attempts this, you should read the exchange I had with the author in the comments on the previous blog post Am I correct in thinking that it starts from [comment 21](https://ayende.com/blog/176801/the-struggle-with-rust#comment21)? &gt; The difficult part is doing it safely in any language. Ah right, and doing it in Rust would put that issue right in your face whereas doing it in C++ hides the issues under the cover and you may unwittingly have very broken code?
We scroll
As for the alignment issues, c++ offers nice ways of portably obtaining the correct alignment of a struct, but it is not used in the implementation
&gt; I think that the C++ code violates the strict aliasing rule in a lot of places, which means there's undefined behavior That's not the case; casting buffer this way is perfectly OK. Strict aliasing rule would be violated only when: * There are pointers of different *non-char* types pointing to the same memory location; * This pointers actually *used* to access the same memory locations.
well first off, I wouldn't actually want to implement a trie with the given restrictions. But in general, you can use `std::alignment_of&lt;T&gt;::value` to find out what `T` needs to align on. It makes implementing custom allocators etc much easier. You can use this with `std::align` so you don't have to do the pointer arithmetic yourself For many use-cases, you might be able to use `std::aligned_union`, or `std::aligned_storage` as well. 
Good luck on your journey!
That's a great point. And we have that data. So, I will tune the scores to take that into account. My current plan to check if the required version matches max_ver, or the version of the last release. Stay tuned.
I want to forbid taking ownership for a certain type, because the variable MUST ALWAYS have lexical scope. Is that possible? I would like to encapsulate some Opengl features. Since Opengl uses global state, I want to push the new state on a stack and create an object which represents this. This object would then implement Drop to restore the previous state. Since I want to implement a stack, taking ownership of a variable would change the lifetime of the object and mess up my stack: struct Binding (GLuint); impl Drop for Binding() { fn drop(&amp;mut self){ pop_from_stack_and_restore_state(); } } ... let binding1 = Binding(set_state_and_push_to_stack(foo1)); let binding2 = Binding(set_state_and_push_to_stack(foo2)); // binding 2 now shadows binding 1 let binding3 = Binding(set_state_and_push_to_stack(foo3)); do_somthing_really_stupid(binding2); // oops, now binding2 gets moved and might get destroyed before binding3, // or not at all, which would mean binding1 would get destroyed before binding2! // In either case I completely messed up my stack! I guess I could check if the order of destruction is correct and panic! if I messed up. But this would clutter my code with a lot of unnecessary panic!s that the compiler could not optimize away. I'd rather avoid this. I already tried to solve this problem by implementing the Copy trait, but sadly there seems to be no way to implement a non-trivial copy: struct Foo { i : u32, } impl Clone for Foo { fn clone(&amp;self) -&gt; Foo { let n : Foo = Foo{i:self.i+1}; n } } impl Copy for Foo { } fn main() { let a = Foo{i:1}; let b = a; let c = b; println!("{} {} {}",a.i,b.i,c.i); // prints '1 1 1' and not '1 2 3' like I wanted. - meh } Another example, why global state is bad. But I can't help it, since Opengl is not my API. 
Hmm I was curious about this, and looked at some discussions about strict aliasing, it seems that the accepted interpretation is that while you may inspect any T object through a char\*, you can't inspect a char object (like a char[]) through a T\*.. so this would violate strict aliasing
FWIW, I do actually think the requirements are reasonable. For context, the OP works on databases, and this kind of operation on these kinds of data structures seems like something one might want to do. Technically, the interview question as specified doesn't actually say you can't do explicit deserialization, so if you did take that route, the code would still contain unsafe but it would be much easier to say, with confidence, "this code is safe." However, if that deserialization turns out to be costly, then it might be worth going the extra mile to do try and do it safely without deserialization. (N.B. I think "deserialization" in this context means "memcpy your `*char` to your `*struct node_header_info` instead of just casting the `*char`. It's not portable of course, but I think you avoid alignment and aliasing issues.)
Was the C++ solution checked in valgrind? 
I don't know. I didn't personally check it. But that's a good point. There might be other stuff lurking. :-)
Sure, it has its uses, I do something similar (with memcpy) for reading an fst. Doesn't mean I actually want to do stuff like that. I do wonder how much less efficient it would be to store the nodes separately from the payloads though.. that would remove a lot of the alignment pains at least. On a different note, while OP insists on keeping everything in the memory slice without doing any memcpy, I'm unconvinced that this makes things any faster in practice, so it seems like a weird restriction he has..
For the first time I think I'm starting to understand HKT and ATC. I don't know if you are a really good teacher or if its because this is associated with something I'm familiar with (rust), probably both. But this makes so much more sense then previous stuff I read about HKT. Thank you for this explanation.
Note that you run into horrible numerical issues with that algorithm. 
You're welcome! It's seriously cool stuff. It also helps not having to write all the underlying tokio code so this is great stuff. Definitely looking forward to a 0.11 release!
I was curious to see if llvm would just convert it to a memmove call, turns out it doesn't even manage to remove the out of bounds checks #[no_mangle] pub extern fn shift(n: u32, slice: &amp;mut [u32]) { assert!(slice.len() != 0); for i in (1..(slice.len())).rev() { slice.swap(i, i - 1) } slice[0] = n; } fn main() { let mut x = [1,2,3,4,5]; shift(6, &amp;mut x); println!("{:?}", x); } Compiles to https://gist.github.com/anonymous/0237273e2247ae97a855939d28d89710
Almost. The OP expects the trie to be modifiable even while backed by a byte buffer. So you should be able to add or delete keys to the map. (The C++ implementation written by the OP even includes memory defragmentation.)
I think the most idomatic way to avoid dealing with this in rust is the same way you deal with it in C. Never just dereference a field, `memcp` it (or `clone`) instead. Clang/GCC will convert memcp into a dereference if it can prove alignment. 
Which issues are those? I know it limits the range, as it uses f64 which only has 52 (53 counting implicit) significand bits, and there can be some numerical issues when the numbers use all those bits. That's why the `assert!(n &lt;= 75)`. As far as I can see, it should work on all 0 &lt;= n &lt;= 75 without issue, am I missing something?
I think it depends on whether a given type should be able to implement the trait more than once. For example, `AsRef`.
It would hopefully have more predictable speed, to start with. CSPs can be worse than exponential complexity, in the worst case.
Thank you for helping me laugh! :)
&gt; are contempt with Unrelated: this should be "are content with." Contempt would be hatred! Also, I expect these numbers to change a _lot_ in the next few weeks...
Yup! That's why I'm generally in favor of this.
I thought the same, as I commented below, but I just realised that means you can't implement your own allocators without invoking undefined behaviour, so I'm probably missing something 
Your assessment rings true for me personally. I finally understood how to approach problems I'd been solving with OOP in other languages in Rust after reading the wonderful [Rust for Functional Programmers](http://science.raphael.poss.name/rust-for-functional-programmers.html) post. 
You're entirely correct, sorry. I've missed the assert here. But generally speaking, the floating point algorithm isn't faster than one using the [Matrix representation](https://en.wikipedia.org/wiki/Fibonacci_number#Matrix_form).
cc /u/comex
Building on my previous library [lsp_rs](https://github.com/smith61/rls_proto) and my strive to create an open source Language Server for C/C++ in Rust, I've been building libraries to help others create similar Language Servers for other languages. This library provides asynchronous reading, writing, and handling of messages with the use of Tokio and Future-rs, while maintaining proper ordering of responses. Any comments on improvements to the implementation or features to add are welcome.
It's all good! :)
Thank you! This helped a lot. Now here is my solution, it could use a nice macro to wrap around, but it works just fine: use std::cell::Cell; struct State { i : Cell&lt;u32&gt;, } struct Defer&lt;C&gt; (C) where C:Fn() -&gt; (); impl&lt;C&gt; Drop for Defer&lt;C&gt; where C:Fn() -&gt; () { fn drop(&amp;mut self) { self.0(); } } impl State { fn new(i:u32) -&gt; Self { State{ i:Cell::new(i) } } fn with_push_i&lt;C&gt; (&amp;self, i : u32, body : C) where C: FnOnce() -&gt; () { let old = self.i.get(); self.i.set(i); let atend = Defer(||{ println!("Current value: {}, Restoring {}",self.i.get(),old); self.i.set(old); }); println!("Old value: {}, Pushing {}",old,i); body(); } fn print_i(&amp;self) { println!("Value is {}",self.i.get()); } } fn do_something_stateful(state : &amp;State) { state.with_push_i(2, || { state.print_i(); state.with_push_i(3, || { state.print_i(); }); state.with_push_i(4, || { state.print_i(); panic!("Let's see if those values get restored!") }); }); } fn main() { let state = State::new(1); do_something_stateful(&amp;state); } it prints Old value: 1, Pushing 2 Value is 2 Old value: 2, Pushing 3 Value is 3 Current value: 3, Restoring 2 Old value: 2, Pushing 4 Value is 4 thread 'main' panicked at 'Let's see if those values get restored!', src/main.rs:42 note: Run with `RUST_BACKTRACE=1` for a backtrace. Current value: 4, Restoring 2 Current value: 2, Restoring 1 
The trait system is already turing complete and therefore undecidable (unbounded complexity). So all you have is trying to make the common or important cases terminate properly and perform well in practice. Which is basically what constraint solvers do.
which is cool, but I want it as the *foundation* of the UI framework. as the default way to do things. This should make doing the right thing easy, the wrong thing possible (but blatantly wrong and hard), and stupid mistakes easily seen. Add in some multi-platform goodness like most of the rest of rust and this would be *golden*.
I'm not sure how to understand the question. Do you want to implement the drawing yourself and just have an array to write into? Then sdl2 is the thing for you. Or do you want something that does the drawing for you? In that case have a look at cairo. I think cairo let's you save surfaces in various formats, but I never used that. Sdl2 let's you save as .bmp - well, that's not much better than .pnm.
Having one writable borrow has nothing to do with concurrency. It has zero role in making spawned threads safe. It does help keep scoped threads safe but really only inasmuch as it keeps everything else safe. http://manishearth.github.io/blog/2015/05/17/the-problem-with-shared-mutability/
You may want to be asking in /r/playrust
So, in the C standard you have this rather complicated language: &gt; The *effective type* of an object for an access to its stored value is the declared type of the object, if any.87) If a value is stored into an object having no declared type through an lvalue having a type that is not a character type, then the type of the lvalue becomes the effective type of the object for that access and for subsequent accesses that do not modify the stored value. If a value is copied into an object having no declared type using memcpy or memmove, or is copied as an array of character type, then the effective type of the modified object for that access and for subsequent accesses that do not modify the value is the effective type of the object from which the value is copied, if it has one. For all other accesses to an object having no declared type, the effective type of the object is simply the type of the lvalue used for the access. &gt; 87) Allocated objects have no declared type. Thus if some memory is part of an allocation (i.e. using `malloc`, or presumably using platform-specific interfaces not mentioned in the standard, such as `mmap`), you can write to it using a pointer of any type (if suitably aligned), and that will change the effective type; then it's legal to read using that type. You can do this multiple times with the same memory. So if your custom `malloc`-like function hands out a pointer, as long as the client code does at least one write through the type it wants before reading, everything's OK. You just have to use a buffer obtained from a system allocator as backing storage, as opposed to, say, a `char` array (either a global or local variable). Though, oddly, it seems to be impossible to implement your own `calloc`-like function, since callers might rely on the implicit zero initialization and do reads before any writes. (Note that while the C standard does not guarantee null pointers have all bits zero, it does guarantee that all bits zero is a valid [possibly not the only valid] representation for 0 in all integer types. Anyway, if you are on a platform where null pointers have all bits zero, it's okay to rely on that, since it's implementation-defined behavior, not undefined.) Anyway, the C++ standard has no directly corresponding clause. In a blog comment on Ayende's site I incorrectly claimed that nothing similar exists, but [this Stack Overflow answer](http://stackoverflow.com/a/31483328) cites some language that seems to provide similar semantics somewhat more indirectly. (Quote: "This has the rather odd implication that unused allocated storage (returned from operator new) contains an object of every trivial type that fits in that block of storage, at least until the block of storage is used.")
Please someone correct me if I'm wrong, but aren't requirements like his trivial once custom allocators and placement new is implemented? I realize that's a ways out, but it seems like this is on the horizon for Rust already, and would give it a way of implementing safely and _simply_ something that is quite difficult in C++. * The trie will be represented by single 32 kilobytes byte array. Solved with a trivial allocator/`Placer`. * You cannot store any additional information about the trie outside of the array. This isn't as much of a magic trick as it might seem, system allocators will sometimes allocate metadata side-by-side with data, or reserve a portion of assigned memory for their own use. * You cannot use any in memory structure other than the byte array. But it is fine to allocate memory during the processing of the operations (for example, to turn the string key into a byte array) This is a place where placement new shines in Rust over C++. There would be no casts, and it keeps all of Rust's usual memory safety (modulo the correctness of the allocator.) All of the "hard part" is in the allocator, and `&amp;mut` references are totally safe to mutate thanks to memory safety. No undefined behavior, no double frees or dangling pointers. It may be frustrating to Ayende that the timeline is "some day", and that's a fair criticism, I think. 
That's been my struggle. I work on databases and database like systems (embedding subsets of a database inside for a specialized application). That's the conclusion I've come to recently, writing database code in Rust is hard for that problem space. Explicit control over data layout in memory is highly desirable. Both due to density (less space on disk, less space in memory) and control over locality. Not only that but ideally you want to avoid explicit serialization/deserialization due costs. You could argue some of this is zero-cost due to compiler optimizations, but people writing databases are unlikely to rely on compiler behavior that can change. Just look at LMDB, which is quite popular, it mmap in the ondisk data structures. You need this kind of control with the datastructures (trees), indices, actual row (tuple) or column, data on the wire, log / journal layout. I would say that databases the a poster child for software when nice layers of abstraction tend to get violated. While you generally can abstract things out like a query parser and such other internals tend to be pretty intertwined. Generally query execution and processing is tightly related to the memory model and on disk model of the database (and indexes) it's tightly related to transaction semantics, which are tightly related again to memory, disk and journaling. And then journaling is tightly related to some of these. And everybody accepts that as the cost of doing business because databases are judged by the speeds logically or illogically (like dumb TPC benchmarks). I think some of yelling from the tree tops in this forum has been colored by my problem space. I'm still exploring Rust my personal projects (also databases) because I think the language brings a lot to the table in terms of safety. Despite having unsafe, it's sometimes hard to opt out of these things. I'd personally would like to see more escape hatches in unsafe. Labeled goto (blazing SQL parser on x86_*). Escape out of the borrow checker (I'm waiting 3 years and counting on non-lexical borrows). At least you can do ptr arithmetic in unsafe and cast it to badly aligned struct. On some archs that works fine (x86) is quite fast (recent intel processors) and lets you write code to work with more compact data structs without a ton of serialization / deserialization code. But you still have no control over alignment of struct members in Rust. 
Anybody who works in filesystems, databases or interfacing with weird hardware will need to be familiar on how to handle these situations.
Rust is strong on optimizing high-level-looking code down to something very efficient. That is probably unexpected for people coming from C.
&gt; fn shift&lt;T: Copy&gt;(n: T, slice: &amp;mut [T]) { &gt; if slice.len() &gt; 0 { &gt; unsafe { &gt; let ptr = slice.as_mut_ptr(); &gt; std::ptr::copy(ptr, ptr.offset(1), slice.len()-1); &gt; *ptr = n; &gt; } &gt; } &gt; } It has already been noted, but I feel it worth reiterating. The real answer is that instead of copying the stats over and over at each iteration (!) one uses a ring buffer for this kind of things. A crude example here: https://play.rust-lang.org/?gist=2317e916d5d6c812095d5cec9ab0f01b&amp;version=stable&amp;backtrace=0
I totally agree. The fact that [the official documentation](https://doc.rust-lang.org/1.10.0/book/references-and-borrowing.html) doesn't even mention the lexical borrows issue is particularly bad. In fact the length of that part of the book is really far too short. Borrowing and lifetimes are without a doubt the hardest part of Rust and they only get a few pages. The really confusing module system needs much better explanation too. I definitely experienced "Oh cool I understand lifetimes and borrowing now. I'll just `foo.set(foo.get() + 1)`. Wtf Rust?!! Why do you suck so much?" rather than "Oh cool I understand lifetimes and borrowing now *and the fact that there are some current limitations that mean I will have to write code in an unnatural way*. I guess I can live with it for now."
The borrow checker protects against iterator invalidation, a very common issue in mutable languages. The following C++ program has undefined behavior: void crasher(std::vector&lt;std::string&gt;&amp; vec) { for (auto&amp; e: vec) { vec.push_back(e); } } If it were a Java program it would throw a `ConcurrentExceptionModification`. In Rust, it's a compile-time error.
You can write wonky C++ code from day 1. It appears to be working as far as you can tell. You are happy. Of course, if you truly learn C++, you'll realize with horror that your `List` was completely broken because you allocated memory but forgot to define a copy constructor. But on day 1, since you didn't copy it, you didn't realize it. And you were happy.
You should use `std::aligned_storage&lt;BUFFER_SIZE, 2&gt;::type` to guarantee the alignment of your underlying buffer storage. Without it the cast to `trie_header_info*` is undefined behavior. Using an alignment of `8` would guarantee that your `long` are usable without dynamically computing an alignment (because your `struct` are 8 bytes long, and thus preserve the alignment: a couple `static_assert` could ensure they stay that way).
&gt; Could you also comment on your expectations with portability? Sorry, I realized I didn't answer your question here. Anyhow, there's not a simple answer to portability here. Obviously certain things will never be portable, but I feel like they should be possible in unsafe. Other things can be made portable by the compiler. A lot of databases in the past (and even some now) can only be opened only on the same platform by the same software (maybe with the exception of recovery tools). Because they people writing it made assumptions about endianees, pagesize, alignment rules. I believe LMDB thats this approach; I haven't read that code in bit so forgive if I'm wrong. GCC makes some things "portable". For example, when you're doing tight packing, on platforms that do not support un-aligned reads it generates code to read those values correctly. This only works for struct members (not if the whole struct ptr is not aligned). It's slower on those platforms since it'll over read (possibly reading two adjacent words and bithacking it back into one value) but works. And works (x86, new arm) and is fast (on new x86_64). 
You can do OS stuff in Rust, you'll just have to use `unsafe` to get there. [Redox](https://redox-os.org/) is a desktop microkernel based OS written entirely in Rust.
The first compiler suggestion is quite weird, because it just desugars lifetime elision and doesn't change the code. Anyway, I'm not sure why, but this version works: fn bar&lt;'a&gt;(&amp;'a mut self) -&gt; &amp;'a mut any::Any { |q: &amp;'a mut Box&lt;T&gt;| -&gt; &amp;'a mut T { &amp;mut **q }(self).bar() } I think it's actually a bug (or a limitation of closures, at best), since the closure should be equivalent to fn closure(q: &amp;mut Box&lt;T&gt;) -&gt; &amp;mut T { &amp;mut **q } which compiles without problem
Scarily wonderful? Wonderfully scary? Brb teaching some semantics to my 4yr old so he'll be ready on his birthday...
Why not? trait Engine {} struct Car&lt;E: Engine&gt; { engine: E } struct SpecialEngine; impl Engine for SpecialEngine {} This seems simpler to me than dealing with macros.
This sounds about what I expected to hear, but I wanted to make sure I wasn't missing anything. Thanks for the elaboration. :-)
touché
Have you seen the other comments in this thread? (In particular, the comment from /u/comex.) As far as I can tell, the `char*` exception is only from `struct -&gt; char` but not from `char -&gt; struct`. 
There's also a lot of microcontroller stuff. Zinc.rs, which has been abandoned because the design wasn't great, but it worked well before, there's crates for specific boards like the STM32 Discovery and the Teensy boards, etc., etc. Either way, there's lots of options there, and lots of ways to build stuff. I've been building something that abuses the borrow checker/type system horribly to get me compile time validation for board configuration, and it's really cool that Rust allows you to do stuff like that. The moment when you realize your program just failed to compile because you tried to do PWM output on a wire that's not connected to a PWM is just completely mindblowing. 
Reddit does not allow modification of titles or submitted links.
That _is_ mindblowing! How on earth did you manage to do that?
Your `RingBuffer` index will eventually overflow. This will panic in debug mode and probably wrap in release mode (which could cause the index to be incorrect). You can avoid this by using `self.index = index.wrapping_add(1)` instead of incrementing it directly. The `wrapping_add` is necessary to handle the special case where the size of the slice is equal to `usize::MAX`. Editited example: https://play.rust-lang.org/?gist=113c891fcd5141a4c5c0a2aed6bdb7e9&amp;version=stable&amp;backtrace=0
Recently, stdx was unveiled, and I commented that I'd see a place for a stdx-dev, to numerous upvotes. So I wrote it. It's pretty minimal for now, and if you spot an error or have another suggestion for improvement, feel free to comment here or send an issue or PR.
While I agree in principle, `usize::MAX` as the size of the slice seems rather unlikely (unless using ZST). I had forgotten about those pesky 32 bits platforms on which overflow is a concern though; thanks :)
I hoped moderators could :(
Huh, fair. I've had to write fast C++ code in the past and usually goto doesn't speed things up. But that's anecdotal.
The best way to handle file io is to use the futures CpuPool and read the file in an off thread. You can then make a stream of it, and pipe that into the Body.
&gt; If it were a Java program it would throw a ConcurrentExceptionModification. IIRC this is not guaranteed, right?
&gt; Please kindly add a minimal documentation about std lib, its' structure and the the main functionalities Does https://doc.rust-lang.org/stable/std/ not do this for you? &gt; its better if doc has a completed section to guild how to escape from OOP This is in the TOC. It's not written yet.
There's a test of the implementation [here](https://github.com/nikomatsakis/chalk/blob/master/chalk-rust/src/lower/test.rs#L68) if you want to check it out yourself. Pretty cool stuff.
I don't think it is, they only promise a "best effort". I know for certain that it's unreliable in multi-threaded scenarios, because of data-races (never solved that one :(). Not sure in single-threaded scenarios.
I'm basically autogenerating a whole bunch of code that encodes a state machine of possible transitions for each hardware piece, something like [this](https://is.gd/RoOGsy) (really simplified). In reality there's also a bunch of autogenerated trait impls like `DigitalOut` that allow you to pass things around to generic functions over `T: DigitalOut`. The way it prevents mistakes is by just not generating `to_pwm_output` impls for outputs not connected to a PWM. I've been thinking of switching the state newtypes to public and then using `Into` for state transitions, so I can do stuff like `pin1.into::&lt;Pin1&lt;DigitalOutput&gt;&gt;()` instead of manually encoding transitions.
If you are really set on doing it using functional constructs such as map, and keeping it as a one liner you can do something like this (1..101).map(|x| if x%2+x%3==0 {"FizzBuzz".into()} else if x%3==0 {"Fizz".into()} else if x%2==0 {"Buzz".into()} else {x.to_string()}); We can also use match and remove some whitepspace to get it in at 106 characters (1..101).map(|x|match x%2+x%3*2{3=&gt;"FizzBuzz".into(),2=&gt;"Fizz".into(),1=&gt;"Buzz".into(),_=&gt;x.to_string()}); But seriously it's going to to a lot clearer by using a for loop over an iterator 
that's actually really cool
Is there any data structure that operates on a fixed capacity block of memory, i.e. no heap allocations, but otherwise works like a vec and keeps track of how many items are actually "filled in" and allows single-call append/remove?
&gt; Ragel exposes options for generating finite automata using either tables or gotos, and it's my understanding that gotos are generally faster. Exactly, the goto one is faster enough over table driven to be worth it if you're willing to made a code size trade off. The impact is much better on x86 mostly due to the amazing branch predictor on Intel processors.
Perhaps [arrayvec](https://crates.io/crates/arrayvec) might work for you?
A callback works as well: type Fizzbuzzcallback = Fn(&amp;str) -&gt; (); fn fizzbuzz(n: u32, cb: &amp;Fizzbuzzcallback) -&gt; () { match (n % 3, n % 5) { (0, 0) =&gt; cb(&amp;"fizzbuzz"), (0, _) =&gt; cb(&amp;"fizz"), (_, 0) =&gt; cb(&amp;"buzz"), _ =&gt; cb(&amp;n.to_string()) } } fn print(s: &amp;str) -&gt; () { println!("{}", s) } fn main() { for i in 0..20 { fizzbuzz(i, &amp;print); } }
I expect some sort of futures-fs to exist at some point. Something like that would be able to plug in with hyper or any other tokio-based protocol very easily. For reqwest, there will be 2 Clients, at least to start with. The current `reqwest::Client` will stay exactly as it is, simply using asynchronous hyper internally, but still exposing a synchronous API. This kind of client is useful for applications that only need a few HTTP requests, and don't benefit from writing it in an asynchronous style. A new `reqwest::AsyncClient` (name TBD) will exist that wraps hyper's asynchronous client with the batteries-included approach reqwest is taking, but with Futures versions.
&gt; standard would be self-contradicting Strict aliasing is IMO is one of the most defective parts of the standard and there is a lack of agreement on its precise interpretation.
The elaborate further. The way branch prediction works is beneficial for these kinds of cases (parsing, OP code execution). In the table (or table and then switch) case you end up with a large selection of choices making it hard to predict it. You also often end up having to check for the default case. In the labeled goto case, you end up transition from one state to another directly by a goto. In the single transition case there's no prediction necessary. And if you multiple transitions and generated logic it's easy for the branch predictor to guess based on past patterns.
There's already an exponential complexity. fn main() { let a = (0, 0); let b = (a, a); let c = (b, b); let d = (c, c); let e = (d, d); let f = (e, e); let g = (f, f); let h = (g, g); let i = (h, h); let j = (i, i); let k = (j, j); let l = (k, k); let m = (l, l); let n = (m, m); let o = (n, n); let p = (o, o); let q = (p, p); let r = (q, q); let s = (r, r); let t = (s, s); let u = (t, t); let v = (u, u); println!("{:?}", v); } 
I've just created a pre-RFC proposing to integrate pdqsort into libstd and libcore. Please give feedback if you have any :) https://internals.rust-lang.org/t/pre-rfc-unstable-sort-and-sort-in-libcore/4680/6
I should preface this by saying that though those are common in Haskell I doubt they will be common in Rust, primarily because various types that are conceptually monadic do not implement the Monad interface - such as Iterators, Futures, and Streams, and so on. There is no bridging this as far as I can tell. However, they also just don't work will with Rust's ownership model, and aren't that useful in a language with eager semantics in the first place. They're also relatively tricky to implement because they want a higher kinded self constructor, which interacts funkily with Rust's method syntax. That said, something along these lines: https://is.gd/pmyQAF
I think Rust is a great choice. :-)
Perhaps that's because you cannot replicate modern C++ shenanigans, due to say, lack of move constructors. I tried [replicating zero cost small string optimization](https://gist.github.com/xfix/b705bb4bc00aa58eadb578681fd54a3f), only to realize move constructor is not here (for what I assume probably is good reasons, say, making `unsafe` code simpler, but it's still a limitation). (not entirely serious here, is there any use case for move constructors than small list optimization)
Nice. Can you give us some feedback between c# and rust ?
To add, I see a lot mentions about Rust &amp;str vs String, yet c++ also has a String vs char duality issue. In complement, I could add that giving more "batteries" for unsafe rust might appeal to some programmers as well, especially those whose wants to perform funky tricks or be more platform dependent.
Swapping is more than just shifting, though.
I don't know how this optimization is implemented; why do you need move constructors?
I have done a fair chunk of PHP and JS in my resent past. I had a look over the bits that I think are the strengths of both of those. I did have some trouble finding where Functions as Variables section was, so some improvement with that might be nice.
&gt; for what I assume probably is good reasons, say, making unsafe code simpler, but it's still a limitation Also, move constructors just slow down things like vector reallocation since you need explicit calls everywhere. The *reason* move constructors exist in C++ is because of a lack of dynamic drop; so moves need to be implemented as an explicit destructor-aware invalidation of the previous owner. So all smart containers need move constructors. Things like std::string optimization are just bonuses, not the raison d'être. Rust doesn't have this problem, the compiler can handle dynamic drop with drop flags (as the language is designed around affine types instead of them being bolted on later), so most of the motivation behind move constructors doesn't apply. Yeah, you lose out on some optimizations like this, but you gain some others. 
One way of optimizing `std::string` is to sometimes have the pointer field point in the stack area where the len/cap go. Since strings are null-terminated you don't need a separate length field and you get 2 words worth of characters, which is enough for most smaller ASCII strings.
Really cool, looking forward to checking out the code when I get in front of my computer. As someone learning rust with a large C# background, this is an awesome project for me to learn from.
&gt; might be better if we had a way to highlight the important content or to remove unwanted contents. I find use of bolding to be really jarring and it interrupts the flow of reading. I think many folks do. E.g. the bolding in your comment made it harder to read, for me. I suspect that this impacts people who sight-read more. Italics work out okay, but those get used for emphasis, not highlighting key bits. 
I see, as opposed to using a discriminant.
You can always kinda mock pointers by using indices. Look at arena implementations for example. There are other ways as well but I do not know of a complex non-acyclic structure which does guarantee type and run safety. I'm far from an expert though. 
&gt; I'm here for the safety. I hope you find it, but trust me - a lot of experienced C++ programmers look at memory errors in the same light as a paper cut. They can be unpleasant, but after you've had a few, you get better at avoiding them. I'll agree with you on race conditions - if you haven't applied discipline the whole way through your development, you're going to regret being multi-threaded. Btw, I'm not sure Rust can claim to avoid deadlocks, and I believe you're still going to need to be disciplined to make threads reliable. 
Rust RTOSes make so much sense. I expect there will be lots of them and they will go hand in hand with the growth of iot to make pervasive embedded computing accessible to everybody. If I wanted to get a production quality Rust RTOS up as fast as possible I would probably do a straight port of FreeRTOS. The codebase is small and accessible. Somebody could do it quite fast. Then I'd use what I learned from that experience to evolve Rust-safe idioms for RustFreeRTOS. 
Oh sure of course, but I was approaching it from a new learners perspective (the op I replied to cited trying to learn C++ multiple times) and pointers + other Cisms that still exist in C++ would confuse a Javascript programmer (again, op). 
Rust is great for ensuring static safety where possible, and it should theoretically be suitable, but in practice many embedded platforms aren't going to easily be supported because they don't have LLVM backends. However, systems based on ARM or MIPS should work fine, so long as everything is documented such that drivers can be implemented in Rust. Some other problems are loosing the existing knowledge base from existing code implementations which likely aren't in Rust and any tools which won't work with Rust. Basically, it will probably be more feasible in the future, but it is possible now, and would likely provide some benefits, though not without trade-offs. However, I will say that I have only used C for embedded and haven't given Rust a try yet for it, so take my words with a grain of salt.
zinc is not an RTOS. I'd say it's more of a HAL.
For super large projects, there is [Eco](https://github.com/pistondevelopers/eco). We use it in the Piston project.
[removed]
[removed]
I know about FFI section but there arent any examples for C#
Yay thank you!! The new book isn't finished, so we haven't wanted to send people there yet :)
I'm going to assume you want to call Rust code from C#. You probably want to build a DLL from your rust code, exposing various structs and functions as "C"-style exports. Once you have this DLL, you can load it up in C# just like any other DLL using the C FFI layer. Rust is designed to be compatible with C in this way. I'm on my mobile, so I don't have links to docs. Hopefully this points you in the right direction for further web searches. If you want to load up C# code from Rust, then you'll need to load up the C# runtime using Rust's FFI, then use those functions to call out to C#. That's presumably "pretty complicated", but if you're lucky somebody already has a Mono crate or something like that
As the person who originally proposed and implemented the `symlink`/`symlink_file`/`symlink_dir` split, I definitely agree that a friendlier wrapper would be nice for cross-platform code. At the time, I considered it, but because symlinks on Windows already required special permissions, you would need to write platform-specific code to deal with symlinks anyhow, and I didn't think it was worth it to make Unix users jump through the hoops of using `symlink_file`/`symlink_dir` just for the sake of compatibility that didn't really exist anyhow. However, if it's true that in the new version of Windows 10 they will be easier to use, that's a big win and writing cross-platform code with them will become easier; even if it's only in "developer mode", that does make a big difference for writing cross-platform developer tools. In that case, I think I would advocate for `std::fs::{symlink_file, symlink_dir}`, but not including `symlink_auto`; `symlink_auto` seems a little too easy to screw up, especially considering that it can create dangling symlinks on Unix but not on Windows, and as you mention, you generally do know what you are creating a symlink to anyhow. And as when I proposed the original change from `std::fs::soft_link` to the platform-specific symlink variants, I really don't think that `soft_link` is a good name, no one calls them that unless they are contrasting it with a hard link.
https://doc.rust-lang.org/book/functions.html#function-pointers I may have missed it in the new book, but this is what I mean.