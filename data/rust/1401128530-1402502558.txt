Using 0u makes range() output `uint`? Can you make it output any numerical type? Edit: But if I'm using i to calculate `100-i`, I then need to convert the result from uint to int to assign it to array. Does it matter when I convert the variable?
*facepalm* Yeah, now it all works. :-)
Got it: This works: `let mut array2 = [0i, ..100];` This also works: `let mut array2: [int ,..100] = [0, ..100];`
Use `=` instead of `:`.
I wouldn't rule out CPU temperature as a contributing factor, but I think having the CPU caches, memory management and operating system a bit of time to register the new workload will probably benefit the result. YMMV. "Microbenchmarks are the beast" -- Alexey Shipilev (Java performance guru, creator of JMH)
The original criterion does include a warmup period of 100 ms, then does another warmup of 10 ms, and the second one is used the find the number of iterations and estimating how long the benchmark will take. The port is currently missing the first warmup, because I was unaware of its purpose. But now that you mention that it's used to fill the cache/inform the OS about the workload, I'll make sure to check how its inclusion affects the measurement.
I think that, unless you have very specific memory/performance requirements, you'll want a [Vec](http://doc.rust-lang.org/std/vec/struct.Vec.html) instead of a stack array. Since you're used to dynamic languages, I think that Vec's flexibility will feel more familiar. For instance, you could do the initialization above with: fn main() { let mut v: Vec&lt;int&gt; = Vec::with_capacity(100); for i in range(0, 100) { v.push(100 - i); } println!("first element of the vector: {}", v.get(0)); println!("last element of the vector: {}", v.get(99)); } Or, using [Vec::from_fn](http://doc.rust-lang.org/std/vec/struct.Vec.html#method.from_fn): fn main() { let v: Vec&lt;int&gt; = Vec::from_fn(100, |idx| { (100 - idx) as int }); println!("first element of the vector: {}", v.get(0)); println!("last element of the vector: {}", v.get(99)); } Note that indexing Vecs doesn't work right now, that's why I used `.get()`. AFAIK, this is in the TODO.
Nice! I'll be sure to check their methodology. Thanks for the link.
100ms seems a bit short to me, but of course that depends on the workload. I'd at least run the benchmark for 1-2 seconds before actually taking measurements. Edit: Or even better make it configurable.
`range()` can output anything it can add and compare.
It sounds like a rule that's detrimental to any growing project in Rust. Say if I want to split up my project into several crates, suddenly the rules for trait impls change, and refactoring will be much harder than it should be?
I guess I just concocted it in my head. Maybe it was `main.rs` vs `lib.rs` that I was corrected on. Either way, I'll fix the article.
only if you your use case indicates that the rules should be relaxed further :) you don't *have* to extend if you don't want to. I'd be happy if it was a compiler warning that you have to manually quiet, to alert people to the hazard you mention. "warning impl of another crates's traits- won't allow building a library"... IMO , it would speed development up, by allowing more decentralised experimentation. One doesn't need to wait for one blessed picture to emerge - I think you'd get further by writing the methods you need, then you just need to confer with the community and rename when people agree what to call those methods.. instead of waiting for committee consensus before getting things done. Take a look at whats going on with strings now. its crazy to expect one library designer can accurately predict everything needed for every user
How do you declare the return type? You can't just say `[T, ..*]` because the compiler doesn't know that the `*` here is the same as the `*` from before. You could add a special-case rule saying "assume it is the same", but then you can't handle functions that take two arrays (unless both arrays have the same length).
That proposal hasn't been touched in 5 years, and the community has moved on to trying to fix records properly instead. Many functions don't 'belong' to one of their arguments, and could require analysis of arbitrary arguments to be resolved. a.foo(b) is convenient and works well with classic OOP, but it doesn't scale to multimethods.
You can use privacy for this. What you can't name, you can't change. You can also ensure arbitrary invariants, not just immutability. This does not help for "internal" codes. As far as I know, currently there is no way to express that. You are not missing anything.
Personally, I find that a pity. Yes, it does not scale to multimethods, but it does not try to. It is a good proposal for what it does. The perfect and the good are mortal enemies indeed.
I don't actually see the source for `cxx2rust` anywhere. As near as I can tell from skimming that post and googling for it, it's a project that's being worked on, but isn't available yet. Is that right? Given that, I can only speculate that `cxx2rust` is actually generating `extern "C"` wrappers for the C++ APIs, and those wrappers are what Rust calls in to. 
Would it be feasible to at least have an annotation + linter? Coming from Java and working with big teams has taught me that unless I put safeguards in place, the next guy (who could well be me in a few weeks) will invariably mess things up ;-)
Also...the only reason I am boxing the vector in this example is because this comes from a larger example where the vector is stored in a data structure as a Box&lt;Vec&lt;u8&gt;&gt;
The "error" you're getting is that you're hitting End of File, I think: Err(e) =&gt; println!("{:?}", e), results in (for my first test case) std::io::IoError{kind: EndOfFile, desc: "end of file", detail: None} Hopefully that at least gets you a little less lost 
in my head it was because the Vec can change sizes but now that I am looking at the implementation of Vec it is just a header with an internal pointer so I don't actually need to...good catch
I think I found the cause: Even though you've initialized your `Vec` with 1024 capacity, it has zero length (no elements). So, when you take a mutable slice from it, it has zero size, and can't be written on (rust won't allow access to uninitialized memory outside of `unsafe` code). There are a few solutions to this: 1. Initialize the vector with zeroes: let mut vec = Vec::from_elem(1024, 0u8); 1. Use a stack array (which is automatically initialized): let mut vec = [0, ..1024]; 1. Use the [push()](http://doc.rust-lang.org/std/io/trait.Reader.html#method.push) method instead, passing a length: let mut vec = Vec::with_capacity(1024); file.push(1024, &amp;mut vec); You should also take a look at using a [BufferedReader](http://doc.rust-lang.org/std/io/struct.BufferedReader.html) instead of `File` directly, as (quoting the link) "It can be excessively inefficient to work directly with a `Reader`". **Edit**: Looks like `BufferedReader` [uses unsafe code internally](http://doc.rust-lang.org/src/std/home/rustbuild/src/rust-buildbot/slave/nightly-linux/build/src/libstd/io/buffered.rs.html#54-69) to get the speed boost of not initializing the memory, but keeps that behind an implementation detail to remain safe. Seems like another good reason to use it.
That will protect me against others who try to modify t from the outside, but won't protect me against future code changes inadvertently writing to t. It's not about the interface, it's about adding safeguards and expressing intent for future maintainers.
If this is a problem, it likely means you should be writing smaller modules.
&gt; You might as well use the real types But there are no real types if you can't include the C headers? You have to create fake pointers with the "type Something c_void;" and use "*Something" in your code. Or I am wrong somewhere?
You should define the C types in Rust and then use the correct types in the C functions definitions. If it's an opaque type, using `enum Foo {}` works well. There's no reason to give up basic type safety or obfuscate the function definitions.
Define `Final` in its own crate and you won't be able to see its privates, and the member variable `t` is private by default. Besides, explain to me how having a type wrapped in 'Final' does not express intent?
There's no way to modify the `t` field outside the module in which `Final` is defined because the `Final` doesn't publicly expose any mutation and the field is private. (i.e. one should have a module specific to `Final`.)
Ah, I see; sorry, I misunderstood your comment. Would that imply a performance penalty for using Final (because of the ref created), or can the compiler optimize that away?
It is worth noting that something like the following still works struct Foo { x: Final&lt;int&gt; } impl Foo { fn bar(&amp;mut self) { self.x = Final::new(2) } } i.e. it's not "true" immutability. (However, this problem still exists (somewhat) with true immutable fields too, since you can overwrite an whole instance of the struct with a new one.)
Well, it makes it (more) obvious that something is wrong, by requiring a new `Final` object to be created, i.e. `Final` expresses intention.
Agreed. A lint-checker that warns against overriding the Final type would then still be possible.
Yes, my memory is terrible, so if I don't express my original intention to the compiler, to some extent there's a risk that I'll be that "next programmer" coming along and messing things up. But you can work around limitations by forming good habits! Coming from Java, Rust gives me more guarantees, but in this aspect it actually feels a whole less safe. It is something like being able to 'pin' the design at that point, and having that guarantee propagate through the code through compiler checks. That one 'final' shuts down a whole lot of failure paths in my head, so simplifies the analysis.
Thanks for all the feedback and contributions! Please open issues if you find any bugs. If you have questions, just ask.
That's a shame. Yes, I see what you are saying about overwriting the whole struct. That isn't possible in Java because everything is behind references. I guess I need to keep on searching for a Rust idiom that lets me reason about the code in a way somehow equivalent to what 'final' let me do in Java.
Exactly my sentiment. However, I'd actually be OK with a lint (perhaps in conjunction with the proposed "Final" struct). Then teams in need of a safety net can run the lint on compile/check-in time while the lone programmer who only feels restricted by the lint can deactivate it. In the future I hope for a Rust IDE that shows lint warnings while I type.
well in practice you have both the namespace and all the arguments selecting the appropriate 'f' ... I don't think its a problem
Hi, interestinly i was setting up a new project in rust today and was looking for some best practices, etc. First my try was rust-empty, but eventually i ended up using rust-mk. I think it would be great to have a more complex standard package. My usecase is that i want to have an executable, which uses some libraries i write and also external libraries. For me it was confusing what cargo is and cargo-lite. Is cargo usable for some playing around? How do i install it? How do i set up a more complex hierarchy besides /src/main.rs? In the end i used rust-mk and set up: src/my_executable/main.rs src/my_lib/lib.rs src/my_lib/some_mod/mod.rs and added the one line to my Makefile and it seems to work. Unfortunately it took me like an hour or more just to figure out basic file, directory structures, etc. How would i implement that in rust-empty? Also, i'd like to add external dependencies as well (github libraries like rust-zmq). 
Oh, I should have said module instead of crate. And LLVM is very smart, it will optimize that away.
Is there a warm-up phase including in `#[bench]` ?
Cargo is a package manager that is being worked on, but isn't really ready for anything yet. Eventually, it will be ready, and there'll be good answers to your questions, but not yet. I'll leave the questions about rust-empty to /u/long_void :)
This would be a great habit for all Rust projects to get into.
Oh, that information is in the readme! https://github.com/bvssvni/rust-empty Sorry if this was confusing.
It sort of warms up for 100ms, I plan to add to the braindump an explanation of how `#[bench]` works (for comparison), but here is a (rough) summary of what it does: One sample is 50 measurements, each measurement executes `f()` (function under test) `iters` times and returns the average time. And the logics goes like this: loop { sample = get_sample(iters); summ = get_summary(sample); // get mean, median, etc sample = get_sample(5 * iters); // sample for a longer time summ5 = get_summary(sample); if at_least_100ms_passed() &amp;&amp; // this is like the warm up medians_are_close(summ, summ5) { return summ5; // stable measurement } if timeout() { // I think this is 2 seconds return summ5; // not stable but return what we got so far } iters *= 5; // sample for a longer time in the next iteration }
Let's Rust!
Ok. I guess that's what I will start having to look into and then use bind-gen to do the rest. Thanks.
We use it in the Piston game engine project (https://github.com/PistonDevelopers/piston). The way we do it is setting up symlinks from target/cpu-vendor-os/lib from project to project. I use 'make symlink-info' which generates a file '.symlink-info' that contains the status of the remote branches. This tells me whether my forks are synchronized or if I need to send pull requests. Unfortunately there are no other options that are less painful. We are discussing and trying new options all the time. We are waiting for Cargo to make this easier. Rust-Empty is fairly prepared for the transition. If you want your library to usable in bigger projects at a small inconvenience cost, then I recommend using it.
I was having similar thoughts a few weeks ago during the great mut-pocalypse threads, where the subtle distinction between safety and "manners" was mentioned. I think that's a good way of thinking about it, even though there is still a safety aspect to language features that promote "manners": they're like the maintenance-crew safety stickers you see on machinery that say "NO STEP", "NO GRIP", or "CAUTION: JET INTAKE": the maintenance crew isn't armed with the entire knowledge of those who designed the plane, but they can still wreak havoc. I'm a 'dark matter' developer by day, and even though I normally loathe Java's verbosity, I do appreciate that I can enlist the compiler to help me guide less experienced colleagues towards working within an intended design rather than unwittingly subverting it. I like the annotation/lint idea. 
Related to this question, I'm struggling to discover which methods are implemented for the primitive array `[type, ..size]` ? I was assuming that it was the same methods than for `Vec&lt;&gt;` but I'm realising it's not quite the case, is there a way in the source code or the documentation to see precisely what are the methods implemented for this kind of array? I'm actually quite confused by the difference between these two arrays. 
[The actual source](https://github.com/mozilla/rust/blob/1e2bb09bbbbae4470fef295d245304fe08e1acab/src/libtest/lib.rs#L1289-L1350) is pretty readable.
All habits would be great for every human endeavour to get into. Your turn. ;) (but yes, I agree)
I wonder if people are more inclined to leave comments if they see there are already comments.
I miss Servo updates! :)
Is there a particular dearth of comments on your TWiR posts? Usually I just try to maximize asymptotic signal-to-noise, specifically by not polluting threads with my would-be inane contributions. Did you update the layout of TWiR after moving it to this domain? I could've sworn it looks better now than it did last time (on this domain). *This comment brought to you by the hypothesis validation project.*
I'd like to know this too, the tutorials are not very thorough...
Yes, I test it every now and then. No, it cannot render most web pages. In fact, I doubt you could find a single web page you'd want to look at that it *did* render properly. But, it's getting there very fast!
In Rust we trust!
Lars, who usually writes them, had a baby!
Not usually, but I'm bored today. And yes, I changed both themes and blog engines. I'm now using the Python-based Pelican, rather than the Ruby-based Octopress. Not as popular, but I find it to be *much* higher quality.
Wasn't it pretty decent at Wikipedia?
Am I strange if I'd want to look at Acid2? :)
We don't like your kind around here.
I think the article's language might be a little on the fervent side, but once you cut through that, there are some suggestions that rusticians could definitely afford to learn from with respect to leveraging the type system for domain modelling. Here were some takeaways that stood out to me: - If there are a finite number of possibilities, use an ADT to represent them. - Use a wrapper type to encapsulate the desired structure; make it impossible to create invalid instances. - The static knowledge we have, or require about our domain objects should be captured in types. - Quickly translate interchange formats into data structures that reflect the knowledge we require, and can only hold meaningful and valid states. - Treat your types as the only real documentation. - Constrain argument and return types to a named alternative, that limits possible states. This is all maintainers should need to know about your functions.
Cross-posting from /r/programming. I thought this was a very well presented talk. It is interesting to realize the tradeoffs while trying to maintain language backward compatibility. Hopefully Rust will never need to pile on such hacks.
WOOOOOOO \o/
It was. For some time. It's broken right now.
This is old but I think insightful for anyone writing an HTTP server: http://www.and.org/texts/server-http 
&gt; Use a wrapper type to encapsulate the desired structure; make it impossible to create invalid instances. The cool thing is that this is cost-free in rust. &gt; Treat your types as the only real documentation. But don't forget the other only real documentation, which is the documentation ;-) &gt; Constrain argument and return types to a named alternative, that limits possible states. This is all maintainers should need to know about your functions. This is sometimes useful, but it can require a lot of adapter code to bring types of different code bases together. Also, while YAGNI usually applies, future-proofing library APIs sometimes requires widening the value domains.
Wrapper types are basically cost free in earlier languages like C too. struct ValidPointer { struct inner *ptr; } struct AsciiChar { char c; } but it's not exactly a common pattern, and of course, no language features to enforce safety or privacy.
Can someone give an example of how to create a type that only has 3 possible values? Like 0, 1, -1 for the compare function that the article mentions? Also, how is it represented by the computer? 3 possibilities need at least 2 bits, but 2 bites is already 4 possibilities...
 enum Ordering { Less = -1, Equal = 0, Greater = 1 } which will be stored in a `i8`, the smallest type that can store those values.
Thanks. Why is it `i8` the smallest type for this, BTW? It only has 3 options.
As a caller, I would like to do this: foo.unwrap(); foo.unwrap(default =&gt; "Fizz"); it makes no sense to me to have to memorize `unwrap` and `unwrap_or_default` The benefit for huge signatures like above is obvious: the code is self-documenting and doesn't suffer from argument order mistakes. I dread having to deal with addNewControl("Title", 20, 50, 100, 50, true); That's literally meaningless to me. At the same time, overloading based on static types is generally a bad idea (it's counter-intuitive and tries to fake dynamic dispatch while not providing it). Overloading based on arity is much less problematic and more easily understood. This offers the benefits of overloading based on arity so you can have one method name to cover more cases. But because you have keywords you can even have two methods of the same arity be called with different keywords. It offers a much better version of overloading than, say, Java's. I mean just look at the std lib now: fn split&lt;Sep: CharEq&gt;(&amp;self, Sep) -&gt; CharSplits&lt;Sep&gt;; fn splitn&lt;Sep: CharEq&gt;(&amp;self, Sep, uint) -&gt; CharSplitsN&lt;Sep&gt;; this could just be fn split&lt;Sep: CharEq&gt;(&amp;self, separator: Sep, maxTimes: Option&lt;uint&gt; = None) -&gt; CharSplits&lt;Sep&gt;; not sure if optional arguments should be an Option or if there should be more syntax like `?(maxTimes : uint)` for arguments that are completely optional and follow different code paths
Sorry but `unwrap` and `unwrap_or_default` is a bad example. The two function have a totally different behaviour and such justify a different name. To the topic: I think the only language who got named arguments right is Smalltalk (although it technically does not have named arguments) and I'm not sure if I like it there, it is too verbose...
Actually, 3^5 &lt; 2^8, so I think you could pack 5 instances in a byte. 8-) I have no idea how to do it, though...
That's even less efficient, since it requires modulo and division operations, e.g. getting the first one in a byte `x` is just `x % 3`, the next is `(x / 3) % 3`, then `(x / 9) % 3`, ... etc.
OK, I just took the examples from the earlier thread. It's probably supposed to be `unwrap_or` instead. But I dislike smalltalk named arguments because of this: addNewControlWithTitle: "Title" x: ... you have to mash your function name with your first parameter it also encourages things like ifTrue: ifFalse: ifFalse: ifTrue: those are two different type signatures because order matters, but those have to be declared separately even though they do the exact same thing also there's Array method calls like with: with: with: with: with: with: with: with: with: with: with: with: with: with: with: with: with: with: with: with: with: Yes, I'm serious, that's a shortcut for making an array, someone literally made 6 different methods for your convenience I would like to write ONE method signature and just have it be called different ways with some optional parameters.
Nice! I'm also curious about the status of rust w/emscripten, which I think also falls under the broad "web" umbrella. I assume the changes to break up std are going to make that easier? But I'm not sure how much of std (or how many third party libraries) you'd have to give up.
I enjoyed reading this. I'm not sure I'm 100% on the same page with respect to naming being overrated, though. I understand the argument being made: given a sufficiently strong type system, the structure of the code is the only truth for reasoning about the code. The underlying assumption here is that reasoning about the code is the only thing of value. What about _communicating_ to someone else reading your code, whether it's a current member of your team or your successor? Sure, you could argue your team and/or your successor should be reasoning based only on the types, but now you're also assuming that these programmers are ideal spheres of uniform density. In the real world people are busy and fallible and you should throw them frickin' bone already. Also, you really wouldn't be _communicating_ so much as leaving things as an exercise to the reader. Now back to the other underlying assumption that you're fortunate enough to be working in a language with a sufficiently strong type system. Everybody raise your hand if you never have to deal with lesser languages. Good for you, now check your privilege. Some of us are chomping at the bit for the opportunity to use a better language even once. Naming is a hard enough skill to master that you should practice it at every opportunity. (Caveat: a "great name" for something might just be a single letter, like T for a type parameter, or i for an index.)
&gt;… plus most people will want a fancy ORM, a full‐stack framework, &amp;c. ad infinitum. I feel like most of those would take a big, steaming dump on the performance people come to Rust for.
Off topic, but I just wanted to say that the CSS rule in this subreddit which alters the way your name is displayed should be changed from &gt;GET /chris-morgan to &gt;GET /user/chris-morgan
Yup.
You’d be surprised at how efficient these things can get, doing their work at compile time. Certainly if the SQL that ends up being produced is inefficient, that’s problematic, but a Rust ORM can make producing that potentially inefficient SQL remarkably cheap by extensive use of the type system.
&gt; I wonder how hard would be for this guy to explain Rust's lifetimes :) Looking forward to reading *Effective Rust* and *More Effective Rust* …
Actually, Rust may not be too far. struct FooParams { a: int, b: int, } static DEFAULT_F: FooParams = FooParams{a: 3, b: 18 }; fn foo(title: ~str, f: FooParams) {} fn main() { foo("Hello, World!".to_owned(), FooParams{a: 4, .. DEFAULT_F}); } And here you, the rustic equivalent of `def foo(title, a = 3, b = 4)`. With a bit work on inference, and building on the absence of overloading, we could allow eliding the `FooParams` struct name; then all that would be left is the ability to name *all* parameters. Is it necessary to complexify the language for this ? It really seems like a niche case of syntactic sugar.
Don't underestimate how appealing a *broad* offer is. If look at a F1 race, there is only a couple drivers, but a whole crew of mechanics and supporting staff. Similarly, not all parts of an application need performance, especially when most of the time is spent waiting on the database... ... however it is *very* compelling to write the whole system in a single language! And for this, you need a language that can really perform well for those few hot spots. And guess what: Rust has close to C performance, and once optimized could even challenge Fortran (aliasing knowledge) and yet is high-level enough that writing stuff is *easy* (ADT! Closures!) and *safe*.
Indeed, I think the discussion on [Issue #6973](https://github.com/mozilla/rust/issues/6973) has some similar notes. In particular I have pushed in the past for adding fields with default values to structs and then allowing one to write something like `FooParams{a: 4, *}` or something along those lines, as I [proposed in that same ticket](https://github.com/mozilla/rust/issues/6973#issuecomment-24123745)
I wouldn't use an ORM at all, we have an AST macro system, why not making SQL a first class citizen on the framework?
This is reddit, it should be `GET /u/chris-morgan`
Shifting and masking would be much faster... but you still don't want to pack data that tight without good reason!
&gt; The problem is the pattern match: each arm produces a different type of CellCollection, which makes the compiler unhappy. Unsure how to fix it or restructure the code. Yes that is the problem. The returned type must be unambiguous. One solution is to make an enum, with a variant for each cell type. pub enum CellVariant { Unpadded(UnpaddedCell), Padding32(Cell32), // etc } You can implement the Cell trait for CellVariant and then it can be used the same way regardless of what cell type it actually represents. The size of a CellVariant will be the size of its largest variant. 
Oh interesting.
Great, thanks! I'll give this a shot. &gt; The size of a CellVariant will be the size of its largest variant. This, however, could be problematic. The reason for these different types is because each struct has a certain amount of unused padding to prevent cache line false-sharing. If the CellVariant size ends up as the largest variant, it defeats the point of different sized padded structs :) Is there perhaps a better way to align memory other than arbitrary padding?
Yes, TWiR only goes from Saturday 12:00pm to Saturday 12:00pm, each week.
You can wrap the different CellCollections in an enum, rather than the Cells.
Ahh, I see. Thanks! Will play around and see if I can get something working :)
I once used Rust. It was *Super Effective* ;)
And now I've been CSS'd as well. Curse the mods :p curl http://www.reddit.com/r/rust/ | ~/node_modules/.bin/html \\ | grep '\\.css' | sed 's/.*"\([^"]*\.css\)".*/\1/g' \\ | xargs curl | csstidy - | grep -B5 -A5 DELETE out: a.author.id-t2_4mzys:before { content:"&amp;"; } a.author.id-t2_gem85:before { content:"DELETE /"; } a.id-t2_7b7l2.moderator ~ span.score:before { content:"1844674407370955161"; } (t2_gem85 is me.)
You'll need to just implement the traits yourself, rather than deriving them automatically.
&gt; With an ORM, poorly optimized database calls get injected all over the place I feel like ORMs are so different you can't make this comparison, exactly. Which ORMs are you familiar with? This doesn't match my (admittedly biased) interactions with ActiveRecord, for example.
Yeah, seems to be the case, was hoping that I didn't need to. Thanks for the fast reply!
I don't know and I don't like it. Just kidding, I think it's fun that they do.
what I like about it is, you better leverage fewer symbols &amp; definitions; it's not about functions with lots of parameters - it's extremely useful, IMO, for functions with just 2,3 arguments. eg fn slice(&amp;self, from:int, to:int); fn slice_from(&amp;self, from:int) {self.slice(from, self.len());} fn slice_to(&amp;self, to:int) {self.slice(0,to);} // notice how many times 'slice, from, to' are repeated vs vs slice(&amp;self, from:int=0, to:int=self.len()); // use the named args to do any of the above.. // one function does the work of 3 // less repetition in implementations // automated way of generating a set of conveniences // fewer jumps to see what the call does It would be a great opportunity for Rust to fill this space IMO, which C++ can't because the grammar is set. Overloading can be a double edge sword because you can't see what a function call is, you need more context. but named parameters move in the opposite direction, reducing the amount of code to navigate. without default params you create more helper functions, which means more to document, more to search through. (I would suggest , alongside the struct initialiser issue, moving to '=' for initilialization/assignment/binding - so its used for let, struct-initializers,and function default args; and use a different operator like &lt;- or := for mutative assignment, but even =&gt; as suggested by the OP would be great IMO.) I agree with the point about self-documenting too, e.g, you give an example valid use in the function signature, the one you anticipate is most common, and of course the names at the call site make it easier to see whats going on. r.e &gt; introduces too many subjective choices where the design was previously obvious. I would argue that in providing an inbuilt way of rolling conveniences, a number of arbitrary naming decisions is reduced, which is a good thing. I'd accept that default arguments would be a lower priority for the language - e.g. for me, type-deduction would be way more important - but it would be a shame to declare that the language will never have it, and there is the issue of API design where defaults could clean up standard features (like .slice() ) 
My feeling is that Rust could be especially interesting for web stuff because of its safety. Many languages and frameworks are not fast, but fast enough. But are they safe enough?
That sounds like an impl that should be defined in libstd proper, wherever `Rc&lt;T&gt;` is defined. Want to submit a pull request adding it? To answer the more general question, to implement `Show` on an arbitrary type requires wrapping it in another type. For example, `struct MyRc&lt;T&gt;(Rc&lt;T&gt;); impl&lt;T:Show&gt; Show for MyRc&lt;T&gt; {...}`. 
Yes, that is my point: storing them more loosely as one value in 2 bits (aka 4 in 8 bits) is more efficient than 5 values in 8 bits.
It's just something we have to PUT up with I guess
Linux-&gt;Linux with different architecture works, but I've had issues with cross compiling to both Windows and OSX. While cross compiling is available, it is not working yet.
Yes, it's a bug in the stdlib if we don't implement one of its own traits on any of its own types, since users have to jump through hoops to do it themselves.
That is something I will be experimenting with one of these years—type safe SQL. That may or may not work out to be practical; taking it more in the direction of LINQ might be beneficial. But this much is true: anything LINQ can do, we can do more efficiently.
Too obvious.
This seems awesome! I'm not much of a back end dev or maybe its a rust idiom, but why does the example handle function take a `&amp;mut Response` instead of just returning a `Response`?
[Not necessarily less efficient](http://drpetric.blogspot.com/2014/03/bit-gathering-and-base-2-to-base-3.html).
&gt;I'm far more interested in type and memory safety. Have you looked at F# and Ocaml? They're language ghettos, but they're EXCELLENT language ghettos. F# ruined me, I can't code in any other language without feeling frustrated now.
For those of us that need a towline, this is pretty fantastic. +1
If you're interested I wasn't able to build just a clean fork of the rust repo. First it failed during `make` with: &gt; g++: error: unrecognized command line option ‘-stdlib=libc++’ I guessed it might be because in the documentation it says to build with gcc 4.7 and I have gcc 4.8. So next I tried `./configure --enable-clang` After that it failed during `make` again with: &gt; rust/src/llvm/include/llvm/ADT/iterator_range.h:22:10: fatal error: 'utility' file not found #include &lt;utility&gt; ^ 1 error generated. I have clang version 3.4-1ubuntu3 on kubuntu
Isn't there also some difficulty interfacing directly with the DOM via the JS compiled with emscripten? Some sort of compatibility layer coule be developed I suppose, similar to how emscripten fakes things like file io for easily porting existing code.
&gt; similar to how emscripten fakes things like file io I can see how this could work with File IO, rendering graphics and such, but not for the other things that people do with javascript: how/why would you abstract that you want to set the `style` of div `#foo` to `"color: red"`?
Can you tell which exactly issues do you have?
https://github.com/mozilla/rust/issues/12859
It’s a matter of blending it with bindings exposed in JS. The task is not at all insurmountable.
So all of the types in stdlib are supposed to implement Show? Because some of the collections like Bitv and BitvSet don't (Bitv has a to_str method though). 
I am the author and it's simply because it contains a mutable writer and because it will be used later, by the server, to finish up the write. I don't remember if the writer caused any ownership issues, but that's an other reason. I will check later if it's possible to change. It would look nicer. :) Also thanks for pointing it out and for your positive reaction :) Edit: I tried to change the handlers to receive `Response`, but it fails due to [issue #14377](https://github.com/mozilla/rust/issues/14377). It will have to wait.
I would really like that. C++11 introduced the declaration of a default value for fields, and it seems like a conceptually simple change yet it's very handy; it notably helps harmonizing default values for those fields across multiple initializations sites painlessly. The only question is what the default could be ? For example, allowing `vec!()` would be great. Should this allows any function/macro then ? *(without a way to capture `self` since it's not initialized yet)*.
Aha, it seems to be Win-only issue as I was able to cross-compile easily OS X -&gt; OS X/iOS/Android
&gt; macro Macros are irrelevant: it would just allow any macro that expanded to a valid expression (i.e. macros that expand to things that are legal to write out in full).
The difficulty is not to interface with the DOM (this is surprisingly easy), but to map some of the existing rust libraries and concepts (IO is a good example) to what the web platform exposes. It requires a bit of work but emscripten shows that it's doable and that it scales well to big codebases (unreal engine and friends). 
The latter problem seems like an issue with clang itself, `&lt;utility&gt;` is a C++ Standard Header and should thus be automatically found by the compiler.
Indeed, I was just citing it because the expansion of `vec` is a block-expression; so I guess my question would thus be: Should this allow any arbitrary expression ? Or be restricted to constants or constants + function calls ?
I am starting to enjoy rust more just due to the names of all the libraries and frameworks. I like the play on words style naming conventions much better than the JThis, JThat or NThis, NThat for .NET (or this.js, that.js, etc...).
Oh… yeah. I was meaning to put that in. “You can be confident that the client will be good because Servo."
Yep, that's one of the consequences of our implementation of traits. We're trying to avoid another problem, where executable A is using libraries B and C, and both of those libraries implement the same trait for the same type. You can read more of the justification why we chose our approach [here](http://smallcultfollowing.com/babysteps/blog/2012/10/04/refining-traits-slash-impls/).
If this was what a language needed to get wholesale adoption, we'd see a lot more F#, Ocaml, Scala and Haskell.
Ruby has much of that too - though puns and clever names are more common than wordplay. It can be a problem for discoverability though. I hope someday Rust gets an equivalent of the [Ruby Toolbox](https://www.ruby-toolbox.com/)
If you would like to do some compiler hacking, I would recommend either the #rust or #rust-internals IRC channels (see sidebar).
I wasn't trying to hack so much as just do a build
And typos: `nokogirl`, `bundle`...
I understand wanting to detect integer overflows, but I'm not fully sure what the program should actually *do* in that case. Would such an overflow halt the program by default? Would it implement saturating addition? What would the control of this look like from the programmer's perspective?
Are you the person who added these username altering rules to the CSS?
Don't worry, Go has the correct way of handling this: x, err := 1 + 2 if err != nil { return 0, err } return x, nil 
Why does `ToStr` even exist instead of `to_str` being a default method on `Show`?
Only if you're enjoying the whimsy of it all. If you have complaints, then I hereby place all blame on dbaupp.
I'm enjoying it so much that I wanted to gild the person responsible, so that's why I needed to know :)
You're welcome.
Yay finally someone prominent is saying this.
This stuff is great. I created https://github.com/hjr3/rust-hal to allow myself to use Hal (hypermedia) responses with rust. I am excited to tie this together.
I'm not that familiar with hypermedia APIs, but if I understand it correctly, it can be built on top of a regular REST server. Then, I guess, it would not be a big problem to build it as a second layer or as some kind of middleware, when I add them.
[The mill](http://millcomputing.com/) does it right. It has four flavours of arithmetic operations: wrapping, saturating, checking and widening. Unfortunately it doesn't exist yet. But I hope I will write Rust on Mill soon :)
Math operations could return a Result-style type. To make this more bearable, there could be two classes of integers; ones that can be proven at compile time to not overflow, and the ones that can overflow at run time. So `StaticInt + StaticInt` would return `StaticInt`, but `StaticInt + DynamicInt` would return `Result&lt;DynamicInt&gt;`
I think he was kidding.
&gt; Matthew Flatt tells me that Racket’s performance penalty due to software overflow checks probably exceeds 100% for jitted tight loops that do lots of integer math. I wonder what *relative* overhead would apply when trying to implement Option&lt;...&gt; nonwrapping arithmetic functions in Rust, without changing the language to accommodate for this.
History mainly.
Adding two `StaticInt`s could overflow. Checking for overflow everywhere would be painfully slow, and using result for every integer math operation would be absurdly tedious, not to mention slow.
Please don't antagonize other programming languages. Flame wars don't benefit anyone.
Really? I know there's a hardware overflow flag for it, but I don't know how well that plays with concurrency
The problem with polygon triangulation is that you can make many optimizations based on the knowledge you have about the shape for a certain context. This can usually be done in linear time while most general algorithms takes O(N*log N). It turns out that general polygons are not neither that useful or common in game programming. A rasterization algorithm for a polygon is much simpler, but then you have the expense of shifting shader program on the GPU which makes finding the optimal algorithm even more complicated.
How would this work with SIMD?
If only there was some sort of condition system...
That is how I would expect it to work. 99.9% of the time it's completely unexpected for integer operations to overflow. I wouldn't mind using a different form of arithmetic operation for the few cases where I want it to overflow. It would also be a nice form of self documenting code when people see an overflowing arithmetic operation.
I was probably thinking of asm.js when I wrote that. I guess just using the non-asm.js backend for emscripten would make it easy enough.
It looks nice with a small example, but when your handlers are functions, it makes it more painful to create the equivalent of controllers. In the real world, whatever handles your calls usually need a handle on the persistence layer, and with simple functions, it forces you into having global variables.
Awesome work! :D I'm newbie to game dev. This is a hybrid engine, right? Both 2D and 3D are supported.
3D is supported through using an external library like [hgl-rs](https://github.com/cmr/hgl-rs/).
&gt; Adding two StaticInts could overflow By default overflow of StaticInt would be compilation error. There could be something like `#[overflow(wrap)]` for cases where overflow is actually intended, which I imagine would be relatively rare. StaticInts would never overflow at runtime. &gt; Checking for overflow everywhere would be painfully slow Presuming hypothetical architecture with HW check for overflow and sufficiently smart compiler, I'd imagine it wouldn't need to be painfully slow. As a last-resort there could be UnsafeInt which would forgo all checks but would be only usable in unsafe blocks. &gt; using result for every integer math operation would be absurdly tedious, not to mention slow. There could be one thing that would make the idea more palatable: BoundedInt, which would have programmer-defined bounds for the value. Afaik Ada has something of this sort. With those, the compiler could forgo runtime overflow checks in cases where the bounds ensure that no overflow is possible (or in rare cases ensure that overflow always happens). I think small amount of programmer overhead would be acceptable for the safety guarantees achieved, same way that there is some programmer overhead already accepted for the memory safety of Rust. And of course code complexity comparisons should be made against actually correct C code, instead of the fast and loose code that C allows. Ultimately I think that the effects of overflow safety can only be determined experimentally. Of course the performance might be difficult to evaluate; you'd need to use "exotic" architecture (MIPS and Alpha were mentioned in the article), or take the results as extremely pessimistic worst-case scenario. But the code complexity/verbosity should be apparent even if the code compiles to sub-optimal machine code.
That's my trouble too, a.foo(b) the name foo is local to a only, which is a poor solution for methods like a.add(b) Is there a syntax suitable for having local names in multimethods? Perhaps `(a, b).add` &gt; the community has moved on to trying to fix records properly instead. What's this fix? Does it involves local name resolution?
It was only a little joke :(
That's true, but I don't see where a database can't solve that problem (except for cache). One problem with persistent states is concurrency, both within one server instance and between multiple instances. I may have misunderstood what you meant. Can you, please, give me a more concrete example of when it becomes a problem?
 You probably want your functions to take &amp;mut rather than taking their arguments by value. Then you can pattern match using ref mut and don't have to rebox xs.
 fn handler(request: &amp;Request, response: &amp;mut Response) { // hello, I am a static reference to a connection pool // good day, global variables // goodbye, testing let con = connection_pool.get_conn(); // do something with the connection } On the other hand, if your routes expect something implementing a Handler interface, you can do clean dependency injection: struct MyHandler { conn_pool: ... } impl Handler for MyHandler { fn handler(&amp;self, request: &amp;Request, response: &amp;mut Response) { // No global variable let conn = self.conn_pool.get_conn(); } }
Making a handler trait has been in my thoughts for some time and this may be a quite good reason to implement it. I realize that I can't know every possible use case for a generic framework and this would allow more freedom to the user, but it's not without its own costs and problems: * It will require some kind of packaging/boxing, at least until DST comes, unless generics are used. The problem with generics is that it will limit the handlers to one type. * Multiple endpoints with the same handler has to be taken into consideration. Function pointers are easy enough, since they have no state, but a struct has to contain shareable data, act separately or be boxed in a shareable container. This is an effect of the tree structure in the router. * Mutability may be limited by the concurrent nature of the server, but I guess some well placed locks may solve that problem.
The bit where you have to introduce a new scope and bind a mut reference if you want to modify data outside of the || closure seems to introduce a lot of lines. Do you think people might pass around more references in the future to avoid writing `let x_ptr = &amp;mut x`?
Sorry, but we already get enough hate in the Go community as it is, and that makes me sad. Let's not encourage a culture of mockery.
There are proposals for syntax sugar that we could consider if this proves to be onerous.
It was more friendly banter than mockery.
It could trigger the trap if any value overflowed, for example.
This will help me to get rustc running on WebFaction’s CentOS 6 servers, which have glibc 2.12 (my Arch machine produces binaries that need glibc 2.14), preventing conveniently building locally, and a too-old version of libstdc++ to run rustc there. This is why [arewewebyet.com](http://arewewebyet.com/) is not Rust-powered at present. Although this is a good workaround, I am concerned about the underlying problems; it’s going to cause a lot of pain for servers often have old software. I don’t suppose there’s much we can do about the glibc 2.14 problem, but the libstdc++ one—can we include libstdc++ in the nightly builds?
All CPUs have overflow flags. They're talking about a hardware trap, like a hardware exception (FP operations have this). But don't forget that some operations (especially crypto, random number generation, hashing, etc) rely on the wrapping behaviour. To me that is what i32 or u32 means -- you've specified that you want wrapping 32-bit arithmetic. So you can't take away our wrapping integer types or make them unsafe! They are perfectly well defined operations and what we want in many cases. If you don't want wrapping integer maths (CPU-native operations) then you need to implement it in software (in assembler, probably), except on some DSP chips which do support bounded integer maths.
crypto pretty much essential for anything web related
I searched "site:bugzilla.mozilla.org integer overflow" and came up with these two Firefox security exploit reports on the first page: https://bugzilla.mozilla.org/show_bug.cgi?id=255067 and https://bugzilla.mozilla.org/show_bug.cgi?id=727401 . Wrapping has always been and continues to be a serious source of bugs in Firefox.
These would not be exploits in Rust. But I take your point.
Well, I hope it ends up being a bit looser because not being able to use "simple" functions to initialize `static` is a pain right now.
r.e. the need to have a reference when you want to mutate a value in the calling scope: will the compiler throw out errors to catch existing cases. (i.e. not just capture a mutable value and mutate some temporary in the closure) How does mutating a captured local variable work at the minute, and why does it have to change; (is the implementation just too messy, or is it just too inconsistent with something else that needs to be fixed.) is this a little bit of complexity in rust to deal with the absence of how C++ does references (which of course are messy in their own way) Is this all omitting the need for a 'capture clause' where it may have been efficient to pass some things by value, and an mutable by reference in C++ . 
Our `vec` module was hilariously insecure for a while due to integer overflows until it got fixed.
Thinking about it -- since the size of int/uint is not fixed, no-one can count on a particular wrapping behaviour, so it seems feasible to leave the definition of int/uint in Rust open to add trapping on CPUs that support it, i.e. it might trap or it might wrap depending on CPU support. In this were the case, if someone wanted guaranteed wrapping behaviour, they should use i32/u32/etc.
`int` and `uint` have a well-defined, predictable size. They are guaranteed to be the same size as a pointer. The wrapping is entirely predictable and changing this would be a major backwards incompatible shift in semantics.
There are already intrinsics for checked overflow in Rust. The raw intrinsics introduce a large overhead, and using `Option` instead of the signature returning both the result and the overflow indicator almost always increases it further.
Rats. Is there any hope for an approach where you can pull in cross-crate impls provided you explicitly name the crate it's coming from? Or is there a deeper problem? (symbol clashes in the linker maybe?)
Rust explicitly doesn't allow life-before-main, so we'd need to evaluate any `static` initialisers during compilation, and const-exprs (or CTFE, compile time function evaluation) are a non-trivial feature.
ok thanks. some discussion on IRC cleared this up for me aswell. &gt; will the compiler throw out errors to catch existing cases. (i.e. not just capture a mutable value and mutate some temporary in the closure) &gt; r.e. 1 - will existing mutation (without explicit &amp;mut) be a compiler error, to make correctly refactoring existing code easier ? Initially this panicked me a bit, but mutation is the minority case (or something i don't mind having to type more to do), and it does sound more useful overall.
PSA: if you're writing a library with some procedural macros, put the procedural macros in a separate crate to the core library functionality. If you don't do this, then you impose a *run time* dependency on libsyntax, even though it is almost always only needed at compile time. (Among other things, this means that you cannot easily create a statically linked executable, because libsyntax only comes as a dynamic library by default.) (Fixing this was one of the "incredibly quick enhancements" to Rustful.)
… but they still require newer libstdc++ than is available on WebFaction’s CentOS 6 machines.
Then why don't we remove it before we hit 1.0?
Registration is open for Strange Loop, but it sounds like it is filling quite rapidly.
Yes—for now I will dissuade anyone that is wanting to use Rust for web things, though I will be pleased if they do not heed my dissuasion. I am looking forward to being able to actually recommend it for use.
Hey, you might know the answer to this. I set up a fork and added a `Show` impl to `Rc` the standard way. However, when I run `make check` to see if it works, it always says: &gt; ~/Desktop/rust/src/liballoc/rc.rs:184:9: 184:14 error: macro undefined: 'write' &gt; ~/Desktop/rust/src/liballoc/rc.rs:184 write!(f, "Rc \\{ \\}") as if it doesn't know about the `write!` macro. It's weird because in the same file it uses `assert!` macros which are from the same module. It might also be because rc.rs is in liballoc as opposed to libstd, I don't know. Any ideas? 
Vektah just released [a plugin](https://github.com/Vektah/idea-rust) for IntelliJ IDEA.
Does anyone know if the talks will be accessible online during or after the conference?
Native threading is typically called 1:1 rather than N:N.
Your `ErrorMessage` should be replaced with the standard library type [`SendStr`](http://doc.rust-lang.org/std/str/type.SendStr.html).
They will be made available afterwards.
You’re right, we will need password-hashing things, MAC algorithms and so on. I think that’s actually an area where we are closer to ready. I’ll check it out some time and add it to the page.
Thanks for the heads up. Yeah, I'm not one to come up with good examples. It worked for the two cases I tried, but obviously not for any other. I'll get that function fixed. (The goal was to do more than checking for two characters though, but I decided not to) Edit: Fixed
Fixed.
Yes, I was aiming at CTFE here. I am actually pretty glad for the "no life before main" rule as I've been caught by exceptions thrown before main in the past and the likes, and it's pretty annoying... that and the initialization/destruction order fiasco. I do understand CTFE can be non-trivial, though I believe procedural macros have it (with a restriction to external lib crates). I would also understand if CTFE were limited to "pure" Rust functions... ... actually, my main grip here was the impossibility to call a type `new`. I'd rather not leak the types internals, but it also means no-one else can define a static entity of that type. It's jarring.
&gt; Besides, considering Rust is a systems language, exceptions and stack unwinding are unacceptable. Wait, Rust does have stack unwinding; it's actually crucial as it executes all those `drop` methods to get the system back in shape (avoid leaking memory, locks, ...).
If you define a static initialiser in the module containing a type it should work, e.g. [like the atomics](http://doc.rust-lang.org/master/std/sync/atomics/index.html#reexports). (Unfortunately we don't have generic statics (yet) so it doesn't work for things like `Vec`.) &gt; I do understand CTFE can be non-trivial, though I believe procedural macros have it (with a restriction to external lib crates). Yes, but they generate AST before any interesting compiler passes run, specifically, before the privacy pass, so they are subject to the same internals rule you describe.
First of all: what are you trying to achieve? (why do you need Any) Dot does auto dereference but that is not the problem here, compiler is telling you that there is no method as_ref for Any. You can't just take anything and convert it to T, you have to use unsafe transmute for that.
You need to bring [the `AnyRefExt` trait](http://doc.rust-lang.org/master/std/any/trait.AnyRefExt.html#tymethod.as_ref) into scope to call methods from it.
&gt; You can't just take anything and convert it to T, you have to use unsafe transmute for that No! Use the (safe) methods that `Any` provides via [its extension traits](http://doc.rust-lang.org/master/std/any/). (These use `transmute` internally, but only after ensuring that it is correct.)
My bad, never user Any before. Connection from Any -&gt; AnyRefExt in the docs would be cool also.
I figured others may enjoy other approaches to low-level memory access that's safe: &gt; Can we twiddle pointers and still get the nice safety assurances of high-level types?
Yep, I agree the docs are poor. (Pull requests accepted. :) )
Should I be worried that `size_of::&lt;ProgramResult&lt;()&gt;&gt;()` is 112? Doesn't that make for some really unnecessary `memcpy`'ing even in the success case?
Doesn't rustdoc usually spit out a "trait implementations" section? (eg StrBuf's docs)
The "fix" would be something like `Box&lt;ProgramResult&lt;()&gt;&gt;`, but one definitely needs to benchmark to work out if that is actually more efficient (allocation has its own cost: e.g. the cost of actually doing the allocation &amp; free, the reduction in memory locality).
Get your ticket before it's too late: https://www.regonline.com/Register/Checkin.aspx?EventID=1562983
An extra pointer in the CPU cache has a cost too, they add up.
Any particular reason you suggest OCaml/F# over Haskell?
Or at least `type ProgramResult&lt;T&gt; = Result&lt;T, Box&lt;ProgramError&gt;&gt;;` so you'd only eat the allocation cost in the error case? I'm really just wondering how our approach of nested enums for every return value compares to the throw/catch-style PR of there only being a cost when you actually throw an exception.
That works too, but it does still add code (the deallocation) bloating the instruction cache. Also, even "zero-cost" exceptions aren't actually zero-cost, e.g. they are relatively difficult to reason about, meaning compilers have to optimise conservatively when exceptions are around, making code slower than it otherwise would be (unfortunately, Rust can actually still incur this cost due to stack unwinding via `fail!`).
Ah! That was indeed the problem. Still getting used to having to 'use' traits. Thanks a lot.
This is exactly what [dynamically sized types (DST)](http://smallcultfollowing.com/babysteps/blog/2014/01/05/dst-take-5/) covers.
Thanks for the article. Just a quick question, what's the best practice for monitoring and restarting failed tasks?
I feel like Haskell's execution model is both very experimental and very rigid. Conversely, Ocaml and F# are great "Get Shit Done Languages", because you have it all at your fingertips - imperative, functional, and some excellent facilities for meta-programming (F#) and genericism (Ocaml). The jump is also not as great; for one, you can do I/O in Ocaml without knowing about monads. I personally don't care about purity for the sake of purity, I just want to be able to reason about code easily. Wrapping side-effects in an I/O monad is usually overkill for me. Another thing is that F# has much, much, MUCH better tooling than Haskell; and yet another is that the lazy programming model has a much trickier memory footprint profile than strict programming, and memory issues are really not fun to debug. I use Haskell when I don't know exactly what I'm trying to do, just exploring at random. It's great to learn some abstract algebra too. However, if I have a program in mind, I'd rather not jump through the Haskell hoops.
TaskBuilder::future_result (ie. TaskOpts.notify_chan) is what Servo uses for this (http://mxr.mozilla.org/servo/source/src/components/util/task.rs#17).
That's in the doc for the module. However, on the page for the Any trait there's no indication that there is an `impl AnyRefExt for Any` lying around. (on the other hand, these are all listed on the same page for StrBuf). Might be a trait vs struct thing.
I don't know if it works, but even if it does, you won't want to add another level of indirection.
&gt; I guess this is because different threads could call incr() at the same time? Is mutable static always unsafe? Yes. It *can* be safe if you don't use tasks at all, but that premise is easy to break, so the compiler lets you manually check that portion of code. This attitude is indeed a major difference between C/C++ and Rust. `unsafe {}` block is not a way to mark the code as unsafe; it is a way to mark the code as *safe* even when the compiler fails to recognize that. The same thing applies to the type system in general: there are some programs that are safe but rejected by a type system [1], but that does not necessarily mean that a type system is bad. [1] Actually, there *should* be such programs when the type system is [decidable](https://en.wikipedia.org/wiki/Decidability_%28logic%29). There is no guarantee on the decidability of Rust type system as far as I concerned, but it is still likely that such programs would continue to exist.
&gt; `libicudata.so` is 25 Mb of special cases with a 35 Mb test suite. Oh, fortunately, data required for implementing `to_lower` would be not *that* large. With an appropriate optimization the primary table would weigh only some 30 KBs, as done by [CPython](http://hg.python.org/cpython/file/tip/Objects/unicodectype.c) [1]. As always, though, I agree that a proper Unicode handling is not a scope of libcore. It would be still desirable to have some kind of table-based Unicode support in the standard library, possibly in libstd. [1] As many lower-upper pairs have a fixed offset, it is possible to optimize the table by having a conversion table from the index to the actual offset. A large chunk of Unicode mapping consists of letters without a case (think of CJK ideographs and Hanguls), so a trie would greatly benefit as well. This approach is very common with Unicode handling, and [rust-encoding](https://github.com/lifthrasiir/rust-encoding/) uses it a lot to keep a total table size under 600 KBs. ---- EDIT: There are multiple levels of Unicode support. As a first-order approximation, each level's complexity is ordered as follows: 1. A support for Unicode character in at least one Unicode transformation format. (No actual overhead, but not very useful either) 2. A support for case conversion... 1. ...with only character-to-character ("simple") mapping. (~30 KBs of space overhead, negligible otherwise) 2. ...with a full, locale-independent character-to-a-sequence-of-characters ("full") mapping. (negligible additional space overhead, some API change) 3. ...with a full, locale-dependent character-to-a-sequence-of-characters mapping. (see 5. for overhead) 4. ...with a case-folding mapping as well. (space overhead similar to those for lower/upper/titlecase mapping, but not sure if they can be combined) 3. A support for normalization... 1. ...with NFC and NFD only. (~100 KBs of space overhead, very complex algorithm) 2. ...with also NFKC and NFKD. (~50 KBs of additional space overhead, quite complex algorithm, difficulty in explaining the difference between canonical and compatibility normalization) 4. A support for character encoding. (~600 KBs of space overhead for full WHATWG coverage, moderately complex and hard-to-test algorithm) 5. A support for collation and locale dependency. (~200 KBs of space overhead for collation, lots of API change and possible format support) I hope libstd to support at least 2-2 and 3-1, and the umbrella standard library (either fully supported or just curated) to support 4 as well.
What's up with DST? It has been decided to add it to the language for ages, and it seems that Niko Matsakis is supposed to implement it as a paid employee of Mozilla, but there's no public code implementing it, not even work in progress. Is it being developed in secret or what? 
Thanks, I'm an idiot. I read that post when it came out, but I haven't really paid much attention to the trait stuff. Wasn't thinking about trait objects then so I focused on trying to understand the `[T]` situation. I haven't seen an RFC (besides Niko's blog posts) but this seems to be the tracking bug for DST: https://github.com/mozilla/rust/issues/12938
nrc has been implementing it, has had PRs landed, [and his branches are of course public](https://github.com/nick29581/rust/). What makes you think Niko was implementing it?
Absolutely, I'm not saying anything to contradict your first paragraph. In fact, my last sentence is saying the exact same thing. What I mean is that I disagree a bit that "it is a way to mark the code as safe even when the compiler fails to recognize that". What if you do the equivalent of char buf[16]; memset(buf, 0, 1024); or something to that end, inside an unsafe block? Needless to say, bad things will happen. I think of unsafe { } as saying "things can go wrong here" to the person reading the source, more than I think of it saying "this code is safe, even though you don't realize that" to the compiler. In reality, of course, it's a bit of both, but the first part usually applies.
The second part is pretty important though: you need to make sure that any unsafe code you write still fits into the assumptions that the compiler makes in the safe portions (things like aliasing, type safety, etc). There's a danger of people thinking 'oh, I can do anything I like in unsafe so long as it's something which would be OK in C'.
I'd rather have no `.to_lower()` function at all than have one that's ASCII-only, which is trivial to implement yourself if you really want it.
Think of it as saying "the safety of this code has been verified by the programmer, not the compiler." The complier treats this as a directive that the code is to be assumed safe, even though it can't prove that. This is basically necessary for the compiler to work at all in the presence of such escape-hatches. To a human, that same flag signals that the code should not be trusted until verified externally. This seems like a contradiction, but what's going on is a shifting of responsibilities. Normally the compiler checks several safety properties, in unsafe blocks it is the programmer's job and not the compiler's. You have your job, and the complier trusts that you did it.
I'm not entirely sure what you meant to say, but your classification of normalizations seems a bit of to me. Once you have NFD/NFC, you pretty much get NFKD/NFKC for free in terms of algorithmic complexity. However, implementing NFC/NFKC is considerably more complex than NFD/NFKD. Rust already has support for the later two.
&gt; Things like fn to_lower(char) -&gt; char are hopelessly broken Please [file bugs](https://github.com/mozilla/rust/issues/new) if you notice something is wrong. &gt; why are Unicode string literals mandatory? Other than the small number of character escapes, `rustc` (correctly) treats string literals as UTF-8 black boxes; there's no unicode tables/information required for handling them.
To add to what cmr said, Nick has [even opened a PR with initial support for DST for `[T]`](https://github.com/mozilla/rust/pull/14397).
&gt; I haven't seen an RFC (besides Niko's blog posts) but this seems to be the tracking bug for DST: The DST design all happened before the RFC process was invented.
#9363 is [fixed](http://doc.rust-lang.org/master/std/char/trait.Char.html#tymethod.to_uppercase). &gt; String literals are the same declared type that is validated UTF-8 in the constructors, which pulls in char (also a language item) which pulls in unicode.rs. Could you rephrase, I don't really understand this. There's definitely no runtime validation of string literals, and string literals work perfectly fine in a freestanding context: #![no_std] #[link(name="c")] extern { fn printf(fmt: *u8, ...); } extern "rust-intrinsic" { fn transmute&lt;A, B&gt;(x: A) -&gt; B; fn abort() -&gt; !; } #[start] fn main(_: int, _: **u8) -&gt; int { // manually nul terminated let s: &amp;str = "hello world\0"; unsafe { let (ptr, _): (*u8, uint) = transmute(s); printf(ptr); } 0 } #[no_mangle] pub extern "C" fn rust_stack_exhausted() -&gt; ! { unsafe { abort() } } Specifically: there's no mention of `char` or `unicode.rs`.
Added to https://github.com/mozilla/rust/wiki/Docs#presentations
For comparison, there's the [D language's support for class invariants](http://dlang.org/class.html#invariants). Also, I've always been leery of turning off safety features for the "release" build, since the release build is the thing exposed to users and thus has the most need of said safety features.
Ah, I failed to `s/quite/very/`. 3-1 and 3-2 indeed has the same degree of algorithmic complexity. I think it is not very useful to have only NF(K)D and not NF(K)C, just like having only uppercase mapping, hence the grouping.
To answer the other part of your questions, a static variable is something that has the lifetime of the program, and immutable ones are stored in the read-only data section. They are still only available in the scope that are declared in, but they do not go out of scope. Mutable static is always unsafe. The detailed description is available [here](http://doc.rust-lang.org/rust.html#static-items).
&gt; "It never generates -0.0 either?" That's never been a problem. Code with extraneous checks,however, was unacceptably slow. So if there was any difference between -0.0 and 0.0 that needed to be handled - it was the correct choice to overlook it. all a question of priorities... 
yes, but in a game on a console, you can't have something 'fail' and display some sort of error message. it either works or it doesn't - so the 'safety features' are useless. They can only be used as a debugging tool, by developers and the test department
ok i get it .. you're some sadist that wishes me to be cursed with header files forever.
Man this would be so cool! These things have been bugging me, so I'm glad you took the time to write them out.
Video games are increasingly connected to online services, and it's unacceptable to leak sensitive user data to an exploit. If you want unchecked indexing, use the unchecked indexing methods provided by Rust. The division between safe and unsafe code is fundamental to the language. The contract of the ***safe*** array indexing operations are that they are ***safe***, while the ***unsafe*** unchecked operations are correctly marked as such. I don't understand why this is a problem. C++ provides no way to turn off the `std::vector::at` bounds check. If you want sugar for the unsafe operation rather than the safe one, Rust is the wrong language.
If you click through to the actual description of the feature, it says that it only occurs in debug builds.
The vast majority of data is immutable - if it was created outside of your curated channel, it could be validated, once, on loading. repeatedly checking things that are immutable is a waste. 
Feel free to use the unchecked indexing operations provided by Rust whenever you're sure that the check is unnecessary. The compiler will already eliminate many of the checks or hoist them out of loops, but when you're sure you know better you can just start with the unchecked operations.
Write wrappers for your immutable data that use `unsafe` internally but expose a safe interface. (However, not that immutability doesn't imply safety, e.g. a vector can be immutable, but indexing into it still has to be bounds checked for safety, unless the indices are *guaranteed* to be in bounds.)
&gt; Change @-patterns to allow a pattern on either side. I could probably get behind this, but I'm not certain that it's really begging to be changed. Only allowing an identifier on a single side could be seen as enforcing consistency. I'd be fine either way, as long as it doesn't mess up Striegel's Device. :P &gt; Add alternation (|) to patterns properly. I'm not sure that `let Ok(e) | Err(e) = some_fn();` would ever be able to appear in practice. How often does every variant of an enum contain a value, where each of those values is the exact same type? &gt; Add pattern guards (pattern if condition) to patterns properly. It feels as though it would be difficult to come up with irrefutable patterns that contain pattern guards. I'm also concerned about the idea of this being allowed in function signatures. When I look at a function signature I want to see what types it accepts, and putting logic up there obscures this information. &gt; Treat repeated variables as implicit guards for equality. Hm. If equality comparisons like this are frequently occurring in the wild, then I could get behind this. But if they're rare, then I can see this being a gotcha down the road, as it makes patterns susceptible to typos. Somewhat magical.
&gt; Change @ to have a pattern on both sides instead of just on the right Question that should be discussed in the RFC: what happens for incompatible patterns (e.g. `None @ Some(_)`)? Also, note that the `@` matching comes from Haskell, which also allows only name binding on the LHS. &gt; Treat repeated variables in the same pattern as a guard for equality I'm not so keen on this, since accidentally putting two variables of the same name (e.g. typoing `(a,b)` to `(a,a)`) can dramatically change the behaviour of a program. I guess this may very rarely actually compile, since types wouldn't match, but that also means that legitimate uses of `(a,a)` would rarely work. I.e. I have a feeling that this is introducing complexity and scope for bugs without offering a lot in return.
&gt; Also, note that the @ matching comes from Haskell, which also allows only name binding on the LHS. Really? I was under the impression that Haskell allowed `pattern @ pattern`. Or was that OCaml with `as`?
&gt; Question that should be discussed in the RFC: what happens for incompatible patterns (e.g. None @ Some(_))? That would be an error—I’ll put a note in the RFC explaining that if I go ahead with it. *Edit: added note about incompatible patterns to the gist* And yes, Haskell *does* only allow identifiers on the LHS—but there’s no reason Rust can’t do better 😉 Regarding repeated variables, right now `let (a, a) = (1, 2);` doesn’t even throw an error. With this proposal, `(a, a)` in a `let` pattern would be considered refutable and not compile. Within a `match` it *might* compile, depending on the other arms. But in any case, with or without this proposal, something like `(a, a)` instead of `(a, b)` would not compile as soon as you tried to use `b` anyway. I just think that my proposed behaviour for `(a, a)` is intuitively what *should* happen—when I was new to Rust, I tried this, and was disappointed although not too surprised to see that it didn’t work.
The main reason I include pattern guards as part of regular patterns in this proposal is so that `match`s take nothing more than a pattern preceding the `=&gt;`. I can’t really think of a decent use for it, but maybe there is one! There is maybe a little too much magicity with the implicit equality guards, but I think there are a few uses for it—basically any time you check for equality in a guard, you can use this instead.
&gt; Striegel's Device. ?
I wasn't just referring to Result though, I'm having a hard time imagining when you'd ever end up with an enum of this nature. For example, if you had this: enum Foo { Bar(int), Baz(int), Qux(int), Quux(int) } ...it feels less redundant to have this instead: struct Foo { i: int, f: Foo2 } enum Foo2 { Bar, Baz, Qux, Quux }
How is static variable different from an immutable variable you define in the beginning of the main() function?
Yes! A while ago I would have totally needed &gt; Treat repeated variables in the same pattern as a guard for equality
It is not the difference of `static` vs non-`static`, but the difference of mutability. The problem comes when you have multiple threads of execution accessing the same memory location in a non-synchronized manner and at least one mutates the location. For multiple threads to access it you need *aliasing* which Rust prevents on `&amp;mut` references, however `static` by their very nature are accessible from anywhere and therefore *always considered aliased*. And since *aliased AND mutable* is unsafe, `static mut` is unsafe.
&gt; Treat repeated variables as implicit guards for equality Ah! I've hit that a couple times already. It seems so natural to express it thus.
&gt; I seem to recall that the main motivation for disabling exceptions in LLVM was that they wanted to disable RTTI which exceptions rely on, because RTTI itself involves a lot of overhead (and pressures the i-cache) whereas the tables of table-based unwinding can sit in cold sections and only be brought in if ever an exception is reached. Exceptions and RTTI are independent features. Table-based unwinding causes lots of missed optimizations and has enormous compile-time overhead. It's only zero-cost in the sense that it doesn't introduce any instructions to the non-exceptional code paths, but it can certainly makes them slower when optimizing. &gt; I did not know unwinding could cause missed optimizations, would you mind pointing me at some explanations of what is missed and why ? It adds lots of new flow control paths and anything able to unwind is impure. It cripples the compiler's ability to move around code and reason about it. &gt; Also, how does Rust proposes to support `drop` without exceptions ? Doesn't it require to forbid `fail!`(or turn it into an `abort`) ? It would mean aborting on logic errors (bugs) instead of unwinding. Failing in a destructor during unwinding and out-of-stack already abort.
OK, I tried this program and it worked: fn main() { static a: int = 1; fn print_a() { println!("{}", a); } print_a(); } Does it mean that `static a` is available from every scope? What happens when I define something static inside a function?
Nope, just name binding in Haskell: Prelude&gt; let f (Just _)@(Just _) = 1 &lt;interactive&gt;:3:15: parse error on input `@' I don't know anything about OCaml, but it seems that [`as` patterns there](http://caml.inria.fr/pub/docs/manual-ocaml/patterns.html) are also just name binding.
To call methods defined on a trait, the trait must be in scope: you must `use` it.
That's because `MutableMap` is already imported for you. This is an extract from `std::prelude` module, which is `use`d by default in every Rust module: #[doc(no_inline)] pub use container::{Container, Mutable, Map, MutableMap}; This means that `MutableMap` is used by any Rust module. Hence you can call its methods on every implementing type.
This is the only change that interest me, the rest seem to be unnecessary
[The prelude.](http://static.rust-lang.org/doc/master/std/prelude/index.html)
&gt; The first doesn't work because functions can't capture local variables. The second works because the literal "1" is static. But isn't print_1() also a local variable in the environment of test()?
No, it's not a local variable (others will be able to explain better than me what it actually is).
Adding a new type doesn't seem to buy you anything.
[This](http://www.google.com/patents/EP2633747A1?cl=en) is the only thing i could find for Striegel's device
Functions defined inside a function are still global (they have 'static lifetime), they are only in scope for that function 
I'm curious - what sorts of bounds checks exactly can't get optimized out due to unwinding? I'm having a hard time imagining a compiler optimization that would be inhibited by that - even in C++, any data accessible by some exception handler in a called function would also be accessible by the function itself. I'm probably missing something obvious, though.
Just as a FYI, look at [JMatch](http://www.cs.cornell.edu/Projects/jmatch/). It is a vast generalization of pattern matching that would allow these to be defined as library functions. It is an interesting point on the spectrum between pattern matching and full logic programming.
Actually i found my mistake. Apparently i was thinking i must misuse generics somehow in line: pub fn get_common_trait&lt;C:CommonTrait&gt;(input: &amp;'static str) -&gt; Box&lt;CommonTrait&gt; whereas everything works as i want to with: pub fn get_common_trait(input: &amp;'static str) -&gt; Box&lt;CommonTrait&gt; Are there other/better ways to implement something like this or would the be pragmatic Rust? 
You're using too many boxes…why does Struct1::new have to return a boxed value? That is unnecessary you can do that later if needed.
&gt; Treat repeated variables in the same pattern as a guard for equality I'm wary of this. I don't want patterns to invoke user code (`trait Eq`). I also don't want to have any wired-in primitive notion of equality distinct from `Eq`.
It's an item. Its placement inside another function only changes its visibility, it's still semantically a top-level function.
Statics behave the same no matter where they're defined; they just live in different namespaces. The namespace of the interior of a function is inaccessible to anything outside of that function, so in the example you gave, no code outside of `main` can directly access `a`.
I imagine it will get removed. I doubt there's much code that relies on it being a trait anymore.
I'm still new to Rust, so this isn't gospel, but `a` is in the dynamic environment (it's in `main()`'s frame of the call stack). [Named Rust functions do not support closures](http://static.rust-lang.org/doc/master/tutorial.html#closures), so you can't access another function's frame. Using `static` would be a closer analog to the function example. It's now "visible" to print_1, so it compiles. So this would work, although as far as I know it's not very idiomatic: fn main() { static a: int = 1; fn test() { println!("{}", a); } test(); } 
I can say that after having written over 300k C++ LOC in the last 10+ years, this is my single biggest gripe with Rust. Other than this, Rust is pretty much nicer than C++ in every possible way. People often complain about various parts of Rust here, in IRC and on the issue tracker but IMO we need to say the following more often: THANK YOU Rust devs for creating a truly awesome language! Disclaimer: I wrote the RFC.
Yeah I get confused a lot because sometimes the compiler does it automatically and sometimes it doesn't. I'm not sure if this applies but if there is a cost to derefencing non-raw pointers then removing the syntax might hide that cost (which would be bad) but if there isn't than this seems like great simplifying change. And leaving it for unsafe pointers makes sense if people want to play with low level code and know what they are doing.
Under this proposal, `let (10, x) | (x, _) = some_func();` would indeed work, much in the same way that the same pattern works in a `match` today. However, I don’t see how this relates to allowing arbitrary expressions that test for equality within patterns. I don’t find your code example very surprising—the following code: let c = 10; let (c, x) = some_func(); doesn’t look like it should behave in the same way as let (10, x) = some_func(); and indeed doesn’t with today’s pattern matching. Your `@` syntax seems interesting, and is indeed something I had previously considered putting in the RFC. However, I decided that it is adding too much complexity for something that can already be done with guards.
While there's a cost to derefing pointers, in practice it's not something you should worry about; the CPU will usually predict the deref and will pull in the data ahead of time. Even before that, the compiler might elide away the deref during optimization. And if all that fails and you hit the full cost of the cache miss, if that's the major reason why your algorithm is slow _and you care_ then you're already going to be careful with pointers in that section of the code. In other words, don't worry about it. Caring about the cost of a pointer deref is truly super-low-level, extremely rare optimization and this change won't really make it harder to fix those. Let's not make pointers harder to use 99.9999% of the time so that in those extremely rare cases, the programmer has a _very slightly_ easier time finding the last pointer deref. 
One of the main reasons ~ turned to box was because we didn't want to people to use the heap unnecessarily and without thinking about the costs but I think the major cost there is the allocating and freeing, not the derefing. But you are right, we need to be consistent - either get rid of * for safe pointers or make it needed everywhere, not some confusing in between.
Noob here, why can't the compiler just do this automagically? Or rather, why is it a bad idea to make the compiler do it automagically? Seems like it would be a good convenience feature and I've already seen two posts about errors from not bringing traits into scope.
Allocating &amp; freeing memory is orders of magnitude more expensive than derefing pointers, which is free most of the time, and not something the user should care about as a perf hit almost always. And if they do, they're already careful about pointers. I think we agree though; making allocation more explicit and in-your-face is important, while derefing pointers doesn't need the same treatment.
What do you mean by "cross-crate traits?" You can already do this: struct Foo; impl ::std::ops::Eq for Foo { fn eq(&amp;self, other: &amp;Foo) -&gt; bool { true } } If you mean something like this: use othercrate::Blah; impl ::std::ops::Eq for Blah { ... } I don't think that's going to happen -- trying to resolve what happens when two crates implement the same trait for the same type is just too messy, and you can already use wrapper structs (i.e. `struct MyBlah(Blah);`).
Can you send a function outside of its "visibility range"? Like, define a foo() inside baz() and make baz() return foo()? Or reference to foo()?
I really hate the `*` sigil. It just feels like I have to throw stars at the compiler until it's happy sometimes. Kind of like I have to throw `ref`s at it sometimes until it lets me pattern match. Plus, it's the same symbol as multiplication, so I hate having it around in my code. As you can tell, I'm not a C/C++ programmer.
Note that unicode is not utf8. There are actually other things that unicode support would touch: * string length (this one is pretty easy with most encodings) * character classification (e.g. isDigit, isWhitespace, etc.) * regular expressions: classes like \d, \w, ... (probably implemented in terms of the above classification functions) * text rendering/display (e.g. in piston or other UI libraries; this is more in the scope of those libraries)
Actually, what i was going to implement was to have a the CommonTrait being in a task and the owner of the Struct and other tasks communicate with it or even directly access it read-only, i don't know yet, what is possible ;) Somehow i thought that unboxed values live on the stack of a single task and cannot be accessed from another task but maybe that's also not true for boxed values anyway.. 
Wow, this will be really helpful. Could use more samples, though. :-) edit: I'd also suggest putting a link in the sidebar.
One of the biggest things that came out of all the discussion was changing closures. Part of the brokenness was because there was a hidden extra kind of pointer only used in closure captures, and the new by-val upvar capture doesn't have the &amp;only pointers any more. (as far as I understand, I haven't explored this in too much depth)
&gt; Note that unicode is not utf8. I don't think I ever said/meant that: the first item reads "a support for Unicode character in at least one **Unicode transformation format**", which includes UTF-8. &gt; string length (this one is pretty easy with most encodings) There are multiple ways to define a "string length", and most of them are not very useful. Even the grapheme length as in [UAX #29](http://www.unicode.org/reports/tr29/#Grapheme_Cluster_Boundaries) is of the limited use (well, Twitter character limit?). The most *useful* length is a UTF-8 byte length, though its current name `len` is quite confusing and there were some debates on renaming that. &gt; character classification (e.g. isDigit, isWhitespace, etc.) Well, I forgot to mention that; think as if the second item includes it. In practice though, additional character properties would compress well with the identical record optimization. &gt; regular expressions: classes like \d, \w, ... (probably implemented in terms of the above classification functions) [We already have them](https://github.com/mozilla/rust/blob/master/src/libregex/parse/unicode.rs)! &gt; text rendering/display (e.g. in piston or other UI libraries; this is more in the scope of those libraries) Yes, in general text segmentation, Bi-di handling and other specialized algorithms are beyond the scope of the standard library.
Thanks. So, you're saying it's all been made moot by the unboxed capturing work?
I agree that my main point, to pattern match on existing variables, is different from your proposal. I like your `(a,a)` feature either way. A potential problem with your proposal though. The following redefines `c` today, right? let c = 10; let (c, x) = some_func(); What about this then? Is this valid today? And would it use the expression, and be equivalent to `(10,x)`? let c = 10; let ((c), x) = some_func(); Assuming yes, then your proposal would change its meaning, right?
&gt; Note that unicode is not utf8. I just wanted to clarify this for readers of this thread, sorry if you misread my intention. Otherwise full ack. ;-) And thanks for the link to the unicode regexps. Cool thing, that.
I don't know if a consensus was ever reached, except that closures needed a reform. I think it became clearer towards the end that mutability as it is now is okay.
The code is public, so people can/should submit nice examples: https://github.com/rust-lang/rust-playpen/tree/master/static/sample
`((c), x)` would have exactly the same meaning in a pattern as `(c, x)`. (Today brackets are only used for tuples, so no, that would not compile today.) In my proposal, brackets gain the extra meaning of grouping, which has no effect whatsoever on the actual semantics.
Yes, you can do that, but why would you?
No reason, I'm just messing around... You can't do the same with a closure, right?
&gt; edit: I'd also sugges tputting a link in the sidebar. Good idea, but I'll wait until it's at `play.rust-lang.org` (i.e. no `test`), and also for [a bit of beautification](https://github.com/mozilla/rust/issues/14575).
Depending on what you want to achieve your first idea would have worked just fine, you just return the concrete type from the function instead of a boxed trait object. This way you can take advantage of Rust's powerful generics: impl Struct1 { pub fn new() -&gt; Struct1 { Struct1 { x: 1 } } } impl Struct2 { pub fn new() -&gt; Struct2 { Struct2 { y: 2 } } } pub fn get_common_trait&lt;C: CommonTrait&gt;(input: &amp;'static str) -&gt; C { match input { "x" =&gt; Struct1::new(), "y" =&gt; Struct2::new(), _ =&gt; fail!("No implementation for {}", input) } } Also implementing the new() methods for Struct1 and Struct2 as returning the bare type rather than boxing it at this point allows you to choose the method of allocation when creating them: let struct_stack = Struct1::new(); // stack allocated let struct_heap = box Struct1::new(); // heap allocated
You can, but that doesn't make it possible to do things like function decorators, as in other higher-level functional languages. I guess it could be used to cache the desired action after some expensive decision, however: fn main() { let action = decide_action(); // call action multiple times } fn decide_action() -&gt; fn() { if expensive_decision_that_doesnt_change() { return action1; } else { return action2; } fn action1() { /* ... */ } fn action2() { /* ... */ } } I guess whether going through the function pointer (does that classify as a virtual call?) is cheaper than making the decision every time will change with the context.
&gt;You can, but that doesn't make it possible to do things like function decorators, as in other higher-level functional languages. I'm only aware of Python's function decorator, where `@decorator` is just sugar for `foo = decorator(foo)`. If you can have functions taking and receiving other functions in Rust, I think you can have a decorator as well, no? Or are you talking about some more complex concept?
I meant function decorators as in Javascript. I'm not very familiar with Python, so I'm not sure if that's the same thing. In JS, it's possible to do this: function callTwice(f) { return function() { f(); f(); } } With Rust functions (AFAIK) that's not possible, because you wouldn't be able to capture the `f` function. You also can't do that with closures, since they're tied to the stack frame, and as such can't be returned from the parent function (that will change when unboxed closures land). I was able to do that with a `proc()`, but that involves heap allocation: fn call_twice(f: fn()) -&gt; proc() { return proc() { f(); f(); } }
Note that the following program works (if you define "works" as "recurses infinitely until the stack overflows"): fn main() { foo(); fn foo() { bar(); } fn bar() { foo(); } } Functions defined with `fn` are not closures. They are accessible from anywhere in their scope, and their behavior is identical regardless of the placement of their definition. This is why it wouldn't make sense for them to be closures: at what point in the function would they close over anything?
&gt;I meant function decorators as in Javascript. I'm not very familiar with Python, so I'm not sure if that's the same thing. I think it is the same, your example can be done in Python as well.
&gt;Functions defined with fn are not closures. They are accessible from anywhere in their scope, and their behavior is identical regardless of the placement of their definition. This is why it wouldn't make sense for them to be closures: at what point in the function would they close over anything? I understood this, but now I don't understand why decorators don't work. If a `fn` doesn't close over anything, what's the problem of calling it from anywhere? This program doesn't work: fn main() { fn foo() { println!("hello") } fn baz(arg: fn()) -&gt; fn() { fn call_twice() { arg(); arg(); } return call_twice } let foo = baz(foo); foo(); } With this error: **can't capture dynamic environment in a fn item; use the || { ... } closure form instead.** What environment is it trying to capture?
I still don't see the point of this change. ~ was already a zero-overhead allocation and beutifully simple. I don't want to be too negative, but making this uglier just to get at detractors who complain about "too many pointer types" is a big misstep. Even in languages like C or C++ you don't and you shouldn't think about performance in each line you write. You *can* do that, but it's not productive. Most programs have plenty of room for indirections when it's convenient.
&gt; ~ was already a zero-overhead allocation AFAIK that was heap allocation, and thus far from zero-overhead. &gt; and beutifully simple Part of the problem is that it was also a bit simplistic for the use case: It doesn't support alternative allocators.
Zero-overhead means that it had no header/padding anymore (which was the case in early Rust.)
But in your example, `foo` is referred within the body of `bar` (and `bar` within `foo`) without either of them having any parameters.
I'm not saying it's free. The view in this thread seems a bit fanatic on the "no allocations at all" side, which I don't understand. Sometimes it's useful.
On the contrary, I believe the view in this thread is "make allocations obvious". If allocations weren't useful, they just wouldn't be in the language.
But a ~ is not invisible.
Ultimately, it was less visible than we would have liked. Heap allocation is not a panacea, and it was too easy to blindly toss sigils around in a misguided attempt to appease the compiler.
I disagree with the fix of making it more cumbersome to read in code. Recursive structures are often a good way to solve a problem at least in the prototype stage, and they do exist in Rust, say for example the Tree containers. So while the enum types in Rust are beautiful, recursive ones are no longer that. I also think your argument is a bit too vague, I mean, we can try to catch beginner mistakes but how far should it go? This is a successor to C++ after all. Sure it will be both better and smarter and nicer to work with, but it also allows that possibility of deciding between allocation style etc, so it does require the programmer to think.
Having never seen a Cap'n Proto schema file before, I'm amused at how coincidentally similar its syntax is to Rust. :P
As a successor to C++, `box` may be longer than `~`, but it's still far shorter than `std::unique_ptr` and `std::make_unique`. :)
Whoa, this is super cool! Can it safely execute arbitrary code? If so, I wonder if http://rustbyexample.com/ could provide a link to run each of its examples in this environment.
True, but the main problem is not `box xyz` but `Box&lt;MyType&lt;T&gt;&gt;`; I don't exactly love `&lt;&gt;`'s, even if there is support for closing `&gt;&gt;` built into the language from the start :)
Thanks for the clarifications.
&gt; Can it safely execute arbitrary code? The sandbox implementation is linked to from the [rust-playpen](https://github.com/rust-lang/rust-playpen) repository. &gt; If so, I wonder if http://rustbyexample.com/ could provide a link to run each of its examples in this environment. It exposes a JSON API with CORS enabled along with understanding a GET query like http://playtest.rust-lang.org/?code=fn+main()+{+println!(%22foo%22)+}&amp;run=1 in the frontend.
Brilliant! This makes it so easy for newcomers to experiment. Great work!
I'm still not exactly certain what your question is... are you just trying to get your example to compile? This works: fn main() { fn foo() { println!("hello"); } fn baz() -&gt; fn(fn()) { fn call_twice(arg: fn()) { arg(); arg(); } return call_twice } let qux = baz(); qux(foo); } If what you're looking for is the ability to have a function that takes input dynamically and returns a function that operates on that input, you'll need to use a closure for that. But returning closures currently has harsh restrictions that we're working on lifting as part of the ongoing closure reform, so depending on what you're trying to do it might not currently be possible. If none of this answers your question, then feel free to implement a program in some other programming language and I can tell you how it would be done in Rust.
I'm just trying to understand the rules in Rust. In Python this is possible: def baz(arg): def call_twice(): arg() arg() return call_twice def foo(): print('hello') foo() #prints 'hello' foo = baz(foo) foo() #prints 'hello' twice BTW, I had a mistake in my example, it was supposed to be `let foo = baz(foo);` of course. I edited it.
This works in Python because `def` creates a closure. Python has no notion of bare functions that exist without an environment, as Rust functions are. Note that the following program works today, but probably not in the way that you expect: fn main() { fn make_call_twice(f: fn()) -&gt; proc() { let call_twice = proc() { f(); f(); }; return call_twice; } fn foo() { println!("hello"); } let foo = make_call_twice(foo); foo(); } Note that a `proc` is a closure that is allocated on the heap, so it can easily be returned from a function. But procs have another property: they can only ever be called once. So if you tried to call `foo` again up at the end of that example, you'd get a compilation error. There are good reasons for this, but nobody's quite satisfied with the current design and this is why we're revamping it. In the future, `proc`s will probably be going away entirely and replaced with a regular closure. Thus, in a hypothetical future Rust, your example would look like this: fn main() { fn make_call_twice(f: fn()) -&gt; || { let call_twice = || { f(); f(); }; return call_twice; } fn foo() { println!("hello"); } let foo = make_call_twice(foo); foo(); } Note that the `proc` has been replaced by a regular closure. And unlike current Rust, this version would let you call `foo` as many times as you want.
I think I understand what you are explaining. I just was confused because I assumed that the so called *bare functions that exist without an environment* should **not** be able to call other bare functions that exist in the environment. But if they apparently can, then why can't they also take them and put them inside another function they define? I guess it doesn't quite make sense to me logically because I don't understand the technical implementation.
[Algebraic types][1] [1]: https://developer.apple.com/library/prerelease/ios/documentation/Swift/Conceptual/Swift_Programming_Language/Enumerations.html#//apple_ref/doc/uid/TP40014097-CH12-XID_189
`let` for immutable variables and `var` for mutable variables. That's actually quite clever. The `?` for nullable types is from Ceylon if I remember correctly. edit: Also this example from their book looks really familiar: enum OptionalValue&lt;T&gt; { case None case Some(T) } 
nothing new, scala has val/var
Just based on skimming through the documentation... it's not exactly Rust, but given the size of the whole space of programming language design and the number of different directions they could have gone in (and where they were coming from: Objective-C), it's *remarkably* close. The biggest philosophical difference seems to be that it's somewhat higher level, using ARC for memory management. Considering also Microsoft's "M#", it seems like everyone is on the same page with regards to the direction programming languages should be moving in, which is encouraging. I wonder how long this has been in development.
At a glance: Similar: * Swift's protocols look somewhat like Rust's traits, which both look like Haskell's typeclasses. * Both use Option instead of null. * A lot of sameish syntax choices (type annotations come after variable names, braces, no parens around conditionals). * Statically typed with local type inference. * Bounds-checked arithmetic and array access. * No/few automatic coercions. * Forced initialization. * ADT's via enums. * Pattern matching. * Generics. Different: * Swift doesn't have a concurrency story (or at least hasn't told it yet), Rust does (tasks, no data races, channels). * Swift looks like it just uses stack allocation and Arc for memory management; Rust gives you much more control. * Swift semicolons are optional. * Swift uses separate keywords for defining value and reference types (struct vs class). * Rust has macros; the Swift book doesn't mention any metaprogramming features. * Rust is open source and already works on a bunch of platforms, Swift looks like its going to be proprietary and only work on Mac and iOS. * Swift will automatically get massive adoption, Rust will have to compete on its merits. * There's some pretty impressive tooling available for Swift out-of-the-box.
it looks like some rust/C# child
&gt; The biggest philosophical difference seems to be that it's somewhat higher level, using ARC for memory management. That's a huge difference though. Rust is *about* safety without garbage collection. (Reference counting is a form of garbage collection.)
Thanks! I had a suspicion from what I'd read that Rust pretty much outclasses it in every way I'd care about, but I didn't have the knowledge to back that up. I do realize you're most likely biased to some extent though :P.
&gt; Rust pretty much outclasses it in every way But Swift is still a huge improvement over Objective-C.
No, `let` and `var` in Javascript don't convey any information about mutability. `let` just allows you to declare variables with a scope that you expect, as compared to `var`'s ridiculous hoisted-function scoping.
Ah, but Obj-C doesn't use cycle collection, no? I thought they were all about weak pointers. Without CC, it's much less arguably a GC... though I still don't think it'll ever be a direct competitor to Rust. :)
Yes, I know they are the same size as a pointer, but the pointer size is not fixed by the language -- it varies from one platform to another. So coding for int you can't rely on it being 32 bits long because it might be 64 bits long. So if you multiply a large width*height on 64-bits it will be a big number, but on 32-bits it might well overflow. (This example from the mozilla bug posted above). So it seems reasonable to trap in this case on 32-bits, if the hardware supported it. The programmer's assumptions have been broken. There is no way the programmer could expect 32-bit wrapping maths when int could be 64-bits long. If you're currently relying on getting 32-bit wrapping maths from an int, then that is surely a bug ... right? (I can't see how it isn't.) Although you might be multiplying and then masking afterwards, and yes trapping would break that ... although maybe LLVM could detect that case. It would be better to use a fixed-size int (u32,u16,etc) for that kind of thing anyway. Anyway, Rust has much bigger things to worry about, but the point of this thread was how to get integer overflow trapping into more common use. This was the only way I could think of, given the need for also building for platforms without these checks. If you're going to use trapping at all, it needs to be on a common type, not a special type which no-one ever uses, or that people avoid because it has overheads on some platforms. So it needs to be a 'might wrap or trap, depending on CPU support' type. I can't see any other way given the performance emphasis of Rust. Either that or we forget the whole idea (most probable outcome).
Swift appears to have a `nil` concept, mentioned in [the docs regarding initialization](https://developer.apple.com/library/prerelease/ios/documentation/Swift/Conceptual/Swift_Programming_Language/Initialization.html#//apple_ref/doc/uid/TP40014097-CH18-XID_266). I'm not sure how it relates to conventional `null`, but it appears not to provide the compile-time guarantees offered by Rust.
&gt; The ? for nullable types is from Ceylon if I remember correctly. Vala had it earlier. 
Swift's `nil` is like `None` rather than `null`. It's only assignable to optional variables. 
AFAICT it appears comparable to `None` in Rust, but built-in to the language rather than defined in a library.
You're looking for /r/playrust. This subreddit is dedicated to Mozilla's Rust programming language.
Ah, cool. Thanks. I saw a snippet somewhere of `if variable == nil` and thought it was analogous to the same usage in Lua.
I prefer `let mutable` over `var` as it's longer to type so people will default to immutability through laziness. 
It reminds me a lot of Scala, but given that Scala has heritage in both ML and Java... I can totally see the rust/C# child connection.
Ah, you're right. I lost my head. The `let` keyword brings block scope.
Sure. But going in, I might've expected much bigger differences. (Oh, say, dynamic types.)
I was reading this earlier and it's very cool, but I didn't see if you could declare recursive types with it. Like: enum Expression { case Add(Expression, Expression) case Sub(Expression, Expression) case Value(Int) } I certainly hope it does because that'd be the worst implementation of ADTs ever, if it couldn't. **EDIT:** As pointed out, rust doesn't support ADTs as they are also written here and requires some pointer type to make this work. When writing this example I had that in mind and didn't make it clear. Swift doesn't appear, at least from my reading about it, to really offer any pointer types or any way to box up a value type and store it on the heap. Which leads to my concerns over being able to express recursive structures using ADTs.
I said rust mostly because of almost identical function definition syntax
Rust never sleeps. Rust waits. *And listens.*
I hadn't even looked at that, but you're right. That's very similar syntactically.
Weekend project: Learn Rust while creating a simple game :) I'm liking the language, but still lots to learn. Will probably do a more complex game.
I can imagine it having other frameworks, if it's ever opened.
Whoa -- the new Apple Swift language uses trapping integer arithmetic, and has special operators &amp;*, &amp;+ etc for the wrapping operations.
Rust doesn't allow that either, since it would be infinitely sized. It needs a pointer wrapping the sub`Expression`s to give `Expression` a well defined size.
It was more of an example (since I don't know Swift). I know that rust requires it to be a pointer wrapping the sub-expression and I was wondering how Apple handled the similar need for recursive ADTs in Swift. I see that structs are value types and classes are reference types (very similar to C#) and one would assume enums would be value types for efficiency reasons, but I haven't been able to find anything in the Swift documentation about boxing up value types into an ARC so that they can be used to build recursive structures like expressions.
Wrong subreddit.
LLVM was done as a research project by Chris Lattner at a university; so yes, LLVM was indeed open sourced before Chris went to Apple (so it's not an apple product, but they use it extensively). Clang was made by Apple and open-sourced by Apple, which hopefully they'll also do with Swift (I don't see why they'd keep it closed)
its almost 'Objective-Rust'? (superficially) but,unfortunately, it doesn't seem to have the ability to drop down to true low level code with raw-pointers etc like Rust unsafe blocks? 
application/systems shouldn't have to be mutually exclusive IMO; just imagine if they added unsafe blocks to swift. it looks like its got the generics required to implement collection classes
This looks like an equivalent of Vala for Apple's legacy ObjC system (and Vala itself is probably the closest language to it, except for the Scala/JS/Rust-inspired instead of C#-inspired syntax). Just like Vala, it's probably going to greatly reduce the pain of using the ugly ObjC language interface, but is not going to be suitable for serious work due to lacking both GC and solid non-GC memory management. The result of that choice is that any complex program will easily run into cyclic garbage, FFI reference counting bugs (failing to correctly declare whether a C function takes/gives ownership of RC pointers), and bad performance due to reference counting everything. Of course this one has better backing than Vala, so it might turn out a bit better. 
It's been on my mind for a few hours now, but I'd really like to ask some people who know much more about Rust than I do: * Do you think Swift usurping a nonnegligible or even a significant part of the would-be target audience for Rust is a realistic scenario? From what I hear, Swift is single-platform now, but who is to say it will stay that way?
closures are compatible with Obj-C Blocks, so I assume GCD should "just work"
Rust's quasi moto as of late seems to be "Memory safety without garbage collection". Its a little unclear to me if Swift is actually memory safe, or just more memory safe than Objective-C. It also appears to me that its not possible to write non-trivial idiomatic Swift code without using reference counting, which is a form of garbage collection. So, my initial reaction is that it doesn't seem to be competing in the same space as Rust. It might pick off some developers that would have otherwise decided on Rust for a particular project, but I don't think its going to replace the need Rust. It does look pretty cool though.
No, they have completely different use cases.
Now almost all examples [1] have a link [2] that reads "try `hello.rs` in the playpen!" that will load the source code of the example into Rust [playpen](http://playtest.rust-lang.org) and execute it, so you can tinker with it in your browser! [1] I excluded examples that timeout (timers) and examples that touch the filesystem (I/O and `io::fs`) [2] Perhaps it'll be better if I used a button instead of a text link, but my drawing skill are poor :P
There's a link that says "try `hello.rs` in the playpen!" under the source code of the example.
The playpen is actually using a CORS-enabled JSON API, so it would be possible to have a "run" button that shows the output inline, and even make the code itself editable in place. (I'm not sure of the details of the API.)
Quite true. Clang was also the backbone for Objective-C, since GCC wasn't making it a priority. Microsoft got away with a closed environment until now, so maybe Apple will do the same.
Can we please use `ARC` when talking about "automatic reference counting" and `Arc` only for our "atomic reference-counted smart pointer"? Preferably, we should spell out the one that is less relevant to Rust.
&gt; They open-sourced Clang because it gave them the manpower to quickly get a working C and C++ compiler up to snuff. Really? How much stuff was contributed by non-Apple employees?
You might be interested in [Piston](https://github.com/pistondevelopers/piston/)
I first saw the `?` for nullable types in C#. It didn't really serve the same purpose, though: it can be used to make value types nullable.
Being a C++ programmer I've been waiting for this since I discovered Rust.
What about the look of disapproval? ಠ_ಠ Having that stare at you from all your unsafe code would make you think twice.
That would be interesting to know, but ultimately it doesn't matter. When faced with the problem of needing to write a high-quality C compiler from scratch, making your code repository public isn't going to make you any *slower*. At worst, your number of community contributors is zero. So when making the decision to open-source the code, what was there for Apple to lose? C was a language that already had several high-quality implementations, so it's not like they were going to be able to gain any competitive advantage by keeping it a secret. This is in contrast to Swift, where the argument could absolutely be made that secrecy confers a competitive advantage. How would Apple gain by making it easier to make Swift code run on non-Apple platforms? I certainly hope that this doesn't end up being the case, but Apple as a company is not exactly renowned for their openness.
Preferably, we should just change the name of our library type since it's doing us more harm than good. `AtomRc`. Make it happen.
"Closed-source language" sounds like a contradiction in terms. 
&gt; it just did not seem like anything especially new to me as compared to existing languages I think it wouldn't necessarily be a good thing, if they included anything that wasn't in any existing languages to date.
&gt; but is not going to be suitable for serious work due to lacking both GC and solid non-GC memory management. What qualifies as serious work if nearly all Mac apps and all iOS apps are excluded?
Not going to read through the manual again for the right syntax, but I would guess you could do it by wrapping it in a `class`, given that classes are reference types: class ExpressionC { exp: ExpressionE; } enum ExpressionE { case Add(ExpressionC, ExpressionC) case Sub(ExpressionC, ExpressionC) case Value(Int) } EDIT: Or to make it more general... class ARC&lt;T&gt; { val: T } typealias Expression = ARC&lt;ExpressionVal&gt; enum ExpressionVal { case Add(Expression, Expression) ... etc. ...
Plus you'd also have to implement `Eq` first ;)
&gt; Unclear how or if you're allowed to implement the iterator protocol yourself. A bit hidden, but pretty clear: “A for-in statement allows a block of code to be executed once for each item in a collection (or any type) that conforms to the Sequence protocol.” &gt; Visually confusable .. and ... operators for exclusive and inclusive ranges. That's a fun troll - Ruby has the same, but reversed: `..` is inclusive and `...` exclusive.
For any case where you need a language that gives you low-level control, only Rust will be appropriate. But besides being lower-level, Rust is also superior in terms of expressiveness, ergonomics, correctness etc. to basically any of today's (ostensibly "higher level") mainstream languages. If Rust were already a mature language, and in the absence of any externally imposed compatibility constraints, for any given project my decision would almost surely come down to either Rust or Haskell. Along those lines I think Swift could be a competitor to Rust not in the domain of low-level languages, but in the domain of languages which don't suck.
have you found any decent info on what these can do? Just been looking at what comes up in the playground.. ... I see its' got a 'move' method, some things for initialising from others.. but can you do arithmetic on it, and recast pointers to values... e.g. to implement an arena allocator? does it have 'sizeof(T)' equivalent for pointer arithmetic? I almost got the impression these exist mostly for providing types to pass/return from FFI, rather than actually implementing unsafe code? 
Will this replace `unsafe` or will it be used to say "stop whining and just compile my code"?
I hope this rfc is accepted, and implemented!
It is making `YOLO` a synonym for `unsafe`, e.g. YOLO fn foo(x: *int) -&gt; int { *x } YOLO { foo(0 as *int) }
Drawing? You can just use something like "&gt; Run in PlayPen^(TM)" where "&gt;" is a play icon (well, I think Unicode would also work). This is awesome nonetheless :D.
Which isn't really enough. I propose *replacing* `unsafe` with `YOLO`: https://gist.github.com/nathan7/a549d0c312ee76b03127. This ensures that if you use unsafe code, *anyone* who sees it will question you.
I think it depends on *why* they're choosing the particular language (in your example C++). C++ and Rust give you strong low level control. Objective-C does so only to the extent that it incorporates C. Swift doesn't really. If you need low-level control, you will care about this. If you don't, however, and get to choose based on language quality more generally, then the situation is like this. Objective-C really sucks. C++ sucks considerably less. Both Swift and Rust suck tremendously less.
I aliased exceptions to ಠ_ಠ in a C# project at work.
&gt; We are also experimenting with a new design that replaces the Game trait with an iterator. That sounds quite interesting; iterators are a good match for FSMs. What would Game iterate over?
I didn't really read it before, but it's obvious. This is probably the best thing I have seen today. I'm all for it!
It will iterate over the events in the game loop, such as rendering, update etc. The iterator takes care of the FPS and UPS, so you can write a game with the stack environment. Example https://github.com/PistonDevelopers/piston/blob/master/examples/image_iter.rs
Sounds cool, but I'd wager most implementations will keep the event matching as a sparse method, calling out to actual render(…) etc. methods, perhaps on a "scene"/"chapter"/"act" implementation. Would it make sense to have a default implementation for this? Otherwise I think that exiting the game loop on a "None" event looks strange. I'd rather have a "Exit" or "Quit" event. Also would it be possible to generate / schedule events within the code, perhaps in a separate task?
Rust needs to actually be released and prove itself first before anything else happens. If there are advantages to the way Rust deals with memory, it will slowly gain in popularity. Memory safety may be useful, but it isn't particularly flashy, swoshy or spiffy. (Unlike Swift?) 
The original intention was to use a `for` loop, but we can't since the `Iterator` trait does not got lifetime constraint on the `&amp;self` parameter. It could use some more design, somebody suggested using a macro. I'm most concerned about getting stuff up and running at the moment. Please open issues in the project about the stuff that interests you, all contributions are appreciated.
Also: I need iTunes in order to download the [language manual](https://itunes.apple.com/us/book/swift-programming-language/id881256329?mt=11)?!
Should this example work (with `unsafe` instead of YOLO)? It's not working.
[It needs to be inside a `fn main`](http://playtest.rust-lang.org/?code=unsafe%20fn%20foo%28x:%20*int%29%20-%3E%20int%20{%20*x%20}%0A%0Afn%20main%28%29%20{%0A%20%20%20%20unsafe%20{%0A%20%20%20%20%20%20%20%20foo%280%20as%20*int%29;%0A%20%20%20%20}%0A}). (Change the `-O2` to `-O0` to see it segfault.)
IDK if it competes, because it looks like swift might only be usable on OSX/iOS.
See the link in my previous comment.
Cool, thanks. Missed the linked.
No: https://developer.apple.com/library/prerelease/ios/documentation/Swift/Conceptual/Swift_Programming_Language/index.html#//apple_ref/doc/uid/TP40014097-CH3-XID_0
Language is defined by its specification, not by its reference implementation.
I believe that the only point for Rust on iOS is low-level cross-platform development instead of C++ - so you can create a foundation and provide platform specific wrappers for Android/iOS/other mobile platforms. It means that while Swift is Apple-only language it doesn't intersect with Rust at all. 
I believe that's how arrays and strings behave in ObjC as well (which would make sense, as I imagine many of the core swift objects are just cocoa objects under the hood); both are basically values on the heap, and methods return new copies (which in the array case just means copying pointers and incrementing retain counts) with the requested changes instead of mutating the input object itself. In much of Cocoa, the mutable version of a data type is a subclass of the base, immutable version. You can pass in an NSArray to a method, which makes a mutableCopy to get an NSMutableArray, operates on it, and returns it as an NSArray so the caller doesn't know the interface to mutate the object is available. Of course, Objective-C is a dynamic message-based system, so the caller could technically send a message that NSMutableArray implements but NSArray doesn't and it would succeed if the returned object really is mutable.
Yeah, when I got to thinking about it more this is what came to mind.
I'd wait to declare it closed-source until Yosemite ships. I don't think Apple pushes new code to open source until the corresponding OS has publicly released. I've seen a few tweets from members of the language team at Apple calling for bug reports and feature requests on the language, specifically saying they want the developer community to be involved in the language going forward. That combined with the fact that the language is basically a new clang front-end that links against Cocoa/Foundation, it sure seems more in their interest to open source the code along with clang/llvm than to keep it closed source. There's an open version of some of Cocoa/Foundation out there anyway.
I assume the implementation in the language will literally just be sugar around GCD anyway. I'm personally just interested to see what form that syntax takes. the libdispatch queue system is just a little bit of bookkeeping (and a channel type) away from go-style concurrency.
As much as I personally like the lengths matching between `let` and `var`, I agree with you that the added keyword is a good way to nudge developers.
The way I see it, if you don't care about low-level control, then the arena suddenly becomes intractably crowded. Suddenly you're competing against Python, Clojure, Scala, Ruby, Haskell, Ocaml, Go, and, really, pretty much every other programming language out there. Rust won't gain a foothold in that environment (and nor will any other single language). The sacrifices necessary to wrest low-level control have severe costs in other areas, which we gladly accept because we understand the importance of our niche. So sure, you could consider that we're a competitor to Swift, but only in the sense that we're a competitor to every other language out there for anyone who's considering a greenfield project with no constraints.
Even if they did introduce unsafe in Swift, it wouldn't have the same deterministic memory management as Rust. Besides, since Swift is inheriting a lot from ObjC, it's very likely that the runtime is heavy. Although I must admit that I *can* imagine Rust being used in application-level software effectively.
I watched it, I liked it, but it tried to cover too much ground too quickly. Kind of felt like everything was being rushed through.
If those are your criteria, then Haskell and Ocaml have already beaten us to maturity. Nobody who is currently using either of those languages ought to have any interest in Rust. &gt; Do you have anything in mind here besides explicit reference types and clone()? Explicit lifetime annotations, closures that are nowhere near as general as in a dynamic language, explicit macro invocations, no canonical "dictionary" type, and our bountiful cornucopia of attributes. Not to mention the syntax concessions made to appeal to C++ users, which are all but a requirement to compete in this space.
This thread has been linked to from elsewhere on reddit. - [/r/ProgrammerHumor] [yolo.patch: Add support for the YOLO keyword to Rust](http://np.reddit.com/r/ProgrammerHumor/comments/277ffc/yolopatch_add_support_for_the_yolo_keyword_to_rust/) *^If ^you ^follow ^any ^of ^the ^above ^links, ^respect ^the ^rules ^of ^reddit ^and ^don't ^vote ^or ^comment. ^Questions? ^Abuse? [^Message ^me ^here.](http://www.reddit.com/message/compose?to=%2Fr%2Fmeta_bot_mailbag)* 
This only works if you compile with the flag U+1F4A9 PILE OF POO enabled
&gt; Memory safety may be useful, but it isn't particularly flashy, swoshy or spiffy. the big deal is that Rust really does eliminate the need for C, with unsafe blocks - from what I've seen so far, i'm not so sure if Swift does. Swift only seems to be safe by using stack &amp; refcounting, whereas Rust does everything C..C++11 does, but safely.
Will any of Apple's (anticipated) LLVM changes for Swift be useful for Rust's compiler? Or do you think Apple only needed to write a new front-end?
It's still up in the air as to what "idiomatic Swift" looks like, but it definitely has everything it needs to make it as a functional language.
That *is* nice! Weird to see Perl being pretty, but credit where it's due, I like it.
I've got the impression that the devs are currently stripping the language down to its bare essentials, in preparation for a nice clean 1.0 release. Once 1.0 is out, we'll all have a lot more time to spend bikeshedding about the syntax :P
In a way this is a failure of rust - why would rust not be useful for full app development, while Swift is? Can rust not have wrappers for accessing iOS apis?
&gt; Can you give an example of this? I find Rust's closures very notation-light already &gt; yes I prefer Rusts closure syntax - but 'do' notation reduced *nesting* - functions that take closures become more like writing 'custom control flow'. if the nesting isn't a problem, why don't people demand "if(condition,{put your code here...});" or "while(condition, {..});" etc. Whenever I cheat on the brace - merging a ({ and }) as one indenting level, I always regret it when moving something later. My use case would have been passing closures around in data-parallel iterators; for each, par_for_each looking similar in invocation. Seems like closures are a great tool for reducing the need for unsafe code. If I could only have one out of Option-acess sugar or do notation, I'd go for the latter.. as it can help with the former problem. do if_some(my_option) |x|{...} &gt; Those will fall off of the Indexable trait revamp IIRC I wasn't talking about the accessor- I was talking about the syntax for declaring vector collection types; these keep signatures easier to read - between vectors and assoc-arrays, you can do most common tasks; the sugar allows writing types with less nesting, so its not just about reducing characters. well r.e. nesting , perhaps there's a text editor solution like variable indent levels .. (indent 2 inside a match..). similarly a text editor can substitute shorter symbols e.g. emacs 'pretty-mode' , but the nesting of angle brackets is harder to deal with 
I wouldn't go as far as saying "a failure of rust" - this has happened more because Apple have a strategy of creating artificial lock-in - not for any choice by the Rust team or community. The extent to which Apple copied Rust shows how suitable Rust would have been :) .. but I wholeheartedly agree that App and Systems needn't be mutually exclusive- it should be possible to create a language who's primary purpose is Systems, but additional features make app development pleasant aswell. (the division between unsafe/safe goes a long way already, IMO) Some choices in swift - like default params and sugar for data literals - will make swift more pleasant for app development - and those needn't conflict with efficiency IMO (don't pay for what you don't use)
Maybe Apple would be more likely to actively sabotage any demands Rust might place on LLVM... :(
Yeah, I would rather see a solid, if small, 1.0 release than wait longer for the devs to work out kinks in the sugar. Especially because rust macros don't make me want to kill the language designers like c macros do.
I'd argue that a many of these get handled *well* by the macro system. For example, literals for data types and matching over special cases of e.g. Option: let mymap = hash!{ 1: true, 4: false }; let sum = option_let!{ x = mymap.find(&amp;1), y = mymap.find(&amp;4) in x &amp;&amp; y } these are, at least in my view, as good as any specialized built-in sugar for maps and matching. And [easy to write!](http://pastie.org/9254833) All without any changes to the language as it exists. The one feature that I think might be interesting—not strictly *necessary*, but at least interesting to consider—is named parameters, and even that could be [faked by the macro system](http://damienradtke.com/using-and-abusing-macros/) if it's really necessary at present.
That's an interesting idea, we could replicate the playpen inside RbE. I'll add it to the wishlist.
The main language creator Chris Lattner [discusses the language on his homepage](http://nondot.org/sabre/): &gt; I started work on the Swift Programming Language in July of 2010. I implemented much of the basic language structure, with only a few people knowing of its existence. A few other (amazing) people started contributing in earnest late in 2011, and it became a major focus for the Apple Developer Tools group in July 2013. &gt; &gt; The Swift language is the product of tireless effort from a team of language experts, documentation gurus, compiler optimization ninjas, and an incredibly important internal dogfooding group who provided feedback to help refine and battle-test ideas. Of course, it also greatly benefited from the experiences hard-won by many other languages in the field, drawing ideas from Objective-C, Rust, Haskell, Ruby, Python, C#, CLU, and far too many others to list. So the language intended to replace Objective-C at Apple has Rust listed as the first influence after Objective-C itself! C# is also in the mix. An accurate assessment from Graydon then, after only a light read of the manual.
What about a main fork for experimental features, and language design experimentation, but with a more corservative main trunk , only adding the really good things from the experimental branch if they are really helpful? A main effort to reach the 1.0 goal on main, but not letting the experiments die on a lab branch? Particularly im interested in the more pragmatic features and sugars proposed on swift that could be easely adapted to rust.. Im working in a application platform that is mix of browsers, p2p and mobile application platforms.. so im hoping to deliver a nice Api so developers can program for the platform with ease.. I've decided to use rust for that not so long ago.. so some "swift sugars" would be nice (i miss a nice way for single inheritance.. and think the way Go does it its nice.. just by compositing the structs creating a mix.. of course the method from the impl should be inherited too) .. Sorry for my bad english in advance..
At least quite a bit of sanitizers and runtimes were contributed by Google.
In fact, it was founded six months ago by a user named "Swiftapple". Pretty sneaky, Apple developers.
select{} is the feature that go has which other csp-alike systems lack -- i might be missing it, but i don't see a direct equiv from GCD/libdispatch. However, GCD has many "higher-level" features that you have to implement by hand in Go (groups, counting semas, barriers, queue-local data, etc...)
Well, GCD doesn't deal with channels at all, so select doesn't really make sense currently. GCD is just the goroutine/lightweight threading side of the equation. We'd need channels to implement a csp-like system, and I agree select would be important there.
println! cannot be "just" a function using variadic generics, because it performs the validation of its format string at compile time. 
Let’s not forget that Swift still has less control over memory management – it’s not quite a C++ish language if you care about every bit of performance.
Interesting, thanks.
Just be aware, that Qt is **not** using C++. Qt uses C++ with some syntax extensions which is send through a preprocessor because bare C++ apparently is too painful. So yes Rust is suitable for app development probably even more than C++.
2nd billing is top honors, considering that Objective-C interoperability was an unavoidable constraint. I'm still amazed at how many chess pieces they quietly moved into place over the years in order to prepare for this. 
How much of the standard library will be marked as stable?
Perl6 is a lot prettier in general than previous versions, but it hasn't really taken over as the canonical Perl; I get the impression (though I could very well be mistaken) that most Perl users are still using Perl5.
How about a mixed approach .. some concessions to features that rust *might* adopt.. stated as not being part of 1.0, disabled by default (need a #...) ... to let the community evolve more in parallel without being blocked by the need for 1.0 eg. imagine if default parameters were added to the AST, but just not used, but forks could experiment
&gt; named parameters... and even that could be faked by the macro system but can it match the elegance of this - compare.. fn slice(&amp;self, from:int, to:int); fn slice_from(&amp;self, from:int) {self.slice(from, self.len());} fn slice_to(&amp;self, to:int) {self.slice(0,to);} // notice how many times 'slice, from, to' are repeated vs fn slice(&amp;self, from:int=0, to:int=self.len()); // use the named args to do any of the above.. // one function does the work of 3 // less repetition in implementations // automated way of generating a set of conveniences // fewer jumps to see what the call does Its a widespread concept in other languages - been around since Lisp I think , so its more likely support will appear in IDEs Furthermore what if we *did* want to use Rust for binding to platforms that already have API's in languages with keyword arguments or defaults - you'd need to do a load of clunky renaming or pairing of functions with macros 
Objective-C objects aren't just C structs; it's trickier than that. Instance variables are non-fragile, so using them requires loading an offset from global data using a special linker relocation and then using the offset to load from the object. It also requires special data in a section for the linker to process.
Version 2: now with ☢ https://github.com/mozilla/rust/pull/14631 fixes issue #666
Foo returns an int, so the unsafe block returns an int, so main returns an int. Need a semicolon in there somewhere (either after foo, or after the ☢ block).
I find remembering all the named parameters in ObjectiveC is kind of a pain, I am constantly looking them up, whereas I easily memorize parameter orders in other languages. I could probably get used to it, but I did not like that aspect of ObjC at all. At least it is not as bad as people who make every function take one hash parameter and fake named parameters, that is pure evil, (in other languages, not Rust).
I guess the benefit of lots of variations of similar function names with is they're discoverable with simple text-based code completion; other than that I think its more the case that you remember fewer things and they combine - (as this example tries to illustrate (slice, from , to .. combined in a general way)). Once you have an IDE.. the IDE shows you everything anyway The benefit for reading is of course that meaning is placed closer to values
I think that the representation of a unique vector type should be `[T]` instead of `Vec&lt;T&gt;`. Because `[u8]` is more consistent with fixed-size vector type (`[u8, ..5]`) and with slices (`&amp;[u8]`).
True that changed. But that's why a C-API exist to create them. 
And don't forget the overhead caused by the dynamic message dispatch.
For the ones that have specifications, anyway.
The YOLO keyword should be a lifetime descriptor where the variable dies after being referenced once.
Yeah this is my issue, Xcode's editor makes it "easy" with code completion, but makes every other part of text editing a huge pain in the ass when you are used to vim. In fact Xcode literally fights with me trying to write stuff I don't want it to write if I do use it (and crashes all the time, but that is another thing entirely) My vim completion works when the identifier is anywhere in an open buffer somewhere, which works out perfectly fine for me in every other programming language it seems.
This is valid code, it's just the compiler needs a bit of help to recognise it. The trick is to move out of `mutIter` into a temporary local before matching on it, so that the compiler correctly understands that the internal reference is basically stealing the `mutIter` borrow. [Runnable example](http://playtest.rust-lang.org/?run=1&amp;code=%23[deriving%28Show%29]%0Aenum%20List%20{%0A%20%20%20%20Cons%28int%2C%20Box%3CList%3E%29%2C%0A%20%20%20%20Nil%0A}%0A%0Afn%20main%28%29%20{%0A%20%20%20%20let%20mut%20l%20%3D%20Cons%281%2C%20box%20Cons%282%2C%20box%20Nil%29%29%3B%0A%0A%20%20%20%20println!%28%22{}%22%2C%20l%29%3B%0A%0A%20%20%20%20{%0A%20%20%20%20%20%20%20%20let%20mut%20mutIter%20%3D%20%26mut%20l%3B%0A%20%20%20%20%20%20%20%20%0A%20%20%20%20%20%20%20%20let%20tmp%20%3D%20mutIter%3B%20%2F%2F%20the%20trick%0A%20%20%20%20%20%20%20%20%0A%20%20%20%20%20%20%20%20mutIter%20%3D%20match%20*tmp%20{%0A%20%20%20%20%20%20%20%20%20%20%20%20Cons%28_i%2C%20box%20ref%20mut%20next%29%20%3D%3E%20next%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20Nil%20%3D%3E%20fail!%28%22Ignore%3B%20not%20important%22%29%0A%20%20%20%20%20%20%20%20}%3B%0A%20%20%20%20%20%20%20%20*mutIter%20%3D%20Nil%3B%0A%20%20%20%20}%0A%0A%20%20%20%20println!%28%22{}%22%2C%20l%29%3B%0A}) (the change is adding the `let tmp = mutIter;` and changing the match to `match *tmp`).
I just watched the Apple State of the Union address where they briefly mentioned how concurrency would be used, and it is indeed using lib dispatch. they pointed out that the ruby like last-argument-is-a-block syntax makes this really nice: dispatch_async(queue) { ... } No more ugly closing parentheses after the end of the block! This should make lots of continuationy patterns really nice (Monads anyone? I'm quite excited to see how easy it will be to make a protocol for the monad pattern; it seems like you could almost have really nice syntax for it)
(Featuring Rust's Nicholas Matsakis.)
How about a function that can execute only once? `proc` tells me nothing, `yolo` tells me everything about that function's lifetime
Thanks for the update! That could indeed open up some cool possibilities. I'm kinda hoping for APIs to regularly return (returnType, err) to enable simple chaining like `let (ast, err) = fileOpen("myFile.txt")?.lex()?.parse()`, where failing at any step provides a nil AST and a readable error object with details of the failure.
Is this a limitation of the type system, or just the compiler and it can be 'fixed' eventually?
It doesn't seem ridiculous for it to be fixed (since the work-around is so simple), however I believe adding new lifetime inference (etc.) rules is a low priority now (especially for this one, since the work-around is so simple).
&gt; yes I prefer Rusts closure syntax - but 'do' notation reduced nesting I apologize, I've misread the post. I took it to mean indentation instead of nesting - which very often come hand in hand when talking about "semicolon languages" like rust. I agree that something like what you suggest would be great. I just wonder if it would be clear enough that the closure is passed as the last argument to the called function, as it doesn't look like a normal parameter. In Ruby, which is the language on which I've seen this kind of construct, the signature for taking block parameters is different from normal parameters. &gt; I was talking about the syntax for declaring vector collection types I see what you mean. I'm not very fond of `&gt;&gt;&gt;` either. Is it possible for macros to expand into types?
I'm not sure that'll work, since the tuple returned will not be nill, only its first argument will be. With monads you could use something like this on the otherhand: enum Either&lt;T&gt; { case Left(String) case Right(T) } func bind&lt;A, B&gt;(Either&lt;A&gt; x, A -&gt; Either&lt;B&gt; f) -&gt; Either&lt;B&gt; { switch x { case Left(err): return Left(err); case Right(a): return f(a); } } Right(1) .bind() {a in if a%2 == 0 { return Right(a.simpleDescription) else { return Left("Expecting even number (\(a))") } } .bind(){Right($0.length)} I'm hoping less verbose syntax can be used (&gt;&gt;= seems like a valid operator name). an Async monad a la Haskell's async package would also be quite easy to write using libdispatch which would provide future-esque things. This looks like a lot of fun, I'm quite excited!
&gt; `&gt;&gt;=` Yes; it's even an operator right now: assign-right-shift. I.e. `x &gt;&gt;= y` is equivalent to `x = x &gt;&gt; y`.
Leave it to the creator of C++ to complicate the introductions in a way that ends up leaking Niko's intro. :P
right, but from what I can tell, that won't work (at least as you've described). It's the difference between Maybe (a,String) and (Maybe a, Maybe String) using ? on the tuple will never fail in the case of (returnType,error), but the functions like lex will need to check if the returned value is nil. This is why something like Either is more appropriate for what you want, because you either get the returned value or the error that needs to be passed along.
well to reduce the amount of type nesting i guess we can always alias .. i would be using 'OptBox&lt;T&gt;' and 'VecBox&lt;T&gt;' rather than Option&lt;Box&lt;T&gt;&gt; and Vec&lt;Box&lt;T&gt;&gt; ... those are my main cases. r.e. the closures, surprisingly Swift, with Apple and their reputation for user-friendliness has something 'even stranger' i.e. the block can just appear after the function call - with Rust, 'do' makes it look more like one block in the AST.. just like any inbuilt control flow forms. 
All programming languages borrow from each other, and it's encouraging to see a language borrowing from Rust. The spread of modern language features, particularly into a language likely to be heavily used by people who may only have worked with Objective-C before (which lacks many of these features) will be a truly good thing. One of the difficulties of new language concepts is the confusion and lack of familiarity when they are first introduced. The longer people have to work with them and the more languages that implement them, the more people are likely to feel comfortable with modern developments.
The full quote isn't that much longer, no need for the '...'s Of course, it also greatly benefited from the experiences hard-won by many other languages in the field, drawing ideas from Objective-C, Rust, Haskell, Ruby, Python, C#, CLU, and far too many others to list.
OOM: abort. Fail: unwind, running destructors as you go. Fail in destructor while unwinding: abort.
Qt uses standard C++. Meta object compiler is used to generate runtime metadata CPP files, but the actual CPP files are passed as is to C++ compiler. I believe it's possible to avoid moc phase with C++11 somehow.
Note: the OOM abort situation is not necessarily the long term plan, it is just an edge cases, and aborting solves it well enough for now. (Also, the aborts are performed via an illegal instruction as seen in the original post.)
I wonder what that long term plan may be, because it seems to me (from my limited experience) that once you started to try to solve these kind of problems (recovering from out-of-memory errors), you kind of end up with something more or less similar to the C++ exception mechanism. Does this mean rust may introduce a (perhaps more light-weight) exception mechanism similar to what C++ has?
I hope not. An easier thing to do might be to have a version of all the memory operations (both safe and unsafe) that return a Result. (Not sure, not a core Rust contributor)
So `box 1` would return a `Result&lt;int&gt;` instead of a `Box&lt;int&gt;`? Would this require too much of a change to current code? I surely hope rust can come up with a better idea, but from what I can think of, exceptions seem to be pretty much unavoidable if we want a mechanism that does not interfere with normal program flow.
Something like `safe_box(1) -&gt; Result&lt;Box&lt;int&gt;&gt;` but this can get icky, I agree.
I really liked them in Python so I was super excited when they got added to C#, and then never ended up using them. Except to preface some random bools as documentation. The thing I don't like about them is that it exposes your argument names, so you can't change them later. But...that might not be so terrible; forces you to think a bit harder about what you name them, which makes for better documentation.
Love Andrei's quote about garbage collection in there. That's a striking statistic.
I was hoping that Apple developers would get to weite about making Swift and talk about their inspirations for the language, and it's great that it is Chris Lattner himself that's doing so. It's obvious the language heavily borrows from Rust, C#, and others and it's great to see him acknowledge this. I hope that Apple will open source Swift because it's a very intriguing language to be sure, but who knows if that will ever happen. All I know is it is great to see that others see the value in the direction rust is going and that validates the hard work everyone involved in developing rust have put into the language. The next few years are going to be an exciting time for programming languages as we see the rise of natively compiled, safe languages.
I would say Rust would still be compelling for games that want a high level of performance. For less intensive applications though, Swift could be rather nice, especially with their tooling.
its just managing tables of function pointers, right? surely between the macros, syntax extensions, and traits, you could get something workable
Niko in some illustrious company! We also heard that even Rust wants to be nice to "non-optimal code", [I feel vindicated.](http://www.reddit.com/r/rust/comments/272i7p/rfc_autodereferencing_nonraw_pointers/chx5w30)
I don't agree. But lets stop that here the discussion would get endless. ;)
You should have a look at sqlite. It was designed to fail gracefully even in the face of oom.
Being nice to non-optimal code in general is different to being nice to pointlessly non-optimal code (specifically, many uses of `~` were completely pointless, and could trivially be replaced by either the reference operator `&amp;` or just outright removed). And anyway (and more importantly), [there are (at least) 3 reasons](https://github.com/rust-lang/rfcs/blob/master/active/0014-remove-tilde.md#motivation) for the move away from `~` other than the syntax encouraging allocations: most importantly, the `~` syntax does not allow for custom allocators.
SQLite might not fail due to OOM, but the out-of-memory condition still exists. When memory is allocated by some other library you're using (or perhaps by your own code), the OOM killer will still activate.
I've given up on the idea of recovering from OOM, at least at the library level. Too much complexity, almost 0 benefit, and no clear solution.
Why not immutable borrows? 
It's not so much that the parameter names are exposed. Rather they become part of the function signature. You can think of them as part of the function name. Personally, I don't think they give you much. The IDE should help you out, showing the info you need, when you write the call. If not you would still have to remember the parameter names, and possibly their order. When Reading code the arguments themselves are usually self-explanatory in the context they are used. If the expressions are complex then you should generally compose them using named temporary variables anyhow. When designing a programming language it's important to choose your features wisely. The bloat will have to be carried for as long as the language remains. Some sugar is certainly beneficial! (end of rant)
Because then I can't change the data in-place? I'm not saying that it's actually necessary, it's just what I wanted to try. Let's say I have an array of a billion numbers and want to take a square root of all of them, which should be embarrassingly parallel, I think. And maybe I don't want to create a new array. Wait, even if do create a new array, I'll still need to borrow it to different tasks to let them write in the results. Which brings me back to the same problem...
Of course. Worse, your application may or may not be responsible for the OOM condition. So handling it greatly depends on the environment that the program runs in and the guarantees it will have to fulfill.
&gt; Being nice to non-optimal code in general is different to being nice to pointlessly non-optimal code I don't agree. Other similar issues in Rust are solved with adding lints that push you that direction instead.
Ah, but higher up? Vec::push could have the interface `.push(T) -&gt; Option&lt;T&gt;` and return `None` on success and `Some(T)` to give back the value if allocation failed, pretty amusing API, but I don't think it's useful.
This is an example of safe pattern which is currently prohibited by the borrow checker. However, most of time you don't use static indices - they are computed dynamically, and there would be a possibility of mutable aliasing if this were allowed, so this is justified somewhat. Currently such patterns have to be implemented using `unsafe`. Some of them are already provided by the library. In your case, I guess, you can use [mut_split_at()](http://doc.rust-lang.org/std/slice/trait.MutableVector.html#tymethod.mut_split_at) method.
In general the compiler cannot track borrows out of an array (doing so would be solving the halting problem), and, in fact, it doesn't even try at all. That is, every `&amp;mut x[i]` borrows the entirety of `x`. One way would be using iterators: let mut array = [0i, .. 2]; let mut iter = array.mut_iter(); let p1: &amp;mut int = iter.next().unwrap(); let p2: &amp;mut int = iter.next().unwrap(); (Note `p1` and `p2` have type `&amp;mut int`, *not* `&amp;int`.) Another approach is to use [the `mut_split_at` method](http://doc.rust-lang.org/master/std/slice/trait.MutableVector.html#tymethod.mut_split_at): let mut array = [0i, .. 2]; let (p1, p2) = array.mut_split_at(1); let p1: &amp;mut int = &amp;mut p1[0]; let p2: &amp;mut int = &amp;mut p2[0]; --- In any case, a `&amp;`/`&amp;mut` reference cannot be passed between tasks in safe code (they do not [satisfy `Send`](http://doc.rust-lang.org/master/std/kinds/trait.Send.html)). The ability to do this is often referred to as "data parallelism", and Rust is aiming to do this really nicely, but we aren't quite there yet. [The `Share` trait](http://doc.rust-lang.org/master/std/kinds/trait.Share.html) was added to enable data parallelism to be safe, but there have been a few blocking issues (I think these are resolved now) and AFAIK, no-one has yet put effort into developing a safe data-parallelism library. A "main" reference for this is Niko's old [Data Parallelism in Rust](http://smallcultfollowing.com/babysteps/blog/2013/06/11/data-parallelism-in-rust/) post, but parts of it are now out-of-date (e.g. the `Isolate` trait is no longer part of the plan). There is also the related issue [#12619](https://github.com/mozilla/rust/issues/12619).
You think that being nice to non-optimal code in general is *identical* to being nice to unnecessarily non-optimal code? (That is, code for which more efficient patterns are either equally usable, or *more* usable, compared to the slow ones?) It would be interesting to see a lint for `Box` used outside of trait objects and recursive data structures though. In any case, this has been discussed at great length previously (covering the points you raised in your original comment, among others): - http://www.reddit.com/r/rust/comments/24elup/rfc_remove_in_favor_of_box_and_box/ - http://www.reddit.com/r/programming/comments/24kcl4/is_being_removed_from_rust/ - http://www.reddit.com/r/rust/comments/251grm/librustc_remove_expr_type_and_pat_from_the/ - https://github.com/rust-lang/rfcs/pull/59
It seems to me that OOM can mostly be handled by task failure (aka unwinding), i.e. the "exception"-like system Rust has now (we would need to be very careful to ensure there's no possibility of infinite loops when if any allocations during task failure fail themselves). I misspoke about failing while unwinding (was on my phone when I wrote it originally, sorry): AFAIK handling that safely is an unsolved problem in languages without GC, e.g. even [C++ doesn't try to handle it](http://www.parashift.com/c++-faq-lite/dtors-shouldnt-throw.html) and just terminates like Rust.
Is there a way to do it unsafely (assuming I made sure myself I won't have any overlapping references)?
&gt; Fail in destructor: abort. This is actually slightly more subtle: it is only failing while already unwinding that causes an abort, e.g. use std::task; struct Ok; impl Drop for Ok { fn drop(&amp;mut self) { if !task::failing() { fail!("ok drop") } } } struct Bad; impl Drop for Bad { fn drop(&amp;mut self) { fail!("bad drop"); } } fn main() { // un/comment any of these lines to see behaviour change let _ok = Ok; // let _bad = Bad; // fail!("main"); } [Live example](http://playtest.rust-lang.org/?code=%23!%5Ballow(dead_code%29%5D%0Ause%20std%3A%3Atask%3B%0A%0Astruct%20Ok%3B%0Aimpl%20Drop%20for%20Ok%20%7B%20%0A%20%20%20%20fn%20drop(%26mut%20self%29%20%7B%0A%20%20%20%20%20%20%20%20if%20!task%3A%3Afailing(%29%20%7B%20fail!(%22ok%20drop%22%29%20%7D%0A%20%20%20%20%7D%0A%7D%0A%0Astruct%20Bad%3B%0Aimpl%20Drop%20for%20Bad%20%7B%0A%20%20%20%20fn%20drop(%26mut%20self%29%20%7B%0A%20%20%20%20%20%20%20%20fail!(%22bad%20drop%22%29%3B%0A%20%20%20%20%7D%0A%7D%0A%0Afn%20main(%29%20%7B%0A%20%20%20%20%2F%2F%20un%2Fcomment%20any%20of%20these%20lines%20to%20see%20behaviour%20change%0A%20%20%20%20let%20_ok%20%3D%20Ok%3B%0A%20%20%20%20%2F%2F%20let%20_bad%20%3D%20Bad%3B%0A%20%20%20%20%2F%2F%20fail!(%22main%22%29%3B%0A%7D&amp;run=1).
Cast to a `*mut T` and pass that to the other task, and then reference back to a `&amp;mut T`. Something like let x: &amp;mut Type = ...; let ptr = x as *mut _; // _ infers to `Type` let (tx, rx) = channel(); spawn(proc() { let inner_x = unsafe {&amp;mut *ptr}; // this task is finished. tx.send(()); } // synchronise to stop the frame that owns `x` ending while the task(s) are still running rx.send(()); Note that you have to ensure that the task is only using the `&amp;mut T` reference for a limited time, specifically, while it has unaliased access to the data and the data is still valid.
There are no doubts that it is possible to use Rust for full app development for iOS, the main question is actually how comfortable it will be and the answer is "not pretty much" even comparing to Objective-C.
That's nice to see, thanks for the links. I don't see the point you tried to distinguish, that's all. You decided this particular thing was 'pointlessly non-optimal', and other things can then be 'non-optimal with a reason' then I guess, but I don't see why you make that distinction.
Many uses of `~` are utterly non-optimal: instead of writing `~foo`, one can just write `foo` or `&amp;foo`. That is, the `~` allocation is literally achieving nothing semantically, and is just eating CPU cycles. Other "non-optimal" code (like using GC) rarely has a simple *local* change to a more efficient version like the two `~` changes I mention above. Moving away from a shared ownership model almost certainly requires a re-architecture (to some degree) for many applications. (A simple search-and-replace that removes all `~`'s is likely pretty close to directly compilable, for many programs.)
Slightly off topic, but do we know yet how fast swift runs, and how fast playgrounds are relative to the base language?
Well there can easily be a lint for stack variables allocated with `~` and never passed on, that would catch a lot of those.
&gt; The thing I don't like about them is that it exposes your argument names, so you can't change them later. On the other hand, their order doesn’t matter anymore.
Awesome!
That won't help in a map/reduce setting, as the goal of the split is to use the array slices independently from each other.
I guess the ellipses were to draw attention to the fact that Rust is there, it gets a bit lost in the full list.
[All I've seen is this](https://9to5mac.files.wordpress.com/2014/06/screenshot-2014-06-02-14-47-01.jpg?w=704&amp;h=392) although its obviously not enough to judge its speed overall and I doubt its a true representation, being directly from Apple, but it hopefully means that it performs pretty well. 
It casts the `&amp;mut T` reference to a `*mut T` raw pointer.
Yes it will. Array slices are just fat references, meaning the same fundamental approach works, after rewriting the two casts to let raw_slice = transmute::&lt;_, std::raw::Slice&lt;T&gt;&gt;(slice); // and let inner_slice = transmute::&lt;_, &amp;mut [T]&gt;(raw_slice); respectively. 
Nice that they stuck python in there to inflate their figures
Thanks, I didn't know that.
If you're segmenting the job into tasks then I guess a task will fail and the rest will carry on. Isn't this also the erlang approach? Any component may fail at any moment? The application will keep bumping up against the memory bound and tasks will keep failing, but if task error-handling is done right then that should be detectable and manageable. The other thing you can do when hitting the memory bound is to try and release some memory somewhere. In a GC system you can do a GC. In Rust (with no garbage) perhaps the only possibility would be if some memory areas were marked as cache that could be automatically freed if necessary, like a weak reference. Needs runtime support though.
See "Crash-only software". (In which any component should handle power failure or kill -9 gracefully, and that is the normal termination method.)
Yeah it seems that is all there is until someone does the work themselves and posts it. The computer language benchmarks game doesn't have objective-c (and it would be on linux) so it is hard to tell. Even there python is 100x slower than C at most. I'm guessing it will be in the realm of go and ocaml, about half the speed of C and C++ (or else they would have compared it to C++).
My guess is that `putstrln "Foo" + "Bar"` appeals to use cases, where you would normally use a scripting language.
At first I thought "maybe we just need a mechanism to recover from an allocation `fail!` that is deep on the stack!". Then I realized I'd just invented exceptions
It's also interesting that those who get to 1.0 first are kind of stuck if the (friendly) competition comes up with a better idea or better feature tradeoff.
As someone unfamiliar with both kernel and embedded programming, I ask: How does non-exception (read: C) code usually handle this problem at these environments?
Now I know how it's called, but still don't know what it does. :-) Is a raw pointer something you *can* send into task? Why do you have to cast it back? 
There are programs that will overcome a kill -9 (because they will spawn an 'angel' process to restart themselves whenever they get killed).
You should be able to have multiple immutable borrows, so no, if you create a new array, you won't run into this problem. 
What I meant is, I need a new array to write the data in. But to write data in from a task, I need a mutable borrow of a new array. Although now that I'm thinking, I can write data to a new array in a non-parallel way. Although this won't be exactly what I was trying to do. (And I don't know how to use channels yet :-) )
Python's fairly commonly-used for scripting on MacOS (for the first few versions, it was the only bundled scripting language that was even vaguely up to date), and Swift might displace some of that use. Also, people are often inclined to think that things that look like scripting languages must be really slow; they may have wanted to avoid that idea.
Raw pointers are one of the lowest-level features of Rust. Mainly they are needed to interact with foreign C code and to implement abstractions such as `Rc` or `Vec`. You can find more in the [official tutorial](http://doc.rust-lang.org/guide-ffi.html).
Yes, crash-only software generally has other processes around monitoring things and restarting things if they crash or misbehave. But because the software is designed for this, an OOM failure is not catastrophic for the overall system, and other components should not fail unexpectedly either if one component out of the set suddenly drops out due to OOM. I think this is the way tasks are intended to work within Rust too, since the task is the unit of failure. So Rust supports a crash-only design in terms of internal task handling.
Yes, that was the idea.
Given Rust's emphasis on safety, I'd imagine `box` would return the `Result&lt;Box&lt;int&gt;&gt;` and `unsafe_box` would return `Box&lt;int&gt;`. But yeah that's too unwieldy.
There is also an advantage to an earlier 1.0. The sooner a language is stable the sooner people and companies can start to invest in learning it and building with it. While the pre-1.0 first-adopter community is great, a stable community of real world users is absolutely necessary for language success.
Failing the task would allow the application to attempt recovery using normal task error handling (i.e. another task notices that the task has failed). For example, say image decoding is run in a another task and it allocates a huge buffer and fails, which releases all memory associated with the decoding, and then the waiting task notices that and puts up a corrupt image icon. That seems like sane handling of the problem. If the memory limit is reached with many small allocations, then which tasks dies is random but biased towards the active ones. There is still hope that a monitoring task mostly waiting and not doing allocations will be able to recover things.
Go is a really good example of this. It's debatable as to how much, but some feel Go 1.0 is missing a few features. However, it remains a stable language in the 1.x releases, and promises to remain stable. As such, it's been adopted and used quite heavily.
Okay, yes, I've found that mentioned now. I guess with Erlang it is distributed, so only one VM in a network of VMs dies like this. Well, if (say) the whole Rust process is going to die because making an individual task fail turns out to be unhelpful, then to safely handle any code which risks running out of memory you'd need to use a second OS process, and either take your chances with the OS OOM killer or apply process memory limits with OS calls. Either that or somehow find a way to run a Rust task group (e.g. everything spawned off a given task) with a different heap with software-defined limits applied. Then failing the allocating task or even the whole task group on exhausting that heap would be predictable and correct. Not sure if that is feasible.
the code is not displayed for me (firefox), it is briefly shown when loading the page and then dissapears, only the run button is shown
AAaaah. Scroll bars.
ok, works now
Usually malloc or kmalloc will return NULL and the programmer handles it (hopefully gracefully). For example if you're in a kernel module for a network device, and you want to allocate a buffer but kmalloc fails, you can't simply panic() because this is going to hang the entire system (as you are in kernel space). Neither can you bail out and unload the entire module (because user will no longer have access to the network device). What you can do is maybe report the error to user space, or lose some packet, or temporary suspend the operation and try again later. The point is that the program cannot simply fail (and killed by someone else, after all who is gonna kill the kernel?), and must recover gracefully to ensure system stability.
Sorry I could have been more explicit. I *was* talking about exceptions. What I was trying to say is that if rust were to have a generic error handling mechanism, it basically would have to reinvent exception.
So awesome.
&gt; And when do you check for the NULL? Every time you do a malloc? If you want to guarantee you will not have out of memory crashes then yes. This is why embedded code will often go to great lengths to avoid dynamically allocating memory in a restricted memory environment.
Let's anyone just quickly edit the example and rerun too, awesome!
A lot of work has gone into this over a long time. This is using strcat's amazing [rust-playpen](https://github.com/rust-lang/rust-playpen), which is also what drives the `rusti` bot in #rust. Playpen is hosted at http://play.rust-lang.org and features a cross-site XHR API so anybody can do things like this. acrichto has set up the infrastructure on EC2, and finally today Sergio Benitez [used that API to make this happen](https://github.com/graydon/rust-www/pull/26).
&gt; What you can do is maybe report the error to user space, or lose some packet, or temporary suspend the operation and try again later The right action really depends on the context then, that's what I thought. Then the `malloc`/`kmalloc` can't be too deep behind an abstraction layer (say, a collections library, like Rust's), right? Otherwise you would have to keep returning `NULL` through the stack until someone handles it (which would be essentially a manual exception).
Well...it does, unless every call to the function everywhere uses the named args.
Alex didn't give me permission to post this, but I thought it was neat. This is after the [pending runtime extraction](https://github.com/mozilla/rust/pull/14638)
Very excellent. One suggestion is to save the code between visits to the page, and have a button to reset it back to the original example code.
I was thinking about any allocations that occur when first triggering the failure, that is, by the failure subsystem: it might be easy to hook it all up to achieve that, but it definitely requires checking. (Certainly an OOM inside a normal destructor would be handled correctly automatically.)
Just to make it entirely clear, `foo -&gt; bar` means "foo imports bar".
/r/playrust
&gt; It absolutely cannot do everything C..C++11 does at this point in time, so let's not get ahead of ourselves. &gt; i didn't say it has every feature C++11 does - but unlike swift, it can drop down to the lowest level with pointer arithmetic and do things swift can't. its' collection classes, unique pointers, refouncted pointers are all implemented *in rust*, 
So looking at this, I see that librand is only dependant on core. Does that mean then that if someone wants to use the rust rand functions on a bare metal runtimeless environment they should be able to correct? If so thats pretty neat! I did see that it was recommended to use the interface exposed by std::rand, but since you won't have a runtime without std this could still be used if desired right?
How hard is to target LLVM IR for a new language? As opposed to CLR/.NET or JVM. I have some ideas for a new language I'd like to try out, and if LLVM takes care of all the optimizations for me, that'd be sweet.
can this be integrated into rustdoc? It would be awesome if you could opt into generating runnable code docs.
Yes.
Calling `.to_str().to_string()` is just a left over from when `~str` still existed, it should be able to be written as just `.to_str()`. All this string handling stuff has been in a lot of flux recently (after the old dynamic string `~str` was removed) so there's not any sense to be found in the various methods that exist. It will become more and more sane as people work on it.
To the best of my knowledge, `value.to_str().to_string()` is creating two strings for no reason. At first I thought this was a documentation bug, but searching the rust codebase for `.to_str().to_string()` yields 50 results. I wonder if there is a reason for this, or if it's just an artifact from some automatic conversion, given the multiple `&amp;str/String`-related changes recently.
The [Unsafe and low-level code guide](http://doc.rust-lang.org/master/guide-unsafe.html) covers raw pointers too.
Thanks!
The left hand side of a "let" statement is a destructuring pattern, so generally you use the same syntax as to construct what you're taking apart. You could also just dereference it: let b = *a;
This is probably the more idiomatic way to unbox.
Every now and then I get really frustrated with the time library and think to myself, "well shit, sao, you should just port Joda over to rust". And then I try for a day or two, and then cry in agony as I realize how complex dates and times are. its about time for me to do it again pretty soon, so maybe I'll be more successful this time. I do know for a fact though that any moderately decent port of Joda is going to require better internationalisation/locale support. 
&gt; Rust does everything C..C++11 does, but safely. I don't mean features either, I mean capability. In the future it is planned to be able to do all that C/C++ can do, but currently it can't, the language is not even done yet. I'm talking about confusing this ideal future version of Rust's potential with it's current state, and I feel like we should phrase our statements about Rust on it's current state, not it's potential when it is at 1.0 and beyond. They way some people are making comparisons about it you'd think you could pick it up today and accomplish everything C and C++ can do, but that's not true.
That seems to imply that the "box" keyword should be removed from this usage.
Yeah, they really are. I think the only way to build something like that is to fully expect that the first version will be full of holes. Just work with the community, add regression tests while you go on and eventually it'll be 99% good enough.
These seem to be the remains of all the renaming that has happened around `StrBuf` and `String`. I have just opened a pull request to change all occurrences of .to_str().to_string() to just use .to_str()
/u/aochagavia has fixed it: https://github.com/mozilla/rust/pull/14667
Is it save?
It is running in a [strong linux sandbox](https://github.com/thestinger/playpen).
I agree. I think i'd start without timezones and add date/time calculations for the local timezone. This way you'd postpone a lot of headache and provide a good portion of functionality.
Is porting Boost an option?
Reading from the discussion it appears as if the authors are not aware of recent work in this area, most notably by [Ceylon](http://ceylon-lang.org/documentation/1.0/introduction/) (Principal typing, union types, and intersection types). I am unsure if I like the *T1 + T2* syntax, but that's perhaps because I approach type systems from a mathematical background. Following this, intersection types would be denoted *T1 ∩ T2*, *T1 × T2* or at least *T1 * T2*. *T1 ∪ T2*, *T1 | T2* or *T1 + T2* would then denote a union type as in Ceylon.
&gt; Almost no code handles OOM correctly even when the process isn't going to be randomly terminated instead. The OOM killer is a deterministic algorithm based on a score heuristic, so *randomly terminated* is a misrepresentation of how it works. There's an exposed `oom_score_adj` knob for each process along with the ability to use it on a more granular level with memory control groups.
A project like the Linux kernel avoids dynamic allocation whenever possible, and explicitly handles the error conditions when it's necessary. It's not a workable solution for a standard library providing generic abstractions.
I would be stunned (appalled?) if they *didn't* open source Swift. Apple have traditionally open sourced all command-line developer tools. But they are a little sluggish about it (they typically develop behind closed doors and open source within 3-6 months of leaving beta). With closed-source bastion Microsoft open sourcing C# it is clear that modern development demands an open source compiler. You can avoid open sourcing in the short term but longer term, developers want to take a look to understand the quirks and bounds of their tools.
To be honest though, the existence of named parameters in Swift is mandatory for compatibility with Objective-C (methods are identified by their "selector" which is the method name including all named parameters). They're existence in Swift shouldn't be taken as an indication that languages need or should have named parameters.
&gt; (Principal typing, union types, and intersection types). I'm not really sure how any of these relate to this RFC. AIUI, the core of the RFC is essentially just adding a trait-bounded form of `auto`/`decltype(auto)` from C++11/14 (that is, inferring the return type of a function from its body), to make it sane to return iterator chains, and possible to return unboxed closures. Could you explain how Ceylon's typesystem relates in a little more detail, for people like me who aren't keeping up with the research? &gt; T1 + T2 Note that this is the syntax Rust already has `+` for groups of traits (i.e. the intersection): fn foo&lt;X: Foo + Bar&gt;(x: X) { ... } trait Foo: Super + Traits { ... } etc. Specifically, a complaint about the `+` syntax is not specific to this RFC (and so shouldn't be regarded as a problem with it).
Niko is certainly aware of union/intersection types and we even considered them in the past. But this is doubly orthogonal because (a) the trait composition syntax is not the subject of this RFC and (b) trait composition is for traits, not types. The Rust equivalent to union types is `enum`.
Correct me if I'm misunderstanding what you're saying, but the `+` syntax for intersections is already used, e.g. fn my_func&lt;T: Foo + Bar&gt;(a : T) -&gt; T{ a } Is already valid Rust code, and I don't think the `+` there is going to change. Given that, it seems like the only reasonable thing to use in this proposal.
The `+` there indicates a union, not an intersection.
Actually, a lot of these are the same thing: generalised operator overloading. * sugar to access Option types with less nesting * sugar a common Vector type [] * sugar for a common associative array type Value[Key] These are all implemented in the Swift standard library (not in the language itself) using operator overloads – along with the vast majority of the operators in Swift (dot, parentheses, goes to and assignment being the only obvious exceptions).
Funny that Hoare calls single-inheritance plus multiple protocols "very C#-ish" when C# took that idea from Objective-C in the first place (not that Objective-C invented it).
Really? A union would mean `T` is *either* `Foo` *or* `Bar` (or both?), whereas in the example I gave, `T` needs to be *both* `Foo` *and* `Bar`. That's an intersection, right?
I still don't understand how these union/intersection types from Ceylon relate to this RFC, which is just about extending where/when type inference can work. The trait bounds thing is "simple", in that we already have syntax for it via `+`. (Your `Eq &amp; Show` "example" would be written `fn foo() -&gt; impl Eq + Show` under this RFC: that is, `foo` returns some fixed-but-unspecified type that implements both `Eq` and `Show`.)
It is an intersection: `T` is in the intersection of set of types that implement `Foo` and the set of types that implement `Bar`.
Porting JodaTime is something I'd be willing to actively contribute to. Dates and time are crazy complex and I want to see rust have a good date time library and not just the typical attempt at it that we see in most standard libraries.
Its a union of the set of trait methods, and a intersection of the set of types implementing each trait. As always in these situations, union and intersection are interchangeable as long as you havent exactly specified them :P
Thanks for giving creds *high fives back*. I would just like to point out that the `content_type!` macro was also copied, and that I don't mind. It was nothing to write. Anyway, I'm excited to see where this will go and how differen/simmilar to Rustful it will be, eventually. Correct me if I'm wrong, but I think the main difference will be that Rustful is aiming to stay quite minimal, be cluster friendly and have more of the bare essentials, while Oxidize is meant to do more "magic" and go more in the direction of, for example, Django or Rails. Am I right?
Where? Every CPU configuration lists "no program" for Rust on all the benchmarks, for me.
Can confirm. Has the benchmark code been moved within the distribution recently?
Are there any libraries that these can be ported from?
Boost is almost entirely C++ templates, with very little code to link with. So "porting" Boost to Rust would essentially be redesigning Boost from the ground up using Rust features instead. Also probably not the best fit for Rust.
Well, how many thousand years are you interested in? Down to what accuracy? How many timezones? The combinations are small enough that you can just exhaustively check against another implementation with a loop, can't you? e.g. first and last second of each day. (I did that once, but I wasn't trying to handle Julian/Gregorian handover or anything like that.)
When you say 'port' do we mean copy the API and logic of that library? If so there are some [nice datetime libraries](http://crsmithdev.com/arrow/) for python. [Requests](http://docs.python-requests.org/) is very well designed http client library.
Bleh, you're right, to me "intersection" there implies that `T` would only implement the methods that both `Foo` and `Bar` have in common. But I can see now how we're being ambiguous. :)
l12n? What's that, innternationalissation? /s
Well, you didn't ask what's up with Rust. The shootout site hasn't changed as far as I can tell. Perhaps no one has written Rust programs and maybe the person who runs the site is not interested in including Rust.
Maybe you need to write some Rust programs
The rust distribution has included programs for the shootout for some time. However, those programs have either gone missing, have been moved within the distribution, no longer compile or the shootout maintainers have stopped including them for another reason. My question is: Which of the aforementioned option is it?
Well, if they ever were on the site, I'd suggest asking the site maintainer as opposed to reddit.
If you've written a Rust program that you'd like to see published on the benchmarks game website, create a ticket in the project ticket tracker by following these instructions -- http://benchmarksgame.alioth.debian.org/play.php#contribute 
I think that is a good thing, since `into_str()` already returns a `String`.
Might be relevant: https://github.com/mozilla/rust/issues/14248
If someone decides to port some library to Rust, and said library is originally GPL or LGPL, could it still be included in the Rust distribution? Breaking down in two questions: a) If a given library is originally LGPL, is the port required to have the same license? b) If it is, can/will Rust include such library in the base distribution?
Ah, yes, that seems like a probable reason.
DAG: Dependencies abstract tree? Did you use any tool to generate this PDF from the tree?
You are correct! I'm not going to a full stack built in like Django, but I do want all the tools to be available to a person should they just want to use the defaut tools. That was the most frustrating prt about Django was it grat up until you needed to do something it was never meant to do and then you were just out of luck, so I wanted to make oxidize much more modular than that to avoid this problem. There is no forced database connection, no built-in auth module (the plan is to make it as a middleware), no mandatory server to connect it to. A little note on that, but my idea stems from my complaint that there is no good way to rewrite your large existing application into another language. Say you have a massive PHP app that you want to rewrite into Go. The first step I would take is to rewrite one small part of the application and then put that part live, so you now have a part PHP, and a part Go web site. But because Go frameworks almost always rely on Go net library to provide the server code, it make interoperability a more difficult since your need to ProxyPass to Go Server and now you are running two servers instead of one. Anyway, enough with that tangent. I'm not too sure what I'm gong to do about cluster servers and such since I have no experience with them. Maybe someday I'll figure that out though. I'm sure our projects are going to be pretty beneficial to each others though and I also think they are going to be for different audiences as well.
Ah! C++ cheats by requiring that implementations set aside some memory so that they can unwind when propagating `bad_alloc`; I suppose Rust does (or should do) the same, for example by reserving some space at the beginning of the stack.
The reason why Rust might be a great research language for game development is that is already a research language with interesting ideas, cross platform, easier to learn than C++, and allows building zero cost abstractions without using a garbage collector. All these attributes contribute to hit a sweet spot between performance and flexibility you often need in games.
The cluster part is more of a philosophy than a feature. The server should not rely on shared memory and it should be able to start and stop any time, to allow it to run on several machines while providing the same service. This goes hand in hand with the stateless parts of the REST philosophy. This doesn't say that it shouldn't provide shared memory, but it must not require it. I'm in fact working on some features for resource cache and handler storage at the moment, which will allow things to be temporarily stored to save time and computational power.
DAG: Directed Acyclic Graph.
Who is "we"? Piston looks very interesting, but I'm interested in the people behind it!
By the way, we're re-licensing the ones in the official repository under the license preferred by the benchmarks game. Ideally this will be done for 0.11, but it depends on how quickly people respond. https://github.com/mozilla/rust/issues/14248
Idly thinking how much of this you would get by exposing QtCore to rust (as temporary solution). Or even glib
If you are thinking about set theory then your reasoning is correct. The '+' operator is usually used when doing a *union of two disjoint sets* (which in my opinion, reflects exactly what one think would happen when combining two traits). An intersection in set theory is when you only take elements present in both sets. The notation for this is '∩'. However as pointed by dbaupp, these two notations are *equivalent* in Rust (you want to combine the functionality of both traits, hence why the '+' operator makes more sense since two traits will always be disjoint). Source: http://en.wikipedia.org/wiki/Union_(set_theory), http://en.wikipedia.org/wiki/Intersection_(set_theory), http://en.wikipedia.org/wiki/List_of_mathematical_symbols
A DAG is a directed acyclic graph. They are unique in that it is possible to sort them and produce an order of compilation so that a crate gets compiled only when its dependencies have also been compiled.
I'm honestly just throwing out some random suggestions here (that is, suggestions which might apply to any other game-engine library), but I figure that if you're using Rust, you must like to experiment with new ideas. So have you considered... * **An imperative input API?** I find that having an event-driven system for key-presses and mouse-clicks tends to be misleading for beginners and inconvenient for more experienced users. I always end up writing a wrapper layer which gives me query functions to answer questions like *"did the Z key transition from the 'up' state to the 'down' state during the previous frame?"*, or *"how many X-pixels has the cursor moved since the previous frame?"*. I suspect that most users will find a similar, query-based API to be more intuitive than an asynchronous, event-based one. * **Including a software renderer for 2D graphics?** OpenGL drivers have very inconsistent performance at best, and they're full of bugs at worst. Modern CPUs (with SSE2 being practically universal at this point) can render blits, scaled blits, simple rotations, color transformations (and so on) more than efficiently enough to fulfil the needs of most 2D games, even at 60fps; but I've personally seen an OpenGL driver whose render-to-texture support was so poor that it made this task impossible. Once Rust supports SSE, providing a simple blitter API, backed by a software-renderer, might not be as ridiculous as it sounds. EDIT: &gt; Rust-Image: A pure Rust library for image formats I take this to mean that you'll be writing pure-Rust decoders for GIF, JPEG, etc. Have you liased with the Servo people about this? They (will eventually) need a similar library, so if you make sure your API is compatible with their requirements, this could prevent some duplication of effort.
I have no aversion to writing crypto in Rust, think it's a great idea, and encourage it whenever I can. All crypto everywhere should be written in Rust, eventually :) I do have an aversion to shipping crypto for which I have no strong assurances about the security properties. That's why I outlined a conservative strategy of starting with bindings then someday moving to a Rust implementation. 
&gt; If someone decides to port some library to Rust, and said library is originally GPL or LGPL, could it still be included in the Rust distribution? The GPL is copyleft, which means that distributing derivations on GPL'd code requires that such derivations be licensed under the same terms. AFAIK, the LGPL adds an exception to this for dynamic linking. But Rust also supports static linking, and I'm not sure how the LGPL would apply there. I don't know the answers to your other questions.
[For anyone interested in how horrible this can be](https://www.youtube.com/watch?v=-5wpm-gesOY)
"We" are the people in the open source PistonDevelopers community, which is an arm of the Rust gamedev community, which is an arm of the Rust communiy. Here are the people working on or indirectly participating in the project: https://github.com/orgs/PistonDevelopers/teams/pistoncollaborator *Edit: it seems the link only work for people in the organization. The project was started by me (bvssvni) but I got a lot of help from Couvre early with Rust-Graphics and Piston. ccgn joined 3 days ago and contributed with over 6000 loc to rust-image. gmorenz is the mind behind the game loop algorithm, Christiandh ported it to windows and gl-rs, Denommus did a lot of the research work with dependencies and the others has helped collecting information, giving feedback, done lots of pull requests to fix stuff and participated in discussions. Some picked up Rust very recently so they are doing an amazing job. There is no single person controlling everything, I am just encouraging people to come up with ideas and then we set some goals to work toward. Everybody has equal access, but I am the only owner so people can't maliciously transfer the project to their own account. I also got the best picture of what is happening, so they leave the code review to me and often do tasks I set up. As the project grow I will probably leave some of this work to someone else. I am particular interested in Rust-Graphics, Couevre in Rust-Event and ccgn in Rust-Image. Today I talked to mindtree who are intested in audio and who has made software for procedurally generated music and designed the sound for an upcoming indie game called "Limit Theory". I hope he got time to contribute. I am the creator of an animation software called "Stickman &amp; Elemento" which suffers a lot of scaling problems I hope Rust is going to solve. The animation software is used professionally by a few and very successful projects, such as ZombieSmash which reached #1 in App store. It has a lot of potential but unfortunately the technology is outdated. My background is C# programming for many years. I try to use new and untraditional ideas in software architecture and bug the people in the IRC channel from time to time. I am also the writer of Rust-Empty (link in the post). The question about "we" can be answered "we are different people with special interests and obsessed of using Rust". If you have a particular subject you are interested in, you can just start opening issues and I or others will help you get stuff up and running!
The idea that things mean "the opposite" within patterns was also really strange to me at first, but in some places it's really, really useful. As you've noticed, patterns are possible in `let` statements. But where they're also possible is in the argument lists of functions and closures. Here's an example: fn main() { let x = [1u, 2, 3, 4, 5]; let y = x.iter().map(|a| a + a * a - a / a); } ...but, wait, this doesn't compile: error: mismatched types: expected `uint` but found `&amp;uint` The reason is that the closure used by `map` has a single argument, and that argument is a *reference*. To appease the compiler here, we might first do this: fn main() { let x = [1u, 2, 3, 4, 5]; let y = x.iter().map(|a| *a + *a * *a - *a / *a); } ...which works, but it's gross. But remember how patterns do the *opposite* of what they look like? We can exploit that fact to our benefit, to produce this: fn main() { let x = [1u, 2, 3, 4, 5]; let y = x.iter().map(|&amp;a| a + a * a - a / a); } ...which works with the benefit of not being cluttered up with five explicit dereferences.
The first question: https://github.com/PistonDevelopers/rust-event Your use cases might not apply as much to Piston because we use a fixed time step for update and extrapolated time for rendering. This makes it very easy to rely on the application representation. However, we like to track states on a higher level which Couevre has done a lot of research. Rust-Event uses context types and the Observer pattern, but we got some issues with it concerning lifetimes and borrowing rules. Perhaps we will use a cursor expression state design instead (yes, we totally made that up, but it seems to work). We'll see how it goes. I would be very glad if you took a look at it and provided some feedback. It is all in the issue tracker. The second question: https://github.com/PistonDevelopers/rust-graphics I believe 2D graphics on the CPU can not compete with properly designed 3D graphics api to cover the use cases due to rapidly increase in screen resolution, if designed properly. Another problem is that 2D is not simply 2D anymore. Game developers want to combine it with 3D like effects but they want the simplicity of a 2D api. To solve the problem of efficiency and accuracy we use triangle streaming with matrix transformation, so part of the work is done on the CPU. This will make it easier to do CPU deforming effects etc. later. I can't see why Rust-Graphics couldn't have a software rasterizer as back-end and features for this in the BackEnd trait. It is just not high priority right now as I have a specific research goal: Deforming by least square method that requires GPU for real time. Take look here http://bwwd.deviantart.com/gallery/ I am very open to suggestions and I think a software rasterizer is interesting. Feel free to open issues and discuss your ideas!
I'm not sure if Alex used a tool to generate the dependency data in the first place, but the visualization software is almost certainly Graphviz.
From looking at [the documentation](http://doc.rust-lang.org/sync/raw/struct.Semaphore.html) I think an equivalent would be the following: extern crate sync; use sync::{Arc, Semaphore}; static TASK_COUNT: int = 10; fn main() { println!("before spawning tasks"); let sem = Arc::new(Semaphore::new(TASK_COUNT)); for i in range(0, TASK_COUNT) { let task_sem = sem.clone(); task_sem.acquire(); spawn(proc() { println!("task{}", i) task_sem.release(); }); } for _ in range(0, TASK_COUNT) { sem.acquire(); } //this should print after all tasks print println!("after spawning tasks"); } Note that the `Arc` is needed to share the semaphore between different tasks.
The link in your post appears to be to a private page, and 404's for anyone not in the organization.
a) IANAL, but I believe the line is fuzzy, and the answer is generally 'yes'. I would not be comfortable taking a direct port of GPL code. b) No.
Out of curiosity, I created [a small benchmark](https://gist.github.com/riccieri/ea5208c0462fb8f7ed18). Indeed, using a `Semaphore` seems to have about the half of the overhead.
Is it possible to use it with rustc? If so, how?
Umm, well, if that helps you guys. To be clear: - I will not take programs from the Rust repository; - I expect that I will measure and publish Rust programs that are contributed using the benchmarks game ticket tracker.
It's actually almost 4x if you test with 1k or 10k tasks
/r/playrust
I think everything before 1970 century will be wrong. Olson time database only tracks time zones up from Unix epoch. It has some data pre 1970, but it gets wrong really, really fast. For example - A question of how many minutes are in the hour depends, if one considers [Decimal time](http://en.wikipedia.org/wiki/Decimal_time) [A time system so awful it only lasted for eight long years] is considered. If you consider that one it can have 10. History is riddled with shit like this. Not to mention that you can't use Gregorian to calculate old dates, not to mention what the hell did Ancient Egyptian and Ancient Greeks use...
&gt; I will not take programs from the Rust repository; Just to be clear, previously (i.e. a few months ago) this seems to have been false. There have been Rust benchmarks on the shootout, seemingly taken from the Rust repository, they are now not there (prompting this post)... Has something changed recently?
He doesn't need anything - he's messing around. 
I think that statement could mean either of two things. The site maintainer will not try to track down programs for inclusion from anywhere other than the officially supported mechanism for submission. That would mean that you could submit the "Rust repository" programs using the proper procedure, and they would be accepted. Or that statement could mean that there is something unacceptable about the programs which happen to originate from the "Rust repository", and so the site can't accept them (maybe a licensing issue?).
A few months ago, I went out of my way to publish some measurements but that just reminded me how much better things are when we *all* follow the process ;-)
Program authors should contribute their programs directly to the benchmarks game by following the instructions. 
It *does* release the mutex, but without the `signal` call the `wait` call is waiting for something that never comes.
Thank you for doing that previously. :) However, I don't like the implication of &gt; when we *all* follow the process ;-) as if the Rust community was purposely flaunting the process. I certainly had no idea that it was unusual for benchmarks to be taken directly from a language's repository, and I'm sure most other people didn't realise what was happening (or that it was unusual) either. :) It would've been much nicer to just state the whole situation straight out. Your first message could've been: &gt; A few months ago I published some Rust benchmarks without them being submitted via the bug tracker, but this was a maintenance burden so I stopped doing this. I encourage anyone who has written a Rust program to submit it by following these instructions: http://benchmarksgame.alioth.debian.org/play.php#contribute That gives a perfectly reasonable explanation for *why* the Rust benchmarks have disappeared (which is the actual topic of this post, anyway) as well as telling us what we can do to replace them. 
&gt;&gt;when we *all* follow the process ;-) &gt;as if the Rust community was purposely flaunting the process. My take is that igouy is admonishing himself in that quote, so I'd guess that your umbrage is misplaced.
/r/playrust Or, for that matter, http://play.rust-lang.org/…
You could use a [Barrier](http://static.rust-lang.org/doc/master/sync/struct.Barrier.html) extern crate sync; use sync::{Arc,Barrier}; static NTASKS: uint = 10; fn main() { let barrier = Arc::new(Barrier::new(NTASKS + 1)); println!("before spawning tasks"); for i in range(0, NTASKS) { let barrier = barrier.clone(); spawn(proc() { println!("task{}" , i) barrier.wait(); }) } // wait until all tasks reach the barrier barrier.wait(); println!("after spawning tasks"); }
it might not need a format string if it was a variadic function
Correct.
That makes sense, sounds like a responsible course of action. Thank you for the insight.
Yeah, semaphores is another way to do it and it is more correct, but it takes slightly more typing and more abstractions to use it. For simple cases, I think, channels are more than enough, but if you're writing something performant, you should use semaphores.
Perhaps I (or others that have shown interest in this subject) will start with something on top of Rust-Graphics https://github.com/PistonDevelopers/rust-graphics Could you elaborate what you consider a good ui testing framework? There is also a related idea https://github.com/PistonDevelopers/rust-event/issues/43 of designing a GUI that makes it easier to translate intended action back into user input data, so you can use a deterministic model to synchronize a virtual GUI across the network. As a side effect, this reduces the bandwith and designing one protocol for multiple applications. In game development, this means you can design the AI to use the interface to interact with the game (you can dedicate a whole computer to just one NPC character if you like to). This idea is inspired by an indie game called "Limit Theory" (I recommend looking this up on YouTube).
Just an idea I had yesterday https://github.com/PistonDevelopers/rust-event/issues/42 If you like join the discussion, feel free to post comments!
Sorry about misreading it. :)
I don't dismiss Swift because it was written by Apple and I'm sure they have very competent engineers on the project, but it's still a scripting language restricted to the iOS/Mac platforms and only useful for graphical applications with a user interface. Rust and Swift don't solve the same problems.
An alternative here may be to use a `Future`. See http://static.rust-lang.org/doc/master/guide-tasks.html#backgrounding-computations:-futures
But are currently wery slow: running 3 tests test with_barriers ... bench: 99073779 ns/iter (+/- 7977115) test with_channels ... bench: 34947931 ns/iter (+/- 6370735) test with_semaphores ... bench: 10014403 ns/iter (+/- 2052687) test result: ok. 0 passed; 0 failed; 0 ignored; 3 measured 
Why are people referring to Swift as a scripting language? It's compiled to native code and it fills mostly the same roll as Objective C. So I don't see how it's any more of a scripting language than Objective C. 
&gt; The '+' operator is usually used when doing a union of two disjoint sets FWIW, I see it more often as the pairwise sum of two sets, i.e. A + B = { a + b | a in A, b in B }
 rustc --test sem_vs_channels.rs &amp;&amp; ./sem_vs_channels --bench 
I'm messing around with some sort of a fork-join calculation. But I'm sending raw pointers into threads, so I don't need them to return anything, they modify the data in-place.
Thanks. What exactly is happening there, though? Why do you acquire twice, but release once for each task?
Put this one on gist, too. :-)
I mainly do desktop GUI programming for a living (Swing and Qt). For me, the biggest problem is that some bugs are really hard to diagnose and to debug. The two GUI-related bugs I frequently encounter are : 1. Signals ordering problems, when the dynamic interaction of signals (observer pattern) becomes complex. The code depends on a given ordering of some signal, that can be easily broken. It often leads to widgets not being refreshed, and it's hard to track a missing update in debug mode. 2. Mismatch between the data model and the GUI, when using the MVC pattern. This happens when the model breaks the contract it has with the view. This leads to views behaving funny, with no idea where things started to go wrong. I don't really have a solution to prevent these problem to happen, or to debug them more easily.
It seems to me that garbage collection in D does not really give you the safety guarantee. It just requires one innocent-looking delete expression to potentially create dangling references. And I would be surprized if they added runtime checks to safely crash the program in such cases.
https://gist.github.com/arjantop/2d341b9649167f03a394
Debuggability should certainly play a role when designing a UI library. Of course this being /r/rust, we should expect the API written so that some classes of errors will be caught by macros and/or the type system.
I had a similar idea regarding the date/time library. One could sketch a Rust API and implement the stuff via Boost at first, then time after time replace the C++ code with Rust code.
Thanks. Can you explain what happens here: fn with_channels(b: &amp;mut Bencher) { b.iter(|| { You are giving b.iter() a closure with the code you want to benchmark?
I can only assume that anyone who regards swift as a scripting language regards c# as one too, since thats the niche it fills for apple
I am working on a draft to redesign Rust-Event (a Piston project) https://github.com/PistonDevelopers/rust-event/issues/42 It is intended for user input in games, but since it is a step away from the observer pattern we use in the current design, perhaps it might address your concerns as well? The idea is to explicitly define the flow of event and then use a "cursor" structure to evaluate it. For example, if you have lots of buttons you can use an `any` event that triggers when the user presses any of the buttons and implicitly blocks the other buttons from being processed. When the operation is complete, you can create a new "cursor" which can be constructed from the event tree. Instead of forcing the application logic into a GUI framework, you are forcing the GUI framework into the application logic.
Well, events and signals are generally orthogonal concepts. It is rare in desktop GUI programming to use events directly. However, signals are used all over the place to decouple things. For example, a GUI toolkit will generate a "mouse event" when the user clicks a button, but this event will be handled internally by the button GUI widget, that will itself emit a "button pressed" signal. This signal can be listened by anyone. As a GUI developper, you often create your own signals based on lower-level signals (for example converting a "button pressed" signal to a "cancel" signal), and thus build a lot of things based on signals. You rarely handle events directly, and you even more rarely create your own events. I wouldn't know how to program with events only, actually. Maybe I have a lot to learn from game programming.
[Python can be compiled to native code too](https://code.google.com/p/shedskin/). Conversely, Java normally does not compile to native code. The scripting language is just a category based on the usage; many scripting languages are only moderately slower than strictly non-scripting languages thanks to the recent improvements (JS is nowadays less than 5x slower than C for most cases), and when used for the right things, they are often faster. I think Objective-C *can* be categorized as scripting languages due to its dynamism, and so does Swift.
I don't think it's a good idea to refer to Python as a scripting language either, it's much more versatile than that. I guess it's tangential, but really, 'scripting language' isn't a very useful general category.
The semaphore can be `acquire()`d at most `count` times (in this case, `TASK_COUNT`). When it is `acquire()`d the `(count + 1)`th time, it blocks until it is `release()`d somewhere. So that's what's happening here: * Before spawning each task, I acquire the semaphore once. This makes the semaphore counter go to `TASK_COUNT` after the task-spawning loop. * In the task body, I release the semaphore once the "work" is done. This decrements the counter, and will eventually lead it to 0 once all tasks are finished. * After the loop, I acquire the lock `TASK_COUNT` times once again. This will only work end after each task has released the semaphore (otherwise the counter would need to be greater than `TASK_COUNT`)
&gt;I think you'll have a hard time sending pointers across tasks. I'm not sure it's even possible (without unsafe code), as rust can't be sure that they don't overlap. Actually, it seems to work. Unsafely, of course. Check this out: extern crate sync; use sync::{Arc,Barrier}; use std::mem::transmute; static DATA_SIZE: uint = 40; static NUM_CHUNKS: uint = 4; fn main() { let barrier = Arc::new(Barrier::new(NUM_CHUNKS+1)); let mut array = [0i32, ..DATA_SIZE]; //initiate a data set for i in range(0i32, DATA_SIZE as i32) { array[i as uint] = i}; let chunk_size = DATA_SIZE/NUM_CHUNKS; for chunk in array.mut_chunks(chunk_size){ println!("chunk: {}", chunk); let raw_chunk = unsafe {transmute::&lt;_, std::raw::Slice&lt;i32&gt;&gt;(chunk) }; let barrier = barrier.clone(); spawn(proc() { let inner_chunk = unsafe { transmute::&lt;_, &amp;mut [i32]&gt;(raw_chunk) }; //imitate calculation here for i in range(0i32, inner_chunk.len() as i32){ inner_chunk[i as uint] *= inner_chunk[i as uint]; } println!("inner_chunk: {}", inner_chunk); barrier.wait(); }); } barrier.wait(); println!("array: {}", array.as_slice()); } 
That's cool! Maybe you could try to create a safe wrapper around this pattern (to ensure the no-overlap invariant)?
&gt;That's cool! Thanks. That's not my idea though, I'm just blindly adapting stuff other people are proposing. :-) &gt;Maybe you could try to create a safe wrapper around this pattern (to ensure the no-overlap invariant)? I don't have the skill yet. You need to ensure that nothing else creates any more raw pointers to the data while your tasks are running.
Why on earth would they do that? More / different users for LLVM id good for LLVM
Coming from JVM languages, I find this distinction a bit odd. In the JVM, you have high level events and low level events. I always associated signals with OS-level process signals (you know, kill -signal type of thing).
&gt; But are currently wery slow: I wouldn't say that barriers are slow, I would say that they are a bad choice in this case :P. In the case of `with_barrier` you'll end with up to 1_000 blocked tasks (and each one is a native thread!) floating around. In the other two cases only the main task will block and the spawned tasks will finish as soon as they send the message or release the semaphore. It migth be interesting to see this benchmark under the green runtime. 1_000 green threads should be ligther than 1_000 native threads, so I'll expect the `with_barriers` bench to do better. But `with_semaphores` would probably still be the fastest.
`#[bench]` functions follow the setup-benchmark-teardown pattern. Here's an example: #[bench] fn add_assign_1000(b: &amp;mut Bencher) { // setup: create resources needed for the benchmark let xs = Vec::from_fn(1_000, |i| i); let ys = xs.clone(); // benchmark: only this part is measured b.iter(|| { // xs += ys xs.as_mut_slice().add_assign(ys.as_slice()); }) // teardown: xs and ys get automatically `drop`ed (destroyed) here } If you are curious about how the code is actually benchmarked, the implementation is [here](https://github.com/mozilla/rust/blob/master/src/libtest/lib.rs#L1289)
I believe that the `@safe` attribute prevents this? I don't know D that well, but I would assume that it does.
Personally, I'm really excited by all of these new programming languages that compile to native code. We're reaching new levels of saftey, simplicity, and convenience that we never could have reached with c++. The first big difference I think is the memory management. At first, we had D and Go that seemed to start a new category of major convenient languages that compile to native code, but have a pervasive tracing garbage collector. Swift has avoided tracing garbage collection by using automatic reference counting. The downside is that it requires you to think about reference cycles, and break them by using soft references, to avoid memory leaks. And then you have rust, which has taken every approach, and then some: 1. largely static analysis of ownership and aliases 2. Unsafe pointers if you need them 3. Reference counted pointers if you need them 4. Garbage collected pointers, if you need them. There are tradeoffs. Rust is definitely more complicated, and may cause programmers headaches of fighting with lifetimes. But swift's reference counting incurs a runtime cost and has no assurances against memory leaks (the same as rust in the cases where reference counting is used). I'm also interested in how swift doesn't appear to have explicit references. Does it end up being like Java, where you end up with a lot of pointer dereferences, and your data split up among many smaller objects? Rust also has a really great multithreading story to tell. Being able to prevent data races is a pretty huge feature. But I don't think you should underestimate swift. I don't see any reason why swift would remain an apple-only language. I think it creates a new category of simplicity where tracing garbage collection isn't appropriate. But rust might lead to higher performance and higher assurances of correctness. I love the new choices, and I hope c++ will slowly be phased out.
how do you even switch benchmarks to libgreen?
Indeed it's a pretty awful category, as the lines have blurred a lot in the last decade or two. When I look at Swift I see a language with static types and pervasive type inference and native code compilation - none of these things makes me think of scripting languages. I think 'scripting language' actually refers more to what you do with it than any particular characteristics of the language itself.
I'm going to make a wrapper around libressl for now.
C# even compiles to byte code instead of to native code, so “scripting language” works even better for it. (As you can circumvent type safety with reflection)
That's a good question. When using `#[bench]`/`--bench`, the `main` task is hidden, so you can't use the `#[start]` method. I think you'll have to manually use the [pool scheduler](http://static.rust-lang.org/doc/master/green/index.html#using-a-scheduler-pool). (I'll try this later, looks like a good exercise to get familiar with the green runtime)
I've done automated GUI testing with Java and SWT and there is no level where insertion of hooks into the GUI stack can test everything (e.g. what about platform-specific layout or rendering bugs?). We went with a kind of OCR over VNC, which does actually work, i.e. train it on a couple of fonts on a splash-screen, and from then on it can 'read' the display and click where the test tells it to, and the same test runs cross-platform.
I just submitted the link because it's an important RFC. Please no flame wars keep your arguments constructive! :)
This post [here](http://www.java-gaming.org/topics/swift/33427/msg/313920/view) is a pretty good observation of Swift. Although written by a Java programmer, it's a good overview from a non-Apple source.
important change: &gt; I have updated this with `*mut` and `*const`. The only change this RFC is now proposing is to rename `*T` to `*const T` so `mut` luckily stays.
Personally I don't know what would make a good framework, I honestly just started a job/internship and was told to figure out how to use microsoft coded ui testing (for a silverlight app), and have been playin with it for ~5 days. But I can tell you what is painful for me about Bill Gates' way: * It has to refer to a .dll to work (not to terrible) * Everytime it looks for a control it does some search thing starting from the desktop, going through a tree. * It caches controls inconviently (Closed the window and reopened? hah old control instance is cached so you can't click the button now) * The test program sometimes fails to find things on screen. * And worst, it seems to wrap each control in some descendant of UITestcontrol object that isn't a descendant of the actual object you are working with, nor does it really let you get the original control object. I think if the testing was baked into the library we could at least eliminate the last one, maybe make a "test" version of whatever gui library that just had hooks in the event handlers for grabbing events for testing. Other than that finding a better way to have the system find a control would be nice. Of course this is just what I notice/remember after a few days....
Curious now. Is the acyclic property enforced by rust (ie, are circular dependencies disallowed) or is it just the library design?
D'ohh, I see, thank you. I'm still new to lowish-level concurrency primitives.
Love this project.
We can't move post, delete this post and post it again on /r/playrust Also read the sidebar pefore posting from now on
Perhaps a new privacy modifier `crate pub`, which acts like `pub` but is never exposed outside the crate?
Yes, "signal" is a bit ambiguous. That's the term used in Qt, that implements the observer pattern using its signal/slot system. Unix signals are indeed different beasts.
Imo the most important difference is, that Swift needs a runtime to make the dynamic message passing possible. That rules it out for micro controllers.
Graydon (the original designer) apparently took the name from [the Rust fungi](http://en.wikipedia.org/wiki/Rust_%28fungus%29).
I am interested in Rust now, because I was looking for an open source version of Swift. I noticed, that Apple "borrowed" of lot of ideas from other languages. I ignored Rust because of it "misleading" name.
He sounds like a Blub programmer with Stockholm syndrome
I could argue nobody wants Pythons in their codebase either. But actually they do.
Python is not intended for systems programming and pythons are strong...
So why the association to a parasitic fungus?
Dunno for sure, but I always assumed it was a nod to [Mathias Rust](https://en.wikipedia.org/wiki/Mathias_Rust) - it fits with both Graydon's "flying under the radar" approach during early development, and Mozilla's tongue-in-cheek association with Soviet iconography. And, if you want to stretch a point, with "building a bridge" between conservative systems programming and newer language ideas.
I'm not an English speaker, but my theory was that - looking at the language qualities - "rust" was meant to be a pun for "trust" . I'm most probably wrong.
Since we're all (apart from /u/dbaupp) making things up. I like to think of it as a language comprised of a bunch of ideas that are tried and proven, but have been rusting on the scrap heap of research languages.
I haven't played with Julia in a while, so they may be out-of-date observations. Julia is quite different from Rust, both technically and ideologically. Technically, some key differences: - Single threaded, it only supports multi-processing atm. - No shared memory (because there is a single thread) - Dynamic type system, optional type annotations - Multiple dispatch - Garbage collected runtime Julia is designed to a numerical computing language foremost, general purpose second. The whole ecosystem of Julia is designed around this tenant. You'll find dozens of libraries for sparse matrix multiplication, eigenvalue decomposition, stochastic gradient descent algorithms, etc etc. The language itself supports distributed computing on a cluster, but at a very shallow layer (no consensus algo, assumes perfect network, assumes all nodes equal, etc). In short, it's designed for a scientist that needs to get shit done. As an ex-biologist...this is awesome :) But it is very different from Rust. 
Well this association has been mentioned before at least.
AIUI, Julia is being designed from the ground up to be a scientific/statistical language, so will have an edge over Rust in that field. But conversely, this means they aren't focusing on other things like zero-cost memory safety and not needing a runtime at run time. --- I've been thinking Rust might be a good language to write the very core numeric routines (which languages like Julia then call into to get peak performance): - Rust does not require a runtime, i.e. Rust libraries can be used like C libraries inside other languages. - the language is already fast, even though not all the information available is actually being used for optimisations. - there is a lot of pointer aliasing information, maybe allowing for numeric performance similar to Fortran. (This is an example of information that is not being properly used yet.) However, Rust is still new, so I don't have specific examples of this to point at yet.
I actually love the name because of the association. There is a certain appeal to technology that is "old and rusty" (because it bears a certain charme and an air of realiability if it works that long). I love postgres for similar reasons ;). Now, Rust is certainly not an old thing, but it appeals to people that want to learn the "old ways" (low-level systems programming with some memory management).
Completely wrong. But I like your thinking.
I found Julia to be a good fit in mathematics work right until you need to create new temporary items somewhere inside in a loop (which is damn-near always for me), in which case performance really suffered. Which means for the most part it performs well when you're just working on pre-allocated Julia arrays in for loops. Also in Julia, calling closures is about 100 times slower than calling fixed top level functions which don't capture environments (there's an issue for this but I believe it's still open [edit: yes: https://github.com/JuliaLang/julia/issues/1864]). That one was the biggest bummer. This all added up to needing to program in a C like style in Julia when performance is important. In the same role, Rust performed really well. The program took longer to rewrite in Rust because of its lower-level nature, but subsequent development was made so much easier, because 1) the compiler would guide me through changes by highlighting type errors early during refactoring, and 2) I could program in a higher level style, using higher order functions, with very few loops in the entire program 3) the program ran so much faster that I could try realistic problems iteratively during development, instead of waiting and waiting for runs to complete in Julia. This was the reason to translate to Rust and it paid off. The downside of using Rust of course was having to wrestle with the borrow checker during the initial design. Probably this would scare off a lot of mathematics users unless they really like type-oriented programming with generics and such (e.g. anyone doing mathematics with C++ templates would be a good candidate Rust user, but casual Matlab users probably would not). The biggest problem was that at the time (late last year), if you had structures parameterized by lifetimes, then you'd often get internal compiler errors once these structures were composed with each other and passed around. I think these issues have been fixed now, according to the issues closed recently and my latest tests. As far as support for mathematics routines, I didn't miss having builtins because of the huge number of C libraries available. There's a bit of extra friction doing the FFI, but not much, and it also makes clear how to efficiently build the data structures as the native routines really expect them, without expensive translation from a general structure first - this is very important for large structures like sparse matrices where a big sort/dedup operation is usually done before calling C routines in a lot of higher level systems. For linear algebra e.g. I chose Intel's MKL for matrix computations and the performance was excellent. Overall I'm very happy with the Rust solution. I would use Julia again for experimentation but probably not for a big project with code that I plan to actually run a lot. Currently my Rust code is out of date (it's written for 0.8 I think), I'm waiting for closures to settle down before I bring it up to current. Code for both systems: Rust: https://github.com/scharris/WGFEM-Rust Julia: https://github.com/scharris/WGFEA 
I like to say that Rust is the intersection of "trust" and "frustration". :)
[The actual answer](http://www.reddit.com/r/rust/comments/27jvdt/internet_archaeology_the_definitive_endall_source/), as much as there is one.
Yay, biology! Wait! Then why the symbol of the language is a cogwheel?
Nope, I'm just a normal developer these days. When I was in biology, I was a wet-lab scientist: culturing neurons, handling rats, that sort of thing. The most we used software was for creating excel charts for papers :)
Wow, nice work!
Because the Rust team rides bicycles: https://bugzilla.mozilla.org/show_bug.cgi?id=680521
True - I personally know that Rust is a true C/C++ replacement, swift is not. For me the important difference is, no "unsafe blocks", no raw-pointers -which is why Rust remains superior for me. But if you imagine the spectrum of C++ use cases, adding this new point "swift" might pull people in from the uses where rust/swift would have been applicable? .. and consequently subtract from rusts' potential mindshare ? C++ gets used a lot for application code where all its low levelness isn't needed, so for many people it is another c++ alternative (for their subset of its use) What I'm reading is that objC refcounting is superior to general purpose garbage collection. So whilst swift isn't low level, it fills a slightly different slot to Java,C#, and of course it is native code.
Using Rust for production code would be a bad idea; the language changes often and frequently. _This is a good thing_, since we're still looking for the best approach to various problems. &gt; How much would I have to follow Rust development to not be caught by major surprises every release? If you're thinking of only updating to latest Rust on every official release, your code would break and break hard. Tracking Rust `master` branch and updating your code ~weekly is a better idea, but again, don't write prod code in Rust yet.
Thank you for your answer. &gt; don't write prod code in Rust yet. I figured I'd get this answer; I'll just stall the project for the time being, seeing how there really are no alternatives to Rust right now as far as I can tell.
Still, as a name which will one day have to pacify pencil pushers that, yes, this is the language we should pick for our next super important project, it is somewhat unlucky. The first association that comes to mind is one of decay and unreliability, especially if you are an older car owner. And if you then try to be smart and note that *no, it is the fungus!*, then go on about how awesome this third kingdom is, it will end up becoming a C++ or C# project, again.
[Angolmois Rust edition](https://github.com/lifthrasiir/angolmois-rust/) went through six different Rust releases without almost no change on the features, and the changes are as follows: * 0.6 to 0.7 took [1,179 lines](https://github.com/lifthrasiir/angolmois-rust/compare/8eea337...cdcb1eb) of diff. (Angolmois is about 6,000 lines long.) * 0.7 to 0.8 took [1,622 lines](https://github.com/lifthrasiir/angolmois-rust/compare/89b86ea...47af108) of diff. * 0.8 to 0.9 took [1,119 lines](https://github.com/lifthrasiir/angolmois-rust/compare/47af108...661a7d2) of diff. * 0.9 to 0.10 took [2,248 lines](https://github.com/lifthrasiir/angolmois-rust/compare/661a7d2...a05e554) of diff. * 0.10 to 0.11-pre (master) took [1,128 lines](https://github.com/lifthrasiir/angolmois-rust/compare/a05e554...2756aa0) of diff so far. As 0.11 is expected to be released on early July, the final diff would weigh some 1,500 lines. It includes the syntax changes, semantic changes, standard library changes, external library changes (rust-sdl; I indeed upstreamed [some additions](https://github.com/lifthrasiir/angolmois-rust/commit/dfe9877) from Angolmois), bug workarounds and removals and so on. But it seems that the amount of overall change is stable; about 1/4 of the total code has to be changed per each release. (0.9 to 0.10 includes an extensive `Vec&lt;T&gt;` conversion, so it is a sort of outlier.) Fortunately, starting with approximately 0.9, most required changes can be applied by making it compile again (a virtue of strong type system).
I've used D, Python and MATLAB for the past 5 years of my Neuroscience PhD career (with recent simulations being written in Rust), and I've essentially made the same observations. While Python/MATLAB allow you to get started quickly (assuming they have what you need built-in), you rapidly get bogged down in performance and the language issues as your model gets larger. Yes, it took me longer to write my code in D/Rust than the equivalent code in Python/MATLAB, but that time savings was quickly made moot as I continued to tweak the model: most of the time, in my experience, is spent tweaking and running the model and not creating it for the first time. While scientific simulations do emphasize different feature-sets that other applications, ultimately they are still plagued by the same bugs that D/Rust help prevent through structured programming, static typing, ownership etc. At some point, you have to refactor/rearchitecture a scientific model just like you do with any other program. You need a general purpose language for scientific computing. So as a scientific modeler, I really wasn't impressed by Julia (especially not by its 1-based indexing), nor do I see the need for a "dumbed down" language for scientists. What slows my scientific computing is Rust is just the lack of libraries, not the fact that somehow Rust is not in the niche of scientific computing. I've been making some headway in making some [linear algebra](https://github.com/SiegeLord/RustAlgebloat) and [plotting](https://github.com/SiegeLord/RustGnuplot) libraries which I already use for my models, but I'd rather not have to do everything myself as I neither have the time nor expertise to make truly good solutions to the problems I face. My hope is that this pointless "niche" and "Rust doesn't compete with anything" talk disappears, and people start getting encouraged to write scientific computing libraries in Rust in spite of the existence of these purpose-built languages.
&gt; Rust is named after a fungus that is robust, distributed, and parallel. We need this.
Coffee and a powerful animal.
Python was named after Monthy Python.
A name can mean whatever you like. The origin story above is just a historical curiosity. Believing that the name "Rust" refers to rusty metal (or any of the other theories in [this thread over here](http://www.reddit.com/r/rust/comments/27jei5/why_rust/)) is just as legitimate as believing that it refers to an obscure parasitic fungus (*death of the author*, blah blah blah). So I'm not worried about people trying to "correct" anyone. The name is what it is, for good or for ill. Personally I don't associate Rust with decay or unreliability. I associate it with things that are old and well-used, which are both positives when it comes to programming languages in the niche that we're targeting.
And what was Monthy Python named after? :-)
If people are judging the merit of a technology on their liking or disliking of the name, I question if such people should have the power of doing these decisions. I wonder how they would do in a Ruby project, with its Rakes, Capybaras and Cucumbers ;)
&gt; At first, we had D and go that seemed to start a new category of major convenient languages that compile to native code, but have pervasive tracing garbage collector. Many lisps compile to native code, also the ml family and haskell, also oberon(I think).
You could always use [Cyclone](http://cyclone.thelanguage.org/); it's not moving at all, though, so that might be a problem too if you hit implementation bugs.
Had a fun discussion with some DSP engineer coworkers the other day. They do quite a bit of MATLAB modeling, and we discussed how and why various MATLAB performance tweaks worked the way they did. Ultimately they have to translate their models to C or asm anyway to run on their DSPs; it would be great if they could model in a language that could compile directly to efficient DSP assembly.
No matter what explanation the devs will come up with. "Rust" is a terrible name for a new technology and I agree with you it's going to be very weird to propose something called "Rust" to higher management and customer that never heard of it. Rust is associated with old, weak, possible source of infection.. it's a disaster, what were they thinking? But hey.. too late to change it anyway, so they'll have to live with it. 
This is way better than my "Rust for Rubyists commits throughout the ages," thank you! I'm going to be showing this off to people... ([I made a gist](https://gist.github.com/steveklabnik/616e89860446eb2a6732) to share to places that block Reddit.)
You **could**, but do we really want to?
&gt; death of the author Ahhh, you beat me to it. [Here, counterpoint](https://wiki.brown.edu/confluence/download/attachments/74858352/FoucaultWhatIsAnAuthor.pdf). :p
The guys behind [Skylight](http://skylight.io/) recognised Rust’s immaturity and decided to use it despite this because by their analysis there were no reasonable alternatives. It’s certainly *possible* to use it in production, even if it will take a lot of effort and will certainly bite you from time to time.
if they wanted a substring of robust there's tons of other words you can come up with bust, stub, bro, sut, but, bot hmm I see why they went with rust
they love acronyms, so they would most likely choose PHP, LAMP, OOP of course they won't know what those things stand for, but they'll sound cool and technical
Nowadays most of the changes involve moving/renaming stuff from the stdlib (eg ~str/StrBuf -&gt; String). The language still works the same aside from the class names. The next major change might be the &amp;mut/&amp;uniq changes that everyone has been talking about. Normally ungrading code to a new Rust version just involves mechanical renaming of methods and class names, and the occasional insertion of a keyword. The logic of the code doesn't need to change much (last time that was necessary probably was the removal of @). Still not a great idea for production code, but you can of course stick to an old compiler or upgrade every few weeks (a couple of hours of effort).
&gt; The next major change might be the &amp;mut/&amp;uniq changes that everyone has been talking about. I think it's far more likely that the next major change will be various adjustments/improvements to closures (and that `&amp;mut`/`&amp;uniq` will never happen).
Huh, forgot about those. Good point, thanks :)
Nice! I experience the same. I din't look at your code but a python-like yield would make many things easier.
I warn here too: The links of nightlies stored won't be always valid, as the storage on my dedicated server is not unlimited, so I'll have to clean files from time to time.
EDIT: spoke with wycats. He said the below is accurate, and that Tilde is putting three days a week into Cargo, but last week was TC39, so the public repo has lagged a bit. 'Proper' git support (specifying a dependency as a git repository [like this](http://bundler.io/v1.6/git.html) ) is almost done, and they're waiting for that to land before pushing it up. I think most of your expectations are a little too high, because Cargo isn't ready yet. Like Rust, this is what happens when everything is in the open from the start. Give it some time. I have the 'supercontributor' flair and am friends with Yehuda, and even I don't use Cargo. ;) &gt; Is cargo an official Rust project? In the sense that Mozilla is paying Tilde to develop it, yes. It'll be the official package manager when it's done. &gt; Is there any communicated timeline? It'll be ready for 1.0. &gt; What is the policy for tracking Rust master? Every work day. &gt; Can we have any expectations towards the project or should we basically ignore it until it may or may not be finished? Ignore it until it's done, unless you feel strongly about how it's written. Use rust-empty or cargo-lite until then, both have a transition path to cargo for when it's ready. &gt; are there any expectations communicated, checked? Any contractor (for Mozilla or otherwise) generally produces a 'statement of work' which communicates exactly what will be built and on what timeline, along with deadlines and such. I don't think the Cargo SoW is public, though.
As I understand it: * Mozilla hired the Cargo team to make Cargo. * The timeline is basically the Rust timeline of "when its done, don't rush it" * It is in maybe more of an alpha state compared to Rust. All in all, I agree with you, maybe more communication from the Cargo team would be nice. I think we will have the same level of support as rust doc when Cargo is in a usable state.
You probably have to annotate some lifetimes…
&gt; It'll be ready for 1.0. It has to be ready long before that. Stable and main features complete for 1.0 I hope.
This is cool! Remember to add new games to https://github.com/PistonDevelopers/piston/wiki/Games-Made-With-Piston
It could have been worse -- they could have called it "git" (an insult in British English). That doesn't seem to be putting off pencil pushers though.
Say it enough and it just becomes a name. Like everyone got use to saying "git" 20x a day even though it is an insult in British English.
You should be aware of two points: 1. The Obj-C runtime for dynamic message passing is about 40 lines of assembler and a 16 address cache. It's tiny. (http://www.opensource.apple.com/source/objc4/objc4-437/runtime/Messengers.subproj/objc-msg-x86_64.s) 2. Swift only uses dynamic message passing for classes flagged with the @objc attribute Theoretically, Swift could be used with zero runtime overhead and no standard library. To be clear: I'm not saying it's a good choice for such a situation (that's not its focus and I don't even know if you can compile it that way in the current version). But it's theoretically a much lower level, lower overhead language than significant runtime languages like C# or Java.
Like Objective-C and C++ before them, Swift and Rust don't really compete. Swift is a replacement for Objective-C (theoretically low-ish overhead native compiled but closely coupled to a large standard library – Cocoa – which is medium overhead, lots of dynamic memory allocation and focused on UI app design patterns not speed or memory efficiency). Rust is more of a replacement for C++ (lower overhead than Objective-C though requiring greater language domain knowledge to use well). (Rust will also excel at multi-threaded programming in a way that no predecessor does – but that's outside what I'm looking at here.) Both Objective-C and C++ see common use on the Mac without really treading on each other. They're not used for the same problems (C++ for audio, device drivers and games; Objective-C for UI apps). Each *can* solve the other's problems but their libraries won't help you out if you do that. There's not a lot of reason to expect these domain boundaries to shift in the future.
It's designed to run on "bare metal" hardware, and rust is what bare metal does.
Oh yeah, thanks.
General observation on cleaning out old things: please prefer to delete e.g. every second build rather than e.g. the older half of available builds. This is *much* more useful in practice.
&gt; About six years ago, I wrote a really simple function in D that was capable of deconstructing arbitrary types to build serialisation code at compile time. There were similar functions that wrote XML parsers and SQL query generators. Insofar as I am aware, these could not be written in Rust without special compiler support. `#[deriving(Encodable, Decodable)]` gives serialisation at compile time, and `#[deriving]` is just a macro with unusual syntax.
The guy who gave us Bundler is creating a package manager for Rust? God help you.
Although I know where you come from (I've been doing Ruby for more then a decade), this is: a) an answer I exected b) an answer I don't appreciate at all Bundler solved a real problem in the ruby environment - it has its warts and some things might be solved better, but I wouldn't want to go back to the time before it. One of the main reasons for my question is that _I'd like to have something like that in Rust_ and am frustrated by not being able to use it.
&gt; It has to be ready long before that. Stable and main features complete for 1.0 I hope. Stable would also mean: tracking master quickly. Every larger attempt at writing something in Rust tracking master is harmed by a core library/tool tracking master every week only. (FWIW, I do maintain a small library as an experiment - most changes nowadays can be integrated in ~5-10 Minutes).
&gt; spoke with wycats. He said the below is accurate, and that Tilde is putting three days a week into Cargo, but last week was TC39,... Okay, so maybe I was just hitting a particular bad week for taking a closer look. &gt; Ignore it until it's done, unless you feel strongly about how it's written. Use rust-empty or cargo-lite until then, both have a transition path to cargo for when it's ready. Well, I am very interested in the direction, but I just don't want to make the effort of fixing for master first before playing (I do that on too many projects, e.g. when incorporating rust-http). I am perfectly fine with unfinished and changing semantics. But as a statement, that's a good one to work with. Thank you of making the effort of clearing things up!
&gt; One of the main reasons for my question is that I'd like to have something like that in Rust and am frustrated by not being able to use it. I understand that. I'm saying IMO he's the wrong man for the job.
D has varied forms of metaprogramming, from `static if` talked about below, to [mixins](http://dlang.org/mixin.html), which let you define a function which returns a string, execute that function at compile time, and then compile the returned string as source code. It's powerful but not very type-safe at all.
&gt; Please keep unstructured critique to a minimum. &gt; &gt; https://github.com/mozilla/rust/wiki/Note-development-policy#conduct Care to elaborate? This is certainly a position a reasonable person could take, but right now it's basically a flame.
Pythons don't expect the spanish inquisition!
How is that any better than my_macro!(something, blah, blah)? The only real use for methods is dispatch on the first parameter, and that's not necessarily what a macro would do.
IMO - Method call syntax is popular not just for dispatch (in C++ , you can compile-time-dispatch on all parameters) - but for chaining without nesting. `a.foo(b).bar(c).baz(d)` vs `baz(d,bar(c,foo(b,a)))` It's an approximation to infix notation. So, it sticks out less like a sore thumb alongside code that usually tries to stick to method call syntax; sticks out less like a sore thumb compared to inbuilt syntax - (like custom declaration of a struct, but using the macro to roll metadata) So if we're supposed to use macros for some cases other language handle with regular function calls (like n-ary functions, or named arguments), it would make sense that we get the 'method-call' sugar aswell,IMO. Something that matters a lot more in C++ than it should theoretically,IMO, is the asymmetry between function and method syntax. ironically i'd discovered in some ways, C++ is already more open than rust because of function overloading. (its the closed nature of C++ methods i want to move beyond) A big draw to rust (for me) is how with imlps' you have extension methods, so its easier to make use of method call syntax; calling syntax is not conflated with modularity/dependancies/visibility. 
That's funny, I was thinking about this kind of construction, macros with a receiver, a few hours ago. I thought it may be a solution for generating a nice Rust API for the Qt5 bindings, when overloaded methods are involved. Macros can simulate variadic functions, default function arguments, and named function arguments. So, it is somewhat natural to try to do the same on method invocations. Actually, I can't imagine it working. We can indeed add a fourth kind of macro (alongside simple macros, named macros and attributes), but it would not be possible to know the type of the receiver. Maybe there are some uses to this form of macro, but mimicking overloaded methods is unfortunately not one of them.
&gt;&gt;That's funny, I was thinking about this kind of construction perhaps an external event - Swift- got us thinking the same way. (OOP bindings... and other sugar.. why dont macros seem as nice already?). &gt;&gt; "but it would not be possible to know the type of the receiver." But I don't think it has to, for a lot of uses. All you've stated is that the same macro 'pseudo-method' name has the same 'n-ary' / optional-arg behaviour. It can still call an overloaded function of the base same (or related via concat) name on the receiver. e.g. foo.append!(a,b,c) could always be {foo.append(a); foo.append(b); foo.append(c)} All that matters is that the underlying type implements 'append' - I'd be perfectly happy with that. Similarly some set of named parameters - foo.do_something!(x, y); foo.do_something!(x, bar: y) ; foo.do_something!(x, bar: y, baz: z) ; could be.. foo.do_something(x,y); foo.do_something_bar(x,y); foo.do_something_bar_baz(x,y,z); .. and those would dispatch via the receiver just fine. another macro could roll the alternatives you you make_default_arg_dispatch!(foo( bar:&lt;default_expr&gt;, baz: &lt;default_expr&gt; ) for Type::foo_ex) // makes wrappers for Type::foo_ex(a,b), // wrapped as Type::foo(),Type::foo_bar(a), Type::foo_bar_baz(a,b) etc.. // // These supply the given default exprs' for absent parameters // compatable with the above calling convention So users of the same 'pseudo method name' would have to agree on the calling form, but I don't see that as any different to existing macro limitations. 
It'd be in-tree probably.
&gt; I understand that. I'm saying IMO he's the wrong man for the job. That's okay and I understand that some people don't like Yehuda as much for several reasons. Still, he is one of the few people that can actually put "implemented a package manager" on their resume and has a record of successful OS projects (and a couple of unsuccessful ones, sure). So I think that at criticism should be at a higher level then "it's Yehuda, bleh". If you have specific dislikes about bundler, please mention them by name.
You can use the [P-backcompat-lang](https://github.com/mozilla/rust/issues?labels=P-backcompat-lang&amp;milestone=20&amp;page=1&amp;state=open) and [P-backcompat-libs](https://github.com/mozilla/rust/issues?labels=P-backcompat-libs&amp;milestone=20&amp;page=1&amp;state=open) tags on GitHub with the 1.0 milestone to see the breaking changes that are currently targeted for 1.0. The list for P-backcompat-lang is probably more accurate than the list for P-backcompat-libs, in that there will probably be more of the latter created as time goes on.
I agree with your analysis 100%. I'm afraid I wasn't actually answering your question, but just talking about the specific case I had in mind at the moment. The fact that there can be only one definition for a given macro name unfortunately rules out my use case. But I agree these macros may be useful for other use cases.
In my pretty subjective point of view: libraries can(and even should) change, syntax can change(@, ~).. the nevralgic points here would be some semantics consolidationsm or even on the fundamental stones of the language.. that was what i meant with "social contract breakup"
What type safety issues are there in doing this? I'd have thought types would be checked before and after a compile time function is run.
That definitely depends on your way to imagine things. YMMV, there is no reasonable argument to this. This is my personal view and I was never much of a friend of scientific arguments for things that should be left to mood.
I think "not very type-safe at all" was meant that the macros are not very type-aware and could generate code that would not type-check. This is a form of abstraction-leakiness that can cause a great deal of head-scratching and frustration when the resulting type errors refer to code that you didn't directly write. 
IMHO SemVer is the contract you seek. Adding extra overhead before 1.0 seems excessive. If you don't want to keep up with breakage, just wait.
Ah right, thanks. I was thinking that debugging would be a nightmare without a lot of support from the compiler. Is Rust's AST manipulation more type-aware/friendly? I'd like to learn more about it but the basic examples seem a bit intimidating.
&gt; I have a bad feeling sometimes that if some more fundamental features might change, maybe taking out the reasons why i choose the language in the first place... I took that to mean that you fear that the language might change its philosophy - as going hardcore FP, or getting rid of the focus on concurrency. Is that correct? If that's the case, I think you don't need to worry. In general, the core team seems very reluctant in making fundamental changes in the language at this point. If minor syntactical changes are already painful, I find it very hard to believe that there would be some deep philosophical change.
Still, I've wished multiple times that I could make my own traits "derivable".
You can't use `#[deriving]` itself, but you can absolutely make a new syntax extension e.g. `#[libfoo_deriving(Blah, Baz)]`.
Mozilla is funding Cargo with the intent that it becomes the official Rust package manager. The Rust schedule is not blocked on Cargo, but we expect it to mature in time for 1.0. I'd characterize the work so far as prototyping and infrastructure building: the core of the system has been built but - crucially - not completely integrated with git. I expect it to be dogfoodable "soon" and for bold projects to start supporting it shortly thereafter. At that stage you will begin to see people talking about it more; right now it's not quite ready to show off. 
Seems like a useful service. I wonder if we should just bite the bullet and do something like this upstream. Would be useful for projects like piston to just say exactly what revision I should use with the entire framework.
Why? 
Just to be clear, I don't have a whole lot of experience with either Rust's or D's macro systems, but the problems I described are general to macros in typed languages. The problem of preserving types in source-to-source translation is not trivial, and it's generally on the head of the person creating the transformation rule to prove that it's not going to violate type safety. On the other hand, David Herman, who works at Mozilla on Rust and Javascript, wrote his Ph.D. dissertation on the topic of type-hygenic macros; maybe Rust has got a better story for type-analysis of pre-expansion macro forms than most typed languages.
I would love to have a roadmap, and have even written unpublished ones, but so far we haven't been able to get something out. At this stage there are no major semantic changes coming, very few major backwards-incompatible changes, but there will be a fair bit of tweaking. The biggest stuff coming up: * Dynamically-sized types - this is partially finished * Closure changes - these are going to be sugar over library traits, and closure expression syntax is going to be changing to cover a wider set of possible environment passing conventions and capture modes. * Naming and semantics of unsafe pointers - the way unsafe pointers interact with the FFI and safe Rust needs to be formalized. Syntax is changing slightly to hopefully make the semantics clearer. * `box` and allocators - The `box` allocation syntax isn't fully integrated with smart pointers and allocators. The major change you'll see here is that at some point `Rc`, etc. will start being allocated with `box (Rc)` instead of `Rc::new`. * Possibly changes to terminology and focus that could have impact on syntax (`&amp;mut` vs. `&amp;uniq`) but aren't otherwise major. Beyond that there are lots of corners of the type system that need to be massaged a bit, and Niko has a new brilliant unifying idea every week, but the core of the Rust system is solidifying quickly. You are going to continue seeing a high rate of `[breaking-changes]` messages, but these will increasingly be affecting the periphery of the system rather than core semantics, and will be a reflection of Rust's growth, rather than its instability.
I haven't played with it much, but from what I've been reading here and on the list, Rust's macros expand *before* type-checking, so everything is still guaranteed to be type safe. The downside of that is that you can't access type information in your macros though.
No hateful behaviour, especially towards individuals. &gt; There's no need to be mean or rude.
Your example doesn't work due to the fact that the `Map` iterator, references the closure you pass to `map()`. Closures can however only life for the stack frame they are defined in right now. It is relatively easy to implement your own iterator to get around this: https://gist.github.com/Florob/456236b1ec32409a444f
&gt; I was thinking that debugging would be a nightmare without a lot of support from the compiler. At least with D templates and CTFE, it *can* be. That said, you have a pragma to write out messages (for printf-debugging) and static asserts. One thing that sometimes gave me grief was a CTFE function outputting invalid code, which failed to compile, but gave really spurious file locations, since the generated string doesn't really *have* a location. But even *that* can be worked around by generating `#line` directives in the output. It can be a huge pain sometimes, but I've always felt it was worth the grief given how much redundant code it lets you eliminate.
&gt; Do those macros have access to the types of the fields, or are they using some late expansion trickery (i.e. they expand to something that works irrespective of the types until later on in compilation)? No direct access to types, but you do have "AST access", i.e. you can identify tuples and functions etc. "Type based" macros generally work by calling methods from traits, i.e. each field has custom behaviour by implementing certain traits (for `#[deriving]` it's just calling methods from whatever trait you're deriving; for `println!()` it is by calling methods from the various [formatting traits](http://doc.rust-lang.org/master/std/fmt/index.html#traits)). &gt; Can you apply such a macro to a type you didn't define? Nope, can only be on the definition. &gt; Assuming that these are all procedural macros, are there any plans to formalise the procedural macro interface (i.e. the bits of the compiler the macro uses), or are they going to remain notionally rustc-specific? Hopefully. The current interface is definitely not designed to be the final one. &gt; they do work across compilers. (Side comment, but aren't all the D compilers using the same front-end anyway?)
Hi steve.. no problems with breaks or outages... im actually using the head of the repo all the time.. my worries were more directed at the conceptual level of the language.. i think you can see it more clearly in my other posts in this thread.. thanks!
&gt; It's an approximation to infix notation. As an aside, I made a little toy language once that had syntax for infix (and postfix) function notation: |-- These are just random function definitions let prefix(a, b, c) = a+b+c let infix(a, b) = a+b let postfix(a) = -a prefix(a, b, c) a (.infix.) b a (.postfix) I was rather fond of it because it was unambiguous to parse and visually balanced (`a(.dot.)b` versus `a.dot(b)`).
Wait, isnt rubygems (gem) the ruby package manager, bundler is just a "bundler" of many gems?
Long term, Rust will hopefully have "associated items" (related issue: [#5033](https://github.com/mozilla/rust/issues/5033)), meaning something like pub trait Display { type Texture; type VertexBuffer; type Shader; fn build_texture(&amp;self) -&gt; Texture; // etc. } works. Unfortunately, this doesn't help *now*.
I think the point was that such macros, despite being well-typed themselves, could generate code that was not well-typed. The compiler would still catch this, but it would be nice if the compiler could verify that a macro was going to generate well-typed code when the macro itself was compiled. Or, it would at least be nice to be able to report the errors at the abstraction level of the macro itself rather than at the level of the generated code. I'm not sure how feasible either of those things really are; I am under the impression that they're still very much topics of research rather than solved problems, but I am not really following the state of things.
&gt; Or, it would at least be nice to be able to report the errors at the abstraction level of the macro itself rather than at the level of the generated code. I'm not really sure what you're meaning by this, but Rust does connect errors to the macro, [example](http://play.rust-lang.org/?run=1&amp;code=%23![feature%28macro_rules%29]%0A%0Amacro_rules!%20equal%20{%0A%20%20%20%20%28%24e%3A%20expr%29%20%3D%3E%20{%0A%20%20%20%20%20%20%20%20%24e%20%3D%3D%201%0A%20%20%20%20}%0A}%0A%0Afn%20main%28%29%20{%0A%20%20%20%20equal!%282%29%3B%20%2F%2F%20ok%0A%20%20%20%20equal!%28%22whoops%22%29%3B%0A}). The error points into the macro source, but also includes the place where the macro was invoked (the "expansion site" line).
In general yes, but they are a bit interleaved. Bundler, for example, does proper dependency resolution including using a web api if available, which Rubygems doesn't. Recent Rubygems is gaining some features of Bundler. Bundler also does the job of isolating an environment, so that you cannot accidentally load a newer (or older) version into a project after it is bundled. One of the main criticisms to Bundler is that many of those features could be split into smaller libraries (e.g. it ships with a generator for gems with bundler setup and includes tooling for building and releasing those), but that is more a philosophical than an actual technical issue.
Yeah, this is the typical way to do it, and it works great for fairly simple macros. But if you were to get several layers of nested macros with more complex workings, it could be a bit of a mess. Certainly this macro system is 100x more usable than the C preprocessor, though! I'm not complaining. :)
It's nice to see that they have already discussed this, but I don't really understand why they would wait for 1.0 to start implementing this. I have been practicing Rust for about a month now, and this issue alongside with the lack of package manager (cargo) and the only two reasons why I have been sticking to C++ for the moment. 
Oh, I get it. If I forked Rust and implemented the feature myself would they accept it, or do features like this need a discussion process? (note that I don't guarantee that I will be doing it!)
&gt; If you are looking for something to improve your code quality and you come across an article with the headline "Rust" in large letters, would you even read it ? If yes, a lot decision makers don't do this, because they don't have the time to understand every aspect of a problem. I don't actually believe this. Can you provide any evidence that the name is driving people away, and not actually drawing them in? It certainly drew me in when I first heard about the language. If I were a decision-maker (and I am) I wouldn't have time to read every article that comes my way, but if I recognized "Rust" as identifying a new technology that might solve my problem, the quality of that name would not be the determining factor in whether I had time for the article. Granted, I would make an exception for something truly terrible and unprofessional like brainf*ck, but that's exceptional. Now that's just me, but you haven't shown any evidence that the name is actually hurting the language in any way. Counter-proposal: The name is catchy enough to grab people's attention, and incongruous enough with the premise of the language to make people want to dig deeper and find out what it's about. As such it is actually benefitting the language by drawing people in. That's also an unsubstantiated claim, but I see no reason to believe your claim over mine.
I think that you can avoid some trouble by using methods instead of functions. The following is untested: pub trait&lt;T: Texture, VB: VertexBuffer, S: Shader&gt; Display { pub fn do_something(&amp;self) {.... Code goes here ...}; pub new( ... arguments go here ... ) -&gt; Self; // Constructor fn build_texture(&amp;mut self) ; // Assuming this isn't handled by new() fn build_texture(&amp;mut self) ; // Assuming this isn't handled by new() fn build_shader(&amp;mut self) ; // Assuming this isn't handled by new() } 
Nobody does, really.
Has something like Haskells `#[derive(Generic)]` been considered? As far as I've seen it, that provides some kind of opt-in metaprogramming.
+1 
Well, this is my personal experience. We are working on an embedded software project, were Rust would have been a good option. But that I found it was just an accident. I was looking for an open source version of Swift in Wikipedia, where Rust was mentioned and on the LLVM website. It did not attract me, because it sounded esoteric. But it is to late now anyway, we are using C. So there is at least one "victim" of the name. If you do not agree that first impression can be important, you maybe agree, that timing is.
UFCS has nothing to do with macros
&gt; That means that rust is basically a 1:1 map from c++11 features + "unsafe" blocks It's not. C++ has a lot of complexity Rust doesn't have, and, more significantly, there's nothing like the borrow checker in C++ and that is the feature that makes Rust most interesting (the key to memory safety without a GC). &gt; The Julia's approach is what I expect from a language in 2014. The languages have different goals: Julia's approach is probably right for its domain, but it is wrong for Rust's. That is, there's no canonical "modern approach": the "correct" approach for a language is a function of its target domain, not just the era in which it was created. &gt; A final note, if you have to allocate memory in a loop, you don't do it right, irrespective to the language you choose. :) Some languages make it much easier to avoid allocations than others.
As a temporary workaround, I imagine you could use the second solution and then use a typedef to name it?
There is the `macro! ident (...)` form, but I realize now that it is even worse in this case. I wish it would accept paths as well. Then I would rewrite the `router!` macro in Rustful to look somewhat like this and all would be well: let router = router! MySpecialRouterType { "/" =&gt; Get: home, "/about" =&gt; Get: about, ... }; I'm not sure I can do this at the moment.
AIUI, every compatible function is available for CTFE in D, while you have to explicitly opt in in Rust (by writing a procedural macro).
The current trend in C++ is to simplification. I really like to have zero overhead constructs in Rust. I'm just saying that in my opinion, Rust does not worth it in its current state for my use case. But let's be clear: the day Rust allows higher level constructs, it will be awesome! I don't agree with you: in what it is wrong for Rust to allow higher level constructs? Multi-level languages were not even remotely possible less than a few years ago: it was much more easier to call foreign lower-level languages to get higher control over memory. Now, LLVM makes that much more doable (not easy!) and that's what I expect. True for your final point, but a language will never help with all algorithmic deficiencies of the developpers. :)
Yes.
Sorry, I was thinking about deadlocking when using Mutex or RWlocks. Could the same happen with these locks?
Yes, classic deadlock scenarios are possible. Here is a test case provoking it: extern crate sync; use sync::{Arc, Mutex}; fn main() { let x1 = Arc::new(Mutex::new(0)); let y1 = Arc::new(Mutex::new(0)); let x2 = x1.clone(); let y2 = y1.clone(); spawn(proc() { for _ in range(0u, 100) { let i = x2.lock(); let j = y2.lock(); let _ = *i + *j; } }); for _ in range(0u, 100) { let j = y1.lock(); let i = x1.lock(); let _ = *i + *j; } }
In about a year or so, they'll be unified. They were two different programs for historical reasons I don't want to get into here.
From the point of view of writing refactoring tools, macros make life hard. This is unfortunate, but this is really unavoidable if you want to generate arbitrary code at compile time. There would be an ideal language for refactoring where you could essentially treat code, methods, classes, interfaces, etc as structured data and do various 'identity' manipulations on them. Java comes close-ish with Eclipse -- well, close enough to show some of the potential. But I have written Java code-generation code in Java, and obviously that generated code cannot be refactored by tools, just the same as code generated by macros and extensions in Rust. Just it seems that macros are going to get used a lot more in Rust. I think if there are going to be good refactoring tools for Rust, they will have to 'understand' certain macros, and then warn about stuff that they don't understand. Maybe certain classes of macros could be understood by the refactoring tool (at the cost of some implementation complexity), but not arbitrary extensions. There is a kind of balancing act here.
I would just add that there are already smart pointers in C++: unique_ptr can be used in almost the same way as unique pointers in Rust. I'm sure Rust design is superior to C++'s but from the end-user perspective (non rustc developers), the difference is irrelevant. Borrowed pointers are neat. But they come with extra verbosity and complexities (lifetimes, mainly). Again, allowing such things is just great, but from my point of view, those are already a pain to read and maintain. What you did with unsafe code, I would love to be able to do it with low-level code. In other words, everything is high-level and readable except when you need more control or more performance. The choice of low-level everywhere is just not a good idea for prototyping, code readability and code maintainability. The target usage or users subject is irrelevant: in most codes (maybe except in a browser engine :) the bottlenecks are really localized in the code base (just like operations that are actually unsafe). Julia allows just that.
I've just gotten confirmation that we'll be recording the presentation and posting it online afterwards, so don't feel bad if you can't make it! We're starting off slow since this is Pittsburgh's first Rust-centric presentation, so I expect a fairly basic affair with lots of Q&amp;A afterward. I'm excited! And horrified! And I haven't given a presentation since high school! *Dear god, what have I done?*
Yes, you can make a syntax extension that takes an item, like `#[test]` and `#[derive(...)]` , but my case is about instancing.
Deadlocks are not considered unsafe (http://doc.rust-lang.org/rust.html#unsafety). Our type system isn't sophisticated enough for static deadlock prevention.
Say hi to Justin and Carol for me. :)
Would it be possible to implement something like Go's race detector for scenarios like these?
https://code.google.com/p/thread-sanitizer/wiki/DeadlockDetector I don't think we'll have support for this stuff for a *long* time, as it involves a significant amount of backend work and the compiler is too disorganized at the moment.
Yes.
We could even use tsan!
/r/playrust
&gt; The current trend in C++ is to simplification Maybe, but it is still more complicated than Rust. &gt; I really like to have zero overhead constructs in Rust Rust does have these. &gt; Rust does not worth it in its current state for my use case Yes, exactly, it's not worth it for *your use case*, it doesn't make the language wrong/bad as you seem to be suggesting. &gt; the day Rust allows higher level constructs, it will be awesome! What do you mean by "higher level constructs"? &gt; I don't agree with you What don't you agree with? - C++ is more complicated than Rust (this is true) - C++ doesn't have a borrow checker (also true) - The borrow checker is the unique feature of Rust (true) - Rust and Julia have different goals (true) - Julia's approach is wrong for Rust's domain (true: GC is unacceptable for Rust's domain) - There is not one language design that works for *every* situation, i.e. target domain matters for language design (true) &gt; Multi-level languages were not even remotely possible less than a few years ago I'm not sure where you got the idea that Rust isn't a multilevel language: it allows you to write nice high-level code, but then *occasionally* drop down to very low-level code if you really need it (normally for building the high-level abstractions).
&gt; the end-user perspective (non rustc developers), the difference is irrelevant. No it's really not: the language design is very relevant to anyone using Rust (not just compiler developers), because that's what the language is. &gt; Borrowed pointers are neat. But they come with extra verbosity and complexities (lifetimes, mainly). There is definitely additional complexity, but there is not really any other way to avoid GC and achieve both memory safety and efficiency. &gt; In other words, everything is high-level and readable except when you need more control or more performance In my experience, this is true of Rust. &gt; The choice of low-level everywhere is just not a good idea I agree; fortunately Rust doesn't choose this. &gt;maybe except in a browser engine These sort of applications are exactly the domain that Rust is aiming for. i.e. different applications to Julia. --- I'll emphasise my point again: Rust and Julia have different goals; and just being inappropriate for your use case *doesn't* make Rust a broken/poorly designed language, it is just not designed for what you are doing.
Don't worry about it! Only the whole internet will be watching.
Every misspoken word, every source of embarrassment preserved for eternity.
Interesting. As a former full-time game dev in C++, I'm not 100% sure I understood what the specific benefit of Rust's memory model is in terms of loading objects. Typically in an entity system, many objects carry references to each other in the game world, and hooking up those references from some serialized state doesn't seem like a trivial thing to do in Rust, but I could be mistaken. Could you perhaps elaborate? Also, all games that I've worked on have considered a 48 ms lag between user input and visual feedback completely unacceptable. Many gamers will notice that, whereas a 16.6 ms lag (at 60 Hz) is much, much more acceptable. The way parallelism is utilized in game engines is usually task-level, as you mention, though audio should be kept free from the rendering loop as you mention. With careful design, it is possible to run a lot of subsystems in parallel, as long as the foundation for making decisions is separable (i.e., the AI may be able to make decisions about the game world in parallel for several actors). 
I should have specified that this is mostly in regrades to asset loading. Copying data from disk, decompressing it, converting it to any internal format takes time. In rust you can easily move this to an task and asynchronously load it while the main task works away. Once the loading of the asset is complete, you use a channel to send the unpacked data to the main thread. Nothing you can't do with threads in c++ of course. Why rust is a bit nicer then c++ is mostly in memory safety via ownership. A texture that was asynchronously loaded is owned by that task until it is sent to the main task. If the main task gives the texture to the render task to be copied to the gpus memory, the language automatically knows to free the memory after. One idea for loading objects (that I have not tried yet) could have loader tasks send out an object that API is effectively an iterator. Instead of returning borrowed pointers, it can return owned pointers to the data it (once) contained. This is useful because the iterator consumed over several frames, avoiding copying to much data per frame to the gpu. I'm glad you pointed out the difficulty of building an entity system in rust. It is a none trivial problem to solve. If you build it out of GC pointers you can make it work, but you end up with data structure that cannot leave the main task. Arc pointers can probably now be used, since they support Weak pointers. The solution that I decided to go with was to borrow a page from databases and avoid pointers altogether. A reference to another object is implemented using the equivalent of a foreign key. This has some downsides, as it adds a degree of indirection to the operation. But it was an absolute necessity given that entire game state is held in a copy-on-write data structure, so a pointer in an entity would be unsafe to store anyhow. I will be writing much more about how data is structured in Part 2.
Sounds awesome! I've always found TWiR super valuable for keeping me up to date with the development of Rust. I will definitely subscribe to a weekly email newsletter to make sure I don't miss anything!
This sounds awesome! TWiR has been of great help!
&gt; I spend around 3 hours every week sifting through pull requests, reddit discussions, and mailing list posts. Haxe's counterpart of TWiR is developed in GitHub using pull requests: https://github.com/skial/haxe.io so everyone can help.
Using foreign keys and hashmap lookup to access objects is a pattern I've seen used in C as well, and it seems so inefficient to me that GC (any kind!) would be preferable. Or am I missing something? The overhead of doing hashmap lookup of the keys every time you want to access something is one aspect. The other aspect is that you need to clean up the foreign keys and objects manually, i.e. do GC manually in your own code. I wonder whether Rust's libraries could be extended with a reference type which fits your needs? (Digging around in the Rc&lt;&gt; and RefCell&lt;&gt; code shows that the low-level stuff isn't too hard to implement.)
Thanks for the correction. It was just the first thing that popped up in my head.
Hi everyone. This is my first Rust project and Rust is my first systems language, so I would appreciate some feedback. Specifically: I was wondering what the API to this library should luck like. For those who don't know STOMP is a protocol through which you can talk to message queues. For example you can send a message or subscribe to a queue and each time that queue gets a message it will be sent to you. I was wondering how best to do this - at the moment I have something similar to this: fn callback(msg: stompers::Message) { println!("callback got message: {}", msg); } fn main() { let mut conn = stompers::Connection::new("127.0.0.1", 61613).unwrap(); conn.subscribe("a-queue", callback); let msg = stompers::Message::new("a-queue", "hello world"); conn.send_message(msg); println!("Message sent"); // Sleep so that the Task has time to receive and print the message. sleep(50); } Specifically is it a good idea for the callback to be a function or should it be a closure? I orginally avoided closures because I was a bit iffy about exactly how they acted, and they also require lifetimes to be specified. However it seems to me it would be more elegent for them to be closures. Looking around at other implementations, a Python library uses functions, a Ruby one uses blocks and Go uses channels (which I'm guessing is because they are so prevelant?). Which do you think would be the most idiomatic for Rust. Any other feedback is welcome, inclduing if this is the right place to post this. If it's not apologies for waisting your time and I shall promptly delete it. 
Solving it in general, for arbitrary programs, is indeed imposible. But if you add restrictions, you can make some progress. Some references: http://link.springer.com/chapter/10.1007/11817949_16 http://www.sciencedirect.com/science/article/pii/S0890540102931718 There's an entire field of research on devising programming languages whose programs can be proven to terminate: http://en.wikipedia.org/wiki/Total_functional_programming
Here are the timings I got (comparing them with D). The D version is nearest2.d from [here](http://leonardo-m.livejournal.com/111598.html). I didn't use the other ones as they cheat by pre-processing the data. Here are the results: rustc - 4.138s dmd - 3.130s ldc2 - 2.167s Compillation commands: rustc nearest.rs --opt-level 3 dmd -wi -O -release -inline -noboundscheck nearest2.d ldmd2 -wi -unroll-allow-partial -O -release -inline -noboundscheck nearest2.d Versions: rustc - rustc 0.11.0-pre (0ee6a8e 2014-06-09 23:41:53 -0700) dmd - DMD64 D Compiler v2.065 ldc2 - LDC - the LLVM D compiler (0d6c55): based on DMD v2.063.2 and LLVM 3.3 It's pretty silly to be beaten by a compiler from the 70's :P. 
This is fantastic! Thanks for the great work! And, for the record, if for some reason you weren't able to justify spending so much time weekly on this (financially-wise), I'm pretty sure there would be plenty of people that would be willing to pay for a subscription fee
Rust can do OOP, but it'll be different from what you are used to in Java. This is an old article (Rust 0.6) but a lot of the principles still apply: http://joshldavis.com/2013/06/16/the-rise-of-the-gang-of-four-with-rust/ I've been meaning to write an article about "OOP as a newbie in Rust" (because I'm very new and mangling my way through all of this right now), hopefully will get to it soon. Essentially, you build `structs` and then define method on those structs using `implementations`. You can then define common interfaces using `traits`, which act as contracts that implementations must provide. OOP in Rust is more about composition than inheritance. Constructors are often built using static methods on the struct, following a convention rather than enforced by the language. My observations: * Simple OOP is easy, but you'll often find that straight procedural is simpler once you stop thinking of everything as an object. * Abstract classes are not really possible, very clunky work-arounds. Say you have an Abstract class that implements 95% of the functionality, but you want your user to implement the last 5% and then supply the class to your library. This is easy to do in Rust, but requires the user to implement essentially a wrapper around the Abstract struct. It exposes a lot of ugly API to end-users. There are workarounds (using closures, etc) but the resulting API is ugly either way. * Polymorphism and generics are non-trivial and requires hoop-jumping. The mental hurdle for me is that traits != types. I often find myself wanting to declare a variable as a trait: e.v. Vec&lt;Amphibian&gt;, where Amphibian is a trait. I want a vector of amphibians and don't care what the actual implementation is. You can't do this however. Instead, you often have to wrap everything in an enum and then dispatch methods to the underlying type: pub enum Amphibian { Frog(Vec&lt;FrogStruct&gt;), Toad(Vec&lt;ToadStruct&gt;) } impl Amphibian { fn hop(&amp;self) { match *self { Frog(ref f) =&gt; f.hop(), Toad(ref t) =&gt; t.hop() } } } let animal: Amphibian = match some_runtime_call() { true =&gt; FrogStruct::new(), false =&gt; ToadStruct::new() } animal.hop();
But toads *are* frogs!
Is that the version that's parsing a CSV file, or is it the version that's loading a precomputed binary file?
&gt; I didn't use the other ones as they cheat by pre-processing the data. The purpose of my article was to show how to optimize some D code, show problems and more. It was not a mere contest entry. And pre-processing code (very lightly in this case) is often a good idea if you have to call often a program that performs the recognition of handwritten numbers.
&gt; this code compiles with the latest nightly (as of 2014-06-10 12:00 UTC), specifically rustc 0.11.0-pre-nightly (e55f64f 2014-06-09 01:11:58 -0700). Take note, blog post authors: including this information is absolutely essential!
Perhaps it would make sense to add lock_opt() with a timeout. 
nearest2.d loads the CSV file.
Corey's work on TWiR is an enormous boon. The time that he spends on it each week has a multiplicative effect on the productivity of the rest of the Rust community. You have our endless gratitude, Corey!
shhhhh....
This seems like a great idea.
Paging /u/dbaupp, you have some profiling and optimizing to do! :P
&gt; It's pretty silly to be beaten by a compiler from the 70's :P. It's not a fair comparison and it was not meant to be a fair comparison. The nearest2.d version is dumb, it's not what you usually write if you care a lot about performance. The D code is storing the data a ubyte[], while the Rust code uses a pixels: Vec&lt;int&gt;. The distance function in the nearest2.d version doesn't use ranges, unlike the Rust code. You are compiling the D code without array bound checks, but are you doing the same in Rust? And so on. And data preprocessing is something programmers do all the time (outside silly contests).
From the blog post: &gt; I don’t have an F# compiler, so I’ll only compare against the fastest OCaml solution (from the follow-up post), after making the same modification to distance. So the comparison still seems fair, from an algorithmic standpoint.
There's no fairness in love, war and language benchmarks. The comparison is useful to the extent you can draw conclusions about writing naïve functional code in either language. Ideally I'd compare against `nearest1.d` but I could not get it to compile (syntax errors).
You can use default methods to provide partial implementations of traits: trait ResultPrinter { fn get_the_result(&amp;self) -&gt; int; fn print_the_result(&amp;self) { println!("The result is: {}", self.get_the_result()); } } struct MyResultPrinter { result: int, } impl ResultPrinter for MyResultPrinter { fn get_the_result(&amp;self) -&gt; int { self.result } } pub fn main() { let rp = MyResultPrinter { result: 4 // Chosen by a fair dice roll }; rp.print_the_result(); } 
D destroys Rust once again! :o)
That's interesting! So are these closure changes breaking changes? &gt;Fun aside: I made the mistake of running valgrind against my simplest test and was shocked to see how many heap allocations were happening in so short a program. I've been reworking some of my base classes to avoid box and String and it's been eye opening You're clearly being more diligent then me. In my book if a jobs worth doing it's worth doing as poorly as possible :P 
Ah, I didn't think about using default implementation which dispatches to a "virtual" method on the trait. Nice :)
Interesting. Does this basically boil down to the same machine code as using an enum and dispatching yourself?
From hell's heart I stab at thee, Andrei!!! :)
&gt; It's pretty silly to be beaten by a compiler from the 70's :P. Here's the inner loop in Rust: .LBB13_47: testq %rbp, %rbp je .LBB13_51 testq %r14, %r14 je .LBB13_51 testq %r11, %r11 je .LBB13_51 movq (%rbp), %r15 addq $8, %rbp subq (%r11), %r15 addq $8, %r11 imulq %r15, %r15 addq %r15, %rdx addq $-8, %r14 addq $-8, %rbx jne .LBB13_47 And in D: LBB8_2: movzbl (%edx), %edi movzbl (%esi), %ebx subl %ebx, %edi imull %edi, %edi addl %edi, %eax incl %edx incl %esi decl %ecx jne LBB8_2 So the issues are (a) bounds checks; (b) the zip issue; (c) using ints instead of bytes; (d) possibly D being in 32-bit mode while Rust is in 64-bit mode. I think we can fix zip. But this is completely unfair to Rust, because you compiled the D code without the bounds checks without doing the same for Rust.
benh has added a new `zip` method on slices that should fix this. Now it's 1 instruction less than D in the inner loop: http://ix.io/cTV 0x00000001000043f0 &lt;+768&gt;: mov rax,QWORD PTR [r12-0x8] 0x00000001000043f5 &lt;+773&gt;: sub rax,QWORD PTR [rcx] 0x00000001000043f8 &lt;+776&gt;: add rcx,0x8 0x00000001000043fc &lt;+780&gt;: imul rax,rax 0x0000000100004400 &lt;+784&gt;: add r11,rax 0x0000000100004403 &lt;+787&gt;: add r12,0x8 0x0000000100004407 &lt;+791&gt;: cmp r9,r12 0x000000010000440a &lt;+794&gt;: jne 0x1000043f0 &lt;_ZN4main20h5f6b65cf7d878219Tga4v0.0E+768&gt; 
D is not memory safe, so from my perspective it's fair game to disable any and all safety features. The entire point (if any) of this exercise is to check how zero-cost the functional abstractions actually are. But if you insist, I fixed (a) by enabling bounds checks in D, (c) by using bytes for pixels and integers for labels. (d) did not need to be fixed, as it wasn't true. The timings are then: rustc - 3.773s dmd - 4.366s ldc - 2.396s So ultimately, all those things are red-herrings (as the dissassembly also shows) and the real issue is (b). EDIT: I should note that my 'fix' for (a) doesn't address whether Rust was at all affected by bounds checks, rather that D (dmd in particular) does seem to need them for performance. As far as I can tell, there isn't a single bounds check in Rust code anyway, so I wouldn't even know where to put the necessary unsafe functions anyway.
Compiler flags to disable memory safety aren't appropriate for a language that intends to be reliable. We'd rather fix our speed issues in the libraries and the compiler itself, without selling our souls and sacrificing memory safety. :P
There is no flag to remove bounds checks (nor will there ever be, by design decision). You will have to use unsafe code in Rust (unsafe method `unsafe_ref`). Edit: Since it's a design decision, I don't think the bounds checking issue is unfair.
I think it would be fair to allow disabling bounds checking in unsafe code (unless this is already the case?). 
&gt; D is not memory safe, It's not wise to binarize to just true/false the memory safety of a language. D has plenty of memory safeties and other safeties compared to a more unsafe language as C++11, and more are being added.
There are unsafe slice indexing methods that omit bounds checks: http://doc.rust-lang.org/std/slice/trait.ImmutableVector.html#tymethod.unsafe_ref
I think that we should compare Rust to D as it would typically be written. If D programs are typically compiled without bounds checks in production environments, then that's what we'll be measured against, so that's what we must measure ourselves against.
Yes. Yes. Yes. Also yes.
There's a growing number of people who accept functional programming but I don't think it's within the same order of magnitude as OOP, at least yet. Functional code is very nice to work with, and functional languages tend to be very powerful, some (such as Idris) even going so far as to allow you to write provably correct code, and most enforcing totality (that is, unexpected control paths such as a file read failing must be recoverable; OOP languages usually bail out with exceptions which are a side channel which breaks totality: if you don't remember/bother to handle it, the program does something the programmer wasn't expecting). Rust tries to encourage totality - using `fail!` is generally discouraged, the IO library functions return a `Result` type which can either be `Ok(value)` or `Err(an error value)`, and you have to explicitly check which it is (or use `.unwrap()`, which is also discouraged, because it `fail!`s if it is an `Err`). In Rust, you shouldn't try to stick to a particular paradigm like OOP. Things work out best when you use whatever fits the job. Writing parsers maps quite cleanly to functional programming, game engines map to component architectures + data oriented design, procedural is okay for rapid prototyping when you don't have a quick and easy REPL at hand, and OOP is okay for certain things. If you are intent on using OOP in Rust, I suggest shying away from the more dogmatic bits. Don't try to shoehorn patterns into the language, don't try to follow idioms like abstract classes. The Rust OOP, as mentioned in other comments, is mostly a distillation of that which can be implemented without compiler magic. Rust favours ad-hoc polymorphism, in which you can implement any trait for any type (at any point in the code, too: very easy to extent types to support new things). You should try to think of your code in terms of what it really does (and implement corresponding traits for what it does), instead of what it makes sense as a subclass of.
&gt;The Rust code is a nearly-direct translation of the original F# code ... &gt;So the Rust code is about 3.5–4× faster than the OCaml. ... &gt;It’s worth noting that the Rust code is entirely safe I love it when a plan comes together. *&lt;chomps cigar&gt;*
Well, I hope they add inheritance in Rust so you can write struct Toad : Frog { } that way you can start the `Toad` struct with the `Frog` struct without indirection in this manner you can add `Toad`s to a `List&lt;Frog&gt;` or receive `Frog`s out of a `List&lt;Toad&gt;` because you know the proper subtype relation
Rust is not deadlock-safe so it's not concurrency-safe. See? It only works until it makes you look bad.
Rust doesn't advertise itself as concurrency-safe, but does as memory-safe.
While it's good to have a mechanical proof of the absence of certain classes of bugs, generally bugs and mistakes are statistical things. D is a rather memory safe language and a rather safe language in general, and there are plans and discussions to improve its memory safety, improving its type system. The work done on increasing the memory safety (or other forms of safety) of D is not wasted.
Yet another manifestation of [#11751](https://github.com/mozilla/rust/issues/11751): LLVM doesn't understand (non)null pointers enough.
Need to fix [#11751](https://github.com/mozilla/rust/issues/11751), and I'm not a good enough LLVM hacker.
As I said below, this is almost certainly mostly due to [#11751](https://github.com/mozilla/rust/issues/11751): LLVM doesn't understand (non)null pointers well enough.
I was under the impression it was, because of claims like "if you apply the Rust model to concurrency you solve all the concurrency problems too!"
Was the plan to be 33% slower than D? &lt;yanks cigar, throws it in the sink&gt;
I had a vague idea that my knowledge of statistics is limited, but after reading this post I see that it is just absolutely lacking. I haven't recognized any terms but "normal distribution". Any recommendations of statistics books for dummies? I'm comfortable with trig, basic linear algebra and undergrad-level calculus (multiple integrals and whatnot).
I'm not sure I see the Rust connection, given that the article is specifically agitating for these things to be added to C's &lt;math.h&gt;.
FWIW, on my system, the C++ version is faster. Rust may be winning because of the newer LLVM.
&gt; Edit: Since it's a design decision, I don't think the bounds checking issue is unfair. The difference in indexing between Rust and C++ is which operation gets syntactic sugar. In C++, `std::vector::at` has a check, while `[]` does not. In Rust, `[]` has a check while `unsafe_ref` does not. It *is* comparing apples to oranges if you're using the safe operation in one language and not the other. The sugar determines which operation you're going to pick outside of inner loops, but optimized code would avoid the unnecessary checks by using `xs.unsafe_ref(i)` rather than `xs[i]`.
&gt; I was under the impression it was, because of claims like "if you apply the Rust model to concurrency you solve all the concurrency problems too!" I doubt that anyone involved in Rust has made a claim like that. You're just resorting to a straw man. Rust claims to prevent data races outside of `unsafe`, and that claim is true. It doesn't prevent deadlocks or race conditions in general.
There are the `unsafe_ref` and `unsafe_mut_ref` methods, so it's not really manual pointer arithmetic. It's just a bit uglier than using the `[]` syntactic sugar, but Rust doesn't actually have syntactic sugar for the safe operation on `Vec&lt;T&gt;` yet.
&gt; My biggest problem was figuring out how to use arrays. Originally, things just weren't working and I think it's because I was inadvertently copying an array instead of referring to the original. t just couldn't figure out how to create a mutable alias to an array passed into a function by reference. You can either pass a mutable reference to the array, or take a mutable slice from it - with the second one being way more common, and more flexible (as you can take slices from `Vec`s too): let mut my_array = [1u, 2u, 3u, 4u]; take_ref(&amp;mut my_array()); take_slice(my_array.as_mut_slice()); fn take_ref(array: &amp;mut [uint, ..4]) { /* ... */ } fn take_slice(array: &amp;mut [uint]) { /* ... */ } &gt; I understand the reasoning behind explicit integer conversions, but depending on what one is doing, it can add to a lot of explicit conversions, and I also didn't figure out a way to do an unsigned for loop. [There are a number of suffixes](http://doc.rust-lang.org/rust.html#number-literals) you can use to annotate what type a given literal has. On the case of the `for` loop you mentioned (`u` being the suffix for `uint`): for i in range(0u, 10u) { // Here, `i` is a uint } &gt; When creating / using arrays, there is sometimes duplication of the size parameter. Is there a way to reduce that? For fixed-length arrays, the length is part of the type. If your code doesn't particularly care if a given array has a predetermined length, then [you can make your functions operate on slices](http://doc.rust-lang.org/std/slice/index.html) instead. For example, the following function takes a slice and sums all the numbers on it, independent of how many there are: fn sum(items: &amp;[uint]) -&gt; uint { let mut result = 0; for &amp;item in items.iter() { result += item; } result } sum([1, 2, 3, 4].as_slice()); sum([1, 2, 3].as_slice()); sum(vec!(1, 2).as_slice()); // Works with growable vectors too I hope you enjoy learning Rust as much as I am :)
If you're removing the bounds checks in D (i.e. using a less safe dialect), `unsafe_ref` should be used in the Rust code instead of the method with a guaranteed bounds check. Using entirely safe code is optional, and writing only 95% of the code outside of `unsafe` blocks is still significantly better than writing it all in a very unsafe language like C++. Rust's iterators don't optimize down the ideal code because the compiler is currently [unable to communicate a non-null invariant to LLVM](https://github.com/mozilla/rust/issues/11751). It's quite sad, since this would be a non-issue with a different backend like GCC.
It is more general than that, the article talks about any standard math library. Rust currently implements the same functions as C, so it does not include the eight functions the author promotes.
A suggestion: create a gist on github and put the files there. If people want to try out your code, it will be way more convenient :) (they can be downloaded, `git clone`-d, forked etc)
Yes, but he's arguing that going the "C wrapper" route is the **best** choice, not just a pragmatic stopgap: &gt; Given the amount of specialized knowledge required for accurate numerical programming, both tasks are fraught with peril. It would be far better to leave the task of verifying numerical code to the experts, that is, to the existing maintainers of the various standard C mathematical libraries.
`clang --version` gives me `clang version 3.4.1 (tags/RELEASE_34/dot1-final)`, and here rust is still faster (though by 35% instead of OP's 51%)
I tried again without LTO and the C++ version sped up to slightly faster than Rust. Strange :S
&gt; Rust currently implements the same functions as C Rust actually [wraps the C functions of `libm`](http://doc.rust-lang.org/src/std/home/rustbuild/src/rust-buildbot/slave/nightly-linux/build/src/libstd/num/f64.rs.html#31-78) (I don't know if that's what you meant by "implement")
I suggest you really do investigate them, the concept is very powerful - and AFAIK it's way more common to pass slices around than fixed-size arrays. You should also take a look at `Vec`, for growable vectors
&gt; It's pretty silly to be beaten by a compiler from the 70's :P. dmd's code generator dates back to the 80's.
I don't see why you would need inheritance in Rust. There is an interesting [LWN article](http://lwn.net/Articles/548560/) about it from about a year ago.
dmd now sports a new [data flow optimization](https://github.com/D-Programming-Language/dmd/pull/3620) which eliminates many redundant array bounds checks.
I posted [a follow up about error handling](http://huonw.github.io/2014/06/11/error-handling-in-rust-knn-case-study.html). ([Comments](http://www.reddit.com/r/rust/comments/27tuu5/error_handling_in_rust_a_knn_case_study/).)
I want something like what Go has: type Kitchen struct { numOfPlates int } type House struct { Kitchen //anonymous field numOfRooms int } func main() { h := House{Kitchen{10}, 3} //to initialize you have to use composed type name. fmt.Println("House h has this many rooms:", h.numOfRooms) //numOfRooms is a field of House fmt.Println("House h has this many plates:", h.numOfPlates) //numOfPlates is a field of anonymous field Kitchen, so it can be referred to like a field of House fmt.Println("The Kitchen contents of this house are:", h.Kitchen) //we can refer to the embedded struct in its entirety by referring to the name of the struct type } Here `numOfPlates` is directly embedded into the parent struct
Rust could still implement those functions without them being part of the C standard (possibly by wrapping some library).
Ugh, my DNS still doesn't work. When I update my settings, my registrar says "it'll take 24 hours to take effect." soooooo
There's a story on the home page that seems to directly contradict this: http://www.reddit.com/r/rust/comments/27p9ol/a_quick_question_is_it_possible_to_deadlock/ Please correct anyone who says this to you. They're wrong.
By the way, you only have to cast the first element of the vector: let animals = vec!(&amp;frog as &amp;Amphibian, &amp;toad); vs. let animals = vec!(&amp;frog as &amp;Amphibian, &amp;toad as &amp;Amphibian);
I read that thread and I was surprised. That's why I posted this.
In other news, don't use netim to register .rs domains. Their admin tools are _terrible_. Time to wait another 24 hours...
Just because part of the post may be suboptimal doesn't mean that there aren't valuable parts.
Oh, I'm not dissing the post at all. For the goal he states - reliable consistent featureful math everywhere - I think he's right to argue that extending &lt;math.h&gt; is the best solution. Single-language implementations might even make matters worse in that they reduce the pressure for the general fix, probably introduce inconsistency and possibly get things wrong. (On balance I don't think I agree with that second sentence, but it's not an unreasonable argument.) Didn't Rust make more or less the same decision regarding crypto?
I was gonna say... most of my domains resolve in under a minute these days. Maybe .rs's are different.
The philosophy that Rust has arrived at is that unsafe operations should be ergonomically more difficult to use, and be explicitly opted out. To do this, there are a special set of indexing functions that are marked as `unsafe`. The requirement to use `unsafe {}` blocks makes these operations more visible to code reviewers, and are more fine-grained than a global flag.
Gotcha. :) Yes, we did. At least until we get an expert.
&gt; We have the greatest respect for your work and hope to keep up the friendly competition! As someone who used to write D years ago, +10000.
&gt; As someone who used to write D Me too. CTFE is ridiculously awesome. Syntax extensions are a good stop-gap, but nowhere near as expressive. We can draw much inspiration from the hard-won experiences of D.
Understandable confusion, but /r/rust is for the Rust programming language. /r/playrust is for the game Rust, and /r/playrustservers is for servers for the game Rust.
&gt; Whilst you will probably never beat us when it comes to crafting statically checked, typesafe APIs with minimal runtime overhead, We'll see.
[Challenge accepted!](http://www.youtube.com/watch?feature=player_detailpage&amp;v=6lvwZgq4ydc#t=94)
[for reference](http://www.youtube.com/watch?v=FPQlXNH36mI)
Incidentally, that's a very nice macro at just the right level of triviality to make a good example for explanation. Not so trivial that is embarrassing that it's not already in the standard library, while still being easily comprehensible and not contrived.
I wonder if the "breaking change" commit message quotes would be improved by being hover-to-show (or click-to-show), to keep the information density high when just glancing over it (i.e. just the single line summaries by default). (Thanks for TWIR &lt;3 )
Thanks for writing!
&gt;Before you ask: Rust lacks conventional exceptions (since these are hard to make memory safe without a garbage collector, as I understand it); Could anyone elaborate on this? If you use RAII for adquiring/releasing resources in C++ exceptions are "resource safe" (memory is just an example of a resource). P.S.: nice post, i enjoyed it.
I'd do it without macros at all, stdlib gives almost everything but result_from_option (I hope it will be there, btw) fn result_from_option&lt;T, E&gt;(a: Option&lt;T&gt;, f: || -&gt; E) -&gt; Result&lt;T, E&gt; { match a { Some(a) =&gt; Ok(a), _ =&gt; Err(f()) } } fn slurp_file(file: &amp;Path) -&gt; Result&lt;Vec&lt;LabelPixel&gt;, SlurpError&gt; { use std::{result, option}; let file: File = try!(File::open(file).map_err(|e| FailedIo(e))); let mut file = BufferedReader::new(file); let lines = file.lines() .skip(1) .map(|line| { let line = try!(line.map_err(|e| FailedIo(e))); let mut splits = line.as_slice().trim().split(',').map(|x| from_str(x)); // .and_then is flattening Option&lt;Option&lt;int&gt;&gt; to Option&lt;int&gt;. let label: int = try!(result_from_option(splits.next().and_then(|x| x), || InvalidInput)); let pixels: Vec&lt;int&gt; = try!(result_from_option(option::collect(splits), || InvalidInput)); Ok(LabelPixel { label: label, pixels: pixels }) }); result::collect(lines) } 
&gt; `for i in range(0u, 10u) {` BTW, you only need one suffix, the other will infer (e.g. `range(0u, 10)` is fine).
The pseudo-syntax-highlighting is cute, but the current version of the code that performs it is clearly completely broken. 
&gt;There is now a lint that warns when one enum variant is vastly larger than the others. It is allow by default. Very nice to see (although in my opinion allow-default lints are... bleh). Is there a set threshold for what is deemed "vastly larger"?
What makes coding with recoverable exceptions tricky, I think, is that you have to pay a great deal of attention to make sure that the state is completely rewinded in case of an exception (i.e. having no side effects). This is known as the [strong guarantee](https://en.wikipedia.org/wiki/Exception_safety) (Abrahams). What you typically need to do is to do all mutations (that might throw) on *privately owned copies* of the data, and then when all of that is complete do the final switch (that in itself is exception safe). Thus, in case of an exception you can fully recover because the only mutations that took place were on your copies. Example: fn transaction() { // Code that can fail copy_data(); mutate_copy(); mutate_copy_more(); //------------------ // Code that cannot fail swap_copy_with_original(); } 
Same here, it seems LTO is the issue
I updated the perf. results from the original post; I decided to try the C++ code again, without LTO this time, and I was surprised to see that the results improved significantly. Without LTO, I get these results: clang PerformanceTest.cpp dsp.cpp -std=c++11 -ffast-math -O3 -o PerformanceTest Apple LLVM version 5.1 (clang-503.0.40) (based on LLVM 3.4svn) Target: x86_64-apple-darwin13.2.0 Thread model: posix Iterations: 955 C results: 100,043,846 shorts per second. LTO doesn't seem to affect the Rust build's performance. Rust is still very competitive against this result, and I would hope to see the gap narrowed or eliminated as the compiler continues to be improved.
&gt; Am I missing something [...]? It's all a work-in-progress: things are still being iterated on, and improvement is incremental. For your specific example, I have seen /u/burntsushi and others discussing generalising the regexes to work with `char` iterators. I can't track that discussion down now.
&gt; thus are only defined over strings Strings are just sequences of elements of an alphabet, which is just a set of symbols with an equality defined: a regex could theoretically act on any stream of such symbols (it doesn't care if the alphabet is `char` or if it's `BigInt` or whatever). Of course, working out how to write a string to describe such a regex would require an extension of the conventional language. &gt; the current regexp implementation can back-track I don't believe this is true, it uses the same basic [algorithm](http://swtch.com/~rsc/regexp/regexp1.html) as [RE2](http://en.wikipedia.org/wiki/Re2), which doesn't backtrack.
&gt; things are still being iterated on Haha
I second the notion of click-to-show commit messages.
You can put everything in a `Vec` or `HashMap` and build the tree structure as a separate construction using keys. Pointers are just integers which can be replaced by any kind of key you want.
&gt; Ok, then my knowledge is outdated already. If this is the case, great! :-) It was the case forever: libregex has always been non-backtracking. :) &gt; Even if this would be feasible, why would you use such an abomination when it is more readable, debuggable and probably faster to just write the query in code? I was thinking theoretically, not as a serious proposal for libregex. :)
Ok, so my information was not even outdated, but wrong. However [precompiled regexes](http://blog.burntsushi.net/rust-regex-syntax-extensions) currently use backtracking. &gt; I was thinking theoretically, not as a serious proposal for libregex. :) I thought Rust is a deeply practical language. Then again, I've yet to implement something bigger than hello world in it...
Precompiled regexes use precisely the same algorithm as dynamic regexes. The algorithms are not your traditional backtracking regex algos. They are based on the Thompson NFA which bounds worst case time to O(mn). I think there are some very good reasons to let regexes work on streams of characters, but I'm unsure of the utility of generalizing beyond that.
&gt; Regular expressions constitute regular languages Most modern regex implementations are actually context-sensitive, not regular.
And it's a bit nicer than in Ruby, where [1,2,3].map{|x| x.to_s } turns into [1,2,3].map(&amp;:to_s) Though the trick is neat, it's _very_ hard to search for.
&gt; So are these closure changes breaking changes? I believe so. If I've understood things correctly (big 'if'), they're looking to remove a pointer type that's only used by the underlying compiler which guarantees the soundness of closures. There are details in these two RFCs: [1]: https://github.com/rust-lang/rfcs/pull/77 [2]: https://github.com/rust-lang/rfcs/pull/97
That `sum` could also work as a `collect`, yeah?
It's in the commit message. &gt; This commit adds a lint that can warn when the largest variant in an enum is more than 3 times larger than the second-largest variant.
Yes, this is something easy to forget when looking at regexps from a theoretical angle.
Here's the issue: https://github.com/mozilla/rust/issues/14015 I would very much like some input on what the API would look like. Ideally, the number of methods on `Regex` would stay the same while making the search text polymorphic. Maybe a trait that can yield an iterator of characters? Or maybe an `io::Reader` would be better? Not sure.
The situation you describe happens only if: - you have a function that takes some state by mutable reference (in C++ this includes `this` within objects), - there is the possibility of some operation failing (you better always assume that _everything_ can fail), and, - in case something fails, you want to provide the strong exception guarantee (i.e. in case of an error the mutable data remains in the same state as when the function was entered). In this situation, achieving strong exception safety is, as you describe, not tricky at all. Just three steps: - copy the original state (overhead: the original state is like a backup in case something goes wrong), - operate on the copy (no overhead here), - swap the copy with original (this is almost free in C++ &gt;= 11). How does Rust do better than this? Does it avoid the performance impact of creating the copy? I hardly doubt it but I would like to know.