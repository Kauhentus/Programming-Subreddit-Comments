I still really do not follow at all. I have a deb option in the Makefile of all of my Rust projects, and `cargo deb` could easily do the same thing. A `cargo deb` subcommand could be able to automatically generate a `debian/control` and `debian/copyright` just from what is already in the Cargo.toml file right now. You could also automatically generate a changelog from the git log, although optional. One could simply make a list of any additional assets in `Cargo.toml` and where they go in the filesystem. It's not required, but you could also provide a section for a post installation script. It's entirely feasible.
Ah, thanks, I missed that.
But, there's already a bunch of open issues for various parts of the standard library; commenting in those threads is better than opening a new issue, though opening a new one is okay.
Sadly, on a high-DPI display the logo looks [rather ugly :/](http://i.imgur.com/Yz2S86d.png)
I only skimmed through the sources, but I'm guessing [this](https://github.com/polydraw/polydraw/tree/master/src/os) is why.
Except macros aren't powerful enough, so you kinda have to resort to the black art of procedural macros.
I'm not entirely sure which tools your missing as I haven't found anything in particular myself that was missing. Would be great to better understand what you mean by that. Insecure package management only in that packages are not yet signed and verified I assume?
Yea the Tokamak package for Atom is really great. https://atom.io/packages/tokamak
Just added Tokamak for Atom!
Why doesn't Rust compiler catch this leak?
I guess she either called mem::forget on it, she used unsafe code, or she simply introduced a reference counter cycle. Memory Leaks however are considered safe in Rust: http://huonw.github.io/blog/2016/04/memory-leaks-are-memory-safe/
What surprised me most was the compiler team actually *using* this data. Public hosting (github, gitlab, bitbucket) have existed for a while, but Rust is the first language I know of that actually *uses* this code to check for regression pro-actively rather than counting on beta-users to report issues.
Ok, sure. If you define RAII in a way that excludes initialization, then it will not help with initializing values... AFAICT, RAII generally does refer to both ends of the lifetime though, not just destruction. If you've already been doing the first half of RAII though, I can see why you might think it's all about the secound half though. If you spend enough time working with a codebase that does neither end of RAII, I'm sure you'd come to appreciate the first half more. Eg: https://developer.android.com/reference/android/app/Fragment.html#Lifecycle
Yeah, exactly, C++ has type system features that fill the role of Rust's proposed `impl Trait` syntax which allow unboxed returns, but in either case, there's still no garbage collection (since using `Box` is fine). As I'm sure you know.
Please see rule 2.
I don't think so. Perhaps a build.rs can do it.
(The author is a she.)
Sorry about that. Updated.
I don't read Russian, but I see logos for Kaspersky and CoLaboratory. Are they using Rust, or are they just sponsors for the meetup? If it's the former, I'd like to find a way to contact people at those companies to see if they'd be interested in being on the Friends of Rust page at https://www.rust-lang.org/friends.html .
Most of these companies other than Mozilla or uses (like the Dropbox use-case) are still to be of any significance. Some like Autumn are dead and others are wanna be startups.
On holidays and taking a break from my JS compiler project, instead working on a bastard child of the compiler - JSON crate: https://github.com/maciejhirsz/json-rust
&gt; If everyone is welcome, there's no need to specifically enumerate a list of groups. Why list some and not others? It's because you're specifically concerned about those groups, and less concerned about people who aren't a member of those groups Actually, it ist not a list of groups, it's a list of categories that apply to everyone. Everyone has a certain level of experience, a gender, a gender identity, a sexual orientation, (exception: disability), personal apperance, etc. If it would be a list of groups, then it would have been written as: We are committed to providing a friendly, safe and welcoming environment for all, (even|also|especially) for inexperienced people, women, transgender, gays, mentally disabled people, blondes, hobbits, black people, native americans, teenagers, jews, germans, or other similar groups.
&gt; Exceptions are ~6x faster in OCaml than C++ Not in the non-throwing case. Which is why C++ uses unwind tables. &gt; That would be ~10x slower than necessary. No, Rust `Rc` is not atomic. A non-atomic reference count bump is basically equivalent in terms of performance to a write barrier.
Can one make a Debian package that statically links all Rust stuff, but dynamically links all native C stuff (from `-sys` packages)? That way a native C package could do `depends = name-of-its-debian-package` and `cargo-deb` would just build a Debian package that, when installed in any computer, use the Debian libraries (and not C libraries built by cargo). This kind of stuff would be awesome. Also: I think that the package should be saved in `target/deb` or other directory in `target`.
I suggest: https://haveibeenpwned.com/ And enabling 2FA on github: https://help.github.com/articles/about-two-factor-authentication/
Yes, and I checked against haveibeenpwned.com.
This is awesome!
Ok.
`dpkg-buildpackage` still requires that you manually create much of the information. The eventual goal is to not rely upon any Debian packaging utilities but to be written entirely in Rust. I will be fixing it so that it strips Rust binaries automatically soon, and I will ensure that no warnings are given by lintian in the long run. Currently, the only warnings are that there is no changelog file That's a fairly proper Debian package if you ask me.
Rust dynamically links all C stuff by default so you do have to define the dependency in depends. For my systemd-manager application, my depends like looks like so: depends = "libc6, libgtk-3-0 (&gt;= 3.16)"
Not publicly available, but organisation controls list 2FA status for each contributor.
While I'm not a Rust contributor, I'm still wary of enabling 2FA for fear that something will go wrong and I'll be locked out. (And then there are sites like Humble Bundle Inc. where I *can't* enable 2FA because Authy-based implementations provide no way for people without a mobile device to retrieve the raw TOTP secret for inputting into TrayTOTP for KeePass2.) I partially mitigate the reduced security by using not only a unique, hard-random 20-character string for my password each site, but also giving every site a unique e-mail alias as part of my spam tracking and control solution.
&gt; &gt; &gt; In other words, the performance of exceptions in C++ is great as long as you don't use them. That is exactly the right tradeoff. You optimize for the non-throwing case, while making the throwing case possible. Exceptions are exceptional! (I'd also object to the framing of the situation as "the performance is great as long as you don't use them". You *are* "using" exceptions even if you don't throw any during some given run of the program. Just as you're "using" Unix error codes even if by good fortune none of your syscalls happen to fail when you run your app.) &gt; Except you incur that cost whenever you handle a reference rather than whenever you overwrite a reference. Not typically in Rust, thanks to borrowing and moves. You can, and usually do, borrow or move your strong reference.
Note that Rust having 1,400 contributors doesn't mean that there are 1,400 people with commit access. Very few people have commit access to the Rust repo (it would be even fewer if Github had a better permissions system...), and it's exceedingly rare that anyone commits to the Rust repo directly rather than going through the integration bot, so a direct commit would raise red flags and the integration bot has a very small number of registered commit-approvers.
Great job! This should help with getting Rust into APT repositories. Do you have any plans for a RPM equivalent subcommand?
A wild guess: perhaps the implementations are thin wrappers around OS APIs, and do whatever the OS does.
A significant proportion of rust-lang members do not have 2FA enabled. Perhaps we should require 2FA for members. I believe the servo organization already does this.
I highly recommend SuperGenPass. It's not replacement for 2FA, but it's a quick and convenient way to generate unique passwords for every website. It also has an Android app (not sure about iOS). I also recommending changing the password length to at least 12 letters as soon as installed.
Same thing, mostly. With repr(C) rust types are completely compatible with C types. It's still the best practice to convert to non-null Rust types immediately in the interface method. If an existing spec specifies alternate behavior for null values, check for null and turn your pointer into an Option&lt;&amp;x&gt;.
I can't speak for what exactly happens on linux/osx, but on windows, `seek` on a `File` is implemented by calling `SetFilePointerEx`, which allows you to seek past the end of the file and write, at which point the file is automatically extended (of course that won't work if the file was not opened for writing)
&gt; It has been stated here before that writing a cargo-deb command is not feasible, but I am here to prove that wrong. What were the supposed obstacles and how did you surmount them?
This runs `dpkg` and other commands internally. This means it will not run on other linux distros, or other OSes. It would be more useful if the tool could generate the files directly. It is just a file format after all. Some searching shows that `deb` is a compressed `ar` archive of the package data and some metadata files. Given appropriate libraries it should doable in pure Rust. 
&gt; So you're saying it was designed to need a GC and then somehow the fact that it works without a GC now is evidence of a massive failure to meet that design goal? There is no need to speculate about the past. Just read their posts about trying to get GC working on LLVM. &gt; Seems like the designers must be pretty incompetent by that measure. Around 2012, when the Rust developers were posting on the LLVM dev mailing list about their pain when trying to get GC working with LLVM [I pointed them at HLVM](https://twitter.com/jonharrop/status/247787886311788545) because it had already solved the problem in 2007. Just to put that into perspective: Mozilla's team failed to do in months what I had already done by myself in days. They chose not to listen and focused on avoiding GC altogether. Now I come to benchmark GCless Rust and find it is slower than GC'd F# and OCaml. The logical conclusion seems self-evident to me: they prematurely optimised and then threw the baby out with the bath water. I'm not saying anyone is incompetent or that Rust isn't academically interesting but the jury is still out when it comes to whether or not GCless Rust will be any better than existing GC'd languages. The main benefit I can see is getting the few remaining C/C++ developers to use something memory safe so we should see fewer crashes from their software in the future (yay, about time!). 
&gt; That is exactly the right tradeoff. You optimize for the non-throwing case, while making the throwing case possible. Except C++ makes the non-throwing case slow too because you have to unroll the stack because of RAII. You're doing things like calling virtual destructors that don't actually do anything. Hence if you want to write fast C++ code you don't use exceptions and you don't use RAII. You just write C code using `longjmp`. &gt; Exceptions are exceptional! As I said before, that is something I've only ever heard from people who struggle with slow exceptions. If you look at OCaml, for example, exceptions are fast so they use them for control flow. &gt; (I'd also object to the framing of the situation as "the performance is great as long as you don't use them". You are "using" exceptions even if you don't throw any during some given run of the program. Just as you're "using" Unix error codes even if by good fortune none of your syscalls happen to fail when you run your app.) Ok. However you cut it the whole "zero cost abstraction" coolaid is just a triumph of hope over reality. &gt; &gt; Except you incur that cost whenever you handle a reference rather than whenever you overwrite a reference. &gt; &gt; Not typically in Rust, thanks to borrowing and moves. You can, and usually do, borrow or move your strong reference. I think that is more selection bias. You've restricted yourself to cases where that is typical because it is too slow in the general case. Consider trying to use reference counted purely functional data structures in Rust, for example. Won't you borrow asymptotically fewer counted references that you touch? 
&gt; The eventual goal is to not rely upon any Debian packaging utilities but to be written entirely in Rust. https://www.reddit.com/r/rust/comments/4ofiyr/announcing_cargo_deb/d4cb9tf
It was already posted, but discussion over at /r/programming is nothing but subjective poison. I'd like to know what Rustaceans and Rust authors think of Checked C. Also, I tried to comprehend what bugs exactly does it prevent, does it do it statically or dynamically, does it have lifetimes, etc., but couldn't find anything reasonably good in official project docs. If all the project is is just several bounds-checked types, then it has very small scope. Besides, dynamic bounds checking doesn't require any language modification. So I don't get what exactly do they check. Maybe someone can clarify some of these points.
Read the last iteration [here (What's coming up in imag (8))](https://www.reddit.com/r/rust/comments/4mcs8b/whats_coming_up_in_imag_8/)
[I've revoked write access to everyone without 2FA](https://internals.rust-lang.org/t/two-factor-authentication-now-required-for-members-and-r-on-rust-lang-rust-lang-nursery-and-rust-lang-deprecated-repos/3608).
&gt; Except C++ makes the non-throwing case slow too because you have to unroll the stack because of RAII. You're doing things like calling virtual destructors that don't actually do anything. Hence if you want to write fast C++ code you don't use exceptions and you don't use RAII. You just write C code using longjmp. This doesn't make any sense at all to me. First we were talking about the details of exception handling; next you move to RAII, and then you talk about no-op virtual destructors. You're randomly shifting to random problems of C++ (which I don't think I buy either, but anyway) to try to obscure the fact that the exception handling implementation of C++ is the correct one for its domain. Nobody in their right mind writes C++ code using longjmp to work around supposed problems with performance of exceptions. SJLJ exceptions are a toy that people use only when the OS is hostile to proper exception handling. It's never been a serious suggestion floated to handle panic in Rust, for example, because it makes no sense. &gt; As I said before, that is something I've only ever heard from people who struggle with slow exceptions. If you look at OCaml, for example, exceptions are fast so they use them for control flow. Again, exceptions in OCaml are slow, because they are not zero-overhead in the non-throwing case. The use of exceptions for control flow is suboptimal. &gt; Ok. However you cut it the whole "zero cost abstraction" coolaid is just a triumph of hope over reality. Yeah, I'm not just taking your word for that. Zero cost abstractions obviously work; it's the reason why Rust achieves top-notch performance while remaining ergonomic. &gt; Won't you borrow asymptotically fewer counted references that you touch? No. There's no reason to "touch" reference counted data structures at all unless you're appending to them. You're thinking in GC terms, assuming overheads that are only present when you have a garbage collector are inevitable.
To keep the story short - I want to adapt Rust for some microservices for work, and while I really like Serde and what it can do, our APIs (built in Node and Java right now) tend to be chaotic to say the least, thought I could build something that better fits my needs. It so happens I've another project that's a JS compiler, so I could repurpose some code out of it for a parser and code gen. Looking for feedback mostly on the API of the library with data manipulation as opposed to just creating and reading JSON.
It's not that a cargo-deb command that produces a Debian package is infeasible; as you've found, it's quite easy. It's that a cargo-deb command that produces a package *for upload to the Debian archive* is infeasible. Thanks for doing this - it's useful for developers who want to distribute their binaries in a simple way. But it's not likely to be used by Debian maintainers for software in the repositories.
It should be *reasonably* easy to call out to dpkg-shlibdeps to generate the required dependencies automatically...
May be interested in https://github.com/rustbridge/helix Helix allows you to write Ruby classes in Rust without having to write the glue code yourself.
Yeah, I think being able to extract commonly used types into separate declarations would be great. Maybe even have a `IntoIterator&lt;Item=(Key, Val)&gt;` become `HashIterator`. The bigger problem are impls that have several items and way to denote you're implementing trait for certain types instead of just specialized cases `impl&lt;T&gt; Trait for Option&lt;T&gt;` instead `impl Trait for i32`
Aren't all the asserts busted because they'll unwind (!) into Ruby?
Sure! And thanks!
Because sometimes you pull in system resources from outside the scope of the JVM: Sockets, File Handles, etc. Those *should* get released in the destructor for the object, but that is called by the Java GC, and the GC only fires when the Java heap is getting crowded. So if you have more heap space than you do file handles, if you don't implement the AutoClosable interface, you will most likely run into system resource exhaustion. So, in short, you use RAII in GCed languages when dealing with external resources outside the scope of the GC.
I like your library! What I'm kinda missing is some sort of iterator to iterate over objects. As far as I can see you don't have the possibility to iterate over an unknown json object (for whatever reason). Such iterators should also provide you the key not only the value! Something like ``` for val in &amp;json_obj { println!("key: {}, val: {}", val-&gt;key(), val-&gt;value()); } ``` The best json library I have ever used (in c++ though) is https://github.com/nlohmann/json - If you need ideas it might be a good idea to look there for example. I'm new to rust so please excuse if my comment is dumb and your library already supports what I proposed.
On POSIX, exactly what happens depends on what you've opened. A character device (like a serial port) doesn't support seek. A block device (like a hard-drive) has a fixed capacity, and nothing useful happens if you seek past the end of it. An ordinary file lets you seek to any offset, and if you write the file will be extended to that length, but the sectors between the former end of the file and the new end aren't necessarily written to disk; the filesystem just records that a gap exists and the kernel will fake up a bunch of zeros if you try to read from it. So you can have a file containing a terabyte of zeros but using no actual disk-space to store them, which is quite neat. :)
Absolutely. I am of the opinion that anything else is broken (Rust has debug_assert to opt into this behaviour).
Awesome! I will also take this for a spin and provide feedback if you want? I have been looking for something like this for a while. Nice work! (Can probably look at it towards the end of the weekend).
Even unsafe rust trumps a language with C's features. It lacks high level constructs.
I need a solution that works on all distributions, not just Debian ones, especially considering I'm developing this from Arch Linux.
Seems very helpful
The project is kind of strange because they're just ignoring all other languages except for C (including C++ and it's smart pointers). It sort of adds on to C in the same way that TypeScript adds on to JavaScript (which is not a bad strategy). They have two array pointer types (for dynamic or static checking) but it seems like in the vast majority of cases one would need to use the dynamic one. Maybe this will serve as a stepping stone to rust for some people, but I doubt it will actually see much use. Very few people use C by choice, and it seems like those are the only people this project would appeal to.
I'm new to rust and I'm looking for a way to give many instances of different structs access to the "same" global instance of a RemoteProcess struct that manages memory access to a remote process. What is the most elegant way to achieve something like this in rust? In c++ I would just declare an extern unique_ptr&lt;T&gt; and be done with it, how can I achieve something similar with rust? For now I will only work in one thread, so data races etc. are not important at this point.
The last time I needed this, I went with Option 3 (vector and indices as references), because for the problem I could use 32bit indices. In practice there isn't an extra indirection, as the base pointer of the vector will be kept in cache as long as you work on the same graph. Using 32bit indices reduced the total memory needed for the graph by almost half (the data at my nodes were mostly references to other nodes, and a node was larger than a cache line) which also reduced the cache pressure when walking over the graph. I'm not saying that going with 32bit indices will give better performance in general. As there are too many factors influencing this, that's something to benchmark. It did work for my problem though. It comes with some verbosity, but I found the code still very readable.
Makes perfect sense for Microsoft, who presumably has mountains of legacy C lying around. Static checking is easier than porting. 
You use debug_assert. Assert is straight-up saying "I am relying on this for correctness". If you turned off assert you could index out of bounds.
On the contrary, the technical report explicitly calls out adding these checks to existing code and making it easy to do so incrementally.
The types of `a` and `b` are `&amp;mut BigInt`. The [`BigInt` type](http://rust-num.github.io/num/num/struct.BigInt.html) has implementations of `Add` for `BigInt` and `&amp;BigInt`. Using the `&amp;*` re-borrows the `&amp;mut BigInt` as an `&amp;BigInt`, allowing the impl of `Add` for `&amp;BigInt` to be used with the `+`. Just using `*` doesn't work because that would move what `a` references, which is illegal. You could make this a little cleaner by making `a` and `b` be `BigInt` types instead of `&amp;mut BigInt` and only use `&amp;mut` when passing to `swap`. fn ex25() { let mut a = 1.to_bigint().unwrap(); let mut b = 1.to_bigint().unwrap(); let mut idx = 1; let threshold = num::pow(10.to_bigint().unwrap(), 999); while b &lt; threshold { b = &amp;a + &amp;b; std::mem::swap(&amp;mut a, &amp;mut b); idx += 1; } println!("{}", idx); } I also corrected the start of `idx` to `1` and made the threshold an explicit variable so that it's not recalculated each time.
How do I import the rust compiler into my own program? I know its a crate. I know its not thread safe, but what is the crate name? I can't find it on crates.io. I was looking into playing around with `dlfcn.h` bindings. [the crate is done](https://github.com/valarauca/lib_symlink)
I don't know about this. I think it'd probably be somewhere between C and C++, much closer to C++. We don't have a few features that C++ has which, I think, makes C++ a little higher level: HKT, integer generics, for example.
Are you using `debug_assert!` in your own code when you write in Rust? Or are the benchmark numbers you're citing actually in standard library code with asserts you can't change without forking?
I'm sure they'd enjoy a PR to introduce `catch_unwind` at their FFI boundaries, now that it's stable. :)
You can run `rustup target list` to see all of the available toolchains, and it looks like the target is called `arm-unknown-linux-gnueabihf`
Yeah, I should've known to include the compiler error. My bad. Thanks a lot for this explanation. Really cleared up a few things, and I should really have read the `split_whitespace` method more clearly. I also didn't realise that a `HashMap` was essentially a dictionary, so I was trying to make the closest thing to one using a `struct` in a `Vec. Two questions though: 1. When would I want to use a `Vec`? It seems pretty difficult to modify it while iterating, so is it essentially read only, ordered storage? 2. Is there a difference between `to_owned` and `to_string` when working with string slices? Both of these are just different ways to take ownership of a `&amp;str` by making a new `String`, right? 
The only time I needed to create a deb package I simply used `checkinstall`: `checkinstall cargo install` with appropriate root flag. RPM's and Slackware packages can be created in this way too.
&gt; The problem is that you can't use : after an expression. Ah, right. I usually remember that about the time the compiler yells at me :). &gt; There is actually a crate that lets you do just that with serde's Value type already Thanks for the link! Hadn't seen this.
You'd have to use your own custom target via a `json` file and cross-compile a set of crates for that target first. https://github.com/joerg-krause/rust-cross-libs/ Or maybe look at what third parties like Zinc have to offer.
if by bare metal ARM you mean Cortex M microcontrollers, those cross compilation targets are not (yet) in the compiler and can't be installed via rustup. However, there's an [RFC](https://github.com/rust-lang/rfcs/pull/1645) to add them to the compiler and make them easily installable via rustup. (disclaimer: I'm the author of the RFC) if you actually meant bare metal Cortex-A processors as in "I want to build my own OS for ARM", you'll have to write a custom target specification file and cross compile the core crate yourself. I think there's not much information on the topic of custom target specification files, but [Xargo](https://github.com/japaric/xargo) can help with the "cross compile the core crate" part.
&gt; This doesn't make any sense at all to me. First we were talking about the details of exception handling; next you move to RAII, and then you talk about no-op virtual destructors. You're randomly shifting to random problems of C++ The stack is unwound in order to call destructors. That is the relationship between these things. &gt; (which I don't think I buy either, but anyway) to try to obscure the fact that the exception handling implementation of C++ is the correct one for its domain. I'm not saying it isn't the correct one for its domain. I'm saying it is slow. &gt; Nobody in their right mind writes C++ code using longjmp to work around supposed problems with performance of exceptions. SJLJ exceptions are a toy that people use only when the OS is hostile to proper exception handling. It's never been a serious suggestion floated to handle panic in Rust, for example, because it makes no sense. Of course. Panic in Rust is a different kettle of fish. &gt; Again, exceptions in OCaml are slow, because they are not zero-overhead in the non-throwing case. My performance measurements show otherwise so on what basis do you say that? &gt; The use of exceptions for control flow is suboptimal. Exceptions are literally a form of control flow. That is the sole purpose of exceptions from a programming language semantics perspective. &gt; Yeah, I'm not just taking your word for that. Zero cost abstractions obviously work; it's the reason why Rust achieves top-notch performance while remaining ergonomic. I am unable to reconcile your claim with the available data which show that [Rust is slow](https://github.com/c-cube/hashset_benchs). I am as excited as the next person at the prospect of a memory safe non-GC'd language with decent performance and I sincerely hope Rust gets there one day but, let's be honest, Rust isn't there yet. &gt; No. There's no reason to "touch" reference counted data structures at all unless you're appending to them. On the contrary, you just have to retain references to different versions of a collection and the reference counts must *by definition* be updated if they are to reflect the number of references to the data. &gt; You're thinking in GC terms, assuming overheads that are only present when you have a garbage collector are inevitable. What overheads? I was talking specifically about the cost of incrementing and decrementing reference counts (which is the reason reference counting is so slow). 
Thanks! I did not know that the while loop would recalculate the variabel each time as I assumed the compiler would do what you did on it's own.
In release mode it probably would but there are no (or very few?) optimizations in debug mode. For this problem in debug mode it's the difference between a 5 second run and a half second run.
They in fact did say so on the issue I filed :)
I've been informed that about six hours ago a pull request was submitted to Snappy to add a rust plugin.
Say it again y`all!
More eager to see rust-based libraries, utils and apps get into Debian repositories.
Nice move.
The problem is that the situation is actually more complicated than "Rust destructors are like Java's finalizers." In pure Rust code, the only way `b` can prevent `a` from running `A::drop` is by diverging (that is, exiting the program or going into an infinite loop): struct A; impl Drop for A { fn drop(&amp;mut self) { clean up an invariant } } fn a() { let _guard = A; b(); } fn b() { panic!("Let's unwind!"); } If Rust is going to allow setjmp to be used in safe code, the guard pattern won't work, and [BinaryHeap is unsound](https://github.com/rust-lang/rust/blob/master/src/libcollections/binary_heap.rs#L870). 
If the asserts were checking for something that could violate memory safety, disabling asserts is like disabling bounds checks. Even if they don't, they usually are simple boolean checks with little performance penalty (and if they caught a bug before data corruption happens, even a single time, it might as well be worth it)
https://ruslanspivak.com/lsbasi-part1/ this tutorial series may help. You could use the same patterns but I'd suggest understanding what you need to do in each exercise and experimenting with how to implement it (iterative vs functional, traits, etc)
I think that the downvote button shouldn't just stand for "disagree"
I *think* they use Makefiles because their final artifacts are full blown "images" (bootloader + kernel + userspace). OTOH, Cargo can only generate either libraries or binaries/executables. On my Rust on Cortex-M microcontrollers project, I'm exclusively using Cargo because the final artifact is a single executable (a ELF file). That executable can be flashed to hardware or executed under QEMU.
You can use a [build script](http://doc.crates.io/build-script.html), or just write it [directly in the `Cargo.toml`](http://doc.crates.io/build-script.html#overriding-build-scripts).
Hahahahaha this is amazing. I haven't seen this podcast posted before but the idea of show notes in rustdoc form is just brilliant.
See also [this](http://rustbyexample.com/generics/assoc_items.html). They aren't exactly mutually exclusive. Associated types make certain generic types more convenient to handle, especially for the user of the trait. This is particularly useful for information hiding.
If you are to ignore offsets, you can use chars() instead of char_indices().
Thanks, I wasn't sure what to do with that value. 
I'm a Debian (and Ubuntu) maintainer. I think it very unlikely that you'll be able to develop this into something that would make my life as a maintainer easier - particularly if you want this to be uncoupled from the normal Debian tools. One of the benefits that distros bring is integration, and that requires the ability to change based on the current state of the Debian - or Ubuntu! or Mint! - archive. That doesn't mean that it's not valuable, of course. It is! Just not for packaging into Debian repositories.
`reverse` doesn't actually need to consume its argument, so pass a `&amp;str` instead. fn reverse(s: &amp;str) -&gt; String { s.chars().rev().skip(1).collect() }
Oh, thanks. When would it be appropriate to use a String instead of an &amp;str?
I'm very confused with variable 'shadowing'. I thought variables were immutable, but maybe I've misunderstood what immutable really means. For example, if variables are immutable, why does this compile without error?: fn main() { let x = 1; println!("{}", x); let x = 2; println!("{}", x); } Now I know the answer is "shadowing" but to me this is just means variables are mutable. The value of x was 1, and then it was changed to 2. The value of x has changed. Therefore x is mutable. Or I'm completely misunderstanding what mutable means?
That helps me understand a lot of things. Thanks a lot, your explanations are really clear.
Actually each `let` starts its own implicit scope, within which the binding is active. The `x` in your second scope is not the same variable as the `x` in the preceding binding. Note that shadowing is usually used to bind transmuted values or recursive call arguments, so the values at the same name should have something to do with each other. [clippy](https://github.com/Manishearth/rust-clippy) has lints against shadowing, but they are allowed by default.
That's technically not true. The C++ templates are untyped, which makes them equivalent to Rust macros, not generics. https://github.com/paholg/typenum this actually has an equivalent implementation to what C++ does Rust strives to be BETTER in this regard, so type-level integers are still wanted precisely because the template/macro solution is not ideal
I don't think it's going to work with combining codepoints (decomposed forms or combinations which lack a precomposed form). For instance 👳🏾 (should appear as a dark-brown-skinned man wearing a turban) is U+1F473 MAN WITH TURBAN (👳) followed by U+1F3FE EMOJI MODIFIER FITZPATRICK TYPE-5 (🏾). Your reverser seems to work by codepoint only and would thus turn "👳🏾" into "🏾👳". 
I submitted "Radiance HDR image decoding" PR to the image crate.
This is orthogonal to learning Rust, and knowing grapheme clusters ([handled by this crate](https://crates.io/crates/unicode-segmentation)) helps in other languages as well :)
You're saying "embedded". How embedded are we talking? AVR? ARM? Rust's embedded ecosystem is in very early stages right now.
Probably more industrial than embedded. It is an ARM based industrial PC running Linux.
I'm new to Rust myself but I'd say if lack of developers in the area is an issue then chances are that it'd be hard to find a good C++ dev, so I'd lean towards the safer language.
Theres also nim, which is harder to find a programmer but way easier to write. http://nim-lang.org/ http://nim-lang.org/docs/nimc.html#nim-for-embedded-systems
I don't think it is so unreasonable, given that Java, Python, and Javascript all have a documented syntax. But I do agree that having helpful compiler errors would also be good. For that matter, it might be a good idea to just allow them in either order.
Hey, thanks for the podcast. I'm not quite caught up, but I started listening to your podcast last week and its great so far.
why not both? ;)
[removed]
We're lacking a lot of context here. Non-functional requirements: - Do you work in an industry where standards are enforced (MISTRA/JSF/...)? - What's the impact of a crash? Can the program be restarted? - Is performance really an issue? Are we talking hard/soft realtime? Functional requirements: - Is correctness really an issue? Team requirements: - Which languages do you know? And your boss/others likely to have to step in? - Which paradigms do you/they know? - Are you/they willing to learn? --- If performance is not an issue; then there is a whole lot of languages out there which will not crash on you like C or C++ and will probably be easier to recruit for (Java, for example). However, few languages will protect your from an infinite loop... If correctness is an issue; then C++ is a step up from C and Rust a step up from C++ (as long as you don't need units...) however there are other languages out there: Ada, Haskell, Idris, ... As for Rust, coming from C++ I find it somewhat easy to grok, as long as I don't delve into unsafe code (where it's back to maintaining the invariants manually and I'm never too clear on what they are...). Rust definitely has advantages over C++. To cite one: **session types**. A great tool to model state machines, and what program doesn't contain state machines? On the other hand, if you want the power of Boost.Units or Eigen, it's going to be an uphill battle in Rust, and the error messages might not be pretty...
Awesome, thanks. I really like Gankro's book, [Learning Rust With Entirely Too Many Linked Lists](http://cglab.ca/~abeinges/blah/too-many-lists/book/), as a sort of crash course in compiler error messages.
It looks like arguments against rust are pretty well covered, so I'll give an argument for. While you are more likely to find people that already know C++, if you can't (due to location say) it's probably easier to train programmers used to higher level languages in it than C++. Further rust lacks many of the pitfalls of C++ making it slightly less dangerous to have an inexperienced programmer working on the codebase.
That's what I said ;-P Jokes aside, I stand corrected :)
I saw that linked somewhere the other day, but never gave it a go. I'll try it, thanks!
&gt; As we are based in a regional town (in Oz), we find it very difficult to find good developers. My manager is concerned that using Rust will make it even more difficult to find additional developers. I don't think a choice of language really matters if your goal is to hire good developers. A good developer will be able to pick up a new language on the job (if you're willing to let him) just fine. I've seen it happen many times. I frequently hear that "language X will make it hard to hire new people" - well, sure, but that depends on what kind of people you want to hire. You'll certainly get less bad and mediocre programmers, since it's highly unlikely that those kinds of people would go out of their way to learn a new language (or be interested in one) that isn't widely used. There are little-to-no jobs that require those languages, so why bother learning them? Now, there is nothing wrong with hiring mediocre programmers (or being one for that matter). But if you want to hire good programmers, well, a niche programming language should actually help attract them. (So called [Python paradox](http://www.paulgraham.com/pypar.html), from back when Python was still more of a niche language.)
As I said, there is a way to just write these linker args directly in the `Cargo.toml`. I think that besides the linker args, pretty much everything a build script is used for, such as generating code or building libraries, will require a certain amount of code rather than just static configuration; and even the linker args will probably need some code if you want them to be portable. As far as the build script being a Rust source file, rather than arbitrary script, one of the goals of Cargo is to be a workflow tool that encourages a particular standardized workflow. That helps for ecosystem consistency, as well as encouraging portability. The only language that Cargo can assume is available is Rust itself; while software intended for Unix-like platforms can generally assume that a Bourne shell is available, hence autoconf compiling down to a shell script, that isn't true on Windows. There isn't any interpreter that you can assume will be available across all Tier-1 platforms that `rustc` and `cargo` run on. By requiring the build script to be written in Rust, you guarantee that anyone who has Cargo is able to build the build script itself, and allow people to use Rust and Cargo themselves to provide common, portable libraries for common build tasks. For instance, the [gcc crate](https://crates.io/crates/gcc) allows you to invoke the native C compiler, whether that's GCC, clang, or MSVC, and link to the result, including handling more complex situations like cross-compilation that you probably wouldn't handle properly if invoking the compiler by hand, and the [pkg-config crate](https://crates.io/crates/pkg-config) wraps `pkg-config` for finding libraries on the system. And hey, if you really want to invoke a Perl script or something else, you can always just have your build script consist of: use std::path::Path; use std::process::Command; fn main() { let script = Path::new(env!("CARGO_MANIFEST_DIR")).join("my-script.pl"); let status = Command::new(script).status().unwrap(); assert!(status.success()); }
Okay but what if Rust won't compile because it can't find `std`? This is precisely why I want to feed a `-L` argument to `rustc`. If I have to write my build script in Rust then it won't compile.
What's not true? C++ does have higher kinded types. It's templates *are* duck typed, but that doesn't mean that it isn't higher kinded. That's actually something C++ programmers have gotten screwed up by when coming over to Rust, our templates aren't that powerful. http://stackoverflow.com/questions/2565097/higher-kinded-types-with-c
&gt; You seem to be grossly underestimating maintenance efforts for this project. Let's say you get fired or quit or shifted to another project. They pretty much won't be able to find anyone else to work on this project for maintaining it, and then your boss are going to ask "why the hell can't we find anyone?". On the other hand, if a relatively competent developer looks at a codebase in an unknown language and tries to modify it (for example adds an extra thing to the list of things) then I think it is much more likely that he succeeds if the language is rust compared to c++. It is more likely that he accidentally breaks something with c++. Based on my experiences, it is also an option that the program is a mess and undocumented and no one will know what it does after 5 years. The language it was written in will be the least of their problems. Sorry OP, I know nothing about your business and it is not meant to represent it.
I just untarred the Rust installation. I didn't want to run the install script. But setting the environment variable when calling `cargo` fixed that.
What caused the error?
&gt; The reason you're being downvoted is that a total of centuries of experience (summed over various community members) writing performant code disagrees with you. The reason I pressed the downvote button on that comment is that "This basically makes Rust useless to me as a language for anything serious." is an unconstructive and inflammatory way to express onesself. This is especially true since now the complaint is just that the user wishes the distinction between these two statements was better documented.
I know MISRA C fairly well, and I'd pick Rust over MISRA C any day of the week for its actual safety. The counter argument for Rust instead of C++ is a lot of people think they know C++ but actually don't. Given the numerous ways that C++ gives to hang yourself, the argument can be made that it is safer to teach somebody Rust than have someone that halfway knows C++ modify your code. The actual most conservative (NOT the safest) approach would be to use plain C, potentially with some added MISRA rules.
Yes, it does explain that.
Swift is backed by Apple and Apple seems to be pushing it as the new standard for iOS and OSX app development. Considering that there are a lot of people who want to develop apps for apple devices Swift is quickly becoming popular as an alternative to the much-hated Objective C. Rust, on the other hand, isn't a core option for *any* platform. Even though it has Mozilla's backing there's no situation where there aren't at least two or three other languages equally viable to be used. I'm guessing that far more companies are willing to branch out into Swift as a result since it's viewed as a direct step-up from Objective C, whereas companies that are using C or C++ don't have as much confidence that Rust is a viable option.
&gt; Actually, it ist not a list of groups, it's a list of categories that apply to everyone. This is a distinction without a difference. Categories of people define groups of people. The CoC still defines a set of categories that the rust community cares about, and leaves out all the other categories.
Swift: backed by one of the biggest companies in the world as the ideal choice for building apps for their platform. Is practically guaranteed to have a huge ecosystem and numerous developers and jobs within five years. Rust: backed by a nonprofit as a way for them to build a safer web browser in house. Adoption will come organically, if at all. Swift: basically the same programming model as every other garbage-collected imperative language. Rust: forces you to write provably-correct memory management. Swift: used for apps that are likely to have a lifetime of less than five years. Rust: aimed at high-reliability native systems that could easily last fifteen, twenty, thirty years. To pick two semi-random examples from history, look at Java and Python. Java was developed at Sun when Sun was still a powerhouse, had a huge marketing push, and became a language taught to most undergrads less than five years after its release. Python was developed by some guy, and didn't really become popular for about 10 years after its first release.
I was thinking of Ruby, for which the analog to assert (which is just `raise if`) is always on. I suppose there may not be any language with many users for which the word `assert` does not refer to a check that only fails in certain builds. There are certainly lots of ways the docs around this could be improved. The feedback you've given in this comment is good, but buried in a reddit thread - I encourage you to file an issue or even open a PR.
&gt; As we are based in a regional town (in Oz), we find it very difficult to find good developers I get the impression that many of the people in here may be overlooking the sheer difficulty of finding experienced developers in small urban areas (though maybe your area is not so remote as you make it seem?). I grew up in rural Appalachia, and my first programming job was in Middle-Of-Nowhere, Pennsylvania, at a company that had spent a *year* trying to find *any* candidates with prior programming experience, any at all, without luck. I was fresh out of a CS degree (fortunately for me, no employers realize that computer science != programming) and my resume wasn't on the market for eight seconds before it was snapped up by a recruiter who ~~begged~~ asked me to interview for this firm. (The job involved writing RPG-LE, a legendarily arcane language that hasn't been taught in schools since the early 90s, but the only pertinent interview question was "have you ever heard of, or--be still, my heart--*used* HTML before?". Their expectations could not have been lower.) My point is, OP, if you really live in a place where you can't hire experienced devs, then you can ignore the arguments in here about finding good Rust devs in a pinch. Teaching a fresh new programmer Rust is going to be an order of magnitude easier than teaching that same programmer C++, even if that Rust codebase uses unsafe code liberally, which I assume any embedded project will. Of course, there are still plenty of other persuasive arguments against using Rust, especially if your domain is safety-critical (though I'm not sure that specific argument is in favor of C++ either). I feel like I've linked this page about a thousand times this week, but if you're still looking for testimonials of companies that have had positive experiences using Rust, see https://www.rust-lang.org/friends.html (you can mouse over each logo for links to various blog posts and code repositories).
Rust does have duck-typed templates when you implement them as macros. https://github.com/TeXitoi/rust-mdo Technically, Rust macro system is about on par with C++ templates.
Rust hasn't been stable long enough for people to claim they worked in projects that met the market.
Option 4: Don't swim against the current; write a tree/graph container using unsafe pointers.
&gt; Of course swift can be used for apps, but does that explain why there is so much demand for it? Yes. Swift is the "standard" language for coding applications on Apple platforms now and for the foreseeable future, replacing objective-c, so all the obj-c jobs from iOS development are becoming Swift jobs instead.
Two different use cases.
Don't forget to post them on reddit ! I'm passing here offently, and I'd love to read them !
&gt; Hey there im Shredder, one of the leaders of Rage clan, and today we're looking for some new members to join our clan. We ask that you speak good english and use Team speak. Please fill out this application: Age: Timezone: Dedicated player: Amount of hours able to be on a day: pvp skills out of 10: Willing to grind: Maturity level out of 10: And add me on steam http://steamcommunity.com/profiles/76561198263250963/ Okay thank you
With the advantage that the code can at least be parsed without executing the metaprogram ☺
At the moment the Rc count is zero, the object pointed to is dropped. From that point on all `upgrade()` calls from any weak pointer will return `None`. So there's no way to observe any possible future allocation at the position (barring intentionally unsound unsafe code).
Then Rust doesn't have RAII, because only types that implement Drop have destructors in Rust.
CPython isn't GC'd if you don't count RC. Unless you count RC with a bit of cycle detection as GC
There is more demand for iOS apps than for programs you'd typically use Rust for. 
&gt; If everyone is welcome, there's no need to specifically enumerate a list of groups. There is a need to specifically enumerate a list of groups when other projects that claim to welcome everyone don't welcome them. The English "everyone is welcome" is unfortunately not equivalent to `for person in earth.people.all { person.is_welcome = true; }`
&gt;The C++ templates are untyped Not *necessarily* true. You can opt to restrict parameters to certain types with [`type_traits`](http://en.cppreference.com/w/cpp/header/type_traits). In addition to the traditional SFINAE.
&gt; Rust bindings for iOS and Android Can you be more specific? Rust already supports development on both Android and iOS (although, technically, Rust isn't one of the "blessed" languages on iOS so it's probably guaranteed to get your app rejected from the app store).
&gt; Exceptions are literally a form of control flow. That is the sole purpose of exceptions from a programming language semantics perspective. When people say "exceptions are for exceptional cases", they usually aren't talking about the performance of exceptions (I've heard the sentiment in slow dynamic languages where the cost of exceptions isn't relevant to the discussion). In my opinion, people are trying to distinguish between "expected" errors ("I tried to open a file and it wasn't there") which happen usually when interacting with the operating system, and unexpected errors (contract errors or bugs like "bounds check failed"). Rust differentiates between these two errors: the first is `Result` and doesn't automatically propagate at all; the second is `panic!`, which is intended to unwind to an isolation boundary (configured by the application, and at the limit aborts the entire process via panic=abort). In Rust, we wouldn't say "exceptions are for exceptional cases", we'd say "panics are for bugs, while Result is for expected errors". It's easy to follow your nose about "exceptional cases" in Rust because of the fact that an application that embeds your library can configure panics to become aborts. Since this is the case, you would never use panics for recoverable errors in general purpose code. I hope this helps unpack a little bit of what @pcwalton meant when he said "exceptions are for exceptional cases" in Rust, and helps us get more specific than the broad debate about this topic that has been raging for decades (I've said "of course exceptions are control flow! that's their entire point!" myself in the past ;) )
Thank you. 
I have now merged :)
&gt; I tried replacing the hash functions and it makes no difference. Are you kidding?!? You had people doing detailed analysis of different hash functions (with noticeably different performance) *for you* when you posted the benchmark to /r/rust originally. Let me remind you: - https://www.reddit.com/r/rust/comments/4dd5yl/rust_vs_f_hashset_benchmark/d1s2tpp - https://www.reddit.com/r/rust/comments/4dd5yl/rust_vs_f_hashset_benchmark/d1qnn2i The [whole thread](https://www.reddit.com/r/rust/comments/4dd5yl/rust_vs_f_hashset_benchmark/) is a good discussion of the Rust aspects benchmark, which you seem to have conveniently forgotten. 
This may interest you https://github.com/rust-lang/rfcs/pull/1317
Yeah. It's basically the "Java takes up half the programming shelf at the local bookstore because Sun is marketing the **** out of it" effect all over again. (No joke. Maybe 10 years ago, our local Chapters had two of the wall shelving units (roughly 5-6 ft. wide, running from the floor to maybe 8 ft. up) labelled as computer programming and one was entirely Java.)
I would recommend taking `u64` and casting to `usize`. The cast can only overflow on 32-bit as the two types are equivalent on 64-bit machines. You don't even need to check the word size of the machine; an unconditional assert like the following will likely be optimized out (for sieve maximum number `n`) on 64-bit machines: assert!(n &lt; ::std::usize::MAX as u64, "Given sieve max number is too large: {}", n);
Isn't compiling with panic=abort, or just not using asserts (it's not clear to me what they're *actually* accomplishing) the more correct choice here?
aborting would be okay, but then, well, you abort. I haven't dug into the code enough yet to know if the asserts are really correct.
Actually, Microsoft is paying cash for LinkedIn, making that 26.2 billion of real money.
/u/GolDDranks, the code for this one is actually not too hard to follow: https://doc.rust-lang.org/src/alloc/up/src/liballoc/rc.rs.html#448-470
kibwen's probably thinking of a certain infamous rule that existed for all of five months back in 2010: http://mashable.com/2010/09/09/apple-guidelines-developers/#Q7fTov1vbuqi (Today, interpreted languages are allowed iff your app never downloads code for it from the internet - you can include the code in the app bundle or, in the case of a coding app, let the user write it themselves. JIT is blocked by the sandbox. There is a special exception for JavaScript in web views. Yes, the whole thing is dumb.)
Yeah I suspect the asserts are simply incorrect, since this is trying to provide a Ruby interface, and whatever those asserts are intending to do, I can't imagine the original Ruby interface does.
&gt; In every other programming language asserts are used during development and can be disabled in release builds if desired. This is what the majority of people have taught themselves about `assert` because they didn't understand the actual point. Yes you can disable them in release builds if required. You can in Rust also, you just need to use a different method - `debug_assert`. Imagine you are working on the project, you put in some code that you want to assert during production (see the many reasons below for why). Now another developer comes in and puts some test assertions that are extremely slow in. He disables the production flag because he thinks it is an accident (as most people would). Now your very important assertion is removed. That can't happen in Rust. Asserts are for terminating a program when it is in an irreparably broken state. Debug_asserts are for helping a developer code. Review of why you should use asserts in production --- Sources: * https://web.archive.org/web/20090707025230/http://www.ddj.com/blog/cppblog/archives/2007/07/assertions_vers.html * http://stackoverflow.com/a/1081418/1394698 * even here in the same thread, many people misunderstand, but in the comments they come to see why it is for use in production - http://stackoverflow.com/a/1245881/1394698 * http://stackoverflow.com/a/1081442/1394698 same thread * http://stackoverflow.com/questions/17732/when-should-assertions-stay-in-production-code * http://c2.com/cgi/wiki?ShipWithAssertionsOn * http://c2.com/cgi/wiki?DoNotUseAssertions (kind of a backwards title huh) * http://docs.oracle.com/javase/1.5.0/docs/guide/language/assert.html - 'Error was more appropriate to discourage programmers from attempting to recover from assertion failures. It is, in general, difficult or impossible to localize the source of an assertion failure. Such a failure indicates that the program is operating "outside of known space," and attempts to continue execution are likely to be harmful' Note that `assert` is disabled in production by default in Java, because *most people do not need this kind of critical protection - they should just use exceptions* A lot of my sources have conflicting views, but think about it this way. You could leave them in, and have a backup, or remove them and remove that backup. It's pointless to strip assertions out during production because then you remove the entire point of `assert`! If you can't predict the problem, then you have no clue when it could happen! If you have no clue when it could happen then you need it in production! I could keep going on and on about why you should use asserts in production (unless they are causing noticeable performance problems). edit: sorry if this came across as rude, I'm just super happy that the developers of Rust understood the point of assert, and the difference between debugging with assertions and assertions for the point of unexplainable bugs.
Rust really isn't even stable. It seems that almost every project requires a nightly build right now.
From what I see a lot of companies are using Rust, but moving existing engineers over. Apple has declared that Swift is the next language for their platform, so all ios shops (of which there are tons) will have to switch eventually with no choice in the matter. Many are starting to switch now. Rust has no such impetus. C++ shops can continue to use C++ with no change in its applicability. Rust may make life easier, but there's a cost to switching which must be considered. In Swift's case that cost is inevitable.
Correct, cells (anything based on `UnsafeCell`) are not `Sync`. Arguably, things that are `Sync` should be named differently.
I'm [working on one](https://github.com/pcwalton/libui-rs). It's designed to feel as much like Rust as possible.
This is a terminology issue. Academia leans towards rc being a form of gc, software dev leans towards tracing GC being the only kind of GC.
Which projects are these? I haven't felt this to be the case in a long time. I use a stable compiler by default.
Is there a blog-post or documentation somewhere describing "how to develop your app/library on Rust Stable while keeping Rust Nightly around for clippy"?
&gt; Another question is, how easy would it be to take someone on remotely? /u/tl8roy: Where are you in Australia? Perhaps you could explore the talent coming to the meetups around the place? There seems to be a growing amount of interest here.
South Queensland. I have been looking for groups or meetups but haven't found any yet. Remote access certainly is an option. The client isn't local to our area, so we will be in a way, already remote.
Is this going to be usable outside of intellij? e.g. From Emacs?
Just a friendly note that this can be hard in many cases such as corporate build systems. It's hard enough getting one version integrated.
No. But it can be used in IDEA Community Edition.
That [final line of the OP](https://en.m.wikipedia.org/wiki/Flying_Spaghetti_Monster) suggests to me that he might be living at the edge of a beer volcano. So it might be drink or drown ;)
Ahh, that explains it. Thanks. And thanks for /u/oconnor663 for the link.
&gt; The results have been independently verified by many people using many different hash functions. Not true, [this reply](https://www.reddit.com/r/rust/comments/4dd5yl/rust_vs_f_hashset_benchmark/d1qnn2i) on the original discussion replaces the hash function and nearly doubles the speed (2.6s -&gt; 1.4s).
DAO tokens!
The opening kind of rubs me the wrong way: &gt; What I observe in the industry today are software engineers that are fan-{boys, girls} with their languages. They say stuff like “My language is superior because it has generics” or “There is too much cruft in C++ for me to justify using it.” These fan-{boys, girls} engage in discussions and spend hours trying to convince each other that their language is “the best at everything” while the rest of us are building shit and getting things done. The point the author seems to be trying to make is that being a polyglot is a good thing, because you can learn new things from different languages. Which I agree with. ...But it comes off as an argument to moderation: "Having a language preference is bad and you should feel bad. No language is better than any other!" Why can't we just let people enjoy the languages they like without shaming them for it? Edit: the rest of the article is great though!
Unfortunately same thing here for me with YCM, don't know why really. It crashes often and completions don't work, doing a racer complete myself works just fine... VSCode with RustyCode seems to be the best implementation for now... Really wish vim worked though, as VSCode doesn't work well on the computer I'm using atm. ::Edit:: now when I think about it, I think the best tool is in racer for Atom. I've had the most success with it comparing with VS and vim.
An example from the rust book: let x = 1; let c = 'c'; match c { x =&gt; println!("x: {} c: {}", x, c), } println!("x: {}", x) Returns: x: c c: c x: 1 I don't understand why match even gets a match here? The value of c is 'c', shouldn't it fail to match? In fact shouldn't it warn that there's no _ catch all at the end, and also fail to match?
You've encountered the same thing as [this person](https://www.reddit.com/r/rust/comments/4nu1kj/hey_rustaceans_got_an_easy_question_ask_here/d4bfwu4) from further down this same thread. You can't have a non-constant value in a match pattern. What you're doing here is just unconditionally renaming `c` as `x` within that `match` arm, as bare identifiers in match arms are treated as a catch-all that renames the value being matched on. So you're not actually testing anything here. 
That explains it perfectly, thank you!
I'm using YCM, it's the same situation for me.
&gt; It is not a "stupid design flaw". If that's how your hash tables work then it is a stupid design flaw in your hash table implementation. &gt; It only matters with silly poor hash functions that nobody would use in practice. Well, that was the only feasible explanation I heard as to why Rust is so slow on that benchmark. If you can provide a better hash function for which Rust is competitively performance please do. &gt; What a ridiculous claim. Of course you're arguing that F# is better than the language discussed here (which is what you have been doing in language communities for over a decade), and you're doing so in a plainly disingenuous way. Ad hominem. Let's focus on the science: * [Here](https://gist.github.com/jdh30/48d041b5f1558fcc3aee77cd6cc860b4) is a Rust implementation using a simple hash that takes 55s. * [Here](https://gist.github.com/jdh30/05c6cb6adc4861dc6db133163bb9a6fa) is an F# implementation using a simple hash that takes 17.5s. Same algorithm. Same hash. Rust is over 3x slower than a GC'd language. How do you reconcile that with your assertion that Rust is "blazingly fast" and "achieves top-notch performance"? Put up or shut up. **EDIT** I didn't think so. 
Very good points. I think what is also only very rarely mentioned is that in order to be productive with C or C++, you need to have a good understanding of build tools like Makefiles or cmake, and understand how you have to arrange your C++ project with header files and all that jazz, linking, etc. I believe that learning rust - and becoming productive in it - is magnitudes faster than C++, let alone coming into an existing project and figuring out its internal structure. Rust projects are, by design, very very well structured. For experienced devs this won't matter one way or the other, but especially for newcomers this might be a major boon.
Well TIL. But I don't think /u/hxucaa deserves that many dislikes, given it's a confusing subject with mutliple meanings and it's hard to know that ARC is GC when you mostly hear the opposite
You can call gc.disable which disables only the cycle collector so if your code doesn't generate cycles it has no build up of garbage
Wrong subreddit, you're looking for /r/playrust.
This. The prevailing attitude online seems to be that you can't make objective comparisons between languages. Which is false. *Rust is safer than C* is a completely valid claim.
Off the top of my head, I still have the following concerns about Python's determinism: * JIT Compilation * Dynamic memory allocation * Everything is a dict (behind the scenes) -- assuming these are hash tables their access time is non-deterministic * CPython's GIL Some of these could be solved by another interpreter, but I still don't think that Python is a very good choice for realtime applications.
I find it crazy that the author cites type safety as an advantage of Go, but not Rust. Go has the weakest static type system of any modern language (mainly due to lack of generics). Heck, if you use Closure, even Javascript is more type safe than Go. Likewise, Rust's C interop is far, far, better than Go.
See the IDEs page /u/killercup linked above
Interesting article. After you've received on the channel, the concurrent sections are over and it can be desirable to move the value back out of the Arc-Mutex. I wrote the changes you'd need to do if you wanted to do that here: [Playground Link](https://play.rust-lang.org/?gist=efc2ed59874973a7c32903eb6fcac108&amp;version=stable&amp;backtrace=0). It also makes use of the pattern matching over a *Result* sum-type pattern. which I find to be another Rust feature that improves my programming experience.
You probably want to A) make `NoCaseString` a newtype (what Rust people call a single-element tuple struct like `struct NoCaseString(String);`) and B) implement `From&lt;String&gt;` for `NoCaseString` (so you can write `"Hello, world!".to_string().into()`).
I would prefer to see an IDE for Rust/QML development (à la QT Creator for C++).
It's also quite useful to implement `Borrow&lt;str&gt; for NoCaseString` retaining the ability to do `map.get("MyKey")`.
I'd like one drugs worth of bitcoin please. 
Thanks for highlighting how that works. That actually crossed my mind because all concurrent access is already done above...so technically you don't have to go through the Arc/Mutex wrapper.
Note that the bindgen plugin (like all plugins) requires using a nightly compiler. If the rest of your project doesn't require that, best to avoid it.
Good points...I know Go's type safety is not nearly as strong as Rusts. I would still regard it as a type safe language but the type system does break down after a certain point. I've never researched, Rust's C interop but will look into it at some point.
In general, if there's no license, than its "all rights reserved" or the equivalent. And given these authors are US citizens...
:) (They both live in Portland)
FYI the links on the bottom of that page are all broken (i.e. they point back at the same page).
Did you try disabling naggle (aka enable nodelay)?
&gt; For now I will only work in one thread, so data races etc. are not important at this point. You might want to check out http://manishearth.github.io/blog/2015/05/17/the-problem-with-shared-mutability/ ; these guarantees in Rust aren't solely about data races.
I believe you're looking for /r/playrust
"Write all your data structure course homework with Rust!"
I didn't read it that way. I read it as a short rant about fan-boyism, not against having a preferred language. I'm not sure why you would make that argument as it would be like arguing that guitarist shouldn't prefer a certain make of guitar or something.
Smart pointers in C++ are basically equivalent to borrow checker so Rust is unnecessary.
I did a bunch of Rust in 2015 and 2016 - maybe 1000 LOC, fully debugged and tested, entirely refactored two or three times as my understanding of Rust grew. I think the one thing that most sets apart Rust from other language is its ergonomics. Rust's documentation is ~~on par with Python~~ in a class of its own, it has better error messages than any other language I've ever tried, and its tooling is quite polished. The memory and concurrency safety guarantees are what initially brought me in, but by now they're basically a side-show. Yes, they're great, but they're not quite as life-changing as the Rust language's user experience. Rust's type system is subtle and novel, so to learn it requires quite a bit of hand-holding. If Rust had even slightly worse documentation and error messages - say, documentation on par with Java and error messages on par with C# - then it would probably be a dead end. As it is though, it's an absolute joy to pick up and use.
Rust's C interop is almost drop-in. You use Python-style decorators to mark structs which need to cross FFI boundaries, write `.h`-style specifications for C code you're going to call, and annotate Rust functions with `extern` to make them callable from C. Ta-dah. (You can also expose/utilize global variables across FFI boundaries, and it's easy as hell too.)
There are no safety issues with C/C++, the problem is just bad programmers.
It will prevent additional syntax from existing. 
At worst it should just prevent new syntax from being generated by syntax extensions, no? Assuming that any new syntax that is added post-HIR-stability is just syntax sugar for features that can otherwise be expressed more verbosely, that doesn't sound too bad.
The difference is that a new-type affects all uses of the data structure, whereas a comparator need only be specified on construction (and also when naming the type in places where it cannot be inferred).
I would say most projects that use serde.
I wonder if something like this would help: "A seek beyond the end of a stream is allowed **by this trait**, but implementation defined" (emphasis just part of this message, not proposed as part of the docs)
As mod of /r/IntellijIDEA I want to thank you and your fellow contributors for your hard work. I'm really looking forward to trying out this plugin. 
BTW apasel and I (mostly apasel, seriously that guy is great!) built out a design for comparators like a year ago. A few contain-ra collections are compatible with it, and it's designed to slot into std. http://contain-rs.github.io/compare/compare/ However adopting it in std and making it work The Best is all blocked on defaults affect inference (along with every other use of boost-like generic params). DAI has been dead in the water for months, eveb though it has an accepted RFC, and no one has stepped up to champion it :(
Or better yet, `BTreeMap::with_cmp(|x, y| x.foo.cmp(&amp;y.foo))`. Or maybe even `BTreeMap::with_cmp(Ordering::comparing(|bar| &amp;bar.foo))`, where `fn comparing&lt;T, U: Ord, F: FnOnce(&amp;T) -&gt; &amp;U&gt;(f: F) -&gt; impl Fn(T, T) -&gt; Ordering`
First, you can use [cargo bench](https://doc.rust-lang.org/book/benchmark-tests.html) for benchmarking if you're on a nightly version of Rust. It tends to be more user-friendly and accurate than writing benchmarks manually. You should also ensure you're compiling in release mode when you benchmark (cargo bench does this by default), because debug mode can be around 10x slower. Additionally, you can run clippy, if you haven't already, for some additional lints that aren't included in the Rust compiler by default. Some of these may give performance improvements for little effort. Sometimes the best method is to find where the slowest part of the code is through profiling. You can do this in Rust [using valgrind](https://llogiq.github.io/2015/07/15/profiling.html) the same way you would with C/C++. I'm also curious: Is there a reason you're using boxed slices instead of vectors? (There may be a perfectly valid reason that I'm not aware of, but the usage seems odd.)
That's really cool!
&gt; Is switching the function memory-safe? (do you protect it with a lock or something?) It's not memory safe, the actual implementation is that for every function marked as `hotswap` the plugin creates one global variable that acts as a pointer to a dynamic library function, the hotswapped `bin` bodies are just replaced with a pointer dereference to its related pointer, and a function call to that address, as can be [seen here] (https://github.com/Draivin/rust-hotswap/blob/ac92be8ae5c649633e01b97e5c208f65d437cc64/src/lib.rs#L331-L336). As far as I understand, the result of a memory race on the function pointer is that the calling code could go either to the latest loaded function or to the previous one, and as the libraries aren't ever unloaded, both versions are valid (I could be way off here). &gt; Do you spawn a new thread to switch the function? (apparently yes, hotswap_start!(); ends up starting a thread?) Yes, as you've seen, [this](https://github.com/Draivin/rust-hotswap/blob/ac92be8ae5c649633e01b97e5c208f65d437cc64/src/lib.rs#L134-L198) is the entire actual "runtime" that replaces the `hotswap_start!()` macro. Also, as the user could spawn many threads, and could have one thread running in a loop inside one version of a hotswapped function, I don't see how it would be possible to unload one set of hotswapped functions safely.
&gt; Yet Cargo is secure for definitions of "secure" that include usability as a requirement. I don't think there are common definitions of "secure" that include usability as a requirement.
Probably, but having a `RWLock` on a static is pretty cumbersome, and would require that the user crate to depend on `lazy_static`. Also, I'm not sure about the performance overhead of having to go through a lock every time the function is going to do a dereference, since those functions probably would be on pretty tight loops when dealing with game development.
I mean.. I can accept memory unsafety if your stuff automatically adds a `unsafe` qualifier to the function somehow. Or if you throw a compile-time error if the user doesn't mark the function as `unsafe` themselves, somehow. (I don't know how to do this) Otherwise, it stops being Rust I think. edit: perhaps a clever use of atomics could save you from having a lock but still enable memory safety. But I don't know exactly how to do this either.
No, data races can result in torn reads or undesirable caching. For example: fn a() { loop { b(); c(); } } #[hotswappable] fn b() {} fn c() { b(); } The compiler can, unless you do something to prevent it, lift the functio pointer read for b out of a's loop, but leave c alone. So if you swap out b while that loop is running, the call to b directly inside the loop will continue calling the old one, but the call in c will use the new one. [I suggest using AtomicPtr](http://doc.rust-lang.org/stable/std/sync/atomic/struct.AtomicPtr.html)
That just means we first need to accept https://github.com/rust-lang/rfcs/pull/757 :P
hmm.. good catch... for some reason I was doing 240 * 2000 * 60 and calculating based on that. So I guess my PPU is at least 30x faster than the real thing and I should probably focus on the CPU.
Relevant fact that's often overlooked: on all common architectures, AtomicPtr with the `Relaxed` ordering is the exact same thing as a normal variable at the assembly level; it's just excluded from certain compiler optimizations. So the OP can use that without worrying about performance impact.
Make an environment variable called RUST_NEW_ERROR_FORMAT and set it to true.
I'm flattered, but I'm just a smaller contributor, lol. Most of the work is [matklad](https://github.com/matklad) and [alexeykudinkin](https://github.com/alexykudinkin), with various contributions from several others. Stop by the gitter some time! https://gitter.im/intellij-rust/intellij-rust
I've finally cracked the macro problem with [overflower](https://github.com/llogiq/overflower). I need a lot more tests for the support library, and I still need to add traits for casts and abs. [flamer](https://github.com/llogiq/flamer) should get the full macro expansion logic from overflower – for now it only has item expansion. From there on I should change it to use a state machine to allow more modes. I still haven't coded up the trait- and method-lookup for [metacollect](https://github.com/llogiq/metacollect). Also there's a lot of stuff I'd like to do for [clippy](https://github.com/Manishearth/rust-clippy), if I find the time. I'd very much like to change the way we give suggestions (and allow IDEs to apply them automatically), though that'll probably require an RFC. Speaking of which, my [RFC PR #1623](https://github.com/rust-lang/rfcs/pull/1623) could use some more eyeballs. Edit: since some people have asked: Reddit recently changed the rules so that only text posts can be 'stickied' (put at the top). I was unaware of that change when I started this thread, so here we are. Next week, I'll do a text post so everything will be back to normal. Sorry for the inconvenience.
Have you heard of the [Nerves project](http://nerves-project.org/)? It's not Rust but also an alternative. I'm in the same realm of IPC based firm real-time control systems and have been flirting with both Rust and Elixir for a while for use in new projects. 
I don't think it'd be impossible to write an LLVM backend to compile to Arduino-C, maybe someone's already done it.
Point proven
Yes, there is someone working on an [AVR backend for LLVM](https://github.com/avr-llvm/llvm). In the last couple of months development was paused because it was in the process of being merged back into LLVM trunk. But it seems that the two patches have been merged now, so I am not sure what has to happen next. There is also a [Rust fork](https://github.com/avr-rust/rust) that is experimenting with this new AVR backend. /u/shepmaster will probably be able to tell you more than me 
After I published the [9th iteration on imag on my blog](http://beyermatthias.de/blog/2016/06/14/what-s-coming-up-in-imag-9/), I continue to work on my bachelors thesis and learning for a few exams of mine. I really hope I can at least get one or two PRs merged this week or next week, but I really have to hold myself back until 4th July (exams), after that I have another six weeks for my bachelors thesis. What I'm doing is: - Writing a `Ref` library, so one can reference files from imag which do live outside of the store (`links` are supposed to be URLs or internal links, refs are for arbitrary files on the filesystem) - Thinking about using `quickcheck` for testing things - Thinking about libimagstore tests - the problem is that this library abstracts over filesystem and that's rather complex to test actually. I think about replacing `std::fs` with an own implementation which simulates a filesystem... not sure on this one. I also want to implement [scrobble.rs](https://github.com/matthiasbeyer/scrobble.rs) - A library for writing scrobble client/server applications... but I kinda have no time for it at the moment. :-(
On par with Python? I'm sorry but Python's official documentation is very poorly organized. A very simple example is map in [Python](https://docs.python.org/3/library/functions.html#map) vs [Rust](https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.map). This distinction even works with 3rd party libraries that employ custom documentation. Take a look at Tensorflow or Django. Sorry for the bold opinion here, but I'm so sick of Python's subpar documentation and hasty implementations. Of course, this doesn't have anything to do with the language, but I think it's bad enough when in 2016 I'm more comfortable writing something in Rust, even though it has few libraries, just because *those* few libraries are well documented and make sense. Whereas programming with Python feels like an incoherent mix of libraries that employ all kinds of paradigms that don't fit and have utterly different styles of doing documentation.
Maybe adding a namespace sandbox descriptor to my sandbox library, and potentially putting it on crates.io. Maybe getting more of the IMAP parser written. Maybe working on my rustconf talk with my cospeaker. It's going to be a busy week :\ hard to say if I'll have time. If anyone is interested in collaborating on one of the first two maybe's, let me know.
It's not hard. All you do is add this to the build system, which is what Travis does. curl -sSf https://static.rust-lang.org/rustup.sh | sh -s -- --channel=$channel And then you can choose which toolchain to compile against.
Obviously one can make comparisons between languages, and there are things some languages do better than others. Otherwise, we all would happily write our binaries in a hex editor... But discussions about languages often degrade quickly, and most of the times is not meant to discuss the language, but to belittle people using the (inferior) language and to establish a social order among programmers. So, of course we can discuss languages and tools, but we should be careful to stay on a non-personal level. 
true, they could be part of the example to mimick the GC Go has!
Oh right I think it's 32.
I am actually writing a REST API using Iron (with its router crate) and I'm wondering : how can I access database ? Do you use an ORM and if so, which one ?
&gt; With that changed it's blazingly fast, so much that the game is unplayable :D - it's almost full speed in debug mode. That's what I like to hear! :)
There's wrappers for pretty much every popular SQL database, as well as Redis and a few fledgling ORMs. Have a look at this page: http://www.arewewebyet.org/topics/database/ [Diesel](http://diesel.rs/) is growing in popularity as an ORM for Rust. I haven't used it myself, but I hear that the API design is quite elegant.
If someone would like to sell me linkedin shares for apples I will be happy to make that trade.
I got my first [PR](https://github.com/intellij-rust/intellij-rust/pull/469) merged for the [intellij-rust plugin](https://github.com/intellij-rust/intellij-rust). I'm now working on context-aware attribute completion.
Firefox doesn't seem to like that websites certificate very much.
Thinking about it, a really cool way to implement this would be to store the module in an `Arc`, and then build on `Arc::downcast` to get references to the functions on the module, while tying their lifetime to the module's lifetime. Each hot-swappable function is then just a `RwLock&lt;Arc&lt;FnType&gt;&gt;` (ie. must be cloned before calling the function, equivalent to the ref-count manipulation I described before)
I know this isn't the best answer, but the highest concentration I've found of people trying to solve the same problems can be found in irc.mozilla.org/#rust-gamedev RE: The JNI. Is there no way to use JNA instead? This would make the wrapping process much, much easier to automate. JNA wrapped code is also more efficient (citation needed) This link implies that JNA can be made to work for Android: https://github.com/java-native-access/jna/blob/master/www/AndroidDevelopmentEnvironment.md Thanks for putting this together. It is definitely of interest to me, I just haven't yet *needed* to get started on it. I think a few people are in the same boat. Maybe this will be a catalyst to initiate progress. 
I like that you guys have your own thing going. Had some issues trying to figure out how to use it with a multi cargo project folder. Live linting like when coding in Java with IntelliJ would be awesome as well. For now I'm still using Atom/VSCode, but I love the progress this is making.
Data races don't just show up in multithreaded code. For example, in C++, it's easy for a single-threaded program to have iterator invalidation bugs. The borrow checker will protect against those.
That makes sense, I haven't tried it yet. It's high on my list of new languages to try (I usually learn one or two new languages every year).
Hum... hadn't thought about inference. However... what's the type of the `BTreeMap`? (I can't find the function in the docs... and Google is misbehaving (returning 0 result)). Lambdas have unique types, so how do I pass that specific map to a function (or hold it into a `struct`), do I have to be generic on the comparator parameter? The only other alternative I can think of is boxing and dynamic dispatch.
If the type of the comparator is not part of the type of the `BTreeMap` this suggests type-erasure (requiring boxing) to me. Doesn't seem ideal either...
I guess it's the same here. I've just had experiences where writing Java was a better experience, which to me seems incredible considering the fact that I find Python quite a smart language overall.
For JNI you could try an approach similar to SafeJNI, a C++11 metaprogramming library for doing JNI calls. https://github.com/MortimerGoro/SafeJNI I guess a possible Rust solution would be to use macros instead.
http://manishearth.github.io/blog/2015/05/17/the-problem-with-shared-mutability/ covers why the rules are important, even for single-threaded programs.
Wow, indigo looks really neat! How hard was it to adopt WebRender as your rendering layer? 
Therefore, any time you talk about the type of the map (function parameter, data-member, ...) you need to give the type of the comparator (or make an alias for the type of the map). It isn't mentioned *only* at the constructor site, unless the map is throw-away OR all use sites are generic over the comparator. This is exactly the same problem with a new type for the key; by the way. Where you do gain, is when using the map afterward. Though if the new type implements `Into`/`Borrow` it does not seem too horrendous.
I would argue that there *must* be some extra work involved, in a similar way that adhering to const-correctness may add some extra work when writing in C *(the rules need to be followed, and there are cases when its tricky to get right... or cases the C spec doesn't support).* Of course your effort may be rewarded later, and by the sounds of it, once you get the hang of it - its well worthwhile, even in the case of single threaded code.
Any extra work involved is primarily going into your thought process of how you think about memory access, and ownership. What is accessing this allocation? Is it read only? Write only? This extra work is well worth it as it'll serve you well in languages other then Rust. &gt; and by the sounds of it, once you get the hang of it - its well worthwhile, even in the case of single threaded code. Once you internalize the rules, you don't even think about memory management. Writing Rust starts to feel like writing a high level memory managed language. 
The docs for `std::mem::drop` say: &gt; While this does call the argument's implementation of Drop, it will not release any borrows, as borrows are based on lexical scope. So it is connected for some types.
To be clear, with_cmp doesn't exist -- comparators are an entirely hypothetical construct in Rust. You're correct that naming a comparator created in the manner I specify is impossible. Without impl-trait the only things you could store would be a whole custom comparator (painful as you describe), the Natural comparator, and the Reverse comparator (and compositions there-of). Code which doesn't bother to be generic will only work with `BTreeMap&lt;K, V, Natural&gt;` just as `HashMap&lt;K, V&gt;` is sugar for `HashMap&lt;K, V, RandomHasher&gt;` today. Code that adds a generic param for it can be passed any BTreeMap.
I had some luck. Someone else wrote a [sample project here](https://github.com/kevinmehall/webrender-experiments) which let me know it was at least possible :-). I also looked at [Servo's code](https://github.com/servo/servo/blob/master/components/layout/webrender_helpers.rs). Its a little confusing because they (for now I guess) maintain their own display list that they just convert to a WebRender display list. Between those two sources, I was able to gather enough at least for this proof of concept.
Doing more work on a DICOM reading library (eventually writing too). File Meta information is read in properly at this point. Next up is building the in-memory object map while reading in tags. I ran into a situation where I need to import thousands of constants defined from a standard into rust code. I started another rust project to download HTML pages and export tables into CSV -- uses Hyper, CSV (from /u/burntsushi), Scraper, and Docopt (also burntsushi). It took me about 4 hours -- ~2 hours to get everything working then another 2 for cleanup. If I wanted to dive further into learning iterators right now I could make it even nicer but I wanted to move back to the main project. As-is however I used it to pull down ~8 tables with lots of data to define as constants.
That or higher-kinded types
Here's the [HN thread](https://news.ycombinator.com/item?id=11936983) for more interesting discussion. I submitted it earlier today after discovering afl, and afl.rs, which was [mentioned on the project home page](http://lcamtuf.coredump.cx/afl/). Thanks to the author! :)
There is - the [README](http://lcamtuf.coredump.cx/afl/README.txt) and [quick start guide](http://lcamtuf.coredump.cx/afl/QuickStartGuide.txt) are very useful, although the process is somewhat complicated. From what I gathered (and briefly tried), you have to install `afl-fuzz`, create / pick small input test cases that don't cause your parser to crash, and instruct afl to use them. If you want to use multiple cores, you can; I couldn't find the docs online but on my system, they are bundled in plain text in `afl`'s doc directory (`/usr/local/Cellar/afl-fuzz/2.15b/share/doc/afl/` on my system - `afl-fuzz` was installed with Homebrew).
How was your experience using CSV? Anything I can do to make it better? (I actually have a rewrite of the public API in mind, but I'm curious to hear how others are finding the current API.)
C++ vector iterators seem to be the most commonly quoted example, but are there others?
What a cliffhanger it ends on! &gt; That's because Rust is memory-safe by default, but also because not many people have tried afl.rs yet! Over time we will update this section with the most interesting bugs, whether they're logic errors or memory-safety problems arising from unsafe code.
Prompted by a recent thread in this subreddit: Is there some way I'm missing for me to *safely* produce *shared* mutable references to `Copy` data, with no performance hit, and without needing to write `unsafe` all over my client code? Some way for me to explicitly say "my program is single-threaded and the type `T` isn't at risk of invalidation by having multiple owners, so give me the ability to produce a bunch of simultaneous `&amp;mut T`s please"? I swear I have a genuine use-case for this, I'm not just trying to be lazy about ownership. I'm currently using smart pointers `Foo&lt;T&gt;` with handwritten `&amp;mut self` methods for mutating each field in every possible `T`, but that's exactly as clumsy and hacky as it sounds. `UnsafeCell`'s documentation talks about safely creating "aliasable mutable data", but I don't know whether it forces the compiler to treat `&amp;mut T` as aliasable if `T` contains an `UnsafeCell`, or whether you're forced to use `*mut T` instead.
Any situation where changing an "exterior" bit of a value modifies the type or location of the interior. e.g. vectors, rust enums, inheritancey things, other collections, etc.
I might be missing something, but wouldn't that just be [Cell](http://doc.rust-lang.org/std/cell/struct.Cell.html)? It has a `set` method that works on shared references by allowing only `Copy` types. It's implemented using `UnsafeCell` underneath.
Been waiting for someone to do this! :D Well done
Great work! Your questions/doubts/reflections are excellent discussions for improving Rust, I would like to see Rust offer great answers for each of your remarks.
I regularly find myself yearning for Rust’s ownership model when writing Python. Because in Rust you *know* if you own something or not. In Python you’re left wondering whether it’s OK to mutate this parameter or whether you should make a copy of it first. And you invariably end up with either slow code or subtle hidden bugs. Sometimes you manage both.
Hi there. I'm sorry if I'm breaking any rules, but I'm really scratching my head at what this piece of code does. fn gpio_set(port: u32, enabled: bool) { let fun = match enabled { true =&gt; GPIO_SET as *mut u32, //as far as i can tell they are setting GPIO_SET as a mutateable unsigned 32 bit raw pointer false =&gt; GPIO_CLR as *mut u32, }; unsafe { *(fun) = port; //what is going on here? } } It is from a raspberry pi 2 kernel. The entire kernel is as follows: #![feature(lang_items, start)] #![no_std] const GPIO_SET: u32 = 0x3F200020; const GPIO_CLR: u32 = 0x3F20002C; const GPIO47: u32 = 0x8000; const GPIO_BASE: u32 = 0x3F200000; fn gpio_set(port: u32, enabled: bool) { let fun = match enabled { true =&gt; GPIO_SET as *mut u32, //as far as i can tell they are setting GPIO_SET as a mutateable unsigned 32 bit raw pointer false =&gt; GPIO_CLR as *mut u32, }; unsafe { *(fun) = port; //what is going on here? } } #[start] fn main(argc: isize, argv: *const *const u8) -&gt; isize { gpio_set(GPIO47, true); 0 } #[lang = "eh_personality"] extern fn eh_personality() {} #[lang = "panic_fmt"] extern fn panic_fmt() {} This kernel is a modified version of the one [here](https://gist.github.com/dignati/65ffc031eabea8a9b246) Which is from this tutorial.[here](http://blog.thiago.me/raspberry-pi-bare-metal-programming-with-rust/) I'm really sorry if this is against the rules, but I am new to rust. If anyone could help me out it would be very appreciated.
afl.rs maintainer here. If anyone has any questions, let me know.
How do I generify my listener trait? I am not sure if my misunderstanding is about generics, references, or lifetimes here. Please let me know if I'm doing anything odd. This compiles: pub trait Listener { fn listen(&amp;mut self, message: &amp;[u8]); } impl &lt;'s&gt; Listener for Listener1&lt;'s&gt; { fn listen(&amp;mut self, message: &amp;[u8]) { // work } } impl Listener for Listener2 { fn listen(&amp;mut self, message: &amp;[u8]) { // work2 } } fn handle_message(listener: &amp;mut Listener, message: &amp;WebSocketResult&lt;Message&gt;) { match *message { Ok(ref ok) =&gt; listener.listen(&amp;*ok.payload), Err(ref e) =&gt; error!("Skipping message, got err {:#?}", e) } } This does not: // error: cannot infer an appropriate lifetime for pattern due to conflicting requirements [E0495] // Ok(ref ok) =&gt; listener.on_message(&amp;*ok.payload), // ^~~~~~ pub trait Listener&lt;T&gt; { fn listen(&amp;mut self, message: T); } impl &lt;'a, 's&gt; Listener&lt;&amp;'a [u8]&gt; for Listener1&lt;'s&gt; { fn listen(&amp;mut self, message: &amp;[u8]) { // work1 } } impl &lt;'a&gt; Listener&lt;&amp;'a [u8]&gt; for Listener2 { fn listen(&amp;mut self, message: &amp;[u8]) { // work2 } } // The compiler help suggests these lifetimes, but this still doesn't compile. fn handle_message&lt;'a&gt;(listener: &amp;mut Listener&lt;&amp;[u8]&gt;, message: &amp;'a WebSocketResult&lt;Message&lt;'a&gt;&gt;) { match *message { Ok(ref ok) =&gt; listener.listen(&amp;*ok.payload), Err(ref e) =&gt; error!("Skipping message, got err {:#?}", e) } }
I believe the problem is you have not stated the lifetime of the slice the `Listener` handles. The `Listener` must be known to handle only the slices with a lifetime at least as long as the `WebSocketResult`. Try this: fn handle_message&lt;'a&gt;(listener: &amp;mut Listener&lt;&amp;'a [u8]&gt;, message: &amp;'a WebSocketResult&lt;Message&lt;'a&gt;&gt;) Edit: I think we found a solution further in the thread, [here](https://www.reddit.com/r/rust/comments/4oxe5u/hey_rustaceans_got_an_easy_question_ask_here/d4lj4ij)
 let fun = match enabled { true =&gt; GPIO_SET as *mut u32, false =&gt; GPIO_CLR as *mut u32, }; unsafe { *(fun) = port; //what is going on here? } In Rust, blocks like `if` and `match` can be used as expressions. Like how in C you can say `int x = condition ? value1 : value2`, only more general. So the first four lines are setting `fun` to either `GPIO_SET` or `GPIO_CLR`, depending on the value of `enabled`. The last three lines are setting the value pointed to by whichever one was selected to `port`.
Thank you! That really clears everything up. Just one more question: Why are their brackets around fun in this line? *(fun) = port; 
No reason. They're not required.
Ooh, my answer is too long for Reddit. :-) I think I managed to get it working like this: use std::collections::HashSet; use std::hash::BuildHasherDefault; use std::default::Default; use std::hash::Hasher; pub struct FnvHasher(u64); impl Default for FnvHasher { #[inline] fn default() -&gt; FnvHasher { FnvHasher(0xcbf29ce484222325) } } impl Hasher for FnvHasher { #[inline] fn finish(&amp;self) -&gt; u64 { self.0 } #[inline] fn write(&amp;mut self, bytes: &amp;[u8]) { panic!("Cannot hash arbitrary bytes."); let FnvHasher(mut hash) = *self; for byte in bytes.iter() { hash = hash ^ (*byte as u64); hash = hash.wrapping_mul(0x100000001b3); } *self = FnvHasher(hash); } fn write_i32(&amp;mut self, value: i32) { let FnvHasher(mut hash) = *self; hash &lt;&lt;= 10; hash ^= value as u64; *self = FnvHasher(hash); } } type Set = HashSet&lt;(i32, i32), BuildHasherDefault&lt;FnvHasher&gt;&gt;; fn Empty() -&gt; Set { let hasher = BuildHasherDefault::&lt;FnvHasher&gt;::default(); HashSet::with_hasher(hasher) } fn iterNeighbors&lt;F&gt;(mut f: F, (i, j): (i32, i32)) -&gt; () where F: FnMut((i32, i32)) -&gt; () { f((i-1, j)); f((i+1, j)); f((i, j-1)); f((i, j+1)); } fn nthLoop(n: i32, s1: Set, s2: Set) -&gt; Set { if n == 0 { return s1; } else { let mut s0 = Empty(); for &amp;p in &amp;s1 { let add = |p| { if !(s1.contains(&amp;p) || s2.contains(&amp;p)) { s0.insert(p); } }; iterNeighbors(add, p); } drop(s2); return nthLoop(n-1, s0, s1); } } fn nth(n: i32, p: (i32, i32)) -&gt; Set { let mut s1 = Empty(); s1.insert(p); let s2 = Empty(); nthLoop(n, s1, s2) } fn main() { let s = nth(2000, (0, 0)); println!("{}", s.len()); } Compiling with `rustc -O` it takes 1.83s. Making equivalent changes to the F# code, for example: open System.Collections.Generic type [&lt;Struct&gt;] P = val i : int val j : int new(i, j) = {i=i; j=j} let cmp = { new System.Collections.Generic.IEqualityComparer&lt;P&gt; with member __.Equals(this, that) = this.i = that.i &amp;&amp; this.j = that.j member __.GetHashCode this = (this.i &lt;&lt;&lt; 10) + this.j } let inline iterNeighbors f (p: P) = let i, j = p.i, p.j f(P(i-1, j)) f(P(i+1, j)) f(P(i, j-1)) f(P(i, j+1)) let rec nthLoop n (s1: HashSet&lt;_&gt;) (s2: HashSet&lt;_&gt;) = match n with | 0 -&gt; s1 | n -&gt; let s0 = HashSet(cmp) let add p = if not(s1.Contains p || s2.Contains p) then ignore(s0.Add p) Seq.iter (fun p -&gt; iterNeighbors add p) s1 s2.Clear() nthLoop (n-1) s0 s1 let nth n p = nthLoop n (HashSet([p], cmp)) (HashSet(cmp)) do let timer = System.Diagnostics.Stopwatch.StartNew() let _ = (nth 2000 (P(0, 0))).Count printfn "%fs" timer.Elapsed.TotalSeconds And compiling it to x64 with optimisations enabled (I was just using the REPL before) it takes 1.82s. You can make the benchmark take longer by increasing the argument at the end. For example, with `n=6000` [Rust](https://gist.github.com/jdh30/48d041b5f1558fcc3aee77cd6cc860b4) takes over 3x longer than [F#](https://gist.github.com/jdh30/05c6cb6adc4861dc6db133163bb9a6fa) to complete this benchmark. Any more and the Rust just crashes. 
Note that "type" in the above comment is really a stand-in for "safety-critical invariant", but it is easier to talk about types :)
Author here, I'm in the process of finishing up this library but it is mostly ready right now. Planned improvements include output to HTML and better performance. If anyone has something they'd like syntax highlighting for, I'm very open to suggestions for changes/enhancements. Also if you reply here I can answer any questions.
I'm on mobile right now so I can't really test but try this fn handle_message&lt;'a, 'b: 'a&gt;(listener: &amp;mut Listener&lt;&amp;'b [u8]&gt;, message: &amp;'a WebSocketResult&lt;Message&lt;'b&gt;&gt;) This explicitly says the reference to the result does not outlive the contents of the message, decoupling their lifetimes.
The general practice is to use `let` for values which won't change and `let mut` for values that will. For the majority of code that people write, the immutable case happens more often. What the author is meaning I think when he's discussing copying of variables in the first section is what can be referred to as an 'implicit copy'; something that is &lt;= than the systems pointer size. This is pretty performant. The other type of copy, a deep copy, is what the author talks about later with decorating the structs with `#[derive(copy)]`. Where memory is allocated and the values are copied from one instance to another. This can be slower because it goes through the system's memory allocator. Edit: Ahh reading further I see what you mean about the copy. I've never used Redux, but copying the struct for every function invocation isn't usually the best for performance. But if you need a new instance of a modified object it's not bad per-se. I have the feeling that people new to rust (and I did it too) tend to copy more than they need to as it seems like the borrow checker eases up on you. You can always pass by by mutable reference without creating a new object instead.
You could have just used their code; you didn't have to try to do the integration yourself if you didn't understand what the code did. Anyway, [here's](https://gist.github.com/scottgw/1702cda28a5519e8f861423276e4ef03) a combination of Aatch and Veedrac's pairhash minus the explicit set sizing. It runs in about 0.5s at size 2000, and 5.60s at size 7000 on my system.
this is really useful, even when rustc already checks a lot of issues. I fuzzed the hell out of some nom parsers with it, and it found some integer overflows in code I had written manually outside the parser :D
I'm not an experienced programmer in any way, and mainly 'just interested' in Rust. I contribute to an old gnarly codebase in C, which is single-threaded alright, but boy do I wish there was a borrow checker.
Swift doesn't use GC for things like integers, and possibly other objects, so it may be clear in Rust that you're using it, but the fundamental difference is not that one has GC and the other doesn't. I don't know where the downvotes come from, though.
Interesting idea. I think it would be possible at MIR level. But no matter how you implement it, I thought about another possible implication: Could Rust make fuzzying a first class citizen? So you could just use a very simple macro syntax (see ideas posted in the afl.rs issue tracker) to define entry points and use cargo to execute them? Optimally all of this with stable rust, because rustc (and the standard library) implement it. I think that would encourage way more people to use it. I mean even when only using safe code it obviously finds many possible bugs in many important libraries (see trophy section in afl.rs README), so it might be a second important pillar next to testing. 
As of CocoaPods: if you use something different than Xcode and ObjC/Swift, then the only painless way is to distribute an already built fat binary. Will this work better for you? Besides that, there are other ways to manage dependencies on iOS: 1. first one is (widely adopted) [Carthage](https://github.com/Carthage/Carthage). AFAIK it works only with Xcode projects, but you may take a closer look to see if it fits or not. 2. second one (adopted by two developers) so called [Components](https://github.com/AlexDenisov/Components). Very similar to what FreeBSD ports system does, highly customizable, written in Rust. I think it's a perfect fit for your case from technical point of view, but probably will not work if we are talking about a client and business.
Definitely! The Rust community is great. I know about intermezzOS. I'm planning to help out once I'm done with a few things that are more urgent.
I thought JNA was slower than JNI (cf [FAQ](https://github.com/java-native-access/jna/blob/master/www/FrequentlyAskedQuestions.md)). Still, a JNA example would be great! For the project I'm working on, I'll use JNI, but I'm interested in compiling a list of examples of what can be done to use Rust libs in mobile apps.
Yes, I think macros could do the job too
I'm using CocoaPods right now, but I'm not opposed to integrating other examples. The easiest for users would be to make universal binaries and distribute those, since it would not require installing the Rust toolchain everywhere to use the libraries. Could you do examples with Carthage and Components? I'm not familiar with those tools.
And the pull request are coming in! Thanks reddit!
Great! That was kind of what I was thinking, but I had to be sure. Looking forward the next one, good job 👍
I'm looking forward to working with you on this!
No idea if i answered this correctly?
&gt; is this something you've looked at at all? I haven't. I've also thought about it, but haven't made any attempts to implement it. Nothing you said sounds incorrect, though I'm actually not too familiar with Rust's own internal representation formats, so I can't speak too much on the topic.
There are issues with C longjmp if it jumps over Rust stackframes, e.g.: https://github.com/jcmoyer/rust-lua53/issues/37
Well I guess that's okay. Though I think I read something about default arguments which would be a big improvement nonetheless. 
We're working on both Emscripten support, and eventually wasm support. It's not fully there, but in af ew months...
&gt; As far as I understand, &amp;mut T always has to be a unique reference, so the compiler can do optimizations based on that fact. This is also what `UnsafeCell` does, fundamentally: it removes this optimization.
Dropped in the channel on IRC and a dev indicated they were aiming to release it by the end of the month.
I think that's a good approach. It's mostly just something that's been on the back of my mind for a while now. Hypothetically, you could make the registry generic in the coordinate-type (eg: a node, or node + dimensions), but it's moot until I find the energy to sketch something. Anyway, It's really cool that someone's working on this. I started on something similar, but didn't get nearly as far.
Hi everyone, I try to start an article series to learn Rust. I have a Node background and try to compare Rust with Node. Maybe this helps other developers with a Node background to learn Rust. I try to update it biweekly. https://twitter.com/PipoPeperoni/status/745217093252038656
Right, I've dealt with such code-bases too. OTOH, there are small, stable, well managed, fast C code-bases that manage to mitigate many of the common pitfalls (well tested, use static checkers to find errors, well documented... etc). These are likely far less complex then the kinds of software Rust was intended for, so the question in my mind is - If I'm going to be happy using C to solve a problem - is there a really good reason to invest time in Rust? (Yes, with scope creep, C can become unwieldy, nevertheless, a C code-base can be managed well and kept maintainable).
The main problem with that is that it just doesn't compose as well -- I no longer have a `Map&lt;K, V&gt;`, I have a `Map&lt;SortaK, V&gt;`. Messes up iterators and all sorts of other things.
You must also use the new-type whenever accessing the map, which you don't have to do with a comparator.
Come help! We have so much work to do - built in telemetry/or disconnection alerts - support legacy collectd/statsd - A critical eye! Tons more already listed on issues!
Awesome, glad to hear it! :)
Only if the new-type does not implement `From`/`Into`/`Borrow`, if it does you can use the unwrapped type in many cases. You will indeed still need to mention the new-type a *few* times; but is that really unbearable? I do agree that a comparator type will be mentioned (slightly?) less often; I am just not seeing an overwhelming advantage to it, and therefore wonder if it really is worth the cost.
Wow, this thread is intimidating! I'm trying to learn Rust, and I feel the best way to do that *is to actually make something*. I'd like to make Pong in Rust, using Piston. This way I get to: * Attempt to learn Rust * Maybe learn a bit about game dev. I think /r/rust, /r/rust_gamedev, and I are going to become great friends this week :P
I'm not joking at all, everyone here is working on some pretty neat libraries, and I'm trying to recreate an old simple game :P
Just starting learning, and am recording going through the official docs. This is more of a documentation of my experience than a teaching resource. Feedback on official docs so far: the [?Sized section](https://doc.rust-lang.org/book/unsized-types.html#sized) was difficult to understand as presented
I saw a comment about using #cfg directives to import the real vs mock code. Of course, for that to work, you need the not-yet-available mock versions of IO, but the idea looked pretty interesting. 
&gt; Once you internalize the rules, you don't even think about memory management. Writing Rust starts to feel like writing a high level memory managed language. While Rust doesn't have the overhead of a GC, I wouldn't say hiding memory allocation from the developer is *always* a good thing. This may obscure cases where memory usage could be much more efficient (allocate memory for re-use or memory pools). 
&gt;allocate memory for re-use or memory pools Actual to a degree [jemalloc](https://www.facebook.com/notes/facebook-engineering/scalable-memory-allocation-using-jemalloc/480222803919?_fb_noscript=1) will have your back if your objects are smaller then 32kb. It handles creating/maintaining memory pools (on a per-thread basis) if you start allocating/de-allocating small chunks of memory frequently. And Jemalloc is the default allocator in Rust. Edit 1: Jemalloc memory pools are 4Mb, but objects have a max size of 32Kb that it'll auto-pool. Edit 2: This information is correct circa 2011 so it may have improved Edit 3: Yes you can get more preference by writing a per-thread-per-object memory pool. That may allow for better packing due to better alignment. But at this point you've almost got a bald yakk. 
[removed]
Ping me.
As I wrote recently on rust-users, the awesome compile-time guarantees helpful compiler, and not least the great community lure us into punching above our weight – to our collective surprise and delight often successfully. The downside is that to outsiders we must be looking frighteningly smart.
https://www.youtube.com/watch?v=-WHpQVMZbjo
I believe emscripten already exists? I recall seeing Hematite running in the browser a year ago.
This looks very incorrect (i.e. unsafe). It seems that you can have more than one mutable (also called unique) reference to the inner buffer: E.g. when you create a view and later call `push` on the main vector, you have one immutable reference (the slice from the view) and a mutable reference (to write to the vector). This violates Rust's requirements for unsafe code.
It sorta kinda works if the moons align. It's not as easy as `cargo build --target=emscripten`.
Sure I'm at posix4e at gmail
So glad that this behavior is unacceptable in our community. "Programming through pain" and "only want the best" are elitist ideas, and Rust is doing a great job of making the community inclusive.
Why doesn't everyone switch to incremental LTO?
I thought it took a while to compile in comparison? Never really done it before... can you do it through cargo?
The elements that a view could already see does not change. The length only grows, letting all views see more elements. The vector is never relocated. Where did you read such a requirement for unsafe code? The point of lock-free data structures is to provide mutable aliases -- more general than this one. Unsafety is caused by undefined behaviors. Behavior is undefined when reading from/writing into unallocated locations, or from reading/writing inconsistent data during data races. I'm not sure if in Rust memory model, atomic operations can be reordered with stores. If so, my code is incorrect since the length increment during a push can be reordered to before the actual data is written, exposing inconsistent data to other threads. EDIT: added more details
Because we don't have incremental LTO yet (there are no LLVM releases that offer it). This is _brand new_. I don't know if LLVM 3.9 is going to advertise this as stable enough or finished, but that is the first step that needs to happen for _people_ to use it (and even then it will probably only be used in MacOS and FreeBSD, on Linux one has to set up `lld` and/or gold).
Just reading this to get a feeling of how well structured it is and I'm liking it. Simple, enough information, chapters not that big and one important thing most tutorials forget, the versions you got and you are using to explain stuff. And actual configuration of development environment for both languages. Most articles skip that completly and just assume you know what you are doing. I liked it. Altought I haven't programmed in nodejs for a long time or anything similar.
Does GCC toolchain have LTO?
Why is histogram exposed at the metric creation layer? I'd let reporters make summarization decisions on their own. Writers should just emit StdMeter largely without knowledge of metric processing. This makes the writer surface area smaller/simpler, and pushes the summarization policy deeper into the metrics system where it can be centrally configured and smarter. (This is based on 10 minutes of reading, so it's a fairly shallow opinion. Apologies if I've missed something big.)
Yes, try `-flto`.
Rust's ownership model gives you two things that greatly simplify thinking about code: * The model prevents you modifying/destroying/whatever data when this could affect distant parts of the program. * Ownership semantics are explicit. If you're writing a function, you know from its signature whether something belongs to it or is a reference to something else's object, and whether you can safely mutate it without stepping on other parts of the code. (And whether code in other threads might be messing with your data.) (It's not.) Aside from others' comments listing the classes of bugs this prevents, the result is a very strong sort of encapsulation. You can write code in a piecemeal fashion and be sure that everything you're able to access is something that you can safely access, everything you can mutate is inaccessible from anywhere else, etc. Even as part of a much larger system you can sort of wall yourself in and think of each function in isolation. This makes code easier to write, but more importantly *much easier* to verify is correct. Rust's type system in general also facilitates this, and things like newtypes and other zero-cost abstractions let you define your function's view of its environment much more precisely. But two huge things regarding pointers (do they point somewhere, and will that somewhere change out from under me) are given to you by default by the ownership model. The end result, helped by some other things like immutable-by-default variables, is that you can often mentally prove functions correct for all inputs, simply because "all inputs" is such a small precisely-defined set. This gives you assurance about the functions that call yours, and so on. None of this has anything to do with threading.
Why do you think they're doing it wrong?
All that should be handled by llvm and/or the linker. Apparently you just pass some flags to it saying "do incremental LTO, put your cache in this directory".
&gt; longjmp Good point! This could be the difference between a scripting language being usable, or not. It turns out Python for instance uses `longjmp` in 2 places, however they're off by default by the looks of things (`--with-fpectl`, readline when `select` isn't available). So from the looks of things they try to minimize use of this, nevertheless, 3rd party extension modules may still use.
It's worth noting that if you can make a datastructure out of standard components (eg. `Vec`, `Box` and `struct`) it's normally fine. The problems arise when your datastructure requires something novel (eg. allocators) or has strange ownership semantics (eg. arbitrary graphs).
Like mutabah, I'd recommend reading the official book and skipping the parts that cover things you already know. Several features used heavily in Rust are not really in C++, such as traits or pattern matching.
Is it easy for someone relatively new to contribute towards?
&gt; Simply put, the Rust compiler has a mindset that there is no such thing like a mutable shared reference (i.e. it is an Undefined Behavior in Rust). Isn't the `unsafe_cell` language item to mark stuff as having internal mutability and thus could be "mutably aliased"? I just had one of my own question answered: &gt; I'm not sure if in Rust memory model, atomic operations can be reordered with stores. If so, my code is incorrect since the length increment during a push can be reordered to before the actual data is written, exposing inconsistent data to other threads. This can't happen, due to `Acquire` and `Release` semantics. _All_ writes before a `Release` store is guaranteed to be visible to all reads after an `Acquire` load (on the same atomic object). Thus the pointer write will never be ordered after the `length` store. Thanks for the replies...
I don't want to bash them. It's an extremely powerful project. I'm just old school and don't want the http and Protobuf dependency(although I guess it does support text mode as well)
I get it, but alas my summarization policy isn't fully baked out then. I get not making the user think about a histogram and allowing them to just time things. I just need to make it
No can do, sorry. You might find that `struct MyTuple((i32, i32));` is more suitable to whatever you're doing, but that's just cheating really.
&gt; Isn't the `unsafe_cell` language item to mark stuff as having internal mutability and thus could be "mutably aliased"? You are right, it can be used to "mutably alias" things. But it is not an unlimited license: it cannot be shared across threads, it cannot be used to circumvent the aliasing restriction of ordinary references (the data should have been inside the unsafe cell already), and most importantly, *references made from the unsafe cell should still obey the aliasing rule.* If it is possible to make aliased `&amp;mut` references or simultaneously aliased `&amp;` and `&amp;mut` references, you are out.
There are some big differences. Traits, in particular, are not the same feature at all in Rust as in Scala (though they have some superficial similarities).
&gt; _references made from the unsafe cell should still obey the aliasing rule_. My logic is: just as a `Vec` has `split_at_mut` to split into two mutable and mutually exclusive memory regions, `CVec` has a mechanism to split at the end such that everything before the end is shared and immutable, and whatever that comes after the end (the allocated, uninitialized memory) is mutable and exclusive. Btw, the user is never exposed to a `&amp;mut` within the buffer.
&gt; I am aware that I could simply do: &gt; increment_tuple( (test_instance.0, test_instance.1) ); &gt; That would create a new tuple from the fields in my struct Such a "copy" will be optimized out in release build. So don't worry about the performance aspect of it.
Yeah, I've overlooked the original source code. Sorry! Please read the reply to the neighbor for my analysis.
That snippet is not correct. `CVec` has no `clone` method. The correct equivalent snippet is: use std::thread; let v = CVec::with_capacity(1000); // there is no new let vv = v.view(); thread::spawn(move || { let mut v = v; loop { v.push(some_val()).unwrap(); } }); thread::spawn(move || { // this will observe the growth loop { println!("{:?}", vv.as_slice()); } }); Even though the other thread observes the growth and continues to borrow the slice. My claim is that it will never be able to observe junk (uninitialized) values (at the end). `self.inner.buf` _is_ synchronized w.r.t the corresponding `length` change. The `Acquire` and `Release` semantics guarantees that (see &lt;http://en.cppreference.com/w/cpp/atomic/memory_order&gt;). &gt; Imagine what happens when the slice cannot be resized in place. It is never resized out-of-place. See [this line](https://github.com/critiqjo/cvec/blob/847e390/src/lib.rs#L73). `double_in_place` fails if it cannot resize it in-place. [`try_reserve`](https://github.com/critiqjo/cvec/blob/847e390/src/lib.rs#L89) fails when `reserve_in_place` fails. Both of them always fails in my system -- may be the current allocator does not support it...
&gt; My claim is that it will never be able to observe junk (uninitialized) values (at the end). EDIT: I didn't realize this part, which is disconnected from my argument about atomic pairing of `self.inner.buf` and `self.inner.len()`. I have another concern about this part, however: `shrink_to_fit` will update `self.inner.buf` independently from `self.inner.len()`, which would easily jeopardize any outstanding borrow from other view threads. &gt; `self.inner.buf` *is* synchronized w.r.t the corresponding `length` change. The Acquire and Release semantics guarantees that (see http://en.cppreference.com/w/cpp/atomic/memory_order). In my understanding it prevents the reordering of accesses to *that value* only. What you are imagining is very stringent requirement (serializable reads and writes for every access?!) and probably should only be done with the explicit memory fence, or `SeqCst` reads and writes in the limited fashion. The canonical reference should be [the LLVM documentation](http://llvm.org/docs/LangRef.html#memory-model-for-concurrent-operations), by the way. I think it does not prevent *any* reordering of non-atomic accesses [1], in fact. [1] To be exact, those "equivalent" with respect to the "program order". (It might be possible that Rust has very strictly defined program order in this sense.) &gt; `double_in_place` fails if it cannot resize it in-place. [...] Both of them always fails in my system -- may be the current allocator does not support it... Ah, indeed. I misread the documentation (oops). I believe jemalloc can grow the memory in place, but I'm not sure.
&gt; The canonical reference should be the LLVM documentation, by the way. It links to [this page](http://llvm.org/docs/Atomics.html#acquire), where it says: &gt; Acquire provides a barrier of the sort necessary to acquire a lock to access _other_ memory with _normal_ loads and stores. And, both of them refers to the C++11/C11 standards as the reference, which says for acquire: &gt; This ensures that _all_ writes in other threads that release the same atomic variable are visible in the current thread. And from my understanding, "all" refers not only to the same variable, since the introduction says this: &gt; `memory_order` specifies how regular, non-atomic memory accesses are to be ordered around an atomic operation. So, does all this mean that my implementation is safe (though not very useful, in general, without resize-ability. As I said, I have a very specific use-case which doesn't really need resizing.)
Okay! I tried to be as maximally formal as possible I can. Probably wrong, so please use your best judgement. ---- Let's assume that there were the following reads and writes to `ptr` and `len`: 1. Wptr1/Wlen1 (W1 combined) for the initial write to `ptr`/`len`. 2. Wptr2/Wlen2 (W2 combined) for the second write done in any `&amp;mut self` method on the owner. (We actually have Rlen2 following Wlen2, but that is irrelevant.) 3. Rptr3/Rlen3 (R3 combined) for the read done on the view. We are sure that W1 is done much earlier than W2 and R3. We don't know if Wptr2 and Wlen2 would be executed in this order assuming a single thread execution, and so do Rptr and Rlen. Keeping this in mind, let's construct the *happens-before* order A -&gt; B: * All normal guarantees on the single thread execution: W1 -&gt; W2, W1 -&gt; R3. * Since Wlen2 is done with Release ordering and Rlen3 is done with Acquire ordering, assuming that the write did happen before the read (in the temporal order) they are paired: Wlen2 -&gt; Rlen3. * There are no other guarantees made. Now consider what Rptr3 reads: it is not volatile, and since there is only one write *happens-before* Rptr3 (namely Wptr1) it will see the original value of `ptr`, and not the updated value. Note that Rptr3 can see only one write, since the owner is unique; we would never see *undef* for Rptr3. If you follow this reasoning, you may wonder if Rptr3 is never possible to observe what Wptr2 has written. [1] This is complemented by another atomic updates provided by `Arc`---they are only temporally ordered (no Release-Acquire pairs formed except at the final `Drop`), but that's enough for ensuring that Wptr2 is eventually available. Of course it is still not enough to pair Rptr3 and Rlen3. [1] You can imagine a fantasy architecture where there are per-core data caches and they can only synchronize on requested bytes.
&gt; Make sure to add tests to ensure the two types have the same result from `std::mem::size_of` Doesn't `transmute` check the sizes of its arguments already, making this test unnecessary? From [the docs](https://doc.rust-lang.org/std/mem/fn.transmute.html): &gt; Both types must have the same size. I just tried to transmute types of different sizes, and it results in a compile-time error.
&gt; If you were modifying your previous code I just took your version. &gt; you should also stop creating and overwriting the mutable hasher that write_i32 is passed. You can just self.0 &lt;&lt;= 10. Thanks for the headsup. 
"This video is unavailable." :C
`#[unsafe_no_drop_flag]` is an extra thing, you don't need it, and it will be removed in the future. It specifies that the struct doesn't want to have a drop flag that specifies whether the struct has been dropped before, but these drop flags are going away anyway.
Not yet.
No runtime checks happens.
We explicitly rejected 1, as a design decision.
I think this does make sense, actually. Context-independent functions are used in many places and their prototypes should say as much as possible about how to use them, so that users don't have to go rooting around in their definition. On the other hand, context-dependent functions are used near where they are defined (and this definition is usually short), so no "rooting around" is needed anyway.
I just tried them and both videos are working for me. I edited my post to emphasize they are two different links. I think originally they were squished together and looked like a single URL.
There aren't really any downsides; `MyTuple((i32, i32))` is a "newtype" around an `(i32, i32)`, which means they're basically the same type except in name.
Another name for [dependent type](https://en.wikipedia.org/wiki/Dependent_type).
Why doesn't it make sense to let the programmer choose which makes sense for the current context? We have `#[allow(missing_docs)]` for a similar reason, no? The complexity and clarity of a function depends more on the programmer than the type of function chosen imo.
Uppercase "pi", here denoting the [dependent product type](https://ncatlab.org/nlab/show/dependent+product+type), one of the type constructors in many dependent type theories. It's a sort of generalization of functions, allowing application of a type-level function to compute a type.
&gt; I applied `.count()` to the end, to force evaluation Yes, in Rust, we use `for` loops to cause lazy iterators to evaluate. There is also a `foreach` in the itertools crate, if you prefer the fully-functional style. I would write this code more like this: fn to_upper(reader: &amp;mut Read, w: &amp;mut Write) -&gt; io::Result&lt;()&gt; { for line in BufReader::new(reader).lines().into_iter() { let mut s = try!(line); s.push('\n'); w.write(s.to_uppercase().as_bytes()); } Ok(()) } 
&gt; It also guarantees that when the same variable is Acquire loaded, *all* the pending stores until the *paired* Release store is flushed to the other (core running the other) thread. Probably this is the only difference between my reasoning and your argument. I believe this is not true, and anything that makes this seem true actually either... 1. ...involves memory fence, that can be paired to any memory reads or writes (indeed, the preshing article explicitly deals with fences); or 2. ...is enforced by the program order. The latter is what I'm not really sure. I've assumed that `expr1[expr2]` does not imply that `expr1` runs before `expr2` in the program order (so the former automatically *happens-before* the latter), but AFAIK Rust already has a defined argument evaluation order (left-to-right) and it might have a similar rule for every AST.
&gt; Another name for dependent type. Much more cool looking another name for dependent type. It boosts the RFCs chances for acceptance and also makes the author look smarter.
Your number 2 request is covered by the abstract return types/`impl Trait` discussion. There have been many proposals for this, but none have yet been accepted. There is currently an open [minimal `impl Trait` RFC](https://github.com/rust-lang/rfcs/pull/1522), that has gotten quite a bit of discussion and is in final comment period, and which links back to the previous proposals. This minimal proposal only allows this as a return type, and only for freestanding functions or inherent-impl methods, not anything else like trait methods; the idea is to start small, and then generalize it to be usable in more places. For number 3, you can always use `_` in place of type parameters that can be inferred. For example: mod foo { pub trait Bar&lt;A, B&gt; { fn baz(self) -&gt; (A, B); } impl&lt;A, B&gt; Bar&lt;A, B&gt; for () where A: Default, B: Default { fn baz(self) -&gt; (A, B) { (A::default(), B::default()) } } } fn main() { let x = foo::Bar::&lt;usize, _&gt;::baz(()); let y: u32 = x.1; println!("{}", y); } Or more commonly, something like: let v = (0..10).map(|x| x * x).collect::&lt;Vec&lt;_&gt;&gt;();
LLVM doesn't know anything about tuples, really. It just sees that you're doing a multi-step copy of a contiguous block of memory, so it converts that into a memcpy. Then it sees that the thing you're copying from is never used again, so it elides the copy entirely.
&gt;Your number 2 request is covered by the abstract return types/`impl Trait` discussion. There have been many proposals for this, but none have yet been accepted. There is currently an open [minimal `impl Trait` RFC](https://github.com/rust-lang/rfcs/pull/1522), that has gotten quite a bit of discussion and is in final comment period, and which links back to the previous proposals. This minimal proposal only allows this as a return type, and only for freestanding functions or inherent-impl methods, not anything else like trait methods; the idea is to start small, and then generalize it to be usable in more places. Only having this as a return type is a non-starter for me. My most common use case for this problem isn't return types - it's structs. Iterators and closures are incredibly annoying to store, basically to the point where I end up "coding around" this problem, e.g. reinventing iterators or adding a bunch of runtime allocations. &gt;For number 3, you can always use `_` in place of type parameters that can be inferred. For example: &gt; mod foo { pub trait Bar&lt;A, B&gt; { fn baz(self) -&gt; (A, B); } &gt; impl&lt;A, B&gt; Bar&lt;A, B&gt; for () where A: Default, B: Default { fn baz(self) -&gt; (A, B) { (A::default(), B::default()) } } } &gt; fn main() { let x = foo::Bar::&lt;usize, _&gt;::baz(()); let y: u32 = x.1; println!("{}", y); } &gt;Or more commonly, something like: &gt; let v = (0..10).map(|x| x * x).collect::&lt;Vec&lt;_&gt;&gt;(); Ah cool, for some reason I didn't think that you could use `_` in this context.. It's still annoying to have a bunch of type parameters all set to `_` (e.g. for the cgmath traits), but better than specifying them directly!
This is true, yeah.
Sh*t! Why is it not `reallocate_inplace`?! It's supposed to shrink, dammit!!! :) I will remove that method. Thanks much!
I can imagine that it tries to minimize the number of "holes" whenever possible... :)
&gt; If the program order always dictates that Wptr2 -&gt; Wlen2 Let's hope so! I think the compiler wouldn't dare cross anything involving atomics!! :) &gt; your code didn't have any memory fence Now, I think it is better to load `len` Relaxed, and apply a fence only when needed ([commit](https://github.com/critiqjo/cvec/commit/3bdbffd8)). This will probably make inspecting just the length faster (without looking at the slice). And thanks a lot for the discussion! I learned a lot (hope you did too)! This is my first time significantly tinkering with memory ordering. And (re-)reading those articles really cleared much doubts. _EDIT:_ `SeqCst` in `drop` was changed back to `Acquire` ([commit](https://github.com/critiqjo/cvec/commit/9682e1ab)), since I felt `SeqCst` to be an overkill in ~~any~~ almost all situations (global ordering of _all_ `SeqCst` operations in addition -- scary!!)
you seem to have not read anything I said. I'm not going to argue with someone who ignores points that have already been made. I'll give you a starting point to go back and see all the stuff I said that you missed. &gt; If you can't predict the problem, then you have no clue when it could happen! Thank you for basing your entire argument off of ignoring this sentence. The fact that you cherry picked the comments around this one sentence shows you aren't worth arguing with. 
This is the one I'm co-speaking! Thanks!
Yeah also co-speaking, thanks. It's gonna be fun!!
Misleading title. The listing mentions that "[the ideal candidate should be proficient] in at least one modern programming language such as Java, Go, Rust, Clojure, F#", not that this is a Rust job.
https://doc.rust-lang.org/nomicon/meet-safe-and-unsafe.html
Calling it "PlayRust" will only add to the confusion. :D
Which is a function type, not to be confused with Sigma, which is the dependent sum, which can be used to encode products ie tuples. 
This deserves its own confusing subreddit. /r/rustplayrust
I would like to formally lodge a complaint against rustconf's so-called "blind" proposal review committee. No less than 6 sessions are run by core team members, with aaron and niko both taking opening keynotes in addition to their own provate tutorial sessions! Meanwhile I submitted no less than 27 proposals describing why an individual syntactic choice renders Rust useless because it blocks the adoption of critical features like higher kinded modules and dependent currying. And what do I get? Nothing! I see now that the Rust project isn't a meritocracy -- it's a dictatorship! I will be attending this conference only to protest the totalitarian regime I see before me. I'll be sticking to languages which were designed and governed properly, like C.
That should be for a rust AI in rust.
There was [an RFC for allowing more type parameter elision](https://github.com/rust-lang/rfcs/pull/1196) but it seems to have been deferred in favor of a yet-to-be-proposed-but-more-complete solution.
I am quite surprised to see Java in that list... the others are quite recent but Java? And if Java, why not C# (seeing as they have F#)? It's a weird list...
I have no idea what I'm reading here. Is this a joke, a serious comment or what? I'm clearly missing some sort of reference.
It's definitely a joke.
The intention as I understand it is to eventually allow existentials everywhere, but return types are where its starting because they're the most common problem.
&gt; My most common use case for this problem isn't return types - it's structs. Couldn't you just use a template parameter for that?
I see, I'm not here enough to remember who the resident jokers are. I'm just here for the trifecta. Thanks!
&gt; Meanwhile I […] describing […] higher kinded modules and dependent currying That sounds like a webinar I'd totally watch/attend/whatever-you-do-at-these-things!
Will this only be on Saturday (minus Friday training) ?
No, of course it's not purely Rust-based position. I just check occasionally for Rust position in the UK and it's one of a few that I've seen for long time, I was just happy to see that progress. To the big three I would add streams and optional, it's very important to me and how they integrate.
The talk about VLC looks interesting. Is it some experimental fork of VLC, or there are serious plans to integrate Rust code into mainline VLC? I see G. Couprie is in top 20 VLC developers, this looks promising!
Will any of it be streamed/made available online?
Maybe you will find [The Power of Pi](https://cs.ru.nl/~wouters/Publications/ThePowerOfPi.pdf) interesting. I think it is a great introduction to dependent types.
Thanks. I've heard of dependent types before but I haven't really delved into that sort of thing, and I find the syntax behind all of this stuff really arcane and burdensome. I'll give this a read.
Fellow newbie here. I took a stab at making one, and you're welcome to look through my implementation if it helps. Reading about converting infix to postfix expressions helped immensely, and you can read more [here](http://condor.depaul.edu/ichu/csc415/notes/notes9/Infix.htm). Evaluating user input can be broken down to ~three steps: * Tokenize the user's input * Convert the tokens from infix notation to postfix notation * Evaluate the postfix notation. Again, [this](https://github.com/Ophirr33/calculator/blob/master/src/main.rs) is some very ugly code, but it works and you're more than welcome to use it as a reference.
I don't think that there are any actually released multiplayer games in Rust although I was working on a very simple websockets game a while ago. The code is very messy though and it is unfinished but if you want to look at it, the repository is at https://github.com/benjamin-l/hyperagario. Game development in Rust has nowhere near the library support of C++ but the ecosystem is being actively worked on especially with the Piston project. Conrod is great and by far the best experience I have had with GUI programming. Glium is also really easy to work with if you are making a 3d game or need shaders. For simple 2d Piston's graphics library works very well. GFX is another alternative for more direct control but i prefer glium. I have never used enet-sys but since it is a raw binding it probably will not be very idiomatic. Depending on how much work you want to do it might be worth it to use something with an mio backend.
That makes a lot of sense, I like the comparison. Have you built any large, commercially-viable projects in pure Rust, or have you found it tends to find its place more in specialty components that require the speed or low-level capabilities of say C? The thing I worry about with pure Rust is that I'll spend too much time writing code for "solved problems" (e.g. distributed computing, service discovery, whatever) instead of the actual problem at hand. Even looking at something like hyper to use a REST api to provide access is a bit daunting. Thoughts on how to alleviate this?
That is interesting. Scala definitely feels like it embraces the 'everything to everybody' model which gives it a longer learning curve.
Scala was my language of choice before I switched to Rust. It's hard to "think" of Scala in conrete terms, without handwaving, but it's a known point that Scala's type system is a little too powerful, it's easy to make something in it that would be hard to grok later. Although Rust has been getting typesystem tricks too as of late, like using the `Borrow` trait in collections.
If you're going to have to know Java anyway, and if you're tied to the JVM, then why wouldn't you just use Java? It's simpler and better supported.
That's were the languages go their separate ways. Scala is DSL-friendly by design and Rust is by design explicit. So in Scala you see those pretty and novel-looking Actor DSL interfaces like Spray. And in Rust you see what's going on, down to the error handling bits. &gt; Is there anything you know of more analogous to Spray that makes it really easy to hook stuff together polyglot style? Can't say that I have, not my cup of tea. I like to stay close to the underlying protocols. For HTTP I'm using either Apache modules (where Rust works directly with the Apache C structs) or SCGI servers, the latter is just a map of incoming HTTP headers and a stream.
Using a type parameter means leaking internal details to the outside interface, so if I want to nest this wrapped thing inside a struct, it's the same problem over again. And it's common to have structs nested several layers deep, so leaking tiny details through all of those severely breaks abstraction boundaries. And it doesn't even communicate the right thing, really. A type parameter says "plug (basically) any type in here", where I'd like a way of saying "there's one very specific type here.. but it's awful to write."
He'll most likely be talking about nom, his parser combinator framework. You can check it out on GitHub 
I wrote this a while back https://github.com/tedsta/reforge. I didn't finish or release it though. It's multiplayer, but not real-time. Beware messy code.
This is also a stupid Hyper/Iron question. If I have a [hyper::status::StatusCode](http://hyper.rs/hyper/v0.9.9/hyper/status/enum.StatusCode.html) and want to convert it to an [iron::status::Status](http://ironframework.io/doc/iron/status/enum.Status.html) (this is just the same enum, [re-exported](https://github.com/iron/iron/blob/master/src/lib.rs#L142)), what do I actually have to do? Static casting with `as` doesn't seem to work, nor does re-assigning to a new variable with a type annotation (fails to compile with "mismatched types"). What really obvious thing am I missing?
They are looking for a Java developer, it's in the title.
It was a long time ago, before the RFC process, so my memory is a bit hazy. I believe it was brought up, but not seriously considered, as it still has those problems, albeit, just for you and not for your users.
That's fascinating, I had no idea. So the fact that these two languages both strike my fancy must not be a coincidence :) And certainly I agree, IMO the Rust type system is as minimal as it could be without being "deficient," in a sense. It seems to make many things that would be scary in Scala a lot less scary -- e.g. adding new functionality to existing types (implicits are much harder to maintain than adding traits to existing types, it would seem). May I ask what got you involved in the Rust community, specifically with Servo? 
I think you've identified Rust's biggest weakness right now; a lot of other languages have much better library support than Rust for many use cases. Rust is still a very new language, after all. I think at the moment, Rust is most suited for - a) something you'd write in C/C++ right now, like something intensely graphical, b) embedding some fast, reliable, low-overhead component inside a system mostly written in a scripty languages like Python, Ruby, or JavaScript. There's been a lot of work on this with Ruby in particular.
My actual structs don't look much different than your placeholders, pub struct Listener1&lt;'a&gt; { counts: HashMap&lt;&amp;'a str, u64&gt;, total_count: u64, } #[derive(Hash, Eq, PartialEq, Debug, Clone)] enum Color { Red, Blue } #[derive(Debug)] pub struct Listener2 { ids : HashMap&lt;u64, Color&gt;, counts: HashMap&lt;Color, u64&gt;, total_count: u64 } It looks like the iterator parameterizing self on the lifetime is why the message needs to live past the loop, but it's still fairly opaque where the compiler is upset with the lifetimes. It's unfortunate that this has been so far from straightforward to generify, or more specifically that annotating lifetime parameters when they are not inferred is so mysterious. Do you think the general organization of the code that is more or less correct for this? The threads will never exit the loop, so I could have passed everything by value (although that would have involved a better understanding of how to static dispatch the trait, which I couldn't quickly get to compile). Before I had a copy of the fully inlined handle_socket + handle_message loop for each Listener, but then I ran into the much deeper than expected hole of deduplicating code :)
I think people are working on "Scala LLVM", FWIW.
There are benefits of Scala, but they *don't* make sense in a corporate setting. The language is complex and rather cryptic. Tools requiring advanced type-system features tend to require in-depth knowledge and are hard to debug. Java doesn't have these problems, and far more people know Java than Scala. &gt;By the same argument, wouldn't I be able to say if you're tied to Linux x86 you may as well use the better-supported and simpler C language over this newfangled Rust business? I don't see how that's a reasonable comparison. Are people choosing Rust because it runs on Linux x86 with C compatibility as often as people are choosing Scala because it runs on JVM with Java compatibility? No, because "Linux x86 with C compatibility" offers much less than "JVM with Java compatibility" does in terms of portability and ease-of-use. In other words, portability is a big reason people choose Scala, and it's not so much the reason people choose Rust.
This is super cool that Mozilla is able/willing to do things like this! I'm really looking forward to what carllerche and the mio team is able to do with mio. Especially since I'd say this project or ones built on top of it, such as some coroutine/green thread libs, are a *very* common "feature request" for Rust. I'd also love to see this sort of thing continue, even if it were in the form of 60 projects getting $500 or something (and not just libs/bins, think docs, podcasts, anything!). Imagine the boost to the ecosystem! Hopefully that doesn't come out wrong; that wasn't a knock on this super generous donation! mio is extremely important and well deserving library. I'm speaking only about hypothetical futures.
Ergonomic improvements? Wonder what that means. mio's great but it definitely has a steep learning curve.
[removed]
I have partially implemented a Phantasy Star Online server emulator in both Rust and Scala. [The Rust version is here.](https://github.com/BygoneWorlds/idolapsoserv). I will say, it is _very, very hard_ to do anything video games related in Rust. This can be a big turn off when a lot of game development is iterative; you have long compile times, strict syntax rules, and all sorts of other issues to deal with. Cargo is still not quite so great at handling multi-module projects, which is the only reasonable way of handling very large codebases, the kind you will certainly see in a video game of any reasonable scale. That said, if you can find something that works for your application, it is very nice to work in. The server I wrote depends heavily on passing around Rc&lt;RefCell&lt;_&gt;&gt; even within the same thread to manage mutable state. It's not ideal, and can be frustrating syntactically at times. Without tooling it's very easy to make mistakes that result in compile errors, where compile times on Windows are already pretty long even for ~10k lines of code. I don't make use of macros much, except for the enums that wrap all the different message types in the protocol, and all the serializing for them is hand-written. Find a way to use a scripting language for your gameplay code since by nature it is more iterative and won't benefit greatly from the static checks done by the compiler. Also, dear god, sorry about the spam. Reddit broke entirely for about 5 minutes.
I like where this is going and it is an oft requested feature indeed. That said, I do not see any relation between this and "const fn'' which IMO is a redundant feature that overlaps macros. (let's continue improve macro ergonomics instead!) 
I hope MIO gets TLS support with this.
This year? There was only a single day of talks last year too. :P
There's an issue out there describing the new API, I'm pretty sure [this](https://github.com/carllerche/mio/issues/360) is it.
So, there are actually three types here, not two. You have `&amp;[T]`, `[T]`, and `[T; N]`. `[T; N]` is an array of fixed length. `[1, 2, 3]` is an example of that. `[T]` is an unsized slice. It is a dynamically sized type, and can only appear behind a pointer (so, `&amp;[T]` or `Box&lt;[T]&gt;`). When used like this, the pointer is "fat" and has a second length field. So, both `&amp;[T]` and `Box&lt;[T]&gt;` are a pointer with a length field, with differences in ownership. Slice implementations are done on `[T]`, using `&amp;self` and `&amp;mut self` methods only (since `[T]` cannot exist without being behind a pointer). All array-ish types dereference to `[T]` (`[T; N]`, `Vec&lt;T&gt;`, `Box&lt;T&gt;`, etc). Since Rust has autoderef, `[1,2,3].iter()` is actually `[1,2,3].deref().iter()` or something similar. I suspect you mean `[T; N]` when saying `[T]` in your post, but I'm not sure. Note that for loops have a sugar where `for i in &amp;x` is desugared to `for i in x.iter()`. 
I think what /u/brson is saying is that Scala has influenced the _way_ that Rust has evolved, as opposed to specific language features. I would say that Scala and Rust share similar features given shared common ancestry in the family of ML-derived languages. Rust was originally written in OCaml, where it gets algebraic data-types and exhaustiveness checking of match expressions, and its traits are inspired by Haskell (to name a few things). 
&gt; May I ask what got you involved in the Rust community, specifically with Servo? /u/brson works for Mozilla Research :-)
And we can [use the dependent product to encode the dependent sum](https://github.com/agda/agda-stdlib/blob/master/src/Data/Product.agda#L20)! Which as you said, can be used to encode products. We really milk these words for all they're worth...
Yes, *please*.
And much more. They're actually developping core vlc modules in rust to remove C vulnerabilities. I guess he'll talk about it.
Yes. The choices were 4 US cities, Berlin, and Tokyo :-/
If it's just reexported then the types should be interchangeable without any fuss. The conflict you're getting is probably from having a different version of Hyper than what Iron is using. Make sure they are *exactly* the same.
Nice work, looking forward to use it!
There is a lot of work already in progress / in the pipeline. I'll be talking more about it soon :)
&gt; I'd also love to see this sort of thing continue, even if it were in the form of 60 projects getting $500 or something (and not just libs/bins, think docs, podcasts, anything!). The paperwork involved in paying $500 to someone (especially a natural person) makes this a very costly endevour.
PlayRust is play.rust-lang.org, right?
Congrats /u/carllerche! I've been using for Rust for a while now, so it makes me happy to see more and more companies put money behind Rust projects. I get that this is Mozilla in this case, but it's not like the list is full of Rust projects. 
to be fair, a one-time 30k isn't enough for someone to quit their (programming) job anyway, so I'm not sure what position that puts the author in. Needing to spend enough time to be worth 30k, but presumably not being able to quit their day job.
Also only just occurred to me that this isn't going to handle non-integer strings properly... guessing I need a try! in there or something.
&gt; what happens if the usize in the machine you are compiling is smaller than the one on the target? Suppose that the usize of the target is equivalent to a u64 and the usize of the compiling machine is equivalent to a u32. Then, compile-time usize should just be translated to u64. (run-time usize too will also be translated to u64 at some point) If Rust ever gain compile-time floats, perhaps the compiler will need a software floating point implementation, yes.
&lt;now irrelevant&gt;
You should seriously look into studying the standard library's types. Try writing down all the methods provided by the `Result` and `Option` sum types, for example. You could drastically reduce the amount of lines of code that you have wasted in your example above, and provide extra error handling more easily. In example: let client_id = request.param("clientid").and_then(|id| id.parse::&lt;i32&gt;().ok()).unwrap_or(-1i32); How you handle the error is your choice. You can return a default value as above or you can create your own custon error enum, or you can go all out with a more functional approach to the error handling, like so: request.param("clientid").and_then(|id| id.parse::&lt;i32&gt;().ok()).map_or(String::from("{}"), |client_id| { let conn = Connection::connect(&amp;*postgres_url, SslMode::None).unwrap(); // ... })
Oh, totally missed that.
Like pegasos1 said, we're referring to the posts on the /r/rust subreddit that were intended for the /r/playrust subreddit. The talk is about building a machine learning pipeline that can classify posts on /r/rust that were intended for /r/playrust, entirely in rust. We're going to try to change the title to make this clearer.
For a web service, I'd probably expect to get a 400 bad request response if I made a request to this route without an ID or with a non-numeric ID. It looks like Nickel's [`Response.error`](http://docs.nickel.rs/nickel/struct.Response.html#method.error) makes that easier; there's a decent example of using it in [Nickel's static files handler](https://github.com/nickel-org/nickel.rs/blob/e70ef20298990d74ebd51040d95cf40d5f7ed211/src/static_files_handler.rs#L59-L68). I don't think I'd use `try!` here since `try!` would return the `ParseError` and you'd want to return the `BadRequest` error. So I'd probably do something like this, using match statements: let client_id = match request.param("clientid") { Some(id) =&gt; match id.parse::&lt;i32&gt;() { Ok(i) =&gt; i, Err(e) =&gt; return response.error(StatusCode::BadRequest, format!("Cannot parse `clientid`: {}", e), }, None =&gt; return response.error(StatusCode::BadRequest, "`clientid` is a required parameter"), }; There's probably a nicer way to do this with methods on `Option` and `Result` as /u/mmstick pointed out; I kind of like seeing the cases on different lines like this though. 
Just a guess: Intel retweeted a story about intel employee Josh Triplett giving a talk at rustconf https://twitter.com/josh_triplett/status/743556267214438400 
Yeh it is just an over-excited hiring manager who wishes he had a better job, tossing in the cool stuff he read about on HN to the job description in the hopes that the reflected glamour will attract someone half-decent to the position. It is just another corporate java dev position like a million others.
I actually got involved with Rust after failing to get involved with Scala. I wanted to learn how to build a compiler, and starting hacking on Scala (I may even have a single patch in Scala somewhere), but it was a big project already and hard to just dive in. Rust was announced and was very small at the time and easy to grasp, with lots of obvious contribution opportunities. So I started hacking on Rust, then joined Mozilla. Only after that did we start to build Servo.
I can't got into specifics, no. It's been many years since I've worked in Scala. I just remember the sense that it has a lot of clever ideas that work cleverly together, but as a whole it's very difficult to understand what things are doing. I think that's a pretty common reaction to Scala.
https://www.reddit.com/r/rust/comments/4pbehh/rustconf_program_is_up/d4knj8n
https://tilde.wufoo.com/forms/rustconf-scholarships/
but what about next year
Do you think that getdns could be integrated with mio easily?
Scala strikes me as over-engineered and unnecessarily complex. By default I use Clojure &amp; Rust.
Is there any way to achieve "associated statics" in Rust? I need to store and retrieve a type info pointer (from C ffi) for certain Rust types of interest. I know C++ could achieve this with a generic struct and static data members.
I don't think there's anything inherent about Rust preventing less boilerplatey frameworks. It's still early times. I fully expect to see them.
It's difficult or impossible to write some iterator and closure types. And type aliases are ultimately fragile - if I decide to use a different iterator, now I need to painstakingly rewrite the alias, and if I forget, the compiler _knows I'm wrong_ and will _tell_ me what the right type is (roughly), so I'd like to be able to just use whatever type the compiler infers.
I believe the plan is to allow people to contribute if they wish; I'll find out.
Some pointers? Don't you mean references? :o)
I'll take your word for it, and it matches my experience with C++11 and C++14, it just feel inconsistent to mention only Java among those newer/lesser known languages :)
Yeah, steve reminded me of the action-at-a-distance bugs that plague Haskell et al. Edit: although I don't necessarily agree that functions are the right place to draw that line. In a perfect world, maybe non-`pub` functions could still allow elided type signatures, but that involves a whole discussion of syntax and whatnot.
You would effectively wrap an ugly type in a pretty one. You would need a wrapping type for each ugly type you wanted to prettify. That being said, you could use an enum to help using the code. It's a fair amount of boilerplate though.
I am used to big monolithic C++ libraries so my tolerance is already high :D
Oh, I'll be keeping my ear to the ground then!
Maybe :) Once I have something to show, I'll write a blog post. It'll be easier to talk about when I have something more concrete.
How long do you think the tickets will last by approximation? Don't know if I can afford a ticket until later July.
I have no idea. It's only the second year we've done this, and it's in a totally different city...
Thanks, Steve. Any idea if I can refund the "scholarship" ticket?
Thanks for making it a Saturday conference. I'm psyched for my first visit to Portland.
Not sure how usable this is, but a quick google got me [this](https://github.com/dati91/blurz/blob/master/examples/test2.rs). But yeah, you probably want to bind to bluez, which is the standard way to use bluetooth in userland linux
Sorry, I did not parse this. What do you mean by refunding a scholarship ticket?
[This](https://tilde.wufoo.com/forms/rustconf-scholarships/). I could apply, but I'd rather leave this for someone who definitely won't be able to afford.
Oh, so you mean like, if you applied for a ticket, could you refund it later. I was thinking "we just announced the program how did you get one already?" I am honestly not sure what to say in this situation. Like, if we awarded a ticket to someone that couldn't use it, we'd give it to the next person on the list, for sure. 
Wow... this is VERY tempting, shame I already put together a trip this year and I'm now saving up for the future with my better half. MAYBE next year summer, since I'll have had time to actually make something with Rust + Python rather than just dream about it. Will RustConf talks be placed online?
(That’s the joke.)
Yes!
It seemed that the above comment implied that this was a regression from our only previous conference.
At my current job I'm getting close to using Rust on a backend service using Iron/Hyper/SerDe and I believe the current development is going well. So I would say from a technical standpoint, it is certainly feasible. The biggest problem I have faced is political, people fear there aren't enough Rust programmers to work on the project. However, I work in a company that prides itself on having good programmers, and as I have explained many times, good programmers can adapt to new languages. Since you are discussing using elm well, it seems that you have free reign to experiment, so I would say go for it (assuming you are comfortable with Rust).
Ahhh I was thinking more about the future when I said it. I get it now :)
Oh wow - didn't know that history! Glad we got you on board!
For me it was mainly the huge flexibility in the syntax that makes it hard for me to understand other people's code, and the poor type inference :(
Is it just me, or is $199-$219 kind of steep for an up-and-coming language conference? Also, $199 is kind of low for training?
Elm is pretty backend agnostic, and you can use Rust's enum and Elm's 'type' to model algebraic data types. You probably want to compile your Elm separately, then use ports to initialize it with correct arguments in your server. 
Yes I already clone my message, while this may be not perfect, this is a price I like to pay. Thanks for your hint about bus, will look into it (I still wonder, if there is no easy solution, which is included in rust as I assume this is a problem almost everyone faces with multi threaded apps).
I hadn't seen this and the first one is coming up tomorrow! Let's pitch in!
Yes, which is why I like the ring approach of using a C library for crypto and a pure Rust implementation of the protocol in webpki. 
It's certainly not a model of great code, but the backend web server &amp; scraper for http://rusty-dash.com/ are written in Rust. I've been planning to write something a little more substantial about the experience, but some quick thoughts: Big pluses: * I love writing Rust. * While it's the case for lots of Rust I write, I still am delighted by how often things "just work" once they compile. Very different experience from the Django apps I maintain at `$DAYJOB`. * Performance is generally fantastic. * Explicit error handling has prevented 100% of my dumb mistakes from crashing the process (those not caught by the type system, that is). Mixed results: * serde (which is in several ways preferable to rustc-serialize) requires nightly for good developer ergonomics, as does diesel. Not that big a problem, but upgrading nightly invites issues with aster/quasi/etc failing to build. * diesel is a really fantastic database client, but it still has a little ways to go before it can cover all of my use cases (associations being #1 here, I have to drop to raw SQL for complex joins) * iron has some occasionally boilerplate-y ergonomics. I like the router macros, but I do not like fetching URL or query parameters, it's very verbose. * In iron, needing to return an IronResult from endpoints (unless I've missed something here) means that I can't just `try!` a serde result unless I first coerce the serde result type to the crate's result type (orphan rule/coherence prevents impl'ing From/Into between error types I don't own, so I have to use intermediate types). * BTreeMap/HashMap are easily iterated over as `(key, value)` pairs, but JavaScript objects are clunky when used as dictionaries like that. It means that I often have to rethink the return values of Rust functions if they need to be sent over the wire and to collect them into a `Vec&lt;(K, V)&gt;`.
Documentation is something I'm finding particularly troublesome. It was a toss-up between Iron and Nickel, they both seem pretty similar for the most part but it's hard to judge which will win out in the longer term. I might check that out because I'm clearly not married to my decision! 
Wait, did libstd get Cargoified? Also, that won't work because then Cargo will be trying to link bitcode files together.
By indentation-based I mean syntaxes in which indentation is significant, e.g. Python's syntax.
I ... think? Not sure. Not sure about fixed length or what intoiter has to do with it.
You can do full static type analysis on your code if that floats your boat.
Scala seems bloated with too many features. I guess Scala is to Java what C++ is to C? I come from the Haskell community so maybe it's pot calling the kettle black.
https://www.reddit.com/r/rust/comments/4phgaa/rustconf_has_an_official_scholarship_program_this/ !
im a huge elm and rust fan, but if youre going to have anyone else working on this project, then dont use either.
Python has a dedicated lexer that produces a series of normal tokens and special INDENT/DEDENT tokens inferred from the indentation. You can use the same approach, though you would need a parser generator with custom lexer support.
Right, that's new to 0.17?
How secure is such a stack against SQL injection and similar attacks. (See the Django security page for a list) https://docs.djangoproject.com/en/1.9/topics/security/
Unless they're also excited about the prospect of learning new, reliable languages 
Please, please make sure you record them with a decent mic and camera set up :)
I am not sure what the plans are this year, but we used Confreaks last year, and they have a _lot_ of experience :)
[removed]
Hey, thanks!
You are looking for /r/playrust.
Yes, see https://doc.rust-lang.org/std/iter/index.html#for-loops-and-intoiterator for details. &gt; is there a reason fixed length always don't implement into_iter It isn't easy to get right, right now. See https://github.com/rust-lang/rust/issues/25725. The problem is how you drop only those unused elements. 
Because the application doesn't directly accept input from users, it's open to fewer of these issues. XSS, CSRF, Clickjacking, etc. all rely on malicious user content stored in the database and/or authenticated users whose accounts can be abused. ~~Diesel escapes all identifiers before they go to the database~~ (EDIT: and I'm misremembering terminology, what's valuable is that all statements are prepared, see reply below). Like most things in Rust right now, I think if you have mission critical requirements, you should carefully audit the libraries you use, and be prepared to handroll a few parts of the stack.
That's why I said we need to continue improving macros :-)
To put it simply, control plan in Clojure, data plan in Rust.
I suspect you can do this kind of dynamic linking only through the use of dlopen and dlsym. If you try to statically link with version of a library that does not have function you are importing, linker will scream at you.
I'd love to, unfortunately I'm on vacation without my laptop at thr moment. Wait — You're steve klabnik! I absolutely LOVE your talks. Damn, rust is an awesome language to use, work on, and talk about. Really good.
Do the users of your library know which version of the native library they have installed on their system? Then you could add feature flags to conditionally add newer bindings (i.e. add `#[cfg(feature = "foolib1.5")]`).
I've never tried it, and haven't really done any D or Rust, but [vibed](http://vibed.org) seems to get a lot of favourable comments.
Don't worry, we'll certainly make other rust doc days. @steveklabnik1: Damn, you even have fans! I'm jealous!
How big are the messages? If they are very small I guess the best solution is to just clone them. If they are big, and assuming that the receivers just need to read the message, can't you pass them by immutable reference (and destroy them once the receivers have finished sending them over the network)? Or maybe just put it in an `Arc`.
I'm not the author, but I agree that it's great to see more Rust in production :)
You can extract the `ar` archive which is the `rlib` version of the library, they all have LLVM bitcode files in them.
&gt; Just a warning, u32 does not always mean int. u64 does not always mean long too. It all depends of your computer architecture, but for example on x86_64 both int and long are 64bit. And on i386 both int and long are 32bit (long long are 64bit though). I think he's talking about C's `int` and `long` types here. Rust's `u32` isn't always the same size as C's `int`, because C's `int` isn't always 32 bits wide. In the same way, `u64` isn't always the same size as `long`, because `long` isn't always 64 bits wide. But Rust's fixed-size types are indeed the same size across all supported platforms.
Thanks for the writeup. What service/program do you use to make those graphs at the bottom of your post, that show microservice performance?
Yes, I used Arc to avoid cloning in my similar implementation.
For Data serialization what crate should I be targeting? I'm looking to use JSON and while it sucks that I have to write it all by hand (or use nightly) it would suck more if I used a crate that's likely to change or EOL for my traits. This the main reason I've gone back to go for the time being. 
This is just a guess but it looks like the heroku dashboard
There are other options than `rustc_serialize` or `serde_json` if you are willing to give up being able to deserialize json to custom data structures. http://terhix.com/doc/json/ will let you work with JSON as `BTreeMap`s,`Vec`s, and primitive rust types.
What crate(s) are you using for your microservice? An item which concerns me about moving a service to rust is the stability of hyper. Is it stable? What do you use to make sure it stays up? Do you front it with nginx/haproxy?
Sure, just add a lot more types. For example various structs representing networking protocols (like HttpRequest), as well as some more shapes like Sphere, Spheroid, etc.
&gt; Won't there be less inter-library friction if we all converged on to an agreement of internal representation? This friction will instantaneously become a semver hell once a breaking change lands in that definitions crate. There's been a precedent with libc 0.2 and what is now commonly known as the libcpocalypse. 
True, that why I think we should avoid Impl, and ONLY have a bunch of structs.
One 'obvious' choice would be serde with its json backend. Another option is the json crate that was recently announced.
Isn't a http request quite rigorously defined by the HTTP protocol?
First, we've been on Vector 1.0 for hundreds of years. HTTP grew from 0.9 through 1.0 and 1.1 without a breaking change. Second, the wire protocol doesn't define how to represent supplementary state the endpoints need to keep.
That's what `libc` is.
We should get everyone to agree on y-up vs z-up while we're at it. /s
Actually, I was talking about `macro!` not procedural macros. Having a DSL-friendly syntax is not a free lunch. There are costs associated too: - implementation and maintenance cost, - potential opportunity loss (might inhibit other features or seriously hamper the design space), - potential explicitness loss: is that regular Rust, or is that a clever DSL that appears much like Rust but does something different? There are advantages to DSL, but there are disadvantages too like always. There are also alternative designs. A fluent interface might be sufficient, for example, without going full DSLy. As another example, the `regex!` procedural macro would not be required if Rust had a sufficiently powerful `const fn` implementation. DSLs can be useful, they are not a silver bullet or the only alternative.
Have you messed with the associations that are on master at all in diesel? Not saying that they will solve your use case, but I'd love to get some specifics on what your use case is (both in the SQL that you're having to write, and what you'd like to be writing if you have an idea) You are dead on with where we're at though. It's definitely usable, even "production ready". But it's incomplete and has several API holes that have yet to be filled (associations and joins are by far the biggest)
Identifier escaping doesn't have much to do with SQL injection protection. Diesel is well insulated because we *never* interpolate dynamic elements, everything goes through prepared statements.
Some might prefer `type Vector2f = [f32; 2];` and for something like matrices it would be even harder.
If you're going to have Purescript at the front end you might as well have it as the backend. Just easier in general so you don't get confused and can switch between backend and frontend easily.
I agree! Actually I am quite surprised at the 10 ms response time. I am used to services with a ~7 ms response time, because the database slows us down and takes ~5 ms to write to the disk (and flush). 10 ms is slow for a simple in-memory aggregation, and really fast if that aggregation is temporarily flushed to disk to ensure persistence. Without knowing what is done, it's hard to understand whether it's a feat and if using Rust really was necessary. By the same token, I was quite surprised at the 100 MB footprint. I wonder what the percentage of useful data is, and what the overhead of code + data-structures is. Given we are talking aggregation, I would not be surprised to learn there are a few dozens of thousands of counters in there at any time, but without such metrics it's hard to gauge whether 100 MB is really good, quite good or actually not that good.
Sure.
I haven't yet tried the associations on master, but I should! Maybe I'll have time to give that a go this evening. I *think* this example is one that's not currently supported by 0.6, although I'd be happy to be wrong. One case where I recently had to use a full query in raw SQL was like this: SELECT i.repository, i.number, ic.id, i.title, pr.merged_at IS NOT NULL FROM issuecomment ic, issue i, pullrequest pr WHERE ic.body LIKE '%@bors%retry%' AND i.id = ic.fk_issue AND i.is_pull_request AND ic.created_at &gt; NOW() - '7 days'::interval AND pr.repository = i.repository AND pr.number = i.number ORDER BY ic.created_at DESC I'm lazy, and would prefer to declare primary/foreign key equality in WHERE clauses instead of explicitly (inner?) joining. For single-table queries I really like being able to write diesel very similarly to how I write SQL, and I have no idea whether it's feasible, but it'd be cool to have options analogous to raw SQL when writing multi-table queries in diesel. Some like below would be awesome, but I know I'm not necessarily representative in how I write queries. select(( issue::repository, issue::number, issuecomment::id, issue::title, pullrequest::merged_at.is_not_null())) .filter(issuecomment::body.like("%@bors%retry%") .filter(issue::id.eq(issuecomment::table.fk_issue)) .filter(issue::is_pull_request) .filter(issuecomment::created_at.gt(interval!("7 days")) .filter(pullrequest::repository.eq(issue::table.repository)) .filter(pullrequest::number.eq(issue::table.number)) .order(issuecomment::created_at.desc()) .load(&amp;connection); I think at one point you showed me some hypothetical examples that used explicit inner joins I think another issue with hypothetical multi-table queries in diesel is that I'd guess it'd be really easy to have name collisions in the `schema::table::dsl::*` imports. I know you can `use foo as bar`, but that's a lot of boilerplate to write and makes glob imports of the DSL worse...not sure what a good solution would be. Anyways, I've been going on too long as it is!
Also mathematicians, sometimes. mumble mumble dual space mumble mumble.
"near" 0 downtime in four months is not impressive though. I've had multiple years actual 0 downtime on both Python and Java servers. I love Rust as a language but a language won't make your uptime good unless you make it good.
I remember getting frustrated when I was trying to add something to an empty list, and it was complaining that `empty` was of type `list[Any]`, and not of type `list[Num]` or whatever. That's around when I stopped poking it.
The downtime occured due to the hosting provider, not the service itself. I'm also surprised to hear you've left a Java server running for years. I've experience many problems with the kernel OOM killing java services after leaving them running too long
I built it with iron currently, though I feel as though its falling behind lately
OOM? Java VM downtime in my experience has been to pointer leaks or bad gc tuning (which has been a major pain to get right). It did take more than a few iterations and some object reuse to get that server completely stable (on JDK 1.5) Edit: ah, the out of memory killer, yes - it takes careful Java to not grow, and it might take more than its worth. But at 100-200 rps we still only grew with a few hundred bytes a day once tuned, which allows for several years uptime (I think the longest we went before a upgrade was just above two years). I'm the end we switched to Python and predictability. 
On another machine I have an in-progress multiple sub-query multiple table query I was playing with that I could pass along if you're trying to break things. I'll link it in gitter at some point when I'm back to that computer. Re: globs, I guess I could also do a module alias when importing the table the same way I do `FROM table t ...` in SQL. Haven't tried that, but I should.
rustc_serialize has been superseded by serde. build.rs hacks aren't too bad, considering they're a temporary measure – sooner or later, procedural macros will be available on stable.
Great stuff. We've had similar experiences in Rust. crates.io was hacked together rather quickly way back when writing a web service in Rust was barely feasible and /u/acrichto often remarks that it just works and points to graphs similar to the ones in this post. There are a few other pieces of Rust infrastructure written in Rust (parts of crater, rusty-dash.com, the new play.rlo at least) and I think it gives everyone a lot of confidence in them. I feel like when I write simple web services in Rust I don't have to think very hard - just hack it up until it seems to be working, then it probably is working; deploy it and forget about it. When I've worked in node.js it's been difficult to know whether I've handled errors in complex code paths correctly, and I end up doing a lot more thinking and testing up front. 
Please do! I've been out of commission for the last week or so, but I will definitely see it eventually if it's in gitter.
I'm getting an error regarding libc when I try to do this in src/libstd: https://ghostbin.com/paste/azcju
Neither. y is **down**, z is **forward**!
As I see it, this is kind of backwards. Having the same internal representation isn't the same as having compatible semantics, and only the latter is necessary for interoperability. Semantics are encoded in the (unwritten) laws for a type or trait: Add and Sub should be essentially opposites; the operation from Neg should be involutive; Eq implementations should be transitive (if a==b = true and b==c = true, then a==c = true), reflexive (a==a = true), and symmetric (a==b = b==a). All crates that agree on trait semantics can be used together, and their generics Just Work. But if you have two data structures with the same definition (e.g. String and Vec&lt;u8&gt;), there's no reason to assume they're interchangeable or interoperable. Implementation details are just details. What we should have is a set of crates defining common, agreed-upon traits (for example, numeric hierarchies), and have other crates implementing those traits on types they define. At boundaries between crates with commensurate but differing types, the newcomer crate writes Into/From instances. The intuition here is that what really matters with a value is not what's inside it (implementation) but what you can do with it (interface). This is the entire idea behind building abstractions. Sharing implementations can buy us *efficiency*, and we should do it where we can, based on what has been written before, but defining least-common-denominator or catch-all datatypes in a separate crate (the two options we have, since platonic ideal definitions *don't exist*) doesn't help. The former will be incapable of some desired operations and the latter will be inefficient for all uses. Incidentally, if you're defining a homogeneous type (i.e., tuples of N values of type T, as you might see in vector libraries or colorspace libraries), please do it as an array, and write named accessor functions. Someone *will* eventually want to iterate over those values (even if they just want to map over them all), and there's no reason to complicate that or force them to unroll that loop.
Wrong subreddit. You may want to post this here /r/playrust
Source: https://github.com/maciejhirsz/json-rust/blob/benches/benches/log.rs Which is mostly a copy of: https://github.com/dtolnay/serde-json/blob/2acc39d95a228c41a77183adfc0c6c146851c29d/json_tests/benches/bench_log.rs With the extra addition of serialization benchmarks.
As I said, dynamic typing makes Elixir harder to refactor, but if you follow the philosophy for error-handling (which you should), it really doesn't introduce many drawbacks, and genuinely has some benefits. A lot of things are well-documented type-wise, and the fact is that Elixir/Erlang has an extremely basic and small range of possible types in the first place. I'd recommend trying it out, depending on the type of project you want to build. I started using Elixir after primarily using Haskell for the previous 8-9 months (using Yesod). Trust me, the jump to dynamic typing was a big one, but The Erlang Way is sufficiently different from any other paradigm that you might just be surprised at how well it all works together. You literally can code "the happy path" to great success in these languages; it is usually non-idiomatic to have explicit error-handling basically anywhere. And as mentioned, if you do want to exercise discipline and get rid of a certain class of unit-testing, the Dialyzer tool can do static analysis using success-typing. No, it's not innate; but it exists and it works quite well.
The compile-time safety is nice, but the amount of overhead to get there can be extremely high, especially if you're newer to the language. Lazy evaluation also tends to hurt more than help. I'd recommend Yesod to people genuinely interested in learning Haskell. I wouldn't recommend it just because it has very high-level static typing guarantees though.
The CoC is making my code run slower than the equivalent Go, have you considered adding higher-kinded types to fix that?
unique and shared ptrs make c++ just as safe as rust but without the borrow checker headaches.
nim
How can I improve my framerate in this game?
No memes, please.
I like how serde is actually faster when parsing to a struct, while rustc is slower (since it has to go through the intermediate map).
&gt; “Within this method/module/crate, use wrapping operations” The problem with this is that within a given scope, you sometimes need to use wrapping operations *and* non-wrapping operations. In fact, I think that's usually the case. For that reason, and also for simplicity's sake (no compiler plugins to install, nothing new to learn), I'd rather just use `Wrapping` when necessary.
In fact, most of the crypto in *ring* is done in assembly language to (1) avoid timing side channels, and (2) for performance. Most of the parts that are done in C will likely be replaced with Rust code or assembly language code soon. For example, I'm replacing most of the ECDH and ECDSA code with Rust + asm code now. I'd love to get help with replacing the rest of the C code with Rust code. Someone already landed a PR for replacing the ChaCha20-Poly1305 C code with Rust, and another person replaced the high-level parts of RSA signing with Rust, and (IIUC) neither had any crypto experience. I'm happy to help newcomers to cryptography with these kinds of tasks. For example, replacing `crypto/cipher/e_aes.c` and `crypto/modes/gcm.c` (except `gcm_init_4bit`/`gcm_gmult_4bit`/`gcm_ghash_4bit`) with Rust code would be awesome.
It is an io library. It allows you to create an eventloop that listens to one or more sockets https://github.com/carllerche/mio/blob/getting-started/doc/getting-started.md
what happens on overflow in debug vs release? (vanilla rust)
Absolutely! Overflower should augment, not replace the existing options, and even with overflower, you're free to use the specialized methods if you want a different behavior.
Ah, thank you :)
Might be cool to see how a naive JSON implementation works with pest: http://pastebin.com/cZmJtxmU
You could check how gtk-rs does this. They have features you can enable/disable based on the GTK version you need.
I have to thank you for this. I rarely listen to podcasts like this; between ear problems and English being my third language I can often not make out what people say. This podcast, however, has great audio; you have very clear, measured pronunciation; and somehow you manage to make your interviewee clear enough to understand as well. I would still like a text transcript, but this is perhaps the first English language podcast I didn't just give up on within a few minutes. Thank you! 
Very intriguing work. I am reminded of the [Comparison in C++](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0100r1.html) proposal† which among other things suggests precise, named operations for relational and equality comparisons. I.e. providing `partial_less(a, b)` and `total_less(a, b)` where currently `a &lt; b` does both jobs, but it's not always obvious which during review. There's certainly a tension between using terse, mathematical-like notation and making it explicit which semantics are being used in the current context. The former is important because verbose or unwieldy syntax acts as a barrier to entry to programmers, while the latter matters especially in generic contexts. Your approach gives both, and I love that. †: I haven't followed the C++1y process closely enough to know whether this particular proposal has a good shot of being considered for inclusion
Thank you. Yes, this was my intention, to allow for the best possible ergonomics while making the operation semantics explicit.
Good to know. I'd love to use hyper. I'll have a look if I can add it.
What's wrong with the build.rs hack? It accomplishes the same thing as the nightly macros.
This is basically how I've been debugging the AVR compiler for libcore. Produce the LLVM IR file, reduce it, submit a bug report. Been very useful.
Historically, most people who participate in the higher-reward bug bounties don't do it for the cash. They do it for fun and the recognition is really nice for them. I would imagine this is somewhat similar - people will write these rust projects because they enjoy it, and having Mozilla send a bit of cash and appreciation would be a nice morale boost
Wow these are awesome! So many talks!
Well the ref-counting makes the gc entirety predictable sans cycles, so as long as you don't leak pointers it is/should be safe. 
It was at a lower level than that. The objects were being freed, no references, but the chunks of memory allocated weren't being freed because there was still some parts of them being used. Over time these allocated chunks built up (kind of like fragmentation in older OSs which you had to defrag occationally), and while the process was actually only utilising 10% memory, the allocated memory was pushing the max and it would crash the process.
Will there be a livestream + video recordings?
From a [previous thread](https://www.reddit.com/r/rust/comments/4lwt69/ann_rust_belt_rust_conference_speaker_lineup/d3rmtt9): &gt; The 2nd day talks will be recorded, the first day workshops won't be since they're likely to be more interactive :) AFAIK, there are no plans for a livestream. If you or a company you know would like to sponsor something like that, I believe we'd be open to discussing the details. 
I think that some of the most common complaints were path dependent types and the 3 kinds of implicits: http://lambda-the-ultimate.org/node/4575 subtyping. polymorphism and higher kinded types were difficult combinations, Odersky wrote at least a few papers as well: https://web.archive.org/web/20110805160730/http://research.microsoft.com/en-us/um/people/akenn/generics/index.html
Wow - thanks for the post! I'm not sure the library is quite ready for use though - there's still some weird bugs caused by ncurses. I was planning on moving to termion, but it still lacks some important features, with input for instance. But even though cursive itself has received little love lately, I still haven't forgotten about it and plan on keeping it alive!
I was actually looking for something like this a while ago, and I haven't found anything very enticing even outside the Rust community. The design of your library sounds great, though! Can't wait to see it grow.
 mod foo { ... }
&gt; Here's an example. Say a Vector2f is simply a struct Vector2f(pub f32, pub f32);, that's it, nothing more. Could you use tuples like `(f32, f32)` for this? They’re already built into the language.
Reminds me that I still need ~~an excuse~~ _a use case_ to play with LALRPOP. Niko's last article on it (["Nice Errors in LALRPOP"](http://smallcultfollowing.com/babysteps/blog/2016/03/02/nice-errors-in-lalrpop/)) sounded pretty great.
That worked, thank you!
I quickly patched a mysterious issue with the vertical line in boxes (ncurses clearly deserves its name), and it's now at least usable. I published a version to crates.io as well, which makes it a bit easier to use.
It's usally an antipattern to take owned values as arguments, when you only read from them. So, instead taking `keys: Vec&lt;&amp;str&gt;`, take `&amp;[&amp;str]` (not `&amp;Vec&lt;&amp;str&gt;`, because references to vecs are kind of pointless). That change also allows you to simply write `&amp;keys[1..]` to very quickly create a shortened subslice. Returning `Result&lt;&amp;'a Value, String&gt;` is a better idea – it should be up to the caller to decide whether to `clone` or not. Also, you could rewrite `test` without using `clone`! Also, it's kind of pointless in calling `as_array()` when you've already pattern-matched. You do that in the `test`, so you know how to do that, so maybe that's just a copy-paste bug. [Here's the full code](https://gist.github.com/krdln/9980d67d607263aa0d3cfeaba862fbf6) with all the mentioned fixes (slightly changed, so I could compile it as a standalone crate). --- You may want to have a look at the [`split_first`](https://static.rust-lang.org/doc/master/std/primitive.slice.html#method.split_first) method. It covers the "pop front" and checking `len() == 0` in one step. The slight disadvantage of using that would be a rightwards drift or restructuring your code [like that](https://gist.github.com/krdln/0af34d21ad0d50f15d5f21ce8009c197), which looks quite weird ([let-else RFC](https://github.com/rust-lang/rfcs/pull/1303) unfortunately wasn't accepted). You may also want to have a look [how's the `Value::find_path` implemented](https://serde-rs.github.io/json/src/serde_json/src/value.rs.html#110-122). It's iterative, though. --- If you want to make your error handling more idiomatic, it would be better to return a custom struct, which impls `Error`. See [this advice](https://doc.rust-lang.org/book/error-handling.html#advice-for-library-writers) and the chapter in general. If you want to stay with strings, you can consider [`Cow`](https://static.rust-lang.org/doc/master/std/borrow/enum.Cow.html)`&lt;&amp;'static str&gt;`, so your "not found" errors are not so costly. This may be a premature optimisation though. --- Oh, and raw strings are nice for creating jsons! Example: r#"["test",{"name":"slappy"},2,3]"#
While I think the point they were trying to make is that you can't **concurrently** modify the elements in a `RwLock&lt;Vec&lt;T&gt;&gt;`, the rest of the paragraph goes on to talk about not being able to modify the elements at all, which is indeed incorrect.
Great feedback, thank you! This is exactly what I was looking for. &gt; Also, it's kind of pointless in calling as_array() when you've already pattern-matched. You do that in the test, so you know how to do that, so maybe that's just a copy-paste bug. Right, duh, I didn't even think of that. Makes sense. I also didn't know you could do `match *call() { ... }`. That's really handy because I tried that, I had to do `&amp;`s before each match. Is the \* what gets rid of the need to clone as well? Thanks again.
This depends on what OS / distro you're running: if you're on Linux, just search for llvm using your distro's package manager, and install the appropriate packages. If you're on Ubuntu, you may have to make a symbolic link from `/usr/bin/llvm-config` to `/usr/bin/llvm-config...` as it installs versioned binaries.
Do you happen to know what packages I should install for Ubuntu? There are a lot of packages named `llvm` or something similar, and I see a lot of instructions online that suggest adding some repository or another.
Thanks for the writeup. It's a bit over my head yet, but I'm excited about it. With the news this week that Google is training so many people in ML it seems like this is something everybody's going to have to understand. We might in the future be expected to put little sprinkles of ML into everything we write, so I'm happy we're getting some approachable Rust projects in this space. I took the liberty of cross-posting this to /r/MachineLearning.
`cargo rustc -- --emit=llvm-bc` will let you emit the entire cargo project to a bc file.
Just a random comment, impl else thing looks like pattern-matching ~~at compile time~~ for trait dispatch.
Watched the first part, and so far I am not impressed. Regexp matching using the 'derivative' is precisely naive parsing the input by walking the syntax tree of the regexp. The only way that will ever produce output in a reasonable time is by using memoization aka throwing quite a bit of memory at it. The approach probably has its uses, but fast, efficient regexp parsers have been around for a long time now (e.g. in emacs and vi, the two editors he lambasts), so I'm failing to see anything useful. Am I missing something?
You probably want llvm-dev: http://packages.ubuntu.com/xenial/llvm-dev
One note: differentiating public vs private modules is a nice idea, doing it by using colors unfortunately is not. I'm slightly color-blind, and it took me a while to notice there were two different colors used... at first I was just wondering what this "public modules" and "private modules" headlines were...
/u/wezm is right (though you might get away with just installing `llvm`), and you might also need to install `libz-dev` and `libedit-dev` - I was trying to install llvm-sys and llvm-alt on Ubuntu and got a cryptic error when building, which turned out to be because of those dependencies being missing.
That is indeed a very fair point. Thanks matthieum! Being the initial release of `cargo-modules` it has quite a bunch of [rough edges](https://github.com/regexident/cargo-modules/issues/1) still. That should not be an excuse though. So let's take the opportunity to make this work for you. I opened a dedicated issue for improving the plain mode and would hence kindly ask anyone affected by this shortcoming to leave a quick comment/vote: https://github.com/regexident/cargo-modules/issues/2
Is `foo` always the same object? You could pass `&amp;mut foo` to your closures as an argument, and give it to `init` to pass to each one in turn. I think this will be the start of a solution in any case.
Hello, haven't done much Rust in past years so I've just thrown together simple CLI weather tool to see what Rust looks like nowadays. Would be great to hear what can I improve code-wise(more ideomatic/efficient/succinct), PRs are welcome! Currently Wunderground is the only supported weather source and forecast types are limited but that's what I use on daily basis.
Those are good points, I should have mentioned the lack of atomic operations. I updated the article accordingly. Thank you for the comment.
Parsing with Derivatives extends Janusz Brzozowski's derivatives of regular languages and extends them to the recognition of context free grammars. The value of Brzozowski's (and that of Antimirov, &amp;c) is that you can easily construct a DFA or NFA from a regexp for relatively speedy matching. The main value of Might's work is to show that compared to say, GLL or GLR parsers, it's possible to have a (IIRC) O(Kn) parsing complexity with a relatively simple algorithm. The downside of the original paper is that there's a significant constant factor in the size of the grammar. The subsequent paper apparently shows provides some quite significant performance improvements, but I've not read it yet.
Wow, this looks impressing. I just browsed the documentation and what I was looking for was [this](https://gyscos.github.io/Cursive/cursive/view/index.html) - am I right that these are components one can use to compose the UI instead of having to program everything on your own? I really want to play with this right now. This library _could_ be the library that powers an [imag](https://github.com/matthiasbeyer/imag) terminal UI (though this may be in the far future... but who knows...)
O(n) parsing complexity? I must have missed that. Isn't there a proof that arbitrary grammars can't be parsed faster than matrix multiplication, i.e. O(n\^2.something)? I've found [a quote](https://news.ycombinator.com/item?id=11976769) that says parsing with derivatives complexity is cubic, and Matt Might seems to confirm it, which makes sense, as it's as good as identical to dynamic programming methods applied to parsing. Perhaps you're referring to memory complexity? I think the general case requires O(n\^2) if you want to return the entire parse forest.
My bad, I forgot to add `Arc` in those definitions. You are correct: once you get a mutable reference to the vector, you should be able to mutate both the vector and its values, except if it's a vector of `Arc` references (because they coerce to immutable references but not mutable ones). I meant concurrently mutate the values, like adudney mentions below. In short, the first form allows us to mutate all elements of the vector in parallel if we wish to do so, while the second allows only one thread to modify the vector (and its values), leaving other threads waiting for the lock.
Not having the operators defined is a major drawback though. Especially since a different add function is needed for each because size
A Menubar would be nice (not check whether there is already such a thing, though).
What does ANN stand for? I thought it stood for artificial neural network; but it turns out that's not the case :) 
I feel like this will have major coherence issues :/ I'm not well-versed enough in the design details of the orphan rule to be able to tell you what they are though, and if it can be overcome. I do recall thinking about "negative impls" in the past and realizing that they have coherence issues that I wasn't able to come up with a way of solving in a stable fashion.
Maybe Currosive. Easier to Google and looks like a portmanteau of "curses" and "corrosive".
I installed `llvm-dev`, but I've run into two issues: 1. I can't figure out why, but `llvm-sys` can't parse my version from `llvm-config --version`. When I run `cargo build`, I get "Could not determine LLVM version from llvm-config." from the `llvm-sys` build script. 2. I seem to be running an old version of LLVM, because `llvm-config --version` outputs `3.4`. Maybe it's because I'm running Ubuntu 14.04 (Trusty), instead of Xenial? But I can't upgrade my Ubuntu version right now.
There is actually prior art for this approach, proposed under the name "instance chains" for Haskell: [paper](http://web.cecs.pdx.edu/~mpj/pubs/instancechains.pdf), [paper](http://homepages.inf.ed.ac.uk/jmorri14/d/final.pdf), [GHC issue tracker](https://ghc.haskell.org/trac/ghc/ticket/9334). I don't know if it has ever been implemented in a real-world compiler. (GHC ended up adding closed type families instead, which are somewhat similar.) But research papers are nothing to sneeze at, and you can probably borrow most of the detailed design for it directly. Some specific thoughts that occurred to me (I think I read the paper a really long time ago, but didn't do so right now, just skimmed the GHC discussion): * At the time this was essentially proposed for GHC as an alternative to `OverlappingInstances`, which everyone loves to hate. Which, interestingly, is very very close to Rust's "impl specialization". So many of the same tradeoffs might apply. (Although specialization seems to be different in the respect that the *primary* use case is optimization, which makes it more of a counterpoint to GHC's rewrite rules, and behavior-changing specialization is just packaged along for the ride, IMHO problematically. Whereas `OverlappingInstances` is used almost exclusively for the latter.) * The `fails` instances seem quite similar to the "negative impls" that have been proposed for Rust! So, translated, you'd have something like impl&lt;T: Foo&gt; Trait for Type&lt;T&gt; { ... } else&lt;T: Bar&gt; Trait for Type&lt;T&gt; { ... } else !Trait for Type&lt;T&gt; * There's discussion of GHC not having backtracking, meaning that instances are selected solely based on the `instance` head (the part immediately before and after the `for` in `impl&lt;...&gt; ... for ...` in Rust, *not* including any trait bounds in the `impl&lt;...&gt;` or any `where` clauses). Whereas I believe Rust *does* take `where` clauses on `impl`s into consideration when selecting. But does Rust's impl selection algorithm use backtracking? (I actually don't know!) Are these things in 1 &lt;-&gt; 1 correspondence?
I'm putting together a Lisp interpreter, a la [Build Your Own Lisp](http://buildyourownlisp.com), but I'm having trouble implementing `std::convert::From`. Compiling this code gives an unsatisfied trait bound error (E0277) (as `bool` doesn't implement `Into&lt;String&gt;`), but uncommenting the `From&lt;bool&gt;` impl gives a conflicting impl error (E0119). Is there a way around this? [Playpen link](https://is.gd/RYiIaW) #[derive(Debug)] pub enum Atom { Bool(bool), Int(i32), Float(f32), Str(String), } // impl From&lt;bool&gt; for Atom { // fn from(x: bool) -&gt; Self { // Atom::Bool(x) // } // } impl&lt;S: Into&lt;String&gt; &gt; From&lt;S&gt; for Atom { fn from(x: S) -&gt; Self { Atom::Str(x.into()) } } fn main() { println!("{:?}", Atom::from(true)); }
I think this is because nothing guarantees that `bool: !Into&lt;String&gt;` for any distant future. However you can simply drop the `Into` bound, use `From&lt;String&gt;` directly and let callers do the required conversions.
I did a bit a searching, but I haven't found a good discussion about what type coherence is. Anyone have a summary?
I guess this was a bit of premature optimization on my part. Thanks!
Looks exactly like what I thought of!
Hm. Ok, that makes sense. That's how I expected the routing to work by default. I guess that explains why I didn't get the point of `mount`. :p
I get the impression it was posted more to inform and inspire potential developers because of points of commonality with the developing Rust ecosystem such as: * Define a schema which can be verified as much as possible at compile time, rather than error-prone hand-written parsing code * The parser should be (or at least feel like) a library, not a separate tool * The parser should be at least as performant as hand-written code (zero-cost abstraction) * Comprehensible failure messages are important (eg. When borrowck rejects something, it does so in a helpful way) To me, all those points feel like defining points of how rustc, serde, Diesel, hyper, and various other such parts of the ecosystem differ from what other languages tend to offer. Also, the "we need parser generators compatible with non-blocking I/O" feels like encouragement to work on RFCs for things like continuations which could streamline implementing such parsers. I know it's inspired me (a less infrastructure-oriented developer whose main interest in Rust is minimizing the number of unit tests I have to write to enforce a certain level of code quality) to increase the priority of trying out things like [nom](https://github.com/Geal/nom).
For those that missed it, like me: http://web.archive.org/web/20160401180706/http://www.redox-os.org/
Thanks a lot! This is exactly the kind of feedback I was looking for.
`Foo` is a public API, `RefCell` - a crutch that will need to use by user. Is it possible to give `Foo` the same guarantees offered by `RefCell`?
Even if not ready, it looks really nice! You *might* have gained a future user, depending on when/if I start some of my "maybe-one-day"-projects.
I also found this talk very interesting and insightful. (And now I feel the urge to write a derivative parser in Rust ;-)) I have to admit that I also used to do parsing by hand (use split and map, etc.) instead of using a real parser. Luckily for Rust there are a couple of good parser crates available: [nom](https://github.com/Geal/nom), [combine](https://github.com/Marwes/combine), [pest](https://github.com/dragostis/pest), [LALRPOP](https://github.com/nikomatsakis/lalrpop), [chomp](https://github.com/m4rw3r/chomp), [parsell](https://github.com/asajeffrey/parsell), ... just to name a few. When I have more time I'll try to compare and benchmark all these crate as I did with the parallelization crates ([mandel-rust](https://github.com/willi-kappler/mandel-rust)). 
I absolutely agree. The Rust ecosystem has evolved amazingly and continues to do so. It looks like a very nice and birght future for Rust developers.
Agh, I messed up my notes. You're absolutely correct, it's `Rc` that's both `!Sync` and `!Send`. Thank you for setting me straight.
I have not that much time this week as the exams are coming up in this and the following two weeks. Anyways, I managed to [write a blog article on what's the purpose of imag, my personal information management suite for the commandline](http://beyermatthias.de/blog/2016/06/21/the-purpose-of-imag/). I hope I will be able to get some cool stuff merged in [imag](https://github.com/matthiasbeyer/imag) and maybe I can get some code into the PR for the bookmark module. I also want to contribute to [cursive](https://github.com/gyscos/Cursive), but I'm not sure whether I can do this because of the lack of time. I am really looking forward to september, when I will have a full month time for hacking on imag and other projects.
I am currently using Rust to implement a physically based raytracer with the help of [pbrt](http://www.pbrt.org/). Just to understand the concepts for physically based rendering.
I've never posted in these threads before. I started with Rust about 6-7 weeks ago. I've got a number of things that I'm working on as I'm basically working on various components that I need for some much larger projects: * [disassemble.rs](https://github.com/endoli/disassemble.rs/) - Tools for recreating basic blocks and soon loops and such from disassembled code. * [message-format.rs](https://github.com/endoli/message-format.rs) - A good start on implementing ICU message formats. (I could use some help finishing up some parsing using nom to construct plural and select formats.) * [lldb-sys.rs](https://github.com/endoli/lldb-sys.rs/) - Low level / raw / unsafe bindings for the LLDB API. * [lldb.rs](https://github.com/endoli/lldb.rs/) - An early start on higher level, safe bindings for the LLDB API. I have some other things that I'm working on ... like a markup library for handling document ASTs as I don't really care about the surface syntax ... I'm figuring out how to start implementing a computer algebra system ... and I've got interests in a whole bunch of other things. I welcome help or comments on any of the above!
Not in Rust, but it's very much related: I gave `atom-beautify` a nice little [update](https://github.com/Glavin001/atom-beautify/pull/975) for `rustfmt` 0.5, which should be published soon. :) It's not just much faster than before, but now it also allows you to correctly beautify `lib.rs` and `mod.rs` files (i.e. those containing `mod xyz;` references). As soon as the update is out, I can only recommend you to finally switch on beautify-on-save. For those who can't wait: You can copy [this](https://github.com/Glavin001/atom-beautify/blob/master/src/beautifiers/beautifier.coffee) and [that](https://github.com/Glavin001/atom-beautify/blob/master/src/beautifiers/rustfmt.coffee) file into your `$HOME/.atom/packages/atom-beautify/src/beautifiers` folder. Atom will pick it up on the next restart.
Yaml is also indentation based 
Now I feel urgent need for the `?` syntax construct.
I've been working on an xscreensaver [replacement](https://github.com/meh/screenruster) because I wanted to make my own fancy savers and the xscreensaver APIs are limited for what I want to do. The design is similar to the one from xscreensaver where the savers are actually in another process, but instead of just having a one shot process spawn where everything is passed as arguments it uses JSON to communicate with the spawned process using its stdin/stdout and using stderr to print logging and whatnot (which fits perfectly with env_logger). The Rust API to implement savers uses glium and a trait to not have to reimplement the rendering loop and communication with the master process. The locking/unlocking part is still missing because I'm focusing on the stuff around it for now, since adding it is just a matter of hooking up PAM. 
I don't think this suffers from the same coherence issues that negative impls do. If you look at `impl else` blocks as a bunch of independent `impl`s, none of them on their own suffer from the coherence issues. Which means the only place coherence issues could lurk is in the block. But since each `impl` has coherence: * a type which matches the first `impl` in the chain will always match that `impl` * a type which doesn't will always not and will thus try the rest of the chain
Trying to squeeze more performance from [json-rust](https://github.com/maciejhirsz/json-rust) became a fun game. The API is pretty stable by now, and there are no major bugs I'd be aware of, so I can spend time polishing things out.
Thanks!
I think getting some inspiration from npyscreen would be a good idea. 
Why? There isn't a single try! in the example.
The general rule is to use the least powerful tool for the job. `&amp;T` is less powerful than `&amp;mut T`, which is less powerful than `T`. Conversely, `&amp;T` can be used in more contexts than `&amp;mut T`, which can be used in more contexts than `T`. If you use the least powerful parameter type that allows you to do what you need to, it'll be easier to use your API. For read-only access (or anything you can do without a unique reference or ownership) to a non-Copy type, **always** borrow. It makes things simple at the call site, makes no difference in the function body thanks to auto-deref, and is usually the best performance. For mutable access, you have a choice between mutable borrowing or taking ownership. A function like fn improve(s: &amp;mut String) { s.push_str(", motherfucker"); } and fn improve(mut s: String) -&gt; String { s.push_str(", motherfucker"); s } can be called in most of the same contexts, and do the same thing with the string they're working with. The first version is more general, since the second version requires that the caller owns the value, while the first version can be called by either an owner or another mutable borrower. Because of that, I suggest the first form when possible. Taking ownership is more general, since it lets you transform the type (e.g. `fn something(s: String) -&gt; Cow&lt;'static, str&gt;`. There are some nice APIs that are based around stuff like `fn open(f: ClosedFile) -&gt; OpenedFile` to prevent you from trying to perform invalid operations like reading from a closed file. It can also be useful for performance, if you force your API's consumer to handle copying and allocating.
Thanks! I thought that by having "my" type inside, Node, that Rust would consider Option&lt;Box&lt;Node&lt;T&gt;&gt;&gt; as "belonging" to me and would let I implement external traits on it. :-p
Yeah, that makes sense I guess.
There is a lot of `unwrap`, however.
"type" creates an alias, and an alias alone: they're considered the same type for everything else than what you can write in the source code. An easy mistake to make, though, and it's why I don't find it to be all that useful, personally.
If UDP is okay, I contribute to [coap-rs](https://github.com/Covertness/coap-rs), which uses mio as the core.
&gt; you probably want to dedicate a thread to IO (mio) and dispatch your logic to a thread pool for processing. This is what Dropbox's mio-based I/O stack does. We saturate 10Gbps NICs routinely, and get 25Gbps in + 25Gbps out or so on 40Gbps NICs (using multiple I/O threads, granted).
&gt; Imagine a function barify(f: Foo) -&gt; Bar that takes a Foo, messes with its data (e.g., replace each occurrence of "foo" with "bar" somehow) and then returns a new type of data, Bar. Usually (or at least often) you'd want to take a borrow, there, and copy things over. You only consume things if you *don't* copy, that is, `Bar` refers to things in `Foo` in a way that makes still accessing `Foo` after you're done in some way unsafe, or at least semantically bogus. That generally requires access to `Foo`s internals: Tear it apart just as a destructor would, but re-use parts of it to create `Bar`. Really, you can think of such functions just like C's `free` function... with a more useful return type. More generally speaking: You take ownership exactly in those cases where after you, noone must be allowed to access the thing.
According to some benchmarks, in C++ it is faster to just copy small data instead of taking it by ref. For example a `fn dot(self, v: Vector3) -&gt; T {}`, I assume the same is true for Rust? 
This is exciting! Great documentation has got me out of a few complex situations in Rust.
I need to read an UTF-8 string of known size from binary stream. Right now I'm doing: let length = try!(input.read_u16::&lt;BigEndian&gt;()) as usize; let mut bytes = vec![0u8; length]; try!(input.read_exact(&amp;mut bytes[..])); try!(String::from_utf8(bytes)) 1. As far as I understand, there is only one heap allocation (inside vec!). String::from_utf8 will take ownership of bytes and will not do any copying. Is that right? 2. I don't need the string to be mutable. Should I be using str instead? 3. Is there a way to avoid zeroing bytes? It will be overwritten anyway. Or, should I trust optimizer to take care of that?
Yep. And also, session types are awesome.
&gt; String::from_utf8 will take ownership of bytes and will not do any copying. Is that right? http://doc.rust-lang.org/stable/collections/string/struct.String.html#method.from_utf8 &gt; This method will take care to not copy the vector, for efficiency's sake. (So, yes) &gt; I don't need the string to be mutable. Should I be using str instead? It's not clear to me what you're asking about... `bytes` is indeed mutable, and you need to mutate it. &gt; Or, should I trust optimizer to take care of that? I would argue more that you should profile first, then work on improving speed. No sense in worrying about something that's not a problem in practice.
What, if there is any, is the best way to use Cargo's build in bencher (using the test crate and #[bench] I mean) with a custom number of iterations?
The biggest troubles I have had, are when to put the module in it's own folder with a mod.rs file, or when to just add a file at the root of src. Other than that it feels more natural than I expected! Thanks for reaching out! I might add to the PR if I can think of something after work that is less boring if you would like.
Maybe setting up a more realistic or complicated example would be useful? Stuff like: * Importing external crates at a library top level and then using them in modules * Organising multiple binaries What might be nice is to take a dive through the organisation of some real open source crate, or part of the std lib? Contrived examples, even though they're shorter, are sometimes more confusing than real ones (ie the organisation of a json parsing library or something) Also, it seems like stdlib does some funkiness with modules in the networking stack to achieve platform independence with the net_imp stuff. Would love to see a clear explanation of how that works.
Rust largely employs the same LLVM passes as Clang, so in general it's safe to assume that any optimization details that apply to Clang also apply to Rust. A more comprehensive answer would require an explanation of why passing a Vector3 would be faster than passing a &amp;Vector3. For types smaller than the platform size the answer is simple, but if the reason is to avoid indirections then the answer is obviously going to be more complicated than simply the choice of how the parameter is passed.
I'm ready to see the Servo technical preview!
Not strictly related to Rust, but still interesting.
So there's actually a reasonable reason that explains why this self.w8(addr, self.dec8(self.r8(addr)); is actually different from this: let x = self.r8(addr); let y = self.dec8(x); self.w8(addr, y); If we have a type `T`, and a method `method`, that takes `&amp;mut self, arg1: U, arg2: V` as its arguments, and we have an instance of `T`, let's call it `some_t`, then a call such as `some_t.method(arg1, arg2)` is *actually* sugar for `T::method(&amp;mut *some_t, arg1, arg2)`. That is, `some_t` is evaluated *before* `arg1` and `arg2`, because in Rust we evaluate arguments left to right. So if `arg1` turns out to be `some_t.some_method()`, then this can become a problem. This becomes observable if `some_t` is actually inside a smart pointer that performs some observable action when `Deref` or `DerefMut` is triggered. So in your case the first statement is actually equivalent to `T::w8(&amp;mut *self, T::dec8(&amp;mut *self, T::r8(&amp;mut *self)))`, and *not* to your three line version. :) Edit: Oh hi Steve, I guess you aren't the one who wrote this blog post, so s/you/Andre/g ... More edit: [this](https://play.rust-lang.org/?gist=1bce391582a5220412d726a4188ff5e5&amp;version=stable&amp;backtrace=0) playpen sort-of illustrates the evaluation order.
[removed]
I'd add to this the fact that `use` is _always_ relative to the crate root unless `self` (or `super`) is used, while other uses are relative to the current module unless there's a `::` prefix. Before I understood that, I was confused why `std::cmp::min` would sometimes work and sometimes not, and the seemingly random `use` declarations I needed to sprinkle in every time I started structuring my code in modules.
Thank you for this explanation. I have run across this and always just dealt with it as a "quirk" of rust code. Knowing what is going on really helps my understanding. 
Nice! I do want to repost a comment by @pornel: &gt; Silly question. The RFC says: &gt; &gt; you cannot write something like &gt; &gt; fn produce_iter_static() -&gt; Iterator&lt;u8&gt; &gt; &gt; Why not? In languages with inheritance or interfaces this syntax just works, so as a newcommer to Rust I'd expect that to work. My thinking is that trait is an abstract thing, so you can't actually return a trait, so a trait in return type position means returning something that is compatible/coerced to that type. As a rust beginner and reading through the rfc comments, it seems like there is a certain form of "tunnel vision" that requires more and more context, which feels like it obstructs a more simple and semantic design. 
This is interesting. Since a function could return various types which only need to impl some certain traits, I wonder how does compiler allocate memory
Since i'm lacking the proper knowledge, would it be possible if you could elaborate a bit more on the difference between `Box&lt;Trait&gt;` and `Box&lt;impl Trait&gt;`? In what situations would I want to use either one or the other?
It'd monomorphize the functions, and allocate memory needed for the concrete type. 
Any chance you want to merge any of those optimizations in serde_json? I saw you are beating be at serialization ;)
Newbie question: Why do they have different types?
My biggest struggle was figuring out how to call methods that were in sibling or cousin modules. 
The second one needs to store the value of `x`, while the first one doesn't store anything. That means they're different sizes, so they have to be different types. Rust makes all closures be different types, not just ones with different sizes, to help with efficient code generation.
Make your struct generic, or box the return value! It's like how you store closures which types are sort of like `impl Fn()`.
Perhaps I am becoming a functional programming zealot, but I found the iterator version more readable than the loop. I wouldn't be surprised if it was faster, too.
You could do fn make_closure(be_weird: bool) -&gt; impl Fn() -&gt; u32 { let x = 3; if be_weird { return || { let _ = x; 0 }; } else { return || x; } }
Those would be the same size (if the optimizer didn't get clever), but still different types because they're different closures. You would need to put the branch inside the closure to use the `impl Trait` feature for this.
With `Box&lt;Trait&gt;`, I can do this: struct A; struct B; trait Trait {} impl Trait for A {} impl Trait for B {} fn some_fn() -&gt; Box&lt;Trait&gt; { if coin_flip() { Box::new(A) } else { Box::new(B) } } [This will compile just fine](https://play.rust-lang.org/?gist=1635317b95154603253bd633f6a33b32&amp;version=stable&amp;backtrace=0). It would not work if it returned `Box&lt;impl Trait&gt;`, because the `impl Trait` part gets replaced by a specific implementation of `Trait`. In the example above, it would complain that I'm trying to return `Box&lt;A&gt;` and `Box&lt;B&gt;`. However, given a function that only returns one implementation of whatever trait you're messing with, `impl Trait` is faster.
A [Scheme interpreter][1] written in Rust. Any thoughts, suggestions, etc? [1]: https://github.com/DemiMarie/rusty_scheme
Boxing adds a runtime cost, and making your struct generic breaks encapsulation. You are able to know what the type is going to be, then why should your struct be generic? And then how do you justify the fact that if you want to store that struct in another struct, you'll have to make that other struct generic as well? If the RFC stays as is, they will have to add some sort of syntax that automatically resolves to the return type of a function (like `decltype` in C++), otherwise that feature will become cumbersome. 
Quoting from that SO answer: &gt; In itself, moving an object is still at least as expensive as passing by reference. However, in many cases a function will internally copy an object anyway — i.e. it will take ownership of the argument. &gt; In these situations we have the following (simplified) trade-off: &gt; 1. We can pass the object by reference, then copy internally. &gt; 2. We can pass the object by value. &gt; “Pass by value” still causes the object to be copied, unless the object is an rvalue. In the case of an rvalue, the object can be moved instead, so that the second case is suddenly no longer “copy, then move” but “move, then (potentially) move again”. But I'm afraid I don't what this is getting at. A move is a memcpy, so "copy, then move" has the same cost as "move, then move". There's no way to avoid this cost, either the data is copied into your stack frame or it's not. Unless C++ has some magic about inlining rvalues across function boundaries that I don't know about?
This worry is misplaced. There will be no impediment to implementing such a feature in the future, and in the meantime even the restricted form of this feature will be immediately useful.
I think the idea is that `fn f&lt;T: Trait&gt;(n: T)` could be shortened to `fn f(n: impl Trait)` in the future. I like the idea, but this makes three different ways to write the same thing (with `fn f&lt;T&gt;(n: T) where T: Trait`)
Same questions as with lifetime parameters. Inference will save us in most cases. I don't see why we are trying to hide the generics all of a sudden (see proponents of `impl Trait` in argument position, which is a redundant feature). `impl Trait` in return position plugs a hole in the type system and allows us to express functions that could not be written before. It needs expansion to fully plug the hole (usable in trait methods, etc), but I don't understand the enthusiasm for adding syntax that is 100% redundant to the current generics (except worse, because you can't name the type parameters -- just imagine the error messages).
Yeah, the syntactic sugar is cool, but it should definitely just be &lt;out T&gt; without the syntactic sugar instead of some weird hack it seems to be now and it should not behave any different to associated types.
All TWiR issues will now be published every Tuesday instead (so that we can link to latest issues of other "This Week in ..." series which are usually published every Monday).
I'm working on a [nsq.io](http://nsq.io/) client driver in Rust. I'm trying to migrate a little microservices monster project I have in Go to Rust.
It's been a topic for discussion, one of the contributors to serde_json is aware of what and how I'm doing that makes the serializer faster, so I reckon they will catch up soon :). There is even a possibility this could lead to speeding serializing integers with `format!` / `write!` in stdlib (which I'm not using, but serde is).
I found the modules chapter a bit confusing, too. Maybe it would be better to shorten it a lot. Since I'm still quite new to Rust – I could be wrong – but in the end the chapter boils down to this: * Modules are declared using the `mod` keyword. * A declared module can either be: - in a folder which has the same name as the first `mod` keyword parameter and a file called `mod.rs` in it, containing the code. - or in a file, named after the first `mod` keyword parameter with a `.rs` postfix. * To access a library, use: `extern crate [library_name]`. All those points need small examples. Also there is more to say about aliasing, `super` and `self`, but in the end that's it. 
Adding to my last comment, if those wrapping arithmetic methods bother the author, they can either switch overflow checks off on debug (either by switching off debug assertions completely or by an unstable compiler argument whose name I just can't remember) or use [overflower](https://github.com/llogiq/overflower) to declare their module as `#[overflow(wrap)]` (sorry, you'll need a nightly rustc for that).
Additionally, I come from a Java, Scala, C#, world, where one package equals one file. I think it would help me at least with some more examples for when you would do something other than that.
Yeah, talking organization before modules would make very good sense I think
Cool! When can we expect it to be in the stable version?
The playground is now implemented in Rust! Cool!
There's no way to answer this, but the shortest possible bound would be that it lands in nightly right now, then waits one whole six-week cycle, then is stabilized. So that'd be 1.13. But, given that the RFC already has some unresolved questions that we need time and experience with to answer, it's almost certainly going to take longer than one release cycle of being unstable, so...
When you declare a module in one file, you are expected to provide a different file by the right name in the right location with the contents of that module. I don't know if it's cargo or rustc that wants this, but it really threw me off in the beginning.
Such a simple rule yet so few follow it, it's so much easier to improve something simple (like that algorithm) than it is to fix (or even get working) something complex.
With Box&lt;impl Trait&gt;, the compiler and the user know that there is one specific structure which implements that trait, but the function just isn't saying what it is. With Box&lt;Trait&gt;, not only do you not know what is implementing the Trait, but the function could choose to return two different structures even with different sizes inside the box.
`impl Trait` in return position is just as redundant as `impl Trait` for struct members. You can eliminate any need for `impl Trait` by using enough code, and it will mean using generics like you're suggesting. But, much like it's a pretty big save to have `impl Trait` for return values, it's useful to have it for struct members too. Parameters typically hint to the user "hey, you can change this thing here and get a different result". It's very roundabout to provide a parameter if you're only ever expecting it to be one specific value, especially if your code is all written as though that's basically the only value it will ever take on. And it's annoying to communicate that properly to the type system. That's why `impl Trait` is such a big improvement for functions. And it offers similar improvements for structs, too, since the alternative is to thread generic parameters through layers and layers and layers of structs, even though they're not really generic and not really parameters, is a lot of unnecessary work and code cruft.
How large do you suggest the threadpool should be in a general crud application?
Edit: I definitely would like the "private type in public interface" error to be a warning. Maybe there's some horrible implementationy reason why it's actually an error, and can't be disabled. That's a little different, since it means naming the type explicitly. For some types, that's just complicated: fn double(i: i32) -&gt; i32 { 2 * i } fn range_to(i: i32) -&gt; impl Iterator&lt;Item=i32&gt; { 0 .. i } fn foo(i: i32) -&gt; impl Iterator&lt;Item=i32&gt; { range_to(i).map(double).flat_map(range_to) } For other types, it's basically impossible, since you can't write down a type associated with things like closures: fn foo(i: i32) -&gt; impl Iterator&lt;Item=i32&gt; { (0..i).map(|i| 2*i).flat_map(|i| 0..i) }
&gt; even the restricted form of this feature will be immediately useful Not to library developers. Or, alternatively, library developers could use that feature, but return types become less useful for library users. It is like global type inference. It could useful, but does more harm than good. (Edited)
If you have fn f() -&gt; impl Trait; fn g() -&gt; impl Trait; Then `f` and `g` may return different types. If you want a struct that can store either, it must be generic. If you only want to store one of them, how do you select it? Like this? struct S { t: returntypeof(f); } That seems.. odd. And - what's the alternative? Not have `impl Trait` at all?
Is there any use case to assign to a moved or an uninitialised struct ?
Rather than reply to everyone: thanks all! Great suggestions.
This is a fantastic heruistic, much simpler than the literal rules.
Last week I started learning Rust a bit more actively, by writing a simple language interpreter. I overshot the simple part a bit and wrote a [whitespace JIT compiler](https://github.com/CensoredUsername/whitespace-rs), which was a nice learning experience. It also taught me that x64 instruction encoding is hell. The writing of the JIT was pretty simple though. In the process of writing it I hit an annoying error in rustc (it was ignoring the clobbers passed in the asm!() entrypoint of the JIT, causing some fun bugs that only appeared in --release, and which disappeared if I tried println!() debugging), but outside of that it was a very nice experience. Currently I'm working on writing something like [dynasm](http://luajit.org/dynasm.html) as a compiler extension for rust, as my main annoyance with the JIT I wrote was that it the assembling part was relatively run-time heavy, and to keep it fast there wasn't a lot of error checking in it. As the choice of registers and operands is already known at compile time, the assembling could be replaced at compile time by just a series of Vec.push statements and a few bitwise operations at worst, which would both speed the process up and allow for more detailed error checking. It'd also be a nice tool for writing JITs in general.
How do I gracefully stop a HTTP server in hyper? I took a look at the documentation and there seems to be no mention of stopping.
I'll be writing a blog post about my experiences in creating this, and if anyone has any questions, please let me know and I'll attempt to address them! [A direct link to the playground](http://play.integer32.com/) for us lazy programmers.
There's a few things you can do. Function pointers are trivially copyable: fn takes_fn(f: fn()) -&gt; _ {} However, they cannot capture their environment (thus are not closures). You can place a `Copy` bound on your closure parameter: fn takes_closure&lt;F: FnOnce() + Copy&gt;(f: F) -&gt; _ {} This, of course, requires that all the captures of the given closure are copyable. You can change your closure trait bound to `Fn()` and put it in an `Rc`, then clone and move that into your derivative function: fn takes_closure&lt;F: Fn()&gt;(f: F) { let f = Rc::new(f); } Like the previous, except taking the closure by-reference: fn takes_closure&lt;'a, F: Fn() + 'a&gt;(f: &amp;'a F) -&gt; Box&lt;Fn() + 'a&gt; {} fn takes_closure&lt;'a&gt;(f: &amp;'a Fn() + 'a) -&gt; Box&lt;Fn() + 'a&gt; {}
Given that a new RFC is necessary to discuss the syntax... 6 to 8 weeks, as per Jeff Atwoods.
Excited to be mentioned! 
No questions, just a thanks for sharing and look forward to reading your post on the subject!
Wow. How about using crates, is this planned? Or done?
It also removes allocation, which is helpful.
None that couldn't be expressed by initializing or reinitializing the struct in my opinion.
How in any way does this do harm?
Is this linked to the official playground being rewritten in rust? (Mentionned in TWIR).
&gt; You are able to know what the type is going to be But you are not able to know what the type is going to be, which is why this feature is likened to generic output parameters. It's up to the callee, not the caller, to determine the type, and the callee is allowed to change the concrete type being returned without breaking the function contract (as long as any OIBITs don't change).
&gt; You are able to know what the type is going to be You = the compiler.
Can someone explain how this is relevant to Rust? Otherwise, this is quite off-topic, and will be removed.
Sorry for replying after all this time, but I just wanted to thank you for the ideas, I ended up implementing something really close to this. The functions are being stored in a global `RwLock&lt;Arc&lt;fn&gt;&gt;`, and after replacing the global arcs with arcs to the new library, the previous library arcs are tracked, and once all of them have a `strong_count` of 1 the old library is dropped.
By the way, in the iterator version, it is obvious the filter can be moved up (because only the squares of odd numbers are odd), and thus `(1..).step_by(2)` is another option.
At most /u/burntsushi who does Rust's regex crate does it with FSM which he talked about at the Boston Rust meetup not to long ago. Even that is a stretch. Otherwise I don't see a reason for it to be here as it isn't Rust specific nor does it compare a Haskell to a Rust implementation. This is a topic better suited for /r/programming or /r/haskell
if anything I would do it like that: fn f&lt;out T: Trait&gt;() -&gt; T; struct S { t: f::T; }
I liked this!
We try to be inclusive to all skills levels here (and it was not necessarily a lack of skill; I could have made the same omission or it could have become a problem when more data was added later), and we hope that Rust allows us to open browser development up to a wider audience. The only problem here, and a minor one, is why no one caught it in review. The next person came along and fixed it. This is the system working as it should.
Ah! I think I finally grok what you meant. I was thinking that generics were infectious anyway, because either you keep them generic or wrap them behind a polymorphic interface. But here you are complaining about the impossibility to use a generic function internally but expose a concrete type externally. I agree that this is the case. An alternative could *almost* be to create a type synonym or an actual concrete type for the internal function, and then wrap it (without exposing it) in a concrete struct in the public interface... except that it does not enable storing lambdas, because their types cannot be named. Another alternative to `typeof`/`decltype` could be to allow writing `struct X { value: impl Trait, }` (and therefore also allowing `impl Trait` in enums and such)... ... though personally, to be honest, I would prefer just naming the return type: fn justdoit&lt;out X&gt;() -&gt; X where X: Trait { } and then using the regular associated syntax to access the name: `justdoit::X`. After all, we have associated types and constants, we could have those on functions too... and even have special named ones for the arguments tuple/return type, and in the latter case: fn justdoit() -&gt; impl Trait { } and `justdoit::Self::Return` would work quite well. (in case `justdoit` is generic: `justdoit::&lt;X, Y&gt;::Self::Return`)
I mean, that will be entirely dependent on the work being executed on the thread pool. If you never block, probably start w/ the number of cores on the box.
What about this? https://is.gd/BhuFCO pub type InnerIter = SomeContainer::internal_function::I; // Alternatively: // pub use SomeContainer::internal_function::I as InnerIter; impl SomeContainer { pub fn iter(&amp;self) -&gt; Iter { Iter(self.internal_function()) } fn internal_function&lt;out I&gt;(&amp;self) -&gt; I where I: Iterator&lt;Item = u32&gt; { ... } } pub struct Iter(InnerIter); Note that you wouldn't even need to wrap it this way. The typedef would suffice.
Yes, that's why I'm saying: &gt; If the RFC stays as is, they will have to add some sort of syntax that automatically resolves to the return type of a function (like decltype in C++), otherwise that feature will become cumbersome.
How is it redundant in return position? I guess because you can use a trait and an associated type instead? For structs I was thinking of code like this: fn map&lt;I: Iterator, F: Fn(I::Item) -&gt; O, O&gt;(it: I, f: F) -&gt; impl Iterator&lt;Item=O&gt; { /* ... */ } struct MapResult&lt;T&gt; { res: ??? } let mr = MapResult { res: map(it, |x| x * x) }; What's the type of `res`? I don't think it can be `impl Iterator&lt;Item=T&gt;`, because then you could only ever use `MapResult` for calls to `map` with one particular closure, right? So I guess I don't understand the whole argument about storing the results of `impl Trait`-returning functions in structs. I also may have misunderstood the whole thing.
It's not done, but it's definitely a popular request. I've opened [an enhancement request](https://github.com/integer32llc/rust-playground/issues/4) to at least investigate it.
Yeah, we have a whole futures-based async stack built on mio. http, ntp, dns resolver, etc. Our in-house rpc system.
I don't dispute that it may be interesting to some in the community, but demonstrating explicit applicability to Rust is a minimum bar that I like to enforce. If you would like to write a blog post that references the OP and discusses how this would be interesting to Rust programmers, then I would gladly permit that.
[removed]
You're looking for /r/playrust.
It's not going to be removed, it's just not finalized yet. `impl Trait` is what it's starting out as, and the goal is that experience with the implementation will help everyone come to a decision on what the final shorthand should be. The presumption is that it won't actually be `impl Trait`.
Yes, but you wouldn't need static data. The `Connection` isn't `Sync` after all!
Cool to see this, I've been wanting to write an SMTP parser in nom and this is a loosely related example I can have a look at.
&gt; but it seems that most people have the intuition that the receiver of a method is evaluated after the other arguments rather than before. Intuitively `self` gets borrowed "when the function is called", and you can only actually call the function after you have all its arguments evaluated. Intuitively, `self` isn't a real argument.
Wait, what's the difference between `struct Foo&lt;T: impl Trait&gt;(T);` and `struct Foo&lt;T: Trait&gt;(T);`? I know it's not part of this RFC, but I want to know what the imagined feature actually _is_.
I honestly am not sure.
"[…] itself has submodules and those merit their own files size-wise" Keeping a module "foo" in "foo.rs" is alright if its submodules, say, no more than 10-20 lines long. I also never use "use self::…" or "use super::…", as it makes reorganizing modules a great PITA. Not only do you have to fix all uses (from elsewhere) of the moved module, but also all its internal imports, too. Which takes time, breaks one's current flow of thought and adds unnecessary changes to the git commit. (Relative imports are like BASIC's branching by line number. Works well for immutable code. Otherwise not so much.) And worst of all: Moving "foo::bar" to "foobar::bar" can lead to undesired and even uncaught false imports when bar imports "super::baz" and "foobar::bazblee" also is a thing (which will either result in strange compiler errors if the API's don't match or wrong behaviour at worse, if they do. With absolute paths we have none of these problems and I don't have to know the file path (which is sometimes not obvious or readily available at all) of a rust file in order to make sense of its contents.
Absolutely; I personally only use `use super::` in tests, and never use `use self`.
So when I installed the lldb extension for VSCode, while on OSX, it keeps looking for target/debug/project_name.exe instead of just target/debug/project name. Thanks for the thoughtful tutorial though!
&gt; One interesting problem I’ll need to solve is for instance: how do I get a Z80 program dump from a binary file converted into a Rust array inside a source file. Would the include_bytes! macro work for this?
The intent is to have both shorthand and longhand syntax for both universals and existentials. Longhand syntax for existentials has not been determined yet.
Ah, yes, I forgot about that. I am actually tracking the fact that a network request is happening, but not showing that anywhere... oops! Adding a little spinner-esque UI element should be straightforward.
This :) I'd add one last condition: take ownership on methods, when you are going to store the parameter somewhere. You could borrow and clone, but it's better to leave that choice to the caller, that way your code will be more efficient when there is no further use for the argument by the caller.
Servo is a tiny team working on a very large project. This code was part of a massive improvement by a student project to the mostly-neglected network stack that added cookies. The array was missed during code review. It was fixed later. These things happen. 
Use a connection pool that will manage the lifetime of your connections. The pool will be Sync and you can share it. Quick search on crates.io says that r2d2 (https://crates.io/crates/r2d2) is the most popular.
But if I understand things right, a "type parameter instantiated by `foo`" and resolved at compilation time is just a type, right?
So, firstly, the (1) itself was ban-worthy. (3) insinuates that jdm is a bad programmer, and also subtly that metajack is dishonest. At least, that's how I read it. Sure, it is possible for someone to misspeak, especially when English is not their native language. _this is why I left a warning comment_. I mentioned a factual error, and mentioned that the tone was unsuitable for this subreddit. I agree that my comment after that about "not going to argue this with you" could have been less harsh. /u/heckerle clearly didn't understand that (3) was non constructive when they wrote it, and I should have acknowledged that (and made the comment more like (6)). I apologize for this. &gt; Of course if a person is not willing to adapt it should not be tolerated. I think part of the issue was that I sort of read (3) as an unwillingness to adapt :) In retrospect, I was wrong. &gt; Providing a friendly, safe and welcoming environment for people of all ethnicity for me also means to be constructive when a person is not accustomed to the tone of discussions in the US and also how to argument well Agreed. I'll try to be more understanding of non-native speakers in the future.
Thanks for the clarification! :) I guess when you in many many cases people are really rude on the internet its very hard to assume innocence at first and having the patience to explain the same points over and over again. I really appreciate the work of upholding the code of conduct! At least for me and I guess also for heckerle this helped us learn a lot and express ourselves better next time.
Huh, Alpine Linux supports Rust. Has this been true for a longer time already?
Also, the error you encountered looks like [rust-lang/rust#33778](https://github.com/rust-lang/rust/issues/33778) and should be fixed by a `cargo clean`.
I know this wasn't your question, but anyway: &gt; To generate the short URL identifier it uses the first seven bytes from the SHA-2 hash of the original URL. Again, I may have overlooked any issues with this, but the chance of collision is unlikely. This is true as long as you don't want to compete with bit.ly etc. I'm no expert in statistics (so please correct me if I'm wrong!), but after having generated 10 million urls (where the "random part" has 56 bits), the probability of the next url being a duplicate is already 0.07% (1.7% after 50M, 6.7% after 100M).
This has been around for a while apparently, but I hadn't noticed. I've recently started using ArrayFire in my research, and am really excited to see Rust bindings for it maintained by the ArrayFire folks themselves!
Panicking with a pyrotechnic picnic is probably the proper plan.
&gt; What I tried to say was that, if you think "hey (s)he screwed up" and you don't tell the person when you meet him/her again where and especially why (s)he "screwed up", that this would be dishonest, since you thought badly about someone's job but kept it for yourself. Right, I know that _now_, but this was not at all clear about your comment. &gt; Assuming that this was not a mistake telling that person would lead him/her to become better in his/her passion. This would neither mean the person is not a "true" software developer nor that (s)he is unfit for a job. It simply states an area for improvement. Yes, but you went far beyond calling it a mistake, calling them a bad programmer ("bad sign", really). Point out the mistake as much as you want, don't make claims about how good a programmer someone is. &gt; I sometimes feel like going crazy because I seem to be incapable of seeing where and why someone might find something I said rude. "bad sign" and the part I explained about dishonesty. "bad sign" is talking about the skill of the programmer. Without the further context you provided later, the "dishonest" part effectively says that you think metajack is being dishonest. Both are not okay. Furthermore, "nor do I think that I was even remotely offensive" makes it seem like you aren't willing to change your tone, which doesn't help others read your comment favorably. Basically, there was a lot of context here that was missing, which _you_ knew (and thus didn't think your comment was rude), but readers of the comment didn't. Always try to read what you right from an outside perspective to ensure that it will be clear to people who aren't you :) &gt; BTW: Do I understand that correctly that I can use "they" instead of he/she? Isnt't that plural? Yes. It is both singular and plural, though usually singular they only crops up when you want to say he/she. (It is still correct to call someone "they" when you are aware of their gender)
Your actions, attitude and language around the deletion of this story has soured me a bit about /r/rust. The _rules you are enforcing_ seem capricious. I feel like I am at /u/kibwen 's house and being told arbitrarily not to play with your toys. A materially similar story has escaped your filter. 
I'm guessing it's going to be hard with a real filesystem for him to cause I/O errors and things like out of disk space.
I did something similar on a livestream. Here's a timelapse https://www.youtube.com/watch?v=Ae44aZAac3w. But this was a long time ago so the Rust ecosystem is probably much better now. But obviously yours is more advanced with actual database support. Nice work man!
 &gt; I don't need the string to be mutable. Should I be using str instead? You can't use `&amp;str` to take ownership of the vec, so unless you need to keep the vec around for something else, there is no gain in not transforming it into a `String` the way you do now. &gt; Is there a way to avoid zeroing bytes? It will be overwritten anyway. Or, should I trust optimizer to take care of that? You could do something like: let mut s = String::new(); if try!(input.by_ref().take(length).read_to_string(&amp;mut s)) &lt; length { /* Handle EOF */ }; ...but I doubt there'll be any noticeable difference in performance.
I think requiring sudo to run tests is dangerous. Nothing like running experimental code that specifically deals with filesystems as root...
Now with support for arbitrary conditional modules (not just `#[cfg(test)]`) and much improved accessibility. [preview.gif](https://github.com/regexident/cargo-modules/raw/master/preview.gif)
Personally, I'm hoping for something related to associated types, like: struct Foo; impl Foo { type Closure = impl Fn(i32, i32) -&gt; i32; fn make_closure() -&gt; Closure { |n, m| n+m } } struct Bar { closure: Foo::Closure, }
There is [still some pending changes](https://github.com/rust-lang/rust/issues/27787) holding this up. I think you are looking for the [`range`](https://doc.rust-lang.org/std/collections/struct.BTreeMap.html#method.range) operator. The [`Bound`](https://doc.rust-lang.org/std/collections/enum.Bound.html) enum allows you to define the start/end and Inclusion/Exclusion of your end points.
rust has supported the musl target for a little while now. It got much easier to cross-compile to musl when multirust/rustup.rs added the ability to dl the standard lib for different targets. Now, cross-compiling for musl is just a couple commands.
You are returning a borrow to some data that goes out of scope when the function returns. So you are working with a freed pointer here. This is absolutely dangerous and unsafe. Why are you not just returning the Vec instead? Another possibility would be a boxed slice.
Indeed! It is very cool that this is first party support.
We had a talk about this a while after 1.0 at the Belfast Linux user group. It went down pretty well. Plenty of interest from the C++ and old school Linux guys in the crowd.
Yes, it can be. (Note that the "unprintable span" is a bug -- there should be a span.) Keep in mind that what is or is not regular rust syntax has no bearing on the syntax the macro accepts. However, we can extend it to accept trailing commas. There are three ways: - it could be changed to _require_ trailing commas, by changing the various `$( ... ),*` to `$( ... ,)*`, [like this](https://is.gd/8BP6lg) - it can accept arbitrary numbers of commas after the lists, [like this](https://is.gd/eEUfVw) (so `{ f1: u8, f2: u16 ,,,,,,,,,}` would be accepted) - we can quadruplicate the code to allow at most one trailing comma, [like this](https://play.rust-lang.org/?gist=0e9274cb11ffa13d8947b2e796a4d1f9&amp;version=stable&amp;backtrace=1) Because the macro system has no one-or-zero matcher (an oversight, in my opinion), there is no middle ground between the second and third options.
And there is a SQLite connection manager too. Sweet! I will start adding this in. Thanks for the tip.
Why don't you just borrow the closure instead?
You should replace `String::new()` with `String::with_capacity()` and then you should get a noticeable performance difference. :) Otherwise the string will be [resized](https://doc.rust-lang.org/nightly/src/std/up/src/libstd/io/mod.rs.html#345-374) whenever new data is read.
Good point!
&gt; &gt; &gt; Since we know that we are filling the vector with actual data right after this we know what this code is safe. Unless you have a misbehaving Read implementation; while it's *recommended* that `read()` (and friends) do not read the data you sent there uninitialized, there is no compiler guarantee. That's why I didn't dare to recommend that :-)
Modules tripped me up pretty hard for a little bit. In fact, I'm still not sure I'm using the best method for organization. Say you have three files. main.rs, testA.rs, and testB.rs. My goal is to use main.rs to launch real_function() that resides in testA.rs. real_function() will then call other_function() from testB.rs. Where do I place my mod testB line in order to be able to use that function? My answer has been to put mod testA and mod testB in main.rs and then "use testB" in testA.rs to call testB::other_function(). I don't know if this is "correct" or if there are better methods, but I struggled quite a bit at first on where to place what just to simply get my program to compile/work as intended with that sort of architecture. I apologize if I missed it, but I didn't see anything that labelled this out in the new documentation either.
Follow the behavior. Personally, I find the idea of treating numbers as bags of bits annoying. I would have favored specific "bag of bits" types (which would NOT be signed! it makes no sense to sign a bag, except if you're Gucci) to avoid conflating mathematical operations and bitwise operations (shl, shr, not, bitand, bitor, ...).
Why do [these impls of From](https://play.rust-lang.org/?gist=3f193e93f58b2a3d8a232c50a7cbd437&amp;version=stable&amp;backtrace=0) all conflict? I'm not sure, but I have not seen `AsRef&lt;str&gt;` for i32 and similar `Into` in std library. Code: struct Test; impl From&lt;i32&gt; for Test { fn from(i: i32) -&gt; Self { Test } } impl&lt;T: AsRef&lt;str&gt;&gt; From&lt;T&gt; for Test { fn from(i: T) -&gt; Self { Test } } impl&lt;'a, T: Into&lt;&amp;'a str&gt;&gt; From&lt;T&gt; for Test { fn from(i: T) -&gt; Self { Test } }
The efficiency loss is quite impressive. I can only wonder if some tricks could not bring this performance upward... for example, a `struct SliceFinger&lt;T&gt; { finger: *const T, before: isize, after: isize }` allowing the pointer to be in the *middle* of the struct, seems to be implied as one possible win according to (c) offsets.
I went with the 1st way (require trailing commas) because I always put trailing commas in struct definitions. I also put a "#[allow(unused)]" over the struct in the macro which silenced the internal compiler error instead of using a "file-scoped" #![allow(unused)]", even only having those is kind of bad because Rust won't tell me now if I have a real unused field in one of the structs (I hope this "bug" get's fixed =)). Two more things I don't like about this though: 1 - can't put an impl block immediately after the struct because the macro doesn't allow it 2 - there seems to be a macro recursion limit of 63, i.e only 63 structs could be put in the Common_Fields!() { } macro block. With that said, I thank you for trying and hope macros are allowed to be called inside structs at some point.
1. You can put the impl block outside the macro. Extending the macro's syntax to allow them is also possible. 2. This is true, but you can increase the recursion limit (the error message tells you how). Another possibility is adding extra rules to the macro to "unroll" the recursion: add a rule that outputs two structs and then recurses, or three, etc, to cut down the recursion depth by a constant factor. How many structs do you have‽‽
&gt; The major bugs encountered centered around a handful of negative array indices being used to access slices of referenced arrays, the -- operator being interpreted as a no-op (repeated negation) ... Yeah, using -- (as in https://is.gd/svBm9c) should really show a warning ...
Ha, I should have paid more attention and read the readme from the [itoa crate](https://github.com/dtolnay/itoa), the benchmarks there might be relevant. I tested my naive implementation against that `itoa`, and it's quite a bit slower (4x for `u64::MAX`), but it's still saving me time compared to `write!`.
&gt; I have not seen `AsRef&lt;str&gt;` for i32 and similar `Into` in std library Those are not there _now_, but in principle they could be added in the future. If those implementations were added then you would have conflicting implementations at that time, which is a breaking, backwards-incompatible change. So in order to make adding trait implementations a backwards compatible change, the implementations must be known to never conflict. Arguably the error message could convey that better. I don't know if it's accurate, but right now I think of the net effect as "bounds are ignored for generic impls when determining conflicts". It's almost as if the compiler sees `impl&lt;T&gt; From&lt;T&gt; for Test`, which clearly conflicts with `impl From&lt;i32&gt; for Test` if `T=i32`.
Thanks for confirming for me! You should consider putting a small example like that in the documentation. It took quite a bit of project hopping on github and guess work to arrive at that when I first started Rust. 
&gt; in Rust, you typically don't "die", you "panic" Maybe this is an unpopular opinion, but I don't think `panic` is the right answer for _expected_ errors. Printing a nice error message and exiting with a non-zero exit status makes for a much better UX than the very Rust specific panic message. I've actually made the habit of having a `die!` macro similar to `panic!` but for "graceful" exits in cases like OP's. As an aside, it would be nice if there were a built-in `println!` that went to `stderr`.
I'm not a Rust expert, but I recently replaced all of my `panic!`s with what you described in my applications. The `panic!` message is very offputting, whereas a nice `println!` looks really nice and approachable.
While I conceptually agree, there are occasions where signed right-shifts are useful. For left-shifts, there's usually no difference.
Ideally you would print it to stderr, that can be done with `writeln!(std::io::stderr(), &lt;normal println syntax&gt;)`, which you might want to wrap in a `print_err!` macro.
The rust language [has a very permissive license](https://github.com/rust-lang/rust#license) and is available for free. You may however try asking in /r/playrust, which I believe has a much more restrictive license.
Idiomatic rust doesn't use `&amp;String`, it uses only `&amp;str`. Your program is subject to regex injection. Using ( as a separator will not work. You can abstract your "or die" pattern in a generic function. Else I didn't see any error and found the code clear. Continue to enjoy rust! ;-)
What exactly is the distinction here? Why would an `Impl` be less backwards compatible?
Can I somehow exclude i32 from this T? And if not: is there a way around? And if not again: is this meant to be like this forever? In my opinion, it just cuts expressiveness much. 
&gt; we created our own version of their algorithm Pied Piper at Hack Week. Brotli isn't middle out. :trollface:
Whoops thanks
It looks like both rust-brotli and brotli-rs only implement the *decompression/decoding* part of brotli, not the *compression/encoding* part... right? Am I missing something?
Meta issue: doc comments are actually sugar for an attribute...
I [added a third rule](https://is.gd/JmJPOw) that passes through an item (such as an impl block), so you can put them inside the macro. The macro expander tries rules in order, so it will first try the rule that expects a struct, and if that fails, the rule after it accepts any item. Hmm you're right... I could have sworn that error used to give instructions on increasing the limit. I'll submit a bug.
Making that assumption in release mode would trivially lead to violations of memory safety. The check in debug mode is to help you catch some buggy code in testing.
https://github.com/danielrh/brotli/tree/seccomp &lt;-- yes, the same SECCOMP technique works on the C code, both compressor and decompressor.
&gt; Telling the Rust allocator to avoid zeroing the memory when allocating for Brotli improves the speed to 224 MB/s. Are we explicitly telling jemalloc to zero newly-allocated memory, or is that just the default for jemalloc? What mechanism is used to control this?
Welcome! :) I'm curious if your LLDB efforts are aimed at improving Rust support to the same level as GDB?
mmap on linux returns 0'd memory...but I think mmap in general is not guaranteed to do so--so a cross platform code may memset things in addition. I haven't checked, but i have this sneaking suspicion that Rust touches memory upon allocation (maybe by zeroing it), forcing the OS to actually deliver the zero'd pages and having rust memset them to zero again. The benchmarks seem to indicate something odd is going on wrt memory and zeroing.
Yeee! Thank you! &gt; Extending the macro won't be easy (I find macros difficult). Hm, it seems it was pretty easy after all, although I doubt I would've arrived at this or similar macro only by reading the [chapter](https://doc.rust-lang.org/book/macros.html) about macros... maybe with a lot of trial and error =).
I'm on Windows and I have tried that version and it hasn't worked. Could you give me a download link to rustc 1.9.0-dev just incase?
Could you please report whoever is threatening you to a moderator? Threats and name-calling are not allowed, even over private message. And removing all the context for a complex conversation is annoying. I understand it, but it's still annoying.
That's correct for brotli-rs. Decompression is fairly straightforward, while compression allows for certain decisions to be made which result in tradeoffs between compressed size and speed. In my case, I felt I'd need to research better before I'd tackle the compression direction, and just haven't gotten to it.
Ah, see, you are clearly confused. The green block cursor is from the emacs keybindings. If you didn't have that before, you clearly weren't using the right configuration. Seriously though, I've created an enhancement request to add those options.
I don't know, but I'm curious! Could you expand a bit more on what benefits something like that would have? I do know that there's some work towards allowing Rust to compile with Emscripten as the target, which opens the door for a strange and exciting Ouroboros of web-related software.
This definitely sounds intriguing, but far beyond what I'd be able to do now. Having _any_ kind of long running service is going to blow apart the current sandboxing model. I hope you follow through with your exciting ideas and I can't wait to see what awesome thing you can create next!
How can I safely traverse a DAG (using `Rc&lt;RefCell&lt;Node&gt;&gt;` as edges) without pointlessly bumping the reference counts as I go along? See http://stackoverflow.com/questions/38114271/safely-traversing-a-directed-acyclic-graph
What proper `Result` error handling do, `panic` almost always does, and `process::exit` never does, is cleanup. E g, maybe you have created a temporary directory that you want to remove before process exit. `process::exit` might work for small programs (where you are certain there is no cleanup needed) but it will be problematic to keep track of needed cleanups when the program is larger. YMMV. &gt; I don't think panic is the right answer for expected errors. For expected errors, proper error handling (via `Result`) is most often the right answer. &gt; a much better UX than the very Rust specific panic message It is possible to use `std::panic::catch_unwind` at `main()` level to catch all panics, print them out in the way you prefer and then exit with a exit code of your choice, but then again, I believe using `Result` would be better for expected errors.
That's because there is no word yet :)
Aww, eagerly awaiting in that case
&gt; Mind clarifying that? Here's a sketch https://play.rust-lang.org/?gist=6eb80fef86a2887df68fd12335855c22&amp;version=stable&amp;backtrace=0 How does it look?
Thank you! &gt; Idiomatic rust doesn't use &amp;String, it uses only &amp;str Where can I read something about this? Do I just have to replace `&amp;String` with `&amp;str`? (and why) &gt; Your program is subject to regex injection Haha, you are right! Thanks. &gt; Continue to enjoy rust! Happy to learn!
Ok thank you! I will have a look at this problem!
&gt; Do I just have to replace &amp;String with &amp;str? Generally yes. &gt; (and why) `&amp;String` is a reference to a heap-allocated string, so you need a matching heap-allocated string. It's also a pointer to a pointer to the string's buffer (`String` is a pointer, and `&amp;` is a pointer). The string slice `&amp;str` is just "string data I don't own", it may be backed by a heap-allocated string but it may also be backed by a string literal (stored in the binary's data section) or by a utf8 binary slice stringified without allocation (e.g. [`std::str::from_utf8`](http://doc.rust-lang.org/std/str/fn.from_utf8.html)). As a result, `&amp;str` is much more flexible and simpler to generate on the caller side, and thus preferable if you don't *need* an actual String for some reason (e.g. possibly reallocating operations). Same with `&amp;[T]` versus `&amp;Vec&lt;T&gt;`.
At least it's an error for unsigned integers. So that's something.
I'll answer my own question, in case somebody cares: You can install the musl version of rust on your build machine (which would be a more standard glibc linux distribution) via rustup by doing rustup target x86_64-unknown-linux-musl (also, check out *rustup target list* for all supported targets, an impressive list) Then you can run on your own project cargo build --target x86_64-unknown-linux-musl --release and you get a pretty package without any external dependencies! vegai@harmony ~/git/chaos ±master⚡ » ldd target/x86_64-unknown-linux-musl/release/chaos not a dynamic executable 
Discarding shifted bits was always expected behaviour to me. Maybe because I did some embedded development... To me, normal numbers always represent some amount/count of something. Operation 3 apples + 2 apples = 5 apples is meaningful. But what does 3 apples shifted to left by 3 means? Bit shifts are low level tricks and should be treated as such. If by shifting you mean multiplying (dividing), use appropriate operation and let the optimiser do it's work. It will also make the code more readable.
you need to create a .proto file. this file will then be compiled to rust code( or c++ oder java or c# or ...) . have a look at the google example: [https://github.com/google/protobuf/blob/master/examples/addressbook.proto](https://github.com/google/protobuf/blob/master/examples/addressbook.proto) 
I agree with this. Panic is great for convenience during development, but should be replaced with proper handling before release. 
Aside from all the good advice in other comments, I just wanted to add this :) warning: The function/method "replace_all" doesn't need a mutable reference, #[warn(unnecessary_mut_passed)] on by default --&gt; src/main.rs:49:22 |&gt; 49 |&gt; .replace_all(&amp;mut s, &amp;*match args.arg_d { |&gt; ^^^^^^ help: for further information visit https://github.com/Manishearth/rust-clippy/wiki#unnecessary_mut_passed
That makes sense. Thank you.
I won't be able to attend :( Will there be YouTube or Mozilla Air videos posted of the talks? Perhaps of the training as well? Looking forward to grabbing some popcorn and watching Rust talks on a raining day 😁
&gt; Clippy Wow great!
For future reference: [Taking Rust everywhere with rustup](http://blog.rust-lang.org/2016/05/13/rustup.html)
Well, it's a comment on Haskell :P and probably should be rejected by the Rust lexer under "it's too silly" grounds (while `- - x` would still be a double negation). But now this would be a breaking change..
Wait, one can use the debugger in VSCode with rust? I should really start using that debugger more :D
The talks will be recorded, yeah.
You have two possibilities: 1. Use `unsafe` and cast the `&amp;mut` to `*mut` and then to `&amp;mut` back again. Make sure you won't return the same `&amp;mut`-reference twice, that would cause bad things. 2. Withouth unsafe – make your `MapIterMut::slice` a subslice containing only the remaining elements. The next element would be in the head of the slice. You can use [`split_first_mut`](https://static.rust-lang.org/doc/master/std/primitive.slice.html#method.split_first_mut) or similar method to safely split a slice. You will have to assign the "remaining" slice back again to your `slice` field. That may cause problems like "can't move out from borrowed variable", I guess that you'll need to temporarily move out from the field using `std::mem::replace(&amp;mut self.slice, Default::default())`.
I've just started learning rust a few days ago. I am working on three micro projects: * a [csv delimiter converter](https://github.com/rap2hpoutre/wsv) * a [tax calculator](https://github.com/rap2hpoutre/vatax) * a [unicorn name generator](https://github.com/rap2hpoutre/mynia) These projects are only for learning purpose. I need more ideas to learn more :)
I am not saying that one should not use right-shifts on signed integrals. I am saying I would rather having to cast the integral to a bag of bits (of a given size), apply the magic there, and then cast back. Once it's a bag of bits, go wild with all the bitwise operations.
https://doc.rust-lang.org/book/strings.html
Not only that, I just submitted a PR to [rust-brotli](https://github.com/dropbox/rust-brotli) to avoid a number of clippy warnings (though I left a few for other contributors to fix,,,).
You can use `mem::transmute()` which is roughly equivalent to `reinterpret_cast()`: extern crate libc; use libc::*; use std::mem; extern "C" fn some_func(_: int32_t) {} fn main() { let addr = some_func as usize; // Function address here as `usize`. // Slightly cleaner than passing the to `mem::transmute()` directly, IMO. let func: extern "C" fn(int32_t) = unsafe { mem::transmute(addr) }; // This works too let func = unsafe { mem::transmute::&lt;_, extern "C" fn (int32_t)&gt;(addr) }; func(0); } Edit: fix code snippet
&gt; I think I will try using raw pointers; even though it is technically unsafe, on such a small portion of code it should be easy to manage. I think that `unsafe` should be used only when something is just not possible with safe code, or when `unsafe` provides measurable performance gains. I'd recommend avoiding `unsafe` here for two reasons – you'll learn something about `&amp;mut` pointers, which will be certainly helpful later on, and because `unsafe` makes code less refactorable – every change inside the module should be carefully reviewed (for example, if you for some reason added `rewind_to_beginning` method to your iterator, suddenly, you got undefined behaviour). &gt; But why it is required, and more importantly why does it work with immutable but not with mutable ? "Immutable" reference (`&amp;T`) actually means "shared" in Rust, and "mutable" (`&amp;mut T`) means unique. Because of that, you can have as many shared references to the same piece of memory, but each `&amp;mut T`, has to be disjoint. Thus if rustc sees that two `&amp;mut`s point to the same memory, it complains. In your case the `slice: &amp;mut [Option&lt;T&gt;]` and `&amp;mut Option&lt;T&gt;` overlap, so it's forbidden. It doesn't matter that you don't actually use the returned index via `slice`, the mere existence of overlapping `&amp;mut`s is forbidden. Anyway, if you want to stick with unsafe, it's recommended to use `*mut [T]` instead of `&amp;mut [T]`, so you don't lie to Rust about the uniquess of the pointer. There's a [discussion](https://internals.rust-lang.org/t/tootsie-pop-model-for-unsafe-code/3522) going on now whether that kind of "lying" should be considered safe or not, so that recommendation may change. See how's the `IterMut` in std [is implemented](https://static.rust-lang.org/doc/master/src/core/up/src/libcore/slice.rs.html#1014-1018). [Here's my attempt](https://gist.github.com/krdln/2bbd058c1b95b204e4afff4bc885f877/revisions?diff=unified) to make a safe mut iter in your code. It compiles, but I haven't tested it, so I only hope it works correctly. There's also a problem here with rustc's error messages, which are a little bit misleading. The compiler says something about "conflicting" requirements, not that two overlapping `&amp;mut`s are forbidden. That's because when you type `self.field.method()`, and the `field` has type `&amp;'a mut T`, the lifetime passed to the `method` is not `'a`, but some `'s` (assuming the caller's `self` is declared as `&amp;'s mut self`). That error message might be improved. ---- Now I think that you can put the `std::slice::IterMut` in your struct and just call `next` on it. I guess that will be the simplest solution.
I'm glad to see that the overall trend is one where Rust is doing well and is quite healthy! It's also good to see where we as a community can focus our efforts over the coming year, namely stabilizing plugin support and other good features only Nightly can do, tooling, ide support and libraries.
Your solution works like a charm ! I tried it and it works fine, I can modify the contents of the map without trouble. I think I understood why it did not work, too. That error message can definitely be improved because even after the solution I still cannot understand what it tried to say. Thank you again !
Am I going crazy or was this posted before?
Check out [the docs](https://doc.rust-lang.org/std/path/struct.Path.html). `Path` is a slice. It is an unsized type and must always be behind a pointer. It's quite like `str` actually. `Path` is to `PathBuf` as `str` is to `String`.
Can the raw data set be made available to the community? (After all personally-identifying information is stripped, of course.)
Hey thanks for posting it to reddit! It's being maintained by Pradeep (from arrayfire) with heavy contributions from Jason (one of our first users after going open source). The API keeps changing because Rust keeps adding new functionality that make it easier to implement a numerical library. I think we are still in the process of finalizing the API after introduced some cool new features. Any input about improving the library is much appreciated. **EDIT:** Thought I'd link to [Hyper Adaptive Learning](https://github.com/jramapuram/hal)(HAL for short) library by Jason being written in rust using arrayfire-rust bindings. 
Is there a performance penalty for using `buf.as_path()` every time I want to use the path?
There are only a couple of things which are intended to be "never stabilized". Among them are compiler intrinsic functions (some of which have been stabilized piecemeal such as `transmute` and soon `discriminant_value`). And other stuff like compiler plugins will never be stabilized in their current form, but the use cases will become possible in other ways. But for almost everything, you can follow the progress towards stabilization on the appropriate tracking issue.
Nope, it's pretty much free like slicing a vector. There's even an implementation of impl Deref&lt;Target=Path&gt; for PathBuf, so &amp;buf can be used where &amp;path is expected
This is a paper my classmates and I wrote comparing concurrency in Rust and C. I'd love to get the community's feedback. Thanks! Edit: Thanks for the feedback, it's been very helpful. We're students so we're bound to make some mistakes, thanks for being understanding. We'll be working on this further and resubmit when we've addressed the issues you've all raised. Thanks again! If you'd like to see the resources for our investigation, check our repo: https://github.com/rjw245/ee194_final_proj This includes our benchmark code.
I think the 'crash report' question was too easy to misunderstand. For example I answered 'yes', because truth be told, I once installed stable over a nightly and it predictably broke my clippy build ;-P I also once updated some Rust 1.1 code to 1.7 (can't remember the repo) and while there were errors, those were pretty straightforward to fix.
Does anybody else find it surprising to see Rust lag behind C? How do you explain this?
~~This isn't that surprising to me. gcc has been in development for decades, I'd expect it to produce more optimal assembly than rustc.~~ Never mind, the Rust code wasn't compiled with optimizations: https://github.com/rjw245/ee194_final_proj/blob/master/benchmarks/dot_product/rust/test_local_sum/Makefile#L2 That's why you see the vast difference.
~~I find it very sad.~~ May be the solution might be to transpile the safe rust code to gcc? EDIT: I feel happy now! ;)
If there's no accepted RFC for it, I wouldn't rely on things. Note that all features usually have an associated tracking issue that you can find in the documentation, which can give you a good overview.
The paper doesn't mention how the C code is compiled.`rustc` doesn't compile directly to assembly; it compiles to LLVM IR, and uses LLVM to compile to assembly. If the C code in this paper is compiled with `gcc`, any performance gap could be caused by a difference between the GCC and LLVM optimizers. To isolate any differences between C and Rust, it would be better to use `clang` to compile the C code, since it also uses the LLVM optimizer and backend. 
Thanks. I'll take a look as soon as I can spare a little time.
You forgot to specify [the `--release` flag](https://github.com/rjw245/ee194_final_proj/blob/933e56c1337c7ddd13648ad259201ae5f57881c6/benchmarks/matrixMult/rust/Makefile#L8) for Rust as well as e.g. the `-O3` flag for gcc 😓. After fixing that the "matrix multiply" code is only about 20% slower than the C code and I'm pretty sure we could fix that too, since most of the time is probably lost in the bounds checks on the slices.
I am really looking forward to the Rust Language Server (RLS). I tested TypeScript a couple of days ago and their IDE integration is pretty sweet. Actually that's one specific area of Rust I would like to try to contribute to if there is good mentoring. I am probably greatly underestimating the complexity, but it seems a fun way to learn about the compiler and help the community! :)
Yup, just discovered this myself. Consider this paper an alpha release :) I will hopefully get around to fixing this and other problems y'all are uncovering and resubmit this. Thanks
It would be better if you could have the Python call back into Rust when it's done with the objects and then they can be freed. Or you could have Python allocate memory and Rust copy data into it. You don't want to have memory allocated by one allocator and freed by another (leaking is one way to ensure that of course).
Rust auto-dereferences variables for method calls.
I tried to get into Rust and saw a lot of upside in it. The learning curve combined with immature tooling forced me off it as I didn't have the time to commit to learning the language and workflows that will not be useful long term.
The phrase usually means "the code we wrote is written in language X." If I write a Ruby program that uses strings, I'm using a library written in C. But nobody would suggest it's not "written in Ruby." Basically, the term is fuzzy.
We all shouldn't be too happy yet... I've fiddled around with this a bit more and Rust is still horribly, _horribly_ slow. Kinda concerning actually... I believe it's because the Rust compiler causes too many additional branches, while the C compiler can quietly rely on undefined behaviour etc. I've reduced the Rust version of "matrix multiply" down to [this](https://gist.github.com/lhecker/1472ed7491472ac68cb25e73a4037fba) and the C version to [this](https://gist.github.com/lhecker/8f0e703dc1eb91dc6d3b162fc5a0cfc3). The C version (compiled with gcc though) is consistently about **TWICE** as fast as the Rust version for this very simple case. (Methods like `get_unchecked_mut()` didn't really help speeding up the Rust version btw.) **EDIT:** A [Java](https://gist.github.com/lhecker/0d39cbe3de7666e49480585378fc54bd) version and one for [Javascript](https://gist.github.com/lhecker/a1fefa41b5ad1c3199b625ed28743b3d).
The term I took issue with is 'entirely'.
I'm just working off of the original title in the other subreddit, I don't have the source myself. :P
Yeah, I guess I still see it used that way to refer to the code that was written for the project itself.
Is it because varargs are unsafe? Or is it just a TODO?
The truth is more subtle. The general-purpose data race prevention tool is `Mutex&lt;T&gt;`, but `Arc&lt;T&gt;` is useful for sharing references, especially to immutable data, and is safe from data races of the reference counts themselves (quite risky in C). In addition, `Arc&lt;T&gt;` can be used to share data safely even in the presence of some mutation, thanks to `get_mut()` (effectively a copy-on-write operation).
It's because varargs require hidden runtime overhead, and that's not something that Rust really tries to minimize.
The problem is that function return type does not need to be parameterized. It is concrete type, not generic, just anonymous. I do not want (and don't actually need) to parameterize every struct that depends on that function. All rust need to do is create a way to name those types. 
Absolutely. Perhaps that would have been better worded at "Arc&lt;T&gt;, while preventing data races in the reference count (otherwise Rc&lt;T&gt; could be used), is primarily used to prevent dangling references or double-frees when sharing data between threads."
A move is not necessarily a memcpy. Moving a vector, for example, is just a few very fast pointer swaps.
From some quick tests I did here, the difference is due to SIMD. Check the assembly. `get_unchecked_mut()` is unlikely to help because all the bounds are static, so the optimizer can remove them.
This was great. The most advanced Rust game I've seen. Thanks for sharing.
Are there some things that you think that Rust should lift from this?
It did look for 'target/debug/project_name', but didn't find it. The fallback to ${program} + '.exe' was supposed to be Windows-only, but due to an oversight this fallback also activates on OSX :-\ You should probably specify the program path as '${workspaceRoot}/target/debug/project_name' though, as VSCode's current directory is not guaranteed to be the same as your workspace.
Sorry about that. If an admin could change the post to link to https://github.com/rjw245/ee194_final_proj/blob/master/report/final_report.pdf it would be much appreciated.
Is it just me or is everything pretty buggy *and* slow? `./runservo.sh` took &gt;5 seconds before the window showed up. The window does not scale well on my HighDPI screen (I certainly don't expect that at this stage of development :P). Then I opened GitHub and tried to resize the window quickly, which lead the window to flash between black and the site content. Also the CPU was pretty busy and displaying the page didn't seem fluid at all. After that I opened the transparent boxes demo which again uses quite some CPU and runs with around 50fps on my i7-6700HQ. Are those normal values? Finally, attempting to close it via [X] just resulted in more CPU usage and nothing else. The interface -- although probably just temporary -- looks beautifully minimal. And the images in `resources/` are hilarious \^_\^ 
Hm, the build is not signed on OS X, which makes it a dance to get it working. I know this is a preview release, but that has been noted as a bug for Rust (also on Windows) for quite some time now. https://github.com/rust-lang/rust/issues/27694
&gt; How can I find out in what mode Servo is rendering? Software rendering indeed sounds like a bad choice... I think you just run glxinfo? Servo doesn't do anything special to switch graphics cards. &gt; I have two GPUs but NvidiaPrime is configured to use the integrated Intel HD Graphics 530. I don't know what level of performance Servo needs, but integrated cards are pretty nice by now and I'd expect it to be enough for a browser (right?). I will try it with my Nvidia (940MX I think) later today! I've been running Servo on an integrated intel for years; only rarely bumblebeeing it to Nvidia. Integrated intel is fine. Software rendering surprisingly works decently too, but it can suddenly go boom especially on more resource-intensive pages. We do a _lot_ on the GPU, with the expectation that the GPU ... well ... exists :)
Huh, certs sound quite expensive! Anyway, as noted in the Servo downloads page, one can open the app from the context menu instead of double-clicking or Cmd-O to manually override that "protection".
This survey indicates that Rust-style memory management is an obstacle on the learning curve. The memory management also incurs some additional complexity when writing and reading code. An interesting followup question would be whether it's worth it. How do the advantages of Rust-style memory management stack up against the disadvantages? GC vs no GC is one of the critical questions for future languages, and I think experience with Rust will determine the path of language design. We don't call garbage collection Lisp-style memory management. Is there a term for Rust-style memory management? 
llvm produced code is still considered slower than gcc as far as I know. 
Thanks for your suggestion! I got it working now.
Same here, first load very slow, then starts almost instantly.
Apparently my openssl is too new for it – I have 1.0.2h, it requires 1.0.0. :-/
First of all kudos to the developers for making it this far. I'm quite impressed by what I see. Having said that, I have a couple of observations: - If I understand correctly browser.html tries to renders ui itself without relying on external toolkits like gtk. While you're doing it, can you also get rid of the titlebar? Seems to me like it's a wasted space. This is mostly a non-issue for unity desktop but gnome and kde still rely on buggy extensions that tries to hide the titlebar of maximized windows but breaks in almost all new releases. - There seems to be an option to pin the tab bar which looks nice and space efficient but I can't use it to switch tabs for some reason. Looks like it is only used to close tabs. Am I missing the use case? - Images in new tab page looks crispy for some reason although I can easily live with it. - Closing the window didn't work for me so I had to &lt;C-c&gt; from the terminal to kill the process instead. I really appreciate these nighly builds by the way. I have been meaning to play with servo/browser.html for a while and these make it quite painless.
&gt; Looks like it is only used to close tabs. Am I missing the use case? You can switch tabs with it, just that hovering there for too long (read: half a second) will make the close button appear. Might want to file an issue at https://github.com/browserhtml/browser.html/ 
&gt; hey guys check out this awesome Rust Let's Play vid I made
Perhaps the binary builds should statically link everything? Since they are being offered as .tar.gz instead of distro packages.
Rust is a verbose language like Java. `Result` is like Java's checked exceptions, you need to be explicit about all error cases upfront and it just doesn't scale for real world applications.
The borrow checker is so restrictive that it prevents writing most data structures. You need unsafe code even for simple things like linked lists and graphs.
I know how it works, but the non-signedness of Rust project software has been a long-standing bug I care about. OS X and Windows software is expected to be signed.
The ones from the 3 (quite tight) loops. If gcc uses SIMD and loop unrolling it will do much more actual work per iteration than the Rust version and thus hit far less branches (which as you probably know are the most common cause for slowdowns in modern CPUs).
Ah... right. SIMD. If you combine that with loop unrolling, we probably get the reason for the bad performance. It's still strange though since Rust _is_ capable of auto vectorization and loop unrolling AFAIK.
Sorry, but I consider these threads to be in violation of the "no memes" rule. This isn't the first of these we've had. :P
If you need Rust to keep you from writing buggy programs then you probably shouldn't be programming at all.
Adding `ptr::volatile_read()` after `ptr::write_bytes()` seems to work: https://is.gd/PfdIld Compare the assembly with the last line commented out and uncommented. It seems to prevent the optimizer from eliding the call to `memset`. You can tell `ptr::volatile_read()` is preventing the elision because switching to use `ptr::read()` ends up with both the `memset` and dereference being elided.
&gt; For stable Rust, 83.6% of users did not experience any breakage in their project as they upgraded to the next stable version. Previous research based on automated testing against the ecosystem put this number closer to 96%, which is more in line with expectations. Do you want to pick the value you want to believe or one that answers the question? The 96% number is not comparable, since it's the number of crate revisions (not crates but crates times revisions, and every crate is equal) that did not break, the survey number is the percentage of users that experienced breakage.
`Sized` or `Unsized` - these terms do not provide understanding of what's happening here. It would be better to think about `String`/`str` or `PathBuf`/`Path` as owned container and non owned reference correspondingly. So, it you want to **return** something, you can only do it with owned values, because if you going to return a non-owning slice/reference, there is an owner leaved still there and he is going to be destroyed (`drop`'ped). ``` fn foo() -&gt; Path { let owner = PathBuf::new(); return owner.as_ref(); } ``` 
`#![no_std]` is stable since Rust 1.6.
Yes, but for simple loops like this, the translation to assembly is straight forward, so the difference in auto vectorization is likely to be due to the difference between llvm and gcc, not rust and C. clang 3.4 didn't auto vectorize either. Auto vectorization will be harder for code that does have bounds checks though, so I think writing fast code in rust will often require more tricks than writing fast code in C. The safety benefits of rust are great, but it's not free and you should expect that converting C code to rust is going to give slower code unless you put some effort into it, and even then you'll probably need to resort to unsafe.
There's an [active RFC](https://github.com/rust-lang/rfcs/pull/1645) on making Cortex-M a first class target. The [RFC text](https://github.com/japaric/rfcs/blob/cortex-m-targets/text/0000-cortex-m-targets.md) does a pretty good job describing the roadmap, and some of the alternatives. Basically, what needs to happen is that the RFC discussion needs to finish, making some of the decisions on the alternatives presented, then everything described in the RFC needs to be implemented. I don't think that there's anyone working on this full time, I believe that the Coretx-M work is entirely free-time projects for the people working on it, so I don't think you're going to see any kind of more detailed roadmap with dates, but I think the RFC lays out pretty well the scope of what needs to happen, and should give you a sense of the progress if you follow the RFC process and then the implementation process once the RFC is merged (I don't think that merging this RFC in some form is particularly controversial, there are just some alternatives that need to be decided between). There was a [previous PR to do some of the work](https://github.com/rust-lang/rust/pull/32651), but it was closed as it was seen as needing to go through the RFC process to flesh out the design a bit.
Here are [some thoughts](https://www.reddit.com/r/rust_gamedev/comments/4q34qc/xpostanyone_have_experience_programming_games_in/d4pznbx) about the subject from Piston creator
The mutable vs shared difference should be answered well here: http://stackoverflow.com/questions/25730586/ Some names in Rust changed since the answer was written before Rust 1.0, but the explanation is the same.
Replacing the title bar with a custom decoration (like Chrome or Firefox) breaks in some windowing environments. Particularly the custom window dragging behavior can be terribly buggy. I understand the space saving point of view but I prefer apps with native look and feel. Disabling the title bar is an option in many environments anyway. I don't have title bars in any windows, which makes Chrome's custom title bar doubly annoying.
Here is an earlier proposal http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3814.html that outlines concrete use cases * Generation of common functions * Type transformations * The Struct-of-Arrays vector, replacing array-of-struct with struct-of-array * Compile-time context information, replacing assert I haven't written macros in Rust, but compile-time reflection should add expressivity to what is possible with macros. From the first cut, https://doc.rust-lang.org/book/macros.html Rust macros look like a syntax aware templating language. To achieve what the document outlines, one would need to [go nuclear](https://danielkeep.github.io/quick-intro-to-macros.html#the-nuclear-option) with [compiler macros](https://doc.rust-lang.org/book/compiler-plugins.html).
[removed]
Okay, this is pretty cool!
Maybe overflower could warn you of use of ambiguous semantics?
Awesome! Unfortunately on Nixos 16.03, x86-64, I get a "no such file or directory: ./servo" when I try to run "./runservo.sh". The file is really there, as for instance "cat" is able to find it, and there are no permission problems.
This is great! Has it been audited yet, since I know optimizers can do horrible things to constant time operations, or decide that you really didn't need to zero out that memory. cc /u/bascule and /u/briansmith
I think it prints the framerate to the log? Yeah, it might not be 2fps, but it's usually far from 60.
Have only checked the code examples and not the rest, but this seems to be more about parallelism then concurrency. I also think there is no such thing as "concurrency in C", because it's a language that is not very opinionated about that, and there are dozens of [incompatible] frameworks for concurrency and parallelism. E.g. there are big differences between usage scenarios for things like pthreads, openmp, libmill/libdill, coroutine implementations, tbb, cilk, ... 
It's not been audited, but I should have been more specific when I wrote "doesn't do its own crypto": This library implements the state graph of SSH, relying for all IO operations on libsodium and mio. Neither constant-time operations nor crypto primitives have been written in Rust in this crate. (this means "100%" is almost a lie). 
Note that all the other browsers also statically link to their security libs. But they have their own update mechanisms, Servo doesn't. But Servo is a demo only right now.
I actually am kinda sorta developing that with [stateful](http://erickt.github.io/blog/2016/01/27/stateful-in-progress-generators/). It essentially is trying to add new control flow mechanisms to rust. One is a proper state machine mechanism to capture this exact flow to simplify serde. Currently it supports generators, and maybe will also implement async/await. 
I knew it's the kind of things you would have thought of, which is why I asked ;-) 
:) I only get time for it every now and then though. If you end up wanting to hack on it just ping me and I'll walk you through the [code](https://github.com/erickt/stateful). 
Another possibility is, if you plan to go to Rust Belt Rust, we can take some time to do it there.
LLVM won't unroll or use SIMD in such a straightforward chain? Can one at least force the behavior?
Cool! Getting this via darcs was less painful than expected. I've scrolled through the code a bit, ran rustfmt, and fixed some of the things clippy mentioned. I actually managed to have darcs write a patch, but I have no idea if this works for you :D https://gist.github.com/killercup/370bd36115e029eede556abee50f9cda
&gt; Getting this via darcs was less painful than expected. That's because darcs is the *least* painful RCS ever written. Well, with the exception of Pijul, we're not far from being ready ;-) It was painless for me too: curl -O https://gist.githubusercontent.com/killercup/370bd36115e029eede556abee50f9cda/raw/eda9453ccbaa3912051e4d094acc923fab303942/apply-some-of-clippy_s-suggestions.dpatch darcs apply apply-some-of-clippy_s-suggestions.dpatch darcs push
I've got a BTreeMap of strings which are encoded in the Rust way, eg {"en": "English", "es": "Ingl\u{e9}s", "fr": "Ingl\u{e9}s"} and I'm trying to convert it to the JS way: {"en": "English", "es": "Ingl\u00e9s", "fr": "Ingl\u00e9s"} Is there a standard way of converting Rust's unicode standard to JSON-standard? I'm currently using a regex, which is working ok, but then I need to convert back to a string to write to a file, which then re-encodes the strings I've just converted :(
Thanks. That RFC is very detailed.
Is kerberos support for authentication on the roadmap?
What does "rust allocators only work on nightly" mean? 