That was already possible with `cargo-profiler`which actually provides cargo integration. Building and profiling a regular, cargo based, rust project is as simple as: `cargo profiler callgrind` `(--release)` The urlo announcement provides a more advanced example.
That would be grand.
 trait Entity { fn name(&amp;self) -&gt; &amp;str; } struct Character { name: String, } impl Entity for Character { fn name(&amp;self) -&gt; &amp;str { return &amp;self.name; } } struct Player { name: String, xp: u32, } impl Entity for Player { fn name(&amp;self) -&gt; &amp;str { return &amp;self.name; } } fn main() { let ch = Character { name: String::from("Foo") }; let player = Player { name: String::from("Foo"), xp: 0 }; let entities: Vec&lt;Entity&gt; = Vec::new(); // error: use enum(s) instead... // entities.push(ch); // entities.push(player); } What's the point of traits... when you can't put entities in an array/vector? 
Emphatic\_Snake\_Case
Well I can only hope the people who made those PRs are still alive so I can get them merged sometime during their lifetime.
I'm not sure I like this... In principle, it's great that perf gets Rust support. But our current symbol mangling scheme is historically grown without much of a long-term plan behind it, afaik.
Another solution is to use compositions: struct EntityBase { name: String, hp: i32, } struct Player { entity: EntityBase, exp: i32, level: i32, } struct Character { entity: EntityBase, } And then directly pass the entities to whoever requires it instead of the full `Player`/`Character`.
At least CPython does pre-allocate small numbers, so `x = 0` doesn't allocate (an allocation happens at program start, but that's obviously O(1)).
Building something atop WebRender sounds like it would be the ideal - letting us get rid of the gtk2/gdi/imperative, CPU-based drawing legacy. 
Oh man I totally misread how that was written. That's really cool!
Perhaps you should borrow them and have the compiler keep them alive?
I kind of wish there were implicitely defined `HasField` traits for each field a struct has. That way you could make field-generic code just by doing `fn get_name&lt;T&gt;(t: T) where T: HasNameField`. I'm pretty sure it would only need to encode the field's record offset and type.
This isn't just about Rust, practically every other language supports generic programming as well. In particular, C++, Java, Python, and Javascript. Go is pretty much unique among the languages I am familiar (apart from C) with in not letting you implement clean, reusable code. Go really feels like a language that was written 20 years ago and hasn't evolved since. It is true that I haven't completed any projects in Go, but that's mainly because the one time I did, it was so painful that I never finished. And now that Rust is here, I probably won't try again, because Rust is basically what Go should have been.
I think it's putting all dependencies in git/version control(not necessarily in main repo git has support for version control links/dependencies)
Using your macro, I tried to implement base initialization function (while not particularly useful as it would be overrode in every 'class', was an idea to play with macros. Old: impl Entity for NPC { fn new(name: String) -&gt; NPC { NPC { name: name } } } Macro: macro_rules! entity ( ($($t:ty)*) =&gt; ($( impl Entity for $t { fn new(name: String) -&gt; $t { $t { name: name } } .... However, this doesn't work: &gt;src/entity.rs:14:17: 14:19 error: expected expression, found `NPC` src/entity.rs:14 $t { name: name } According to the error message, it found 'NPC {name: name}', which is a valid rust expression. Why isn't it valid in a macro? EDIT: This one is even more confusing: &gt;src/entity.rs:14:24: 14:26 error: expected one of `.`, `;`, `?`, `}`, or an operator, found `NPC` src/entity.rs:14 return $t { name: name }; 
winapi is pretty comprehensive. I've used it for file I/O, socket I/O (winsock2 stuff), shared memory, and Windows services.
I thought about it but I don't think it makes a lot of sense currently. It assumes quite a lot about Sentry's API.
**TLDR:** Use `:ident` instead of `:ty`. **Boring pedantic crap:** the thing before the braces in a struct initialiser isn't actually supposed to be a type. I *think* it's supposed to be a path. Even if they look the same, they're not *internally* the same, so the compiler freaks out, screams "I don't know what you want from me!" and drops all the plates on the floor creating a right old mess. It's got very *delicate* sensibilities...
I am a scala expat to rust. I'd strongly advice you to start with understanding basic rust constructs before delving in fp aspects of it. Particularly I'd advice spending time with references, lifetime, borrowing, traits, trait objects , closures and iterators. By spending time, I mean repeated reading of the official rustbook. Thereafter, here are few posts that you might find helpful http://science.raphael.poss.name/rust-for-functional-programmers.html https://mmstick.gitbooks.io/rust-programming-phoronix-reader-how-to/content/chapter02.html https://mmstickman.wordpress.com/2015/01/27/functional-programming-in-rust-1-0-alpha/ http://www.lambdadays.org/static/upload/media/1456827608770660functionalreactiverust1.pdf http://cis198-2016s.github.io/schedule/ 
Thanks a lot. This was just what I needed. 
I just came back from GopherCon where a talk, multiple questions during the Q &amp; A panels, and a lightning talk all took place with vendoring and dependency management as the central focus. It's definitely a pain point in the community, though there seems to be some optimism around the official solution enabled by default in Go 1.6 .
&gt; i am bothered about what is repetitively implied today. If it is a curiosity kind of bother, I get it. I just did not get how Rust is seen negatively due to it. It serves the purpose it claims (absolute safety without compromising performance) better than any other language I have read about. How it evolved to serve it is of no concern, to me at least. PS: Sorry if I sounded rude before...
Yep - this Go was certainly discussed when talking about early iterations of cargo and rustfmt, as an example of how tooling can be a big boost to a language's adoption rate.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/golang] [\[X-POST\] Was Rust ever designed to be a Go competitor? • \/r\/rust](https://np.reddit.com/r/golang/comments/4ss406/xpost_was_rust_ever_designed_to_be_a_go/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Yeah, but those template errors, though. Not having a strongly-typed generics system is a deal-breaker.
Some Rust projects do not build with the musl target. Do you know why? Since 1.10 some do not seem to link and have an infinite link step at the end.
That doesn't seem to be a response? I don't understand. musl is a libc...
it's not the system libc, my system uses glibc but my program doesn't depend on it
Looks really good.
And template errors are better than macro errors! Still, while do you get one hairy error from time to time, in general the failure occurs in the first or second template instantiation so it's not so bad. The project uses C++03 (still...) and in C++11 it could have been made even more user-friendly. The key though, is that for strong typing we don't use *much* templates actually; mostly you need some instance of "strong typedef" to create new types based on existing ones cheaply and then a bit of magic for the validation layer but it's not that deep or complicated (just a layer or two of SFINAE). For deep magic, the experiment to bring compile-time reflection to C++ was *much* more scary. Heavy usage of both macros and templates; it's a beast that I personally find too complicated for real-world usage.
This type of compositional question (not unique to Rust!) is often addressed with an ECS: https://en.wikipedia.org/wiki/Entity_component_system
Don't believe everything that Heise writes. I often have the feeling they are the "Bild" version for IT news.
You can do a lot with FP in Rust using [`Iterator`s](https://doc.rust-lang.org/std/iter/trait.Iterator.html). There's also the [`itertools`](https://github.com/bluss/rust-itertools) crate which adds more useful methods onto iterators. However, iterators do require you to have a solid understanding of, at the very least, references, borrows, and types, before you can efficiently code with them. The links /u/conikeec posted should be helpful to you, as well as the Rust book. You'll also naturally get better the more Rust code you write, so creating a new project or contributing to an existing open-source project are great ways to improve your Rust.
Definitely get what you are coming from as I have felt that exact pain of getting to long declarations and not knowing where to put the type // This get really long let my_long_function x y z: LongType -&gt; LongerType -&gt; LongestType -&gt; Return = ... // But this looks weird let my_long_function x y z : LongType -&gt; LongerType -&gt; LongestType -&gt; Return = ... I am not really sold on Haskell's declarations either though as then you need to type the name twice which is rather redundant. let my_long_function : LongType -&gt; LongerType -&gt; LongestType -&gt; Return my_long_function x y z = They also complicate parsing somewhat (though not enough to make that a strong argument). Ocaml also allows types directly on the arguments which makes the declarations closer to Rust and C which might be an idea. // Not sure about the placement of `Return` let my_long_function (x : LongType) (y : LongerType) (z : LongestType) : Return = ... I would love to hear about any other alternatives though (or arguments for and against these kinds of declarations).
I think you'll find an improvement. I would be surprised if Java was faster than Go 1.7 for any task and Go would use much less memory. 
Great, thanks! Glad to hear it.
Nitpicking: I believe `?Sized` means "not necessarily sized". In that respect, every data type is `?Sized`. So, I'd prefer to say `Path` is *unsized*.
The most recent benchmarks I have at hand still use Go 1.6, unfortunately: http://benchmarksgame.alioth.debian.org/u64q/go.html ; of course, there probably is more than the language here (and Java itself can lag quite a bit behind C), but I have not analyzed them closely.
I've seen some very funny things happen when you reach into that internal list and change the cached value of zero to something else :-D
If you want to make it work *and* avoid unnecessary copies, /u/CryZe92 's suggestion to use `Cow` makes a lot of sense. See [this](https://is.gd/Zy0Xyb).
The only reason the macro works is because the impl happens to match with the types. As soon as something changes (e.g. `Player` has `hitpoints` instead of `hp`) it stops working. So it doesn't make sense to generalize this or build it into the language.
I'm using MSVC x64. I tried an MSYS2 installation, and then updated everything like the site told me to, and it crippled itself, deleting both of the toolchains it installed. I hate having to use Microsoft's compiler, but it's working better then MinGW =/
Ok well that right there says you have the 64-bit version of freetype-6.dll so my initial suspicion was correct. Uninstall all of your MSYS and MinGW installations and reinstall MSYS2 fresh from here: https://msys2.github.io/. Also use https://www.rustup.rs/ if you're not already. Should work (famous last words).
Like I said, MSYS2 crippled itself after I tried updating it. I'm already using rustup, it's so nice to be able to switch toolchains so easily =P
I have a suggestion. Instead of saying: &gt; I haven't kept up on recent improvements, but last I checked it was actually slower than Java. try saying: &gt; I haven't kept up on recent improvements, but last I checked Go was actually slower than Java in the benchmark game. It would save everyone a lot of time, and it would make your point (and its scope) a lot clearer.
&gt; still use Go 1.6 That is the latest major release. 
Usually if I build something with File I already have a Path if I still need it. If it's a library i would think it's to hide some of the functionality of it and the file it accesses, though I'm not sure what you want from it in this case. I would think that if you're requesting a file you want the handle so you can read write etc from it rather than know where it resides. Is there a reason you need the functionality? If so it might be a legitimate thing to bring up via RFC if enough people want this to be in the standard library as an option or as an option via trait.
Unfortunately, I value a consistent look and feel very highly, so that wouldn't be an acceptable solution for me.
&gt; I haven't kept up on recent improvements, but last I checked it was actually slower than Java. Actually, this comment of mine does not relate to the benchmark games: it was in a simple benchmark of a REST server with two simple services (one reading from a cache, the other from a database) that my company used to evaluate a number of languages for its own usecases (along Go and Java were C++, NodeJS and Python). The benchmarker was quite enamored with Go (easy concurrency and async I/O is very valuable for services, as you can imagine), but did not manage to get it to perform much better than NodeJS, and both were lagging behind Java (Python was, predictably, slower than all others). It was a while ago though (Go 1.1 or 1.2 maybe?), so I only have the benchmark games to evaluate a more recent version of Go; and I am fully aware that (1) those games depart significantly from Go's target market and (2) Go's performance has improved by leaps and bounds since. Then again, the comment I was responding to was: &gt; I would be surprised if Java was faster than Go 1.7 for any task and Go would use much less memory. which is something I personally do not know about. I would be surprised if Go started slower, but in terms of latency/throughput on various tasks it's not quite clear to me which of the two should come ahead. The benchmark games seem to reflect this vision as well, but unfortunately they tend to compare different algorithms so it's hard to judge from them :x
The reason for this is following. I use tmpfile library to create temporary file for tests (the functionality is fs dependent), but the tmpfile function returns Restult&lt;File&gt;, which does not contain the temporary file's Path which is what I need. Sure, one way is to update the library so it would return the Path and the File, but from my perspective (comming from Python) the File object should have the information (or the file Metadata), and surely this is not such a waste of memory since Windows for example still hold 256 characters absolute path limit :)
You're looking for /r/playrust; /r/rust is about the [programming language rust](https://www.rust-lang.org/en-US/). But actually, /r/playrust doesn't allow livestreams, so I think you're out of luck.
I installed mingw-w64, and it isn't working. It also isnt a problem with my config like I thought, as I'm still getting the error with MinGW 32. Any other advice that might help?
Not a huge waste of memory really, though I can see why some people wouldn't want it as part of the type. Namely as /u/stebalien puts it, the file path isn't guaranteed to be valid. Like most engineering problems it's about tradeoffs and I guess in this case the std library went against including the path with it.
Yes, I understand, the problem is there is NO way of getting the Path from File (even an Option if there may be none). I would understand it to be optional, but this way seems a bit unwieldy.
In this case, the `File` struct might be named in a misleading way because it's really a reference to a file on the file system and not an object that includes all the metadata associated with a file (which is what many other languages use in their standard libraries). `FileRef` or `FileDescriptor` might be better names for this functionality. In the end, this will help you avoid situations where you're writing to a file that doesn't actually exist (in other words, this is an instance of the standard library using Rust's type system to enforce correctness). This is a pattern you see all over the place in Rust, and it can take some getting used to when coming from other languages that tend to have much more stateful APIs in their libraries. So based on that I would say this is actually a bug or misfeature in the temp file library you're using, because it's effectively just giving you a file descriptor and not an actual path to a temp file, which is often what people really want. You could still get a path from the file descriptor, but then you have to deal with the possibility that it might not be valid. ----- I think an ideal temp file library would return it's own struct that lets you get a `Path` and `File` for the temp file. It might also automatically delete the temp file once the struct is dropped.
Yes, this is the function, I have a workaround with TempDir and creating the file myself, that is also a way. You mean the comment that on Unix systems the file is actually not on the filesystem. I overlooked that, thx. But anyway, wouldn't it be nice there was a way to get the Path back? Even if it would be the way it is in the Debug implementation from file descriptor, but on the File struct.
and that workaround function has the hilarious bug that it fails if the file is zero bytes, because it cannot be mapped into memory. Man, MSDN sample code sometimes...
Ok, thank you everybody for the responses. I'll use the workaround, this would not be a good idea to have in std library :) Thank you!
&gt; No central repository of packages locally, meaning that you might have to redownload something even though it exists somewhere else on the machine You can put multiple directories in GOPATH, and it'll pick up the dependency from the first place it finds it, so you can keep a central set of cached dependencies in (say) ~/go/ and still have per-project vendored dependencies. This isn't really mentioned anywhere as far as I can tell, and I only recently learned about it...
&gt; AFAIK everything is semantically a reference in Go You can pass structs by value in Go. I don't know whether they actually end up on the stack or whether that's optimized away with some sort of COW implementation, but semantically you can definitely pass a struct as a value.
hilarious :D
You don't want to do that in practice though, because support for long paths is still terrible, such as files having a path too long to delete using explorer...
It can be conditionally compiled, just having it in your dependencies doesn't mean it will be actually used for every features of your crate. See https://github.com/sunng87/handlebars-rust/blob/master/src/lib.rs#L266-L269 for example, in this case serde is not compiled if you aren't using the feature `serde_type`
*sigh* Heise. There's an podcast interview with me on that page where I said that Rust looked a bit similar to Go when it was first announced (it had channels baked in and a concurrent runtime). I mentioned that I had an impression that Rust has taken a different path as Go was already there and good, so there was no need to build something else. I never mentioned that it was intended as a competitor.
It would also require either tying the lifetime of the `File` to the lifetime of the `&amp;Path` you used to open it, or making a heap allocation to hold that path as a `PathBuf`. Right now files are just a single integer under the covers, so this would be a pretty big change in behavior that a lot of low level code would need to work around.
Not directly as there is no UWP API binding that I know of, however you can do it the usually way of embedding Rust in C#, exposing your Rust code via "extern C" and call it from C# via PInvoke. It worked for me the last time I tried it, albeit I only tried a few simple cases.
Thanks. I guess that should work for x86. Need to see about arm support for Windows 10 mobile
This works! thank's. Do you know whether there is a way to do this directly with a closure instead of function? If I change `filter` to a closure and the type argument from `fn(&amp;char) -&gt; bool` to `FnMut(&amp;char) -&gt; bool` the compiler complains about `Sized` not being satisfied. So I guess the size varies depending on closure, but can I somehow tell the compiler *look, the type argument is the type of this closure over here*?
Just out of curiosity, what kind of file wouldn't have a path?
Modular implicits seem like a really nice feature which I would really like (though the exact semantics may be a bit tricky to get right). I don't expect to implement them for some time though as I the complexity to usefulness ratio leans quite far into complexity. In the idea above is that not just how modular implicits work or am I missing some specific idea (I read through the Ocaml proposal a while back so i may be misremembering something)?
`FnMut(&amp;char) -&gt; bool` is the syntax for trait object which is not `Sized` (i.e. you always must access it through some kind of pointer like a `Box` or a `&amp;`). `fn(&amp;char) -&gt; bool`is a concrete type of a function pointer. To store a closure in a struct you need to make the struct generic over `FnMut` Like so, `fn new&lt;F: FnMut(&amp;char) -&gt; bool&gt; -&gt;Lexer&lt;Filter&lt;I, F&gt;&gt;`, but then you run into another issue. Rust is unable to name return types that are determined by the function body. All type paramters in a `&lt;&gt;` must be able to be infered by the caller. There was an [RFC recently merged](https://github.com/rust-lang/rfcs/pull/1522) to fix this hole in the type system. But I don't think its been implemented even on nightly. You can work around it by using concrete types like that function pointer /u/thiez used or by returning a trait object like so: impl&lt;I&gt; Lexer&lt;I&gt; where I: Iterator&lt;Item = char&gt; { fn new(iter: I) -&gt; Lexer&lt;Filter&lt;I, Box&lt;FnMut(&amp;char)-&gt;bool&gt;&gt; { Lexer { input: iter.filter(Box::new(|&amp;c| c.is_whitespace())) } } } 
What would you call real IDE?
Yeah, I décided to use radians over degrees even though SVG uses the latter. Applications using lyon to render actual SVG content will have to do the conversion (or perhaps I'll add something for that in the api if people ask for it). I think that working with radians is a lot saner at this level so I would rather expose degrees as an optional/secondary thing.
The nice thing about Rust iterators is that they don't create intermediate data structures - unless you call the method `.collect` (or for more flexibility, something like `.fold` IIRC). That is, if you have an iterator of the elements of a vector, when you call `.map` it returns another iterator, not a vector. Then you can chain a `.filter` and so on, and only at the end collect the resulting iterator into a vector. When you collect, the code will execute the whole chain in a single pass over the underlying vector. This is awesome. And the thing is, you can collect into other data structures as well, you're not limited into collecting into a vector.
Ah awesome, I've been playing with my own mirroring with the registry value in the cargo config but the local-registry is much nicer and doesn't need a web server so a nice improvement.
You'd have to do some stuff with RawFd and unsafe. I had too to do piping when I first started using Rust. It's probably why it's not available. Nothing that I know of in the std lib is marked as unsafe.
I can. It's just the freetype issues.
Thanks for the correction!
With `from_raw_fd` you can have a unix pipe, a standard input/output stream or even a TCP/IP socket wrapped in a `File`.
I'm pretty sure that `less` just reads from stdin. 
Yeah, but all of those have a path *somewhere*, like in e.g. `/proc`
Only on Linux (if proc is even mounted).
If a file is deleted while open it would still have at least one path(on Linux) just not the original path : /proc/{pid}/fd/{File#}
For some reason using the subreddit search for Corrode, this didn't come up. Only came across when trying to submit a link myself :P Looks like an awesome project, curious if it'll be helpful with embedded libraries like arduino and mbed. While I hear arduino is mostly C, it's got some C++ in it where I guess Corrode would have a problem?
Files returned by `tempfile` don't *have* usable paths (by design): * On Windows all files have paths. However, the path isn't usable because I lock the temporary file on creation and immediately mark it delete-on-close. * On Linux, I create the files without ever naming them (Linux has so many nice little features...). * On other *nix operating systems (e.g., MacOS), I unlink the file from the filesystem immediately after opening it. The file remains valid, it's just unnameable.
Yeah because this is just C to Rust. C++ is it's own level of complexity to convert.
I'd be pretty happy if I could just interface with arduino/mbed libraries and write my own code in Rust. Made quite the post a week or so ago on this subreddit but wasn't able to get much responses. Is it difficult to port my C code to Rust and use these libraries that are compiled to an embedded platform? I'm mostly experienced with higher level languages, I'd like to use Rusts language features which would be super helpful, if it's possible to do the opposite of Corrode and transpile Rust to C that'd be a great solution?
So much fear to change :|
Thank you for all the great answers!
I didn't know what RPM meant, here it is https://en.wikipedia.org/wiki/RPM_Package_Manager. My brain mistakenly read it as NPM and I was trying to figure out whether the blog writer thought to use it because it was better than cargo in some way, or because they wanted to use it in a node project or something :P
Awesome. Thanks!
Is there a framework or good place to start for doing source to source transformations of Rust code? For example, suppose I wanted to change mut-&gt;const wherever possible.
Oh, I see. Well, Rust, currently lacks full "real IDE" from your definition, and the best bet would be to use Atom or VSCode with plugins (linter-rust, racer, rustfmt for Atom, RustyCode for VSCode). There is a development of plugin for IDEA, but it's far from being ready to use.
What do you mean? I thought it was the "default" web framework in Rust. Do you think it is dying? Should I use another non-stagnant one?
Hi Nicolas, glad to see your work shaping up, keep up! :)
Wasn't vala designed specifically for creating gtk+ apps? I thought the integration with GObject was on purpose..
We're in the process of writing automatically-generated bindings: https://github.com/Boddlnagg/winrt-rust/ I think I'll be able to publish something on crates.io soon.
One problem is that the full set of bindings results in a ~12MB, ~150k lines Rust source file, which takes a few minutes to compile. I think we need to find a way to split that up.
We're doing just that https://github.com/Boddlnagg/winrt-rust/. It's heavily work-in-progress and severly lacking documentation right now, however.
&gt; I have spent a few weeks reading different posts in here and dotted around online, and heard a few people mention Rust can be quite a hard language to learn. I'm not sure Rust is such a hard language to learn, but the ownership and borrowing system is hard-checked by the compiler, if you've got a somewhat antagonistic view of compilers that can be a bit hard to swallow whether you come from a higher-level language (different semantics so constructs you're used are either invalid or more finnicky in Rust) or a lower-level one (due to compiler and type-system limitations Rust will reject or provide little way to express constructs you know are safe[0]). All in all, I'd recommend giving it a shot, if it turns out too frustrating you can just bail. [0] though I've also seen commenters find that it was a hubris-check and that the constructs had not, in fact, been safe. I don't think that's a majority of the cases and there are well-known limitations/annoyances of the type system when it comes to collections or to carrying both an object and "quick access" references to that object.
Learning Rust is not hard but with current resources it can be harder because most are not written for beginners. Even on Rust doc some code samples are more complex especially for an absolute beginners. we have to give simplest examples to easier to understand the concepts. Btw have you checked http://cis198-2016s.github.io/schedule/ ? 
When it is said that Rust is hard it's often because of the constraints it imposes to guarantee memory safety. You will often hear the borrow checker and lifetimes causing some trouble. I personally find the borrow checking rules very intuitive, lifetimes on the other hand are a bit abstract to learn at first but with time you will get the hang of it. In the beginning (and later too) you will get a lot of compilation errors. Don't fear them, they will be your friends as most of the compilation errors in Rust are of very high quality and you will often learn new things by reading them. I think Rust is one of the best programming languages to ease you into systems programming. It will not throw you in a sea of pointers where you either swim or drown. It has a very high level feel, you can get away with only using the high-level part of the language for a pretty long time and when you feel comfortable you can reach lower. I would strongly encourage you to give it a try! :) The [Rust book](http://doc.rust-lang.org/book/README.html) and the [next iteration of the book](http://rust-lang.github.io/book/) (draft) are very comprehensive. There is [rustbyexample](http://rustbyexample.com/) for complementary code snippets. If you have any questions, don't hesitate to ask them here, on irc or in the [Rust forum](https://users.rust-lang.org/)
many thanks. so I have plenty of time here to learn, so that suits me, and I will be approaching as a hobby, rather then for a career. For me if I could get some useful projects on github, then that is win for me. Going to give it a go, something my bones is really warming me to rust - I don't know exactly why as yet, so its nonsensical, but just a hunch I will enjoy the language.. &gt; P.S. My podcast might be of interest on point (2), depending on your learning style. on it! listening to hello world now. 
I had not seen that, so bookmarked and will go through it. thx!
thx! will read over the official book!
If `r2d2_redis` re-exported `r2d2` and `redis`, then how would have you updated `redis` to `0.6` while `r2d2_redis` continues to depend on `0.5` of `redis`? To me, it seems like things are working as would be expected, perhaps with a bad failure mode. Think about what you're trying to do: you're trying to use `redis 0.6` in combination with a dependency that requires `redis 0.5`. In Rust's crate land, Cargo treats `0.5` and `0.6` as semver incompatible, which is essentially the same as how, e.g., a `1.0` and `2.0` would be treated. The right answer here is that `r2d2_redis` has to be updated to support `redis 0.6`, even if "to support" just means fixing the dependency constraint. Having this automatically update isn't tenable, because `0.6` *could* contain a breaking change that causes `r2d2_redis` to stop compiling.
You could use a procedural macro (or even a rules-macro) to do this, if you are OK with annotating your code. Otherwise [syntex](https://crates.io/crates/syntex) should do what you want (mostly) on an AST level.
For now I published a new version of r2d2_redis to require redis 0.6. I will think about the other option.
For those like who didn't even know [what Vala was](https://en.wikipedia.org/wiki/Vala_(programming_language)). I have some doubts wrt. its memory management strategy: &gt; For memory management, the GObject system provides reference counting. In C, a programmer must manually manage adding and removing references, but in Vala, managing such reference counts is automated if a programmer uses the language's built-in reference types rather than plain pointers. Would someone know whether it's got a cycle detector/collector? Otherwise, I am afraid it'll be easy to leak.
I don't want to do compile time expansion. I want to build a tool that takes in Rust source code and outputs Rust source code. Sort of like Rustfmt.
If you are using msvc, I really recommend following the instructions I provided in the [`freetype-sys` readme](https://github.com/PistonDevelopers/freetype-sys#-pc-windows-msvc).
The big advantage of vala is that you can also write gobject library, thus avoiding writing C code. With python, you can only use gobject libraries. Rust may be appropriate to write gobject library.
[#247 is still not fixed](https://github.com/rust-lang-nursery/rustup.rs/issues/247) this is troubling. [to quote myself](https://github.com/rust-lang-nursery/rustup.rs/issues/247#issuecomment-219221204): &gt; exactly that’s what always happens and what frustrates me. decisions like that are implemented ad-hoc, the fixes put on backburner, and once people would have time for it there’s suddenly inertia in the wrong direction and people have built tools relying on the hacky ad-hoc behavior that was never meant to last. &gt; we have to switch now or we will do it never. &gt; and to be honest, we should have thought about that from the inception. 
As someone who is essentially learning Rust as their first real language (with some toy experience in other languages), let me just say that the most critical part of learning a language is drive. If you really want to learn a language, you can do it. Even "hard" languages like C/C++ or even Assembly can be learned if you have the determination to do so, you just have to go in properly prepared for the learning curve. You're probably not going to pick up Rust as quickly as you did Python, but that doesn't mean Rust is too hard. When you really think about it, there's not much else you can do beside keep pushing yourself. The best way to get experience coding Rust is to spend a lot of time coding Rust. Maybe some concepts can be learned easier in another language but you still need to figure out how Rust implements them. The community will help, google searches help, the Rust book will help, but the ultimate decider in if a language is too tough is your own will to press on.
What's with people referring to [Vala](https://wiki.gnome.org/Projects/Vala) in the past tense, and calling it a failure? Looks like it's still actively developed to me. I'm not sure what it's userbase is.
I was working on Rust's current lineage from 2006 onwards. Mozilla decided to give me permission to work on it in fall of 2009, and right around the same time the Go team announced their initial release. I do not believe either was motivated by the other; though I was certainly surprised when Go emerged, and I'd studied many Pike languages in the past (and cited them in the initial Rust manual). Tbh I read Pike's ca. 2000 postmortem on Alef (Plan9's initial systems language): &gt; although Alef was a fruitful language, it proved too difficult to maintain a variant language across multiple architectures, so we took what we learned from it and built the thread library for C. ... and figured that he didn't think any more languages were needed in the space; after all, if you've got no type-system or runtime-system support for memory isolation, then "channels" really are just a threading library primitive: waitable queues. If I'd known he was going to do another, I might not have started on Rust. I always liked his languages. It's worth recognizing that developing languages, and then committing to supporting them publicly (much less using internally) is a very ... delicate thing, and a lot of organizations aren't entirely clear whether they want to do it -- both in terms of funding or market position -- until relatively late in the game, often well past proof-of-concept. The pros and cons are ... far-reaching. So seeing a new language emerge "suddenly" without much warning, or for that matter seeing an organization incubate a language most of the way through only to have it get cancelled or folded into a set of enhancements on another, is not terribly unusual.
Wait, if people are refcounted, does that mean I can clone them and they'll stay alive?
Sounds superficially similar to Swift...
Ah woops my bad. I'm not sure what the state of communicating with those libs would be but FFI is probably your best bet, though that would work in terms of a computer communicating to the Arduino. As for being on it itself, like I mentioned it will have to get compiler support. I've never heard of using IR to turn it back into source, just to binary. Especially C. I'm definitely interested if you have seen a project that does that
Replying to myself; it seems that I missed that the trailing hash reduces conflicts. I'm still not sure it's a great idea to follow C++ but it's not as bad as I thought. It's a shame that this perf work wasn't just upstreamed to BFD; but maybe I can reimplement it there.
&gt; we have to switch now or we will do it never. I've got to say I can only hope for never, in my experience platform-specific configuration directories of CLI tools is a giant pain in the ass yet almost completely worthless, the only pushers seem to be BDS lovers who don't actually give a damn about about platform-specific directories and just spread BDS everywhere (witness [#cargo/2127](https://github.com/rust-lang/cargo/pull/2127) where all unices (except OSX) get switched to XDG Base Directory paths despite the vast majority of them — including most linux distributions — not caring for it or following it). It wouldn't be so bad if XDG were opt-in, but no they just went and asserted that every system out there must have an `$XDG_RUNTIME_DIR` and/or have every single application print a warning, just make stuff up along the way (I mean "If `$XDG_CONFIG_DIRS` is either not set or empty, a value equal to /etc/xdg should be used"?) and refused to follow existing conventions (why do caches default to `~/.caches` and completely ignore [/var/caches](http://www.pathname.com/fhs/pub/fhs-2.3.html#VARCACHEAPPLICATIONCACHEDATA), so your backup script needs more exclusion rules to avoid backing up worthless application caches?). Hell, you can't even shut up XDG and rollback to previous behaviour: setting `$XDG_CONFIG_HOME` and `$XDG_DATA_HOME` to `~` (because you're just fine with what's always been done on unices) will make the worst days of My Documents look tidy by comparison.
very cool!
Your feelings about this seem quite heated, so I'm reluctant to respond, but I want to ask some clarifying questions. &gt; It wouldn't be so bad if XDG were opt-in, but no they just went and asserted that every system out there must have an $XDG_RUNTIME_DIR and/or have every single application print a warning, just make stuff up along the way This isn't consistent with the behavior of any program I uses that follows XDG, which silently use XDG default directories, because I have not set any variables. Does the PR you linked include issuing a warning? That does not sound like the correct behavior. &gt; and refused to follow existing conventions (why do caches default to ~/.caches and completely ignore /var/caches, so your backup script needs more exclusion rules to avoid backing up worthless application caches?). The point of `~/.cache` as I understand it is that applications are already storing cache data under the home directory to avoid requiring setting up write access to a directory in `/var`, but we want to only have to set 1 exception instead of 1 for each application with a cache. cargo currently stores its registry cache in `~/.cargo/registry/cache`, how is that better? &gt; Hell, you can't even shut up XDG and rollback to previous behaviour: setting $XDG_CONFIG_HOME and $XDG_DATA_HOME to ~ (because you're just fine with what's always been done on unices) will make the worst days of My Documents look tidy by comparison. Is this comment just about the convention that under `.config` and `.cache` these files are stored in e.g. `cargo` and not `.cargo`?
Mostly because XDG is only really a standard on not even sure, Linux? In any case on my Mac depending on the tool I need to find it's config files in `Library/Application Support/X`, `Library/Preferences/X`, `.config/x` or `.x`. Then when I go to Linux I need to look in two places again and on Windows it becomes a wild mess. I have seen `C:\Users\armin\.config\x` which was never a standard to begin with. I really expect developer tools to just put their stuff into one consistent location no matter which OS I am on. It's especially frustrating when some tools change their opinion later and move from `~/.x` to `~/.config/x` or something similar. As far as I'm concerned they are all the same. I don't see a significant benefit of one over the other. However I do see a massive downside of having to consult documentation to figure out where configs are for the same tool on a different platform. This is particularly true if you need to access config files from other apps. For instance I regularly open `~/.gitconfig` from my own scripts to find some git config data. Now imagine I need to handle all the places where git might put the data if it would follow XDG, windows, osx and god knows whatever else convention.
Is there any particular reason `String` implements `Add` but not `AddAssign`? Edit: https://github.com/rust-lang/rust/pull/34890
Yes, and some of the more recent GTK applications - mostly for gnome - are actually written in it. And if I recall correctly, it adopted newer features of C# as async. But it's still is a painful experience. At least it has been for me, last time I tried it. For example, if you want an extended version of some GUI elements you sometimes can't simply inherit from that but have to go through a little documented mechanism I can't recall correctly. However, the biggest problem IMHO is the lack of documentation regarding GTK itself. It is baffling how little the more recent widgets and deprecations of older widgets are documented. And then there is just pure madness, like applying a CSS-like style class to combine buttons.
That's not really the same thing and sure you could just return /proc/&lt;pid&gt;/fd/&lt;fdnum&gt; but that doesn't really help for many cases including op's.
the RFC has [only positive reactions](https://github.com/rust-lang/rfcs/pull/1615). &gt; despite the vast majority of them — including most linux distributions — not caring for it or following it). care to elaborate what you mean? how can distributions follow it or not? &gt; It wouldn't be so bad if XDG were opt-in, but no they just went and asserted that every system out there must have an `$XDG_RUNTIME_DIR` and/or have every single application print a warning if you use the runtime dir, and use a certain lib that handles those dirs, yes. if you don’t use the runtime dir (like in many applications’ cases), or use some other lib or your own implementation, you can just choose your own default runtime dir and not warn. &gt; setting HOME as your XDG_CONFIG_HOME and XDG_DATA_HOME will make it literally unusable. just leave it as default and be happy that almost all config is in `~/.config`. that’s what i do, and i’m only annoyed by apps that are younger than bash and still decide to create their own special snowflake `~/.${appname}{/,rc}`.
IIRC, the AddAssign trait wasn't designed when String, in a last-minute PR before 1.0 gained the Add impl. Presumably a PR adding the impl could change this.
if mac has certain conventions, obviously nothing should put stuff into `.config`. but that’s the beauty of XDG: even if your applications are not that smart, you can simply set the env variables to make them behave. no such luck for bullshit in `~/.imaspecialsnowflake` git btw is especially cool since it simply checks `$XDG_CONFIG_HOME/git/config` and (if not found) `~/.gitconfig`
&gt; if mac has certain conventions, obviously nothing should put stuff into .config. but that’s the beauty of XDG: even if your applications are not that smart, you can simply set the env variables to make them behave. Except actually doing that will not work because of how environment variables for for different types of applications on OS X. In particular apps launched through Finder will not see any envvars. &gt; git btw is especially cool since it simply checks $XDG_CONFIG_HOME/git/config and (if not found) ~/.gitconfig And plenty of tools will break if the config is not in `.gitconfig`.
&gt; In particular apps launched through Finder will not see any envvars. there has to be a way to globally set them. i mean, sure, on my KDE system, i have to create a file in ~/.config/autostart or so to get it to run in a context that’s inherited by everything, but it’s *possible* &gt; And plenty of tools will break if the config is not in `.gitconfig`. how crappy of them.
&gt; the RFC has only positive reactions. Not everyone always weighs in right away. I'm fairly negative, but haven't left a comment yet.
I'll just chime in and say that Lua lets you overload using square brackets (`x[i] = y` and `y = x[i]`) using the [__newindex](https://www.lua.org/pil/13.4.2.html) and [__index](https://www.lua.org/pil/13.4.1.html) metamethod respectively, which are also used for method/field access. It allows similar overloading of all the operators and whatnot, just like python.
go go go.I can, you can.
To learn Rust I'm writing an app that grabs a list of movie or TV Show files in a directory, creates hash values for them to be used for searching for subtitles on opensubtitles.org. It uses four worker threads to do the job consuming the file list from a shared vec,- perhaps overkill but I love playing with threads and synchronisation and it's so easy in Rust. Last bit is the RPC bit and as I realised just now; rust-xmlrpc hasn't been updated for two years and doesn't compile.
It's worth mentioning that Go has a win above Java in a couple of those test categories. And in the past tests there were also times when Go was on top of all the other languages. It really flip flops as languages issue improvements to their core and 3rd party libs. 
I would much rather have this junk die, and XDG is the most promising way to get there, without inventing an [n+1 standard](https://xkcd.com/927/): # MacPorts Installer addition on 2009-09-11_at_08:45:35: adding an appropriate PATH variable for use with MacPorts. export PATH=/Users/lambda/.cargo/bin:/Applications/git-annex.app/Contents/MacOS:/usr/texbin:/opt/local/bin:/opt/local/sbin:/Users/lambda/bin:$PATH:/Users/lambda/src/chrome/depot_tools:/Users/lambda/android-sdk/tools:/Users/lambda/anddroid-sdk/platform-tools # Finished adapting your PATH environment variable for use with MacPorts. export PATH="$HOME/.rbenv/bin:$PATH" eval "$(rbenv init -)" ## # Your previous /Users/lambda/.profile file was backed up as /Users/lambda/.profile.macports-saved_2014-12-15_at_01:59:44 ## # MacPorts Installer addition on 2014-12-15_at_01:59:44: adding an appropriate PATH variable for use with MacPorts. export PATH="/opt/local/bin:/opt/local/sbin:$PATH" # Finished adapting your PATH environment variable for use with MacPorts. Also, this junk: $ ls -a ~ | wc -l 273 `Library/Application Support` is only really appropriate for desktop applications, I'd prefer to just have consistency between unices by using XDG rather than adding an extra level of fragmentation. And of course, besides all of this, without something like XDG, having `$HOME` be on a networked filesystem just becomes either unreasonably slow as temporary files and caches are stored on the networked filesystem, or a huge pain to manage as you have to figure out application by application which files to put where and add symlinks or override on an application by application basis where they look. So, what problem does XDG cause that outweighs all of the pain points mentioned above that it can solve, or at least help solve as applications move over to it?
&gt; For instance I regularly open `~/.gitconfig` from my own scripts to find some git config data Now imagine I need to handle all the places where git might put the data if it would follow XDG, windows, osx and god knows whatever else convention. Why are you reading `~/.gitconfig` instead of `git config &lt;name&gt;` or `git config -l`? The latter will pull from the appropriate place on all platforms, including handling per-repository config, global config, and so on.
&gt;Basically that: &gt;1. Dependencies are specified as imports On the plus side, it makes source self describing instead of needing a json or xml or makefile recipe &gt;2. Dependencies can be (and, in my limited experience, frequently are) specified by Github URL e.g. `import "github.com/user/stringutil"` This really only pertains to support provided by the Go tool and not the language spec. Projects are go-getable from sites that can report some metadata in their http response. But these import locations also fallback to a disk location relative to GOPATH &gt;3. Dependencies are cached locally in a directory specified by the $GOPATH environment variable Correct. But most Go developers use a single GOPATH which means the previously installed package is reused. The project specific stuff can further be isolated by vendor directories. &gt;4. You have to "go get" packages to install them This is actually false. All go get is doing is using some logic to determine where to clone a repo from if it is not already in your GOPATH. You can simply place any project into your GOPATH without the convenience of go get &gt;This is kind of a mess for a few reasons: &gt;1. No central repository of packages locally, meaning that you might have to redownload something even though it exists somewhere else on the machine As mentioned above, Gophers tend to use a single GOPATH (+ optionally a vendor location for a project) &gt;2. If a repository changes (e.g. if a library makes an incompatible change), there's no way I've seen to specify a previous version. You always get the latest version, which can be great until you have to update your app in order to compile it at all because of some backwards-incompatible change in the master branch. Again this is a tooling concern. The core go tool took the approach of not solving dependency versioning and just work with master. The community has been solving this problem via dependency managers. These track individual commits or tags and the solutions have been converging over time as the community reviews the success of each experiment or approach. That was the stated goal of the Go core team as well. &gt;I'm not sure if any of these kinds of issues have been fixed (I don't think so), and none of them are showstoppers by any means, but it just always feels like a neat idea, poorly considered. Well it does sound like this review was performed on a version of Go close to the 1.0 release. Or without much insight on what the community is doing. Might be based on just the go tool 
&gt; (why do caches default to `~/.caches` and completely ignore /var/caches, so your backup script needs more exclusion rules to avoid backing up worthless application caches?). Because `/var/cache` is owned by `root:root` and is `u=rwx,go=rx`, and any subdirectories, if made world writable, would be subject to all of the security issues that a globally shared `/tmp` is. `/var/cache` is the place for systemwide daemons to store their caches, while `~/.cache` is the place for per-user applications to store their cache. I'm not sure how avoiding XDG compliance would help with the "more exclusion rules" problem. Without it, you have to add exclusion rules for things like `~/.cargo/registry`, `~/.fontconfig`, `~/.ri/cache`, `~/.darcs/cache`, etc, etc (and this is just a few random caches I see looking around my home directory). With it, you just have `~/.cache` to deal with. Sure, if only one application used it, then it would be an [n+1 standard](https://xkcd.com/927/), but taking a look at one of my Linux systems, there are a lot of apps that do use it that you'd otherwise have to use individual exclusion rules for: lambda@gherkin:~$ ls ~/.cache/ bijiben-shell-search-provider chromium dconf event-sound-cache.tdb.272459208364d0fc1e57dea852e935d6.x86_64-pc-linux-gnu evolution folks fontconfig gdm gegl-0.2 gegl-0.3 geocode-glib gnome-calculator gnome-control-center gnome-software goa-1.0 gstreamer-1.0 icedtea-web libgweather lxc mc media-art mozilla pip pocket qt_compose_cache_little_endian_272459208364d0fc1e57dea852e935d6 sessions shotwell smuxi telepathy thumbnails totem tracker vinagre virt-manager webkit webkitgtk wocky yelp I don't feel particularly strongly about using XDG universally or across unices for development tools, versus using platform specific directories like `~/Librarary/*` on OS X, `AppData` et. al. on Windows, etc. But I do feel very strongly that applications should use either the platform specific dirs or XDG, and stop littering the top level of my home directory with more config files, cache directories, their own special snowflake bin directories that they munge my `.profile` for or beg me to do so upon install, etc.
I don't see (or think Go team ever defended) that Go wanted to replace C++ directly. It was built for specific problems Google had faced over the years and was created to write servers. Certain problem spaces were better suited for Go than C++ and it was more a situation of people from whatever language wanting to solve problems in Go. Not necessarily language X problems in Go.
`rustc` (version 1.7.0) is complaining that `AddAssign` is unstable, but the docs seem to suggest that it is stable. What's going on? The exact error is: &gt; use of unstable library feature 'op_assign_traits': recently added (see issue #28235)
Oh, of course, I totally understand why you can't get a path back from a file. I just read this comment and couldn't think of a file type that had no path at all, so I was curious if there was some kind of file type that I didn't know about.
Since people asked, no, this isn't Chapel's model (or at least wasn't when we wrote this paper). Instead it was a proposed model based off the Cyclone work.
Wrong subreddit. 
I am currently writing a library implementation of [openpixelcontrol protocol](http://openpixelcontrol.org/) in rust: https://github.com/latrasis/opc-rs. With Open Pixel, there is a `Message` Packet Format, so for Encoding and Decoding I first thought I should have used `serde` or `rust-serialize`, however I realized _serialization_ is different then _encoding_, which got me confused. Since this is my first time writing a packet protocol, I was wondering if there is a formal library pattern to encode and decode a specific data packet? Do you provide a `Client/Server`? A `Reader/Writer`? Do I use a Trait? For example, the [golang implementation](https://godoc.org/github.com/kellydunn/go-opc) uses `Client` and `Server`. But understanding further they are just a custom implementations of a `Writer` (Client) and `Reader`(Server) ? Another question, how should a decoder handle errors? What if a single byte is offset, would this invalidate the whole stream?
Because all traits that can be used as object types [automatically implement themselves] to do dynamic dispatch through to the dynamic type of the object (in this case, bool). The `self` parameter of `Example::report` has type `&amp;Self`, so an `&amp;Example` can be passed directly as `self`. Rust always prefers the implementation which allows it to directly pass the receiver value without any additional referencing/dereferencing, so it picks the automatic impl on `Example`, which dynamically dispatches to the impl on `bool`. If you add an [extra level of indirection], Rust will pick the impl on `&amp;Example` instead. Interestingly, the automatic impl of a trait for the corresponding object type [seems to do a form of specialization] even without specialization being enabled. The object type `Other` shows that the blanket impl of `Example` applies to object types, and yet the automatic impl of `Example` for `Example` is not considered to conflict with the blanket impl. The automatic impl of a trait for the corresponding object type therefore seems to be able to specialize from a blanket impl. [automatically implement themselves]: https://play.rust-lang.org/?gist=e7ccaa7fdb0a49a145a86777aec19a6e&amp;version=nightly&amp;backtrace=0 [extra level of indirection]: https://play.rust-lang.org/?gist=19e8a2071d52ad3f09a6aa21bda4cc5c&amp;version=nightly&amp;backtrace=0 [seems to do a form of specialization]: https://play.rust-lang.org/?gist=cb3ea1a983c0ab357ab90d5da8d8ccf9&amp;version=nightly&amp;backtrace=0
I don't know anything about rust specific conventions, but the following may help. In general, use Client/Server for the structs that implement the client/server logic that isn't related to encoding/decoding. The client and the server often both need to encode and decode packets, so it doesn't make sense to apply just one of those labels to them. Encode/decode and serialize/deserialize usually refer to converting the packet to/from a byte array, without actually sending/receiving it. They may interact directly with a Reader/Writer though. eg you can implement encoding by using write functions on a Writer (and you may just call the entire process writing in that case). For OPC, it looks like it is a simple unidirectional protocol, so either Client/Server, or Reader/Writer would make sense. For error handling, if there is a well defined protocol then return an error for anything that doesn't match the protocol. Don't try to do error correction unless the protocol defines how to do that. For OPC, I don't see any way that you can reliably recover from errors.
I don't know much about opc, but if there are identifiable individual messages which need to be encode and decoded, I'd definitely look into nom (https://github.com/Geal/nom), or another parser combinator crate (though I personally only have experience with nom). It allows for making, through smaller parsers, a parser to read any kind of data into a usable rust structure. Nom only does decoding, but I've found encoding for most formats fairly easy to write by hand. You might also be able to use serde with this, but in my understanding it's more for serializing lots of different classes in one defined object format, rather than parsing specifically formatted messages into specific data structs. As for data errors, I'd probably go with an invalid byte in a message invalidating that message (up to the end of the message as defined by the message header) , and an invalid byte in the message header (I think OPC has those) poisoning the entire stream.
Good idea! BTW, why there is `Object(Box&lt;HashMap&lt;String, Json&gt;&gt;)` instead of `Object(HashMap&lt;String, Json&gt;)`?
That would be awesome!
&gt; the RFC has only positive reactions. Of course it does it's an XDG pile-on, people who don't care for XDG aren't going to obsessively look for XDG issue to express disapproval, and dissension in pile-ons isn't generally met with kindness, witness this very thread with /u/the_mitsuhiko at -4 and mine hovering between 0 and 1 (ah make that -1 right now). &gt; care to elaborate what you mean? how can distributions follow it or not? By following it for the tools they provide, providing XDG's wackiest paths (`/etc/xdg`, `$XDG_RUNTIME_DIR`), setting XDG's envvars and publishing paths guidelines to that effect as [Arch](https://wiki.archlinux.org/index.php/default_applications#Default_mimeapps.list_files) [does](https://wiki.archlinux.org/index.php/XDG_Base_Directory_support). &gt; you can just choose your own default runtime dir and not warn. So are applications supposed to follow XDG or are they not? Because `$XDG_RUNTIME_DIR` is part of the BDS, why should they support `$XDG_CONFIG_HOME` and ignore `XDG_RUNTIME_DIR`? &gt; just leave it as default and be happy that almost all config is in `~/.config`. I don't want that, why should I be happy about it? &gt; that’s what i do I'm real happy for you but I don't care for XDG's guidelines and my main systems don't follow it.
&gt; This isn't consistent with the behavior of any program I uses that follows XDG Then they don't actually follow XDG. &gt; which silently use XDG default directories There is no default `$XDG_RUNTIME_DIR` (and of course that XDG defines default dirs in case of *missing* envvars is one of my issues with it, XDG isn't opt-in and you can't opt out of it either). &gt; That does not sound like the correct behavior. It's the behaviour *specified by XDG*: &gt; If $XDG_RUNTIME_DIR is not set applications should fall back to a replacement directory with similar capabilities and print a warning message. &gt; Is this comment just about the convention that under .config and .cache these files are stored in e.g. cargo and not .cargo? It's not "just" about that convention it is specifically about that behaviour of XDG implementations which means there's no way to opt out of XDG.
I believe the term you're looking for is "refinement types", if you're looking for more information on the topic. It's already possible to write types which add constraints, it's pretty verbose though, and definitely not as nice as proper refinement types would be.
did you mean _encoding_ and decoding? edit: from the body it seems to be obviously the case.
I always like to look at other slides on the same topic to see what I want to copy or do differently. E.g.: Members of the core team have given introductory talks about Rust at some conferences, [here](http://rustaceans.cologne/archive/) are the talks (some with slides/video) from the Cologne meetup, and [here](http://rustaceans.cologne/assets/20160203-rust-with-confidence.pdf) are some slides I made in February (feel free to use/copy those!).
Thanks for the name. From name, I got this years old link about typestate in Rust, which was abandoned- http://pcwalton.github.io/blog/2012/12/26/typestate-is-dead/ What I think I have found here(with both typestate and branding pattern) is, the constrain is really part of type system. So, runtime also knows the constrain within the type. But I was anticipating something where the constrain is not really part of type-system. It will be used just in compile time to enforce type integrity but will be erased at runtime. It like enforcing the code to check if a file is writable before actually writing to the file, if the write function takes a file with isWritable constrain. Compiler here will not try to determine whether constrain is satisfied(the opened file is writable) or not. It will enforce the programmer in form of code. But at runtime, only the data remains. It moves the valid parameter checking responsibility from function designer's end to caller's end.
No, because the methods were removed from PathExt. If there were actual conflicting methods, it would have been a compile error.
&gt; I mean that you can choose your own default runtime dir if the env variable isn't set I never said otherwise, but according to the XDG-BDS spec you're supposed to display a warning in that situation: &gt; If `$XDG_RUNTIME_DIR` is not set applications should fall back to a replacement directory with similar capabilities and print a warning message.
ah, OK
He should feed the sqlite.c source code and create a purely rust version
You could make a system where the compiler complains if it can prove that it won't work, behaves normally if it can prove that it will work, or insert runtime checks if it can't prove either way. That goes against Rust's explicitness, though.
WIP, but there's https://github.com/killercup/rustfix
The short answer: no The long answer: It's not that easy to make something that fixes things based off the error message. There's some effort being made in the Rust ecosystem. /u/killercup is working on a project called [rustfix](https://github.com/killercup/rustfix) to do what you would like. Compared to Scala the Rust ecosystem is immature. It's only been stable for a year. At this point in time you're going to have to learn what the error messages mean and fix it yourself. Honestly that's not a bad thing. It forces you to learn the language and you'll learn from your mistakes. There's effort being put in to make the error messages better as well so it'll be easier to parse and understand for new users. Sorry that this isn't the answer you'd like to hear but if you learn this stuff now you'll not really need that quickfix tool.
The language Clay had something like this. It was heavily reliant on generics (used whole program type propagation), and it had so called [pattern guards](https://github.com/jckarter/clay/blob/v0.1.0/doc/language-reference.md#patternguards). Mostly a curiosity now, but I really liked the look of that feature in the language. It even supported overloads based on the pattern guards.
maybe it sounds like this proposal on Π-types(dependent types)? https://github.com/rust-lang/rfcs/pull/1657 https://github.com/ticki/rfcs/blob/pi-types/text/0000-pi-types.md
Syntactically it's almost identical to Swift, right down to the `weak` keyword. However the ObjC runtime's ARC support can eliminate redundant inc/dec counts, so Swift's implementation has less runtime impact. In fact Vala is incredibly similar to Swift: even the error handling is pretty close. As a C# clone built on top of GObject, it's excellent. Unfortunately the world is moving towards ML style languages, so it's getting left behind slightly, and the Vala team seems to be pretty small, which is unnerving.
I think you've received downvotes because you're expressing yourself aggressively (and mitsuhiko even more so), which is making this conversation unpleasant to participate in.
I think Rust might have made the wrong choice here; it doesn't seem good that a backward compatible upgrade can silently switch the dispatch from _your_ method to a method defined in the library, which could have significant semantic differences. Unfortunately, it would be a breaking change, possibly a significant one, to make method dispatch raise an error in this case.
I think you are on to something though. I had a fleeting idea of something similiar at the same time as I was removing type classes. I was coming from the perspective of having OCaml's optional arguments on steroids. let f ?optional mandatory = ... f mandatory f ?optional mandatory But optional arguments wouldn't just be of type `Option a` but they could be of any type and when they are left out the compiler looks for an implicit binding with the correct type. let implicit ord_int = ... // Ord Int let implicit ord_float = ... // Ord Float let ord_string = ... // Ord String let ord_list = ... // ?Ord a -&gt; Ord (List a) let sort ?ord xs: ?Ord a -&gt; List a -&gt; List a = ... sort int_list // Selects ord_int sort float_list // Selects ord_float sort string_list // Could not resolve implicit sort ?ord_string string_list // ord_string given explicitly sort list_string_list // Selects (ord_list ord_string) The last one would need to call a function to actually create the instance which feels a bit icky but it may work well enough in practice. I think this is pretty similiar to what you had in mind though coming from the other direction and I don't presume to have any solution for the unification problems such as inheritance (the module alias solution in the paper seem a bit of a hack and not really intuitive). Anyway for any solution I really want to avoid OCamls (and MLs) problem with having both a 'module language' and an 'expression language'.
You will also get a slowdown simply from clearing the buffer and and the sha256 hashing. Is a vector always heap allocated? If so, I would suggest in this case not using a vector and instead using an array on the stack. Just make a new array every time the buffer fills rather than a reused vector. Further optimizations would come front using uninitialized memory, but that is unsafe.
There is a very compelling reason to use accessor functions for fields: API stability. If you refactor your struct, rename or repurpose the fields, you won't break the interface.
When you switch to a [`BufReader`](https://doc.rust-lang.org/std/io/struct.BufReader.html), you also get access to the buffer that it uses. That means you only need to: 1. Perform one allocation. This will be less than the entire file. 1. Perform one copy per buffer amount; from the system to user space. This reads and hashes 16087252992 bytes in 100 seconds - about 160 MiB/sec: real 1m40.423s user 1m31.904s sys 0m7.337s You didn't mention which crate you are using to perform the hashing, but my timing seems to indicate that most of the time was spent there (high user, low sys). I am using the [sha2 crate](https://crates.io/crates/sha2). extern crate sha2; use std::fs::File; use std::io::BufReader; use std::io::prelude::*; use sha2::Digest; use sha2::sha2::Sha256; fn main() { let f = File::open("IE11-Win7-disk1.vdi").expect("Unable to open file"); let mut f = BufReader::new(f); let mut hash = Sha256::new(); loop { let buf_size = { let buf = f.fill_buf().expect("Unable to read data"); if buf.is_empty() { break; } else { hash.input(buf) } buf.len() }; f.consume(buf_size); } } 
Have you looked into [1ML](https://www.mpi-sws.org/~rossberg/1ml/)?
Atom does all of that with much less work using the Tokamak plugin, and ensuring you have racer, clippy, and rust-src installed.
I've seen this crate, ~~but I was looking for something that would work on stable :(~~ Oh wait, looks like it does. Thanks for your suggestions! Indeed, I do not need to have a Mutex as a callback won't be called concurrently.
 &gt;Each time I tried Vala, I quickly gave up because of the poor or missing documentation [...] Same here. I tried it multiple times some years ago, but in every attempt I was defeated by the lack of documentation or examples. 
Yes! I want to be a bit careful with what type system features I incorporate as there is value in keeping the typechecker small and simple as well. That being said 1ML is extremely interesting and I hope to incorporate some of the ideas. Currently I am focusing on polishing what already exists though, the next type system feature will likely be RankNTypes as I suspect there may be a soundness issue without it, will need to investigate though (I mentioned this a bit in the r/programming thread https://www.reddit.com/r/programming/comments/4sgtba/been_working_on_a_programming_language_for/d59awv6)
Yeah, I totally hear you on the polish part! Seems to be some rough edges that seem to need cleaning up - but that's ok. :)
There's now documentation, including a basic client and server test/example. See the same page.
Btw, as the author and OP, I do not endorse that idea either, but I wouldn't call anything "dangerously stupid" on the internet. I'm not really a beginner. Memory safety is certainly not the only thing, but it's the cause of almost all known security problems in OpenSSH, libssh and libssh2 (the rest is mostly math problems in the crypto).
Thank you! I'll look at it to see what I can mix in my talk. :)
Vala is to GObject what Swift is to Objective-C (albeit that it predates Swift by many years): a way to use the well-established GUI platform libraries in a modern, object-orientated language. For that purpose, it's actually great. The biggest problem with it is that it has never had any buy-in from the core Gnome developers, who preferred to carry on writing libraries in C and recommended JavaScript for writing application code. It's a shame, because it's actually a great language, very easy to use, and vastly preferable to C (and, for me at least, JS). I'm not sure why there was such resistance from the Gnome community -- whether it was internal politics, personalities, or just a distrust of the fact it was based on C# (given the Gnome/Mono saga). If Vala had had the same backing from the Gnome folks as Swift has had from Apple -- if it was presented as THE way of writing both libraries and end-user applications -- I think it could have been much more successful. 
Accept a `Box&lt;for ...&gt;`. What's the 'r lifetime for?
ah, that one is extraneous. (now removed) I haven't been able to get a version like that to compile.
You can't if you use generics. The problem is that every closure is a new type and rust monomorphizes generics at compile time. This means that each time you recursively call `foo` with a different closure, you're actually calling (and forcing rust to generate) a *new* `foo`.
But this isn't an inherent limitation. Here's the same program written in Haskell: {-# LANGUAGE RankNTypes #-} foo :: (forall a. a -&gt; b) -&gt; b foo f = foo (\x -&gt; f x) main = print $ foo (\x -&gt; ()) GHC has no problem with this and it's not just skirting around the issue by including type information in the binary.
It's not about the lifetimes; they're literally different closures. When trying to monomorphize, you get the following infinite series: 1. A closure that returns `()` 2. A closure that calls 1 (a closure that calls a closure that returns `()`) 3. A closure that calls 2 (a closure that calls a closure that calls a closure that returns `()`). ...
Ok, but there should be a version of the program that does compile where that call dispatch is dynamic. How do I write that?
Thanks, that's helpful. It's good to know I'm not the only one who couldn't make `FnBox` work here. I'm going to see if I can make `FnMut` work for my case.
I am having trouble understanding why/how passing a value to a function as a parameter differs from assigning. Both actions are supposed to move, aren't they? Yet the following code compiles: fn take_mut_ref(ptr: &amp;mut i32) { *ptr += 1; } fn main() { let mut x = 42; { let ptr = &amp;mut x; take_mut_ref(ptr); println!("{:p}", ptr); } println!("{}", x); } Shouldn't passing `ptr` to the `take_mut_ref()` function **move** it into the function and therefore make compilation bail out on the `println!(ptr)` line? When I assign the reference to another variable then it errors out as expected, with the message `use of moved value ptr`. So what's going on? 
`&amp;mut T` is a `Copy` type, AFAIK. But even if it weren't you'd be moving the mutable pointer into the function, not the value itself.
`&amp;mut T` is actually not `Copy`, as you have already figured out yourself. The compiler is actually implicitly reborrowing here. You can see this effect more clearly in the MIR output of your example (with the `println!()` invocations commented out to remove the noise concerning the formatting framework): https://is.gd/CFLRXa fn main() -&gt; () { scope 1 { let mut var0: i32; // "x" in scope 1 at &lt;anon&gt;:6:7: 6:12 scope 2 { let var1: &amp;mut i32; // "ptr" in scope 2 at &lt;anon&gt;:9:9: 9:12 } } let mut tmp0: (); let mut tmp1: &amp;mut i32; bb0: { var0 = const 42i32; // scope 0 at &lt;anon&gt;:6:15: 6:17 var1 = &amp;mut var0; // scope 1 at &lt;anon&gt;:9:15: 9:21 // **Implicit reborrowing here** tmp1 = &amp;mut (*var1); // scope 2 at &lt;anon&gt;:10:18: 10:21 tmp0 = take_mut_ref(tmp1) -&gt; bb1; // scope 2 at &lt;anon&gt;:10:5: 10:22 } bb1: { return = (); // scope 2 at &lt;anon&gt;:8:3: 12:4 return; // scope 0 at &lt;anon&gt;:5:1: 15:2 } } fn take_mut_ref(arg0: &amp;mut i32) -&gt; () { scope 1 { let var0: &amp;mut i32; // "ptr" in scope 1 at &lt;anon&gt;:1:17: 1:20 } let mut tmp0: (i32, bool); bb0: { var0 = arg0; // scope 0 at &lt;anon&gt;:1:17: 1:20 tmp0 = CheckedAdd((*var0), const 1i32); // scope 1 at &lt;anon&gt;:2:3: 2:12 assert(!(tmp0.1: bool), "attempted to add with overflow") -&gt; bb1; // scope 1 at &lt;anon&gt;:2:3: 2:12 } bb1: { (*var0) = (tmp0.0: i32); // scope 1 at &lt;anon&gt;:2:3: 2:12 return = (); // scope 1 at &lt;anon&gt;:1:32: 3:2 return; // scope 1 at &lt;anon&gt;:1:1: 3:2 } } This is just a special behavior of the `&amp;mut _` references as they're a type the compiler intimately understands, as compared to a type declared and implemented in Rust code like `String`.
Thanks, that's it! Special cases, yay! 
learn some, thank you all you.
Still working on rust bindings for Cryptominisat. Right now I'm trying to get the C API upstreamed. I also still need to figure out a way to make the repository layout compatible with cargo package.
You can also implement a ~10-line wrapper over `Vec` that implements indexing from `usize` to `[T]`, which lets you use the syntax `nums[i][j]`.
I'm more concerned about what .clear does. If it is just an index number update, then there is no benefit. If it is zeroing the whole array, then there may be benefit in just making a new vector everytime vs clearing. If that is the case, then a stack allocations will be faster than a bunch of heap allocations. That said, rust does zeroing on allocation, so that is where the unsafe uninitialized memory comes into play.
[You can], but then you can't specialize the trait for any types you don't own unless they implement `Deref`. The reason is that if two impls might apply to the same type, one has to be strictly more specific than the other. In the case of types that implement `Deref`, [this is fine]: `Box&lt;T&gt;` is strictly more specific than `U where U: Deref&lt;Target=T&gt;`. However, for types which *don't* implement `Deref`, you can't say that `bool` is more specific than `U where U: Deref&lt;Target=T&gt;` since there's no relationship between the two. Now, the obvious response is "yeah, there's no relationship *because `bool` doesn't implement `Deref`*, so the two impls don't overlap in the first place!" And you'd be right, except that making this statement requires applying *negative reasoning*. In order to make this statement, you have to depend on the non-existence of an impl of `Deref` which applies to `bool`. Rust [typically avoids] letting you apply negative reasoning about implementations unless you own either the type or the trait—and in this case you own neither—because that would unreasonably restrict what library authors can do without potentially breaking client code. In this case, it seems like there probably isn't any harm. If the impl were allowed, and then an impl of `Deref` was added for `bool`, nothing would break because the impl would then be strictly more specific than the blanket impl for `U where U: Deref&lt;Target=T&gt;`. Basically, it would be permitted either way, so there shouldn't be a problem. It's either strictly more specific or it doesn't need to be, so while you need to apply negative reasoning, you aren't truly depending on it. It's quite possible that this latter reasoning would be accepted. The effect of negative reasoning on specialization is still an [open question]. In fact, for all I know, this is intended to be allowed, and the reason it doesn't work is because it hasn't been implemented yet. After all, the error is the same as if specialization didn't exist. Before specialization, you really *would* be depending on the non-existence of an impl in a foreign crate in order to be able to write the specialized impl on `bool`. [You can]: https://play.rust-lang.org/?gist=cfb1e8257fdeb9a47f3dffb805f5b54d&amp;version=nightly&amp;backtrace=0 [this is fine]: https://play.rust-lang.org/?gist=d9e4a0e1a5995b94f22a36115c6e7fe4&amp;version=nightly&amp;backtrace=0 [typically avoids]: https://github.com/rust-lang/rfcs/blob/master/text/1023-rebalancing-coherence.md [open question]: https://github.com/rust-lang/rust/issues/31844
I like the new logo!
Ah, mixed up `&amp;` and `&amp;mut`. Sorry about that.
vec.clear() does not zero the memory, it just drops the items and decreases the length of the vec
Skip to [this point](https://youtu.be/S9sS55s-oR0?t=518) for rust stuff if you don't want to see the entire video.
I hope to get some PRs into [imag, the personal information management suite for the commandline](https://github.com/matthiasbeyer/imag), mainly the git support for the store backend and the `todo` module, which integrates [taskwarrior](https://taskwarrior.org). I also hope to get one major issue with the `StoreId` type solved and maybe another PR which abstracts the FS operations away, so we can test the store implementation more intensively. Besides that, I hope to get some progress in my bachelors thesis.
I'm still struggling to find a correct design for command buffers in vulkano. Basically what you need to do in order to ensure safety is to keep track of which slices of which resources are accessed by the GPU and in which way (read/write, but also a thing called image layouts). But doing so would be very expensive, so what I'm trying to do is find a system that's cheaper but at the same time lets you use all the patterns that a game engine may want to use (like using sparse memory, or making multiple resources alias over the same memory). I already made a leap forward by realizing that using an iterator-like API to add GPU commands to the list of commands to be executed is more appropriate than a state machine, as it lets you keep the types of the resources used by the commands instead of forcing you to use trait objects. This alone solved a ton of problems. 
Working on [lyon's path tessellator](https://github.com/nical/lyon). Working on the API and the maths to prepare for a resolution-independent tessellator. Also added some github issues and marked the easier ones with the [contrib label](https://github.com/nical/lyon/issues?q=is%3Aissue+is%3Aopen+label%3Acontrib) in case anyone is interested in helping me out and is looking for something approachable to get started.
For those who watch this until the Q&amp;A at the end: I claimed during that segment that [graphviz](http://www.graphviz.org/) would be a good tool for authoring object diagrams (e.g. for use in tutorials). Further discussion led me to rethink that claim, and I am now at work on a different approach (something that is more like [a2s](https://9vx.org/~dho/a2s/index.html), but with an input syntax that is a bit closer to typical ASCII art conventions).
Nice! Does the reference returned optimized? (for not needing to follow it) 
Anyone know of any good resources for writing generics in Rust? I've read The Book, but was just wondering if there were maybe any more real-world examples.
About all the points he said about rust can also be said about Swift or Go. Only that both seems a little bit more intuitive (easier to adopt) and Apple + Chris Lattner or Google have imo. more credibility than mozilla... I think the biggest advantage to C++/C is rather the - hopefully in the future provable - safety compared to C without the additional runntime overhead of GC in Go or ARC in Swift.
The other thing about Swift is that if you want to be on an iOS device, it's one of just 3.5 games in town. And for a lot of people Objective-C just feels downright *weird*. I say this as a diehard Apple fanboy who wanted to love it.
Im generally with you, but swift and go are for some problems fast enough to be considered an alternative to c/c++ (+Rust). Considering that some people even put Java or Python on microcontrollers make me wonder if a little bit of overhead will be accepted in the future.
Question: why did you include the Windows API bindings when this appears to be running on a unix system? Also, when you said it allowed crates, I was kinda hoping it supported `cargo-script`'s awesome embedded manifest feature, but no such luck. :P Still, 's a cool improvement over no crates at all!
Thanks! &gt; why did you include the Windows API bindings There are two reasons: 1. The script really is the top 100 crates and those are popular. 1. They are used by other, non-Windows-only crates: [[package]] name = "time" version = "0.1.35" source = "registry+https://github.com/rust-lang/crates.io-index" dependencies = [ "kernel32-sys 0.2.2 (registry+https://github.com/rust-lang/crates.io-index)", "libc 0.2.14 (registry+https://github.com/rust-lang/crates.io-index)", "winapi 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)", ] &gt; it supported `cargo-script`'s awesome embedded manifest Yep, sadly we can't do that as it would allow arbitrary code execution on a machine connected to the internet. It would also increase the execution time.
A [direct link to the Playground](http://play.integer32.com/) for us lazy people.
&gt; Anyway, you're already allowing arbitrary code execution on a machine connected to the internet. Could you elaborate a bit on this? As mentioned in the article, the code that a user submits is run in a docker container with networking disabled. There shouldn't be any way for the arbitrary code to access other machines. If you mean while pre-building the top 100 crates before the user input is present, then that's technically true. I'm using "popularity" as a rough guideline for "trusted" (this is where security-minded people's heads explode), so I'm not calling that truly arbitrary.
I suppose I'm wondering why you can't build the crates inside the same sandbox. I mean, I get that cargo isn't designed for that, and you'd probably have to re-implement large parts of it, but I mean *in theory.* "It's hard, so I didn't bother" is always a reasonable explanation when you're working for free. :D But really, I was just noting the incongruity of saying you can't do something because it would allow code execution in a service whose whole purpose is to allow code execution.
The same thing we do every week, llogiq - try to take over the world!
&gt; cargo isn't designed for that, and you'd probably have to re-implement large parts of it, but I mean in theory. I think that Cargo would be fine here: cargo fetch # With internet connection cargo build # Without connection The main problem with this would be the time required to download and compile the crates. The obvious answer is caching, but then you've got all sorts of interesting problems... &gt; it would allow code execution in a service whose whole purpose is to allow code execution. My point is that it doesn't allow arbitrary code execution *on a machine connected to the internet*. That's how DDoS machines are created.
[removed]
_Edit: My answer below addresses only the Student T-distribution._ So far as I'm aware no such thing exists in Rust yet. The [rand crate](https://doc.rust-lang.org/rand/rand/index.html) supports sampling from many distributions - but doesn't include pdf/cdf. I began implementing some basic distribution functions in [rusty-machine](https://github.com/AtheMathmo/rusty-machine/tree/master/src/stats/dist) and would welcome a contribution adding student T distributions. It seems if you really need it in Rust you'll have to implement it yourself :)
I feel we've long ago passed that point. Some teams are still putting a lot of effort in optimizing their code, but some are just slamming components together and using memory like crazy to compensate poor design. MS Word was the first program that made me realize that implementation speed and featuritis trump a clean design in corporate context.
You can just write a custom trait and use it here, really.
What will and will not the compiler evaluate at compile time? For example, I suppose that something light like `2 + 2` will be evaluated at compile time. At the other extreme, I suppose that something heavy like `compute_record_amount_of_digits_of_pi()` will not, even though it is just as deterministic. I am unsure about intermediate cases like `((1f64/2f64.sqrt())*(std::i32::MAX as f64)) as i32` (which would be unfortunate to have evaluated at run time on a processor without native floating-point arithmetic). So, what are the rules for what gets evaluated at compile time and run time, respectively? Can you indicate to the compiler that you want some things one way or the other? (Am I right in thinking that these things are properties of the compiler, rather than of the language?) Edit: Can I annotate things to tell the compiler to pre compute it? Can I run some test afterwards to check that it really happened?
I also tried that. I also couldn't make it work.
&gt; (Am I right in thinking that these things are properties of the compiler, rather than of the language?) Yes. Technically, Rust doesn't support performing anything "at compile time"; there is a nightly-only feature, `const fn`, which has not made it into stable yet. It will allow certain things to be computed at compile time. Right now, stuff like `2 + 2` will happen to be computed at compile time, but that's something LLVM does as an optimization.
I think languages get popularized by the products they are used to build; and therefore I think Rust may get more popular as it is included more deeply in Firefox and other projects using it get more popular. I would say it's a bit "oh, they build that in Rust? Well, for our similar project maybe we should consider it too". Of course, it makes *starting* hard (cue D).
&gt; What does the "GenericImage + 'static" annotation mean? GenericImage is a trait. The `Trait + 'a` synatx means "this trait contains references with a lifetime `'a`". Now, `'static` is the longest lifetime, and so `+ 'static` means "contains no non-`'static` references" which is also true of no references at all. If that makes sense. &gt; What does the 'static annotation mean in the return type (specified in the where clause)? `'a: 'b` means "the lifetime `'a` must live at least as long as the lifetime `'b`. https://doc.rust-lang.org/stable/book/syntax-index.html can be helpful for bits of syntax that you don't know about, as well.
Thanks. Using your explanation I came across an extended discussion here: http://stackoverflow.com/questions/26212397/references-to-traits-in-structs If I understand correctly, the 'static ensures that even if the GenericImage trait is implemented for a reference of a type, it cannot be passed to this function. My next question is, why would you impose such a restriction? Can't you just use lifetimes to ensure that a reference is correctly handled?
Well, while I was translating the cdf of Student's T in Boost::Math, I discovered that it depends on a lot of functions like incomplete beta which depends on another complicated function gamma. I felt like it's too much for me since I'm new to program translation.
I don't know this library well enough, so I'll make an analogy: `thread::spawn` restricts the closure you give it to only accept the `'static` lifetime, specifically because you can't know when those threads will be joined. Therefore, a lifetime itself won't work, you need a different interface to know this, hence the various 'scoped thread' crates. Does that make sense?
Are you using docker as the sole sandboxing mechanism? I'm [lead to believe](https://opensource.com/business/14/7/docker-security-selinux) that it is more designed for environment isolation than security, and thus additional layers are necessary when executing arbitrary code.
I think there are at least two crates that implement the gamma functions. Just search on crates.io for "special function".
Today was my first time writing Rust. I wrote a little program that takes a string dir path arg, loops over files, then loops over the contents of each child dir. Eventually I'll read out the contents and do something with it. 1. How should I improve this, generally? 2. How can I make it more in the functional style? It's currently quite imperative. Thanks! https://gist.github.com/devth/3421a5d6eef2fbcd613c2f36bbac69e4
I decided to do my first "real" rust project this week. I'm implementing an ar-like archiver with a compression utility built in. I did it once in C, so it's a good way to to learn Rust concepts. 
Does this work: trait MyTrait: Add&lt;T&gt; where T: MyTrait, Self::Output: MyTrait
&gt; The linear algebra libraries intended for games typically only implement up to 4x4 matrices. The advantage is that the code optimizes a lot better if you know the dimensions at compile time. I'm thinking of cases where a matrix can be of any size, but the size is known at compile time (because it represents a particular physical system, say). This is actually the reason for my other question in this thread about what the compiler does and does not figure out and hard code at compile time. &gt; If you want to maximize performance, you should probably use a library that binds to BLAS/LAPACK. What about if you run it on a small microcontroller. Is BLAS/LAPACK to big to fit there, or will it become small if it is known at compile time that the matrices are small?
I just started learning Rust, and I decided that I wanted to learn it by writing a simple space simulator. I started writing a very simple scene renderer, but I ran into some problems when considering the design, with regards to dynamic/static dispatch. So far I have something like this: use glium::Surface; trait SceneRenderable { fn render(&amp;mut self, surface: &amp;Surface); } struct SceneRenderer { renderables: Vec&lt; Box&lt;SceneRenderable&gt; &gt; } impl SceneRenderer { fn new() -&gt; SceneRenderer { SceneRenderer { renderables: Vec::new() } } fn render&lt;S: Surface&gt;(&amp;mut self, surface: &amp;S) { for renderable in self.renderables { // Render each renderable by passing it the given surface... Of course, this does not work. } } } This doesn't work, because: error: the trait `glium::Surface` cannot be made into an object [E0038] The Surface trait from glium has generic methods, and I think I understand why this does not work. Now, I could simply use a match expression in the SceneRenderer::render() function, and then do all the Surface-dependent code there, but that means that the trait implementations cannot decide how they render themselves, i.e. an OOP-based design, which seems to be more flexible. Does anyone have a suggestion for how I can come up with an idiomatic, clean design for this problem?
A great point, and thank you for the link! I [should certainly drop root privileges](https://github.com/integer32llc/rust-playground/issues/29) inside the container, I don't really need that for anything. Beyond that... I'm using docker more as a resource limiter, I suppose. It disconnects the network and limits the memory. Running time is limited by `timeout(1)`. Do you have any further suggestions on how I could shore up the security?
Hey man, you are looking for /r/playrust. /r/rust is about The Rust Programming Language. extern crate internet; extern crate internet_reddit; fn main() { let subreddit = { let r = Internet::get_reddit(); r.get_subreddit("playrust").expect("The subreddit no longer exists!") }; println!("{}", subreddit.get_posts().unwrap()); // I'm a lazy developer. }
You can lift the parameter up: trait SceneRenderable&lt;S&gt; where S: Surface { ... } struct SceneRenderer&lt;S: Surface&gt; { renderables: Vec&lt; Box&lt;SceneRenderable&lt;S&gt;&gt; &gt; } impl&lt;S: Surface&gt; SceneRenderer&lt;S&gt; { fn new() -&gt; Self { SceneRenderer { renderables: Vec::new() } } fn render(&amp;mut self, surface: &amp;S) { // `for _ in self.renderables` would mean trying to iterate `renderables` by-value for renderable in &amp;mut self.renderables {...} } } Having a concrete type in the parameter of a trait will make the trait object-safe. The one drawback is that this parameter will leak all the way up to the level where you know the type of `S`, though inference should help you avoid naming the exact type wherever `SceneRenderer` or its container is placed on the stack
Can you use `AsRef`? I don't know if it'll work but I'm imagining something like fn render&lt;S, RS&gt;(&amp;mut self, surface: RS) where S: Surface, RS: AsRef&lt;S&gt; { let surface_ref = surface.as_ref(); // pass surface_ref to renderables }
The "why" is because there are a lot of things about Rust that are attractive besides its performance. That *is* attractive—very attractive—but so are its type system and its approach to solving a lot of problems in a way that feels familiar coming from languages a lot of other developers are familiar with while pulling in much of the best of ML-descended languages. Having a light, pure-Rust, ships-everywhere runtime (rather than the fairly heavy CLR)—something built on e.g. automatic reference counting, rather than the heavier GC used in C♯—would be a huge win for those of us who love Rust for all those other things but don't necessarily always *need* "to-the-metal" performance. To that specific comparison: C♯ has a lot of ceremony from the everything-is-a-class model it inherited ;-) from Java, and its type system is fine but nowhere near as nice as Rust's. **Edit to add/clarify:** I've said *often* of late that if I had a version of Rust that was basically "easy mode," where I could just tell the compiler, "just automatic reference count everything I don't manage explicitly myself," I would probably use Rust for *just about everything I do*. As it is, it's close to that anyway (just because I really like it), but there are definitely times I miss that. Even in the context of ML-descended languages, I'd take Rust's particular blend of syntax and semantics over F♯'s, for example, though I'd take F♯ over C♯, too.
I think that the idea that `'static` is also true for any struct with no references at all is the most confusing part of `'static`. It makes sense now that I know, but I don't remember it being explained well anywhere in the official doc. (But it has been a while since I read the book and I know you ahve been working hard on improving it)
&gt; How should I improve this, generally? Prefer `&amp;str` to `&amp;String`. They function almost identically since they're immutable, but the former works with literals and `String` objects and can easily work with anything else that has `Deref&lt;Target=str&gt;` or `AsRef&lt;str&gt;` implementations. The latter does not. &gt; How can I make it more in the functional style? It's currently quite imperative. In general, use iterators and methods on them like `map` and `flat_map`. You create functions which operate on specific types and do one small thing. You'll still need some kind of top-level loop in order to consume the lazy iterators and actually execute the task. I didn't actually test that this was a solid conversion, but here's how I might approach it more functionally, discarding most errors: use std::fs; use std::path::PathBuf; fn main() { let dir = "/home"; println!("Looking in {}", dir); let files = fs::read_dir(dir).expect("unable to read dir") .filter_map(|result| result.ok()) .filter_map(directory_as_pathbuf) .filter_map(|p| p.file_name().and_then(|f| f.to_str()).map(String::from)) .inspect(|app| println!("{:?}", app)) .filter_map(|app| fs::read_dir(format!("{:?}/{:?}", dir, app)).ok()) .flat_map(|entries| entries) .filter_map(|result| result.ok()) ; for file in files { println!("{:?}", file.path()); } } fn directory_as_pathbuf(app: fs::DirEntry) -&gt; Option&lt;PathBuf&gt; { let p = app.path(); if p.is_dir() { Some(p) } else { None } } 
A cursory look of [`vm.rs`](https://github.com/zack-bitcoin/forth-in-rust/blob/master/vm.rs) suggests that you are using tons, *tons* of `clone()` (which is a deep copy, as Rust doesn't make use of managed references at all). While it is safe, it is doomed to be slow if you don't use any borrowing.
This made it _very_ fast! Im pretty sure it is faster than the erlang version. Thank you so much! :)
Ahhh, you're totally right. I misspoke.
I took almost all the clones out. It is even faster. Thanks. 
I haven't had any official conversations about replacing it. :-) When I started, I had some ideas about replacing the official Playground with a newer, better one. However, the official Playground already addressed most of the concerns I had, so the pure set of benefits are less. However, [brson mentioned](https://www.reddit.com/r/rust/comments/4tf0kj/qa_on_the_alternate_playground_implementation_now/d5hiihe) that the official Playground may not be as actively developed as they would like...
How am I supposed to use an external crate's macros in anything except `lib.rs`? I have been struggling with this for awhile and google is no use at all. So I have: // src/lib.rs #[macro_use] extern crate lazy_static; pub mod lexer; And in lexer.rs: // src/lexer.rs lazy_static! {...} But I get a build error: src/lexer.rs:61:9: 61:20 error: macro undefined: 'lazy_static!' src/lexer.rs:61 lazy_static! { ^~~~~~~~~~~ error: aborting due to previous error error: Could not compile `rust_math_parser`. To learn more, run the command again with --verbose. And I cannot for the life of me figure out how to use the macro. Separately, can someone _please_ explain in simple terms how Rust's module layout is supposed to work? I get completely lost with all the `mod`s and `use`s and `reexporting` and such.
You can probably add Copy to fixed size structures.
[Sure, I wrote one a couple weeks ago](https://crates.io/crates/alewife). It's very much a first draft, and doesn't have the most ergonomic interface, but I'm using it in an application and it's certainly doing its job. If you take a look at it, then I'd like to hear what you think. Shoot me a PM or find me on #rust.
He has some good points on technology being backed by reputable institutions and power of lock in and network effects but I think he had a few things wrong. - The idea that not wanting to have children has been bred out is wrong, people attitudes on children are passed on societally not via genetics. Having few or no children is a relatively recent trend that's becoming popular virtually everywhere. - He says that cargo was inspired by npm except it was far from the first language package manager and Cargo was built by the creators of ruby's Bundler. - Also rust syntax may be more familiar then haskell or clojure but it's differ a decent amount from c/c++/java/c#/javascript syntax.
The statement is that *if* you believe that API and prose documentation should be separate, it will work fine with rustdoc. If you want to use tango and rustdoc together, you will have issues.
Hey, thanks for the reply! I wasn't so specific in my original post about where the error came from, but it was actually this line: trait SceneRenderable { fn render(&amp;mut self, surface: &amp;Surface); // &lt;-------- error comes from here } I suppose I could use a similar generic style for the SceneRenderable traits as well, i.e. fn render&lt;S, RS&gt;(...), but then SceneRenderable has generic methods and is no longer a trait object, which means I cannot box it. Do you see any way around this?
 calatinova implemented Document.referrer canaltinova implemented the referrer property on the Document object You so awesome calatinova that you implemented Document.referrer, not once, but twice! And in the same two weeks too! :P (And fixed in https://github.com/servo/blog.servo.org/pull/102)
Excellent suggestion, thanks! I think this should be workable, as the hierarchy here should be quite shallow.
I think so llogiq...but isn't Regis Philbin already married?
Wow, haven't heard of include_bytes. I'm not sure whether there will be enough time to mention it but I may make some additional notes for the discussion. I've heard of error_chain but didn't try yet. It's worth mentioning though. Do we know each other?
learn much, thank you all.
Rust relies on llvm to do a lot of heavy lifting. In this case in particular, I wouldn't be surprised if even_word/uneven_word weren't be converted into switch/case unless building with optimizations (this could be alleviated by perhaps using `match`)
"forth", not "fourth". I misread it at first as well.
&gt; Advantages over log create Probably 'crate'
&gt; You mean that it's more like a RSS feed than a message-dispatch then right? Yes, something like RabbitMQ, ZeroMQ, ... but locally. &gt; Note that you can box big messages, and thus have the enum only have a Box&lt;BigStruct&gt; which is just 8 bytes. The main issue is that I can't use messages declared in different parts of my code (or different crates). 
You should check if there are well documented libraries to access your dataset, this is probably more restricting.
Cool, I don't understand why the counter example needs all that atomic reference stuff though. Wouldn't it also work to pass a normal usize to a call before and after? There's no threading there and even if there was I don't see why logging would have to care as long as the log message is only modified on one thread.
When debugging code, the compiler builds in a number of additional safety checks that are really helpful, but hurt performance by some relatively small amount. However, when debugging code, you also want the program to be as close to the code that you wrote as possible, so that you can look at values and step through the code in ways that make sense. With no optimizations at all, runtime performance can be garbage. Most programmers aren't actually that good at writing code. The compiler can perform seemingly miraculous optimizations that improve performance dramatically, and the execution is logically equivalent to the code you've written, but it doesn't correlate 1 to 1 with your source code very much, which makes it a lot harder to debug logic errors where you wrote bad code. That's one explanation. Compiler technology is a huge field, it isn't something that can be summarized in a few sentences. 
I was wondering if there is any project for a scientific library in the scope of numpy and scipy for python ? Also why using macros instead of functions for common task like println ?
That could be a neat improvement for [imag](https://github.com/matthiasbeyer/imag) to make the logging more ... structured? At the moment we just `debug!()`/`info!()`/`warn!()`/...
thanks bud, my data's pretty straight JSON so i should be fine. What are your experiences with setting up authentication and DBs in a rust server app? Are we talking days or weeks? I'd like to try something new (100% node/js developer). Thanks for your time
thanks for your answer and the clarifications :)
 What is macros 2.0? Is there a sketch for it?
Background: [discussion on /r/emacs about this](https://www.reddit.com/r/emacs/comments/4tk5iv/rust_autocomplete_hide_paths_where_stuff_is_found/).
Days (or lesser, depending on implementation details), from my experience.
Does that mean that the -O option also changes how the rust compiler generates LLVM IR, and not just which LLVM passes are run?
So I figured out the issue, in the same `src/` directory, I also had a `main.rs`. When I deleted the `main.rs`, then `cargo build` did not complain. I suspect it had something to do with `cargo` compiling `main` which used `lexer` as well but `main` did not import the macros. To be clear, my `main.rs` did not have a `extern crate &lt;my_crate&gt;` in it could not be compiled correctly.
1. Because they're all synchronisation-related and there's really no benefit to modules with only one or two members? 2. Because `vec` is *significantly* more frequently used than the others, plus it probably existed *before* `collections` did? 3. Because needlessly long paths don't buy you anything except annoyance? 4. Because `Default::default()` is a function. More than that, it's not marked `const` (since that feature didn't exist at 1.0), so if those constants didn't exist, you couldn't initialise `const` or `static` atomics. 5. Because `Rc` is a pointer type, and adding inherent methods to it could potentially mask methods on the thing it points to and suddenly which method you call changes depending on whether you use `&amp;`, `Box`, `Rc`, `Arc`, *etc.* 6. Because `box` is a keyword. 7. Because it's simpler than having to stuff parts of `Path` in a completely different part of the library and then having to integrate them across module boundaries? Because sometimes code running on non-Windows machines has to deal with Windows concepts? 8. Because that's the name someone chose and no one ever raised an objection and now it's stabilised?
There's this crate but it's not exactly what are you looking for https://crates.io/crates/bus
The "insecure playpen" discussion took place months after the individual had left the community, that's why the discussion was necessary.
Thanks.
Loggers (and so value-keys assigned to them_ can be passed around the threads so they need to be `Send+Sync`). The particular example is simplified (so no actual thread is spawned), but the `Send+Sync` must still hold.
I did look at the code: trait Callback { fn callback&lt;'a&gt;(self: Box&lt;Self&gt;, subs: Substitutions&lt;'a&gt;, expression: &amp;'a Expression&lt;'a&gt;); } should work, pass around `Box&lt;Callback&gt;`, and call `box_callback.callback(..)`
I've been working on [Statrs](https://github.com/boxtown/statrs) (docs [here](https://boxtown.io/docs/statrs/head/statrs/)) which might suit your need for a Students T distribution, granted it's only v0.1.0
I only see the full path when completing module names, whereas (in the background thread), you are looking at the behavior when completing other things. Try completing `use std::io::B`. Does it still show the full path? If it only shows the path when completing modules, is that a problem for you?
This crate might work for you: https://github.com/BurntSushi/chan
Ah, you are right! I would still rather it didn't show anything at all. It just looks like a mess, everything is really ugly: http://i.imgur.com/HjQnT93.png Does yours look like that?
Regarding `println!`, there's some discussion in [this thread](https://github.com/rust-lang/rust/issues/17190). One reason is that Rust doesn't currently support functions with a variable number of arguments, and macros are a way to work around that. Another reason is that expanding the macro allows the compiler to check both the number *and the type* of the arguments against the format string at compile time, which wouldn't be possible if print were a function that just accepted "any number of arguments of any type". For example, check out this expression: println!("{name:.*}", 3, name = "abcdefghi"); That prints `abc`. If you try to change the `3` to a string, or delete that argument entirely, the compiler will catch it and give you an error.
Still working on my HTTP / API testing framework [noir](https://github.com/BonsaiDen/noir). Got the macro for creating form upload data ready as well as most of the related diffing code, AppVeyor builds are finally running (Yes, getting hyper to compile with OpenSSL turns out to be quite tricky). Besides that, I wrote a ton more tests and sketched out a rough layout for the tutorials.
I considered that possibility, but figured I'd clarify since I had misread it the same way at a glance.
Fair enough then, there doesn't seem to be a way around it. I guess I am just being picky. Dont you think its really ugly? Like what the fuck is that please_link_me_dont_reference_me thing...
I have no idea about this, beyond knowing docker probably isn't enough for executing arbitrary code and, like, a few random [links](http://codepad.org/about) that I've only skimmed. My only suggestion would be researching alternatives or add-ons that harden it.
I was closely involved in many of the events you're describing (being on various Rust teams), and you have the timeline incorrect. I'm sorry you feel hurt by your time working on rust-playpen. Also, I have a feeling your comment is... not entirely relevant. The project under discussion here is an alternative to rust-playpen, and I'm just talking about this new alternative's choice to use only docker, with no thought as to other projects that may or may not be using docker or other programs for what they want to do, nor any thoughts about why the alternative exists: purely trying to make sure this project hasn't got big holes.
Ah, so more of a broadcast type thing. Not sure what the best way to handle that would be, perhaps there's a crate. As for performance, you only have three options - use an enum, which has memory overhead + a check of the variant, use a Box&lt;T&gt;, which is going to be even slower due to dynamic dispatch, use multiple channels - one for each type. I'd go for the enum, it's probably the fastest method.
6. box is a keyword, you can't name a module box.
&gt; Auto-Deref? Autoderef is the problem here. Autoderef only happens if the parent type doesn't have the method, and it falls back to autoderef. You expect smart pointers to autoderef, but if the pointer type itself has a method that can block autoderef.
I'm back from vacation so all errors in CoTW and notable changes are mine. ;-)
Nice, it seems to do at least part of the job.
I'll go with the enum I think, thanks for the answer it helped a lot. You can't declare parts of an enum in different files right ? Something like partial class.
https://doc.rust-lang.org/nomicon/hrtb.html
If the api is small, I would use [Bottle](http://bottlepy.org/), maybe with [Peewee](http://docs.peewee-orm.com/en/latest/). For larger apis, check Swagger.
No question is dumb here in the rust community. We've all been there before learning it. Jump on irc and ask your questions! People there are always happy to answer
Dropbox is going to use Rust for more than just magic pocket? I'm excited!
&gt; You can't declare parts of an enum in different files right ? Something like partial class. Nope. You could have an enum of enums I suppose, where the inner enums are declared elsewhere.
"forth" is the name of a computer language discovered by Chuck Moore. 
Wrong subreddit - try r/playrust :) 
I added a file to the github about it: https://github.com/zack-bitcoin/forth-in-rust/blob/master/speed.md tldr; the rust one is 6.25 times faster.
I am programming a Rust project at work and I need to eventually add a logging crate that can send output over udp, so this looks like it could be very useful, thanks!
What does box do?
I don't think it's stabilized yet, but it's used for placement construction. So in its most basic for, `box val` is equivalent to `Box::new(val)`, except that even without compiler optimizations it constructs `val` in-place in the allocated memory instead of creating it on the stack and moving it into the allocation. Without it, [this](https://is.gd/2DU3za) program doesn't work in debug mode. Future plans are to generalize it for all sorts of different allocators and containers. So you could say `box&lt;Rc&gt; val` to get an `Rc` containing `val`.
Will definitely try this out later. Thanks for this!
IIRC that's post-llvm-opt IR? I think rust produces the same it in both modes, aside from things like debug assert and integer overflow.
Oh alright thanks for the notice :D
&gt; What do you mean by this? Suppose I want to know what objects are associated with the `Arc`. For this I need to read the description of each object, so I reach to `Weak`. It would be better if there was a module `std::sync::arc`, which contains all its dependent objects. I want the standard library was intuitive for beginners.
Well, to start, `GetProcAddress` doesn't appear to have a problem linking. The error message tells you that `LoadLibrary` is the symbol it's having trouble with. And the reason is that `LoadLibrary` doesn't have a symbol with that exact name, it's got suffixes based on what kind of string you're feeding it: `A` for 8-bit ASCII strings, `W` for 16-bit UTF-16 strings (i.e. **W**indows strings.), and also `LoadLibraryEx` with either suffix that has an expanded API that handles things slightly differently, I'd have to read the docs to be sure of *what*. By the way, why not try [a crate](https://crates.io/crates/kernel32-sys) that binds [these](https://retep998.github.io/doc/kernel32/?search=LoadLibrary) [functions](https://retep998.github.io/doc/kernel32/?search=GetProcAddress) for you? It's also got all the type definitions you'll need in the main `winapi` crate. 
a2s looks very interesting. I use [plantuml](http://plantuml.com/) for UML diagram generation. Its readable in text form, but takes some time to get used to.
Both [Elixir](http://elixir-lang.org/getting-started/introduction.html) and [Phoenix](http://www.phoenixframework.org/docs/overview) have decent guides. I really like Elixir as a complement to Rust. I also feel that both the Rust and Elixir communities are both really welcoming, which I find a plus. 
There's the solicit crate for http2 but I assume it's not solid enough. What are the goals for the new implementation?
I didn't really understand the async Storage api, is there any point in having 2 levels of thread pool? (from what I can understand the code calling the Storage is already running from a thread pool).
Good to know that people can stop using 107374182400...
Kind of 2 levels. Raft-store is an abstract storage. And raft-store can write to low level storage by using pipeline/batch. The write flow looks like: kv-server -&gt; txn-scheduler -&gt; Raft-store -&gt; RocksDB.
perfect work.
Running [clippy](https://github.com/Manishearth/rust-clippy) on your code gives a lot of warnings, fixing which will easily improve the code quality. If you need help, let me know.
 Yes. Please see https://github.com/pingcap/tikv
Hm, how do you run clippy if you're using stable rust? Is there a precompiled binary somewhere?
Yep, i3-gaps. [Here](https://github.com/hicksy994/Dotfiles) are my dotfiles if you want to copy anything.
I believe we don't, but I'm looking forward seeing someone with interest in rust in person.
I was able to read and follow it, and I'm not a guru - so I'd call that a success!
No, as clippy is a plugin it needs a nightly rust installed. Worse, it only runs with the Rust it was compiled with. But you can use rustup to get stable &amp; nightly rust in parallel. Or use https://clippy.bashy.io
Ha, well deserved, with that amount of flairs on reddit. Thank you for that little piece of backstory and insight, I was a really joyful read.
I'd love to, but it was never released online (remember, this is from pre-MTV), and since I only own ¼ of the copyright, I'd have to get permission from the others to release it, also our German GEMA would probably want some fees from me if I do so. :-(
Thanks, that's probably a good idea.
I'm trying to have a trait `X` which requires that anyone implementing `X` can convert to other implementations of `X`. So I'm trying to make the declaration of `X` enforce this like so: trait X&lt;T&gt; : From&lt;T&gt; where T: X {} But the compiler is telling me that it doesn't find any type arguments in my specification of `T`. Is it not possible for a trait to refer back to itself this way? Or should I implement this in some other way?
Seems like hiceki isn't a native speaker, it might be good to take that into consideration when observing their tone.
There is a bit of pointer arithmetic and aliasing because of the fact that I'm dealing with raw, mmap'd buffers and doing atomic operations. It's pretty freaking ugly, but it works. I'm looking for pointers on how to make it more idiomatic Rust and improve clarity. 
Are you using [seccomp](https://docs.docker.com/engine/security/seccomp/) and [apparmor](https://docs.docker.com/engine/security/apparmor/) with docker? Also, you've probably found this, but let me leave it here for others :) https://github.com/docker/docker-bench-security 
Per the OP, I posted the question to [SO](http://stackoverflow.com/questions/38483890/require-trait-implementations-to-be-convertable-between-eachother) so others can more easily find it in the future :)
FYI, I've refactored your VM implementation and made some significant changes to avoid unnecessary allocations: https://github.com/Stebalien/forth-in-rust/blob/master/src/vm.rs. I've tried to keep the stack-based style as much as possible which means that it's not quite as fast as it could be but it should be a bit faster than your original implementation. Feel free to *not* look at my code if you want to do this on your own. I mostly rewrote it as an exercise. *edit: words are hard*
This is great! I think in future I'll depreciate my own stats module as this achieves what I wanted to and a great deal more!
Why return a slice instead of a value that owns the allocation and frees it when dropped?
Mostly because my target is more specialized buffers such as Iobuf or Appendbuf. They would implement their own drop against the block alloc. I'm open to ideas for how to encompass a trait to make interop with a variety of things easier. e.g. an AllocatedBuffer trait which can act like a buffer but also takes a ref to the allocator so that it can drop() itself. 
Ich verstehe die Sprache der Musik!
Is there a website for tikv? 
Oh i see down below. Thanks!
What a wonderful post! I commend you.
Makes sense but it'd be nice to provide a TypedAllocator&lt;T&gt; on top of it, probably even in the same crate.
Is formatting like this possible to be implemented via a macro, or it would require some changes to the compiler: fn main() { let my_i32: i32 = -1; let my_f32: f32 = 6.28; let my_bool: bool = false; let my_string: String = fmt!( "i32: { my_i32 }, f32: { my_f32 }, bool: { my_bool }, an expression: { 1 + 2 }\n", ); // this works now, but it get's messy for many {} // let my_string: String = format!( // "i32: {}, f32: {}, bool: {}, an expression: {}\n", // my_i32, my_f32, my_bool, 1 + 2, // ); print!("{}", my_string); // =&gt; i32: -1, f32: 6.28, bool: false, an expression: 3 } 
The winapi question [is tinkered there](https://www.reddit.com/r/rust/comments/4tf0kj/qa_on_the_alternate_playground_implementation_now/d5gr56v). As for the keys and the cursor, I agree they're unconventional. Let's file an issue or something.
There has been a report for 'blog post about nothing' – as a mod I see those reports, albeit anonymously. Now while I have to agree that the backstory is off-topic, I posit that donating the writing (mostly about Rust) to the public domain and offering to write about Rust for others is apropos. So whoever thinks this shouldn't be in /r/rust, please discuss here.
Is it possible to create a macro that takes another macro in, then calls it? I tried using a token tree, but couldn't get it to work. Example: macro_rules! foo { ($m:some_fragment_specifier) =&gt; { $m(1); $m(2); $m(3); } } foo!(println!);
Right, my bad. Too bad there's no way to have a constructive dialogue about this then. Because I'm genuinely interested if I did something wrong.
Much of Rust's most ergonomic features, especially those regarding iterators, are fast because of aggressive inlining of closures. Without that, your code has to jump at each stage of the iterator pipeline, causing instruction-cache misses. With aggressive inlining, the entire pipeline gets inlined and runs just as fast as a C-style loop.
I'm looking forward too! :)
Neat! I bet this was really fun to write :) Couple random notes as I read through: * https://github.com/rrichardson/block_alloc/blob/1564c05e46111d2ee1bbdabc8239bdc7f1d17991/src/block_allocator.rs#L66: It would be a little nicer if you defined your own `enum Error` type that implemented `std::error::Error` and one of the variants was a `MemoryMapFailure(mmap::Error)` or whatever error type this call can return. * I think that you might need some `UnsafeCell` in here to safely handle type aliasing of the memory you return without invoking UB -- I'm sure there is someone else here who could give a more definitive answer. * For general tidiness, it is nice to put benches into `benches/bench.rs` and tests that don't rely on accessing private fields by being in the same module into `tests/test.rs`. * `#![deny(missing_docs)]` at the top of the crate will help you force yourself to write doc comments on everything, which is really beneficial if you want people to actually consume your library. * Check out [`travis-cargo`](https://github.com/huonw/travis-cargo) for automatically building and uploading your library's docs to gh-pages among other things. * Take this as you will, but it is a bit hard for me to follow all of this pointer arithmetic and CASing without any comments nor already being familiar with the approach used. I had a lot of comments here where I was trying to make sense of this stuff and how cells are determined whether to be in use or free or not, but I wasn't really getting anywhere so I've removed it. I think you could elucidate this a lot more for readers. * When I see non sequentially consistent atomics in use, I get nervous. I remember reading a quote (maybe from Linus?) about the "real cost" of clever lock free algorithms being the time spent debugging them when you're missing a memory fence or something on some random architecture with slightly weaker ordering guarantees than the author assumed. I liked that quote. This isn't to say that lock-free is bad or anything, just that it takes care and deserves very thorough documentation about correctness. I don't see that kind of documentation here, which makes me a little nervous to use the crate.
Well normally you don't depend on winapi directly, it is pulled in as a transitive dependency. My question is basically why you have to depend on winapi directly. If you were using cargo it would have been pulled in automatically as a transitive dependency.
I have to concede it's a bit meta – writing about writing about Rust.
&gt; This is not about me, it’s about the writing, the knowledge contained within. I really like this reasoning and I think it can also be applied to code. The small project I have on my github is released under CC Zero because of this (which seems to me to be the best way to put code in public domain)
If anybody here cares a great deal about the RedMonk ranking, I would encourage them to join the Rust community on StackOverflow and help out users of the language in the process. :)
Thank you @annodomini. It's quite common to use GB. You may take a look at the configuration of redis :) http://download.redis.io/redis-stable/redis.conf
Wrong subreddit, please post on /r/playrust instead.
&gt;It's one of the top 100 most popular crates, so it would be included. Which means you need to provide a Windows option for your playground so people can actually use one of the most popular crates. :D
I thought this was /r/programming until I clicked the comments and was about to complain that rust deserves to be higher. At least higher than prolog...
I like how you used the term discovered, instead of created. Like he was digging in the garden one day, and uncovered a box with Forth in it.
&gt; yinz's Please no.
You're looking for https://www.reddit.com/r/playrust/. 
This is an interesting concept. In the past half a year of me learning/using/playing with Rust, I've barely used Stack Overflow at all. I have gotten a few answers by finding other questions, however, this was relatively infrequent; a lot of the questions were outdated or never really answered--at least in a way for a beginner to understand. Nowadays, I almost always just head to the IRC channels if searches don't turn up any solutions. I suppose this is almost resulting in a cycle where lack of answers might lead to lack of questions on SO. 
That's why Servo exists, in my humble opinion. Having *both* a bootstrapping compiler and production-level project does help shaping the language more realistically.
[removed]
If you don't care about including the bang (`!`) at the invocation site you can use plain `ident` fragments: https://is.gd/Wa2Z2P macro_rules! foo { ($m:ident) =&gt; { $m!(1); $m!(2); $m!(3); } } fn main() { foo!(println); }
Got back to working on my text editor this week ([iota](https://github.com/gchp/iota)). Been playing around with syntax highlighting - got an initial version working here: https://github.com/gchp/iota/tree/syntax-highlight Right now its implemented as a (probably extremely naive) lexer/tokenizer. The source is tokenized before being drawn, then colors are assigned depending on the token type. So far it works, but I don't know if this is the best way to do it. I'm going to just keep throwing use-cases at it and see if it holds up :)
Disclaimer: I have zero experience writing allocators, and close to zero experience writing concurrent data structures. 1. It seems to me that the address that is passed to [`free_raw`](https://github.com/rrichardson/block_alloc/blob/1564c05/src/block_allocator.rs#L126) is not checked if it is out of bounds or if it is misaligned. If it is the responsibility of the user to invoke it correctly, it should be marked `unsafe`. 2. I don't really understand the `free_raw` logic. Where is the list of free slots? The head points to the last freed slot? Is it meant to be used as a stack? If so, why does `free_raw` take an argument? I'm not convinced that it is threadsafe...
Hi, i want to learn rust and i've been thinking that a git pre-commit hook is a good start for an useful learning example. So i want this hook https://gist.github.com/tommy-muehle/260dbbe0c30947b990df written in php implement in rust. But can anybody give me a hint, how can i interact with the git binary? Thx!
The optimizer will most likely elide the initial zeroing so I wouldn't worry about that. You can transform your loop to this more "idiomatic" version using iterators, but it's more verbose, IMO, and kind of obscures the intention in the process: for (k, state) in state.iter_mut().enumerate() { *state = k as u8; } Clippy isn't always right, of course. It catches common mistakes but sometimes what one person might consider an antipattern is actually the better solution. It all depends on the situation. Edit: in fact, LLVM actually unrolls the entire loop and stores a static copy of the array in the binary! Press "ASM": https://is.gd/xMJ1Rr Edit 2: both versions unroll to the same static array and SIMD copy: https://is.gd/s4IXAi
Also why the public and transparent RFC process is such a great boon.
Perhaps you could use [gaol](https://github.com/servo/gaol)?
[This](https://www.reddit.com/r/rust/comments/4tgv4k/performance_and_functional_programming_with_rust/) may be interesting.
Good point. But SO is the front page of the internet for many developers (me included, sorry). Maybe we could provide examples on SO and reference the actual Rust doc in these examples.
Wouldn't it be more helpful to include the examples in the documentation, and only link to the existing documentation on SO?
Ok I thought it's a good idea to add posts on SO, but now, I'm not sure anymore. Thank you for your opinion!
Ah sorry ... I found it by myself. https://doc.rust-lang.org/std/process/struct.Command.html
Is there a way to run code at library load time?
[Is Math Discovered or Invented?](https://www.youtube.com/watch?v=X_xR5Kes4Rs)
The RFC process is extremely new in the history of Rust's evolution. The experiences gained from Servo were much more instrumental in shaping Rust's design pre-1.0.
Not related to the content of the post, but I somehow just realized that your username is llogiq with two Ls. I had been reading it as ilogiq (with a capital i) and pronouncing it in my head the same way as "illogical." My eyes and my brain were somehow not connected on this one. :P
I take issue with this article on two points. 1. The author provides zero examples supporting his argument. What's an example of a compiler that's been unduly influenced by the bootstrapping process? Without any concrete examples, I'm not convinced that this is a real phenomenon to worry about. 2. Just because the design of a compiler is different than other more stateful programs, does supporting good compiler design necessarily come at the expense of those programs? For example, if I wanted a language to write a compiler in, I would make sure to include sum types and pattern matching, but those are useful features in other kinds of programs too. He says "it will stop us being distracted by shiny language features that solve issues compiler writers have that few other programmers share." But what are those language features? I honestly can't imagine what kinds of features would be actively detrimental to different applications. I don't necessarily disagree with the author wholesale--I, too, can imagine a scenario where optimizing a language just for compiler construction could be somehow maybe bad, but it's difficult for me to realize that idea without any basis in reality.
I reckon this does bear some explanation :) It is a linked list as a stack. The "next" pointers in the each node are offsets to a buffer in the mmap'd region. The head of the list is stored in the top level object of the allocator itself. It is initialized here: https://github.com/rrichardson/block_alloc/blob/master/src/block_allocator.rs#L73 When alloc is called, it looks up the next free offset by atomically loading 'head', finding the buffer using the offset stored in head, using that buffer to find next-&gt;next so to speak, then CAS'ing that in. If that cas fails (someone else has already swapped head in the intervening time) then the new head is returned by the CAS and it starts the process all over again. Using a single CAS, there is a risk of the ABA problem. That is: A takes the value of head. B takes the value of head, does some work, and then swaps in the value of head that is the same as when a took the head value. A performs the CAS and it succeeds because the value of head is the same as when she took it. This is why I combine the offset with a counter and CAS them together. This makes the above scenario impossible, as even if the offset is the same, the counter is different. On free : The function calculates the offset of the freed pointer and uses that offset to put the value into the new head of the list. It does so with the same CAS mechanism of alloc(). 
1. You're absolutely correct about free_raw. Nice catch, I need to check both that the supplied pointer is within the bounds of the mmap'd region, and is aligned to the block_size. 2. It is a linked list based stack using Treiber's algorithm. I describe the process in a bit more detail in response to /u/WrongAndBeligerent
Oh yes, I realize that GB is common for referring to 107,374,182,400; but it's also common for GB to mean 100,000,000,000 bytes. That's where the trouble lies; the fact that both interpretations are common, and thus can cause confusion. If you buy a hard drive, for example, it will mean 100,000,000,000. There can be a lot of confusion, if you have set a quota somewhere using multiples of 1024, but then run out of underlying disk space because the underlying disk was sold in multiples of 1000. If you use GiB, it is unambiguous what you mean.
I think the name "SO documentation" is a bit misleading. From my understanding, one of the bigger goals is to collect "solutions for common problems". E.g. the Rust book has a nice chapter about concurrency, but it does not go beyond the scope of small examples (rightly so!). In contrast, the Java concurrency section of the SO documentation already contains topics like "Producer-Consumer". I think such examples are out of the scope of the official Rust documentation, yet they are interesting enough to be written down somewhere.
That's what we have rust by example for, which is searching for maintainers. It would be sad if people moved their efforts towards some third party platform, while rust-by-example can be shipped easily alongside the main project and integrated into our infrastructure, e.g. CI and our playground. See the current [call for participation](https://this-week-in-rust.org/blog/2016/07/19/this-week-in-rust-139/#call-for-participation)
What problem does this solve that http://rustbyexample.com/ doesn't? (beyond Karma :) )
People actively look for "Stack Overflow" when searching for answers.
You want /r/playrust. This is the subreddit for the Rust programming language.
Something I've been working on, with help from the awesome clippy contributors. (Actually [landed](https://github.com/Manishearth/rust-clippy/pull/1091) 2 days ago.)
(Never mind) ~~Is it possible to bundle the scripts so one wouldn't need to whitelist cloudflare to use this?~~
^exactly what you said. my auto trading framework does both historical market replays (for research) and live trading. The performance for me is mainly a benefit in the historical replay department, but if we were doing HFT type trading then obviously the performance would be a big deal live. The safety is also obviously a big plus. This is the first project I'm doing in rust, I've wanted to learn the language for a long time and also wanted to update my trading infastructure so this seemed like a good project to work on
That and because Mozilla wants to build a parallel browser. :-)
I don't really think that it's possible to do this, since anyone can implement a trait (including your library's users) and as soon as they did, they would invalidate all the other implementations of your trait as they wouldn't know how to convert to the new type. I may not have explained that perfectly. What is the actual problem you're trying to solve? 
To add to this, a good amount of compiler books focus on writing compilers in/for C-like languages, and C is *terrible* to write compilers. The single feature that makes compilers easier to write is sum types, and those are good for a lot of other reasons ...
1. How about the immense number of parsing libraries and the ease with which they can be written in Haskell? Admittedly, since GHC has remained the most complicated Haskell program for a long time, it's probably an extreme example of a language that's geared towards stateless pipelines like compilers. 2. Radical immutability. It's great for stateless programs with no user interactivity, but for programs that are heavily user-driven, most of your code has to use the "escape hatch" of choice (whether that's Monads or uniqueness types). It's been hotly debated whether unique ownership is another such feature; I'd argue from the experience of C++ programmers that it's not, but Servo, with it's huge garbage-collected and reference-counted mutable graphs, is not a shining example of that (I know Servo is an unusual case, since those graphs are mandated by the specifications it implements).
It's a new thing they're trying out. http://blog.stackoverflow.com/2016/07/introducing-stack-overflow-documentation-beta/ TL;DR &gt;Documentation is community-curated, **example-focused** developer documentation, based on the principles of Stack Overflow. 
I absolutely agree with /u/fgilcher here. The preference should be to improve the official documentation, not to put all the time and effort toward putting everything on this new Stack Overflow thing.
There are a lot of old answers that are no longer relevant on SO, using syntax that no longer even exists. It's actively harmful in my opinion, but what can you do? The SO moderators will not tolerate the removal on old questions for any reason, so the situation remains. 'It's better to ask a new question the same as an old question that targets a new version of the language than deface an existing question.'
It does: let x: [i32; 5]; // do stuff x = [0, 0, 0, 0, 0]; It just won't let you _partially_ write to the uninitialized array.
Thanks for your detailed feedback. Yes, better error results is one of those things that needs to exist if this is going to be a legit library. Re: UnsafeCell, I have a vague Idea that I *should* be using it, but I don't fully understand how it could help. Re: The atomics, I used the simplest and most well understood atomics scheme that I know of, which is the Treiber Stack. I explain it more below. I should probably put that explanation into the alloc and free methods. 
Ah! Much better! That's a nice algo! More stuff: 1. `block_size` must be &gt;= 4 (size of `u32`, not just &gt; 0) 2. You can't do a `Relaxed` load [here](https://github.com/rrichardson/block_alloc/blob/1564c05/src/block_allocator.rs#L101). You should do an `Acquire` load instead because what went [here](https://github.com/rrichardson/block_alloc/blob/1564c05/src/block_allocator.rs#L146) might not get reflected [here](https://github.com/rrichardson/block_alloc/blob/1564c05/src/block_allocator.rs#L111) (in a different thread , of course). 3. I'm not sure if `SeqCst` is absolutely necessary at those two CaS-es (an `AcqRel` will probably do)... I'll read about it when I get more time. 4. The slice passed to `free` can be of any size and alignment. _EDIT:_ added a 4th, and more!!
Thanks so much for writing this! I've been hoping to write an experimental WebRender backend for Conrod soon. Your walkthrough has helped to clear a lot of fog from the path :)
&gt; I just can't get into the flow of writing when I don't have the freedom to freely reorder to optimize comprehension of the written material because the ordering of the embedded snippets of code has a more fragile semantic meaning of its own This is a good point, and may be an argument for why Knuth's original system (tangle/weave) completely decoupled the literate presentation of the code from how the code is actually generated. (Having said that, I'm still in favor of trying to force the document to follow the general code structure, for better or for worse.)
This "Documentation" thing is still unproven, I wouldn't throw too much effort in that direction. On the other hand, I would *definitely* port the stuff on Rust by Example to the Stack Overflow Q&amp;A format. It's the first port of call for programmers of every kind looking for code examples.
Nice! I'm hoping that Rust takes the world by storm, and consolidates as a heavely used language. Such a great language! 
I never encountered such problem on SO. Most of my questions related to rust has been answered quickly and friendly.
When a ladder rank rust as popular, everyone think it prove that rust is improving. When it doesn't everyone remember how biased are language ladders. Confirmation bias for people who think that rust is going to run the world.
WHOOPS thanks, you're right, it should just be yinz'
My apologies, my intent was not to demean Stack Overflow or this new feature they've introduced. There is a difference between "answering specific questions people have about Rust" (which we do not only on Stack Overflow but also here on /r/rust, on the Rust users forum, and in IRC), and adding documentation to this new Stack Overflow offering. In a world with unlimited time and resources, doing such addition would be fine, but otherwise I believe it's preferable to improve the official Rust docs, rather than invest in an unofficial documentation source controlled by a third party, regardless of that third party's popularity.
Very cool. Thanks for sharing.
No argument there. That's why so much of my documentation for my Python creations is written in docstrings. (Though, admittedly, I have a ton of projects I'm still trying to find time to bring up to my current standards.) I'd go even further if GitHub supported a clean, reliable way to build my README from them. Heck, the only reason I avoid Sphinx so much is that it has no mechanism for treating "source file completely unreferenced by autodoc" as an error (or, at the very least, as a warning). I tend to ensure my creations are either Py2-only or Py2/Py3 hybrid-compatible so I can use ePyDoc for my documentation since it supports a "get me everything in this tree of folders" mode of operation that can't let annotations like @todo or the reST equivalent (which I'm gapping on at the moment) slip through the cracks.
My wishlist is: - one place to hold examples - easy to upload examples - easy to discuss the examples The clear winner here is SO - it simply wins by default. I like and appreciate (and I'm grateful) for the Rust documentation and rustbyexample.com. However... It's virtually impossible to enhance or discuss the docs. It's virtually impossible to enhance or discuss rustbyexample.com. git clone/submit PR == virtually impossible to enhance. discussing docs/rustbyexample.com in r/rust is inappropriate. 
A big thing in rust documentation is compiling and running code examples to ensure they (continue to) work. That's something SO so far does not support (automatically). I would be very skeptical of any movement to write new docs in a place that did not have this feature.
&gt; git clone/submit PR == virtually impossible to enhance. &gt; discussing docs/rustbyexample.com in r/rust is inappropriate. Could you expand a bit about why you feel this way about these two points? What parts of the git PR process feel virutally impossible? Does the github in-browser editor help at all? Why do you think discussing docs/rustbyexample.com is inappropriate for r/rust? I've seen plenty of posts, [this one for example](https://www.reddit.com/r/rust/comments/4cfyyq/how_to_make_rust_library_reference_docs_suck_less/?ref=search_posts), that had a lot of great discussion here. What about [the users forum](https://users.rust-lang.org/)? What about an issue on the repos for rust or rustbyexample? What about those makes them inappropriate places for discussion?
&gt; Examples can be edited collaboratively (without going through Git) Can you explain why not going through git would be useful? The history and commit messages about *why* changes were made are invaluable information that I would hate to lose. I don't usually see SO edit explanation messages have the level of quality that git commit messages have.
I'm curious - why don't you want to whitelist cloudflare scripts? They're served over HTTPS and the url they're served from is, if I recall, unique to the webpage - so whitelisting one, even globally, should have little impact.
I've updated the crate and put the [source](https://malleusinferni.github.io/rust-alewife/alewife/) and [documentation](https://malleusinferni.github.io/rust-alewife/alewife/) online. The API itself hasn't changed, but I added a README with some example code. As I said, actually sending and receiving a variety of messages is currently pretty obnoxious. I'm looking at a couple different options for fixing that, and one seems clearly better than the rest. But I would very much appreciate some feedback, so please do take a look and let me know what you think.
From all areas of computing, compiler design always had the first place on my list of favourite papers to read. And used to spend endless hours on comp.compilers. Since Fortran days, the opinion against bootstrapping compilers seems to be more prevalent among the C crowd. Everyone else is quite happily bootstrapping their languages. Usually the ones that don't do it, the main reasons tend to be lack of resources or the desire not to delay their work.
git PR process - github in-browser editor: the editor isn't the problem (but it doesn't address any of my problems either). - The git PR process means that as a submitter I'm asking a single person to do a lot of work reviewing and dealing with feedback. Because the person is human there will be some bar set to maintain quality/relevance. I feel like I will never meet that bar. It's me, not you. I will feel way too guilty about taking up someone's time for the review and feedback cycle for my little helpful paragraph. On SO I can submit endless little snippets for how to handle little idiosyncrasies about Rust design/compiler errors/crates/etc. and the *community* will comment/fix/enhance. I can't ask a single person to do a bunch of work for me for free. I'll happily toss solutions/ideas over the wall to a large and growing community that has the ability to easily ignore me or comment/fix/enhance purely based on their interest/time. Wrt r/rust =&gt; this is a pretty fantastic group of brilliant people. I'd worry that if I spammed the list with little snippets/ideas/etc. that I come across on a daily basis that I'd hurt the s/n - and it just seems like abuse on my part... 
&gt; I will feel way too guilty about taking up someone's time for the review and feedback cycle for my little helpful paragraph. I know that just saying this doesn't solve the way that you feel about it, but I'll say it anyway: I am extremely grateful for each and every PR, no matter how small, and am very happy to work through any issues with PRs with their authors. It is not a waste of my time; it's one of my favorite things to do. &gt; I can't ask a single person to do a bunch of work for me for free. I'm paid by Mozilla to do this. ;)
This by itself doesn't help with build times, does it? Cargo already does not invoke rustc when nothing has changed.
Ah, I see - they are indeed served from cdnjs.
See my other reply in this thread.
&gt; The clear winner here is SO - it simply wins by default. It fails at the first item in the wishlist by default, since there is currently one central place to hold Rust examples, http://rustbyexample.com/, and so SO would add fragmentation. &gt; It's virtually impossible to enhance or discuss the docs. &gt; &gt; It's virtually impossible to enhance or discuss rustbyexample.com. &gt; &gt; git clone/submit PR == virtually impossible to enhance. I wonder if these issues could be addressed by just including two links on each page; one to the edit link for the Markdown (for example, https://github.com/rust-lang/rust/edit/master/src/doc/book/closures.md), to make it trivial for people to send in drive-by PRs to fix something, and one to a pre-populated GitHub issue or users.rust-lang.org thread to ask a question or request clarification about a topic, such as https://github.com/rust-lang/rust/issues/new?title=Question+about+closures.md&amp;body=Question+about+the+[closures](https://doc.rust-lang.org/stable/book/closures.html)+chapter+of+the+book
Very interesting project. Will def. look into. Edit: What does the proof actually prove?
&gt; Re: 4, that ptr will get checked within free_raw. No, the user might pass a slice with `len()` half the `block_size` (correctly aligned) and expect it to only partially deallocate the block!!
&gt; I will feel way too guilty about taking up someone's time for the review and feedback cycle for my little helpful paragraph. A PR doesn't take up any more of anyone's time than a change to SO documentation would. There can be the balance between PR style review before the fact vs. wiki style review after the fact, but in general, someone is going to spend the time reviewing what you wrote. In the "review after the fact" model, you have the issue where what you wrote may not be correct or may be confusing, and so several other people may have come through and gotten misled by it in the meantime. Or, in a lot of cases, it has the problem that lot of little individual contributions from independent people, without some people who are more involved and have ownership, can lead to a fairly disjointed presentation. &gt; On SO I can submit endless little snippets for how to handle little idiosyncrasies about Rust design/compiler errors/crates/etc. and the community will comment/fix/enhance. Having been a [pretty heavy StackOverflow Q&amp;A contributor over the years](http://stackoverflow.com/users/69755/brian-campbell), and also having spent some time doing Wikipedia vandalism patrol, I can say that depending on "the community", in the abstract, to comment/fix/enhance is a gamble at best. It takes a lot of time and effort to shepherd high-quality material in such an environment, keep it from being vandalized or bitrotting, not let poor quality but not quite bad enough to delete material dominate, and so on. It may seem like it's more work to have a single person or small group handling PRs, but actually, it's a ton of work, and mostly invisible and thankless, to patrol, clean, maintain, and organize wiki-style resources. In addition, Rust, and even the Rust docs, doesn't have a single maintainer that has to deal with every PR. There is a team of reviewers, any one of them can review PRs. Of course, /u/steveklabnik1 probably does the bulk of it, but that's because it's his full-time dayjob. This approach gets you a good compromise between the two worlds; there's no one person being a bottleneck, but there is a smaller group of experienced people who talk together a lot to help keep everything well organized and high quality, rather than the sprawl of variable quality and poorly organized material that you frequently see on wiki style sites without a huge investment of effort from a few people. I've suggested in [another comment](https://www.reddit.com/r/rust/comments/4tw9r2/the_rust_documentation_is_almost_empty_on_so_lets/d5la8cc) a few ideas to reduce the barrier to entry for asking questions and submitting changes. The one thing that you mention, that I don't think currently has good coverage, is examples using crates outside of the standard library. Right now, rustbyexample does focus on the standard library, and there doesn't seem to be a good place for submitting examples of how to solve things using the rest of the ecosystem, like Iron, Nickel, Piston, Serde, or any of the large set of crates that are widely used. It might be good to start with trying out StackOverflow documentation for that purpose, or it might be worth investigating whether making such a set of examples a part of rustbyexample or another companion resource would be a good idea.
I'm almost willing to let them go ahead and fail in that spectacular fashion. :)
There are many people who are quite off-put by StackOverflow; they find the the voting, the closing of questions, and so on all make it a very unfriendly environment. There's also the issue that the quality is quite variable, and the voting only goes so far in helping with that. I'm actually a big contributor on StackOverflow, I like it a lot for what it's good at, but I also find the community quite frustrating in a lot of ways, including being quite unfriendly and unhelpful to beginners and people for whom English is not their native language, and the tools frequently lead to poor, uninformed decisions (high-rep users being able to edit anything, low rep users being able to submit edits that are then reviewed by people with no experience with the topic at hand, etc), and can understand that many people wouldn't want to work with it. Given the problems with StackOverflow, I do not think it should ever become an official resource for any part of the Rust project. Of course, it will always be available and there are community members who will use it, but if there are issues with contributing to and improving Rust documentation and Rust By Example, we should fix those issues, rather than delegating them out to StackOverflow.
You want /r/playrust . This is the subreddit for [Rust](http://www.rust-lang.org), the programming language.
That keyboard sounds familiar. Are those Cherry MX Blue switches I hear?
Would you be able to explain the difference between WebRender 1 and WebRender 2? I assume it must be some major shift given that one doesn't normally give version numbers to pre-alpha projects. :)
This code is basically proving that the code you run it through with Lean is pure correct?
Good work – the previous wiki was workable, but never navigable to the extent I wanted. Thank you!
That is a really cool list and I have a question about the panic lint. Why does panic!("WTF {}"); compile? What's the difference between panic! and println!? Why is there a difference, at all?
I'm glad you brought up the 128 bit operations. I don't think any common modern processor supports 128 bit atomic operations do they? Are you sure it is just a language issue? I don't think C11, C++11, or ISPC have any 128 bit atomics. That being said I think you might be able to fake it with a loop that has multiple CAS loops. In any event with 4kB blocks you can still index 16TB 
On a very naive level, it wouldn't be too difficult - see if the structure of the program depends on an argument, i.e. a for loop parameter. However it would not be exhaustive by any means, which could lead to a false sense of security. If you want to detect the possibility of a timing attack down to the cache, well, that is computer-dependent so I don't think that will happen any time soon. Perhaps there's a tool that looks at asm code to see if it can have its time manipulated?
There's a section on the Servo wiki about this: https://github.com/servo/servo/wiki/Webrender-Overview
Actually I've now realized it's not a general-purpose CDN but CDNJS, which only hosts relatively popular JS libraries, that's why there's no unique domain. I guess it's fine. Amusingly, when I wanted to examine some cloudflare urls in the wild, the few sites from their case studies page I looked at all used cloudfront. Go figure :)
I don't believe we should: * there's already lot's of content (and probably links to that content) here * these kinds of posts are not *that* common and * they are usually removed rather quickly 
You are right, I was way off. It looks like Window 8.1 64 bit needs 128 bit compare exchange and won't run without it. Other sources say that early AMD 64 bit processors didn't support it, which I take to mean most 64 bit processors do. That's some great news. http://www.pcworld.com/article/2058683/new-windows-8-1-requirements-strand-some-users-on-windows-8.html 
Working on building support for Rust into [Kythe](http://www.kythe.io/) which is Google's framework for generically writing tooling for any programming language. Essentially I'm building a compiler plugin that extracts cross-references, callgraphs, and type information and pushes it into Kythe's language-agnostic knowledge graph. Check it out [Google/kythe](https://github.com/google/kythe). Also working on ways to publish it cleanly but I've written a cargo subcommand so when you type: ``` cargo index ``` It'll index your package and its dependencies and open a web UI for viewing cross-references.
The cache thing is just extreme paranoia, afaik there's been no published side channel attacks based on cache timings. But it's theoretically possible, and I'm not sure there's a way to prevent it due to how complex x64 is.
Can't we just get a banner across the top of the subreddit saying "The Rust Programming Language" or something?
The compiler makes basically no assumptions about raw pointers. The thing is, the optimiser is free to propagate assumptions we tell it about `&amp;mut`-ptrs, so you still have to be careful. Ultimately, the details around how references and raw pointers interact is still unclear. There's some active work going into specifying it, which will help with this kind of thing in the future.
&gt; Re: The atomics, I used the simplest and most well understood atomics scheme that I know of, which is the Treiber Stack. I explain it more below. I should probably put that explanation into the alloc and free methods. Yes, after reading those comments, I felt a bit closer. Definitely think that kind of treatment belongs in the sources. Good luck!
Well, I see at least one such a question per week in my RSS feed reader.
Ah, my bad - I've actually seen the aes vulnerability paper before, but it's been a while and I assumed it was due to some other, more obvious, timing issue. I should have double checked before embarrassing myself!
https://www.youtube.com/watch?v=Uy5QcMp4kCs edit: I posted this as a cheeky comment, but now that bot has made this joke obnoxious. Anyways, look forward to all the details being aired. I think this could be a very valuable addition to the rust ecosystem.
As I suspected, using the slice (or PhantomData in my own version of the slice) results in the borrow checker complaining if I try to allocate more than one (mutable) thing (which one would expect). I guess I'll have look into the Cells. Anyone got any ideas? I'd prefer to reduce runtime overhead if possible (so I'd like to avoid reference counting).
Working on optimizing the NES emulator (https://github.com/Redattack34/Corrosion) I wrote a few months ago. I used Cargo's benchmarking, running one full frame per iteration because that seemed like a good way to test the whole system. I ran these benchmarks with callgrind to figure out what needed optimizing. Fairly standard stuff. I kept having this problem where the benchmark time in valgrind would improve after an optimization, but stay the same when just running cargo bench. To cut a long story short, just a couple hours ago I realized that Bencher tunes the iteration count based on the time it takes to complete an iteration. Valgrind added enough overhead that the benchmark didn't execute more than five frames before starting over. The two ROMs I'm using don't even enable graphical output until about frame 6 or so, so valgrind was totally missing some major parts of the code I was trying to optimize. Lesson learned: verify that you are measuring what you think you're measuring before you spend four days optimizing things. It wasn't a total loss, though - I learned a bunch about reading LLVM IR/ASM and got to fool around with the SIMD crate a bit. While I'm here, a question. Given a tuple of numbers (x, y), I can use the pattern (0...240, 0...360) in a match expression, but I cannot use (0...240, 1 | 2). Why? If I can use ranges when destructuring, why not |?
Mesos could use so much Rust. Clickable clicks * https://att-innovate.github.io/torc/ * https://github.com/att-innovate/torc_sub_scheduler 
&gt; Treiber Stack TIL * https://en.wikipedia.org/wiki/Treiber_Stack * http://www.cs.tau.ac.il/~shanir/nir-pubs-web/Papers/Lock_Free.pdf * https://www.cs.bgu.ac.il/~hendlerd/papers/scalable-stack.pdf
I don't think that constant time is compatible with an optimized, high level language. Anything that should be constant time should be written in assembly or compiled to assembly with a custom purpose compiler.
SO really only has high visibility because it shows up in search results. If you search for Rust issues, you end up on users.rust-lang.org half the time, so it works just as well for the most part. Nobody who wants an actual answer to a problem is going to narrow their search to just SO.
No it's not, `*mut T` is effectively just a number. You can get `&amp;mut`/`*mut` aliasing without using unsafe: fn aliased(_: &amp;mut i32, _: *mut i32) {} fn main() { let mut a = 0i32; let mut_ref = &amp;mut a; let mut_ptr = mut_ref as *mut i32; aliased(mut_ref, mut_ptr); } edit: Note, you can't *mutate* `a` through `mut_ptr` (using unsafe code) and then read via `mut_ref` (or vice versa IIRC).
&gt; it's unclear about whether or not &amp;mut + *mut is UB Doesn't that make it UB?
Lower overhead to contributing.
That may be true for the initial publish, but wiki-style things tend to get surprisingly hard once you want to _change_ things.
If you are arguing about other peoples time, be aware of all the work that comes downstream when it comes to documentation. Rust moves - there's deprecations, bugs, and new ways to solve things. The documentation must reflect that. The people running the project need to have some way to quickly get around and check things. rustbyexample - I can just build that on top of the compiler, run all checks and see if they still work. I can easily ship it alongside with all our documentation, I can literally do anything I want and have time for with it. SO docs - it's just a collection of notes of varying quality, submitted by individuals. Trying to change that is incredibly tedious from a projects perspective. I agree that submission to RBE should be easier, especially as the whole toolchain around it is a bit unwieldy, but that can be improved - it brings a lot of benefits.
Perhaps the stack overflow documentaton pages could contain links to the standard rust documentation. We have the book and Rust by example. Sometimes the faq and autogenerated docs are also important.
I wonder if something like Warnock subdivision could be used instead of uniform tiles for the "binning" stuff. See e.g. http://www.codersnotes.com/notes/warnock-subdivision-for-deferred-lighting/ Briefly, it does a "perfect" 2D subdivision based on a bunch of input bounding boxes so that each output tile has a list of all the input boxes that overlap it (with no "slack" due to rigidly sized tiles)..
This is not important.
By examining parts of the program that haven't changed, it can identify a large portion of code which may not have meaningfully changed. It's great for development so that your focus doesn't break for a quick recompile. 
I believe your edit is actually not true either; something like let mut_ref = &amp;mut x; { let mut_ptr = mut_ref as *mut T; mutate(mut_ptr); } read(mut_ref); basically has to be legal to allow function to use raw pointers derived from mutable ones (and where else do you derive them from?). AFAIK the compiler just assumes that any write to a pointer invalidates everything, unless it can prove otherwise. This is basically what C already does.
The two are *not at all* equivalent. In the C version, you pass a pointer to `data`, into which `wkhtmltopdf_get_output` presumably writes its output. Here, you're passing a totally random pointer to memory that may or may not exist that the function then proceeds to try and scribble over.
thanks. that helped. still took me a while to reason out the fix, but probably for the better. After realizing `ptr::null_mut()` didn't work either, it took a bit of mind-bending to realize I didn't want a mutable null pointer, but rather a mutable pointer to a null pointer. This seems to be all I needed: `let data: *mut *const c_uchar = &amp;mut std::ptr::null();`
It does heavily rely on MIR, yes. It even needs MIR-trans to be fool-proof, although that is something that will not show in many smaller crates. (see https://github.com/rust-lang/rust/issues/34151 for some more information)
You're *way* over-thinking this. You should be able to just directly translate the C code. let mut data = std::ptr::null(); let len = wkhtmltopdf_get_output(converter, &amp;mut data);
&gt; Btw. is it just me or has Rust gotten slower with the last few releases? There has been this: https://github.com/rust-lang/rust/issues/34891 A fix for this is already in the queue ([34917](https://github.com/rust-lang/rust/pull/34917)). When that has landed, performance should be back to the level it was before.
&gt; I wonder if these issues could be addressed by just including two links on each page; one to the edit link for the Markdown (for example, https://github.com/rust-lang/rust/edit/master/src/doc/book/closures.md), to make it trivial for people to send in drive-by PRs to fix something, and one to a pre-populated GitHub issue or users.rust-lang.org thread to ask a question or request clarification about a topic Definitely a must have !
Surely multiple .o files with lto is equivalent to a monolithic .o file for runtime performance?
 &gt;Incremental compilation meant to bring "good" performance but the resulting program will certainly be slower. If that is true, will it be possible to add some compiler flags to enable the old behavior, so the final program has the maximum performance?
It seems to translate Rust to definitions in Lean theorem prover. It uses MIR. It seems comparable to C to Isabelle definition translator included in seL4.
That's part of the "extra rules" I mentioned. Figuring out what the precise rules *should* be is hard, and we've been discussing it here and there for a while now. Especially since different people have different ideas about how it should work. 
The problem with your trait is that you required Instruction to by Copy, which implies that it's Clone, and Clone is not an "object safe" trait because of the function `fn clone(&amp;self) -&gt; Self`, which depends on the compiler knowing the concrete type "Self", so it cannot be dispatched dynamically.
It doesn't look like instr_table is ever modified? You should just make it a const in that case, i.e: const instr_table: [&amp;'static Instruction; 256] = [&amp;InstructionStruct1, &amp;InstructionStruct2, ...] Or if the structs need runtime initialization, use [lazy static](https://crates.io/crates/lazy_static).
You're right, but that would imply moving the instruction table outside of the Cpu struct or using the associated constants experimental feature. Since I am just learning the language I would like to stay away from the latter for now. I had considered moving the instruction table outside of the Cpu before but it felt like a workaround to the problem rather than a solution. I think I will go with this for now though.
Ahh. I've never heard Razer switches before. I'm still trying to decide on whether I prefer Cherry MX Blues (a Rosewill [RK-9000I](http://www.rosewill.com/rosewill-rk-9000i-mechanical-keyboard-with-cherry-mx-blue-switches.html) that I got on sale) or buckling springs (a [pre-2013](http://www.pckeyboard.com/mm5/graphics/ProductNews7-25-13.pdf) (104-key *and* standard right modifier block layout) [Unicomp Classic 104](http://www.pckeyboard.com/page/product/UNI0446) that I finally managed to get a hit for on eBay).
Yes. It could definitely allocate a 2d array of slices.. there would still be some ptr arithmetic though, because it puts the "next offset" value into the start of each buffer. I could also make that a struct and transmute the head of the slice into it. But for some reason I actually think this is more clear :P 
I have no affiliation with Honeypot (even though I enjoy honey from time to time), I just saw that they open sourced this and it is written in Rust. They wrote a bit about it [on their blog](http://blog.honeypot.io/open-sourcing-searchspot/).
That's just because in Java, it is *impossible* to have anything outside of classes.
As someone struggling with 15-20 minute build times, I can't wait for incremental compilation. Really glad to see solid work emerging in that direction.
Every time I see more companies using Rust it just makes me happier knowing it's being used in production and is filling a legitimate need. It's not so young and untested anymore! :D
Re-tagging is possible; we could create a "rust 0.x" tag and either add it to suitable Rust questions or replace the "rust" tag with it.
IRC and SO provide different long-term benefits though. The goal of SO is to provide long-lasting questions/answers so that users can search and get their answers without anyone repeating them again. It's made to scale out. IRC is not as searchable, the interacitivity makes for a more tailored answer, however the unstructured nature of the conversation... in the middle of other conversations, makes it difficult to search and re-use. As Rust grows, I think that having "static" documentation (such as SO), will be necessary to scale. However I'd like to note that SO isn't good at explaining complex topics, providing tutorials, etc... so it's not enough.
Thanks.
&gt; There is some value in stopping and asking if effort towards one of these locations might be better directed into some more central effort. Does this really need to be centralized? I think that having various channels of being able to contribute docs/examples is a good thing, as those channels would compete or complement each other. If the rust docs on Stackoverflow Documentation is 'empty' as this post is implying, it could mean that the rust community, in its current state, has been doing fine without SO Documentation. But SO Documentation itself could help grow the rust community; it's simply another choice for rust devs to learn from/contribute to. After all, we all have different learning styles. I elaborate more on this argument in another comment in this same thread: https://www.reddit.com/r/rust/comments/4tw9r2/the_rust_documentation_is_almost_empty_on_so_lets/d5mlgzo ------- I don't think we as a community should shift all/most of our focus into using/contributing SO Documentation. I had interpreted this post to bringing the attention of the rust tag on the Stackoverflow Documentation site; but that's just me :). 
Annnd.... posted! https://kha.github.io/2016/07/22/formally-verifying-rusts-binary-search.html
By the way, I'll also note that I was responding to a post that said: &gt; My wishlist is: &gt; &gt; * one place to hold examples And I was pointing out how SO Documentation actually makes that problem worse, given that Rust by Example already exists. &gt; If the rust docs on Stackoverflow Documentation is 'empty' as this post is implying, it could mean that the rust community, in its current state, has been doing fine without SO Documentation. We should also note that Stack Overflow Documentation was just released yesterday. It's brand new, and its emptiness doesn't mean much (nor is it particularly empty any more). I do find it a little frustrating, though, how sometimes people have a tendency to get very excited about something right when it's released, push hard to get people to use it, and then forget about it later on. SO Documentation still has yet to prove itself as something of lasting value, as opposed to something that will get a lot of attention at first but go less and less maintained as time goes on.
That's amazing, I'm impressed :o
There is no formalization of Rust semantics, so this didn't prove anything about the Rust program itself.
&gt; LTO takes [independent compilation] away again because it needs everything in one big compilation unit That sounds like whole program optimization, not link time optimization. I'm used to WPO meaning "all sources compiled and optimized in one big translation unit", which is how I interpret your description, and LTO meaning "separately compiled and optimized units are merged then optimized again". Put another way, I'd expect LTO to operate completely on multiple object files that have some metadata in them and not require sources at all. I'd expect LTO and incremental compilation to work together to give reasonably fast builds with very good performance, though perhaps not as fast as incremental compilation without LTO nor as high performance if all sources were recompiled in one big unit. I'd be completely surprised if one precluded the other. Am I misunderstanding you or does LTO have a different meaning to Rust?
To follow up on this, I've filed a few issues against various Rust documentation projects to help try to address these issue: * I propose [adding some helpful links to the docs to make it easier to submit PRs, issues, and start discussion threads](https://github.com/rust-lang/book/issues/141), all via web interfaces * I propose [expanding the scope of Rust by Example, or starting a companion book, for topic-based ecosystem examples](https://github.com/rust-lang/rust-by-example/issues/775) * I propose [moving the example code inline in Rust by Example](https://github.com/rust-lang/rust-by-example/issues/776), to make it easier to edit a single file to change both the prose and code when updating examples Do you think that these changes, or something similar, would help lower the barrier to entry to contributing or discussion? Or is there something else about the SO experience that you find preferable?
My understanding is that the author formalized the semantics of a subset of Rust. That's okay if the program analyzed actually only uses this subset. And it's practical because Rust was designed to support separate compilation (it can compile a crate that only contains the binary search). Now, to *run* a verified Rust program, you also need to also verify the compiled binary (like the sel4 guys did) or the compiler itself (like the compcert guys did). Proving some Rust source code correct is not enough.
I think it would be a great feature of an IDE to highlight functions and tell you if they are pure or not.
It actually surprises me that so many languages are self-hosting. The article mentions a few reasons but I don’t find them very convincing. Ultimately it boils down to “use the right tool for the job”. And surely not every programming language is or should be the optimal tool to write a compiler with?
I stand corrected!
If it is a DSL no. But if it is a Turing complete language, then it helps to get a feeling of the language ergonomics and fix design errors before it is too late. Language designers that use a third party language as implementation mechanism tend to get out of touch how the language is supposed to be used, because they spend 90% of their time implementing new features and bug fixing, not actually using the language itself. 
Thanks for the heads up!
That's really neat! I've been waiting for rust to break ground in this space. Did you work on this? If so, how'd you find it? Did you run into any particular pain points with the language our our ecosystem?
You're right that this is not a formal end-to-end proof. But as far as I know, that is also true of most verifications on top of a formal semantics, since you still have to relate that semantics to your compiler output for that. I only know of the amazing seL4 project that provides a non-trivial, true end-to-end (C-to-assembly) proof.
This work is cool! The picture on this page makes me sad, though. Rust's logo is a sprocket, not a gear. Space them apart and put a chain around them :)
&gt; Rust only imposes one opinion on your program: that it should be safe. I would argue that the only real difference at the runtime level between programs you can write in LLVM and programs you can write in Rust is that Rust requires all of your programs to be safe If the goal is to emit only safe code, that means you're limited to the safe data structures that Rust provides. That might not be flexible enough for a compiler. For example, I don't think there's a way to take two mutable references out of a HashMap at the same time, short of calling `iter_mut` and looping through the whole thing to find the keys you want.
&gt; I really want the discussion to be at the bottom of each RbE page. It would not be intuitive to be pointed to a list of discussions about all pages (how github works). Hmm. There are a variety of documentation systems that have discussions at the bottom of each page, like PHP and Apache. Most of the time, the discussion winds up being something incredibly specific to a single user, something that only makes sense at a particular time, or something where someone suggests something that is not particularly helpful. On SO, a lot of the discussion, especially well after a question has been originally answered, winds up being people trying to ask more, only slightly related, questions of the person who answered, since they seem to know what they're talking about. I generally find that discussion and documentation has vastly different lifetimes in which it's interesting. Documentation should be of long-term value. If there is something of long term value embedded in a discussion, it should probably be extracted out into somewhere more organized. But having the discussion lingering at the bottom of the page just kind of junks it up. I feel like links to open up a discussion, issue, or PR from the documentation would be the right balance; allow you to easily submit one of these more ephemeral discussions, but without having it permanently sitting down at the bottom of the official documentation, confusing people who come by later. In general, I find that discussion on SO works well when used for it's intended purpose, quick clarifications or the like, but so much of it is just people asking more questions in the wrong place. &gt; Works for projects that don't use github (none of my projects are allowed to live in github). Can you clarify this a little? Do you have public projects that can't use GitHub, and you would like them to be discussed? Or are you talking about proprietary, non-public projects? In what ways would those be documented on Stack Overflow Documentation?
&gt; I had previously considered waiting for a random amount of time before returning and before beginning calculation, but then I realized that since the attack depends on comparing average times regardless, this wouldn't do anything except raise the average time. What you can do is: let target_duration = random_time(); let start_time = now(); do_crypto_stuff(); sleep_until(start_time + target_duration); that way the amount of time is independent of the amount of time the crypto takes, as long as `random_time` is set to always return times larger than the crypto will take.
I think the best approach would be to write it in a high level language, combined with a custom compiler. That would make it easier to verify the implementation, while also ensuring that the compiler isn't doing any funny business. In the case of a block cipher, simply verifying a few test vectors is enough to ensure correct functioning (except in the face of a deliberately perverse implementation, which should be easy to see in the source code). With bignum arithmetic, it's a lot easier for subtle bugs to slip in, so you'd probably want at least an SMT solver of some kind for verification.
I have now looked into it. One nit: your 'mutation to let binding' translation may actually change behavior if the scope ends early, e.g. see let mut x = 0; { x +=1; } x vs. let x = 0; { let x = x + 1; } x
I think you are not taking care of double frees... And the head probably becomes a circle?
This will only work well for sufficiently Rust-like languages. As an extreme example, there's no large benefit in compiling brainfuck programs to Rust (as opposed to LLVM).
The intention with compiling to Rust is to not write your own type system. Let Rust do that for you instead. I agree it would be redundant to do both. 
You can write plenty of safe Rust code that will behave like a scripting language, the only issue is that you'll have lots and lots of Rc's, Cell's, RefCells, Arc's, and various sync methods. Which looks messy, but is oftentimes what a scripting language is actually doing behind the scenes. So I think the idea is that you can write a scripting language that targets Rust and have your languages compiler identify when to do the 'nasty plumbing' for you. In theory the resulting executable should still be performant compared to many other scripting languages, as it is compiled and optimized with LLVM. And another advantage is that you don't have to get down and dirty with LLVM/bytecode programming if you are unfamiliar with it, and with rust you can't screw up a million different things that you'd be at risk at with LLVM. That said, I don't think implementing it in Rust would be any easier, but it might make maintaining the language and working through certain bugs easier after a large initial effort. The further away from Rust your language gets, the harder it would probably be. I should add that I do agree with you that Rust is missing some safe low level features that would make this task easier. 
It's kind of done already. https://github.com/dpc/slog-rs/issues/16
True, but I've always wanted a statically typed scripting language with great type inference. This might be an easy project in Rust, just slap some inference on top of your program and generate correct Rust type signatures. It would be a very long undertaking starting from LLVM.
&gt; Though if you see your language being used for more than a thesis, or something to talk to your coworkers about, I think native code should be what your compiler spits out at the end. No compiler ACTUALLY does this, they all compile to more intermediate steps. Then some part somewhere generates native code from some other intermediate step. Technically all of those steps can be individually generated.
I just wanted to point out that the article would be more agreeable if this restriction was stated up front.
If you are forced to use Rust's type system, then your language is no more than an alternative syntactic outfit for Rust. Maybe someone REALLY hates Rust syntax and that would be worth it for them, but I don't see the point.
The cryptic error in rust should not be a problem for a highlevel language. The compilers are not expected to produce ill formed result. They should check their input to guarantee that.
This is cross posted from /r/programming and /r/linux but I think the fact that a few pieces were written in Rust made it relevant for /r/rust.
Check out some of the ongoing work for making errors easier to read. It's gorgeous: https://internals.rust-lang.org/t/new-error-format/3438
Have you looked at Clang hardening passes? I am specifically thinking about CFI (Control Flow Integrity) which hardens not only jumps but also function pointers/virtual function calls.
Ideally, `configure --target=x86_64-unknown-linux-uclibc &amp;&amp; make` should Just Work but the Rust Build System doesn't work with "custom" targets (everything that's not in `rustc --print target-list`). The proper way to make this work is to add a `x86_64-unknown-linux-uclibc` target to the compiler. Check [this PR](https://github.com/rust-lang/rust/pull/31078) to get an idea of the changes that are needed. Given that this new target is similar to the existing `x86_64-unknown-linux-gnu` one, adding the target specification and tweaking the build system shouldn't be that hard. The beef of the work is going to be adding uclibc support to rust-lang/libc, [this PR](https://github.com/rust-lang/libc/pull/122) and [this PR](https://github.com/rust-lang/libc/pull/163/files) should give to an idea of the amount of work involved. I'm not familiar with buildroot so I can't comment on that but even if it doesn't use the existing Rust build system, someone is still going to need to add uclibc support to the libc crate.
No, I've never heard of clang hardening passes. Link? (I'm searching now) Would those be reusable passes that exist as a library within llvm that I can reuse in building the compiler?
&gt; Do you think that I could just remove that flag in the file mentioned to get it to produce a dynamic library? That *might* work; you'll have to try it yourself. I tried something similar before: making the mips-musl target link to musl statically (because it links to it dynamically), but that didn't work because libunwind doesn't support MIPS. You are going to need more changes than just that `-static` flag though; [this old gist](https://users.rust-lang.org/t/static-cross-compiled-binaries-arent-really-static/6084/7?u=japaric) should give you a better idea of the required changes.
I saw your blog, and I greatly appreciate you responding to me. I've been trying to follow that pr (first one), and the steps for the past day or so to get exactly that, but I've been having trouble. I didn't know about the other two that you mentioned. I'll read over those and see if I can't glean anything. Did you see what I was talking about with the problem I mentioned of getting a gnu rustc? I really need help with that right now, that's my current hurdle.
Thanks, I'll try it then.
I'm having trouble with `nom`. I have parser that is a tuple of two subparsers that accept the empty string, but the whole thing does not. Example: named!(tuple_opt&lt;(Option&lt;&amp;[u8]&gt;, Option&lt;&amp;[u8]&gt;)&gt;, tuple!(opt!(tag!("A")), opt!(tag!("B")))); With this definition I expect `tuple_opt(b"")` to return `Done((None, None))`, but it returns `Incomplete(Size(1))`. Why does it do that, and how can I get it to work as I expect?
Oh cool. :) I didn't know about any of those things! Thank you so much!
oh awesome!!! Thanks! Yeah, I'll share my cross compile code changes of rust on github soon and pm you.
The github description still needs to be updated. "A small Python utility to compare Rust micro-benchmarks." :D
[CloudABI](https://nuxi.nl) is another very interesting sandboxed POSIX environment. One binary for all operating systems, based on Capsicum – but your app starts already in capability mode instead of entering it by calling cap_enter, all file descriptors are passed from the outside by the launching process. Works out of the box on FreeBSD, requires a modified kernel (for now – goal is to ship as a loadable module) on Linux and NetBSD, uses a userland syscall translator on macOS. I hope Rust will support it as a target someday… Speaking of Capsicum, I made [rusty-sandbox](https://github.com/myfreeweb/rusty-sandbox) which is designed around its capability model, but also works with macOS's sandbox. Should be trivial to add OpenBSD's pledge, but I haven't got around to doing that.
Thank you!
Actually, with current Rust, there is no longer any difference between `to_owned()` and `to_string()` (for `String` / `&amp;str`) thanks to specialization.
This looks great. You should do a PR to RustyCode.
[yasteroids](https://github.com/kvark/yasteroids) is an example of a project which uses both specs and gfx, if that's what you are looking for.
That's pretty neat.
Totally forgot to, thanks! PR is [here](https://github.com/saviorisdead/RustyCode/pull/151)
Dev here. Rust is currently on LLVM 3.8.
Empirically, which is more common when calling `realloc()`: the allocator resizing the allocation in-place and returning the old pointer, or the allocator taking a new allocation, copying the data in the old allocation to it, and returning the new allocation? Context: https://github.com/cybergeek94/buf_redux/blob/master/src/lib.rs#L378 It looks like I'm just reimplementing `Vec::reserve()` here, but I figure I can save the allocator from having to copy the data in the old allocation to the new one if there's no useful data left. If there are bytes remaining in the buffer, I can instead move only the valid bytes to the new allocation, and at the same time move them down to the beginning of the buffer so there's more room for reading at the end. However, this prevents the allocator from simply resizing the allocation in-place because I have to take the new allocation before freeing the old one. So if reallocating in-place is the more common scenario, it's worse average performance for this method to implement the buffer resizing myself, because I'm doing a copy I wouldn't have to do otherwise (though I'm still moving the bytes down to the beginning, so it's not entirely wasted). Of course, I can call `alloc::heap::realloc_inplace()` myself and only do the reallocation+move if it can't be done in-place, but that requires using unstable APIs and reimplementing even more logic from `Vec`. When custom allocator support is implemented for `Vec`, I can probably do it that way as well, but that would require moving the `pos` and `end` fields into the allocator so it can use those to decide what to copy to the new allocation (or if any copying is needed at all). 
so this no longer valid? https://medium.com/@ericdreichert/converting-str-to-string-vs-to-owned-with-two-benchmarks-a66fd5a081ce#.f6kefuuqm
Built an Erlang version of one of my old benchmarks, and took the opportunity to bring a lot of other stuff up to date: &lt;http://github.com/BartMassey/ttt-bench&gt;. tl;dr: Rust is about 100x faster at solving tic-tac-toe than roughly corresponding Erlang.
You probably meant to post this in /r/playrust.
/r/playrust
I forgot it is a private repo. I fixed it.
love it ~~I used to be a Rails guy. Just curious. https://github.com/farcaller/shiny is what you saying about rspec like BDD with compiler plugin? 
Yes ! But in my experience stainless ([crate](https://crates.io/crates/stainless), [github](https://github.com/reem/stainless)) is more maintained. The thing with compiler plugins is that they brake _often_. Like, _ohgodno why did I `rustup update`, whyyyy_ ? So, you really need a updated project. And being able to use nightly, which sometimes you don't. I hope this will helps people like me !
Did they actually open source the server?
Woo! This is great! I only played a little bit with termion, but I was very impressed! On a different note, I'm really glad to see another library go to v1.0 (after the preparatory announcement from /u/burntsushi about regex some time ago). I think the Rust ecosystem suffers from a overabundance of caution and sticks with prerelease versions for too long, making diamond dependencies really difficult. I really wish we'd go to 1.0 more readily and then just bump to 2.0 and 3.0 if we do need to break the API. Most of the time when I have a diamond dependency problem in rust, turns out just bumping the numbers in my direct deps fixes it without any additional changes. 
Well, Signal's server is open source. Don't really see how that makes a mess.
Reddit lets you submit from anywhere; including the front page. If you do that, you have to type the Subreddit in...
I'm friends with the maintainers of rspec, and one of them has been working on some stuff for Rust in his spare time. I'll point him to your repo, I don't think he uses Reddit.
Crazy idea: Set Cargo's default version to 1.0.0-rc.1 😄 ^(no don't do that I'm just kidding)
One letter short or mispronunciation from the name being an innuendo. Great project! :)
How does this compare feature set wise to libvterm? (Is it for drawing to the terminal like ncurses or is it to help with implementing a terminal emulator?)
/cc /u/carols10cents
Examples get compiled as part of cargo test, but they're just supposed to be .rs files, not while sub-packages.
... and this is /r/rust (which I didn't notice in my inbox). This comment is completely off topic and pretty racist. Please refrain from making such comments in the future.
[removed]
Did you know Honeypot are also involved in running Rustfest? :)
Ah, you beat me to the punch! I was going to blog about the release. I guess I'll have to hurry up the other features I have in the works, then there is something new to write about. 
Ah, sorry! Give me a heads up next time and I'll hold back. :-)
http://www.reddit.com/r/rust/comments/4nls4a/_/ describes a good incremental method (by /u/carols10cents) There was another similar post around the same time about porting (musl?) libc, but I can't find it.
I have a project that does some heavy, slow data processing. I'm used to using the valgrind toolset for profiling, and I've never used flame graphs or anything else. I've definitely 'hit the wall' performance-wise with the code and this is *very* helpful and relevant to what I'm doing.
Yeah, I should have said something. It's fine, now I get to hint at great new features that have yet to be revealed \^\^
Wow this is great, I definitively will use it soon ! Thank you so much for writing this library !
Damm, these abstractions! Great post, very helpful for those new to native code profiling.
Fermion? How on earth is that an innuendo? 
Boobs are made of fermions! Boobs!
I would definitely recommend bringing up ideas on the issue tracker before spending significant work though. I tend to feel pretty strongly about keeping these types of tools simple. :-)
Nice, I love when idiomatic safe wins over unidiomatic unsafe (index without bound-checking).
I've been waiting for a crates.io release since the code is no longer using unstable features and turns out it's 1.0, awesome. ^^^Even ^^^got ^^^a ^^^mention ^^^in ^^^the ^^^changelog, ^^^yay
Hey folks - am I right in thinking that there's no safe/idiomatic way to allocate a custom dynamically-sized type? ie, no safe way to produce a `Box&lt;(i32, [u32])&gt;`? If I were to allocate that type unsafely using `malloc` or `heap::allocate`, is there any idiomatic way to produce a custom fat pointer, or would it just be a matter of transmuting from `(usize, *mut ())`? If so, is that representation of fat pointers stable? Context is that I'm writing a custom smart pointer type which points into a custom heap, and I'm having to write a lot of special-case code to support both single allocations and array allocations... are there any active plans to make `?Sized` generic parameters more powerful?
They have nothing in common really. msgpack is binary json and protobuf is a serialization format thag needs an external schema. 
Great blog post, really enjoyed the quality of your writing! Just a heads up, on your blog archive page you have \&amp;lt;/div\&amp;gt; Which means I see `&lt;/div&gt;` on the page, which may not be what you want. 
Found the musl thread!! https://www.reddit.com/r/rust/comments/4nqwfg/baby_steps_slowly_porting_musl_to_rust/
I wrote original post. I've since worked with the code a little more and can't find any reason why it's a bad idea. Has anyone used the described pattern? Does anyone know reason I shouldn't use it?
Really? I'm kinda scared that `get_unsafe` completely broke the optimizer. That really shouldn't happen.
Thanks! And thanks for the heads up :).
Press F1 and paste link to join { client.connect 207.65.153.12:28015 https://rust-servers.net/server/82179/ Oxide Modded - Zlevels - Remove tool - better loot out of barrels and boxes - Set homes up too 5 - Live map http://map.playrust.io/?207.65.153.12:28015 - info panel - quick craft - Rank me - Map size 4.4k -High Fps Noob friendly come play, have some fun. MORE INFO AT OUR WEBSITE http://www.i4ani.com/ ALSO SANTA COMES AT NIGHT. 
/r/playrust
Well, as I understand, the SFI components and the sandbox complement one another. You don't really have much without an entire system for mitigation. 
The main issue with bounds checks, when it comes to optimisations, is that it prevents vectorisation. Often the bounds checks have little effect on overall performance, as the branch predictor will eat a lot of the cost they may have imposed. Remember, the bounds checks themselves are just a compare-and-branch. The panic, however, is killer when it comes to certain loop optimisations, since it's essentially complex control flow within the loop, and therefore inhibits vectorisation. One thing to try, based on the ASM in the post, is to read the checkpointed value before the loop, something like: pub fn get(&amp;self, bwt: &amp;BWTSlice, r: usize, a: u8) -&gt; usize { // self.k is our sampling rate, so find our last sampled checkpoint let i = r / self.k; let checkpoint = self.occ[i][a as usize]; // count all the matching bytes b/t the closest checkpoint and our desired lookup let count = bwt[(i * self.k) + 1 .. r + 1].iter().filter(|&amp;&amp;c| c == a).count(); // return the sampled checkpoint for this character + the manual count we just did checkpoint + count } The reason being that LLVM won't hoist the load above the loop due to the bounds check in `bwt[start..end]`. Looking at the `perf` outputs, it looks like a good chunk of time is spent loading from `self.occ` at the end. By hoisting it up before the loop, it should be able to hide the latency, since the usage isn't until after the loop. No guarantees of course, but if the loop is very hot, it might help.
 if let Some(val) = foo().or_else(|| bar()) { doStuff1(); doStuff2(); doStuff3(); } [Documentation](https://doc.rust-lang.org/std/option/enum.Option.html#method.or_else).
Further investigation shows that it is possible to get a vectorized loop in [safe code](https://play.rust-lang.org/?gist=aeb548c54f61c875881cca8143f39a28&amp;version=nightly&amp;backtrace=0). It appears to be what you got with the last version (the unchecked indexing), since you said it's slower, I decided to investigate further (it's rare that vectorized code is *slower* than the regular version). It's been an adventure, let me tell you. So, for those that don't know, vectorization is a tricky optimisation. You have to ensure that the semantics of the original loop are preserved and deal with things like alignment. What this means is that the original loop is normally kept around for cases that require one-at-a-time processing, with checks that use it when appropriate. While you end up with a lot more code, it's often a lot faster due to being able to process multiple elements at once. For the code above, what gets generated is a loop that keeps two `&lt;2 x usize&gt;` vectors around for the counts of the first, second, third and fourth elements in each block, at the end of the vectorized loop it adds the two vectors together, then adds the elements in that vector to get the almost-final result. The last thing it does is handle any trailing elements. If there are, say, 21 elements to process, it has to handle that last one separately otherwise it might accidentally read off of the end of the array. This is where it starts to get interesting. See, LLVM can figure out that the number of elements to process is `r % self.k`, since `bwt` is split into `k`-sized chunks and you only process a single chunk. So if `r % self.k` is less than 4, it knows to skip the vectorized code. The problem is that in most of the code (tests and benchmarks) I've seen in `rust-bio`, `k` has been set to 3, meaning that it never hits the vectorized loop. The precise reason why it's slower *then* is harder to ascertain, but the checks at the start aren't free and you've lost some memory locality. I'm not familiar enough with this stuff to know if 3 is a reasonable number in the real world, but if it is, it would explain why the "C-like" version ended up slower. In a weird twist, the prevention of an optimisation is what helped! Also, the equivalent function in C is similarly vectorized, so it would likely have similar performance.
I investigated the unsafe case and figured out what was going on. Turns out, it would match C speed! https://www.reddit.com/r/rust/comments/4udxmr/rust_performance_a_story_featuring_perf_and/d5ppecq
Why did I watch it? I feel like I permanently lost couple of IQ points. :( Anyway, OP: This is subreddit about Rust programming language, not the game.
Wow, such a quick answer. Thank you. Can you explain to me why you choose to use `or_else(|| bar())` instead of `or_else(bar)`?
(Note that `Option::or` requires that `bar()` is cheap and has no side effect [:)](https://xkcd.com/541/)
There is no big reason (there is some case where you are actually required to use the former, especially when `bar` is a generic closure, but anyway), but I assume that you've used `bar()` as "any expression with a possible side effect" so I wanted to give a better template.
Great, the front page of message pack doesn't link to this repository while it implement it for other language.
The MIR flag is `RUSTFLAGS="-Z orbit"` (without the underscore). Also, using a combination of inline and unsafe helped ~20%. I think it's a bit too ugly to use, but it may give some idea about where improvements are possible. #[inline] pub fn get(&amp;self, bwt: &amp;BWTSlice, r: usize, a: u8) -&gt; usize { let i = r / self.k; let count = bwt[(i * self.k) + 1..r + 1].iter().filter(|&amp;&amp;c| c == a).count(); unsafe { self.occ.get_unchecked(i).get_unchecked(a as usize) + count } }
Do you know why the iterator adapters weren't vectorized? Does this mean that normally it would be better to not use the adapters?
How does this compare to gfx-rs? Given the name I was expecting (and looking forward to!) some physically-based lighting model, and was somewhat sad that it was just another graphics API.
I think you can just create a [Slice](https://doc.rust-lang.org/std/raw/struct.Slice.html) object with the appropriate len and let it point to your data. Then you simply transmute that struct into a pointer.
You can use tuples like this: if let (Some(val), Some(val2)) = (foo(), bar()) { doStuff1(); doStuff2(); doStuff3(); } Playpen: https://is.gd/FuEiew edit: This doesn't actually solve op's problem, see comment reply 
Why I must provide a type for the parameter in the closure here? This works: extern crate iron; use iron::prelude::*; fn main() { Iron::new(|_: &amp;mut Request| Ok(Response::with((iron::status::Ok, "Hello World")))) .http("localhost:3000") .unwrap(); } This does not: extern crate iron; use iron::prelude::*; fn main() { Iron::new(|_| Ok(Response::with((iron::status::Ok, "Hello World")))) .http("localhost:3000") .unwrap(); }
For the `Drop` trait: I would expect a generic `OnExit&lt;T: FnOnce() -&gt; ()&gt;` to work quite well, without even a macro. struct OnExit&lt;T&gt; where T: FnOnce() -&gt; () { destructor: Option&lt;T&gt;, } impl&lt;T&gt; OnExit&lt;T&gt; where T: FnOnce() -&gt; () { fn new(destructor: T) -&gt; OnExit&lt;T&gt; { OnExit { destructor: destructor } } } impl&lt;T&gt; Drop for OnExit&lt;T&gt; where T: FnOnce() -&gt; () { fn drop(&amp;mut self) { let destructor = self.destructor.take(); destructor(); } } EDITED: from --&gt; for
Great post! A common thing I need to do when profiling C++ code is to remove some optimizations that make the profile hard to interpret (eg -fno-remove-stack-pointer). Do you know if there are such flags in rust that need to be disabled ?
I think there might be a little subtlety here. Does it matter if bar() gets executed? In his example bar() won't be executed if foo() is some. I'm not sure whether that's the case in your first example.
That doesn't work though, this will only match if both `foo()` and `bar` succeed, the OP wants a match if *either* succeeds.
Looks interesting! How do you handle shader parameters?
I just fixed the panic handling in [overflower-support](https://crates.io/crates/overflower_support)'s tests and am in the process of using macros to extend the tests to cover the usual types. Also do the usual TWiR routine and I should prepare the next Rhein-Main Rust Meetup.
A small text editor inspired by antirez's `kilo`, [mutxt](https://github.com/moosingin3space/mutxt) and a terminal emulator inspired by `st`.
I've been working on a [rust port of socket.io](https://github.com/vibhavp/socket.io-rs). The underlying library, [engine.io has almost been completed](https://github.com/vibhavp/engine.io-rs) (my first crate, I'm kinda new to the language), albeit the only supported transport right now is JSONP, since I've yet to find a websocket solution that would work with Hyper/Iron. I'm trying to have an API that would be similar to the official one for node.js, but still be idiomatic rust.
Can you compare luminance, glutin, and gfx?
`rustc` complains at the second `let` that `foo` is still borrowed mutably until the end of the function, why is that? struct Foo {} impl Foo { foo(&amp;mut self) -&gt; Result&lt;(u64, u64), &amp;'static str&gt; { // calculations and stuff Ok((a, b)) } } fn main() { let mut foo = Foo {} let (a, b) = foo.foo().unwrap(); let (c, d) = foo.foo().unwrap(); }
I'm curious about why did you guys choose to reuse mio EventLoop for the scheduler even if it's not handling any IO.
I'm building the breeding logic for my evolutionary algorithm that evolves programs in my brainfuck-based esolang, Semantic Brain (which is available as a crate called sbrain)
&gt; I don't understand this error completely as I cannot change this trait. Yes, this is an issue with mixing the `num` crate with `std`. `product` is a function in `std` that requires that you implement `std::num::One`, but it seems that `BigUint` does not. You should file a bug with `num`.
First of all, thanks for writing this! I use it. Is there a reason for the difference in naming between `rmp::encode::write_array_len` and `rmp::decode::read_array_size`?
One thing I'm missing is the confidence interval. If a change reduced the average, but drastically increased variance, I may have second thoughts before applying it.
Ooh, nice! That'll be useful for me, thanks.
Just merged and released new version of RustyCode :)
I'll do so. Many thanks for confirm my suspect. Is there any way to inject dependencies, something like the "pimp my library" pattern does in scala?
The following compiles, but I am not fully sure if is solves your original problem. println!("Locking..."); let value: bool= *data.lock().unwrap(); println!("Unlocked"); if value { println!("Locking..."); let later_value = data.lock().unwrap(); println!("Locked."); } else { }
Because there's no produced expression to "inherit" the scope. Although, the condition living for the same duration as the whole `if` *is* weird. There is no way to actually capture any value through, it's just a `bool`.
You're right, I misunderstood the post... I thought that it was a case of "the compiler knows better" and idiomatic code was faster than manual optimization. But since it's actually a case of not-optimized-when-it-should, then it sure is a bad thing!
My examples were very simplified, but yes, I ended up going with something like this. As far as I can tell, it's equivalent, given that in both cases there is an opportunity for modification between locks. Edit: if (|| *data.lock().unwrap())() { //... } else { } does the trick too.
Thank you. Very instructive.
Why not just fold it into Racer?
I still see 0.14.7 as the version available in the VSCode package manager. Do I need to do anything or just wait?
Try to restart VSCode, I think it's bug of marketplace.
But it only works if `impl` reference `traits` in the same crate. In my case, it's not possible to add an `impl` to `BigUint` without forking all the crate.
I'm going to go back through your guide and modify my Travis config accordingly. If I can figure out some caching stuff, I'll let you know!
Just did this. Excellent suggestion. Thanks.
Yes.
Depends, is it a web browser game? Then, sticking to websocket/socket.io is your only choice. Otherwise if it's a native app, you might consider using UDP if the game is fast paced and loosing a few position updates here and there doesn't matter.
They're related in the sense that they're trying to avoid the overhead of function calls, and specifically avoid blowing the stack with temporaries. However they're generally different in practice. With TCO, you observe that the last piece of code you execute is a call to another function. As such, there's no need to hold onto any of your existing state. So you can (conceptually) pop off your own stack frame before pushing on theirs, and have them just return directly into your caller. If the tail call is a call to yourself, this can be further optimized into just mangling your body into a loop (mangle state, jump to start). One nice property of TCO is that it's totally transparent to your caller -- you look and act like a totally normal function. However it can support arbitrarily deeply nested function calls as long as they're all in tail position. You just keep pushing and popping in place. With RVO (return value optimization), you generally need to setup your calling convention to support it. You specify that the caller needs to pass you a pointer to enough uninitialized and unaliased space to store your return value. Then in your body you can use this pointer liberally as scratch space for constructing your return value. In theory, this can allow a function to use up no space on the stack at all, even with catastrophically huge outputs! Similar to TCO, if you delegate your result to another function, you can thread the return pointer into it. It's a bit more flexible because the sub-calls needn't be in tail position! For instance, the following code is clearly not in TCO form, but can potentially be transformed as follows: fn foo() -&gt; T { let x = sub_call(); println!("{}", x); return x; } optimizes/desugars to: fn foo(out: *mut T) { sub_call(out); println!("{}", &amp;*out); } Unfortunately this can't support arbitrarily nested functions with no space overhead, because you need to maintain enough state to remember where to jump to on each return. TCO is necessary for zero-allocation calls. Note that this calling convention is the foundation for NWBI&lt;- (NeW, Box, In, or &lt;- syntax). However taking advantage of these optimizations is hostile to things like stack traces, and certain features like destructors (and unwinding?) can interfere with ones ability to perform these optimizations in a semantic-preserving way. 
/r/playrust
As someone who actually worked on a genetic algorithm earlier in this year for an honours project, one thing that is wanted is repeatability of results. If I have measured a specific result for my algorithm, I want to be able to say "Under this seed, and on these machine specs my algorithm performed thus." What I mean is that I want to be able run my algorithm again and get the same results, if I have specified the same seed. A (admitedly quick) glance at your code didn't show a way to do that. Please feel free to correct me if I missed it. Anyway, congrats on releasing a crate! I'll probably stick to the Rand crate for my project though.
I published the [modesetting](https://crates.io/crates/modesetting) crate a few days ago. Currently it's usable for getting connector, encoder, and CRTC information from your system. Soon I'll be implementing framebuffer support and see if I can get glium to work with it.
The denomination for that would be Rust/Ruby Polyglot. And yes, it's a neat trick. I like it.
Thanks for your feedback. As far as I know, the rand crate doesn't offer you something like random choice. Yes, you are right, I don't offer the possibility to specify the seed. I thought, a random seed and several performance tests would be a more realistic test. The performance results are always roughly the same.
Huh, I wonder if this is the same Cristina Cifuentes that wrote some seminal decompiler texts, many years ago.
I refactored an [old lint](https://github.com/Wafflespeanut/rust-sorty/) of mine (which checks whether the declaration statements are sorted). I didn't publish it before because I thought rustfmt will come up with this check soon. Since it hasn't been implemented in rustfmt (yet), I went ahead and published it :)
This is horrendous and I love it!
 leaking temporaries relying on Drop is a bad idea this form compiles.. and doesn't abuse unwrap use std::sync::Mutex; fn main() { let data = Mutex::new(true); if let Ok(_) = data.lock() { // ... } else { } println!("Done"); } .
&gt; As far as I know, the rand crate doesn't offer you something like random choice. What about [`rand::sample`](https://doc.rust-lang.org/rand/rand/fn.sample.html)?
Thanks! I think RustyCode is fantastic :)
There is no reason except my inattention. Thanks! I'm going to make aliases for such symmetric functions.
I found a big list of learning projects on a C++ forum, so I'm [working through those in Rust](https://github.com/whostolemyhat/learning-projects) to get a better understanding of syntax. The current challenge I'm working on is [finding permutations of a word](https://github.com/whostolemyhat/learning-projects/blob/master/11-combo/combo/src/main.rs), which seems to be ok up to about 6 letters or so.
We probably can more aggressively use noalias. idk
rustc can certainly use noalias more aggressively, however each new introduction of noalias need be checked quite extensively because introducing erroneously could cause a lot of issue. Might be worth waiting for the reflexion on what is legal unsafe code.
I just might try to go to the Berlin one. Not that far away from Poland and perhaps my employer will help me minimize the cost ;)
How about you join us over in /r/ShittyProgramming? I think your talents would be put to very good use there ;)
I believe this should also work #[allow()] mod x {} /* Ruby code here # */ Rust code here
(The `code` links don't seem to be styled any differently to the non-link `code`, maybe it could be tweaked to be more distinguished?)
Well, that's not the same, because you can't pass the probabilities of the samples, right?
Rust (with unwinding) can't be more aggressive because noalias is busted in the face of unwinding. iirc noalias is a C semantic (restrict), and unwinding is a C++ semantic (throwing), and evidently no one really built the two to know about each other. noalias can only really be used for immutable things if `panic != abort`, as the bugs llvm has amount to "incorrectly eliminated a write before calling a function that can unwind".
Servo sure owns the Doge in the logo :)
Oooh good idea!
Naming this post was tough, but I decided to write out a pattern that I was explaining to someone the other day, and wanted to jot it down for posterity's sake. Feedback very welcome! I'm not used to writing in a medium that can't automatically check my code; I miss rustdoc!
Thank you for this. I was able to do a long evening coding session with Rust while enjoying it for the first time thx to RustyCode and VS Code.
&gt; Another title for the post could be "how to disable the struct literal syntax for the users of your code and yourself" =). Quite possibly, that's even _less_ catchy though, hehe. &gt; why would the author of the code would ever want to disable the literal syntax for himself? Consistency. If you want a type to be constructed via a function, then you almost always want it to _always_ be done so. Consider it as something like a more advanced lint. Ah nice catch with `uninitialized`. Tricky!
This is so cool. Nice job! 🍻
Since my [previous article about pattern matching](http://xion.io/post/code/rust-patterns-ref.html) turned out to be quite popular, here I am explaining basic Rust concepts again :) This post is basically a lengthy answer, with plenty of additional background, to a common question: "Do `for` loops use the `iter()` method?". I explain the `for` loop desugaring and talk about the iterator traits.
;)
How do I make a struct that has a member containing references to another boxed member? For example, how would I construct Metainfo below so raw_data has a lifetime bound to raw_str? (I'm pretty sure I can do it with Rc, but I'd prefer a clean solution using Box.) struct Metainfo { raw_str: Box&lt;str&gt;, raw_data: BEncoding&lt;_&gt;, }
I mean, this works, but does anyone else think this solution looks way too hacky? I'd like some language-level support for this kind of thing. Like a `pub readonly` modifier, maybe? Or something like `#[constructor(private)]`?
&gt; why would the author of the code would ever want to disable the literal syntax for himself? What's good for library clients is good for other modules also. Treating every module as a totally encapsulated little library is really good for code maintainability in my opinion. If I need to break that encapsulation, it means I need to consider whether or not I've structured my system correctly and redefine the structure of my modules or their interfaces. (also, not all code authors are 'himself's; 'themselves' is preferable :-) )
I think the attribute (whatever its called) would control the privacy of the literal syntax, not `new`.
Ah!
I thought this was going to be about the odd 'overloaded-namespace constructor' thing: pub struct P {x: i32, y: i32} pub fn P(x: i32, y:i32) -&gt; P { P{x: x, y: y} } fn main() { let p = P(1, 2); // Create a P. } 
 rust:~ lambda$ curl http://words.steveklabnik.com/structure-literals-vs-constructors-in-rust | pandoc -f html -t markdown &gt; /tmp/test.md % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 17737 0 17737 0 0 74842 0 --:--:-- --:--:-- --:--:-- 115k rust:~ lambda$ rustdoc --test /tmp/test.md running 9 tests test _0 ... FAILED test _a_href____words_steveklabnik_com_structure_literals_vs_constructors_in_rust__Structure_literals_vs_constructors_in_Rust__a____article_title__6 ... FAILED test _a_href____words_steveklabnik_com_structure_literals_vs_constructors_in_rust__Structure_literals_vs_constructors_in_Rust__a____article_title__1 ... FAILED test _a_href____words_steveklabnik_com_structure_literals_vs_constructors_in_rust__Structure_literals_vs_constructors_in_Rust__a____article_title__2 ... FAILED test _a_href____words_steveklabnik_com_structure_literals_vs_constructors_in_rust__Structure_literals_vs_constructors_in_Rust__a____article_title__4 ... ok test _a_href____words_steveklabnik_com_structure_literals_vs_constructors_in_rust__Structure_literals_vs_constructors_in_Rust__a____article_title__3 ... ok test _a_href____words_steveklabnik_com_structure_literals_vs_constructors_in_rust__Structure_literals_vs_constructors_in_Rust__a____article_title__5 ... ok test _a_href____words_steveklabnik_com_structure_literals_vs_constructors_in_rust__Structure_literals_vs_constructors_in_Rust__a____article_title__0 ... ok test _a_href____words_steveklabnik_com_structure_literals_vs_constructors_in_rust__Structure_literals_vs_constructors_in_Rust__a____article_title__7 ... ok failures: ---- _0 stdout ---- &lt;anon&gt;:3:1: 3:2 error: expected expression, found `}` &lt;anon&gt;:3 } ^ error: aborting due to previous error(s) thread '_0' panicked at 'Box&lt;Any&gt;', ../src/librustc/session/mod.rs:171 note: Run with `RUST_BACKTRACE=1` for a backtrace. ---- _a_href____words_steveklabnik_com_structure_literals_vs_constructors_in_rust__Structure_literals_vs_constructors_in_Rust__a____article_title__6 stdout ---- &lt;anon&gt;:15:16: 15:17 error: expected type, found `{` &lt;anon&gt;:15 Point: { x: 0, y: 0, _secret: () } // this is still allowed! ^ error: aborting due to previous error(s) thread '_a_href____words_steveklabnik_com_structure_literals_vs_constructors_in_rust__Structure_literals_vs_constructors_in_Rust__a____article_title__6' panicked at 'Box&lt;Any&gt;', ../src/librustc/session/mod.rs:171 ---- _a_href____words_steveklabnik_com_structure_literals_vs_constructors_in_rust__Structure_literals_vs_constructors_in_Rust__a____article_title__1 stdout ---- &lt;anon&gt;:2:18: 2:23 error: `Point` does not name a structure [E0422] &lt;anon&gt;:2 let origin = Point { x: 0, y: 0 }; ^~~~~ error: aborting due to previous error(s) thread '_a_href____words_steveklabnik_com_structure_literals_vs_constructors_in_rust__Structure_literals_vs_constructors_in_Rust__a____article_title__1' panicked at 'Box&lt;Any&gt;', ../src/librustc/session/mod.rs:171 ---- _a_href____words_steveklabnik_com_structure_literals_vs_constructors_in_rust__Structure_literals_vs_constructors_in_Rust__a____article_title__2 stdout ---- &lt;anon&gt;:13:18: 13:43 error: field `x` of struct `foo::Point` is private [E0451] &lt;anon&gt;:13 let origin = foo::Point { x: 0, y: 0 }; // this is not fine ^~~~~~~~~~~~~~~~~~~~~~~~~ &lt;anon&gt;:13:18: 13:43 error: field `y` of struct `foo::Point` is private [E0451] &lt;anon&gt;:13 let origin = foo::Point { x: 0, y: 0 }; // this is not fine ^~~~~~~~~~~~~~~~~~~~~~~~~ error: aborting due to 2 previous errors thread '_a_href____words_steveklabnik_com_structure_literals_vs_constructors_in_rust__Structure_literals_vs_constructors_in_Rust__a____article_title__2' panicked at 'Box&lt;Any&gt;', ../src/libsyntax/errors/mod.rs:622 thread '_a_href____words_steveklabnik_com_structure_literals_vs_constructors_in_rust__Structure_literals_vs_constructors_in_Rust__a____article_title__2' panicked at 'couldn't compile the test', ../src/librustdoc/test.rs:281 failures: _0 _a_href____words_steveklabnik_com_structure_literals_vs_constructors_in_rust__Structure_literals_vs_constructors_in_Rust__a____article_title__1 _a_href____words_steveklabnik_com_structure_literals_vs_constructors_in_rust__Structure_literals_vs_constructors_in_Rust__a____article_title__2 _a_href____words_steveklabnik_com_structure_literals_vs_constructors_in_rust__Structure_literals_vs_constructors_in_Rust__a____article_title__6 test result: FAILED. 5 passed; 4 failed; 0 ignored; 0 measured 
This kind of setup is highly un-idiomatic in Rust. You can't do it with safe Rust constructs, because the borrow checker would be afraid to ever move the struct, for fear of invalidating the pointer (it doesn't know that a Box's pointer never changes). There is a crate, `owning-ref`, which uses unsafe code to express this fact and may be useful for your situation if you can't go without the internal pointer.
You win the Internets.
make r/playrust great again
I think you got the wrong sub dude. Maybe /r/playrust.
I'm pretty sure it's not so much unidiomatic as something that doesn't that often. Maybe you mean unusual instead of unidiomatic? Because there are definitely valid use cases for this pattern in idiomatic Rust. The issue is really just that lifetimes aren't expressive enough to indicate that something that is `Box`ed won't actually move. It looks like `owning-ref` is the way to go for now, thanks.
&gt; The “two-semicolon” variant of the for loop doesn’t exist in Rust. Why doesn't it exist? It's a general loop which you can't emulate in Rust (because of the continue statement) and it's rather concise. Instead Rust endorses iterators and their metric ton of boilerplate code (how much lines of std:Vec's implementation have to do with implementing the various std::iter traits?). 
I've found that looping over an iterator covers about 95+% of the cases. In other situations you can use a `while` loop. You can get c-style for loops with a macro, but why bother?
One case where I wanted (or thought I wanted) something like this was to create an iterator over a `Vec` behind a lock. The following code doesn't compile, but my attempt to use the owning_ref crate was fruitless as well, I think because it couldn't connect the lifetime of `Iterator::Item` to the guard: use std::slice; use std::sync::{RwLock, RwLockReadGuard}; pub struct Data(RwLock&lt;Vec&lt;i32&gt;&gt;); impl Data { fn iter(&amp;self) -&gt; Iter { let guard = self.0.read().unwrap(); Iter { _guard: guard, iter: guard.iter(), } } } pub struct Iter&lt;'a&gt; { _guard: RwLockReadGuard&lt;'a, Vec&lt;i32&gt;&gt;, iter: slice::Iter&lt;'a, i32&gt;, } impl&lt;'a&gt; Iterator for Iter&lt;'a&gt; { type Item = &amp;'a i32; fn next(&amp;mut self) -&gt; Option&lt;&amp;'a i32&gt; { self.iter.next() } }
Wow. I've only been working with Rust since the beginning of the year, but this is probably the best explanation of the borrowing/move system that I've ever read. I feel so enlightened. 
I don't (it was pretty small time) but I could definitely post a synopsis or something if you're interested. There's an academic paper that corresponds to my talk, but I feel I have learned more since that submission and I can do better explaining in casual language, anyway, so I wouldn't mind writing one up. 
Decided to take a plunge, and make my first ever [pull request](https://github.com/StefanoD/Rust_Random_Choice/pull/1) to a library. Whether you want to merge it or not is up to you. Just felt like doing something.
&gt; It's a general loop which you can't emulate in Rust (because of the continue statement) I don't understand what this means. `loop` `match`, `break`, and `continue` can model any control flow graph without `goto`-like jumps. However, Rust's iterators do allow code which you *can't* emulate by indexing because the API contract requires that each item yielded by `next` be distinct from the others. This allows multiple mutable references into the collection, which Rust's aliasing rules can't allow for indexing. That is, the first of these works and the second doesn't: let mut iter = vec.iter_mut(); mem::swap(iter.next().unwrap(), iter.next().unwrap()); mem::swap(vec[0], vec[1]); Also, Rust's for loops are a universal iteration interface for all kinds of collections, including those which can't be performed with a C style for loop (e.g. pulling the code points from a UTF8 string, pulling the key value pairs from a HashMap). There are also optimization advantages I believe which I do not understand well enough to defend, maybe someone else can chime in. &gt; (how much lines of std:Vec's implementation have to do with implementing the various std::iter traits?). The by reference iterators are actually defined for slice, not vec. The slice and vec modules are about 3750 lines total, and about 500 of them are spent on defining the four iterators (iter, iter_mut, into_iter, and drain). I don't think this is excessive.
If you typed something up I'd definitely read it. Also that paper if you don't mind linking. What's your field? Regarding the evolution of learning Rust - that pretty much reflects my experience. I have several iterations of a simulation flying around, the first of which I pretend do not exist. ;-)
`.iter().enumerate()`
Was this intended as an explanation of the original comment or an expansion on mine? Some of your terminology is not at hand for me, but I believe this is an elaboration on what I said (I edited while you were posting to include a reference to continue/break). What I still don't understand is what CFG you can express with a C style for loop (*not* `goto`) but that you can't express in Rust (I believe there are none).
If you add named `continue`/`break` then you can model any reducible CFG (i.e. a CFG where all loops have a single entry point). Without `continue` there are CFGs like this that you can't express: L1: while ... { L2: while ... { if ... { break L1 } L3: while ... { if ... { break L1 } if ... { break L2 } if ... { continue L1 } while ... { if ... { break L1 } if ... { break L2 } if ... { break L3 } if ... { continue L1 } if ... { continue L2 } } if ... { break L1 } if ... { break L2 } if ... { break L3 } if ... { continue L1 } if ... { continue L2 } } if ... { break L1 } if ... { break L2 } if ... { continue L1 } } if ... { break L1 } }
 let mut array: [u16; 256] = unsafe{ std::mem::uninitialized() }; for i in 0..array.len() { array[i] = (i * i) as u16; } 
Basically she says that security needs to be built into our languages because programmers will make mistakes if they are allowed to ("security for the masses") and she identifies four main areas of vulnerabilities that could/should be tackled by the languages, namely (1) Memory safety, (2) Injections (SQL, etc), (3) Access Control, (4) Cryptography (mainly about bad APIs). She concludes that no language exists which fixes all four of these issues (can be seen in the video starting at minute 29:30).
Or you could use for i in (1...9).filter(|x| x != 5) { println!("{}", i); } Which should get optimised to the same result as your while loop. (Didn't actually test the code, typing on mobile)
Unless I'm wrong for i in 0..9 { if i == 5 { continue; } println!("{}", i); } works. The only problem you'd have is if instead of skipping you want to move back and forth across the array.
You pass them to closures in either `ShadingCommand`, `RenderCommand` or *isolated updates*. My idea is to disallow the user to carry the uniforms around as they’re just bridges to GPU’s variables (so they should live in programs, not in user space). Thus, you only access them via function arguments. Because `Program`s are strongly typed, you access the shader parameters via a type **you** give to luminance. There’re a lot of advantages to do so: - bulk updates: you can update several uniforms once and let the backend decide how to deal with the bulk update (the current `GL33` backend doesn’t even have to `glUseProgram`, because everything gets batched! exception for isolated updates, which the point is exactly that). - selective updates: you can update only a few shader parameters by omitting the one you don’t want (i.e. `|(_, _, t, resolution, _)| { … }`) - because the user cannot carry the shader parameters in their scope (they have to escape the uniform via unsafety to do so), luminance can have nicer assumptions and optimize the use of uniforms, especially the selective updates, without having `dirty` booleans or that kind of horror The single drawback is that I often have to annotate the type of the lambda, and I have no idea why :(. I asked for help on #rust and #rust-gamedev and in the end it sounds not possible yet. I migrated luminance from Haskell and in my Haskell version, I don’t need the type annotations (though ghc has a way more advanced type system than rustc). It can lead to utterly annoying type annotations. I still don’t really know where I need to put them. Sometimes it’s needed, sometimes it’s not.
Not a search engine. The title is misleading.
does this mean that container implementers need to write `into_iter` for `T`, `&amp;T` and `&amp;mut T`, as well as `iter` and `iter_mut`?
I want to leak a slice of 2-element arrays (`&amp;[[f64; 2]]`) to an FFI function as a void pointer, in a struct. Calling `send_coords` from an external program gives me nonsense values across FFI, though: #[repr(C)] pub struct Array { pub data: *const c_void, pub len: size_t, } // Build an Array from &amp;[[f64; 2]], so it can be leaked across the FFI boundary impl&lt;'a&gt; From&lt;&amp;'a [[f64; 2]]&gt; for Array { fn from(sl: &amp;'a [[f64; 2]]) -&gt; Self { let array = Array { data: sl.as_ptr() as *const c_void, len: sl.len() as size_t, }; mem::forget(sl); array } } // Build &amp;[[f64; 2]] from an Array, so it can be dropped impl&lt;'a&gt; From&lt;Array&gt; for &amp;'a [[f64; 2]] { fn from(arr: Array) -&gt; Self { unsafe { slice::from_raw_parts(arr.data as *const [f64; 2], arr.len) } } } #[no_mangle] pub extern "C" fn send_coords() -&gt; Array { Array::from(vec![[1.0, 2.0], [3.0, 4.0]].as_slice()) } // Calling this after I've copied the result of send_coords, in order to free its memory #[no_mangle] pub extern "C" fn drop_float_array(arr: Array) { if arr.data.is_null() { return; } let _: &amp;[[f64; 2]] = arr.into(); } What _should_ I be doing?
As others have said, you can either `enumerate()` and `zip()` if you need to loop over multiple arrays together, or you can use ranges.
Yes, as stated in the blog post, it's the service Honeypot uses as search engine. It's a set of macros and traits that with elasticsearch/rs-es and iron provide an easy way to create web services dedicated to the search (mapping, sorting, full-text, filtering...). I'll ask the title to be changed so that it will be more accurate. Edit: I just noticed that titles can't be changed. Sorry.
This is true. At the beginning it was called honeysearch, then it became searchspot (search spot / search_pot). I just don't like how "Searchpot" sounds, I think Searchspot is more natural.
Why not? It's a poor choice for a large project, but it's a perfect structure for explanatory, instructional, or proof-of-concept work.
It kills SRP. Small descriptively named functions provide much better documentation and explain themselves better. When you want to prove that a function works you can use asserts to check your invariants when entering and leaving the function.
Very well written sir: it's easy to read through, and yet seems to tick all the quirks. Hats off.
I agree that it's bad for larger scale projects. Literate programming is for communication first, functionality second. It exists because markdown/latex + a code file can't be put together in one gist, not because people prefer prose to assert() calls.
While I agree that you should use something like sodium instead if you can, there is a huge infrastructure established based on OpenPGP. For example, it is probably desirable that rustup uses the existing OpenPGP instead of a more modern Ed25519 signature.
This implementation uses libsodium, and also OpenSSL for RSA keys. It doesn't support Ed25519 too well, by the way, as I have no complete and working implementation of PGP+libsodium (my version of GnuPG still has very experimental support).
But I already have a MessageBox example, and mine is better because it accounts for unicode! https://github.com/retep998/winapi-rs#example
You cannot *create* a borrow out of thin air. You have to borrow *from* something, either re-borrowing another borrow, or borrowing from something that's owned. At some point, there has to be an owned value, *and you cannot create something and return a borrow to it.*
Look mah no continue! for i in (1..5).chain(5..10) { println!("{}", i) }
It's a bug in how the error messages are printed. It gets confused by [re-exports like this](https://crates.fyi/crates/flate2/0.2.14/src/lib.rs.html#54-61). The type is actually called `flate2::read::DeflateDecoder`.
The Vec gets dropped at the end of `send_coords`. Dropping an array reference actually does nothing at all. You should send the parts of the Vec itself (`v.as_ptr()`, `v.len()`, and `v.capacity()`) and reconstitute using `Vec::from_raw_parts`.
Thank you for taking the time to write this with the explanation of steps!
Thanks for creating the API! Life saver for Win programmers.
There's an open RFC to address this, I'm very excited about it. It's one of our weak spots right now for sure.
Very cool. We need some signing solution for rustup and this looks promising. I don't know exactly what I want to do yet: ideally it would be compatible with gpg, but I am also very strongly in favor of pure(er)-Rust solutions because they are easier to build (and to promote Rust crypto). For example, I really don't want to take an OpenSSL dependency on Mac/Win. Yesterday I was considering using ring / ed25519 + a custom serialization format for the signatures, but something compatible with GPG seems better. [rustup signing issue](https://github.com/rust-lang-nursery/rustup.rs/issues/241).
That would also sound more like Silk Road competitor
Blame [Donald Knuth](http://www-cs-faculty.stanford.edu/~uno/lp.html) for the terminology.
Are you talking about [this](https://github.com/rust-lang/rfcs/pull/1657)? Very exciting indeed!
Well, this crate was partly motivated by your post on the rust-users forum. Pijul was also calling gpg, which is called "gpg2" on some linux installations, and "gpg" on others. After reading RFC4880, and some of the thousand of lines of GnuPG, I am now convinced that the OpenPGP format should not be used except for backwards compatibility. RFC 4880 leaves way to much space to interpretation for a security/privacy format. Here are some issues: - PGP stores too many things unencrypted, even for "encrypted" packets. - GnuPG does not respect the specification, for instance by not including a modification code in packets that must include them. - Private/Secret keys are encrypted by a key (for instance an AES key), which is itself derived from a hash. The way to get from a passphrase to the AES key is defined in a really ambiguous way: a salted passphrase is "hashed iteratively until n bytes have been hashed", where n is an integer specified in the file. GnuPG (and other implementations) hash the prefix of length n of the infinite repetition of the salted passphrase. It doesn't mean PGP is wrong. It's really great, only the format is wrong. The web of trust is really cool. My crate can be used to parse keys, messages, and convert them to a format that has yet to be defined. How about a cbor-based encrypted+signed thing? I believe the CBOR RFC to be well-defined, although I've never implemented it. Encryption needs to be done in such a way that the file looks random, but this isn't really difficult. I'm definitely willing to help defining a better format. 
I think PGP is overkill for rustup, just use libsodiums crypto_sign_open directly with a hardcoded public key.
&gt; something compatible with GPG seems better. I think this only makes sense for using the existing PGP keys. If we decide to use custom rustup keys, a simple libsodium crypto_sign_open with a hardcoded public key should be enough.
The other comments have given specific examples, but the general principle is that you can iterate over a range to get the same behavior as any reasonable C-style for loop without manually incrementing the counter. An even more general principal though is that you rarely actually want this, because the thing you're processing is rarely integers in a range, but rather elements of some collection or stream or whatever.
BTW: You may want to add your google email address to your github account in order to be shown as contributer to the random choice lib. Currently you are not listed as contributor, because your google mail address you comitted with, is not associated with your github account.
You can tell them about [rust-bio](https://github.com/rust-bio/rust-bio) which is a library for doing some bioinformatics stuff. I don't know anything about it but it looks like it's under active development and someone recently wrote a blog post about their work in optimising it further.
&gt; Reserved crates &gt; These are the ones that are reserved for future use: not to criticize but isn't that a bit abusive of crates, why not just create them as you go? why create 392 placeholder creates?
Right, the `'static` lifetime is key. It means that it lives at least as long as any other lifetime (that is, for all lifetimes `'a`, `'static` is a subtype of `'a`). In reality, the memory for the string literal `"Cannot find "` is probably in some special place, *not* on the heap or the current stack frame.
Now it's valid bash, too! #/*[/**/include &lt;stdio.h&gt; #define print(_) int main() { if (sizeof('a') &gt; 1) { printf("Hello from C\n"); } else { printf("Hello from C++\n"); } } #define X */[allow()] macro_rules! p { ($($t:tt)*) =&gt; (fn main() { println!("Hello from Rust") }) } p! { """:" false || "exec" "echo" "Hello from sh" " """ print((0 and "Hello from Ruby\n" or "Hello from Python")) #define Y }/* #define A ]-[-------&gt;+&lt;]&gt;-.-[-&gt;+++++&lt;]&gt;++.+++++++..+++.[---&gt;+&lt;]&gt;-----.++[-&gt;+++&lt;]&gt;.++++++++++++.---.--.[-&gt;+++++&lt;]&gt;-.[-&gt;+++&lt;]&gt;++.[---&gt;+&lt;]&gt;----.+++[-&gt;+++&lt;]&gt;++.++++++++.+++++.--------.-[---&gt;+&lt;]&gt;--.+[-&gt;+++&lt;]&gt;+.++++++++.+[++&gt;---&lt;]&gt;-.*/ This brings the grand total to: Rust, Ruby, Python, C, C++, Brainfuck, and POSIX Shell. 
In our (mostly Max Krohn's) experience implementing PGP at Keybase, and also using the standard library implementation in golang, there's an enormous number of real keys out there that GnuPG will accept but which other implementations will not. Every couple of weeks we get a new GitHub report with some confusing error and a bad user experience, that sends us digging through the subkey parsing code to hack around some random duplicated packet or invalid flag. As a library, of course you can only implement a subset of what GnuPG supports. But for an *application*, using such a library can cause a ton of problems as soon as you have to work with keys you didn't create. It's a limited set of apps that don't need to work with keys In The Wild, but which do want to make signatures that are easy for people to check by hand. (Package manager sorts of things, like the rustup example below, are the best use case I can think of.) I'd want to be a little cautious about encouraging new projects to use a PGP reimplementation, if what they're ultimately going to need is a GnuPG wrapper.
&gt;you cant emulate in Rust this is [patently untrue](https://github.com/huonw/cfor) Learning a new language implies that you learn its idioms. Rust does not use nor like C-style for-loops, and so does not have them. Languages without them have gone far (see: Python).
I think we are talking about different things. When did literate programming stop being that shit Knuth was writing about?
el-greco, thanks for pointing out. GSL is a great library. 
So wait, just to be clear, the following function is always safe so long as the pointers passed in are valid, non null, and there are no references to these values (but there may be other raw pointers, including *const) (ie they can be aliased to the same place): fn foo(ptr1: *mut i32, ptr2: *mut i32) { *ptr1 = *ptr2 + 1; } 
Hey, thanks for the feedback! That's also my feeling, but after looking at how the RFC specifies things, at the size of GnuPG, I'd be also cautious about using the PGP format at all! I guess the main point of this library is to be able to parse the output of GnuPG to convert it to some better-defined format.
Yes, but why construct them a certain way when you can just change them after construction any way you want? If there are some invariants to be maintained at construction, it seems unlikely that you can just assign any value afterwards.
Agreed, and the bit referring to the 'definition of aliasing seen above' is very confusing, because reading along, the piece hasn't even finished defining aliasing.
I suppose you're right. Only something like `pub readonly` would solve that problem.
Yup!
Thanks, I've used it a fair bit before when writing C code (I'm new to Rust). Good luck!
Type-level integers are not only used for performing stack allocations but also for encoding strides in the types, passing information about alignment, encoding the bounds of views of sub matrices, and many many more things (eigen really takes this up to 11). In a perfect world no performance difference should be observable between putting those integers in the types or passing them as function arguments. For whatever reason, constant propagation and inlining seems to be broken in all major compiler backends.
&gt; Cool stuff! Wish I'd had you as my TA ;) &gt; Maybe we'll realize that we both made bad life choices, and it'll happen: I'll realize I want to do a PhD, and you'll decide to abandon the dark side and join us in CS :)
Saw a thumbnail of a Windows message box on /r/rust and was surprised to see that the post _wasn't_ from our friendly neighborhood bunny.
You write: &gt; with the definition of aliasing seen above But you never actually, explicitly define what aliasing is... You have: &gt; First of all, we must define a pointer, if we’re to understand what “aliasing” is. But then you never make the connection to aliasing. Maybe you could add a few lines on that?
Ok, I'm just not a fan of something like this. These are the reasons I'm dubious about something like this. I don't like the idea of pulling in a bunch of libraries that I may or may not use. One of the things that is attractive about rust is that it doesn't come with a lot of stuff. It has a very minimal runtime. grabbing a bunch of stuff that may or may not be useful seems just a bit heavy handed. I wouldn't really like it if upgrading the platform causes a break. I would also not like to depend on the platform to remain up to date. Further, what happens if a package falls out of favor? How does something get removed from the platform? What if I still want that thing to stay up to date? Now you have to know exactly what is in the platform and what was in the platform. Seems a bit like a maintenance headache. Other headaches come into play when you depend on crates that may depend on older versions of the platform. So now you are left to figure out "does this crate actually use these dependencies" and "Will it break this crate to go up a version?". Further, what if the crate depends on a newer version of the platform that your code is currently incompatible with. I do like the idea of a curated list of 3rd party software that is "awesome". I just don't necessarily like having it all bundled together as a dependency. I feel like that is something that should be maintained by the individual owners of their crates. I'm probably just being overly cautious, but I've just dealt first hand with the dependency hell that comes from dependencies being too wide/broad in the java community. I'm much more an advocate of smaller and fine tuned dependencies that do exactly what you need over frameworks that do everything you might need. Because when a framework/dependency is too broad, upgrading that dependency becomes somewhat of a nightmare. Just my 2 cents as a jaded java dev.
One problem I don't see addressed here is google search results. Let's say I have a problem, I google it and find an example, I try the example, and it doesn't work because it's for Rust Platform 2, but I'm using Rust Platform 6. (But of course I don't know that that's why it doesn't work.) So I waste a lot of time tracking down the error, finally realize I need to revert to no later than Rust Platform 3 because the API changed in 4, only to discover some other part of my later project depends on something added in Rust Platform 5. That said, I don't know how to fix this other than maintaining backward compatibility forever, and only deprecating stuff instead of removing it.
Have you looked at Vulkan (as opposed to OpenGL)? The drawing API in Vulkan seems like it would fit better for the application described in your blog post. With Vulkan, you can access the GPU API across threads. To draw, you don't use an immediate API like OpenGL (which represents many modern GPUs inaccurately). Instead, you get a queue and create a command buffer, which you fill with commands (begin a renderpass, bind all the stuff, draw stuff, end the renderpass). Command buffers can be created in parallel across threads, if you want. You submit your queue to the device (the GPU) to be executed. Vulkan enables drawing in parallel across threads (actually creating command buffers which contain drawcalls, in parallel), using multiple GPUs at the same time, and a more efficient shader language called SPIR-V (an intermediate representation which you compile your shaders to at compile time instead of at runtime). Vulkan can't entirely replace OpenGL because of legacy hardware, but it works at least on modern GPUs (at least 2012+), so that's probably most GPUs in use today (with the latest drivers). The benefit is of course better performance (from e.g. creating command buffers in parallel, and a more accurate representation of GPUs hardware-level, and less driver overhead), and an API that is more sensible to use and better fits into many applications (though it is more verbose).
&gt; It has a very minimal runtime. grabbing a bunch of stuff that may or may not be useful seems just a bit heavy handed. If you declare a dependency in `Cargo.toml` and don't actually use it, is it included in the final binary? &gt; I wouldn't really like it if upgrading the platform causes a break. Nobody likes it, but isn't the point of major versions being able to make backwards incompatible changes, whether it be a library or some sort of framework? It's very difficult to improve existing APIs without some backwards incompatible changes. &gt; Further, what happens if a package falls out of favor? How does something get removed from the platform? What if I still want that thing to stay up to date? Add it as a separate dependency? Cargo and crates.io already do a fantastic job for that. &gt; Other headaches come into play when you depend on crates that may depend on older versions of the platform. I'd actually hope very few public libraries would use the platform and they be explicit with their own dependencies. A platform like this seems more useful for applications rather than libraries.
All good concerns. &gt; It has a very minimal runtime. grabbing a bunch of stuff that may or may not be useful seems just a bit heavy handed. Crates in the platform that you don't use will have ~0 compile-time overhead and no runtime overhead. &gt; I wouldn't really like it if upgrading the platform causes a break. I would also not like to depend on the platform to remain up to date. ISTM that the breakage issue is the same as for any crate. The platform libs are just crates. Not sure what you mean about depending on the platform to remain up to date. &gt; Other headaches come into play when you depend on crates that may depend on older versions of the platform. So now you are left to figure out "does this crate actually use these dependencies" and "Will it break this crate to go up a version?". Further, what if the crate depends on a newer version of the platform that your code is currently incompatible with. This again seems to me just a problem with dependencies generally. 
Would the rust plataform also provide nightly dependencies? I always have a lot of trouble when setting up a new project with clippy
&gt; I do like the idea of a curated list of 3rd party software that is "awesome". Yeah, like [awesome rust](https://github.com/kud1ing/awesome-rust). I can live without a platform as long as a list of this type is maintained. 
On that last note, there could be a rule that crates.io packages aren't allowed to have metapackage dependencies. Similar to the rule that they aren't allowed `*`-version dependencies.
Can you elaborate on specifics? Note that this isn't literally the same as the Haskell Platform.
 &gt; being on the unsafe guidelines team Has such a team already formed? Because [RFC 1643](https://github.com/rust-lang/rfcs/pull/1643) is still open. (And I was thinking of perhaps applying - if there is an apply process? - once the RFC was merged, because these kind of questions interest me.) 
I wonder how similar this would be in practice to something like what Anaconda Python has done for scientific computing with python (or even just general python niceness)
I think fundamental problem with Java is it has no module system. They are trying to create one with Jigsaw project. Currently Java does not provide a clean declarative way of specifying dependencies etc. Maven/Gradle etc do help a bit but there is no way in existing Java projects with 100s of Jar files in classpath to find out what all is really required to build and run a project. Rust on the other hand has a module system from the beginning so I think it is going to have far less or no issues like Java style heavy weight frameworks. 
It's not a problem with std, at least not until Rust 2.0. I guess it's a problem with some random package with or without the platform.
An additional concern: how many crates are currently at 1.0? I mean, not even [libc](https://crates.io/crates/libc) is at 1.0 yet. It seems a bit premature to form a stable platform without having stable dependencies for the platform to build upon...? 
So that nobody else could take them before me leaving me in a situation where I couldn't get the crate name I wanted. I did discuss it quite a bit with Rust team members before actually reserving all of them and they were okay with it. That said, I intend to get rid of all those sys crates in winapi 0.3 and consolidate everything into one big crate. 
I totally agree here. I don't feel I need it today and I think it will just complicate things in the future. 
Has anyone actually been asking for this? I'm on this sub and the users/internals forums all the time and I've seen very little noise for this sort of thing. It's especially perplexing given how uninspiring the Haskell platform has been, I don't know anyone who's serious about that language that uses it. Seems like a big waste of time and effort, especially at this stage where we're still largely waiting for libs you'd even want in a platform. Really disappointing to see the core team distracted by this.
Do whatever python does. A searchable directory of doc and code. And tools to make external libs easy to install.
Yeah, that's clearly not the same thing. A standard library has a connotation of being core to the language, not a collection of 3rd-party libs whose promise of support is the hand-wavy standard of "curation". When there's a deal-breaking bug and the maintainer has checked out, who's on the hook to fix it? If it's the core rust team, why not just bite the bullet and make a standard lib? If it's not, then how can I take any promises about maintenance into the future seriously? EDIT: this, btw, is my perception of the biggest problem w/ the Haskell platform. Some of the main people involved at the beginning got bored/busy and weren't keeping it up-to-date or solving its big problems (upgrades), or split off and built a competitor they could charge money for.
Yes. This is the plan for making it possible to use clippy with stable.
I'm not him, but there are no warm, fuzzy feelings coming from any of the Haskell communities that this idea is being posted in that I'm seeing. This notion of a Rust Platform is not some light undertaking, and I personally feel this is a matter where caution is highly advised. Rust is not hurting for lack of this new Platform concept. It seems like several high profile Rust members are pushing heavily behind this concept here on Reddit, but I don't know what to make of it. Such forward pushing could be used to collect a lot of data on how the Rust community feels about the idea, in order to make a decision, but it might also indicate that several core members have already decided for themselves that this is the way Rust should go, and now they're trying to convince everyone else. I like the fluid, seamless way that Rust operates right now. It doesn't feel like the standard download needs heavy renovation at all. Such efforts would seem to be orthogonal to the progress and success of Rust -- neither pushing it forwards nor backwards, just sideways. But, I'm just an opinionated community member. I don't have any special source of knowledge that really predicts the future. Perhaps the Rust Platform is the best possible thing to happen to Rust, but I don't feel that way.
Can you explain how this gets there from here? I didn't get that from aturon's post at all.
&gt; but there are no warm, fuzzy feelings coming from any of the Haskell communities that this idea is being posted in that I'm seeing. This still does not answer my question: I'd like specifics. Yes, there are people in Haskell who do not like the platform, but the downsides they see are different than in Rust. These differences matter. &gt; Such forward pushing could be used to collect a lot of data on how the Rust community feels about the idea, in order to make a decision, but it might also indicate that several core members have already decided for themselves that this is the way Rust should go, and now they're trying to convince everyone else. There's a reason this is a post on Aaron's blog, and not even at an RFC stage yet. Yes, a bunch of people have been talking about and working through ideas here, but nothing happens unilaterally in Rust. Getting the temperature of an idea like this is exactly the intention here. It's why there's multiple calls for "please let us know what you think" in the post itself.
the extension of your idea might be to have cargo do something like meta-packages for new; something like `cargo new --meta=gamedev` which could draw from a (curated?) crates list file... 
&gt; If it's the core rust team, why not just bite the bullet and make a standard lib? If it's not, then how can I take any promises about maintenance into the future seriously? It's about balance. The language itself is a fairly heavy-weight process that moves slowly, since anything that goes into the standard library must be maintained indefinitely into the future. Things in the ecosystem have zero process. This proposal is attempting to combine these two approaches: take the best of the ecosystem, add a bit more process, and get some sense of stability without needing to go through the full RFC process for every little change.
Clippy becomes a binary you can fetch via rustup. This binary can be compiled on stable the same way libstd is. This isn't evident from the post, but it's part of the plan :)
Don't pull the libraries in if you don't want to. And even if you did, Rust is still minimal. Libraries only get linked if they're actually being used. &gt; I do like the idea of a curated list of 3rd party software that is "awesome". I just don't necessarily like having it all bundled together as a dependency. I feel like that is something that should be maintained by the individual owners of their crates But it isn't. You are free to include those packages individually, just that folks who don't want to spend time choosing libraries can just pull in the bundle. And the way I see it these packages continue to evolve the way they do now. This is really just a curated list, with the added ability for people to say "give me everything in this list" (again, this doesn't have an extra cost) 
The main problem with a thin stdlib is the lack of: * Discoverability: It is often hard to discover an appropriate library for tasks that are not directly supported by stdlib. * Distributability: It is often hard to download and setup an appropriate library. * Design for cooperation: It cannot be assumed that 3rd-party libraries are always cooperative to each other. * Durability: It cannot be assumed that 3rd-party libraries are maintained as much as required. While I pretty much agree to the idea of "curated 3rd-party libraries", the "platform" approach does not solve all of those problems. Especially the platform maintainer cannot directly affect 3rd-party libraries, so the latter two points are not directly addressed; the platform can only check for problems, not solve them. I feel simpler but equally effective approach is possible. We already have a good distributability by default, thanks to Cargo. So if we are going to embrace the discoverability, can we just ship some chosen 3rd-party documentations by default and do NOT ship the libraries themselves? The user will search for, say, HTTP library, and see that HTTP client is provided by a crate named `hyper`. The documentation would have large print directing the user to put some dependencies to `Cargo.toml` (we can do better by making [`cargo-edit`](https://github.com/killercup/cargo-edit/) a part of Cargo). We will still have to update the documentations from time to time (especially for major revisions), but it won't break any user experience. How about this "limited" approach?
Yes, but its a pity that it was abandoned.
Yeah, I'm thinking in a similar direction. It would be very useful to externalize knowledge about crates that are considered to be the gold standard, or, (especially), if some oft-used package is now considered deprecated (see `rustc-serialize` -&gt; `serde`). But in my mind it would be (almost?) enough to publish a website that, in a sort of directory approach, highlights these curated packages, and then leaves distribution and dependency tracking to tools we already have (i.e. cargo and crates.io). Searching for a particular crate on crates.io already works quite well. You also have the link to reverse dependencies, which can be a very powerful indicator, which could be made more prominent, maybe by including a rank, i.e. this is the package with the 5th most reverse dependencies on crates.io. Then, what's left is really more "folk knowledge" about changes that you can't get from that data, like the aforementioned serialize -&gt; serde move. However, in some cases, where the original authors agree that their crate is now worse than some other crate, it would probably be more powerful to also communicate that right their on crates.io. I guess the general theme here is, yes, federated is good, but more federated is better. Make better use of mechanisms/data that already exists, rather than trying to half-centralize stuff. And if some infrastructure is not good enough, we could also investigate, for example, running cargo test automatically on crates if one of their dependencies gets updated. This would provide much more benefits, and allow more natural evolution, than doing this for a smaller set of curated packages only.
Thing to consider is that being a systems programming language, Rust can be expected to have several different groups of users who will have different requirements for their development ecosystem. Desktop application developers are not embedded developers are not game developers are not backend developers. The comparison group of languages that have something like a Language Platform are often ones that are overwhelmingly used in the backend development role. Over at C++, people have trouble even with the small C++ standard library when they're working in resource-constrained environments, need extreme performance or are shipping middleware that must not conflict with whatever library ecosystem the client has chosen. At this point, we don't really know what the requirements of eg. embedded Rust developers are going to be like if there is to be a mature embedded Rust development ecosystem. Is it likely that the Rust Platform as currently envisioned would be a no-go for them and will this be a problem re. the best allocation of language development resources at this stage?
That must have been more than ten years ago? http://www.scala-lang.org/api/2.5.0/scala/Nil$object.html
Also I think saying something like "std is where libraries go to die" shows an unhealthy aversion to standardizing mature + stable APIs. Sure, std isn't the place for everything. A given standard library module might not be the best for every use case, and it might make tradeoffs that make it useless for some. The point is that the standard library at least gives users of the language a starting point when looking for a feature/utility. They can pull in an external third-party library if they want an optimized experience, but at the end of the day most people won't need that. They just want something that's reasonably easy to use and reasonably well documented. Languages with large standard libraries (or library distributions) have historically seen more uptake than languages with small standard libraries. Don't let perfect be the enemy of good.
I think maybe a good compromise is enabling some sort of metapackage declaration on Cargo.toml that would auto extern crate all inside of it. Maybe a [framework] section, that way is not so ad-hoc, and you can amend errors on a given framework after 10+ years of experience by just promoting a new one, and also there could be a benefit in competition between frameworks.
Pointing out the obvious: a version being below 1.0 **does not** imply it isn't stable, or that it's necessarily any more unstable than something at 1.0+. Even at 1.0+, the author(s) could find an issue that requires an immediate bump to 2.0.
It's a lot less heavy handed. If crates.io was better about rating / categorizing packages, there'd be no need to ship a "platform". Crates isn't particularly bad, but it's somewhat lacking. # of downloads isn't the most informative metric, and you can't even sort the results of a search.
python and ruby have huge stdlibs. the question is how you get at least some advantages of a huge stdlib without having to write and maintain a huge stdlib in core.
&gt; Why can't we leave well enough alone? This doesn't break any workflow though, just introduces an alternative way to do things. Which you are free to not use. Looking at the HN thread it seems like the problems with the Haskell one don't apply in this case.
This isn't specific to rust-platform, it's a general versioning issue. Due to the way semver works, `mio = 1.2` actually means `mio &gt;=1.2, &lt;2.0`, so this is fine. 
Holy shit dude. /r/playrust is the subreddit you want, and you've been told that at least three times as far as I've seen. 
Have you tried reading through your code without the supplementing text? I do not know if this is your usual way of programming or is this the result of using literate programming, but I find it hard to follow what is happening without the text.
I think think that rather than having a "Rust platform" what we need is to improve tooling and crates.io in the following ways: - Add a way to mark _your_ crate in crates.io as always "works together" with other packages: that is, crate authors willing to make their crate "work together" opt-into this and it is up to them to curate their own crates. - All crates in the "work together" list are automatically built together/linked/... into some "meta-crate" to test that using those crates compiles fine (using their latest stable version). This builds are triggered by version bumps, addition of new crates, rustc releases, crater, .... If bumping a version of a crate or adding a new crate breaks the build, either the version bump or the addition is reverted or the version of the whole "works together" list is not incremented. The authors are notified to fix this, and once the whole list compiles again its version can be bumped to include the new changes. This might lead to a list that is almost constantly broken, but as long as every now and then it is made to work progress happens. This way we get a list of packages that always work together, and that are curated by their own authors who opted into this mess for karma and glory. The rust team could then just focus on mentioning these crates in the website, and maybe curating their "keywords"/"descriptions" so that they are easy to find. This would be a less controversial significant improvement of the status quo, and much less work for the rust team than what is proposed. Once we are there we could reconsider the rest (meta-crate and its versioning, removing extern...), but I would guess that at that point those things are not worth it. EDIT: somebody mentioned in the haskell thread that one could call this "cargo cult", I think "cult" could be a great name for the tool that makes sure that all these packages compile fine together.
Winapi is supposed to be just raw bindings, no helper methods or extra stuff. That's what [wio](https://github.com/retep998/wio-rs/blob/master/src/wide.rs) is for which has helper methods for converting wide strings.
The stack approach seems to be doing ok, no? 
Am I right in thinking that this would be a *development* platform, rather than a JRE-style *runtime* platform? That is, at some point between development and distribution, all of the libraries in "the rust platform" which I'm not using will be stripped away? I'm very excited for the prospect of being able to use non-`libstd` libraries without worrying too much about abandonment/licensing issues/drama/etc., but if the price for that would be a 50MB runtime dependency, I absolutely wouldn't be able to take it.
This "Rust Platform" sounds like it has much the same goals as a Linux distribution (integration, stability, batteries included and ready for use). So if you really want more process for some libraries, you could adopt the things that distributions do. Like a central bug tracker and central mailing list/means of communication and stable versions. You could let a library opt-in to working with these central, official, Rust distribution tools, or let someone else serve as the stable maintainer for a package if they want. Vet the maintainers to make sure they produce good quality, integrated libraries. Then you could surface these stable, official, integrated "distro versions" differently in Cargo. No need for some "Rust Platform", just have a different level of stability and integration that a library developer can opt-in to of their own free will, and enforce that level by vetting maintainers.
This problem is not inherent to libc. It comes back every time some important library updates and another common dependent does not. It's no fun at all.
if someone don't know what is Aerospike, http://www.aerospike.com/benchmark/
In that case I remain very excited!
I understand the feeling, but at the same time using this would require talking about conversion from UTF-8 to UCS2 (as retep998's does) which might detract from the focus of the post. Or do you mean as an appendix at the end, post a link to the winapi's example?
It's quite unhelpful that `wio` is not documented and not linked to in `winapi` introductory documentation.
Unresolved issues since 2014, mystical "version 0.3" since autumn of 2015, 1 closed PR during a month. Is it not an abandoned? And yes, reserved crates instead, so noone have a chance to fork and continue. Guess, you need to agree about move winapi to the nursery: https://github.com/retep998/winapi-rs/issues/268, instead of sitting on eggs.
&gt; It would be interesting to see if you tried to solve this problem by methods described there. Sure, but why I haven't read that book, so why don't you solve it using those techniques and show me?
I definitely agree with the documentation point. IMO, this should go all the way to the stdlib documentation. Eg. https://doc.rust-lang.org/stable/std/?search=http should have a link to the official HTTP library, just as another line highlighted in another colour. 
Something like a link at the end, or a short paragraph, saying that there is a "better" way to do it, and can be seen at &lt;link&gt;.
Another project that follows the 'Platform' idea and does coordinated releases is Eclipse, and they had their share of stability problems, though they got better with the last releases. I for one am excited I won't need to tell people to grab a nightly to use clippy. Otherwise I'm a bit lukewarm on the proposal, because I feel we haven't surveyed enough of the library design space to standardize on current solutions just yet.
You didn't actually answer the question: when 3rd-party libs in the platform break, who's responsible for fixing them? You all do astoundingly good work, and this really isn't meant to be a knock, but how long has it taken to ship rustup? To get box/non-zeroing drop/-&gt; impl Trait/incremental comp/etc out the door? Again, its totally understandable/legit that these things take time, not knocking it, but the point is that the core team already has a lot of work on its plate, work for things that people have actually been asking for. When you talk about things like "integration tests across the whole platform" I just hear a giant blackhole of opportunity cost - that's boring, grinding work that *will* take core team time to get done to any reasonable standard.
I tend to share the general skepticism, but perhaps it is because I don't have a very clear idea of what a given rust platform would look like in terms of scope. Could someone prototype out a rough list of crates that would end up in the first version of the rust-platform? Even if it's not something definitive, it could at least help us see if we are talking about the same thing. Is it a dozen crates, and hundred of them, or a thousand we are talking about? The post quotes haskel platform's 35 crates but it's not clear to me whether that's what we are looking for here. I am also not sure I fully understand the motivation ("batteries included" is a means to solve certain problems, it is not a goal in itself, in my opinion). Are we trying to ease up the onboarding experience to new rust coders? Are we trying to make a set of crates very discoverable? Are we trying to encourage people to agree on a set of almost-standard crates to unify the ecosystem? Is there something else? Lots of questions in one message, sorry! Edit: Removed some of the questions which were actually answered in the post. I should have read it more carefully.
I think that if a package system were to come to light, the developers of the libraries bundled in the package (and the maintainers of the package) would cooperate together more easily. This is the "North Star" mentioned in aturon's post, really. The package is a good occasion to discuss the direction and settle for a common set of dependencies for all the libraries in the package, ensuring their cooperation. Also, the package effort means that integration tests can be maintained *ensuring* that this cooperation occurs (no need to translate type, extraction of common abstractions in new crates to allow inter-compatibility) etc... So I think that the idea of packages is really a good idea (the Rust Platform... not quite as sure).
I really, really, really hope this does not land. Instead the work should go into supporting better peer dependencies, separate public vs private dependencies and maybe improve the import/use system so that internal modules do not occupy the same namespace as crates. The latter in particular is one of the reasons I feel dirty pulling in more and more crates as they often come with equally generic names as my own modules.
What if `cargo new` generated a project template with the expanded curated list, with an option of `cargo new --bare|--empty`, and a `cargo new --list-included-packages`? This would let new users see the curated choices explicitly, using the current best known versions, with the option of removing anything they seem like not using. It seems like has some of the benefits without much of the effort of meta-packages and special treatment. Would only the platform have distributed binaries? Could this feature be added to crates without the platform itself? I'm not a big fan of having two ways of including the same package. Let's say `iron` is part of the platform, but I've decided to not use the platform on my project. From what I understood, there would be an `extern crate iron` on my project. Considering if a new person comes to my project, needing to explain why on this project there is an `extern crate iron` when others don't, and what needs to be explicitly extern or not would be a confusing point. I would definitely be very trouble if someone had to explain to why on this languages some packages have special treatment.
Not a big fan of that idea either. I'd prefer being able to upload meta packages and install them like `cargo install --meta web` but even after thinking a bit it's not that great I think.
&gt; This is clearly a graph traversal problem. As I see it, I see it as: // follow the current direction until we hit a dead end; // while we follow it, remember the forks that the path did, so that // when we hit a dead end we could go to a fork we've seen and take the // direction of one of it's paths which we haven't taken before // It seems to pass the tests on hackerrank (which doesn't necessarily mean it's a good solution though): ["follow-the-current-direction"](https://gist.github.com/anonymous/6f767331f5d6647337680d8cd1d8eedc) PS: I am kind of ignorant about graphs =). 
Damm right. I have worked with aerospike in my previous job including some pretty performance critical projects. The database isnt the easiest to work with but it's insanely fast and robust.
Im Working on a book of how to learn rust. Using an html interface similar to codecademys but different with tool tips explaining the function and action of what it might be doing.
I'm not sure if vote brigading will help here :).
&gt; I did discuss it quite a bit with Rust team members before actually reserving all of them and they were okay with it. oh ok then! nevermind! :) 
Maybe, but having to specify a *few* versions is still easier than specifying *hundreds*. Also, when bumping the package version, it's time to pause and consider whether your special-cases are still special-cases or not. For example, you might have wanted to grab a more-up-to-date version of a certain crate for a number of features/fixes and this version may now be in the main crate so it no longer need to be special cased.
Perhaps updating crates.io search to promote results that appear in awesome rust would be a start.
In that case, wouldn't it make sense for crossbeam to deprecate their implementation and point to yours?
Or maybe upgrading crates.io with categories and somekind of rating system (either automatic, community driven or both). Let the users of crates.io create the meta-packages which can also be categorized and rated. Maybe even integrate crates.io into the Rust homepage so new users easily find their way to the core packages they need. 
I think it's worth distinguishing between...I'm not sure what to call these..."internal dependencies" and "API dependencies". If our libraries use regex on the inside, it shouldn't really matter that I use a different version of regex than you do, since we're probably not passing compiled regexes in and out of our APIs. (Cargo supports compiling multiple versions of regex into the same binary, right?) But for the types that show up in our APIs, like String and Vec and the numeric traits, it's super important to get everyone using the same thing, ideally by including them in std. I think this is why the core team is working on standardizing a Future type. Maybe if we manage to get all the right API types into std, we can have all the dependency chaos we want in our regular not-providing-fundamental-types-for-everyone-else-to-pass-around libraries?
[removed]
You'll never get all the API types into std, it's a pointless endeavor. The set is simply open-ended. For example, within a company, you might have a `EmployeeId` type which would just make no sense in the `std` but is shared by multiple crates within the company. --- On the other hand, I do agree on the internal/external division of dependencies. It could indeed be useful in reducing the number of dependencies which need to "agree on a version", however this does involve some book-keeping and would require enhancements to the compiler (so that it errors out if you attempt to use an internal crate's type in your public API), for example as a new lint passed by cargo.
Secure languages that distribute binaries on MITM'd HTTPS connections? HMMMMM
I think you're in the wrong place friend 
I have a module named "log" and there is a crate named "log". Sure i an rename but it's making everything more cofusing. 
What commercial products are you hoping to use Rust for?
Desktop applications. Actually we use Java and we are planning to change for Rust.
Interesting! What GUI libraries do you intend on using?
No `platform`, no dropping need for `extern crate`, no bundling with rustc, etc.
&gt; It's a lot less heavy handed. If crates.io was better about rating / categorizing packages, there'd be no need to ship a "platform". So, the qualm here is more about _who_ is deciding what crates are in this set of "awesome crates", more than the actual concept itself? (And regardless of all of this, I agree that crates.io could use a lot of new stuff. I've been trying to figure out how best to get people to pitch in...)
&gt; No implied promise of maintenance or stability. Just so I understand you here, you're saying that these are bad things? &gt; No platform-wide integration testing deathmarch. We actually already test a number of ecosystem crates on every commit. More specifically, Cargo and Iron and all their transitive dependencies. No deathmarch here. &gt; Still requires an active declaration of individual dependencies in projects. Ah, cool.
&gt; ISTM that the breakage issue is the same as for any crate. The platform libs are just crates. Not sure what you mean about depending on the platform to remain up to date. You could move up your dependencies of platform included libraries, however, most will rely solely on the platform for those updates. There will be push back to include a dependency that is included in the platform. Every dependency you include which is already in the platform decreases the value of the platform for your application. &gt; This again seems to me just a problem with dependencies generally. Yes, but it is magnified when you have a package of packages. Dependencies move at different rates, and in general, I believe that libraries should limit their dependencies to minimize this problem.
Yes, there'd be a bit of work to do; I was trying to say that the groundwork is already there.
So, we're not at that level of discussion yet, but as I mentioned below, I believe that the platform would largely be a set of karat dependencies. So bugfixes should be pulled in when they get released. But, I'm not 100% sure that that's true, it's in-the-weeds enough that I haven't worked through my feelings on this specific thing yet.
I think that handpicking crates is inherently flawed. In particular, it would be very difficult for a new crate to ever beat out a handpicked one. If crates.io should present you with the information to search "serialize" and make a moderately well informed decision of which package to use. Right now that search show serde first, but I can't tell how they are sorted, there are 10 pages, and downloads is not the greatest metric. It's very difficult to 'score' packages and show them in the right order (eg. abating the momentum of being at the top), but showing more metrics and making it more traversable would help. 
&gt; I think that handpicking crates is inherently flawed. In particular, it would be very difficult for a new crate to ever beat out a handpicked one. Interesting, I feel the opposite. A system based on votes favors existing crates, which have had time to accumulate more votes. A library that's been out five years will have more github stars than mine which is out for one, and makes it extremely hard to de-throne. Having some form of curation allows you to make these kinds of calls; that's the entire purpose!
([Cross-posted](https://internals.rust-lang.org/t/proposal-the-rust-platform/3745/56?u=aturon)) Thanks, everybody, for the excellent feedback so far -- both here and on various other forums. I've been reading and digesting all the comments. To state the obvious, it's quite clear that the overall response to the proposal as written is negative. Which is fine! I think people are raising a lot of good points. On the other hand, there is definitely room for improvement in the areas under discussion (e.g. discoverability, maturity, interoperability), and many people have been proposing lighter-weight approaches for doing so. Given the large number of comments here and elsewhere, and the repeated themes, I'm not going to try to respond individually. Instead, I'm going to put together a short follow-up post, gathering the downsides people have pointed out, looking more closely at the goals, and summarizing some of the alternatives being proposed.
That's what I wanted, thanks! Btw, is there a way to define this in the main.rs file or Cargo.toml file for good? Something like APPTYPE = WIN32, or APPTYPE = CONSOLE etc.,
You need to rewrite your code like this: trait Getter { type Item; fn get(&amp;self, i: usize) -&gt; Option&lt;&amp;Self::Item&gt;; // Notice the change to "&amp;Self::Item" here } impl&lt;T&gt; Getter for GregVec&lt;T&gt; { type Item = T; fn get(&amp;self, i: usize) -&gt; Option&lt;&amp;Self::Item&gt; { self.v.get(0) } } Edit: Maybe I should explain a bit further. `fn get(&amp;self, i: usize) -&gt; Option&lt;&amp;Self::Item&gt;` is short for `fn get&lt;'a&gt;(&amp;'a self, i: usize) -&gt; Option&lt;&amp;'a Self::Item&gt;`, i e, you exchange a borrow of `self` for the borrow of `Self::Item`, but the lifetime is the same. And second, you can't express a trait that for one type is an exchange of the borrow of `self` for the borrow of `Self::Item`, and for another type something that is not such a borrow exchange. (Side note: The [Index](https://doc.rust-lang.org/std/ops/trait.Index.html) trait has the same limitation.)
What are your most beloved but not widely known Rust projects? I would love to add more to list :)
This looks to me like simply a transient failure of the CI system. Perhaps someone who has access rights for that system can restart the build an see if it fails again?
You could also implement the trait for `&amp;'a GregVec&lt;T&gt;`.
I think there is actually a very distinct, different issue here. The `rust-platform`, if it strives to be batteries-included, will include *many* packages with *many* dependencies, and so "so far this hasn't caused major issues" does not seem justified. Semver and upper bounds have caused major problems with Haskell, and the Haskell Platform exacerbated these issues *tremendously* for users, to the point where many recommended against it explicitly.
According to [RFC 447](https://github.com/rust-lang/rfcs/pull/447) which defined these rules, generic parameters must be used as part of the "impl trait" (`Getter`), the "self type" (`GregVec`) or "predicates" (being specified as an associated type in bounds or where clauses, which you don't have). Associated types in the impl itself don't count towards constraints; I'm not sure why and don't see a justification in the RFC beyond "that can be done other ways". In your example, `'a` describes the lifetime of the borrow of `self`, which could be anything depending on how it's called. It has no relation to the impl trait or the self type and so is "unconstrained" by them. Even if there were an implied relationship due to the borrow, you'd run into an issue since function signatures in trait impl's must match the signatures in the traits. Having the parameter in the impl but not in the trait would give an error. Here's a working example: https://is.gd/9aZTSo
It reminds me of this project and I also was not a fan of it then for the same reason. I'd prefer a curated list or bundled packages for certain Things like webdev for instance. I don't want packages added that I don't need to my project. https://users.rust-lang.org/t/stdx-the-missing-batteries-of-rust/2015
Rather than have a Haskell platform I think an approach like Haskell's Stack would be a better idea with it's LTS assuming we want to go that route. It provides libraries that play well together, tied to specific compiler versions that work and allows stability. It also doesn't force an import of all the libraries one would never use. Just a curated list that works together. That might be a better alternative
Well, you wouldn't need the platform to run on every commit, necessarily.
It has not technically been formed yet, no.
Since you created `GregVec`, you can always add something silly like: impl&lt;T&gt; GregVec&lt;T&gt; { fn get(&amp;self, i: usize) -&gt; Option&lt;&amp;T&gt; { Getter::get(&amp;self) } }
[palette](https://crates.io/crates/palette) is really nice for working with colors. Also, it feels really good to see a crate of mine featured on your list, especially since it's such a short list, so thanks for that. :D
is moving from java to rust really going to make that big a difference? seems like you're about to enjoy lots of pain for something that is easy peasy in java gui land and i'm curious what benefit could be had
It's possible, you'd have to ask /u/aturon. Personally I'm in favor of smaller, more targeted crates, so it seems perfectly fine to have a standalone thread pool crate.
Note: your two links are equal.
I love this enthusiasm!
I'm not sure if this is an easy question, but it stumped me and I eventually gave up. I'm writing a program in Rust to read in several rather complex text files, store some data from them in a Map (actually a BTreeMap), then write out the data in a different format. Since processing each file is complex, but independent, I wanted to process each file in its own thread. I decided to create the master BTreeMap in the main thread, and pass it to each thread and use a Mutex to control access to it. Simple, right? I've done the same kind of thing in C, C++, and Java. But Rust refused to allow this. I tried two basic approaches: 1. Create the BTreeMap in the main thread (using ::new), wrap it in Arc and Mutex, and pass a clone to each thread. Compile called this an error, pointing out that the lifetime of the BTreeMap was shorter than that of the threads I was creating. I used JoinHandles to make sure that the other threads all terminated before the main thread, but the compiler does not know about those. 2. So, figuring that the lifetime issue was a problem, I moved creation of the BTreeMap out of the main function and made it static, with an explicit lifetime of 'static. This caused the compiler to throw an error because BTreeMap has a destructor, and the destructor would never get called. (Presumably because the lifetime never "ends"?) Anyway, even though I used the Mutex correctly, and got no errors on that portion of my parallel code, Rust refused to let me create and access a simple BTreeMap from multiple threads. I was unable to find a way to convince the compiler that I was engaging in safe behavior, so I gave up on the idea of processing the files in parallel. My program works, but I feel depressed and disappointed that simple mutex-mediated access to a data structure seemed to be impermissible. Any ideas on where I've gone wrong? Is there a conventional way of doing this in Rust?
Rust can run without C code in bare metal situations. If it's running on top of an OS kernel that's written in C, it's still relying on C code anyway, so avoiding libc is pointless.
Thanks! I am very proud of what we have achieved!
Great stuff! Super impressed!
Hey /u/jackpot51, as a Rust enthusiast I've been following your project from the very beginning. What happened to your "This week in Redox" series? It was the only way I kept up with new advancements.
It'll be available at this link sometime afterward: https://air.mozilla.org/bay-area-rust-meetup-july-2016/
My first thought is that the Vec is too large for the stack: &gt; Each Mac OS X process is launched with a default stack size of 8 Megabytes. https://developer.apple.com/library/mac/qa/qa1419/_index.html
Hmm, the Repository has a couple open pull requests...
Great work!
Will the advantage of setting up a project into many crates fade with incremental compilation? eg. [servo](https://github.com/servo/servo/tree/master/components) And is there any insight into why servo doesn't make 100% of modules into crates / where to draw the line?
Other than introducing unsafety and possible exploits in the Rust runtime since the C code cannot be validated.
Is there an *active* Github Organisation for rust community documentation projects? Should there be? A place where tutorials like this can live out there life, get regularly updated and their PRs reviewed?
I thought you were referring to raw pointers (which can be very rare except for FFI and lower-level libraries) until you mention the size of data structure. We use *tons* of heap-allocating structures, which are most commonly a `String`, `Vec`, `HashMap` or less commonly `Rc` or `Box`---quite necessary for anything which size is not known before the execution. There is of course a [3rd-party library](https://crates.io/crates/smallvec) for optimizing smaller data structures, but it is extremely common to have such data structures.
I just ran into this example (edit: formatting): fn value_plus_square_ref(x: &amp;mut i32) { println!("Mutating value"); let a = *x; *x = a*a + a } fn main() { let mut x = 32; println!("x = {}", x); value_plus_square_ref(&amp;mut x); println!("x = {}", x); } I'm confused about the deference operator ( I think that's what it's called), the * in *x. Is it safe/recommended to use it in this way? Is there any documentation anywhere for how to use this (I'm having trouble find it, maybe I have the terminology wrong)? I tried to accomplish the same result without using it, but end up with a variety of errors (multiply not implemented for type &amp;mut i32 etc).
Yeah, it seems to think it's Github.com 😯 
If I'm understanding this correctly, this is kinda like the .NET Standard Platform? You expose a common API surface but only include the parts you actually need? The metapackage thing seems very similar too. If it's NOT like the NETSP, I'm curious as to what the main differences are since then I can get a deeper understanding of why they might be this way, and hopefully the various trade-offs that favor one approach over another, and how the design or use of Rust pushes them in one direction or another 
I am processing incoming UDP packets and would like to count how many packets have been received from each IP address. What data structure will be the most efficient for doing it? HashMap, BTreeMap or maybe something else?
Does Redox have support for Sandboxing? Can I just execute random binaries without fearing for my systems security?
Can I force the dylib to use jemalloc? Edit: Forcing the consuming program to use `alloc_system` works, forcing the dylib to use `alloc_jemalloc` still segfaults. Thank you!
Are there any examples of using Rust for the applications? All the examples I see are using C. I would really love to program an Cortex-M3 completely in Rust.
I would note that in general I would use fuzzing (such as with quicktest) in complement to a regular test suite in which the special cases are explicitly tested (for example with a table-driven approach). So I would use quickcheck to discover new interesting test cases and then add those to the test-suite. How do *you* use quicktest? Do you use it exclusively or just for exploration of the space?
I haven't messed with allocators in Rust, yet. But extrapolating from other languages I guess: * You get two separate instances of jemalloc if you link it statically, so they're still not compatible * You need to compile the allocator as separate dll and link it dynamically everywhere.
Thanks :)
No, not yet. With the support of `strace` coming at some point, we will be able to implement sandboxing in userspace
We were using github.io to host, which did not allow for https with a custom cname. I am fixing this now and moving www.redox-os.org to the server that runs https://doc.redox-os.org and https://static.redox-os.org, which both have correct https support.
Well, all the time Haiku has been developed (and still is!), Linux/BSD sources were available as well. It's certainly helping, but my understanding is that the main problem was bugs. Stuff that corrupts memory, filesystem, crashes the kernel - that drags down any efforts on new features, and there are still major issues, after ~15 years of work. In that way, Redox has a chance to go the fast curve. Go Redox go!
Couple of basic questions: 1. Can I run it on STM32L1? 2. Why is it only for Cortex-M ? 3. How is it different from Zinc? (sure there are lots of differences, but it would be nice to have the devs explain it in a few sentences)
This is fixed now
It is not too surprising to see it starting to pick-up. Once you have good interfaces down in the kernel, building on top of that becomes a lot easier. I would add that because they're not supporting any type of standard display interface (Unless things changes since I last looked at the source) supporting display is much easier - and that's where a lot of the flashy stuff comes from (Not to say the flashy stuff isn't hard). And with that, once you have a functioning C library like they do (They ported newlib), porting software is surprisingly easy. That's not to say this isn't a lot of work, but once you have a base kernel down it is much easier to start pulling in stuff from elsewhere and adding on features using already developed interfaces. That said, I think a key to keep a look at are features and speed. It is *really* easy to cut corners and create bottlenecks if you're just trying to get something working, so I wouldn't expect this kernel to be outperforming something like Linux anytime soon, even though it is much simpler as far as features go.
I've been using ndarray for some data processing/ transforming and I quite like it. Is there a way to get interop with rusty-machine and ndarray? Perhaps a Matrix::From(ndarray) ?
I thin 1 can be solved by scoped threads. https://crates.io/crates/thread-scoped. although there are sevral other implementations out in the ecosystem. E.G. crossbeam::Scope
You need to [explicitly cast](https://doc.rust-lang.org/book/casting-between-types.html) your f32 into an f64 before you can add it.
I really like this piece of advice [from the container docs](https://doc.rust-lang.org/std/collections/): &gt; To get this out of the way: you should probably just use `Vec` or `HashMap`. These two collections cover most use cases for generic data storage and processing. They are exceptionally good at doing what they do. All the other collections in the standard library have specific use cases where they are the optimal choice, but these cases are borderline niche in comparison. Even when `Vec` and `HashMap` are technically suboptimal, they're probably a good enough choice to get started.
Thanks
I'm making a `Circle` struct where I can pass in the radius and get back a `Circle` with radius, diameter and area; I'm using a function to calculate the area. When I have the `area` function inside the struct, I can't work out how to call it (not `self::area()` since it's a static method), but when I move it outside the struct the compiler complains that the function is never used. // pos1: outside struct the compiler complains it's unused fn area(radius: f64) -&gt; f64 { std::f64::consts::PI * (radius * radius) } struct Circle { radius: f64, diameter: f64, area: f64 } impl Circle { // pos2: inside struct. how can I call area() in ::new_with_area? fn area(radius: f64) -&gt; f64 { std::f64::consts::PI * (radius * radius) } fn new_with_radius(radius: f64) -&gt; Circle { Circle { radius: radius, diameter: 2.0 * radius, area: area(radius) // area() called here } } } // Circle::new_with_area(12); Am I missing something really obvious? How can I call a function inside a struct from another function in a struct?
Oh, so what do you suggest I should do? Close and re-open? Or ask brson to restart the build as pointed out by /u/pykube ? Edit: Nvm, brson approved the PR already.
Does slack have voice chat?
I also like ndarray (and took a lot of inspiration from it for parts of rulinalg). Right now there isn't a way to do so - but I think it might be worth having in the short term. It shouldn't be too much work to add a crate lying between the two which allows some simple conversions between the matrix types. It's not really a priority for me right now but I'd be happy to support any efforts to get the work done! Oh and I say _short term_ as I'm not sure if in the future it is worth having two libraries which have such a large overlap in functionality. Maybe rulinalg should use ndarray for it's underlying data types and implement the linear algebra on top. I'm still waiting to see what the best course of action is going forwards.
Isn't really hard. A little list about the motives behind: - evolution too slow. so many things that we are waiting from 9 and 10. Time is money. We can't wait. - so many ways to solve things (packages), but whats is the best? - classpath hell - no more support for NPAPI on browsers - a lot more So we will move from Java to Rust using FII/JNA app by app. Would be nice use also to support mobile development, but I still don't know if it's worth with Rust.
I’ve seen `png`and `bmp` support. Are you aware that we already have pure rust decoders for that in `image` (or dependent crates?)? So you don’t need to use libpng.
Yeah, /u/pegasos1 and I have worked on a Random Forest that has a similar interface to what you've described here (fit/predict) but it's built to work on ndarrays. So rather than having a totally separate ecosystem with the only differentiation being the matrix implementation it may make sense to try to consolidate.
What are the "performance enhancements" -- a skim reveals none.(more compact float serialization...?) ARE YOU THE FASTEST SERIALIZATION SYSTEM IN THE UNIVERSE FOR ALL WORKLOADS WITHOUT ANY CALIBRATION AT ALL YET?
This is a sore spot that I should write a blog post about to explain. Through a combination of our particular choice of a dynamic loading model for processes and the lack of support in LLVM for a particular PIC variant, it's impossible to write apps in Rust, currently. This is totally artifactual and could be solved in a few ways but we just haven't done it, particularly since we mostly have been using processes to host C code not ported yet, like the nrf Bluetooth serialization library. *edit: spelling*
I haven't looked at tessel specifically, but it has two cores. One of them (the router-on-a-chip) I believe is an application grade processor meant to run, e.g. Linux -- so not on that one. The second is a SAMD which is actually pretty similar to our one of our current target platform (the SAM4L), but doesn't have an MPU, so could only run capsules on it I think.
He thanks, this thing could be the first application written in Rust that I use. And the first one I can actually contribute things to. Cool. :)
I was going to ask if this was allowing anonymous trait implementations, but decided not to be lazy and just look up the RFC. If I did misclick anything it's this one: https://github.com/rust-lang/rfcs/blob/master/text/1522-conservative-impl-trait.md Not really what I thought it was but still sounds very useful
/r/playrust
/u/jackpot51, as one of Redox's design goals is a microkernel, would it be possible to build networking completely in userspace? Redox could gain a strong niche in packet processing applications with a good zero-copy userspace networking library and libpnet support.
Yes, it would not only be possible, but we are working on it already. 
They're actually called trait objects, not trait references, because they aren't really references at all. If you have a dynamically sized type, I think the reference to it actually doesn't store the length. Instead, the length is stored in the part of the item that is dynamically sized.
The calendar example is still a bit unergonomic, the closures hack is just unfortunate. Still way better than before.
I totally agree. While discord is free, I think the Rust community could make good use of its features. However, I don't think Discord even has a payment model yet. 
Hey now, I wasn't trying to insult you or anything. I just wanted to make sure people know they're called trait objects, not trait references, because they aren't called trait references, and they aren't even references, and calling them trait references could be misleading to a beginner. BTW `Box&lt;Trait&gt;` is basically the same thing as `&amp;Trait` except it's on the heap. I don't know the exact memory layout of DSTs (and I'm not sure that it's actually specified anywhere), which is why I said "I think", but still I think references to unsized types don't include the size of the thing they're referencing like slices do. (Which would make more sense if you think about it, because references are really just pointers.) But yeah, I don't really know.
&gt; And except for trait references, which also store a vtable. Well, they have a _pointer_ to a vtable.
The `cmdloop()` stuff offered by the Python `cmd` module is not something that I think could be easily replicated in Rust. It is *possible*, but it is harder than it would be with a dynamic language like Python, and I'm unaware of any attempts to implement it in Rust. Something like `rustyline` ([github](https://github.com/kkawakam/rustyline)) ([documentation](https://kkawakam.github.io/rustyline/rustyline/)) might make it pretty easy to build a small REPL. `nom` ([github](https://github.com/Geal/nom)) is the proverbial elephant in the room if you want to do any kind of parsing, so you could **definitely** build a REPL with that, but `nom` is not necessarily easy to use. It really depends on how exactly you want to approach the problem, but there is no direct correlation to `cmd` in Rust that I'm aware of.
Hmm, I thought that's what I was doing when it gave me the lifetime error, but I'll try again using your suggestion.
I will look into those, my goal isn't to build a "rust repl", just a CLI like what you would get on a Cisco device. (mycli) show files Then when receiving that command I would want to run a specific function.
Thanks, gonna look over it tomorrow as there were some things I was unfamiliar with and it's getting rather late. :D
&gt; I would also name the function something more descriptive, like count_neighbors. Having forgotten the rules of the game of life, it took me a while to figure out what the function was doing. Never was good at naming, sorry. &gt; Is there any reason why cells is a HashSet? It seems to me that something more matrix-like would be a better representation -- if nothing else, it would give you O(1) neighbor-checking instead of O(n). Funnily enough I chose a HashSet for performance reasons, as it's a lot quicker to search for a value in a HashSet than, lets say, a Vector (plus, no need to check for repeating cells) and you don't need to store and iterate through a lot of values like in a matrix, just having the living cells is enough. Also, the neighbor-checking algorithm could probably be rewritten to have a constant time, as searching for a value in a HashSet is an O(1) operation and all you need to do to is to check if the adjacent cells exist in the HashSet, but I'll work on that later. :p &gt; I've just now noticed that there is also flicker once there's a decent number of cells in the game. It also takes up a full core of my processor, even if there are no living cells. A sleep each cycle should fix this second issue; I think using a matrix (or 2d array, or Vec that you treat as a 2d array or whatever) of bools for the cells instead of a HashSet would fix a lot of your performance issues. Printing, then, would just be a matter of iterating over the matrix, and painting either "@" or "." depending on whether it's true or false. Hmm, strange, I thought I solved the flickering issue by only having the screen redrawn after an update, gonna need to look into that one. As for the performance, as I've said before, I think it would be more efficient to print only the living cells. Anyways, thanks for the feedback, much appreciated. :)
Is there any way to have a vector of boxes, with references held to the contents of the boxes, while being able to push new boxes onto the vector? This is theoretically safe because the references point to the allocations, which don't ever move, even when the boxes move. However, I don't think the borrow checker can handle it. On further thought, I think this is a very tough problem to solve, because push() is safe but pop() is unsafe in this case, but it's hard to think of a type system that could distinguish push from pop. You'd probably need dependent types or something like that.
Thank you for taking the negative reception so well. I know it really sucks to have ideas shot down like this. I do think focusing on improving visibility is a great way to go. Even if you decide to revisit the platform, having a strong community ranking can only be a boon. In other words, I think this is a great direction to go.
If you're running on a proprietary platform, then libc is the least of your worries. If you're running on an OSS platform, then libc is still the least of your worries. If you're able to audit Linux, you're able to audit musl.
No, not exact bounds. Lower bounds (with an upper bound on 2.0) That is exactly what I've been talking about this whole thread. It solves the slightly rare problem of 2.0 updates needing to be coordinated somehow. This is not a major issue right now, but it is annoying. Unlike Haskell, where each and every update needs coordination. I see how this can be a problem. I don't see how rust-platform will have anything to do with it being a problem. They are orthogonal. rust-platform is not too different from the current usage of packages. Currently people add semver lower bounds for a bunch of packages and rarely bump them (even though newer packages will be using later versions). It is still possible for packages to work together because cargo is smart and follows semver. This proposal in essense is doing the same; people may end up having slightly out of date versions in their manifest, but that is no different from what already happens and is solved by cargo. I have yet to see an argument explaining what rust-platform introduces to the existing system that will destabilize it. And the evolution plan for rust-platform is not the one you propose. Packages continue to evolve independently. Every rust-platform release, the versions included get bumped to the latest. You can early-bump with an override in your package anyway. If your deptree contains a later version of the package it will autobump anyway during resolution. I feel like the people who designed rust-platform are aware of these problems (going by the comments), and also feel its not going to affect Rust because of some crucial differences.
People really like writing editors, don't they? :) I'm working on syntax highlighting work in [xi](https://github.com/google/xi-editor), using [syntect](https://github.com/trishume/syntect). It's at the point where it updates live, though still with pokey performance. 
Steve! I've been watching all your presentations as I take the Rust journey :) Thanks for all the fantastic work on the Rust docs etc! The book says "You'll also notice we added an asterisk (*) in front of y, making it *y, this is because y is a &amp;mut reference." I have a handle on ownership and borrowing, but I'm *really* struggling to see the big picture of what dereferencing actually does and why it's needed - maybe I'm overcomplicating it. Why is it needed specifically for &amp;mut references (is it also used elsewhere)? When I do finally grasp it I'll write it down to make it easier for anyone else who's stuck in the same place. Edit: what I understand is that we are taking a mutable borrow of x, so any mutation has to be done on y not x. What I don't understand is why we need to mutate *y instead of just mutating y, and I don't understand what the semantics of *y really means or what it's doing.
Did anyone actually answer your question before they got side tracked talking about what the differences between pointers and references were? I'm not 100% clear what your question actually was... but: &gt; I was wondering if anyone could describe a real world example where they felt that pointers were necessary. &gt; At what point would you normally consider a data structure "big", and are both the pointers/pointees normally on the heap There are two issues here: First, when you don't have a pointer, your objects are allocated on the stack, not the heap. That's a finite scope memory block for your function. For example, this code will segfault: fn main() { let foo:[i32;5000000] = [0; 5000000]; for i in 0..500 { println!("{:?}", foo[i]); } } That's because allocating a buffer size of 5000000 on the stack will fail out. A buffer of 500 would be fine; you could expect the stack to be on the order of (I believe?) a few megabytes? Beyond that, you have to move your memory onto the heap, using pointers. It's unavoidable: #![feature(box_syntax)] fn main() { let foo = box [0i32; 5000000]; for i in 0..500 { println!("{:?}", foo[i]); } } Now there's a second part to your question, which is (I think?) what you're actually asking; when would you need a pointer (or reference) *inside* a struct, like this: struct Foo { bar: &amp;Bar } instead of: struct Foo { bar: Bar } ...and is it when the structure gets 'too big', and how big is 'too big'? No, it's not. There's no specific reason why you would choose to use a pointer instead of an internal structure representation because it's too big *unless* it was so big that an instance of it wouldn't fit on the stack. ...in which case, it would be if the structure contained data on the order of a few megabytes in it, eg. in pixel buffers. I would say there are only two cases where pointers *need* to sit inside structures (vs. just being convenient): 1) Where you need a relationship to common data. Most of the time pointers are used because you need to represent relationships between objects. For example: A &lt; reference to &gt;---- B C &lt; reference to &gt;---- B As a concrete example, if A and C are database ORM objects, they both want a reference to the database handle so that A.save() and C.save() can both save to the database, without having *two* open database handles. That doesn't scale. (and yes, single ownership, mutexs, locking, etc. It's true, but fundamentally this pattern does exist, and that's the solution). 2) Where you need to represent an object that cannot be represented without pointers. For example, a linked list. A linked list can be dynamically expanded by adding a reference to a new item, or shorted by removing a reference to a new item... but this won't work: struct Node { data: i32, next: Option&lt;Node&gt; } The error you'll get is quite obvious in its meaning: error: recursive type `Node` has infinite size Quite so; Option&lt;Node&gt; is not 'no memory used' or 'node'; it's a block of size at least 'sizeof(Node)' in your struct. Which is recursive, because that block size is sizeof(int) + sizeof(Node)... etc. etc. You get the idea I'm sure. Finally, there's one last part to your question: &gt; and are both the pointers/pointees normally on the heap? There's no specific requirement either way. This works fine: #![feature(box_syntax)] fn main() { let foo = 10u32; let bar = &amp;foo; let foo2 = box 11u32; let bar2 = foo2.as_ref(); println!("{:?}, {:?}", bar, bar2); } foo, bar, and bar2 are on the stack while foo2 is on the heap. You'll also get pointers on the heap if they sit inside structs. I would say that pointees typically sit on the heap, and pointers typically either sit on the stack, or on the heap in a struct. so..... tldr; Linked list, graph, tree &lt;--- examples of data structures requiring references. was it necessary because of the size of the data structure &lt;--- no or the frequency of working with it &lt;-- no. I don't know what this means. At what point would you normally consider a data structure "big" &lt;-- when sizeof(T) -&gt; stack size and are both the pointers/pointees normally on the heap &lt;-- not specifically, but normally pointees sit on the heap. Does that actually answer your question? (edit, oh, and the title, which is a different question entirely; 'How common is it to use pointers in rust, comparing to C and C++?' &lt;-- probably a bit less common, because of rust's move semantics, but you could argue that 'modern' C++ move semantics provide the same capacity, it's just more seldom used by C++ programmers generally)
Now all you have to do is figure out how to pass callbacks with state to functions that don't have user data. Dynamic thunk generation perhaps?
/u/exobrain: I checked the Storm site and while it mentions the hardware, it does not mention where it can be obtained from. Is it obtainable?
Cheers back! I felt the same way. All these great protocols exist, µTP, UDT, etc., but there was no secure file transfer program.
Servo already supports the CEF api. It hasn't been worked on for a while now though. Keeping servo embeddable is a goal though. 
If the callbacks don't cross threads (which they usually don't but you have to make sure), I usually use a hashmap in TLS, keyed by the value of the pointer to the FFI object (if you can be sure it has a stable address). In some of the Windows stuff I was doing (which I may get back to at some point) I think I was using `HWND` as keys. Works all right, kind of a pain with `RefCell` and making sure an API call doesn't end up invoking another callback, causing a panic from a double mutable borrow. 
I am not so confident as you, as if your comments are any indication, the protestations of Haskell Platform users (and victims) are being ignored because "Rust will do it better". Okay.
Thank you for your thorough answer. Others provided good ones too but yours is probably the most helpful. I'm reading the beginning of *Hacking: the art of exploitation* and it went from sounding a little too basic to diving right into assembly. I'll have to find more and more material that's just like that to really get this stuff and understand all of what's going on. 
Wrong subreddit. You want /r/playrust.
Ostensibly, that's what `Rc` would be for. If you don't need mutable references, that's probably the best way to go. You can get mutable references with `RefCell` but that wrapper adds so much noise to my code that I avoid it where I can.
You should try compiling with -O in rustc or --release in cargo. (/r/playrust)
So Rust will never replace C given that it needs C alive for its runtime.
*sighs* The problem with having a language and a game with the same name.
This doesn't quite work, because that `f` is now a function pointer, and you don't have anywhere to store that pointer so that your `ffi_Callback` can know what it is.
It's more a problem with people doing absolutely *no* research *whatsoever* before posting. It's like someone walking into an "Alcoholic's Anonymous" and doing shots assuming, based on the name, it's a place to get soused in privacy.
The only way I'm aware of is to use a mobile client that doesn't display subreddit specific information *and* do a blind post, specifying the subreddit from the "new post" interface.
Sorry I'm new to Reddit and I looked at it to see if it was about the literal word rust but I saw a few posts that seemed like it was about the game 
Ahh great, thanks! The warning was definitely from the function being used - I added two in the example for clarity but there's only one copy in my code.
They're not being ignored -- but nobody has yet provided a reason why the problems in Haskell will appear in rust given the fundamental differences between how the two handle versioning. Its not "Rust will do better"; it is "Rust has done better", because the rust platform doesn't introduce new factors into the existing versioning system -- a system which mostly works.
See https://internals.rust-lang.org/t/follow-up-the-rust-platform/3782?u=aturon, too. This is not ignoring, it is the opposite. 
I am new to Rust (and certainly bow to the wisdom of steveklabnik1), but it strikes me that your example is more complicated than it needs to be. Let me break it down in more detail. First of all, you are certainly not breaking any of Rust's borrowing rules. Your code will comile and run just fine and produce the intended result. It is not very readable though and can, IMHO, be improved by avoiding passing that mutable reference into the function in the first place. Mutable borrow &amp;mut T tells the Rust compiler to temporarily take ownership of what's passed into the function and be able to change it and then pass the ownership back to the caller. In your case, instead, you can pass an immutable borrow &amp;T to the function and return an i32 back. This tells the compiler that your function temporarily borrows the ownership, but it can't change the original value. This can help you avoid the whole dereferencing business altogether making your code a lot more readable: fn value_plus_square_ref(x: &amp;i32) -&gt; i32 { x * x + x } fn main() { let mut x = 32; println!("x = {}", x); x = value_plus_square_ref(&amp;x); println!("x = {}", x); } 
You could post from the front page in a browser, fail to notice that rust isn't in your list of subscribed subreddits, and ignore the text &gt; **submitting to /r/rust** &gt; **The Rust programming language. For the Rust video game, see /r/playrust** What really gets me is all these posts make it clear that many people don't even glance at the sidebar before posting on a subreddit. THAT'S THE WHOLE POINT OF THE RULES THAT VIRTUALLY EVERY SUBREDDIT HAS, PEOPLE, TO TELL YOU WHETHER WHAT YOU'RE POSTING IS APPROPRIATE!
The dereference operator * simply says, "the value of what a reference is pointing to". It is not only used for mutable references. Consider the following code: fn main() { let x = 5; let y = &amp;x; // y is an immutable reference to x let z = *y; // take the value of what y is pointing to and bind it to z println!("{}", z); // prints "5" } Hope this helps
I'm trying to find extension methods in the conservative impl trait RFC without luck. Can you elaborate on which semantics did your implementation had about those? I remember your impl trait solution to the calendar example last year and I don't remember it having any of those hacks. &gt; Making it automagical for every closure? I would like it to be automagical and explicit =/ Something like, if I call `.clone()` on a closure it is then checked whether the closure can derive clone and then derives it. EDIT: I forgot the most important thing! To say THANKS! For all your work on this, and for inventing and making the Calendar-Driven Development philosophy succeed! :D
&gt; t's a lot quicker to search for a value in a HashSet than, lets say, a Vector (plus, no need to check for repeating cells) and you don't need to store and iterate through a lot of values like in a matrix It is almost true. It is true if we consider computational complexity, but I believe that CPU cache and auto-vectorization (if possible) would make whole program faster. So in general make boards using continuous memory instead of fragmented one.
Or maybe it's a problem of steep learning curve...
Is anybody working on an interpreter?
I'm a C++ dev, and C++'s build times are just as bad as Rust's -- probably worse, in fact. Like the other C++ devs you mention, I accept slow build times as a price for using a powerful, expressive, and performant language. Rust offers the same trade-off. I'd love it if Rust's build times could be improved, but not if it means any compromise on Rust's core strengths. Ditto for C++. 
Incremental compilation is one of [Rust in 2016 goals](https://blog.rust-lang.org/2015/08/14/Next-year.html). I have high hope we will get something to test before 2016 ends.
As much as visibility matters, I cannot help but think that ensuring compatibility *also* matters (which is why I really liked the idea of curated packages where the constituent libraries would use a common set of dependencies and align their APIs together).
DMD's secret sauce is "not using LLVM". See [an LDC issue](https://github.com/ldc-developers/ldc/issues/539). LDC is a D compiler using LLVM.
The secret sauce is named Walter Bright. The man has implemented many compilers, and he's always focused highly on performance in every single one. Compilation speed has always been a premium goal of D, I expect they have had to trade-off some things for it.
Not if it's running on UNIX or Windows, no.
Let me know if you've got any questions about libpnet, happy to help. Even if you don't end up using it I'd be very interested to know what challenges you face/what's missing etc.
[And some people minused me when I wanted to talk about a solution to this issue.](https://www.reddit.com/r/rust/comments/4tz8hy/rustcity_admin_abuse_review/#thing_t1_d5lhtbr) Someone even said "these kinds of posts are not that common". Is Reddit able to migrate all the posts to another subreddit, let's say, /r/rustlang? That would be great and it will fix this issue forever!
Given the large popularity of `libc` that seems like a big issue to me. Either libc would need to be redesigned or cargo needs to be patched to allow overrides. Besides, `flate2` is only needed if one wants to write png images and `libc` is gone now.
Ok, I will take a look. Thanks for helping! At some point I want to PR the changes we made to libc to rust-lang/libc, which would fix this issue. However, our C library is not complete and the libc crate changes are pretty ugly
I thought the all idea of Rust was to be a memory safe systems programming language. Apparently I misunderstood it. 
I guess main reason is that size of key will be only 4 bytes (I work only with IPv4 addresses for now), so I thought straight approach of a BTreeMap in using byte values as keys will be more effective than machinery of hashing and collision detection of HashMap.
I agree with all the criticism, though I don't know enough about the Haskell community to know if it's try what they say, though that came up a lot. I do know being part of the standard library is the death knell in the Python ecosystem. I also know that the way Python does things works out pretty well. There are conservative libraries in the stdlib that can generally get you started but then youre normally off to other libraries that build on them (nose builds on unittest, etc.). Additionally the community has done a good job of reaching consensus on it's own for interop between libs. The scipy/numpy/matplotlib trifecta is a great example. You normally work with all of them. Additional libraries tie into them like OpenCV and PIL. I think discover ability of these clusters of libraries is the only real problem, as the community will naturally gravitate towards a limited set of high-value libs. I already see it with Hyper and mio and Piston in the Rust ecosystem. Towards this end I think a tagging system for crates.io and searching at the project level (maybe include a rendered README on the main crates page that is searchable) would help. This should be fairly low-hanging fruit I would think. PyPI kind of does this already such that there is an easy way to vet the library.
Looks like TypedArena is exactly what I was looking for.
I have really good SSD yes, but I am on windows 10
No reason to over think it, until you have a more specific problem.
To be clear. Rust itself calls to `malloc`, `realloc` and `free` (basically): - in a `Vec`, a call to `realloc` may not actually allocate memory (if the memory block in use in sufficient) - also, the underlying memory allocator is likely to reuse the same memory blocks (after they were freed) for efficiency reasons (hot in CPU cache, notably) For valgrind specifically, though, unless you purposefully use a pool (directly or indirectly), you should see valgrind reporting the allocations. As you noted, it just appears there's a bug currently with jemalloc.
Right, they're a bit unmaintained right now. Might want to ask zmike in #servo on Mozilla IRC. The basic idea is that any application that can use the CEF api to embed chromium should be able to embed servo, I think. Again, this isn't something being actively worked on right now, and I'm not sure if it even works.
There's nothing magical about the way the D compiler is implemented. It's just that it doesn't do optimizations and doesn't have the same kind of type system.
Or cargo-check.
Is that really the crux of the matter? For development (where incremental builds matter), I would not expect optimizations to come into play. Also as far as I remember the compilation time of rustc was mostly spent in code generation (baring type-shenanigans such as `typenum`) and not in the type system, that is trans+LLVM. Did it change? Am I mis-remembering?
One little piece at a time. Because it is ironic to be dependent on C and sell safety at the same time.
Compilation times have not necessarily been a huge focus up until now (some sporadic performance killers were patched up, and some regular improvements were made, but more on an ad-hoc basis). As /u/sanxiyn noted above, 2016 should be the year of incremental re-compilation in rustc: today rustc compiles the whole crate each time it's invoked, even if it contains 50 files and you only changed one line in one file. Niko just opened a PR 9 days ago (or so) which is a first stab at incremental recompilation. If I got it right, it creates one object file per module within the crate (roughly corresponding to a file), and therefore only needs re-compiling the one module changed (and its dependencies, obviously). There has also been work toward a much more fine-grained tracking of dependencies (per item); I am not sure where it's at. Also, for development, you might be interested in the cargo check subcommand. A significant part of the slow-down is code generation: cargo check just checks that your code *could* compile, but doesn't actually produce any binary, so is quite faster.
Cargo is already much closer to Stack than Cabal.
Exactly! That's why I love it. I even prefer it since it isn't an abstraction over a worse tool like stack is to cabal.
&gt; I have high hope we will get something to test before 2016 ends. https://github.com/rust-lang/rust/pull/34956#issuecomment-236062233 :wink:
(it landed two days ago, it was opened nine days ago)
`cargo build` already doesn't turn on optimizations.
Let me edit that.
&gt;Has there been any progress towards IDE tools so far? Things I know of: - [The Rust language server RFC](https://github.com/rust-lang/rfcs/pull/1317) has been approved - [Discussion to stabilize rustc json output](https://internals.rust-lang.org/t/stabilizing-json-compiler-message-output/3691) - [Rustw](https://github.com/nrc/rustw) a prototype for a web front-end to the compiler. As far as I can tell, one of the big blockers for RLS is incremental compilation which itself was waiting on MIR. (anyone correct me if I am wrong) 
There's a plugin for cargo, `cargo check`, that does this as well.
Wow this is a really big roadmap! They also have in stage 2: `async/await` and `memory model`. They expect this to be long: &gt; it will take more than a 12 months to design and build, and we want to make sure to take time to do it right. I wonder who will get them first, Rust or Swift? Given that the Rust community has been talking about both for much longer than that, it is certainly hard to tell. Anyway, good luck Swift contributors! :)
Apparently, it would be `opt-in`. Maybe something similar to `unsafe` keyword in Rust?
That flag is not related to the memory model — they want to use that to allow the swift 4 compiler to compile swift 3 source, to give it some level of source compatibility.
&gt; I didn't realize that those are the goals for 2016. We're also working on tooling to make what our goals are more clear :)
I also think that this can make easier to bridge Rust and Swift.
So your swift3 code will still compile but you won't be able to use the new stdlib and any new libraries that depend on it? Sounds like massive breakage in disguise.
I think what you want is a blanket impl. Something like impl&lt;T,U: ?Sized + MakeNoise&gt; MakeNoise for T where T: std::ops::Deref&lt;Target=U&gt; { fn make_noise(&amp;self) { self.deref().make_noise(); } } This says `MakeNoise` is implemented for anything which can dereference to something that implements `MakeNoise`, and it delegates to that underlying impl. Should cover the usual types - `&amp;T`, `Box&lt;T&gt;`, `Rc&lt;T&gt;`, etc.
Alternative implementation with only one type parameter: impl&lt;T&gt; MakeNoise for T where T: Deref, T::Target: MakeNoise { fn make_noise(&amp;self) { (**self).make_noise(); } }
Looks much cleaner, thanks.