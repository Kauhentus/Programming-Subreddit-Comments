Interesting, this is one of those things I never knew I wanted, but which makes a lot of sense. Also for documentation purposes. The rand crate, for example, has various generators which all have almost no methods. The real code is actually in all the traits and you need like 3 traits in scope to use them (Rng, RngCore, SeedableRng). This is very confusing at first. It would be great of if you could call something like "from_seed" directly and the document would just show something like "fn from_seed(...) &lt;inherited from SeedableRng&gt;" as one of the methods.
Let's talk about this code: use std::collections::HashMap; fn main() { let mut db: HashMap&lt;String, i32&gt; = HashMap::new(); db.insert(String::from("User1"), 44); db.insert(String::from("User2"), 77); println!("User1 - {:?}", db.get("User1"); } This works . But why can I pass a `&amp;str` into `db.get()` ? Should it not be expecting a `String::from()` ? Or does `db.get()` simply auto-do it for me? 
I agree with that. One cool trick I saw is Proxy can be used to make a generic `show . read`. I was thinking maybe PhantomData has some use towards that. 
Well...looks like you're not a beginner anymore! :-)
&gt; I'm looking for feedback on the many things I must have done wrong You misspelled pong.
I believe you're looking for /r/playrust. This is the subreddit for the Rust programming language.
[github](https://github.com/nerdypepper/eva) &amp;#x200B; this is my second fully matured rust project. i wrote it to learn the basics of compiler design (lexing, parsing and such). It uses the [Shunting Yard algorithm](https://en.wikipedia.org/wiki/Shunting-yard_algorithm), with a few modifications to handle unary operators and functions. code contributions are most welcome. feel free to open issues! &amp;#x200B; hope y'all enjoy using it! ill be looking for criticism down below :\^)
This is probably more for my understanding: I'm confused about \`pong\_server\`. Is it a binary or a library? It has both a \`lib.rs\` and a \`main.rs\`.
No more than one day since the announcement of WASI and we already have a 3rd party implementation! You go guys, keep it up! üëç
Take a look at [the get signature](https://doc.rust-lang.org/std/collections/struct.HashMap.html#method.get). You can pass anything the key can be borrowed as, and `String: Borrow&lt;str&gt;`.
That's really awesome work, thanks for porting and sharing it
Ah OK. And because a borrowed String::from is a &amp;str, it works. Got it. Thanks!
Brilliant, I am a Rust/programming beginner and just finished a far worse/simpler version of basically the same thing both as a learning exercise and to use as a desk calculator. I'll probably use your tool and I'll definitely check out the code! 
Sure, but if my code opens up a file, and miri returns that opening the file failed, then code that I intended to be tested won't be tested either. I'd rather add a workaround to load an exemplary file from a byte-slice, than not using miri at all.
It's great on mobile. Wanted to ask how it's done :o
The `parking_lot` locks are not only faster than the standard ones, but also come with dead-lock detection built in (you'll get a panic telling you which threads and which locks are involved).
My understanding is that RustBelt proves soundness of the subset of rust with respect to some very specific memory model (section 3.2 of the paper describes the model). Even for safe code, you need formal memory model to formulate what ‚Äúsound‚Äù means
I've made edits to reflect this. Again, thanks for enlightening me!
I've always been partial to these layered APIs. They are both easier to test and easier to reuse.
&gt; so it is not relevant for safe rust. That's not QUITE true. For example, specifying the memory ordering of atomic operations can be done in safe Rust, and different memory ordering will lead to different observable behavior, therefore it is important to understand the impact of the memory ordering on the other memory operations; which is also part of the memory model.
What is the font you are using in the screenshot? Looks great!
Thanks! We had a bit of a head start, though - we got to preview WASI to give early feedback. It looks like we started swapping it in about 3 weeks ago: [https://github.com/fastly/lucet/pull/35](https://github.com/fastly/lucet/pull/35)
LINQ is one of those things that I envy
Sorry about the delay... My brain must be running a stop-the-world GC. (-: This is the work I'm familiar with on the subject, but it's been a long time since I read it: https://www.cs.cmu.edu/~guyb/papers/gc2001.pdf 
I'm 90% sure that it is not a screenshot, instead, it is just a fancy code block.
If you never say monad you can easily have monads. Just `Trait Iterable` or something. They won't even know it's a monad
Looks like it's because of a named tuple. window: xlib\_types::Window(window as \_), &amp;#x200B;
You can see that it does not implement Serialize, only Deserialize for some reason. But it does allow conversion from it to a HashMap, a HashMap can be serialized, so you can either convert it to a HashMap before. Or implement a NewType that wraps it and implements serialize. As StrMap does not implement that many methods and traits it should be an easy option.
You can see that it does not implement Serialize, only Deserialize for some reason. But it does allow iteration over it, so you can wrap it in a NewType and implement it yourself. Also you could ask the developer why there isn't an implementation and request one (if you provide a PR it will probably be accepted more easily).
Co-author here; sorry about that! Send over a PR and we can fix it!
What I think would be the best way is simply start with re implementing a stable Qt4 or Qt5 in Rust. During this process, you get rid of all the crap and legacy bound to c++ itself or Qt backward compatibility choices, and restructure classes the fresh way. The APIs should be the simplest possible, and ONLY for GUI design purposes. Then a big dev community will find their marks easily while using a fresh new code base we all like. &amp;#x200B; :)
Your calculator seems to ignore syntax errors, e.g. ```sh eva 'sqrt(4' # returns 2 eva 'sqrt 4' # returns 4 ```
I don‚Äôt know, perhaps with a carefully chosen set of symbolic links?
Indeed. You can see it in action at https://doc.servo.org/servo/.
Nice!!! Good job!
That's a load of bollocks.
It looks like he set it up to be both, so you could run it as a standalone or inside another program (here presumably so you could run the server inside your game)
Very nice, good work
&gt; One of the many things they do is abstract imperative control flow into a declarative form. Oh! Okay, now it makes even more sense... I think.
I think you might understand more clearly if I explain to you what _Unsized_ types are. - `[i32]` is an unsized type. Its size isn't just "not known _right now_" at compile time like a type variable `T`, it's size is _unknowable_. It's an arbitrary slice of memory containing sequential `i32`'s. - `dyn Iterator&lt;Item=i32&gt;` is an unsized type. It represents a chunk of memory that contains an unspecified value that implements `Iterator&lt;Item=i32&gt;`. You can then have a vtable on hand and do dynamic dispatch. - Any tuple or struct that contains one unsized element is also unsized. (They cannot contain two, because we need to know the memory offset of the beginning of each element.) _Everything else_ is sized. Especially pointers to unsized types. All generics (IIRC) have a hidden `Sized` bound unless you specifically state otherwise, simply because it is so rare that you want to actually deal with unsized types.
Nice, I knew about symbolic links and \`ln\` but I had never create them using \`cp\`. I updated the OP to use `cp -rl ...`, which worked for me.
So you can just add both and there won't be any issues in doing this way as opposed to keeping the library in a separate directory with its own config and everything?
Yes, although I do think you have to do some configuring in the cargo.toml if you're using main.rs instead of a file in the bin/ folder. 
In "the book" there was a second where they converted a binary to library. They suggest moving main.rs into bin/ folder if you want it to act as a binary and library.
Yeah, using the bin/ folder is definitely the best option for this situation. 
Status: Closed Reason: Duplicate Description: this is feature which finds mistakes and intelligently fixes them before calculating result
In general, or are you referring to one of LINQ to Objects (`IEnumerable&lt;T&gt;`) or LINQ to SQL (`IQueryable&lt;T&gt;`)?
Note that a later paper relaxed the formal proof to include weak memory accesses. But yes, I suppose there is a "basic" memory model for safe code. It's only the "edge" where it really matters though, imo. I guess my point is that though it may not be formalized, the eventual formal memory model of Rust we know will permit all currently safe code. And "the abilities of safe Rust code" works as an informal memory model for me at least.
SQL is awesome, don't be afraid to learn it :-).
&gt; dailytao.org Um, does that still work? I get a self-signed certificate error, then a DreamHost site not found error.
Yeah, just checked it. http://dailytao.org/
https://github.com/rust-lang/miri/issues/670
\&gt; Built on top of Cranelift Awesome! I'm glad to see Cranelift moving forward
If you have code like ```rust some_u64 as u32 ``` Then the value gets truncated to fit into u32. Using `as` will not catch an overflow, you should only use `as` if you specifically want the truncation.
Fortunately I am both a fan of books and work at a university, so that's something I've been intending to try. :) I hadn't considered the prospect of my local library system having something so technically specific as PiA2012, though I do live in a major city so it couldn't hurt to try!
I am currently getting a little familiar with amethyst and while I honestly can't say much about the state of VR with amethyst, I do want to point out that I love amethyst because they have an, in my opinion, amazing community which the most helpful discord I've seen so far. I would recommend learning a little bit about amethyst and getting familiar with game programming so that you are ready when VR finally does come out. Also, Amethyst just adopted a new rendering backend, so they will probably still wait some time before they pick up serious development regarding VR. I don't know what there plans are but that's certainly something to keep in mind, that it will certainly still take quite some time.
Yeah. I agree with most of them, but with some I'd do it slightly differently, with others drastically differently. * Use rustup * Use stable, avoid nightly as much as possible. * Use clippy * ~~Use Cargo fmt frequently~~ Make your editor of choice run rustfmt/Cargo fmt whenever you write a file. * Also better tips on setting up an environment (debugger, etc.) * ~~Use few dependencies~~ Avoid trivial dependencies, too much risk for very little reward, use quality dependencies * Use TDD * Structs should always implement Debug also PartialEq and Eq for all the ones that will be used directly (meant to be data), Show for all that are public and exposed. Debug is meant for debugging, and you, other developers will find themselves thankful when debugging the code. All data should be comparable as the same data or not, hence the Eq, at the very least for testing. Any type the user can see they may want to print in a pretty fashion, Show is for that. * ~~Better to be too open than too private~~. Layer, and let each layer expose generously (while hidding the layers underneath). * Which gives us a new one: when possible split into more crates. Lets you control privacy on different levels, and forces your code to be more reusable. * It's always easier to expose something that was private, than privatize what was public. You never know what things users can and cannot be allowed to do safely until the end. * Use `String` over `&amp;str` for all the strings you create. Use `&amp;str` for all the Strings someone else created. * &amp;str for Input, String for output. Same as above really. * ~~Use clone() at will !~~ When possible just move things around. If not possible to move away consider cloning, borrow only when no alternative is possible or as optimization at a later date. * Extending the above, ownership transfer is generally easier to handle. It also means it's best to give ownership with things you return when possible. When not possible finding ways for people to each own their own copy is pretty good. Borrowing or any form f sharing is a code-smell. * Counter: methods should borrow self by default. (see below) * ~~Never have~~ &amp;¬¥a in your structs ^(is a code smell) always ^(prefer to) own ~~your~~ data * Extension: explicit lifetimes are a code-smell and should make you wonder: is there maybe a different way of thinking of the problem that doesn't require you to care about complex borrowing (simple borrowing doesn't need lifetimes, or has trivial lifetime mapping). * Prefer templating to dynamic traits (boxed traits) * If rustc tells you ‚Äúunknown type size‚Äù, try templating. If it don‚Äôt work, use reference * I agree with the sentiment, would maybe change the wording to not make it sound like such a implement-without-thinking method. This kind of error deserves better understanding. * Methods: first &amp;self, then &amp;mut self, then self, other parameters follow standard rules. * ~~Never ever use Rc, Arc, Cell, RefCell, UnsafeCell. If you do, you are probably doing it wrong~~ Rc and Arc are hard to use, and generally a code-smell of issues when sharing stuff. Avoid sharing. Cell, RefCell and even more UnsafeCell are even harder to use fully and a stronger code-smell. Avoid as much as possible, reconsider the solution as there may be a different way of doing it (similar to complex lifetime magic), if really no alternative know that using these can be hard and you should read on them. * ~~Atomics (std::sync::atomic) are great. Use them at will~~ Atomics are great, but avoid them as much as possible. Not only do all the above matter (atomics are way to share across threads) but they also require thinking in threads and are not intuitive. Use other higher-level abstractions. A lot of work is needed to understand Atomics fully. * Learn the multiple methods of Result and Option and use them * Learn about multiple iterator methods use them. * Don't be afraid to mix these with control systems. * Prefer Vec&lt;T&gt; rather than raw arrays for returns, for inputs consider IntoIterator&lt;T&gt; * Don‚Äôt return an Iterator, a Vec is easier * Variant of the above really. Vec&lt;T&gt; are better outputs most of the time, and you can always change into an Iterator&lt;T&gt; or a FromIterator&lt;T&gt; * Corollary: there's cases where Iterators are needed, and Iterators are generally a better optimization *long-term*. Initially avoid it. * ~~The difference between .into_iter(), .iter(), and .iter_mut()~~ Know the difference between owning operations, mutating operations and read-only operations. * owned, &amp;mut and &amp; * FnONce, FnMut, Fn * into_iter, iter_mut, iter * etc. * Learn the From trait and the conversion methods * Also see above * Try and experiment with the Rust Playground
This code came from dacite-winit. I thought it would cause problems because if this is a 64bit pointer it's unlikely the value won't wrap a u32.... Though I'm not sure it this is an opaque pointer to an xlib structure or if it's a protocol level u32, I guess that's up to xlib. I just don't feel like looking at headers.
I don't know that lib at all, but if it's an actual pointer then it shouldn't be either of u32 or u64. Pointers are their own type.
&gt; Think of monad less as an "ordering" object, and more as a "programmable semicolon" [...] What are you ordering? Surely the semicolon is for defining the ordering of actions? Monads are about composition with context. I think this is best seen when considering monads in the light of [kleisli categories](https://en.wikipedia.org/wiki/Kleisli_category): ```haskell id : a -&gt; a compose : (b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c) ``` vs. ```haskell id : a -&gt; T a compose : (b -&gt; T c) -&gt; (a -&gt; T b) -&gt; (a -&gt; T c) ``` The nice thing about this formulation is that the monad laws look exactly like the laws a category must uphold (left and right identities and associativity).
I see what you're getting at, and for the quick example I made up it would probably suffice, but it kind of sidesteps the main point of the example and isn't really the same. So to me it's a weak counterpoint at best. - the function loses all type information for the caller, and precludes annotations since the return type cannot be named - the function does not preserve the structure of its input: an `Option` is no longer an `Option`, everything becomes lazy even if its source was not, and it's unclear to me what would even happen in the `Future` case - it's not necessarily possible for the caller to get back to the original structure in a consistent way; you can't `collect` into everything - the argument assumes you have control over the signature and implementation when the goal is to write a method that works for anything with the same structure even if it came from another crate - the argument implies that there would always be some unifying conversion trait from the container you have into something the implementation can work with and a representative trait the caller can work with afterward, while the monad approach directly talks about the container type itself While sometimes a function like that is certainly sufficient, I don't think it's fair to say it's what you really want, and it's certainly not enough to cover all the use cases.
I'm on a cellphone now, so I haven't tested the answer, but the correct return type for the log func should be the one with self::ok and self::err. Additionally, the error message is complaining about the type of Err being wrong.
&gt; they're often more than that. You are not wrong. Each monad has its own special sauce that makes it tick, and be useful. Without the non-proper morphisms they are not. There's also the `MonadState`, `MonadError`, `MonadReader`, `MonadLogic`, `MonadPlus` classes and a myriad more. Then we have `Alternative`, `Traversable`, and `Foldable`. When you compose and combine these you can build interesting and useful structures.
Does WASI have a garbage collector? If not, will different languages have to bundle their runtime/garbage collector inside of the WASM object files and make calls to the WASI malloc/free implementation at the proper times? If Rust is compiled to WASM, there's no need for a GC. If Java, Go or JavaScript is compiled to WASM, they will need a GC.
Playground permalink: [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=c7839503d516b45829efe372e4e765f7](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=c7839503d516b45829efe372e4e765f7) Essentially my question is why does this not work: trait T { const CONST: usize; type TYPE; } struct SG&lt;G: T&gt; { arr1: [G::TYPE; G::CONST], arr2: [G::TYPE; &lt;G as T&gt;::CONST], } 
I had a look a the code, and I liked it. &amp;#x200B; There is just this construct that doesn't feel very idiomatic, but I don't have anything better to suggest. for i in (0..games_guard.len()).rev() { if games_guard[i].is_active() { games_guard[i].tick(); } else { games_guard.remove(i as usize); } } Does anyone have an idea ?
Yes
This looks very cool! Nice work!
&gt; While sometimes a function like that is certainly sufficient, I don't think it's fair to say it's what you really want I think my `half_evens` is generally preferred because it's performance correct, it abstracts over a boundary that well-represents the hypothesized problem space, and it's a more powerful, more flexible API. Whilst, yes, I wouldn't be able to reuse it for something further afield, like `Future`, I also wouldn't want to. That ties together two systems which have no business depending on each other, would impede all manner of specialization, and is sufficiently far-fetched that it seems to have no practical impact on my day-to-day programming.
One of my favorite articles about monads is: https://philipnilsson.github.io/Badness10k/escaping-hell-with-monads/ which illustrates the generalization nicely. &gt; So if functors are just things that can be mapped over... well, just about anything can be mapped over, no? Yep; pretty much. A few good other articles: - http://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html - http://www.cse.chalmers.se/edu/year/2017/course/TDA342_Advanced_Functional_Programming/lecture3.html - http://www.cse.chalmers.se/edu/year/2017/course/TDA342_Advanced_Functional_Programming/lecture4.html A functor `T` is a container that can have some elements in them, be they `T`s of `A`s or `T`s of `B`s, or some other type of elements (the functor doesn't care). A functor then lets you `map` between `T`s of `A`s (`T&lt;A&gt;`) to `T&lt;B&gt;`s given some function `f : A -&gt; B`. Moreover, given `xs : F&lt;A&gt;`, `f : A -&gt; B`, `g : B -&gt; C`, we know that `map(xs, compose(g, f)) == map(g, map(xs, f))` and that `map(identity, xs) == xs`, `map`. For a more in-depth mathematical view, see [Functor on Wikipedia](https://en.wikipedia.org/wiki/Functor).
I don't think they are so substantially different. The only difference wrt. Haskell is that laziness demands purity and some way to order IO and monads are great for that. Otherwise monads are, in my view, useful in any language that has them. Region typing and linear/affine typing are anti-modular things and so they pose challenges for monads. Once those are solved I think there's nothing particular about Rust. I think what should be demonstrated is why monads *wouldn't* be useful in Rust but useful in Haskell, Idris, ...
Looking at the proposed signature of `bind`, I'm pretty sure you're right. But since that's an unnameable type, I think that some sort of `impl Trait` has to be used. My gut feeling is that it should be `-&gt; impl M::SelfTrait&lt;u64&gt; + Monad&lt;u64&gt;`. Are multiple bounds in `impl Trait` allowed?
&gt; Each monad has its own special sauce that makes it tick, and be useful. That's not what I meant. While `IEnumerable&lt;T&gt;` in C# is a monad, LINQ to SQL (`IQueryable&lt;T&gt;`) is a different beast. But people used to say "yeah, LINQ is just the List monad, nothing new here"; same for F#'s async workflows (which I never really grokked) vs. `Task`. I'd be perfectly fine with "hey, your data structure looks like a monad, let's see if we can use CT to make it do some extra cool stuff". But the usual discourse was rather (or so I remember it to be) "These are called monads, they solve your every problem and then some, so you must throw away everything you know about error handling, list comprehensions, LINQ, state, backtracking, IO, CPS and threads. Monads do all that, and you don't need anything else". That's not just dismissive, but often limiting or false. For me, Rust is the sweet spot: it doesn't have a baked in `Monad` trait, but its story about mutability is much more ergonomic, while recognizing and offering great support for some of the more common ones.
If your library doesn't have it, ask them about inter-library loan. 
University libraries are usually better at that kind of thing, admittedly! But the public library system has surprised me a few times with what they've been able to acquire. Some of the larger cities have really diverse public collections, and smaller library systems get to tap into those for inter-library lending. YMMV of course!
&gt; But people used to say "yeah, LINQ is just the List monad, nothing new here"; same for F#'s async workflows (which I never really grokked) vs. Task. Not all list monads are the same. Not all writers are the same. Not all async monads are the same. There's a lot of local trade-offs that can be made for different encodings. I can clearly package up a `data Norm a = Norm Any a` monad and give it a nice interface for writing normalizations that are only partially about monads and which make most value out of [ScrapYourBoilerplate](https://hackage.haskell.org/package/lens-4.15.1/docs/Control-Lens-Plated.html). &gt; But the usual discourse was rather (or so I remember it to be) "These are called monads, they solve your every problem and then some, so you must throw away everything you know about error handling, list comprehensions, LINQ, state, backtracking, IO, CPS and threads. Monads do all that, and you don't need anything else". That's not just dismissive, but often limiting or false. I'm sorry that this is your experience, and I agree that sentiment is limiting, but it isn't my experience that folks say "monads solve all your problems". I think: &gt; I'd be perfectly fine with "hey, your data structure looks like a monad, let's see if we can use CT to make it do some extra cool stuff". is more the case. When I use some monad in Haskell I treat it both abstractly as a monad and concretely with its own special abilities. Monads are not the end all be all of abstraction; there are many other generic abstractions we can make that compose well with monads. The specific s of each monad matters much as well. &gt; For me, Rust is the sweet spot: it doesn't have a baked in Monad trait, but its story about mutability is much more ergonomic, while recognizing and offering great support for some of the more common ones. (Aside: Haskell Monad class is merely defined in the standard library, and do-notation is more or less a macro...) That's great. I'm fine with async/await, try, etc. If we could have solved these specific things with monads that would have been need, but for technical reasons we cannot, so we must look for other solutions for now.
My summary: Sled is great if you don‚Äôt care about losing data
this does not work because it would require const generics, https://github.com/rust-lang/rfcs/blob/master/text/2000-const-generics.md , wich is not yet imlpemented into the compiler
It's a bit complicated, because these are foundations. A good example of something that a solid Functor foundation would allow us to do is [Recursion Schemes](https://blog.sumtypeofway.com/an-introduction-to-recursion-schemes/). The link takes you to an example of implementing them on Haskell. The advantage is that we can very easily describe very complex transformations, but they translate to very optimal code. Think of it like Iterators, but far more powerful and that work for any kind of data structure. But before we can do that we need to implement `Functor&lt;T&gt;` for any type, and then allow them to chain.
Very cool! If you could try set up a similar example using the Laminar networking example the authors would greatly appreciate any feedback. 
Read the readme, the first case is considered a feature ("autobalancing of parentheses"). The second however does seem like a bug
&gt; well-represents the hypothesized problem space I think yours well-represents the behavior of the examples but the problem space I was working in is about abstraction we cannot achieve today. I used monads that operate within a context without destroying it because it was related to the article. Yours works today, destroys context, and doesn't work for all of them. So I think it falls in a different solution space. The purpose was to arrive at something like `bind(container, function)` which works for all conforming container/wrapper types, and I feel you're still side-stepping that main point by suggesting you wouldn't want to do that anyway, even though you acknowledge the more general usefulness with &gt; This is not to say that there aren't appealing aspects of a monad abstraction in Rust, but I really don't think your example made them clear. And that's fair enough. I wasn't exactly trying to extol the virtues of monads altogether, more attempting to answer the parent comment's concern about code you can't write today that you could with the post, motivated by something contrived but concrete. Perhaps I should have been less specific.
I get the impression you don't have a history of targeting Linux. (Before containerization technologies like Docker, it was a major hassle to make a single build that depended on old enough versions of things like glibc to work across all platforms.) Or, for that matter, a history of building compiled modules for platforms like like Node.js and Python. We tend to shy away from compiled modules because it's just so much easier to distribute a `.js` or `.py` file that'll run on all of these combinations without hassle: * 32-bit Windows * 64-bit Windows * MacOS * The myriad of x86 Linux distros running on different versions of glibc * The myriad of amd64 Linux distros running on different versions of glibc * The growing set of Linux distros on *any* architecture running musl-libc for its benefits in producing lean containerized OSes. * The growing set of ARM Linux distros running on different versions of glibc and different versions of the ARM architecture (eg. Low-level Raspberry Pi devices are `armv6` which lacks various CPU extensions, newer ones and most ARM devices are `armv7`, and then you have `aarch64` (ARMv8) becoming increasingly popular.) * Android and iOS on `armv7` or `aarch64` ...and that's just the popular stuff. (I didn't even get into stuff like MIPS (popular in routers) and RISC-V (young but on the rise). WASM has the potential to provide the convenience of "Screw it. I'll just send them a `.py` file" with performance in the same ballpark as fully native compiled code.
There is no garbage collection support in the WASM virtual machine yet. Currently, the Go and AssemblyScript compilers bring their own garbage collector that tracks objects in linear memory. The GC proposal ([https://github.com/WebAssembly/gc/blob/master/proposals/gc/Overview.md](https://github.com/WebAssembly/gc/blob/master/proposals/gc/Overview.md)) brings all of the primitives required for garbage collection. This will help implement GC'd languages more efficiently, but more importantly, it will allow that GC to interact smoothly with the JavaScript engine GC in the browser. This is super important for the future of WebAssembly and JavaScript interop. GC is, however, still a proposal and has a ways to go before it is widely available. Some smaller parts of the proposal have been broken off to make them available sooner ([https://github.com/WebAssembly/reference-types/blob/master/proposals/reference-types/Overview.md](https://github.com/WebAssembly/reference-types/blob/master/proposals/reference-types/Overview.md)). Somewhat related, even with Rust and C you still need some cooperation with the host environment to do a good job with memory management. My colleague Frank wrote this blog post on the subject: [https://www.fastly.com/blog/webassembly-memory-management-guide-for-c-rust-programmers](https://www.fastly.com/blog/webassembly-memory-management-guide-for-c-rust-programmers)
&gt; I'm fine with async/await, try, etc. If we could have solved these specific things with monads that would have been need, but for technical reasons we cannot, so we must look for other solutions for now. There are more costs than the compiler support, like ergonomics and the learning curve. I've been exposed to Haskell, but my eyes would glaze over code like that in the post. That SYB library is like a foreign language to me. I can make out some of the basic structure (it looks like a generalization of Functor), but some of the functions are not something I'd want to see in the standard library of a language, to put it like that.
Right, I don't have a history of targeting anything. I was just asking about how it works since I don't know. Thanks
Your code does different things for the different scenarios, maybe your benchmarks aren't exactly correct. For example your libuv python code does a single write to the stream of all the pongs at once. Also it doesn't filter the incoming lines by the PING substring. On the other hand the tokio example filters the incoming lines and writes each to the stream one by one. These might or might not make a difference but equalizing the benchmarks would be a good start.
How long time did it take? And how do you think: if you write on another language, would it be faster?
 pub fn parse(input: &amp;[u8]) -&gt; &amp;[u8] { (|i: &amp;[u8]| i)(input) } Fails to compile, because rust can't figure out the lifetimes are ok, if I remove the type annotation (as below) is succeeds. Is this working as intended? pub fn parse(input: &amp;[u8]) -&gt; &amp;[u8] { (|i| i)(input) }
&gt; There are more costs than the compiler support, like ergonomics and the learning curve. The ergonomics of monads are great in Haskell. Do notation is wonderful. If this is to have compiler support in Rust we'll need to make sure we have similar ergonomic quality. As for learning curve, sure. Monads are a concept you'd need to learn, but its one concept instead of one per each effect you are hard-coding into the language (async, try, ...). &gt; That SYB library is like a foreign language to me. This is no different than any other library which introduces concepts you have no familiarity with. If you throw me into some advanced machine learning library, changes are I will understand very little. The SYB library is immensely useful for any sort of application that does many transformation on trees (especially compilers) however even if it takes a bit of time to understand. &gt; but some of the functions are not something I'd want to see in the standard library of a language, to put it like that. It's not part of Haskell's standard library and shouldn't be. The `base` package is much less complex than that. Aside: The value of SYB is that we can encode a function such as `transformEvery :: (Monad m, Data s, Typeable a, Data a) =&gt; (a -&gt; m a) -&gt; s -&gt; m s` which allows you to find all the occurrences of `a` within `s` and monadically transform them bottom-up. For example, you can imagine `f :: Expr -&gt; Norm Expr` which is a function from an expression to a normalized expression; then you can get `transformEvery f :: CompilationUnit -&gt; CompilationUnit`.
not necessarily right? proof of stake doesn't require any sort of idv..
I think the difference between the mio and romio versions is essentially that the romio version copies all the data through the mpsc. The mio has a fast path where it reads into a pre-allocated buffer and writes from the same buffer, and falls back to slightly-less-fast path of writing to an unsynchronized VecDeque if the socket isn't ready for writes. I can't read the parts of the flamegraphs that aren't `send`/`recv`/`epoll_wait`, but I bet it's futures code dealing with mpsc synchronization.
These kinds of functional, high-level abstractions can lead to very general, coherent APIs, which is one of the attractions ‚Äî but it's difficult to get a good picture of just how useful they'd be in reality without being able to play around with them. If we actually had the two language features mentioned here (i.e. GATs and associated traits) in some usable form, I'd like to experiment with these abstractions in something like the standard library, to see just how well they could fit into Rust from an API design standpoint. I think there's real potential for improvement, but it's hard to justify without having the facilities to try.
Fair enough. Native compilation really is a double-edged sword in that respect and people who are used to scripting languages *really* notice the downsides.
So this? https://github.com/fitzgen/dodrio
Sounds like all traits are unsizable and I get that the last member in a struct can be of variable size, but there is only one such position. So how does one put multiple traits into a struct?
almost 20-30% of `tokio` is making extra `write` and `send` calls. Also the difference in RPS is ~28% between `mio` and `tokio` so I think that is certainly where it is. 
redis-benchmark seems to only write 1 PING command per client at a time, so batching PONG responses doesn't seem to make a difference (i.e. you always send 1 PONG). 
That first article is pretty neat, and is the best explanation of "What are monads and why do we need them?" I've encountered so far. Thanks!
Congrats!
Happy to help! :) I recommend looking through the others as well tho for less motivational aspects and more the understanding-how-it-works.
As for pedagogical explanations of monads &amp; functors, I think pictures and diagrams are a great tool here. Explaining these things in text doesn't provide the same deep conceptual understanding.
Looks like [Iosevka](https://github.com/be5invis/Iosevka/) to me. And you're right, it does look great. An amazing font for programmers.
I took another look at the romio and mio versions with strace. The romio version calls `epoll_ctl(EPOLL_CTL_DEL)` before closing each file descriptor, but the mio version just closes them. The mio version also never calls `futex`, but the romio version calls it all the time. There must be unnecessary mutexes in the implementation.
Thank you
Please make sure you've reported these feedback items [here](https://developercommunity.visualstudio.com/spaces/8/index.html)! Rust is getting more popular internally at MS so we want all the feedback.
I would have gone with Groucho or Lamarr
Thanks a lot for testing the program! \#1 is a feature! It's an attempt to replicate Google's calculator app :p \#2 is definitely a bug. I'll work on it later today. If you do find more errors, feel free to open an issue!
Value ranges seems like stepping stone 0 on the path to dependent-types. So I wouldn't be opposed to that. Proving the upper bound of memory usage and other things in the vein of Zig's allocator also seem like a good path for Rust to explore.
It's a custom build of Iosevka, made to look like Monaco. I'll upload the otfs when I get some time!
This actually should *not* require const generics but is in fact a known bug with associated consts: https://github.com/rust-lang/rust/issues/42863
Associated constants and const generics are in a similar space semantically but what the asker is trying to do should be possible currently, it's just either buggy or not implemented: https://github.com/rust-lang/rust/issues/42863
I'm struggling with Piston to try and figure out how to clip a drawing area within its own bounds. I have an item that is drawn at the screen at xywh point (100, 100, 200, 200). I have tried using draw_state of DrawState::new_inside(), and no matter what I do, I can still draw outside of the bounds, and Piston still happily draws beyond the clipping area. I've tried using the Clipping example from Piston, but it doesn't make things clear. I also don't know if I should modify the viewport, the view, or the scissor (which I have NO idea what that means). Hopefully someone has an idea?
Why didn't you just upgrade to 2.0 and used the same name?
Dumb question, but why is it necessary to "take WebAssembly beyond the browser, and build a platform for faster, safer execution on Fastly‚Äôs edge cloud. "? I thought the whole point was to run compiled languages in the browser, whereas a cloud is comprised of servers that can just run x86, no?
It's also useful to do that if you want to generate docs.
Yes https://www.reddit.com/r/rust/comments/5penft/parallelizing_enjarify_in_go_and_rust/dcsgk7n/
Previously I‚Äôd experienced an issue with the lifetime of the builder. In that case you‚Äôd need to assign the builder to a variable before doing your chains. 
I mean Vec uses unsafe, it doesn't mean unsafe is idiomatic. It is a part of the language, but not something you use all the time nor something to be expected in most codebases.
This kind of thing was actually one of the first things I wrote for myself (in C++ 10 years ago), and I was very happy when I got it done. Mine supported custom functions, units, and all sorts of fun stuff. That being said, I use `qalc` on my desktop these days, since it's much more mature than what I used. If anyone is looking for an offline calculator, I highly recommend it. It even does stuff like currency conversion, implicit multiplication, unit conversion, and much more.
You should try `qalc` for a mature tool before getting too deep into implementing your own (unless it's for fun).
This is hilarious, I used the exact same algorithm, Shunting Yard -&gt; RPN for my calculator that I called "ogc" for "offline google calculator" 10 years ago. I made some modifications for implicit multiplication and units as well. I recommend trying `qalc`, though, because I gave up trying to redo mine in rust after I saw their solution.
It compiles if you explicitly introduce a lifetime `parse&lt;'a&gt;` and tell the compiler that the lifetimes are the same. [And this demonstrates the same or similar error with `'static` lifetime](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=40eceffeb5a8e89407f51054c687d2a9) I suspect that this is related to limitations the type system has about the types of traits and/or limitations the language currently has for naming and describing types. We want to say that the type of the closure is something like `impl for&lt;'b&gt; Fn(&amp;'b [u8]) -&gt; &amp;'b [u8]` and the compiler isn't quite putting all the pieces together. It seems to me that when the compiler infers a lifetime for an argument of a closure it can't infer that lifetime as longer than the closure. And there might even be a good reason for that. Most importantly, this can be fixed by explicitly naming the implicit lifetime of `parse`.
AhahahahHah 
 at least I don't have a small penis lol
Is there a less terrible way to get data out of JSON? I'm looking at a specific API ([https://hadoop.apache.org/docs/r2.4.1/hadoop-yarn/hadoop-yarn-site/HistoryServerRest.html](https://hadoop.apache.org/docs/r2.4.1/hadoop-yarn/hadoop-yarn-site/HistoryServerRest.html)) and dreading all the boilerplate involved just to get data out of it. I expect getting the structs right will take considerable time and lots of rework. Then I'll still need to do some simple analysis and write the results into a DB. I don't need this to be blazing fast, but the cost in dev time seems extreme compared to what I've dealt with in Clojure and Python. 
I believe the answer is basically that WebAssembly provides very fast, low-overhead sandboxing of *untrusted* code.
&gt; [...] but it's also about HKT. Higher kinded types are barely discussed. And varkor addresses many of the points therein. &gt; The problem is that without currying at the type level, higher kinded polymorphism makes type inference trivially undecidable. Haskell imposes restrictions wrt. type lambdas to make type inference decidable. We could do the same, or combine GATs with injectivity annotations (to improve inference). HKTs provide for considerably superior ergonomics however. Moreover, Rust already has undecidable *type checking*... not just undecidable type inference (at least because rank-`n` types for `n &gt; 2` has undecidable type inference and we have rank-`n` types for arbitrary `n`s already). Another reason for undecidable type inference is multi parameter traits. You can witness this problem with `let vec = iter.collect();`. Moreover, Haskell 2010's type checking is decidable and retains the principal type property without some extensions and it still has HKTs. Moreover, I think that decidable type inference is not an important property to preserve even if we could (which we cannot). There are other useful type system extensions that are known to result in undecidable type inference, e.g. `for&lt;T: Debug&gt;` (also higher ranked types), various dependently typed schemes such as expressive const generics, and sometimes also GADTs (well, there's [Complete and Decidable Type Inference for GADTs](https://www.microsoft.com/en-us/research/wp-content/uploads/2009/09/implication_constraints.pdf)...). I think that having the compiler throw an error every now and then about uninferrable types is fine; rustc already does this and it works in practice. Undecidable type checking is much much worse. &gt; In order to add higher kinded polymorphism, we'd have to restrict the sorts of types you could use in a way that would feel very arbitrary to users. Not necessarily for reasons aforementioned. &gt; In contrast, generic associated types don't have this problem, and directly solve the expressiveness problems we do have, like Iterable (abstracting over everything with a `.iter(&amp;self)` method) I think GATs will feel like a kludge when you try to use them for things like abstraction over smart pointer and such interfaces. In particular, it is not ergonomic to force the invention of `struct BoxFamily;`. I think is also conceptually backwards. Abstraction over smart pointers or generalization over mappable, foldable, and traversable things are expressiveness problems *we do have*. &gt; (Don't get me wrong, you can write something to abstract over some monads like Option and Result using generic associated types. But its much less ergonomic than Monad in Haskell, even for those). Yes precisely. As for the other points unrelated to HKTs... &gt; IN CONCLUSION: a design that works in a pure FP which *lazily* evaluates and boxes everything by default doesn't necessarily work in an eager imperative language with no runtime. Laziness is irrelevant here. Idris and Agda are both strict yet they provide for monads and do-notation. Purity is somewhat relevant for preserving the equational reasoning you get with monads. However, Haskell gets away without totality and so we can play fast and loose with purity like we do with `iter.map(...)` anyways. Moreover, `const fn` can enforce determinism. Most of Rust's problems wrt. monads are not being an imperative language (it is a hybrid language, but functional and imperative...), it is due to borrowing. &gt; the signature of &gt;&gt;= is `m a -&gt; (a -&gt; m b) -&gt; m b` the signature of Future::and_then is roughly `m a -&gt; (a -&gt; m b) -&gt; AndThen (m a) b` &gt; &gt; That is, in order to reify the state machine of their control flow for optimization, both Future and Iterator return a new type from their &gt;&gt;= op, not "Self&lt;U&gt;" This is why the blog post brings up abstracting over traits so that this can be solved. &gt; Also, our functions are not a `-&gt;` type constructor; they come in 3 different flavors, and many of our monads use different ones (FnOnce vs FnMut vs Fn). This should be solved by associated traits (which we should add for other reasons, e.g. abstracting over HashMap and whatnot). Also, as noted in the blog post, these problems would be faced by linear haskell as well. &gt; Rust's imperative control flow statements like `return` and `break` inside of do notation also doesn't make sense, because we do not have TCP preserving closures. This is addressed in the blog post.
i, too, had close to no programming experience, prior to rust. I wrote eva to learn more about modules, unit testing, publishing etc. most of the parsing logic was taken from Matlab's math expression evaluator. (ill make sure to link it when I get back home). hope you learn something new from my experiment. I would love to see what you've got going! &lt;3
Facebook has been building other tools based on Abstract Interpretation: https://github.com/facebookincubator/SPARTA I‚Äôm looking forward to this project providing advanced possibilities to MIR, such as further optimizations which currently seem to be tricky to perform.
It sounds like you need serde and serde\_json. You can convert json into a hashmap or even directly into a struct. [Playground link](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=a1c1f02dbee910f95ffed39fb7341222)
ahaha looks like calculator programs are the new hello world. i know of only two ways to solve math expressions, Shunting / RPN and Abstract Syntax Trees. honestly i don't plan i using this program much interactively. i might use it in some scripts as a drop in for `bc(1)`.
You're right that that using an explicit lifetime is also a fix, it's what I've come to using. It works but is unfortunate because in actual use that lifetime is being generated un-hygienically in a procedural macro. 
Is something that complex needed? it'd be neat if `cargo doc`/`rustdoc`/`whatever` could just link directly to it, it should know where it is and everything, and it should always be there. Probably best done through an opt-in flag, `--use-bundled-std-docs` or something
It‚Äôs mildly amusing to me that the pro-monad people (I like monads, to be clear, just talking about this thread) refuse to provide concrete examples. Just like how monads are a pretty abstract concept. I do think the questions are meaningfully different. We could add classes to rust as well. Classes are useful. But I‚Äôd want to see specific rust code where classes make things better before deciding if they‚Äôre a good fit for rust.
&gt; It‚Äôs mildly amusing to me that the pro-monad people (I like monads, to be clear, just talking about this thread) refuse to provide concrete examples. Just like how monads are a pretty abstract concept. No one has refused to provide concrete examples. They are provided in the blog post. We can go through other concrete examples of all the functors, monads, applicatives, traversables (e.g. `sequence` generalizes `transpose` that was recently stabilized), foldables, and $random_other_higher_kinded_abstraction that already exist in the Rust ecosystem and those should be plenty. In particular, functors are quite a pervasive idiom. &gt; I do think the questions are meaningfully different. We could add classes to rust as well. Classes are useful. But I‚Äôd want to see specific rust code where classes make things better before deciding if they‚Äôre a good fit for rust. As you know, classes have known problems with inheritance over composition as well as problematic amounts of subtyping (we have enough already) that makes reasoning about the type-system (and future extensions) a mine field. We have good reasons for not including those and I wouldn't want to include them in any new language.
&gt;They are provided in the blog post. The blog post mostly shows how some code would be written. It does not show why that code is useful. There's no "here's a problem in a real example in Rust code, and here's how these features solve it." Even the do notation example is purely abstract examples. &gt; We can go through other concrete examples of all the functors (etc) that already exist in the Rust ecosystem If they already exist, you have not shown why this feature helps. They already exist. &amp;#x200B;
&gt;I am also not saying HKT is impossible in Rust either, just that if there are challenges or friction, this kind of thing is where they come from. &amp;#x200B;
`remove` shifts all following elements to the left, so in addition to being quadratic, this will actually skip the element after each removed element entirely, which is probably not intended. Use `swap_remove` and only increment the index when an item was *not* removed instead.
The blog post is *research* material; not a motivational post leading up to an RFC. &gt; There's no "here's a problem in a real example in Rust code, and here's how these features solve it." Many RFCs are typically not written in terms of "here's a full body of code"... rather, indicative snippets are used often including `foo` and `bar` that make it sufficiently clear how something can be used. &gt; If they already exist, you have not shown why this feature helps. They already exist. No idea what the point here is. If we have many concrete examples of functors and so on in real world Rust, then surely being able to work abstractly over them is useful. This doesn't feel substantially different than showing why generics are useful in the first place. &gt; That is a challenge. But I think it's a necessary one. And one we shall endeavor to meet. No one is asking for a free lunch here. The good news is that monads can be encoded as a library feature so once we have abstraction over traits we can begin testing it out. As for abstraction over traits, they are justified by things unrelated by monads (well at least this is something I plan to show).
&gt; then surely being able to work abstractly over them is useful. Again, this is an \*abstract\* answer. &amp;#x200B; I don't think we're going to get anywhere here. Have a good night.
That's fair enough. I think I have shown why decidable type inference should be considered a non-issue however.
I fixed it the root Drain can not return an Error, you can have errors panic or ignore them like I did, by wrapping the Drain in another Drain.
i published my second crate! [eva](https://github.com/nerdypepper/eva) - a calculator REPL 
It depends on what kind of overhead you're trying to eliminate. If your #1 priority is memory, then sure, this might be worth looking into. If you're interested in those two specific types (4 x `u8` as 1 `u32`), then it's actually pretty easy to do. Check out https://doc.rust-lang.org/std/primitive.u32.html#method.to_ne_bytes My suggestion is to try it, and then benchmark it. Often your intuition about which will be faster in optimisations like this will turn out to be wrong. :) If you want to go next-level crazy with this stuff, you could check out things like https://github.com/rust-lang-nursery/packed_simd ‚Äî maybe your computation could actually be performed _without_ unpacking 4 separate `u8`s!
With a u32, you have to worry about endianness; also your alignment is higher (4 bytes instead of 1 byte). Stick with a [u8; 4] or even a struct with named fields. If you `repr(C)` the struct and declare the fields in order of what they appear in array form, you can even transmute between the two seamlessly.
Wow &amp;#x200B; So I'd basically be doing something already done?
I understand everything you said, but I do not know what repr(C) means. What does that mean?
If you apply a `#[repr(C)]` annotation to your struct, it asks Rust to lay it out the same way a C program would. Typically, they're used for passing data back and forth between C and Rust, but it's also useful to turn off Rust's struct optimization which can reorder fields in memory to minimize space lost to padding without incurring the access overhead of a packed struct.
Is it normal to be excited about this?
As someone working to migrate off Python (chosen over a decade ago for the memory safety, ecosystem, and high-level APIs), I certainly find the amount of power Rust gives me exciting.
sled is alpha due to still changing quite rapidly in several correctness critical areas. It's better tested than many far more popular systems though, so while I basically tell people to only use it as a cache until the file format stabilizes (we'll become forward compatible at that point) I would be happily surprised (and thankful for the issue) if you found a dataloss issue for me to fix and turn into another systematic test
FYI, I've incorporated your key trait suggestion (second playground) into my code, as it seemed like a much more applicable suggestion. I definitely like the fact that there's type inference here; it makes for some very interesting code. I'll have to figure out how to use these for my Callbacks as well - this will make coding life so much easier. Do you have any suggestions as to how I might make my BaseWidget/CanvasWidget more extensible using macros?
I went from Lua to HTML to JS to Python to Julia (and other python-like languages), and now Rust. I'm fluent in maybe 4 of those. Rust is multipurpose, I have found.
It makes me happy that you're able to get the gist of some of the system without too much effort :) I'm always under the impression that the code can be cleaned up quite a lot, but it's a hard trade-off when I feel intense pressure to get it past alpha ASAP, which is more about stabilizing the on-disk format for forward compatibility. We use i64 on 32 bit systems because AtomicI64 is unstable, but this will go away when it stabilizes. Blobs are only used to store page fragments that are too large to fit in the single storage file log. This might make your reading far easier: https://github.com/spacejam/sled/wiki/sled-architectural-outlook Also you might have fun reading the various quickcheck and crash related tests, since event log and debug_delay seemed to interest you :) Have fun!
Nice! Any benchmarks how fast Smithy is on bigger projects/simulations?
No, more like Imba in that case. While dodrio first builds the entire virtual DOM tree and then calculates the difference, doduo would make changes to the DOM as it's going through the VDOM. A note on the name: dodrio uses three bump arenas and doduo uses only two. (Like the number of heads) But that's also their *only* similarity: bump allocation.
It's just scaled down to the page width. Which means it looks acceptable on mobile (but still strange, since the code's font size depends on the width of the image and how much it is scaled), and outsized/gross on desktop.
https://pest.rs
The code execution is sandboxed. That‚Äôs the whole point. It‚Äôs much safer to run untrusted executable binary.
Trademarks!
Welcome! And thanks for your work!
Please do!
I think they are saying that you could replace the Mutex implementation during tests, much like some people replace allocators to catch unexpected behavior with them.
I'll include links to the code in github in the next post. I'm using images because it is how I organize the review process.
I went over the architecture documents, yes. Take into account, building database engines is literally my day job, so I have a fair bit of familiarity with the topic at hand. One of the reasons that I appreciate the event log / debug_delay is that I have had to write similar stuff to shake bugs / investigate, and it is good to see this level of maturity from the codebase. And I haven't gone through the whole codebase yet, but I haven't found anything that jumped at me as messy stuff. The `i64` comment wasn't on 32 bits, but actually on why use `isize` at all? Because you want to have `AtomicISize` ?
So what would you say about this? https://github.com/cloudflare/boringtun/blob/34cc88c1d6599cc88cf0c336693ab51a00e962b1/src/crypto/blake2s/mod.rs#L327
I've got quite a stable of languages myself (Python has just been my favourite by far over the last ~15 years) and even a few I've flat-out forgotten that I used to be fluent in (eg. QBasic). I just didn't want to toot my own horn. Rust is definitely multi-purpose. Unfortunately, two of my biggest purposes are still waiting on the ecosystem to catch up (Django's ecosystem of [reusable components](https://djangopackages.org/) and mature bindings to Qt's `QWidget` APIs.) and one of my other purposes is held back while I wait for a good option to emerge to build a [YAPSY](http://yapsy.sourceforge.net/)-like plugin system despite Rust's lack of a stable ABI and the C ABI's painful limitations.
Again, I have to preface this with the disclaimer that I apparently have no idea what I'm talking about when it comes to unsafe code... but... I think that's "safe" to do what they're doing there, because they're transmuting to a type whose alignment divides the source type alignment. Going the _other_ way would be super-bad, because you might create a `u32` that is misaligned. Hopefully somebody else who actually deals with this stuff can comment on whether that's right. I've somehow written tens of thousands of lines of Rust code without ever having to touch `unsafe`. :)
I just published a companion crate that provides a `Position` type that implements `Location`, and a `here!()` macro for construction one that references the current source position: https://github.com/casey/position I've been experimenting a little bit with how easily attach a position with a location, but I'm not sure what would be most ergonomic.
Amazing!
I was able to accomplish this with types. I also got dacite-winit working.
Assuming that you mean for each pixel to be RGBA data, then jumping your data from \`\[u8;4\]\` to \`u32\` has no effect \_except\_ that your alignment is now 4 instead of 1. This isn't the worst thing, but it's also fairly useless. The real gains to be had is when you move from alignment 1 up to alignment 16, or even alignment 32. Then you can sweep up four pixels at a time into SIMD arrays and batch process. Of course, the SIMD registers have to be transposed around so that you have registers for four blue channels, four green channels, and so on. It's tricky, but very fast once it's been carefully written out.
For some reason, it didn‚Äôt occur to me that I could use strace. I will definitely take a closer look into the syscalls. As to the mspc, it does not impact the performance here, indeed. I believe it has a much higher throughput than network stack. My initial version was trying to send the response in the loop with receiving the request, but it turned out to be slower as it blocks the loop until all the data is sent.
Truly awesome thing. Thanks for sharing it! I've been trying to get my stuff together and do some abstract interpretation again forever after a long hiatus. I think it has a ton of power for this kind of analysis.
Thanks for this; I've been interested in this area for a while and I found your post to be a great easy to follow introduction to it :) 
Well, whether or not it was meant to be, the torn was hooted. I barely touched C as the tutorials seemed really obscure to start. What's an ABI and could you extend it?
Yes, that is exactly what I mean. SIMD sounds like an option, just not today's special. I don't how to use that. lol
Wow.
&gt; What's an ABI and could you extend it? It's the low-level definition of how to format and organize data in things like structs and function calls. Every time you're cooking up a way to convert Rust constructs with no C equivalent to the C ABI, you're essentially defining an ABI of your own. The problem is that I don't trust myself around the `unsafe` keyword. I'll stick to writing my applications as Python applications with Rust modules until someone else comes up with a plugin analogue to `rust-cpython` (without which, I'd have stuck to pure Python programs).
Yeah, basically everywhere that a usize / isize is used now will become u64 once AtomicI64 / AtomicU64 is stable, for exactly the reasons you flagged in the post this is desirable to be portable. &amp;#x200B; I use isize vs usize in a couple places as a defensive measure to prevent accidental misuse of one type of coordinate for another. We have physical file offsets (which might be reused as sections of the single file get GC'd) and we have strictly monotonic logical offsets, and in the past I accidentally wrote a few nasty bugs due to confusing the two. This is mostly a defensive coding measure, but could also be accomplished by using a newtype etc...
The leading `::` means that `std` is resolved at the global namespace. * [Clash with module in current namespace](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=0a8f39d8d2cdc7febe35cb6f0b5ec55c). Leading `::` avoids this - try removing them. * One step further: [referring to items that aren't in the namespace at all](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=f3da5ad06f03b0d08686a237d2f56c99). Again, try removing the `::`. If you look at source code for a lot of macro based crates they should really format their imports like this: as long as `std` is linked this will work. You never know how someone has their project configured so generally this is done to ensure compatibility. I also think there's a slight argument for readability, especially in large codebases with lots of `use` statements everywhere, but that one's subjective! 
Alignment is only a problem with types that contain references. If you transmute an array of integers by value all you have to worry about is that you handled endianness correctly(it's not undefined behavior to handle endianness incorrectly).\`mem::transmute\` will make sure that the arrays are the same size.
Ah I see. Would it make sense to use the Rust ABI as a base?
What is the best way to check if multiple slice items equal certain data? I'm checking the header of a file, which is in a `&amp;[u8]`, and I want to make sure the first three bytes equal the correct characters; but I also do not want to panic when it has issues. I *don't* want to do something like this: match data.get(0) { Some(n) { if (n as char) != 'E' { return ESError::InvalidHeader; } }, None =&gt; return ESError::InvalidHeader } // Do the same thing for the next two characters This is obviously.. messy, especially when you extend it over several characters. I imagine I might be able to do something with if-let and the `?` operator but that wouldn't be returning the reason behind the error. Is there a better way of doing this?
Great writeup! Very easy to follow, a cool problem to solve, and a satisfying solution. I didn't realize how easy it was to get started with tensorflow in Rust, having never used tensorflow itself before.
[https://github.com/rust-unofficial/awesome-rust#parser](https://github.com/rust-unofficial/awesome-rust#parser)
The Rust ABI is unstable. (in the same way that an API can be unstable) They're allowed to change it from compiler version to compiler version and they've already done so at least once (when they implemented the struct packing optimization based on reordering the fields). That means I only have two options using it: 1. Require that plugins be built using the exact same version of the compiler as the application itself. (Not much more useful than compiling them into the application and not having a proper plugin system) 2. Fork the Rust compiler so I can ensure that it never changes, and require anyone who wants to build plugins to use my custom builds of the Rust compiler. (Not worth the effort and, besides, I have no experience with Rust compiler internals.)
Thanks! I didn't know about that function!
&gt; the trait isn't object safe I don't understand what you mean by this? You can store a collection of trait objects of this type.
This is all very obscure to me. Thank you for taking the time to write this.
I was actually surprised how quickly I had a working prototype after working on the project for only three or four hours! With the help of the compiler the errors were quickly fixed.
Around these parts you'll be applauded, not banned, for asking novice questions with a pure heart and an open mind. That's how we roll in Rust. I know I learned a bit from the responses.
Err lol funny you should mention. I wasn't referring to this post. I uh like to joke. You know what? Let's just go back to being applauded. Yay thanks yeah I'm the best woo hoo
This technique is usually described as incremental DOM. It doesn't support many composition properties of Virtual DOM. If you want to learn more about this technique, I'd recommend to look at angular ivy instead of Imba.
You can use [slice patterns](https://doc.rust-lang.org/nightly/edition-guide/rust-2018/slice-patterns.html) for this. [Playground link](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=48c1a1cb042e831b454d5ea49af8bc31)
How different is amethyst than Godot/unity/unreal. I‚Äôve seen they provide a software for graphical stuff. But I‚Äôve never seen such thing in amethyst !
Try if data.get(0..3) != Some(&amp;[b'E', b'E', b'E']) { return Err(ESError::InvalidHeader); } 
Thanks! It looks like I'll have to do https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=de891d7851ee85094acb13ccb5141d6c in case it doesn't extend as far as I want it, and also capture only the first three. That's a lot better than the many lines of match statements though, thanks.
Interesting! thank you for the detailed explanation.
Ah. I was answering ‚Äúis it possible for me to do this today‚Äù. If you‚Äôre asking ‚Äúwould it be possible for cargo or rustdoc to do this some day with some consensus-building and implementation work‚Äù, then almost anything is possible. Consider filing in issue to ask for it and discuss a possible design.
Very fun. Maybe add a second of delay before the play resumes after the ball is reset? The computer surprised me a bit.
Can you? Is there something I'm missing in [this playground example](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=55a375d49d47e913b0dc8c211a9bd9b4) ?
Yeah totally appreciate that - it looks great as a cache! Wouldn‚Äôt any crash between fsyncs potentially result in data loss?
I've been vaguely attracted to Ada since the nineties when I used gnat on Irix a bit. Learning VHDL later also gave some fuzzy feelings. I found myself often thinking I'd like a more strict replacement for C with native compilation, low overhead, and a nice build system. Especially if it could also target bare metal on controllers. It seems to me that over the years Ada has kept falling farther and farther away from that while Rust keeps striding closer. I hadn't really thought of it that way before this post, but maybe Rust is becoming the Ada I kept waiting for.
Thanks! That seems to work even better. I hadn't realized you could pass a Range to the `get` method.
Just enable parking_lot deadlock_detection cargo feature during tests. What I'm saying is that they have no idea which kind of Mutexes you are using, and that for them to know they would need to have some way to, given some snippet of code, say: "that's a mutex", which is impossible.
I thought the comparison is unfair, because tokio does more work than python+libuv. So, I [made a pr](https://github.com/frol/libuv-vs-rustmio/pull/2). At least on my machine, the numbers are now close, around ~130k rps.
serde_jspn has support for dynamic structures
Another good benefit is that this will allow easily running your software on a non-standard architecture (e.g. RISC-V)
I honestly have no idea how the do notation presented there is supposed to work. It doesn't make any sense to me. It obviously cannot work for even simple instances like iterators, because iterators are non-strict and the closures they store execute once you've left the function body. So at the point they execute what should you make of references to local control-flow constructs?! Maybe I'm missing something.
Oh, I see. Does this get you where you want to go? ([playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=88bf61656e0a58bb2588878c1dc7aa19)) I haven't really studied what you're trying to do carefully, so maybe this isn't it.
&gt; A pattern, not a thing. It surely can be a thing, so long as your language lets you express that thing. It's also most definitely a theoretical "thing", the same way natural numbers or functions are a thing.
Is that 130k for the tokio version? What's the difference between tokio and py on your machine in terms of rqs? &amp;#x200B; I'm playing around with this as well. It's rather interesting.
Average for 3 runs on a noisy laptop: tokio: 129861.67333333334 python: 131950.62666666668 So about 2%
No problem. I enjoy explaining things and we all had to start somewhere.
Does alignment still apply in arrays? I thought it was mostly about memory allocation. A 2-element array of u32 should be of equal size to one u64, otherwise arrays get incredibly inefficient... I understand this is not the case with bool, because bool is of the size of u8 but what's stopping arrays from addressing u32 elements? Pointer size?
Because Go is really good at the thing it's designed to be good at - writing heavily concurrent backend services.
Why don't you ask over at /r/golang? While there will no doubt be some people on this subreddit who have experience with Go (and possibly like / prefer it), I don't think it makes much sense to ask people who use Rust to explain the benefits of Go. As for your project, languages such as Java and C# have wonderful garbage collectors, and they do have generics.
Go is easy for people coming from a runtime-typed language to learn (partly because you don't have to understand the arcane parts of generics), has a garbage collector (which you have already noticed can be convenient), and has a good stock of libraries in areas where Rust is still figuring it out (notably webapps). Go supports something that looks more like object-oriented programming, which is comforting to OOP enthusiasts. Go is a Google language: don't underestimate this factor. Arguably the biggest success factor behind C++ in the early days was that it was an AT\&amp;T language. People like a language that is actively supported by a tech giant: it means medium-term stability and a market for programmers is kind of built in. If you want to write GC-ed mostly-OO programs in a modern, fast, well-supported, widely-used language Go is as good as anything except maybe Java. Java has an awful lot of baggage associated with it, and tends to be pretty boilerplatey. Java's OO-only status is often annoying. So far Oracle doesn't look like a very good steward for Java. Besides, the cool kids are using Go.
(Sorry, this is not my project haha)
Not quite, though it's similar to what I have in the repo right now. Basically I want to be able to feed a struct a URL every time the URL changes. The struct will extract the data it wants from the URL, and return this "data" that implements `PartialEq`. I want to compare it to the data it returned from the previous URL, and run `start` or `stop` based on some rules on whether the new data and old data differ. [Here](https://github.com/bschwind/rs-frame/blob/9ca80beb6cbac84d430eabb6069cd1554e955b70/rs-frame/src/lib.rs#L16-L22) is the trait as it's currently written in the repo, and an [example implementation](https://github.com/bschwind/rs-frame/blob/9ca80beb6cbac84d430eabb6069cd1554e955b70/coach/src/main.rs#L8-L29) Currently I'm storing the "params" on the structs themselves, then hashing the entire struct to see if it changed or not. While this works, it seems strange, and if I had other state on the struct unrelated to data from the URL, it wouldn't work as expected.
Amen to that. Go is very well designed to be extremely productive very quickly in a domain that it‚Äôs heavily biased towards... a domain where genetics aren‚Äôt a good thing ‚Äî they very often aren‚Äôt ‚Äî and where the error handling it provides makes actual sense. Outside that domain it‚Äôs painful. But then so is using plasma torch as a hammer.
The rust book is \[literally free\]( [https://doc.rust-lang.org/book/](https://doc.rust-lang.org/book/) )
The rust book is [literally free](https://doc.rust-lang.org/book/)
Ironically, Java‚Äôs strength lies outside of the language. The language is heavily outdated but you can easily use Kotlin and still get all the ecosystem and JVM benefits.
Isn't this offer about getting the printed version?
Here is why: 1- You can write memory safe code without Borrow Checker, which means any Java-C#-Perl developer will feel quite comfortable 2- You don't have an interpreter, which means a really low footprint, even with GC compared to Java 3- The language is terribly simple. Like dead simple. I read a book about Go and without programming a single line of code I was able to send a small comment/fix for one of the authors examples. Go syntax and keywords allows you to be able to writing code with it in about a few days or a week. 4- You can write concurrent programs really easily. Specially backends for the web that scale well. &amp;#x200B; Basically, it is a lower overhead, simpler, more lower level -kind of- Java. Go and Rust don't compete in the same sport. it just happens that a lot of stuff that was being developed in a mix of C++/Java fits Go really well. And probably the most import point of all: It is perfect for the big players. It is perfect for Amazon, Google, Microsoft... Anyone who handles a lot of data in the backend really benefits from this kind of language where you can just put a group of programmers in a room and they will start prototyping good code really fast. These players are the ones shaping the industry. And when I say Google I mean all your multinational level business that has backends, say Uber, Airbnb... Small players are jsut heavily influenced by what the big players do, and if they can write code that from the start will easily allow them scale, why just not? I can pick, right now, any Go codebase and if I understand the types and premises of the code I could understand what it does. If been studying Rust for quite some time and I wouldn't be able to do that ath this point right now.
&gt; Its error handling is... bad. I had totally forgotten about this gripe. Like the Rust error story needs maturity but I'll take some messiness/extra ceremony with `Result` over "Don't forget to check `err != nil`!" any day of the week.
What's the difference between this tool and `miri` ?
For a CDN, you need *very* fast response times. But with normal cloud tech (virtual machines or containers) that means you need to keep your customer's code always running because startup times are a problem otherwise. With this tech every thing to run is just a native binary with zero startup time, while still being safe.
Even better: if !data.starts_with(b"EEE") { return Err(ESError::InvalidHeader); } https://doc.rust-lang.org/std/primitive.slice.html#method.starts_with
What about C#? my understanding is that Java is steadily fading into relic-status, while C# has seen a recent resurgence. Thoughts on that?
Story-time: Was doing an OS course. We had to write some in-kernel code. we had to do memory allocations. My partner `literally` commented if we really need to check if we get NULL or not from `kmalloc`... The fact that you *have* to handle a `Result` is one of my favourite things about rust 
Yeah, I've decided to start over in C# - still early days in the project
MIRI is an interpreter: take a function, some arguments, get the result of the execution. MIRAI is an abstract interpreter, similar to [this other one by Facebook](https://www.reddit.com/r/rust/comments/b6rupo/mirai_abstract_interpreter_for_mir/ejmqexm?utm_source=share&amp;utm_medium=web2x): take a function, analyze its properties. So, completely different. Similar to Valgrind vs CppCheck.
Aside of lifetimes, my intuition tells me that orphan rule may also be the source of headaches. As well as `Send` / `Sync` or autotraits in general. But I do not understand it well enough to provide an example.
I have just added u/plhk's implementation to the scoreboard. It is now on par with mio, yet Python implementation is still consistently the fastest which amazes me.
https://github.com/lalrpop/lalrpop
So, is this basically containers 2.0 then?
[u8;4] size is equal to u32, what sort of overhead are you talking about? You can transmute between without problems like I do here https://github.com/DoumanAsh/lazy-bytes-cast :)
No they are digital copies of the books.
Check if you have winapi functions like in this SO answer: https://stackoverflow.com/questions/8046097/how-to-check-if-a-process-has-the-administrative-rights/8196291#8196291 If not, use msvc toolchain
you're probably wrong
I AM using the winapi crate, so I do. I'm now concerned about whether rust's mem sizeof would match `sizeof( TOKEN_ELEVATION );`
Go is very simple language without much features which means it is simple for even lazy person who wants to start writing good stuff in one day (spoiler you cannot write good Rust day 1)
It depends on whether winapi gets this struct right, but I think it should match so don't worry. Peter, the author of winapi crate, took a great deal of care to make winapi
How do I make an null handle? std::mem::zeroed?
It's the same thing with Go making you check the errors.
do you mean `HANDLE`? If it is pointer type then just `std:ptr::null_mut()`
I‚Äôd love a mobi or ePub-version for my Kindle, but I can‚Äôt find that anywhere. There was a repo to convert the original rust-book created some time back, but it hasn‚Äôt been updated in a while and now fails. So AFAIK there is no Kindle-compatible version I can get for free. Please correct me if I‚Äôm wrong. 
ah ncie it expects Elevation to be LPVOID in GetTokenInformation
I'm curious how this project compares to [Wasmer](https://github.com/wasmerio/wasmer), both in terms of project goals and, if they are similar, in the particular differences between the two. It seems like both of them do not *properly* support Windows yet, though Wasmer has experimental support with some very big caveats. Lucet doesn't seem to support Windows at all (yet?).
That's still an option. I already had the two crates. The `psl` crate was a "cache". It managed periodic downloads of the list and caching it. Unlike the `publicsuffix` crate which had already reached `v1.5`, it was still at `v0.0.3`. Since I wasn't happy with its design and planned to re-write it anyway, I decided to use it for the re-write as I was still exploring this new design. I personally prefer `psl` (an abbreviation for Public Suffix List) because it is shorter but `publicsuffix` is more descriptive so if more people prefer that name, I will publish `psl`'s code as `publicsuffix` 2.0 and drop it.
```if GetTokenInformation( hToken, TokenElevation, Elevation, std::mem::size_of_val(&amp;Elevation) as u32, &amp;mut cbSize ) == TRUE {``` expected *-ptr, found struct `winapi::um::winnt::TOKEN_ELEVATION` note: expected type `*mut winapi::ctypes::c_void` found type `winapi::um::winnt::TOKEN_ELEVATION`rustc(E0308) 
Well be sure to check types, with api docs it should be easy :)
\&gt; For example your libuv python code does a single write to the stream of all the pongs at once. It would do that only if redis-benchmark send a batch of commands, but I use \`-P 1\` which limits the number of batched commands to 1. Thus, as u/singron wrote, it does not make any difference in this particular case. I agree, that I need to clean things up to compare apples to apples, but I have spent a few days already on this, so I decided to share my findings earlier to avoid useless work (in case somebody more knowledgeble spots the issue right away).
Not that I know of, but you can pay $15 and get those versions with the Humble Bundle. 
Miri is a runtime analysis tool (it interprets a program given on some input and detect incorrect behavior at runtime). Abstract interpreters are static analysis tool.
Is it the same one?
I can't convert it to LPVOID though And sure
Yeah it is...
MIRAI = Êú™Êù•("the future" in Japanese) ?
&gt; LPVOID You can convert reference to pointer
Is MIR stable ? Or does that mean that this project will need to be updated every time something gets changed in MIR ?
Nom and pest are top-down parsers which are inherently different from LR parser generators like yacc. LALRPOP is most likely to be what you are looking for.
Whatever it is, I can't figure it out, because `as *mut LPVOID` didn't work
&gt; Can you reimplement `publicsuffix`'s api with `psl`? I could, however, due to `psl`'s design it takes much longer to compile and the list cannot be updated at runtime. These are serious drawbacks and I know that they may not be acceptable in some situations. This is why I made `psl::List` a trait and this is also one of the reasons why `psl` hasn't been released as `v1.0` yet (the other is deciding whether to go with `publicsuffix` or `psl`). The static implementation (`Go`'s standard library implementation is static too) has been proven to be quite fast as shown in the OP but I also want to explore an alternative design that may not be as fast but doesn't come with these drawbacks. &gt; Does Cargo support a deprecated/replacement attributes? Last time I checked, it doesn't. To "deprecate" a crate, I think you need to:- - yank all the published versions - remove all the code - drop a `std::compile_error!` in `lib.rs` or `main.rs` with the deprecation message - update the documentation accordingly - push a new version to crates.io. &gt; Seems like tooling could help get users migrated quickly/quicker? Just spitballing... Definitely.
I haven't needed it on my Sony PRS-505, so I don't know how well this works, but have you tried this? 1. Hit the [Print this Book](https://doc.rust-lang.org/stable/book/print.html) button and cancel the Print dialog when it pops up. 2. Save the resulting single-page HTML copy of the book to your hard drive. 3. Load it into [Calibre](https://calibre-ebook.com/) and ask it to convert to mobi or ePub. (Calibre also offers scriptable command-line conversion via the `ebook-convert` command.)
Fading how? If you wanted to create a lot of server side services, including for (relative) high load, what language/platform would you choose, if not jvm?
Do you have any blog posts specifically about how you approach a code review of a project like this? 
I think you're just going about this in a fundamentally problematic way. Instead of starting with a real problem that users are facing with the language and working backwards, you're starting by trying to conceptualize how to add significantly more expressiveness to the type system, and then trying to find problems that it solves. I also think you are overly dismissive of the learnability of monads, and the devastating consequences it would have to Rust itself if they were added as a core concept that folks need to learn how to use. And personally, I don't think you get to do this. I know I don't. I don't think monads are that big of a deal _for me personally_, but I've spent time studying PL research and have written plenty of Haskell programs. You're even worse---you're in a world class PL research program. These are bubbles that make it very difficult to understand how hard it is to learn concepts like monads. I mean, I think we're already struggling with teaching folks to use the type system we have today, and it has nothing to do with the fact that it lacks expressiveness, but that expressiveness is itself the problem. I think we have a complexity budget and Rust has either already blown it or is teetering on blowing it. Adding the kind of expressiveness that monads bring without some supremely well motivated concrete examples of _real code_ is going to completely kill it. I, like Steve, also think this conversation probably won't go anywhere. We've already chatted loosely about this in the past, and we're clearly of different minds about this. But you're part of the group of people that gets to decide about this, and I feel so strongly against the introduction of monads that I think it's worth piping up about this to make sure that it's heard.
go is very easy to learn, and thus very easy to read other people‚Äôs code. it compiles very fast. it is very easy to spin up threads. the standard library is very good and complete. 
Oh... 
Bought it for the python and R books. Its a good bundle, but the rust book is free
I am curious. Can we use smithy with actix web framework? I did not try but just want to know. If using it with actix(or others) it will be realy nice.
C# is a much better language than Go, IMO. What hasn't been mentioned yet here about Go is that it was designed to be super simple to make it possible for a low-skill programmer to pick up someone else's code. Go apps might get lengthy or twisted, but they never get _abstract_ in the way that another OO or functional language might. In short, Go isn't a solution to a coder's problems, but to a large multinational's problems. It's a technical solution to the workforce problem. Mind you, that doesn't preclude smart people from using it to build good stuff. Go has some other qualities. But it's good to understand that it is purposefully limited.
C#? I find it a much more pleasant language. The JVM is pretty amazing, but .NET provides custom value types, it doesn't have type erasure... C# has things like `var` and `async` / `await`. Also LINQ is much more powerful than Java's streams because of expression trees. I also prefer the standard library, and with .NET Core you are no longer tied to Windows.
&gt; Instead of starting with a real problem that users are facing with the language and working backwards, you're starting by trying to conceptualize how to add significantly more expressiveness to the type system, and then trying to find problems that it solves. I disagree with this description of what I'm doing. I already know that this solves real problems for users, existing ones and new potential users. Moreover, when doing language design research, you may know that something solves some problems, and then you try to find out if it can solve more. As for associated traits and trait generics, which this blog post builds upon, I know there are also real problems it would solve. Part of the research that goes into making a fully formed proposals involves investigating the fuller picture of the problems that it would solve. Also, frankly, every RFC I've hitherto proposed has been, in my view, about real problems. &gt; I also think you are overly dismissive of the learnability of monads, and the devastating consequences it would have to Rust itself if they were added as a core concept that folks need to learn how to use. Perhaps. Personally, I think describing the learnability impact as *devastating* is quite a stretch. &gt; And personally, I don't think you get to do this. I know I don't. I don't think monads are that big of a deal for me personally, but I've spent time studying PL research and have written plenty of Haskell programs. You're even worse---you're in a world class PL research program. These are bubbles that make it very difficult to understand how hard it is to learn concepts like monads. I'm not a PhD nor a researcher at the university or anything like that. I think you overestimate how much *"in the bubble"* I am. Moreover, part of being a student in one of these places is that you get to teach and talk about functional programming. My experience is that teaching monads is not difficult. It can be intuitively explained in pictorial form. As far as I was able to tell, 1st and 2nd year students had no major problems understanding these concepts when I explained them. It was no more difficult than understanding mutation and side-effects (which many found difficult when trying to understand Java). These students are certainly not PLT researchers and most won't become that either. &gt; I mean, I think we're already struggling with teaching folks to use the type system we have today, and it has nothing to do with the fact that it lacks expressiveness, but that expressiveness is itself the problem. Perhaps. It's not my experience. I personally think the contortions and brick walls some libraries face and export to their users is a bigger problem when it comes to the type system. Also, I think the complexity of unsafe code is a much bigger issue. We all have different things we're good and bad at. &gt; I, like Steve, also think this conversation probably won't go anywhere. I suspect so. &gt; But you're part of the group of people that gets to decide about this, [...] I want to emphasize that I'm just one person; there are a variety of viewpoints within the language team about this issue. &gt; [...] and I feel so strongly against the introduction of monads that I think it's worth piping up about this to make sure that it's heard. It's definitely heard. It's difficult to miss. I also want to reemphasize that varkor's blog is experimental research. None of this is likely to be focused on for quite some time. For example, while I'd like to focus on GADTs and associated traits today, it's not in our roadmap goals and GATs haven't even been implemented on nightly yet. In practice, we take quite an incremental approach to change.
Can you share code? most likely you need `&amp;&lt;var&gt; as *const T as *mut T`
&gt; I disagree with this description of what I'm doing. I already know that this solves real problems for users, existing ones and new potential users. For clarity, as someone who has been writing Rust for years daily, and has written tens of thousands of lines of code in it, and has shipped it in production at work and in my hobby time, I am actually not aware of any compelling use cases solved by monads _in Rust_, and I can't recall anyone try to make a serious _non-abstract_ argument for them. What I mean by this is that I've never seen anything that makes me say, "I want to use that in this spot and their additional complexity is worth it." This is in contrast to something like GATs where I can actually point to [concrete real examples](https://docs.rs/fst/0.3.3/fst/trait.Streamer.html) where I've struggled without them. So while from your perspective it might be obvious what's being solved, from my perspective, it isn't. There might be a messaging or communication problem. I'm not sure which. &gt; I also want to reemphasize that varkor's blog is experimental research. I realize that. But I want to emphasize how hard it is to know when exactly is the right time to voice opposition to a feature. Going from, "nobody has any good ideas on how to introduce monads into Rust" to "here's a comparatively simple design" sounds like a good a spot as any.
This makes me wonder, why can't MIRAI just be implemented as a clippy lint ?
flush gives you control over this if you're relying on local durability for some reason instead of replication, but that's generally considered a terrible idea 
&gt; [...], I am actually not aware of any compelling use cases solved by monads in Rust, [...] Sure; I understand your view. &gt; [...] and I can't recall anyone trying to make a serious non-abstract argument for them. What I mean by this is that I've never seen anything that makes me say, "I want to use that in this spot and their additional complexity is worth it." This is in contrast to something like GATs where I can actually point to concrete real examples where I've struggled without them. I have seen it and moreover, I think many of the GATs examples are ones where HKTs would be more appropriate as GATs impose costs in terms of contortions, complexity, and subpar ergonomics in APIs. &gt; So while from your perspective it might be obvious what's being solved, from my perspective, it isn't. There might be a messaging or communication problem. I'm not sure which. Perhaps it is. Mostly however, it is a matter of workload and what I have the time to provide. I already spend exorbitant amounts of time working on Rust stuff (and as many, without pay) and I cannot spend the necessary time right now to provide you with a list of examples, especially because it is not a focus. &gt; Going from, "nobody has any good ideas on how to introduce monads into Rust" to "here's a comparatively simple design" sounds like a good a spot as any. Fair enough; but I would urge you to consider the article from the author's POV of "here's an interesting thought experiment I want to do because I enjoy to think about these things".
That's awesome, looks very smooth and the game seems addictive! I don't know the most accurate way to find this out, so I'll ask. What is the fps, memory footprint and CPU usage by this game? 
https://en.wikipedia.org/wiki/Mirai_(malware) And also the botnet.
You can't avoid the copy if you want a `Vec&lt;i32&gt;`. `Value`'s representation is completely different than i32. You'd have to create wrapper `ValueVec&lt;i32&gt;` which owns the `Vec&lt;Value&gt;` &amp; then implements methods to access the i32 inside the wrapped `Vec&lt;Value&gt;` Alternatively, a `ValueSlice&lt;i32&gt;` could contain a reference to `&amp;[Value]` if you don't want to take ownership
&gt; I know, and I'm grateful for it. I put a lot of trust into the lang team because they almost always make the right call. That's why I often do not get involved in lang discussions, because I know you good folks have got it taken care of. &lt;3
&gt; if we really need to check if we get NULL or not from `kmalloc` Well, there's `__GFP_NOFAIL` :-).
I have not tried, but it should be easy to e.g. Share types across crates and have your actix web or Smithy app fail to compile if the types are mismatched. But this isn't unique to Smithy! You can do the same thing with any of the wasm frontend frameworks.
How would one go about defining a grammar that has same keywords as Rust? Having prefixes or anything like that for the grammar seems kind of roundabouty.
This is a parser generator but it's not equivalent to lex/yacc, it uses a different algorithm.
You could use use a proc macro and leverage rustc‚Äôs parser and ast if you‚Äôre reusing most of the rust grammar
How is C# on non-Windows platforms these days? I know Mono has been a thing for a while now, but the last time I looked into it (long ago, now), it was still lacking, performance-wise, on Linux. I dismissed C# back then because I didn't want to be tied to Windows and didn't want to give up performance, but I'd love to look into it again since I do think {C,F}# are really good languages.
It's also interesting to see that Rust scored quite high at the Go Survey: [https://blog.golang.org/survey2018-results](https://blog.golang.org/survey2018-results) \#3 - The first choice, most preferred language. \#8 - Most preferred language overall. \#13 - Most experience with language. Just wanted to share. It's interesting to see that experience is way less then preferred language however.
&gt;I'm not impressed with 32-bit wasm when it should be 64-bit from the get-go. That's WASM's fault, not Rust's. You're not the only one annoyed with it.
I don't really understand or use Piston, but that terminology seems to come from OpenGL. In OpenGL, what you want can be implemented using the [scissor test](https://www.khronos.org/opengl/wiki/Scissor_Test) to not draw pixels that fall outside an axis-aligned rectangle. (Also note that "fragment" is the OpenGL term for a point of data within a mosaic. Pixels are a special case of fragments: fragments of color data intended for display on a screen.) OpenGL "viewport" has to do with the coordinate system transforms. Imagine four security cameras on a single monitor. If you want that effect in a video game, multiple viewports make sense. Anyway, both are state-based effects. In OpenGL, you set them up and they affect drawing commands until they are restored to default. So if they're not functioning as you expect, you'll have to read the docs and source code and figure out what Piston is doing between your code and OpenGL.
IMHO, C# has a lot of niceties, but lacks the huge JVM library ecosystem.
How does Lucet compare to Wasmtime? They seem to have overlapping goals.
Compile time is certainly a problem. For example, it takes ~7 seconds on my machine, running smithy-todolist.robertbalicki.com locally, to make a change in my IDE before I see it updated in the browser. So, we definitely have a ways to go! I haven't done any performance profiling though.
Thanks for linking it, /u/ErichDonGubler! If anyone is curious, here's what I ended up building so far: - [top-level pipeline](https://github.com/notion-cli/notion/blob/master/azure-pipelines.yml) - [templates the root configures](https://github.com/notion-cli/notion/tree/master/ci) One thing I hope the Azure team eventually lands which would make this *much* smoother sailing is a fully-supported setup with Rust pre-installed, as they have for many other environments. *Especially* for Windows boxes, that would make a huge difference. For the Notion setup I was working on, it would literally cut the CI cycle time in half.
MIR is not stable at all.
C# depends on dotNET or dotGNU and it's only really popular on Windows with the former. That's a problem when Windows is pretty much dead on mobile devices and a minority platform on servers. (This situation seems really odd to me on technical merits. Windows has pretty good server-oriented features but it's a *nightmare* on the desktop. Both are overpriced, but the desktop is particularly overpriced. The current situation is certainly a result of market inertia - and that comes from anticompetitive practices in the past if not continuing to the present. Then again, perhaps server Windows is better *because* it has needed to compete.) So I'd put C# in the same bucket of platform-bound languages with Objective-C and Swift (though Swift seems to be at least trying to escape).
The typesetting of the book you buy is *much* nicer, of course. (And the money goes to charity)
I frequently run into a situation where I want remove the last n elements from a vector or deque. This is usually because I am storing time-related data and I want to evict old entries. Currently I doing it like this: ([playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=42fdf5d3a85d6276efe5a304b81a2578)) let mut numbers = vec!\[1,2,3,4,5,6,7,8,9\]; while let Some(n) = numbers.last() { if n &lt; &amp;5 { numbers.pop(); } else { // we've got them all! break; } } But it seems kind of hacky. Is there a better way? I feel like I should be able to write: numbers.pop_while(|n| n &lt; &amp;5);
If you want a `*mut T` chances are you should be taking a `&amp;mut _`.
Very normal. Welcome to Rust!
I don't think it would be that hard to write a script to convert it. 
The post kinda stops abruptly. Will there be a part 2?
&gt; any valid purely safe `#[no_core]` program cannot exhibit UB, Well, yes, but since those programs have no I/O that's not exactly meaningful until you compose them into something that does. (Also there's a hard-to-fix compiler bug that lowers `loop {}` to llvm code with UB. So this statement isn't true in practice yet.) The entire value of safe Rust comes from the composability of safety: if J Random Unsafe Coder preserves invariants, they don't have to worry about Q Random Safe Coder accidentally introducing a corner case which has UB.
Can you give me tha name of the text font ? I want to use it.Thank you
.NET Core is a thing.
The .NET Framework stuff can only reasonably be used on Windows, but .NET Core is cross platform. These days one can easily create a service in C# targeting .NET Core and host it on Linux.
It's pretty good, and getting better :) [https://en.wikipedia.org/wiki/.NET\_Core](https://en.wikipedia.org/wiki/.NET_Core)
Here's a question: what _is_ the lifetime `'a` in your `TestError` declaration?
...and is unlikely to become stable.
You haven't said a lot but what you have said is correct. Imagine you write a function which takes an argument with type `&amp;SomeStruct`. This struct contains a `Vec`. Within a loop, your function calls a method to get the length of the `Vec`. It's a simple method; it just reads that value out of the Vec, which is embedded within `SomeStruct`. So first the compiler copies the method inside your function, an *inline function call*. That doesn't require a memory model. But then it sees this repeated read operation inside a loop. It's best to avoid needless work inside a loop, so it would like to read once before the loop starts and keep the length in a CPU register. Is that optimization valid? It depends on the memory model. rustc/llvm *will* actually do this optimization today. It's able to assume that, because `SomeStruct` is borrowed with `&amp;`, it cannot change no matter what happens inside the loop. This assumption could be false. There's nothing that prevents you from putting a breakpoint inside the loop and changing the struct manually. There's nothing that prevents `SomeStruct` from existing in shared memory and being modified by a device driver, or a signal handler, or even another reference that exists at the same time. Instead, there's a convention that says `&amp;`-borrowing implies that the borrowed location won't change unexpectedly. If those things happen, it's *their* fault for breaking the program. The optimizer is allowed to make this change to the code. But, and here's the challenging part, those conventions aren't completely established. There are general principles (including "only unsafe code should have to worry about bugs caused by disagreements with the optimizer") but not a complete design.
Nice, it's like resque / sidekiq
Here's a [solution](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=60bd18bfb7f46ba9c067df23030d9f44) that does not remove the lifetime on `TestError`. Long story short, lifetime elision happens and since you don't specify which lifetime applies to &amp;self, it will infer it and it will be used for TestError. Which means that "self" is potentially still borrowed through TestError after test1 returns. The solution I propose avoids lifetime elision by making lifetimes explicit and making sure not to tie &amp;self's lifetime to TestError's. 
/r/playrust
Awesome! Your repo is without a license.
results on my linux 5.0 machine with an intel 9900k using master version 866f5c7d &amp;#x200B; std: 161k/s romio: 191k/s mio: 232k/s tokio: 241k/s python libuv: 250k/s &amp;#x200B;
Nah, just some keywords.
This seems the closest thing to ANTLR I've seen in the Rust ecosystem. I wish ANTLR had a Rust output target. I prefer its grammar and ability to handle a lot of different left recursive cases.
By the same measure, Objective-C is *technically* cross-platform and *technically* you can write C against Posix or even glibc and run on Windows. But I think the bar for "cross platform" should be a little higher, at "that's a reasonably popular thing to do; here are well-known projects that are doing it." 
Hey! I would love to know if I can extend my initial struct definition through traits.
&gt;You'd have to create wrapper ValueVec&lt;i32&gt; which owns the Vec&lt;Value&gt; Wait, I don't follow it. I still need to unpack the value in the wrapper? Also this is not circular? &amp;#x200B; In rust, Vec&lt;..&gt;, turned Vec&lt;Value&gt; so I can operate in the lang, turned Vec&lt;..&gt; again? &amp;#x200B;
When i play rust for like 1 hour then i just get disconnected 
So if anyone know how to fix it pls help me &amp;#x200B;
It won't work if test1 actually returns an Err with a reference into the Test though. If you have fn foo(p: &amp;mut (i32, i32)) -&gt; Result&lt;i32, &amp;i32&gt; { p.0 -= 1; if p.0 == 0 { return Err(&amp;p.0); } Ok(p.1) } and you want to abstract the first block into its own function and try to do fn bar(p: &amp;mut (i32, i32)) -&gt; Result&lt;(), &amp;i32&gt; { p.0 -= 1; if p.0 == 0 { return Err(&amp;p.0); } Ok(()) } fn foo(p: &amp;mut (i32, i32)) -&gt; Result&lt;i32, &amp;i32&gt; { bar(p)?; Ok(p.1) } it won't compile. Is there a way to fix this?
This isn't using an async redis client, is it?
So are you asking how to remove a certain range of indices in the vector, or a certain range within the ordering of the elements? It sounds like you're kind of conflating the two. If you just want to drop the last 5 elements in the vector, that's as simple as `vec.truncate(vec.len() - 5);` (assuming a check for `vec.len() &gt;= 5`). If you want to drop all the elements greater than a value, you can try `vec.retain(|n| n &lt;&amp;5);` (the expression should be `true` for elements you want to *keep*).
Yes, you still need to unpack Values in the end. The wrapper I was suggesting only avoids allocating a new vec
My bad. Don‚Äôt know if I was blind or just looked too quickly but I didn‚Äôt see the mention of dodrio in the readme üòÖ
You are in the wrong sub. This one is for the Rust programming language. Try r/playrust
I think your choice of projects should be guided by your interests, so it's tough to make concrete suggestions. But for your reference I can show you a series of programs I wrote while learning rust. I chose simple command line programs because I was already comfortable with that area. [Rust Exercisms](https://github.com/exercism/rust): A nice community project with exercises. Available in rust and many other languages. [hex_dump](https://github.com/focusaurus/rust-basics/blob/master/hex_dump/src/main.rs): a simple hex dump utility [tirefox](https://github.com/focusaurus/rust-basics/blob/master/tirefox/src/main.rs): scan the dictionary and generate 2-word combinations [rustblock](https://github.com/focusaurus/rust-basics/tree/master/rustblock/src) an exercise in mining fake cryptocurrency (just for learning what "mining" actually is in theory) [slugger](https://github.com/focusaurus/slugger) Rename files to sluggified versions [cody](https://github.com/focusaurus/cody) transcode a few really basic data encodings [tealeaves](https://github.com/focusaurus/tealeaves) Get details about your ssh keys and linter-style warnings about security issues. This was much closer to a full-on program.
Just to set your thinking off in a different direction perhaps, Instead of having three functions (`params`, `start` and `stop`), how about only having one `process_url` function? If that function, which has to be implemented once for every controller, needs to call a common function for comparison logic, just extract that into a separate function that `process_url` calls. And the old value could be stored inside the controller. It's certainly a different design and maybe it's not applicable, but it gets around the issue of having a generic type parameter on the trait, which is what makes it not object safe.
Don't underestimate the advantages of static linking. C# in theory is more productive that Rust as it has a GC but the assembly versioning woes.... If only they had static linking and cargo...
That's right, it uses async Actix actors managing communication with Redis via a sync Redis client (so from a client point of view, it can still handle thousands of simultaneous connections with only a single Redis connection, though this is configurable). I did get a proof of concept working with async Redis clients, but found that Actix handled things so well that it wasn't really necessary (at least for the use cases I tested it for). I'm planning to go down that route in future though. Last time I checked I was still waiting on either some actix/redis async changes before going ahead (I forget what exactly, have notes on another PC somewhere). The actual project is currently live in production at my workplace, it's been running for ~5 months now without any issues.
I was thinking Unreal would be a better fit with Rust. Plus Epic's cut is pretty good as they're trying to give steam a run for their money. Seems to me like a lot of game devs prefer Unreal over Unity.
[Here you go!](https://github.com/be5invis/Iosevka/releases/download/v2.2.0/iosevka-term-ss07-2.2.0.zip) &amp;#x200B; you can find other prebuilt variants of iosevka over at the [releases tab](https://github.com/be5invis/Iosevka/releases).
 Thanks to take the time to explain this again. Much appreciated. I can still not see the full picture but i think i understand the problem now: You have collections of your own types and need to "unwrap" them to operate on the contained primitive typed values. Two things come to my mind now: \- do you really need to convert the slice/vector into the primitive type before operating? If your target operation is scalar, you should be able to fuse Val(...)-&gt;primitive and f(primitive) In this fashion you should only need to allocate one vector of results. \- the crazy idea: (This is somehow what u/__s commented) I think the problem here is in the vector itself. A vector stores a collection of homogeneous items. Those are your Val in this case. Having a vector (or slide for the case) puts all your data in sequential memory and your Val will add extra padding between the elements to handle enum behavior. For this reason you can not just cast and iterate over the vector as it was full of integers. Now: if you don't store data in the enum, you could swap between the slice of Vals and the slice of primitives with "zero cost" Your values would then look something like: enum Val&lt;'a&gt;{ Int(&amp;'a[i32], usize) ... } which means that you wont store data in the Value enum but instead the location to a externally owned collection. What do we get from this? we allocate a much larger vector (at least for integers), we add an extra indirection and we still need to allocate a vector to hold the result. This approach would be very cool to try (some sort of zero copy intermediate representation) but I guess that the parser and will become a nightmare then. The performance benefits is still doubted. &amp;#x200B; &amp;#x200B; To be pragmatic, I would try to lazily evaluate your collections one element at the time (instead of convert everything up front) hoping that the compiler does the right thing. In the end you should only need to allocate one vector to hold results. &amp;#x200B; Sorry for the original comment, I was both interested and confused with your original text. Good luck with your interpreter thingy. 
i am using a custom build of iosevka. you can download it [here](https://github.com/be5invis/Iosevka/releases/download/v2.1.0/iosevka-term-ss07-2.1.0.zip).
Since Rust is so new and Amethyst is among the first Game Engines written in Rust, it's still in the early stages of development I'd say. On the other hand, Unity, Godot and Unreal have been around for many years and are highly suffisticated. Amethyt just isn't as ripe as other engines and doesn't have as much to offer yet.
Much thank!
I quickly wrote simple API for that https://github.com/DoumanAsh/windows-win-rs/commit/e2246361b1b5f55b7f15ba98dd007d63db262be4 If you're still having troubles
What about `repr(packed)`? Does that still reorder fields?
As C# becomes more platform-agnostic it will gain market share over Java: it's a slightly-better version of Java. Also, as I mentioned earlier, Oracle is doing what it can to scare people away from Java. Mono is a problem for C#, as is the funny "assembly" stuff which feels pretty Windows-specific. I suspect Swift is the new Javalike, but again it will depend on whether Apple can really develop a plausible cross-platform story: the language is fairly cross-platform now I guess, but most of the userbase isn't as far as I can tell. I dunno: haven't really tried Swift yet.
Kotlin is a thing to watch. My suspicion is that, like most alt JVM languages, it will kind of go along in the shadows. Honestly, if I want something for JVM I just write Java: it's good enough.
Yeah, but the no starch version is a pretty to look at offline PDF.
Another useful resource: https://github.com/rust-lang/rustlings
Yeah, something closer would be LALRPOP.
This sounds really awesome and I would love to see it when it's released (if ever)!
It depends entirely on what you're going to be doing -- if you're just reading things (i.e., doing forensics like I was), then it's fantastic! If you need to actually do more than that, it might not be appropriate for you. Either way, it's nice to see somebody excited about it! :)
Man, I really underestimated how much of a train wreck Java actually is until I started writing it professionally. We also use Scala in some placrs, but my coworkers are all really reluctant to write it because the existing tooling is very bad compared to Java's.
Shouldn't 'evaluated' be 'elevated'? I'm fairly certain I wouldn't know what to do with 'evaluated' based on name alone.
I've only used Baidu's SDK, they've posted on here and been helpful with the occasional query from me. They also push a docker image with most of the tools needed which can save you a lot of time. It covers some of the Intel SDK in a more "rust like" way, though some parts are just thin wrappers e.g: remote attestation. I hadn't seen EDP but keen to give it a look.
I definitely want my thinking to go off in different directions here, thank you for the suggestion. I'm going to try it out tomorrow among some other ideas and see if I find a good pattern. Regarding the "old value could be stored inside the controller" bit, that seems a bit inconvenient from an API standpoint if every controller has to manually remember this, unless that common functionality could be contained in a trait somehow. It's not necessarily that I can't implement this _at all_ in Rust, but more that I'm trying to come up with as nice of an API as I can. I'll never match Clojure's conciseness in Rust, but the closer the better.
According to [the nomicon](https://doc.rust-lang.org/nomicon/other-reprs.html), &gt; This repr is a modifier on `repr(C)` and `repr(rust)`. ...so I'd assume that, since no `repr` is synonymous with `repr(rust)`, then: 1. `#[repr(packed)]` would be synonymous with `#[repr(rust, packed)]`and allow reordering (and [null-pointer optimization](https://doc.rust-lang.org/nomicon/repr-rust.html)). 1. `#[repr(C, packed)]` would forbid reordering. Whether it *actually* reorders to minimize the number of fields which are unaligned in a packed `repr(rust)` struct, I don't have a definite answer on.
You might be right, I could correct it
I'm not home rn lol But thanks
This is probably too big to be in Clippy, but yes, I too think this totally should be a rustc lint plugin, not a rustc_driver consumer.
In Rust, sizeof(T) is rounded up to the nearest multiple of alignof(T). This means an array of sizeof(T) is spaced exactly sizeof(T) bytes apart, no more, no less. Eg. sizeof((u32, bool)) == 8 even though there is unused space at the end.
I do web stuff in C# and .NET Core both on Windows and Linux. There really isn't much difference between them these days. Most libraries are targeting the lowest common denominator (.NET Standard), and are fully cross-platform. The whole tooling is (optionally) CLI-driven. You can use any editor with an LSP server, but I prefer JetBrains Rider. It has an [EAP version](https://www.jetbrains.com/rider/eap/), which is free to use and has proven to be pretty stable in my experience. The profiler and test coverage currently don't work on Linux, but they are in the works.
&gt; With Lucet, Fastly‚Äôs edge cloud can execute tens of thousands of WebAssembly programs simultaneously, in the same process, without compromising security. I find this to be a very bold claim given what we know about Spectre attacks. The V8 developers at Google basically [came to the conclusion](https://arxiv.org/pdf/1902.05178.pdf) that the only real protection is process isolation.
Rust is not the most easiest language to learn in my opinion, but once you get the hang of it many other languages might be more easy to learn. I came from C/C++ myself when I started learning Rust, and I actually think that Rust has many things that improve over C/C++. The only reasons (which pop up in my head) on using C++ vs Rust, is that C++ currently still has more job perspectives since it's so old and has (for as far as I know) more libraries / more mature libraries. But with a bit of luck Rust will be able to catch up to that :) Other answers already provide great idea's!
Thanks for the reply. I haven't explore much the encoding for slices because the use of lifetimes complicate things so much, and also then I need to think in how use Rc/Box for the memory model. &gt; do you really need to convert the slice/vector into the primitive type before operating? No need need, but think could be better? Is just that if I have Value(i32) can take value "directly" but [Value(i32) , Value(i32)] not. So I'm wondering if is possible...
Thank you Steve for everything you‚Äôve done and continue to do for Rust!
&lt;3
 Think on your list comprehension but instead of doing a map to i32+collect , call the convert operation and the ultimate operator inside the map lambda. This may require of some iterator tools in the case of binary operations, where you would want to zip the map iterator of the two inputs. 
... which itself is named after the word 'future' in Japanese.
Rust lurker. In the https://github.com/rbalicki2/smithy/blob/master/crates/smd_macro/src/parsers/core.rs I see code like this: named!( match_opening_tag &lt;TokenTreeSlice, (String, Vec&lt;AttributeOrEventHandler&gt;)&gt;, delimited!( apply!(util::match_punct, Some('&lt;'), Some(Spacing::Alone), vec![]), tuple!( apply!(util::match_ident, None, false), many_0_custom!(super::attributes::match_attribute) ), apply!(util::match_punct, Some('&gt;'), None, vec![]) ) ); Can someone explain please what is it this code doing with broad strokes to someone that knows Python.
I'd mainly be interested as a learning tool because I'd love to understand wmi better (or possibly at all).
Phew don't scare me with a title like that, I though it was about leaving rust, glad that's not the case!
Having the conferences been growing size? Also, thanks for all your hard work. Building a community is n NP-complete problem. 
Generally, yeah. Slow and steady, but always at least the same size and harder. You're welcome!
Ha, sorry about that! Apparently I'm bad at non-scary titles when it comes to things like this :)
I have been using async-redis with success. It's got a fairly intuitive API. PM me if you ever want to discuss.
Cool. I have to go to the one in Oregon this year. 
Thanks for everything! Hoping to attend my first Rust conference this year.
Awesome! Hope you enjoy it, they're a lot of fun.
 const PONG: &amp;[u8] = b"+PONG\r\n"; for _ in 0..item { dst.reserve(PONG.len()); dst.put(PONG.as_ref()); } Shouldn't the reserve be (PONG.len() * item) outside the loop in order to reduce the number of times that the Buffer's size has to be changed (and eliminate potential multiple allocations)?
steve.clone() isn't supported yet
It might need to be some kind of a singleton. Or maybe a service? Steve as a Service?
thanks!
thanks!
Yup. It's using the `nom` parser combinator library. Essentially, this is a macro for creating a function that: * takes a slice of `TokenTree`s (you can think of it as an array of `TokenTree`'s). An individual token tree is something like a literal (`"someone"`), an identifier (e.g. `name`), punctuation or a group (`{ let name = "someone" }` is one token tree). * matches things off of the beginning of that vector * if it matches, it returns `Ok(( what_was_matched, rest_of_vec ))` (in some form, I don't remember exactly what the type is in this case.) So this macro invocation here is creating a function named `match_opening_tag(t: TokenTreeSlice) -&gt; NomResult&lt;(String, Vec&lt;AttributeOrEventHandler)&gt;`. If the beginning of this token stream matches all of: * an opening brace `&lt;`, followed by: * an identifier, and any number of attribute pairs (see the `match_attribute` function) * followed by a closing brace The `delimited!` macro ignores the results of the `&lt;` and `&gt;` matchers, so we essentially end up with a function that returns `(String, Vec&lt;AttributeOrEventHandler&gt;)` Hope that helps! ---- `apply!` is just a way of currying (passing additional arguments to) a more generic function with the wrong signature. So, in this case, we can pass additional params to `match_punct`, such as which character we're matching, whether the character needs to be immediately adjacent to the next character, etc. ------ There's a lot of these, and indeed it's hard to follow, and it's very "inside baseball". Cleaning this up is a priority for me, though.
I have just pushed an update to better align all the implementations. This fix is also there.
Doh! You caught me. I said something about Steve.clone() not being supported yet and the need for a working group to do so ..
Hope it surpases cpp one day. Truly and honestly, for the majority of production uses, I hope it takes the cake.
https://github.com/softdevteam/grmtools should be the most similar to lex/yacc
Next blog post: Title: I am dying Body: ... to tell you about the great things happening in the Rust community!
so is the python: [https://inventwithpython.com/cracking/](https://inventwithpython.com/cracking/) and r: [https://the-eye.eu/public/Books/humble\_books\_20180509/thebookofr.pdf](https://the-eye.eu/public/Books/humble_books_20180509/thebookofr.pdf) &amp;#x200B; they are almost all free online lol
That one, RustConf, started in 2016 as a one day weekend conference in a single hotel ballroom. In 2018, it had grown to two rooms in the Oregon Convention Center on a weekday. Now, it's spread across two days. Quite impressive growth!
Thank you. I didn't know you could transmute like that. 
How many people on average go?
Depends on which conf, of course. The smallest are like, 100ish? And I think rustconf was 500? I am not sure at all though, but I‚Äôm sure the organizers would be willing to share numbers.
I've mainly used [FLARE's open-source reverse-engineering efforts](https://github.com/fireeye/flare-wmi) to spur my own rev-eng on WMI. You might find their materials interesting, since they're available now! There's some errors in the material that I intend to report, but unless you're writing a parser yourself those particular errors won't be relevant. One thing I've been thinking of making is a good explanation of what WMI is and how WMI actually represents its data. Would that be interesting to you? :)
"pretty good"? It's fast enough to completely saturate a massive companies network structure: [https://www.techempower.com/benchmarks/#section=test&amp;runid=8ca46892-e46c-4088-9443-05722ad6f7fb&amp;hw=ph&amp;test=plaintext](https://www.techempower.com/benchmarks/#section=test&amp;runid=8ca46892-e46c-4088-9443-05722ad6f7fb&amp;hw=ph&amp;test=plaintext) it could go even faster once they upgrade their hardware. I would consider that a little better than "pretty good" :P
Wow! Next time there‚Äôs one in Cali I‚Äôll try and go. Seems very fun
RustConf is in Portland -- maybe that's close enough for you?
They do. I gave some context as to why both exist on HN. [https://news.ycombinator.com/item?id=19520728](https://news.ycombinator.com/item?id=19520728) We want to collaborate more with the Wasmtime team, but were limited by being closed source while they were trying to accomplish the same things as open source.
I gotta say, I love checking my result. Something about it is very satisfying Perhaps that it is easier to do inline than with other languages? 
Syrus gave a good summary of that here. [https://news.ycombinator.com/item?id=19515476](https://news.ycombinator.com/item?id=19515476) Wasmer aims to provide universal binaries on all platforms, which is an excellent goal and one I want to see succeed. Lucet is focusing specifically on the demands of powering Fastly's edge cloud, which is a narrower scope.
I don‚Äôt think your second link is an official copy, it looks like somebody bought it before from Humble Bundle and uploaded it somewhere, ie piracy. You can probably pirate every single book in existence, but you probably should not.
Nah, I would be relying on others for transportation as I can‚Äôt drive, so going out of state would be out of the picture. San Fran is possible though
Reminds me of that doctor on Arrested Development: https://www.youtube.com/watch?v=yawiHC0yDu8
This was a great read! 
RustCamp was the trial run for RustConf, and it was in a single room in a community center in Berkeley.
God dammit I was so scared.
I don‚Äôt see why (given time, and it‚Äôs current trajectory) it wouldn‚Äôt. I‚Äôve used C++ for years and rust does everything I used C++ over C for better. The only downside currently is momentum. 
This post reads like a parent watching their kid move out, go to college, and land their first job. Congratulations
thanks :)
Sorry!
Lol, no problem! =)
As you can [see here] the fps is fairly high while the CPU and GPU usage is fairly low. The only thing that could be better is the size of the wasm file. I think it is around 1.1Mo which is a bit big for what the game does.
Thanks!
Thanks. That's a great feeling!
Because it has a garbage collector, so people who CBA to deal with the intricacies of memory management don't have to.
I wouldn‚Äôt advise Scala, it‚Äôs too complex, has bad Java stdlib interop and implicits make it almost impossible to debug. Also, from what I hear, there hasn‚Äôt been a new version in the last three years.
AFAIK, the thing people really like about Go is the lack of features. Really similar to how people like C over C++. Feature wise, the language hasn't really changed much from Go 1.0. The mentality is "I can understand everything that is happening because nothing complex is allowed to happen". Compare that to Rust/C++ which have been pulling in new language features fairly consistently over the same time period. Different strokes and all.
Another useful resource - https://learning-rust.github.io/
With the new release cycle, definitely feels like a lot of features languages like kotlin/groovy/scala eventually end up in Java proper. As that happens, there is less of a reason to really want to use them. That isn't to say that Kotlin doesn't have a lot of nice features (and I'm currently using it for some newer code. With intellij, it is pretty good).
This is because Microsoft was actively hostile to the open source community for far too long. The C# ecosystem was basically .Net. This has changed, but it did a lot of damage IMO.
Zola does have Sass compilation built-in. What do you mean by "listing JS"? I haven't used it myself but you can maybe use https://docs.rs/crate/html-minifier/ for HTML. As for JS, I am not aware of any minifier so you would have to use node for that bit.
Which the botnet is named after the show Mirai Nikki or The Future Diaries.
I know this is strong, but I believe the JVM is the best... vm? jit? I don't know what to call it but thing that compares to other language VMs like V8. For example, the JVM has a pluggable GC system with multiple implementations (ZGC and Shenandoah close the gap between Java and Go in terms of pause time). It now has a plugable JIT and Graal is looking really impressive. And performance wise, the JVM ends up running most languages nearly as fast as something written in C/C++/Rust. I know I'm gushing, but I think people really underestimate how good the JVM is in multiple areas. Once project Loom matures, I don't see much benefit of Go over Java. Fibers are going to be a game changer for Java. &amp;#x200B; With all that said, these Javascript engines have really started to become impressive. I've been surprised to see that javascript run very close to WASM in a few of my own benchmarks (converted JS -&gt; Rust -&gt; WASM ... Surprisingly not hard for math heavy stuff. Javascript keywords are close enough that you almost just have to fill in the the method signatures to convert).
I would recommend checking what's out there about VR. You can also learn a lot from other game engines, such as Unity. Creating content for VR is not that simple as it seems at first.
That‚Äôs what she said.
Welcome to Austin!
Thanks!
&gt; I‚Äôve used C++ for years and rust does everything I used C++ over C for better. i‚Äôll agree on this when we‚Äôre talking about memory management, but not compile-time-evaluated/generic code (templates in C++). As much as I love rust, I still have a much easier time understanding compile-time-evaluated code in C++ than Rust. Maybe that‚Äôs because I have some real world experience with C++, but in Rust it often looks much more like typical calls to foreign crates (copyless) and I have a much harder time grasping what the final assembly will look like, where as in C++ it‚Äôs this funky templating type::input::output everywhere, which feels very hard to read, but winds up ironically bring easier to spot as a result than the same purpose code would be in Rust. So I can tell that the funky templating code will be compile-time calculated, and it sticks out like a sore thumb. I‚Äôm not saying that the C++ syntax for generic code is good, but it tends to be immediately recognizable. I would love to have my mind changed on that, but I can‚Äôt yet recognize when the code I write in Rust will have a compile-time benefit or instead a run-time cost.
But it probably will be possible in future. See this migration tools which under development: [https://github.com/esmbly/design](https://github.com/esmbly/design)
https://www.reddit.com/r/rust/comments/al17ys/gfxhal_ui_toolkit/
That is one of my complaints about rust actually. It‚Äôs very difficult to *spot* compile time code. I think it‚Äôs the price you pay for cleaner syntax. In practice that hasn‚Äôt changed too much for me while developing code. I could see it becoming a problem with code maintenance though. I think it‚Äôs just a different development mindset that we (c++ people) need to get used to. I‚Äôve found myself using Godbolt a lot more to validate assertions in rust, but I‚Äôve also been pleasantly surprised at how good rustc is at optimization.
Ah, fair, it was just the second result when I googled it so just assumed. Usually you have to type at least "free" to get results like that if you're going for piracy
Lol, the last December a Rust conference was in 30 minutes drive from my home and I had a free ticket, but it was too early in the morning and I wanted to sleep too much, so I didn't go. I guess Steve is suffering for our sins traveling all those places instead.
Ha! I was there, we coulda said hi :) Sleep is good too.
I think you‚Äôre on to something. Making it difficult to spot at the cost of a more consistent and familiar syntax is just a trade off, and not necessarily a bad one. Hopefully just takes getting used to, and maybe trusting the compiler more. But for now, I also use godbolt or a disassembler on Rust a lot more than any on other language I write.
You're quite misinformed.
Same, but also that‚Äôs just being new. I‚Äôm 1.5 years into rust, and 15 into C++.
amethyst will be using rendy which in turn uses gfx hal
Is there a good way to find out what Rust conferences are happening where?
Yeah sync writes to another node would be good - that‚Äôs on the roadmap right?
Nice. Is it hard to add "1 year in seconds" kind of evaluations? 
Because it is run from async code (server) and potentially will call async code while being run (e.g. fetching a low res. avatar image to embed into the mail). Making it sync would mean having to add thread pools, one-shot callbacks etc. One the other hand using it in a `sync` context just means adding a view `.wait()` calls. (Well, expect when you use smpt/send which uses tokio).
Maybe there should be a style convention about keeping it in a separate file, or making it stand out some other way?
thanks, I somehow missed that one
I typically go over the code in lexical file order. This is _not_ an efficient way to navigate around, but I find that it works nicely for me to get a good grip on what the code is actually _doing_ rather than what it _looks_ like it is doing. Mostly because I'm reading each bit out of order, so to speak. I'm writing a lot of notes along the way and don't try to figure out if I'm right immediately, but focus on getting a feel for what is going on.
Amethyst ist not a gui library though. And if I wanted to use the gui library I'd pull in audio and a lot more, all of which I don't want to use - because it's highly coupled. See: https://github.com/amethyst/amethyst/blob/master/amethyst_ui/Cargo.toml
There isn't a better show than this one. Oh my god it's so good.
Yes, alignment still matters with arrays. If you have a `[u32; 2]`, it has an alignment of four bytes, so can only be located at memory offsets of four bytes (ignoring packed structs). A `u64` on the other hand, has an alignment of eight bytes, so can only be located at a memory offset of eight bytes. As such, you can have `[u32; 2]`s located in places that `u64`s cannot.
Thank you
[The Rust calendar](https://calendar.google.com/calendar/embed?showTitle=0&amp;showPrint=0&amp;showTabs=0&amp;showCalendars=0&amp;mode=AGENDA&amp;height=400&amp;wkst=1&amp;bgcolor=%23FFFFFF&amp;src=apd9vmbc22egenmtu5l6c5jbfc%40group.calendar.google.com&amp;color=%23691426&amp;ctz=Europe%2FMadrid) (linked from [the community page](https://www.rust-lang.org/community) on the website) appears to have them. 
Agreed. That statement was a little too strong. Additionally, a memory model will directly affect the optimization of safe code which is also observable.
&gt; Why is a modern language missing generics, error handling, etc getting traction? What is it that I'm missing? A quote from Rob Pike might help you better understand the motivations of Go: &gt; The key point here is our programmers are Googlers, they‚Äôre not researchers. They‚Äôre typically, fairly young, fresh out of school, probably learned Java, maybe learned C or C++, probably learned Python. They‚Äôre not capable of understanding a brilliant language but we want to use them to build good software. So, the language that we give them has to be easy for them to understand and easy to adopt. ‚Äì Rob Pike 1
Learned a lot, thanks a lot!
You want r/playrust. This sub is about the Rust programming language, not the game.
It works the same in C. The main reason why alignment exists is because a processor is either completely unable to do misaligned memory accesses, or they will be slower than aligned accesses. With that in mind, it should be easy to see that something needs to be aligned no matter where it's stored.
ahh lol 
Ok all accept that. I didn't look too closely at the code (not that I'd understand it all). Was just wondering because unless you are sending mail to some exotic smtp server it will be writing to disk and have quite limited throughput which shouldn't overwhelm any threadpool you might have. Putting everything into a thread would make the code less crazy and more future proof right?
&gt; I‚Äôve found myself using Godbolt a lot more to validate assertions in rust Completely agree. I happened upon a pretty ominous compiler anomaly issue in Rust, myself https://gist.github.com/gmbeard/7ac7167bfea6e5ededdc8559dd65b5e7 TL;DR, if I told the `match` statement in the `encrypt` function to ‚Äúdo nothing‚Äù (`_ =&gt; {}`) on a non alphabet character, the compiler just refused to vectorise the loop. This would be easy to overlook for anyone, experienced or not.
Do this once a week! These were great! ;) 
Error handling it provides? Go provides no error handling at all, everything is manual like in C.
I'm still confused, I think. Why store the params instead of the derived value? You could add a method to the trait to compare the values‚Ä¶
Wow welcome to Austin. Can we hope to see you at the Rust Meetups sometime?
I was at the last one! I hope to attend more often than not :)
Steve, you are a fucking champion. Been a big fan since I started learning Rust in 2014. Take a breather man, you've earned it!!
Thanks!
i missed the gui part of your title! imgui is great and can tie into hal but its meant more for debug guis less in game guis. but you can make it work for that sometimes
Awesome. I missed the last one, as it is mostly about Gamedev from what i looked up last time. This changes everything for me. I am suddenly like a little kid that found ice-cream in the fridge. Have been an active Rust enthusiast since the very beginning. 
Useful tips, Saghm!
Would anyone be willing to help me check out something fairly simple? 1) clone https://github.com/xi-editor/xi-editor 2) get rust 1.31.0, `rustup toolchain install 1.31.0` 3) switch to 1.31.0 `rustup default 1.31.0` 4) rustfmt, `rustup component add rustfmt` 5) cd to xi-editor/rust 6) run `cargo fmt` 7) check `git status` to see if anything has changed Let me know what the `git status` check is? There *should* be no changes, but I am finding them on Windows 10. Curious to see if others can reproduce this too?
Can you specify further what you mean by "expand my initial struct definition"? Do you mean: "add new fields"?
Your branch is up to date. rustc 1.31.0 (abe02cefd 2018-12-04). rustfmt 1.0.0-stable (4a4e508 2018-11-29). GNU/Linux.
Once a variable is out of scope, its memory is freed
No, it provides no _exception_ handling... its error handling mechanism _is_ the non-nil`error` return value and the idioms for checking and responding to its presence... that‚Äôs very heavily manual, yes, but it‚Äôs considerably easier and more consistent than C.
I think ‚Äúgarbage collector‚Äù refers to a system of more complicated memory management. Rust does the cleaning for you, but based on the simple idea of when things go out of scope. I guess if you expand the concept of garbage collection, this could fall under it. 
I get no change on Windows 10. rustc 1.31.0 (abe02cefd 2018-12-04) rustfmt 1.0.0-stable (4a4e5081 2018-11-29)
You might wanna see [RAII](https://en.wikipedia.org/wiki/Resource_acquisition_is_initialization)
rust inserts the clean up code at compile time. with c you write the clean up code yourself. with garbage collection clean up is figured out at runtine
Indeed, welcome to the ATX! 
The one I went to was about a ray tracer, and only kinda incidentally. Anyway, see you at one in the future!
Ooo that‚Äôs a good one.
Think of it more like an older sibling who reads all your homework after you‚Äôve fallen asleep, sees where variables exit scope, and adds the deallocation calls precisely where they‚Äôre required to ensure that memory is freed the instant it‚Äôs verifiably no longer required. Like C but with a guardian angel.
`error` is just an interface in the standard library with a single method, which returns some string. Apart from that method, there's nothing different from what you can do in C. And idioms aren't provided by the language, it's all on those who use the language. It's great that at least the community agrees on how to handle errors, but that's not the same as having error handling support from the language. Rust doesn't have exceptions either, but it has algebraic data types, pattern matching, `?` to check for errors, and compiler support for enforcing checks - that's error handling. Go has practically nothing.
If your goal is to print debug stuff, you should be aware of the [recent `dbg!` macro](https://doc.rust-lang.org/std/macro.dbg.html).
Amazing I love it
When a value is not moved somewhere else (eg:through a function call),it is destroyed at the end of the scope,its destructor function is called and then that of each one of its fields,releasing any resources it owns,including memory.
I didn‚Äôt say or imply that Go has much, just that what it does have is more often than not sufficient to the tasks for which the language has been engineered to be most suited. And, unlike in C, the error handling idioms of Go are both simple and ubiquitous throughout the standard library and the wider ecosystem. That very widespread adoption and promulgation of what is by design a straightforward idiom has led to quite a lot of fully armed and operational error handling written in the language. Does the language hold your hand? No. Rust does, and it does well, but at the cost of having a much sharper learning curve and a much longer lag between starting and productivity, both of which impede its uptake in the admittedly limited areas where Go‚Äôs kungfu is sufficiently strong. Don‚Äôt get me wrong, I prefer Rust... but I admire some of the engineering-first choices in Go... its syntax is much simpler and small enough to learn and memorize very rapidly, while also being dead effective at the heavily parallel, small-scale services it was always focused on.
Btw, `unimplemented!()` isn't special, but only a `panic!("unimplemented")` shorthand. And "works regardless of type" comes from `!` aka "never" aka "`-&gt; !` functions never return" - an expression of type `!` can never be fully computed, so it can be used as any type. This is also why `match x { A =&gt; return, B =&gt; f() }` has the same type as `f()`, the type of a `return` (or `break` or `continue`, they all work here) expression is `!`, because they "go elsewhere", control-flow-wise, so for the original code, it's like the expression never finished executing. It also means silly things like `return return return x` work just fine (only the innermost return happens, the rest is just dead code).
More specifically Rust injects calls to [`Drop::drop`](https://doc.rust-lang.org/std/ops/trait.Drop.html) where appropriate, which is key for freeing memory in all but trivial cases. It is worth pointing out that underlying all the fancy high-level memory management are C-style allocation primitives `alloc::heap::allocate` and `deallocate` being comparable to `malloc`/`free` in C. The key difference is that Rust enables to build (compiler-checked) safe abstractions (such as `Box`) on top of these primitives, unlike certain other languages.
Thank you very much!
Thank you very much!
By the way, `divide_opt` can be written as follows: fn divide_opt(x: Option&lt;i32&gt;, y: Option&lt;i32&gt;) -&gt; Option&lt;i32&gt; { x?.checked_div(y?) }
If this is something you're doing frequently, you could write an extension trait and implement it for vectors and dequeues. [Playground link](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=a9d881830f1f05861085676787e670a3). You can then use it as you said you wanted to be able to. If you actually want the items, and not just discard them, it probably wouldn't be too hard to make the function return a type that implements `Iterator` and lazily pops, rather like drain does but without always consuming.
The abstraction of stack owning heap and being unable to "leak" stack allows drops at scope boundaries to kill all heap implicitly at the exact point it's no longer reachable. https://medium.com/@knappador/why-the-machine-b9803a77fa29
Basically, a garbage collector periodically walks around looking for garbage and frees it. In Rust there is a no garbage to collect because the moment something would have *become* garbage, it gets freed immediately.
Steve Klabnik once likened it to compile-time garbage collection. I do remember some pushed back on the language due to historic precedence, and the general marring of distinctions. But to say Rust has automatic compile time memory management is absolutely true, and is something that those without the intuition that this is possible to derive from lifetimes and scope, it can easily elude new comers.
You are awesome!
Is there somewhere to see this in the source code? I‚Äôd love to learn
We're using Rust since ~1.9 (about 2016-05-25) it took just a few months to go to "production" ("IoT", rpi3, gpio, etc.) It works like charm ever since, and we're hooked! Thanks!
I was clarifying that a value does not always get destroyed at the end of the scope of the variable that it was assigned to,it could happen before (ie:inside a function call taking the value) or after(ie:if you send the value through a channel). Destruction before the scope ends: \`\`\` { let x=String::from("hello"); drop(x); // the String was moved out of x and destroyed inside the function } \`\`\` Destruction after the scope ends: \`\`\` { let x=String::from("hello"); send\_to\_process(x); // the String was moved out of x } // the String may not been destroyed yet,because it was sent through an asynchronous channel. \`\`\`
In C the variables that are scope managed are called "auto" variables, because they are automatically allocated - on the stack, just like in case of Rust. Rust "simply" took that concept and went a bit further with it.
Hey, that's actually pretty neat. I thought you could only use `?` on the `Result` type. Looking back at the [relevant chapter](https://doc.rust-lang.org/book/ch09-02-recoverable-errors-with-result.html#the--operator-can-only-be-used-in-functions-that-return-result) in The Book, they really gloss over the fact that you can use `?` with the `Option` type: &gt; The `?` operator can only be used in functions that have a return type of `Result` &gt; --snip-- &gt; This error points out that we‚Äôre only allowed to use ? in a function that returns Result&lt;T, E&gt;
A more interesting variant is the [`unreachable_unchecked()`](https://doc.rust-lang.org/std/hint/fn.unreachable_unchecked.html) function. `unimplemented` and `unreachable` are both just panics, but that one actually changes the resulting code to something that only works if the spot isn't reached. Like, if you do `if values.len() &lt; 5 { unreachable_unchecked(); } return values[4];` and it will remove the bounds check entirely.
&gt; In C and C++...you have to physically release the memory, but in rust, I don't see that I might need to ask you more about the C++ you've been reading, but in my mind Rust and C++ are almost identical in this regard. Allocations are tied to some kind of object (like std::vec in C++ and Vec in Rust), and deallocation is taken care of by that object's destructor. Destructors run automatically when objects go out of scope. The key difference with Rust is that the compiler keeps you from using pointers to an object after it's destructed. But the destruction itself is the same.
Stacks have always been the automatically managed memory long before C. Even GC predates C. Heap allocated memory can outlive it's allocating stack-frame, which is why in C, handling this is called memory management. So in Rust, that portion is now automatic. Hence, automatic memory management. Even in C++ you have the dreaded "rule of 5" to contend with, just to even get some semblance of automatic management off the ground. Rust goes further in a very specific direction which had not been particularly obvious, and it required delegating ownship of resources whereby which memory would be freed, but also tracking lifetimes as heap allocated memory would have it's ownership transferred. While reallocation still happens at the end of a scope, it's not the scope that manages the memory.
Here is an example: fn main() { { let foo=Hello::new("big".to_string()); println!("act 1:"); // foo gets destroyed here by calling &lt;Hello as Drop&gt;::drop, // because foo is not moved, // then it recursivelly calls Drop on every field. } println!("\nend of act1"); println!("act 2:"); let bar=Hello::new("small".to_string()); // bar gets destroyed in drop() by implicitly calling &lt;Hello as Drop&gt;::drop , // because bar is moved into drop() and // not moved somewhere else inside(ie:through a channel), // then it recursivelly calls Drop on every field. // // drop() is defined like this:`fn drop&lt;T&gt;(_:T){}` drop(bar); println!("\nend of act2"); } struct World{ str:String, } struct Hello{ world:World, } impl Hello{ fn new(message:String)-&gt;Self{ Hello{ world:World{ str:message, } } } } impl Drop for World{ fn drop(&amp;mut self){ print!("{} world!",self.str); } } impl Drop for Hello{ fn drop(&amp;mut self){ print!("hello "); } } This is what that prints: act 1: hello big world! end of act1 act 2: hello small world! end of act2
Further evidence that both are OK: the standard library uses mem::transmute to implement the [from\_ne\_bytes](https://doc.rust-lang.org/src/core/num/mod.rs.html#3895) and [to\_ne\_bytes](https://doc.rust-lang.org/src/core/num/mod.rs.html#1991) functions on integers (both of which are safe functions). Of course, the fact that both of those are in the standard library means that you should probably just use them and avoid thinking about safety at all...
&gt; In C and C++ (I am unfamiliar with how you clean things up in those languages so forgive me, I am a OO guy not an FP guy, but FP might come in handy later on) you have to physically release the memory, (Disclaimer: This is my understanding of it, sorry if any part is wrong) You don't have to manually free memory in C/C++. In C/C++ local variables get cleaned up at the end of the scope. It's only when you allocate stuff on the heap, ie with `malloc`, that you have to `free` it when you're done. Rust works the same way. In those cases, you have to manually clean up in C, and C++ uses the [RAII](https://en.cppreference.com/w/cpp/language/raii) idiom, where you use classes to wrap resources, and they clean themselves up in the destructor. Files will `close` themselves, memory `free` itself, etc. In rust the destructor is the `Drop` trait. For even more complex cases, theres stuff like reference counted pointers, in C++ `std::shared_ptr`, in Rust `std::rc::Rc`. These wrap manual memory management(`malloc`/`free`) for you, they count how many versions of themselves exist, and `free` the memory when they're the last one. Structures like these, that wrap resources, are generally part of the standard library, so you can go a long way in Rust/C++ without manually managing memory yourself, the standard library will do it for you, and everything will be cleaned up when it goes out of scope(which generally means you arent using it) This style of memory management is generally referred to as "manual", because somewhere along the line memory is being manually dealt with at defined, static, compile-time points. Rust and (modern) C++ work more or less the same here. ---- (Disclaimer: This is simplified and this explanation may have errors, and im not entirely well versed in garbage collection) In contrast, the style generally referred to as "garbage collected" has a separate system running underneath your code, the language runtime, and this runs the garbage collector. In such a language, almost everything is like a reference counted pointer. Everything is on the heap and counting itself, and the garbage collector will interrupt your code every now and then to check through it's lists and see what isnt being used anymore and clean it up. In a system like this, memory isnt freed when it goes out of scope, it simply becomes inaccessible by you, it wont be cleaned up until the GC gets around to it, which may be awhile since it doesnt want to interrupt your code too often and hurt performance. Such systems also generally check for cycles and try to break them, a cycle is when two reference counted objects reference each other, so they both technically are being "used", but only by each other, so they should be cleaned up. This is relatively expensive to check. ----- TLDR: C++ Destructors do it.
Ah sorry, I'm carrying over the terminology from the project I'm referencing and the names are indeed confusing. I want to store the derived value the controller extracts from the URL. Wouldn't this comparison method on the trait have to be generic over things that implement PartialEq?
Fun fact: if you're writing C with the glib APIs enabled, you can actually get a bit of automatic memory management: g_autoptr(T) data = return_floating_ref (); g_autofree T *string = return_heap_data (); Both will automatically call `*_unref ()` and `*_free ()` when their scope ends. What if the pointer returned is `NULL`? `g_free ()` and friends only attempt to free / unref values which are not `NULL`. `g_clear_pointer ()` will also set pointers to `NULL` after freeing them.
https://github.com/goffrie/plex is designed to function like lex and yacc
Unfortunately I can't say how to address this with YCM, but I can say that [deoplete](https://github.com/Shougo/deoplete.nvim) with RLS and [LanguageClient-neovim](https://github.com/autozimu/LanguageClient-neovim) definitely completes from external crates.
Although this does work in C: g_autofree gchar *string = heap_allocated_string (); On drop, `g_free` is called on `string`. They also provide an "autoptr" for more sophisticated types. g_autoptr(GVariant) p0 = g_variant_get_child_value (params, 0); Which can be used on types that define what functions to execute on drop. In this case, `g_variant_unref (GVariant *self)`.
This website is currently helping me out so much. Thanks for the tip.
&gt; you have to manually clean up in C Actually, see [g_auto, g_autoptr, g_autofree](https://developer.gnome.org/glib/stable/glib-Miscellaneous-Macros.html#g-autoptr).
I'm not sure about a minifier, but https://github.com/swc-project/swc is a lightning-fast javascript transpiler written 100% in rust. It might do part of what you want. 
You are looking for /r/playrust. This is the subreddit for the Rust programming language.
Note that is specific to glib (the gnome library, not to be confused with glibc, the gnu implementation of the standard C library). It wields the non-standard \_\_cleanup\_\_ attribute, which is why only gcc and clang are supported.
And other interesting one they added recently(in nightly) is [todo!](https://github.com/rust-lang/rust/issues/59277) .
That‚Äôs an interesting idea, and I applaud you for speaking up and sharing it when others like myself don‚Äôt have any idea, but I‚Äôm just more sure that‚Äôs the right vein of thought... Let‚Äôs say I‚Äôm implementing a generic array class for a custom type that needs to be on the heap sometimes and doesn‚Äôt others. I don‚Äôt want to have each call to compile-time code in another file, as there will probably be about a half dozen or so. It ought to be in-line, though still distinguishable, imo, so I don‚Äôt have to require too many files or have a file per one liner of compile time code, especially when such code is so often driven by context and compiler type constraints in the first place such that splitting into a new file can easily sacrificed the assurances altogether. As for how that ideal solution would look in my dream Rust.... I‚Äôm really not sure how to say today. But, I don‚Äôt think splitting into other files is the key, so much as a more obvious syntax for strict constraints would be. Or, if not a syntax, a selection of more obvious functions (to avoid the hideous C++ template syntax the world hates today in exchange for canonical functions in Rust used only for compile-time optimization code)
As long as the type of the old value depends on the controller's type, I don't see a better option than storing that value inside the controller. The other option is to go fully dynamic, using things like `Box&lt;Any&gt;` and downcasting, which means runtime type checking, but it seemed like that was what you wanted to get away from in the first place. I don't know Clojure at all, how does it verify that `paramReturn` in `start` and `params` are of the same type, runtime or compile time?
This is a very unusual usage of the acronym "aka".
Thank you!
I found [Ale](https://github.com/w0rp/ale) to be the best language plugin. 
 I‚Äôm really amazed that no one has posted this yet. https://words.steveklabnik.com/borrow-checking-escape-analysis-and-the-generational-hypothesis Rust has a static garbage collector. 
No need to post a niche exception to the rule 3 times.
&gt; In Rust there is a no garbage to collect because the moment something would have become garbage, it gets freed immediately. One exception: It's possible to create reference cycles using something like `Arc&lt;Mutex&lt;...&gt;&gt;`. Unbroken cycles turn into leaks in C++ and Rust, while most garbage collectors in the normal sense of the term actually detect garbage reference cycles and clean them up.
Hmm...that's certainly an interesting one. I'd think that would be among the last things to optimize, though, given its obvious unsafety.
I found that without `extern crate x` lines I couldn't get completion working with external crates. Don't recall if it was that plugin specifically though.
In a language like C/C++, heap-allocated objects are created with `malloc()` (or in C++ with the `new` operator), and when you no longer need them, they are deleted with `free()` or the `delete` operator. The problem is, if you aren't careful, you might lose track of an object permanently without free-ing it. This is called a memory leak, and if it goes on for long enough you will fill all of your RAM with unusable objects and crash your system. But, if you delete an object before you are done using it, it might get overwritten with other (potentially secret) information, causing at best a crash and at worst a security hole. Long story short, manual memory management is complicated. An example C program is provided: ```C int main(){ int * i = malloc(1000 * sizeof(int)); doSomething(i); free(i); //delete this line to create a memory leak. } ``` In Java, objects automatically track every reference to them, and then are deleted when there are no references left. This means that you don't have to delete objects yourself, because they automatically delete themselves. For instance, ```Java public static void main(String[] args){ int[] i = new int[1000]; doSomething(i); //you don't need to free() anything, since Java will notice at runtime that the above 'i' variable no longer exists and will delete the array for you. } ``` This means that you never get memory leaks or use-after-free bugs, but the garbage collector can be somewhat unpredictable time-wise, and it's complex. Rust uses an approach very loosely similar to that used by Java, but at compile time, so the final app is still as simple (and predictable) as if it were written in C/++, but without any chance of `free()`-related glitches: ```Rust fn main(){ Box&lt;[i32, 1000]&gt; i = Box::new(); //in Rust, a Box is simply a pointer that will delete whatever it points to when it goes out of scope, I.E. at the end of the function. doSomething(i.as_ref()); //you still don't need to free() anything, since the rust compiler notices that b is going out of scope, and will delete the array inside of it for you. } ``` Compile-time garbage collection isn't quite as powerful as runtime garbage collection, however, so whenever the rust compiler doesn't know what to do it produces a compile error.
I don't think you can call it niche when a lot of C software on Linux is using it.
A little click bait never hurt nobody. 
tldr; Steve has discovered his love for Guile Scheme 3 and the GNU philosophy, and will be evangelizing his love for expressiveness 
Odd... Most of this stuff was in the first version of rust book, IIRC. Has it not been added to the second one?
Honestly, you'd be silly to avoid using glib in C projects, though. It does provide better cross-platform support, with better primitives and functions. The simplicity of automatic memory management with glib is one such example. Less error-prone than manually managing memory in C without it.
Guilty! I like "aka chains" so much I forgot the third thing isn't aka-able.
Except, not at all! miri is already symbolic in its handling of allocations, and we plan to add more symbolic features for MIR optimizations (e.g. constant folding/propagation) and unification of constant expressions (for const generics). Facebook may have had its reason, but IMO it was a mistake not to talk to /u/oli_obk and /u/RalfJung, at the very least.
I still prefer my formulation, which symmetrically uses "trait generics" for the category objects and arrows: https://gist.github.com/eddyb/5fcbc9ee442b6ce9c63bbf1ba54e70d6#gistcomment-2690193 The bulk of the gist above only requires GATs, I think, but is much messier.
Yes, that is effect we can observe, but OP asks what causes it.
Linting* sorry.. autocorrect üò©
You could return the reference to self instead of a unit struct or you could redefine the Error to not have a reference.
I picked up learning rust after I watched your video on the history of rust, around a year back. Been an amazing learning experience since then. Continuing doing the great work Steve.
Is the project needed _just_ to learn rust or is it meant to be about learning rust while building something useful? If it's the former, then you can check out "advent of code" exercises or codewars. For the latter, you can try to build command-line utilities, web-assembly libs/apps for running on nodejs or in the browser, or server-side web-apps. Machine Learning in rust isn't yet at the point where you can do something related to it as a learning project, AFAIK.
Rust does have garbage collector. Think like this- you're a neat guy. You enter the kitchen. And every time you leave it, you leave it perfectly clean. You enter the bathroom and you leave it perfectly clean. Then there is javascript and other "garbage collected languages"- they enter the kitchen, leave mess. Go enters the bathroom and leaves mess. But every morning, they'll go in once and clean the place like there was nobody there. Both approaches have their ups and downs. To go a bit further- the rule in your house is simple, whoever drops something in the kitchen cleans it. Now you peel onions and your wife cut them. Who should clean up the onion peels? Go/JS don't care, tomorrow morning one designated person will clean it up. Rust does. Hence programming it is harder. All that borrow stuff you see? That's Rust deciding who should clean the onion peels. Rust also has trouble with cyclic references. But the good thing about Rust is everything is (almost) always (almost) perfectly clean. Also, no morning kitchen downtime. (This is technically called GC pause) 
Isn't Google that company with the very difficult job application interviews about complex algorithms and obscure data structures that one is supposed to know by heart? I find it surprising that Go would be aimed at them, surely the employees at Google, of all places, can be taught generics? :-p Especially when they already know Java.
try the coc.nvim plugin with the coc-rls extension. 
Sorry i didn‚Äôt read thoroughly, but it seems compilation energy price id not taken into account, right ?
Interesting point. But I don't think compilation should be considered. As potentially you could run a program a decade after compiling it once. Runtime comparison is far more relevant.
as far as i can say, everything except some println! formatters are in the second edition
Then why is it every time I use Mozilla quantum or even npm, all my CPUs fire up and my laptop's fan turns on?
You might be right for certain scenarios. But considering a big team of several developers, contributing to a same codebase (so compiling several times per day per developers) for a program used on a single server, execution ¬´ energy cost ¬ª might be less relevant than compilation time.
Previous discussion of the research: https://www.reddit.com/r/rust/comments/704c27/rust_is_one_of_the_most_energy_efficient_languages/
Depending on the complexity of the grammar you want to parse, you may want to see if it's LL(1) first. If it's small enough and you are able to construct the correct, non-overlapping FIRST and FOLLOW sets, then it may be LL(1), which means you'll be able to write a simple recursive-descent parser _yourself_, without the need of complex libraries. The parser keeps track of the matching parentheses, because they're part of the grammar. The symbol table keeps track of the scope since it's part of the _semantics_ of your language, which is context-dependent and cannot be handled by parsers for context-free grammars, like LL(k) or LALR(k).
I think your point is quite irrelevant because all language types have their development process, regardless of whether they are compiled or not. If you want to compare development process of all language types, you would have to define a baseline that is fair to compiled, virtual machine and interpreted languages alike, and then create suitable benchmark for it. I think it would be quite impossible to pull off in practice.
Because they're talking about the language's potential for efficiency. In the case of Quantum, it's probably because you're loading, rendering, and executing third-party JavaScript and no language can force a programmer with too-tight deadlines or insufficient expertise to write efficient code. As for NPM, I'm not sure about how it relates.
great read, thanks for that!
Better cross-platform support? It can't compile with MSVC and isn't standard at all. I don't have the library installed on any of my machines. 
If your fans turn on when running nothing but a package manager you should probably clean out your laptop.
Babbage assembly language. Case closed.
It was all in the second edition, except the padding formatter IIRC. Less people read the book than you might think.
Thanks, I'm too out of touch with complexity theory to be able to do anything new... :|
Not necessarily. NPM *is* a hog as far as package managers go. It's not unimaginable that feeding it some combination of options could get it burning so much CPU for so long that such a situation is entirely its fault.
Well OP may be conflating C and C++ in this regard - there certainly aren't any destructors in C. And if he programs C++ in a C way, he likely wouldn't use RAII, which may be contributing to a more obvious difference.
Reminds me of the Simpsons https://m.youtube.com/watch?v=OzGT3rPniew
All error handling is "manual", that's why it's called error *handling*. But it requires considerably more work in C because C doesn't support the multiple return type paradigm supported by Go - it usually involves checking the value of an error code instead. Matching on an Option type in Rust is also manual, it's just arguably more powerful and flexible than both C and Go, but none of the languages support exceptions (thankfully).
I don't think so, since the derived value is hidden inside the struct? ([playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=66512d36e1f9d08c759f03fb58c96fe4)) I dunno. I pretty clearly don't understand the target domain well enough to offer much.
Don't know why you got downvoted, ALE has "tab complete" which is somewhat limited in options but works pretty much the same as ycm/deoplete when you use RLS with it. 
I feel like you haven't even read my comment. Let's repeat: idioms aren't provided by the language. Syntax is a part of the language, semantics, type system. What does Go have there for error handling? Nothing. What you are wordily saying is that the standard library is consistent with error handling unlike C and other developers are following the conventions. That still doesn't help me unwind the stack, or not ignore errors accidentally, or get some useful information from errors, etc.
Well, you can probably feed just about any package manager a set of options that would blow op somehow. But just running npm for everyday use really shouldn't get there. Unless you have some ancient device, or it's dusted to hell.
&gt; I'm dying! &gt; To tell you about new perf optimizations!
Matching in Rust is manual, but if I don't need to match, I unwind the stack with a simple `?`, and it's all written in one line. In Go for every useful line I write three boilerplate lines. Unless I forget to write them, in which case the error just gets lost, and the compiler is totally OK with this. In C I can at least use macros to reduce the clutter, but not in Go. Multiple return types are a poor mechanism for error handling as well, because when I get an error, in general I'm not supposed to use the successful value. It should be an algebraic data type. But in Go all you have is two separate values, so it's a sum type instead.
Did you actually measure in Mb, or is it just a typo? I would assume that measuring in MB would be more appropriate.
One of Guido‚Äôs (creator of Python) key insights is that code is read much more often than it is written. So I'd assume that the situation where code is rewritten often enough to have any significant compile time compared to overall runtime is gonna be rare.
I think the ·∏øain bottle neck for romio is that romio's reactor always run in a background thread. As far as i can tell this is not the case for tokio. romio::reactor is currently private so its not possible to change this currently. I ended up trying it anyway (editing romio locally) and came up with [this](https://gist.github.com/pluth/674c1dca6ca29b07948158cce206981a). It requires a public reactor, a public LocalPool::poll\_pool method in futures and some unsafe functions to turn the reactor handle into a Waker. It runs as fast as tokio for me, however its not really fair for the benchmark since it isn't actually available functionality. &amp;#x200B; (I was also marginally faster without the mpsc for me)
It's more that, in my experience, NPM does everything more slowly than its counterparts in other niches and, unlike its counterparts, it makes it worse by also not providing a command-line option syntax amenable to batching up operations to amortize the comparatively huge amount of time NPM takes to start.
I read, and understood, so drop the patronizing... rustc, cargo, etc aren‚Äôt part of the language either, so if we ignore idioms we can also ignore the work of the compiler and other tooling. The rust _language_ then certainly adds some very useful types that have more methods than Go‚Äôs `error`, as well as some syntactic sugar for working with them, but little else. If you use the language in a less than idiomatic fashion (for instance by simply unwrapping everything), then they‚Äôre mooted, but use the language idiomatically and I‚Äôll completely grant they‚Äôre more expressive and helpful than the very minimal provisions of Go. The rest of the Rust machinery around error handling is certainly greater in quantity than Go‚Äôs, but ultimately there‚Äôs a type that represents an error state and some behavior associated with it, all of which must be used in an idiomatic fashion in order to be effective. My only points are that Go‚Äôs very minimal choices in this area are more than sufficient for a _great_ many tasks, and that they‚Äôre also greater than the ones present in C, thanks to a community that‚Äôs embraced the fact that Go‚Äôs minimalism isn‚Äôt enough for you; that‚Äôs fine. C‚Äôs even more spartan approach isn‚Äôt enough for me. But those preferences don‚Äôt make the languages any less capable at the tasks for which they have proven themselves capable. I just appreciate the fact that you can strip away all the nice affordances of Rust and still do meaningful work quickly.
It's completely possible to parse programming languages in nom, see for example the [Tagua VM PHP parser](https://github.com/tagua-vm/parser). Some parts of nom make language parsers a bit harder, especially the error management, but there are very good tools to help you, like [nom-locate](https://github.com/fflorent/nom_locate), which gives precise position information on what you parsed, and [nom-trace](https://github.com/rust-bakery/nom-trace), which logs positions in the input and parsing tree for debugging purposes. Parsing languages with parser combinators is a bit different from the usual techniques: you don't need a separate lexer phase, you can parse everything in one pass, and ambiguities are not resolved at compile time, so you have to be careful with the order of sub parsers in branching combinators (like \`alt\` in nom). in parser combinators, parentheses matching is typically done through recursion, cf [the JSON example](https://github.com/Geal/nom/blob/master/tests/json.rs). This might seem a bit tongue in cheek, but I've seen it's easier for people to get started with parser combinators if they're unencumbered by previous compiler course knowledge :p
I see what you're getting at with your example, it's similar to what diwic mentioned in the comments as well. To be honest I'm probably trying too hard to match the API to what's in clojurescript. I don't think it's you not understanding the target domain as I'm sure you and anyone can grasp it easily. I probably haven't explained it well enough, and haven't expressed the overall goal and motivation for what I'm doing. I'm going to try some alternate approaches to what I'm doing and post back here if I find a good pattern for this.
I was super scared for the moment. Haven‚Äôt done much rust lately but this gentleman was a major source of help, documentation and advice when I was learning. 
I think it would have been better to talk to Oli and Ralf, for coordination purposes and maybe to get some of their hard learned lessons early, however it's not too late either! MIRAI is still young, so now that it's announced there's ample opportunity for communication.
&gt; I don't know Clojure at all, how does it verify that paramReturn in start and params are of the same type, runtime or compile time? It just doesn't. You could use clojure's spec framework to verify it matches a certain shape at runtime but otherwise most data is just lists and maps. It's extremely flexible and fast to code in, but hard to return to code you wrote a few months ago, or know the shape of the current data structure you're working with. This allows for some extremely clever and terse APIs, but yeah that's not very compatible with how Rust does things. I'll keep trying though!
The key is that rust, *at compile time*, can tell when memory is no longer used. It is able to do this because of the borrowing rules (and specifically something called lifetimes which might be what you want to look up). So the compiler first figures out when the last time the memory is used is, then inserts a call to ‚Äúdrop‚Äù for that variable when it goes out of scope (that‚Äôs normally just deallocates the memory but sometimes you want special behavior, especially with unsafe code, so you can override it). The difference between that and a garbage collected language is that in GC langs, they cannot tell when memory is no longer used at compile time, so they essentially have to run another program at the same time that checks and manages the memory. 
Aren't memory orderings somewhat separate from iteractions between e.g. borrowing and unsafe code? Because I thought we said we use C11's memory ordering model, which puts safe code back into "pretty well-defined" territory.
Cool project, cudos! I had no idea that you could find that kind of script-data online. Anyone have tips for other cool data sources out there somewhere?
"it could" should be taken with a grain of salt, it's tricky to look for files if you don't know you should be looking. I suppose `mod foo;` could fall back to `foo/*.rs`, if it doesn't find `foo.rs` nor `foo/mod.rs`. But some people wanted to get rid of `mod foo;` completely, which IMO is a nightmare waiting to materialize into the implementation, and into the UX, under various conditions on various platforms.
To be honest, I've had similar issues with C++ too. Optimizers are just super finicky, a single step out of the golden path and it's over... This is why I am looking forward to high-level APIs like faster where you *explicitly* expresses your intent to get vector code, rather than cross your fingers.
Thank you! I really struck gold with that organised data set.
If you are talking development cost, though, you also need to account for the energy used by the IDE, the browser used to look things up, the coffee machine used late at night to "find that last bug", etc... and suddenly development methodologies will start affecting things, as well. Oh, and let's not forget the cost of running tests and the whole CI pipeline... it could be argued that statically typed languages require less tests, but some will oppose the need to test types in dynamic languages (only capabilities matter!), etc... Honestly, I have a hard time thinking that a realistic measurement can be made :/
&gt; But some people wanted to get rid of mod foo; completely, which IMO is a nightmare waiting to materialize into the implementation, and into the UX, under various conditions on various platforms. Yeah, I agree, particularly with respect to UX. I'm very glad it was kept.
Naturally, JetBrains is using it to push InteliJ and Clion licenses.
I just mentioned that it affects the observable behavior of the program: a.store(5, std::memory_order_relaxed); b.store(7, std::memory_order_relaxed); Allows seeing the write to `b` without seeing the write to `a`, for example. It's orthogonal to borrowing/unsafe (unless used in unsafe code), indeed, but I would argue it still is part of the memory model. (And I agree that piggy-backing on the C memory model was a good idea; much easier to map to LLVM too!)
Hardware already has to turn atomic memory accesses into message-passing between cores, to implement cache-coherency (look up ccNUMA, it's... quite something). `miri` can do the same, and track any locations ever accessed by more than one thread as such, and with a bit of control-flow analysis, "lock-gating" of further unsynchronized accesses (to data conceptually protected by the lock) can be found. You can try to infer data patterns by analyzing the code that processes the atomically read values: if it loops back around, that indicates a locked state. Or you can separate accesses into atomic and non-atomic, and assume that *some* lock must be locked and unlocked in between non-atomic accesses for them to be correct (and go looking for such a lock). dead-locking risks can be found by analyzing the locking nesting: if not all threads obey the same nesting of lock-unlock ranges for all locks, that means aligning the threads differently in time could have led to a dead-lock. In fact, miri could do rr-like tracking of those inter-thread accesses, and fuzz thread interleaving for maximum contention and dead-lock risks. A lot of what miri does is deterministic, so maybe miri could optimize that fuzzing by caching thread-independent chunks of the execution, or even symbolically track some dependencies of the execution on inter-thread accesses, to be able to tell if the outcome (e.g. parallel computation of a value) can differ based on thread interleaving (which is arguably harder in work-stealing situations), or at least if control-flow could have been stuck in an infinite loop, for at least one interleaving (dead-locks).
At first you equated idioms and language elements, now you are also equating tools with the rest. No, let's keep everything separate because tools can be changed, but the language stays the same. Is Gradle a part of Java or not, for example? I doubt anyone says it is. But I still achieve the same level of convenience as with cargo. But no matter how badly I want algebraic types, I won't have them in Java because JLS has no mention of them. So let's keep separate things separate and not mix language, conventions, and tooling. An idiom is a certain way to write code that is not a built-in feature of the language. If you stop using `Result` in Rust, you've stopped using a feature. Obviously, that means you lose some of the language-provided functionality. But what do I lose if I start writing in Go using my custom errors and not the ones from the standard library? I will have the same level of convenience (with the exception of having to convert standard errors to mine). Because I didn't stop some error handling-specific feature of the language, I just switched to a different idiom. Yeah, you can take away features and work without them, but that doesn't make the lanugage better. There is an enormous amount of important software written in C even though the language has no OOP, no error handling, no safety features, no functional programming support, and so on. The same with Go, you can definitely write software without error handling support from the language. But Go still doesn't have it, which was my original point. 
Unwinding the stack isn't error handling, you're just letting the program crash. All the ? operator does is kick the can down the road to the caller, so you're not "handling" the error unless either a) the calling procedure matches against the returned value anyway, or b) you use the ? operator all the way up into the main function - in which case you're not "handling" the error either, you're just printing the result of the program. I'm not saying either way is better or worse, but you can't say Rust is less boilerplate-y if you use the ? operator and never match against it *somewhere* - it's not a fair comparison.
Why never handle? I do handle in one place, but in many others I just propagate to the caller. And convenient stack unwinding until I reach the place where the error can be handled is very important.
Keep in mind that a hand written recursive descent+pratt parser is also a valid option.
yeah, the rough roadmap is: 1. transaction support 2. stabilize the file format for forward compatibility 3. replication
I collected a links to tutorials, docs, code snippets, talks etc. Maybe this could be useful for you as a reference or guide not to get lost: &amp;#x200B; [https://github.com/gruberb/learning-rust](https://github.com/gruberb/learning-rust)
That seems like a good use of `#[cfg(miri)]` indeed.
What I mean is that we can talk about as a separate component, and declare we follow the C(++)11 equivalent, for safe code to be defined before we figure out unsafe code.
Speaking of padding, Rust can even do padding with a dynamic width (I only figured this out recently): ``` println!("{:&lt;0pad$}", 8, pad=10) ``` But don't ask me what the grammar for this stuff is, the `$` in the end still confuses me.^^
So is that possible to create game using dogot with Rust? Does it support VR? Thanks 
Apart from Windows, there's also some non-glibc Linux distributions. If you want e.g. small docker containers, using Alpine Linux is a great choice, as it's much smaller.
Awesome, man!
An emotionally compelling tale that would better serve its purpose on r/playrust, the subreddit for Rust the game. This sub is for Rust the programming language, which does not feature nakeds or bags of tea.
I just made the same mistake you guys should change your name, rust the game is much more popular
No, it was meant for the programming language: use std::io::{self, Write}; &amp;#x200B; print!(" &gt;You've just spawned in the game "); print!(" &gt;Mining a stone node "); print!(" &gt;Another naked appears from behind the bushes "); print!(" &gt;You both spot each other "); print!(" &gt;Pause "); print!(: &gt;They mimic you "); print!(" &gt;You back away from each other "); print!(" &gt;Part ways "); &amp;#x200B; io::stdout().flush.unwrap(); &amp;#x200B; &amp;#x200B; &amp;#x200B;
We seem to be at cross purposes, and your persistence in repeatedly explaining things _I am already familiar with_ seems to indicate you‚Äôve either missed or ignored my response to your original point. Go _does_ have error handling support, it‚Äôs just _extremely_ minimal... the C language does not have a first class, canonical notion of an error _at all_, the Go language has one in `error`, and additionally has some runtime support for deferring, panicking, and recovering in response to error states. Your original point read literally is simply false given my definition of error handling. Now if you don‚Äôt consider error handling to be a mechanism for flagging and adjusting control flow to an encountered error state, then that‚Äôs fine, but it also seems to be an exceptional definition. And IMHO it‚Äôs also exceptional if you arbitrarily set the threshold for what constitutes error handling at some feature set greater than what Go has provided. The writers of Go believe that a minimalistic approach to error handling results in less convoluted code. Arguably they‚Äôve had considerable success with that point of view, though, again, primarily in the relatively narrow domain the language ‚Äî and it‚Äôs dead minimal and highly manual error handling ‚Äî were originally intended for. The fact of that success tends to point towards the idea that the features of a hypothetically _better_ language need not _necessarily_ include a more complex approach to error handling. Now I would happily agree that I‚Äôd find Go more generally useful if it did have _more_ support for error handling, I just can‚Äôt agree with the notion that it has _no_ support for it.
In the future it'll be usable on all types. `?` is an operator like any other, called [`Try`](https://doc.rust-lang.org/std/ops/trait.Try.html). For more info: https://github.com/rust-lang/rust/issues/42327
The line graph for memory usage vs language belongs on /r/dataisugly.
i'm using coc.nvim, it works great.
If that program is doing enough work to justify all these man-hours, then the cost of the workload on that single server should still be much higher than the cost of developing it.
Also I'm pretty sure npm on the client doesn't even use Rust. 
i think you need r/playrust..
The article dates from May 2018, and I believe to have seen the study in 2017 first, so the results may be taken with a grain of salt, as many of the benchmarks have been optimized since then, also the compiler and standards library should have improved in the meantime (for example, current versions give aliasing information to LLVM, which older versions didn't due to a LLVM bug that's since been fixed). It would be interesting to see if one can replicate the results with a current software stack.
Is that for [the same reason Haskell code doesn't have side effects]?(https://xkcd.com/1312/)
There are two reasons to worry about energy consumption - money and limited supply. Concerning money, I'd argue that during the development phase the cost of the developers' time should usually dwarf the cost of electricity used for compilation (unless the compiler is written in Java) Concerning limited supply - when compiling the supply is usually not limited. Developers are working on their desktops or docked laptops, and even if they unplug their laptops to work on a couch or something they usually have a power plug nearby. And of course - if they use a build server this really is a no-problem. But when running the code? It could be running on a phone, or an embedded system powered by AA batteries, or even a tablet or laptop used by a field worker that doesn't have regular access to electricity. So the problem of limited supply is much more relevant here.
`tealeaves` doesn't seem to recognize `known_hosts`.
Writing a discord bot is always an option. 
When you said "Simpsons" I was actually thinking of this: https://www.youtube.com/watch?v=mDEtCeeSp2k 
Glad it helped üòä
I don't know what you already know, I'm just judging from what you said. Of course, Go has some very minimal error handling support. I didn't expect my words to be taken completely literally. I meant exactly what you said as well: it's extremely minimal. As to how successful this approach is, remains to be seen. Go has gained relative popularity quickly, but I'd say it's despite what the language had to offer in terms of error handling. How robust the software written it is compared to other languages also remains to be seen, I don't know if there's any research on that.
I'm a regular humanbeing please.
Are you .... the ... geal of nom himself...?!? Seriously, thank you for coming here to answer and I can completely agree with the last part. I mean I've written a full fledged compiler with common subexpression elimination and other fancy stuff and now it's been two days and i don't know how to use this `nom` thing!
The difference between Objective-C and C# is that Windows, Mac and Linux are all tier-1 platforms for .NET Core - the same runtime supports works on all of them, there are [binary releases](https://github.com/dotnet/core/blob/master/release-notes/2.2/2.2.3/2.2.3-download.md) for all the platforms and features are designed with cross-platform support in mind. Tooling-wise VS Code and the C# language server work on all of the platforms and JetBrains has a proper [cross-platform IDE](https://www.jetbrains.com/rider/). Compare this to Objective-C: As far as I know, you can't just download an Objective-C compiler for Windows or Linux, at least not an officially supported and/or up-to-date one. Objective-C doesn't really have a standard library, so it can't be used outside of macOS/iOS. The only IDEs for language only run on macOS (XCode and AppCode). No one really writes Objective-C for non-Apple platforms. As for specific examples: [Unity](https://unity.com/) is the most popular game engine on the market by the number of users and released games, supports literally dozens of platforms and uses C# extensively. It uses Mono instead of .NET Core because of historical reasons, though.
&gt; I don't have the library installed on any of my machines. That's highly unlikely, as it's used by a lot of open source software. If you need to guarantee that your C software will compile on all platforms with the same familiar APIs, glib is the only way to go. GIMP, for example. &gt; Better cross-platform support? It provides a cross-platform standard library that's useful as a drop-in replacement for platform-specific libc implementations, thereby giving you the same types and functions on all platforms, and guaranteeing that the types and functions are implemented with the same behaviors on all platforms. Many functions available on POSIX systems aren't available with the libc support on Windows, for example, but with glib, you can use the same POSIX APIs on BSD, Linux, Mac OS, and Windows. glib will implement the missing functionality, and provide a lot of useful utility functions beyond what a libc implementation would provide.
I'm talking about [GLib](https://developer.gnome.org/glib/2.60/), not glibc.
Holy clickbait
üôÑ
Sorry if this is stuff you already know but I thought I'd share my experiences. I've used a lex/yacc style tool, ANTLR, and eventually a number of parsing combinator libraries. Of these, the parser combinator libraries were much nicer to use because I was just writing regular code. That made it much easier to code and debug because I could use the tools I was already used to instead of stepping though nasty generated code or having to use other, usually poor quality tools. &gt; I am specifically looking to parse a programming language. Is this something nom can handle? Yes, this is something any parser combinator library should be able to handle. &gt; Who keeps track of parentheses matching and scope? It kind of depends on what you're trying to do. Usually, I try to eliminate those kinds of special case syntax as quickly as possible. For example, if I was writing a math parser, I'd turn `2 * (3 + 4)` into `Multiply(Num(2), Add(Num(3), Num(4)))` in the parse step unless there was a really good reason to retain the parenthesis. 
You have been judging incorrectly, and rather ungenerously. At the start of this thread I said that the error handling Go provides is sufficient to the needs of its primary problem domain, and I stand beside that. You responded with it ‚Äúprovides _no_ error handling _at all_‚Äù (emphasis mine) ... that certainly seems pretty unequivocal, but I admit I might have been more generous and assumed that by ‚Äúno‚Äù you meant ‚Äúnot enough‚Äù. As for popularity; for me personally what decided Go vs Rust was very emphatically _not_ the minimalist error handling, it was the then-and-still-current state of Rust‚Äôs tooling. Go promises to get me productive in the short run, and it delivers on that in a time frame in which Rust is still _painful_, but Rust (via cargo) delivers greater productivity over the long haul. Advantage Rust, if you‚Äôve got the time to make it past the learning curve.
Wondering how they categorize "imperative", "object-oriented" and "functional" programming languages. These concepts are not nearly as mutually exclusive as bad teachers want to make us believe.
But then again, does it really matter compared to the cost of programmers?
That doesn't make his point irrelevant though. If the development cost of a few select languages (compiled or not) is substantially higher than for others that could be an interesting discovery.
Thank you- this was an informed opinion and I guess I agree with the part about dealing with generated code not being so good. I guess I'll go the parser-combinator way.
That is almost certainly the case, and it should be for any practical measure, since the energy cost of compilation is amortized to effectively nothing assuming the application runs long enough. 
I can't claim Rust is a garbage-collected language either just because my code wraps everything in `Arc`. I think this comment is irrelevant for that reason: We are discussing the language itself (not libraries that exist for memory management), because this pretty much defines how your interaction with the majority of the ecosystem will look like. Regardless of whether your app already depends on glib.
When you put it like that, it really wouldn't be irrelevant. The problem of making fair comparison still remains however.
I just wanted to add that from my experience shifting in and out bits from a `u32` is quite a bit slower than indexing into a `[u8; 4]` unless you compile with the `lto` flag. With `lto` they seem to be pretty much equal.
I forget to join threads all the time. Maybe one could write a newtype around Joinhandle that reintroduces the join-on-drop behavior. Does this exist somewhere already? It wouldn't be *real* scoped threads because drop is not guaranteed to be called, so it wouldn't allow anything a regular thread doesn't. But it would be a nice convenience.
I‚Äôm not sure!
I'm really surprised how well Pascal did for memory and speed, and the fact that both Pascal and Go use less memory than C and Rust!
Would think assembly. If written well.
I agree!
It can also be handy as a refresher. Even reading the book, you may not remember the smaller details like these.
&gt; We are discussing the language itself (not libraries that exist for memory management) It's less a library, and more a compiler attribute that's supported by GCC and Clang. A destructor is defined for each supported type, and then any variable which is tagged with the cleanup attribute will have that destructor inserted where the variable is dropped. That's very much a language feature, even if it's not officially part of the C specification. Failure to acknowledge that it's possible to have automatic memory management in C is misinformation, at best. It may be optional, but it's very much possible, and many C projects today; especially on Linux, where GNOME writes much of the Linux desktop in C; are using it. The GLib API merely provides macros that makes using these attributes, and many others like it, ergonomic to use in practice. After all, GLib is both a portability and utility library, used by GNOME to enable ergonomic uses of best practices in C, as they develop complex applications in C, regardless of what OS their software is running on.
I fail to understand how TypeScript and JavaScript has such a staggering difference in their metrics.
OMG An actual project using a Twitter API, I swear every Rust library for Twitter has like no good documentation
EE Embedded guy here who's heard of Rust-the-programming language. Thank you muchly for the ambiguity warning.
https://github.com/rust-lang/crates.io/issues/326
Assuming that i have a workspace with \`root\` crate and its children crates \`A\` and \`B\`, where \`B\` depends on \`A\` and \`A\` have a feature that i want to disable at custom build stage, how can i pass that feature option to \`A\` when building \`root\` and its child \`B\`? Problem that i want to solve: i have multithreading code that i need to compile as singlethreaded on wasm target and i don't know how to ensure that it will replace pieces of code that cannot run on wasm.
Code is run far more often than it is read. Otherwise there'd almost be no reason to automate it with a computer in the first place.
&gt; there certainly aren't any destructors in C That's a common misconception. There are destructors in C if you are using Clang, ICC, or GCC. They support a `__cleanup__` attribute for tagging variables to insert their defined destructors at their drop sites. Realistically, using compiler attributes directly isn't very ergonomic, so you're going to want to use the GLib library to gain access to a more ergonomic representation in the form of `g_autoptr (T)` and `g_autofree T*`. You can even emulate borrowing and ownership to a degree if you combine this with `g_steal_pointer ()`. Here's an example of automatic memory management with GLib: void function (const gchar *name) { g_autofree gchar *value = g_strdup_printf ("Hello, %s", name); if (g_str_has_prefix (name, "A")) { transfer_ownership (g_steal_pointer (value)); } } The data that `value` points to will not be freed when its ownership is transferred through `g_steal_pointer ()`. A destructor for `value` is added by the compiler at the end of the function because it was marked with the `g_autofree` attribute, which uses `g_free ()` as the destructor. `g_clear_pointer ()` works by setting the `value` pointer to `NULL`, and then returning a new pointer, which in this case will not trigger a destructor. This way, when `value` is passed into the destructor at the end of the function, it will be a no-op. It's nowhere near as good as what Rust can do automatically, but if you have to write C code in your career at any point, this is good to know.
Do I use a line graph? A bar graph? Argh this is too hard I'll use both!
The thing I am mostly amazed about is how bad Haskell has performed in this particular research. 
Please allow me to mention a few limitations and shortcomings of parser combinators. First of all, parser combinator is more of an implementation tool instead of a design tool, it by nature cannot do an overall analysis of your grammar at compile time, i.e. it cannot detect ambiguities (which will default to a particular possible result) or left-recursions (which will cause stack overflow at runtime) in your grammar, while a parser generator usually will give very detailed warning about how and where these problems may occur. If you are doing some original work instead of implementing existing grammars with parser combinators then you'd better analyse the grammar thoroughly beforehand. The bad news is, eventhough parser combinators are widely considered superior in terms of integrity and reusability, LR parser generator is more or less the de facto standard method of parsing: Java, Python and Javascript among other most used languages include left-recursive grammars in their language references: [https://docs.oracle.com/javase/specs/jls/se11/html/jls-15.html#jls-15.18](https://docs.oracle.com/javase/specs/jls/se11/html/jls-15.html#jls-15.18) [https://docs.python.org/3/reference/expressions.html#binary-arithmetic-operations](https://docs.python.org/3/reference/expressions.html#binary-arithmetic-operations) [https://www.ecma-international.org/ecma-262/8.0/#sec-additive-operators](https://www.ecma-international.org/ecma-262/8.0/#sec-additive-operators) Converting these grammars to LL(1) may not be as trivial as eliminating immediate left recursions. Also in general, as a tool, parser combinators are less formal compared to parser generators. They certainly can handle LL(1) grammars, but anything beyond that is not immediately apparent. Most parser combinator libraries provide backtracking (enabled by default in nom) with unobvious consequences. The following grammar is a famous example: S -&gt; 0S0 | 00 The grammar seems to recognize strings of 0 of even length, a naive implementation with nom may be: #[macro_use] extern crate nom; named!(even&lt;&amp;str, u32&gt;, alt_complete!(pair | value!(2, tag!("00"))) ); named!(pair&lt;&amp;str, u32&gt;, do_parse!( tag!("0") &gt;&gt; inner: even &gt;&gt; tag!("0") &gt;&gt; (inner + 2) ) ); fn main() { for i in (2..=16).step_by(2) { let s = "0".repeat(i); println!("{:?}", even(&amp;s)); } } The output may or may not be surprising to you, depending on your familiarity with recursive descent parsers.
No, it currently only cares about the actual keys themselves. I should probably make it away of \~/.ssh/config and \~/.ssh/known\_hosts.
Also it breaks when companies do the stupid mitm SSL proxy thing
You can read my original comment as "almost no error handling at all" if you want me to be precise. It's sufficient for the domain, but I'd say the same would happen even if Go didn't have what it has. It's sufficient, but is it robust or convenient? But yeah, if you have less features, the learning curve is less as well, that's certain.
Looking at TypeScript numbers along with JavsScript it looks like at least some compilation it's taken into account
Kotlin was created by Jetbrains to he'll with their pain of maintaining humongous Java codebases. They can't drop it. And if since the Java interop is completely seamless, it would still make sense even if there was no library written in it. It's been picked by Google as an official Android languages and is quite popular there. And Java definitely isn't good enough for me.
Another reason to worry about energy consumption is environment, depending on where your energy comes from, energy from your binary (or development process, compiling, CI, tests, etc) might come at a cost for the planet ?
Hey guys, I have a problem with match expressions that return booleans: [https://pastebin.com/USFHsX2x](https://pastebin.com/USFHsX2x) &amp;#x200B; The upper code shows a working version of what I want to do - the lower one is how I want to do it.
And only about 10% of the code in Firefox is written Rust.
After a quick google search: https://godotengine.org/article/godot-3-vr-and-ar-support I don't know about Rust though, sorry about that.
It is a great experiment! I am currently looking into the difference in syscalls between Python implementation and all others, which are slower, and it seems that to solve the issue properly, there is a need to implement something alike Python asyncio.Protocol, which is aware of concurrent connections and thus can leverage a single epoll_await instead of a number of recvfrom to only hit EAGAIN
That seems like a feature, not a bug. Authentication breaking when being wiretapped should be a mandatory and unavoidable thing, surely?
&gt; In C and C++ you have to physically release the memory, Not necessarily. Depends on what kind of memory we are talking about. I C/C++ as well as Rust there are three basic kinds of memory areas: (1) You have a "stack" per thread which is for storing function-local stuff including your average int variable. So, in C you can write int fib(int i) { int a = 0, b = 1, tmp; while (i &gt; 0) { tmp = a + b; a = b; b = tmp; i--; } return a; } and you don't have to worry about manually allocating and freeing the memory that is used to store the variables. They are either placed on the stack or the compiler optimizes it in a way that they only ever stay in CPU registers. Managing this kind of memory is super easy. The compiler generates code to do that in the functions. In C and C++ this kind of memory is also called "automatic memory", presumably because you don't need to care about allocating and freeing it on your own. (2) You have "static memory" which is allocated while the operating system loads your program into memory. These typically include the memory for string literals and any other global state. This memory doesn't need any complicated management either. It's there during the whole duration of the program and doesn't need to grow. It's just there. (3) you have "heap memory". This is the kind of memory one has to manage manually in C. You explicitly allocate it using `malloc`, `calloc` or `realloc` and you explicitly release it using `free`. This is something you could use a garbage collector for. So, instead of making programmers to call `free` for every explicit allocation, you could let the garbage collector figure it out when memoey isn't needed anymore and then let it release it. The trick of C++ and Rust is that these languages allow delegating memory management (as well as resource management in general) to *objects*. You can make an object responsible for releasing memory. This has been done in the standard library so that you don't have to bother with these low-level details. So, for example, in the following case fn foo() { let mut v = Vec::new(); for i in 0..100 { v.push(i); } } we're creating a `v` on the stack. By pushing values onto v, it will automatically allocate enough memory from the heap because it is implemented that way. And when the program execution leaves this function and cleans up the "stack frame" of this function, the Vec's implementation of `Drop::drop` is invoked automatically. And Vec was written to release the memory that it previously allocated in its drop function. There's no magic involved here apart from hidden calls to `Drop::drop` for function-local objects that the compiler just inserts automatically. It's not more complicated than managing the stack. We add just a coupld of function calls at the end of scopes for non-trivial life objects that need to "clean up" somehow, just like Vec. That's what we call "ownership". The Vec "owns" its elements and is therefore responsible for managing their lifetime.
Should be configurable to operate insecurely. Some of us need to be able to access it inside a company firewall, even if the firewall is hurting security
I lost access to my github account because of 2fa and basically lost access to my crates. They would be there forgotten forever and nobody would be able to maintain it because of a stupid requirement of the company I work on. I was lucky and had the SSH key so github was able to turn off 2fa, but if I hadn't those crates would be dead and the user wouldn't even have a warning.
People have been talking about how the new `Pin` type will allow you to create self-referential structs, but as far as I know, it only lets you create immovable types. So how would you construct a self-referential struct using `Pin`?
What about JIT, where every first run on any client has to compile. On browsers there's tons of redundant code, imagine how many sites use eg. A different version of JQuery 
That's a big if. Nowadays, you'll have to be *very* good at writing optimal assembly to beat the compilers.
Why? Lazyness usually means that computation is deferred, and that, too, has a cost, that is not borne by eager languages.
 He's talking about reading the source down the road. Not literally doing a read on a file.
Is it really saying 0mb memory for multiple languages? That's impossible
&gt;LR parser generator is more or less the de facto standard method of parsing languages Not sure this is the case: off the top of my head, clang, gcc, rustc, swiftc, kotlinc use a hand-written parser. LR is certainly a standard when teaching a compiler‚Äôs course, but not when implementing production compiler.
Did they include browser compilation? Typescript first transpiles to a specific version of JavaScript. When used in s browser they both end up getting compiled as JavaScript
The problem with self-refs is literally that moving invalidates the reference.
Right, but doesn't `Pin` also introduce an auto trait `Unpin` which I believe you can explicitly unimplement for your own types?
No, just 1 or 2 MB if you look at the table in the paper. The memory axis is measured in 100 so I guess that's another point for dataisugly.
It's based on benchmarkgames iirc, so it could easily be that the code is just totally different.
I would expect the results to improve \~somewhat correlated with the improvements to Rust's perf on benchmarkgames, which in some areas is quite a lot. I do wonder though if some of the improvements, like improved use of simd, would have a significant impact on energy.
Builds near instantly even for large projects, runs pretty fast, is easy, often displacing Java or Node things which have worse problems than Go's. I'm not much of a fan of Go either tbh.
Remove the \`{...}\` surrounding the if condition.
That's something that could happen to any of your accounts though.
I don't know why you think I'm talking about reading a file... I'm talking about how where the costs for software are allocated. The only way you'll be burning more resources writing software than running it are if nobody needs it to be run in the first place.
But a cargo.io account wouldn't be inffluenced by a company policy about github. Also being by email I could recover through it, but github doesn't allow that. So it's a github specific problem. We can adopt other alternatives without that problem and make our own. I lost access to a bunch of accounts when I deleted facebook, oauth only is stupid, I recovered by passing them to email login because they were in websites I paid for a service on.
Unless you use a password manager with strong encryption like keepassxc. I keep all my 2fa barcodes in png and raw string format, plus recovery codes. This does beat the 2fa's true application you may say but "something you have" can be physical or digital. In my case, it's a cellphone and a strong kdbx file for fallback. 
That's a great experience report. Keep up the good work.
If yes, that means c used 9 mega bytes total. Usually the measurements of one implementation on a language is messy. If you pick c++ or python, there's a bunch of right ways to solve the same problem. In Python certain functions can pass data for a c API to execute, vs slower performance when using the same goal but not using c API How do you know platform X doesn't have a performance loss that affects one s output from another, At least on Windows, you can degrade performance by printing STDOUT. Is it a valid result if these tests are far shorter than an average program? What about modular compiling? Some languages allow modules to skip re compiling unchanged code. What about compile times for optimization flags? Or debug vs release? basically there a million factors so it might as well be apples and oranges. Doing different compiler version and flags in the same language seems more useful.
I thought you could email GitHub for support.
If you have 2fa enabled and lost it you can only recover the account if you have a approved SSH key, otherwise the account is lost.
callbacks
I don't quite understand your spec. Does this do what you want? ([playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=f901eeb30eb816f1b82a46049401363b))
This sent me on a very pleasant Wikipedia journey through some comp sci topics I hadn‚Äôt yet encountered. Very cool!
The trick to learning any language is you need to have a problem that you want to solve, and enough fire in you to work through the inevitable frustrations. I have started recently with rust and started writing a system management tool I desperately wanted to rebuild. 
Thank you, ill try to take a look at all of them. I believed i tried Rust Exercisms but they just seem like exercises to use the language more than building something. 
Learning rust while building something useful. Cause i can just "learn" rust by just taking examples I did in Java and just writing them in Rust, but i want to see if there is like a project that lets me to get more use to building something while also learning things along the way.
The problem is that I have enough experience in some sense, but not enough to know like what problems that need to be solve. Like you can be decent in Java but unless you have dived into doing any projects, you don't really know what Java is really meant for or what you can do with it. 
The thing is that i've written a discord bot in Java already though, in a more developed library as well. So why would it make sense to make a new one in a different language in a less developed crate?
They give you a bunch of recovery codes when you turn on 2fa. You are supposed to save those. 
I did, they didn't work for some reason, anyway shit happens. 2fa using a hardware is the most stupid thing ever, they may break or get robbed.
I feel like comparing programs that are hyperoptimized for speed (and thus non-representative) isn't a reliable way to get an idea of what a programming language's average power usage is. Also, Rust's ability to parallelize easily is pretty good for power usage (the servo team did a bunch of research on this).
Ah, of course, that does make sense. While Haskell's compiler does try to optimize many cases where lazy vs strict would not observably matter, that definitely is not always possible. 
Yeah... AFAIK there literally is no existing large compiler project that uses a parser generator. I'd be very surprised to see one that did.
Yes. thanks you very much. Later I figured that it was the wrong approach, I had to add the info at a later stage in the program, but nevertheless have to add a payload id and the number of expected packages. I am working on a simple client server program similar to scp, but using my own bacon "protocol". bacon can encrypt an arbitrary struct, and gives some freedom over the cipher. I could send a file encrypted in chacha20 or speck or any other cipher (which is easy to implement) using bacon, to a remote server. That's the idea more or less :) https://github.com/aspera-non-spernit/bacon/blob/dev/examples/bcp.rs 
I'm sort of confused. Why is company policy relevant? 
Btw you can do `self.peek(1) == Some('i')`
By default Free Pascal doesn't link against C-anything, and uses its own allocator implementation. You can opt in to using a `malloc`-based allocator that does call into the system LibC if you just include the `CMem` unit as the first thing in your programs uses clause, though (which is a good idea if you specifically need very high multi-threaded performance.) I'm not particularly sure why you think Pascal would use a lot of memory to begin with, though.
Or with recovery codes that you can encrypt and store elsewhere.
Wikipedia elaborates that "garbage collection" originally refers to any kind of automatic memory / resource management. "Tracing garbage collection" is what is usually meant by "garbage collection": an out of band system that tracks / traces resource references and periodically cleans then up.
[Crates.io](https://Crates.io) is not tied to github, to my knowledge. You can publish your crates wherever - right? Github is just being used as an identity manager in this case.
Because without the policy related to a third party service I wouldn't have 2fa enabled. If it was a crates.io account the policy wouldn't change anything. Since github is a third party with more usages besides publishing a crate it was hit by a unrelated policy.
Which didn't work for me, I had them saved and they didn't work. Idk why, I'm just stating facts. And even then, why does that change anything? Having problems with a third party system is not a legit reason to keep a crate unmantained.
I guess I still don't see why company policy is impacting your [crates.io](https://crates.io) usage. Do you publish at work? If so, account management should be handled by your org - resetting the account creds should be something they can always do. If it's a personal account then I'm back to being confused about the company policy.
The [docs for pin](https://doc.rust-lang.org/std/pin/index.html) has an example of a self-referential struct, does that help?
And in the distant future... todo!("maybe use a hashmap here instead?"); let map = BTreeMap::new() Borken. 
Is there a way to make a self-referential struct without a Box?
Example: Google encourages employees to use their personal account. They mandate however that you use 2fa.
I actually am fine with having crates unmaintained if developers can't be responsible with their accounts. It's unfortunate and perhaps we need a [crates.io](https://crates.io) recovery system that is out of band of github. Sucks that your recovery codes didn't work - that's really strange and unfortunate, I would hate to see a crate go unmaintained because the identity system had a bug.
I publish my crates but I use the github account for both, but as most services go you use it for both. With facebook for example you can't even have another account, github also doesn't expect that, they incentive it by allowing you to belong to multiple organizations and have separate work. It's how github works.
Interesting - and they don't bring your account into their github organization? How do they enforce 2FA?
So you use the same account across work and home - got it. My expectation is that you join your companies github organization and that they can manage your creds - hence their ability to enforce 2FA. But maybe that is not how it works?
To join the company's organization you must enable 2fa in your github account. Just that. The company has no access to my creds.
Yes, I don't expect them to access your credentials. My expectation is that if you join an organization, lose your password and 2FA and ssh key, then the organization can act as another point of auth when recovering. I would be kind of surprised to find out that that's not the case. But it's a moot point since this wouldn't matter in the event of a non-org account, and you'd still get locked out. But I stated elsewhere that that's a fine default.
No they can't. It's github responsibility.
This actually works today, though. Valgrind reports no memory leaks. int main () { g_autofree gsize *integers = g_malloc (1000 * sizeof (gsize)); do_something (integers); }
I haven't looked at the code, but might you be able to use a build script to produce the tables instead of const\_fn?
That surprises me, I work at a company that I believe allows personal accounts, and I'd be very surprised to find that I couldn't roll their creds. But I don't believe I've ever had to. &gt; It's not a fine default when cargo uses it. Agree to disagree I guess.
I just went digging a bit, and it appears that this site has the details of the test programs and compilation settings: https://benchmarksgame-team.pages.debian.net/benchmarksgame/faster/typescript.html It looks like they‚Äôre more-or-less even, except on one or two benchmarks, where ES drastically outperformed TS, which presumably skewed the average. OH. The ES implementation is *parallelized*, and the TS implementation is not.
Alternatively, use `gopass` and keep your GPG private key on a Yubikey or other OpenPGP Smart Card and then you keep the "something you have" factor. My passwords, random access tokens, etc are all stored in a Git repo. The passwords and TOTP codes are stored and managed by `gopass` and `git-crypt` makes sure that files in my `dotfiles` with passwords are transparently encrypted at rest. (I'd like to see a Rust rewrite of gopass+git-crypt in a single tool, one of my current side projects)
Also a great fondue place.
You're required to enable it to be a member of the organisation. It's still my personal account, and I guess I could disable 2FA (if I wanted) if I left the organisation.
If Cargo were less [crates.io](https://crates.io) oriented, this would be less of a problem too. This will be hard to solve unless [Crates.io](https://Crates.io) starts doing user management themselves, which changes the complexity a fair bit (features, security, risk, etc). &amp;#x200B; I wish it would've been more like Go: the standard library was un-prefixed, but everything else requires some sort of namespacing in the naming/references. This just isn't an issue for Go.
This is horrifying to me. If anything, I want GitHub to allow the Crates.io application to specify that it can only allow logins from users with 2FA enabled. We're talking about access control to publish code to a public code repository. I think 2FA should be the bare minimum at this time. While I think it's important for Crates.io to not be tied to GitHub, I'm sort of really sad that "intentionally weaken security" is one of the reasons it's being asked for.
&gt; So if you don't want to provide support for problems with github allow email only and pass to email login if the integration fails. Or accept the risk that crates may be abandoned because of problems with a third party service. Sounds a lot better than reading "Hundreds of applications affected by compromised common Rust package" because someone pushed a Crates.io update with the same password they use on their Starbucks account. The more I read this subthread, actually, the more I really want Crates.io to only allow GitHub accounts with 2FA enabled.
GitHub allows the org to say "accounts must have 2FA". Google encourages you to use your personal account (ie: a GitHub user account, not a LDAP account). Google pulls your user into their GitHub org, thus causing you to need 2FA.
&gt; It's unfortunate and perhaps we need a crates.io recovery system that is out of band of github. It won't change a thing. It would basically requires Crates.io to become a full fledged user management system, or requires them to implement a "shared security recovery" mechanism which... well... to put it nicely, has the same usage-difficulties as GitHub's recovery codes.
2fa is not secure because your phone can be stolen.
Yeah, that's basically what I was thinking as well.
Or even better, a macro?
I would think so. While one vector op will likely draw more power than a single scalar op, they do more work. And other areas on the CPU (e.g. instruction decoder) have to.do less. There is one problem with the benchmarksgame entries: Most are optimized for an old CPU.
My main grievance with glib is the implementation and design, mostly *because* of some of the things you said. It aims to be yet another [standard library](https://xkcd.com/927/), one that behaves like a program more than a library with all its unique types, globals and bloat everywhere. Say you want to do some IO with glib. Great use case - different under the hood for different implementations, but could be easily unified. That's great, except you're also loading into memory code for "spawns", regex, markup, internationalisation and many more. You might say the solution to this is to use the code with static linking, which might have been fine, except the glib implementations get deprecated and replaced faster than I change my socks. This would have been absolutely great if it were a small project, but the monolithic nature of glib should call for more stability. Not to mention the whole "GObject" architecture, home to some of the easiest exploits related to IPC. libc is cross-platform for every platform that's POSIX-compliant, or mostly so. That is, about every platform but Windows. I agree that it's not good enough, but I strongly dislike the design of glib. You're right about it being installed, though. I checked my laptop (Ubuntu), I guess just about everything with GUI has it installed. Archlinux server is glib-free, though.
If you control the C code, you should add a `void *` to your callback signature, then you should be able to use the approach detailed in the first link you gave. Adding a `void *` will also make the API more versatile for C clients and is pretty much necessary for language bindings. Sorry cannot provide more detail ATM, am on phone.
This is not C and cannot be implemented in standard C.
Nice article! Just wondering if you considered changing out `std::collections::HashSet` `hashbrown::HashSet` (since you mentioned changing the HashMap to an ArrayMap)? 
&gt; That's a common misconception. That's certainly not a misconception. There might be destructors in a nonstandard extension of C. But that is not C.
What are some tips on sharing projects? Who to share with, how to format, whether or not it needs documentation, etc..
There's nothing wrong with being a standard library when you are, in fact, a standard library for C. The API is less than ideal in a handful of areas, but as far as C-based software development is concerned, it's the only reasonable choice available if you want a solution. The alternative would be to write your own subset of a standard library from scratch, and handle OS-specific APIs in more scenarios than you ought to be handling. The issue is that C lacks an adequate standard library for the needs of most library and application authors, and the C spec itself has holes in a handful of areas that could have been avoided. GLib does a good job of making C software development somewhat more approachable, driven by practice than committee. If you're trying to develop an application, you may expect for a language to officially support the usual suspects that applications require. UTF8 strings, arrays, maps, iterators, etc. As well as a handful of utility functions that you would expect to use with those types. Internationalization is a thing as well, as is required by most open source applications. GObject and other features are built on top of GLib, but you aren't required to link them if you aren't using them. Though due to the nature of C software development, monolithic libraries are often preferred over small bits of of functionality. The suite of GLib libraries includes many features which GNOME and GTK applications commonly use.
Sounds awesome! I know a little bit of erasure coding (Reed-Solomon, GF2\^8 arithmetic) but I havn't yet heard about RaptorQ let along Fountain codes. Will definitely dig deeper when I find the time. &gt; Switching to const fn resolved this and even improved performance by moving the precomputation to compile time. It was a little more painful than I had expected because const fn doesn‚Äôt yet support loops or if statements, so I had to manually unroll loops to several hundred lines of code, but it was still worth it. It's possible to add another build step and let cargo compile and run a small program that generates a table which could then be included into your main crate using `include!`. That's what I did for the crc24 library (IIRC) and possibly (not sure anymore) for generating a multiplication table for GF2^8 for my version of Shamir's secret sharing.
Does anyone have a guide to when const fn will support \`if\` statements and possibly loops. Conditionals (in C++ TMP) allow for a lot of expressivity at compile time.
Yet here we are, and it works with all major compiler vendors that jointly agreed to implement it.
Ah, the "No true Scotsman" fallacy. Nice.
Why would a macro be better? A const function lets you calculate a runtime constant at compile-time causing no overhead at runtime. I understand that a build script would be functionally identical since it'd just dump the same calculation as code right into a file.
I would be fine with r/rustlang. That way we wouldn't have this confusion anymore.
Greetings fellow hunter!
A closure is a piece of data along with the function pointer, and therefore it cannot be properly passed in given the current C signature. A proper solution that allows for closures requires the C callback to take an additional "data context" argument, making the code something along the lines of the following code. // In C void c_function(void (*callback)(int, const void* callback_ctx), const void* callback_ctx) { // ... callback(x, callback_ctx); // ... } // In Rust fn rust_function&lt;F: FnOnce(c_int)&gt;(callback: F) { extern "C" fn callback_wrapper(a: c_int, ctx: *const c_void) -&gt; { let f: F = ptr::read(ctx as *const F); f(a); mem::forget(f); } unsafe { c_function(callback_wrapper, &amp;callback as *const F as *const c_void); } } Note, however, that for this specific code to work `c_function` must execute `F` while it is in "scope" (i.e. not asynchronously). For asynchronous operation to work, you will need to "box" the closure and free it inside the callback instead.
`std::shared_ptr&lt;T&gt;` is directly equivalent to `std::sync::Arc&lt;T&gt;`. Rust also has `std::rc::Rc&lt;T&gt;` for non-atomic reference counting. `Rc&lt;T&gt;` is marked as `!Send`, so the compiler forbids sending it across thread boundaries. There are a handful of crates for handling memory efficiently. [object-pool](https://docs.rs/object-pool) and [generational-arena](https://docs.rs/generational-arena) come to mind. There are graphics APIs available in Rust should you go that route instead. [gfx-hal](https://docs.rs/gfx-hal) seems to be the go-to Rust API,
Use a pen and paper and any language will perform the same.
I meant it would be better to use a macro than a build script, if using const fn is not possible due to some limitation. A macro is conceptually simpler in my mind than having an extra build step and "manually" generating source code. A const fn would be most elegant, if possible, of course.
Thank you, I just went through this exercise for my own site last month. This is an issue tailor made for me to contribute to.
GHC uses a parser generator.
I would recommend just diving into the language. Start small, let yourself get a feel for how Rust does things, before you consider porting your larger application.
This is UB. Calling a `FnOnce` drops it, so you're dropping the same value twice, and additionally you're making a copy using `ptr::read` without requiring `F: Copy`.
I didn't know about hashbrown. Thanks for the pointer! I'll have to take a look at it
Soon‚Ñ¢
Do these support Windows?
Try Rust. It will feel natural to you.
I wrote this crate a few months back and it's been stable long enough that I thought it was worth submitting. It allows you to create "module constructors" that run at load time for an executable or shared library or "module destructors" that run at shutdown time. It also has an alternative to lazy_static where you can run code to initialize globals that aren't const.
Here's the [tracking issue](https://github.com/rust-lang/rust/issues/49146) for `if` statements, along with a [tracking issue of tracking issues](https://github.com/rust-lang/rust/issues/57563) for other `const fn` topics.
Ahoy! Not gonna lie, I could‚Äôve done this in like Python or something, but I was thrilled to have a project I could do with two things I love: Monster Hunter and Rust. :)
It's awesome. Packs iteration over a set into simd iterators when possible.
Pretty sure you can use CLMUL instructions to just directly multiply in gf(2^8) and everything that supports avx2 probably has aes-ni Near work!
Sapphire Star guide you!
What's a generational JIT?
Peter has tests for the size and alignment of structs for both 32 and 64-bit targets
Several features you've mentioned (besides reference counting memory tracking) are not in Rust (yet). Rust does not have placement new--there was an unstable implementation that was removed. See [https://github.com/rust-lang/rust/issues/27779#issuecomment-378416911](https://github.com/rust-lang/rust/issues/27779#issuecomment-378416911) for more details, and links for discussion. &amp;#x200B; Rust also does not currently have C++ interop as of yet. There has been discussion, as several commercial users of rust have indicated value in that feature. All of this being said, there's a lot to like with Rust, and your voice and interests can help shape (and prioritize) Rust's future.
If you trust other code at about the same level of trustworthiness required of other code which uses `unsafe`, sure. But the new pinning stuff doesn't really change that situation. `PhantomPin` prevents moving something if and only if that thing is the target of a `Pin` pointer. So if you're hoping to expose an immovable type to safe-untrusted code, you have to put it behind `Pin&lt;Box&lt;T&gt;&gt;` or similar
Just to make sure everything is understood, a very basic explanation of JIT(just in time compilation) means taking an intermediate language (in C#'s case, CIL) and turning it into machine code. This allows having a common binary distributed amongst machines, and then the jit compiler can figure out how to optimize it for the specific machine it's running on. Now, a generational JIT does the above, but in multiple steps. JITting is expensive, as it has to figure out and optimize for the current machine. Doing the JIT in steps, such as doing nearly no optimization in the beginning, will allow for faster start up times (which would be near native startups). As an application continues to run, a generational JIT compiler will more aggressively optimize functions that are called, and replace the less optimized versions on the fly. Currently c# takes a middle ground, and optimizes once, but not all the way, to try to get a balance between speed and start times. When they get the generational JIT finished, it will be a massive improvement as starts will be MUCH faster, and the longer running process will run even faster. This matters a ton in web servers as docker containers should be made to be able to spawn hundreds of processes instantaneously to allow for massive spikes, but then run great as that process is alive. Currently, the JVM does this and it's basically the only reason it's still ahead of the CLR. Even though Java is pretty terrible, the JVM is pretty amazing. 
Since \`termion\` is a dependency, no, it doesn't. I've got a branch of \`termion\` with a lot of Windows support implemented, but I'm not sold on the free function API it exposes.
You are looking for /r/playrust. /r/rust is about the [Rust programming language](https://www.rust-lang.org/), which is entirely different than the game.
All them dislikes but rust has twice as many subs on their page haha
I have a feeling that your reliance on shared pointers and "passing around raw pointers" are both symptoms of architecture design that Rust's lifetime semantics will highlight, and you'll probably bang your head against the monitor trying to fight the borrow checker. So two things before you dive in: 1. Don't use unsafe code just so you can use raw pointers instead of references. 2. The borrow checker is your friend, not your enemy. Also I've done a lot of what you're talking about but the more I work in Rust the more I realize how little you need reference counting. You basically just need it for interior mutability or shared resources across threads, and Rust's semantics really help alleviate complexity in those regards. As for placement new, I feel you. Not yet in stable Rust. But you can get around that with unsafe code using volatile reads/writes, it's just you know, unsafe. 
Is it though?
Try wrapping each `match` in parenthesis.
Sounds like a dumb premature optimization to me.
At this point, every week there is some kid lost here..
Here's a version that compiles and should be UB-safe (https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=5a1aa3e302c528f4dfa4e29ef5c59d66): use std::ffi::*; use libc::c_int; // C translated to Rust fn c_function(callback: extern fn(c_int, *const c_void), callback_ctx: *const c_void) { // ... callback(42, callback_ctx); callback(43, callback_ctx); // ... } // In Rust fn rust_function&lt;F: Fn(c_int)&gt;(callback: F) { let callback: Box&lt;dyn Fn(c_int)&gt; = Box::new(callback); extern "C" fn callback_wrapper(a: c_int, ctx: *const c_void) { let f = ctx as *const Box&lt;dyn Fn(c_int)&gt;; unsafe { f.as_ref().unwrap()(a) } } c_function(callback_wrapper, &amp;callback as *const _ as *const c_void); } fn main() { let f = |i| println!("i = {}", i); rust_function(f); rust_function(f); } 
If I'm not mistaken, Fn implies Copy + Clone (ie: https://github.com/rust-lang/rust/issues/28229)
Ya, I'm coming from a C background (granted having been doing it since the 80's.... but it's been a pretty long time now) and that's basically what I've been doing. I picked up the O'Reilly book, started the Rust track on exercism.io and have been looking into something small I can contribute to the embedded Rust work (since I do embedded work).
Perhaps Redox could greatly improve its hardware compatibility by using shims that allow drivers from other operating systems to be used when no native driver exists? Haiku AFAIK has a shim to allow FreeBSD network card drivers to be used. FreeBSD has a DRM shim to allow Linux GPU drivers to be ported over. There was also of course the good ol' ndiswrapper which allowed Windows XP wireless drivers to be used on Linux back in the days when Linux driver support was awful.
TFS seems more like a vapourware dream than an actual software project. There is no working filesystem at all, yet alone one with the rich features and good performance that the project aimed for. Only a few loose components that the author imagined would some day be used to build a filesystem with. Often, successful projects happen by building a working prototype first which actually serves a practical purpose and then iterating on it to improve it, rather than starting with a grandiose abstract design idea. Ticki himself said in his "leaving open-source" blog post that his mindset is better suited to theoretical/analytical things like mathematics and the sciences, rather than engineering or software development. That said, the TFS project resulted in a few good libraries and algorithms, like seahash, which I personally really like.
This is the library that [inventory](https://crates.io/crates/inventory) uses, right? Seems extremely useful, thanks for writing and sharing it!
Do one of you be kind enought to explain me why this is so important please ? I programmed in JS for quite some time and now that I'm doing C++ Qt to program apps I never had to determine if the code is compile time or not. Also I never used C++ templates I think.
No word on multithreading? I don't know if this is something possible here
same experience
This. Just read the rust book when you get time, it's very short and will give you a great explanation to how borrowing works. After that you're all set. It's very similar to c++ and after a couple months I was not fighting the borrow checker at all. It's not very hard if you know what you're doing so if you have experience with c++ and you read the rust book you're probably going to have a great time
Ooh, shiny. I'll probably experiment with unsegen in a toy project if that ever gets far enough. Nice work :)
Parser combinators are just helpers for writing a recursive decent parser in a particular concise style. It's worth trying to do it yourself just to see what's going on under the hood.
&gt; you'll probably bang your head against the monitor trying to fight the borrow checker Only for like 12 hours
Curious as to what you meant by this?
Yeah, seahash is something in particular that I want to work on, but there are PRs to fix bugs and add features that haven't been addressed in months. Since it's being hosted in the Redox org, it would be nice if someone could take over even if it's just to approve PRs
I fail to see how integrating the two different projects would be useful. They serve entirely different purposes. What is your use case for wanting the automated file en/decryption features of a git extension merged into a gpg-backed path-based password management tool? 
I'm on mobile so I won't look for it right know but I searched how to pad numir and I found this. I don't remember the syntax using the "&lt;" char but for numbers the padding was done with zeroes and with spaces for strings.
If you check "new" regularly you'll see it's often multiple times a day.
I don't mean this to sound rude *at all* but I'm a little curious about what you want to achieve. It sounds like you have a working engine for generating voxel planets, and are trying to see if you can port **exactly what you already have into Rust**, and the benefit you think you'll get is a more focused feature set in the language. This might be naive, but I think some of your solutions might not be as necessary in Rust, or that there might be alternative ways of solving the problems you face. Now, I can guess, but I'm not 100% sure what problems those are, you've only defined the solutions. Anyway, I'm assuming this voxel planet generator (I'll call it VPG) is fairly large and has multiple classes and libraries. Why not pick small portions of the VPG and only port those at first? Some structs, then some simple libs, then some more meaty things. It'll give you an ability to dip your feet in the water, see how Rust does things, how it performs, and whether it's going to give you the benefits you want (or others, or not worth the effort etc) 
They both keep data in a Git repo. They both use GPG for encryption. They both fundamentally encrypt and decrypt files. I have more than a couple places where I use both in a single repo and resent having to ensure I have both tools available/built/toolchain-available in various places where I want to access those repos and their contents. I also want a Rust project to hack on, don't like writing C (git-crypt) and don't like the `gopass` codebase. Given the fact that their application code would be very similar, and my use-cases, it's an interesting project to me.
`sys_common::at_exit` ?
&gt; Basically I use a lot of reference counting, in general something like std::shared_ptr but I implement it myself and generally keep the reference count in the object itself.. Furthermore I typically only use reference counted pointers it instances of objects. I still pass raw pointers around a lot for speed when calling routines. Rust makes a lot of improvements in this area, and also has few things you'll want to watch out for. * There's the standard library's `Rc` and `Arc`, equivalent to `std::shared_ptr`, if you just want to get started. Writing your own internal ref counting might be more tedious in Rust, but check out [intrusive-rs](https://github.com/Amanieu/intrusive-rs) for some inspiration on how you might improve the situation. * Using RC pointers only in objects is idiomatic in Rust, as you can use normal references elsewhere and the borrow checker will prevent you from keeping one around longer than you should (at which point you can just switch it to an RC pointer). * On the other hand, if/when you *do* pass RC pointers around outside of objects, there's no extra overhead- the pointers are word-sized, and they're moved by default (you can opt in to incrementing the ref count with `clone`), which is less tedious than babysitting everything with `std::move` in C++. * There is not yet any custom allocator support for things like `Rc`, so even if you wanted to use it instead of your hand-written internal ref counting, you wouldn't be able to use your custom heaps. This will hopefully change in the near future. &gt; The other thing I use a lot in C++ is placement new. In fact I rarely use an unadorned new. I have a huge library of custom heaps that I have built up over the years. My current go to heap for this project calls the Windows VirtualAlloc function and I can have several of these heaps working simultaneously, sometimes for different threads. Rust doesn't have its equivalent to placement new (yet), but it's less of an issue because Rust doesn't have `new` or constructors anyway. You should be able to write your custom allocators just fine without going through language-feature intermediaries. Rust should also be able to help here with your thread-specific heaps, via the `Send` and `Sync` traits that enforce how things can be moved and referenced (or not) across thread boundaries. &gt; Also since I'm using DirectX, I still need to call C++. I'm assuming this shouldn't be a problem but if there are any issues, I'd being interested in hearing about them. Rust doesn't do C++ (just C), but fortunately DirectX is already designed for that, because it's just COM, which you can call fine. It can be a bit tedious because Windows APIs are looser with types, but the [winapi](https://github.com/retep998/winapi-rs) crate should be enough to get things working. &gt; Finally I write a lot of templates, so some template like meta-programing facility would probably be useful although I can get by without it. It's unclear what part of template meta-programming you're looking for- Rust generics are similar to C++ templates, but without compile time duck typing, overloading/SFINAE, or value parameters. Instead they use traits to constrain what you can do with type parameters (much like C++20 Concepts, but actually enforced at the definition site as well as the use site). Value parameters to generics are coming eventually. There is also an equivalent to constexpr that is quickly gaining power, if you want to go that direction instead. The macro system is also much more powerful- it's more like Lisp's than C's, in that it operates on token trees, is hygienic, and allows for arbitrary Rust code in macro definitions. So that may be an alternative if generics don't give you what you want.
Short answer: Java\*
Actually, re-reading your post, I might recommend trying to write one of the heaps in rust as a first step. "Read the book" is the best way to really know what you'd be getting into, but if you don't have the time and do the "just show me examples and I'll try it" approach like me, something basic like a Unix utility or a data structure lib is a good place to start.
&gt; static INITED: AtomicBool = ATOMIC_BOOL_INIT; FYI you can use `AtomicBool::new(false)` instead of `ATOMIC_BOOL_INIT`.
I've been using gdb dashboard for a while and I'm really excited to try this out as a possible new preferred debugger env. Thank you for making this!
I agree with you, but the book is not short... Let alone very short. Lol &gt; 500 pages is rather large for a programming book. 
An email to help@crates.io from an email listed as an owner on the crates would be able to solve this as well
&gt; perhaps we need a crates.io recovery system that is out of band with GitHub We do. We deal with these sort of support requests on a regular basis. 
Ocaml uses a parser generator 
crates.io is indirectly using this today through inventory! Great work!
Don't ask to ask. Just ask
Interesting. How do you establish identity? Publish tokens?
It was conjecture, as is some of the following: Using callbacks ‚Äî references/pointers to functions ‚Äî instead of inlining will save memory. Pascal emphasized modular programming and, being more of an educational language, may not have typically been optimized for speed via inlining. Go is known for extremely quick compile times, but is also typically slower than C/C++. You can‚Äôt have the former when globally inlining, and the latter is a natural consequence. 
This. I downvoted your post OP, don't take that as a "no" just don't post about asking to post :P
I find it awesome ! As gdb feels more user-friendly to emacs users, this feels really great for vim user like me. :) As I'm mostly working with c++ right now, I recommend implementing a move / resize (maybe kill?) a window, that would be really awesome. Moreover, gdb auto completion in the gdb window would also be very cool :D. Also, if you switch to assembly while debugging and single step instruction, the window goes back to source code also, which is probably not what's intented. I feel ashamed not being able to code in rust, I'd really like to help on this project because I think it'd probably improve a lot my usage of gdb. Nonetheless, great work !
Aha good point. I deserve that. &amp;#x200B; &amp;#x200B; Your wheat is mine...
&gt;This library will also work as expected in both bin and cdylib outputs, ie: the ctor and dtor will run at executable or library startup/shutdown respectively. Will it works in cdylib ?
Is there a place I can track the progress of the generational JIT?
Yeah, I think that it would have been just fine to have to specify a dependency in the manifest as `crates.io/serde/serde` or, if we had [Clojars-style namespacing](https://github.com/clojars/clojars-web/wiki/Groups), as `crates.io/serde`. That would have left a wider window for alternatives to crates.io to come through.
It should work in all the library/executable forms on all unixy platforms (if not, I'm happy to add support)
That's correct- I'm impressed by the magic in that crate!
API token or an email sent from a verified address on the account
Ah yeah, thanks for the pointer! I'll update the examples.
Thanks, very useful crate! 
I think that's confusing and wrong to say Rust has a garbage collector. Every compiled language, at termination of a subroutine, will decrement the stack pointer to basically throw away all the data from that subroutine so it can be overwritten by subsequent instructions. If Rust has GC, then every compiled language has GC.
Thanks for sharing 
I disagree with that essay. He's probably way smarter than me, but if that's his definition of GC, then wouldn't C have GC? Because when you compile C, it creates the machine code equivalents of decrementing the stack pointer when a subroutine terminates. Right?
I don‚Äôt think your description of the choices is correct. There are several orthogonal considerations when processing expressions, and the shunting yard algorithm (marked ‚Ä† below) and RPN vs AST (marked ‚Ä° below) lie on different axes. \[*TL;DR: You might want to look at* [*Pratt parsing*](https://www.oilshell.org/blog/2017/03/31.html)*. For a calculator, all other options here are overkill; actually, you don‚Äôt need RPN nor ASTs either. You might still want to read further if your goal is to learn compilers. People still mostly hand-roll their parsers, though. Sigh.*\] * How do you describe the language to be parsed? Either use a convenient class of grammars \[[regular](https://en.wikipedia.org/wiki/Regular_grammar), LL(1), LR(1), [LL(\*)](https://www.antlr.org/papers/LL-star-PLDI11.pdf), [context-free](https://en.wikipedia.org/wiki/Context-free_grammar), [PEG](http://bford.info/pub/lang/peg), *etc.*\] or realize that expressions don't need this generality and define a precedence relation (with simple numerical precedences or even a full-blown [precedence DAG](http://www.cse.chalmers.se/~nad/publications/danielsson-norell-mixfix.pdf)). * How does the parser recognize the input? Either from the largest logical unit down (‚Äútop-down‚Äù) or from the smallest one up (‚Äúbottom-up‚Äù). Among other things, this choice influences which diagnostics the parser will be able to provide. * What is the specific algorithm used in the parser? Bottom-up parsing algorithms include [shunting-yard](https://www.cs.utexas.edu/~EWD/MCReps/MR35.PDF) (‚Ä†) for operator precedences, [derivative-based](https://research.swtch.com/yaccalive) and [NFA-based](https://swtch.com/~rsc/regexp/regexp1.html) for regular grammars, LALR(1) and [LR(1)](https://harrymoreno.com/assets/greatPapersInCompSci/2.5_-_On_the_translation_of_languages_from_left_to_right-Donald_E._Knuth.pdf) for eponymous grammars, and [GLR](http://scottmcpeak.com/elkhound/sources/elkhound/algorithm.html) for arbitrary context-free grammars; among top-down ones are [Pratt](https://www.oilshell.org/blog/2017/03/31.html) for operator precedences, LL(1) and LL(\*) for eponymous grammars, [GLL](http://www.cs.rhul.ac.uk/research/languages/csle/GLLparsers.html) for arbitrary context-free grammars, recursive descent and [Packrat](http://bford.info/pub/lang/packrat-icfp02) for PEGs. * How do you organize the parser source? Either write the whole thing by hand, perhaps with a library of ‚Äúparsing combinators‚Äù, or transpile a language specification into your programming language of choice. Embedded domain-specific languages and metaprogramming can blur the line here. Handwritten parsers are usually either Pratt or recursive-descent, although shunting-yard parsers do occur; the prototypical parser combinator library is [Parsec](http://book.realworldhaskell.org/read/using-parsec.html). Transpilers are available for NFA-based ([re2c](http://re2c.org/), [Flex](https://www.gnu.org/software/flex/)), LALR(1) ([Lemon](https://www.hwaci.com/sw/lemon/)), LR(1) ([Bison](https://www.gnu.org/software/bison/), which also has limited support for operator precedence), LL(1) ([LLgen](https://www.cs.vu.nl/~ceriel/LLgen.html)), LL(\*) ([ANTLR](http://www.antlr.org/)), GLL ([Haskell gll](https://hackage.haskell.org/package/gll)), GLR ([Elkhound](http://scottmcpeak.com/elkhound/)), recursive-descent ([peg and leg](http://piumarta.com/software/peg/)), and Packrat (ANTLR again) parsers. Embedded domain-specific languages (like [LALRPOP](http://smallcultfollowing.com/babysteps/blog/2015/09/14/lalrpop/) for LR(1), [Rust gll](https://github.com/rust-lang/gll) for GLL, and [LPeg](http://www.inf.puc-rio.br/~roberto/lpeg/) for recursive-descent parsers) can straddle the line between the two choices. * How do you specify the language at runtime? Either drive a prepackaged parsing algorithm using a table of constant data or encode the language directly in the structure of your program. (The code in the directly-encoded version is that in the table-driven one [specialized](https://en.wikipedia.org/wiki/Partial_evaluation) for your specific table.) Handwritten parsers are usually directly encoded, although precedence tables are sometimes left as data. Transpilers traditionally generated table-driven parsers, but newer ones can usually do directly-encoded ones as well. * How does the parser present its output? [Concrete syntax trees](https://eli.thegreenplace.net/2009/02/16/abstract-vs-concrete-syntax-trees/) (CSTs) include a node for every grammar production used during parsing; abstract syntax trees (ASTs) (‚Ä°) collapse this down to only include the parts the client code cares about. The distinction depends on the problem; if you‚Äôre writing a universal frontend like [Roslyn](https://medium.com/microsoft-open-source-stories/how-microsoft-rewrote-its-c-compiler-in-c-and-made-it-open-source-4ebed5646f98) for C# or [estree](https://github.com/gibson042/estree/blob/gh-41/spec.md#concrete-syntax) for JavaScript, your AST can include comments and whitespace. Going further, you can generate (real or virtual) machine code directly from the parser. When done for a simple stack machine limited to arithmetic, like your calculator does, this becomes translation to reverse Polish notation (RPN) (‚Ä°). When done *awesomely* for a slightly more advanced stack machine, this instead becomes [C in four functions](https://news.ycombinator.com/item?id=8558822). When done skillfully for a real machine, this becomes the [tiny C compiler](https://bellard.org/tcc/). Finally, you can *execute* operations directly from the parser as well. This would be simplest for a pure calculator, but of course you‚Äôd have to go back to code generation once you introduce functions. The shunting yard algorithm that you use in your calculator was designed when RPN was the state-of-the-art intermediate language, so naturally its expositions were geared towards generating RPN. It‚Äôs not tied to RPN, though; it can [produce an AST](http://www.engr.mun.ca/~theo/Misc/exp_parsing.htm#shunting_yard) or evaluate expressions on the fly just as well.
Consider using 2018 edition syntax, no need for \`extern crate\`s and easy use of libraries. &amp;#x200B; In [cmd.rs](https://cmd.rs), consider creating a implementation for your struct Command. example: \`\`\`rust pub struct Command { pub verb: String, pub hex: \[u8;4\], pub int: isize, pub rep: usize, pub unset: bool, } &amp;#x200B; impl Command { pub fn new( ... ) { ... } &amp;#x200B; pub fn whatever( ... ) { ... } } \`\`\` &amp;#x200B; In canvas and pointer, consider including the new function inside the impl block. &amp;#x200B; I hope I helped you :3 
What is the difference between including it in the impl and somewhere else?
That is an excellent article.
I can't really comment on Go as I don't know enough about it, but all of that is sheer nonsense with regards to Pascal as currently implemented. Which compiler did you suppose the benchmarks were using, exactly?
2 things, convenience for using \`Self\`, and cause it is more rust style, as example: ```rust pub struct Command { pub verb: String, pub hex: [u8;4], pub int: isize, pub rep: usize, pub unset: bool, } impl Command { pub fn new(verb: String, hex: [u8, 4], int: isize, rep: usize, unset: bool) -&gt; Self { Command { verb, hex, int, rep, unset } } } ```
whew I'm glad I didn't sit through this waiting for some machinima humor about Rust
Cool, makes sense.
Woah I didn't know you could do that! And you're right, that is more Rust style, I agree.
Really? I'd probably estimate the average book to be around 500 pages. 500 might not be particularly short but it isn't particularly long either. Regardless, I'd say that the accessibility and clarity of the Book still makes it a fairly quick read for someone who has a lot of C++ experience.
Then you will spend another 12 hours banging your head on the wall because you didn't see those issues until the borrow checker poked you repeatedly over them. &amp;#x200B; me: Of \*course\* that would cause an issue! It's so obvious now. WTF was I thinking? &amp;#x200B; Rust is worth it for this alone since it's actually made me a better c and c++ programmer because of it.
I appreciate your concern and the technical usage of the term 'garbage collector' is such that Rust DOES NOT have one. But I wanted to explain like a layman for him. 
According to [this](https://docs.microsoft.com/en-us/dotnet/core/whats-new/dotnet-core-3-0#tiered-compilation), it will be released in full when .net core 3 is released, [which will be some time in the second half of this year](https://github.com/dotnet/core/blob/master/roadmap.md) &amp;#x200B; C#'s team is calling it tiered-compilation: [here is their thread about it](https://devblogs.microsoft.com/dotnet/tiered-compilation-preview-in-net-core-2-1/) &amp;#x200B; It has been in preview, but you can opt into it to try it. It does not work for everything quite yet, and the performance isn't all their, but in .net core 3, it should be good for everything and will be on by default!
&gt;hex should have type [u8;4] not [u8, 4] Sorry, it was a typo xD
if you don't say it, i don't know Tao of JKD
Thank you so much. Other than that, uh... Not too ugly?
Well, it is also very common to have the struct contents private and have functions in the `impl` block to get them Like this: ```rust pub struct Command { verb: String, hex: [u8;4], int: isize, rep: usize, unset: bool, } impl Command { pub fn verb(&amp;self) -&gt; String { self.verb } // The other one as well } ```
Oh right because it is allowed to operate on itself and this helps with data safety. Right?
very cool project and optimization
Funny enough I asked a question regarding this a bit ago and found the neon thread, but never got around to making a custom constructor library. Thanks!
&gt;Oh right because it is allowed to operate on itself and this helps with data safety. Right? YEap, right :3 &gt; What does the example mean? How would I modify values? It's a simple getter, you also can modify using other functions to mutate it, taking `&amp;mut self` or `mut self` and modifying it inside the function 
Good job on this! I was wondering the other day if there was a better frontend for debuggers, although I would use lldb probably usually. The TUI framework is excellent as well. IMO, you made a good choice for demo applications, since I tend to use the jsonfui viewer everyday constantly and the pager as well as some of my most used terminal applications. I was actually considering building a framework myself, although one step higher level than the widget level for quick terminal applications, so I'll have a look at unsegen as a backend. I think terminal applications are my favorites for pure speed, and so I want to make them more popular these days.
Yes, I actually already wrote all the functions I needed. But now I get this error, error[E0507]: cannot move out of borrowed content --&gt; src/canvas.rs:54:3 | 54 | self.buffer | ^^^^^^^^^^^ cannot move out of borrowed content error[E0507]: cannot move out of borrowed content --&gt; src/cmd.rs:22:3 | 22 | self.verb | ^^^^^^^^^ cannot move out of borrowed content error: aborting due to 2 previous errors I think I know how to fix it, but I'm not entirely sure and I don't want it to get *too* convoluted.. lol.
I'm new to fountain code. It seems like a great concept for UDP communication though. Has anyone used it for that?
&gt; You should be able to write your custom allocators just fine without going through language-feature intermediaries. Isnt that all or nothing right now? Either everything uses the custom allocator or nothing does? Whereas OP uses several heaps simultaneously?
Point. I've managed to steer clear of Java. I was comparing it to system package managers and package managers for languages like Python, Ruby, Rust, and Haskell.
Just post "I made a thing that does x, but I think I maybe did it poorly. Can anyone give me pointers?" It's ok, Rust doesn't really have a toxic community yet. (Which to this day baffles me a bit, because the ideas behind it are super smart and usually that turns into an elitistism situation) But actually, we want you to succeed. Specifically, we want *you* to succeed. Let us help you, give us the gift of practicing giving advice, and tell us when things aren't clear so we can improve. We're all just meat bags pounding on keyboards trying to figure shit out, we're in this together.
There's talk of using similar tech with QUIC.
One of many things to make my day and honestly that entire comment deserves whatever upvotes it gets (: I don't have a lot of skill with the internet or people in general, somehow, so I thank you all very much for making this very easy on me. &amp;#x200B; Here is a good question: Is there a better way to parse things? And is it bad that I parse it twice (once from text to Commands, and then from Commands to output)?
Sweet, congrats on the launch! I'll be trying this out in the future. Could you provide an overview of what ugdb provides over cgdb? 
Can this drive LLDB via MI?
Re parsing: I'd say it depends on the problem you're trying to solve, or the end product you're trying to build. I think I'm somewhat clear on the solution but pretty murky on the problem. In general, the bread and butter of most software I've seen in the past 30 or so years is to take some input, structure it into something the software can understand, then do something with it. So transforming from text -&gt; structured data -&gt; text seems like a good approach to me, and probably a definitive object-oriented approach. Re social skills: all good, we all gotta start somewhere and we all have our awkwardness
I'd wait for JAI instead. Or maybe look into D. Rust isn't all that interesting for complex high performance real-time systems like video games; for highly constrained systems like embedded systems on the other hand I think Rust is great.
Yup, turns out that the draw_state stuff I was doing was correct. The helper functions that were introduced in piston don't clip, so I had to use the implementations that the helper functions used. As soon as I did that, the issue disappeared. It was definitely the use of scissor, which works great!
This looks great, I'll give it a try. I'm also working on a GDB front-end (a GTK GUI). The main problem I'm trying to solve is, normally with plain gdb (or even with gdb-dashboard) you have to do so much typing to even the simplest things. I usually need to watch a a few stuff, with sometimes a dozen breakpoints. I want to print the watched values after every breakpoint hit. This is just too painful for me. Investigating the stack frames, locals etc. also need so much typing. So in my GUI I'm showing all these information by default, requiring 0 typing. &amp;#x200B; In addition, I've added a new kind of "watchpoint" in my GUI for when you interested in value of an expression, but do not actually watch it for reads or writes. So the value is updated after every breakpoint it, but otherwise the expression itself does not add any new breakpoints or watchpoints. &amp;#x200B; I'll now try ugdb, I don't know how much of these stuff it already does but perhaps I can contribute to ugdb instead.
I am very interested in your reasoning behind your claims. Especially why a GC'ed language is inherently better for "real-time" and how rust falls behind in the performance category.
That's because you skip lifetime of TestError in return Type it's elided with '_ (anonymous lifetime that &amp;mut self have) so Result take '_ lifetime if you have to change like this [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=7219adc0078ee64b11a1e41e056300f7](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=7219adc0078ee64b11a1e41e056300f7) or `Result&lt;(), TestError&lt;'static&gt;&gt;` you mentioned
If you use the global allocator interface, yes. What I'm referring to there is just allocating and constructing objects yourself.
Should items tagged with ctor/dtor be marked unsafe?
Hi, to\_vec() and to\_string() copy everything, you might want to return slices [playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=518d04f342f5ab12d3deeb4831a574b0)
TIL Mb and MB are not the same thing
Is the hashbrown implementation replacing the stdlib one?
It's funny, what I was first writing my comment I was going to include a line about that. However when I went to find something backing that up I couldn't. To be fair I didn't look very hard.
As /u/ErichDonGubler correctly points out, the termion dependency means that it only works in ANSI terminals. However, due to the way unsegen is structured, it would not be *too* much work to add Windows support via some other terminal library. One would only have to touch (possibly translate) how input is represented and how the internal buffered is actually rendered to the terminal. As I don't have a Windows machine at hand, I will probably not implement this all by myself, but would be open to contributions that add Windows support.
First off thanks for all the great responses. I didn't really expect so many comments. It seems like you guys have a great community. I'd like to say I'm not adverse to doing things the Rust way. I was mainly describing what I do in C++ to give some idea of what I need. I started programming in Fortran IV, then C, then did years of assembly language so I tend to breakdown higher level concepts into low level concepts in my head. For instance, to me a reference and a pointer are nearly the same. I mainly use pointers in C++ because when looking at the code they don't disappear so much. I do use references when specifically passing out parameters from functions. That being said, I think there are certain things that are pretty much a requirement for me. For instance I don't see a good way around using some form of reference counting. My voxels are built as an octree and are very explicit. Voxels have shared walls, walls have shared edges etc... Trying to manage memory without reference counting (or a GC) would be a nightmare. The reason I'm thinking about Rust is it's one of the few languages that doesn't push you into using a GC, although I use a GC for one specific data structure because it needs aging, but it's a design decision not something I was forced into. Also speed is very important for this project. The reason is there is little actual data on disk. The world is built via functions (simplex noise, voroni etc) at run time. Also these aren't Minecraft style voxels. They generate smooth terrain. As a player moves around, the world is built and un-built. This is because I'm targeting actual planet sized worlds so there is no possible way to fit data on disk. I'm constantly doing all sorts of computation that isn't required in a normal game engine or even most voxel game engines. Also I'm doing a lot of allocation and deallocation but almost all of it is done with free-lists in the heap code and in most cases that is inlined. There are all sorts of tricks being used to get this thing fast enough so I can't really afford to give up much performance. Given some of the current limitations that some of you have outlined in Rust, I think it would probably be better for me to try it out in one of the side tools I still need to write that isn't so performance critical. I plan to write a voxel tree builder tool. I can wrap my voxel stuff in a plain C interface and I should then be able to call it from Rust. Thanks again for all your input!!!!
&gt; I find it awesome ! As gdb feels more user-friendly to emacs users, this feels really great for vim user like me. :) Great to hear! :) &gt; As I'm mostly working with c++ right now, I recommend implementing a move / resize (maybe kill?) a window, that would be really awesome. Could you maybe elaborate how working with c++ wants you to kill/move/... a window? I've been thinking about adding window layout configuration via a config file, but haven't gotten around to it, yet. Maybe that would help? &gt; Also, if you switch to assembly while debugging and single step instruction, the window goes back to source code also, which is probably not what's intented. Oh, right! I've been meaning to fix that before the release, but somehow forgot about it, apparently. Maybe I'll get around to it this afternoon. &gt; I feel ashamed not being able to code in rust, I'd really like to help on this project because I think it'd probably improve a lot my usage of gdb. I would really appreciate it!
I didn't know about this crate! I think it might solve a problem I have.
Here is a really quick comparison using the features section of the [cgdb website](https://cgdb.github.io/) as a basis: cgdb provides, but ugdb does not: * Searching source window (using regexp) * Tab completion * Key mappings (macros) ugdb provides, but cgdb does not: * Builtin TTY * IPC for settings breakpoints for the current line from vim * Expression table Additionally, both cgdb and ugdb provide some form of mixed assembly/source view, but with a somewhat different concept. cgdb interleaves source and asm, while ugdb has two panes with linked positions, inspired by [https://godbolt.org/](https://godbolt.org/). I tend to prefer the latter, although I guess that's hardly surprising. ;) I've actually been using cgdb before I started ugdb. The primary motivation was that they removed the separate TTY window and I REALLY did not feel like going through the dance of opening another terminal window and attaching to it every time, especially since the application I was/am debugging quite often, on occasion requires user interaction via the terminal.
Given the warning - "The code that runs in the ctor and dtor functions should be careful to limit itself to libc functions and code that does not rely on Rust's stdlib services." - the macro should disallow functions not explicitly marked unsafe.
I've actually never tried that up until just now. Currently, out of the box it looks like it doesn't, although a cursory glance suggests that this is due to lldb not liking some command line argument that we are passing to gdb (probably -tty). That might be fixable if lldb follows the gdb mi [spec](https://sourceware.org/gdb/onlinedocs/gdb/GDB_002fMI.html). Funny story: gdb does not and I've had to incoporate more than one workaround to deal with that...
"Bad" `ctor`s and `dtor`s can cause panics, but can they cause UB? If so, it should be the _implementation_ that needs to be unsafe, not the function. But if there isn't a trait, I'm not sure if that can be expressed.
&gt; My voxels are built as an octree and are very explicit. Voxels have shared walls, walls have shared edges etc... Trying to manage memory without reference counting (or a GC) would be a nightmare. The reason I'm thinking about Rust is it's one of the few languages that doesn't push you into using a GC, although I use a GC for one specific data structure because it needs aging, but it's a design decision not something I was forced into. One thing to note is that for collections of similar connected objects like graph-like structures, arenas have been used as alternatives to reference counting. There are multiple implementations that I could find and I'm not sure which would be best, but it could be worth looking into. It might be worse than reference counting, but you can't really know until you've tried it.
&gt; In addition, I've added a new kind of "watchpoint" in my GUI for when you interested in value of an expression, but do not actually watch it for reads or writes. So the value is updated after every breakpoint it, but otherwise the expression itself does not add any new breakpoints or watchpoints. Looks like we had a similar idea. You can try to put values that you want evaluated over and over into the expression table (the top right pane). &gt; I'll now try ugdb, I don't know how much of these stuff it already does but perhaps I can contribute to ugdb instead. Contributions are always appreciated. :) I do want to note that there is no special support for "regular" watchpoints at the moment. Can you elaborate what you did with them in your front-end? Also: I guess you are using the [gdbmi](https://sourceware.org/gdb/onlinedocs/gdb/GDB_002fMI.html) interface as well (?). If so, how did that go? Are you as frustrated as I've been at times? ;)
That's very nice to hear! I've actually put quite a lot of effort into properly documenting unsegen before the release. It would be really nice if that pays off. :)
I'm not sure if they can. The proc macro can enforce that whatever it's tagged with is unsafe, thiugh.
Oh, and autocompletion would be really cool indeed, although I don't really know how to go about that so far. I don't think gdb helps you with that in any way, unfortunately. I will have to look into how other projects deal with that.
Thank you very much.
&gt;My voxels are built as an octree and are very explicit. Voxels have shared walls, walls have shared edges etc... Trying to manage memory without reference counting (or a GC) would be a nightmare. I am not sure if I understand your approach. Keeping all voxels/walls/edges behind reference counting pointers does not look optimal. Shouldn't be managing of voxels in big chunks be more efficient? So you would create and clean-up voxels not one-by-one, but in much bigger chunks. I've noticed several times that Rust sometimes forces you to rethink architecture, initially it seems like a needless churn, but the final result is often significantly better than the state you start with.
I read your post and the inside comment. I would say- don't worry about power. Rust can do absolutely anything, you'll just need to use the unsafe construct. Unlike others- I would say, use unsafe liberally (but very carefully and well-tested)- there is no shame there. Since you say you've programmed in assembly before, I would say you have nothing to fear about Rust, you will be very comfortable using 'unsafe' 
&gt; Can anyone give me pointers?" let m=&amp;a; 
I'm not familiar with JAI. My understanding of D is that it forces GC upon you, like so many other languages. Also I was hoping to try something out that has at least some level of traction. I guess this is why C++ has survived so long. There hasn't been anything that really challenges it in the areas it's strong. 
That's a reference, sir. 
```rust impl Command { pub fn new(verb: String, hex: [u8; 4], int: isize, rep: usize, unset: bool) -&gt; Self { Self { verb, hex, int, rep, unset } } } ``` You even can use `Self` in the constructor. The `clippy::use_self` Lint warns you about that, if you turn it on. Subtext: you should use Clippy, if you don't use it already ;) 
Yes. But this makes it impossible to use containers as they use global allocator.
`std::shared_ptr` is not exactly `std::sync::Arc` `std::shared_ptr` contains two pointers, one which it returns from `get()` method and one that it use for doing reference counting and destruction. `std::sync::Arc` works similar to `std::shared_ptr` constructed with `std::make_shared` which places both value and counters into single object.
Isnt only the global allocator API stable?
We making not voxel based, but game engine in Rust. And we use lots of `Vec`s with indices instead of passing shared pointers around and storing them. Both for performance reasons and cleaner mutable access. We also never had any issues with default global allocator. Partially because we avoid too much allocations in the first place, partially because default allocator is nice. Also I'd like to point that calling `VirtualAlloc` is much slower than calling any other general purpose memory allocator, so if I understood you correctly and you call `VirtualAlloc` on every allocation you'd better not to ;)
Not really, as long as function is not calling some unsafe code, it is perfectly safe
This is really useful as Rust really sucks at statics right now
I wish clippy would support dynamically linking user-defined lints.
already tried that yesterday and it didn't work. The other comments worked great though :D
You can try using the `packed_simd` crate to get portable SIMD across all architectures :)
Ah. No one ever has to explicitly call the ctor or dtor so actually it's not _too_ weird to require the function to be unsafe, in the absence of an unsafe trait to implement.
Yes, there's serious work going into it here: https://github.com/rust-lang/rust/pull/58623
Just as a note: AFAIK Java‚Äôs direction/future is *not* dictated by Oracle. The reference implementation is OpenJDK (an open source project steered by many companies including Google, Amazon, IBM, Oracle, Red Hat, etc). All other JDKs take the OpenJDK and ‚Äútweak‚Äù with a small amount of ‚Äúsecret sauce‚Äù
There is in fact chunking but it's done for different reasons. First I only send down a new chunk to the GPU when it changes. Also chunks are divided into two sections, a border section and a core section. If a neighboring chunk changes I may have to send down the border section of this chunk (which may contain resolution transition voxels) to match, but I don't send down the core. The second reason for chunking is LOD (Level Of Detail). As you get farther away from the camera the voxels get larger and larger. Also chunks in those voxels get larger and larger so the memory usage of each chunk is roughly the same. This is necessary because the scale we are dealing with is so large. Keeping chunks of the same size would really make them useless at distance. They would soon all contain a single voxel and that really isn't good for anything. The program constantly goes though a re-chunking process as needed. Chunks are split and merged. Finally the chunks themselves are octrees. Only leaf voxels have geometry. All this is done to save memory. I need to hold a whole planet in memory. It doesn't really rework the entire planet as you move it just tries to fix up the parts that change so the reference counting is only used on a subset of the voxels on every iteration. I'm not claiming any of this is the optimal solution only that it seems to work OK right now. The CPU intensive stuff is really the fractal functions and actually looking for where geometry exists in 3D and that's a whole different optimization. For instance it builds light weight trees in separate threads (I call them ghost trees for lack of a better name) and then when it finds geometry it converts them to the real voxel trees. There is also a lot of cacheing of fractal functions values and that's made more complex by the fact that I'm looking for data in a sphere near the surface of the planet. It has a special barycentric coordinator system based on an icosahedron. Actually the voxels aren't even cubes. They are prisms which is the other shape that's easy to make into an octree. This is done to keep a roughly even voxel density and orientation around the planet. OK, so It's kind of a monster, LOL .... but well..... I guess it works, so I'm leaning towards keeping what I have and just cleaning it up. If that involves some significant code change that's OK, but at least for now I want to try to roughly keep the data structures and algorithms because I've messed with them for a long time and now they are working decently. 
What parts of Rust doesn't allow you writing high performance real-time systems? Especially since `unsafe` superset allows you to express same semantics as C does.
I'm trying to write some code which will essentially pipe the io from a process and read/write it to a websocket. I'm using actix-web for the server and trying to find some way to asynchronously read/write to the process. I found tokio-process, but I'm not sure how to put it into practise when combined with actix.
I meant [The Book](https://doc.rust-lang.org/book/).
I honestly have the opposite opinion. C++ got a lot of better since C++14, where important design flaws of C++11 have been corrected. It gets more similar to Rust, especially with `unique_ptr` and the move semantics around it. More importantly, C++ *allows*, while Rust *forbids*. I still cannot decide which is better. After a lot of thinking, I tend to let programmers simply do the ugly stuff and use tools to point out flaws, instead of forbidding many techniques upfront.
While we're super nitpicky, let me add that Arc&lt;T&gt; is probably closer to `shared_ptr&lt;const T&gt;` than to `shared_ptr&lt;T&gt;`. :)
Could you please clarify the safety story here? The docs mention some restrictions on the implementation of ctors/dtors, but do not seem to mention the relationship between those restrictions and memory safety. Even if I've doesn't exist at all, I think it might be worth explicitly addressing it.
It seems like calling rust apis here is bad, it's unclear how bad it can get.
Could you please publish ugdb on [crates.io](https://crates.io)? Seeing that it uses just a regular `cargo build` workflow makes it suitable for installation via simple `cargo install ugdb`.
To my understanding D is optionally Garbage Collected. The issue is a large portion of the standard library was written under the assumption that you used GC. If you don't you're limited to a subset of the standard library. Last I heard there was an effort to clean up the standard library for use without the GC.
I don't think this is right. There is nothing _unsafe_ about the function itself. E.g. you can call this function from safe code and be certain of no UB. This is more akin to implementing an unsafe trait (like `Drop`) - the onus is on the implementor to uphold any assumptions made by the unsafe code in the ctor crate.
The thing is the subset Rust allows is trivial to machine check, while the subset C++ allows is probably not possible to find all the errors in
Exactly
And Rust has been designed to be used with whatever strategy you like. It's way ahead in adoption and flexibility
oops
This implementation also existed in my `slotmap` crate but I removed it with the introduction of `HopSlotMap`. Since then I've gotten multiple requests for it so I will bring it back when I have some time to work on the crate.
Agreed, which is why I'm learning it instead of D (I'm in embedded, where it seems like a good fit). Just trying to correct the "D forces GC upon you" that the OP posted.
Hm, I just looked into it, but it seems like cargo does not allow publishing anything that has a local dependency within the repository that is not on crates.io as well: ``` &gt; cargo package Packaging ugdb v0.1.1 (/data/dominik/git/ugdb) Verifying ugdb v0.1.1 (/data/dominik/git/ugdb) Updating crates.io index error: failed to verify package tarball Caused by: no matching package named `gdb-expression-parsing` found ``` I don't want to publish these crates on crates.io because they are really not ready for public consumption. This is especially the case for `gdb-expression-parsing` which I only split off into a separate crate because of long compile times.
Well pretty sure anything related to panic/unwinding is bad because these facilities are set up before main gets called. By I imagine anything else would be fine
It does rather sound like a lot of the 'pro non-GitHub' supporters are coming from a place of wanting to use something less secure, which I'd argue is tantamount to debating in bad faith when discussing authentication.
&gt; There's the standard library's `Rc` and `Arc` equivalent to `std::shared_ptr`. Almost equivalent. Neither `Rc` nor `Arc` authorizes internal mutation, while `std::shared_ptr` does. `Rc` and `Arc` are more akin to a shared `const_ptr` (I‚Äôm not even sure C++ has that, and you can still cast-const it away)‚Ä¶ The `std::shared_ptr` equivalent in Rust is `Rc&lt;RefCell&lt;_&gt;&gt;` / `Arc&lt;RefCell&lt;_&gt;&gt;` + `borrow()` and `borrow_mut`, but again, that depends on what you need. If you don‚Äôt need mutation‚Ä¶ then you don‚Äôt need the `RefCell`.
\&gt; I still need to call C++. &amp;#x200B; deal breaker. The friction between C++ and Rust will negate any potential benefits. It's only viable if you can wrap your C++ in C . (Rust/C FFI is seamless) besides that, Rust can do everything. 'placement new' might be kind of covered by customisations to a box operator I think (work in progress , or is it ready now?) - I think they did understand and have a plan for expressing custom allocation \*within collections\* (similar to C++ emplace\_back)
I will defer to you guys on this. It's just from my reading but I'm happy to be wrong. 
Yes. I understand it. My point is, why do you like to program in Turing complete languages, if it's proven impossible to verify correctness? 
The one thing to be wary of is that if the underlying dependency experiences breaking changes, it may inadvertently cause you to introduce breaking changes to your crate without intending or realizing. It's not unheard of to re-export types from other crates, but you can also consider the newtype pattern here if you want more control over your public API.
I use VirtualAlloc for large chunks only. The main advantage if VirtualAlloc is on 64 bit machines because the address space is large. You can guarantee large contiguous blocks of memory without actually allocating the memory. You first allocate some address space and then page in memory as you need it. The actual heap is simply built on top of VirtualAlloc. After the program has been running for a while, most allocations are simply free-list pops and are even inlined. 
Sometimes it is unavoidable to have external types as part of your API, and then the breaking changes are a problem whether you re-export or not. So I think the question is: given that the external types are part of your API, should you re-export those types as well, or should you require the user to import the types from the original crate? My opinion is that it can be much more convenient to re-export because that ensures crate versions are matching, and I don't see any downsides to doing so.
Yes, but note that upgrading 3rd party crates falls under semver too, so you cannot upgrade to breaking version.
So another real quick question: I have a module that imports some enums(Token, TokenType, DataType) from another module. They're all imported via node::{Node, NodeType, DataType}. In one function I'd like to not have to write absolute definitions of all the variants though. So instead of \`NodeType::Num(DataType::Int(l)), NodeType::Num(DataType::Int(r))\` I'd like to just write \`Num(Int(l)), Num(Int(r))\`. Can I put \`use NodeType::\*;\` etc. only in the scope of a function?
If I'm not mistaken, most people tend to reexport just the crate via `pub extern crate typenum;`. This allows them to use the types just fine through your crate. Relying on others to use the crate isnt the best cause if they use a different version of said crate they might run into compilation problems and the like.
Also: Can I assign a variant that can hold data to a variable? What I want to do is: Assign Enum variant that the final result will have to a variable \`a\` based on pattern in some variables \`l\` and \`r\`. Find pattern in variables \`l\` and \`r\` and do some calculations that result in \`x\` - then do a return of \`a(x)\`. It's guaranteed that the enum variant can hold the data in \`x\`
&gt; I use VirtualAlloc for large chunks only. This "large" should be really big one for good performance. Few pages is not big enough ;)
I'd say the distinction between the two is that if you export the types directly, when you upgrade the major version of that dependency you *must* increase the major version of yours (because a downstream user may be relying on version-dependent compatibility of multiple dependent crates, which may no longer exist), whereas if you use newtypes you merely *may* have to increase the major number, if the API has actually changed somehow.
Right, but the point I was trying to make is that there are actually 3 options: use in API and don't re-export, use in API and do re-export, or don't use in API and use new-types instead.
One additional twist: the hashbrown crate uses fxhash or whatever for the hash function, so this move won't get \*quite\* the same boost as the blog post by default, as it'll still be using siphash by default in the stdlib. You can of course still choose to use that hash too, but it's still going to be another step.
Sure. That doesn't prevent you from writing your own allocator anyway.
&gt;I was mainly describing what I do in C++ to give some idea of what I need. I think what people are trying to caution you about is that Rust's rules tend to influence architecture. So, if you expect to be able to write the same code the same way, you may end up with more frustration than if you tried to go \*with\* the grain of the language. There are usually ways of accomplishing the same tasks, just in a different way than you might be used to. &gt; For instance I don't see a good way around using some form of reference counting. My voxels are built as an octree and are very explicit. Voxels have shared walls, walls have shared edges etc... Trying to manage memory without reference counting (or a GC) would be a nightmare. So, for example, here, the "vec + indices" strategy of something like [https://github.com/fitzgen/generational-arena](https://github.com/fitzgen/generational-arena) \*may\* be an alternate approach that works better. It really just depends though. &amp;#x200B;
The Book is 520 pages printed.
Jai is a programming language that hasn't been released yet.
Interesting responses so far, I hadn't even thought about the concerns mentioned here so far, although they are certainly valid and relevant. I was coming more from a "moral" perspective. From what I've seen, re-exported types are not really distinguishable as such (e.g. in the generated documentation), so I thought it might be "frowned upon" so to speak. All I use are the \`Unsigned\` trait and some unsigned integer types and these are likely going to be replaced by const generics in the near future, anyway, in my crate. From what I've read here, it seems to be totally fine to re-export 3rd party types and perhaps even necessary to ensure compatibility in the face of potentially changing 3rd party libraries. &amp;#x200B;
Interesting responses so far, I hadn't even thought about the concerns mentioned here so far, although they are certainly valid and relevant. I was coming more from a "moral" perspective. From what I've seen, re-exported types are not really distinguishable as such (e.g. in the generated documentation), so I thought it might be "frowned upon" so to speak. All I use are the \`Unsigned\` trait and some unsigned integer types and these are likely going to be replaced by const generics in the near future, anyway, in my crate. From what I've read here, it seems to be totally fine to re-export 3rd party types and perhaps even necessary to ensure compatibility in the face of potentially changing 3rd party libraries.
There _are_ languages where #2 is a feature, though? In ML, Haskell, and CoffeeScript, function application is juxtaposition, so you can write the sine of two either as `sin 2` or as `sin(2)`. The traditional `f(x,y)` is then `f` applied to the 2-tuple `(x,y)`. The issue is associativity of application: is `f g h` the same as `(f(g))(h)` (ML and Haskell) or `f(g(h))` (CoffeeScript)? The former works better with curried functions (`map f xs`), the latter is closer to conventional mathematical notation (`sin arccos x`).
Nevermind, I changed the in-repo crates to be modules again for the time being. Compilation times are not of concern as much anymore: We have incremental compilation within a crate now. ugdb is now published on [crates.io](https://crates.io/crates/ugdb)!
Awesome, thank you!
This is really great work! I'd love to have tool built only around `unsegen_jsonviewer` + `jq` with interactive queries updating the view. 
Ideally, on the C side, there would be a `void *` parameter to keep the function context. For instance, ideally the function would look as follows. void function(void (*callback)(int, void *), void *ctx) { callback(10, ctx); } Then we could call it as follows. use libc::{c_int, c_void}; extern "C" { fn function(callback: unsafe extern "C" fn(c_int, *const c_void), ctx: *const c_void); } fn function_rust&lt;F: Fn(c_int)&gt;(f: F) { unsafe extern "C" fn wrapper&lt;F: Fn(c_int)&gt;(value: i32, ctx: *const c_void) { (*(ctx as *const F))(value); } unsafe { function(wrapper::&lt;F&gt;, &amp;f as *const F as *const c_void) }; } fn main() { let x = 1; function_rust(|y| println!("{} + {} = {}", x, y, x + y)); } This doesn't handle unwinds, which means this [will abort when Rust panics](https://github.com/rust-lang/rust/issues/58794). Well, specifically it's UB currently, but there are plans for this behaviour to change. It's possible to handle panics with `std::panic::catch_unwind` and `std::panic::resume_unwind`. If there is no context parameter, it's possible to store the function in a thread local variable. It's not preferable, and will break when the C program spawns a thread. use libc::c_int; use std::cell::Cell; use std::mem; use std::process; extern "C" { fn function(callback: unsafe extern "C" fn(c_int)); } fn function_rust&lt;F: Fn(c_int)&gt;(f: F) { thread_local! { static CTX: Cell&lt;*const dyn Fn(c_int)&gt; = Cell::new(&amp;|_| { println!("C function spawned a thread or stored a function pointer"); process::abort(); }); } unsafe extern "C" fn wrapper(value: i32) { CTX.with(|ptr| (*ptr.get())(value)) } CTX.with(move |ptr| { // Erase lifetimes let f = unsafe { mem::transmute::&lt;*const dyn Fn(c_int), *const dyn Fn(c_int)&gt;(&amp;f) }; // Storing the old value to handle recursion let old = ptr.get(); ptr.set(f); unsafe { function(wrapper) }; ptr.set(old); }); } fn main() { let x = 1; function_rust(|y| println!("{} + {} = {}", x, y, x + y)); } 
It's some size you set when you create the heap. I can set it to whatever. It rounds it up to a page boundary. 
Reexporting will still be a breaking change, because if I happened to have constructed a typenum directly or via some other dependency that did something similar (since I know what you use underneath‚Äîyou exposed it, after all), it ceases to be compatible with your crate.
As an occasional game dev, I'm not sure which is scarier... the fact that this game needs its own project management software to play, or that I want to give it a try... Though I play Factorio, so I suppose I can't throw stones. :-P
Rust‚Äôs Macros are hygienic now? I thought that was still something that had to be done for the very final version of Macros 2.0 to be considered complete? I lost track of the Macros 2.0 situation a while ago.
\`macro\_rules!\` is (mostly) hygenic. "macros 2.0" was a code name for procedural macros. They are not currently hygenic, that will be added in the future, yes.
Yes.
Not really. Enum constructors are functions and you can manipulate them just like any other function object in Rust. So you can `let a = MyEnum::Int;` However there is no type of `a` which is consistent with both execution paths. There are tricks that can be done with dynamic dispatch and/or the traits in the `num` crate. https://github.com/rust-num/num However, I'm not sure that abstraction will improve the readability or performance of your code - it very well might introduce runtime checking or the opportunity for operators to mysteriously do something unexpected. If it doesn't have those costs it will likely create type errors for the same reason.
Maybe it should be called `#[unsafe_ctor]`? Seems polite to uphold the invariant that you can find potential memory safety issues by grepping for `unsafe`.
Yes, that's why I like type safe languages do that my programs are safe from certain types of errors. Like null pointer deferences (goodbye, Java, C++)
I love those instructions! Someone built a really fast CRC32-IEEE implementation with them as well: https://github.com/srijs/rust-crc32fast
This is not universally agreed upon, though many agree with you.
Let me tell you about Fractal that is written in Rust and crashed on me with a segfault. Crappy software is in most cases produced by bad programmers and not by bad tools. Once again C++ has many tools ready to be as safe as with Rust, *if you opt in*. This is what I wanted to say.
True about `Rc` and `Arc`, but that's gonna come up in general and probably even deserves its own section here. Rust doesn't have a straightforward equivalent to C++ pointers as far as mutability is concerned. I don't know that I'd straight to `RefCell`, though- that's a far bigger hammer than you usually need, introducing its own counter on top of `Rc`'s. `Cell`is much cheaper, and will feel much closer to how C++ pointers work. ...with one caveat, which is that Rust doesn't have struct field projection through `&amp;Cell&lt;T&gt;`, even though it's safe. This means you often need to put `Cell`s in your types (often annoying, but probably fine here where the OP is already baking in ref counting). It also means you sometimes need to do the `.take()`, method call, `.set()` dance, like when cloning a `&amp;Cell&lt;Rc&lt;T&gt;&gt;`.
The other significant difference between Rust safety features and "modern" C++ safety features is Rust's are compile time and zero cost. Whereas C++ they are runtime and can in some situations be far from zero cost and if you need that performance you have to drop the safety. 
&gt;if that's his definition of GC, then wouldn't C have GC? Because when you compile C, it creates the machine code equivalents of decrementing the stack pointer when a subroutine terminates. Right? In some sense, yes. This is why the C standard calls them variables with the "automatic" storage class. &gt;GC *should* be considered runtime management of heap allocations by something other than the running application IMO. I think this is a reasonable definition too! I'm not sure that it's the one that most people use. Then again, neither is mine; or rather, it really depends on who you mean by "most people". My point with this essay was more to get people thinking about this stuff and what it means, and how it applies to langauges, than it is to say that somehow I am in charge of what these words mean. Being a bit provocative can help.
r/playrust
&gt; Can you elaborate what you did with them in your front-end? Sure, for a "regular" watchpoint I just create a variable (with `-var-create`, instead of a watchpoint) and then update all variables every time execution stops (with `-var-update`). I'm showing watchpoints (with read, write, or read+write settings) and "regular" watchpoints in the same widget. It looks like this: ``` Expressions --------------------------------- Expression | Type | Watch foo | int64_t | &lt;-- NOT a watchpoint! just a gdb variable bar | bool | "r" &lt;-- read watchpoint baz | char* | "w" &lt;-- read+write watchpoint ``` User can click to the "watch" column and add a watchpoint for an expression, or disable a watchpoint and make it a variable. &gt; Also: I guess you are using the gdbmi interface as well (?). If so, how did &gt; that go? Are you as frustrated as I've been at times? ;) Yep, and it's not great :) I found some bugs and reported them [here][1]. It was frustrating at times but thankfully it's simple enough so after a few hours I was basically done with the mi syntax parsing. To me the biggest frustration is how hard it was to do parsing in Rust. nom is terrible, and lalrpop-generated code is too slow to compile. I ended up rolling my own parser without any libraries and the experience was terrible. [1]: https://sourceware.org/ml/gdb/2019-01/msg00046.html
Nobody is stopping you from using `unsafe` in Rust. That puts you in C territory. If it crashed without `unsafe` it's a compiler bug
The counterpart to this is that if you expose types from an external crate as part of your API, and that crate releases a new version, people who try to use the newer version with your API will have it mysteriously not work. Whenever I do this I try to make sure I re-export the original crate entirely, so people can always easily use the version you expect. 
I consider C++'s smart pointers a complete different beast, I don't see how devs keep comparing them as almost equivalent to Rust's pointers, they're a lib only facility, no builtin compile-time guaranties, etc.
Well my practice is _always_ re-exporting external crates rather than types alone. So this is no-issue as long as're taking care with how you place 3pp items in your public API. P.s. I really hate when people don't re-export crates entirely 
Ohh I C. 
You might find my lldb-sys and lldb crates useful ... and I'm happy to help resolve any issues that you have with them if reported!
This is the official position of those involved, yes. I find it a bit hard to reconcile with the ongoing [Oracle *v* Google](https://en.wikipedia.org/wiki/Oracle_America,_Inc._v._Google,_Inc.) adventure.
Rere parsing: yeah, my solution works, so I see what you're saying. I guess the problem is unintended behavior. Parsers are a pretty wonky thing, it seems. Rere social skills: üôÉ Lol. 
I'll do some thinking about what safety means from the perspective of the crate. I believe as long as you only use the `ctor` methods you should be memory safe, but the `ctor` statics are a newer bit of code and need me to really think about their story. There's also the concept of "std safety" where you could hit minefields where parts of `std` aren't initialized before trying to use them (or are torn down before you need to call them!)
I am not 100% sure if this code is available to regular `std` library users now. I'll research this and update the docs if it isn't. 
&gt; There's also the concept of "std safety" where you could hit minefields where parts of std aren't initialized before trying to use them (or are torn down before you need to call them!) Can this cause memory unsafety? (Sounds like it?) If so, then ctors _must_ be declared unsafe.
What unintended behavior do you get?
I'm not defending Oracle (not a fan really), but the case, if I'm not mistaken, was that for you to make your own Java-compatible VM and put the "Java" logo on it you need to get Oracle certification and Google didn't do it. I think now Google uses OpenJDK for Android.
This is tricky as there are clearly `unsafe` things that can happen in a `#[ctor]`, but there's also things that are more along the lines of "wrong" - ie: using bits of the `std` library that aren't initialized yet. There might be ways to prove that the code is "not wrong" inside the `#[ctor]` using the proc macro that generates each function. I could explore a `#[ctor(unsafe)]` model for code that wasn't provably correct for a 1.0.0.
No - at least in the cases I've encountered, you get panics rather than UB. I'm not averse to just forcing all `#[ctor]` functions to be `unsafe`, although I do have a small concern that this automatically hides code that would normally be in an `unsafe` block and should be audited more closely. I'm definitely open to design help/patches/etc on this!
I begun developing this almost three years ago, when I was still learning Rust. I've been using it daily since, along with [my own X11 window manager](https://github.com/mjkillough/lanta) which is a little rougher around the edges. I thought I'd fix it up (and upgrade to Rust 2018!) and share it in case it's of interest to others. :) I started the project because I found many other status bars relied on polling external programs to update their state (such as polling to get the volume/mute status), and this made them both unresponsive and resource intensive. I wanted to write something which could listen to external events to update itself only when needed. (Although it does unfortunately still poll in a few places). I liked projects such as QTile and dwm which allow users to customize the tool using code (rather than external configuration/scripting), and so created it as a Rust library which could be used/customized in a simple command-line wrapper. The project includes a simple example in `bin/cnx.rs`, which is the configuration I run it with. The project uses `mio`/`tokio` in order to handle the asynchronous events/updates coming from each of the widgets. I haven't been keeping up with the latest developments with async Rust, and one of my TODOs is to try porting it to use the new std `Future` and seeing how it works out.
Actually, none yet, but I feel like several matches in the same scope and using the same variables is bound to have some unintended consequences sooner or later. But I'm pretty sure I catch most errors with generic output so maybe it's not as bad as I would think. I'll probably rewrite it someday. 
Actually, none yet, but I feel like several matches in the same scope and using the same variables is bound to have some unintended consequences sooner or later. But I'm pretty sure I catch most errors with generic output so maybe it's not as bad as I would think. I'll probably rewrite it someday. 
gdb does do autocompletion for fully qualified symbols though...
Sure: struct Foo&lt;'a&gt; { x: Vec&lt;u8&gt;, y: Option&lt;&amp;'a [u8]&gt;, } let mut z = Foo { x: vec!(5), y: None }; z.y = Some(&amp;z.x); The problem is that afterwards, the borrow checker will prohibit almost everything with that struct, except dropping it, so it's almost useless. Btw, there are some useful crates for doing these kinds of things, which all predate `Pin`, e g [ARef](https://docs.rs/reffers/0.5.1/reffers/aref/struct.ARef.html), OwningRef and Rental.
I'd take a peek at [https://github.com/rust-lang-nursery/futures-rs/issues/683](https://github.com/rust-lang-nursery/futures-rs/issues/683), but the gist of it that you should use \[\`futures::either::Either\`\]([https://docs.rs/futures/0.1.25/futures/future/enum.Either.html](https://docs.rs/futures/0.1.25/futures/future/enum.Either.html)) for an early return. It'll be easier once async/await lands.
Well that's super-cool. Very clean codebase that highlights the power of Rust in working with X stuff. Many thanks for sharing it!
This may be a disadvantage, but also shows that you can implement the behavior with the means already provided. I like both ideas, smart pointers in C++ and the equivalent native move semantics in Rust.