I am not saying that actix is the example to follow. On your specific example I feel that if you have a type `Foo`and a module `foo` then the type should not be inside the module but beside it. Then I understand that `crate::foo::*` contains utilities for, or parts of `crate::Foo` which are generally only needed in specialized cases. If the only thing you want to expose is the type (but the module contains the implementation) the module itself shouldn't be public, only the type inside which is then reexported outside. This seems to be the case, and the stuff contained in the registry module seem to be very specific functionality that 90% of the time I won't need. What do I need to know about the different implementations that a registry has and how to specify I want to pick one, when registry is already an add-on functionality (it didn't seem to be used on the examples and appear at the end of the guide?), and then even when I do use them I don't really care much about the internals it seems. If I wait to use a registry but see 6 versions why did one get chosen over the other? By moving those into an internal module I realize they are internals of the library and that the code is doing something tricky. Again this is an issue when I'm reading code someone else wrote and I'm trying to understand it.
Yeah, i am also pretty amazed by this. Run the example without any problem – i was a little bit surprised by this coming out of nowhere.
Can someone please explain me the idea behind packaging packages from other package manager, to make distro-specific package? I mean, we already have package manager for rust, and if you want to install some crates you just install them. Same thing with `pip`, `npm` and other package managers for different languages. I mean that even vim plugins are often shipped as a system package, and I have no idea why. This breaks updates, or delays them because it is upon your distribution maintainer now, not on package maintainer. Despite the fact that distribution may care about stable packages, I've thought that this leads to all kind of problems, and it is way better to use such *tool specific* package managers.
&gt; On your specific example I feel that if you have a type Fooand a module foo then the type should not be inside the module but beside it. Then I understand that crate::foo::* contains utilities for, or parts of crate::Foo which are generally only needed in specialized cases. If the only thing you want to expose is the type (but the module contains the implementation) the module itself shouldn't be public, only the type inside which is then reexported outside. Okay, this is fine, but literally every crate author has a different system to this, so when I'm programming it's not gonna make a difference because there's no pre-defined style to follow &gt; Again this is an issue when I'm reading code someone else wrote and I'm trying to understand it. It's not though,not with the docs.rs search functionality. As I've said before, I think the best system is to have the module structure exported, but also have a `prelude` module that exports ALL the types and functions. That way I can code way easier, and if someone wants to look up a type, they can just search it in the registry and find the module it's a part of. 
The point isn't that Reddit *should* use a single server, though I don't think your scaling issue matters at that point, but that "webscale" solutions should made under the understanding that you could, and if you're needing to throw hardware at the problem you should go back for a rethink.
Now I think you have it. There needs to be a guideline and style for module and type definition. I am not a fan of prelude, because it doesn't make it explicit where some things come from, but I and not against it being a standard if that's what the community likes. I do completely agree with you that it's annoying that there's not logic. Sometimes there's a prelude, sometimes there isn't. Sometimes types are in the module with the same name, sometimes they're outside. There needs to be better guidance of top level design and conventions for rust.
&gt; I notice that its design doesn't have separate traits for Complete and Partial parsers, as they state: "If partial parsing is not supported this can be set to ()" for the PartialState. Seems like we had the same idea :) My original plan was to have two separate parser traits like that but as I worked on it I realized that it doesn't actually work (without a lot of duplication). The lynchpin here is that we need "partial parsers" to be able to use "complete parser" and vice versa. For this to make sense for users we need `trait PartialParser: CompleteParser { }`. That is, a `PartialParser` offers additional capabilities that a `CompleteParser` does (namely that it can recover on partial input). But if we bound it like that it becomes a problem when implementing all of the combinators. As an example, consider the [many](https://docs.rs/combine/3/combine/fn.many.html) parser. Since we definitely want this to be a partial parser we try to write a `PartialParser` impl and then we should just get the `CompleteParser` impl for free, right? Well, yes and no. We do get a `CompleteParser` impl but since we wrote this as a `PartialParser` we also put a bound on `P` like this impl&lt;F, P&gt; PartialParser for Many&lt;F, P&gt; where P: PartialParser, F: Extend&lt;P::Output&gt; + Default, { .. } This means that we can only use `many` if the inner parser `P` implements `PartialParser`. It is not enough for it to implement `CompleteParser`. While deliberating this I also realized that strictly speaking, a partial parser only needs to have the capability to redo the same parse again, only with more input. Assuming that the input supports (which is the case for slices, the most common thing to parse), it is entirely valid to throw away everything that was parsed and start again from the beginning (this is what nom does). Recovering in the middle is "only" necessary for the sake of performance (`combine` does support recovering at any location in any parser in the crate though, barring any bugs). The consequence of this is that it is fine to use a non-partial parser as part of a partial parser, the only drawback of that is that if the input ends inside the non-partial parser it needs to resume at the beginning of that parser. But that does not mean it needs to resume from the beginning of the entire input. Due to this I found it simpler to just have a single parser type and its up to each `Parser` implementation to decide what level of partial parsing they can or want to support. &gt; My only gripe is how (in their example) you have to use a 'I::Error: ParseError&lt;I::Item, I::Range, I::Position&gt;' bound on your combinator functions. That is a bit ugly, but I don't know if its possible to get rid of it currently (it cites rust-lang/rust#24159). (Direct link to the bound issue https://github.com/rust-lang/rust/issues/24159) Yeah, I really hate how that became necessary. Ironically, the [parser!](https://docs.rs/combine/3/combine/macro.parser.html) macro used as a workaround for lack of `impl Trait` does not need this as the macro itself can supply the bound. Getting rid of it would mean to get rid of user supplied errors which I don't believe to worth it (and hopefully the issue gets fixed someday). &gt; ... which one is faster, ... I recently updated the combine http parser and it is possible to really close to nom's performance https://github.com/Geal/parser_benchmarks/pull/18 . The remaining ~3-10% gap is, as far as my measurements go, partly due to `combine` relying more on LLVM to optimize out the abstractions and partly due to combine's more detailed error handling. &gt; I will switch a parser I was working on over to combine to see whether nom or combine gives better error, which one is faster, and which provides the best abstractions for streams vs complete data (my guess is combine will win there). I'd really love to hear what your results are!
I think what also deserves a little attention is the [rust-cpp fork](https://github.com/ogoffart/rust-cpp) [i hope this gets upstream] while looking at [this](https://github.com/woboq/qmetaobject-rs/blob/master/qmetaobject/src/listmodel.rs#L141) for example basically you can inline c++ code in rust in which you can inline rust code.
I prefer using only the system package manager if possible. One unique view of all installed packages installed, so it is easy to track which stuff is unused, clean my system when dependencies change, etc. Sometimes language specific packages might depend on a system level package (interface to C dynamic library): this external dependency is outside the scope of the language specific package manager. Thus you must install it manually, remove it manually if you stop using the language specific package that required it, etc...
I got curious and looked up a 2U server from Lenovo : https://lenovopress.com/lp0645-thinksystem-sr850-server 112 CPU cores with 3 TB of LRDIMM or 1.5 TB of RDIMM That is one beast of a machine. 
Just saying, it may be a good idea to enable [GitHub login for Discourse](https://meta.discourse.org/t/configuring-github-login-for-discourse/13745). It's very easy to set up, and convenient to use.
Exactly. Having a separate package manager for every language or project is not far away from installing dependencies manually.
Tokio itself (via tokio-executor) uses tokio-threadpool, so it seems quite ready.
It'd be cool if the compiler supported an 'all' module or something, added by default which would just export every symbol that you could publicly get to - this way people wouldn't have to add a prelude module.
In this particular case it's stored as is, I.e as a raw 4 byte memory dump of the original float 
There doesn't seem to be a canonical way to generate Rust code. For example projects like [bindgen](https://github.com/rust-lang-nursery/rust-bindgen) and [gtk-rs](https://github.com/gtk-rs/gir/) have their own custom solution. I find that code generation is usually not very elegant. Not knowning about your problem, I think that your current solution is enough for simple situations. One thing that could help is to move more code to macros that you use in the generated code.
Package managers of distros are for users and system builders. If you are just developing a Rust app and that's your main business: use rustup. If you want to build on a distribution level, getting all software from one source and not having to care about the details of cargo, npm, pip, etc. becomes a net win.
nom also predates 1.0, a lot of things that made it absolutely necessary to use macros are now gone. /u/geaal should know more about that.
This is an extended abstract about ongoing work to formalize source Rust in submission to a workshop. So, it's rather short and high-level, but I thought folks might be interested. 🙂
Welcome to the weird world of mathematical discrete combinatorics. If you think of it as a number sequence then each image has a number. Since the sequence contains all images, then the number you look up in the sequence contains the same information as the image itself. This is because there exists some algorithm mapping images with a one-to-one relationship with the numbers. In mathematics the terminology is that the numbers and the images are isomorphic, they are essentially the same object with respect to some definition of equivalence. I've had the exact same idea before. It's not going to show anything interesting for the expected lifespan of the universe. However, here is an idea of how to fix it: Add a feature where people can select a date (you need scientific notation for the year) so people can watch what image it will show in e.g. Jan 5 2000^30.
Also debian should dynamically link at runtime to reduce disk usage, whereas cargo just statically links everything.
Wow.
From what I understand of the compiler, it doesn't really look into the bodies of function calls when doing a borrow check. Looking at the function bodies, you can clearly see that the borrows don't collide, but if you look at only the function signature, all you see is that both calls borrow `entity`. One thing you can do here is to do a single function to borrow multiple parts at once: fn get_mut_fx_x(&amp;mut self)-&gt; (&amp;mut Vec&lt;f32&gt;, &amp;mut Vec&lt;f32&gt;) { (&amp;mut self.fx, &amp;mut self.x) } Also, the loop in `body_force` will probably have a bounds check on the `x` vector. Replacing the loop with this will remove the need for a bounds check, although it's no longer requiring that `x` is at least as long as `fx`. for (fx_item, x_item) in fx.iter_mut().zip(x.iter_mut()) { *fx_item += 10.; *x_item += 1.0; }
I sure will, thanks for your help.
This has been preventing C projects from converting part of their code bases to Rust. Adding an external requirement through rustup, which won't receive security or other updates with the rest of the distribution, is simply not acceptable for established software that has already been packaged. Language-specific package managers are simply not a sufficient option for widely distributed software. Users shouldn't have to care what languages are involved in the implementation's dependency tree.
[As of 10 days ago](https://github.com/tokio-rs/tokio/pull/378), that disclaimer has been removed from the readme \- just doesn't seem to have filtered down into a release (and by extension onto [Crates.io](https://Crates.io)) yet.
You have obviously not used a lot of larger Python libraries. They have exactly the same behavior here as Rust libraries, in that there are lots of submodules, and that some people reexport stuff from submodules, and some don't.
pip and npm are the only methods that i have managed to get my system in a state where upgrading was impossible due to conflicts.
The fork was [merged](https://github.com/mystor/rust-cpp/pull/28) yesterday. The macro cpp_class is very interesting and makes interfacing between C++ and Rust rather elegant. However, as the [documentation of cpp_class](https://github.com/mystor/rust-cpp/blob/e727d746bc62f17b57c7555bf6ddd42d49c4ba61/cpp/src/lib.rs#L120) writes, there is a caveat. cpp_class create a Rust struct that has the same size and alignment as the C++ class and adds Rust methods via an impl block. This way, C++ objects can be used on the stack which avoids costly allocations. However, this only works if the object is relocatable.
ogoffart sent me a draft of the blog yesterday. It's impressive work. The procedural macros do the work that is normally done by a `moc`, a helper program of Qt. The demo program `examples/todo` was ported from Rust Qt Binding Generator. The next milestone, IMHO, would be if it can run the [demo program](https://www.vandenoever.info/blog/2017/09/04/rust_qt_binding_generator.html#demo-application) of Rust Qt Binding Generator.
The Rust ABI isn't stable though. IIRC dynamically linking a Rust lib would require everything to be compiled with the same exact version of the compiler.
You know, I always wanted to try them, but for a production system, I need to be able to log to files, and to debug the code, and instrument it in retrievable ways, etc. What is the solution (if any) to these things? I read an interview from a couple of years ago, where Lars Kurth (Xen) saying it's doable, just needs to be done - has anyone actually done it?
Generally debian would use the same version of Rust for all libraries. And a *release* of debian (as opposed to testing) probably wouldn't update rust until the next release. (I guess if they did they would recompile all the packages anyway). Debian doesn't really change much between releases.
Done, thank you
http://libraryofbabel.info/
I posted it above, but you are essentially talking about the same concept as http://libraryofbabel.info/. http://np.reddit.com/r/InternetIsBeautiful/comments/3715ui/a_complete_list_of_every_combination_of/
So send in a PR to the crates you do this to locally. I can't imagine it being too much of a hassle to convince them to add prelude modules.
Could you elaborate a bit more? I figured it was like on the jvm where you’d have a thread for each. 
Realistically this will be because you mixed and matched package managers. Both of these package managers have the concept of local and global installs. The system package manager gets to manage the global ones, and for anything else (including updated versions of the global ones you need) you should be using a local install. For python these are called Virtualenvs, and node does this as node_packages in the current folder.
I've already tested a hyper "Hello world" server and a hyper client :) actix-web is something I haven't looked into though
Interesting. Do you have a link?
A distribution package manager makes it easier for users to install software. As a user and as a sysadmin, I don't want to deal with a dozen different package managers at the same time. In particular, if a package depends on other packages in a different package manager because they are not written in the same language, it becomes a dependency hell.
Debian doesn’t rebuild for a stable release, the packages are uploaded to unstable and then pushed to stable during the next release. Some of the packages in stable have been built many years ago.
The cpp_class! part was merged, but the [rust!](https://github.com/mystor/rust-cpp/pull/31) part is not yet merged
re: https://github.com/ian-p-cooke/grpc-rs/tree/ipc_secure_test is there any way passing a non-null *mut variable to C would result in a NULL value on the C side? I've added a few extern "C" functions to grpc-sys to access the auth_context and a call to `grpc_auth_context_peer_is_authenticated` works fine but a call to `grpc_auth_context_peer_identity` appears to end up with a NULL pointer on the C side. Here's what I used to run the server: GRPC_TRACE=all GRPC_VERBOSITY=DEBUG cargo run --example greeter_server 2&gt;&amp;1 | egrep '(grpc_auth|self.ctx|isten)' and the client: cargo run --example greeter_client and I see I0608 07:10:12.489980426 159097 security_context.c:201] grpc_auth_context_peer_is_authenticated(ctx=0x7f8fa461f100) self.ctx: 0x7f8fa461f100 I0608 07:10:12.490196213 159097 security_context.c:251] grpc_auth_context_peer_identity(ctx=(nil)) how is that second call 'nil' when my pointer is definitely non-null (asserted as such and printed value) and the previous call has the right value? this is only the second time I'm working with FFI and I feel I must be missing something but I can't see what it is. The extern "C" functions are [here][(https://github.com/ian-p-cooke/grpc-rs/blob/ipc_secure_test/grpc-sys/src/lib.rs#L414), the wrappers [here](https://github.com/ian-p-cooke/grpc-rs/blob/ipc_secure_test/src/call/server.rs#L104) and the calls are [here](https://github.com/ian-p-cooke/grpc-rs/blob/ipc_secure_test/examples/hello_world/server.rs#L54)
There use to be an [`aster`](https://github.com/kbknapp/rust-aster) crate, which would let you do things like this: let builder = aster::AstBuilder::new(); let expr = builder.expr() .add().u32(1).u32(2); // prints `1 + 2`. println!( "{}", aster::syntax::print::pprust::expr_to_string(&amp;expr)); However, it has been discontinued, and I don't know what the intended replacement is. I see that the kind of functionality that I am looking for seems to exist in [`libsyntax`](https://github.com/rust-lang/rust/blob/1b4c921103ff4ae225f2d84a8b13f1616dcb538e/src/libsyntax/print/pprust.rs#L3189-L3200), but that might be considered internal to the compiler, and not stable and normally accessible: let abba_ident = ast::Ident::from_str("abba"); let decl = ast::FnDecl { inputs: Vec::new(), output: ast::FunctionRetTy::Default(syntax_pos::DUMMY_SP), variadic: false }; let generics = ast::Generics::default(); assert_eq!(fun_to_string(&amp;decl, ast::Unsafety::Normal, ast::Constness::NotConst, abba_ident, &amp;generics), "fn abba()");
&gt; I mean, we already have package manager for rust It's good for libraries, but not really for applications. I mean, most users of an application probably don't want to install the Rust compiler and have to build it (and all its dependencies). 
Ohhh, I do feel strongly about this particular issue! This is closely related to the grammar composition problem: how to stitch together HTML, CSS and JS parsers so that you get support for the whole html, *including* `&lt;script&gt;` tags and `style=` attributes. This is a fascinating theoretical problem: although composing CFG directly is possible (and is, in fact, how BNF works in the first place), the composition does not necessary have the properties of components. That is, gluing together two unambiguous grammars might result in an ambiguous one. And the fun fact is that we can completely dodge this problem by just not solving it at the parser level. The trick is essentially the one I've described in the blog post, when talking about supporting escape sequences. You handle all the injected code in the lexer by parsing it into one big token by over-approximation. Then, completely outside the parsing process, you create a view inside token's text, and parse it with whatever other parser you have. For HTML/JS example, the HTML lexer would parse the contents of `&lt;code&gt;` tag just as a sequence of bare words, so the parse tree for HTML would have a giant "contents of &lt;script&gt;" token. Then, JS parser would use the text of that token as input. &gt;The problem is that you have to run your parser inside the brackets to determine where the interpolation stops. You dont! You need only a covering grammar! Here's how Kotlin does all this: 1. In the lexer, we parse `${}` just counting braces: https://github.com/JetBrains/kotlin/blob/020d941c276f21b78a93329bfbfae758e3141c30/compiler/psi/src/org/jetbrains/kotlin/lexer/Kotlin.flex#L163-L173 2. Then we declare that stuff inside `${}` can have a secondary parse tree attached: https://github.com/JetBrains/kotlin/blob/020d941c276f21b78a93329bfbfae758e3141c30/compiler/psi/src/org/jetbrains/kotlin/psi/KtStringTemplateExpression.java#L28. `implementes PsiLanguageInjectionHost` does this 3. Finally, parse the contents of the token as an expression and attach it to the outer tree: https://github.com/JetBrains/kotlin/blob/020d941c276f21b78a93329bfbfae758e3141c30/compiler/frontend/src/org/jetbrains/kotlin/psi/psiUtil/StringTemplateExpressionManipulator.kt#L35-L36 (called from `ElementManipulators.handleContentChange(this, text);`) The super cool benefit of this approach is that you don't have to decide which language to inject at a parse time. You totally can base injection on name resolution or type-inference results. For example, this is how tests look in IntelliJ Rust: https://user-images.githubusercontent.com/1711539/41159685-0becaf5e-6b36-11e8-83b6-c59bc4929729.png They are written in Kotlin, but code inside string literals is highlighted as Rust or Toml, because the parameters of which the string literal is a value are annotated with a `@Language` annotation: https://github.com/intellij-rust/intellij-rust/blob/cb18f124dbb628dbcb635104ef8f12503f1ea118/src/test/kotlin/org/rust/FileTree.kt#L57-L58. 
Hey hi, glad you really like Rust! Remember to also check out the [forum](https://users.rust-lang.org/) in case you haven't already.
Usually kernel debug can be sent to a serial port.
There are many types whose size is much larger than the meaningful data they store (e.g. an empty ArrayVec, a big SmallVec that uses the heap). The current `memcpy` the whole type approach makes these types impossible to implement efficiently in Rust. 
Sure, once I finish converting one of my parsers over I can write up a comparison.
This looks promising! 
Many software in Linux distributions use python. And those software and dependencies must be installed via os level package managers. That is why many python packages are distributed with apt. Same goes for c and similar compiled languages. If you want to compile a software that is already packaged, OS must provide everything you need. That is why we'll probably see lots of outdated rust crates on debian repos. Don't use those, they are for already packaged software. If you need a crate, do whatever you're still doing right now, rustup! 
&gt; This still takes a lot of time dude, more than the docs suggestion probably! (especially since you're fishing out the import errors from all the other errors / warnings in the compile) Usually you would fix any other errors before introducing a symbol without `use`, and even if not, you could quickly jump to the right error in your editor. &gt; What if I said to you - 'Don't bother putting trait bounds or lifetimes in, just let the compiler give you suggestions' That's not necessary, you already have all the info necessary for that when writing the code. 
&gt; That's not necessary, you already have all the info necessary for that when writing the code. Imagine having all the info necessary when writing the code when it came to using crates, i.e. KNOWING sometihng is in a module
&gt; It's not going to show anything interesting for the expected lifespan of the universe. This is what I'm concerned about and one of the reasons I added [the 1-bit per pixel generator](http://seeanything.org/1bpp.html), but that will eventually get too slow to be interesting as well. I have plans to add another generator that randomly picks a number between 0 and N where N is 2^bpp^pixels (256^76,800 in this case) and maps that number to an image, without repeating the number choice. The challenge here is getting that number and using it in a computationally fast algorithm. Python and Ruby have no problem calculating that number, but struggle to map it to an image in less than 5 minutes. Since you've already thought about this subject, I'd love some help implementing new generators and features if you have the time!
And the [IRC](https://www.rust-lang.org/en-US/community.html) :)
Maybe you can take inspiration from how serde treats nested types?
Yes, this is why cargo install has been limited historically; we want to encourage real packages for end users.
Concurrency doesn't necessitate multi-threading. As an example of a famously single-threaded yet concurrent language, look at Javascript and Node.js! Take a look at the following diagram: http://www.aosabook.org/images/twisted/threading_models.png
It sounds like "that's the way it is and we don't want to change it". The client side rendering is really laggy and noticeable. It's unfortunate that only one person is responsible for the decision and she apparently doesn't care.
This is like Gödel numbers + the infinite monkey theorem but automated, to not harm any monkeys..
&gt; One of the big problems is that in rust, contrary to C++, objects can be moved in memory at will. This is a big problem, as the C++ object contains a pointer to the rust object. So the rust object needs somehow to be fixed in memory. This can be achieved by putting it into a `Box` or a `Rc`, but even then, it is still possible to move the object in safe code. This problem is not entirely fixed, but the interface takes the object by value and moves it to an immutable location. Then the object can still be accessed safely from a `QJSValue` object. I believe what you need is an [emerging `Pin` API](https://github.com/rust-lang/rfcs/blob/master/text/2349-pin.md) that solves exactly the same problem for async/await ecosystem.
Library crate packages basically contain the source code in a structured format that can be used for offline building. The users who want to use a binary crate package, will only need this, meaning the end users will never have any librust\* packages installed in the default case. These library packages are build\-dependencies, so they will be required for building the binary crates, and those will mainly contain the statically linked binary. This means that apart from source incompatibilities with newer compilers or newer versions of the dependencies, no repackaging of library crates will be necessary. Effectively, the problems with compiler incompatibilities will be a lot easier to handle once compilers supporting epochs land in Debian. Of course it makes sense to rebuild binary crates once the libraries received updates.
Can you split all these fields into disjoint sets that each have to be borrowed at the same time? Then you can return view structs that contain [split-borrowed](https://doc.rust-lang.org/nomicon/borrow-splitting.html) mutable references [like described here](https://blog.rom1v.com/2017/09/gnirehtet-rewritten-in-rust/#mutable-data-sharing). If you can't separate the members into disjoint sets, maybe you can just put the code that uses them in the same module, so that it can access the private struct members directly?
It's better for end-users to centralize every package into a single repo than having to learn and maintain many.
Thanks for your work on this. I'm looking forward to a future where we can actually prove some parts of our Rust code to be correct :)
I read it as filf at first
&gt; The Rust ABI isn't stable though. IIRC dynamically linking a Rust lib would require everything to be compiled with the same exact version of rustc. Even a stable ABI wouldn't be enough. Generics are monomorphised on usage, so if a library exposes a generic function, its compilation for the specific type you use happens at the moment the using crate is compiled.
How to use deps.rs for a custom crate?
Or https://gitter.im/rust-lang/rust . Because it's more active and user friendly even on weekends :)
What command are you using to try to compile it? Rust codebases are generally built around the assumption that `Cargo.toml` and `cargo build` will handle incorporating dependencies into the build process.
That's why I always cringe when I see someone suggesting installing a language package with sudo. pip and npm both offer user global installs rather than system global installs. 
Thanks. Is it also suitable for offering only remote freelance work (working from home with Rust, PureScript etc)? (I know some french btw..)
I kinda get where you're coming from (I'd personally prefer it if they got rid of Ember), but at the same time I think it's a bit uncharitable to say that they don't care. The impression that I got from that comment was that they need to be more confident in stability before making any sweeping architectural changes, not that they're ruling them out altogether, and I think that's fair \- I'd rather [Crates.io](https://Crates.io) was a little sluggish than it be unstable/down more often.
Benefits of having native packages: 1. Reduce duplication, all libraries are dynamically linked to each other without reinstalling over and over 2. Distribution patches if needed, 3. No centralization and possible offline / intranet usage, 4. Version tied to the distribution which means during the lifetime of the distribution, you get the same library version.
Long time no chat! You talk about a "formal semantics" and mention in the abstract that you prove progress and preservation, but you don't discuss in what format it's formalized. Is it just pen-and-paper, or is it written up in some proof assistant?
Thanks so much for the read; this is great!
Ah ok, thanks for the reply. I ran &gt; cargo build and now I get error: linking with `cc` failed: exit code: 1 then it throws out a huge string of libraries. I will investigate further on Google, but please let me know if you have any idea. Thanks again for the polite response :)
If you pastebin the full error message, I don't mind taking a look to see what went wrong.
Right, and currently the examples all run single threaded, where I want multi-threaded or async.
Hah, funny to see you here! It is currently pen-and-paper with the beginnings of a mechanization in Coq, but I'm only a noob Coq hacker and so I'm hoping to have more confidence that everything fits well together before proceeding further down that road.
Thanks, it's much appreciated :) From what I can gather it looks like an issue with OSX though.. [Here](https://pastebin.com/pRxC5z0w) us the pastebin
Yep, this is how we do library crates in Fedora too, with source-only devel packages.
Good point. Added.
I often refrain from installing software that comes only in npm or in pip. I want everything in one system. That's the point of a package manager, I would say.
My current job is 100% remote (and everybody in the team is also working remotely), I had to go to the headquarter once in the last two months. But most employers in France seams to be reluctant for full remote, they easily accept one day a week, but rarely more … But it costs nothing adding your profile to this website though ^^. 
The important bit is the lines immediately following the really long one: &gt; = note: ld: library not found for -lSDL2 &gt; clang: error: linker command failed with exit code 1 (use -v to see invocation) In other words, the project depends on libSDL2 (via the `pistoncore-sdl2_window` crate, according to its `Cargo.toml`) to be cross-platform. The [rust-sdl2](https://github.com/rust-sdl2/rust-sdl2) README has instructions for several different ways to set up SDL2 on OSX so Rust can find it.
I managed to resolve the issue when running cargo build (brew install sdl2 for anyone who stumbles on this thread in the future), but I'm still getting the same error as before when I compile using &gt; rustc level_out.rs error &gt; error[E0463]: can't find crate for `phf_macros` --&gt; level_out.rs:3:11 | 3 | #![plugin(phf_macros)] | ^^^^^^^^^^ can't find crate error: aborting due to previous error Is rustc not the correct command to use here?
Wouldn't it be part of the ABI to define the behavior in that case ?
Yeah, it was late at night when a wrote that. I now don't think that is a good solution. My other solution is a bit better, but I am leaning toward suggesting putting the methods in some trait and generating common code with a macro. I may or may not have time to work that through today, but hmu if I forget or one of the better programmers here hasn't helped you out yet.
Ah yes thanks, I managed to resolve this. Sorry for wasting your time looking at the logs.
I'm working on a method to generate infinite 3D geometry using 2 group generators and functional programming. Here is a blog post about the basic principle: http://blog.piston.rs/2017/02/06/the-mathematics-of-infinite-things-space/ This is a way you can generate content that is more interesting: You construct some objects that repeat themselves infinitely and then you combine those sequences with each other. For example, if you connect one pair of circles, you get a cylinder, so you have a mathematical function that outputs infinite amounts of cylinders. I want to generate points and colors so I don't have to use physical springs. Points are summed and colors are multiplied.
`rustc` is the "calling the compiler directly" approach that I was referring to when I said it's only for debugging problems with `cargo`. `rustc level_out.rs` won't work because it's not going to go looking for crates. It expects to be called by `cargo` with a lot of extra stuff on the command-line to tell it exactly which crates to link in and where to find them.
Ok, thank you. I wasn't sure what other way there was to compile it (since the description on github describes level_out as a seperate project), so I will look into cargo further. 
`Cargo.toml` specifies that Cargo should build two separate binaries: [[bin]] name = "emulator" path = "src/main.rs" [[bin]] name = "level_out" path = "src/level_out.rs"
«More active»? How can it be more active and still being readable ? ^^
Thanks, I managed to get it working :)
Or https://discord.me/rust-lang :)
Oh my. It has a search function.
Glad I could help. :)
Thanks everyone! 
This looks really cool! There was one thing that took me aback for a moment ("Further, this decision enables us to simplify our model by treating _moves_ as _mutable borrows_ since both require full ownership represented by a whole capability."), but I think I see how this is made safe. My initial concern was with the following: fn foo() { let mut x = 1; let mut y = &amp;mut x; bar(&amp;mut y); // What prevents `bar` from moving out of `b` and leaving it empty? } However, I think I see how it can be made safe: fn foo() { // P = {} let mut x = alloc 'x 1; // P = { 'x ↦ (u32, 1, {}) } let mut y = borrow mut 'y x; // P = { 'x ↦ (u32, 0, {}), 'y ↦ (u32, 1, { ε ↦ 'x }) } bar(&amp;mut y) fn bar(z: &amp;mut &amp;mut u32) { let w = *z; // PREVENTED: Would move out of `y`, leaving it empty, requiring deallocation, but `y` has metadata } } However, this did make me realize a different issue, which I'm not sure can be resolved without reasoning about lifetimes: const A: u32 = 3; fn foo() { let x = 1; let mut y = &amp;x; bar(&amp;mut y); fn bar(z: &amp;mut &amp;mut u32) { let b = 4; // This is kosher, because 'static outlives 'x (and in fact, outlives everything): *z = &amp;A; // This is not kosher, because 'b does not outlive 'x; *z = &amp;b; } } Thoughts?
What should 'all' do if two names collide? (eg. two different `Error` types which follow the convention of "`io::Error`, not `io::IoError`")
ohhh yeah i guess if you have namespacing issues internally you're kind fucked, I've always thought that's a really whack api when you have 2 identically named symbols and only the modules differentiate
Warning: Incoming pedantic correction! \&gt; ~~node\_packages~~ node\_modules
It’s not just for saving disk space but also to avoid duplication of shared libraries to make security maintenance easier.
Humm, seems people have been here before: [https://github.com/alexcrichton/cargo\-vendor/issues/58](https://github.com/alexcrichton/cargo-vendor/issues/58) My workaround for now is to keep an internal fork of socket2 and remove the targets I don't need from its Cargo.toml file.
Well, it's Rust convention to translate a namespace like `io_*` to `io::*` and not `io::Io*`, so you're likely to run into that whenever you have terms with broad applicability like `Error`.
I appreciate the feedback! As I mentioned on another comment, I will write up my results in a blog post and I will let you know. Thanks.
Good catch
https://docs.rs/futures/0.2.1/futures/executor/struct.LocalPool.html Have you tried this? I can hack on a code sample later, little busy right now.
...and another problem I noticed today. With large dependency graphs, the downscaling to fit the page renders the text unreadable. I'd suggest some kind of zoom-button overlay in the top-right corner of each dependency graph. It could be linked to the image itself for JS-disabled users and then progressively-enhanced into something more convenient. Beyond that, I'd suggest rendering it as SVG. It's [widely supported](https://caniuse.com/#feat=svg) by browsers and, if you embed it using `&lt;object data="/path/to/svg" type="image/svg+xml"&gt;&lt;/object&gt;` then you'll get the following advantages: 1. In Firefox, SVG embedded using this syntax is the only way I've found to keep the built-in page-zooming support from doing a blurry upscale of the original downscaled render, preserving users' intuitive expectations of one way to read the text. 2. If you hyperlink each crate in the graph to its page using the `href` node attribute that Graphviz offers, users will be able to click on crates in the dependency graph. (If the SVG rendering looks muddy, attach a `shape-rendering: crispEdges;` CSS rule to the root `svg` element.)
I was trying to use it recently but wasn't able to, clippy doesn't compile since june 2, there is an open issue and the problem was actually solved but a bug in crates.io makes it impossible to upload the patch. So I can't use it, sure it probably won't last, but it's kind of a bummer, I was trying to show the toolings to my coworkers and looked like a fool.
async can still be single-threaded, that's what everyone is talking about here. If the future is mostly waiting (e.g. on an HTTP response), there's no need to allocate an entire thread to do that.
No there is no alignment high enough so that the unaligned bits can hold a `usize` (since that would consume all of the bits in the pointer), and having to unmask the actual address is a cost that often would override saving a word on the stack even if it were possible.
I guess I'm just a bit confused by all the options available. I don't know what to use where, I don't think the JVM gives you this level of fine control over futures like Rust, so it just leads to a bit of confusion about where to go.
Oh, yeah, that makes sense. I think using something like the [im](https://crates.io/crates/im) crate could do what you need.
What /u/MEaster said, basically. You can't borrow separate fields using separate methods, because once you've borrowed one field from the struct (via a function), the compiler doesn't know which you've borrowed and has to assume you could have borrowed any part of it, so you're prohibited from taking a mutable borrow from the same struct again. However, you can borrow disjoint fields at the same time if you do it in the same function, so the compiler can see that's what you're doing.
I think of “disjoint sets” as the name of the abstract data type that we most often implement using the concrete data structure called a “union-find.” But maybe that's wrong. Anyway, I could rename things before 1.0. I think I'm stuck with the crate name, but maybe the types should be `DisjointSets`, `ConcurrentDisjointSets`, and `DisjointSetElement`?
[removed]
It depends, really. Screen estate is valuable too. There's a threshold at the screen level: when it fits on the screen it's easy to grok (looking things up and down by shifting eyes), as soon as it takes more than one screen, and you have to scroll to look things up and down, then difficulty increases a lot! Scrolling not only means coordinating eyes and hand, it also means that lines change position so going back to the line you were looking at is not as immediate. Therefore, in situations where formatting on one line vs four lines saves on scrolling, one line wins. Otherwise, I may indeed prefer you r formatting.
Some folks out there must be feeling a false excitement of being something bigger than themselves by putting their functions into deeply nested namespaces... ¯\(°_o)/¯
Oh sorry I think I misunderstood then. The problem the link solves is finding a sequence of changes to make a valid parse whereas you seem to be talking about getting a partial parse of invalid input.
Nowadays, Tantivy (a rust search engine) gives similar perf as lucene (elastic search uses lucene) and returns the same BM25 score by default. 
For starters cargo isn't a package manager it's build utility with dependency management. Package manager supposed to handle...binary packages. They supposed to be built around target release. Like built around different openssl versions, libc, etc.
I am not fond of their ranking personally. I maintain a popular search engine crate called Tantivy and it is only visible on page 6 if you type search... Previous result contain a lot of unrelated stuff.
The easiest thing is probably to just use the tokio default runtime. See the example in the [crate docs](https://tokio-rs.github.io/tokio/tokio/index.html).
As another option, since `f32` is `Copy`, just make every `Vec&lt;f32&gt;` a `Vec&lt;Cell&lt;f32&gt;&gt;` - now you can use immutable references everywhere (as long as you don't change the number of items in the struct, which you don't seem to be doing).
Still very new, but having trouble wrapping my head around how/if can use multiple traits. I read on SO that you can't do like 'impl Trait1, Trait2 for Something{}', but can you make a trait that is really just a grouping of traits, composing them into one set? (resulting in 'impl CompositTrait for Something{}' If no need then why? (my exposure to interfaces is from Java.) 
5. Post install steps like shell completions, manpages, license files, etc. 6. No *requirement* to compile from source 7. If you do compile, custom steps can be used like native CPU optimizations :)
Great to hear. If you *really* want to level up your Rust-fu, be sure to look for projects with mentored issues. [This week in Rust](https://this-week-in-rust.org) has a weekly updated list.
&gt; However, this only works if the object is relocatable. Just to be clear, by relocatable you mean "move constructing" is just "bitwise copy"?
Hadn't heard of that one before. Tantivy looks interesting, even uses SIMD features and the like.. Thanks for mentioning it!
&gt; "Further, this decision enables us to simplify our model by treating moves as mutable borrows since both require full ownership represented by a whole capability." Have you ever found yourself in the position of wanting a function to reuse an existing buffer, to avoid allocations? There are two possibilities: - Imperative: `fn reuse(buffer: &amp;mut Buffer) -&gt; ();` - Functional: `fn reuse(buffer: Buffer) -&gt; Buffer;` If you can *in the function implementation of `reuse`* guarantee that a "reusable" object is returned, and *in the caller* guarantee that the returned buffer is put back where it had been stolen from, you have safely managed to replace "mutable borrow" with "poke hole, change, fill hole". The trick, of course, is that it should also work in the presence of `panic` which I am not sure is possible in Rust itself.
But counting brackets doesn't work, does it? val x = "${ "}" }" println(x) However you go and grab the entire string has to have at least knowledge about how to grab an entire string. val x = "${ "}${ "}" }{" }" println(x) So there's definitely a complex(ish) problem that needs to be solved to determine what brackets need to be balanced and which don't. And this assumes your grammar has matched braces in the first place. _Thankfully_ mine does, even though I'm not matching the round and square ones.
Which as I mentioned before, is not what I want. I Don't want to bind to a tcp port or anything like that, I just want to know how to do async or multithreaded calcs with futures. I don't need any networking functions like this.
Rustbyexample was moved to the official rust docs, and the run button is present there. [https://doc.rust\-lang.org/rust\-by\-example/hello.html](https://doc.rust-lang.org/rust-by-example/hello.html) I'm not sure why it's still showing up under the old URL; last I saw that was redirecting to the official docs URL.
Exactly why I switched to arch. 
&gt; Should I be training myself to think of solutions in this way, or is there a simpler way to do it? There's very little training, but there's one thing to understand: function calls are opaque to the borrow checker, while direct attribute access is not. So when you borrow a field directly the checker knows it's a partial borrow (and can hand out non-overlapping borrows to other bits) but when you borrow a field through a method the checker has no idea and can only assume the entire structure is borrowed.
Can you give more detail what this project is? Is valve using rust in dota2 now? Or is this replacing valve's opengl dynamic library with a rust-metal shim?
I don't see Steam running, so I'm assuming it's some sort of development build and not some crazed hooking and shimming thing. What is this?
[gfx-portability](https://github.com/gfx-rs/portability) is our shim library around [gfx-hal](https://github.com/gfx-rs/gfx) that implements Vulkan Portability. On MacOS, we can directly replace MoltenVK (that Dota2 optionally ships with to enable Vulkan-based renderer) with our portability implementation and get something rendering. It's super early (first screenshot!) but here we are, celebrating :).
Didn't Apple just deprecate OpenGL? Metal also looks geared towards Apple.
You are right, some context is needed to avoid confusion. I commented nearby, and would like to clarify in addition that no, Valve isn't using Rust. But we can route over Dota2 on MacOS to go through Rust code that does Vulkan -&gt; Metal translation.
If you don't need to do IO, a more appropriate tool than tokio would be [rayon](https://github.com/rayon-rs/rayon).
I mean, you're not wrong. The conventional naming in the algorithms community for this mess is scattered and confusing: you've just inherited it. Really, "union-find" isn't a data structure name so much as the names of provided operations on an unnamed abstract data type. "disjoint sets" isn't really a great name for that abstract datatype, I think: it implies that general set operations will be available rather than just "union" and "find". I don't have a great alternative suggestion: maybe "partitions" and "partition tree" or something? Anyhow, I probably wouldn't bother to rename anything at this point. Just put some careful notes in your top-level crate documentation about what this is and does. Folks who are familiar with algorithms will know what's going on; folks who are not will probably never discover or use this crate no matter what you do.
The issues with `crates.io` lately are really concerning to me. As someone who has watched the Haskell Hackage / Stackage debacle unfold, I really hope we aren't headed anyplace similar.
Apparently, Steam may not be required. I'm just running the stock Dota2 executable installed by steam, overriding `DYLD_LIBRARY_PATH` environment.
This is such incredible work. Is there a larger goal being accomplished other than being an alternative to MoltenVK? (Not that that is any small feat.) Is gfx looking to be a portable backend layer in commercial games separate from using gfx directly? And with the news of OpenGL being deprecated on macOS, is it possible for this base to be used to implement an OpenGL layer in the future?
Hm, wait, I might be misunderstanding how interpolation specifically is actually implemented in Kotlin... Upon closer look, I believe my core assumption, that the boundaries of interpolation are handled by lexer by counting brackets **as tokens** is true. That is, there's no back and forth between lexer and parser, lexer determines the boundaries by itself. It can count `{` correctly because it, being a Kotlin lexer, can lex Kotlin code. With flex, it is achieved with the help of non-exclusive states. However, the bit about injection/secondary parse is wrong: while in theory you can handle string interpolation in a completely separate phase, it is not necessary, precisely because outer and inner languages are the same, so you can just reuse the tokens. &gt;And this assumes your grammar has matched braces in the first place. Thankfully mine does, even though I'm not matching the round and square ones. I think this is a reasonable assumption. FWIW, I think that ability to handle "any" grammar is a bug, and not a feature, of a parsing technique. *Arbitrary* CFGs are not a reliable way to describe languages, because of the ambiguity problem. If we want maximum expressiveness, then why constrain ourselves with context-free languages :-)? Who doesn't want Rust where it is a syntax error to accidentally loop forever on some input? 
Thank you! &gt; Is there a larger goal being accomplished other than being an alternative to MoltenVK? Metal is just one of the backends for us. We also have Vulkan (d'oh), D3D12, with D3D11 and OpenGL coming later. &gt;Is gfx looking to be a portable backend layer in commercial games separate from using gfx directly? Yes, using our portability implementation as a product (where Rust is just an internal detail) would be great to help games and apps get wider audience. This is what Vulkan Portability initiative is for, and it's going to be very important in Windows world as well soon with UWP becoming mandatory. &gt; And with the news of OpenGL being deprecated on macOS, is it possible for this base to be used to implement an OpenGL layer in the future? No, we translate Vulkan -&gt; anything, not OpenGL -&gt; anything. For that, one could look at [MoltenGL](https://moltengl.com/moltengl/) or ask [Angle](https://chromium.googlesource.com/angle/angle/+/master/README.md) folks to implement Metal backend.
Well, I don't think I have time to dig more into it, but it looks like maybe I can maybe do multiple 'impl [...] for' blocks. If I'm correct then go ahead and chime in. Curious, but may not be able to tinker more this weekend. 
Congratulations! I'm very excited about this.
Uuuugh. I have been fighting Travis, for some reason it keeps re-building old docs and then pushing it over my branch. Time to look at the settings again...
ahhh this is so cool, congrats!
&gt;Didn't Apple just deprecate OpenGL? Yes &gt;Metal also looks geared towards Apple. Yes, Apple is mandating that accelerated graphics on their platform go through Metal which is why this work (automatically translating Vulkan API calls to Metal) is so important because it allows developers to continue to target the open standard without losing market share.
Is text intentionally obscured here or just incomplete? Great work btw!
I like how [Piston](http://www.piston.rs/#) organizes the code\-base into a swarm of tightly focused crates that can be mixed and matched freely to accomplish a particular goal. I think there is something to be said for this sort of style.
&gt; Metal also looks geared towards Apple. Metal is Apple's proprietary gfx API. It works on similar low-level low-overhead principles as Vulkan and Dx12, but predates Vulkan and was released around the same time Dx12 was announced (early-mid 2014, Dx12 was released in mid-2015 and Vulkan in early 2016).
Text appears to be broken, although everything else looks surprisingly fine :) It's the first screenshot, and the game is not playable at all yet.
Maybe, but thanks for all you do anyway!
Hehe you’re welcome. It’s cool. This transition should have been easy, but ended up being way harder than it shoulda been, so I’m just slightly frustrated. This is the fourth or fifth time something was supposed to be done and working, and then surprise! It’s not.
Yeah, if IRC doesn't seem active you may be on the wrong one (e.g. on Freenode rather than Mozilla IRC).
Not sure tackling the general case of recursive types is the best plan here. I'd probably start by not allowing `#[derive(Validate)]` to work for types it doesn't understand: this seems essential. Then I'd add support for deriving for `Option&lt;T&gt; where T: Validate` and see if I got any more bug reports.
Yeah, this isn't really supported by Cargo internals right now. I had a similar issue and have been chewing away at bits and pieces in the direction of fixing that, but for now it's a bit low on my priority list. If you have a commercial need for this, I'm happy to consult with you and get it fixed in cargo and cargo-vendor upstreams. 
Agreed, a pure functional language like Idris would be much more suitable IMHO. Then I could actually trust the correctness of the smart contracts.
&gt; eing way harder than it shoulda been, so I’m just slightly frustrated. This is the fourth or fifth time something was supposed to be done and working, and then surprise! It’s not. Thank You!! I thought it was my browser or some ad-blocker first, so I tried it in all browsers. But anyway, thanks for fixing it.
Sorry, I didn't mean to say tokio is the best fit for you, so much as a starting point if all you want to do is get up and running. There are many options as you said; if you want to figure out the best approach, I think your best bet is to read a bit about the various crates and compare against your requirements.
Do you have any numbers on performance vs [MoltenVK](https://github.com/KhronosGroup/MoltenVK)?
If you want to have discussions about this with Valve (who knows, maybe they'll be interested?), you should contact either Plagman (Pierre-Loup Griffais) or Dan Ginsburg.
Spending the weekend trying to interface Python and Rust. There are several examples/blog posts/videos that show passing a simple number or single string between Rust and Python, however are there any examples passing more complex data structures? For example, perform calculation in Rust submodule and pass results to appear as a dict to Python. Either as a one-time transaction or as in the Rust code may update the data in the dict again at a later time. Pointers to more demo code appreciated. Thanks!
Good Bot.
On nightly it is possible to use intrinsics like [atomic_fence](https://doc.rust-lang.org/std/intrinsics/fn.atomic_fence.html)
Wasn't Mantle (from AMD) that basically evolved into Vulkan first and the inspiration for both DX12 and Metal? Or do I have things mixed up?
Vulkan definitely stemmed from Mantle, but the ascendency of Dx12 and Metal is less clear: Mantle was announced late 2013 and I believe the first releases (not public SDK) date back to early 2014. So might be a case of the idea being in the air, fundamentally Mantle was inspired by low-level console-style APIs, so others could have noticed this would be a good idea.
I rather like Rust. I tend to do most of my coding in a text editor (vim until recently; learning to use emacs with evil\-mode now \- it was easier to get Racer to work), and having all those really good command line tools is really good. Not only does having Cargo handle all your prerequisites by itself make things much easier than C, but not having to deal with shit like makefiles is also an improvement.
I think you're looking for /r/playrust. This subreddit is about the programming language with the same name.
I have, in some cases, experienced code running thousands of times faster in idiomatic rust vs idiomatic Python. Generally I have found that I can expect minimum 3-5x for idiomatic data science code (which is often just C under the hood for Python anyways).
&gt; require everything to be compiled with the same exact version of rustc. Well, that is the type of thing distro package managers handle and handle fairly well, usually. First you build and package rust. Then you use that Rust to build and package the crateas, and your rust package, with its specific version as a prerequisite in the crate packages. Update for rust? They do it all again, and for an end user rust and the installed crates all update together (although the majority of crates would update to the same version. 
For now it's just calling git only once and not using the rbenv command.
not yet, but it's definitely on the radar
Or use [ANGLE's Vulkan backend](https://www.youtube.com/watch?v=QrIKdjmpmaA) on top of gfx-portability :P
I'm aware of that, yes, but the problem here is that Rust is not linear - it's affine. The guarantee you're talking about is implied by linearity, but in an affine type system, `reuse` is free to "lose" what it was given, breaking the equivalence. This has some funky interplay with fractional permissions, even - if you can "lose" some fraction of a permission, for example, you've just reimplemented `Box::leak`, because now you can no longer recombine all the permissions to get a `1` that can be deallocated. (However, this only implements `Box::leak` returning a shared reference, not a mutable one. For that, you would need your fractional permissions to be a little richer, to separate deallocation from mutability.) In the paper, their metadata system basically "converts" the affine system to linear _only_ for the fractional permission, by turning "losing" a fraction into recombination. &gt; replace "mutable borrow" with "poke hole, change, fill hole". Incidentally, this is where linear types and lenses have a very productive interaction - specifically, the `%=` lens operator, which does exactly that, can compile down to in-place update when the LHS is a linear type.
There was a recent [forum thread](https://internals.rust-lang.org/t/any-rfc-for-units-of-measure/7154) about this.
Thanks. I couldn't see that there were any consensus about what was the currently preferred way, though.
It has english subtitles :)
Excellent work! Do you know of the same compatibility shim that exposes Vulkan but targets OpenGL for those GPUs that don't have official Vulkan drivers?
More official information on the state of gfx-hal would be nice. I regularly check but still have no idea what is happening.
I would **love** to get feedback on this. It implements .NET inspired execution contexts which let you propagate data implicitly along the logical thread of execution. Right now obviously you need to manually pass it onwards but my hope is that something like this could at one point be happening implicit (with thread / task hooks). The API is not my idea, someone at Microsoft came up with the concepts behind it and I fell in love with it quite a bit. I think this is fundamentally one of the best approaches to thread/task locality because it lets you branch off at any point clearly. Not just that, there is a standardized system to suppress flow propagation which is super helpful in many situations. This is what this looks like as an example: flow_local!(static TEST: u32 = 42); assert_eq!(*TEST.get(), 42); let ec = ExecutionContext::capture(); TEST.set(23); assert_eq!(*TEST.get(), 23); ec.run(|| { assert_eq!(*TEST.get(), 42); }); Here are some examples where I would love this being used: * hooking stdout for tests * propagating http request information for systems like sentry which depend on it for breadcrumb data collection * provide more convenient and correct open tracing APIs * associate the current locale/language/culture with the flow of execution for localization * propagating security contexts and policies * tenant separation for multi tenant application code * support for automatic audit support code 
this sounds awesome, I was reading with great interest (as mentioned below) with the recent apple GL/CL deprecation (GRRR). What happens with shaders
gfx consumes SPIR-V shaders (like Vulkan). Shaders are generated from GLSL, HLSL, or anything else that can be converted to SPIR-V. For example, most people use [glslangValidator](https://github.com/KhronosGroup/glslang) at the moment. There is an interesting effort to generate SPIR-V from Rust too, which is called [RLSL](https://github.com/MaikKlein/rlsl).
I'm not sure about Metal but DX12 was definitely influenced by Mantle. One of the earliest documentation PDFs of DX12 was a straight-up copy-and-paste from Mantle documentation.
Interesting. So you will translate Vulkan -&gt; D3D11 and Vulkan -&gt; OpenGL later? It seems a bit odd to translate a lower level API into a higher level API. That's a bit like translating assembly into Java.
Besides the Metal backend, gfx is working on an OpenGL backend (as well as DX12/DX11/Vulkan). So you could use gfx-portability (which implements Vulkan) with gfx-backend-gl (https://github.com/gfx-rs/gfx/tree/master/src/backend/gl) for example.
I've since fixed that memory leak and made it sound by using an atomic spinlock. Not sure that is the best way but at least it operates correctly at the moment. I figured out a couple test cases here as well. Thanks for the help.
Python data analysis can be fast as long as you can do *everything* with native array operations from numpy and the like. As soon as you have to start doing per-element computation in the Python interpreter the constant per-element overhead compared to native code becomes massive, and it's clear why this happens if you start thinking about how many CPU instructions the interpreter has to execute to run operations over a Python array.
it's compatibility layers all the way down
I've been doing software for about 20 years now. In that time I've gone from PHP to Python and then to Go as my "go to language". (I've also dabbled in Java, JS and ruby along the way as well). I've now made the jump from Go to Rust, and am very happy that I have. For many reasons (which I could go into if you'd like), I think Rust is the language of the future, and investing in making it you "go to" language that you know inside and out will definitely pay off in the years ahead. 
The idea is to support as many devices as possible, especially older devices that are limited to D3D11 or OpenGL (and OpenGL ES/WebGL). Some concepts essentially become no-ops in a higher level API, while other parts may be difficult or impossible to support. The goal is to figure out which subset can be reasonably supported and implement that, hopefully still covering most of the Vulkan API.
Yeah, that looks fine now. :) I'd just advise using `parking_lot::RwLock` instead of `std::sync::RwLock` because it's typically much faster.
As someone who has *no* experience with this concept or why you would need it, is there a good primer for ExecutionContexts that you can point me at? I'm specifically interested in how it helps with security. 
The most obvious one with security is tenant isolation in a multi tenant app. If you have a system like github you have multiple organizations and users on the system as well as oauth clients. What you want in an ideal situation is carry a security context around internally that clearly states: this is the current org (tenant), this is the current user, this is the current oauth client. This way code can safely assert that no data is accessed that should be out of reach. More importantly you can also carry an audit log around that can be fed with events from anywhere in the system. I have seen a lot of buggy code where someone built an API that takes an object's primary key for modification and just *assumes* that it belongs to the right tenant but in reality belonged to someone else. An execution context can help avoid this problem.
I'm writing an angle crate right now, and I legitimately have 15 constants named `PI`.
Not to the extent people want to experiment. Macros allow for subset, like \`async!\` and \`await!\`, however, something like \`try fn\` wouldn't be possible through a macro, nor would \`?\`. The broader point is that rustc should primarily invest into features which would allow more language experimentation outside of rustc.
I'd love to hear the reasons, in the same right now, went from python to java to c++ to c to go now in my 10 years of development, going to university this year for my first ever classes. Making the jump to rust feels right but I'm just curious on some of the reasons!
I actually wrote my own RwLock around a AtomicUsize simplified from spin-rs.
You know you want to learn Rust so go ahead. 
Debian is embracing Rust, so is Fedora. I think Rust by its nature will find it's way into the hands of library and application development. C is everywhere but many people never touch it, Rust will probably be somewhat the same. Go taps more into the Python/Java crowd who want a more direct solution to backend services and distributed computing. I hope that Rust maintains its consistent upward rise for a good while to come. I'd be fantastic if you saw companies, as their guru C++ coders retire, that their flagship applications get ported over to something like Rust. Rust might become the goto application language for cross-OS products, where the developer doesn't want to maintain a C# and Swift version of some codebase. "RIIR In Due Time where sensible?" RIIRIDTWS... may those letters appear in that order ever again, but you get the point. I also think Rust has a real strong chance to dominate the embedded space in 10 years. It might not push out Ada or Misra C from the automotive or gov. contractor space) but most certainly could overtake C in the embedded enterprise world for things that LLVM targets. Finally, the HPC world is due for a new language... Rust can surely defeat Fortran if nothing else right? RIGHT? *Sharpens stick* Now if someone just creates an MPI with cross-node lifetime semantics, then I think we are really talkin!
Anything we tell you will be speculation. This isn't a commitment: I recommend adding Rust to your tool kit, along with other languages. Building a diverse toolkit of languages lets you apply a proper tool for whatever challenge you face.
Same thing for arch (and any distro, probably), rebuilding only happens if there's a special reason to do it.
I think what you want is to use `failure::Error` with backtraces turned on. Note that having `assert!(true)` is actually a no-op, and will not have any effect on your `file!()` and `line!()` macros. What it means is that if you have something like: macro_rules! mac { () =&gt; ((file!(), line!())); } When you use `mac!()` you do not get the file/line where `mac!` is defined but where the macro is called. If you don't want to use `failure::Error` and still want to exhaustively enumerate and match on any relevant error your function could generate, you can add a `failure::Backtrace` to your error variants manually.
Cool. So gfx-portability could be added to vulkan-icd-loader and then one could use it like people have been using other shims. Does that sound right? I wonder if Blink and Firefox have started on such a project to keep supporting WebGL on the next macOS.
Cool. So gfx-portability could be added to vulkan-icd-loader and then one could use it like people have been using other shims. Does that sound right? I wonder if Blink and Firefox have started on such a project to keep supporting WebGL on the next macOS.
And this is exactly why I use debian-based distributions. Different strokes for different use-cases, I guess.
This. When you provide stability and security guarantees to something as large as an operating system distribution for years, being the exclusive vendor of the software that comprises the system becomes the only sane strategy.
You right, I'll go ahead, thanks 
I understand, thanks 
How can I get the line number and file from an error type that is like rand::Error when converting it into an error that is compatible with failure so that ? will automatically convert it to MyError but with line and file information? 
I think that's enough to convince me, thanks for the info
Dipping my toes in the Rust\-y waters as an experienced C# developer, "magic" context is one of the things I hate most about C# (though particularly ASP.NET). There's something to be said for always knowing the current tenant, or adding locale information automatically, but it's also global, mutable state, which means something bad is going to happen eventually. That doesn't mean this project isn't cool or useful (I think both are true), but I worry about people using it as a crutch instead of a tool.
The reason I learn Rust is because it’s the only language with mainstream hope which also sits at the frontier of programming language theory (PLT). C and Go arguably comes from the same age PLT wise, while Python, Java, C++ the next, and MyPy, C++ template the next, but all of them quite dated. Rust is mostly caught up with modern PLT and the explorer in some aspects (like lifetime), so it is more future proof in some sense. Rust community is also the best. No flame no bs, all business with inclusion and kindness in mind. Open but principled.
Anyone knows of a GUI tool for configuring cargo and executing any related functions that's independent of an IDE?
The community is a big thing, tried to learn Haskell, and along with the difficulty from purely functional languages, the community made it very hard to continue. The PLT is something I never thought about as well
It sounds interesting. I haven't really looked into qtum in detail yet, so I'm probably not qualified enough for the job, but I still have a question :) I've become skeptical of smart contracts in general after the DAO hack etc. How are you addressing the problems of scalability / sharding, the problems of proof-of-stake and updating / fixing of deployed contracts? :) Also, how does Qtum's vision differ from Cardano's?
The problem is that very often you just can’t pass stuff because there is no place to attach data to. In particular systems like Sentry depend on being able to automatically collect relevant information and without there being an implicit flow context they can’t be very useful. Also I disagree with ExecutionContext being global state. 
You will need to introduce a macro call in order to capture file/line information. You might be able to abuse something like the following (untested, macro2ish syntax): struct WithSourceLocation&lt;T&gt;(T, File, Line); macro locate!($it:expr) { WithSourceLocation($it, file!(), line!()) } external_result().map_err(|e| locate!(e))? impl From&lt;WithSourceLocation&lt;ExternalError&gt;&gt; for MyError { ... }
If you like emacs with evil-mode, check out [spacemacs](https://github.com/syl20bnr/spacemacs) too! It's basically emacs with preconfigured addons and layers. I don't use anything besides this for development in Rust.
what about memory usage? were both versions single threaded?
The best no-op macro is the no-op macro: macro_rules! noop { () =&gt; (); } Using this macro will have absolutely no effect on your code, and in fact will not even be compiled into your debug builds! The `file!` and `line!` macro documentation says that the macro doesn't always spit out the file/line where it's written. This is correct, but it does not fuction how you seem to think it does. Rather, the file and line refer to the location where it was expanded. That means, if you have a macro that expands to contain a `file!` or `line!` macro call, the resulting metadata will be for where the outer macro was called, not where it was defined. This is true through any number of macro calls, which is what the documentation refers to. Wherever you call the `macro!()` from within a `function()`, that's where the `file!`/`line!` metadata gets expanded and turned into the containing file's name and line number.
I wouldn't compare the growth of languages to decide which one to learn. I always just learned the one that I liked more (e.g. I dislike GCs and I like strong static typing and fast performance). I've been using Rust almost every day since 2014 and I've been using it in production at my current job for over a year and I'm very happy with it. (Before that I was using C/C++ and D but I was looking for a C++ replacement without a GC, so I completely skipped over Go when it came out.) Will Rust grow as fast as Go? Hard to say because technically it's better™ but simpler languages usually have more traction.. But does it matter? You only need 1 Rust job and I'm sure there will be many more as time goes on. Also you could introduce Rust at a job that wasn't previously about Rust (that's what I did). If you learn Rust, I'm sure you won't have difficulty finding a job in 5 years (or even now, if you're willing to relocate). You have to decide what you care about, idealism vs pragmatism. If you're totally pragmatic and worried about not finding a job you'd just stick to the popular but simpler languages but it might not be that interesting or rewarding. If you're totally idealistic you'd maybe use Haskell (and even then you'd probably find a job). I think Rust strikes a good balance between idealism and pragmatism, it's a language that's very useful and practical but also incorporating many nice things from PLT research. So I'd choose Rust over Go any day.. (I also chose PureScript over Elm for writing the frontend to my Rust server at work.)
You just implement any traits you want, one `impl` block for each trait. 
Welcome to the community, hope you stick around! I wish I had run into something like Rust when I was in school, I didn't really branch out into different languages until a few years after I had started working.
Isn't that the direction that Cardano is going in?
This all sounds exactly like I wanted to hear. And even if I don't get a rust job, pet projects in rust will be fun and fast as well. Also Cargo seems awesome
TIL Rust is supported on OpenVMS... I haven’t touched VMS since 1998.
A good and common pattern is to use a log manager at the system level and run a process to ship logs from there to an off-host logging aggregation service. For example: Your app logs to stdout, journald captures stdout and manages storage capacity/log rotation, and an agent like fluentd or logstash works on sending logs from journald off to Elasticsearch/Loggly/Logentries/whatever.
I think [this article on C#'s ExecutionContext](https://blogs.msdn.microsoft.com/pfxteam/2012/06/15/executioncontext-vs-synchronizationcontext/) is a good primer. They become important when you're working asynchronously, so a logical unit of work might be executed on multiple threads at different times. You need any ambient context to follow that logical execution wherever it's picked up. In order for all this to work you need the points at which your logical unit of work moves around to all integrate with the execution context. In .NET, there's a global thread pool in the runtime that will implicitly do the work of capturing the execution context and making it available on the new thread. In Rust I think this would hook into task-local data with futures? I'd like to see something like this too because, like u/mitsuhiko described, there are cases where ambient state makes a lot of sense, even if the concept itself seems a bit dirty. Besides locales, the case I really care about is contextual logging, where you can build up context to log without having to explicitly pass loggers as arguments. It's pretty much the same as the tracing usecase. I spent a little while [playing with a similar-ish idea](https://github.com/KodrAus/log-enrich) which was specific to the logging case.
Turns out that nearly every 'new' JIT and interpreter languages want to have their own package managers for cross OS installs though. From the top of my head i can think of maven and pip. These invariably fail hard when the users try to use 'native' libraries though and even for the platform independent bytecode it often requires hacks to distribute on the foreign package manager, so i'm glad to see a project on debian to make it easy for rust. 
Do you plan to use immutable mapping like PEP 567 in Python?
I doubt it. Languages rarely win out based on merit alone. *Seriously* ask yourself if you can picture a horde of college hires effectively making use of Rust, during a 4 month stint at whatever Megacorp. It's not a slam against Rust, but think of why an objectively terrible language like JS tops those charts.
Are you calling `std::process::exit()` to exit your program? If so, no unwinding will happen and nothing will be dropped.
Thanks, I understand how it works now. I find the official Rust documentation to be very confusing and commonly find myself misunderstanding it in ways like this :(. Thanks for your suggestion of how I may go about this, it will give me something to work with. I also find the Rust error handling situation to be very lackluster, it's unfortunate. Honestly my two least favorite parts of Rust are the documentation and the error handling situation, so pardon my negative sounding rhetoric but I'm simultaneously dealing with my two least favorite parts of an otherwise stellar language. 
Traditionally, the script that tells the daemon to shut down is responsible for removing the pidfile; you don't want to depend on the daemon to clean up itself, since there's many ways for a daemon to exit that don't involve calling `Drop`. You don't mention exactly what you're trying to do with this daemon, but as somebody who's written production network services, I heartily encourage you to *not* daemonize, but instead use a smarter service manager like [supervisord](http://supervisord.org/) or [nosh](https://jdebp.eu/Softwares/nosh/index.html) or [systemd](https://freedesktop.org/wiki/Software/systemd/). Moving the smarts from the service to the manager means that all your services get simpler and easier to debug, easier to monitor, and easier to control.
edit: text is fixed with https://github.com/gfx-rs/gfx/pull/2129
I think what mgw854 is saying is, while it's not global to the program, it is shared by every component operating in the same flow. For all intents and purposes, it behaves like a global in a single-threaded program. (In Rust terms, an ambient ExecutionContext behaves, in effect, like it has a `'static` lifetime.)
With TWIR now being pinned for a couple of days should this become a valid place to submit crate/quote nominations? Would that increase submissions (I'm not sure)? 
I find Rust's documentation to be some of the best for any language I've used; where are you coming from that you feel Rust's is lackluster? I'm genuinely curious; if there's an ecosystem out there that does it better I want to see it so I can hold myself to that standard. All of the previous languages that I've used -- Java (JVM), Kotlin, C# (.NET), Swift, JavaScript -- all of them suffer from awkward to navigate documentation such that Google rather than the docs is where I turn to find information. And contrary to what seems to be your position as well, I find Rust's error handling to be very nice. It forces you to consider how an operation might fail and handle that, as opposed to exception systems where everything might fail in any way and bubble up through your entire stack. That said, if you want to "fire and forget/log then recover generically" with Rust errors, `failure::Error` is the way to go. _Any_ `std::Error` or `failure::Fail` can be put in that box, and you get a backtrace along with it -- your whole unwound stack of file/line information as you'd see in other languages' exceptions' backtraces/stack dumps. But I'm sure we agree that at least Rust's error handling is better than Segmentation Fault. Core dumped.
Actually I think I was trying to conflate error handling and logging essentially, I will instead separate them and make use of [https://docs.rs/log/0.4.2/log/](https://docs.rs/log/0.4.2/log/)
Aha I get it. Thank you! I’ve also seen a few mistakes like this, and it gets worse when a codebase becomes large so I can definitely see the usefulness. 
Or rust-lang.slack.com. :) Or even the Facebook page!
If you know what the word blockchain means, then you have enough experience on the blockchain side, though knowing some solidity and how current contracts work would be useful for designing a better way for them to work. For scalability, our plan is to rely on off-chain networks like state channels, payment channels, lightning etc. We do not think any platform currently has a practical and secure way to do sharding without compromising the decentralized benefit of the blockchain. Right now we are focused more on payment channels and to extend that to state channels later so that smart contracts can be scaled. For fixing deployed contracts, we are side stepping that problem by making it as simple as possible for people to integrate governance (and thus upgrade mechanisms, emergency stops, etc) into their smart contracts and Dapps. We developed a solution to this called Decentralized Governance Protocol and this is currently deployed on our public blockchain to handle certain parameters like block size, gas schedules etc. It is implemented currently as a Solidity contract though, so it should be very easy to integrate into your own smart contracts. We want to make governance a first class thing you include even in hello world because most smart contracts have some degree of centralization, and even if they don't the DGP stuff is compatible with a completely decentralized system as well (ie, relying on user votes, etc). For a more... "forced" form of governance, EOS is a project that seems to be going the other way of doing governance. In EOS, validators can vote to reverse payments, freeze accounts, etc. We believe in this being an opt-in system so that each Dapp can choose their own form of governance and how centralized they wish to be. Cardano's primary benefit according to their community seems to be that it is written and uses a functional language (ie, Haskell and Plutus), but this does by no means guarantee security. Even the buzz word of "formal verification" does not guarantee security in this space. There are also a lot of things promised that do not yet have an answer. We have quite a few other differences. We will allow multiple programming languages on the platform, functional and non-functional alike. We use a more satoshi-like version of proof of stake where anyone can stake, and there is no fixed number set of validators or voting required to take part in it. Also, Our philosophy is quite different. Our tag line is "the blockchain ready for business." We specifically pride ourselves on NOT being a research project. We feel there are enough projects that are testing new technology, new approaches, and new research. Our approach is to use tried and tested technology and to let others work out the kinks before integrating it. The x86 VM was planned exactly because it is so well known and so much existing technology and tooling exists around it. We believe that research is important, but right now the space needs stability, rather than more ways to lose hundreds of millions of dollars with programming bugs. Either way, we are not maximalists and welcome more projects in the space and alternative approaches. There is definitely room for more than one blockchain in the world. 
Yes, Cardano does [semi-formal development](http://www.well-typed.com/blog/2018/05/semi-formal-development/) using Haskell and QuickCheck.
Primarily I have worked with C. Maybe the thing I am used to is just extreme straightforwardness and simplicity that comes from C, which makes it hard to ambiguously interpret things. I mean, I totally can see how the file! and line! macro information was intended to be interpreted, but I imagine you understand how I misinterpreted it as meaning that it logs the line that the previous macro was called from linearly through the code rather than in depth of nesting. It is ambiguity like this that I keep finding myself stumbling on with Rust documentation, essentially I find that my intuitive understanding of ambiguously phrased things in Rust is contrary to the correct intuitive interpretation, whereas in C I never really ran into this issue. I was considering making another thread for this question, but I don't want to clutter the main page and it is related enough. Why do Error types have display associated with them for essentially the error message? Shouldn't the error message be logged prior to return the error type that has the message associated with it? Here is how I did error handling and logging in C just for reference: #define logMsg(message) loggerF(message, __FILE__, __LINE__) #define logErr(message) loggerF("Error: "message, __FILE__, __LINE__) #define logWrn(message) loggerF("Warning: "message, __FILE__, __LINE__) And void loggerF(const char *message, const char *file, int line) { size_t maxTimeStampBytesize = 200; char timestamp[maxTimeStampBytesize]; /* Get the current timestamp, if this fails ensure timestamp array is NULL * terminated */ if( !getTimeStamp(timestamp, maxTimeStampBytesize) ){ printf("Error: Failed to get timestamp for log file\n"); timestamp[0] = '\0'; } /* Output the log message to the terminal */ printf("%s in %s : %i at %s\n", message, file, line, timestamp); /* If initLogFile has not been called then we are done logging. There is * no need to NULL check file or message, they are not dereferenced and * the printf functions will printf them as the string (null) */ if( gLogFile == NULL ){ return; } /* Log the message to the log file */ if( fprintf( gLogFile, "%s in %s : %i at %s\n", message, file, line, timestamp ) &lt; 0 ){ printf("Error: Something went wrong logging to the file\n"); } /* For some reason it is never writing to the file without this TODO */ fflush(gLogFile); return; } Using it (snipped): /* Block waiting for the cloned redirector process to signal it initialized */ if( read(stoplight[0], &amp;throwAway, 1) == -1 ){ logErr("Failed to determine if network redirector initialized"); return 0; } /* Put this process (not redirector) into a new network namespace. Note that * this requires the CAP_SYS_ADMIN capability. */ if( unshare(CLONE_NEWNET) ){ logErr("Failed to isolate with network namespaces"); return 0; } return 1; A caller of that previous code snippet: /* Isolate the process from the network */ if( !isolNet(REDIRECT) ){ logErr("Failed to isolate the network"); return 0; } So coming from something like this I am rather overwhelmed by the Rust error handling situation where there are numerous third party packages and seemingly hundreds of error types that need to be converted back and forth to each other and the error types have messages associated with them but there are also third party logging crates, and it is like why do errors have messages associated with them if you log separately from the error type itself? It is just very complex and confusing to me coming from C but I think I am starting to get the hang of it a little. 
Is this basically a Cow? You take a ref at some stage to get access. If you mutate it then it will split off. 
I'm somewhat surprised to hear that about the Haskell community. Would you mind elaborating on why you felt discouraged by them?
I think that's a good argument for why Rust won't replace *every* language, but I don't think that's a good argument for why Rust won't succeed at replacing *some* languages.
Re: growth of Go, don't forget that its backer has plenty of resources to ensure that all the necessary infrastructure libraries get written and battle-proven. Even though I think Rust is a *far* better designed language than Go is, it's not always about how sensible and fast and "nice" something is.
Essentially I'm making a [rocket.rs](https://rocket.rs) webserver that will be made into an archlinux package with a systemd service. I was also looking at the `systemd` crate, but I couldn't find anything in it concerning pidfiles, which (as far as I understood), I need for the systemd `.service` file. Am I correct in thinking that?
Here are my reasons: 1. It's the first language I've come across that is able to play in *every* space. You can use it in micro-controllers and to build operating systems (competes with C). You can use it to build low-level libraries that can be called from other languages (Competes with C and C++). You can use it to build applications (Competes with Java, Go, PHP and Python). You can use it to run web applications in the browser via wasm-pack (Completes with JS). And finally, in the space that I work in (blockchains), there's a lot of foment in figuring out how smart-contracts work. I'm convinced that wasm will win out as the smart-contract platform in the end, and Rust is well positioned to own this space as well. 2. I've always been a big believer in static analysis. At every job I've worked in, I've been *that guy* pushing to get static analysis as part of our testing workflow. In nearly all other languages I've worked in, static analysis of code to ensure that it's correct has been an optional add-on feature afterthought. In rust, static-analysis is part of the damn compiler, and it's amazing. If you get the RLS hooked into your IDE, such that static analysis happens *as you code*, your workflow, which was once code-run-debug, now becomes code-debug. If your IDE is happy, your program is probably correct. It's amazing, and makes you extremely productive since there is almost no context-switching. 3. There are a lot of decision points in making a good programming language, and rust has made nearly all good decisions. Dependency management was done correctly with cargo. Using the LLVM as the backing toolchain was the correct choice. It follows the return-error / throw-panic error-handling pattern (same as Go), which in my opinion is the correct way to handle errors. It's done Traits and Generics in a really great way. 4. It's *fast*. SO FAST. The only thing that concerns me about rust right now is that the community seems to be heading down the event-loop / futures approach to idiomatic concurrency, which IMO is the wrong choice (I prefer green-threads / channels). Overall, I think Rust is the language of the future. 
No, I'm not.
Ahh I think I finally figured it out. From the failure arm of a match statement: error!("{}", MyError::SomeError); Err( MyError::SomeError)
This is getting pretty exciting! I've been only casually observing this project and didn't realise it had come so far! More generally in gfx land, do you think there's any real future to a pre-ll style API on top of gfx-hal, or if anything like that exists would it be only for compatibility / easing migration to a more sophisticated high-level API (e.g. based on framegraphs)? I am by no means married to any particular API, so please don't take this as nagging about a compatibility layer. Rather, I'm curious about your thoughts on what the future of Rusty graphics APIs _should_ be — i.e what excites you?
I am in a similar predicament after trying to write a non standard machine learning algorithm in Python. Because the algorithm is non standard, I'm not able to fully use the numpy or tensorflow primitives so I wrote parts of it in native Python. It is insanely slow \- several orders of magnitude slow. I'm a former C programmer but that's an awful language so I'm learning Rust instead. I'm sure the speed improvement will be worth the effort!
On a related note, I've written Rust programs which take 11 minutes to run when compiled in debug mode and run in *1 second* compiled in release mode -- a 700x performance improvement! Given that a dynamic language like Python typically doesn't perform *any* static optimizations, an equivalent Python program likely would have taken closer to the 11-minute figure than the 1-second figure.
I'm more an observer than a coder, but I thought I recall that integrating green-threads required significant changes to the language. I may be mis-remembering.
If you have a service named `/usr/bin/my-web-server` that runs in the foreground (doesn't demonize) and writes errors to stdout/stderr, then you can create a `.service` file for it that looks like: [Service] ExecStart=/usr/bin/my-web-server ...and that should be enough, you don't need to mess with the `systemd` crate or anything like that. By happy not-so-coincidence, that's exactly the kind of program you'll get if you create a new Rust program with `cargo new` and add the `log` and `env_logger` crates, nothing special required. If you want to do fancier things, like have systemd automatically listen for incoming requests and only start your service when one arrives, or if you want to log arbitrary key/value pairs instead of just lines of text, then there's things in the `systemd` crate that will help you, but that's just icing on the cake, not required functionality.
Yep, returning an error type is like the `return 0` that you're using in the C code as a magic value for error, but typed and with associated information. If you just want to log it, you'll want to `eprintln` or `log` it. Error handling is hard because error recovery is hard. I'd argue Rust is making you consider some of what "just worked" in C but was really a ticking time bomb of a problem waiting to happen. (Does it handle concurrent access? Do you actually check the return value and handle the error case? And so on) And on the documentation side, keep in mind that the C documentation has had _aeons_ to be revised for perfect clarity. Rust is a lot closer to C++ in complexity and is much younger, so the docs are a lot younger as well. Documentation shortcomings are considered bugs though, so feel free to open a bug at rust-lang/rust on GitHub describing whatever shortcoming and hopefully giving us an actionable direction to improve, it's one of the project's explicit goals to provide a best-in-class documentation experience.
Wow rust huh. Impressive.
"Use a scripting language when you want code fast, use a compiled one when you want fast code"
Possibly for suggestions, but I'd like to keep voting in one place, and as much as I love our subreddit, it's not the official medium.
Didn't know this slack before. It is new, and hosted by core member?
Not OP but anecodotal evidence from my perspective: * There are good beginner buides by the community, and there are good articles from devs / academics but not much community effort on bridging the gap * The jargon. As someone who is quite familiar with the maths side of CS, the jargon that the Haskell community likes to use is fine for me, but it could be daunting for other users * Fragmentation of the community. With different language features, it has caused some amount of fragmentation in the community These are the 3 things I could think of, but all in all the Haskell community is pretty good with their interactions. The barrier to entry is more of an issue of the language initiaially having much more of an academic focus than a community issue.
I though Rust had green thread support beforehand, but removed it later on (before 1.0). I'm not sure how much of the rationale or discussion from back then is still available. 
You should look into Cython in the future. It compiles your python to C, which usually makes it faster. Add a few type annotations and you get an even bigger speed boost. It works especially well for scientific code. I have found it is a good first step before going to another language.
Aha, didn't notice that, sorry. It is primarily intended for embedded use. Writing into HW registers and such.
You're creating dangling pointers here: let result = format!("{}: {}", server, status); results.lock().unwrap().push(&amp;result); first line creates new `String`s, second one saves a reference to them and lets the `String`s backing them go out of scope where they are dropped. Instead, save the `String`s directly and return `Vec&lt;String&gt;`. A `String` is just a fat pointer, anyway. After joining you must consume the `Arc` and the `Mutex` to get the inner value out or you will again have dangling references. Arc::try_unwrap(results).unwrap().into_inner().unwrap() Lots of unwrapping because in principle the Arc could still be shared and the Mutex could be poisoned (i.e. some thread panicked while holding the guard). Also, `'a&lt;Arc&gt;` is not valid syntax. Full code: http://play.rust-lang.org/?gist=b4cc6e5e9b311ca97efff19fc7a3e468&amp;version=stable&amp;mode=debug The above gets the types right. Another issue is that you're manually handling raw threads which is rarely the right abstraction use. Look into some crates like `rayon` which lets you iterate in parallel.
I'm not sure I see a similar trajectory as for Go. Go is pretty good for it's niche, doing backend code and being able to just crank out good enough code even when you're a junior dev. And there's a *lot* of demand for backend code that's not terribly technically complex. The niches Rust is good for are things like complex libraries and native desktop applications, and there isn't a similar long tail of junior developer friendly work opportunities there like there are with Go. And if you're thinking of setting up a brand new shop, Rust has the learning curve. Even if Go didn't have the existing market advantage, you would have an easier time teaching new hires Go than Rust. You'd need a good reason why Rust is better for your particular problem than Go to pick it up.
It's not new but not officially supported. Only the forums (users and internal) and the IRC are officially supported.
Just checking here: Does the current version of Clippy (0.0.206) not compile on the current version of rustc (1.28.0-nightly), or is it just a problem I have? For reference, this is the error I get when running `cargo +nightly install clippy`: error[E0532]: expected unit struct/variant or constant, found tuple variant `hir::Visibility::Crate` --&gt; /home/jolson/.cargo/registry/src/github.com-1ecc6299db9ec823/clippy_lints-0.0.206/src/utils/inspector.rs:56:13 | 56 | hir::Visibility::Crate =&gt; println!("visible crate wide"), | ^^^^^^^^^^^^^^^^^^^^^^ not a unit struct/variant or constant help: possible better candidates are found in other modules, you can import them into scope | 5 | use rustc::session::search_paths::PathKind::Crate; | 5 | use syntax_pos::symbol::keywords::Crate; | error[E0532]: expected unit struct/variant or constant, found tuple variant `hir::Visibility::Crate` --&gt; /home/jolson/.cargo/registry/src/github.com-1ecc6299db9ec823/clippy_lints-0.0.206/src/utils/inspector.rs:348:9 | 348 | hir::Visibility::Crate =&gt; println!("visible crate wide"), | ^^^^^^^^^^^^^^^^^^^^^^ not a unit struct/variant or constant help: possible better candidates are found in other modules, you can import them into scope | 5 | use rustc::session::search_paths::PathKind::Crate; | 5 | use syntax_pos::symbol::keywords::Crate; | error: aborting due to 2 previous errors For more information about this error, try `rustc --explain E0532`. error: failed to compile `clippy v0.0.206`, intermediate artifacts can be found at `/tmp/cargo-installKEzSxd` Caused by: Could not compile `clippy_lints`. To learn more, run the command again with --verbose. 
It doesn't really matter. Any reasonable company will find your knowledge of Rust valuable even if they don't strictly use Rust in production... it shows your heart is in the right place and you care about learning the important stuff. What you learn using Rust is applicable in every language and it will change the way you approach code structure. Go and Rust have pretty different targets, Go is all about productivity and simplicity, it'll always have a bigger user base.
These things. Also, when you uncomment the rest of `main` in the `play` above you'll find more lifetime errors. Rust can't tell that the threads are being `join`ed in `ping()`, so it worries that the slice containing `server` will be dropped before it is used. Here's what I ended up with. Seems to work. Notice that all the lifetime annotations are gone, which is nice. Cloning `server` is slightly inefficient, but it's hard to believe it will matter in practice. I prefer to return a `HashMap` so that I can do other things with the ping results. On my Linux box, the flag to get a ping count is `-c`. This won't run on `play.rust-lang.org`, since it requires a `ping` command that is nonexistent there. But it should build there and should run on your box. &lt;https://play.rust-lang.org/?gist=600b30ceca028120b47b0c388bf06d64&gt;
Internally it uses an immutable hashmap. 
It behaves similar to a thread local that can “fork”. Thread locals are not considered global state.
I have bought two of them ebooks with 20usd thanks
It is not necessary to remove pidfile. Basically you store the pid in the pidfile and on startup you check if the file exists and if the process with pid from file exists. If you want to be even more certain then you can also check executable name
Any chance you have an example of this? I'd be curious to see what kinds of things can be optimized that heavily
Similar. Yes. It’s based on an immutable hashmap internally. 
Is this using Vulkano?
My toy ray tracer has similar characteristics. Tiny functions, often with just a few arithmetic operations each, get inlined and auto\-vectorized. Many of these functions return several pieces of data simulatenously, such as a (projection, rejection)\-tuple. If only one of these is needed at the call site, they are also subject to dead code elimination after inlining. What amounts to several dozen function calls in debug mode can end up being optimized into just a hundred or so SIMD\-instructions in release mode. When sitting inside a hot loop this makes a huge difference: the ray tracer is \~30 to 100 times faster in release mode depending on the scene.
You could take a look at Sentry, as a remote logging solution.
I had a similar experience with python/pandas and rust. I rewrote a relatively straightforward etl from pandas into rust; it was 100x faster and used 20% of the memory. (And there's easy parallelization not factored in yet). In the pandas version, I had to melt and then pivot, whereas in the rust version I just used a hashmap. So it was kind of apples to oranges. (But I guess idiomatic for the languages/framework). Still, with rust, not only was it easy to do a fast version naively, it was easier to optimize. It was also easy to see the shape of data at all stages of transformation. This actually makes breaking code into functions a lot easier for me, since I always know the shape of data at function boundaries. And, of course, there's just fewer weird bugs. Now, I just have to convince my company that the rust version should be the official version 😀.
Almost all "zero cost abstractions" are only zero cost thanks to the optimizer. It peels the abstractions away.
Here's the RFC to remove them https://github.com/rust-lang/rfcs/blob/master/text/0230-remove-runtime.md
&gt; I find the official Rust documentation to be very confusing and commonly find myself misunderstanding it in ways like this Please file bugs! I'm happy to improve things, but I can't do so unless I know what's confusing.
There's a number of packages like https://crates.io/crates/pyo3 that may help; I'm more of a Ruby person so I can't tell you which is best.
And if you absolutely must run a service which daemonizes and creates a pidfile... you want to use `PIDFile=` which already "will remove the file after the service has shut down if it still exists"
No. It uses [gfx-hal](https://github.com/gfx-rs/gfx)
Is there a way we can see your code? I'd like to see how you did it, I also like to see your implementation of the python code :) 
Here is the Original Presentation: https://fosdem.org/2018/schedule/event/rust_qt_binding_generator/
Yes I think so - i mean no one is required to use all the features of rust all the time... even if you use it to replace simple scripts on your desktop with rust, there is huge benefits from the language. just pick the bits you need.
Hello, this is something that's been on my mind for a long time. My first approach to building nom 4 years ago was using functions and closures, but Rust was very far from supporting that. With impl trait 
Another route to try is to annotate your numeric functions with the numba @jit decorator. 
Hello, this is something that's been on my mind for a long time. My first approach to building nom 4 years ago was using functions and closures, but Rust was very far from supporting that, so macros were (and still are) a good way to write those parsers. Especially with nom 4 that removes a lot of the issues people get compiling parsers, and showing good error messages at compile time. With impl trait available, we have a shot at getting nom where I planned it. I'll be rather cautious about it, though. A [huge number of projects](https://crates.io/crates/nom/reverse_dependencies) now relies on nom, so I have to at least retain some backwards compatibility when changing the internals. Mainly, finding a way to replace the code of macros, to update old parsers easily. There are two things I'm especially wary of: compilation times (macros generate a lot of straightforward code, so it compiles faster), and futures-style huge compilation error messages. If I can tackle those, we'll probably get a fun new nom to play with :)
as a side note, about the idea of returning a closure for partial parsing: this is something I wanted for nom at first, but it was not possible in Rust. It might be possible now with impl trait, but I don't think it's worth the effort. Streaming parsers get much easier to write when you have well defined "stop points" in your state machine, and it gets very easy to define such [state machines with generators](https://github.com/Geal/generator_nom/blob/master/src/main.rs#L71-L159)
Why is it so much cheaper at packt (10 Euros) than amazon (30 Euros) for the ebook version? I'll give it a read for sure :)
Cargo is such a pleasure..
Decorate your core functions with @numba.jit and watch a huge speedup
&gt; Rust is mostly caught up with modern PLT Uhh, Rust is great and all that, but we don't even have (the building blocks for properly doing) Monads yet!
Please refrain from language bashing. Sure, ECMAscript has its warts, it may even have its "WAT"s, but it's not all terrible, and there is nothing objective about calling it such.
I believe slog supports this with their “drains” feature, especially once you pair it with slog-async. Out of curiosity, why are you interested in file rotation? Are you concerned you’d drop logs if you just logged to stdout? 
Actually I want all my logs persisted and archived daily so I don't end up with a huge file. I don't want any logging to stdout, only fs. I prefer to tail if I want to trace live logs.
Theano is pretty flexible. 
Yes, both single threaded. Python memory usage, especially pandas, is generally atrocious. 
if rank\_a \&gt; rank\_b { if self.change\_parent(b, b, a) { return true; } } else if rank\_b \&gt; rank\_a { if self.change\_parent(a, a, b) { return true; } } else if self.change\_parent(a, a, b) { self.increment\_rank(b); return true; } I see issue with this code. Imagine rank\_a == rank\_b, but some unions happened in parallel. And real rank(a) is higher know. CaS wil still succeed and now we have b as a parent of a but with potentially lower rank. This leads to rank being less then maximum hops from point to its representative and hence invalidating complexity guarantees.
Understood. Just so I can give you good advice: what environment are you running this service/application in?
On AWS \+ Ubuntu linux.
simd has been stabilized for a while now.
&gt; Fortunately, the book is on GitHub and accepts pull requests, so I have plans to send in suggestions for some alternatives. This is my #1 favorite thing about Rust: it's one of the few communities (tech or otherwise) with a governance model that is actually open and collaborative in practice (how many projects would let you submit pull requests against their official blog and regularly merge them?). And IMO this is one of the main reasons why other parts of the language design, etc are so well done. All (well most) of the things that 'everyone' knows should be changed actually get changed rather than being left for political reasons. Now to find company to work for with comparably good governance!
As I said to the author on lobsters: Glad you liked TRPL! I agree that some of the examples can be too abstract at times, it’s tough. Totally open issues if you want to talk about it; I’m open to improving things, but I’m also pretty particular about the book.
Lobsters taste with their legs and chew with their stomachs.
I've learned that it's unnecessary with systemd, but thanks. :) Are you getting `PIDFile=` from the `daemonize` crate? Because I didn't see that in the documentation.
Bad bot.
Thank you, DrKarlKennedy, for voting on AnimalFactsBot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
It's not quite an answer to what you asked, but I've had a fair amount of luck with logging to stdout/stderr and using [runit's svlogd](http://smarden.org/runit/svlogd.8.html) to collect those. For context, I generally like to allow operational concerns to be handled by the deployment environment; but then again, that's evidently not how everyone sees these things.
Unfortunately, this is the kind of cases where the developer has more information than the system. For example, the developer may know that: 1. Given the magnitude of the numbers, the maximum deviation is less than X, 2. A maximum deviation of Y is perfectly acceptable for this algorithm. For example, in the case shown here, since all values are small integers, there is no error at all: small integers are exactly represented in floating point numbers (and in f64, "small" means fitting in 52 bits). However, were you to sum 0.5, 1 &lt;&lt; 52 and 0.5, the sum differs depending on whether you aggregate the two 0.5 first (see [Playground](http://play.rust-lang.org)/?gist=4a1ebdbecab39459e8bcf2ad50857074&amp;version=nightly&amp;mode=release): fn main() { println!("{}", 0.5 + (1u64 &lt;&lt; 52) as f64 + 0.5); println!("{}", 0.5 + 0.5 + (1u64 &lt;&lt; 52) as f64); } which outputs: 4503599627370496 4503599627370497 So that if you care about precision, you should first sort the floats by magnitude (absolute value) and start summing from smaller magnitude first (to let them aggregate). --- If you can apply sweeping changes at the program level, you may want to consider [LLVM Fast-Math flags](https://llvm.org/docs/LangRef.html#fast-math-flags); in your case the `reassoc` flag looks right up your alley... though I'm not sure if rustc can emit it (or `fast`)...
It is? `std::simd` seems to be unstable: https://doc.rust-lang.org/beta/std/simd/index.html. The `stdsimd` and `simd` crates also seem to need feature toggles.
Looks much better than crates.io Simple search would be nice though
DrKarlKennedy has been unsubscribed from AnimalFactsBot. I won't reply to your comments any more.
Fantastic. How would I go about logging to the systemd journal? From what I can see `log` doesn't hook into it. For example, how would I specify the unit name? Would utilising the `systemd` crate accomplish that?
&gt; This post is the beginning of that adventure: my first impressions of Rust. I chose to evaluate Rust first rather than one of the other aforementioned contenders for a few reasons. [...] Also, I've been fairly critical of Rust in the past because that view was in-line with the opinions of my friends, but now that I've decided to go out of my way to learn a new programming language, I might as well use this as an opportunity to see if my criticisms were unfounded. Nice to see the author putting their foot where their mouth is :) Evaluating a language through rumors is rather unfair, although it is decidedly expedient and there's just not enough time to do everything by oneself.
&gt; Thread locals are not considered global state. That's a surprising opinion; I've personally always considered thread-locals as global state since they have the exact same properties. I'd love to hear your reasoning.
Yes, the sum is not exactly equivalent to the vectorized sum (that is the rounding I mentioned). However, it should be possible to restructure the program to give an identical output to the vectorized version. Then the auto-vectorizer could, in theory, vectorize it. It would be something like keeping 4 intermediate sums and summing the intermediate sums at the end. I have tried doing this in a few different ways, but I cannot seem to get LLVM to vectorize it.
[No, it hasn't](https://doc.rust-lang.org/stable/core/arch/x86_64/index.html). It's [stable in beta](https://doc.rust-lang.org/beta/core/arch/x86_64/index.html), but not in the actual current stable release.
I've never liked "implicit" data access, however I've generally had the luxury of NOT operating within a rigid framework, and therefore the ability to just pass my data implicitly as needed, which I recognize may be a fairly exceptional experience. I've actually operated within a distributed environment which had some implicit information (authenticated user properties, middleware/routing data) automatically forwarded in all sub-queries and it was invaluable (and transparent to the user) although it did not require implicit state as making a call to another service had to be done through the framework which was free to add any relevant data. I think the idea of ExecutionContext is a good improvement overall compared to global state (thread-local or not), most notably because of the `capture` facility. I have, however, two concerns: Leaks and Performance. Performance numbers will require some experimentation and maturity of implementation, so let's put that out of the way for now. Leaks, however, are always a concern about "global" state. As a simple example, imagine storing a credit-card information into an ExecutionContext to implement Amazon's 1-click functionality; in most cases the framework should ensure to clear this data before starting serving another user... but what if there's one code-path where it *forgets*? Have you given any thought about protecting oneself against such accidental leaks? Would it be possible to use **scopes**? For example, tying together `UserLogin`, `UserGeoData` and `UserPaymentInfo` in such a way that changing `UserLogin` (the scope identity) automatically purges the current `UserGeoData` and `UserPaymentInfo`?
Also, debian has strong policies when it comes to software freedom. They categorize the software after license. This way you can easily avoid proprietary software and all the issues that come with it. Stuff like npm, Cargo, etc are different here. You can e.g. easily upload binaries or minified js to npm and use them, but in debian that's not allowed: the source code of a a package must be present in source form, that is the preffered form you use to edit it. This issue showed up in the rust community [when the debian maintainer wanted to package mdbook](https://github.com/rust-lang-nursery/mdBook/issues/495).
&gt; Or even the prerequisite notion of type constructors. Meh. It's like complaining that a new house doesn't have solar panels yet. Given that there are plans to integer type constructors in Rust, I really don't see the argument here. It's coming, the language is ready for it so it won't feel like a bolted on addition. Everything's fine.
&gt; I'm more an observer than a coder, but I thought I recall that integrating green-threads required significant changes to the language. From a language perspective, there is no difference between an OS thread and a green-thread: both are threads. It would require a fork of the runtime and `std`, possibly pointed changes to the compiler, but those are implementation details and the new "forked" toolchain should be able to compile most existing Rust crates (there may be issues with crates linking to C libraries themselves using OS-thread-local storage).
Fast _enough_, I'd say. It's a pretty narrow path. Also I wrote python full time for years before rust so this was not a naive attempt. I still prototype models in python though because I haven't been able to figure out gpu + rust. 
&gt; I'd be fantastic if you saw companies, as their guru C++ coders retire, that their flagship applications get ported over to something like Rust. Hey, no need to retire me, I'd love to convert the C++ applications I work on to Rust ;)
Sorry, this was a systemd comment, not a rust one :). `PIDFile=` is a directive used in the `[Service]` section of a systemd .service file.
Jesus christ can we ban some of these bots already? I was fine with most of them but there really is no need to have 3 comments from 2 completely different bots that add absolutely nothing to the discussion.
Thanks for the confirmation. That'll work. :) 
&gt; ugh i'm the worst. False! :). You're awesome, Steve. Thanks for fixing this.
Global state is global as the name implies. If you flip it in a function it affects the rest of the application and code that runs concurrently. TLS does not have those problems. It’s obviously still a stateful API but nor global. Flow local data does not even have the problems TLS has as it’s async aware. 
Yes
&gt; Leaks, however, are always a concern about "global" state. As a simple example, imagine storing a credit-card information into an ExecutionContext to implement Amazon's 1-click functionality; in most cases the framework should ensure to clear this data before starting serving another user... but what if there's one code-path where it forgets? Execution contexts don’t have that issue as the flow “ends” somewhere so data is unlikely to reach a new request. It never propagates up the stack. 
&gt; TLS does not have those problems. I half-agree. All applications I have worked with use thread-pools or equivalent, as creating OS threads is so expensive: - so, indeed, TLS avoid concurrency issues, - however, it does affect any further computation on the thread as any global would... and since that thread lives from the beginning to the end of the application, there's little difference in that regard with globals. Also, TLS and globals both suffer from async issues: they are not coroutines/generators friendly, especially when those migrate across threads. Which is why, for me, they have such a similar set of problems (with mostly data-races to set them apart), that I might as well throw both in the same bag: "global state".
I've tried to convince LLVM to keep intermediate sums, \[and I think I've succeeded\]([http://play.rust\-lang.org/?gist=6c0b579888458efd7ec843dd5394aabe&amp;version=nightly&amp;mode=release](http://play.rust-lang.org/?gist=6c0b579888458efd7ec843dd5394aabe&amp;version=nightly&amp;mode=release))! I must say that the code is not the most straightforward one.
Yes! That seems to be right on the money. I had tried something similar, but without that `assert`/`assume` it wouldn't vectorize. Thanks a lot.
I completely missed this aspect of them. Could you explain how it works with: for user in &amp;users { if user.has_payment_info() { USER_PAYMENT_INFO.set(user.get_payment_info()); } // oops, forgot the else handle(user); } What prevents the second user, assuming it has no payment info, from reusing those from the first user?
TLS not working with async is why execution contexts exist. As of negative experience with TLS: everything is gray and nothing is black and white. People that build stateful APIs with TLS should not do that but throwing out the baby with the bathwater is absurd because it takes away a lot of very valuable possibilities. If TLS is all we get then async will forever be neutered because no TLS API works with it. Tokio already has contexts which are similar in nature but complex to work with and they leave many issues unsolved. 
Red Hat uses many languages. Golang is mostly visible in projects related to Linux containers - podman, cri-o, skopeo but they started using Rust also, for example in storage-related project stratis: https://github.com/stratis-storage.
I meant a rebuild of all the libraries if they updated rustc. Sorry that sentence is a little ambiguous.
&gt; If TLS is all we get then async will forever be neutered because no TLS API works with it. So... we are in violent agreement that your proposal of ExecutionContext improves on the current state since TLS has so many problems? In any case, I am *glad* and I want to extend you my *thanks* for working on this issue :)
That first `assert` seems wrong though. With some tinkering I managed to remove the inner assert by fixing the outer assert and increasing chunk size to 16: http://play.rust-lang.org/?gist=6cbb76e520db72dd040bb31e4026d843&amp;version=nightly&amp;mode=release Why it switches over at 16, I don't know. Compilers are mysterious things.
Post code? Anonymized vars is fine
Consider how long it usually takes for a theory to go into mainstream production, I’d somewhat consider 1990s “modern” and 2000s “edge” and 2010s “cutting-edge”. I look forward to when Rust will have Monad. 
Nothing. But that’s not what execution contexts are used for. This issue would also happen if you reuse a payment manager object or similar. The idea of the execution context is that you track TLS through the flow. It does not solve the resetting magically. It gives you an api where you can reset all in one go (suppress flow and run). 
&gt; I look forward to when Rust will have Monad. Oh, no doubt! But it will require from the team a serious amount of designing, experimenting, etc. before then.
Log to stdout or stderr, have syslog capture it, and use syslog to ship logs. All involved software is then stable and battle-tested, it's very easy to capture or observe logs in testing without having to reconfigure anything, and it's fairly difficult to accidentally produce unbounded amounts of log files.
Unfortunately the Haskell community has not been close to the Rust community in commitment to inclusivity. They’re improving though, and if you want to advance PLT-wise after Rust, (such as Monad and Monad Transformer,) Haskell is still the de facto gateway drug. 
There's also no clear governance structure. Who is in charge? Who it's guiding the language? There's no one guiding the overall course of the language so it's just kind of stagnating in a way. I used it for work but I tend to stay away from the community for the above reasons and this one. Why invest time in something I don't feel can succeed will and be run by people I trust
std::simd is not stable, std::arch is stable but in the next release in two weeks.
Thanks for spotting that! I've probably meant to use `%` instead `&amp;`. &gt;Why it switches over at 16, I don't know. Compilers are mysterious things. Yup, that's magic. I wonder which part of code is responsible for that and whether we can rely on that (probably we can't). Perhaps \&lt; 16 is just not big enough for LLVM to try harder?
By default, anything a service writes to stdout or stderr is sent to the journal, filed under the unit name (so if you have a file named `my-web-server.service`, you can read its log output with `journalctl -t my-web-server`. The `systemd` crate will let you specify *more* details, but even without that, it records a lot: try `journalctl -o json-pretty` and marvel at all the fields you didn't even realise were things that somebody might want to record.
on Linux at least logging to stdout and stderr is big performance problem, something about having to lock stdout on every line write you will not notice problem if you log less than several gigabytes of log files per hour though 
Yeah; it's true that the C stdio routines will aquire a lock around writes to the underlying file descriptor. But if that's an issue (and that's a big if) it's possible to take log4j2's approach, and log via a queue and writer thread.
first don't log to stdout/stderr, they are slow on Linux, and probably other OSes (just use standard file logging) second use log rotation, set to rotate log each hour, and have background job (shell script for example) that moves files each hour to different server (always leave current and previous hour on disk/don't move since current is being written into and previous might not yet been synced/unlocked). if you are filing disk in less than 2 hours you should really look into reducing amount of logging you do (or getting more disk space)
at previous job we noticed logstash kills CPU (uses 100% of a 28 core CPU) if you go over 10gb of log files per hour (this was 8 months ago, things might have improved), so we had to do custom solution, fluentd might not have same problem, so i suggest you try that one first
logging to stdout or stderr on Linux system can be very slow because of locking if you log a lot, better to use file logging
Disclosure: I work at Amazon. This advice will be beyond the scope of what you asked, so my apologies. That said: consider using the [awslogs](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html) driver in tandem with ECS/EKS. Our services generate around a gigabyte of logs per hour per host and it's _so_ much less operational overhead than writing to a file with daily backups. We write our logs to a container's stdout, and they appear in CloudWatch Logs seconds later. You can then forward those logs to Kibana/Elasisearch, Google's [Stackdriver](https://cloud.google.com/stackdriver/) or [Honeycomb](https://www.honeycomb.io/). To answer your original question, you can setup `slog-json` like this: ```rust fn log() -&gt; Result&lt;slog::Logger, Error&gt; { let log_file = File::create("/tmp/app.log")?; let drain = slog_json::Json::new(log_file) .set_pretty(false) .add_default_keys() .build() .fuse(); let drain = slog_async::Async::new(drain).build().fuse(); let log = slog::Logger::root(drain, o!("version" =&gt; "0.1")); Ok(log) } ``` To standard out: ``` fn log() -&gt; Result&lt;slog::Logger, Error&gt; { let drain = slog_json::Json::new(std::io::stdout()) .set_pretty(false) .add_default_keys() .build() .fuse(); let drain = slog_async::Async::new(drain).build().fuse(); let log = slog::Logger::root(drain, o!("version" =&gt; "0.1")); Ok(log) } ```
Thanks for the details. I'm already relaying the container logs cloud\-watch. In my Java servers, I simply configure log4j2, set an async rolling file appender and things just work. There should be something similar in Rust ecosystem. Regardless of deployment infrastructure. 
I just released my point process simulation crate: https://crates.io/crates/point_process
This is so awesome!
You can try installing from the latest source: `cargo +nightly install --git https://github.com/rust-lang-nursery/rust-clippy.git clippy -f` You'll need a relatively recent version of nightly (I used 2018-06-08).
No way I didn't know they started using rust, cool
Yeah I may use rust for now and later try Haskell again
Very true didn't think about that
Makes a lot of sense, let's hope my future hire will be like you 
Thanks! It works. What do I need to do in the future to stop using the latest source and get back on the regular release track
Once a new version is posted on crates.io you can run `cargo +nightly install clippy -f`. 
Well, at least in the Discord community I would ask questions like a new user of a language would ask, like what would *foo* be the best thing to use (think map vs foldl vs forM) and I would get people either a) ignoring me or b) acting like I should know what to use even though I had no idea since I was new. Putting 🤔 a lot after my questions as well lol Compared to the Rust discord community which would answer quickly and even get interested in what I was doing. Don't get me wrong, a couple of people in the Haskell discord were helpful but that was only a couple out of many. Especially when compared to other communities I've been in (Go,C++,Rust) it wasn't a fun time
I like all those ressons, I may Port my pet programming language to rust because of the speed (plus enums lol). Seems like the community really backs up the language in a lot of situations as well
I hope so!
&gt; It gives you an api where you can reset all in one go (suppress flow and run). This is what I had understood indeed. So this is where I was wondering whether it would be useful to be able to reset only *part* of the context. My reasoning is that you may receive a request containing multiple items, and want to use a scope of EC for the request details (originator, authorization, etc...) and another scope for each item (reset between each item). At the moment, it seems possible to simply a single large item per scope, which may be the best way to handle this anyway (as it encourages batching accesses to the EC).
This project shows testament to Apple wasting their time. It brings a tear to my eye.
Not OP. I have a use case - I'm writing a fairly complex server side git hook. Any stdout get sent to the client. I'd love to keep a reasonable amount of logs around in case something goes awry.
Right, the gfx backends (through portability) can be loaded through the regular Vulkan loader. For example, the gfx DX12 backend is currently being tested against Dota2 also, but it's loaded through the Vulkan loader. This is in contrast to macOS where the Metal backend is dynamically linked at the moment.
The Rust compiler itself certainly knows about dylibs, and using the `dylib` crate type in conjunction with `-C prefer-dynamic` will result in dynamic linking (`rustc` is still linked like that, for now, due to how syntax extensions used to work). Cargo is a bit different, in that it [allows setting the crate type](https://doc.rust-lang.org/cargo/reference/manifest.html#building-dynamic-or-static-libraries) but I'm not sure if further dependencies will also be dynamically linked. For Rust code, shared libraries are perhaps useful for saving space, but *not* avoiding the recompile of a dependency, as there is no tool to guarantee the result is even compatible (incremental recompilation is recommended instead). Note that this is different from dynamic loading for e.g. plugins, which you can do safely by compiling the plugin against the same dylib that is loading it. Again, there is no way to guarantee compatibility if you use different versions of the common library, *unless* you're using only a C API, which you don't change.
68 minutes to seconds. I am willing to bet you were doing something VERY wrong in your python code. We need to see code in order for this type of extreme claim to be considered true.
Done.
One of not too many gui applications in rust, and great functionality, very nice!
I don't see a problem? Writing the Encoder and Decoder impls for your codec type (that framed() takes) is up to you, so you can do whatever to implement the encoding/decoding to/from an enum. (But note that, as far as I know, just implementing serde's traits is not sufficient.)
A useful, featured rust app! Very nice!
That's a very precisely aimed question, I give you that, and a painful/hot topic, given the variety of (strong!) opinions our community members express. One direction of higher level abstraction is building libraries for state tracking, resource management, and framegraphs, and making them compatible with each other, letting the user pick what they need. This is pushed, almost singlehandedly, by /u/omni-viral with [gfx-memory](https://github.com/gfx-rs/gfx-memory), [xfg-rs](https://github.com/omni-viral/xfg-rs), [gfx-render](https://github.com/gfx-rs/gfx-render), and [gfx-chain](https://github.com/omni-viral/gfx-chain) libraries. Another one is to provide a `gfx_device_hal` backend for pre-ll, which I suggested in [an issue](https://github.com/gfx-rs/gfx/issues/1721) but nobody picked up yet. Finally, the third direction is to have a Metal-like abstraction layer. It's low level enough to ripe the benefits of next-gen, and high-level enough to be usable by mortals. Imagine this coated with strong Rust types and proper borrowing/lifetime semantics, and you'd get what I see is a perfect graphics API for our community.
[SpiderMonkey](https://en.wikipedia.org/wiki/SpiderMonkey) is the JavaScript engine that is being used in Firefox. The upgrade captures almost two years of development on SpiderMonkey.
I believe this is a compiler bug: [**rust-lang/rust#44168**](https://github.com/rust-lang/rust/issues/44168).
[Pure Python is not good at all for performance.](https://benchmarksgame-team.pages.debian.net/benchmarksgame/faster/python3-gcc.html)
The object you are iterating through is being converted to an iterator using the IntoIterator trait. IntoIterator for Vec&lt;T&gt; returns an iterator that yields T, whereas IntoIterator for &amp;Vec&lt;T&gt; returns an iterator yielding references (thus allowig you to loop multiple times over the same vec, unlike the non referance example). Check this: [for loops](https://doc.rust-lang.org/std/iter/index.html#for-loops-and-intoiterator) 
Using the object directly will move it. Try and print out the vector after the loop \- it'll [become apparent](https://play.rust-lang.org/?gist=e6248c4b28ea1e4110fe6da66a0d178c&amp;version=stable&amp;mode=debug). Taking an immutable reference doesn't move the vector (just gives you a reference to it) and so it can be reused later(IE if you try printing it out)
Ah, that sucks, do u know of a way I can work around this?
I really do feel awkward when a function references values that are not in its function signature. Why insist on TLS or Task local state? If those are the only options, I'd prefer passing the context around manually. For people who haven't read it yet, I wrote a draft/brainstorming RFC with a very different solution for the same root painpoint: https://github.com/rust-lang/rfcs/issues/2327
Nvm, just figured out a way around it, thank you tho :)
&gt; Why insist on TLS or Task local state? If those are the only options, I'd prefer passing the context around manually. Because code does not carry around all the context, it carries around its own context typically. So many other systems that want to transparently work will have troubles with this. This is especially bad if context is not carried around due to a bug or API limitation. Sentry is a good example of a system that really is not very useful unless implicit context access is possible and it's far from the only ones. &gt; For people who haven't read it yet, I wrote a draft/brainstorming RFC with a very different solution for the same root painpoint: https://github.com/rust-lang/rfcs/issues/2327 Sadly that one is useless because it does not propagate to code that is unaware of the context. At that point you might as well give up on the entire idea.
I agree that TLS is a crutch :)
&gt; My reasoning is that you may receive a request containing multiple items, and want to use a scope of EC for the request details (originator, authorization, etc...) and another scope for each item (reset between each item). Code should not stash away god and the world in execution context local data. It's preferable to attach large objects and drag out data from that object instead of storing countless small ones.
What's your opinion on Haskell? Do Haskell and Rust have different use cases, like some people suggest, or is it more a matter of taste?
Wow so friendly! I'm very happy for all your help and guidance! My goal is to be able to program my bachelor project in rust on my 7th semester. I would take rust over c++ or c anytime. The syntax is beautiful compared to c, c++ and go. It fits my way of thinking really well :) Why not go for a language where you save time and just focus your energy on getting better with algorithms, data structures, and logic. I will be looking forward to join this community.
He's upfront about the rankings being opinionated, which means it will be only relevant for the crates he knows or heard about. Tantivy shows up in the top 10 in `cargo esr -s search engine`. And it's the first relevant result (didn't work on combining relevance with crate scores yet).
This is a very interesting proposal under discussion for C++20, which takes a drastically different approach to the existing Coroutines TS. Some highlights: * It removes the mandatory (but often optimizable) allocation of the Coroutines TS. * It simplifies and consolidates the mechanisms for overriding coroutine behavior. * It expands the applicability of "co_await" to encompass Rust-`?`-like scenarios.
I believe it. I've written rust programs where switching to release mode from debug mode changed the time usage from a minute to 0.02-0.04 seconds. Considering python is interpreted, I'd imagine the equivalent python program would be closer to the debug compiled version than the optimized version.
&gt; It's preferable to attach large objects and drag out data from that object instead of storing countless small ones. Yes, I realized that when I remember that it would require a persistent hash map for implementation. The less keys, the better performance.
That's what I meant by "simpler languages usually have more traction", because they often have more corporate backing, e.g. Java back in the day. Nowadays react (with the same old JS) vs e.g. PureScript..
Excellent, thank you!
&gt; Additionally, "New Contributors" section is removed in favour of Rust Contributors site. (And also because it didn't seem fair to list contributors from just one Rust repository). Just in time for my first contribution :&gt;
LLVM can vectorize that. It should do it for ints automatically, for floats you have to tell it that vectorizing floats is ok, which I can't recall if Rust has a way to do it. SIMD intrinsics will be stable soon then you could do it by hand, or using https://github.com/AdamNiederer/faster 
"[&lt;-]" - looking ugly and not effective for typing!
Ohhh! Because the the scope of the for loop, v is moved in, and we only want to borrow v for that scope. Makes sense!
Which workaround did you use?
Maybe [catapult](https://crates.io/crates/catapult) can be used instead? &gt; Catapult is a small replacement for logstash. It has been designed to read logs in various places and send them to other places. &gt; Catapult is used in production to fetch Docker logs from container and send them to a central location. 
Use a logging implementation that locks the output stream. Since you're consuming it, you can't have anything else writing there regardless. Pipes themselves can pass data stupidly fast. My by-no-means fast laptop pushes 2GB/s through one.
Good bot!
There is a strong desire in the C++ committee to minimize the number of keywords, as any new keyword risks conflicting with existing variable names. The work-around is to use combinations of sigils in ever more perplexing ways. I much prefer Rust's take on "editions".
When can we see this propagated into Firefox?
The part of Servo that uses SpiderMonkey isn't used yet by Firefox. Firefox uses SpiderMonkey directly.
I wouldn’t suggest that. Sentry is meant to handle only error in application, not all logs. So if you want to review all logs then use something like fluentd and if you want error reporting the Sentry is great (these two are the best when used together). 
A publisher moves in mysterious ways, is all I've got by way of explanation. :) 
I look forward to reading your retrospective! 
I replaced the "outer" buffer with another digest, which would have had to happen when returning the result otherwise.
This is a pretty impressive proposal, thank you very much for bringing it to my attention. I am not up-to-date on the Rust generators story, so it is not quite clear to me how the two compare. I would expect that efficiency-wise they are somewhat similar, though Rust's implementation seems more straightforward (no tail call required, as there is no "unwrap" customization point).
But i'm also gonna add a "Block" type in the "ConstBlockSize" trait which will be the same as what i was trying to gethere
Yes, I was speaking of application errors :) if they need extensive logging capabilities for regular events than Sentry is not the right solution.
Efficiency-wise they do look basically the same. I'm not totally sure from the proposal how it handles nested coroutine awaits, which in Rust require nested resume/suspend. (This may be optimizable using tail calls, and of course is mitigated by inlining.) Complexity-wise, there's one thing in Rust that remains to be seen that may or may not wind up tipping the scales. The async/await design is intentionally a separate feature from the (yet to be written) generator design, partly to allow for composing the two into "async iterators" or "streams." If/when this happens, we'll be able to compare the result with a single general customization point. (Though I'm not totally sure how C++ would do that either, under the TS or this proposal.)
good idea, log to pipe that goes to separate process, and in background that process can stream writes to disk in blocks, think one of implementations of log4j did something like that
Nim isn't a scripting language though? It's compiled.
I understand their reasoning, but 30 years of tooling have given us the ability to refactor a variable name. Not always the easiest thing to do, but not horribly difficult. And upgrading large codebases to the new standard is always a headache. 
I think /r/rustjerk is a better home to get upvotes for such posts. 
fixed it. the signature for `grpc_auth_context_peer_identity` was wrong (it returns by value and I had it as a pointer) which somehow messes up the stack when you call it. See [this issue](https://github.com/pingcap/grpc-rs/issues/193) for details. 
Generally, you want something that's designed to be embedded from the start. That's what makes Lua such a popular choice. Here are some others I've heard of which are also designed for embedding: * [Squirrel](http://squirrel-lang.org/) (According to Wikipedia "It is used extensively by Code::Blocks for scripting and was also used in Final Fantasy Crystal Chronicles: My Life as a King.[2][3] It is also used in Left 4 Dead 2, Portal 2 and Thimbleweed Park for scripted events.[4]") * [AngelScript](https://en.wikipedia.org/wiki/AngelScript) (Wikipedia: " Amnesia: The Dark Descent,[5] Amy,[5] Dustforce,[5] Gekkeiju Online,[5] King Arthur's Gold,[5] Legend of the Guardians: The Owls of Ga'Hoole,[5] Overgrowth,[6] Penumbra: Overture,[5] Penumbra: Requiem,[5] Puddle,[5] Rigs of Rods,[5] Sine Mora,[5] Star Ruler,[5] SuperTuxKart,[5] Warhammer: Mark of Chaos,[5] Warsow,[5] Sven Co-op,[5] Jazz Jackrabbit 2 Plus,[5] Urho3D.") * [Wren](http://wren.io) (No Wikipedia page, but the site has a very appealing elevator pitch.) ...and, of course, if you're willing to relax the "designed for embedding" constraint a bit, there are already easy-to-use bindings to embed Python or Ruby into Rust programs, both of which have been used for logic scripting in various games.
Being old school is not a coherent reason to not choose a thing. NIM is a compiled language. Why do you need a scripting language, specifically? What is a scripting language? 
There is a subset of Nim that is interpreted directly, it is mostly used for compile time code execution and other ways of metaprogramming, but can be embedded as a scripting language pretty much like Lua.
If you want to embed NimScript in your application, I'd rather use Nim for low level stuff than Rust.
We are using Rust extensively at System76, as are many other companies, so a job using it is very much a possibility.
That’s basically what happens in a standard rebase (as opposed to merge) workflow. First you rebase your branch on top of current master, then you rebase master on your branch (which is now a simple fast-forward of HEAD). It works great if you’re fine with linear history
C++ also needs to worry about the C preprocessor w.r.t. such cases :-)
This is really cool stuff. Keep up the good work! For what it's worth this works for me on Android using Google Chrome.
See also: [http://arewegameyet.com/categories/scripting.html](http://arewegameyet.com/categories/scripting.html)
Great to see someone using my webgl bindings for stdweb :)
Your right. What I mean with old school is, it designed with really old principles and the initial development of Lua wasn't that great. It has become better the last couple of years. But nim has an incredible GC performance with the option to turn it off, so it shouldn't be noticed in-game. It outperforms Lua-jit. The reason for a scripting language is to make it more accessible for UI developers or data science specialists that have python experience. 
I doubt this is funny on any subreddit...
Why would you do that when it doesn't have the same security in the language as rust do? Ofc it depends on the project, and the requirements needed. But let's say you went for a bigger project with a team. Wouldn't you still choose rust to get closer to native performance? 
Thy for your suggestions! Bus as AlexKotix pointed out "There is a subset of Nim that is interpreted directly", and from what I have seen and read about nim it should be able to outperform lua-jit. I was just thinking if you had to make a bigger project with including big data analysis and ui development. Maybe nim could be good shot with its python like syntax? 
Do you think it would be possible or a good idea to use nim in this way i'm thinking? 
FWIW when my coworkers and I built [cernan](https://github.com/postmates/cernan) to cover just this use case. You'd emit logs to cernan via its [native protocol](https://github.com/postmates/cernan/wiki/SourcesNative) and then emit via, say, [kafka](https://github.com/postmates/cernan/wiki/SinksKafka).
The only one big difference between Rust and Nim is GC. However GC in Nim is pretty well designed, it can be used for games. On the other hand development in Nim is faster because you don't have to fight with borrow checker. Nim compiles to C, so I don't see in what way Rust is faster, executables generated by Nim compiler is much smaller.
Thy a lot! It's interesting see these options! Do you have any experience with any of those?
I'd rather use Nim for everything, as it will be faster than embed it in Rust. You will need to compile NimScript as a seperate C library, create bindings for Rust, compile and link it together.
So in what situations would you use nim before rust, and opposite? 
slog has a native journald backend crate which preserves structured data.
In what situations would you use rust compared to nim; opposite?
Works on iOS
To be completely honest with you I would use Nim in most situations because of development ease and more pleasant syntax. I would use Rust for some situations where I can't really live with GC, something like operating system. But it is my own opinion, and bare in mind that Nim is not 1.0 yet, there still may be some breaking changes to be done in some corner cases of the language.
Well, besides backwards compatibility there is another reason to prefer as few keywords as possible: any keyword can't be used as a variable name anymore. There's been many times where I wanted to name something "type" but couldn't.
One important detail to keep in mind: Explicit lifetimes don't let you define new behaviour. Their only purpose is to identify which of the behaviours that the compiler can foresee is correct. (ie. I can predict that, of input variable lifetimes `'a` and `'b`, the output variable must match one of them... but which one?)
Awesome work! It looks like a cube consistently falls through the trimesh in the terrain demo. Is the timestep just too coarse?
Can you link to the documentation on nonlexical lifetimes you found? Your example 3 looks like a correct understanding of lexical lifetimes, including the consequence 4.1, which is true for lexical lifetimes. You cannot access `my_s` directly while `x` or `y` is in scope under the current lifetime rules. I'm not sure what you mean by 4.2. If I understand nonlexical lifetimes correctly, in your consequence 4.1, the lifetimes of `x` and `y` end if `my_s` is accessed directly. Any further reference to `x` or `y` is invalid past that point.
How much do you want to bet, and who do you propose to judge?
I think in my case it was mainly iterator-heavy code that the optimizer was able to improve so significantly. Iterators *can* be compiled into simple loops equivalent to what you would have written by hand, but without optimizations they generate huge numbers of intermediate structs, `Option` operations, calls, branches, etc. It’s actually very impressive that the optimizer (mainly the LLVM itself IIRC) is able to see cut through all that complexity. 
I think that you are over thinking this. The model is that `my_S` is some data, and that data will live a certain amount of time, and after that will be freed. `x` is a reference to that data, so this reference is only valid for as long as `my_S` lives. In `f`, when you have the return type `&amp;u8`, the compiler knows that that reference must come from somewhere, but it doesn't know where. When you say `fn f&lt;'a&gt;(x: &amp;'a mut S) -&gt; &amp;'a u8`, you are telling the compiler that the reference you are returning is derived from x in some way, so the result can only live as long as x does. The `'a` is the lifetime of the input, and you use it to constrain the lifetime of the output. I hope this helped. If you have any more questions please ask.
If you mean in the cargo metadata, it is possible already. [Here] (https://gitlab.com/nazarmx/libnftnl) is an example of a published crate.
I've been sort of interested in whether or not there were plans to work on new versions of Spidermonkey in Rust the same way that Servo has pioneered a new css engine and other features. I would imagine that the js engine a browser uses has a huge impact on performance because there's so much js on many web pages now. It seems like certain engine functions, like the initial compilation into bytecode, could be done in parallel as well. Maybe there is a project that I just haven't heard about.
Can you give me some information on how to achieve this. I've only found this guide in the book [https://doc.rust\-lang.org/cargo/reference/publishing.html](https://doc.rust-lang.org/cargo/reference/publishing.html)
I did not know about assume! Thanks. 
&gt; Again, there is no way to guarantee compatibility if you use different versions of the common library I think it's worse than that; afaik Rust hasn't committed to a stable ABI for dynamic linking Rust to Rust, so even if you update the compiler it could break.
I hate the keywords `proc` and `in`. I often work with data in terms of processes, inputs, and outputs and those are excellent variable names when I'm trying not to be verbose. 
&gt;This is what I'm concerned about and one of the reasons I [..] Do you know how big that number is? You'd have to compute the whole universe a zillion times to make a dent in it. You'd have better luck doing *anything* else. Speeding up your algorithm is pointless, because the thing you're making it compute is fundamentally nonsense. If you go around asking people what the worst sorting algorithm is, they'll tell you "brute force" because it's really really bad. But it's not the worst. The worst is to make an algorithm that intentionally avoids detecting a proper sort state. And algorithm that doesn't even sort is the worst sort of sort. Images are only meaningful after they are interpreted, and you're purposefully ignoring any correlation with that interpretive machinery in your image generation procedure. You're doing far worse than brute force. 
Thanks. Here is [RFC 2094 about NLLs](https://github.com/rust-lang/rfcs/blob/master/text/2094-nll.md) I was reading, and the documentation about [Lifetime Annotations](https://doc.rust-lang.org/book/second-edition/ch10-03-lifetime-syntax.html#lifetime-annotations-in-function-signatures). &gt; I'm not sure what you mean by 4.2. I think 4.2 was a mistake from my side. My problem with "Variant 3" is that with NLL I can still access `my_s` once `f(x)` disappears. Therefore, when I start analyzing `x`, it's original isolated lifetime must end before the `println`. Therefore, some magic process must extend `x`'s lifetime, once `f(x)` comes into play, as otherwise it would not encompass `y` anymore. I thought a bit more about the whole situation. My **actual** problem is that `f` magically connects input and output lifetimes. To quote the documentation: &gt; by specifying the lifetime parameters in this function signature, **we’re not changing the lifetimes of any values passed in or returned**. Instead, we’re specifying that the borrow checker should **reject any values that don’t adhere to these constraints** Following that logic: * `x` has a lifetime that effectively ends upon calling `f()` (since `x` itself isn't used anymore) with NLL. * `y` has a lifetime between `f()` and `println`. * If "no lifetimes were changed", then `'a` could only point to something "just around `f`". * However, then the compiler still would not have any reason to forbid me accessing `my_s`. 
Sorry, by "versions" I mean "compilation artifacts", I'll go edit the comment now.
hard to tell, but using rust will make you a better c++ programmer
Thanks! However, in my mind this alone does not explain how the compiler can infer that I should not have access to `my_s` anymore between `f` and `println`.
Can you elaborate on what you mean for frontend development? I have seen a few libraries targeted towards frontend use of Rust, but nothing that seems like it has a straightforward path to success.
pypy can be faster still. But if you want maintainable code I'd stick to rust.
Thanks! I still think this model does not fully explain what is happening: &gt; The `'a` is the lifetime of the input, and you use it to constrain the lifetime of the output. If I only read it as "the lifetime of the return value were constrained", then yes, `y` could not live longer than `my_s`. However, it also seems to do "something magic" to `x`: 1) Under NLL, the lifetime of `x` should not go beyond `f`. 2) According to the documentation "we’re not changing the lifetimes of any values passed in or returned". Yet, adding `f` seems to suddenly extend how long `x` lives. 
It's a bit slow and flickery, but it works on my phone. Android with Firefox 60.0.2.
Oh yeah, I also used to name output streams `out` and input streams `in`. Can't do that if `in` is a keyword.
[Cargo.toml metadata documentation](https://doc.rust-lang.org/cargo/reference/manifest.html#package-metadata). The `repository = "..."` key is just a URL to the user-facing location of your code; you could link that to `http://spam.spam/spam_me_please` and it would work (though likely might get that domain blacklisted if your crate became popular....)
Oh, you're asking how to log in on [crates.io](https://crates.io) without having a GitHub account. It was very unclear from your post what the real issue was. There's [an issue](https://github.com/rust-lang/crates.io/issues/326) for this, but no real work has been done. It'd be awesome if you'd be willing to chip in here! Unfortunately GitLab is very much a second\-class citizen in the Rust ecosystem. 
I found the manifest file docs to be helpful. The [package metadata](https://doc.rust-lang.org/cargo/reference/manifest.html#package-metadata) section lists the repository field.
Thanks, sorry for not being more clear in my question... The main issue was getting the API key...
Thanks, this is realy helpfull
There is a lot going on in this example. Here is the blog post I wrote while I worked through it: https://lliwynd.blogspot.com/2016/11/rust-for-loops.html
When I run Clippy twice, I get warnings only the first time. Is that intended behaviour?
We recently un-reserved proc.
How recently? Is it in 1.26? 
You will unfortunately need to have a GitHub account. As mentioned in the other comments though, that's all you need! The crates themselves can be hosted anywhere as long as you log in to crates.io with GitHub.
https://github.com/nbp/holyjit
There's something so supremely satisfying about moving one of the corner edge balls and watching the entire thing collapse. Cool work man.
I can’t remember off the top of my head, if you search the github you should find it.
Interesting that the default "boxes" demo behaves as you'd expect, with the cube of disconnected boxes tumbling all over the place once it hits the floor, while in the "balls" demo the individual balls only bounce vertically, and the cube of balls settles into the same cube of balls on the floor.
Can really recommend looking into popular crates to see how they organize their code. Also episode 20 of New Rustacean talks about just this topic: https://newrustacean.com/show_notes/e020/index.html
I haven't seen KubOS talked about much in the Rust community, but it's an interesting project and company, so just splatting this link up to show it off. They are creating software for satellites and some of the core software is in Rust.
I'm not sure where you are getting `f` extends how long `x` lives. It doesn't change any lifetimes. Can you please explain further?
A sub repo with the [the-patch-section](https://doc.rust-lang.org/cargo/reference/manifest.html#the-patch-section) of Cargo.toml will work. :-)
I think the simulation is using an optimization of disabling physics for objects that aren't moving and "locking them". You'll notice the simulation time drops substantially as soon as they stop moving. If you touch any of the balls, even slightly then the whole cube collapses and the simulation time increases dramatically.
I am speaking of rust wasm. 
I spent a ton of time in Python, so I name input and output "file-like objects" with names like `fobj_in` and `fobj_out` or, if I'm not being less picky about comprehensibility, `fin` and `fout`.
[Cargo workspaces](https://doc.rust-lang.org/book/second-edition/ch14-03-cargo-workspaces.html)?
Jusy FYI, you're mixing up a couple of idioms there. "Putting your foot in your mouth" means to say something embarrassing. "Putting your money where your mouth is" means to act on something you've said.
I have a long position in Rust as my main language going forward for many reasons that I will list below. Balanced, yet flexible language design: There are a lot of languages out there, but Rust manages to strike a balance between performance, ergonomics, and expressivity that will appeal to many advocates of other languages. Rust *can* look as simple as python, yet be many times faster, so there is an appeal to those users. Rust has a package manager that appeals to C++ and C users who still want their low level usage. Rust has a type system that appeals to Go users that want more freedom with the same performance. Rust's community: The Rust community and leadership that are its main strength. Rust is developed in the open by very conscientious and considerate engineers/mathematicians that have a corporate sponsorship to full-time work on it, with a relatively fast release cycle who are open to suggestions and integrating features of other modern languages. Documentation/errors: Rust also places a first class importance on documentation and error ergonomics. It literally has a working group for those two things. Rust's future: Rust **will** get specialization, async generators, procedural macros, higher kinded types, non lexical lifetimes, improved lifetime ergonomics, a stable ABI, and I believe even things like garbage collection and green threads implemented as a crate. For me, I always liked managing my own memory, probably because my first language was C++, the behaviour is less surprising to me, and it's easier for me to understand. However, once I learned Python, my productivity went wayyy up for simpler tasks, but once a simpler task became more sophisticated, I found that I missed types a lot (and also I get package manager problems from python way too much and they don't teach you about virtualenv in the python manual). I then learned about Go and liked the ability to make nice web servers and compiled programs that are statically built *quickly compile* and have a nicer standard library (which C++ also lacked, e.g. network support). I've used a few other backend/system languages here and there: Java, Elixir on ErlangVM, Scala for a few years through work. And of all of them, I like Rust the most for these reasons. Therefore, I decided to make Rust my main language, write all of my side projects in Rust, and become extremely proficient in it. Even going so far as to choose Rust for a simple script over Python because I believe that if I am proficient enough in Rust, I could produce a working example faster in it than I would while still getting the benefits of a more powerful language, like training a muscle. I wouldn't do this if I didn't rationally believe in its long term future. It took me years to master the corner cases of C++ and yet I still didn't feel as confident programming in C++ as I do with Rust.
The lifetime of `x` does not end upon calling `f`. The variable `y` depends on `x`, so the lifetime of `x` only ends after the lifetime of `y`. You might ask why `y` cannot instead rely on `my_s` directly. I suspect the reason is merely that it wasn't considered useful enough to implement. To check whether `f` was doing anything to the lifetime of `x`, I inlined `f` into `main` and the program still compiles. It also does not compile if the last line is uncommented, just like with the original example. https://play.rust-lang.org/?gist=32da1ff72a0f037861e5d4cf62d859d7&amp;version=nightly&amp;mode=debug
I'll try to explain in the most human-natural way possible, I believe it makes it easier to understand. First of all, a function declaration isn't the basic building block for lifetimes. The basic block is the following check: * When taking a reference, you must not use the origin earlier than the last time you use the reference. Also there are the following notes: * The time from the moment the reference is created until the last use (the time using the origin is not allowed) will be called the lifetime of that reference. * The destruction of the origin counts as a use. * Only exception is you are allowed to create shared reference during lifetime of other shared reference (no mutable allowed though). For example: let mut x = 0; // Our origin // xxxx let y = &amp;x; // We take a reference. Lifetime is created. x // Not allowed to use the origin x. x // x = 1; is not allowed x // x println!("{}", *y); // This is the last use of y. x // xxxx x = 1; // We can finally use x again The next concept is copying references. * When copying a reference, both copies count towards that lifetime. Therefore in let mut x = 0; // Our origin // xxxx let y = &amp;x; // We take a reference. Lifetime is created. x // Not allowed to use the origin x. x let z = y; // Copied reference for the lifetime denoted x // by 'x' x // x println!("{}", *y); // This is the last use of y. x // x println!("{}", *z); // This is the last use of z. x // xxxx x = 1; // We can finally use x again the lifetime ended on the last use of any copy of the reference taking, in this case it the reference `z`. Function signatures are an extension of this rule. What the signature `fn f&lt;'a&gt;(x:&amp;'a S) -&gt; &amp;'a u8` says in human words is: * `fn f&lt;'a&gt;` - This function will reference a lifetime (restriction on usage of a variable) that for convenience will be called `'a`. * `(x:&amp;'a S)` - If the variable x causes a restriction, then that restriction is exactly the thing we defined as `'a`. * `-&gt; &amp;'a u8` - The returned value also causes the restriction `'a`, just as a copied reference would.
It's interesting that that gives such different results for cubes versus spheres.
[removed]
DAE think Apple deprecating OpenGL is embarrassing? Gaming on Macs was bad enough beforehand, it'll be worse now. Nobody cares about Metal.
lol gaming on a Mac
Yes, you're right.
Thanks for all the great information, I definitely am gonna try to get good with Rust So far it is pretty smooth and unlike C++ (I've used it quite a bit but am no means good) I feel like I can use it for simple stuff already that I'd use Go or Python for as well as big projects. What are your favorite resources for Rust as far as tutorials, guides, projects, repos, etc go for new users if you don't mind?
So in this example, what if I wanted to pull in an existing library, and then make changes? Especially if I was making non-breaking changes, I'd like the upstream to get that, but I'm not 100% seeing that in the document. Sorry if this is a dense question, the answer might be there and I'm just not reading straight. Also, I didn't include this in the OP, but how do you handle the new git repo that gets created using `cargo new`? It feels weird just deleting it.
While this is cool. I think u/PetrsGhost was referring to a Rust implementation of a JS engine for use in a browser (a RIIR of SpiderMonkey/v8/etc.), not a Rust interpreter. 
This looks promising. So if I have a feature branch in the library to support a new project, I can simply pull that down and then make changes in that directory? Then whenever I'm moving to release, I can simply merge the branch in the library to master to create a new version there which would become a dependency. Thank you!
I think it's definitely sad. If there weren't amazing people producing crates like these, I might not even build for Mac.
Works just fine in safari 11.1.1 (macOS 10.13.5)
HolyJit isn't a rust interpreter, you are thinking of miri.
Your first attempt is close to the way that I think about it, except your second bound is wrong. `x` has to live longer or equal to `'a`, but `y` has to live shorter or equal to `'a`. I'm a bit fuzzy on this myself, but I think this is related to the way that functions types are contravariant in the input type and covariant in the output type. I assume there must be a subtype relationship between the type of the function you are calling, and the types that can inferred from the arguments and return value of the actual call.
Fantastic! The straightforward api in kiss3d is wonderful, as well as the documentation in all of your crates. Looking forward to using nphysics with [Quicksilver](https://github.com/ryanisaacg/quicksilver), which is in the process of integrating with stdweb (pinging u/Diggsey) :)
@sebcrozet I just want to thank you for your work on Kiss3d. I used it for a small project for work and it was far easier than our existing visualization tool written in C++ with OpenGL. Keep up the good work!
Not only that, but taking an immutable reference/pointer of something is generally safer than mutable if all you're looking to do is read it. Prevents you from inadvertently making a change to it. That rule applies to pretty much all languages, not just rust. Fewer vectors for mistakes you open, the less bugs you have.
Organize things so that the pieces that are related are in the same file/module. Try not to form loops of module dependencies. (I forget if that's even possible in Rust.) If you have a piece of code you want to use in multiple projects, split it off into a separate crate. But a separate crate is a maintenance burden and a pain to keep track of, so don't split them TOO finely.
So I think for an existing library, you could just pull the source and modify it locally, e.g: if your source tree looks like: ``` ├── Cargo.lock ├── Cargo.toml ├── my-budget-app │ ├── Cargo.toml │ └── src │ └── lib.rs ├── money │ ├── Cargo.toml │ └── src │ └── main.rs └── target ``` (i.e, using cargo workspaces), running `git clone existing-lib` from the root would add it like so: ``` ├── Cargo.lock ├── Cargo.toml ├── my-budget-app │ ├── Cargo.toml │ └── src │ └── lib.rs ├── money │ ├── Cargo.toml │ └── src │ └── main.rs ├── existing-lib │ ├── Cargo.toml │ └── src │ └── lib.rs └── target ``` All you should need to do now is add `existing-lib` to the "members" list in the workspace `Cargo.toml`. If you `cd` into the `existing-lib` directory, then you should be able to behave as if it were just a regular crate, so any changes you make should be able to be pushed upstream without issue. I don't think you'd need to delete the git repo. As for the `budget-app`, you can add the local copy of `existing-lib` as a dependency via the following line underneath `[dependencies]` in `my-budget-app/Cargo.toml`: ``` add-one = { path = "../add-one" } ``` I'll just note here too that I'm *pretty sure* all of this is right, but I might be wrong so you might have to do some trial and error to find out what works.
My guess is that it's due to how collision points are calculated. In that setup ideal spheres would contact in at most 6 points (if penetration is not considered), whereas for cubes that would be much more complex. So, in case of cubes float errors accumulate much more faster and prevent the simulation from ending in an unstable equilibrium state. Would be interesting to hear the author's explanation though.
That's the joke, isn't it? They're putting their money where their mouth is, and, because they're proving their former position wrong, making their former opinions an embarrassment.
Shot in the dark. From the RFC: &gt; we wish to compute liveness for lifetimes. We can extend a variable-based analysis to lifetimes by saying that a lifetime L is live at a point P if there is some variable p which is live at P, and L appears in the type of p You are analyzing with NLL incorrectly. Say x has anonymous life time 'x. From the signature of f, we see: let y: &amp;'x u8 = f::&lt;'x&gt;(x); So that: println("{}", y) // y dead here // x dead here, 'x ends here
Recursive module imports are allowed. I've found this to be an occasionally useful property, but agree at least in principle to not go crazy with it. Recursive crate dependencies (compilation unit) are not allowed.
It's in 1.27. And also being unreserved are `pure`, `offsetof` `sizeof`, and `alignof` (though not `typeof`).
Note that async/await is just a small bit of sugar over generators, so async streams will just desugar the async bit into a plain generator as well.
As far as I remember Felix did a talk about subtyping where he also talks about lifetimes: https://youtu.be/fI4RG_uq-WU
Have a link to any discussion around removing `sizeof`? I was browsing the RFC for keyword removal and it seemed pretty light. Curious as to opinions on the subject. 
OMG THANKS! This actually solves my problem, now everything makes sense! 
There's a Ruby version called mruby, built specifically for embedding. It's also rather fast. Firaxis used it in some games. There's bindings for Rust. (sorry, no links, I'm on mobile) 
That's a good point. The collision points between coplanar surfaces are almost certainly not going to get consistently computed as perfectly aligned with the center of mass.
Works on firefox android
Not really, but we haven't yet found a way to override it.
Err, yeah, I suppose you're right. But it's definitely not for JS.
Any examples of Debian embracing Rust? That's exciting news, given my particular, idiosyncratic set of interests.
Oof, yeah, you're right. IMO that readme could be a lot clearer, but that was my bad.
I don't think anyone doubts that. But the worst case scenario taken from your link is a problem where python is around 140 times slower than Rust, not (at least) 4000 times as in the OP's case. That's not the same at all.
https://github.com/rust-lang/rfcs/blob/master/text/2421-unreservations-2018.md#rationale-for-sizeof-alignof-and-offsetof is the section that discusses this. To paraphrase it here, since we already have the ability as a function (std::mem::size_of), there's absolutely no reason for it to be a keyword.
&gt; Rust's future: Rust will get specialization, async generators, procedural macros, higher kinded types, non lexical lifetimes, improved lifetime ergonomics, a stable ABI, and I believe even things like garbage collection and green threads implemented as a crate. You sound a bit over-hyped here. There's no guarantee Rust will get all of these. Even things that we thought were going to be a sure bet (type ascription, placement new) have been backed out when it was found they wouldn't work. Also, non-lexical lifetimes and improved lifetime ergonomics are mostly the same thing?
The only one I'm not sure 100% about is higher kinded types, but the rest of them have RFCs except for stable ABI. There already exists an implementation of a garbage collector for rust by someone that I read about long ago, but I believe it was mostly experimental. So, in short, I disagree; I am not overly hyping those things. And non-lexical lifetimes is a major one, but no the only lifetime ergonomic improvement. There's still stuff like borrowing a struct member without borrowing the whole struct that I *think* is being considered right now.
Cant you implement a Trait you define in the consumer crate on your protobuf types? Given that for third consumers you need to Import both the protobuf definitions and the impls, if thats not a problem
Are you sure that in first example last line is allowed? With non-lexical lifetimes `x` is borrowed until scope of `y` ends.
If you don't want to write to stderr or -out, consider writing to a file and use logrotate(8). You only have to reopen your log file on a signal (or any other rpc you implement). That way, you can configure log rotation like all other tools (as a simple file in /etc/logrotate.d).
Modules are like Java packages and classes in one. The smallest subpackages are more like classes, with only one responsibility. The higher ones are more like packages and may re-export things from their submodules (`pub use`). The guidelines are not as strict as in Java, because in Java a class is the one concept that rules everything and there isn't really something like that in Rust. Data is stored in structs, behavior is mostly controlled by traits (though of course you can have inherent impls on structs, but since you don't have inheritance this is not equivalent to classes in Java), and visibility / safe api boundary is controlled by modules (what is public / private). 
Which is why I like Rust mostly nonsensical keywords, I've got no need of `fn` as a variable name :)
As interesting as this interpretation is, I am afraid I was actually mixing the two idioms accidentally.
&gt; The only one big difference between Rust and Nim is GC. I would argue that the ownership/borrowing system is a very significant difference: - it ensures the absence of data-race, which AFAIK Nim cannot guarantee efficiently, - it ensures better aliasing information for the optimizer, which should give an edge to Rust performance wise. &gt; Nim compiles to C, so I don't see in what way Rust is faster, executables generated by Nim compiler is much smaller. There are a number of downsides to using C as a translation layer: C is a simple language, lacking quite a few higher-level concepts, so there is a loss of information. This means that Nim -&gt; C -&gt; LLVM IR will lead to a simpler IR than Rust -&gt; LLVM IR, thereby theoretically nipping a number of optimizations in the bud. I fear the bigger loss is aliasing &amp; constness information. The reason that Fortran beats C in a number of numerical benchmarks is because parameters do not alias in Fortran, thereby opening up more optimizations. The `__restrict` keyword helps a bit, but nowhere sufficiently, to recover this information, mostly because it only applies to the immediate pointer, not recursively, making it difficult to apply consistently. There used to be a bug in LLVM related to aliasing which is normally fixed in LLVM 6.0, and rustc should now seek to take advantage of that; it'll be interesting to see the effect on benchmarks.
&gt; It outperforms Lua-jit. Do you mean the compiled version outperforms Lua-JIT, or the NimScript interpreter? Lua-JIT is a beast, performance-wise, a feat made even more impressive considering the ratio performance/binary size, so I'd be interesting to learn of any scripting language managing to top off its performance.
Is there a workaround?
&gt; The time from the moment the reference is created until the last use (the time you are restricted from using the origin) will be called the lifetime of that reference. Thanks a lot! That video was really helpful understanding a bit of the magic that is added to dealing with lifetime rules. 
Yes, and much more is possible: https://crates.io/crates/query_interface
Here's my view as someone whose day job involves Java. This is a really rough comparison, but the bird's eye view (in my opinion anyway) is something like this: - In practice, a crate is roughly equal to a Java JAR file. The compilation unit of Java is a .class file whereas in Rust the result of compilation is a crate. However, crate is also the "unit of distribution" - something you publish to the world as a single thing, like a library - or something you pull in as a dependency, using Cargo. Similarly, with Java you use Maven or Gradle or some other tool to pull in dependency JARs. In both languages bigger projects may be split into multiple crates/JARs. - A module is roughly equal to a package in Java. Something you use to organize the "real content". Related things belong to same module/package. - The closest equal to class in Java is a struct in Rust. Java has the class per source file mapping baked into the language (it also has inner classes, so it's not entirely 1-to-1) but Rust doesn't have that kind of requirement, and that's something to embrace. Small, related types could go into a single source file. As has already been mentioned, the amount of types defined in a single Rust source file boils down to logical organization, but I would like to add that readability and understandability matter too. - Traits in Rust are very similar to Java interfaces.
As I understand it, you should never hesitate to switch from Rc to Arc if that solves a problem for you. Yes, Arcs are a bit more expensive, but the cost is minor if that in turn allows you to solve your concurrency issues. 
You are correct. This is with NLL, and is the ideal - for now you can only see this on nightly with `#![feature(nll)]`. On stable, due to technical limitations, the definition is changed to be * When taking a reference, you must not use the origin earlier than ~~the last time you use the reference~~ *after the last reference goes out of scope*. So on stable, in order to compile the examples, you will need to add `{``}` around the area marked by 'x', because the compiler can't work with a finer granularity than a scope.
Isn't GitHub deprecated these days? I thought we were all supposed to move to GitLab due to the M$ issue.
Good lord. This is a game changer. I'm gonna be using this all over the place. God bless you.
They’ve just released a new version of debcargo, and have already been packaging rustc and cargo. This new release will let them package even more things even more easily.
Works on Chrome Android
thank you. I am trying out MIO for now, I guess. :) As I am building project to learn, I think it makes sense if I try mio where everything is in my control.
Probably what C++ calls "PIMPL Idiom" aka [Opaque Pointer](https://en.wikipedia.org/wiki/Opaque_pointer).
Exactly. I thought it was brilliant.
I have had great success speeding\-up our pandas code by using the \`pyo3\` crate to create a dataframe from rust via numpy arrays in record time. You can even add / monkey\-patch Dataframe operators.
Thank you very much for your detailed answer. I think I almost get it, but could you, according to your model, outline which lifetime 'a would be tracking in this example? fn f&lt;'a&gt;(x: &amp;'a u32) -&gt; &amp;'a u32 { ... } let mut number = 5; let mut other_number = 10; let mut x = &amp;number; let mut y = &amp;other_number; for i in 0 .. rand::random::&lt;u8&gt;() { if rand::random::&lt;bool&gt;() { number = 0; // &lt;--- Fun fact, we can assign number despite 'x' existing. x = &amp;number; } else { number = 0; // &lt;--- Same here x = &amp;other_number; } } // What is &lt;'a&gt; exactly tracking or extending here? let z = f(x); [Link to Playground](http://play.integer32.com/?gist=dba4709dae2d60b0651664f8c351da2f&amp;version=nightly&amp;mode=debug)
There are many _particular details_ that motivate this sort of thing, but I think the general summary is "to hide some aspect of the implementation." Without looking at mio/futures/tokio specifically, here are some reasons that I've used this strategy: * When my type is natural an enum, but I don't want to expose the fact that it is an enum. * When the type I'm exporting is responsible for its own synchronization. e.g., You might have a `pub struct Foo(Arc&lt;Mutex&lt;FooInner&gt;&gt;);` and maybe you want to hide the fact that there is an `Arc` at all. (This comes up somewhat infrequently in my own work, since this generally makes too many assumptions that impact performance in critical ways. But the principle applies.) * When you want to hide an implementation of a trait. For example, if you have a public type `Foo` that implements [`rand::distributions::Distribution`](https://docs.rs/rand/0.5.1/rand/distributions/trait.Distribution.html) _because it's convenient for your specific implementation_, then you might actually want to avoid the impl on that type because it causes `rand` to become a public dependency your crate. (Public dependencies are sometimes necessary of course, but require great care to manage correctly with respect to semver.) Note that this example isn't specific to `rand`; this applies to any trait defined by another crate. * Another related example of the aforementioned phenomenon that manifests in a different way is `impl From&lt;SomeOtherCrateType&gt; for MyType`, which causes the crate that exports `SomeOtherCrateType` to become a public dependency. This is easy to happen with error types, for example. I made this mistake myself that accidentally caused `regex-syntax` to be a public dependency of `regex`, which was explicitly *not the intent* since inception. * Before `pub(crate)` was a thing, there were various "work arounds" to create parts of your API that were explicitly not public but accessible for anywhere else in the crate. One of those work arounds might involve creating a distinct type that is accessible by the entire crate but is not a part of the public API. There may be others!
Sorry for being unclear. My point is, that calling \`f\`, with the way how it uses \`'a\`, changes the point in code where I can access \`my\_s\` again. For example, were \`f\` to return \` 'static\`, I could use \`my\_s\` earlier again. So the way I use \`'a\` within the method declaration affects how long \`x\` will live.
Can you provide an example of a type you are asking about?
Not exactly, since in Rust, it's generally an actual struct and not just a pointer, though the rationale is much the same. Keeping the "public" struct/methods separate from the "internal" implementation makes it easier to change the details of the inner-workings without things spilling over into the api-breakage zone. The reason that Rust leans toward concrete structs as opposed to pointers is probably in keeping with the "avoid unnecessary allocations" rule-of-thumb, and the fact that crates aren't dynamically linked, so the "binary code compatibility" goal isn't necessary. If you're actually writing a C-facing API with `#[repr(C)]` structs, though, you'll definitely want to use pointers to inner types, since that'll keep both the public functions *and* the public struct layout exactly the same, even if the internals need to change.
Yes, but the reason that `x` does not end is a result of how `f` is defined. And the mechanism by which `f` does that is what I have trouble understanding. 
Yes, that's the one.
How does it change when you can access `my_S` ? Regardless of whether you call `f` or not, you cannot access `my_S` again until the borrow of `x` expires. 
Meh, monad's in haskell are pretty and all, but having to shoehorn everything into a monad transformer stack is painful, and also not particularly performant. I'm sure they could be made more-so in Rust (like everything else) and an opt in abstraction to carry around a computational "context" would be cool, but it aint a showstopper by any means. imho
Meh, monad's in haskell are pretty and all, but having to shoehorn everything into a monad transformer stack is painful, and also not particularly performant. I'm sure they could be made more-so in Rust (like everything else) and an opt in abstraction to carry around a computational "context" would be cool, but it aint a showstopper by any means. imho
What I mean is: let x = &amp;mut my_s; let y = f(x); // I could change my_s here it it were not for f(x). // The borrow for x would be expired here, but is "somehow" kept // alive through the way 'a is being used in f(). // Would, for example, f return 'static instead, I could modify my_s. println!("{}", y);
Haskell is a resounding success at what it was designed to be, namely a research language which other languages like our namesake could pilfer for ideas. As such we should be eternally grateful. But I chose Rust over Haskell because I'm not a language researcher and just want to get stuff done in a pragmatic language. For example, what is the idiomatic/standard way of doing error handling in Haskell? I gave up trying to figure that one out.
&gt; Finally, the HPC world is due for a new language... There is the up and coming Julia language, which is pretty awesome for a dynamic language. Also nobody mentioned fintech, Rust could really shine there
Heads up: You have a duplicate example link on the website. The second "Draw Geometry example: Draw a few colored shapes to the screen" link is really a font example
I'll get right on that, thanks for the heads up.
Also on Firefox for Android
&gt; Recursive crate dependencies (compilation unit) are not allowed. So, you could stop someone from using your crate by using his? Is there some fundamental principle behind this? I mean, it would seem to be the same kind of situation when two functions call each other.
In addition to what /u/burntsushi said, one crucial detail is that the "outer" type is public whereas the "inner" type is private to some module. This allows exposing fewer details in the public API.
To respond more literally, SpiderMonkey is developed together with Firefox and shares its version number: Firefox 60 already uses SpiderMonkey 60.
No promises on the quality, but here's been my attempts at sharing data between Python and Rust using milksnake. If you ever plan on installing to more than one machine, linking against libpython via rust-cpython or pyo3 will probably go poorly. It certainly did for me. Here's the python side: https://github.com/saethlin/rust-lather/blob/master/lather/__init__.py And the Rust side: https://github.com/saethlin/rust-lather/blob/master/rust/src/lib.rs
The compiler isn't confused at all; it's doing the right thing. 1. Intended: closures can't be generic over types. 2. In current stable, I am 95% certain you can't. With unstable specialization... also *probably* not, but it's been a while since I refreshed my memory on that. 3. That's not a type per se; it's a stand-in for a type you either cannot name (because it literally doesn't *have* a name, as is the case with closures), or don't want to name. It's saying "I can't say what this is, but whatever it is, it implements `FnOnce(U) -&gt; T`, so let me use it like that".
You say that if it were not for `f(x)`, you could modify `my_S`. This is not true. You cannot modify `my_S` regardless of `f(x)`. See [here](https://play.rust-lang.org/?gist=615f2f712adba42d66ecf60a0630ce6d&amp;version=stable&amp;mode=debug). You cannot modify `my_S` because of this line: `let x = &amp;mut my_S;` which borrows `my_S`, so you cannot modify it until the borrow expires. Whether or not you can modify `my_S` has nothing to do with the lifetime of `my_S`, but rather the fact that you have borrowed it
&gt; `fn f&lt;'a&gt;` - This function will reference a lifetime (restriction on usage of a variable) that for convenience will be called `'a`. Why is this declaration of lifetimes necessary? Why doesn't just `fn f(x:&amp;'a S) -&gt; &amp;'a u8` work?
Side question 0. : you should have a look at [num](https://crates.io/crates/num), it contains some Traits that you may find useful. 1. This is intended behavior. f has only truly one type, and you basically tell the compiler "it can be any time, but it HAS to `impl FnOnce(U) -&gt; T`". Now since storing an `impl` doesn't make sense because it can't know the "size" of `f` then, it has to decide a concrete type to use. However it can't just yet, because your function is generic over T and U, so it will wait until the first call to decide what are T and U exactly. Next line, it sees that you call it with a U integer and thus decides that `f`'s type should basically be a `FnOnce({integer}) -&gt; something`. The line after that, it sees a `FnOnce(&amp;'static str)`, but wait, we just said that f has the type `FnOnce({integer}) -&gt;`? Well the answer for Rust is simple: it can't be possible, so abort the compilation. 2. I don't think it is possible? 3. You can't name the type of closures in Rust. In the end closures are structs -- but where do you define these structs? There are no annymous structs in Rust. The answer is that you don't, the compiler does. The compiler has a hidden name *and* type for every closure that you create. However, every closure that you create also implements `Fn` `FnOnce` or `FnMut` trait. That's how you are able to say that a type is `impl Fn(...) -&gt; ...`: but the `impl` is not really a type by itself, it's basically a placeholder for the compiler that says "I can accept any type that implements the `Fn(...) -&gt; ...` trait!". I just realized while writing this post that explaining is hard and frustrating, I finally understand why Haskell people get frustrated when they can't explain monads simply.
&gt; `x` has to live longer or equal to `'a`, but `y` has to live shorter or equal to `'a`. How does one know whether a variable should have _at least_ or _at most_ the lifetime it is tagged with?
I'd argue that the spheres demo is the one that behaves as you would expect. If you arrange spheres precisely in such a pattern, there is no force in the horizontal plane on any of the spheres, only gravity pointing downwards, hence what you see is exactly what you would expect. This is however an unstable equilibrium, and so numerical errors quickly cause similarly arranged boxes to deviate from this equilibrium. Collision detection between spheres is much simpler and can be made much more precise than between boxes, so it's quite possible that the results are simply exact for the spheres. Or, as /u/ergzay pointed out, there might be a sleep\-optimization which considers extremely small adjustments to velocity/positions to be negligible, which would also give the same result.
The Servo team has no plan to write a new JavaScript implementation. Making a complete / correct implementation might be doable (though already a significant amount of effort), but making it *competitive* would be extremely hard. A lot of investment has gone and is going into making SpiderMonkey / V8 / JSC / ChakraCore fast. Also, Rust’s memory safety only applies to code written in Rust, but a JIT compiler also generates machine code and then jumps to it. That generated code does not benefit from rustc’s static analysis. SpiderMonkey already uses parallelism for compilation: * [https://blog.mozilla.org/luke/2014/01/14/asm\-js\-aot\-compilation\-and\-startup\-performance/](https://blog.mozilla.org/luke/2014/01/14/asm-js-aot-compilation-and-startup-performance/) * [https://blog.benj.me/2016/04/22/making\-asmjs\-webassembly\-compilation\-more\-parallel/](https://blog.benj.me/2016/04/22/making-asmjs-webassembly-compilation-more-parallel/) What has been happening is Cretonne [https://github.com/cretonne/cretonne](https://github.com/cretonne/cretonne). It is written in Rust with the goal of eventually going into SpiderMonkey. But it would only replace part of it, this is not a complete rewrite. HolyJIT (mentioned in another comment) is another Rust project that could become part of SpiderMonkey.
1. Is it for reasons related to optimizations? 2. Okay. 3. In Haskell, the following is a legal type `forall a. a -&gt; Int` for the function `h _ = 42`, you can even define `type Magic = forall a. a -&gt; Int`. So I'm not sure why the type isn't legal in Rust ... is it illegal to have a proper polymorphic type that cannot be monomorphized? Is it possible to write the function in another safe way to get the example to work? In essence, I'm have a hard time understanding whether this is 1. Type system/checker limitation or 2. a fundamental restriction due to the fact that not all types are boxed in Rust vs. all types of kind `Type` are boxed in Haskell, or 3. a fundamental restriction due to the fact that Rust has lifetimes attached to variables or 4. a combination of the facts above or 5. something else entirely.
The [**`objekt`**](https://github.com/dtolnay/objekt) crate provides a reusable implementation of the technique from the SO page you found. #[macro_use] extern crate objekt; trait Foo: objekt::Clone {} trait Bar: objekt::Clone {} clone_trait_object!(Foo); clone_trait_object!(Bar); #[derive(Clone)] struct Test1 {} impl Foo for Test1 {} #[derive(Clone)] struct Test2 {} impl Bar for Test2 {} fn main() { let foo: Box&lt;Foo&gt; = Box::new(Test1 {}); let bar: Box&lt;Bar&gt; = Box::new(Test2 {}); let foo_clone = foo.clone(); let bar_clone = bar.clone(); }
Rust just *does not have* higher-kinded types, so it's impossible for a generic closure to exist, because there's no way to express such a thing. A roughly comparable feature called "associated type constructors" is being thought about, but I wouldn't hold your breath. Put another way: you can't do `for&lt;U&gt; impl FnOnce(U) -&gt; T` in Rust. You can do `fn f&lt;T, U&gt;(u: U) -&gt; T { .. }`, but `f` does not have a type *until* you substitute concrete types for `T` and `U`.
The article I found is about 3 years old, and I he didn't really try to write optimized code. But it still outperformed Lua-Jit2. Here is the article. http://goran.krampe.se/2014/10/13/here-comes-nim/
Before I explain this example I will explain one more syntax: * `'a: 'b`: 'a counts as a copy of 'b. * `'a: 'b + 'c`: 'a counts as a copy of 'b and as a copy of 'c. Non-lexical lifetimes allows the compiler to check each branch of code separately. There are 3 borrows in this code. Lets call the one on line 6 `'x1_num`, on line 13 `'x2_num` and on line 16 `'x3_oth`. Let's see the length of those borrows in each possible branch. First is `'x1_num`: * Branch 1 - not taking the loop. The last use of x is at the function call. No use of `number` is happening. * Branch 2 - taking the loop, condition is true. At the end of this branch x is assigned a new lifetime so `'x1_num` will not be accessible from then. Until that point x wasn't used, so lifetime ends right after creation. `number` is used after the lifetime ended, so it's ok. * Branch 3 - taking the loop, condition is false. At the end of this branch x is assigned a new lifetime so `'x1_num` will not be accessible from then. Until that point x wasn't used, so lifetime ends right after creation. `number` is used after the lifetime ended, so it's ok. Next is `'x2_num`: * Branch 1 - last iteration. The last use of x is at the function call. No use of `number` is happening. * Branch 2 - more iterations, condition is true. At the end of this branch x is assigned a new lifetime so this instance will not be accessible from then. Until that point x wasn't used, so lifetime ends right after creation. `number` is used after the lifetime ended, so it's ok. * Branch 3 - more iterations, condition is false. At the end of this branch x is assigned a new lifetime so this instance will not be accessible from then. Until that point x wasn't used, so lifetime ends right after creation. `number` is used after the lifetime ended, so it's ok. And finally `'x3_oth`: * Branch 1 - last iteration. The last use of x is at the function call. No use of `other_number` is happening. * Branch 2 - more iterations, condition is true. At the end of this branch x is assigned a new lifetime so this instance will not be accessible from then. Until that point x wasn't used, so lifetime ends right after creation. No use of `other_number` is happening. * Branch 3 - more iterations, condition is false. At the end of this branch x is assigned a new lifetime so this instance will not be accessible from then. Until that point x wasn't used, so lifetime ends right after creation. No use of `other_number` is happening. The function call can get x from any of the three borrows, and therefore `'a: 'x1_num + 'x2_num + 'x3_oth`. This means that in addition to the restrictions caused by those three lifetimes, neither `number` nor `other_number` may be used until the end of `'a` (until the last use of `z`). Great question BTW, couldn't have explained without such a great example to show on.
I'm not /entirely/ sure it'd be relevant to my case (I've used the numpy and pandas, as an example of a low\-level code that holds state). But it is very interesting. Care to elaborate or share some snippets?
&gt; I'd argue that the spheres demo is the one that behaves as you would expect I think this is just "expect from an idealized simulation" versus "expect in reality".
Problem is, that the compiler doesn't like enums at that point, because `sockt.framed(X)` expects a Value not an enum :( Serde is there for bincode. I wrote an encoder like here: https://github.com/tokio-rs/tokio-core/blob/master/examples/udp-codec.rs Still, above compile-error got me thinking/worried...
I don't know if a library crate would have enough info to accomplish this automatically. We can tell in Rayon that a closure is safe to parallelize just by being `Fn + Sync`. In your little snippet, we wouldn't know about the `foo` and `bar` you've captured, and even `do_the_stuff` might be a captured function pointer. Rayon doesn't care, but I think moving to a new node would have to know all of that.
Two compile `crate a` which depends on `crate b` you already have to have `crate b` in compiled form. It's not so much rustc going out of it's way to disallow it, as rustc not including some hack to allow it.
 const ROW_SIZE: usize = 4; const COLUMN_SIZE: usize = 4; fn main() { let array = [[1; COLUMN_SIZE]; ROW_SIZE]; }
This isn't needed from a theoretical point of view. It is probably required either due to implementation details, or a decision that was taken by the language team. A core team member may have an answer to this one.
 const ROW_SIZE: usize = 4; const COLUMN_SIUE: usize = 4; fn main() { let array = [[1; COLUMN_SIZE]; ROW_SIZE]; }
&gt; let array = [[1; COLUMN_SIZE]; ROW_SIZE]; Thanks, exactly what's needed! Strange this way wasn't discussed in the Rust book.
&gt;You say that if it were not for f(x), you could modify my\_S. This is not true. You cannot modify my\_S regardless of f(x). See here. You cannot modify my\_S because of this line: let x = &amp;mut my\_S; Sorry for the confusion. My example was around non-lexical lifetimes. There modifying `my_s` is possible: [https://play.rust\-lang.org/?gist=7ddab54bf828372de0d952c41d3fb694&amp;version=nightly&amp;mode=debug](https://play.rust-lang.org/?gist=7ddab54bf828372de0d952c41d3fb694&amp;version=nightly&amp;mode=debug)
Thank you for your reply. I believe, it wouldn't be possible to solve the task in the most general way, but maybe we could provide some shims using which that would be possible. Similar to what is done in `mio` or `may` libraries that take std types and wrap them into something even more clever. Overall that would probably require substantial work on all levels, may be even full fledged type, autotrait and/or reflection support at compiler level. But maybe this is solvable for some subset of types (like those annotated with `Serialize` or some problem oriented derive). Such types should be `Send + Sync`, obviously, and should allow transparent interaction across the network using proxy types as well as memoization/caching on the library level. 
Hmm, that makes sense, thanks!
congrats!
Combining your answer with Quxxy's, I think I get why it doesn't work. &gt; In the end closures are structs -- but where do you define these structs? There are no annymous structs in Rust. The answer is that you don't, the compiler does. I think our views of what _is_ a type are essentially different, which is probably contributing to the impedance mismatch. From a Haskell perspective, I'm thinking of a closure as something indistinguishable from a first class function type, and that how the closure is represented at runtime -- a struct with an additional environment -- is distinct from what its type is. For the following example: fn main() { let w = 10; let x = identity(w); let y = "hi"; let z = identity(y); } fn identity&lt;T&gt;(x: T) -&gt; T { x } Before this exchange, I would've thought that `identity` has precisely one type `forall T. T -&gt; T` and the fact that you monomorphize it for efficiency is an implementation detail. However, Quxxy's comment &gt; You can do fn f&lt;T, U&gt;(u: U) -&gt; T { .. }, but f does not have a type until you substitute concrete types for T and U. seems to indicate that `identity` by itself doesn't have a type but `identity&lt;i64&gt;` has the type `fn(i64) -&gt; i64` and `identity&lt;String&gt;` has the type `fn(String) -&gt; String` and so on.
thanks!
I actually wasn't sure why this is the case (wasn't intuitive to me other), so I decided to dig a little deeper and ended up writing another blog post: http://www.squidarth.com/rc/rust/concurrency/2018/06/09/rust-threads-detach.html. The TLDR; here is that thread::spawn uses the "clone" syscall on linux, with the CLONE_THREAD flag, which automatically sets the parent of the spawned thread to be the parent of the caller of "clone".
There's also https://datafusion.rs/, but I don't think does what you want.
I believe Go and Rust target different use cases. Go grows faster partly because it is easier to learn but also because the use case is probably larger ‒ not everybody \*needs\* top speed or as small as possible memory footprint. So I think Go will have more developers, but that doesn't make Rust any less of a language. If one company makes passenger cars, it is going to sell more of them. But the company selling trucks still will make profit, even when the market is smaller. I even think some projects will combine Rust and Go in the future.
&gt;Java has the class per source file mapping baked into the language (it also has inner classes, so it's not entirely 1\-to\-1) but Rust doesn't have that kind of requirement, and that's something to embrace. Not exactly true. You can create more than one Java class in a single .java file. Also, you don't even have to have the name of the .java file be the same as the single class it contains, nor, do you have to have the folder correspond to the package hierarchy. All of these are just conventions that are pretty much universally followed, but, not a requirement.
Hm. That would imply that if I have another 'exposing-service2' it would need its *own* trait and its *own* impl of that trait as well. That would lead to a ton of code copied around if I'm understanding correctly.
Great analogy, puts things into perspective
Oh I didn't realize that. I don't know how nll works ¯\\\_(ツ)\_/¯
The API reminds me of Love2D - really liking what I can see so far! 🎉
What about two functions calling each other, or one function calling itself? Is that implemented with a hack?
You can do this in Nightly by using the `Fn__` traits directly: #![feature(unboxed_closures)] #![feature(fn_traits)] use std::marker::PhantomData; fn main() { let mut v = vec![1, 2, 3]; let f = const_(&amp;v); let _x = f(10); let _y = f("hi"); } fn const_&lt;T&gt;(t: T) -&gt; Consted&lt;T&gt; { Consted { ret: t } } struct Consted&lt;T&gt; { ret: T, } impl&lt;T, U&gt; FnOnce&lt;(U,)&gt; for Consted&lt;T&gt; { type Output = T; extern "rust-call" fn call_once(self, _args: (U,)) -&gt; T { self.ret } } impl&lt;T: Clone, U&gt; FnMut&lt;(U,)&gt; for Consted&lt;T&gt; { extern "rust-call" fn call_mut(&amp;mut self, _args: (U,)) -&gt; T { self.ret.clone() } } impl&lt;T: Clone, U&gt; Fn&lt;(U,)&gt; for Consted&lt;T&gt; { extern "rust-call" fn call(&amp;self, _args: (U,)) -&gt; T { self.ret.clone() } }
Thank you for making them! It would be very useful if a new version is uploaded to `crates.io` to include some of the fixes made after v0.1.0. That way it would not be necessary to use the bindings generators directly.
Thank you for testing! Strangely it does not work on my phone (iOS too) so I just assumed it did not work anywhere.
Yes, the timestep is a bit coarse here so the cube does not have a chance to see the ground (there is no continuous collision detection here). 
iPhone iOS 11.4 so nothing extraordinary 
I see. I'm on iOS 11.2.6 so this might be something that got fixed on earlier versions.
When a parent process loves a parent function very much…
framed() accepts the codec type (something that implements Encoder and Decoder), not the Message type itself. 
You're welcome! Glad to know it is useful. The API will be slightly different for those that want to make something that works for both native and web platforms (see for example the [cube](https://github.com/sebcrozet/kiss3d/blob/wasm/examples/cube/src/main.rs) demo). Basically, Kiss3d has to control the render loop on web platforms so you have to implement a trait instead of writing a loop. But the current examples will still work on native platforms though.
I think you might have actually cleared up something that I got wrong all the time: I somehow assumed the (implicit) `'a` of a `let x: &amp;'a u32` would internally track a specific lifetime of the specific reference stored in it; a bit like a magic enum Lifetime\&lt;'T\&gt; storing a particular lifetime 'T value each time it is assigned. Instead, `'a` seems to be just a "union guard", that abstractly captures all possible "timelines" of things that might have been assigned to it. As an implication, when I have `f&lt;'a&gt;(x: &amp;'a T)`, the `'a` is **NOT about about the ACTUAL lifetime of that particular** `x` **being fed in** which just happens to reside in that reference variable. Instead, it refers to the **sum of all possible lifetimes that COULD be within that reference variable**. I think your lifetime trace and explanation was the best thing I read so far about that topic. I really wished this were part of the documentation or Nomicon in a "Behind the Scenes" or "Advanced Example" section. Thank you so much!
You are right. The [comment](https://www.reddit.com/r/rust/comments/8pvgpb/try_nphysics3d_demos_on_your_desktop_browser/e0fqf65) from /u/Andlon explains it very well to.
After checking around a bit, so it seems. If I ever knew those things, I certainly had forgotten about them completely. Thanks for the correction!
&gt; This is just nim in its compiled version This is exactly what I thought, and therefore you have a problem. Compiled Nim performance should be in the order of magnitude of C or Rust. However it's NOT a scripting language (it's compiled ahead of time), and therefore it's NOT a substitute for LuaJIT. NimScript, unless it now has a JIT compiler that I have not heard of, will be interpreted, and thus **vastly** slower than LuaJIT.
Yeah, it's one of those weird things. To me, it would be nice if Rust were even more opinionated about this. Even though Java programmers by-and-large follow the 1 class per .java file idiom where the .java file exists in a folder corresponding to the package hierarchy, I have worked on code-bases create by some masochists that didn't follow the conventions and it was, shall we say, *interesting*. I hope that Rust gets really opinionated about this with its module system to avoid such nonsense.
&gt; It’s worth noting here too that clone is the syscall used by thepthreads C library, which is commonly used for creating threads in C. Its also the library used by rust 😛. Stdlib in rust isn't to hard to read. Clone is also the syscall used by fork 
Nice, I'm looking forward to using this, to port some flash games :) Unfortunately, most of the examples don't work in my chromium 67 on arch, and some also don't work on FF dev edition on Windows 8.1, e.g. in the image example I get: &gt; RuntimeError: unreachable executed bridge.js:405839:1 _ZN3std9panicking20rust_panic_with_hook17ha5aa0941ef69c52cE https://www.ryanisaacg.com/quicksilver/bridge.js:405839:1 _ZN3std9panicking15begin_panic_fmt17h365a66ae287d834aE https://www.ryanisaacg.com/quicksilver/bridge.js:405678:1 rust_begin_unwind https://www.ryanisaacg.com/quicksilver/bridge.js:405623:1 _ZN4core9panicking9panic_fmt17h05a2699a0dd5ff9dE https://www.ryanisaacg.com/quicksilver/bridge.js:566070:1 _ZN4core6result13unwrap_failed17h21bb1458c2edb8d5E https://www.ryanisaacg.com/quicksilver/bridge.js:10073:1 _ZN64_$LT$image..ImageViewer$u20$as$u20$quicksilver..state..State$GT$6update17hd702f7941ea179b6E https://www.ryanisaacg.com/quicksilver/bridge.js:9575:1 update https://www.ryanisaacg.com/quicksilver/bridge.js:173333:1 set_app/&lt; https://www.ryanisaacg.com/quicksilver/bridge.js:264:27 
I think it involves something like Turkeys or Ducks or some other strange bird, does it not?
I mean, rust was designed for that, so a "feature" not a "hack". You could almost call the C version of it a hack though. C was more or less designed to be read linearly, so you could only call functions above you in the file. To get around that issue they invented (or rather, abused) this "function declarations" mechanism that is less than elegant.
I think, if Rust is to support this, it needs something like [GHC’s `StaticPointers` extension](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#extension-StaticPointers), although a procedural macro *might* be enough to support this.
Ahh okay! I think I get it! :) Thank you for taking your time to explain ^^ 
It would be nice to see an example of shapes moving and colliding, since physics is advertised as being supported, but it does seem pretty cool! I especially enjoyed the raycasting demo.
Could you please give a small example of what exactly you're trying to do? These trade offs can be very specific in nature 
I think it's a strtok?
Fork is its own syscall 
Couldn't we implement generic closures without having general HKTs?
It is, but its not used anymore. Fork is written using c. You can use strace, you won't see any syscall called fork come up
Interesting idea. I'm not sure about Rayon but I think something similar can be accomplished by making such a library, [an executor](https://docs.rs/futures/0.2/futures/executor/index.html) for `Future`s and `Stream`s.
Physics isn't advertised, ncollide integration is; however with the nphysics wasm announcement, it should mesh well considering Quicksilver already uses nalgebra and ncollide2d. Their raycast example shows off the use of ncollide [https://www.ryanisaacg.com/quicksilver/example.html?raycast](https://www.ryanisaacg.com/quicksilver/example.html?raycast)
Huh, that is odd. I know there are a bunch of rather new and shakily supported APIs in use, mostly because the Web APIs for gamedev in general are relatively immature. I'm currently working on porting most of the work of binding to wasm to the stdweb crate, because that way more things will be compile-time checked by Rust. It should alleviate these issues. I hadn't actually heard of unrust, having only played with Piston in the distant past and having looked at ggez. It definitely seems to be more 3D-oriented and more in control of your game state, like an engine. The goal of quicksilver is to primarily handle IO, like drawing, playing sounds, or reading input, and let you use a library like [specs](https://github.com/slide-rs/specs) to handle state management.
But continuous collision detection could be enabled for this?
Is it also possible to use custom gl shaders or vector graphics with quicksilver? I want to port some 2d flash games but using procedural shaders instead of static images.
Such library is impossible and will be impossible. Rust type system isn’t any help there and fact that this is compiled to native executable makes it even worse. Distributed systems are hard and even `Send` will not save you from problems as distributed locks are even harder. Sending lambdas through the wire is even harder as in compiled languages you need to share enormous amount of features: - architecture - processor version - libc version - any other library version - runtime system The problem is so complicated that there are whole languages (Erlang) created to somehow handle this problem and even there are a lot of things that aren’t handled as it would be at least hard. For example in Erlang the message delivery is guaranteed only if receiver process lives during the receive, but sender has no way to detect if receiver got the message (like in UDP communication), if you send message to dead (or non existent process) it will happily handle such. Also you got the guaranteed message order but only between each two processes, so if you have 3 processes A, B, and C where A sends messages a1 and a2 to the process B and process C sends message c1 to process B then all of these can happen: - a1 a2 c1 - a1 c1 a2 - c1 a1 a2 But not a2 c1 a1 because that would mean that messages from A came in different order. And this is only tip of the iceberg of the problems that you need to deal with when dealing with distributed systems. 
Aww. I get disillusioned by watching this. :-( There's so many (well intended) surprising things going on automatically behind my back, and then _sometimes_, you need to handle it manually, and you get confused because you thought that the issue weren't there in the first place. Also, I feel that the `&amp;`, as far as I can see, has to do with _two_ things – borrowing and pointers – is constantly getting glossed over.
Isn't this exactly what Apache Arrow is designed for?
Not sure, though I have wondered if the Chapel approach would work for Rust. At last back when I was working in it, it distributed work by intercepting accesses to indices of the array. This let you do some pretty fancy distributed workloads without a lot of boilerplate. 
I am trying to build a Slack bot using the [Slack crate here.](https://github.com/slack-rs/slack-rs) I am having great difficulty with accessing the contents of the on events. I want to match against a Slack Event ([doc here](https://slack-rs.github.io/slack-rs/slack/enum.Event.html)), and the bit I'm interested in is the `Event::Message(Box&lt;Message::Standard(MessageStandard { channel, text, ts, user, .. })&gt;)`. What I've tried is ... fn get_request_(event: &amp;Event) -&gt; Option&lt;MessageUnwrapped&gt; { match event { Event::Message(box Message::Standard(slack_api::MessageStandard { channel, text, ts, user, .. })) =&gt; { Some(MessageUnwrapped { channel : channel.unwrap(), message : text.unwrap(), user : user.unwrap(), timestamp : ts.unwrap(), }) }, _ =&gt; None } } Each of the `channel`, `text`, `ts`, and `user` is an `Option&lt;String&gt;`. My goal here is to convert from this big enum monstrosity, to a struct which is more manageable. However I am really struggling to make it happen. My main questions are ... * When I go to `unwrap`, I get the error `cannot move out of borrowed context`. So how do I get to the string inside? I was thinking something like `text.unwrap().clone()` or `text.map(|s| s.clone()).unwrap()` would work but I can't seem to be able to get to it. * How do I deconstruct the enum without needing to use the `box` feature?
You can start with the basics with the rust book. https://doc.rust-lang.org/book/ (Check second edition)
Sometimes, the killall command is your last choice on unix - but why did you call your program "men"?
Thanks, but I know Rust quite well. I am just unsure which crates to look at. I thought that gRPC looked cool but it didn't seem to be quite "ready".
I don't know, Ashley Williams told me to use it
Currently it isn't possible to have custom shaders, no. The real barriers to custom shaders is OpenGL version differences; WebGL shaders look a lot like OpenGL 3.2 shaders, but they're different enough that you would need to write a shader version for both platforms. Also, with the macOS deprecation of GL, I'm looking to switch to gfx-rs for the graphics backend, at least on desktop. I do *want* to provide them but I'm not sure it's feasible. As to vector graphics, if you can make do with writing a renderer that outputs vertex / index lists of triangles, then Quicksilver supports that. Otherwise, there's not much vector graphics support. I'm definitely open to suggestions on both fronts, as these are things I'd like to support, but don't see an easy way right now.
So it does! No amount of type checking can save me from writing the wrong string literals.
I see, I was wondering if it was something note along the lines of writing Debian tools or operating system components in Rust. I'm very happy about the debcargo work, but it's more or less the same level of Debian integration for a popular language these days.
You probably want r/playrust.
Just to answer your question, no, your post is not acceptable here. Also please in general stop harassing Ashley.
She said that, I'm not harassing that whore
If you want to move out of the event you'll have to take it by value with `fn get_request(event: Event) -&gt; Option&lt;MessageUnwrapped&gt;` and then you can move out of the box with a regular dereference. Otherwise you will need to `clone()`. match event { Event::Message(msg) =&gt; if let Message::Standard(std) = *msg { // do stuff with std } else { None } _ =&gt; None }
Are posts like this eligible for removal by moderators?
Yes. And yes, I removed it. I also banned CautiousPotential from the subreddit.
Based on whether it is an input or an output. The outputs borrow from the inputs, so they can't live longer than the inputs, whereas it doesn't matter if the inputs live forever.
A good way to handle microservices is by using RPC or REST for sending data, and something like JWT (JSON Web Tokens) for authentication. I'm not sure what the state of the Rust ecosystem is where it comes to those technologies, but that's the general way to do things.
FWIW you can use the `extension-module` feature to dynamically link libpython in both rust-cpython and pyo3.
Is fork just a wrapper containing the clone syscall in some fashion then?
&gt; The outputs borrow from the inputs Is this always the case? Could a function not take a mutable argument, create some new object x, set a member of the input to point to x, and then return x? Would not now the input be borrowing from the output?
&gt; Reading through the docs on clone, clone is a syscall that creates a new process is allowed to share the same “memory space” with the calling process. This is in contrast to another Linux syscall used to create processes, fork, which creates processes with their own memory space. This is what the CLONE_VM flag above enables. Actually in about every implementation on Linux `fork` is a wrapper around `clone` and tracing `fork` will reveal that the actual `clone` syscall really isn't used any more: misj@note $ strace -e fork sh -c 'sleep 0.1 ; true' --- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=14165, si_uid=1000, si_status=0, si_utime=0, si_stime=0} --- +++ exited with 0 +++ misj@note $ strace -e clone sh -c 'sleep 0.1 ; true' clone(child_stack=NULL, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLD, child_tidptr=0x7fc7d7027810) = 14718 --- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=14718, si_uid=1000, si_status=0, si_utime=0, si_stime=0} --- +++ exited with 0 +++ Actual fork syscalls haven't been used in a long time any more.
`clone` is a strict generalization of `fork` and `fork` can be implemented in terms of it and that's what glibc, musl, and uclibc do to my knowledge and probably pretty much every other libc of note. `fork` on Linux as in the actual syscall and not the libc wrapper is collecting dust right now and is kept alive for backwards compatibility only pretty much.
That you need to wrap an enum into a unary struct to make its variants private is a pretty ugly hack though and it'd be nicer if you could just make the variants private.
I still cannot seem to get this to work. * With `*msg` I get `expected type `std::boxed::Box&lt;slack::Message&gt;` found type `slack::Message``. * I tried with **msg and I get `cannot move out of borrowed content` again with `std`, and each of the properties I am accessing from the standard message.
Maybe. I prefer to take a broader view. It's more like a minor annoyance that doesn't weigh in my mind than a pretty ugly hack. :) And this is coming from someone that has done hidden enums like this many times. But sure, it's something that probably could be fixed, given the existence of someone to champion the cause.
Having some "Inner" type wrapped in a cell is handy to hide interior mutability. Usually there is always some state to manage, but forcing a user to take &amp;mut is very strict and may not suit API well.
What is your revised code? I have a working slack bot using the same library with pretty much the code I wrote before, not using any box syntax.
This is another good one yes. One thing to watch out for is using unsynchronized interior mutability, which means your type won't implement Sync, which could be surprising/annoying to users of your library.
I guess that it's nice how in OCaml or Haskell you can declare algebraic datatypes without having to constantly give the inner types names and construct them that way which can be unergonomic. Like if you have a product of two sum types you have to give the sum types separate names and derive all the traits from them separately and all that.
https://github.com/rust-lang/book/issues/1063 Apparently they couldn't add it anymore but the maintainers are aware. So it seems it is mentioned in an program example but not at the logical place in the data type section.
It should be noted that arrays can only be instantized like that for types that are `Copy` so if you need need to duplicate a type that can only clone you need to write a more difficult macro that probably has to use some unsafe constructs to loop over uninitialized values [like so](https://doc.rust-lang.org/std/mem/fn.uninitialized.html#examples).
I'm having trouble finding the relevant section of glibc, but the [musl implementation of fork](https://git.musl-libc.org/cgit/musl/tree/src/process/fork.c) is very straightforward. It uses fork if it's available, but falls back to using clone
I don't think your example is possible in rust because it can't prove that x will live long enough. However, yeah I do think it's more complicated than input/output, and this is why I talked about variance in my first comment. But it'd be good if someone who knows more about this could chime in.
Ok. Btw, how do webgl shaders differ from OpenGL 3.2 shaders? I'm asking because I want to write a desktop renderer for shaders that are loaded from shadertoy (using shadertoy to write VJ effects and then importing them my the VJ software that runs on desktop).
Don't guess, `#[bench]`! Also, [looking at the assembly](https://godbolt.org/g/FSfMN8) might help. My intuition is that the second one is faster, since the rust compiler can elide the bounds-checking more easily. Rust iterators are lazy, they don't iterate twice. However, the Rust compiler does emit different assembly, if you `#[inline]` the `r.get_char()` function.
Ugh
I wasn't aware of the laziness of `Iter` until I just read about it (and you confirmed it). The laziness would definitely help with the speed
I have moved the code into a gist. * [Here](https://gist.github.com/joe-askattest/b45f68f771d037dc6d8abc8526e0ac5e) is a gist of the code in my main program. For now I am trying to turn a message event into my own struct, and then print what I have out. * [Here](https://gist.github.com/joe-askattest/53ae7e47be64868ec035471d02eb03af) is the Cargo.toml dependencies. Just slack and failure.
They should be about the same speed. You could write it even more concise as `rules.into_iter().filter_map(Point::get_char). collect ()`.
I suspect the second is faster because it isn't unecessarily allocating an intermediate `Vec&lt;char&gt;`. The equivalent code for the first would be fn gen_word(rules: &amp;[Point]) -&gt; String { let mut string = String::new(); for rule in rules { if let Some(c) = rule.get_char() { string.push(c) } } string } I'd expect the version I just proposed to be more or less equivalent to the second, up to the whims of the optimizer.
`filter_map`..? I keep learning new amazing things about `Iterator` today!
Let's say I have 3 crates and 1 of them contains a lazy\_static so that I pretty much have a singleton of some struct. If the other 2 crates both reference the same version of that crate, will they share the same lazy\_static? Or will it create 2 instances at different places in memory? 
I should really be better at reading the documentation. I had no idea you could push to a `String`
I think you got your implementation mixed up, your *_1 function is actually the second version of the OP, isn't it? So the iterator-based version is indeed faster than the for loop.
Is there any actual evidence that the author of this project is interested in Rust? If not, then this kind of RIIR dogpiling is a *really* bad look for the Rust community, and posting it here is just likely to make it worse. If there *is* interest from the author, then that's great!
You're kinda looking for a distributed actor library. I don't know that anyone's built one of those in Rust yet.
The option was mentioned in a talk the author recently did at a JavaScript conference.
The author of this repository is Ryan Dahl (creator of node.js no less), who almost certainly isn't interested in porting to Rust, as they have made a number of public comments about how they like the design of Go, and has not commented much on rust at all. They did a whole talk about it recently, which is available on youtube. It seems that the issue tracker has attracted the worst of the jump on the hype cycle bandwagon crowd. Hopefully it wont get too ugly!
The author of this repository is Ryan Dahl (creator of node.js no less), who almost certainly isn't interested in porting to Rust, as they have made a number of public comments about how they like the design of Go, and has not commented much on rust at all. They did a whole talk about it recently, which is available on youtube. It seems that the issue tracker has attracted the worst of the jump on the hype cycle bandwagon crowd. Hopefully it wont get too ugly!
I actually made a library to do this – serialization of trait objects (and hence function objects), but it uses a very dirty hack and only works among instances of the same binary: https://github.com/Rufflewind/detrojt To avoid this hack, first-class support from the language would be needed.
While posting there would be unhelpful at best and counterproductive at worst, there does actually appear to be some chance Rust might get used. There are a couple of other (closed) issues that seem to indicate that using Go was more of a proof-of-concept due to the problems involved in putting more than one GC in a single process as Node.js+Go does and that Rust and C++ are both on the list of languages that will be considered, going forward. https://github.com/ry/deno/issues/165 https://github.com/ry/deno/issues/208
In one of Ryan Dahl's conference talks he explicitly mentioned that he's interested in Rust for this project.
I'm wary of this sort of thing, since, in my experience, just like the borrow checker, it rules out perfectly OK stuff that it just can't be sure about and, especially in forcing code across multiple files, it makes editing code without a full-blown IDE a *major* pain. (I use gVim) Please clarify what you'd like to see.
&gt;Esp. rust is unsafe with memory and concurrency, C++ ditto What's this about?
&gt; fn get_request(event: &amp;Event) You still are accepting the event by reference. You need to accept it by value if you want to move out of it.
&gt; Actually in about every implementation on Linux `fork` is a wrapper around `clone` and tracing `fork` will reveal that the actual `clone` syscall really isn't used any more: Sorry but I am confused. Could please clarify what are saying here? 
Yeah, I hear you. I probably wouldn’t have said “embracing” for that reason. It is good it’s happening though.
They fork?
The book is already 520 pages. Can’t mention every last thing. That said, I plan on fixing this specific issue in the future.
&gt; Actually in about every implementation on Linux `fork` is a wrapper around `clone` and tracing `fork` will reveal that the actual `fork` syscall really isn't used any more:
I don't see how; you need some way to *express* the signature of said generic closure. Maybe not HKT exactly, but something similar.
Actix began work on actix\-remote a few months ago, but it would need more interest and contributors to be usable. [https://github.com/actix/actix\-remote](https://github.com/actix/actix-remote)
I would consider locking at [Rocket](https://rocket.rs/) if using nightly does not bother you. Otherwise [Iron](http://ironframework.io/) is quite good too. If you want your server to be async, [Hyper](https://hyper.rs/) is the only solution as of now.
As far as I know, a String is more or less like a Vec of chars (although unicode dosen't really have a concept of chars). Many functions of Vec also exist for String
One could call Rust from Erlang/Elixir using [Rustler](https://github.com/hansihe/Rustler), but would still need to be aware of the complications you've listed.
Thank you for your passive-aggressive reply, I was genuinely asking for clarification. 
A `Vec&lt;char&gt;` is really a `Vec&lt;u32&gt;` (in terms of memory layout) where each `u32` represents a single unicode character. A `String` is a `Vec&lt;u8&gt;` which makes up a valid utf8 string, for normal ascii text this consumes 1/4th the size of a `Vec&lt;char&gt;` (and for non ascii text it is still generally substantially smaller).
AFAIK we just need to allow HRTBs to work with types, too. It would only be a HKT when a generic closure is passed to another function while staying generic, e.g.: fn call_identity&lt;U, F: for&lt;T&gt; FnOnce(T) -&gt; T&gt;(identity: F, x: U) { identity(x) } But that shouldn't be hard to allow either, right?
This doesn't work because of monomorphization. Basically, generic functions aren't compile into generic versions of the functions - multiple copies of the function are generated for each type it's used with. Same with the closure - once compiled, it's no longer generic - you say that it can work with any type, but it can only work with one type at a time. A new instance could work with a different type, but a single instance won't work with multiple types. You don't notice this with functions because the compiler takes care of making the multiple copies under the hood.
Okay—so when people talk about "syscalls" there are actually two different ways this word is used: the _actual_ systcall interface the kernel provides which gets activated by some magic low level control in the processor and setting an interrupt which the kernel responds to and the second usage is the libc wrapper. In the second case there is actually nothing formally different from every other function; it's a C function like any other; kernels themselves don't nearly provide such convenient and nice interfaces as libcs do and in fact the interfaces the kernels provide tend to differ from hardware to hardware so libcs come in and abstract that all. The libc "fork" syscall under the hood does not in fact use the kernel "fork" interface provided on Linux any more and hasn't for a decade or so now; that thing is just there collecting dust hanging around for backwards compatibility. Ituses the kernel `clone` interface which is _really_ different from the libc `clone` interface. For one the kernel `clone` interface on Linux functions much like fork in that it splits the thread in two at the calling site whereas the libc `clone` wrapper at the same name does not do such a thing and instead takes a function pointer as argument. Since libc "syscall" wrappers are really nothing fundamentally different from any other function strace has no way to pick these up and it only reports upon actual kernel syscalls.
[Actix](https://github.com/actix/actix) looks very promising.
Thanks! That was informative!
Hi, your on the wrong subreddit im afraid. This is the Rust programming language subreddit. /r/playrust is for the game
Additional links for OP: - [Are we web yet?](https://www.arewewebyet.org/) - ["Writing a Microservice in Rust"](http://www.goldsborough.me/rust/web/tutorial/2018/01/20/17-01-11-writing_a_microservice_in_rust/), which utilizes Hyper. - [Tower](https://github.com/tower-rs) is a work in progress, but worth keeping an eye on as well as their gRPC crate. - An [announcement](https://medium.com/@carllerche/announcing-tower-a-library-for-writing-robust-network-services-with-rust-67273f052c40) about Tower from April. - [Actix-web](https://github.com/actix/actix-web) is async, includes an api for both server and client, and targets stable toolchains. - ["Writing a Simple Web Service in Rust"](http://danielwelch.github.io/rust-web-service.html)
Seems hacky, and I still don't understand what the `(U,)` syntax means with the additional comma, but I like it :).
This is one of those "pull you finger out" moments in open source. Show, don't tell. :-)
&gt; That's a very precisely aimed question, I give you that, and a painful/hot topic, given the variety of (strong!) opinions our community members express. Haha, sorry about that. I don't want to stir up any flames. Perhaps "precisely aimed" because I remembered seeing some brief mention of some of these things previously by gfx-rs developer(s) and was intrigued, but couldn't remember what was brainstorming and what was more concrete aspirations. Have the main points of the discussion/debate been captured anywhere? Or would my best bet be trawling through Gitter archives? :) &gt; One direction of higher level abstraction is building libraries for state tracking, resource management, and framegraphs, and making them compatible with each other, letting the user pick what they need. This is pushed, almost singlehandedly, by /u/omni-viral with gfx-memory, xfg-rs, gfx-render, and gfx-chain libraries. Thanks for the tip/links! &gt; Another one is to provide a gfx_device_hal backend for pre-ll, which I suggested in an issue but nobody picked up yet. I can't speak for any other gfx users, but I'm not too fussed about a code-compatible migration path. If something like gfx-render were to offer a _similar-ish style_ of API to pre-ll (not sure if I've understood the goals correctly?) while being easier to implement on top of gfx-hal, then I think that would be more valuable. As long as there was some rough mapping of concepts between the two, then it seems like it would allow me to move to gfx-hal and incrementally adopt/experiment with emerging APIs, with not a huge deal of pain up front. Comments like https://github.com/gfx-rs/gfx/issues/1721#issuecomment-355613102 make me worry that `gfx_device_hal` would be preserving code-compatibility for its own sake, and possibly making it _harder_ for people to incrementally adopt new high-level APIs as they come along. Big ol' caveat: this is just an uniformed outsider's view, and I suspect I'm merely rehashing points that have been talked to death elsewhere! :) &gt; Finally, the third direction is to have a Metal-like abstraction layer. It's low level enough to ripe the benefits of next-gen, and high-level enough to be usable by mortals. Imagine this coated with strong Rust types and proper borrowing/lifetime semantics, and you'd get what I see is a perfect graphics API for our community. Regrettably I can't imagine exactly what this would look like (I have no experience with metal). I generally see the most promise in library ecosystems that are shaped like "diamonds": core stuff together at the bottom, a whole lot of different aspects in (sometimes competing/interchangeable) libraries bulging out in the middle, and then a cohesive and opinionated facade sitting at the top. Might the "Metal-like abstraction layer" exist as this top piece, with things like gfx-memory, xfg-rs, gfx-chain, etc. in the middle?
I think you mean "...tracing fork will reveal that the actual *fork* syscall really isn't used any more".
&gt; Thank you for your passive-aggressive reply, I was genuinely asking for clarification. That wasn't a passive-aggressive reply, it was just a concise one. Note that they words are not all the same, they changed one of the original words that was likely used on accident, which should make the sentence easier to understand.
You are absolutely right! I missed the update, which now makes more sense. 
The quote wasn't copied verbatim. /u/eyko fixed a typo in it that may have been the source of the confusion.
Yep, I've just edited my comment in regards to this. 
Not sure. Judging by context, my best guess is that he doesn't understand things as well as he thinks he does and he's referring to one or more of the following: 1. Unlike a tracing GC, `Rc` and `Arc` don't free reference cycles. Therefore, Rust it's unsafe. 2. `unsafe` exists. Therefore, Rust is unsafe. 3. Release builds default to allowing overflow/underflow. Therefore, Rust is unsafe. 4. He's a fan of Go and doesn't really understand how problematic it is to combine two GCs in the same program.
You could reexport the traits and types in `exposing_service` so you'd only need to import that crate. if you want a second crate that has the same methods and `impl`s and they are never going to change, whats the issue with putting them into the `fundamentals-ds`?
There is [not\-yet\-awesome\-rust](https://github.com/not-yet-awesome-rust/not-yet-awesome-rust) (similar to [awesome\-rust](https://github.com/rust-unofficial/awesome-rust)) where XPATH is mentioned. AFAIK People often discuss these kind of request on reddit or the rust user forum. There is even this [issue](https://github.com/not-yet-awesome-rust/not-yet-awesome-rust/issues/30) where a bounty system for feature request etc is suggested. There is also this [blog post](https://www.reddit.com/r/rust/comments/8ow1lq/sponsor_work_on_rust/) from Aaron Turon where he list a couple of people who can be supported via Patreon. I think bringing all this together and organizing them will surely help.
I wouldn't mind having some way to vote on interest in a particular library. About a year ago, I created an implementation of Judy arrays in the Rudy crate. I have since then ceased development because I don't know how much interest there is in the data structure.
Except when you've got Windows.
Thanks! Yes, I know about these concepts. What tech are you experienced with in this area?
I've generally used languages like Javascript and Ruby for microservices, but the concepts should be mostly the same
Cool! Do you know how Actix and Tokio are related? The former seems to use the latter but I am not quite sure what the gain seems to be. Do you know more? Have you tried it?
Nice link collection! Thanks! This should give me something helpful to read for my train ride.
Nice link collection! Thanks! This should give me something helpful to read for my train ride.
To clarify, `impl Trait` not being a real type is orthogonal to Rust not supporting types with quantifiers. If you want polymorphic dispatch, that's what 'trait objects' are for: ``` fn const_&lt;'a, T: 'a, U&gt;(t: T) -&gt; Box&lt;FnOnce(U) -&gt; T + 'a&gt; { Box::new(move |_| t) } ``` Unlike `impl FnOnce(U) -&gt; T`, `Box&lt;FnOnce(U) -&gt; T&gt;` is a real type; e.g. you can declare an alias for it if you want. On the other hand, `impl` is for when you don't need polymorphism and don't want to pay the cost of polymorphic dispatch. But neither of those can have universal quantifiers, except for lifetimes. With lifetimes you can write types like `Box&lt;for&lt;'a&gt; FnOnce(&amp;'a i32) -&gt; &amp;'a i32&gt;`; this feature is known as "higher-ranked trait bounds". By the way, Quxxy has their terminology slightly off. `forall a. a -&gt; Int` is not a higher-kinded type. If you had `type F a = a -&gt; Int`, `F` by itself would be a higher-kinded type, but the version with a quantifier isn't.
Interesting, but I don't think that would solve the specific issue that drove me to milksnake. [The author of milksnake has a talk about it.](https://www.youtube.com/watch?v=zmtHaZG7pPc) I was trying to get a non-Rust user set up with my extension module, but the build system for `rust-cpython` refused to run because it detected multiple available Python installations. Milksnake doesn't care about versions at all, it works just fine.
It's a tuple type. Basically, since Rust doesn't (yet) have a concept of variadic generics, the `Fn` traits need to bundle all the function arguments into a single type parameter. This is done using a tuple, and then there's some special-case syntax sugar on top: `FnOnce(A, B, C) -&gt; D` is sugar for `FnOnce&lt;(A, B, C), Output=D&gt;`. The `Output=D` part is not actually a second generic parameter to `FnOnce`, but rather constrains the associated type `Output`. But… that `=` syntax is only allowed in trait *bounds*, not when declaring which trait you want to `impl`, so you can't use the `Fn` sugar there either. Yet the *non-*sugared form is marked unstable (requiring nightly), because the desugaring might change in the future if variadic generics are added. It's all rather hacky. Hopefully to be improved someday...
Would target-specific implementations be another manifestation of this? For example, if you try to drill down in `std::fs` or `std::sync` to see how things work, you generally [run into a wall](https://doc.rust-lang.org/src/std/fs.rs.html#97) where the code you're actually looking for has been conditionally imported! 
Does any Rust library support generating server-side code from a contract-first Swagger/OpenAPI schema? I would say that is the bare minimum of support you must have before even considering it in an environment with multiple teams collaborating.
Then they CreateProcessEx
I have a solution but I'm waiting until the next edition to propose it :)
He seems to have a branch (deno2) rewriting to c++, not a good sign but doesn't necessarily mean he's against a rust port later...
See Ry's deno2 branch, seems like - at least for the moment - he's going with C++...
I would forget it again, `flat_map` is more powerful.
They should better CreateThread otherwise the child will be fat and ugly.
Oops. Guess I should have watched the talk more thouroughly!
It's more or less just a desugared closure, with some extra genericity. It's using the traits for their intended purpose so I don't think it's a hack (although they've been unstable for years). A closure creates an anonymous struct which implements one or more of the `Fn___` traits. What you really want is something like: fn const_&lt;T&gt;(t: T) -&gt; (impl FnOnce&lt;U&gt;(U) -&gt; T) { move &lt;U&gt;|_: U| t } But since there isn't a syntax for generic closures you have to implement the underlying struct yourself.
This is probably not because of eliding bounds checking, but because the iterator version can preallocate some space in the vector using `Iterator::size_hint`. You could probably make it faster still by doing something like: fn gen_word(rules: &amp;[Point]) -&gt; String { let mut out = String::with_capacity(rules.len()); out.extend(rules.into_iter().map(|r| r.get_char()).flat_map(|r| r)); out.shrink_to_fit(); out } To be clear, the only reason this could be faster than the usual iterator version is that `flat_map` has less information about the size of the inner iterator. You might be better off using `filter_map`, but I still think that the fastest method would be to preallocate a string that's too large and then shrink it (since that's worst-case 2 allocations).
In the balls demo, if you pull the big ball-cube apart, there will often be some groups of three balls next to each other with a forth ball above and in the middle of them and the the group is slowly rolling away. Should not the top, middle ball push the base balls apart?
Any known crates for 1-indexed, zero space overhead array/vector? _Very_ useful for implementing and experimenting with data structures and algorithms.
I do this with rust MPI bindings.
Yeah sorry about that, I was mostly being lazy. Probably should have added a ftfy or similar after the quote.
Thank you, that has sorted it all out and it's working now. Thanks very much. I think I understand why it wasn't working before too.
Given how incomplete the rendering is, performance numbers at this point would be misleading at best. This is an awesome achievement, but correctness must come before performance.
It's more like translating it to JavaScript in order to run on browsers.
JFYI, for the `map` call you don't need the extra closure, you can just pass in the `get_char`function directly, like this: fn gen_word(rules: &amp;[Point]) -&gt; String { rules.into_iter().map(Point::get_char).flat_map(|r| r).collect() }
Hm, yes, the problem is then, that with an enum, I would need to construct a `(sink, stream)`-pair for each and every variant (?) And, of course, also implement `UdpCodec` for each and every variant... seems tedious, if I am right (which I hope I am not)
I implemented a [sudoku solver](https://github.com/mipli/sudoku) in Rust over the last week or two, as an easy way to learn a bit more about how to write good Rust code in a project that was small and had a very clear goal. I would love if someone could me a code review. The [grid.rs](https://github.com/mipli/sudoku/blob/master/src/grid.rs) and [cell.rs](https://github.com/mipli/sudoku/blob/master/src/cell.rs) files are probably the most interesting for that, since most of the logic happen between those two files. Is the code written as idiomatic Rust? There is a lot of repetition of code in the [cell.rs](https://github.com/mipli/sudoku/blob/master/src/cell.rs) file to deal with the `bitflags`. Is there a better way to do [convert between the various types](https://github.com/mipli/sudoku/blob/master/src/cell.rs#L17), or get all [the set values](https://github.com/mipli/sudoku/blob/master/src/cell.rs#L72)? [This extra block to only check the value of a variable](https://github.com/mipli/sudoku/blob/master/src/grid.rs#L89-L102) feels a bit off, but I've also seen people say that it is the proper Rust way of dealing with the borrow checker. Could I have solved it better, or is this a decent solution?
I will graduate from a technical Highschool in Vienna in a couple of minutes (wish me luck). After that I will have plenty of time to work on my current project, a filesharing Application based on rust and actix-web.
Continuing work on the Roguelike Jam, I hope.
filter\_map is \~twice faster then flat\_map for whole function, btw.
As one of the commenters said, both ways can be just as performant as the other. I, personally, find the functional way very confusing and hard to read, so I tend to write explicit loops. That said, you should not worry about performance first. Solve your problem first, then optimize the solution using a profiler or benchmarking.
Hypercore / Dat Project hacking week 11 [https://github.com/yoshuawuyts/okf\-updates/blob/master/week11.md](https://github.com/yoshuawuyts/okf-updates/blob/master/week11.md)
Awesome, congrats!
You could be even more efficient and succint by taking advantage of the laziness of Rust iterators by fn gen_word(rules: &amp;[Point]) -&gt; impl Iterator&lt;Item=u8&gt; { rules.into_iter().map(Point::get_char).flat_map(|r| r) } This will optimize down to 0 instructions and only be expanded at the point of execution. Depending on your specific use case that could avoid the allocation completely, and perhaps not even need to process the full input, saving even more time
I did write the procedural version first though and got the program working. But then I remembered Rust is (at least partly) a functional language, and that got me thinking about the performance
Yeah I know, I must've had a brain fart when I first wrote it. Thanks though :)
I've been writing some rust for a recipe database. The public functions take in an `Option&lt;&amp;Connection&gt;`. If it's `None`, then the connection will be fetched. Here's one example function: pub fn recipes_with_ingredient(conn: Option&lt;&amp;Connection&gt;, ingredient: &amp;str) -&gt; Result&lt;Vec&lt;Recipe&gt;&gt; { let new_conn; let conn = match conn { None =&gt; { new_conn = connect_to_database()?; &amp;new_conn } Some(c) =&gt; c, }; let ingredient_id: i64 = conn.query_row( "SELECT id FROM ingredient WHERE name=(?1);", &amp;[&amp;ingredient], |row| row.get(0), ).chain_err(|| format!("Can't find ingredient called \"{}\"", ingredient))?; let recipes = recipes_with_ingredient_id(Some(&amp;conn), ingredient_id)?; Ok(recipes) } The way I'm screwing around with the `new_conn` and returning a reference to it just feels kind of wrong here. Is there another way I could re-write the first ~8 lines of this function? It works fine, it just doesn't *feel* very right.
https://youtu.be/M3BM9TB-8yA?t=23m25s
[http://play.rust\-lang.org/?gist=a5135914aa35ca7cf99715442617f486&amp;version=nightly&amp;mode=release](http://play.rust-lang.org/?gist=a5135914aa35ca7cf99715442617f486&amp;version=nightly&amp;mode=release) So \`filter\_map\` is a lot faster then \`flat\_map\`, funny enought map(|r| r.get\_char()).filter\_map(|r| r) is faster then just filter\_map(|r| r.get\_char())
Why not make the `conn` parameter mandatory and force the caller to deal with it? I see it's optional in the `recipes_with_ingredient_id` method too. Why? Having database connections being set up in various places will make your code much harder to test.
As Rust Newbie i am continuing porting my SW Renderer from c/c\+\+ into Rust. 
flat_map is more flexible but filter_map is usually more efficient if your use case is exactly what it's designed for, which is the case here.
You don't need the map either: fn gen_word(rules: &amp;[Point]) -&gt; String { rules.into_iter().flat_map(Point::get_char).collect() } and because flat_map is used solely to filter on the Option result, you can use the more specialised (and clear, and efficient) filter_map: fn gen_word(rules: &amp;[Point]) -&gt; String { rules.into_iter().filter_map(Point::get_char).collect() }
Working on finding something to work on
&gt; Before diving in, I do want to point out that I’m using Linux, and the behavior of Rust might differ to what I’m describing here on other platforms. Well, in Windows, processes and threads are different things, and threads don't have parent-child relationships at all.
I'm still new to rust but I'm building a chat bot to help my friends schedule game nights. Also to provide them a stream of memes. It's really helping a lot of things"click" about the language and I'm having a lot of fun.
Is there a reason why you use `into_iter` in `gen_word_1`, but just `iter` in `gen_word_4`?
I haven't used bitflags before but it seems like you could use shifts and bitwise and to read/write the set bit flags.
Definitely, yes. I've done that a few times too in various crates but forgot about it. :-)
w00t!
https://this-week-in-rust.org has a weekly list. Perhaps there's something that suits you?
Finished going through [Raytracing In A Weekend](https://in1weekend.blogspot.com/2016/01/ray-tracing-in-one-weekend.html) trying to port the example code into Rust. It was my first serious attempt at writing a Rust project and I enjoyed the challenge of writing code in a language I am not too familiar with. The project can be found here, and I am open to any feedback, which can help me improve my code (positive or negative :)) : https://github.com/tstullich/rust-pt My goal for the next few weeks is to get the example now ported into OpenGL and then adapting it for other rendering APIs (possibly Vulkan). If anyone wants to work together with me on this, feel free to message me and I can fill you in on more details. 
This looks really nice! I guess it could greatly benefit from more examples (although since I already used \`ggez\` a bit, I managed to do what I wanted).
If I wasn't out of the state right now I would be there! Hopefully I'll see you all in August. 
Interesting idea! As an OSS dev &amp; contributor, I'd sure be interested in learning what people need/want (even if only think they do). 
Nice. You’re gradually building up some good coverage of the FreeBSD APIs. Are you building anything in particular with these crates?
Noob will be getting started with rust, hello world's and what not.
For integration tests, is there a way to get a the: 1) Current test or function name 2) Current test crate name The tests I'm writing need a unique working directory, and I'd love to be able to generate them without specifying/modifying everything by hand, and without generating random identifiers (because I want to be able to debug easily).
After a busy couple weeks I hope to find a bit of time to work on [`uom`](https://github.com/iliekturtles/uom) (type-safe zero-cost dimensional analysis) and continue the thermodynamic temperature vs. temperature interval efforts.
Back to learning rust while I stream! I'm also starting to think of simple programs that I can make with rust that will be used during the day.
This is really cool. One simple thing that would help in understanding how the library works at a quick glance would be to specify the type of `_velocity` in your example. 
Not really, it doesn't change the result though.
I think the low support and missing functionality is a bit overblown really. Maybe it's a fault with [crates.io](https://crates.io) but there hasn't been a huge amount that I haven't been able to find yet.
+1 for Rocket. After coming from Spark that I had to use for a project, it's like a breath or fresh air. The route matching is extremely powerful and eliminates so much of the dumb boilerplate present in Spark.
Windows (10) does have fork capabilities now, right? 
Congrats! Now make a giant party and invite everyone of us! \o/
I haven’t started yet but I plan to take `cheddar` out of `spectra`. `spectra` is my [demoscene](https://en.wikipedia.org/wiki/Demoscene) crate and `cheddar` is a module of `spectra` that implements a shadding DSL (which is actually a FPL superset of GLSL). `cheddar` is very experimental and primitive so far, I don’t expect a lot of people using it, but a few folks have asked for it (even people with C++-only codebases), and I think some blog posts about its designs might be interesting.
I ported my graphics stack from Haskell to Rust to learn it this way. So hang on, and go on! :)
I don't believe anyone would actually come. Howevery, if anyone here lives near Vienna write me a private message and maybe this party becomes a real thing \(^.^)/
Actix is built on top of tokio
WebGL is much more similar to GLES than it is OpenGL 3.2. The differences are sometimes superficial but generally output works differently, attribute declaration is cleaner in WebGL than it is desktop, things like that that prevent shader compatibility. I do have good news on the vector graphics front: someone pointed me towards the Lyon crate, which apparently supports most of the SVG API. Would this fulfill your vector rendering needs?
Thank you for the detailed answer! Whilst I understand, that architectural differences render general solution impossible, still I believe, that we may find a partial solution that would work for a fair number of cases. Also, we may target WASM and achieve platform independence at a cost of speed.
Quick question, since I want to be lazy -- I'm writing an app that's separated into large-ish components (one `ProcessWatcher` type instance that interacts with a child process and a `AdminApp` that takes web requests) that run independently of each other -- what's a good way to get them to communicate from one to another? `main()` is holding mutable references to both the components, and I *could* give `ProcessWatcher` mutable reference to the `AdminApp`, but that seems limiting. The other approach I was considering was properly starting each component up in it's own thread and using [`std::sync::mpsc`](https://doc.rust-lang.org/std/sync/mpsc/index.html) to do a sort of RPC procedure (I'm also unclear on how exactly this would work, maybe I could have a channel of futures and send a future to the service to do something with and wait on it?) Anyone have any better time-tested alternative approaches?
Interesting. However doubt so, since indexed/random access patterns in Rust are often considered suboptimal and iterator/combinator based approach usually shows better results.
Maybe you're right. In a future-based code we naturally have points where computation may be interruptded and/or moved to another thread/node. So it may probably be solved if the whole context of a future is movable.
Yes, actor models quite nicely fit in my description. IIUC, they require the code to be written in a message passing style. Also, sometime ago someone introduced a game engine written in Rust that is based on actors.
When watching the talk and hearing him talk about going Rust I also thought that protobuf should be replaced with [Cap'n Proto](https://capnproto.org/)...
&gt; they require the code to be written in a message passing style. Yes, but all the parts of distributed message passing are things you'll have to deal with in your hypothetical rayon-like library at some level. Distributed computing is really not like threading in many ways. You might be able to simplify the interface a bit beyond that, but implementing on *top* of an actor library is probably close to a bare minimum. I'm not sure if that game engine actor library is general enough or actually distributed.
HEhe yeah sounds great and i will continue, but sometimes i am feeling a little bit confused about correct porting from C to rust. But i think i should skip these thoughts beside. Ok i took a look into libc::malloc and the Drop Trait. For the first time i will skip this hope thats the right decision....we will see :)
Actually getting around to working on [ggez](https://github.com/ggez/ggez/) more, a lightweight 2D game framework inspired by LÖVE. It's been a [busy week](https://i.imgur.com/9MHiUzs.png) but we have a number of key changes merged in and are beginning the ironing-out process: * Ratysz replaced `sdl2` with `winit`. Woohoo, we are now pure Rust! * I refactored the drawing API a fair bit, removing some annoyingly stateful bits and replacing every function taking `Point2` or such with `Into&lt;mint::Point2&gt;`. This means you can mostly-transparently use whatever vector math library you want that provides conversion traits into [`mint` generic math types](https://crates.io/crates/mint), instead of having to use the ones ggez provides. Plus the usual bevy of minor improvements and fixes.
Is there need to RVSP or just show up?
Adding `RTM_XXXROUTE` and `RTM_XXXNEIGH` packet types to [my netlink library](https://github.com/little-dude/netlink). Once this is done, I'll probably push a 0.0.1 to crates.io.
A NES emulator for the Numworks graphical calculator
I think the best convention would be that if I were to use a leaf module of any given crate, then, the amount of symbols imported into my name\-space would be limited to around 10 items at most. In other words, be granular with leaf modules. Also, be granular with crates (similar to how Piston project does it). In other words, adhere as much as possible to the "Do 1 thing well" principle for both Crates and Leaf Modules of Crates. Should this be "enforced" by cargo and/or the compiler? I would say not. Should it be linted for? Yes, I would like to see available lints for that that should default to warn/on, but, could be configured by project for error or off at the project owners discretion.
Wouldn't downloads, PR's, Issues opened on GitHub (or whatever you used) be the "votes"? There seems to be a lot of ways/places to talk about and suggest these sorts of things. If there is really an interest in something particular, someone or some organization should do one of: \* Develop and publish it \* Start developing it (maybe just layout the requirements and desired interface in Github project and invite others to contribute) \* Make a post here, on the Rust user forum, or on a blog post laying out the desired functionality and rough interface and then try to get people to take interest in it (see above) \* Offer to pay someone(s) to develop the idea Just having a place to "Vote" on it doesn't seem all that useful. The best votes are some sort of meaningful action: 1) Propose a design, 2) Start the project, 3) Download/Test/Submit Issues to already existing projects, 4) Open PR's against existing projects, 5) Offer help on documentation or testing for existing projects, 6) Offer money (via Patreon or other mechanism) for development of something specific To my mind, these are the only kind of "votes" that will ever matter.
What is the idiomatic way to do a string enum? I have a value that should be serialized / deserialized to a string, but it can only hold a small number of values. 
I know that when I first started examining Rust Githubs for various projects, I thought many were unmaintained because there hadn't been any commits in a long time. The more I looked into it though, the more I discovered that many/most Rust crates, once created, work really well and do not require many changes after they've been created and so though many look unmaintained, the fact is, they've required no maintenance (exceptions do exist though).
Having a discussion around missing library functionality is an easy thing for Rust Users https://users.rust-lang.org/ the more important topic is how traits are designed.
I work in the same building! Neat. To bad it’s at 7 :( 
Got no answers in the last easy questions topic, maybe this isn't an easy question? I'll try again before creating a new post: Does anyone have a Tokio example of a server\+client that is not request/response but allows both the client and server to send messages at any time? I went through Tokio's examples but coudn't find anything that fit. 
Paris here. Sorry, next time around! :)
RSVP'ing would let us get a headcount more easily, but feel free to just show up if you're not sure! :)
 let result = counter.send(PlusOne) Will this be a compile-time error if `counter` doesn't handle `PlusOne` messages?
Very excited for the next post. I dipped my toes into Actix and was a little put off by the lack of documentation.
Yes. It is statically guaranteed.
Yes I think putting "Last updated" so importantly on crates.io could be a bad thing, since it makes it seem like it's a reasonable metric to judge a project. I know I do it subconsciously, I see a project that hasn't been updated in a few months and I think that the owner has abandoned it. That is a very good point, thank you for that!
Oh, interesting, thank you! This may be difficult to fix...
In such case you need WASM runtime which defeats most of the reasoning behind using Rust over Erlang. And even in such case you still would need to provide some kind of distributed synchronisation which is enormously hard. Instead I would suggest you raw BEAM and if you need Rust speed on some calculations then you can use Rustler (as /u/memoryruins said) which is dumb easy. And by any means it wouldn't be "Rayon-like" as it simply cannot be made that easy.
I've finally finished the new version of `crossbeam-channel`. The interface is now almost identical to channels in Go. This also applies to the select statement - the new `select!` macro is now really easy to use: let (s1, r1) = channel::unbounded(); let (s2, r2) = channel::unbounded(); s1.send("foo"); // Since both operations are initially ready, a random one will be executed. select! { recv(r1, msg) =&gt; assert_eq!(msg, Some("foo")), send(s2, "bar") =&gt; assert_eq!(r2.recv(), Some("bar")), } Benchmarks: https://i.imgur.com/tRI4HMO.png Docs: https://docs.rs/crossbeam-channel/0.2.0 
I've been using the strum create for it, and it does it's job well for me.
Its not support by Rust, yet. But VMS recently got resurrected by HP, and the [next major release (warning pdf)](https://www.vmssoftware.com/pdfs/State_of_Port_20171006.pdf) is moving to an LLVM based tool chain so it should be possible to run it there in a couple years.
Wow. I’ll need to read up on why they’re doing that investment. 
If you know how to use github search you can find all the examples you need. You have to be logged in: https://github.com/search?q=mio+poll+extension%3A.rs
Finished up work (at least for now) on my pure-Rust GPU-accelerated path tracer. The code is kind of a mess, and there's probably a lot of bugs, but it produces beautiful images like [this](https://imgur.com/a/DVgCqSw) so I can live with that. That's 300k polygons with 2048 paths per pixel, rendered in 3.5 hours. I'll probably start posting the accompanying series of blog posts this week sometime, depending on how long it takes me to finish them. For now, though, I'm just going to enjoy my day off. Cheers!
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/mm5fiBM.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
Oh this looks great. If only it had built-in serde support, but this is a great start. 
Just saw this go by on docs.rs and decided to go read the docs for it. Cool!
Thanks Brian! Quick update: KubOS just shipped the first full flight segment payload integration for a mission going up later this year. The core of it is in Rust.
That's probably true for local workloads, but for distributed ones I doubt the memory access times are that different. That said, Chapel does have their own iterators, parallel iterators, etc, that could act as a good data point.
Also, you can catch up in person with Ryan, one of the flight segment developers, if you're going to be at RustConf in August: http://rustconf.com/program.html#spaace
&gt; `Command executed with exception: Doc-tests nvimpam_lib` I'm assuming this means the process crashes rather than exited correctly. I'm assuming [KillTheMule/nvimpam ](https://github.com/KillTheMule/nvimpam) is the project. Looking at your [appveyor.yml](https://github.com/KillTheMule/nvimpam/blob/master/appveyor.yml), I had a question. &gt; `cargo test 2&gt; $null` If I'm reading this right, you are redirecting `stderr` to nul? Isn't that hiding the output of the failures? Unfortunately, I just ran it locally and I got roughly the same output and a good exit code. I was hoping to have it fail so I could get a better idea of why.
&gt; If I'm reading this right, you are redirecting stderr to nul? Isn't that hiding the output of the failures? That's what I would have thought, yeah. Doesn't seem to be the case though. I'm grasping straws with appveyor here. I've made progress by only running the things in powershell that I really need to. Things have gotten better, but I'm not yet done... very weird environment :D
Actually, there are plenty of reasons to use WASM over alternative IRs, namely that you're not confined to Erlang and Elixir. It also omits features like a GC that can get in the way of low level code.
&gt; Aside from being “an iron oxide” according to Wikipedia, Rust is also the name of a programming language that was originally invented by Graydon Hoare back in 2006. Probably a good idea to mention that it was [more probably](https://www.reddit.com/r/rust/comments/27jvdt/internet_archaeology_the_definitive_endall_source/) named after the family of fungi, given how many people have never even heard of the fungi.
I'm not ready to actually announce it, but since you asked... I'm building [a Wayland "desktop environment"](https://github.com/myfreeweb/dankshell) :) - It's in very early stages right now, just a proof of concept basically. - It's based on libweston, which I wrote [the Rust bindings for](https://github.com/myfreeweb/weston-rs), and gtk(-rs). - It uses [loginw](https://github.com/myfreeweb/loginw) as a setuid wrapper for launching. - Both loginw and the compositor run under Capsicum, I'll try that in the GTK shell too. - `tiny-nix-ipc` is used to pass input/gpu devices from loginw to the compositor and to pass Wayland sockets to the compositor's non-sandboxed "spawner" process that execs the GTK shell and any apps the user would launch (more specifically, that is used for privileged Wayland sockets — they're created as socketpairs in the compositor, which allows attaching permissions info to the server side of the socket). - `pdfork` is used in loginw to fork the process that execs the compositor. The parent uses kqueue on the process descriptor to get notified of the child's termination. `shmemfdrs` came out of a [PR to servo/ipc-channel](https://github.com/servo/ipc-channel/pull/178). I actually had [Servo working under Capsicum sandbox](https://github.com/servo/servo/issues/11625#issuecomment-342243384) :) Also speaking of FreeBSD API coverage, [devd-rs](https://github.com/myfreeweb/devd-rs) for device hotplug notifications comes from [u2f-hid-rs porting](https://github.com/jcjones/u2f-hid-rs/pull/62).
Let's say there was such a place and your XPATH example was top ranked. Then what? 
I'm somewhat curious about actor systems like this and I'm wondering if it's possible to use core functionality of actix with no_std. I'm thinking of making a modular OS where everything is broken into as many independent modules/actors as possible. The idea is that everything should be able to be transparently send between cores, or better yet separate computers over the internet. The latter goal is a stretch, but I'd still like to try running the OS as a collection of independent actors communicating. If this sounds like standard processes, well it kind of is, except a lot of it is still just one binary. My hope is that built in kernel modules communicate with each other with the same mechanism as user processes.
Nice post. Thanks!
Using shifting to figure out how many bits are set in the number is a good idea. Sounds like it would be a bit faster. The `.intersects()` method in `bitflags` is a bitwise AND. Reading through the docs again I see they have a `.toggle()` function that functions as bitwise OR, so maybe I should use that as well to be more consistent.
If you want to get rid of the powershell for the conditional, you could use [`cargo when`](https://github.com/KillTheMule/nvimpam/blob/master/appveyor.yml). The downside is there aren't prebuilt binaries. I have it on my plate to resolve that as I try to [improve the CI process](https://github.com/crate-ci/meta/issues)
I'm trying to understand when to use Boxes to do generic code. What's the difference between the two implementations of Particle (Enum/Box) in this? https://repl.it/repls/BetterSharpNotifications A lot of this is pseudocodish. I hope it gets the idea across. Which one is more idiomatic? What's the performance / stack/heap differences this could cause? How much is a matter of opinion? 
I think either is reasonably idiomatic, though my preference, if possible, would be a third option: avoiding the trait object with by parameterizing `ParticleWithBox&lt;Strategy&gt;`. Or is there a particular reason you need dynamic dispatch? The enum version involves a few branches, but they're all predictable, whereas the trait object involves an indirect call each time you need to update. I expect the enumeration to come out faster than the trait object, but I can't predict how much faster. This would be an easy thing to benchmark, once you turn your pseudocode into real code.
I think its also interesting that Rust is used quite a lot in the Fuscia OS Google is making. I don't know what the purpose of the OS is though.
Parsers in Rust are pretty powerful and just keep getting better. Although I am investigating some things currently, nom has been my go-to parser for a while. It is a very fast static parser and I definitely recommend it. It uses macros to build parsers.
Still working on Spacebox, a FOSS Dropbox sync client. If you're interested in contributing, get in touch and I can get you set-up! https://github.com/spacebox-org/Spacebox
&gt; a lot of it is still just one binary I'm not following what you're saying here, could you expand? &gt; built in kernel modules communicate with each other with the same mechanism as user processes How does your idea differ from traditional microkernels? Is it perhaps that you want to type-check the possible inter-process communication? Or is it about the programming paradigm used to handle communication? Or is it about forbidding shared state (which I guess normal microkernels allow, for performance reasons) between the processes?
The idea would differ from microkernels in that there's still a lot running in kernel space. There's just no real distinction between a kernel-space process and a user-space one other than privilege, and possibly type-safety.
[removed]
Owner of NYAR here! Thanks for the plug. :) Ironically, I haven't pushed on the bounty platform to hard yet because I'm unaware of any demand for it. I'm more than happy to dedicate more of my FOSS time to it, if somebody does want it!
Make sure to run with --release too. I have seen some posts on here where people make something fast and then realize it can go even faster.
Releasing [cargo-tarpaulin](https://github.com/xd009642/tarpaulin) version 0.6.0! So just seeing how cargo install handles the new syn dependency which requires a rust flag to build... Last week I also worked on my embedded rust stuff for the stm32f469 family of processors so should be continuing with that!
I mean. Rust is right, because chunk can return fewer elements is the array does not contain enough data. You assert that with your fix (% instead of &amp;) 
Congrats, and thank you for this excellent tool that is tarpaulin!
I'd never heard of this project (Still prototyping, don't need test coverage... yet). Could you edit the post to include a link to the github?
I found your results surprising and I had a hard time being sure that the functions were testing what I thought that they were (especially with your struck-out comment) so I duplicated it: https://gist.github.com/rust-play/6afd4e234ba587fe8de90aa7a698daa7 I got extremely weird results, notably the order that I ran the benches in affected the results. This was pretty reliable across a large number of runs. `basically what I expected: =========================================================` running 3 tests test tests::bench_gen_word_a_map_flat_map ... bench: 53,012 ns/iter (+/- 27,260) test tests::bench_gen_word_m_filter_map ... bench: 56,218 ns/iter (+/- 11,950) test tests::bench_gen_word_z_loop ... bench: 58,284 ns/iter (+/- 12,795) test result: ok. 0 passed; 0 failed; 0 ignored; 3 measured; 0 filtered `different order: ==================================================================` running 3 tests test tests::bench_gen_word_a_filter_map ... bench: 61,287 ns/iter (+/- 10,744) test tests::bench_gen_word_j_map_flat_map ... bench: 54,019 ns/iter (+/- 9,161) test tests::bench_gen_word_m_loop ... bench: 55,555 ns/iter (+/- 8,689) test result: ok. 0 passed; 0 failed; 0 ignored; 3 measured; 0 filtered out `with one function commented out: ====================================================` running 2 tests test tests::bench_gen_word_a_filter_map ... bench: 55,040 ns/iter (+/- 6,994) test tests::bench_gen_word_m_loop ... bench: 53,861 ns/iter (+/- 8,243) test result: ok. 0 passed; 0 failed; 0 ignored; 2 measured; 0 filtered out 
Oh wow, that’s exciting. I’m currently running GNOME on FreeBSD, but would totally be up for using/testing this when it’s ready. 
How shall I ever forget that dreadful vigil? I could see by his eager face and his eyes that our smiles were turned in an out-house, but no coins were to be found, which may have dated from the roots.
Link is now added! 
Dang, just saw this and I already have Thursday plans! How long do you anticipate the meeting will run? Perhaps I could come late....
Some architectures like `ia64` and `riscv` have never had a true `fork` syscall, only `clone`, and `ia64` even deprecated that for a `clone2`. They're all wrappers around [`_do_fork` inside the kernel](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/kernel/fork.c?h=v4.16#n2050) though, just with different flags.
That's why there's a `maintenance` tag in the manifest, but not a lot of crates use it. 
^The linked tweet was tweeted by [@stjepang](https://twitter.com/stjepang) on Jun 11, 2018 15:53:28 UTC (39 Retweets | 115 Favorites) ------------------------------------------------- A new version of crossbeam-channel is published. This channel in Rust works almost identically to channels in Go. It supports select operation and is fast. Now we just need to make it work with futures. :) Benchmarks: [https://i.imgur.com/tRI4HMO.png](https://i.imgur.com/tRI4HMO.png) Docs: [https://docs.rs/crossbeam-channel/0.2.0](https://docs.rs/crossbeam-channel/0.2.0) ------------------------------------------------- ^^• Beep boop I'm a bot • Find out more about me at /r/tweettranscriberbot/ •
The `select!` macro looks really elegant! I remember previous iterations required the user to loop or something...how'd it cleaned up so nicely?
What are the best resources to learn about how and where to use actors vs. functions vs. futures vs. other ways of dividing up work and resources? Intuitively they seem specifically useful in Rust because they provide a nice solution to stateful resource management -- e.g. ownership of non-transient things is relegated to an actor. Am I getting the correct read on this? Where can I learn more?
Even though I don't get to work with rust as I used to — every time I see a post about actix it puts a smile on me. 
Can't you just use Serde then? 
Great progress! Really looking forward to HTML reports! :D
Alternatively just throw a macro at it, e.g. // Usage: `ensure_conn!(conn)`. macro_rules! ensure_conn { ($target:ident) =&gt; ( let tmp; let $target = match {$target} { Some(c) =&gt; c, None =&gt; { tmp = connect_to_database()?; &amp;tmp } }; ) } But I would really recommend changing all those instances of `Option&lt;&amp;Connection&gt;` to just `&amp;Connection`.
Yikes that function signature. This is why I stay the hell away from C. Four arguments which are "unsigned long" all with completely different semantics and most of them not exhaustive. You can accidentally pass an actual number to any of them and the compiler won't complain.
The alternative of having exclusive references everywhere where shared ones suffice is about as annoying though.
I don't agree. Or at least, I don't agree with your generalization. 
Thanks for posting this! I just got Tarpaulin set up on one of my crates. Including signing up for coveralls.io and building the crate twice, it took me about 15 minutes. Completely painless!
Thanks for all your hard work on getting this up and running on gl-rs! Super awesome to see folks making great stuff like this!
Would it make sense to include actix in the comparison benchmarks?
I used to think that having a very general `Select` struct for building select loops was a good idea. But it wasn't pretty to use and one had to be careful to respect a list of rules... or else you get deadlocks. The old `select_loop!` macro, a simple wrapper around `Select`, wasn't that much of an improvement, either. The feedback I got here motivated me to seek other solutions. At the time, it wasn't obvious how to improve the situation at all - it seemed like the best we can do in Rust. There were a lot of obstacles, for which I created separate issues on GitHub: - https://github.com/crossbeam-rs/crossbeam-channel/issues/1 - https://github.com/crossbeam-rs/crossbeam-channel/issues/2 - https://github.com/crossbeam-rs/crossbeam-channel/issues/3 - https://github.com/crossbeam-rs/crossbeam-channel/issues/6 - https://github.com/crossbeam-rs/crossbeam-channel/issues/8 - https://github.com/crossbeam-rs/crossbeam-channel/issues/9 - https://github.com/crossbeam-rs/crossbeam-channel/issues/10 - https://github.com/crossbeam-rs/crossbeam-channel/issues/11 - https://github.com/crossbeam-rs/crossbeam-channel/issues/26 - https://github.com/crossbeam-rs/crossbeam-channel/issues/39 - https://github.com/crossbeam-rs/crossbeam-channel/issues/46 I suggest reading those if you're interested in how we got from the old selection mechanism to the current one. Anyways, after a lot of discussion with other folks, thinking, experimenting, somehow in the end everything clicked and I figured out how to fix all problems... at once! Hence this [giant PR](https://github.com/crossbeam-rs/crossbeam-channel/pull/41), which closed almost all issues. I wish I could explain the inner workings of `select!` in more detail, but it's a long story. Maybe later in a blog post. Happy to answer questions, though! :) But here's one interesting nugget in the new macro that might've gone overlooked - it's got its own [mini-parser](https://github.com/crossbeam-rs/crossbeam-channel/blob/fc5b5046909d30c2fa837e486b959286e2d8dcdf/src/internal/parse.rs), which validates the input and attempts to display friendly error messages. For example: select! { recv(r, msg) =&gt; println!("{:?}", msg); default =&gt; {} } The parser for `select!` displays the following error: error: did you mean to put a comma instead of the semicolon after `println!("{:?}" , msg)`? --&gt; src/main.rs:43:5 | 43 | / select! { 44 | | recv(r, msg) =&gt; println!("{:?}", msg); 45 | | default =&gt; {} 46 | | } | |_____^ Another interesting piece of code to look at is the [code generator](https://github.com/crossbeam-rs/crossbeam-channel/blob/0b72c5908bb6320d9eaec5946cb8a0dbd49192a6/src/internal/codegen.rs) for the macro. There you can see how the individual cases in `select!` are disassembled into inner library calls.
I've looked into it, I don't see any way to just tell serde to serialize or deserialize the given enum to some constant. In my ideal world I could do this: #[derive(Serialize, Deserialize)] enum MessageType { #[serde(value = "request")] Request, #[serde(value = "response")] Response, } struct Message { type: MessageType, body: Vec&lt;u8&gt;, } And get a serialization like this: {"type": "request", "body": ... } But I don't see any way to do it. 
This finally helps me grasp an understanding of the actix-Actors-Future thing as a rust newbie! The no-previous-rust-experience-assumed approach in this article and the Javascript code comparison really helped me. I wish it was a more popular writing style when reading other Rust introduction blog post. I tend to learn easier and faster when notions are mapped 1 to 1 to existing language or concept. Even when the Rust implementation has some specifics that needs to be warned about. Thank you. 
I'm not really sure what you mean because that is exactly how `Message` would serialize in that case (without those `#[serde(value)]` attributes... don't think that's a real thing). Can you work up a compiling example to show the issue? As a side note, you could combine the structs as `enum Message { Request(Vec&lt;u8&gt;), Response(Vec&lt;u8&gt;) }`. 
&gt;a game engine written in Rust that is based on actors. You wouldn't happen to be referring to [Citybound](http://cityboundsim.com/)? It's not (just) a game engine, it's the game itself that's designed around actors.
Ah, I didn't give the best example, my apologies. In the example, I need the serialized values to be lowercase. In the real-world, I need them to be some other weird value that has hyphens. Basically the idea is "serialize this enum as this string value". 
Well, you can `#[serde(rename)]` (not `#[serde(value)]`) the variants to anything you want. Or even put `#[serde(rename_all="lowercase")]` above the whole enum. 
If you clone a receiver then do all receivers receive a copy of every sent message or only the first one to call `recv`?
Only the first one to call recv.
Thanks for your help! I opened up a pull-request to add this to serde's documentation. https://github.com/serde-rs/serde-rs.github.io/pull/76
Thanks again for the catch. This necessitated a change of strategy to allow atomic updates of ids and ranks together. What do you make of [this](https://github.com/tov/disjoint-sets-rs/blob/one-atomic/src/concurrent.rs#L188-L207)? The tricky part is still when the ranks are equal. The key, so far as I can tell, is that if attempting to increment the rank fails, that means the rank has already been properly updated. This is because another thread will only attempt to update the same rank under the same circumstances. Does that make sense?
Huh, that seems a little self defeating: If a crate is unmaintained, who's going to update the maintained status?
What is the idiomatic way to get the *second to last* item of a vector / slice? Index with length - 2? Reverse and skip the first? Pop and .last()? Coming from python, was surprised to find no support for negative indexing.
&gt; If a crate is unmaintained, who's going to update the maintained status? It's a tough one, perhaps giving users a way to report broken packages? The question then becomes about filtering real from fake reports. Or alternatively these could be reported on crates.io - not sure how attacks can be prevented though.
MIO is great. At a certain point of complexity you start running into common problems that Futures and Tokio solve. I personally found it very difficult to get the right combination of usage of MIO to be properly portable across platforms. Tokio abstracts this away. On top of that, with async io I always find that I turn to state machines for managing the state of connections. The Futures library makes it easy to abstract out different state machines, and gives access to a lot of default and common state machines. This comes at a cost of type complexity, and I personally think, puts Futures into the medium to advanced arena of Rust. If you’re not yet comfortable with nested associated types, and patterns to use those, then you may be in for a ride. You’ll definitely be more comfortable with them after working with Futures.
I don't think there is one. `.iter().rev().skip(1).next()` is probably the most easily "correct" one, since arithmetic on the length can lead to underflow. &gt; Coming from python, was surprised to find no support for negative indexing. Negative indexing interferes with type inference, and is less performant.
I'm the first one here, so it is my turn this time! Wrong subreddit friend! You are looking r/playrust. This is the subreddit for the Rust programming language.
That parser is *wild*. Thanks for going through so much effort to make error messages friendly, I'm sure I'll be thanking you in my head many times in the future, too!
I’m fairly certain this is exactly how Go does select. It makes a lot of sense to have dedicated syntax and parsing around this kind of higher level operation. 
I'm working to get v3.0 of my [serialport-rs](https://crates.rs/crates/serialport) out. This enables arbitrary baud rates on the non-Apple BSDs, Windows, and Linux. Unfortunately I've found a bug on master that I need to fix before I do. This release also introduces support for a bunch of new platforms, so there' some demand for it, so I'm really hoping to resolve it. Even bought some new USB serial adapters to help.
I've been having issues with creating a simple API for using Tumblr in Rust. The problem is that certain fields only exist in a JSON object if a certain `type` is present. For example: "posts": [ { "id": 1, "type": "photo", "photo_url": "some_link" }, { "id": 2, "type": "text", "title": "my text title" } ] That would lead me to create a struct that includes base data (e.g., ID), and then an enum to contain other type\-specific data. ``` struct Post { id: i32, data: PostType } enum PostType { Photo(PhotoData), ...etc... } struct PhotoData { url: String } ``` This is straightforward, but I am having a lot of difficulty implementing a way to deserialize this using Serde. Any help would be appreciated as building a custom deserializer is way over my head. I have the actual project here - https://github.com/piedoom/rumblr
I dunno man we could post our funniest encounters with Rust the language too, could be a good thread. 
Are you able to share any examples?
&gt; They’ve been slowly working towards extending the compiler to support const generics in a clean and nice way wooooooooooh Glad to see this work being taken on and the compiler team growing.
I think `Option::unwrap_or_else()` might be what you're looking for.
Debugging via commits (to test in appveyor) can be time consuming. If you don't want to install windows locally, attach to an appveyor session via RDP and make your tests manually there, step by step, until you succeed. 
Tokio is specifically for non-blocking, aka async-io. Depending on your level of familiarity with Rust it may be a great choice, or a hinderance to your appreciation of the language. That being said the new tokio crate simplifies many things from the original implementation. If you’re going to try tokio, you’ll need to become familiar with Futures, which are very similar to the Iterator combinators. Both are great libraries, but do require a good understanding of Rust.
Speaking of which: https://github.com/rust-lang/rust/issues/51192#issuecomment-394126083
Wow... I wasn't even hoping for 2018.
would using [serde's internally tagged enum representation](https://serde.rs/enum-representations.html#internally-tagged) help here?
Thanks very much /u/thiez! I'll look into both options. I think you're right that I should just accept a `&amp;Connection` and force the caller to establish the connection.
The book "Rust High Performance" has a pretty good section on concurrency at the end (covers sync primitives, futures, and some crates). No coverage of actix though. Actix\-web seems to have a lot more example source to look through than regular actix unfortunately (if that is what you are interested in). More general books (not rust related) on concurrency and the actor model obviously will apply though. 
https://twitter.com/InsanityBit/status/974030817650401280 This one really got me, and it's fitting.
There is no reason to say any of these "wins" given the error bars.
std::net for blocking network code. mio::net for low level async sockets. tokio for higher level async.
Could you go into a bit more detail of the distinction between mio and tokio for me please? I will be reading about them as well and already have a little of tokio, but I would like to hear your description of the distinction eventually in any case :). 
Yeah, indeed. I was referring to this game.
Nice introduction! The only thing that bothers me: &gt; The Rust compiler is often quite strict on what data you are allowed to access at what point in the application logic, because it knows about concepts like threads and potential race conditions. Actually the compiler knows nothing about threads. The safey guarantees result from the borrowing/lifetime system :)
Now that's a good tip if I ever saw one, thanks a lot!
Ah I'd love to get paid to work on a cool project like this. Congrats!
Mio is for sockets that will never block. Calling send or recv (read/write) will return a wouldblock error if there is nothing to do. It is up to you to write code that takes that into account. You have an event loop and mio::Poll object that will tell you when a socket is ready to read or write. Conceptually using mio is pretty simple, but requires lots of work to make something useful beyond trivial protocols. Tokio and futures is an abstraction that generalises behavior into a higher level by chaining operations together that will be executed when sockets are ready. Code kind of looks simpler but requires a lot of effort into understanding what is going on and how to use it effectively.
Congrats . One big step toward industrial/production grade code. I can’t see any mention of embedded or no_std. Can you confirm that code written for such targets can be processed?
Currently it only works on 64 bit Intel and AMD. 32 bit Intel might work but afaik it's never been tested so I don't know. Getting coverage from code running on physical targets can be hard though so for lower integrity levels a lot of people simulate the software for unit tests with coverage then have requirements based integration tests on target with no coverage. So it depends if that level is acceptable. Tarpaulin can test no_std code though!
I was just about to write a small little thing to get exchange rates in rust. These links are kinda what I was looking for thanks.
Note that Rust team members receive no compensation *for being a member* - it's an organization separate of Mozilla or any other company (although I don't think there's a legal entity yet - a Rust Foundation would be really nice). OTOH, some of the team members are employed by Mozilla, Google, etc. and a subset of them also use Rust at work and their contributions go towards improving Rust for that usecase. And everyone else may get sponsored to work on various parts of Rust, either by Mozilla itself or, more recently, via http://aturon.github.io/sponsor/
Will be possible to have timeouts in the `select!`? Something like [`after` in Erlang](https://www.safaribooksonline.com/library/view/erlang-programming/9780596803940/ch04s05.html) would be pretty cool. select! { recv(..) =&gt; .., after 5000 =&gt; handle_timeout() }
are you sure cargo test failed and not the other commands you have there afterwards? because I think the cargo test command actually finishes ok, but could be wrong. you could try cargo make to simplify your test flow and have more visibility of issues. [https://github.com/sagiegurari/cargo\-make#usage\-ci\-appveyor](https://github.com/sagiegurari/cargo-make#usage-ci-appveyor)
I also had the same question, and found out the behavior by building a small test case. Any chance to see this documented in [https://docs.rs/crossbeam\-channel/0.2.1/crossbeam\_channel/#sharing\-channels](https://docs.rs/crossbeam-channel/0.2.1/crossbeam_channel/#sharing-channels)?
I don't think `slice.len().checked_sub(2).and_then(|n|slice.get(n))` is any worse :-) It'd be interesting to see which one the optimizer likes best.
Thanks for your work on this great library and the release of the new version! I previously used a pattern similar to this with crossbeam\-channel 1: extern crate crossbeam_channel; use crossbeam_channel::{unbounded, Receiver, Sender}; #[derive(Clone)] enum UserEvent { A, B, C, } #[derive(Clone)] enum Event { AttachSender(Sender&lt;UserEvent&gt;), User(UserEvent), } fn process_events(receiver: Receiver&lt;Event&gt;) { let mut user_event_senders = Vec::new(); use Event::*; for event in receiver { match event { AttachSender(sender) =&gt; user_event_senders.push(sender), User(user_event) =&gt; { // Doesn't work any more with crossbeam-channels 2, // can't detect whether any receivers are connected. user_event_senders .retain(|s| s.send(user_event.clone()).is_ok()); } } } } Whenever a thread was interested in receiving the user events, it would build a channel pair and send a `Event::AttachSender(sender)` message to the events channel. The processing loop would detect whenever a sender had no more receivers and remove it from the user\_event\_senders vector. This no longer works with crossbeam\-channels 2, because there is no possibility to discover whether a channel has any receivers. Is there a way to detect this through other means, or would I have to implement something different, e.g. sending a `Event::DetachSender` message with appropriate information on how to find the sender that should be removed?
This is the (pre-nll) expected behaviour. What are your expectations ?
What were you expecting would happen? Because to me it looks like it's doing the right thing. `borrower` is mutably borrowing `lender` until the end of the scope\*, so you cannot use `lender` while this borrow is in effect. Doing so would break the aliasing rules. Sure, it's not actually holding any data, but the express purpose of `PhantomData` is to hide that fact from the borrow checker. (\* If you compile with `#![feature(nll)]` you can opt in to the new behaviour where the borrow only lasts until the last use, instead of until end of scope.)
That was my first assumption as well, but that wouldn’t explain that it compiles with the second signature, would it?
With the second signature, lifetimes 'a &amp; 'b are disjoints, if you add a `'a: 'b` (or `'b: 'a`, I can't remember with one is a right one ^^), you should have a borrowck error
Why not? There is no *intrinsic* relationship between Lender and Borrower since Borrower does not actually hold any data from Lender. So the second snippet declares that it create a Borrower with a lifetime bound, but one completely unrelated to the lender's. And thus everything compiles, because the second snippet's Borrower does not borrow from Lender, and thus Lender's borrowing only last for the span of the method call.
I agree on your reasoning and that was what I believed first as well, but the phantomdata doesn’t hold a mutable reference to Lender, it only holds a constant reference with the same lifetime. So if it would be because of non-lexical lifetimes, shouldn’t something like impl Lender { ... fn do_mut_stuff(&amp;mut self) { } } fn main() { let mut lender = Lender; lender.do_mut_stuff(); lender.do_stuff(); } also fail compiling? (Which it doesn’t) Also the following: fn main() { let mut lender = Lender; lender.borrow(); lender.do_stuff(); } also compiles.
I agree, but the only mutable borrow is in the method signature, the phantomdata doesn’t declare a mutable borrow.
AFAIK, lifetimes can't handle change in mutability, so as far as `Borrower` is alive, `Lender` is mutablity borrowed
That sounds like a project I actually could contribute to
The borrow checker doesn't see that. What it sees in the first snippet is that the lifetime associated with Borrower is the lifetime associated with a mutable borrow of Lender, so the mutable borrow of Lender has to extend further than the extent of Borrower's life.
Thanks for the explanation, I’m feeling a bit less confused now.
You're being bitten by "ergonomics". Change the signature of `read_text` to: fn read_text(&amp;mut self) -&gt; GenericToken&lt;'s&gt; Good luck! :)
In addition to `std::net`, I would also mention (net2)[https://github.com/rust-lang-nursery/net2-rs] for synchronous networking API
It's already possible - you can use the [`after`](https://docs.rs/crossbeam-channel/0.2.1/crossbeam_channel/fn.after.html) function for timeouts: let timeout = Duration::from_millis(5000); select! { recv(..) =&gt; .., recv(after(timeout)) =&gt; handle_timeout(), }
&gt;The idea is that everything should be able to be transparently send between cores, or better yet separate computers over the internet. Would you be borrowing ideas from Plan9/Inferno OS?
Is lifetime elision the problem? I'm not sure, but I think the original function signature is interpreted as something like this: fn read_text&lt;'a&gt;(&amp;'a mut self) -&gt; GenericToken&lt;'a&gt; By the way, it looks like the error is much more informative if you compile the code on stable without NLL. Something to do with [this](https://github.com/rust-lang/rust/issues/49397) issue.
Non-lexical lifetimes don't change the situation here. Your code here compiles because each method call creates it's own lifetime, and they end when the method calls are over. There are reasons why you can't currently "downgrade" a lifetime from mutable to immutable. If a function takes in a mutable reference with lifetime `'a` and returns immutable something with lifetime `'a`, you can't be exactly sure whether there is still mutable reference _somewhere_. Besides, the returned thing might be a opaque struct that doesn't even reveal if it contains mutable or immutable references. Or the returned immutable something might just be a "guard" whose purpose is to keep the original borrow "locked" as long as it is alive, and some unsafe API depends on that. Anyway, there would have to be language support for downgrading mutability. That's because currently there isn't and lifetimes work as they currently do; so there is unsafe code in the wild and even in the standard library that depends on the fact that no "automatic" downgrading happens.
How does this compare with ggez??
There is no reason. Actix uses crossbeam-channel and futures::sync::mpsc internally.
Interesting, thank you.
I have found the irc channel to be very welcoming
It is possible to simulate the old behavior of channels: https://github.com/crossbeam-rs/crossbeam-channel/blob/master/examples/mpsc.rs But in your case, I would probably send an `Event::DetachSender` message. Maybe even use [`defer!`](https://docs.rs/scopeguard/0.3.3/scopeguard/) to send the message automatically at the end of the sender thread. match event { AttachSender(sender) =&gt; user_event_senders.push(sender), DetachSender(sender) =&gt; user_event_senders.retain(|s| s != &amp;sender), User(user_event) =&gt; { for s in &amp;user_event_senders { s.send(user_event.clone()); } } }
Wow. Literally no other words. Just wow. Nice work. :-)
If you want it to be generic over the runtime, neither mio nor tokio are really gonna work, as they both bring their own runtime - just at different levels of abstraction. If you want it to be a generic library that can be used with any runtime (or lack thereof) I’d recommend one of two things. 1) An api similar in style to that of httparse. Design everything to have explicit state stored in a public struct (not necessarily with public fields) and provide the user with functions to feed more bytes into that struct (once they’ve read them from whatever input source they deem appropriate) and retrieve the output from the struct when enough bytes have been supplied. 2) Use futures. Despite the strong ties between futures and tokio, futures can exist in a Tokio-free environment and users can supply their own runtime. I believe futures 0.2 has AsyncRead and AsyncWrite traits you could use to be generic, though I don’t know if they’re available in 0.1 (which is the currently recommended version of futures afaik, until 0.3 comes out). Bonus of doing it this way is that people may be able to use your api with Async / await in a year or three. 
I'm not fully getting it, admittedly. It says "Command executed with exception: Doc-tests nvimpam_lib" in what I linked, but if I switch to "cargo test --lib" (so the doctests arent' run), there's some other cargo test thing that produces the exception. I've now somehow got something that runs, but I'll keep cargo-make in my mind, thanks! 
I learned a lot about Actor model via Erlang. You can try elixir if you want to learn about that concurrency model. I consider it the true OOP of what Alan Kay was talking about when he coined it and Alan Kay and Joe Armstrong had a talk about it. 
Thanks so much for this great work!
To add to your post: [this page](http://sans-io.readthedocs.io/), although in context of Python, gives some arguments for push-style parsers (option 1).
I think this a bit unfortunate. Was this needed to get it to behave properly in select? Otherwise why make it like this? Sounds strictly less useful to me.
Do you have a link on how to do this?
Thanks for writing/posting this. One thing that confused me for a bit is that that the original JavaScript version accepted two kinds of messages, `'plus-one'` to increment and anything else to retrieve the value. But the Rust version accepts only `PlusOne` messages. Was it not intended that the JavaScript version would work that way? It might be worth pointing out that it has a bug that the Rust version fixes, or something like that. Otherwise it could be distracting.
FWIW, you could also be clued in by the fact that the type of the value being transmitted doesn’t need to implement `Clone`. Documentation wouldn't hurt either, I suppose.
This has been sitting here for a day with no answer, so here's my best shot: Rust arrays already store exactly the data and nothing more, I they are zero overhead. Vectors reserve up to twice as much capacity as they are using to enable efficient growing, but if you manage the capacity yourself (using `with_capacity`, `reserve_exact`, and `shrink_to_fit`) the standard `Vec` is also zero overhead. As for 1-indexing, that’s just not how things typically work in Rust. But any algorithm that expects that is trivially convertible to 0 indexing. If you really don't want to do that, you can wrap `Vec` to subtract 1 from all your indices.
You're looking for /r/playrust
Thanks for the suggestion! My original thought was to show that type annotations are not necessary. I have the changes made locally but haven't pushed them yet.
I would love to use Tarpaulin, but are there any news regarding https://github.com/xd009642/tarpaulin/issues/23?
Awesome, thanks for the link!
This was already posted last week, to lukewarm reception. I personally find such 'suggestions' without even a trace of effort offensive. If you think it would benefit the project, at least set up a PR to port one function and make it work as a proof of concept.
I for one find 'suggestions' such as this offensive. It shows a complete disregard for the time of the maintainer, to just dump this discussion on them without showing any trace of effort, such as porting a small function and make it run within the project as a proof of concept PR.
Note that `send` doesn't know if there are any receivers, but `recv` still knows whether there are any senders. The reason why we switched to this model is simplicity. If sending can fail due to absent receivers, then it's really difficult to design an intuitive interface for select because lots of odd corner cases come up. See [this discussion](https://github.com/crossbeam-rs/crossbeam-channel/issues/39) on the topic. Channels in Go work the same way (only senders can close the channel), and there's a very good reason for this. See [this comment](https://github.com/golang/go/issues/18511#issuecomment-270425548) by Russ Cox for the rationale. Same story in the [`chan`](https://github.com/BurntSushi/chan) crate. In the end, the switch was not an easy decision as there are tradeoffs both ways. Besides the GitHub discussion, I went digging through Go mailing lists and reached out directly to several people who have criticized the old select mechanism, asking for an opinion. I like thinking of the old model as a two-way "connection" model (whoever leaves first closes the channel), while the new one is a one-way "iterator" model (`Receiver&lt;T&gt;` is just an `Iterator`, and `Sender&lt;T&gt;` fills it with data). Fortunately, it's not difficult to simulate the old behavior (the "connection" model) using the new one: https://github.com/crossbeam-rs/crossbeam-channel/blob/master/examples/mpsc.rs As a proof of concept and additional test harness, I've even wrapped `crossbeam-channel` into an interface that looks just like `std::sync::mpsc`, literally copy-pasted its whole test suite from `rust-lang/rust`, and then ran the wrapper through the tests: https://github.com/crossbeam-rs/crossbeam-channel/blob/master/tests/mpsc.rs
I'm not sure how to read the results but isn't the first number the average? In which case the unsafe version was the best average and the smallest variation. 
Yeah, I have that book and I learned quite a bit from its coverage of the sync primitives. I am familiar with some of the concurrency tools from other languages (Haskell specifically) -- the actor model and how/when to use it is new to me, especially as it applies to Rust programs.
A little over an hour is the plan. Don't stress yourself out if you can't make it -- there will be more if we can at all help it!
I finally published my first crate/crates [mori](https://crates.io/crates/mori) and [mori_parallel](https://crates.io/crates/mori_parallel). I had wanted to make a few minor changes to them before publishing them, but I just haven't had the time. So, I finally just decided to publish them, since the code does what I want it to do anyways for version 0.1. Also, I'm still getting the hang of things with crates.io and unfortunately have already had to make some minor 0.0.1 bumps.