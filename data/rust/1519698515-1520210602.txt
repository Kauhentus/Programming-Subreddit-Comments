The immediate answer is: because that's how it's defined. `Vec&lt;T&gt;` gets [`contains` from `[T]`](https://doc.rust-lang.org/std/primitive.slice.html#method.contains), where it's defined as: fn contains(&amp;self, x: &amp;T) -&gt; bool where T: PartialEq&lt;T&gt; Which means that for `Vec&lt;&amp;str&gt;`, `T` is `&amp;str`, which means `contains`' signature becomes: fn contains(&amp;self, x: &amp;&amp;str) -&gt; bool `contains` requires a pointer because it wants to use `PartialEq::eq` to compare things, and that's defined as: fn eq(&amp;self, other: &amp;T) -&gt; bool `eq` takes a pointer because, if it didn't, *you wouldn't be able to compare complex types without consuming them.* That would be bad. Rust doesn't care that `&amp;str` (or `i32` or anything else) is trivially copyable and you don't need the pointer in this specific case. That's how the signature is defined, no exceptions, end of discussion. For your second question: it depends. *Can* you return a simple `&amp;str`? Do that. Does your function always allocate a new `String`? Just return that. If you could return either depending on the control flow... does it matter? If allocation is common and it's not a memory or performance issue, just return `String` and keep it simple. Is allocation uncommon but sometimes needed, or you need to squeeze every last bit of performance out? Maybe use `Cow&lt;str&gt;`. In Rust, deciding types is mostly just down to understanding what you require, and picking the type that best fits those requirements. If you don't understand your requirements, that's not something a quick answer on Reddit is going to solve. With regards to the last question: the `unwrap_or` is a bit iffy since you've already established that the sequence is non-empty; I'd use a regular `unwrap` for that. Secondly, you shouldn't need that lifetime in `collect`. Really, you don't need anything more than `Vec&lt;_&gt;`. Aside from that, that's pretty much how I'd do it. 
There's always the raspberry pi0W
I think that's a little too big for the project I want(needs to fit inside a device enclosure). One project I've been wanting to work on with Rust was upgrade my electric shaver to have wireless charging and bluetooth/wireless communication to a phone app I make(display battery life).
I'm pretty sure libstd isn't going to do anything that would actually sacrifice parametric reasoning, though. Just optimizations.
thanks for the reference, Unfortunately I needed to make fork of tiny-keccak, because CryptoNote needs some internal state of it. Maybe its time to stop beeing lazy and making a proper pull request with my change :)
I'd say only separate them out once you get enough docs and unit test that it becomes unwieldly. Like, if you have a 400+ line file, you should probably split that into two sub-groups. 200+ lines isn't unreasonable for a single large struct + documentation + unit tests, but if you don't have the documentation and tests which pad that space, you shouldn't need to split. Before that sort of length, I would be very wary of having multiple files. You might want to if you have some struct that's unsafe and requires privacy from the other structures, but even then you can do an inline `mod` without a separate file.
I'm told the rtl8710 works alright with rust, and seems to be reasonably comparable to the ESP32 spec wise.
japaric updated their book earlier today, which includes working with [bluetooth](https://japaric.github.io/discovery/12-bluetooth-setup/README.html) ^^
Bad news https://github.com/rust-lang/rust/pull/48333
This is magic
You can do quite a lot with a vec/vecdeque with an enum type. This way you can place multiple data types in it and just push to back. 
Wow flamer looks really cool!
Being consistent seems unimportant. Why would that be desirable? I guess things would be slightly more familiar, but no one should need familiarity to understand or google 'you indexed out of bounds'. Just make sure your panic makes clear what happened, no need to worry about such levels of consistency.
Oh that's pretty neat! I didn't know that hardware had an accompanying bluetooth module :)
I've read that it's more comparable to ESP8266, but thanks for mentioning that, looks pretty nice!
I'm trying to match on this enum: http://lessis.me/openapi/openapi/enum.ParameterOrRef.html match param { openapi::v2::ParameterOrRef::Parameter(parameter) =&gt; parameter, openapi::v2::ParameterOrRef::Ref(ref_path) =&gt; magic_here, } But I get an error "did you mean `openapi::v2::ParameterOrRef::Ref { /* fields */ }`?" I really do not want to provide an struct and match every single param because I want to get the full param back from here.
I've just ordered the pine64 PADI IoT Stamp (based on Realtek RTL8710AF). There is a guide here using it https://polyfractal.com/post/rustl8710/
Thanks for the kind words, but flamer is pretty simple. All the cleverness lies in /u/tyoverby's [flame](https://github.com/tyoverby/flame) crate.
This could be because of better cache locality - if you use individual allocations in the C version, but contiguous memory in the Rust version, the Rust version will be faster because the CPU can predict what memory to pre-load in the L1 / L2 / L3 cache. If you use individual allocations, you get a lot of cache misses. So it might not be because of Rust but because you used a technique that yields better cache access.
&gt; which you might recognize as strcpy, is unlikely to be vectorized. Why? Because vectorization would require that source and dest do not overlap and this cannot be proven here. Sure, but how is that an example of a dumb optimizer? Vectorizing would be unsound!
Wow fantastic. Will be sure to use this for many future illustrations in my book
OPC UA 0.3 went out end of last week. This week I'm building an http server support for OPC UA servers that want to dump out metrics / diagnostic info. Using hyper + futures for it.
OpenSSL has been verified to be a creeping horror. That's why there is LibreSSL. Hopefully one day that or some other implementation will finally kill that nightmare. Keywords: heartbleed valhalla rampage
Yeah the problem with these is you spend all your time working out the magic combination of characters that will give you the corner or arrow that you want. I would use a proper tool for it instead. All the diagrams shown could be done much easier with IPE, Graphviz, PlantUML/Mermaid, etc. Unless you absolutely *have* to have a readable ASCII version. But I can't really think of any situations where that is true - you can always refer to an external HTML/PDF document.
I didn't do any fancy benchmarking. Each program built up a 10m element heap of ints in contiguous memory, then recorded the time before and after delete-min'ing it, and printed the result, all in its own language. I'd be curious to see someone with more experience than myself try this out in a more legitimate comparison.
Because 'source' and 'dest' are almost always not overlapping. The optimizer should find when they're guaranteed to not overlap, then it CAN optimize. Safe Rust ensures they are non-overlapping, which means the Rust compiler can add annotations for the optimizer. 
This sounds relevant and interesting, but I just don't have 4 hours to sit and watch something...
I believe you might be able to get a partial solution using one of the [arena](https://crates.io/search?q=arena) style crates or writing a form of a stack allocator based on those. Not that I've ever used it but [maskerad_stack_allocator](https://github.com/Maskerad-rs/maskerad_stack_allocator), seems close at first glance. It would require doing all allocations through a different API. Which would likely to be hard to make work for any std collection beyond a Vec/String. I suspect there maybe some other issues. Along that train of thought here's some other examples. The [slab](https://crates.io/crates/slab) crate is used in Mio. There's a recently posted [blog post](https://exyr.org/2018/rust-arenas-vs-dropck/) on how to write an arena to allow for borrow cycles. While not as good as what you're looking for, since you're willing to work around a special API, the default Rust allocator (jemalloc) is an extremely good general purpose allocator for long-running multithreaded programs. It's fairly good at avoid fragmentation, outside of edge cases probably peaks at 20% wasted memory. It elides a lot of global locks, thread caches and a few lockless data structures. It's VASTLY better than the standard Linux allocator for multithreaded long-running processes, I've measured a 100x improvement on production code on many cored system which did over a million allocations per second. Note The Linux allocator was better for single threaded short lived programs in my limited testing, but was at best 2x-3x. I haven't looked to see if there's a crate that can hook into the [full jemalloc controls](http://jemalloc.net/jemalloc.3.html), but if so I suspect you might be able to get something similar to what you want with explicit arenas and tcaches.
I don't think there is an alternative to matching every variable in parameter - you'll need to define a Parameter struct to then put them into. If this is something you need to do a lot though, you can wrap it up into a function thay returns each branch in a more convenient manner. You'll need to use `{` and `}` for `Ref` as well, which is what the actual error is about I believe. struct Parameter { /* fields */ } enum ParameterOrRef { Parameter(Parameter), Ref(String) } fn parameterOrRef(in openapi::v2::ParameterOrRef) -&gt; ParameterOrRef { match in { openapi::v2::Parameter{ /* fields */} =&gt; ParameterOrRef::Parameter(Parameter{ /* fields */ }), openapi::v2::Ref{ ref_path } =&gt; ParameterOrRef::Ref(ref_path) } } that is something like how you might write a function to convert the enum into something easier to work with (totally untested as I'm on mobile) :) 
that's actually pretty cool
In the good ol' days (before 1.0) default visibility used to be public...
Also for really really sketchy window design
Very ... uhh ... nice. But did you try it in Firefox? I has a huge memory leak for me. Pegs the processor for a moment and then eats &gt;4GB until I OOM-Kill it...
As a compiler writer you can't just go around introducing optimizations that are correct "almost always". By that reasoning optimizing `some_byte / 253` to `0` would be fine, because it works "almost always". Vectorizing the given function (assuming it's in c or c++) would be wrong.
Thanks, makes sense, and was sort of the conclusion I had come to but I was having trouble finding the source code(s). I guess the best thing to do is just try len(), and if it doesn't compile back down to count().
I just put it into my termbook roadmap (https://github.com/Byron/termbook#add-replace-support)! This would allow use to integrate asciiart into markdown codeblocks in `mdbooks`, and translate them to actual SVGs when building the book.
&gt; Is it possible to get MIR code for all code related to a crate (including the dependencies and standard library) Yes I am doing that in [rlsl](https://github.com/MaikKlein/rlsl). You need to compile with `always-encode-mir`. Although I have written a custom collector, I don't think the default collector collects over everything. &gt; How stable MIR itself is: could it change completely the 6 (or so) months I work on it? I would say it is pretty stable, I don't spend much time on keeping up to date with the current master. (At least so far) `Mir` is actually pretty straight forward. 
But then when would you ever run code comments through this tool? Do any IDEs support it? I assume not though it would be pretty cool. Though to be honest it would be way cooler if they supported showing doxygen-style @image imports inline. Probably could be done in VSCode easily.
The docs have `[src]` links on a lot of things that will link you straight to the source code.
It would be nice to have a "wysiwyg" editor able to generate the ASCII version.
It'd be cool if there were a way to perform arbitrary transformations on `Secret` values via callbacks. Ideally you'd want some type-level notion of function purity, but I think that `const fn` would serve as a conservative (and somewhat less powerful) version of that? In other words, I don't believe that `const fn`s can be effectful? They can be impure (panic), but technically your wrapper code could just catch those panics and throw them away so they couldn't be used to smuggle data out of the `Secret`.
PlantUML can generate ASCII also
&gt; Getting credentials to git for private repos I can speak to this bit, (though it's not really rust-specific): I'd recommend the Jenkins [Credentials] Plugin for storing credentials, and then the [SSHAgent] plugin to pull them out and make them available to SSH (and git). [Credentials]: https://wiki.jenkins.io/display/JENKINS/Credentials+Plugin [SSHAgent]: https://wiki.jenkins.io/display/JENKINS/SSH+Agent+Plugin &gt; Deploying documentation in a usable form What problems are you having here? `cargo doc` should generate docs for your library. Then you can either save them as a Jenkins artifact (which makes them available from the build.) or push them up to some place to host them. 
I had my first Rust PR accepted :-) Implemented a feature in Gutenberg which allows you to ignore files in the content directory that match a glob pattern(s). Used the https://crates.io/crates/globset crate. Even though it was very simple, I still learned a few things just looking over the Gutenberg source, such as breaking projects out into sub-projects.
You can still host everything on Github, just sign commits/binaries/whatever and verify signatures in the installer. Then you can even download through HTTP and still be sure you get the data you should get.
There's asciiflow.com
No, but you can introduce a precondition. If 'some_byte' is always less than 250, then optimize away. But either you need sufficient context (for a function in C/C++, by inlining) or you need annotations, or both. But we both know enough of how an optimizer works and why inlining gives major performance gains. The optimizer is dumb because it is possible to recognize when it is sound and only then do the optimization and improve the program. And it's really dumb because even if we try to create a pattern, it is always too easy to fail. 
As I just found out I had a few ancient files with names in KOI8-R, and fselect panicked on each when I tried fselect path from ~ where name = '*.txt' | wc -l So I've recoded them all into UTF-8 at last. Cool thing, thanks. :) Now I've repeated the command about 10 minutes ago and still waiting. By the way: $ time find ~ -type f -name "*.txt" | wc -l 1.06user 0.97system 0:02.77elapsed 73%CPU (0avgtext+0avgdata 17244maxresident)k 160inputs+0outputs (2major+14111minor)pagefaults 0swaps 9396 The idea is nice anyway, happy hacking. :)
Could you elaborate on why you would call this project "shit"?
You could add this info (and an example progressbar) into a README.md file in your repository. When people search for libraries, they look for reasons to choose a certain library over others. Having reasons stated in README helps make that decision. :)
Thanks! That's crystal-clear
There is a lot of work going into making rust-in-js as seamless as possible. The pain points are fading fast. Parcel.js, for example, let's you `import` rust modules just like you would import other javascript files.
Good point. Fixed setup cost vs an ongoing cost for every new line of code.
I think it needs to come from more language-level support to teach the compiler how to remove abstractions. My idea of how it would go is that programmers introduce new abstractions _along with_ the way for the compiler to remove them automatically. This has been the subject of [one of my papers on library-defined optimizations](https://dl.acm.org/citation.cfm?doid=3136040.3136043) (the PDF can easily be found online).
&gt; The only real problem I have: absolutely no clue how to estimate the effort for that thing. Study the problem, do some experiments coding small bits of it, see where the hard and easy parts are and come up with an estimate of how long it will take to do. Then take your estimate and multiply it by four. Or six, if other people are also involved.
Following your embedded adventures in Rust has been highly enlightening and enjoyable for me. I've purchased various ARM development boards and devices and have had a hoot wiring them up and hackin' away. Doing this in Rust hits a real sweet spot for me. 
Please note the URL for those docs; that's for Rust 1.14.
I saw a demo of this recently, it's super super cool!
This has rus in the name. Does that prove Russian collusion? /s
Yes, it is.
I was using powershell for complex stuff but now I can do the same in bash! Thanks 
What is the point of this feature? There's already several ways to dispatch differently on types.
or [Diiagrams](http://diiagrams.com/)
For the ASCII / Unicode version you can use [Diiagrams](http://diiagrams.com/)
Looked at the generated assembly for these two functions ([playground](https://play.rust-lang.org/?gist=a1b65a6debfe5d09a6475f00323b3062&amp;version=stable)), how come that `assert` is optimized out in `check2` but not in `check1`? pub fn check1(x: u8) -&gt; u8 { match x { p @ 0x10 ... 0x1F =&gt; { let k = p - 0x10; assert!(k &lt;= 0x0F); k } x =&gt; x, } } pub fn check2(x: u8) -&gt; u8 { match x { p @ 0x11 ... 0x1F =&gt; { let k = p - 0x11; assert!(k &lt;= 0x0E); k } x =&gt; x, } }
&gt; But I would say that introducing Rust and wasm into a previously pure-JavaScript codebase introduces more convolutions for the sake of performance, and maintainabilility suffers, too. And by a larger margin; any decent JavaScript programmer knows what new Function does, but the same is not true of Rust, wasm, etc., not to mention the immense headache of setting up a new toolchain. Many frontend projects these days already have a complex toolchain setup to build them. Babel, TypeScript, Flow, CoffeeScript, JSX, and so on are all compile into JavaScript, along with linters, minifiers, bundlers, LESS/SASS, etc. Probably the main hurdle to make this suitable for the average frontend developer would be to make it possible to `npm install` or `yarn install` to get the whole rustup/cargo/cargo-web/stdweb toolchain installed without having to learn a whole new tooling and package management ecosystem. Learning Rust is another hurdle, but I'd argue that learning enough Rust to write this kind of reasonably efficient code is a lot simpler than using typed arrays to do manual memory management, learn how to monomorphize via `new Function`, learn about how to profile to find hotspots like the argument trampoline, etc. There are obviously tradeoffs for either approach; if you just have a very small hotspot in a large JavaScript codebase, learning a bit about optimizing that one hotspot is probably easier than rewriting in Rust. But if something like allocating lots of small objects is the hotspot, and those objects need to be threaded through everything in your codebase, I'd argue that just rewriting in Rust is likely to be less cognitive overhead and more maintainable than switching to typed arrays. Of course, those aren't the only options. It's possible that rewriting in TypeScript would allow the TypeScript compiler to do monomorphization and avoid slow things like args trampolines, and be more accessible to JavaScript developers (though I don't know if it actually does this). However, since TypeScript doesn't fundamentally change the memory model, I don't think it would help with avoiding allocations.
All of the options I listed are textual (IPE can be saved as XML but you probably wouldn't want to edit it by hand). Not sure what you mean about statically linking into Rust code. Do you mean include it inline?
It has luser in its name...
The Rust embedded story seems to get more encouraging by the day with all the news and blog posts.
&gt; What is the "magic" that converts a &amp;Vec&lt;u8&gt; or a &amp;[u8; 3] into a &amp;[u8]? Pointer coercion. `Borrow` isn't anything magical. The first is a deref coercion, enabled by the `Deref` and `DerefMut` traits. The second is unsizing. If I understand correctly, the traits are still unstable but that's meant to be extensible as well. So the trait you would need to make that code work almost as written would be: trait DataAsBytes { fn data(&amp;self) -&gt; impl '_ + Deref&lt;target=[u8]&gt; } But since `Borrow` is intended for generic programming like this, I think it's the trait you want. Like I said, it's nothing magical beyond being generic. You just call `let x: &amp;[u8] = foo.borrow();` 
&gt; Not sure what you mean about statically linking into Rust code. Do you mean include it inline? The library for converting the textual representation to graphical should be able to be statically linked into my Rust code.
Actually /u/jackpot51 did response to my post. 
Hi comrad! This is definitely not russian. Though my username starts with ivan (which is common russian name, ie: ivan ivanovsky). I also wrote an orm [rustorm](https://github.com/ivanceras/rustorm), which has `rus` in it, not because of `russian storm` or anything, but it's because it just an orm written in rust. Svgbobrus is supposed to be just svgbob (svgbob-rs for the repo), supposed to be a play on bob ross, svgbobross, svgbob-rs, but ended up with svgbobrus. Use my libraries, I guarantee you, your apps are safe. Data is not going to mother country. :) /s Thanks OP for posting my project here. I wonder why there were many stars on my github dashboard.
Hey, I’m the author of this PR. Was a nice surprise to see it turn up here! I should note that 25% is probably the best-case improvement, the changes increase code size and so my computer that has a huge CPU cache probably benefits more than another might. The reviewer only saw a 20% improvement. Also, I benched with the nightly compiler and so the stable compiler might produce different code.
What matters IMHO is that crypto-related libraries are being updated. When developing REST API, this is really helpful to have such crate
&gt;but each request will use a custom allocator to prevent fragmentation, diffusion, and global locks. Rust's default allocator jemalloc avoids many of this pitfalls due to its per-threadpool pooling design. OFC it isn't prefect but its considered fairly far ahead of the default glibc allocator. --- The typical pattern is to have a collection of objects normally `Option&lt;T&gt;` laid out in a flat slab (via `Vec&lt;Option&lt;T&gt;&gt;`) which will be a continuous chunk of *virtual* memory, then you pass around references which are actually indexes to those objects. That being said NUMA-malloc and such functionality is still exposed via the `libc` library, but you have to manage those pointers yourself, and it doesn't mesh well with the standard library. 
&gt; The exact phrasing might not be critical This is all I'm saying. I think given a trivial error message like 'index out of bounds' you would be hard pressed to come up with something that diverges so much from std so as to actually confuse someone.
What are you doing to parse test results? You using something like [cargo-test-xunit](https://github.com/evernym/cargo-test-xunit)?
I seem to recall that GHC has some such "optimizations" where a library comes with a way to lower certain patterns. It's still pattern-based though. And doesn't address *combination*.
Indeed! The problem is that *information is lost*. It's a recurring theme in compilers: - the user cannot convey information to the compiler front-end, - the front-end cannot convey information to the optimizer, - information gets destroyed by a previous pass in the pipeline, - ... If you look at LLVM analysis passes, a number of passes are about reconstructing information which the front-end knew but was did not pass on!
Yay!! Is there any chance than Rust might be able to work together with ready existing embedded technologies like FreeRTOS or MCU headers? Or do you guys think that's the manufacturers responsibility? I'm thinking specifically about ARM MCUs like those from ST and NXP
I'm super happy with where this is going. /u/japaric does some amazing work, and it's great to see a more formal group put together within the rust org dedicated to working in this space. The embedded rust world is undergoing fairly rapid changes right now and I hope this group will bring more attention and manpower to the efforts. I certainly cannot wait to see what is coming, and perhaps add a few contributions of my own. 
The code is on GitHub (https://github.com/jonhoo/tsunami) and crates.io, so you can still use this and read the source if you feel like that'd be better. The intention behind making this a video stream was so that less experienced Rust developers can observe how a Rust crate is structured, how you resolve issue that come up, and in general how to write idiomatic Rust code. It's hard to show those things in another format. I could publish a blog post about how the crate *works*, but that wouldn't help others who want to learn about *writing* Rust code. Suggestions welcome though!
The documentation is in the standard rust documentation. If there's a way to avoid duplication there, I'm all ears.
There's some discussions about making bindings to C RTOSes in [rust-lang-nursery/embedded-wg#45](https://github.com/rust-lang-nursery/embedded-wg/issues/45). I think the idea of having some traits / standardized RTOS interface makes sense. And I think we should at least have a reference implementation of binding a C RTOS done this year. Regarding MCU headers we don't have a specific issue about that in the repo but I have heard some people are looking into implementing the embedded-hal traits for a vendor SDK (Silicon labs emlib, iirc); then they'd be able to use Rusty embedded-hal drivers with the vendor SDK.
great! Thanks for the answer :)
Correct. It's indeed limited to simple pattern-based algebraic rewritings, though. A problem is that it has no notion of optimization phases and interacts in unpredictable ways with inlining (which is crucial in exposing patterns to be rewritten). We address this in our own work for Scala.
An article by the creators of ASAN about using full or partial hardware support to increase the efficiency of ASAN instrumentation in the hope that its overhead becomes low enough as to be acceptable for production usage. Despite the requirements: hardware support, OS support &amp; compiler support, the technique illustrated remains probabilistic and suffers from a number of weaknesses (see page 12).
Can someone tl;dr the overhead? I'm confused because they keep saying it's near-zero and then I see figures like 5%, 7%, etc.
It looks nice, but it's not a solution. x86-64 doesn't support memory tagging, AFAIK, and even if it did, this only discovers issues once they're already in production. To make matters even more interesting, one of the ramifications of shipping something like this is that it would potentially make it easier for bad actors to know about zero-day vulnerabilities as they are discovered. Imagine that a bad actor is highly placed in a large organization with thousands of employees using organization-owned computers. The bad actor could install malware on these company computers that intercepts ASAN reports, literally collecting memory unsafety issues into their own catalog, which they could resell either to Google or to the black market with a small team of people exploring the reports in the catalog and how to exploit them. The real solution is to use languages with strong memory safety guarantees. Right now, that basically means choosing between a garbage collected language or Rust. I don't even think Rust goes far enough right now. There are a number of mistakes in `unsafe` code that we should really be trying to at least warn about. But, Rust is a pretty sane choice on the whole, in my very biased opinion.
&gt; One more benchmark we used was the clang compiler: ADI slows it down by ~4% in imprecise mode and by 26% in precise. &gt;Conclusion: a fully hardware assisted memory tagging (in heap-only mode) could have near-zero overhead except for very malloc-intensive applications, where the overhead will still remain moderate. Another important observation is that zeroing out the memory may have no additional cost if we are already tagging that memory. I guess I'm just confused about 4% being 'near-zero' overhead, let alone 26% - in my mind, near-zero means &lt;= 1%. I only had time to give it a quick read, so take this with a grain of salt, I may have missed something. Not to mention the additional RAM overhead, which appears to be very non-zero as well. This seems reasonable for many applications, but I doubt you'll convince devs (or compilers) to enable this *by default*, which means it'll basically be something a few companies ship with for a few products and that's it. Seems better suited for their canary testing that they discussed, since they could potentially ship more hwasan canaries than they currently ship to. Especially given the caveats - in particular that the ability to leak arbitrary pointers can allow for a bypass. Also seems great for companies like Google who want to run their fuzz tests/ test suite with sanitizers on, but have to disable them because perf is so bad. Anyway, very cool. I think I'd be very likely to use this when fuzzing my code, and I'd consider shipping with it in time, I'd want to think about it's effectiveness against a real-world attacker given the caveats (especially around leaking pointers diminishing the protection).
&gt; The real solution is to use languages with strong memory safety guarantees. Right now, that basically means choosing between a garbage collected language or Rust. This isn't a real option for most people. You can't rewrite Chrome in Rust in the same time frame as shipping a new mitigation, or improving their fuzzing stature. As much as Rust is 'the right way to go' for future projects, it has little bearing on existing codebases. One particular benefit: &gt; Hardware memory tagging (e.g. SPARC ADI), allows testing legacy code without recompiling it Can't get that with rust. That said, I see this as more of a multiplier for finding bugs through canaries, testing, and fuzzing, and less of a meaningful end-user mitigation.
To me, this goes into the same *defensive coding required* approach taken when programming C++. You pick a language which can go down to the metal for maximal performance, and then explicitly code defensively (extra copies, extra checks, hardened binaries) to try and make up for all the sharp edges. *And despite all those costs, it still collapses!*
The overhead and requirements are enough to make this a never-by-default mitigation. We will likely never see code shipped with this outside of a select few companies. And, in the case of those companies, I expect this to be for their canaries only. The real benefit of something like this is that running ASAN on a test suite the size of Google's, or fuzzing at their scale, suffers a lot from overhead. So if you can improve the sanitizer overhead, you can surface bugs more quickly. Plus, you can deploy this across more canaries than normal because you won't be burdening as many users.
&gt; That said, I see this as more of a multiplier for finding bugs through canaries, testing, and fuzzing, and less of a meaningful end-user mitigation. I remember reading an A/B technique from Twitter, where they would pick up a portion of production traffic and replay against 2 current servers and 1 new server and compare the answers: - find out all the common parts between the answers from the 2 current servers (eliminates timestamps and such volatile data), - check that the answer from the new server has all those common parts, flag discrepancies. If the overhead of such a binary remains slightly too high for regular usage (say 10% overhead on your 500 servers fleet doesn't look good...), it seems you could still try and feed it a portion of the traffic *live* and see if it picks up anything. That said, it still means the bug is in production, and non-hardened hosts may be trampling all over their memory :x
&gt; You can't rewrite Chrome in Rust in the same time frame as shipping a new mitigation, or improving their fuzzing posture. Right, but fuzzing is still not a *solution*. It's even cheaper and faster to do nothing at all, and less effective than fuzzing. You *only* talk about Rust in your reply, but I'm suggesting that any memory safe language would be a better choice than C++.
&gt; Right, but fuzzing is still not a solution. It's even cheaper and faster to do nothing at all, and less effective than fuzzing. But fuzzing makes users *safer* and it does it *tomorrow* and not *five years from now*. &gt; You only talk about Rust in your reply, but I'm suggesting that any memory safe language would be a better choice than C++. Pick any language, my post applies to them all. Rewriting a sizeable, potentially legacy project, is a nonstarter and even if it were something a project *could* pursue it would be years before people were safer because of it.
&gt; That said, it still means the bug is in production, and non-hardened hosts may be trampling all over their memory :x That's why canary releases are &gt;6 weeks ahead of stable releases, though.
Oh! Never used such canary releases, the only canaries I've used were deployed alongside the regular versions (they were used for logging, as logging was disabled on the regular hosts).
&gt; even if it were something a project could pursue it would be years before people were safer because of it. Doing it incrementally (a la Firefox) brings those benefits a lot sooner, and it allows you to verify that you're not breaking compatibility. Google has the resources to start on an incremental rewrite, *if* they valued memory safety highly enough. I'm just pointing out that they value things other than memory safety more. This is a half-measure, and it will never solve the problem, but a half-measure is still half a measure more than doing nothing at all.
Oh, yeah. Sorry I should have been clearer - I assume that the canary they refer to in the paper is in fact the Chrome Canary channel, which is effectively a nightly-release channel with special flags for A/B testing.
great article, degoes has a blog post that talks about this as well. http://degoes.net/articles/insufficiently-polymorphic He discusses the relationship between possible implementations of a function and how polymorphic it's arguments are.
&gt; Doing it incrementally (a la Firefox) brings those benefits a lot sooner, and it allows you to verify that you're not breaking compatibility. Right, so this falls into the case where it is not a legacy codebase and you *do* have the freedom to rewrite components of it in a memory safe language. How long did that take firefox? Rust 1.0 hit stability and I think it was about a year before we got a tiny portion of rust code shipping in Firefox. Yes, this is a win, and a security win, but can you definitively say that it was more effective than the fuzzing efforts, bug bounties, and A/B testing that were utilized in that time? It was a great move but it will be *years* before we've really meaningfully reduced Firefox's attack surface with rust. In the meantime users need to be made safer. &gt; Google has the resources to start on an incremental rewrite, if they valued memory safety highly enough. Google has the resources to do a lot of things at once, including improving their fuzzing posture. &gt; I'm just pointing out that they value things other than memory safety more. This is a half-measure, and it will never solve the problem, but a half-measure is still half a measure more than doing nothing at all. Rewriting a browser 50% in Rust would be a half measure. Rewriting .01% of the browser is then, what? Again, users need to be safer *tomorrow*. So when we evaluate how we make them safer *with that constraint*, Rust is not the obvious solution. It's a longer term investment, and a good one to make, and I have no doubt that there are people involved in Chrome (and Google at large) considering it.
One feature I want is mermaid -&gt; ascii --&gt; svg
&gt; can you definitively say that it was more effective than the fuzzing efforts, bug bounties, and A/B testing that were utilized in that time? I will just point you to your next statement: &gt; Google has the resources to do a lot of things at once, including improving their fuzzing posture. which I completely agree with, and this is part of my point. Google could still be doing this fuzzing stuff while working on a broader solution to their problem. &gt; Again, users need to be safer tomorrow. Why not say that users need to be safer today? or yesterday? Setting such dramatic deadlines will always result in a poor fix. As I have stated several times, and I will continue to repeat: fuzzing is better than nothing. I already agree with you there. I'm saying that I wish Google would take things further.
So I have a `RefCell&lt;HashMap&lt;Key, Value&gt;&gt;` (the actual keys and values don't matter). I need to get a list of all the values. If I didn't have the hash map, the return type would be `Vec&lt;&amp;mut Value&gt;`. How do I get a list of mutable references to the values now that it's behind the `RefCell`? I tried making it return a `Vec&lt;RefMut&lt;Value&gt;&gt;` but I can't get it to compile.
While I agree that users need to be safer today, given that the paper advises for *hardware support* (new instructions, new mechanics), Firefox may be entirely written in Rust by the time x64 has the necessary instructions...
ASAN is already fast enough that my team already runs a comfortable number of canary-asan tasks. TSAN is the real problem—it's so terribly slow that not only would the hardware cost be prohibitive, we wouldn't meet our latency/reliability budget either. We barely run any canary-tsan tasks at all. We'd be able to catch more thread safety problems if it faster.
Hardware support isn't a problem if you think of this as a way to improve fuzzing / testing posture.
&gt; It will be years before Firefox, which is actively pursuing rust integration, will have a noticeable attack surface reduction due to rust. I would speculate that in many projects, a large portion of the attack surface is in a small -- but performance critical -- chunk of the code, where many shortcuts were taken. I don't think the attack surface is linear to the total size of the code base, but it is sublinear, since a larger codebase will naturally have a larger attack surface than a smaller codebase, it just might not be linearly larger. The MP4 metadata parser for Firefox is an example of something small that had a large history of vulnerabilities, and rewriting that in Rust was a disproportionately large victory compared to rewriting some peripheral piece of logic that's never had a vulnerability.
Oh, sorry, I'm speaking about server-side stuff, where it's acceptable to me for 1/100 requests to be routed to a canary task that's 5X slower. I don't think that'd be acceptable for desktop software handling 100% of a particular user's workload. That user would be sad.
Well, users of Canary's opt into that sort of A/B testing explicitly.
Haha, yeah, I guess I'm sort of choosing to look at it from the point of fuzzing because that's where I personally see the value :) but you're totally right.
Great work, /u/japaric! Is there a place for me, a person who spent lot of time working on embedded in the past, but is now mainly occupied by higher-level software? I'd love to play with embedded more one day and with Rust, but I'm not sure when and what. (Strictly speaking, I have one interesting embedded project right now, but it's using Raspberry Pi because of great cost/benefit ratio, so I still use `std` etc.)
Yes but if it's like C++ it can be confusing to predict which version will be called depending on coercion rules. 
I'm rolling my own embedded world right now. Maybe I have a few things to contribute (though probably not much). Up until this point I got a very basic threading/scheduling model together that works well enough for small devices, including some synchronization mechanisms and an allocator. Mostly though I hope my work won't be obsoleted before I can enjoy it myself ;)
I think for fuzzing Asan/MemSan/UBSan are in a pretty good spot already, and hopefully TySan (Type Sanitizer) will be too. The one really lagging behind is TSan (Thread Sanitizer). Then again data-race prevention is a hard problem and many a GC'ed language suffers them as well; there it doesn't lead to memory safety issues, but still lead to data corruption/leak which isn't much better.
I haven't really kept up on sanitizers (since I haven't written C++ since rust hit 1.0), that's interesting, thanks. I'm really curious to check out fuzzing perf with/ without asan now.
 let hash: RefCell&lt;HashMap&lt;i32, i32&gt;&gt; = RefCell::new(HashMap::new()); hash.borrow_mut().values_mut() ? You could collect that into a vec, I guess.
I don't think this is possible. You cannot construct multiple `RefMut`s that belong to a single `RefCell` (because you can't clone them, and you can only make one by borrowing). So you can't return anything like `Vec&lt;RefMut&lt;_&gt;&gt;`. The only function that can be useful is `RefMut::map` - you have `RefMut&lt;HashMap&lt;Key, Value&gt;&gt;`, so you can transform it to `RefMut&lt;U&gt;` if you have `fn(&amp;mut HashMap&lt;Key, Value&gt;&gt;) -&gt; &amp;mut U`. You can't make this function return a `&amp;mut Vec&lt;_&gt;` because you don't have anything that could own that vector. Basically you could only have it return `&amp;mut Value`, because you can get that with `|hashmap| hashmap.get_mut(...).unwrap()`.
Doesn't look like they're looking for the game, judging from their link.
This is awesome!
For those, who are not familiar with Lightning Network (LN for short), it's a second layer on top of Bitcoin (or possibly other cryptocurrencies - even combined!) that should provide these benefits: * much lower transaction fees * faster payments * less load on network * higher security for fast transactions *better privacy Also, when multiple currencies are combined, "atomic cross-chain swaps" are possible, which allow people to trade currencies without need to trust exchange or each other - the exchange either will happen or won't. Other implementations are in C (C-lightning/lightningd), Go (lnd) and Java (Eclair). Peter Todd, one of the Core developers made [an interesting comment](https://twitter.com/peterktodd/status/968335665774637059) about this.
Is there an equivalent for `slice::copy_from_slice` for `str`? Like the following but safe fn copy_str(dst: &amp;mut str, src: &amp;str) { unsafe { dst.as_bytes_mut().copy_from_slice(src.as_bytes()) } }
True, though it's easy to imagine an IDE that supported `@image` in any comment. You could probably make a VSCode extension that does it fairly easily.
Thanks. I'll definitely take a look at this, probably today.
SSHAgent looks perfect. &gt; What problems are you having here? cargo doc should generate docs for your library. Then you can either save them as a Jenkins artifact (which makes them available from the build.) or push them up to some place to host them. I was thinking of something a little more user-friendly - you have to use quite a long URL to access the generated docs. Or something that would provide another route to access the docs besides the artifact, so they would be sort of a "side effect" of the build rather than the result. Ultimately what I did was used 'sed' to mine the Cargo.toml for just the proprietary dependency prefix and the crate name and use that along with "--no-deps" to only generate things for the relevant packages. Still not sure what to do about a title page and how to easily package the docs, but I can at least give people a link and have them be able to browse them.
Right now I'm not parsing test results. I'll keep xunit in mind, the link I found seemed to be setting up the warnings plugin. I generally don't do Jenkins setup, so I don't know which is better. Right now I've just been relying on the error codes from cargo test etc and looking at the console output.
Well Niko did some introductory videos https://www.youtube.com/user/nikomatsakis/videos 
For those not in the know, the main reason the lightning network might provide the listed benefits is that the Bitcoin Core (aka segwitcoin, one of several competing forks of the original Bitcoin block chain) network was (semi?)intentionally slowed down to cause higher transaction fees, slower payments, more network load, and lower security for zero-conf transactions. It's the same as when stores jack up prices before a big sale. (And I'm not here to debate this or start something, just to provide an alternate, fact-based point of view that those unfamiliar with the situation might not otherwise see. It doesn't really impact the value of doing this in rust, of course.)
Yeah...that's not what happened at all. Might want to check your "facts", because your statements aren't actually fact-based. Here's what really happened: 1. The load on the Bitcoin network (at this point there was only the one) started getting a little high 2. People began discussing how to fit more transactions in a block. 3. One proposal was to increase the maximum block size. 4. One proposal was to add a new transaction format that takes up less size. 5. People disagreed on which was best, so the chain and codebase forked and now we have both. There was no an intentional slowdown by the Core devs, raising the max blocksize has potential downsides that some developers felt were not worth the risk. On the other hand, the new transaction format (segwit) does not actually have any less security than any other transaction, despite many people claiming the opposite. You say you're not here to debate or start something, but your blatant misinformation has done that already.
Replied to your issues :) As for the videos, one thing you could do is watch them one chunk at a time. The video description has pointers to various topics within the video that serve as decent places to stop/resume.
I have started a [stochastic simulator](https://github.com/Armavica/bebop) for chemical reaction networks. There are already plenty of this, but I always end up reimplementing this from scratch and I wanted to see what Rust can do in this domain.
so you don't work on a AAA game then? all of the AAA games I've played would definitely not work if two thirds of their processing time disappeared. haha
Hoping to release 0.1.0 of [ghlabelcpy](https://github.com/sorenmortensen/ghlabelcpy) (repo is private right now but the link will work once it's released). It's a simple command-line tool to copy the labels from one GitHub repo to another, because I often find myself needing to do that and it seemed like a fun project.
No. `&amp;mut str` could only do constant-length mutations, but UTF-8 is a variable length code. 
Hi! Say I've a `banner()` function that is written to stdout first thing when my program is launched. How would I capture and freeze the compile time 'now' of the binary so it's included in the banner text each time the executable is run? To be clear: I don't want `banner()` to print the time I ran the executable - I want it to print the time the executable was compiled. I'd a quick look at the cookbook, but nothing jumped out. It might make a nice cookbook example, though... 
On second thought, I decided to make it public early, in case anyone sees this. The link should work now.
You may collect a `Vec&lt;&amp;mut Value&gt;` let map_mut = cell.borrow_mut(); let mut values_mut = map_mut.values_mut().collect(); But the muts live no longer than `map_mut`. 
I peeked at the llvm IR. They're different. So, rustc frontend I think? (Not sure if the IR output runs any llvm optimizations.)
What's your use case? Do you process a lot of data? In my experience I didn't notice huge slow down due to TSAN (observable by human). But we don't use it for super-high load.
Markdown + This == Markdeep: http://casual-effects.com/markdeep/
This is a question for folks who've authored a follow-along type blog posts in the style of https://os.phil-opp.com/ or http://blog.japaric.io/ Would you have any tips for storing the intermediate transformations of the codebase so the blog can be unit-tested?
`vec == slice` is perfectly fine but `slice == vec` doesn't compile. I thought one would imply the other? Is this a known bug? https://play.rust-lang.org/?gist=2feba75a545964f6b3b0982f5721a2d3&amp;version=stable
I looked into such things. First actual Rust with Graal/Truffle but I considered MIR way simpler as there are no macros and no type system to think of, borrow check can also be done by the Rust compiler when using MIR. Second: better integration of Rust with the JVM based on JNI. That one exists already and is mostly boring binding-code. Then I had the idea, to use the ownership model of Rust to allocate objects in rust and directly share with Hotspo.... Blew up even before officially suggesting the topic for my thesis.
A server in a Linux container that has ~10 cores (exact number depends on machine type), running at about 80% CPU utilization. Code written in a synchronous style with lots of threads, using some (currently) Google-specific [magic](https://www.youtube.com/watch?v=KXuZi9aeGTw) that's meant to reduce context switch overhead. Each inbound RPC sends ~2 outbound RPCs, takes ~500 usec of local CPU (in a non-sanitizer build), and takes ~few ms of wall clock time. Those are averages; some inbound RPCs are trivial (little cpu and no outbound RPCs) and some are much slower, usually due to the outbound requests taking longer to return.
I have now listed the definition of MODE in the post! Thank you for the feedback! 
One area to contribute would be to write embedded-hal based drivers for the devices (sensors etc) which you are using. Another would be to create implementations for the embedded-hal for your favorite micro controller platform! 
The official game console SDKs tend to be gated behind non-disclosure agreements, which is probably the biggest blocker to getting them officially supported.
But there were plenty of [quotes to choose from this week](https://users.rust-lang.org/t/twir-quote-of-the-week/328/495)!
Operators you can implement yourself via traits aren't reflexive in Rust. `vec == slice` works because there exists a `PartialEq&lt;&amp;[T]&gt; for Vec&lt;T&gt;` implementation. Unfortunately, `Vec&lt;T&gt; for PartialEq&lt;&amp;[T]&gt;` can't be implemented for coherence reasons. So it's a known issue, but it can't be solved without language changes.
Simplest way would to be add a [build script](https://doc.rust-lang.org/cargo/reference/build-scripts.html) that gets the current moment and writes it out somewhere. Most direct would be to write it as text to a file in the build script's output directory, then use `include_str!` to pull it into the binary's code.
that is SO BEAUTIFUL! 
&gt; Another proposal was to add a new transaction format (segwit) that is more compact. It's not more compact, it's a different accounting technique where some of the block is hidden away (the witness data) so that it doesn't count towards the 1 MB limit. You STILL need to store that data somewhere. It's not in ANY way more efficient.
To clarify why what /u/claire_resurgent mentioned could be a problem, consider the following: copy_str(&amp;mut String::from("a"), "か"); The first string is one byte, the second is three. There's simply no way to fit any part of the second string into the first such that the first string remains valid, and all strings being valid UTF-8 is one of the guarantees Rust makes. Rust generally doesn't let you write over the top of string data, because it's just far too easy to get it wrong. In Rust, you're generally expected to build strings up by plain concatenation, which is only *somewhat* incorrect (strings are hard).
As I said above, I was oversimplifying since the nitty gritty details are really complicated, and this isn't really a bitcoin sub.
You can use environment variables for this now, by writing `println!("cargo:rustc-env=FOO=bar")` from the build script and `env!("FOO")` to retrieve it.
Fairly far out of the loop here, what's wrong with the current implementation/how could it be improved?
Did you mean to say "isomorphic" rather than "isometric"?
Sometimes it feels like every issue of decidability boils down to the halting problem. Actually, there may be a theorem stating just that.
I would not be surprised.
Please consider contributing to the official Rust docker repo: https://github.com/rust-lang-nursery/docker-rust The current Rust image on Docker Hub (https://hub.docker.com/_/rust/) is still pretty bare bone, just the image with no instructions for multi stage building.
From the look of it that Dockerfile is supposed to include the rust compiler in it right? One of my goals was to not need the compiler inside the image (hence why I did a static build). Not sure what would be interesting to contribute to docker-rust from my post.
I hear good things about [skeptic](https://crates.io/crates/skeptic) (iff you use markdown).
I guess I'll have to take on that responsibility too, next time.
Not enough free time to take on new responsibilities, but I'll be cheering from the sidelines! Overall, a great initiative.
Yeah, that was my idea too. Unfortunately, the whole thing only uses multi-drop bus. I plan to release drivers and traits, but I will have to make an agreement with a guy I'm helping. (He is Open Source positive, so should be possible.)
The attack against SegWit would only work if not enough nodes upgraded. I believe most of the network did. And since the theoretical threat doesn't affect upgraded full nodes, I don't care because I run upgraded full node. :)
My understanding is that they're fairly equivalent, except the essential functionality of select that you mention. I mean you can use `std::net::TcpStream`, but there isn't really a good way to say "sleep this thread until any of these 10 `std::net::TcpStream`s have input". `mio` provides the multiple selection - you can have any number of different IO connections, connect them together, and then wake up the thread when any of the connections have input. On unix systems, `mio::net::TcpStream` [quite literally wraps `std::net::TcpStream`](https://github.com/carllerche/mio/blob/9afd0ad070700844d5a4b8a677959882004dba87/src/sys/unix/tcp.rs#L20). On Windows, it's implemented differently - probably to integrate better with how Windows does multiple selection.
&gt; Concurrent with the release of Futures 0.2, we plan to release an updated version of futures-await that provides async/await notation with full borrowing support, due to @withoutboats’s great work in that area. 🦀✨
&gt; [make cargo-the-binary version the same as the Rust version](https://github.com/rust-lang/cargo/pull/5083) Nice. Out of curiosity, why did this take so long?
But this is _only_ the case because you use the future using run. Futures are built to be used in _any_ context, such as running them through spawn and returning from the running function. Rust does _not_ employ any kind of whole program analysis, it's working purely on a function level. What you could do - if you absolutely wanted to - is put the counter in a thread local variable instead. I'm not sure that's cleaner, though.
Wow，I didn't expect it to be so quickly。
Stable SIMD was accepted! Wooot! 🎉❤🦀
No, I mean the "not releasing witness data until someone else announces a block" attack that incentivizes miners to build on blocks before witness data is released: https://www.youtube.com/watch?v=A3WNongC18M
effectively none. `set_nonblocking` tells the kernel you don't want reads/writes on a network handle to block . `mio` provides a cross platform way of learning _when_ the kernel is ready for the code to read/write to that network handle. `mio` does the `set_nonblock` stuff internally, but it does so that it registers to get notifications when handle is ready.
I can't post on /r/bitcoin anyway, I'm banned
I'm working on a scheduling problem for work. I'm implementing various combinatorial optimization algorithms. Trying to consolidate a design for an optimization library (local search + constraint satisfaction). I'd love to throw around ideas with anyone else who has experience!
BTW, I came across a gtk+ widget for this tonight. Here is a blog post that describes how to use it from C: https://www.bassi.io/articles/2015/02/17/using-opengl-with-gtk/ The rust gtk bindings support that widget and give you the rust-specific details: http://gtk-rs.org/docs/gtk/struct.GLArea.html
This channel has made some interesting video's: [Basic tutorials](https://www.youtube.com/watch?v=EYqceb2AnkU&amp;list=PLJbE2Yu2zumDF6BX6_RdPisRVHgzV02NW) [Intermediate]https://www.youtube.com/watch?v=-Jp7sabBCp4&amp;list=PLJbE2Yu2zumDD5vy2BuSHvFZU0a6RDmgb
Since last month, [rls and nightlies are much more in sync](https://internals.rust-lang.org/t/nightlies-missing-the-rls-will-no-longer-be-produced/6653). As for news and progress, [theres always the rls repo to watch](https://github.com/rust-lang-nursery/rls/pulse). It appears theres a continuous effort for making our experience with rls an even better one :)
Would the lifetime still exceed the current function if I didn't manually turn the reactor?
&gt; Ultimately, when we reach 1.0, the expectation is that all of these APIs will be re-incorporated into a single futures crate, and the facade will be no more. This seems like a bad idea. Why cause unnecessary pain down the line?
Well yeah obviously, but moreso a "hey there are people in the working group making sure these targets work". Mono has that, Godot has that, Haxe has that. There is nothing stopping an open source project from having individuals make sure popular NDA'd targets don't break. You don't need to bind to the SDK, I don't care about that.
This. Your personal style are certainly not an improvement on a coherent style used by an overwhelming majority. Everyone can get used to a different codestyle. It takes a couple of weeks of active coding. Let's not repeat that same process over and over again.
What's the pain you're envisioning? Migrating? Programmer confusion? If it's migration, I imagine something like https://github.com/dtolnay/semver-trick can work for that case: the facade gets flipped, and the `futures-core` turns into just a compatibility crate that reexports some stuff from `futures`.
I would expect the function to panic if the slices have different lengths. AFAIK replacing one utf8 slice with another is safe
The crates were split so that combinators could be iterated on without breaking library compatibility. Merging them means the existing combinators become set in stone again, and we end up with confusing and ugly `foo2` combinators every time someone realizes there was something wrong with `foo`.
I think you could do the same thing with transaction data, as transactions are separable from block headers.
Hmm... in that case, yes, it should work. At that point I think it would be memory-safe. I'm still not sure how useful it would be in practice, though.
As an alternative, you can use [nix](https://nixos.org/nix/) to manage your build environments and even create self-contained bundles without needing to deal with the esoteric complexity and heavyweight abstractions of docker.
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](https://nixos.org/nix/) - Previous text "nix" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20duya56u) 
Except the network could "operate" without witness data - it's designed to do this to be backwards-compatible with non-segwit nodes.
Don't upgraded nodes reject blocks without witness data if they are supposed to be included? If they don't, there is a serious vulnerability.
How will the upcoming futures release affect tokio and its stability? 
Is there any plan to write a guide just for future sans the Tokio stuff?
Now **this** is bloody cool stuff..
If there are enough individuals who are able and willing, then sure. I imagine that's a bit of a chicken-and-egg problem at the moment though, and would probably need a more specialized group of people behind the effort.
Interesting, this has been in my (long) list of pet projects for a long time!
Yes, there were 3 submissions since last week's issue, but unfortunately, I didn't find any worth making QotW.
Glad you like it :) I just realized, that the proof heavily depends on the assumption that *optimal compiler* implies minimal generated code size. I wonder if an optimal compiler is still impossible if we only demand minimal "execution time". The proof from before would immediately fall apart, as all non-terminating programs are equally (non-)optimal, but maybe there is some other line of reasoning which also excludes this variant from existence.
&gt; We plan to investigate removing `Error` from `Future` and friends This sounds awesome! When toying with my own future implementation and hand-writing `poll`s, I came to conclusion that removing `Error` is much simpler.
Could you please TAG releases on github?
Yeah I know about nix but have never tried it myself. How does deployment of nix systems compare to say kubernetes or similar orchestration software?
I am currently in the process of making a rust toolchain for the nintendo switch (based on homebrew, not the real sdk, but it's close enough). I'll release it soon ;). One thing to realize is that the process of adding a new target to the rust stdlib is actually incredibly easy. The way I did it, I just used ralloc as an allocator (binding the sbrk function), and then reusing the Redox sys folder from the stdlib as a basis for my implementation. You can take a look https://github.com/roblabla/rust/tree/horizon/src/libstd/sys , the horizon folder is the only one I had to touch, really. The biggest problem IMO is that you can't just distribute the resulting stdlib on crates.io and call it a day. You'll have to ask downstream users to clone your rust fork and point their XARGO_RUST_SRC towards it. Brson had a great vision to fix this, a platform abstraction layer for std. See https://internals.rust-lang.org/t/refactoring-std-for-ultimate-portability/4301. Sadly, I don't think anything ever came out of it. I'd need to look at it again, because I really think that this would be incredibly useful to add support for more esoteric environment in a way that is convenient for everybody. A somewhat related problem: The switch being a different OS, it needs a custom target definition (and a linker script). Right now, this is a bit of a pain: every user needs to have the target json and associated linker script in their project, instead of having the support crate providing it automatically somehow. Not a huge deal, but a bit of a papercut. Anyways point being that the current situation isn't sketchy, but it's not great either.
All I know is that it has been painless and quiet since it stopped disappearing all the time off `rustup`, which is awesome! Seems to also be slowly getting smarter month-on-month, which is nice to see.
It looks like `unsync` channels are missing. Are they coming back later or what's the deal with that?
Awesome stuff! When Tokio 0.2 is released, it'd be nice if there were examples on how to maintain a single-threaded program like tokio-core provided (executor + reactor running on the same thread).
If you use the gnu toolchain, you can use gdb or lldb directly, which already support rust.
I think the intention is to use the Docker container to build your code in the first stage and then package off another stage with a smaller base. If the resulting code isn’t being Docker packaged you would just copy the file out via a volume mount to outside Docker-land. I agree an example of a multistage build/package in the README would help. 
[built](https://crates.io/crates/built) is a crate providing build-time information like this.
What kind of beefy machine do you need? What exactly needs to be benchmarked?
I've been on Windows all my life and 90% of my development is in Visual Studio. Is the gnu toolchain usable together with IntelliJ or VSCode?
iirc one needs to use Clion to use Jetbrain's debugger with Rust, but outside of that, the plugin is solid for everything else. [This](http://www.brycevandyk.com/debug-rust-on-windows-with-visual-studio-code-and-the-msvc-debugger/) explains how to setup vscode with msvc debugger. For a front-end to interactively debug with gdb, check out [gdbgui](https://github.com/cs01/gdbgui).
[syn](https://github.com/dtolnay/syn/blob/master/README.md) has some support for error handling at least.
/u/Geal is the wizard behind `nom`, it's a great library and he's put so much great work into.
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](https://github.com/dtolnay/syn/blob/master/README.md) - Previous text "syn" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20ID_HERE) 
Sorry I wasn't being clear, I mean the instructions on Docker Hub on how to use the image in multi stage build. A good example is the asp.net build image: https://hub.docker.com/r/microsoft/aspnetcore-build/
This is about as fact-based as Trump's twitter feed.
VSCode guide is exactly what I need! Thank you! &gt; In my experience, there are still plenty of types with which debugger doesn't yet play nice. Whats the current state of this? So even if I have the debugger there will still be lots of information missing?
[This entry](https://github.com/not-yet-awesome-rust/not-yet-awesome-rust#user-interfaces) already exists in the list so far -- anything you'd like to add?
What is the purpose of the 'static lifetime declaration for a function parameter? For instance, from today's post about Futures 0.2: ``` impl Context { fn spawn&lt;F&gt;(&amp;mut self, f: F) where F: Future&lt;Item = (), Error = Never&gt; + 'static + Send; } ``` If I understand the book correctly (https://doc.rust-lang.org/book/second-edition/ch19-02-advanced-lifetimes.html) this means that f must have 'static lifetime. But doesn't this limit the usefulness - will many futures really have 'static lifetime?
In general, you should use the MSVC toolchain, not GNU. That said, we produce pdbs, so debugging with VS or VS: Code *should* work, as far as I know. Clion works as well.
&gt; MINIX support. Sweet. Added with [this commit](). &gt; Cross-compilation out of the box like with Go. So, what were you envisioning? Just something like [Xargo](https://github.com/japaric/xargo)'s `--target` option for `cargo`? I know that [it's already been proposed to merge `xargo` functionality into `cargo`](https://github.com/rust-lang/cargo/issues/4959), which [may get `rustup` merged into it this year too](https://github.com/rust-lang/cargo/issues/4959#issuecomment-362887699).
I'm quite sad to see that futures is now going down a path of passing some context objects to everything instead of having something like automatic context variables :(
We definitely have some good use cases for that already! See anything that's missing from the [XML section](https://github.com/not-yet-awesome-rust/not-yet-awesome-rust#xml)?
If it's execution time it would need to know when it halts and then measure that and then somehow determine it's the fastest version and now I think we're back at square one which for a computer is undecidable.
&gt; will many futures really have 'static lifetime? They *all* will. *They don't get a choice.* You have some kind of task scheduler running at the top of the stack, waking futures. If one of those futures wants to spawn a new future (and if you never do that, you don't *need* futures at all), that spawned future has to go up the stack to be owned by the scheduler because the currently running future *must* return control back to the scheduler in order for anything else to run. So futures *cannot* have an arbitrary lifetime, because that would prevent them from being spawned in the first place. &gt; "But wait, what if the lifetime was the lifetime of the scheduler that's going to own them?" Ok, let's assume you did that. That still limits futures to borrowing things that exist *outside* the scheduler, and the scheduler is going to be the thing running at or near the top of the execution stack in almost all cases. And even in cases where there is something you really *do* want to borrow from beyond the scheduler, if you don't have a `'static` lifetime, you wouldn't be able to send futures to other threads, massively limiting their performance. &gt; "But wait, what if it was defined as `Context&lt;'a&gt;` instead, so that schedulers could relax the constraints if they wanted to?" You still couldn't get rid of `Send` because you can't have trait bounds as generic parameters, and you need `Send` for threading, and if you're doing threading you want `'static` too. &gt; "But wait, what about things like scoped thread pools that don't require `'static` at all? Couldn't schedulers just do the same thing." Ah, you see... ... Ok, at that point it's probably about ergonomics.
&gt; Right, so this falls into the case where it is not a legacy codebase I 1000% agree with everything you've been saying in this thread, except for this bit: Firefox is *certainly* a legacy codebase.
There's various projects that can do such things with "Python-like" programs, but the Python language is way too dynamic to statically compile like that. Even building a JIT compiler for Python is a challenge, because Python actually has even more places where you can hook in dynamic behavior than Javascript does (PyPy cheats by doing something that is a sort of mix between a proper Python JIT compiler and running a Python interpreter with JIT optimizations). So TLDR no you absolutely cannot do this with Python, you'd have to ship a Python interpreter in webassembly. 
steam://connect/99.165.8.113:28015 for a faster connection
There's an explanation of [how to use MSVC debugger with VSC](http://www.brycevandyk.com/debug-rust-on-windows-with-visual-studio-code-and-the-msvc-debugger/).
I'm personally working on a project named [chore](https://github.com/phynalle/chore) which is directory-based script runner. I've implemented basic functionality lately but cli experience is too bad. So I'm writing codes for better user experience such as printing friendly messages, user-friendly options or intuitive usability.
Big reason other poster didn't mention: it doesn't work with Dynamically sized types. There's been a lot of work recently to try to make DSTs more convenient with e.g. VLA/alloca. Placement has to catch up.
Ah yes that would be a nice addition to the readme indeed :)
Yes got it now. Should be trivial to write it up.
Firefox is an old codebase, sure. What I meant was 'codebase that is still evolving' vs one that is not - many applications are unable to be changed, sometimes the source code isn't even available. Firefox falls into the case of "old, but actively developed and invested in". Legacy was the wrong word.
I remember that Tock-os had some Bluetooth/802.x support, but don't remember much else. Might look into that.
I've never really understood, what advantage is gained by building and deploying with Docker? I'm guessing it's something to do with reproduceable builds but Rust is already sort of easy to setup for that with cargo-vender and using the RUSTC environment variable to point to a specific rust version checked in to version control.
Sure. I'll do that starting with the next release.
Why is that?
You misunderstood me. I don't care if a crate is made that wraps the SDK in rust. They are all C, I can deal with binding that. All I care about is knowing the target exists.
Fearlessly.
Can anyone fill me in on what SIMD actually is/does?
I don't mean that you should put docs into README. Just a simple introduction, a screenshot, perhaps contributing guidelines or a license... You can see some github projects as examples of what I mean: https://github.com/mitsuhiko/indicatif https://github.com/PistonDevelopers/hematite https://github.com/citybound/citybound https://github.com/serde-rs/serde (Some projects have separate LICENSE, CONTRIBUTING, ROADMAP, NEWS and other files, but unless these sections are long, it's simpler to just keep them in README.) Oh, and Crates.io even displays the README file on the crate page, so people can get a quick overview.
This is amazing, thanks for the heads up! Managed to find the thread in internals.r-l.o, here it is for anyone who might be looking for it: https://internals.rust-lang.org/t/a-vision-for-portability-in-rust/6719 . Key takeaway from last post: &gt; The main take-away from the kickoff meeting is that the most important priority is refactoring std to make writing and maintaining ports to new platforms easier. In particular, because std today intertwines platform-specific code throughout, keeping an out-of-tree port up to date requires very painful rebasing. We found that most of the goals we were interested in (including having a “pluggable” std) are ultimately blocked on factoring out a cleaner “platform abstraction layer”. My time is already fairly thin, but I'll definitely try to get involved.
Me too.
Single Instruction Multiple Data. Doing the same thing to many things in parallel, for performance. Many CPUs, and all modern GPUs, have vector operation instructions that this library provides traits for.
Instruction level parallelism. For example, if you're doing the same operation to every element in a loop, you could get a 4x perf boost by doing operations to 4 elements per instruction instead of one. It's much lower overhead than something like threading to achieve parallelism, but it's also much more limited in capabilities. A simple example of it's usage is newline detection in a file. Ripgrep for instance uses SIMD to check multiple characters at a time to see if they are a new line delimiter when parsing files.
[Wiki](https://en.m.wikipedia.org/wiki/SIMD) can
Non-Mobile link: https://en.wikipedia.org/wiki/SIMD *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^154549
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/buttcoin] [Zero cost abstractions, memory safety, and fearless concurrency in the Lightning network](https://www.reddit.com/r/Buttcoin/comments/80xgtm/zero_cost_abstractions_memory_safety_and_fearless/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
What your plan for Rust plus Qt/C++ codebase? You want create dll on Rust that you call from C++ code? Or you want to create rust application and reuse some of C++ classes? The first variant is what I use in my current development. I implement GUI part of application in C++/Qt while business logic is compiled into ".so/.dll" and written in Rust. For GUI/C++ &lt;-&gt; core/Rust I use my crate [rust_swig](https://github.com/Dushistov/rust_swig). The second variant (calling C++ code from rust) as implemented in cpp_to_rust and in [bindgen](https://github.com/rust-lang-nursery/rust-bindgen) seems to me possible path, but more error prone for C++ code. [bindgen](https://github.com/rust-lang-nursery/rust-bindgen) works great for C, but for C++... 
Not sure if it's widely known but there is [These Weeks in Dev-Tools](https://github.com/nrc/dev-tools-team/blob/master/twidt/issue-3.md)
Shameless plug but VS 2017 preview has language server support provided as an extension and there is also a Rust extension I maintain. That way you get the full benefits of the RLS and debugging in VS
&gt; Instruction level parallelism. not meaning to be pedantic, is the jargon a bit more subtle, I count 3 types of parallelism: threads&amp;cores, instruction-level (= pipelining,superscalar), and SIMD/vectorization (='intra-instruction' parallelism?) .. 
I'm new to Rust, so I'm reading [Rust for C++ programmers](https://github.com/nrc/r4cppp), and searching for small and well-written Rust codebases to learn from.
Actually, it works on the current VS 2017 (15.5.7) now. I didn't have a chance to compare it with preview version, but some basic things definitely work (like autocompletion).
When implementing futures, Item and Error are handled the same way in almost all combinators: they are the the computed value. They only need to be treated differently in map, and_then, or_else. Intuitively, it seems that error handling belongs to different layer. Also some futures can't fail and using Result&lt;T, Void&gt; instead of simple T looks weird. Another interesting benefit of not having Error is that we could have and_then and co. working for both Item=Result&lt;...&gt; and Item=Option. Interestingly in case of hand-writing polls, Result&lt;Item, Async&lt;Error&gt;&gt; would be more ergonomic, because usually both Error and NotReady need to be returned early, which would be simpler with ? operator.
&gt; While we’ve been discussing and vetting the 0.2 changes publicly for some time, it’s important to get some real usage prior to publication. We’ve made substantial progress porting parts of Fuchsia to the new release, and expect to have a complete port soon. We will also be coordinating with the Tokio team, which intends to release a 0.2 to integrate with Futures 0.2. TIL: Fuchsia uses the futures crate.
Thats... definitely not intended. The extension was meant to only work on the preview version of VS. Either way it should pretty much support everything that the rls extension in VSCode does.
Is that fuchsia the operating system ?
Yes.
Well, I must admit, the title is funny. :D
Nice. Get the "Rust now faster than C/C++ on benchmarkgames" blog posts ready.
Downsides are losing your coins, having to monitor the channel to make sure other party doesn't shut it down or mess with payment...
It's not pedantic, really. The difference is in the name itself: _Single_ Instruction, Multiple Data. Other types of CPU-level parallelism may involve multiple instructions -- and regular threading obviously does, incl. different execution contexts.
i was awaiting someone to make that joke
Thanks! So basically `a == b` is the same as `a.eq(b)`, and the compiler can't just rewrite it as `b.eq(a)` because that isn't guaranteed to be equivalent. Do you thing it would be possible to add a help message to the error like "try doing b == a instead"? Or maybe suggest using deref coercions, because `slice == &amp;*vec` also works...
I kept polling this page so I could see it as soon as it was posted
Nix doesn't inherently address deployment; you can use it to generate a monolithic binary, or a docker image, or even a bootable image, managed by existing deployment tools. Alternatively, you can tinker with `NixOps` which is a bit more nix-native but isn't super mature.
I have no answer but i've been curious about this too.
MSVC's debugger works fine out of the box.
inb4 "we hire senior developers with at least 10 years experience in Rust programming, 5 years experience in the Rocket web framework and Cargo package manager."
I imagine it can be done with something non-ergonomic, yeah. Part of me thinks the only reason it has `set_nonblocking` at all is so that libraries like `mio` don't have to re-implement _everything_. Even if there's no way to select in the standard library, I think `mio` implements it pretty well. Part of the whole "batteries on crates.io" philosophy, right?
It's a bit out of date now but someone did a [code review](http://blog.mbrt.it/2016-12-01-ripgrep-code-review/) of [ripgrep](https://github.com/BurntSushi/ripgrep). 
No. Is knowing a language alone sufficient even for a junior dev job in general?
Correction. I'm quite sure that spending SegWit output is smaller on top of "tricks" you've mentioned. I just don't have the time to dig the links to the explanation.
The existence of `seq` in Haskell seems like pretty small potatoes compared to trait specialization in Rust.
For Conway's Life, you *can't* update the universe in place because the updates will clobber cells (GoL cells, not Rust `Cell`) which are needed to update their neighbors. This is true in any language. Rust is just making it more difficult to write a state bug than it would be in Kotlin or Java, where equivalent code would compile but not implement GoL. Instead you can do this: - allocate a new collection of cells - mutate the new collection while only reading from the old - replace the old with the new That'll get you started. But you're allocating and freeing a game board once per generation, which isn't ideal. You can instead keep the old one in the structure (see the `std::mem` module for helpful functions) and overwrite it in place. If you *do* want the buggy algorithm to compile, `Cell` will do it. You don't need `&amp;mut` because you can call `set` on a `&amp;` reference. Or, you don't need references. Iterate over indices and the borrowck won't complain.
I'd be more than happy to add something like this to the list, provided you can justify pure FP with a convincing use case. :) I bet you have one!
My example was a little oversimplified. I wanted to use x, y, and z on separate lines.
Aside from graphics, what type of computations are the best candidates to take advantage of this type of optimization?
I'm struggling with the efficiency demons. Reducing those clones is proving difficult while making an accessible API.
So, I see two things that I fit the type of content I'm looking for (which is use cases!): * Better async I/O -- this is in the pipe with `tokio` 0.2 already, and is significant enough that it's actually part of Rust's 2018 roadmap at time of writing. I would add this use case to the list, except that it's the focus of Rust's community this year and I probably wouldn't be contributing any value by adding it in the first place. :( * &gt; there's a lack of tokio database drivers and its difficult to make them share a tokio "core" Lack of async database drivers are something that the community has also recognized, and I'd be willing to bird-dog people on this one. I know that Sean Griffin [stated in January 2017](https://github.com/diesel-rs/diesel/issues/399#issuecomment-357361992) that the fundamental blockers for Diesel getting async were, essentially: * `tokio` immaturity/instability * Lack of satisfying async API design for `diesel` specifically You can still use Diesel in an async environment by using message passing of some sort (a la [a suggestion](https://github.com/diesel-rs/diesel/issues/399#issuecomment-360535059) in the same Diesel issue as above). Does that resolve your use case? If not, what constraints make it so that the above solution isn't feasible? Lastly (and hopefully not coming across as a glib part of my response), given how simple your use case is, I would question whether you need asynchronicity or not. What sort of scale are you expecting for this server, and *how* do you expect it to scale?
To get the ball rolling and some details elaborated, let's start with this: what is [`tokio-postgres`](https://docs.rs/tokio-postgres/0.3.0/tokio_postgres) missing? Have you tried it?
...as an async primitive? Using Unix signals? What do you mean?
You're referring to [this](https://github.com/Netflix/eureka)?
I don't think that this is something I'd want to add to the NYAR list, since getting this recognized and/or fixed can be as straightforward as reporting an issue or creating a PR against the [`rustlang/rust`](https://github/rust-lang/rust) repo. I'm glad that some people have taken action already on your suggestion here!
Yes. We use eureka for our service discovery at work. 
I wrote a long detailed comment explaining it once before on this subreddit: https://www.reddit.com/r/rust/comments/7b3959/announcing_faster_01_zerooverhead_crossplatform/dpfw4tv/ ^ not gonna repeat myself again, I'll just link you to my old comment. Have a read. :)
I think that more fully-featured regexes for Rust would be great! In fact...they already DO exist! Let's examine the [`regex` crate docs](https://docs.rs/regex/0.2.6/regex/) (emphasis mine): &gt; Its syntax is similar to Perl-style regular expressions, but **lacks a few features like look around and backreferences. In exchange, all searches execute in linear time** with respect to the size of the regular expression and search text. We can see that the limited feature set is actually a design choice of the library. However, there already IS another library you can use called [`fancy-regex`](https://github.com/google/fancy-regex#fancy-regex), which actually recognizes its design in a similar fashion to `regex` in its README: &gt; [`fancy-regex`] uses a hybrid regex implementation designed to support a relatively rich set of features. In particular, it uses backtracking to implement "fancy" features such as look-around and backtracking, which are not supported in purely NFA-based implementations (exemplified by RE2, and implemented in Rust in the regex crate). Because of this, I'm pretty sure that your use case is already taken care of by the Rust ecosystem, just not in the `regex` crate by itself. Hope that helps! :)
...so what's the takeaway for Rust? I'm not sure...
Why shouldn't it be? As a junior developer you generally only fix small problems at first and get to know the workflow and the codebase of the different projects the company develops, if I am not mistaken.
[removed]
As I understand it, mio implements it via polling. So I'd think it'd be significantly less efficient in the case where there isn't much traffic especially for large numbers of files / sockets. I'm not sure how it would compare if there was a lot of traffic.
Ty
The point of a 1.0 release is to set what exists in stone.
Most of the options offered in this thread are fairly low effort to try out and see for yourself. You may be surprised how far you can go before needing to pull out a debugger to troubleshoot with rust, and by the time you do, the tooling situation may already be demystified for you. For example, if you wanted to try the gnu toolchain, installing it alongside other toolchains takes a single rustup command “rustup install stable-gnu”, (and yes, it works well on windows with intellij etc). Your toolchain will be more decided by your debugger of choice and the libraries you want to introp with; thankfully, switching between toolchains is painless with rustup. There are multiple IDE and debug options that work well on windows. The next step is to try one! (or in this case, try one again!) On another note, if you haven’t used cargo yet (introduced early in the official books) or have only read about it, you’re in for a treat :)
This is something that probably could be approached by the Rust ecosystem (as opposed to Rust's core development team), but in order to get something deliverable out of this comment, what specifically have you noted is missing from current C++ interop? Another way to phrase the question might be: what hasn't already been done in the current ecosystem, notably: * [`rust-bindgen`](https://github.com/rust-lang-nursery/rust-bindgen) * the [`cpp`](https://docs.rs/cpp/0.3.2/cpp/) crate It should be considered that C++ doesn't even have a standardized ABI beyond its C roots, so I need to add a caveat here that I wouldn't want to put something that's actually impossible on this list like expecting interoperation with ANY valid C++ program. However, I bet there's a subset of C++ that interop could reasonably work with!
When I first read the title of this post, my first thought was, "What the heck would I want with SQL-like queries for finding files". However, I then read the README.md for the project and I am completely impressed. I actually really like this idea and where it is going. I think it has a lot of potential to really be a great solution.
What kind of optimization are you particularly interested in? Continuous, discrete CP?
17 isn't supported, since 18 is the minimum supported by `rustc` (at time of writing, source [here](https://github.com/tomaka/android-rs-glue)), but you can already target specific versions of the Android API beyond that. Your second phrase, though, I don't think I fully comprehend yet: &gt; have the ability for crates (like libc) to expose functionality based on this. You're thinking something like feature gates, but for different Android versions?
&gt; easy to setup oauth system (loging with facebook or google account) for web libraries (rocket, gotham doesnt have it, you have to write it from the scratch) What sort of flow were you imagining? I don't know much about this particular domain in server development... &gt; easy to use, flexible library for creating API wrappers (github api, reddit api etc) It seems like something already DOES exist for this called [`anterofit`](https://github.com/abonander/anterofit); have you tried it? Does it not offer something you were looking for? &gt; https://users.rust-lang.org/t/one-place-for-ideas-with-voting-system/15621 HEY! This is something I can get behind...because I already had an issue on NYAR to [implement](https://github.com/not-yet-awesome-rust/not-yet-awesome-rust/issues/2) [similar](https://github.com/not-yet-awesome-rust/not-yet-awesome-rust/issues/24) [stuff](https://github.com/not-yet-awesome-rust/not-yet-awesome-rust/issues/30) (note: each word is a link)! PM me if you're interested in hearing what my idea is to implement this. I'd love to get more than just myself thinking and working on it!
So, just taking a quick look at http://www.gegl.org/, my understanding is that it's basically like functional reactive programming for images...does that sound right?
No problem. let mut x = ( .., .., .. ); I'm not just being flippant. Invoking a constructor goes left to right. If I understand correctly, there might still be some uncertainty about assignment operations whose left argument contains side effects (Index or Deref) but that's not an issue here. If you want to initialize a tuple in arbitrary order, you can do that too: let mut x: (_, _, _); x.1 = ...; x.2 = ...; x.0 = ...; My point is that there's nothing saying you *must* use `;` to guarantee sequencing. And of course newline is equivalent to space.
You should have scheduled someone to wake() you up when it happened instead!
Cool! I just stumbled over the first Rust microcontroller project (to customize a computer keyboard) yesterday : https://github.com/ah-/anne-key/blob/master/README.md
&gt; If it doesn't even have select, how mature can it be? This is such a strange complaint. Do people not realize that rust can easily and efficiently call C? You might as well complain that C++ doesn't have select.
&gt; mio implements it via polling Where did you get that idea? mio uses IOCPs on windows, epoll on linux, kqueue on OSX, etc. That's the whole point of the library. If all you wanted to do was poll you could just do that with `std`.
Thanks for the reply, I'll do a project using VSCode since the step-by-step guide linked earlier was easy to follow and see where it takes me. I guess the biggest reason I'm so indecisive is that I really want to replace C++ with Rust as my goto language so I want it to be good, really good, so I get a bit pedantic. The recent discussions about Web Assembly and micro-controllers has gotten my really excited again about switching.
The difficulty I see with junior Rust jobs appearing any time soon is that most of the people who have a use case that they can justify Rust with are either: * Highly specialized teams that know or are experimenting to figure out exactly what Rust offers them as opposed to C or C++ (which is C-magnitude performance without many of the drawbacks). Usually this will mean that they already have a deep tech stack and/or require significant expertise for development in their domain, and finding a place for junior development in something with such strong requirements can be challenging. * Early starters that, for some reason, prefer or are willing to be on Rust as one of their initial tech choices. This is simply rare right now, because Rust is still proving itself (and proving itself it is!). I would bet that, in order to get to a point where there are a fair number of junior Rust positions, Rust will need to be a strong enough player to be considered the default in some domain or use case first. This will simply take time...because that's what the community and general perception of Rust will need.
&gt; Merging them means the existing combinators become set in stone again As you'd expect from a 1.0 library.
I don't expect 1.0 libraries to be gratuitously hostile to interop.
Thanks for this explanation. Very nice!
Thanks for this explanation. Very nice!
In my intuition about the current climate, it will be tough to find a junior position that has “you get to write Rust” in the description. Now, I could be totally wrong. Here’s what informs my assessment: - Small companies are less likely to hire a junior developer, since in general they will need someone who can wear many hats and help to grow a team. - Many of the medium-sized and big companies doing work in Rust only started in earnest in the last year or so. It takes a little while for bigger companies to decide to use a new technology, prove it out with a project, and then decide that that project worked well enough to devote more resources to it by hiring junior devs. So I believe it depends on your timing. If you’re looking to find a position immediately, it will be way _way_ easier to find a junior position in a more popular programming language. If you’re looking for something in a year or so, I think your prospects will be much brighter for finding a job with Rust in the description. So, my advice boils down to this: find a job immediately if you need to with your existing Python &amp; Webdev skills. But, don’t give up on Rust - in my estimation it’s an emerging technology with revolutionary potential. In your spare time, keep learning about it and building things. Contribute to some open source projects! If you stick around at your junior dev position long enough, I bet you’ll find a way to use Rust. Maybe your team will need to make a CLI app, maybe you need to replace some old bash scripts with something more maintainable. Maybe some Python or JavaScript component needs to be faster, and you’ll be able to say “hey, I can speed that up by calling out to Rust, it mind if I do a proof of concept?” etc. And then boom - you’ve got a job writing Rust! Plus, if you’re always talking about how rad a technology is at work, you might even get your lead to give it a try. And if that doesn’t happen, well, Rust is really poised to make some big advances with frontend (WASM) and backend web programming in the next year. Having experience with it could set you apart from other candidates when it comes time to find your next gig. Not to mention the benefits for finding future jobs that come with contriguting to a growing programming community like Rust’s. Hope that helps, and good luck! 
Thanks Erich, it does. You have been busy here :-)
That is really cool! I'd love to see more blog posts in that style.
thanks!
thanks!
Understood! There's a lot of well-deserved excitement over wasm/embedded, and there is much more to be explored and improved upon! As far as replacing, no decisions need to be made today. Some practices may rub off positively on the other. Context will help choose which is best for various projects going forward, and thankfully, they can be used together.
Example present with 0.4 release https://github.com/actix/actix-web/tree/v0.4.0/examples/juniper
Example present with 0.4 release https://github.com/actix/actix-web/tree/v0.4.0/examples/juniper
&gt; Interestingly in case of hand-writing polls, Result&lt;Item, Async&lt;Error&gt;&gt; would be more ergonomic, because usually both Error and NotReady need to be returned early, which would be simpler with ? operator. I've been wondering about that as well, in a `Future::Error`-less world. My first instinct would be for `poll` just to return `Async&lt;Item&gt;`, but on its own that interferes with use of `?`, and both that and `Result&lt;Item, Async&lt;Error&gt;&gt;` let you return `NotReady` via `?` like you mention, but I'm not sure that should really be conflated with errors... Perhaps `impl&lt;T: Try&gt; Try for Async&lt;T&gt;`? That way `?` still works, but only if `Item: Try`. It doesn't allow for `NotReady?`, but perhaps async/await and `let...else` would be enough there?
To your question, simply put, yes. I've been meaning to start a "cool snippets you can do with Rust" collection because there are a lot of things people just don't expect because they're accustomed to spending weeks struggling with cmake, memory issues, scripting bindings api, cross compiling, integrating build systems, or DLL incompatibility because they've been taught by C++ that it has to be hard and complex. To your last sentence, it's clearly a different situation. C++ is for general practical purposes a superset of C. In Rust there's additional steps necessary to link with a C library that aren't needed with a Rust crate, so it's still harder than a pure Rust version. If you go the obvious route of "libc", what you get is this: https://doc.rust-lang.org/libc/x86_64-unknown-linux-gnu/libc/fn.select.html This is using C calling conventions, but is immediately hitting somebody with pointers and unsafe code. And there's no documentation. If you have some familiarity with the ecosystem you might look in nix: https://docs.rs/nix/0.10.0/nix/sys/select/fn.select.html This is perfectly usable (so now I've definitely answered the initial question) but it's clearly not widespread knowledge. The initial concern probably stemmed from the notorious ntp post: http://esr.ibiblio.org/?p=7294 That being said, the general point of bringing it up was not to say "this is the way things actually are" but "this is way people think things are".
Hoo boy have I...there was a LOT of feedback this time around for NYAR! :) I'm glad you could be one of the easier cases here, haha...
I'm probably simply wrong then and getting it confused with other tokio / futures mechanisms.
Based on other comments in this thread it seems that is the case... With some batteries required.
Great share! On a related note, the paper [End-to-End Differentiable Prover](https://arxiv.org/abs/1705.11040) goes into a novel way to use vector representations alongside symbolic queries to generate an automated theorem prover. I wrote a summary on the paper here: [Computers Learning to Reason](https://www.aidynamism.com/single-post/2018/02/17/Computers-Learning-to-Reason)
It's all good - I was confused about the ecosystem for quite a while before figuring things out. I'm 99% sure that none of these things use constant polling though. `tokio` is built upon `mio` as well, it just provides a futures interface on top of that. It does poll occasionally, but my understanding is that it's implemented as a loop "wait on any OS activity via os-specific functions" -&gt; "poll every top-level future once" -&gt; repeat. `tokio` does technically poll, yes, but in the most efficient way possible.
&gt; Audience Personally, I would prefer if it was aimed at: - people that are relatively new to microcontrollers (on this level), and - people that are confident in rust (if you are not, go look at the regular rust docs). I've been programming on Arduino in C++ for a bit, but it is nowhere near as lowlevel as some of japaric's examples. But I also see the other argument of people that are confident in microcontrollers from other languages and just want to learn it the rust way. On that note, how does the WG decide on a matter like this? Survey? Internal vote? 
Just to stress this point. If you can come up with a scenario where you want to do a math operation across several data points, you can come up with a use of SIMD. An easy example is averaging let xs = []; let mut total = 0; for x in xs { total += x; } total / xs.len(); Now, the naive implementation of this goes through each item in the loop, adds it to the total. That will take `len() * loop overhead` instructions to execute. Using simd, we could split total into 4 totals. So instead a loop would consist of something like `total[0, 1, 2, 3] + xs[0, 1, 2, 3]` then `total[0, 1, 2, 3] + xs[4, 5, 6, 7]`. This reduces the number of loops that need to happen by the number of items in the SIMD register (in this case by a factor of 4). Except for the ultra wide SIMD instructions (AVX512), SIMD instructions generally execute just as fast as the single instructions. This is because CPUs have a bunch of hardware that would have otherwise been sitting idle while, for example, the loop branch is being calculated. This is a simple example and for the most part your compiler will already catch and perform this for you (hurray!) It's the more complex examples that compilers struggle with and the RFC helps with. Auto-vectorization is an ongoing problem that compiler writers are constantly improving (because the performance gains can be awesome) but it is an area where human pattern matching can often do a better job still.
I also cross posted this on hackernews ([https://news.ycombinator.com/item?id=16488135](https://news.ycombinator.com/item?id=16488135))
Mmm, i'm honestly thinking 'just don't copy older interfaces from popular languages'. Take a look at the popular libraries too.
We are hiring Sr systems programmer with rust experience because we want someone who knows rust idioms and is prepared to embrace them and we need someone who knows full stack programming. Rust is more important than systems programming but if they are a solid programmer in another language we’ed consider it. We don’t want to teach a C programmer rust, they would end up writing c code in rust or writing C code with a rust facade. Been there, bad news. I recommend OP does some open source rust work, no shortage, and apply for everything available, it costs nothing. 
Markdeep is just a .js that you may import anywhere and it does the conversion on the fly. See examples in markdeep site. Its usage is way more generic, example, the Graphics Codex (http://graphicscodex.com/) (from the same author) uses it to ship a kind of ebook on mobile devices.
You probably want to target two of the possible audiences: Controller experience but new to Rust; and Rust programmers but new to controllers. People that know both don't really need more than a README to get going; and people new to both would probably bite off more than they can chew trying to learn everything all at once. And — just my opinion — experienced Rust programmers are perhaps just as well off learning the concepts of microcontroller programming from more generic sources. Perhaps the most productive target is microcontroller programmers looking for a safer, faster way to develop their code. But I think you can cover both cases in a fairly natural way with a single text.
Hey there, this got stuck in the spam filter, and if I approve it now then it's old enough that it will probably get seen by exactly nobody. Would you mind resubmitting this?
/r/playrust
Congratulations on the release!
The guide was cozy to follow during some 0.3 experiments. I’m looking forward to try the websockets client! Congratulations on the new release, as well as the TechEmpower framework benchmarks, and thank you for all the great work!
i didnt write guide for client yet, but client api is simple enough, api docs should help but i am planing to add client guide as well
Btw 0.4 release should perform even better in TechEmpower benchmark. I already submitted changes, but we need to wait for next round
nice :) now please do the oauth integration :P
Hmm, I clicked on the demo link, and my laptop froze, the page ate all of my memory, and my OS (Linux) killed the process. Did it happen to anyone else? Mighty this be a bug in Firefox on Linux? I have no other ideas.
Actix web 0.4 is release. If you need I can help you with integration.
This is a very interesting problem! I'd be interested to hear how some other languages are going about solving it. I know cargo has [plans](https://github.com/rust-lang/cargo/issues/3815) to allow separating the 'create a build plan', 'download dependencies' and 'execute that build plan' steps in order to integrate with other build runners. It seems like a good step in the right direction, but doesn't necessarily solve everything, and we aren't there yet.
we need easy to set up "login with google" or "login with facebook" for existing libraries (rocket/gotham etc). currently if you want to implement it, you need to write it by yourself from scratch. yeah i know about anterofit. i think it still need some work. for huge apis like github you would need to write a lot of code. we need something with macros. i will open an issue on anterofit later. about the voting system, i will check out the issues. actually i had an idea (and implementation) for voting with github if you are interested. I also implemented some of it but not working properly yet. https://github.com/jaroslaw-weber/vote_for_rust If you are interested I can try to do full implementation (i think it may be possible to get data with one request using graphql) 
Yay keyboard firmware! How is the USB HID support going? I see some USB code, but the readme suggests it's not working yet?
Is `std` gratuitously hostile to interop?
USB did somewhat work at some point, as in it enumerated correctly and did send a couple of key presses. The code is rough though, as it was written before I had a good grasp of Rust, and it's pretty experimental and I haven't done a refactoring/cleanup iteration yet. It's been written by building a minimal HID example in C using stm32cube, logging USB register accesses and trying to match them. The main problem I had was (I think) that I'm doing the data toggling wrong. Host and device synchronise by toggling a DTOG bit, and I'm pretty sure I'm not doing that correctly and it just works because the host is fault tolerant and if I'm lucky looks over my mistakes. Then I figured out that bluetooth was way simpler than expected, so I parked USB for a while. I'm hoping that https://github.com/rust-lang-nursery/embedded-wg/issues/40 picks up, otherwise I'm going to look at USB again once the simple other bits on my TODO list are done.
&gt; This is a simple example and for the most part your compiler will already catch and perform this for you (hurray!) It's the more complex examples that compilers struggle with and the RFC helps with. So, we don't need to refactor code to take advantage of SIMD? Keeping it as a loop can be optimized by the compiler to use SIMD instead for us?
As always, it depends. Compilers have gotten good at the simple cases, but that doesn't mean they don't miss things or get tripped up. I wouldn't start by writing SIMD. Write it clean, profile, fix the algorithm if possible, profile again, them consider explicit SIMD. The problem with trying low level optimizations first is they almost always kill future compiler and CPU optimizations. Simple idiomatic is the first to be optimized and what you should generally write.
Discrete. Currently I'm benchmarking and testing against Graph Coloring.
I can't help noticing you're also working on CP. What are you working on?
No problem. I assumed that might happen: I made this account because the crate itself ties to my real name (obvs), and so it sorta looked bad from a spam filter perspective. I'll resubmit.
I think there are some crates available that already let you write vectorized code(xyz + 5, xyz.x + 5)? I guess if they're suitable to use, that any SIMD optimizations would be handled by the compiler or by the crate author?
Thanks for any advice you have!
*What the hell are they talking abo--* &amp;nbsp; **Yes**. Of course. I meant to do that.
The hint should be possible, but I *think* using deref coercsions was already decided against at some point, though I don't recall why. Possibly because the core team didn't want the compiler being over-eager in trying to compare things.
You can say so, yes.
Just ping me if you’d have any questions Chat is here https://gitter.im/actix/actix
It's better than that; a top-level future is only polled if one of the events it's waiting for arises. You only end up with the behavior you describe if you try to stuff a bunch of unrelated futures into a single top-level future *and* don't use the features specifically designed to make that efficient.
If someone who expects C-style `select` is coming to rust, I think hitting them with pointers and unsafe code ASAP is one of the most important things you can do so they don't get any weird illusions about rust not letting you do things.
I'd love to see some kind of benchmarks :D
This is something I've been thinking a lot about as I deal with my company's internal cross-language build system. We get around this problem in a unique-to-us way. We can enforce a layout to our artifacts and allow packages to declare build system plugins for clients to customize the behavior for odd ball cases. One general way of solving this problem is having a system like [conan](http://conan.io/) where you decouple dependency management from build. - You could design it where the build system's plugin into the package system knows how to adapt the file system layout into a known configuration. - Another route is to have metadata files like with pkg-config. - The last is a package system plugin system that is cross-language (known IPC protocol like with `build.rs`)
zero-conf transactions were never secure to begin with.
TechEmpower is actually running a CI server with results continuously being posted. In fact, the tests are running right now for 2/26, since it takes roughly 90 hours for them to all complete. https://tfb-status.techempower.com/
sorry, how to effectively use unix signals in rust
I know about this site. But they run old code at the moment, same as for round 15.
I have some very, very primitive benchmarks on [faster's github page](https://github.com/AdamNiederer/faster). When taking the coarse-grained approach that faster takes, you usually get a 5-6x speedup with u8s and 2-3x with u32s.
FYI: as far as I know, linking directly to HN posts can lead to posts being buried if too many come from the same referrer. Suggest searching for the post instead of accessing it directly.
If the overwhelming majority of your code is nothing but making system calls, then you probably should stick with C. Real projects aren't structured like that.
Yeah, this happens to firefox. Chrome is working though.
I'm not involved in the Gotham project, but reverse proxying behind nginx has all kinds of nice benefits like [easy rate limiting](https://www.nginx.com/blog/rate-limiting-nginx/) for your API, in addition to handling HTTPS. Having HTTPS support in the framework is a nice option, but not something I would consider critical for my projects.
I've tried to try it, but I don't see how to use it without reconnecting every time
Yeah I definitely fit in that first category. I'm an Embedded Software Engineer with C++ and I just feel like everything is a fine balancing act of analyzers and best practices and code reviews just to make everything safe. I'm learning Rust because it looks promising in terms of writing safe code without sacrificing too much.
What do you mean doesn’t support file upload?
On the other hand, very often it's desirable to have encryption-in-transit between your application servers and your reverse proxy. HTTPS isn't always for your end users.
If your reverse proxy resides on the same server as your application, and the HTTP connections are only going through the loopback device, then HTTPS would not have protected you from whatever threat you're perceiving to exist. If you're using multiple application servers, each of them can have their own reverse proxy responsible for securing the connection on server, but then you lose out on anything at the network level except for very basic load balancing, basic IP rate limiting (rather than endpoint level granularity), and other compromises. If you can't trust your cloud network infrastructure, you could put your application servers and proxy server inside their own VPN that you configured. This will hurt performance, but it gives you good security and a good selection of features. There are pros and cons to every approach.
Ah, I didn't even realize! I know less about how `tokio` works than I thought.
Fair point, and agreed! In the situation I was describing where encryption-in-flight is desired, having HTTPS support in the application itself is convenient but not essential.
Rust is great, but it's definitely not anywhere close to _finished_. 1.24.0 was a great release... finally we have both rls and rustfmt on stable. But there are still tons of things that need polish here and there and they don't feel like they really get any attention because everyone is more interested in WebAssembly or whatever else is hot that week. It's just one of the downsides of dealing with a relatively young open source project... people are either volunteering their time or being paid by someone that's not me to work on things that other people care about. If I'm not contributing myself or paying people to work on what I need I just have to report bugs and kind of hope someone gets interested enough to fix them. Unfortunately neither contributing myself nor paying someone to contribute are valid options right now. Still, part of the problem here is just VSCode... another open source project that doesn't necessarily fix what I need fixed when I need it fixed. Setting up Rust debugging requires a few different extensions, as well as manually setting up a weird json config that's different for every project and that I can never get quite right. It's probably all solvable, but at some point you just give up and go back to what you know. With all that said, don't let it scare you off. Rust isn't anywhere close to perfect, nor is the community. But it's a great language and it took me til my 4th project in the language before I really needed a debugger at all. IRC is really helpful, especially steveklabnik and mbrubeck have helped me a lot there.
I don't really see how this is any different from projects using non-language-specific build systems and needing to be used in other projects. ninja/cmake/make/etc are all tricky to use within your build system. Hell, using one project that uses make as a dependency of the other is often tricky because you have to rely on both projects using the same variables and stuff, and passing down the correct things. Most C projects out there just reimplement everything instead of pulling in libraries, because pulling in libraries is _hard_. There are very common libraries that are an exception to this, but overall the thing to do in the C world is writing it yourself. Turns out, mixing build systems is a hard problem. Doesn't matter if they're language specific or otherwise. It's pretty clear the post is arguing from a C point of view; but makefiles/pkg-config/etc are also their own build system ecosystem. It's also nontrivial to use those in other projects; just that these integrations usually _exist_ because C is everywhere and you want your language to be able to talk to C. Honestly this post just reads at "get off my lawn, kids". I do think something like cargo's modularity is a good solution to this problem; but it's not a problem of per-language build systems, it's a problem of build systems in general. 
In addition to what /u/bjzaba said: https://news.ycombinator.com/newsfaq.html &gt; Can I ask people to upvote my submission? &gt; No. Users should vote for a story because they personally find it intellectually interesting, not because someone has content to promote. &gt; HN's software penalizes submissions, accounts, and sites that break this rule, so please don't. It makes sense to cross-link when there's interesting discussion, but doing so before there's been much activity seems like it could be interpreted as asking for votes even if that wasn't explicit (by the software, if not by humans). And, asking for people to search for it instead seems even more suspicious. :)
Last I checked, which I think was back in january, there was no built-in support for it in Gotham, but it's possible to implement it yourself using the "multipart" crate for the actual parsing.
That's not how Try is implemented. It's just impl&lt;T&gt; (without bounds), so it would work well.
I am just interested in optimization in general! I extensively use continuous optimization in my research, and I followed a MOOC on discrete optimization on Coursera some time ago and programmed along. I made a small stochastic solver for [graph coloring](https://github.com/Armavica/coloring/) which I found performed relatively well, I would be interested to know how well it does against a CP approach! I also have a CP solver in the works but I am not convinced I managed to find a good design yet.
Most multipart libraries have ways to handle large file uploads, streaming them out to a temp file location. From my understanding, actix just treats files as normal fields, and so it's up to the user to handle this which is probably Ok for some users.
What do you mean "that's not how Try is implemented"? I'm suggesting implementing it for `Async&lt;Result&lt;T, E&gt;&gt;` and `Async&lt;Option&lt;T&gt;&gt;` as a kind of pass-through, not as a way to let `?` work on `Async::NotReady` values.
Now thàt is something I awaited.
Not everything is a set of cloud-based redundant servers with enormous power. An IoT embedded web server needs efficient TLS, which often means in-process.
Potentially :). The problem is that Rust doesn't know how long the future runs. It doesn't even know that it is finished after turning. Lifetimes are modeled on a function-by-function basis. Rust _also_ doesn't know that main is special in the sense that all data is gone at its end anyways. (which is only true on certain platforms) What it ensures is that the future is safe in _any_ case, independent of context.
I would be very interested in reading an article on how you went about that in Rust!
Being able to use Rust to do USB devices like this keyboard is really cool :) I know the microcontroller stuff has been coming along nicely but wasn't sure if we could create devices that can connect to machinese over USB like keyboards/cameras etc. Thanks for the issue link, I'm looking forward to progress on that. Is bluetooth pretty much the best option at the moment? Or are there some good options for wireless too?
&gt; luminance is an effort to make graphics rendering simple and elegant.
This reminds me: I own the HSL crate but haven't used nor updated it in years. Let me know if someone wants to have that name.
Oh, I see. The thing is that with Async&lt;Result&lt;...&gt;&gt;, you don't need to care about pass-through in futures impls.
[removed]
Exactly my thoughts. We need resources for experienced embedded developers, sure, but it would be awesome to have books that teach embedded development *through* rust. In both cases, I would expect being comfortable with rust to be a prerequisite.
Re: bumping version in Cargo.toml Not sure if this would work, as I don't know what goes into docker hash, but maybe some `sed` trick on Cargo.toml could work? (I mean resetting the main project version to '0.0.1' or something when building just the dependencies)
I may be missing something, but isn't this just basic optimization? Shouldn't Rust be able to just do it, without special type annotations?
This comes to mind: https://xkcd.com/927/
In Kotlin, it's possible to rewrite that: val something = Something() something.complexInit(123) val result = a(b(c(d(something)))) into: val result = d(Something().also { it.complexInit(123) }) .let { c(it) } .let { b(it) } .let { a(it) } Is anything like that possible in Rust, or maybe planned for the future? The main idea is to get rid of nested calls to make code more readable without counting the parens, and initialise objects in a single expression even when they don't have specific methods for that and require multiple separate calls. 
Besides the lifetime stuff, notice that this code contains a data race, since access to `Option&lt;View&lt;...&gt;&gt;` is unsynchronized. let view_available = unsafe { &amp;*option_ptr }.is_some(); if !view_available { // Since there is no view in `view`, we know that we are the first // access, so we can mutate it first let option = unsafe { &amp;mut *option_ptr }; *option = Some(self.storage.view()) } If you were to call `view` from two different threads, it would be possible for both to execute the contents of the if at the same time.
Sounds reasonable
[removed]
Thanks for the reply! I must admit I'm not very familiar with multi-threading in Rust, but wouldn't the types involved have to implement `Send`/`Sync` for you to even be able to call `view` from two different threads? And since `UnsafeCell` does not implement `Send`/`Sync`, nor will the view types...?
It's a good idea but it won't work just like that I'm afraid. The trick about docker cache is that it's based on a hash of all the new content inside of a docker image build step once that step finishes (you can see each cache hash during the builds). So simply doing something like. ADD cargo.toml RUN sed .. ... As soon as the version number changes inside of the cargo.toml (from our commit outside of the Dockerfile) that ADD step will be invalidated and the whole docker build command will start over from that point (rendering the sed command inside of the dockerfile non-effective in resolving this issue). What could maybe work (building upon your suggestion) is a setup where before we run `docker build` first extract the version number, replace the version inside of cargo.toml with a fixed value and then inside of the Dockerfile write the correct value back (it could be stored on disk and read from inside of the Dockerfile). This should be easy to script in bash or Make. I will try to see if I can get some time over and playaround with that, it is a really good idea.
 fn main() { let result = Something::new() .map_it_in_place(|it| it.complex_init(123)) .map_it(Something::c) .map_it(|it| b(it)) .map_it(a); } trait Kotlinish: Sized { fn map_it&lt;F, R&gt;(self, f: F) -&gt; R where F: FnOnce(Self) -&gt; R { f(self) } fn map_it_in_place&lt;F&gt;(mut self, f: F) -&gt; Self where F: FnOnce(&amp;mut Self) { f(&amp;mut self); self } } impl&lt;T&gt; Kotlinish for T {} struct Something; impl Something { fn new() -&gt; Self { Something } fn complex_init(&amp;mut self, _: i32) {} fn c(self) -&gt; Self { self } } fn a(v: Something) -&gt; Something { v } fn b(v: Something) -&gt; Something { v } Also [on the playpen](https://play.rust-lang.org/?gist=037d7d4c02bc4af6b26d29fec4f1ec30&amp;version=stable).
Compare `bytes.iter().position(|&amp;b| b == b'a')` with `memchr(b'a', bytes)`. Compare the regex crate benchmarks with SIMD toggled. Compare the `bytecount` crate's various routines for counting bytes. (SIMD has very broad applicability.)
Are you sure? The type doesn't implement Send/Sync, hence can't be shared across thread bounds.
Oops, I looked at the code too fast... Sorry for that!
That looks awesome!
This is great! I didn't want to write usb driver by myself so used raspberry pi and did my own keyboard (ergodox layout) on rpi zero :) Once I have time I'll try to help with usb and transform my keyboard to real embedded stuff!
Is it related with the other Tokyo rust meetups ? Will it be in Japanese or in English?
That's not a Microcontroller
An IoT device is not going to have a domain name or a fixed IP address, and it will often be the case that IoT devices run firmware that is identically cloned across thousands of devices. I don't see how such a device can P properly run an HTTPS server. No matter what kind of certificate you run, it's going to be considered invalid by every browser, giving a really poor end user experience. It's much better for IoT to act as TLS clients to a remote server than to try to be TLS servers themselves.
I'm not sure async APIs offer a big advantage if the database is within the same datacenter as the appservers. The benefit of async is that it doesn't require a thread on the appserver while it waits for the response from the database server. While threads aren't exactly cheap (several MB of address space and some scheduling and context switching overhead), I'd expect the database server to run into load issues long before the appserver thread count becomes an issue. By contrast, communicating with potentially slow network clients has little cost to the server apart from the thread, so eliminating that cost can be very beneficial. But even there it's only a big deal if you have a lot of concurrent connections. Less than a couple of hundred shouldn't be a big deal even with a threaded design, the famous problem is called C10k after all.
For redis I have a PR in queue which just needs a stable release for the parser (in ~2 weeks or so) https://github.com/mitsuhiko/redis-rs/pull/141. There is also https://github.com/benashford/redis-async-rs though it has less features atm.
I wouldn't necessarily want this just because of any perceived performance gain for communicating with the database, but rather, better interop with servers that are otherwise async. I asked a [similar question](https://www.reddit.com/r/rust/comments/7vzor4/tokio_reform_has_shipped_the_road_to_tokio_02/dtwj7ss/) a couple weeks ago, and depending on the answer to that, perhaps good interop doesn't need an async db story. I don't know.
I remember that in 1999 a few companies set up two servers, one with correct timestamps, one with timestamps set to be in 2000, and then the output of the two servers was compared in order to mitigate any possible Y2K bugs.
We’ve got some interesting stuff at github.com/Althea-mesh. We are building software to let routers pay each other for bandwidth.
sounds interesting, will check it out thanks :)
Yes. I don't think it's super clear, but it's okay. fn view(&amp;'_ self) -&gt; &amp;'_ View&lt;'a, T&gt; The potential danger is that the function deals with two lifetimes, but the type system is perfectly able to reason about pointers to pointers with different lifetimes. { let view_opt_raw = self.view.get(); return unsafe { (&amp;mut *view_opt_raw) .get_or_replace_with(|| self.storage.view() ) } } Rust doesn't have the safe cell abstraction we're looking for, but it's perfectly safe to create and use a single `&amp;mut` pointer to the interior while multiple shared exterior pointers exist. The danger would be if the closure passed to `.get_or_replace_with` could reach `self.view`. But it can't, so this is sound. If other methods call `self.view.get` you'd need to be similarly careful that they don't pass `self` or `self.view` to functions they call. The returned reference is automatically weakened to only make the lifetime and uniqueness guarantees in the signature. 
I have been playing around with this (but in Ruby). Here is a drop in hack for Postgres: https://github.com/socketry/async-postgres I did some basic tests, and found it did scale a lot better, but it's hard to really judge since it's highly use-case specific. https://github.com/ioquatix/plotty/blob/master/README.md#web-server-comparison I will be giving a talk about this in two weeks, so I'm still putting things together, but if you are interested I'll try to provide you with some more rigorous benchmarks. Suffice to say I think it probably is worth it, especially if some of the queries you are a computationally expensive.
I haven't looked to deeply into docs, but I noticed neither the index nor the README contains any example code. You should definitely add some to highlight the most common applications for your crate for the potential users.
I guess part of the issue is I haven't looked into great detail in how r2d2 works. In my use case I've written an Iron middleware that grabs a connection from the pool for the lifetime of the request. I have a suspicion that when these connections are all in use, further attempts to grab a connection from the pool will block until a connection is no longer in use. It could also be the case that the connections retrieved from the pool serve as proxies for underlying DB connections thus mitigating most of the blocking issues. I may have to go digging in the source code to find out one way or the other.
Yeah, that's a *very* specialized environment.
I absolutely agree with this comment. DB is limited resource async db driver won’t give anything. Actix performs very well in TechEmpower DB benchmarks but still uses sync db driver.
No, it's a metaprogramming approach. It would basically allow generating Rust code from Rust code, offering more domain-specific optimizations (e.g replacing matrix * inverse(matrix) by identity at compile time or writing cache size specific algorithms). 
I also really like their approach and tried to build a PoC (https://github.com/msiglreith/meteor). Turns out it's quite difficult and I currently only implemented an AST based representation in comparison to their 'sea of nodes' repr, which makes custom transformation more difficult. It seems to me that Scala's typesystem is more powerful(?) as it basically seems to allow to extend enums with new variants making custom transformation and adding nodes easier. On the implementation side I wrote an procedural macro, parsing input code and generating a new procedural macro, generating the final Rust code, by virtualizing a few language structures and specializing their implementation (nightly required!). Overall, it's quite complex writing it and side-effects are in particular difficult to handle. Debugging is often non-trivial as the macro might just error and provide no hints for the user. I haven't really done much work on it and MAYBE will pick it up in the future again but chances are low :) Would still be awesome to see something similar in Rust for high performance applications (in my case: fluid simulations or shaders).
Thanks for the explanation! Not quite sure if I follow all the details, however. First, by `.get_or_replace_with`, do you mean `.get_or_insert_with`? Second, wouldn't the code you suggested for a brief moment potentially hold a `&amp;mut View&lt;'a, T&gt;` while there potentially simultaneously also exist references `&amp;View&lt;'a, T&gt;`? (Since `get_or_replace_with` returns a mutable reference) Also, what do you refer to as 'interior' and what do you mean by 'exterior' in this case? I would perhaps expected it to be the other around (i.e. having a mutable reference to the "exterior" `Option` while immutable references to the "interior" `View` exist is OK). It would really help my understanding a lot if you could clarify! Thanks :-)
Everything is on the stack, except for the things that aren't. What I mean by that is: unless something is explicitly and deliberately on the heap, it's on the stack. You also need to distinguish between things on the heap and things that point to things on the heap. A `Box&lt;i32&gt;` is on the stack, but the `i32` it owns and points to is on the heap. Same thing with `Rc&lt;T&gt;`, `Arc&lt;T&gt;`, `Vec&lt;T&gt;`, *etc.* and the things they store. In general, owning pointer types are on the heap, as is anything which is dynamically sized. Beyond that, you have to check the definition and/or the documentation. As for returning things, whether something is on the stack or not is completely irrelevant. You can return values on the stack just fine; they get copied out of the function's stack frame into the caller's stack frame. So, no, nothing you've written above involves any heap allocations because you didn't explicitly use any.
Yeah, I'd definitely agree with /u/cthree87 -- there's plenty of space to prove yourself! Just get involved, and enjoy the ride. :) I bet it'll take you somewhere if you're proactive and patiently on the lookout!
Thank you for writing the firmware in Rust, as well as for introducing me to this keyboard! Looks solid and much more affordable than most 60%.
You'd have to ask Arm, not us :)
Naming is hard :) would you suggest any names?
Right now, we are running and examining results from tests to measure the impact of Spectre and Meltdown countermeasures on a "market basket" of representative frameworks in the "Server Central" hardware environment. These are the runs using the same Git commit as Round 15. The Azure runs are also using the same Git commit as Round 15 because we still need to publish Azure results for that round. We are also working on Dockerizing the test implementations for Round 16. Eventually the continuous benchmarker will resume pulling latest from the Git repo. Incidentally, when you view a specific run (such as [this example](https://tfb-status.techempower.com/results/cc50333a-d182-4565-a1d3-b44782a5c262)), you can click the link at the top to visualize the results.
Oh awesome, I didn't see the link for visualizing a specific run and thanks for letting me know that they are all still linked to round 15 commits.
Some minimal examples for getting started would be nice :) Looks really well documented!
Download packages, there is no rate limiting for crates.io, have fun. To get a complete list of packages, just clone &lt;https://github.com/rust-lang/crates.io-index&gt;.
Okay cool!
Thanks! I'll work on higher-level examples for sure now that many people have mentioned it.
Thanks, will do!
OK, will do.
I like filter! It is not possible to make one method for handler for all three option. I tried, but you can chose two :), fn and closure, or fn and future. impl Trait should probably help. I will check
This is a graph database I've been hacking on. It definitely still has some rough edges, but it's ready to be shared. I'd love to get any and all feedback! Some other relevant links: * Website: https://indradb.github.io/ * An example that creates a graph out of wikipedia articles: https://github.com/indradb/wikipedia-example * A python client: https://github.com/indradb/python-client 
I agree that naming is hard. I'm just not a fan of single character functions. Off the top of my head I would go with .predicate or .filter for .p and perhaps just .handle for the others assuming there is a way to overload it with 3 different argument types. 
Thanks for the clarification :)
I believe what is missing here is comparison with existing uniform interface solutions of gfx pre-ll, glium, vulkano, etc
Nice. It'd be nice to have an enumeration for requests (possibly with a "catch-all" `Custom(String)` variant for unsupported/new variants). As for feedback, adding a link to `docs.rs` from your `Cargo.toml` makes `crates.io` link there for you.
Oops, yes, you're right about the method name. I'll edit it in a bit. --- &gt;Second, wouldn't the code you suggested for a brief moment potentially hold a `&amp;mut View&lt;'a, T&gt;` while there potentially simultaneously also exist references `&amp;View&lt;'a, T&gt;`? (Since `get_or_replace_with` returns a mutable reference) Good eye. If the following stars align there is a data race: - `View` is both `Sync` and has fields unprotected by a Cell. - there exists one thread which calls `Deferred::get`, passes `&amp;View` to another thread, and calls `Deferred::get` again within the lifetime of the first `&amp;View.` - machine code is generated which, in the case of `&amp;mut Some(View{..})`, makes a speculative write and undoes it. The details of Rust's memory model aren't stable enough to fully explore 1 and 3. Currently any `UnsafeCell` field with a type protects the entire type from speculative writes through a pointer. This protection is in effect whenever UnsafeCell is part of the type of the pointer. But other implementations could conceivably weaken this protection. So the data race could possibly be observed on a non-cell field of View. Conservatively we have to assume that 3 is possible. It seems crazy to speculatively write to a live location through a pointer, but `&amp;mut` in principle could allow it and sometimes optimizing compilers do crazy things. With that in mind it's more cautious to check the option using only a &amp;-ref // If None, no View references have been returned yet. Test and set are atomic because Deferred is !Sync if (&amp;*view_opt_raw).is_none() { *view_opt_raw = Some(self.storage.view()); } return (&amp;*view_opt_raw) .as_ref() .unchecked_unwrap() .... unsafe impl&lt;'a, T&gt; !Sync for Deferred&lt;'a, T&gt; {} Which is basically your original approach. Writing through the raw pointer is legal within the `if` branch. It does invalidate the reference taken by the first `&amp;`, but that reference isn't reused. It looks like you were taking care to extract the `bool` and let the lifetime of the reference expire, but it's not necessary. References are dead after the last dereference, only smart pointers linger until dropped, and unsafe Rust can take advantage of that. `unchecked_unwrap` from the `unreachable` helps the optimizer. If `Option::as_ref` is inlined the optimizer can see that the None branch is undefined it can just prune that branch. This is cheaper than the forward reasoning that the Option discriminant was tested for Some, set to Some if not, this was exhaustive and no aliasing occurs, therefore the discriminant must still be Some regardless of the branch, therefore the panic never happens. In trivial cases where you're writing unsafe anyway, you may want it. --- &gt;Also, what do you refer to as 'interior' and what do you mean by 'exterior' in this case? I would perhaps expected it to be the other around (i.e. having a mutable reference to the "exterior" `Option` while immutable references to the "interior" `View` exist is OK). I mean the interior and exterior of the cell. Other wrappers have inner types (`into_inner` is a common method name) but the interior and exterior of cells are special because you may alias pointers. With Option or any wrapper type including UnsafeCell, it's legal to borrow a shared pointer to the inner type from a unique pointer to the exterior type. You can't alias them - the `&amp;mut Option&lt;T&gt;` must be inaccessible during the lifetime of `&amp;T`. But you can alias references to the exterior and interior of a cell. `a: &amp;(String, UnsafeCell&lt;T&gt;)` and `b: &amp;mut T` may point into the same tuple, as long as you don't read or reference `T` through `a` while `b` is live. And going through `a` requires an unsafe dereference. 
I'd looooove to see some better approach for a report format that doesn't depend on sending scancodes over the line and hoping that the operating system can figure out what the user actually wanted later. The available page usage tables of the USB HID spec are pretty terrible in this regard, especially for keyboards that are meant to be modified and programmed. Did you have any thoughts about this?
Hmm, looks close to useful, but I think it would need a way to output just some text instead of doing AST transforms. For now I'm just going to use `r"Regex::new\((r?".*")\)` to scrape for the regex.
Now, all the build failures are the same problem: MAKE: /opt/src/rust-1.24.0/bin/rustc -C opt-level=3 -C target-cpu=core2 -C lto -C codegen-units=1 -L /opt/src/rust-libs regexredux.rs -o regexredux.rust-3.rust_run error[E0460]: found possibly newer version of crate `lazy_static` which `rayon` depends on --&gt; regexredux.rs:6:1 | 6 | extern crate rayon; | ^^^^^^^^^^^^^^^^^^^ | = note: perhaps that crate needs to be recompiled? = note: the following crate versions were found: crate `lazy_static`: /opt/src/rust-libs/liblazy_static-15d84378f10b5619.rlib crate `lazy_static`: /opt/src/rust-libs/liblazy_static-3fdf2fb1a5985b8c.rlib crate `lazy_static`: /opt/src/rust-1.24.0/lib/rustlib/x86_64-unknown-linux-gnu/lib/liblazy_static-ed2972774c4bf49a.rlib crate `rayon`: /opt/src/rust-libs/librayon-276aba312e245652.rlib 
Alternative clickbait title: _Cargo examples considered awesome_ :) This post is basically a pretty in-depth description of the rare Cargo feature which is the automatic management of example code. I'm explaining why examples is of great help for both you as a library author &amp; your users, and go through common use cases you may encounter when writing examples for your crates.
In my case, we have a rust library that we want to use for dealing with units... And now we can use that same library on our backends (Ruby/Rails), mobile (iOS), and on our frontend apps, all just with some wrapper/glue code.
Well, with the hardware on the Anne Pro it's either bluetooth or USB. And bluetooth is much simpler, as it's handled by an dedicated chip and you just talk to it via UART.
What would you change? Right now it goes from keyboard scan matrix to HID codes, and then in the OS typically to whatever the native format is. Would you want more expressive scan codes?
I have an optimization question! I got into the [Ackermann function](https://en.wikipedia.org/wiki/Ackermann_function) the other day by accident, and after trying my hand at implementing it in Python a couple different ways (mainly with naive caching), found a very interesting [memoized Haskell](https://stackoverflow.com/a/13088510/3023252) on Stack Overflow. I wrote it up, tried it out, and it worked great! But at higher numbers it really lagged so I took to Rust to see what I could cobble together. After wrestling with the differences between Rust and Python, [I finished it](https://gist.github.com/NoahTheDuke/f14f79d04f1287b0ac594e89c2609b1f#file-ack-rs), and it worked significantly faster than my Python version, which was quite pleasant. But now I suspected that it was the algorithm I used, not the implementation. So I did more research and found [this research paper](https://www.semanticscholar.org/paper/Optimizing-Ackermann's-function-by-incrementalizat-Liu-Stoller/1ad5a6dd8fd4787a3cc1696914f9c7ba7b23f03b) (which you can view by looking it up on a certain hub for science articles), which contained a super clever algorithm, the full version of which is the **aOpt** and **aUse'** functions written at the end. I translated it to Python (the language I think in), and the satisfied with the speed increase did the same in Rust, [here](https://gist.github.com/NoahTheDuke/f14f79d04f1287b0ac594e89c2609b1f#file-a_opt-rs). My thoughts/questions: * I like that in Python I can just return a `(1,)`, because I know that the second and third fields in the tuple won't be touched at that point in the algorithm. I couldn't figure out how to do the same in a tuple in Rust. Is it even possible? * Using a struct made a lot of sense, but it was weird to have to `Some(Box::new(v2))`. Can this be avoided in some way? Recursive struct fields have to be wrapped, I know, but it feels like I'm wasting time/memory doing it. * My apologies for `f, s, t`, I wanted to mirror the paper's functions **1st()**, **2nd()**, and **3rd()**. What are the best practices for struct field names and calling? * Are for loops the best way of doing [this](https://gist.github.com/NoahTheDuke/f14f79d04f1287b0ac594e89c2609b1f#file-a_opt-rs-L44)? Would `(1..(i + 1)).fold(Record::new(1, 0, None), |v, k| a_use_p(k - 1, 1, v))` be more "Rustic"? It seems sliiiiightly faster, but that's hard to tell. * I can't wait for when `..=` is in stable. It's ugly, but it looks so much better than `1..(i + 1)`.
Yeah I've been checking deps when I generate the download list.
&gt; It definitely still has some rough edges lulz
Thanks! There's no equivalent way to do that for vertices yet, but that makes sense if only for the sake of API symmetry. If you're looking for walking _all_ of the vertices of a certain type, you can use map/reduce jobs. It's not well documented since I just rolled it out, but they're available when running the IndraDB server (rather than hooking in directly with the library like the gist you have.) The server has a lua-based scripting layer that map/reduce jobs use (thanks to the awesome [rlua](https://github.com/chucklefish/rlua) package.) For example, if you were looking to count the number of vertices of type `foo`, you could define a script `count.lua` like this: function map(vertex) if vertex.type == "foo" then return 1; else return 0; end end function reduce(first, second) return (first or 0) + second; end return { map=map, reduce=reduce } Then, when the server is running, hitting this endpoint would return that number of vertices: POST http://localhost:8000/mapreduce/count.lua 
Thank you for writing this up! I think I'm following along for most of it, I just need some time to digest it. I suppose the nomicon is a good resource for this kind of stuff - is there other literature you would recommend? In any case, it sounds as if I'm on the right track with my idea. It's also clear to me that if I come up with anything actually useful (which is the goal of my project, of course), I hope I can find some help in the community for reviewing these abstractions, because it seems very easy to get tripped up by some small detail.
Dealing with units? Can you explain a little better, please?
I am just loving the idea of finding good APIs using examples representing the anticipation of real use cases. This blog post is the new reference for how to use and grow examples, and why (to me anyway).
How was your experience using lua as a scripting language to interact with a rust program? 
I would be more than glad. I think the current implementation runs into deadlocks, the abundance of Mutex based synchronization doesn't do well. If it helps, you don't have to keep the underlying interface unchanged. Actor/Future based support is targetted for the next major release so any breaking change can be adjusted for in other implementations.
I’m not a web programmer or machine learning person these days, but I’d say do the smallest thing you could possibly think of - anything at all. Could be a to-do list app with ugly HTML forms for a frontend. Could be a service where you upload images and apply some ML to them. The point is to get your feet wet with the technology. As you progress in the project, you’ll inevitably get bigger ideas for the next project - and since it’s so limited in scope, you’ll get the satisfaction of actually completing it. That’s what I’d do!
I can't recommend [rlua](https://github.com/chucklefish/rlua) enough. It's a brilliantly designed library and makes the integration a breeze. I've also learned a lot of rust by poking around at its source code.
I had some thoughts about it. The big issue is that one would basically need to implement support for a better report format a) in micro-controllers b) in the operating system c) in the USB HID spec and there is a lot of inertia with that. The idea I had was that instead of having the 8-byte report format of ... |modifiers| empty | key1 | key2 | key3 | key4 | key5 | key6 | ... key1 to key6 are transmitted as scancodes, one would have an 32-byte report format like ... |mod_left|mod_rght|char0_1 |char0_2 |char0_3 |char1_1 |char1_2 |char1_3 |char2_1 |char2_2 |char2_3 |char4_1 | etc. ... where charX_y is a 3-byte representation that either - (upper bit 0) transports the 21-bits of an UTF-32 codepoint, or - (upper bit 1) uses the first byte to select a page usage table, and encodes a value of the selected table into the other 2 bytes. This way the keyboard could directly send the desired unicode character to the operating system (making keyboard layout selection obsolete), while also supporting all the existing use-cases and page usage tables.
Sure... Some time ago, my company landed on using the [UCUM](http://unitsofmeasure.org/ucum.html) spec for standardizing unit terms for computer consumption. Since we provide APIs for our own frontends (ember, iOS) as well as customers, we needed to pick something standard. Since we work in Ag, we also deal with a fair amount of non-standard units (eg "10 acre-inches of water per acre")--the UCUM handles that just fine. Anyway, we'd been using the [unitwise](https://github.com/joshwlewis/unitwise) for our Rails backends, but didn't land on any solutions that we've been happy with for all of our non-Ruby apps. I ported unitwise to Rust so we could use it in each of our apps. The plan is to eventually open-source it, but I want to get more usage under our belts before putting it out there. Do you have any specific questions?
But you can run db connections in separate threads and communicate via queue. There is nothing special in actix that prevents you from using similar approach
Join actix Gitter chat and ping me and we can discuss architecture.
Congrats, and very excited to see static file serving on the roadmap for 0.3!
I’d love to discuss that with you over IRC or gitter, especially for `gfx` (pre and both your low level layer). I may have a talk with tomaka about `glium` and `vulkano` as well.
Currently /u/coder543 has summed up our position entirely, using nginx as a reverse proxy is how I've personally deployed a Gotham app over https in production. Longer term I'm certainly keep to see something here, we'd likely look to work in with (or flat out just use) whatever hyper decides to do. That may be [hyper_tls](https://docs.rs/hyper-tls/0.1.2/hyper_tls/). If you'd like to log a [tracking issue](https://github.com/gotham-rs/gotham/issues) that would be helpful for planning purposes.
Nothing yet as /u/forbjok correctly pointed out however you might be interested in following along on [this issue](https://github.com/gotham-rs/gotham/issues/11) which is discussing this space.
So are we, some unique challenges to solve there, looking for as much help as possible!. [Here is the tracking issue](https://github.com/gotham-rs/gotham/issues/55) for those interested in following progress or getting involved.
&gt; I like that in Python I can just return a (1,), because I know that the second and third fields in the tuple won't be touched at that point in the algorithm. I couldn't figure out how to do the same in a tuple in Rust. Is it even possible? Not directly, because a tuple in Rust is of a statically known size. If the trailing elements weren't supplied, then they'd have to be some uninitialized value, and so accessing them would be Undefined Behavior. Forcing everything to be initialized is one of the ways Rust avoids UB in safe code. In cases where you can't supply some simple default value like `0`, you could use a `Vec` or an `ArrayVec` to represent the variable length. &gt; Using a struct made a lot of sense, but it was weird to have to `Some(Box::new(v2))`. Can this be avoided in some way? Recursive struct fields have to be wrapped, I know, but it feels like I'm wasting time/memory doing it. It might make more sense to think of this backwards. Python has to box everything. Think about how much time and memory you save in all the places where you _don't_ have to box :) If you're really worried about the individual heap allocations here, you could consider making a single huge array of records, and then having each record refer to its optional predecessor by index. (I think the story here is basically the same as it would be in C++.) &gt; Are for loops the best way of doing this? I'm a big fan of `for` loops myself, compared to functional fanciness. I'd say either way is Rust-y. &gt; Is there a way to parallelize this? Or any part of it? I don't know enough about the Ackermann function to say, but if there is, there's a good chance that the `rayon` library is an extremely convenient way to do it.
This was very interesting!
Hey! Than you for noticing this :) The macro usage errors were not great and that was frustrating for nom users, `compile_error` was a great help!
I can think of three approaches for type-assisted shader i/o hookup: 1. User defines all the bunding points by index upfront. If backend allows shaders to specify locations, it only needs to check that those match. Otherwise, it needs to enforce the locations at shader linking stage. 2. User specifies names. Backend reflects the shader and builds a mapping scheme. This is done in gfx pre-ll and luminance. 3. User specifies nothing, the types are generated at compile time from the shader reflection. This is the path of glium/vulkano.
Does your [rlua](https://github.com/chucklefish/rlua/) implementation overlap with any other project's implementation, eg [way-cooler](https://github.com/way-cooler/way-cooler)'s?
&gt; Once module becomes too noisy — it gets extracted into the separate crate. By noisy, you mean a radio analogy, right? Would it make more sense then to say "dense"? https://en.wikipedia.org/wiki/Spectral_density I love this methodology for how it helps with code organization and progressive refactorization, which leads to fully [structured programming](https://en.wikipedia.org/wiki/Structured_programming). One example project I've seen that headed in this direction was a [Rust code diagramming](https://github.com/adjivas/ml) project, which also could lead toward even better diagrams that [Infer component groupings based on folder structure](https://github.com/adjivas/ml/issues/3) if this became an idiomatic pattern.
This looks awesome, thank you so much! Excited to try it out later today.
[Yeah.](https://imagesvc.timeincapp.com/v3/mm/image?url=https%3A%2F%2Fcdn-s3.si.com%2Fimages%2F1987-Randy-Macho-Man-Savage.jpg&amp;w=700&amp;q=85)
I just thought of something: if we could use [literate programming](https://en.wikipedia.org/wiki/Literate_programming) to write the examples, `cargo doc` could put them in the generated docsite as tutorials for the library!
&gt; For similar reasons, C++ exceptions use SEH Only on x86; fortunately x64 is far more sane. :-]
It may require implementing a special trait or something. /u/tomaka17 can clarify
I thought the patch to libgit2 explicitly required the new protocols, and therefore, didn't need anything else? That key is for the defaults, but the libgit2 patch forcibly asks for TLS 1.2, rather than just the default. Right? I'm willing to be wrong here...
Ok, so, assuming the libgit2 patch does that, then that's fine. I also went and checked what the change to cargo was doing specifically, and it points people to the same page I linked, so that's fine. The only issue is the implication in the post that you *just* need to install security updates to fix the issue, which I know from personal experience doesn't necessarily work. I saw "suggesting that they upgrade their Windows version" and assumed that was what the warning actually said. Given how old the patch is, it's highly likely almost everyone on Windows 7 already has it. If they're experiencing issues, it's probably because they're missing that key, which updating further won't get them.
Ah, right. Thanks.
I know the survey will cover some of this but what are some areas you see could improve?
**ATS (programming language)** ATS (Applied Type System) is a programming language designed to unify programming with formal specification. ATS has support for combining theorem proving with practical programming through the use of advanced type systems. A past version of The Computer Language Benchmarks Game has demonstrated that the performance of ATS is comparable to that of the C and C++ programming languages. By using theorem proving and strict type checking, the compiler can detect and prove that its implemented functions are not susceptible to bugs such as division by zero, memory leaks, buffer overflow, and other forms of memory corruption by verifying pointer arithmetic and reference counting before the program compiles. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Graph database humor.
I think there are some fairly good options in rust if you want to piece meal things together (caveat: I haven't evaluated any crates since mid 2017). But the single library that stands out to me as a "batteries included cross-platform cli framework" is python's [click](click.pocoo.org/). The single best thing about click, is that it nudges you into best practices for writing clis naturally. You don't have to think about edge cases very often. An obvious benefit is the built in argument parsing, including mapping sub commands to different handling fucntions with python decorators. One more thing that comes to mind is the abstractions over user input like the [confirmation](http://click.pocoo.org/5/prompts/#confirmation-prompts). At first glance, these seem fairly trivial to implement, but what's nice, is that I don't have to think about possible complexity for something like this because the library is mature and handles it. Not to mention, the api is easy to use, and easy to remember how to use. 
'Nomicon, and really try to diagram it out and break things. Jeff Preshing has some really fantastic blogs that introduce inter-processor memory ordering (in C++ but Rust just cribs those rules). I'm still learning and still breaking things. For example, there's the funny edge case that `&amp;mut UnsafeCell&lt;T&gt;` allows the *replace* operation, which writes to otherwise protected memory, so you can't alias it with an inner pointer. (Borrowing the outer `&amp;mut` pointer is fine.) This also suggests that a `VolatileCell` type is just completely unsound. But a `HwRegisterPtr` which implements volatile read and write methods (and not Deref!) is fine and dandy. So `mut AnyKindOfCell&lt;T&gt;` should be a huge red flag if it occurs in a function which contains `unsafe` manipulation of that cell. Yet it's not a hole in the soundness of safe code because any safe cell *must* tie its inner operations to a borrow. *Weird.* (Why is `into_inner` unsafe? Why is `UnsafeCell::get` not called `UnsafeCell::as_ptr`? Why isn't there a `fn(&amp;self) -&gt; &amp;mut T` method? `&amp;*unsafe_cell.get()` is a bit of a foot-gun if you don't bind the lifetime properly - I mean it's not *horrible,* but ideally it would only be used in return value position.) 
Best Rust intro I've seen. 10/10
"Always write the usage code first"
Yep, the solution is to use Make: https://github.com/rust-lang/rust/pull/48572/files#diff-c26d7fbdd8c68ae7fb71184a71f50a2a
Yup, aware of ATS, but its ergonomics leave a lot to be desired. Could be useful for inspiration though - it has lots of great ideas, and was an inspiration for Graydon when he was first building Rust as well.
There's one additional reason to desire an async driver: latency. If I'm using an async http server, I can start start mapping the response database stream into html as bytes arrive and start rendering the page on the client.
The `rust-csv` crate actually pretty much does this. Tooling support would be fantastic.
that requires time, but i'll try, but can't promise 
You should perhaps let us know why you think this is relevant to Rust and what we can learn from it (not watching now, kid's asleep).
Ooh yeah, whoops! Sorry for that lack of clarification! That's like saying we have a library that deals with "data".
Argument parsing is typically done via `clap` or possibly `structopt`. The latter builds on the former. This helps you quite a lot already and gives a nice, colored help output and can even generate shell completion scripts for you. I know there is a library that has some prompting functions: [dialoguer](https://docs.rs/dialoguer/0.1.0/dialoguer/). I haven't used it, but it's by mitsuhiko, the same author as of click. He has a few more cli libraries.
[removed]
Have you looked at something like [libloading](https://crates.io/crates/libloading)? You can't really 'link' at runtime, you have to actually load the library and functions using sys calls.
This really confuses me. `dll`/`so` require to have _all_ symbols be present when built, as _some_ OS's will run `.so` code in a different virtual address space (as every loaded `so` should *ideally* have 1 copy in physical memory). You can pass static pointers into `so`'s to be called back (on some OS's other require additional calls to ensure that pointer will valid across virtual memory transition), which maybe functions, globals, arrays of functions. 
This isn't true. There's a page on MSDN titled [Exception Handling (x64)](https://msdn.microsoft.com/en-us/library/1eyas8tf.aspx) that says "This section discusses structured exception handling and C++ exception handling behavior on the x64." SEH and ISO C++ exception handling are two different things.
Thank *you* for giving us this great library, and for writing up great documentation!
OK，I'll check the source code, thanks for reply
I'm confused. &gt;Existing C++ code which provides a C interface to plugins. `C++` exposes a bunch of `extern C` symbols. &gt;Plugins load into the C++ code as shared libs (so/dylib) `rust` plugin is a `.dll`/`.so` that is loaded by a `C++` application. Which _had_ some `extern C` symbols before it was compiled. &gt;The C/C++ code can run plugin functions (e.g. init(), call()) `c++` application calls into the `rust` plugin `.dll`just like it was a `c`, or `extern C` interface. &gt;But the plugins can also call C functions Wait.. As in the `extern C` API that is part of the underlying C++ program? You can't do that. There is _magic_ happening when your application is loading the plugin. It is passing in a list of call backs. Likely some library/headerfile in the `c`/`c++` library that normally builds the plugin is making this appear as their just static functions. The best example of this I can find the [`JVM-TI`](https://docs.oracle.com/javase/8/docs/platform/jvmti/jvmti.html#jvmtiEnvAccess) which is a `c++` application with very well defined internal API. 
&gt; You can't really 'link' at runtime, you have to actually load the library and functions using sys calls. /u/aggrolite appears to be referring to what the ELF loader does to resolve dynamic linkages. That said, looking at the overall description, it sounds like the intent is to have some C++ code `dlopen` the Rust library, then have the Rust library call symbols defined in the C++ code. (If that's the case, then the problem is essentially that /u/aggrolite wants to call a function defined in a parent scope that the Rust linker is unaware of because it's introduced by `dlopen`ing the Rust code.)
Very cool! I might have to find an excuse to spike with this at work. One suggestion I would have: as someone whose job involves map/reduce jobs and a graph DB that is too large to do full graph traversals, I'd maybe try to make it clear that this is not using the Hadoop framework. In fact, I'd probably suggest using a different name for this feature to be less misleading. I got really excited that it had the capability to do "map/reduce jobs" only to discover that that just meant "parallel traversal on one machine."
I say this as someone who's pretty pessimistic about graph databases, having tried several that underperform PostgreSQL, and I hope it's not too harsh. I'd be excited to use a performant, straightforward graph database, and Rust seems like a likely way for one to appear! But I'm not excited for a database that pretends that map-reduce is a sufficient replacement for real features. Running map-reduce inside your database has always been a terrible idea. There's [even a comic about it](http://b.z19r.com/post/did-you-just-tell-me-to-go-fuck-myself).
To me, I always need a configuration file. In fact I usually need that there can be a configuration file by default which can be overwritten by a file in the home directory which can be overwritten by a file in the current working directory which can be overwritten by arguments passed to the tool. That would be an extreme case, I do not think I've needed all of them together but a combination and always the overwrites on that order.
Thanks so much for the effort! Yes, now is really clear!
You are my hero!! Yes, such great PRs!!! I hope they will get reviewed soon. Yeah, my late-night brain can't write parseable English, it means what you thought it means. It's like regex, so helpful and you do not need to know much about a language to get to use its regex library. 
Got you, thanks!
Vec's going to be faster to access and create than a recursive box because yes, it is homogeneous. If you have a known maximum size, `ArrayVec` is going to be even faster since it's stored on the stack and doesn't allocate at all (it's a wrapper around fixed-sized arrays to allow for uninitialized elements safely).
One thing I noticed is that your `#[should_panic]` methods show the last line of the methods as not being covered. This is correct! If you have the following code: #[should_panic] #[test] fn test_panic() { panic!(); } Then the closing `}` won't have code coverage, because you were unwinding at that point (because of the panic). So when you have `#[should_panic]` tests, you'll never get 100% code coverage on your tests themselves (I've had similar disappointment in c# with the `ExpectedException(...)` attribute on unit test methods). I'm not sure why `Vigenere::encipher` and `Vigenere::decipher` show up as not having full coverage though. That looks like a bug to me.
What do you mean? How is a graph store using lua bindings related to a window manager?
`glium` will introspect the shader at runtime. In other words, option 2. `vulkano` priviledges option 3, but the library also makes it possible to pass information at runtime. Option 2 is not implemented, but could easily be plugged in the design of the library.
Good talk ( programmers should hear this info at least once ). But you are probably conflating two ideas. Rust compile time mutex principle ( ownership model ) is not the same as threads/core mutex. 
Thanks again. This discussion has really helped me!
It would have to be target specific. In addition to static analysis, it would have to account for dynamic dispatch, func ptrs/closures and also interrupts nesting &amp; stack realignment etc. 
Indeed, and a hardfault is not guaranteed as the [memory space](http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dai0179b/CHDDBAEC.html) above and under the ram space may not be invalid.
Not specifically related to IndraDB, but here is some reading about how well some "graph on RDBMS" from the research community look. https://github.com/frankmcsherry/blog/blob/master/posts/2017-09-23.md#paper-2-graph-processing-using-sql
Are you too wondering why the hell you should click on this link because the title says absolutely nothing? No problem, I've got your back! &gt; Fast Search Through Metric Spaces with Rust and BK Trees &gt; &gt; […] scan our picture library, and calculate a 64-bit perceptual hash of every image […] How can we use those hashes to find similar photos in our album?
[ggez](http://ggez.rs/) is good if you're looking for a simple-to-use graphics / game library. [glium](https://github.com/glium/glium#glium) is a safe, minimal-overhead OpenGL wrapper, if you have knowledge of using OpenGL. [piston2d-graphics](https://github.com/PistonDevelopers/graphics) is... somewhat inbetween. It's what I built my rust GUI with, and it has a fairly high-level graphics API to work with. It's also less "whole framework" than ggez, as it does not implement sound or 3d graphics, just window management and 2d graphics. [Examples here (separate repo)](https://github.com/PistonDevelopers/piston-examples).
&gt;Do not abort when unwinding through FFI (this reverts behavior added in 1.24.0) I am not affected by this, but in my opinion such change would demand minor version change, not just a patch version.
I'm writing my first rust library. Initially it's for use in a C++-application (that is undergoing rewrite to rust). However, if the lib turns out well, it might get it's own crate. (It's a storage and checksumming engine for storing blob:s with hashtree-info, might be useful for someone else) Now I'm wondering on the use of regular std::error::Error vs Failure. Which would make most sense for a small(ish) library? Failure seems nice, but I'm not sure it's a good idea to impose the dependency onto someone else wanting to use the crate?
SEH is the underlying unwind mechanism on Windows for pretty much everything, like a libunwind-compatible mechanism generally is on Unix. Did you know that you can intercept longjmp unwinding in GCC C++ by catching a special marker exception type? (At least, you could at some point.)
Yeah, the one thing missing is something that abstracts configuration over every possible input. Environment variables, then global configuration, then user configuration, then local configuration, then command line arguments. Such an abstraction could easily leverage existing crates specialized in some of those areas.
I don't understand how i would achieve pattern macthing, e.g. "give me all edges of type 'foo' incoming to vertex 'Bar'" using [EdgeQuery](https://docs.rs/indradb-lib/0.12.1/indradb/enum.EdgeQuery.html). `Edges ::keys` expect a full `EdgeKey` so the vertex IDs are not optional. Can you elaborate?
Yeah I remember we talked about that introspection. In `luminance`, I have a mapping like /u/kvarkus described, yet, I don’t like how it’s managed for buffer types (people have to pad them by hand, which is very silly).
`Edges ::keys` in [EdgeQuery](https://docs.rs/indradb-lib/0.12.1/indradb/enum.EdgeQuery.html) expects a full `EdgeKey` so the vertex IDs are not optional. Can you explain how you would do pattern matching similar to Jena's [find()](https://jena.apache.org/documentation/javadoc/jena/org/apache/jena/graph/Graph.html#find-org.apache.jena.graph.Node-org.apache.jena.graph.Node-org.apache.jena.graph.Node-) where every component is optional?
`Edges::keys` in [EdgeQuery](https://docs.rs/indradb-lib/0.12.1/indradb/enum.EdgeQuery.html) expects a vector of full [EdgeKey](https://docs.rs/indradb-lib/0.12.1/indradb/struct.EdgeKey.html), where no component is optional. Can you explain how you would do pattern matching similar to Jena's [find()](https://jena.apache.org/documentation/javadoc/jena/org/apache/jena/graph/Graph.html#find-org.apache.jena.graph.Node-org.apache.jena.graph.Node-org.apache.jena.graph.Node-) where every component is optional? For example: "give me alle edges of type 'foo' betwen VertexA and VertexB".
That would not really make any difference though? Ideally such a change should never happen during the 1.x series as correct code depending on the 1.24 behaviour now results in undefined behaviour with 1.24.1.
&gt; rustfmt does it magically make dots evaporate?
Can you explain how you would do pattern matching similar to Jena's [find()](https://jena.apache.org/documentation/javadoc/jena/org/apache/jena/graph/Graph.html#find-org.apache.jena.graph.Node-org.apache.jena.graph.Node-org.apache.jena.graph.Node-) where every component is optional? `Edges::keys` in [EdgeQuery](https://docs.rs/indradb-lib/0.12.1/indradb/enum.EdgeQuery.html) expects a vector of full `EdgeKey`, so no component is optional. For example: "give me alle edges of type 'foo' betwen VertexA and VertexB".
There is also https://static.googleusercontent.com/media/research.google.com/de//pubs/archive/43287.pdf
Do you plan to improve on the [*_vertices()](https://docs.rs/indradb-lib/0.12.1/indradb/enum.EdgeQuery.html#method.outbound_vertices) functions? Currently the user gets no hint whether there are more vertices. Also an iterator over all vertices would be nice.
Whatever it was, [it seems to have happened on the 15th of December last year](https://github.com/rust-lang/crates.io-index/commits/master/io/n-/ion-shell).
Nice, great starting point! In memory data storage seems simplistic (I know it is a design choice but ... :) ), it might be beneficial to introduce some concurrent data structure underneath. As far as I can see, currently it is BTreeMap with RWLock around it. ART and BWTree are good choices, SkipList can provide throughput, but latency is crap. Crossbeam might have something already implemented.
You can, http://laze.rs/ is generated from source written in literate style using https://github.com/pnkfelix/tango. (It's unfinished, though I would enjoy to pick it up again)
There's a section later on in the video where he goes over how a CPU manages multiple copies of data shared between the caches and main memory, and how a data race can happen. I assume that's what the OP thinks is relevant.
Can't you do libc::dlopen(NULL, flags) to get a handle for the main executable, then use libc::dlsym() ?
I believe it provides some good background on how modern hardware behavior demands more accurate and safe use of concurrency in order to fully exploit the hardware capabilities. For many, who do not normally think at the hardware level, this talk explains a lot of the issues in detail. For many others, this might be obvious, but, it seems that a lot of those who question why Rust's memory safety is special, don't fully comprehend these issues. I posted it here because I felt it relevant.
It's relevant because it explains why it is important to have safe, efficient concurrency because of the way modern hardware is constructed and behaves. If we just had one super-fast core, and we could continue to get faster single-cores indefinitely, then the whole notion of concurrency wouldn't be very important. This explains the physical limitations of the hardware and how that forces us to think about different programming models where safe concurrency is probably the most important factor in getting maximum performance.
Well spotted, thanks!
You can, for the most part, smush everything down into a single argv and then let clap deal with merging the configuration. (clap recently gained the ability to permit self-overriding flags, which makes this rather convenient today.)
Seeing this book published makes me very happy. Julia is a very impressive person, that had just recently published a book when she told me she was writing another book, this time on Rust. Having good quality tech books written in Portuguese, that are not translations, is something unusual in Brazil. And I get excited to see Rust being picked up! Congratulations for the book Julia.
Yeah, we only need one lib to abstract all this so all the parameters are only described once.
Looks nice!
I got a name resolution error
A bit late, but you might want to tell about the registry fix needed too. That's the thing that fixed it for me too.
Looks like `structopt`s [subcommand support](https://docs.rs/structopt/0.2.4/structopt/#subcommands) doesn't automatically call functions on the struct. Mind [opening an issue](https://github.com/TeXitoi/structopt/issues) on that? Is there value in having built-in prompt vs using dialoguer like the other person mentioned?
Does [`config`](https://github.com/mehcode/config-rs) handle your needs? It looks like it can read from multiple files and the environment and merge it all together.
Thank you I will post it there
For anyone that is curious the tittle of the book is: Functional and Concurrent Programming in Rust. Here is my attempt to translate the book's introduction: This book presents Rust in a way which makes functional programming inherent to development, while also identifying one of Rust's greatest strengths, concurrent programming. To achieve that, this book is divided in four parts. In the first part, we have a sum up of Rust's potential and its history. We will go through what makes Rust a great option, its main features, and we will also show briefly how TDD can be done in Rust. In the second part, we will show Rust from a functional perspective, comparing it to Closure. Our focus will be mainly in functions, traits, iterators, adapters and consumers. In the third part, we present concurrency in the various ways that Rust offers, such as the creation of threads, data sharing, and data transfer though channels. In the last part, we will present a summery of the previous parts in the context of 4 HTTP frameworks. Two high level ones (Iron and Nickel), one low level framework (Hyper) and a asynchronous one (Tokyo).
Por essa não esperava ;) 
I created [img_hash](https://github.com/abonander/img_hash) as a pure-Rust alternative to pHash with a more permissive license (MIT vs GPL). I'd like to see the author compare the two libraries because I have very little data on how accurate my implementation is. I made an attempt to create a VP-tree to compare the hashes of images but it's incredibly slow to build and search. I wonder if the BK-tree performs better.
Nossa, que massa! Agora posso matar as saudades do Brasil e escrever código na minha língua preferida *ao mesmo tempo*. :)
Sounds like you're looking for `dlopen` (`LoadLibrary` on Windows). What you do is open the library at runtime (using the `libloading` crate) then ask for the symbols of whatever function you want (as a function pointer). I do effectively the same thing at work, just in the other direction. Our application uses `LoadLibrary` to "link" to a Rust module at runtime, then call various functions from the library. I'm also writing [a guide] on doing this sort of FFI thing (shameless plug). It's been a while since the last update, but it should cover most of the concepts you'll need. [a guide]: https://michael-f-bryan.github.io/rust-ffi-guide/
The Book is really good. It will probably repeat some things you already know from C, but it's still really good.
The core language doesn't know what a heap is. The standard library exposes four types that allocate and free heap memory. -`Box` each Box points to its own memory allocation of fixed size - `Vec` unique like Box but holds variable-size `[T]` data and automatically reallocates to grow - `Arc` shared, reference-counted, fixed-size, may be thread-safe - `Rc` like Arc but enables optimizations which are not thread-safe Other types may use similar memory allocations. `String` is like `Vec` but for `str` data. Hash tables use something like ` Vec` internally. When you define a `struct`, the fields may be stored anywhere the user of that type wants to put them.
The Book is free, so buy "Programming Rust" and read both ;)
I think accessing symbols from the calling binary will end up being different across platforms, if that is important to you. One possibility that is a bit more work is to make your API pass a struct to the init() call that contains pointers to all of the functions in the top level binary that you intend the shared object to be able to call.
clap does all that already minus configuration files. But like BurntSushi said, if you can take take the config files and smash their values down into an argv it all works. I've got an open issue to allow one to more conveniently add "additional argv"s so the process is a little more ergonomic than just smashing everything into a single argv.
&lt;3
Oh this is interesting. One of the optimizations that IndraDB's postgres implementation has is the use of common table expressions for multi-hop queries. This speeds things up, but the postgres implementation is still _orders of magnitude_ slower than the other two implementations in microbenchmarks. I'll have to sit down with this stuff and see what other optimizations could be thrown in though.
Sounds like a interesting book, any chance this might be translated to English in the future?
For posterity, that method is for building a `VertexQuery`, which you would then pass into [`get_vertices`](https://docs.rs/indradb-lib/0.12.1/indradb/trait.Transaction.html#tymethod.get_vertices). `get_vertices` will return a `Vec`, so you can get the count there. Do you think it would be cleaner for the transaction methods themselves to return iterators instead of `Vec`s? You can iterate over all vertices with [`VertexQuery::All`](https://docs.rs/indradb-lib/0.12.1/indradb/enum.VertexQuery.html#variant.All). You can also get the [total vertex count](https://docs.rs/indradb-lib/0.12.1/indradb/trait.Transaction.html#tymethod.get_vertex_count). 
And since the Rust team decided to regress the compiler compared to 1.24.0, we now need to reopen [#18510](https://github.com/rust-lang/rust/issues/18510). :-( 
Yeah, I got that. My point is that there's room for a lib that lets you describe your parameters *once*, and then generates the adequate code for clap and a configuration crate. Or maybe something that plugs into clap and handles configuration files from clap parameters, if that's possible.
Any concerns about either of the following crates? - https://github.com/AndyBarron/app-dirs-rs - https://crates.io/crates/directories And yes, one thing we need to do is ensure discoverability of all of these.
Yes, though it's going to be closed again soon; the PRs to re-enable on master are already open.
Yeah, and it's a very coarse-grained lock. But the goal with the in-memory-only datastore is to be a simple demonstration / something to easily kick the tires with, not to be the fastest implementation. It doesn't even persist. The other datastores provide different trade-offs. The postgres implementation provides the best ACID properties, though it's substantially slower. The RocksDB implementation lies somewhere in-between.
Wow, I never really considered dlopen'ing the main executable. I just tried it using libloading crate (as suggested by /u/RetraRoyale), and it works! I guess I was confused thinking that I need to compile some shared lib for Rust to call. But the main executable is what will have the important runtime data. Now, I'm not sure if this is the best plugin design, but it at least gets me in a positive direction.
Thanks for the crate suggestion. By "library" I loaded the main executable of the C++ application, and was able to call one of the extern C functions just great!
This was a great talk! Using NVI in Rust like this was previously one of those things where I could read code that uses it, but didn't really grok when to use the pattern in my own APIs. Now I feel like I better understand the situations in which it would be advantageous. Using it to make object-safe traits out of non-object-safe ones makes so much sense. Thank you!
Both are really good. I started with the first edition of The Rust Book and I found it quite easy to get started with Rust thanks to it. Programming Rust is excellent, I highly recommend it. As the second edition of The Rust Book is still in the works Programming Rust is probably a more complete and self-contained reference at the moment.
Programming Rust speaks more directly to C++ developers. Both the first and second edition of the official books will be useful references and guides regardless. Beyond those, [Rust by Example](https://rustbyexample.com/) and the the [Rustonomicon](https://doc.rust-lang.org/nomicon/) are worth bookmarking.
Oh I see, I understand the disconnect now. My config file is an "rc" file, where you type in the literal command line flags that get prepended to the argv. So in that sense, I still retain a single point of truth, and in fact, that's certainly a large part of why I did things this way. Getting this right at another layer of abstraction in a generic way is a tall order.
&gt; Did you know that you can intercept longjmp unwinding in GCC C++ by catching a special marker exception type? (At least, you could at some point.) pthread cancellation too!
I’m happy to learn of these. I’ll have to study the API to make sure I can have a CLI app on a Mac choose the Unix behavior and not put things in ~/Library Thank you!
you still need to rewrite code to use simd, it will take some time
I’m a newb so you need to cut me some slack, but why would you want to run rust code on the JVM? Isn’t native/bare metal (much) faster?
I wrote #2 and I don't really recommend it unless you absolutely have to (ref bottom paragraph of [benchmark](https://github.com/cretz/asmble/tree/master/examples/rust-regex)). If you already have a Rust library, that's one thing, but writing from scratch, just stay in the langs that are made for the JVM I'm afraid. Now if you have to share code between Rust and the JVM, I've had a good bit of success with Kotlin Native.
I've been working with perceptual hashing recently, and I've seen lots of false positives like that last pair. I'm afraid it's a lot more mundane than the similarity in composition, after rescaling (and grayscaling) to an 8x8 image they're both just two grey rectangles. A blurry picture of a house's driveway or farmlands could've been just as similar. It's a lot more surprising that those rotated images are so far apart. [OkCupid](https://tech.okcupid.com/evaluating-perceptual-image-hashes-okcupid/) has a pretty cool blog post about this. That aside, I love how brilliantly simple BK trees are and I'd like to thank you for writing this crate. I got close to using it a few days ago because the Python package I'm currently using is painfully slow. 
BK-trees do perform quite well, but you have to set a sane distance cutoff. The LISP implementation has a [pretty cool visualization](https://github.com/vy/bk-tree) of quickly the amount of returned items increases. Quickly looking at your code, you seem to rescale using nearest neighbor interpolation. Is that how the original pHash library did it? I've only used the imagehash library for python, which uses lanczos interpolation by default. I've tried other methods, but it did seem to be the most accurate one on my data. 
&gt; much faster Not really. JVM's JIT is *really* good and if it's written in a performance-minded style it can complete with Rust. Object oriented ~~messes~~ highly indirect design patterns can be several times slower than performance-minded code, but Rust makes that stuff painful or impractical. If you're pushing the limits of memory bandwidth, native code is necessary. JVM also tends to have more memory overhead vs a GC-free program.
The main problem I had with the VP tree is that it only gave k-nearest-neighbor when I really wanted to get everything within a certain Hamming distance, so BK trees are already sounding a lot better. It's hard to discover these really specialized datastructures just by searching around. The filter choice is arbitrary, I just assumed nearest-neighbor would be the fastest. I was under the impression that the filter type didn't really matter for downsampling since it's inherently throwing away information anyway. I actually haven't looked at the source of pHash at all, mostly to avoid falling foul of copyright(/left) by inadvertently copying algorithms verbatim (like how ReactOS can't permit any contributors who have seen Windows source code). I built my implementation based off high-level descriptions of various algorithms in a series of blog posts starting [here](http://www.hackerfactor.com/blog/?/archives/432-Looks-Like-It.html) (which mentions pHash but also that the described approach is somewhat different). It is my understanding that pHash only implements the one DCT-based algorithm, but I've actually gone ahead and implemented a number of them, including Blockhash.io. I've actually had the idea to combine DCT sampling with the gradient hash Kravetz describes in a subsequent post just to see how it performs. In fact, a DCT preprocessing step can be combined with basically every other hash type except Blockhash (which doesn't downsample the image). And since watching the [3Blue1Brown video on the Hilbert curve](https://youtu.be/3s7h2MHQtxc) I've been trying to think of possible ways to apply that to image downsampling because it's such an interesting concept. 
gremlin support?
Thanks for the feedback! unsafe is primarily used in the project for 3 reasons: FFI, initializing memory, and getting around the ergonomics of RefCell. Those first two points are the second case and I'm fairly certain are correct. The usage of unsafe in throttle is worth more carefully auditing, but references are self contained and very limited in scope. All that said, I'd like to reduce the usage of unsafe overall. Most of it is used for things which can be altered with little cost, which I plan on doing for the 1.0 release.
I think the sampling methods matters a lot actually, especially because it completely transforms the input. In my specific use case I had to work around cropping and watermarking though, could be that lanczos just performed better for those specific cases. pHash actually has quite a few cool algorithms, unfortunately I haven't been able to test them yet because there was no way I would be allowed to bring C code into our Python project. They have a page with some metrics of their algorithms [here](https://www.phash.org/docs/design.html) if you're interested, and there is absolutely no source code involved. :) Do you have a specific link to that Gradient hash by the way? I remember scrolling through a few pages on his blog, but there's so much on there I must have given up before encountering another hashing algorithm. 
I have no problem with using unsafe for FFI. That's unavoidable. For initializing memory, you don't need unsafe. It looked like you just needed to defer the allocation until a later part of your code in most cases that I saw, where you were trying to avoid paying the cost of zeroing out the memory with the normal, safe allocation. I haven't looked at your particular code in too much detail, but generally speaking, if `RefCell` isn't feeling ergonomic to me, I would see that as a hint that I should restructure things, rather than forcing things by using `unsafe`. There are many ways to approach a problem, usually. Don't take all this as being too critical. The project is rather impressive! I just wanted to share my thoughts on some issues.
Perhaps have a look at Graal/Sulong: https://github.com/graalvm/sulong
It looks like way-cooler stores rlua contexts in TLS. It probably makes sense in the context of how they use it, but wouldn't work for IndraDB because AFAICS the memory won't be reclaimed after a script is finished executing. IndraDB guarantees memory reclamation by just destroying the lua context after it has executed the script.
DCT is disgustingly slow due to the cosine function, though I figured out you can calculate the DCT coefficients ahead of time and then it's just matrix multiplication. I got a literal factor-of-ten speedup doing that, but I'm not entirely happy with my current solution of having a free function that calculates it for one size and memoizes it in TLS. I'd like to calculate the DCT matrices at compile time but that would require the user to know what sizes they're going to use when building the library.
Great explanation of both WHY and HOW to use the NVI pattern with Rust! Thanks for this!
This looks really cool! Is there a sane way that I could drop this into a larger program, as an alternate method of downloading files? I assume that the logic for telling the downloading code (the tracker?) what to do is somewhere in the rpc library, but I couldn't see how it worked from a quick browse through the code. I guess I'd like something like a run() that returns a channel of some sort, and then I can drop commands into that channel and have downloads happen. It looks possible, but I don't see how to do it. 
As I was debugging this, it went from erroring exactly as you said, to just working. Does it work now for you?
I don't see how an async db driver makes a difference for that.
Ah OK. So that means that the Rust code you'd write for the JVM would be different from the Rust code you'd write for doing it bare metal? 
Not really, but it does happen not very often. Basically, if you get lucky enough that the hash they generate does not actually have any invalid characters then you could get a response. But that is random. Try it 10 times I get about 6-8 fails. 
r/playrust
When you use Vec::with_capacity, it does the allocation, but it doesn't initialize any of the memory. If you then "push" data into it in a tight loop, this usually gets fully optimized into SIMD enhanced copies from what I've seen, and you only initialize the memory once.
I may extract out the core into a library at some point, but for now the recommended method of interfacing with the client is through the external RPC (which there is a crate for).
No, what she means is native isn't (much) faster.
Small subsets of LLVM's functionality are already being implemented in Rust, but the design is considerably different - see https://github.com/Cretonne/cretonne and its documentation.
&gt; When you use Vec::with_capacity, it does the allocation, but it doesn't initialize any of the memory. But I need to write directly into the buffer, and the first 4 bytes are the length, so... I could do `set_len`, but that requires unsafe, and follow that with a pop front, but that requires shifting the whole array...
How are you "writing directly into the buffer"?
If I didn't know the length of the data I was going to be putting into the buffer, I would probably use `Vec::with_capacity()` to allocate an un-initialized buffer, unsafely read the first word, and then unsafely use `set_len()` to set it to the proper length. No need to faff about with `mem::uninitialized` or `Option&lt;Box&lt;[u8;bignum]&gt;&gt;`. "Read known-non-null pointer" and "set length" are pretty tame and foolproof compared to the number of ways explicitly handling uninitialized memory can screw up. Even better may be to use `Vec::with_capacity()` to allocate the memory, zero the first u32 (or however long your length field is), then use that as your buffer. Then reading the length out of it (which may be 0 anyway) is perfectly safe, and the only unsafe operation is resizing the vec. Plus it's quite obvious whether the does or does not have meaningful data in it if you want to reuse it.
If you're receiving from the network, you would be using `TcpStream`, right? So, you would do something like this, unless I'm wrong: use byteorder::{ReadBytesExt, BigEndian}; let stream = ...; let incoming_length = stream.read_u32::&lt;BigEndian&gt;(); //BigEndian because IPv4/IPv6 protocols generally prefer BigEndian let mut buffer = Vec::with_capacity(incoming_length); stream.read_exact(buffer.as_mut_slice()); No unsafe required, and you're reading as directly from the TcpStream as Rust users typically do. 
How would you copy data from the network with no copies? There are [typically three distinct copy operations](https://people.cs.clemson.edu/~westall/853/tcpperf.pdf) (page 6) for every network packet: 1. The packet is copied from the NIC to a kernel ring buffer 2. The packet is copied from the kernel ring buffer to a particular socket's receive buffer 3. The packet is copied from the receive buffer into the application's memory space How would you modify the code in [this comment here](https://www.reddit.com/r/rust/comments/81glwp/synapse_a_full_featured_bittorrent_client/dv3ej3m/) to avoid any of these copies? Now, I have actually written zero-copy user-space networking code before, but that was almost 5 years ago (wow, how time flies). The way that you do it is that you write a Linux kernel module to to allocate a physical buffer, set your networking device to dump packets directly into that ring buffer, and then that kernel module provides that buffer (and permission) to your user-space application. Without a kernel module of your own, I don't think it's possible to eliminate stage 2, although the `ZERO_COPY` flag might allow you to eliminate stage 3 purely from user space. I just don't know how someone would do that from Rust in an ergonomic fashion. (certainly, it can be done with enough `libc`)
Hello, what are the technique to profile rust applications. I have a small application I would like see where most of the time spend. Thanks, Murali
Do you mean a rust compiler specific subset of llvm, or replacing llvm by a rust version? In both cases, the effort would be massive, and you'd better have an extremely good reason to do it.
All techniques based on DWARF debug information should work with Rust, too, if you build with debug symbols. What OS are you using? If it is Linux, I've got an old [blog post](https://llogiq.github.io/2015/07/15/profiling.html) for you.
Oh yes thanks, I thought I had checked that but apparently not maybe its time to sleep
&gt; logging USB register accesses and trying to match them Is that really the easiest way? I was planning to use this [teensy USB library](https://www.pjrc.com/teensy/usb_keyboard.html) as a reference, since it's the smallest and simplest looking USB code I could find. Good to know about that issue! I want to rewrite some keyboard firmware in rust, but haven't been feeling up to writing USB stuff from scratch. So I'll probably take the same approach of watching the issue to see if someone comes up with a solution or suggests ways to contribute.
Llvm took what, 10 years and a thousand people to reach it's current state? I think you're underestimating the work. Now if you want to learn rust and compiler backends at the same time, do it, it's going to be an amazing experience.
Yes, I did; but that doesn't mean it's the underlying mechanism for C++ exceptions on x64 (in MSVC). ;-]
You say "this isn't true" then repeat what I said... o_O Yes, SEH _exists_ on x64, but MSVC does not use it for C++ exceptions on x64.
There are about 75 people (according to [GitHub insights](https://github.com/llvm-mirror/llvm/graphs/contributors?from=2001-06-03&amp;to=2018-03-02&amp;type=a)) that have added more than 15k lines of code to LLVM. I'm sure there's a really long tail of smaller contributors, of course, but it's "only" 75 people over 17 years :P absolutely, it's a crazy amount of engineering effort.
I just have to say, that's a really good crate name!
I might be weird but I am in the "use the right tool for the job" camp. `mem::uninitialized` is just a tool. Going out of your way to avoid uninitialized when its the best tool for the job is not seeing the forest for the trees. 
&gt; Going out of your way to avoid uninitialized when its the best tool for the job is not seeing the forest for the trees. As I said, &gt;&gt; There are always exceptions when you need to pull out advanced tools. but *also* as I said: &gt;&gt; I want the compiler to stop me when I'm obviously wrong, and tools like `uninitialized` limit too much the ability of the compiler to help you. If it's the right tool for the job, then great! use it! I believe it is *not* the right tool for most people and most situations.
**Razor strop** A razor strop (or razor strap) is a flexible strip of leather, canvas, denim fabric, balsa wood, or other soft material, used to straighten and polish the blade of a straight razor, a knife, or a woodworking tool like a chisel. In many cases stropping re-aligns parts of the blade edge that have been bent out of alignment. In other cases, especially when abrasive polishing compound is used, stropping may remove a small amount of metal. Stropping can also burnish (i.e. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
MacOs. So I read your post also read http://athemathmo.github.io/2016/09/14/tools-for-profiling-rust.html I am going to try cargo-profiler today. Thank you 
TL;DR: add any of your compiler error pet peeves as a response to this thread. In the wish of improving the ergonomics of the language there're, in my opinion, four big pillars: - Language features: does this code that makes sense in my mind work without/with minor changes? - Documentation: can I find the answers to my problems easily in an understandable way? - API: does it map to my mental model of the problem and is it composable? - Diagnostics: my code doesn't work, does `rustc` tell me why in such a way that I can understand how to fix it? Personally, I have been focusing on incrementally improving the last item. [There're already plenty of things that we're aware needs to be improved][1], but I would I would like to see what people in the larger community would like to see improved. If you have any particularly unreadable, hard to understand or just plain wrong error messages the compiler has thrown at you, please add it to this thread, even if you know there's already a ticket for it. My intention is to use the replies here as a signal to prioritize and extend the existing backlog. [1]: https://github.com/rust-lang/rust/issues?q=is%3Aissue+is%3Aopen+label%3AA-diagnostics
I have opened a [thread in internals][1] to get feedback from the community on compiler diagnostics. Feel free to jump in and add any subpar, unreadable, incorrect, funny or mildly infuriating error you've encountered while compiling your code (including a reproduction case would be greatly appreciated). [1]: https://internals.rust-lang.org/t/compiler-diagnostics-improvement-wishlist/6886
Nice work. I've tried to write a bittorrent engine in Rust before and while it could download torrents, it didn't have priorities and rate limiting. I've explored a lot of ways of implementing the IO and it's quite challenging. I think Rust's ownership models increases the difficulty a lot. How did you implement the IO for this project? It doesn't seem to be using tokio.
I'll look forward to a blog post that incorporates this into a rust related blog post.
[`kcov`](https://simonkagstrom.github.io/kcov/) actually has support for Rust, so I think that the code coverage use case is handled. You can set up `gdb` and `lldb` to [break on panic](https://github.com/rust-lang/rust/issues/21102#issuecomment-69780423), which seem to be the basis of the other use case you were discussing. Does that more or less cover what you were hoping for?
It looks like [`rust-eureka`](https://docs.rs/rust-eureka/0.1.1/rust_eureka/) handles the registration and query flows. Have you seen that lib yet? Maybe you have, and something's missing? :)
I haven't seen it yet! It must be recently added, every time I've searched on crates.io I've found nothing. Thanks, I'll give it a shot!
Sorry!! Crate! I mean Crate! I'm a Python guy cut me a little slack.
They already have /u/tomaka17, what more do they need!
*Programming Rust* adds a really important level of detail and clarity to The Book. It's well worth buying a copy and working through it.
Nice!
Yeah, I noticed that that crate didn't come up a week ago either. :) Glad something just came up!
Great project! But the name is the same with [synapse](https://launchpad.net/synapse-project) launcher and I find there is a binary conflict when install it togather with synapse launcher on archlinux.
WASM is a better poor-substitute for LLVM IR than LLVM IR (bitcode), it runs everywhere and isn't tied to machine specifics. In its current form it doesn't have the same expressivity that LLVM-IR does, but it is good enough. 
What a great project! Libtorrent is the defacto engine written in C, but I'm so glad rust ones are getting written. 
Looks pretty cool! As far as I can see, there is a single thread polling events from all peers and then handling each of these events. I think this will make it hard to scale beyond 1Gbps. The mmap usage for files is really nice, one thing I was experimenting with was optimized parsing of peer messages - if you only read the first 17 bytes of a message, it will be enough to parse every message except for `Piece` (with data). By doing this, you can follow up the initial read with a vectored, zero-copy read directly into memory-mapped files on disk, which should be quite efficient. It would be pretty difficult to add it though, since you have an async job system for file I/O.
I've been working on and off on some rust code to reading GB of binary data that is mostly billions of vertices and triangles(edges) of 3D meshes and then wanting to spatially sort that for queries/algorithms on a subset of the verts/triangles(again spatial area like a cube). Octree seems to be the way to go for a data structure and I've used some graph crate to load data into memory and do some basic queries. Since it's mostly a fixed process to transform the input data where the verts and triangles are disorganized spatially and output smaller chunked files of verts/triangles near one another, is a graph database of any value here? Or am I likely to get better performance with my current approach?
Those things are *counterproductive* for LLVM's goals. Which, again, are different from wasm's.
The problems described in the video are the *exact reason* Rust has the ownership model it does. It's not some abstract academic exercise - Rust was explicitly designed to solve *this*.
With Chrono's `NaiveDateTime::parse_from_str`, why can't I use the format: `"%s%.f"` or some derivative? 
&gt; I am only a novice (but) I personally find all of the problems I run into to be easily solvable I'm very grateful that you feel that is the case, and it goes to show the ton of work that people have put on this front over the years. &gt; if I actually read the words in the error (rather than what I think it is saying by just reading the symbol name) I'd still consider this as a bug—when there is room to improve things. Good design is making things that nudge you in the right direction, even despite yourself (see *The Design of Everyday Things* :) ). Because of this, I would still encourage you to comment on that thread (or here, or in the issue tracker) when you encounter a case like that. Even if you don't see *how* it could be improved, somebody else might. Worst case scenario, the ticket gets closed a few weeks later, but with the potential upside of making lots of people marginally more productive. 
Whoa!!! Nice! If the author is around - could we get a gif of looking at a rendered markdown -&gt; html file, and looking at a wikipedia page? If these two things can be rendered reasonably I can see this being used for more than just a toy web browser. The next thing would be get GitLab rendering.
This seems like it should really be an extension of `#[derive(Default)]` instead of a separate thing.
Talk is cheap. Why don't you start it?
Yep. Or if you did `worker = None;`, I expect. Shouldn't need an explicit drop here.
Not to mention the [Matrix home server](https://github.com/matrix-org/synapse). I guess it's a really popular name.
Indeed. I've heard that verb before but was wondering what it meant in the context of computing. I kind of think it would have to remove rust, though, and I don't want to do that.
It’s not needed, but, hopefully, it makes the intent of code clearer.
I didn't know there is a Option::take method. i use mem::replace every single time. :( Thanks for sharing.
So I'm currently working on the windows port of [alacritty](https://github.com/jwilm/alacritty). One of the new dependencies is winpty a project which serves as an adapter between unix style ptys and the windows console subsystem. The problem is that winpty requires both a dll (Which we actually build statically) and a binary agent. What I really want is the build command for winpty-sys to be able to 'publish' the agent exe so that it is either available in alacritty's build-script build step or passed straight into it's target directory. Is this currently possible or are there any other solutions that could work around the problem?
[image](https://docs.rs/crate/image/) &gt;provides basic imaging processing functions and methods for converting to and from image formats. [imageproc](https://docs.rs/crate/imageproc/) &gt; An image processing library, based on the image library. 
I would really like to go, but I live in Europe. Will there be any recording?
Recently, [Snips' voice assistant was featured on Mozilla's blog](https://blog.mozilla.org/blog/2018/02/21/snips-uses-rust-embedded-voice-assistant/), which also mentions how Rust helped implementat features that do not require a connection.
I've learned it really pays off to learn every method of `Option`, `Result` and `Iterator`. Some hidden gems!
There are companies like Apple and Google contributing to llvm. So you can't just rewrite a big project like this alone.
If you want to work on compilers, why not work on the Rust front-end? That would be easier and have a bigger payback for the Rust community.
The etymology is horrifying. &gt; In Norse mythology, Naglfar or Naglfari is a boat made entirely from the fingernails and toenails of the dead.
As stated below, replacing llvm would be massive amount of work. So, you *probably* will not succeed, which is usually not fun. And there doesn't seem anything outright wrong with llvm to want to replace it. Nobody is going to stop you, and it would be very interesting experience, and, after all, llvm probably started with similar idea of one person „hey, how about writing a compiler“. But I'm also pretty sure there are other very interesting projects that are more doable, maybe already started and with more practical impact. If you like compilers specifically, rust compiler always looks for contributors and there are people willing to mentor.
'having a strop' means a tantrum, so I was looking forward to grumpy threads
**[Live Twitch Clip (Clip + Chat) on Streamable](https://streamable.com/hatpo)** Credit to [twitch.tv / KeepitSour](https://www.twitch.tv/keepitsour) for the content. --------------------------------------- ^(Bot to preserve unique live stream experience forever by rendering chat as part of the mirror video. | ) [^feedback](https://www.reddit.com/message/compose/?to=LiveTwitchClips)
I disagree. Default is used by many things, this crate allows you to derive the constructor methods on top.
wrong subreddit :)
&gt; How would you copy data from the network with no copies? You pass a pointer to a buffer to your network device so that it can directly write to it using DMA. 
Parabéns!
People on the diesel Gitter were talking about that. Maybe weiznich there.
Don't worry about zeroing, often the OS has already zeroed the mem page so this is a no-op. Otherwise, the optimizer might help. In any case, it's best practice to write everything safe even if it's slower, then profile to see if using unsafe will make things faster, and just leave it if not. That way you save cognitive load with no loss of performance (if suboptimal code is not on the hot path).
Maybe this could be merged into `derive_more`.
haskell compiler written in rust: https://github.com/Marwes/haskell-compiler
Cool, but what's the motivation? 
With [mrustc](https://github.com/thepowersgang/mrustc), there have been some projects recently to use it for embedded development like [this one for the ESP8266](https://github.com/emosenkis/esp-rs/blob/master/README.md)
Would you consider this a compiler bug? ``` fn get_log_level(i: usize) -&gt; &amp;'static str { match i % 5 { 0 =&gt; "A", 1 =&gt; "B", 2 =&gt; "C", 3 =&gt; "D", 4 =&gt; "E", } } ``` Complains with ``` 37 | match i % 5 { | ^^^^^ pattern `_` not covered ``` My understanding of match is it is supposed to check all cases are covered...and they are. I had to fix this by putting in a panic for the `_` case.
It's not a bug, exhaustiveness checking just doesn't do a whole lot of introspection. The `_` branch should be optimized out anyway (barring an LLVM bug) so I wouldn't really worry about it.
Even though I haven't used it much, I do like Kotlin. It feels a lot like Rust but on the JVM. It's got a lot of the good parts (null safety, for one) but doesn't need borrowing because of the obligatory garbage collection on the JVM.
I'll continue with the competition tonight at ~18:30 (Berlin/CET) and then tomorrow morning again.
the compiler never treats matches on numeric types as exhaustive — a match on a u8 still needs a wildcard even if every value from 0 to 255 is covered. evaluating match expressions to see what cases are needed would be an interesting feature, but it hasn’t been a focus.
It is using alex/happy, which is lex/yacc of Haskell world. It is definitely not hand written.
On the other hand, the way we build optimizers, including LLVM, is changing. Superoptimizers and feedback-driven fuzzing are becoming much more powerful, with the potential to take over significant amounts of manual effort.
There's a melodic death metal band by the name that's pretty good too.
Somehow I can't find any use for that.. Maybe I'm missing something. Why would one like to implicitly derive a new function?
I was talking with Shepmaster about the opportunity for cargo/rustc to improve user experience on Windows by NOT having a dependency on Visual Studio or the Microsoft Build Tools being installed. I didn't know it if was possible. Apparently it is, and the new release of the D compiler avoids this separate installation step. I'm not sure how clean their setup is, or how big the installer/zip ends up being, but from an outside point of view it certainly seems to make things easier for the end user!
Comment repost from the article, as the discussion seems to be here: &gt; A brilliant solution to a seemingly simple problem! Thank you for posting this. &gt; &gt; I'm wondering whether the `drop(...)` call is required. I'm thinking that the option owns the sender, then you're taking it out (leaving `None` behind). This produces a new option holding owning the value, which will then be dropped automatically when it goes out of scope (which it immediately does). &gt; &gt; Is this indeed right, or am I thinking wrong? Sadly I'm not able to test it out at this time.
This is crazy: I love it! :)
Thanks, fixed!
Thanks for sharing! Looks promising. I am currently trying it and I a boring limitation that should be easy to fix. In the [http api](https://indradb.github.io/http-api.html), the `create_vertex` take a type and return an `uuid`. `uuid` are great because they can be created from the client side, and they have been created for that point! As I can see, there is no way to set the uuid of a `vertex` to `indradb` I'm currently maintaining graph in a postgresql database and I've already have my ids, and more, they are always generated client side. Is it possible to have this behavior in IndraDB ? 
Did you know we *already* ship a version of Rust for Windows that comes with MinGW bits bundled so you don't need a Visual C++ installation? In fact, at one point that was the *only* version of Rust available for Windows.
The LLD linker from the LLVM is nearing maturity for msvc linking once that’s done and rust updates to that llvm you shouldn’t even need msvc anymore. You’ll still need the platform libraries, pdb, etc though. 
Yeah at least as an option. When I'm trying to refactor stuff, I get a ton of unused coffee warnings before getting to the errors.
In the Readme, it is said that the typical use is: &gt;&gt;&gt; :set -XTypeApplications +t It is missing the environment/interpreter in which this can be run. 
Happy to hear that. I really needed to think around a few corners to arrive at the solution :-)
Off topic but yesterday I discovered that `cargo r` is a shorthand for `cargo run` and now I see this guy using it. Frequency illusion?
I have to say, I really want to get into rust... then I see posts like this :( 
I haven't used `rug` before but it seems awfully similar to what `num-bigint` does. Can someone who has used both explain the differences?
I don't think DAW users care whether the program is Web based or native. They care about features, ergonomics, stability and performance. From what you're saying, targeting desktop seems like the best choice to pursue these goals. 
`num-bigint` is written in pure Rust with no unsafe code and no dependency on nightly. The advantage is that it builds anywhere Rust builds and uses only safe code. Rug uses the C library GMP for its computations, so internally it has to use unsafe code for interfacing to that. A disadvantage is that Rug does not support Windows MSVC toolchains; on Windows it only works on the GNU toolchains. The advantage of Rug is that it uses GMP which is very fast, and it also supports many features as GMP is quite feature rich. Also, `num-bigint` only provides bignum integers. Rug provides integers, rational numbers (using GMP as well), floating-point numbers (using MPFR) and complex numbers (using MPC). Rug provides a safe interface to most of the functionality of the three GNU C libraries.
Thanks to you and /u/DroidLogician
This might be more useful if it derived a builder instead of a constructor IMHO.
Oh, that makes a lot more sense why both exist. btw the rest of the `num` series of crates do provide rational and complex types
Many error messages have no explanation: EXXX number. I wish this would improve.
Yes, I got confused, they do provide [bignum rational numbers](https://docs.rs/num-rational/0.1.42/num_rational/type.BigRational.html). However the complex types are not bignum, they use primitive floating-point parts. You need a bignum floating-point type to have a bignum complex number.
That's kind of what I had in mind. I imagine: 1. Fork and get building llvm-core 2. Create Rust project/workspace to build Rust code, Build llvm-core, and link bits built in Rust with llvm-core (kind of like what someone started with rewriting musl in Rust) 3. Begin by replacing the llvm bit-code to AST parser with Rust 4. Get that building and passing all Unit tests etc. 5. continue down the stack from there Does that even sound like a reasonable approach to even begin such a thing?
Ah, you're right, I misread what this was doing originally.
Because I'm looking for insight from people more knowledgeable than I before I just dive right in. It's usually prudent to investigate things a little and get some input from experts. If I'm going to sky dive, I don't just slap a chute on my ass and jump, I would probably want to consult with some skydivers first. Maybe get a few safety recommendations and maybe avoid using a parachute that from a company that is known to have a high failure rate, etc.
So, people asking questions and looking for expert opinions dissuades you from doing something you're interested in? In the words of the orange haired monkey, "Sad"!
But why?
I was comparing my solution to the [series](http://exercism.io/exercises/rust/series/readme) problem over at exercism.com when I thought I could do some benchmarks. I noticed that rylodyne's solution was over twice as fast as mine. I was wondering if someone could look into it and figure out why. This is my solution: pub fn series(digits: &amp;str, len: usize) -&gt; Vec&lt;String&gt; { (0..digits.len() + 1).filter_map(|i| digits.get(i..i + len)) .map(str::to_owned) .collect() } rylodyne's solution: pub fn series2(digits: &amp;str, len: usize) -&gt; Vec&lt;String&gt; { match len { 0 =&gt; vec![String::new(); digits.len() + 1], _ =&gt; digits.chars().collect::&lt;Vec&lt;char&gt;&gt;().windows(len).map(|window| window.into_iter().collect::&lt;String&gt;()).collect::&lt;Vec&lt;String&gt;&gt;() } }
'''So, you probably will not succeed''' Yeah, that pretty much sounds like a given, but, I think I'm going to give it a try. Even if it goes nowhere, I should learn a lot.
Problem is, with question like this it feels more like a research project than a pragmatic language. Almost anything needs nightly and someone wants to rewrite LLVM? Anyways, the community around a language is as important as everything else about it!
As I mentioned, I am not clear how clean their setup is (I saw the mention about MinGW in the article and felt uneasy). That being said, when talking about building for Windows, I did not consider MinGW (or Cygwin, or any other "alternative" toolchain), but the "original" toolchain. How does `-msvc` handle choosing whether to link with the 2010, 2013, 2015 or 2017 versions? Is there a way to choose? Do you think it would significantly improve the user experience if rustc could link against Windows C libraries out-of-the-box?
I'm finding it really hard to grasp the rust ownership model, I've written some code and I'm struggling not to get into trouble reusing the same variable when doing a series of operations on a piece of data. Are there any especially recommended starting guides for the ownership model and borrowing that I can read? I've started with the official docs and it isn't sticking..
The `Complex32` and `Complex64` types are based on `f32` and `f64` respectively, but with `Complex&lt;T&gt;`, the user can create `Complex&lt;Bignum&gt;`
I find the `Assign` trait rather unfortunate. This is nothing against the crate itself, it just shines the light on a specific part of the design of Rust which prevents buffer reuse. We have many `XxxAssign` which mutates the left-hand sign, explicitly allowing buffer reuse, I wonder if it would be possible to adopt a `Assign` trait in Rust which would allow the same. Imagine: let s = "Hello".to_string(); s = "World"; reusing the memory buffer in place (and yes, it is performing the conversion too!). *Note that above, `s` is not mutable. This is consistent with `Drop`.*
My guess is that's the real etymology for this project.
This is an entirely incorrect analogy. People are careful with skydiving because it has deadly consequences. In technology as well as science, people who dive right in and get in way over their heads are liable to discover or achieve something amazing. If they have the ambition and intelligence to choose and persist on a path worth taking. Most successful products if you asked experts beforehand would never get made. Experts are by definition experts in the current or past technology. This has nothing to do with recognizing what is a successful next step.
As soon as you go off the beaten track (default clang settings, compiler passes and ordering) it's not uncommon to find bugs in LLVM. With assertions enabled they'll abort...but running with assertions is not all that common so you get silent miscompilations. Let's take a worked example - https://github.com/rust-lang/rust/issues/37508 This was just me attempting to compile Rust with `-C code-model=large`. It generated an invalid libstd. To cut a long story short, the fix in LLVM was to remove a call to `getImm()` - https://reviews.llvm.org/D27481#change-KfxkqTV2q12H. Here's [`getImm`](https://github.com/llvm-mirror/llvm/blob/release_50/include/llvm/MC/MCInst.h#L76-L79): ``` int64_t getImm() const { assert(isImm() &amp;&amp; "This is not an immediate"); return ImmVal; } ``` `ImmVal` is the `int64_t` value of a union in this class. Except type-punning via union is UB in C++, and disabling asserts has removed the guard rails! By contrast, in Rust programs there is generally no compiler flag you can casually set to throw away fundamental correctness checks. (this puts aside other asserts, hangs and so on I've seen while using LLVM since they're not specifically memory safety)
Wow! Thanks for this. This is the kind of info I was hoping to get.
Maybe try collecting the join handles and joining all the threads at the end?
I'm imagining (likely naively) that simply porting the code from C++ to Rust would not take but a fraction of the original man hours (say 2%). That is probably not right, but, I doubt it will take anything like the original time to completely engineer the original solution from scratch.
I've been working on a modular DAW in Rust for about 8 months now. I was originally going to target the web but switched to desktop for performance and ergonomics reasons. Perhaps we could collaborate somehow? Send me a PM if you're interested. My work is at https://github.com/nwoeanhinnogaehr/flow-synth
&gt;How does -msvc handle choosing whether to link with the 2010, 2013, 2015 or 2017 versions? Is there a way to choose? You can manually configure your environment by running the relevant `vcvars.bat` for the given VC++ version which sets a bunch of environment variables. Otherwise it will automatically use the latest version that it is able to detect. &gt;Do you think it would significantly improve the user experience if rustc could link against Windows C libraries out-of-the-box? Linking against static C libraries out of the box is impossible unless you have all the libraries that static C library depends on. The C runtime is particularly tricky as you must have the exact identical version. Furthermore the static C library might be built with `/GL` in which case LLD is **not** an option because it does not understand LTCG bytecode. Really though, the likelihood of you having a static C library floating around without the C toolchain used to build it are miniscule so this is not a use case that Rust should waste time trying to support. 
Now you see what I mean... :)
Another important thing to note about GMP is that it is licensed under LGPL3 and GPL 2, which have restrictions that most crates don't. Another crate to look out for is `ramp`, which IIRC outperforms num-bigint and also has a native rust implementation, but it still requires nightly.
Yeah, I believe it to be a reasonable approach to the issue. Its what the people porting emacs to rust is doing. Doing it this way you're probably able to get more people interested.
Use `unreachable!()` for the `_` case.
A journey begins with the first step.
Okay, I just can't figure out some of this procedural macro stuff. I have a crate named "registry", and another named "registry-derive" which provides a derive implementation of a trait called FromConfig. The entire implementation isn't that important, but it starts like this: quote! { impl ::registry::FromConfig for #name { This works great from outside the "registry" crate itself, which is where this will mostly be used. However, I want to add some tests to the registry crate. When I try using the derive inside the registry crate, I get an error because it can't find ::registry::FromConfig (which is really just ::FromConfig from within registry). How do I write this line of the macro such that it works both inside and outside of my registry crate?
Apparently there is an unstable feature `toowned_clone_into` that lets you do "World".clone_into(&amp;mut s); 
Consider me schooled!
Doesn't rust usually rely on the allocator to do this type of thing? Or maybe I'm misunderstanding.
Hello, I'm new to rust and I'm having a bit of an issue. This is a sample of the code: fn do_match(&amp;'a mut self, tokens: &amp;[Token]) -&gt; Option&lt;Token&gt; { if self.lexed.len() &lt;= 1 { // ? return None; } let tokens = tokens.into_iter(); let token = match &amp;self.lexed[0] { &amp;Lexed::Literal(_, _) =&gt; Token::Literal, &amp;Lexed::Identifier(_, _) =&gt; Token::Identifier, &amp;Lexed::Operator(token, _) =&gt; token }; for i in tokens { if *i == token { self.current += 1; return Some(token.clone()); } } None } fn previous(&amp;self) -&gt; &amp;Lexed { &amp;self.lexed[self.current - 1] } fn primary(&amp;'a mut self) -&gt; Result&lt;Expression&lt;'a&gt;, ParserErr&gt; { if let Some(_) = self.do_match(&amp;[Token::Literal, Token::Identifier]) { match self.previous() { &amp;Lexed::Literal(ref literal, pos) =&gt; return Ok(Expression::Primary(Primary::Literal(literal), pos)), &amp;Lexed::Identifier(ref identifier, pos) =&gt; return Ok(Expression::Primary(Primary::Identifier(identifier), pos)), _ =&gt; return Err(ParserErr::GrammarError(0)) }; } Err(ParserErr::GrammarError(0)) } And this is the error I am getting: error[E0502]: cannot borrow `*self` as immutable because it is also borrowed as mutable --&gt; src/parser/grammar.rs:77:13 | 76 | if let Some(_) = self.do_match(&amp;[Token::Literal, Token::Identifier]) { | ---- mutable borrow occurs here 77 | match self.previous() { | ^^^^ immutable borrow occurs here ... 85 | } | - mutable borrow ends here I kind of understand the error, but at the same time I don't understand how I am supposed to go around it. The do_match function changes self.current += 1, while previous function doesn't change it at all. Thanks in advance for any help!
This definitely seems to be interesting with respect to Rust evolution.
Can you just do this instead? let matched = self.do_match(&amp;[Token::Literal, Token::Identifier]); if let Some(_) = matched { 
&gt; if the goal is just "let's rewrite it in Rust" without any better concepts and improvements Hence this post.
ghci
I'm curious why anyone would down-vote this comment? Seems capricious and cowardly.
Rust the game is at /r/playrust. We should sue those punks for trademark infringement at this point.
&gt; Something that was close to finished dies in seconds. If you get a hundred people saying that the RFC is bad, then it most likely was nowhere near being finished. That’s exactly the point of the FCP. I remember a few RFCs being shot down like this, and I don’t remember any of them being particularly good stuff.
I adore toy versions of systems that are inherently large &amp; complex. This source code is small enough that one can understand it all within a brief time investment. It’s a great learning exercise for the reader, but even more so for the author. 
Which is somewhat similar to `s.assign("World");`. It's not as obvious than `s = "World";` though.
I shrugged it off as probably using GTK WebKit... but very surprised/impressed looking at the custom parsing and rendering code
I was thinking about dynamic C libraries (DLL) actually, but thanks for the heads up about the difficulties of linking to static C libraries :)
There are actually places where the standard library explicitly allows passing a buffer in to avoid subsequent allocations (see [`BufRead`](https://doc.rust-lang.org/std/io/trait.BufRead.html)), and it matters in benchmarks. On the other hand, I suppose that allowing overloading for `=` would make its semantics less reliable (as it could be incorrectly implemented).
&gt; (zeroing 4Gb is very expensive, in particular if you are using overcommit, because overcommit basically makes it free if you don't touch it). But if you allocate a 4 GB buffer with calloc (which I learned vec! uses when you write something like vec![0; 100000]), then it's not actually zeroed out. The OS just maps in "zero" pages, and the first time you write to one of those pages, it goes and maps in the *actual* page, which is already zeroed out by the OS. The OS should keep zeroed-out pages for this purpose. Or maybe I'm misunderstanding something, but it seems like the performance overhead would be fairly minimal.
Ah, you mean re-using a buffer, rather than just an allocation in general. I don't know if "could be implemented incorrectly" is a reason to shoot down an idea like that. Really anything could be implemented badly. I do however think that `=` is kind of sacred as it stands, and it seems to me that it's the kind of operator that should only do one thing. An `Assign` trait could just have a method `fn assign(&amp;mut self, data: Self)`, right?
The project is written in Haskell.
&gt; if you want to do that you have to use the target for the toolchain those libraries were built with How reasonable would it be for this hypothetical new target to link to static C/C++ libraries built with Clang and linked with LLD? It doesn't seem very useful to lock out C and C++ completely.
That is a little different from the `rug::Assign` trait, which should really expect mutability. For example `&lt;Integer as Assign&lt;&amp;Integer&gt;&gt;::assign` is equivalent to `&lt;Integer as Clone&gt;::clone_from`; but `&lt;Float as Assign&lt;&amp;Float&gt;&gt;::assign` is *not* equivalent to `&lt;Float as Clone&gt;::clone_from`, as `assign` maintains the destination's precision while `clone_from` is just a more efficient clone that reuses the buffer, but takes the precision of the source. So `rug::Assign` does need mutability, as it can keep some state and just change the value. By the way, that is the behaviour that is comparable to `AddAssign` and friends. I very much agree with Rust's current situation where the assignment operator `=` *cannot* be overloaded, as it has a very specific meaning in the language that is not just about value. Maybe another operator could be used for `ValAssign` (named as a peer to `MulAssign` and friends), which just assigns value like `rug::Assign`, but more operators are not something to be done lightly, and then what operator would it be? Perhaps `:=`, but that has very a different meaning in other languages, closer to the current `=` Rust operator. Or perhaps `#=`?
&gt; Or maybe I'm misunderstanding something, Yes, you are missing that not al OSes do this. This only works "like this" if overcommit is enabled. On MacOSX it is always enabled. On Linux this is an option that you can turn on/off for the whole system only. On Windows there is no way to enable this, etc. Also, this only works as long as your allocator _respects_ overcommit if its enabled. To work around overcommit on Linux many people use an allocator that does not perform overcommit to avoid turning overcommit off for the whole system. That is trivial to implement since such an allocator only needs to touch each of the pages it returns to the users once to force Linux to commit memory to them.
Have you already read the rust programming book (second edition recommended)? Otherwise you can post a small code sample where you're unsure about it and I or someone else can explain it. You can also take a look at this example ([rustbyexample](https://rustbyexample.com/scope/move.html). Remember, types like integers implement copy which means assigning a variable to another won't move the ownership. let x = 2; let y = x; // we copied x and assigned it to y // we can still use x even after the assignment println!("{}", x); // prints 2 and no compile error
I thought of this as `operator=` in C++: it may trigger an allocation if the buffer is insufficient.
They also use a phrasing for the C++ view on abstractions which I think is harder to misinterpret and should be adopted in Rust circles: zero-OVERHEAD abstractions
Yes, you're right. Relevant discussion here: https://www.reddit.com/r/rust/comments/81j1gd/blog_stropping_a_rust_worker/dv3wwe9/.compact
Maybe a tad more flexibility. Thinking of `String = &amp;'static str;` for example, it would be useless to be able to reuse the `String` buffer if you first have to create a `String` to be able to assign. So you'd probably want to be more open on what the right-hand side type can be.
It seems like the fancy new colorful compiler errors went away recently. Was there an announcement I missed?
Gotta say I wouldn't be a big fan of a `&amp;'static str` assignment causing allocations :/
I certainly understand the feeling, there are definitely too many places in C++ where allocations are non-obvious because of implicit conversions and it seems like this idea would introduce such a "beach head" in Rust. There seems to always be a tension between "easy" and "efficient" :x
I think a syntax like Python's might make more sense here: s[:] = "World";
It's a technical term, either you know that zero-cost means zero-overhead, or you don't. `zero-overhead` says the same thing, but in a way that everybody understands.
&gt; Now we would like to require the Output types of Add and Sub to be equal... However it turns out we cannot implement this method, because ironically, even knowing the types are one and the same gives us no way to convert between them. You could just use `T: Sub&lt;Rhs, Output = &lt;T as Add&lt;Rhs&gt;&gt;::Output&gt;` right? [This should work](https://play.rust-lang.org/?gist=5b60b3b91bc77c3050b4c6ed5e214300&amp;version=nightly)... 
To be fair to them, they lack the context to know where extreme skill stops and magic begins. (And possibly also have only ever worked on timescales where "highly optimized" is so fast that they can't perceive the difference.) 
I'm confused about the licensing vis a vis the implications of the linking exception: If the library is LGPL licensed, doesn't that mean the it's copyleft wrt Rust binaries since they are statically linked and there's no way to recompile a Rust binary with a new version of the static object without giving out the source? (Or, maybe there is, I haven't really dug into rustc's docs on linking). If your intention was to allow users to use this lib without copyleft restriction, shouldn't you make this code itself MIT/BSD licensed which would be broadly compatible with linking against the shared libraries?
I agree, zero-overhead is much more immediate; it doesn't require being "initiated" to understand it.
I am afraid I did not express myself clearly. I am using `String` vs `str` as an example of buffer reuse, not as the one problem to solve. In the case of the OP, for example, it would be `BigInteger = i32` (type-wise), in which case `my_integer[:] = 1;` would look weird.
Only weird in the sense that you don't typically think of integers as being composed of buffers.
Rug is mostly a safe interface to the GNU libraries GMP, MPFR and MPC that do all the actual work. These libraries are under the LGPL. Licensing Rug as MIT/Apache would potentially hide the LGPL dependency while not removing it, so I think it is the only reasonable thing to do, and I did not really have a choice on license. And as a side point, I like copyleft, and if I did have a choice, I would have probably gone with the LGPL anyway, but that is besides the point here. Also, in some places I look into the GNU library code to do some things efficiently, so even if I had linked to those C libraries dynamically, I couldn't use a different license. For example, I looked into the `mpz_invert` and `mpz_powm` functions to see how they work in order to be able to create an efficient interface in `rug::Integer::invert_ref` and `rug::Integer::pow_mod_ref`.
I feel like the placement syntax could have fit here perfectly if it wouldn't have been discontinued
My home internet connection is fine for certain things (e.g. streaming youtube) but quite terrible for other things (e.g. updating system packages via apt-get). It's also quite bad at updating the cargo registry, and lately I haven't been able to get past the "fetch" stage, as it always hangs after a few minutes (tried leaving it running for several hours to no avail). Is there a way that I can somehow "manually" update the registry without having to go through the cargo CLI? For example, I recently had to update my graphics drivers, but it would hang when I tried updating them via the command line, so I manually downloaded them via chrome relatively quickly and was able to install them manually. Is there an analogous way of updating the cargo registry? 
Ok, just wanted to point it out in case anyone misunderstood the LGPL requirements.
Unfortunately, when I was revamping the [README](https://docs.rs/crate/rug/1.0.0) and the [top-level crate documentation page](https://docs.rs/rug/1.0.0/rug/) for 1.0.0, I inadvertently removed the proper link to GMP, MPFR and MPC, and that is maybe why it was not clear in the first place. I've already updated the [git version](https://gitlab.com/tspiteri/rug/blob/master/README.md) to link to them prominently at the top, so that will be fixed in 1.0.1 whenever that comes out.
Hm, one problem is that in primary you take a &amp;mut self while in previous you have &amp;self (an immutable reference cannot be mutable). I would change the signature in previous to fn previous(&amp;mut self) -&gt; &amp;Lexed { ... } &gt; The do_match function changes self.current += 1, while previous function doesn't change it at all. Do you want to change self.current? You can only do it if self is mutable. Either mut self or &amp;mut self. 
Yeah, I have to say I wouldn't be as bothered by a new operator. Maybe it could be an emplace extension? E.g. `s &lt;- "123456"` because that has more "give something to `s`" semantics and one would expect the receiver to be able to do bookkeeping things.
Interestingly, it does indeed work. I still prefer the `Into` way, because it's more general.
/u/phaylon mentioned it somewhere else on this thread, and indeed `s &lt;- "World";` does have a nice ring to it.
Meh, crate/package/library, it's all the same to me :p
This may be a little tangential but how was your experience using habitat to deploy the application? Thanks for sharing your work.
I don't know enough about the build process in Rust (using too much cargo), but in C you can just release the object files, so people can link them together with a different library (or version of that library). For me personally, that's too much of a headache to care about, so I'd just look for a different solution.
The resulting list is not safe: destructors cannot be relied on to enforce safety (see leak-pocalypse). All that is required is to leak a `Handle`, drop the inserted element, and then pop from the list. The list will try to access the dropped element.
I didn't know what [Habitat](https://www.habitat.sh/) was. You might link to it in the README where you mention that Habitat is used for development.
Indeed, in the presence of leaks no intrusive collection can be safe. I had not considered going into this specifically because leaking handles of any kind will always mess with system properties. This shall be clarified promptly.
Me ... and I have lots of personalities
Rust newb here, how does this usually work in the bare language? If I define a struct am I required to write a constructor?
nice doc, thanks for head-up!
I think your best bet is to have a local registry (https://doc.rust-lang.org/cargo/reference/source-replacement.html). If single file downloads are faster, you could download a zip of the crates-io index and use that as your local registry. Some of the other tools on that page may help as well, depending on how your internet and development work.
Thanks for heads up! Posted.
They work for me (tested stable, beta, nightly on windows). It's possible that some compiler change made it stop working for you - what's the last version that worked? If *all* versions stopped working than it's probably something on your machine.
From https://doc.rust-lang.org/std/thread/ &gt; When the main thread of a Rust program terminates, the entire program shuts down, even if other threads are still running. However, this module provides convenient facilities for automatically waiting for the termination of a child thread (i.e., join). This is the normal behavior of threads in most languages.
That's not true - it's quite possible to make intrusive collections that are safe. For example, if you aim to be usable with *zero* allocations, then you can tie the lifetime of the elements to the lifetime of the list. This supports stack-allocated and static intrusive lists. Alternatively, if you want *some* heap allocation but you just want to minimise it, then you can use a macro/custom derive to enforce your safety requirements, and use reference counting to manage lifetimes. There are a ton of variations you can play with here to get exactly the performance characteristics you want and still be safe. The entire rust ecosystem, and in particular its standard library, are designed to be safe in the presence of leaks - marking methods as safe which cannot cope with leaks goes against that precedent, and is likely to give people reading the blog post the wrong idea when the whole premise of the post is "making intrusive lists safe".
Well I might be slightly biased since I'm one of the maintainers on the Habitat team but I really like it. It's really portable in that you can run your package on any Linux flarvored OS or export your package to a container and run it in a number of container runtimes. Also, the bindings you can define between services is really powerful. You can see an example of that in the demo app with postgres 
Fair point. I'll get that added ASAP. Thanks for the feedback!
Alright, I've pulled the post. There's no way to salvage the thing without rewriting it to explain where I came from and why some things in there are the way they are.
First year student at University. &gt; Simple graphics and interaction They hit the nail on the head with this one. Intro classes for computer science rely on graphics to keep newcomers interested and provide immediate feedback. While I think it has less importance for C++ compared to a simpler dependency management system, Rust already has one. IMO, if Rust wants to be exposed earlier to students, they need a simple graphics library (or multiple ones, with a gradual progression of complexity, i.e. one that uses RefCells internally to one that uses generics and lifetimes)
I think the biggest issue is that Github PRs don't work for discussions of more than ~40 comments. They're stuck in a single huge page which kills mobile browsers. To alleviate this, groups of comments are collapsed into a - click to expand - which can hide important parts of the discussion. There's no search and comments can only be organized chronologically - not by topic or importance or discussion thread. The only way to actually understand what's going on is to buckle down and read the entire thread start to finish which is pretty much impossible, so nobody does it except the RFC author(s) and aturon (exaggerating a bit). Large discussions need to be broken down into smaller parts which can be thought about and discussed independently. This has been done - I remember one RFC used a seperate repo for discussion. A discourse subforum should work too. eRFCs also essentially do this by splitting the discussion into: "overall idea" (eRFC), "implementation strategy" (in PRs and IRC discussions), "final design" (RFC), "stabilization" (tracking issue and stabilization discussion).
&gt; Is there a way to solve this problems without using `unsafe` Rust and/or `mem::uninitialized` that has the same performance? The thing is, `unsafe` is there for a reason. The goal should not be to avoid it but put behind safe abstractions. Admittedly, Rust is lacking safe abstractions for working with uninitialized buffers in standard library. But that doesn't mean they aren't there. `bytes` crate should be a starting point. 
There's an example in the [image crate docs](https://docs.rs/crate/image/) - see section 6. ``` extern crate image; let img = image::open(image_path).unwrap(); ``` The loaded `img` is a [DynamicImage](https://docs.rs/image/0.18.0/image/enum.DynamicImage.html). This is an enum over the various image buffer types the image could be. Its various `to_x` methods (e.g. `to_rgba`) return an [ImageBuffer](https://docs.rs/image/0.18.0/image/struct.ImageBuffer.html), which is a wrapper for a contiguous buffer of the pixel values. This supports 2d indexing, but if you want something more like a raw matrix then you can consume the image by value to take ownership of the backing data via `into_raw` and do what you like with it. 
I see what you want to do now. I didn't read the code, but doing this correctly requires you to trust implementor of the `Read` trait. So you need `unsafe TrustedRead {}` trait to express this correctly. That being said, I did [something similar](https://github.com/Kixunil/genio/blob/master/src/std_impls.rs#L81) in such way that should keep `unsafe` behind safe abstraction, so it should be easy to audit.
I think `zero-cost` is absolutely fine, the abstractions *are* actually zero-cost. Only the stuff under the abstractions has non-zero cost.
I really like [flamegraphs](https://blog.anp.lol/rust/2016/07/24/profiling-rust-perf-flamegraph/).
UniverCity is a game made using rust and lua by /u/Thinkofdeath
Thanks for explanation!
Nice article.
If you want to dabble in compilers, I'd suggest to try to go in the direction of graal/truffle instead, to investigate optimization opportunities at run-time (instead of compile-time).
It's hard to say without the data, but I'd guess your current approach is better, since you have a straightforward way to partition the graph into reasonably sized chunks.
Generating UUIDs is very cheap, so doing it server-side is not a bottleneck by any means. But I suppose the reason you're interested in client-side UUIDs is less that, and more so that you wouldn't have to maintain a mapping of old UUIDs -&gt; new UUIDs?
I have an application that I use personally that uses a bit of the wpa_supplicant DBus interface, but it's not in a form that I could release it as a reusable library without considerable work. The D-Bus interface is also fundamentally different than the wpactrl interface - it's much more object-oriented, whereas this is more procedural. You could maybe abstract them with a high-level interface that's object-oriented, but I'm not sure if that would end up being three different crates or just one crate. This started out more because I wanted to use this for a project and had to make a few patches to get it working, then Sauyon (the original creator) asked me if I could maintain the project, and I realized that making it pure-Rust was not going to be that difficult compared to relying on the C FFI provided by wpa_ctrl.c. The other transport options exposed in wpa_ctrl.c are UDP and named pipes, but these are actually controlled by compile-time flags and I'm not aware of any distro shipping wpa_supplicant with anything but unix sockets being the default.
Thanks. The enumeration idea is good, but I'd rather provide an actual API. The commands are relatively procedural, some take arguments and return data structures. I think it'd be easier for people to use if I just provided a Rusty API. I'd also like to make it work asynchronously before building a largeish API on top of it, but I've had a hard time getting into tokio and there isn't a ton of examples or contextual documentation for the unix sockets library for it: https://crates.io/crates/tokio-uds
That's not a good reason to require Haskell knowledge. Even Pandoc is written in Haskell, but you can use it without knowing Haskell. The "language-rust" project appears to require knowing Haskell to parse Rust code.
Nope! Rust actually has no formal constructors like Java or C++, but by convention we call a nifty function that produces an instance of the struct `new`. There are other options though; you could make all your fields public, or provide some other mechanism to construct an instance, like a builder.
I was playing around with some concepts introduced [here](https://doc.rust-lang.org/book/second-edition/ch06-02-match.html#patterns-that-bind-to-values). #[derive(Debug)] enum UsState { Alabama, } #[derive(Debug)] enum Coin { Dime, Quarter(UsState) } impl Coin { fn value(&amp;self) -&gt; i32 { match self { Dime =&gt; 10, Quarter =&gt; 25 } } fn state(&amp;self) -&gt; UsState { match self { Quarter(state) =&gt; state, Coin =&gt; panic!("{:?}s don't have a state!", Coin) } } } fn main() { println!("The value of a Dime is {}.", Coin::Dime.value()); println!("The state of this quarter is {:?}.", Coin::Quarter(UsState::Alaska).state()); } Getting this compilation error error[E0531]: cannot find tuple struct/variant `Quarter` in this scope --&gt; src/main.rs:22:13 | 22 | Quarter(state) =&gt; state, | ^^^^^^^ not found in this scope help: possible candidate is found in another module, you can import it into scope | 2 | use Coin::Quarter; | error: non-reference pattern used to match a reference (see issue #42640) --&gt; src/main.rs:22:13 | 22 | Quarter(state) =&gt; state, | ^^^^^^^^^^^^^^ help: consider using a reference: `&amp;Quarter(state)` error: aborting due to 2 previous errors error: Could not compile `enums`. I figured since the impl worked for value it would work for state? Would be great to get a description of how my understanding is wrong in the `state` function. 
It *did not* work for `value`. What you've done there is bind `self` to two variables names `Dime` and `Quarter`. The reason it didn't work in `state` is because you added parens, at which point the compiler realised you were trying to talk about a struct or variant. The issue is that variants are namespaced inside the corresponding variant; you need to use `Coin::Dime` and `Coin::Quarter`, or pull them into scope with `use Coin::{Dime, Quarter};`. Also, you can't match on `Quarter` by itself like that. You *have* to do something about the `UsState` payload, even if that's just ignoring it with `_`.
Very nice article, impressive for a high-schooler. Are you going to write more for other parts of a compiler? DM me if you need any help.
But if all your fields are public, do you get a constructor? If you don't get a constructor, how do you make an instance of your struct? Coming from C++ even a struct that doesn't have any explicitly defined constructors is still said to have a default one generated by the compiler. A type with 0 constructors would be uncreatable, so I assume a rust struct with all pub fields still gets a constructor in some sense?
&gt; Furthermore the static C library might be built with `/GL` in which case LLD is `not` an option because it does not understand LTCG bytecode. I wouldn't consider this a "problem"; `/GL` is [already documented](https://docs.microsoft.com/en-us/cpp/build/reference/gl-whole-program-optimization) as requiring an exact MSVC compiler (version) match as the one used to build the .lib: &gt; The format of files produced with `/GL` in the current version may not be readable by subsequent versions of Visual C++. You should not ship a .lib file comprised of .obj files that were produced with `/GL` unless you are willing to ship copies of the .lib file for all versions of Visual C++ you expect your users to use, now and in the future.
That and I guess a database is only useful if you intend to query the data in more than one sitting? Since after this data is processed, it rarely needs to be operated on again afterwards, a DB is perhaps not as useful?
If your type needs a constructor, it's up to you to make one.
There's another one of these at https://github.com/clux/webapp-rs Probably similar but it has a circle ci setup for musl alpine build setups :)
Rust doesn't have a formal notion of constructors, and visibility in Rust works differently from C++. A private field of a struct is only hidden outside of the module the struct belongs to, more information [in The Book](https://rustbyexample.com/mod/struct_visibility.html). 
Looks like a stack of work has gone into this game. As far as I can tell there's been not any releases yet. Is that that true?
I've just released a new version of relm which now outputs beautiful error messages: I hope you'll enjoy. Sorry for taking so much time to fix this issue: it was not an easy task, but now it is done.
It seems to me that this term invites exactly the same misunderstanding as zero-cost.
&gt; *Note that above, `s` is not mutable. This is consistent with `Drop`.* Can someone explain why? I am confused why `s = "World";` can work when `s` is not `mut`.
It looks like version 0.3.0 was released recently.
this is exactly the type of resource I've been looking for. excited to dig in, thanks :)
Why is this a big deal at all? You already don't need Microsoft Build Tools with the MinGW toolchain. I just copy and paste each rustup update into my MSYS2 folder and have never had any issues.
Thank you! I am still learning about compilers, and my knowledge is limited. I really do hope to be able to create parsers and share my knowledge with others to hopefully carve a trail for more people to contribute to an ever-growing compiler research. That was a run-on sentence. And of course. I'd always love help.
Awesome. I am trying to use it and learn it at the moment.
Found this yesterday and learning heaps
a cross platform idiomatic GUI is something that would let me bring rust in for more fun projects... I can't wait to try it.
You may be interested in [#45728](https://github.com/rust-lang/rust/issues/45728) where others have reported problems due to a recent update of ncurses. A pending PR to update the term package has been recently opened.
For something like LLVM, replacing small chunks at a time would be better than replacing everything from scratch. Personally, I'd suggest taking a smaller library and try to translate it wholesale. A C stdlib like musl and curses would be my guesses for a good candidate.
What's the proper way to read several lines of input? Right now I'm doing use std::io::{self, BufRead, BufReader}; use std::convert::From; struct BadError; impl From&lt;io::Error&gt; for BadError { fn from(_: io::Error) -&gt; Self { BadError } } fn get_input() -&gt; Result&lt;u32, BadError&gt; { let buf_reader = BufReader::new(io::stdin()); let mut lines = buf_reader.lines(); let first_line = lines.next() .ok_or(BadError)??; let second_line = lines.next() .ok_or(BadError)??; Ok(0) } Which compiles without errors, but just looks really wrong. Also, why does lines.next() return a Result wrapped in an Option, instead of just an Option?
Structs with public fields don't need constructors, you can build them directly like C, https://doc.rust-lang.org/1.6.0/book/structs.html. If you have private fields you'll need to write some kind of constructor, eg. `fn new() -&gt; Self { ... }`.
&gt; This only works when using nightly and when using these features [...] What unstable features are required for the new error messages? 
I don't see how. The very definition of "overhead" is "ADDITIONAL cost".
But the *implementations* of the abstractions have a cost... the cost of the processes being abstracted. That can mislead people who don't have a firm enough grasp on the theory and low-level details of how things work to understand how much optimization is possible. (ie. For all I knew when I didn't have much theoretical background, it was possible to implement an `O(1)` solution with a tiny constant term for ANYTHING and it was just a question of how skilled the programmer was.)
As a DAW user, a web based DAW doesn't sound that exciting. If you don't want to be Yet Another DAW, I think the focus should be on a good interface, a enchanced, maybe even new, workflow, good ergonomics, things like that. So, I vote for Desktop.
Me too! I missed the these kind of exercises from the book.
How about also handling env vars to tackle the [12FA](problem) ?
I'm using https://github.com/andrew-d/interfaces-rs for this purpose.
This allows for using Rust on ESP8266(and I guess ESP32)? I've used Arduino and PlatformIO in the past with C. From what I can tell the project is about generating bindings to Arduino, so that you can write code in Rust and it'll compile and use FFI to interact with Arduino? I guess that has a bit of overhead? I'm not quite sure why the alternative compiler mrustc is used/needed for. Is it something to do with the C++ binding? Seems really neat to watch progress.
Based on the [announcement to /r/haskell](https://www.reddit.com/r/haskell/comments/81vrbk/ann_languagerust/) it seems that a major motivation is to allow embedding Rust code fragments in Haskell, as is already possible with C/C++, Java and (I think) R. See [the GitHub repo](https://github.com/harpocrates/inline-rust) for an example.
Has GTK on Windows and macOS improved? It's been a few years since I last used GTK apps on those platforms but I remember them not looking all that native. There is a Qt crate/library that's an official KDE maintained project, though it may look more at home on those platforms, it's probably not as good of a development experience.
In theory, you could just supply the rlibs for the version of the compiler you compiled things with. In practice, I agree with your statement: &gt; For me personally, that's too much of a headache to care about, so I'd just look for a different solution.
Also related: - lalrpop https://github.com/lalrpop/lalrpop - gluon https://github.com/gluon-lang/gluon
https://doc.rust-lang.org/stable/std/option/enum.Option.html#method.and_then
The format string doesn't make sense to me.
Years but I remember seeing demos of GTK with some kind of web backend. Can this be used with Web Assembly?
Bazel has plenty of flaws too. Google's name doesn't guarantee quality.
Yes. The development is closed source but monthly updates are posted.
Would (opt-in) LLD for the `-msvc` target make sense or make things easier for cross-compiling from other platforms?
Their README links to [robinson](https://github.com/mbrubeck/robinson). Robinson’s README defines "toy project" and explains motivation, and links to the [Let’s build a browser engine!](https://limpet.net/mbrubeck/2014/08/08/toy-layout-engine-1.html) blog post series that says more.
The `StructName { field_name: value }` expression syntax can be use where all fields are visible. This is fundamentally how you create a new struct value. `derive_new`’s README has this example: #[derive(new)] struct Bar { a: i32, b: String, } The `#[derive(new)]` line invokes a procedural macro in that crate, which generates code like: impl Bar { fn new(a_value: i32, b_value: String) -&gt; Bar { Bar { a: a_value, b: b_value } } } This can also be written more shortly: impl Bar { fn new(a: i32, b: String) -&gt; Self { Bar { a, b } } } `new` is not special, it’s just a function associated to this type. Calling it a "constructor" is only a convention for such functions that return a new value of that type. You can have more than one, for example `Vec::new()` and `Vec::with_capacity(16)`.
`Option&lt;T&gt;` is what the `next()` method on an iterator returns. `None` means the end of iteration, while `Some(T)` is an item. However, this leaves you no option to indicate other conditions, e.g. errors. One of error conditions when reading file line by line as Strings from file could be an invalid UTF-8 byte sequence.
Most likely not; WebAssembly does not have acces to the DOM yet afaik
No public releases yet but i'm aiming to release into steam early access as soon as possible so I can get better feedback.
Statically linked VC runtime is the best option. The `-msvc` target has the smallest binaries by far and static linking of the CRT has only a negligible 100KiB size cost for very good out of the box portability.
When you do `let s = "Hello".to_string();`, `Drop::drop(&amp;mut self)` is invoked when `s` goes out of scope despite `s` not being marked `mut`. The reason is that `mut`, on a binding, is merely a lint. If you have ownership, you can always rebind to a mutable binding: `let mut s = s;`. So, in a world without `Assign`, when you execute `s = "World".to_string();`, the value denoted by `s` is first dropped (whether it's mutable or not), and then `s` is starts denoting a new value. My thought was that `Assign` should follow the same principle, to preserve today's assignment. However it was pointed that overloading assignment, potentially causing it to allocate, would break today's Rust principles that allocations are explicit by hiding the fact that `=` invokes user-defined code. Rust has at least one way to silently invoke user-defined code (`Drop`), but `Drop` is for free-ing resources, not allocating new ones. So the argument is not perfect, but having suffered from implicit conversions so much in C++ I certainly understand where it's coming from. The idea proposed was to use the `&lt;-` syntax instead to signal "assigning", and this one could require that the binding be mutable to be slightly less magical.
It's pretty trivial to use `lld-link.exe` instead of `link.exe` already. Just configure the linker through your `.cargo/config` and in most cases it will work fine. Cross compiling will still be tricky as you have to obtain all the libraries from an existing VC++ and Windows SDK installation.
If you're linking to a static C or C++ library, then you *need* to link to the exact C runtime version the C or C++ library was built with. If you built the C or C++ library with clang using VC++ libraries then you would use the `-msvc` target of Rust. If you built the C or C++ library with clang using MinGW libraries then you would use the `-gnu` target of Rust. The entire point of this hypothetical new target is to rid itself of its dependency on a bunch of C runtime bits, allowing you to have a Rust target that doesn't need to bundle along a C toolchain or depend on an external C toolchain. When you get rid of the C runtime, that precludes the ability to link to static C and C++ libraries because there's no dang C runtime and C and C++ libraries kinda need that runtime.
`&lt;-` would be cool to overload
You can make gtk apps use native looking widgets on windows, since it is all able to be themed and that is how apps like gimp do it on windows 10. What you have to provide I'm not sure of, but it's possible at least. I don't know about OS X. &gt;There is a Qt crate/library that's an official KDE maintained project, though it may look more at home on those platforms, it's probably not as good of a development experience. That project has taken the more feasible route of generating bindings from C++ to Rust, and it seemed... Funky to work with, but I never actually tried, to be fair. I don't know too much about gtk VS Qt, but for rust whichever one ends up easier to work with I will end up using. It seems like for the time being that will be gtk. And relm is a nice effort towards making it nicer to work with.
You can't link against a `.dll` directly without an import library, so as long as you're linking against import libraries, then that should be entirely fine. The import libraries just need to have the correct ABI, as MinGW and VC++ are unable to link against each others import libraries. 
No screenshots in relm.ml link, nor the homepage for the same, nor the Relm GitHub repo. I had to click through "projects using Relm" to find https://github.com/sanpii/effitask where there are screenshots. And those screenshots are impressive. OP: http://makeareadme.com/
Just because a language enables undefined behaviour, does *not* mean code written in it exhibits that behaviour. Neither does writing code in a "safe" language make it bug free or correct. Large projects like LLVM are only robust because there's a huge amount of engineering effort behind them. Any replacement with feature-parity will require a similar level of effort. Believing that a rewrite will reduce bugs almost invariably ends up being a delusion, and the manpower wasted in bringing up the replacement would have made the original project far better.
I'm having a lot of trouble wrapping my head around this code. As far as I can tell, every call you make to `bar!()`, or `moo!()` generates a new definition of `macro_rules moo` in the recursive `foo!(moo)` call. The last time that `moo!()` tries to redefine itself, nobody after calls the `moo!`, hence the warning about it being unused. I think. In this case, I would take the Rust compiler's warning to say, "This code seems to be confused about what it's trying to do with macros that define other macros, or sometimes redefine themselves."
Library is planned for a some near future to be used by fupdate and fdelete utilities. Recursion depth can be specified for each root directory to search with (among other options like follow or not to follow symlinks and search within zip archives). Parallel search is making sense when being done only within physically distinct partitions. Correct me if I'm getting it wrong. Interesting task to implement nevertheless!
Nice suggestion! I added few output formats so search results could be formatted as CSV (not safe), JSON, or \0 separated values (just in case some weird file names).
&gt; The field in question is a string, rather than an integer; does that change things? As all of my schema use the JSON integer type for versioning fields, I haven't actually tested with strings, but I suspect it might already be possible if the field is a string. &gt; I'd love to read a blog-post about using Serde's enum handling to support versioned schemas, if you know of one. Unfortunately, I haven't seen one (probably because everyone who might have written such a post is also using integer version fields) However, the basic idea would be to write a complete Serde schema definition for each schema version and then use the [internally-tagged enum representation](https://serde.rs/enum-representations.html) support to make the root of each schema a variant on an enum where your version field is the tag. Then, after deserializing it, you could `match` to dispatch to your handlers for the different schema versions.
Right, so I added a special paragraph to docs about those deliberate syntax differences with classic SQL. Hope this would help, and make users less frustrated :)
&gt; You can make gtk apps use native looking widgets on windows, since it is all able to be themed and that is how apps like gimp do it on windows 10. What you have to provide I'm not sure of, but it's possible at least. I don't know about OS X. Yeah, but that wouldn't have worked well in the past when you had Win 7 and 8, and now 10. Unless you make themed version for each I guess. With macOS they have light/dark theme don't they? so the GTK app would have to be able to detect and adjust to that, they like Andoid has over the years had several style refreshes which could lead to the app looking out of place. When I used GIMP(2013) on Windows 7 it definitely did not look native. I used FontForge I think and maybe some other linux apps on a macOS in 2016 and had a similar experience of the app looking foreign. Good to know if it's been resolved since somewhat. &gt; That project has taken the more feasible route of generating bindings from C++ to Rust, and it seemed... Funky to work with, but I never actually tried, to be fair. There's a few attempts, the KDE one is meant to be in the best shape, but binding C++ I've heard is much more painful than C which works pretty well. Only reason I think Python has the bindings is because of the popularity of the language and community size(plus PyQt was commercially backed before PySide came about and now the official Qt Python later this year) to justify the effort. &gt; And relm is a nice effort towards making it nicer to work with. Yep Relm looks really good, I'd consider it if I wasn't doing a cross-platform app(GTK hasn't integrated better on KDE yet, so I don't have high hopes for it being good on Windows or macOS).
Occasionally working on [fselect](https://github.com/jhspetersson/fselect), a tiny utility to search files with SQL-like queries (kind of a *find* replacement)
I guess you're talking about [broadway](https://developer.gnome.org/gtk3/stable/gtk-broadway.html).
It should be clear if you expand it by hand. Eventually, you end up with: fn main() { macro_rules! bar { () =&gt; { foo!(moo); println!("Hello stackoverflow"); } } macro_rules! moo { () =&gt; { foo!(moo); println!("Hello stackoverflow"); } } println!("Hello stackoverflow"); macro_rules! moo { () =&gt; { foo!(moo); println!("Hello stackoverflow"); } } println!("Hello stackoverflow"); } Note that the second `moo` definition is never invoked anywhere, so it's unused.
&gt; Just because a language enables undefined behaviour, does not mean code written in it exhibits that behaviour. Unfortunately, reality seems to disagree with you. I have yet to see any non-trivial C or C++ codebase which does not have any instance of undefined behavior. The sheer amount of effort involved in valgrind, the various sanitizers, the various static analyzers, the debug mode of the standard libraries, etc... is testament to how cognizant of this fact C and C++ developers are. &gt; Neither does writing code in a "safe" language make it bug free or correct. Indeed! However, a language free of Undefined Behavior is much more amenable to automatic proving. That is, a proof of correctness must: - in a safe language: prove that the language semantics of a function agree with the specification, - in an unsafe language: as above, as well as prove that the function is free of undefined behavior (including data-races, aliasing issues, ...). The former is much easier than the latter. &gt; Large projects like LLVM are only robust because there's a huge amount of engineering effort behind them. Any replacement with feature-parity will require a similar level of effort. Believing that a rewrite will reduce bugs almost invariably ends up being a delusion, and the manpower wasted in bringing up the replacement would have made the original project far better. I am glad that we seem to agree, seeing as this concurs with my last paragraph.
The aim of UUID is to have a unique ID that can easily protable in many database without mapping. For instance, the uuid is generated in an "event stream" and you create entities relations between them while processing the event stream, as you process it in a RDBMS using same ids. This is the best feedback I can make to you right now. If you want to improve the developper experience, let the user push its own UUIDs in the database. 
The serde impl in petgraph ended up very nice. Use derive for all it's worth, if not on the whole, then on the parts. https://github.com/bluss/petgraph/blob/master/src/graph_impl/serialization.rs
Makes sense now.
&gt; Does the drop have the same effect as killing/halting the thread? No, it does not. When the sender part of the communication channel is dropped, the receiver part will start returning None. As you can see in the example code, the `loop` is replaced with a `while let Some(...)...` or a `for`, to accept messages from the receiver only for as long as it is returning messages (stop when it returns None). When the Sender is dropped on the main thread, the receiving loop in the worker thread will end and the worker will continue executing whatever is after the loop, which in this case is `println!("Bye")`, after which the worker cleanly terminates (the closure ends). Of course, if you use this pattern in an actual program, you are free to do other things after that loop. So no, this does not kill/terminate the worker uncleanly. That would be unsafe.
That‘s a common misunderstanding. WebAssembly has access to whatever you want it to have access to. stdweb can modify the DOM just fine.
Very simple ide demos. I found rust ide support not bad.
&gt; You can make gtk apps use native looking widgets on windows, since it is all able to be themed and that is how apps like gimp do it on windows 10. What you have to provide I'm not sure of, but it's possible at least. I don't know about OS X. The look and feel of a platform is more than slapping a few PNGs over cross platform widgets.
Great! It's kinda shame that filenames are not limited to something sane, and also how ill-equipped \*nix seems to be to handle tabular data. ASCII unit and record separators would be a natural fit here, but they don't solve the filename craziness and as far as I know there is very little tooling to work with ASCII delimited data.
[removed]
Another issue with macros, historically, is that they hide things. A Python annotation can hide a lot of behavior - you may never even call the annotated function in the annotation itself. Rust macros suffer from this same complexity, and this is probably the number one "I'm afraid of marcos" issue - hidden transactions, hidden IO, hidden loops, etc. They're extremely powerful, that's their danger. Hygiene is more like a paper cut. Eval is less horrible because of metaprogramming and more just a plainly obviously horrible thing to use for security reasons (eval'ing untrusted content is a terrible thing to do and it has little to do with metaprogramming issues). Anyways, this is cool and a great overview of where macros are useful in rust. Just saying that the gut "oh god macros, no" reaction is still justified for rust - as with all languages that have powerful macros it's more of how the community encourages their use than how the macros themselves work.
[removed]
I'm not getting it. What are the advantages of using this vs. LLVM?
It's a shame the word 'macro' carries baggage from C, where they are a bit evil (although that system was the correct tradeoff for them to get problems solved early, before the language had evolved enough). Rusts macro system is one of it's strengths in my eyes.. and I do actually wish they'd beef up the C/C++ macro system a little instead of declaring it evil and trying to replace all it's use cases, which they're still haven't achieved.
I use it on macOS periodically... it doesn't look exactly modern with default stuff... but like if you install wireshark with a gui it is ok... if I was starting from the ground up, I might not select it, but it if it abstracted... it should be "worth looking at" at least... as an aside: like 10 years ago I took the time to fink all the packages to do kde and gnome on osx in X11... that was a lot of work to basically have a crappy gui run in a good one :)
It has some bad baggage from lisps too where they are hidden in that they look exactly like function calls. 
certainly I realise lisp's macros also cause controversy (hard to reason about code?) however at least lisp's macros are so powerful that they have enough people around who rave about them.. I've always been envious
It’s rather overrated imo. Sure it’s cool but it’s not at all cool enough to warrant the arrogant bitter talk you seem to see over and over again :(
well they could do the kind of things rust's extra power over C++ can do, and more - and I like what Rust can do; I dont think lisp macros are over-rated. I remain impressed (.. and Rust is a nice compromise, handles quite a bit extra but in a more controlled way).
Macros hide things, yes. That's the point. Just like a function call, as long as the macro is written well, you don't have to *care* about how it works internally. A function call can do pretty much anything in terms of cost that a macro can do. A badly written or straight up hostile function call can do whatever it wants to your process. A badly written or straight up macro can do whatever it wants to your code (But you can *see* what it's expanded to). I do kinda wonder if macros in rust could be made *more* powerful. As in, maybe you could implement string interpolation in them. So `printlni!("{x} + {y} = {x+y}")`, and have that expand out to `println!("{0} + {1} = {2}", x, y, x+y)`. Since that seems to be what a few languages are moving towards in terms of string formatting, C# and Python use it (With Python having a bit of issue with quotes that C# doesnt).
https://cretonne.readthedocs.io/en/latest/compare-llvm.html
Nice demos. One thing that bothers me slightly and means that these videos won't be used as widely is that the dropdowns are in Chinese. Obviously users would know what they stand for but it may not be as useful for people that don't use that IDE
FontForge uses its own custom widget toolkit and, if the [FAQ entry](https://fontforge.github.io/en-US/faq/#why-is-fontforge-based-on-a-non-standard-widget-set) about it is up to date, the closest it got to a GTK+ version was a limited port that the author doesn't want to maintain and extend. 
I mean C++ does it, why can't Rust? It sounds like you're saying I can't dependably use destructors to unlock mutexes or close files. That would be a huge step backwards.
The underlying structopt code uses clap internally which already handles merging env vars with command line flags. What it can't do is merge those env vars with config file options (but config-rs handles that I believe).
I think that's the right approach, and Rust got it wrong. A good macro is one where people don't even realize that they are using one. Giving special call-site syntax to macros has this turned onto the head, into some kind of "as a macro author I can do everything I want, if a user is surprised, it's his fault!".
It is, but it's also reasonable for it to say it's written in Haskell in the readme (it doesn't), and to be confused when Haskell libraries are posted in /r/rust . :-p
Doesn't GitHub infer that for you =P? 
I think the problem with hiding macros at the call site is exactly that “if the user is surprised it’s the users fault”. It’s very similar to C++ references where it looks like you’re passing a value but someone takes a mutable pointer. It’s evil. 
I wish most of these uses of macros didn't exist. ### `vec!` This should just be a varargs function, something like fn vec(ts: T...) -&gt; Vec&lt;T&gt; { let mut ret = Vec::with_capacity(ts.len()); for t in ts { ret.push(t) } ret } I've written code like fn xor(args: &amp;[&amp;B32]) -&gt; B32 { // Do something } macro_rules! xor { ($($e:expr),*) =&gt; { xor(&amp;[ $(&amp; $e),* ]) } } too many times recently because of the lack of varargs. This also causes unecessary cloning of the B32's. I'd like to pass them by value, but the only easy way to do that is to pass a `Vec` into `xor`, and since the B32's are fixed size 32 bit arrays of a reference counted type that's probably more expensive than cloning. We also see lots of places in the standard library would this would be useful, for instance HashMap::new((key1, val1), (key2, val2), (key3, val3)) Instead of the all too common let mut map = HashMap::new(); map.insert(key1, val1); map.insert(key2, val2); map.insert(key3, val3); ### `assert!` and logging I'm going to assume that we've also added optional keyword arguments in this section, because those are also pretty desperately needed. I'd rather this was written something like the following (with magic to make CallLoc work). fn assert(cond: bool, msg: &amp;str = "", loc: SourceLoc = CallLoc()) { ... } Why? Because it makes assert more ergonomic, and you can use `loc`'s in a variety of places. fn verify(x: SomeStructure) { assert!(some_condition_1(x)); for child in x.children() { verify(child) } } When this assertion macro fails, I'm going to have to look through some pretty deep stack trace to figure out where `verify` was called from, instead of knowing immediately from the message. If we passed in a CallLoc it could tell me directly. Not only is it ugly to have to write assertion code as a macro to avoid the situation, I don't think I (reasonably) can since it's recursive. Likewise we have situations like fn add(x: Vec&lt;i32&gt;, y: Vec&lt;i32&gt;) { assert_eq!(x.len(), y.len()); .... } The error messages story would be much nicer if this was fn add(x: Vec&lt;i32&gt;, y: Vec&lt;i32&gt;, loc: SourceLoc = CallLoc()) { assert_eq!(x.len(), y.len(), loc = loc); } And it's not just `assert`, one could imagine instead doing fn add(x: Vec&lt;i32&gt;, y: Vec&lt;i32&gt;, loc: SourceLoc = CallLoc()) { if x.len() != y.len() { eprintln!("Warning: Adding Vecs of differing lens ({} and {}) at {}", x.len(), y.len(), loc); } let len = min(x.len(), y.len()); .... } The `info!`, style logging macros could work in the exact same way. ### Derive, `format!`, etc. These are the legitimate uses of macros. Compile time code generation, and complex verification that isn't easily subsumed by a normal functions type signature.
And that's pretty much the unconstrained evil you get with Rust macros. "Look at the `!`, all your bets are off!" If a user has to know that something is a macro, it's not the user that is wrong, it is the author of that macro.
It would be really nice to know what plugins you used for the various IDEs. What Sublime Text plugin do you use that shows the documentation? I use RustEnhanced and RLS, but it doesn't show the documentation.
Ah ok, my bad sorry.
It doesn't look like its passing by value because C++ users know that how an argument is passed doesn't depend on how the call site looks. Rust does the same thing in the receiver of a method call.
You are being naive. This hasn’t happened for Clojure. 
Put another way: C++ users are constantly paranoid as fuck because everything will screw you over. Not even function calls can be understood by looking at the call site.
Wow, that's really interesting. I still have lots to learn about proficient Rust.
There are other languages which never had these issues.
Dialoguer seems very handy. Thanks for pointing it out.
I do think there's value in having a high level CLI crate that is built on crates like dialoguer. It doesn't mean re-implimenting the same things, but for end users, having use case oriented documentation and consistent feeling apis is very useful. So where an api for a crate changes the feel of the api, it's easy to wrap it in the high level crate.
Okay, fair enough. I guess I'm describing something else, then- a target that does distribute a C runtime. Probably along the lines of the musl targets on Linux- a small, statically linked one.
Posting this before people start filling issues in the wrong places (Xargo, cortex-m-quickstart, etc.)
Hehe, all in due time =) 
I've written a fair bit of both C++ and Common Lisp, and no, you aren't constantly "paranoid as fuck", anymore than Rust programmers are paranoid about making method calls or C programmers are paranoid that a function will screw them over by poking random memory and calling `abort`. 
You should read the [Clap author's writeup](https://clap.rs/2018/01/09/new-years-weight-loss/) about downsizing their binary via removing macro calls. &gt; Macros themselves aren’t the issue. They’re extremely handy. I tend to use them instead of duplicating code, when `borrowck` complains about the exact same code living in a function (because `borrowck` can’t peek into functions). The problem with doing this is that it’s basically SUPER aggressive inlining. &gt; &gt; It was like a gateway drug. Copy one line and everything works? Sure! Turns out that one line expands into several hundred… &gt; &gt; Looking at this code got me to think about my use of macros. It caused me to actually think about what is being expanded.
Add Scala too! `s"hello ${person.name}"`
I'm trying to write a Rust API on the other end of a Ruby Server which outputs [datetime.to_f](http://api.rubyonrails.org/v5.1/classes/DateTime.html#method-i-to_f) which is "a floating-point number of seconds, including fractional microseconds, since the Unix epoch." The above approximates that no? I ended up with the following: let tokens: Vec&lt;&amp;str&gt; = timestamp.split(".").collect(); let secs = tokens.get(0).unwrap().parse::&lt;i64&gt;()?; let nsecs = tokens.get(1).unwrap().parse::&lt;u32&gt;()?; NaiveDateTime::from_timestamp(secs, nsecs) But I would have just liked to do: NaiveDateTime::parse_from_str(timestamp, "%s%.f") 
The `CallLoc` stuff sounds like we'd be going the way of Scala implicits, which (in my opinion) would NOT be a good thing. Too much magic!
Can it handle things like `s"Hello ${foo["bar"]}"` (Without intepreting the inner quotes as matching the outer quotes)?
Yup! The parser is mutually recursive. Idk how much of a complexity burden that brings.
VarArgs don't have most of the drawbacks of function overloading. You can tell exactly what type an argument is by it's position in the call. You can generate the same machine code for every call of `vec` so `let x = vec` is valid (requires a calling convention where you pass a stack allocated array as a pointer and len...). They don't lead to having different pieces of code called depending on what you stick in the argument list. I'd link [this](https://github.com/rust-lang/rfcs/issues/960) as the appropriate issue (but there aren't any substantial comments on it). You have a good point about `assert`. I'm not overly concerned because I usually find I want an explicit message anyways, but it's so widely used it's probably worth keeping it as a macro just for that. Still, the point about being able to pass in a location stands, and the entire argument applies equally well to macros like `info` that don't need that implicit message. As for CallLoc, cool, that solves almost all of the use cases I think. I'd argue it's a bit less elegant, and that the implicit propagation through nested `#[track_caller]` functions is a bit magic. But it's probably the best that can be done without optional arguments.
I guess you could implement them like that, but that's not how I imagined it, as you say too much magic. Just have optional named parameters, somewhat like python but maybe more conservative, `fn f(y : int = 7)` could be called like `f()` or `f(y = 5)`, and maybe (I don't hold a strong opinion) like `f(5)`. Then have `CallLoc` be compiler magic that does that one specific thing and nothing else. Fundamentally getting source locations is always going to be compiler magic, so I don't feel bad about using it here.
Try [https://github.com/libpnet/libpnet](https://github.com/libpnet/libpnet).
Scala, Nemerle.
No need to apologize for an honest misunderstanding.
&gt; I'd rather this was written something like the following (with magic to make CallLoc work). I don't understand why you'd rather have language magic instead of relatively straightforward macros. I also don't think `assert` should be a function, having it be a macro leaves the door open to pytest-style assertion rewriting which would require *even more magic* with a function.
For a narrow definition of native. Calling into JS is *fine*. You can auto-generate that JS from the rust toolchain, so you don't have to actually write that JS either. That's the way stdweb went. The only disadvantage is that going from WASM to JS can be costly.
It seems that in this code snippet we avoided the hygiene of rust macros. 
If you bother to click on the tiny little bar to check, yes. Or you can just look at the "hackage" badge or whatever. It just seems like something that is worth being in the first two sentences of the introduction.
That makes sense, thank you!
I would recommend looking at the `peg` crate. It helps to understand the the theory of grammars and productions (at least for me)
Great, looks like we are all on the same page!
what emacs package did you use?
I mean, it's small, it has gpio pins, and you can run bare-metal code on it. What more do you want?
Thank you zzzzYUPYUPphlumph for voting on WikiTextBot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
Then why this message has been posted in the Rust subreddit?
I mean that function invocations increase the binary size by O(number of invocations), while macros blow it up by O(number of invocations * size of inline code). So there is a very real case for distinguishing between macro invocations and function invocations at the call site.
I think the guarantees c++ makes aren't as strong as you might think. For reference-counted resources you can create a reference loop and then fail to disentangle it properly, and even for stack-allocated or garbage-collected resources there's almost always some way to exit a process or thread without destructing live resources. That will leak mutexes and file handles unless there's some os-level protection against it. Mutexes are a good example of the kinds of things rust tries to guarantee. Safe rust can poison a mutex so that no thread can ever hold it again, but it can't (read: shouldn't) ever cause two threads to hold a mutex simultaneously. Mutex poisoning can stall a program, but it can't cause corrupted writes or other specific weirdness. I should probably mention I don't really know what I'm talking about, so it's definitely possible I've gotten something wrong.
I vastly prefer them to look different than normal function calls, since they can mimic control flow. Having every function call possibly emit `return` or `break` would be awful to maintain.
That's exactly what I would call a bad macro. Macros that would do that would pretty much shown the door on day one, because no one should need to prepare for such a possibility.
Then we'd never have had `try!`, or things like nom. It also allows one to give a semantic name to control flow constructs. In my opinion they bring a lot more positives than negatives.
Bro, no. I tried porting one of my C libraries to C++ and I figured the same thing, it's bullshit. It takes the same amount of effort as the first time, except maybe you'll have learned a thing or two about how not to do it. at best, you're looking at spending 75% of the time it took to write the original.
I'm still learning Rust, but I find macros to be great. I never got the hand of C macros, but Rust macros are so expressive and you can use them in weird places. I have a situation where I have a Result&lt;T, T&gt; and I want to early return the T (Not Err(T)!) or continue. I originally did it with the Try trait, but it's still nightly only. I instead wrote a quick macro that just matches Ok(x) -&gt; x, Err(x) =&gt; return x. It inlined perfectly. I can even chain functions off of the macro. Maybe it is because I never learned C macros, but rust macros seem like a great solution to a problem. It made my code so much cleaner.
Yes, but it's still more than nothing. I never said gimp felt native, but it certainly looks more native than other things. Behavior can be emulated, look in the toolkit is something people have to worry about, and many people think you can't make gtk use native looking widgets.
This looks fantastic. Unfortunately I never was able to get a decent IDE working. I tried a handful of different step-by-step guides, and each one seemed to have a step in it that could not be followed. &gt;Next, in [menu item that is grayed out and not selectable] do XYZ. or &gt;Next, in your console execute command ABC [which results in an error message so cryptic, Google itself never heard of it before]. or &gt; Next, go to [404 error website] and download [piece of software not maintained since 1972]. This sort of thing with every one of them. Rust itself is a dream come true, though. Almost the perfect compromise between the beauty of C# and the speed of C. It rapidly became my favorite programming language, surpassing some I have used for years. But because of the above nonsense, my IDE is whatever text editor I happen to open. Thankfully most do color text based on the .rs extension.
Have you tried Intellij / Clion? The setup was extremely painless for me. 
I don't recall if I tried that one, but I will give it a shot now and post here after.
Thank you. This definitely came in handy.
That sounds like a poorly implemented macro, not a problem users should be dealing with.
This is literal a fundamental aspect of how macros are implemented.
I asked after reading that, as it doesn't actually state the advantages of using cretonne. It mentions the differences in implementations, and if I understand correctly if you use cretonne you have to write your own optimization passes. The aims of the project are unclear to me and neither are the use cases.
I recently stumbled upon this discussion on D vs Rust that has some discussions about the macro system. I have no horse in this race, but thought someone might find it an interesting read https://users.rust-lang.org/t/rust-vs-dlang-i-want-more-experienced/4472
Isn't try more or less obsolete anyway, since `?` was added into the language? Anyway, I'm not arguing against outlawing control flow stuff in general, just that things should be treated equal. &gt; Having every function call possibly emit return or break would be awful to maintain. You can raise a panic from every method you like, and a panic is the mother of all control flow constructs. Either _all_ methods and macros should require `!` or none of them I'm not seeing the reason why the possibility of a control flow construct inside a macro should require a `!`, but a control flow construct inside a method should not.
It's not really more difficult or more complex, but once you pick a scheme it's hard to change.
May all rustlings one day grow up to be wise, confident, and helpful rustaceans. Amen.
There is no reason why the code needs to be duplicated into the call-site, instead of generating a method call to a method that is included with the binary. This issue seems similar to some people's obsession with header-only libraries in C or C++
Oh cool thanks I'll do that
Turns out Intellij is one I tried before. I think that was just a problem with the user, though, when I couldn't get it to work before. I noticed that after I added the Rust plugin, the IDE's default handling of the "run" command was not to compile and run the program, but to only run a test on it. e.g. "cargo test" I was new enough to cargo then, I would have had no idea why it didn't work. Now I do know, so I changed the configuration on that to "run --release" and am in good shape. Until / unless I come across a better IDE, this is now the thing I will use. Goodbye to the text editors, and thank you much, /u/SomeInterestingThing, for pointing me in the right direction.
There's a lot of try-like things that don't have their own operator. And, while I'd love to have panic annotations, I don't really see them as control flow in a similar vein. A panic will never exit a loop and run the rest of my code, unless my code explicitly requests that.
Calling `next` manually more than once is something of a code smell, in my mind. Maybe consider something like: for line in buf_reader.lines().take(2) { ... } One of the issues you have to worry about with `next`, is that you're never supposed to call it again if a previous call returned `None`. The iterator can do whatever it wants if you do, within the bounds of safe code, including returning garbage or panicking. Not an issue here, though, since you're short-circuiting.
This is useful feedback, thanks! I'm going to be writing some more introductory documentation soon, so I'll make sure to cover this. For now, if you have any specific questions, I can answer them here.
Can you share emacs config?
In garbage collected languages finalizers are not guaranteed to run because there may never be sufficient memory pressure to invoke the collector. In C++ destructors always run except in very limited circumstances -- you can abort, throw an exception during exception unwinding, or like you said have a cycle in a reference counting smart pointer, but these are all viewed as unusual. RAII is still the accepted best practice. You separately avoid doing the other things. Looking at Rust file objects and mutexes I think you're supposed to rely on drop.
rustenhanced and rls also
It seems like people are downvoting this under the impression it's a Rust the game twitch stream. 
spacemacs rust layer
Controversial for sure. Is gremlin the only standard query language for graph DBs? Seems like you should implement support for at least one if you're seriously seeking adoption.
spacemacs rust layer
This is also great news for people shipping binaries. As long as you're OK with a nightly compiler and write your vector code appropriately, you can now ship portable binaries that take advantage of the vector code only when the running CPU has it enabled. No need to re-compile!
Wow I love that vim setup, do you have a config/plugin list?
I see. I'd only heard of millis since the epoch. Are you going to use the microseconds for anything that requires such accuracy?
I hate to bug you with questions after you helped me out as you did, but I'd like a new project to automatically use the project directory as the working directory for purposes of building / running. But if I leave that working directory field blank (the default) then it only opens up the dialog box insisting I give it a working directory. I have not found a generic way to tell the program to use all project directories as working directories. So as of now I am stuck giving each project an absolute path to the build, test, and run commands I will use. Do you a generic way to set that path for all Rust projects?
Cool story bro, should submit it to http://chronicle.rs
I wasn't expecting the cross-platform stuff to get merged as well; that's a nice surprise.
For non-generic functions. Monomorphisation means generics increase code size in proportion to the number of distinct actual parameter lists for the generic arguments, and this is very quiet in Rust code.
Part of the problem here is that each line is being treated completely differently, so using Lines as an iterator wouldn't quite work here. I have two more lines after this as well. Maybe that's why it looks so awkward - because I'm inputting things in such a terrible format.
/r/playrust
Rustsync implements an rsync-like protocol, not compatible with the original rsync, but fulfilling the same functions with the same algorithm. Moreover, Rustsync may be used with data coming from/going to asynchronous streams, using Futures.
I'd exclude Scala on the basis that its macros are still considered an experimental, pre-production feature.
You should start here: * Vulkano (https://github.com/vulkano-rs/vulkano) to start developing against the vulkan api in rust And then maybe find a good book or online tutorial about the api. 
Is there anything different for crates like faster? What does this mean as far as auto-vectorization is concerned?
This doesn't have anything to do with auto-vectorization. This is specifically for *explicit* vectorization. (`faster` is explicit vectorization, although I'm not familiar with its internals and whether it relies on auto-vectorization in places.)
I know this doesn't mean autovectorization now but I thought this was a blocker for it. What I'm wondering is if autovectorization is the next step, or if it's further down the road? I'm not familiar with llvm but I'd expect the backend already has some support. So I could imagine it might be pretty easy, or maybe this is totally orthogonal. I thought faster was based on another simd crate and the only major downside I remember for it was that it required nightly. So if this means that when this gets stabilized, faster can be stabilized, it means that stable will have certain tasks can be parallelized trivially with rayon + faster. At that point the only thing missing from the assisted parallelism story on stable would be a gpu_iter, although that might not be practically feasible due to the overhead of passing stuff off and returning it from the gpu.