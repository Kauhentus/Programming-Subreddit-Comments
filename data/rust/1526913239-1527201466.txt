A couple people have posted ticket giveaways here when they weren't able to make it; keep an eye out.
There is no `mem::forget`. Why do you think it might be necessary to maintain safety in this case? All of the resources in a `PeripheralManagement` instance are temporal.
[This seems reasonably up to date](https://github.com/flosse/rust-web-framework-comparison).
It might be helpful to look up some references on how to implement programming languages, since that's essentially the road you're starting to go down. ;-)
I agree that lifetimes are intrinsically tied to the stack. I'm only saying the value of a structure is not. Normally saying "the stack component" is fine, but if someone is confused about what the stack actually is, we should be clear here.
Don't forget that we already need to teach `fn foo(x: Box&lt;Y&gt;) {}`. Different semantics from your examples of course, but they look the same to a newcomer. 
Good choice in books! &lt;3 From what I've seen, most parser generators in Rust kind of subsume the task of lexing into themselves. For instance, the `lalrpop` book example just defines terminal symbols as part of the grammar: Num: i32 = &lt;s:r"[0-9]+"&gt; =&gt; i32::from_str(s).unwrap(); I've used `nom` to as a lexer to generate a token stream, then written another `nom` parser that consumed that to create the parse tree, but it was kind of weird. I wonder if, since Rust's string handling is so much nicer than C's, and we no longer need to make compilers run in 1 mb of RAM, the concept of a separate lexer generator is becoming obsolete? Anyway, on the topic of immutable data structures, ML has a very common idiom of "recurse over, pattern match and mutate/collect an immutable list" which you will have a *hell* of a time translating directly into Rust. You CAN do it, but it's rather awkward. My advice is to change your angle and use mutable `Vec`'s and iterators, it gets you the same sort of effect in a different fashion. (The Hard To Reason About Part is mutable+shared data structures; ML disallows mutation but allows sharing, Rust allows mutation but disallows sharing.) Also, a few small helper functions to box things for you will make life much more concise.
Isn't this sort of re-litigating the issues discussed in RFCs 1951 and 2071? Maybe it is the lawyer in me, but wasn't the appropriate time and place to discuss these issues when the original RFCs were open? Decisions should be final at some point barring some major change in circumstances or unforeseen consequence. I don't have strong feelings on the impl syntax in function arguments, but it seems like this was already decided. Also, I think this new RFC does not take breaking changes seriously enough. I'm fine with breaking changes to fix unsoundness or to add major features as part of an epoch release, but this seems like more of an annoyance than anything else.
Why not a parser Combinator like nom instead? Great lib.
&gt; I'm moving to Rocket You wanted to say *from* Rocket, right?
It wasn't very visible at the time when it was open for debate.. When features are introduced that are controversial or unorthogonal, the debate thread should be more visible and open for longer. I only noticed that impl Trait was also being stabilized for args when it was too late to comment on it, because it was kind of hiding in the shadow of impl Trait for return types.
This worries me, too. I *hate* `impl Trait` in argument position, but I feel like this is not the appropriate time to be doing this. I feel like this should've been posted after 6 months or so with a comprehensive list of the problems the feature introduced, and a survey of the community's reactions to it... not after *one week of stability*. I had a long comment on the issue to this effect... but I'm nobody, so I didn't feel it was my place to, in effect, tell people how they should be raising issues like this.
Same feeling here about having read it too late…
Why does a generic wrapping `FnMut` segfault at runtime when I try to pass a pointer to it to C code but using a trait object with it (e.g `&amp;mut FnMut`) not segfault? https://play.rust-lang.org/?gist=261b18c08af6f57dfac65e151254b141&amp;version=stable&amp;mode=debug
Playing around with better unit testing integration with ggez. Besides the ever-present problem of "how do you unit test something that needs a human to look it and say 'that looks wrong'", Travis and AppVeyor instances don't really have GPU's which makes it rather hard to test OpenGL-y things, let alone being able to compare different GPU's and drivers. That last problem may be fixable just by [running our own CI infrastructure](https://github.com/ggez/ggez/issues/380), which is sort of fun to set up anyway. I played with Gitlab CI, which definitely works but is kind of this big lumbering BEAST of a thing (a bit like Jenkins tbh), as well as Buildbot which is much more svelte but also a bit more work to do more sophisticated things with. Any suggestions?
Having multiple crates in one repo is no problem at all. `cargo publish` will figure it out. For a test-only dependency, use the `[dev-dependency]` section in your `Cargo.toml`.
I too think `impl Trait` in argument position was a mistake, but I recognize that this particular train has sailed. Reversing a stable feature is not something we should ever do. At least not without a very good reason and a years-long deprecation period first. 
My guess is that it will cause a lot of question threads over the next few months, which will give this RFC more weight. The fact that in this short time, many people were already confused and have opened threads about this, is already the biggest argument against it, because it negates the biggest pro-argument that lead to the stabilization (that it supposedly made generics less confusing)..
&gt;again Was Rocket a stable competitor once?
Ugh thank you. That seems so obvious to me now, especially since I said I was using a trait object. I see `FnMut` and don't think trait necessarily, because it's a function. I was thinking in function pointers instead of fat pointers.
I agree. I believe that the `..=` is a awful, syntax that hurt much more legibility than it prevent mistakes. But the train has sailed and I will do with it. 
What I see here is an issue of visibility. Personally, I had no idea impl Trait in argument position was coming, and I'd been using the feature on nightly for weeks. Balancing this issue with opening things to infinite bikeshedding is hard, but perhaps a pre-release would help? If when something moved to beta there were a release-note similar to what we saw in the actual release I bet a ton of people would have said 'nope'.
I don't think it'll be that big of an issue. If you never see it you'll never think of it, the community clearly dislikes the pattern, so I don't imagine it coming up. It just feels more like an ugly quirk than anything else.
This is for rust the programming language. You're looking for /r/PlayRust 
/r/playrust
Regardless of whether or not you agree with the RFC... It might take a years-long deprecation period to actually change this now, but that doesn't mean that we shouldn't think about it and talk about it. Rust has its share of warts, and as C++ and Perl demonstrate, being able to remove bad features is as important for building a good language as being able to add good features. So if we are going to someday think "what does Rust 2.0 look like", then having discussion about this sort of thing is valuable. Rust is incredibly well designed, but is also big and complicated. Mistakes and mis-features are going to creep in no matter what, as long as the language is being improved. So something to start thinking about now is, how do we want to deal with these? Maybe we can decide certain things are deprecated for Rust 2021 Edition.
THANK U 
&gt; If you never see it you'll never think of it Yes, but newbies who stumble upon it (especially considering that the book will get rewritten to introduce `impl Trait` for args before generics) **WILL** be confused and open threads about it.
I knew about NaN is preventing ordering, didn't realize the difference between memory and register width would also have the same effect. Again though this isn't really about IEEE754 -- they just allow intermediate calculations to have higher precision. It was an Intel decision to make comparisons look at the extra bits, and if it's possible for the compiler to choose to not use those wider registers then the invariant could still be preserved.
Completely agree, also with [this](https://github.com/rust-lang/rfcs/pull/2444#issuecomment-390685822) comment: &gt; What we see here is a huge failure of public review process. Simply put, universal-impl-trait escaped attention by latching onto conservative-impl-trait PR. We all waited and prepared for the latter and thus didn't notice this obscure feature before too late.
That doesn't mean there isn't still a significant cost to every backwards incompatible change in editions. Backwards incompatible changes mean you need to know more to understand the language. Make teaching harder because now there are two answers depending on what code someone is looking at. Make it harder to upgrade code to use new features. Etc.
Reverting is clearly not an option at this point. But if a feature gets stabilized and no one likes that feature something went wrong, and that's worth addressing.
In case anyone is interested, I jus generated rust bindings for CLIPS, a C implementation of Rete, https://github.com/mtsr/clips-sys.
Perhaps with RFCs going forward additions to a feature need to either be part of the original RFC instead of separate ones OR those separate RFCs need to wait until the original RFC has been implemented in stable for a version.
I'm pretty sure it was John Carmack who said something like if there's some syntax in your language then given a big enough codebase it'll end up in there. Or something like that anyway.
Hi pretty sure it was John Carmack who said something like if there's some syntax in your language then given a big enough codebase it'll end up in there, I'm Dad!
My biggest gripe with universal impl Trait is that you can't turbofish it. This means universal impl Trait is **not** a strict syntactic sugar of generics. And that sucks. If this ever gets fixed though, I would use impl Trait in place of generics almost all the time. It's just more intuitive to me, and I'm pretty sure I'm not alone.
I also have concerns about having spent months and years on a feature that no one will want to use.
The thing is I don't find universal impl trait at all confusing or overlapping. To me impl Trait is exactly what logancc suggests in the RFC (and has been for a long time): &gt; `impl Trait` is an anonymized type known to the compiler to implement `Trait` The whole univeral/existential mental model seems to be why a lot of people disliek it, and the only way I've seen that mental model explained is via haskell... so I'm wondering if there is a connection there.
Lmfao nice try robot
I fully disagree with this RFC. The RFC process was designed to allow the user base to discuss and try features before stabilization. If something is stabilized then it cannot be removed just because some people does not like it. The only possible way to "fix this" is to include a plugin in rustfix and let it convert between various versions on demand. The user will be able to choose which syntax is preferred by him and she/he will never have to see the other one. 
I'm not sure how much extra time went into getting `impl Trait` into argument positions vs the certainly-necessary return position.
Maintaining backwards compatibility can *also* have significant costs. That's the point of design, to weigh costs against benefits.
See the section about the shadowy process. People were aware of that feature way too late to actually discuss it. I read the proposal only a few days / weeks ago, because I was too busy doing other things and didn’t realize what was coming up.
Congrats on the boldness of the RFC :) I agree with it but I don't think it will change or is worth spending time on. That being said I just want to vent one last time before moving on with my life that I utterly dislike the syntax `impl`, it sounds weird and looks weird to me. This is how I think it should be: ``` use std::fmt::Display; fn f&lt;T&gt;() -&gt; T where T: Display { 1 } fn main() { let i = f(); println!("{}", i); } ``` Someone here in /r/rust explained to me that it would not work, for reasons that I really didn't get. I still think my syntax is aesthetically superior. :D There, final whining complete. I'm ready to move on and embrace `impl Trait` (...in return type position!)
After having spent quite a lot of time learning Rust over the past year I start to be slightly disheartened at how it seems to be changing out from underneath of me, even if in a backward compatible manner. Although it is easier to learn these new additions to the language than to start over from scratch learning the language, I have an uneasy feeling that in a year or two idiomatic Rust is going to be so different from what I learned originally that I will essentially need to learn another language already. This may be hyperbolic, but it is the personal subjective impression I am starting to have. It's particularly touching my nose ridge with two fingers inducing when something that could have remained consistent, such as ... for inclusive ranges, is replaced by new syntax like ..=. This seems nit picky I am sure and I really don't mean to complain, but after investing in the area of a thousand hours into learning what was the hardest language for me to ever learn, it is simply disheartening to imagine that it is all going to need to be learned over again \(even if this is hyperbolic\) in the process of remaining idiomatic. Eventually I think it is necessary to freeze the language in a single form and only to very slowly and cautiously modify it or extend it. This is just my personal subjective opinion of course, but I would feel a lot more comfortable if Rust changed at approximately 10&amp;#37; of the rate it currently is and rather focused on stabilizing the current features and optimizing everything. 
It literally has not been given a chance to be documented yet. When the threads coming in are "I just finished the chapter of the book about `impl Trait` and I'm super confused", that's the point to start bringing this up
I dabbed in Rocket for a bit and really liked its "Request Guard" https://rocket.rs/guide/requests/#request-guards. Does actix-web have something similar?
I can totally see it as being more intuitive, but to me they are all equally intuitive after having learned them all. 
I don't think this is true at all. It had an entirely separate RFC, which was split off from the main one specifically to tackle arguments for/against this form of it from the first RFC (remember when we were all asking whether it should be separate keywords; `some` in arg position and `any` in return position)? The RFC was very specifically focused on `impl Trait` in return position, and that RFC was open for 2 months before it was merged. Just because people weren't repeatedly submitting it to the subreddit doesn't mean it didn't have enough visibility. Just that people aren't watching the RFC repo.
I'm hearing a lot of "everyone hates this feature". And _I_ do personally, but is it actually the case that a majority of people do?
This same argument can be applied to lifetime elision. fn foo(&amp;self, x: &amp;i32) fn foo&lt;'a, 'b&gt;(&amp;'a self, x: &amp;'b i32) fn foo&lt;'a: 'b, 'b&gt;(&amp;'a self, x: &amp;'b i32) fn foo&lt;'a, 'b&gt;(&amp;'a self, x: &amp;'b i32) where 'a: 'b Sometimes you don't need to name lifetimes explicitly. You also lose the ability to provide the lifetime explicitly with a turbofish if you rely on elision. It's rare to need to do so. Similarly, sometimes you don't need to name a type explicitly. You lose the ability to provide the type explicitly with a turbofish in this case. It's rare to need to do so (for type parameters only used in an argument position).
Removing potentials for confusion (`impl Trait` for args) makes teaching easier!
Actix is also the fastest Rust framework on the Techempower benchmarks, in fact it's one of the fastest full stop. The lead Actix dev also created a Gotham implementation of Techempower to benchmark his library against: Gotham fared much worse. The Gotham project lead asked for it to be taken down, which Techempower did.
&gt; Someone here in /r/rust explained to me that it would not work, for reasons that I really didn't get. It wouldn't work because you allow the caller to dictate what `T` must be, but you actually want the callee to specify `T`, it's not universally quantified (can't choose any) but existentially quantified (there's one type that it must be, inferred from the usage).
I don't see how that would allow the caller to dictate anything since the `T` is not used in any argument type?
Certainly not when you still have to teach it since you didn't actually remove it, just made it only available on the original edition. I also think that there is just as much potential for confusion about why `impl Trait` works for return values and not arguments, but that is a seperate discussion.
Yep not yet but the more i read and test rust...the better it sounds and the closer it is to being usable for functional safety. At least that's my point of view as a developer.....I even tried rewriting some code that was troublesome and I made it much shorter and as easy to understand as the code in c and it didn't take as long to implement it. The compiler helped me a the way and it was as stable as the previous code. Actually I'm startIng to regret my previous comment. I thought at the timr that rust was anlong way before being usable for microcontrollers but the more I read I notice we're getting close pretty fast. In The next few weeks i'll probably take one of my dev boards and play around in rust with them. Not sure if you can tell but apparently i'm loving rust. It's been a while since a programming language got me excited
impl trait in return position has been implemented for far longer than impl trait in arg position. It's a bit unfair to stabilize both at the same time
Can the people who down voted me explain why please? Is it because my response is not exactly on topic for the discussion underhand here and rather went off on a tangent related to the poster I responded to? Or is it simply because you disagree with my personal subjective opinion regarding what the syntax for inclusive ranges should have been? I oftentimes find that my responses are down voted here when I say things that could be perceived as critiques of Rust, and I want to know if it is because I am expressing them in inappropriate threads unknowingly \(due to the topic being more stringent than I realized\), or if people are simply upset at me for stating my opinions regarding Rust that are not singing the praises of it. 
The caller are able to use the turbofish to choose the return type: f::&lt;String&gt;()
Do you find yourself frequently needing to turbofish type parameters that are only used in an argument position? The only time I've ever needed to do so was when passing a generic `extern "C"` function pointer. However, I have several cases with functions where one parameter can always be inferred, and the other never can. I'd love to be able to use `impl Trait` to remove the pointless `, _` from every invocation of that function.
The caller can explicitly call `f::&lt;MyType&gt;()`.
This feature was literally stabilized weeks ago. It hasn't even been documented yet. It takes a lot more time than this to realize if it's a mistake (ideally that just happens while it's nightly only like with type ascription, placement new, etc).
&gt;See the section about the shadowy process. People were aware of that feature way too late to actually discuss it. I read the proposal only a few days / weeks ago, because I was too busy doing other things and didn’t realize what was coming up. It does not matter. That's an issue with the RFC process. You still cannot remove a feature without an exceptionally good reason. Your reason is far from that.
Parse ends up with a turbofish a lot.
I'll check it out. Perhaps I was being too narrow in my thinking, but I had already written the grammar for Menhir and found a Menhir port for Rust.
What about the process was shadowy? If the opponents if this did not pay attention to the RFC, why are the supporters at fault?
I don’t know about majority, I just pasted a few links and I see threads showing up / people on IRC discussing it.
Well, it doesn't compile because f cannot fulfill the requirement of its signature. If you have: fn f&lt;T: Display + Default&gt;() -&gt; T { T::default() } You can choose the return type by the syntex mentioned above.
&gt; Just that people aren't watching the RFC repo. Yeah, so it didn't have enough visibility.
Thanks for the reply, the same thread continues under PM_ME_WALLPAPER's response.
Cross-posting from the RFC thread: We are going to take the unusual step of immediately closing and locking this RFC. While there are clearly a lot of feelings on all sides of the issue, the fact is that this feature has shipped on the stable channel and is not going to be removed. This is both because of our basic stability guarantees but also because the decision was made in good faith and pursuant to our process, and we stand by that. After multiple years of RFC and tracking issue discussions (the first one was RFC #105 in 2014!), the Rust Language Design Team ultimately reached a decision to ship this feature. This decision was not reached lightly. We discussed in depth a number of alternatives, including limiting impl Trait to return position as well as using an alternate syntax. Each has its pros and cons, and ultimately a judgement call had to be made. This is the nature of the language design process — indeed, any decision process. Few decisions are clear cut, which is why our process includes a number of points where feedback can be given, including a number of final-comment-period advertisements and the like. As a community, we have made a deliberate choice to slow down development to ensure thorough vetting and input into the process. To be clear: we understand that there are downsides to this feature, and that some people find those downsides concerning. All of us care deeply about Rust, and it can be distressing to see people in power moving things in a direction you dislike. But, at the end of the day, we have to be able to make — and stick with — decisions, striking a balance between long-running feedback and shipping. Rust 2018 will ship with `impl Trait` in argument position. -- @nikomatsakis and @aturon, on behalf of the Rust Language and Core Teams
You could say that makes input/output *types* consistent with input/output *values*: `fn f\(\_: u8\) \-\&gt; u8` takes a caller-chosen u8 value as input, and produecs a callee-chosen u8 value as output.
I'm pro universal traits can be used anywhere a type is expected. I don't get all the hate. It seems to mostly boil down to arguments that it's confusing or that you can't turbo fish it. But it makes perfect sense to treat impl T as a legitimate type with some built-in coercion rules, rather than ad hoc syntax with intricate semantics.
For integration testing of networked services, I like to use cucumber-js to make assertions on how the user would use the API. It feels icky mixing Rust and Node but that was deliberate. The tests are in a different language, so it forces me to use the API as a user would. For instance, here's the tests for a web service I've been tinkering on to learn Rust, The first version was written using Rouille: https://github.com/ericmoritz/rs-events/tree/master/tests For the actix branch, the tests are the same, I didn't have to rewrite them. https://github.com/ericmoritz/rs-events/tree/actix/tests I even do this with CLI commands: https://github.com/ericmoritz/bullet/tree/master/tests 
I used to think impl in argument position was bad, for the usual reasons people bring up, but I've actually come around to it. Biggest pro in my book is that it allows you to segregate between the generic parameters that actually matters for the API, and the ones that only exists because you want to avoid boxing. Consider what a map function would look like: impl&lt;T&gt; List&lt;T&gt; { // This is what map would look like if Rust had implicit boxing fn map&lt;U&gt;(self, f: Fn(T) -&gt; U) -&gt; List&lt;U&gt;; // This is what it looks like today fn map&lt;U&gt;(self, f: impl Fn(T) -&gt; U) -&gt; List&lt;U&gt;; // This is what it looks like under this RFC fn map&lt;U, F: Fn(T) -&gt; U&gt;(self, f: F) -&gt; List&lt;U&gt;; } Option 1 and 2 has vastly better readability IMHO \(but 1 is obviously a no\-go in Rust\). At any rate, this RFC is severely lacking. It acknowledges that it is a breaking change but presents no plan to deal with that. Yet the "Unresolved questions" section is empty? 
Other way around. Impl trait in argument position is still existential, not universal. And anyone who's done an epsilon more type theory than Haskell understands how impls, as they are currently implemented, are exactly sigma types.
Parse's type parameter is in return position, not argument position.
Yes, but that is because it doesn't use a type parameter in argument position. So you can't even use the impl syntax with that one.
There's no possible way to tell. I didn't like it at first but have warmed up to it now.
Universal in my comment was in reference to `#![feature(universal_impl_trait)]` not univeral quantification ;)
`...` is not a syntax that appears in patterns.
I like the feature.
What happened is there were two (or more) separate RFCs. One was for existential impl trait and the other for universal impl trait. If you were to object to UIT on the EIT rfc you'd be told "this isn't the place for that". But when it came time to stabilize the team played by different rules: the longevity of EIT was used to justify stabilizing UIT. In other words, the community just got told "they're the same feature for me but not for thee."
It's the second part. This subreddit is quick to downvote things that are critical of Rust or projects written in Rust. I brought it up when someone asked if people prefer the official forums or this subreddit, and my comment was immediately downvoted into the negatives and people replied that I deserved to be downvoted for saying that people downvote too quick on this sub.
&gt; The whole univeral/existential mental model seems to be why a lot of people disliek it Dislike what? Just because you conflate universal and existential quantification doesn't make the difference go away.. It's **useful** to be aware of the difference, to be able to use the right one! Otherwise, you get people being confused because they are writing `fn foo&lt;T: Trait&gt;(..) -&gt; T` (universal quantification) when they actually want `fn foo(..) -&gt; impl Trait` (existential quantification). No matter the syntax, one should **always** be aware of which kind of quantification you're using, because you have to use the right one that does what you want.. The mental model (distinguishing universal and existential quantification) is not the problem but conflating their syntaxes into one so that newbies don't immediately see when which one is used.
I'm not sure I understand your point. Do you think we should have a bot which posts every RFC in this subreddit? If you care about seeing every feature introduced, you should watch the repo. It's literally only one click.
Just to be clear, are you keeping in mind that `impl Trait` in return position refers to a single type (per monomorphization) whereas arg position is essentially the entire class of all types implementing Trait? That's the universe/existential thing people are talking about. [Here's an illustration](https://gist.github.com/c849af28f9725b209cb1de5cda84e122).
This is a false impression; I love Haskell with every fiber in my being and I think `arg: impl Trait` is fine, readable and a lightweight way to do what I want. I also think that the RFC process needs to be respected and we can't go about making such "undoing" RFCs without significant technical (and not just subjective) reasons.
I'm just not sure what people think the solution here is. There's a `Watch` button on the RFCs repo if you care to see every RFC that goes through. Perhaps we should have a bot that posts here every time an RFC is actually merged.
I'd prefer if our solution to `_` in turbofish is not "impl Trait for args" but do it like D: &gt; in the D language, turbofish allows omitting type params (from the end), they don't have to be substituted by `_`. So then you'd put all the inferrable type params at the end of the &lt;..&gt; list when defining the function so that a caller can omit them. https://github.com/rust-lang/rfcs/pull/2444#issuecomment-390712408
... is inclusive range in patterns [https://doc.rust\-lang.org/book/first\-edition/patterns.html](https://doc.rust-lang.org/book/first-edition/patterns.html) \&gt; You can match a range of values with ...
Wat. TIL
as an alternative, you could use this: type HexData = Vec&lt;u8&gt;;
I think that there's a tradeoff between the two, which is a separate discussion. For me the biggest draw of `impl Trait` in argument position is having parity between `fn foo(x: &amp;Trait) -&gt; &amp;Trait` and `fn foo(x: impl Trait) -&gt; impl Trait`
&gt; Certainly not when you still have to teach it since you didn't actually remove it, just made it only available on the original edition. But the big thing is what the book teaches. Right now the plan is to rewrite the book to teach `impl Trait` for args. If we remove this feature, we wouldn't need to teach this in the book. And newbies will start with the latest edition where it wouldn't be available then. &gt; I also think that there is just as much potential for confusion about why impl Trait works for return values and not arguments, but that is a seperate discussion. The key is that people should be aware of the difference between universal and existential quantification. Having the same syntax for both makes it confusing for people. It's easy to understand why existential quantification is used for return types but universal quantification is used for arg types. Because the caller passes the args but the callee passes the return value, so they dictate the types in each case.
I honestly don't know what's the issue with having duplicate syntax. Like, `impl Trait` is how it works already in other programming languages. For instance, in Java. &lt;T extends Serializable&gt; void serialize(T argument) {} is identical to (minus not being able to specify the generic type as it's not generic in this case): void serialize(Serializable argument) {} It's mostly a question of getting used to it, but I feel myself that `impl Trait` simplifies the APIs, as there is no longer generic type name noise. Sure, it would have been better without having to explicitly use `impl` keyword, but the train has sailed.
The issue came up when release notes were posted. It seems like the easiest solution is to post something like those release notes earlier.
My proposed, loosely defined solution exists in two other posts now.
I'll grant that the feature isn't something I use universally; usually, I'll employ it only in simple cases where it's an unequivocal win in terms of mental or syntactical complexity. I don't have a problem with this existing and being used alongside the exiting &lt;T: Whatever&gt; syntax. Is there some other problem with this that I'm not seeing, or does it really boil down to A) there's more than one way to skin this cat, and B) this way isn't as powerful as the other way?
&gt; This subreddit is quick to downvote things that are critical of Rust or projects written in Rust This thread itself is full of upvoted criticisms of Rust. &gt; people replied that I deserved to be downvoted for saying that people downvote too quick on this sub. This is extremely common on many reddits and forums.
Yes, actix has similar concepts, extractors and route predicates.
I was using that feature in my code for what seemed like decades (...probably six months? I dunno...) before it was finally stabilized.
It's consistent with the fact that if you have something of type impl Trait, the type has already been chosen, whereas if you're providing something of type impl Trait, you get to choose the type.
I see, thank you
Cool. Not that benchmarks are the most important, but that's still interesting.
This will be fixed in Clippy 0.0.204
You can implement `FromRequest` for a custom type in actix and end up with the same. 
I thought one of the biggest benefits of `impl Trait` in argument position was that it could complement a future `dyn Trait` syntax, to make it clearer when different types of polymorphism are happening. I'm surprised so much discussion of the feature is happening without really bringing up that long term plan (whether to support it or criticize it).
I didn't downvote, but I suspect it's because it's off topic. Your parent comment brought up `..=` as an example of something they dislike *that they wouldn't try to remove* because it's stable. You proceeded to talk more about why you dislike it, which... doesn't add anything at best, and stirs up unnecessary arguments at worst.
Great job implementing CDCL in Rust. I know there is [minisat-rust][1] which is a port of Minisat but it is slower than the original Minisat. So, I am curious how your CDCL implementation performs compared to Minisat. If it is indeed faster than Minisat, how is it compared to the state of the art C/C++ sat solvers like Glucose, Cryptominisat? [1]: https://github.com/mishun/minisat-rust
I think this is good to emphasize that this is all connected: parse &amp; collect require a turbofish precisely because they use a universal in an unusual position (the return type), whereas turbofishes are essentially not required for argument position in universals. The position based polarity of `impl Trait` is designed to work well with the way Rust's type inference works. Univerals in argument position and existentials in return position are what you almost always want, and what works very well. When you want something contrary to that default, you can use the more explicit form (generics for universals and a still-unfinished named existential syntax for existentials).
I would checkout the `dyn` trait RFC it lays out the plan better for it. In short: Rust 2018: map&lt;U&gt;(self, f: dyn Fn(T) -&gt; U) -&gt; List&lt;U&gt;; // old Fn(T) -&gt; U is still legal but linted against map&lt;U&gt;(self, f: impl Fn(T) -&gt; U) -&gt; List&lt;U&gt;; In rust 2021: map&lt;U&gt;(self, f: Fn(T) -&gt; U) -&gt; List&lt;U&gt; // mono-morphises equivelent to `impl Fn(T) -&gt; U` map&lt;U&gt;(self, f: dyn Fn(T) -&gt; U) -&gt; List&lt;U&gt; // dyn syntax is the only legal way to write this now I think the discussion of if/when the "naked" trait syntax should swap to be "impl" is still open for discussion. But its pretty safe to say that "Trait" meaning dynamic dispatch after rust 2018 will be rare.
`impl Trait` and `dyn Trait` are completely different features. The former is universally quantified, statically dispatched and makes the function polymorphic in – at least – that argument. The latter is dynamically dispatched and doesn’t participate in the polymorphization of the function (if it doesn’t have any other argument for instance, it’s monomorphized). So both the concepts are orthogonal.
Then what does the string represent? If it's some money amount, why not use u64 or i64 or some BigInteger / BigDecimal implementation? Do you do this casting between dollars and euros (eeew!) so often that you can't just create new instances (e.g. `fn a_to_b(a: &amp;A) -&gt; B { B { s: a.s.clone() } }`? Or you can consume the `A` and produce a new `B`, and convert back later. 
Continuing work on [tarpaulin](https://github.com/xd009642/tarpaulin), last week I fixed my issue where it didn't work properly on the latest nightly. Also, improved automation around automated docker hub builds and added more tests. This week I'm continuing work on tarpaulin and also going to go back and do some more rust embedded with the aim of helping document peripherals in [stm32-rs](https://github.com/adamgreig/stm32-rs) and working on my own embedded-hal library.
No books but, here are two resources for further looking into concepts used in rust: 1. https://github.com/brson/rust-anthology/blob/master/master-list.md 2. https://forge.rust-lang.org/bibliography.html Where the first one is mostly blog posts and the second one mostly papers.
`Trait` meaning `dyn Trait` (ps: `dyn Trait` was partially stabilized in nightly) is definitely going away in the 2018 edition. I've seen arguments against making `Trait` mean `impl Trait`at all because then upgrading from 2015 edition to whatever will implicitly change the meaning of `Trait` from `dyn Trait` to `impl Trait` which will usually but not always break things.
I'm still working on [subgit-rs](https://github.com/samsieber/subgit-rs). It's progressing well and passes a couple of basic tests I have written. I wrote my first unsafe rust code for it today - I had to close file descriptors after double forking so that the git hook I'm writing could run asynchronously. I basically ported this code in the answer from this stack overflow question, using the nix crate for the forking: https://stackoverflow.com/questions/41494166/git-post-receive-hook-not-running-in-background So, all in all, it's going pretty well! The code still needs lots of cleanup, better logging, and a ton more tests.
In short I added another associated type, `Kind`, that allows quantities to be constrained beyond their dimension. The initial implementation is less than [50 lines of code](https://github.com/iliekturtles/uom/commit/3b82692827f40bcbe3f3d0d393a16c580ff6a615#diff-61a2f5e6ec6e94c7849b89378f6b5ecbR19) to add `Kind` and make thermodynamic temperature have a separate kind. I still need to fully explore this solution, but so far it looks promising. Currently the default kind is `()`. Is there a better default? `!` (bottom) maybe? Thermodynamic temperature's specialized kind is `struct Temperature`. Is this the appropriate type? Maybe using an empty enum `enum Temperature {}`. What is the appropriate way to convert between kinds? `From`? Should there be conversions? Tests need to be added. Does everything still work as expected? What about temperature specific operations? e.g. thermodynamic temperature + thermodynamic temperature doesn't make complete sense but thermodynamic temperature + temperature interval does.
Note that Rocket doesn't support *https.* Also note that Diesel doesn't support TLS-protected PostgreSQL connections. I wasted a few hours struggling with Diesel's examples and eventually gave up. Then I got productive in minutes with the [Postgres](https://github.com/sfackler/rust-postgres) crate, including TLS-protected sessions to a remote database, using minimal boilerplate code (Diesel requires quite the boilerplate using macros, env-files, migration management, etc) and allowing transactions, administration, etc. I can't think yet some compelling use case for Diesel.
Is there a way to force the rust integration tests to build a binary before running? The binary for my crate is supposed to be used as a git hook, so really testing it integration style requires copying it into a repo I create during the test. Currently, whenever I run my tests, I have to build the binary, and then run the test. And that gets old.
I'm aware of the difference, I just don't think it should be at all central to your mental model while programming.
Rocket has a configuring TLS section in their guides: [https://rocket.rs/guide/configuration/#configuring\-tls](https://rocket.rs/guide/configuration/#configuring-tls) 
It's quite noticeable that the RFC spends around 18 paragraphs (and a very long list) with rehashing old discussions and bringing counter-arguments to the table again, but only feels like 2 (very small and standard) paragraphs with Drawbacks. It kind of shows that it _doesn't_ spend enough time with the significance that is propose. 
I don't think we should back anything out after just one week. Otherwise the process means nothing. Let's wait a little longer to see how the new syntax shakes out. 
Yes, there is a way to build a test without running: `cargo test --no-run`.
[removed]
[Ask and you shall receive!](https://www.youtube.com/watch?v=UiV8lIlCTo8)
I didn’t mean it one way or the other about actix-web. They’re both great and I’m glad they both exist. I’m watching actix by itself too, hopefully for distributed actors.
Ah, I should have been more clear - I need to build the binary for the crate (In my case, manually triggering it requires "cargo build --bin hook"), but I'd love to have a way to require that the the tests run that command before running.
I should've mentioned that I just picked u64's for the purpose of demonstration. My actual T's don't have Default implementations.
I take it your array has a certain size? How many elements are we talking about?
The loop seems fine to me, but you certainly don't need the unsafe. Use `let mut result: [u64; 4] = [0; 4];` or, as /u/_Timidger_ suggests, `let mut result: [u64; 4] = Default::default();`.
Both arrays are size 32, in my actual program.
First of all, you won't find code using arrays very much in Rust, since their size has to be known at compile time. That aside, the following snippet should do the trick: let data = vec![Some(1), Some(2), Some(3), Some(4)]; let result = data.into_iter().map(|o| o.unwrap()).collect::&lt;Vec&lt;u64&gt;&gt;(); println!("{:#?}", result); the `vec![...]` macro will initialize a `Vec`, with the same syntax as an array initializer. Note that `Vec`'s do have a known size at comile time, since a `Vec` is a triple of its size, capacity, and a pointer to the heap location where the actual data is stored. It is necessary to annotate the `collect` method with `::&lt;Type&gt;` (this notation will also be referred to as a turbofish) since the typechecker can otherwise not narrow down what type exactly `result` should be. Any other type that implements the `FromIterator` trait can be substituted here, which are most often `std`'s collection types. The following approach would also work with the array you have in the original code, since it also implements the `IntoIterator` trait.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/rustjerk] [\[RFC\] Undo universal impl Trait](https://www.reddit.com/r/rustjerk/comments/8l3lr9/rfc_undo_universal_impl_trait/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
I have a really silly suggestion, but you *can* do this without using any `unsafe` code, it's just fucking hideous. So try this: fn unwrap_all&lt;T&gt;([x00, x01, x02, x03, x04, x05, x06, x07, x08, x09, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28, x29, x30, x31]: [Option&lt;T&gt;; 32]) -&gt; [T; 32] { match (x00, x01, x02, x03, x04, x05, x06, x07, x08, x09, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28, x29, x30, x31) { (Some(x00), Some(x01), Some(x02), Some(x03), Some(x04), Some(x05), Some(x06), Some(x07), Some(x08), Some(x09), Some(x10), Some(x11), Some(x12), Some(x13), Some(x14), Some(x15), Some(x16), Some(x17), Some(x18), Some(x19), Some(x20), Some(x21), Some(x22), Some(x23), Some(x24), Some(x25), Some(x26), Some(x27), Some(x28), Some(x29), Some(x30), Some(x31)) =&gt; [x00, x01, x02, x03, x04, x05, x06, x07, x08, x09, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28, x29, x30, x31], _ =&gt; panic!("Your error message here") } } Does it make your eyeballs bleed? Yes, yes it does. *But* it doesn't require `unsafe`, it doesn't require any trait bounds, and it doesn't require any casting. Consider it.
This is the tracking issue for Rocket to compile on stable Rust if anyone wants to keep track of it. https://github.com/SergioBenitez/Rocket/issues/19
Quickly checking the RFC points. &gt; Two syntactic schemes to express parametric polymorphism (universal): I find it interesting that you consider `&lt;T: Trait&gt;` / `&lt;T&gt; + where: Bound` to be a single syntactic scheme :). Either way, `impl Trait` has an advantage of being more compact for most common usecases - having to use a generic type variable is generally rare with this syntax. Continuing the discussion in next point, because they are very similar. &gt; `impl Trait` is strictly less powerful than type variables plus traits, so very limited. While it may be mathematically nicer to have a single powerful construct, it's not always necessary. For instance, consider `print!`. It does nothing that `write!` couldn't do, and yet, it provides an useful default for quick programs. Or for that matter, it's possible to use `loop` instead of `for` for iterating over iterators, but `for` is less error-prone making it more useful. Syntactic sugar is not always a bad thing. Also, consider that because this syntax is less powerful, in can be easily converted into more powerful syntax in case you want to prove some properties about Rust language. &gt; Even some examples requiring only one place type variable cannot be expressed with `impl Trait` (`T: Add&lt;Output = T&gt;`) without a lot of confusion. It's a less powerful construct, it doesn't support this usecase, it's fine. It doesn't need to support 100% of usecases. &gt; It’s very hard to tell what are the variables to monomorphize a function, especially if you mix both the style. Why do you need to know :)? The compiler will monomorphize as needed. From user's point of view, it works. &gt; Confusing because people are used to `impl Trait` as existential. It is always existential. Even on argument position. Here, let me even put a mathematical law even. forall P, Q. (forall x. P(x) -&gt; Q) &lt;=&gt; ((exists x. P(x)) -&gt; Q) Universal quantified types are identical to existential when on argument position. Quite magical, isn't it? &gt; The main argument was that impl Trait instead of generics would make our life easier, for both newcomers and proficient users. The many threads created and confusion out there is a proof by itself that reality is way more blurry than expected. If you check the threads, the feature isn't as much confusing, as it's not quite understood when you should use compared to generic type variables. This is like a week after the feature was released, let's be patient, there are always more questions for new features :). &gt; impl Trait for argument position is non-orthogonal, which makes it harder to decide which syntax to use since there are now two (even three) ways. It’s very likely that even with the book, this question will come up over and over. If you need an advice about that, let me provide one. Always when possible, reserve type parameters for cases when you actually need those or due to backward compatibility requirements. This is much nicer than `&lt;T: Trait&gt;` syntax and less verbose than `where T: Trait`. This is how generics are usually used in languages like Java. I think this actually could simplify the question, because now there won't be really a tradeoff between `&lt;T: Trait&gt;` and `where T: Trait`, allowing the users to pretty much ignore `T: Trait` syntax. Isn't that making the language simpler? &gt; It’s not that easier to learn than generics, because it looks very different from the generics / template syntax programmers are already familiar with (C++, C#, Java, D, etc.). C# and Java do in fact provide `impl Trait`, although here it doesn't use `impl` keyword. Consider the following Java function: static Serializable x(Serializable y) {} This is identical to the following one in Rust: fn x(y: impl Serializable) -&gt; impl Serializable {} As you can see, this mechanism is very well recognized in Java, arguably more common than generic type parameters. &gt; Universal and existential quantification shouldn’t be conflate. Being aware of the difference is a plus for someone to learn Rust. impl Trait should only be used for the existentially quantified variable and &lt;T: Trait&gt; / where T: Trait should be used for universally quantified variables. The impl Trait should stay in the return location and let, const and static bindings. As I said before, it's always existential. &gt; Because Rust is still a hard language to learn, newcomers will still have to read the book several times or maybe spend some time practicing, at least. The argument stating “having impl Trait for argument position allows postponing introducing generics until later in the book” doesn’t really make much sense then. Agreed. Isn't really a point against a feature however. Also, you can delay introducing this feature as needed, it's not really necessary most of the time. I think it would be better to consider this to be a syntactic sugar making code simpler and easier to read, rather than a feature to help learning the language. &gt; For most newcomers, as they come from a mainstream / popular language, they are already used to the angle bracket notation, which is misleading if you consider the impl Trait syntax plus the angle bracket notation. Wait, so users are used to this? &lt;T extends Serializable&gt; void f(T x) {} But not this? void f(Serializable x) {} Checking various Java tutorials, I find this claim to be unlikely. &gt; People seem confused with the three syntaxes. This is a duplicate of "The many threads created and confusion out there is a proof by itself that reality is way more blurry than expected." point. &gt; What makes Rust hard to learn is not universal quantification. It’s more about the borrow checker and linear / affine type system. The raw and bare concept of universal quantification is actually pretty simple to wrap your fingers around. And yet, I see people asking about stuff like this. fn x&lt;T&gt;() -&gt; T where T: Fn() -&gt; i32 { || 42 } It is intuitive on argument position, but so is `impl Trait`. Agreed about borrow checker, linear/affine type system being overall trickier. Not a point against a feature however, and as I said before, perhaps it would be better to not consider this as a feature to help learning Rust. &gt; The argument of symmetry (having impl Trait in argument position mirroring the impl Trait in return position) doesn’t seem to account for history: we’ve been using Rust without impl Trait in argument position for years now and no one has ever felt the need to mirror existential quantification with universal quantification in argument position. You know, Rust dealt perfectly fine with `try!`, why add a weird `?` operator? After using `?` for a while, despite my initial opposition to `?`, I like it much more than `try!`. Perhaps, just perhaps try using it in your code when it makes sense, see if it works better. &gt; People can already write a lot of code without even needing polymorphic code / universal quantification, especially newcomers. Education / teching about type variables and universal quantification can be postponed to the end of the book if the authors are afraid it’s too hard for newcomers. Yes, you can. That's a good argument for delaying postponing this feature in a book. It isn't a point against a feature. &gt; It seems like proficient developers have no reason to use impl Trait in argument position, so why encourage newcomers to do so? What is the real value added by such a feature? It makes the function signatures more readable, none of this `arg: T` stuff, and having to search for what `T` means. &gt; About the dialectical ratchet [...] Again, an argument about newcomers. Yes, yes, I agree, this feature doesn't really improve the language for newcomers. Still not a point against a feature. &gt; The Rust language is already a very, very complex language [...] Newcomers. And as you said yourself, you can delay teaching this feature. &gt; Overlapping concepts harm the consistency and orthoganality of a language. Having several ways to do something forces people to know all the ways – even if they always prefer using one – because they will maintain and contribute to codebases they haven’t read nor written code for before. `impl Trait` would be here whatever it would be allowed for arguments or not. Allowing it for arguments actually increases the consistency of a language. &gt; Turbofishing is impossible with impl Trait, forcing you to use type ascription in other places. However, this is mitigated by the fact the arguments can be inferred most of the time. There are still some cases in which you cannot (when you pass a higher-kinded type, e.g. None – you’ll have to use a let binding or something similar). Yes, but also because `impl Trait` is existential, the impact of this is reduced - generally types need to be explicitly mentioned with universal output types. Still a concern when mixing type parameters with `impl Trait`. I think this could be fixed later, by providing good semantics for turbofish in functions containing `impl Trait` in its signature. &gt; The initial RFC and especially the dialectical ratchet seem to state that people do the same thing in Java in C#. They don’t. In those languages, the functions using such similar syntax are completely monomorphic (dynamic dispatch). The equivalent in Rust would be the trait object or dyn Trait syntax. That's true, but also as long you don't try to modify an `impl Trait` variable, it will work identically to Java. Modifying arguments seems unusual enough, even when Java allows that. &gt; [This is a proposition]: in the future, we could do some A/B testing to determine which change to Rust makes it easier to learn prior to stabilization. [...] Is it even possible to do A/B testing for programming language features? Either way, there is beta. There is nightly. Feel free to submit feedback about features when they aren't stabilized yet.
Thanks for responding. However, in my program I am making a conscious decision to avoid using Vectors. I am basically trying to 'chunk out' my data, so instead of having a list of 75 values (completely arbitrary number), I have 2 vectors of 32 values and then a 'excess' array of Options filled with 11 Some's and 23 None's. The idea is that I can then save those chunks to hard drive when they are not in use. The code in the link comes from my 'push' function. I first check to see if there is any space left over in the excess array, and if so, put the new value there. However, if the excess array is full, I execute what basically amounts to the code above, replacing data with the excess array and result with the new chunk.
The first question is, does increasing the dimension further continue to make it faster ? And the answer is no, the faster is with dimension = 4. The second thing you can see, is that if you replace the `range` call with a call to `contains`, you still see the same strange timings. This means that this not specific to generating the range, but to searching in the btree. The second thing to check is if the difference is because the method is called a different number of time depending on the number of dimensions. This is not the case. Next, if you count the number of call to your `cmp`( from the `Ord` trait) implementation (with a global counter, or by printing something and using `uniq -c`), you will see that time taken is pretty much correlated with the number of call to `cmp`. Now the question is, why does `BTreeMap` need to do more work and do more comparison with dimension = 4 than with dimension = 2 ? And why for the "simple code" version, this is not the case ? I don't know, this may due to how your comparison function compare things, or the order in which you insert the element to the btree, or something else. 
Just in case you haven't heard of it, the [shunting yard algorithm](https://en.wikipedia.org/wiki/Shunting-yard_algorithm) will be useful.
Holy fucking shit, I love this. You should feel proud for writing what might amount to the best code I have seen in my entire fucking life. God damn. It doesn't make my eyes bleed, it makes my heart warm.
Yes, but I am curious to see what your suggestion is anyways. Might learn something new.
First of all don't use constant size arrays - you will have to modify these constants every time you will add new element to data array. Instead use container like Vec. Also don't assume there will be only Some(value) in data... unwrap might panic and you will have something that behaves like python script ;) Instead explicitly handle Some and None, unless you don't mind if code might panic there. That's how I would implement your code (though probably that loop can be even more simplified by hiding it behind some composition of map and filter): https://play.rust-lang.org/?gist=6b8b6f18accac5074020d1d969613b19&amp;version=stable&amp;mode=debug 
The driving program stores all commands input, and then you can have it unload the current dll, load up the newly compiled one, and then replay all the commands. You can also "freeze" the command history to save a spot and then when you reload it'll end up returning to that spot. Right now it's mostly complete, i just need to start making some games to go with it.
You can use [arraymap crate](https://crates.io/crates/arraymap) for that, if you want. Something like: let unwrapped = my_array.map(Option::unwrap)
Another argument against `Trait` meaning `impl Trait`: `Trait` is just a trait, while `impl Trait` and `dyn Trait` are types (the former is statically dispatched, and the latter is dynamically dispatched). Following that, if you see `Box&lt;Foo&gt;` somewhere in the code, then you can infer that `Foo` is a type and not a trait. Currently, in Rust 2015, we don't really know whether `Foo` in `Box&lt;Foo&gt;` is a type or trait. This makes reading code more difficult, IMO.
I see, I got the impression you were a beginner. Perhaps the `unsafe` usage should've made me notice you weren't. In that case, I wouldn't know of a better way to go about it.
Unfortunately, GitHub is sufficiently ... feature-limited ... that doing so means you get pinged for every single comment on every single issue. There is no way to tell GitHub, "notify me when an issue gets opened or closed; when a PR gets opened, closed or merged; or when a certain user (say fcpbot) comments on PRs; but not for any other activity."
Since you want to stick to an array instead of Vec, you can simplify the loop and avoid enumerate() by [zipping the result and data iterators](https://play.rust-lang.org/?gist=4ace11eb2efb2f1ac6598719293312b8&amp;version=stable&amp;mode=debug).
Your T's don't need to implement `Default`, `Option&lt;T&gt;` is already implemented to always be `None`.
No, for the "simple version" it stays about the same, and for the complicated version, it starts rising again at 5 dimensions dimension | timing :--|:-- 2 | 59129456 3 | 33503907 4 | 29705167 5 | 39164566 6 | 44701392 7 | 51789200 8 | 63811768 9 | 78269461 10 | 81365308
Thanks for the formal answer
Thanks for the feedback and the suggestions. I was excited at the prospect of slice matching to deconstruct a Vec, but it sounds like I'm barking up the wrong tree on that one. I have seen iterators make it less likely I need to call foldl \(e.g., getting a a max value\). And re: boxing values, helper functions were the best I could come up with, so it's nice to get some validation that that's a decent approach. I was looking to use the the Menhir port to Rust, since that's what I was using with OCaml. But maybe I need to open up the option of using a different library and just rewriting my grammar. The Tiger grammar for this book isn't gigantic.
Changing the cmp function to just comparing the two vectors straight away did result in the same timing regardless of dimension. So now the question is, why is my cmp function faster for 3 dimensions than for 2?
I don't think a regular macro can help since it always generates a whole expression and not a token stream. You could do it with C-style macros as #define TWO(X) arr[X].unwrap(), arr[X+1].unwrap(), #define EIGHT(X) TWO(X) TWO(X+2) TWO(X+4) TWO(X+6) [EIGHT(0) EIGHT(8) EIGHT(16) EIGHT(24)]
Yes, I do use turbofish for argument position, for debugging. No I shouldn't need to use it in correctly compiling code, but I spend most of my effort trying to figure out why a thing doesn't compile rather than admiring things that do. I'd love for more of Rust to be optimized for "code that does not yet compile" rather than "correctly written and compiling code".
I only recently finished the book, so now I'm making myself go through my standard katas. RPN, shunting\-yard, maze generation, and BrainFuck interpreter. I really need to get some katas that would let me exercise networking or multithreaded programming.
You are one of the owners of the project. You have power to change it!
&gt; But, at the end of the day, we have to be able to make — and stick with — decisions, striking a balance between long-running feedback and shipping. Sure but this does not mean that we can't reflect on why this is all happening. This is happening because a controversial feature was shipped. Why was a controversial feature shipped? Well, that's just how the rust process works: everybody can express their opinion, but at the end of the day the decision is made by a couple of individuals who are part of an invite-only committee. In this particular case, these individuals, which are highly respected, but were not elected by anybody, weighed the controversy and decided to ship the feature. Just because the process has steps were anybody can voice their opinion does not mean that Rust is designed by the community. 
The simplest solution is fn main() { let data: [Option&lt;u64&gt;; 4] = [Some(1), Some(2), Some(3), Some(4)]; let result: Vec&lt;u64&gt; = data.iter().cloned().collect::&lt;Option&lt;_&gt;&gt;().unwrap(); println!("{:#?}", result); } which should honestly be even simpler as follows, but the types don't work out even though I think they should fn main() { let data: [Option&lt;u64&gt;; 4] = [Some(1), Some(2), Some(3), Some(4)]; let result: Vec&lt;u64&gt; = data.into_iter().collect::&lt;Option&lt;_&gt;&gt;().unwrap(); println!("{:#?}", result); } If someone could point out why the `impl IntoIterator&lt;Item=T&gt; for Vec&lt;T&gt;` impl creates an iterator over `Item=&amp;T` thus preventing the second example, that'd be great.
You seem to be confusing dynamic dispatch with static dispatch. The Java snippet you provided doesn’t map to `impl Trait`, but `dyn Trait`. Inclusion polymorphism (i.e. inheritance) is a concept that is completely different than parametered polymorphism (i.e. type variables / generics). I really wonder how the book is going to teach people about the difference between `impl Trait` and `dyn Trait` based on the assumption that `impl Trait` is explained as the Rust equivalent of `void f(Serializable s)`. Again, the Rust equivalent would be `dyn Serializable`, not `impl Serializable`. I truly think you should read about [generics in Java](https://en.wikipedia.org/wiki/Generics_in_Java). Or maybe you’re okay with adding such a big confusion in Rust?
**Generics in Java** Generics are a facility of generic programming that were added to the Java programming language in 2004 within version J2SE 5.0. They were designed to extend Java's type system to allow “a type or method to operate on objects of various types while providing compile-time type safety”. The aspect compile-time type safety was not fully achieved, since it was shown in 2016 that it is not guaranteed in all cases. The Java collections framework supports generics to specify the type of objects stored in a collection instance. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
I use NixOS too but I actually prefer using the nightly compiler for development. Using Rust from [Mozilla's Nixpkgs Overlay](https://github.com/mozilla/nixpkgs-mozilla) is quite painless. I highly recommend it even if you decide to stick to stable Rust.
&gt; Rust process gives the feel of "design by committee/community" it has steps were anybody can voice their opinion, but at the end of the day the process follows "design by dictator". This is not a bad thing, and even with "design by committee", these controversies still do happen when mistakes are made. I really don't know how you come to that conclusion. The RFC process has an _ample_ number of RFCs that were proposed by core team members and shot down by the community and a _significant_ number of RFCs that were written, sponsored and implemented by people outside of the teams. No process is without holes and problems and there's the classic issues of any participation process (mostly: time is finite resource), but framing this as "design by dictator" is a _very_ hostile framing, however nicely it may be presented. As Aaron writes, the decision is _unique_, but it's also a unique situation.
&gt; really don't know how you come to that conclusion. There is a closed group of people who have the final word on whether an RFC is accepted or merged independently of who proposed it. These people are not elected by the community.
&gt; There is a closed group of people who have the final word on whether an RFC is accepted or merged independently of who proposed it. These people are not elected by the community. The Rust process is participatory, but _not_ democratic. This doesn't make it have notions of a dictatorship, though. &gt; That's the technical term. No, it isn't. It's a term in colloquial use. &gt; In "design by committee" this decision wouldn't be possible, because anybody can be a member of the committe, and a single "strongly against" vote blocks progress. Rust also isn't "design by committee".
&gt; Changing the cmp function to just comparing the two vectors straight away did result in the same timing regardless of dimension. So now the question is, why is my cmp function faster for 3 dimensions than for 2? The question is not why is your cmp function faster for 3 dimensions than for 2, which don't believe it is. The question is why does BTreeMap need to do more comparison with your cmp, than with the simple vec comparison. &gt; And why was my cmp function so slow, is there a better way of doing the compare with the most significant element starting at the second element? Your cmp is not slow, it just called more times. See total_cmp in this playground for what I means (especially, see the difference when you change the comparison function): https://play.rust-lang.org/?gist=ca17c6032db5490e4e231c51683b37ba&amp;version=stable&amp;mode=debug So, for me, the question is really why does BTreeMap need to do so much comparison with your comparison function. The more likely cause is that the combination of your comparison function and the insertion order reach a corner case / bug in BTreeMap. Another possibility is that you comparison function is wrong (it doesn't hold the Ord invariants described in the documentation), but it seems correct to me.
OP is trying to remove the `Option&lt;T&gt;` wrapper and go to an array of `[T]`. That `Option&lt;T&gt;: Default` is not relevant because OP must initialize the *result*.
Do you think this is worth filing an issue over somewhere? I would really like to investigate this further, not quite because the performance is so important to me, but because it's very interesting.
You never use the `IntoIterator&lt;Item=T&gt;` implementation for `Vec&lt;T&gt;`, so that might explain why that doesn't work. Do you understand the difference between an array and `Vec&lt;T&gt;`?
I mean, yes, it's different, in Java pretty much everything is implicitly boxed (other than primitive types which pretty much ignore the entire generic type system), and generics don't really exist at runtime due to type erasure (it's possible to check generic method types with method reflection, but that's pretty much the extent of what Java allows). However, it isn't really a difference in terms of how generics behave, any non-primitive type in Java gets boxed. Consider the following example in Rust: fn to_string(x: impl ToString) -&gt; String { x.to_string() } fn main() { println!("{}", to_string(Box::new(42))) } I pass in a boxed argument to `impl ToString`, and this works, just like it would work in Java. Of course, in Rust, boxing here is pointless, but the language doesn't prevent you from doing so.
What is the definition of OOL being used? Does that definition rule\-out Rust for sure? Just curious.
I guess I disagree with pretty much everything you said, but that's ok.
I just assumed you looked at my username, not like I could've made it harder to trip up. Thanks for helping anyway, man.
I don't think that opening an RFC without any proposed changes is the correct way to reflect on these issues.
Whenever type ascription gets stabilized, wouldn't that overall make the turbofish syntax redundant? Would be more clear too, than using a turbofish and not knowing which parameters it applies to.
Oh that's my confusion that `data` was not a `Vec&lt;T&gt;`. Yes I understand the difference.
In the words of the drive\-through window in "Dude, Where's my car?", "Andth Thden????"
Who knows? Here, we can make it even sillier: fn play_also_works() -&gt; Result&lt;u32, ()&gt; { let game = Arc::new(Mutex::new(Game(0))); return Ok(game.lock().unwrap().score()); } I've been assured this is [not](https://github.com/rust-lang/rust/issues/43837) a bug.
(You don't need to pass `input` to the macro, it will capture it.) I had this if the array is taken by value fn unwrap_all&lt;T: Clone&gt;(input: &amp;[Option&lt;T&gt;; 32]) -&gt; [T; 32] { macro_rules! result { ($($xs:expr),*,) =&gt; { [$(input[$xs].as_ref().unwrap().clone()),*] }; } result!( 00, 01, 02, 03, 04, 05, 06, 07, 08, 09, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, ) }
https://i.imgur.com/YwjgsmC.png
Well, what you're trying to do is something like: let x = match something { Some(f) =&gt; "Hello", None =&gt; 5, }; ...now, the compiler cannot possibly figure out whether `x` is an integer or a `&amp;str`, so instead compilation fails. The same applies even in just a match statement without `let x` in front of it. &gt; Can I return any type of error somehow? If your function returns `Result&lt;`sometype`, Box&lt;std::error::Error&gt;&gt;` then most errors can be easily converted into that. There are also crates like `error-chain` and `failure` that can help with creating different types of errors. 
How is this meaningfully distinct from an RFC?
Mystery intensifies: fn play_also_also_works() -&gt; Result&lt;u32, ()&gt; { let game = Arc::new(Mutex::new(Game(0))); if true { Ok(game.lock().unwrap().score()) } else { unreachable!() } }
A few ways. 1) RFCs change over time, and it's hard to know when to follow - you get notifications for *everything* when you follow a thread (I follow a few, it's 90% noise) 2) RFCs can be in many different states, but you may only care about RFCs in specific states 3) Most people are concerned primarily with features that are probably going to land - but by the time an RFC reaches this point there can be a *lot* of noise to sift through Posting the release notes out of band, inlined into a TWIR post, or something similar, without the noise of months of discussion, seems like a great way to show people a feature in full - that's how release notes work, after all.
&gt; Few decisions are clear cut, which is why our process includes a number of points where feedback can be given, including a number of final-comment-period advertisements and the like. As a community, we have made a deliberate choice to slow down development to ensure thorough vetting and input into the process. Is there any obligation to actually conform to that feedback? If not, is providing feedback worth the effort? I'm asking, because of the times I've commented on an RFC, I have never felt that my opinions/arguments/comments actually *mattered* to the people at the top. (Which is also why I have stopped involving myself with the Rust RFCs.) 
&gt; I'm pro universal traits can be used anywhere a type is expected. I don't get all the hate. It doesn't actually let you do this, since in contexts other than argument or return types you care whether it's existential or universal
How many people actually use this syntax, though? If it's never used, kind of hard to see it shake out. I never knew it existed until it got merged into stable.
I'm not sure what you mean by "conform", but all significant points of disagreement/drawbacks raised are expected to be *addressed* on thread/the RFC and taken into account with any final decision.
Bad bot.
Thank you, ehsanul, for voting on im-dad-bot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
There's a difference between writing nothing and the compiler figuring out the details and a new sugar.
But if you're just showing people the feature in full rather than opening up for feedback during the feature's design, how is that different from just posting release notes when the feature is released? It seems like there's sort of a tension in what you want. You seem to want something clean and final and not in flux so that people can comment without following a whole big process, but also something open to feedback that can be changed or scrapped altogether. That seems like a pretty tall order. I do think that's a very good point about how difficult it can be to follow discussions. It should be possible to follow where an RFC is going without getting inundated with every +1 and "What if we called it 'Bob' instead?".
On it!
"definitely going away", as in implying that any code using trait objects in any capacity would break when upgrading? I highly doubt that. It would be, by far, the biggest breaking change in the edition. Part of the goal with the 2018 edition is to give confidence in the edition process, which means breaking very little and getting everyone on board and eager to upgrade, with a minimum of fuss. I can see the new edition introducing a lint against bare Trait, possibly, but certainly not turning it to deny for 2018. What source are you citing?
The RFC process involves people already participating or reading carefully everything on IRC / emails. So I guess a lot of us just didn’t see the memo, plus, it was a bit eclipsed by the `conservative-impl-trait` RFC – which I read at that time. And yeah, the RFC was closed so no need to discuss about whether or not the feature should be removed: aturon explained it won’t. Yet, I think it’s still a good idea to gather our thoughts about it, because it’s clearly very opinionated. A lot of people are against it and a lot of people are for it. Also, I won’t use it either but eventually, you and I will have to read code using it – because you never work on your codebase only, you are to read and maintain others’ as well.
[removed]
&gt; I think it’s still a good idea to gather our thoughts about it, because it’s clearly very opinionated. A lot of people are against it and a lot of people are for it. 100% agreed. This is something we definitely should be discussing. &gt; Also, I won’t use it either but eventually, you and I will have to read code using it – because you never work on your codebase only, you are to read and maintain others’ as well. Yeah, I realized this as well. I hope that, if universal `impl Trait` is going to stay for a while, there won't be too much of a split between people who do and people who don't use it. It would be fairly easy to write a lint that advises for or against it...
The `dyn Trait` RFC suggests that there will be a lint in the current edition against bare `Trait`, but it will be set to deny in the next edition. 
Yeah, I'd considered those. The problem at that point becomes going through an RFC and figuring out what the actual state of it is.
The only condition for joining is paying a yearly fee; anyone that does that is accepted.
You probably mean /r/playrust
A new impl period starting on June 1 should be considered. Otherwise, edition 2018 is not going to happen in 2018, unless you start removing items from the checklist willy-nilly.
Wdym
Thanks for the feedback. The micro-benchmarks must produce a very specific output, so I'm not sure how that would work with Criterion. Still, that is a reasonable concern, since each technology will often have its own way of benchmarking, and sometimes they aren't even measuring the same tasks across languages. The independent repository for this benchmark can eventually hold another harness too. A nightly toolchain is needed for `black_box`, although this can be avoided with volatile reads and writes. This particular matter can (and perhaps should) be discussed and updated accordingly in the future. Admittedly, more optimizations are possible, even without the native target flag. These micro-benchmarks were mostly devised as a means of understanding the performance/usability trade-off of each language. And Rust, even without a strong focus on scientific computing, appears to do pretty well in this perspective.
&gt; GeForce disabled rust due to I believe some anti-cheat problem, can a dev or someone please give an update on when we will be seeing rust available again to play on GeForce Now? oh thank you lmao 
&gt; Perhaps we should have a bot that posts here every time an RFC is actually merged. I'd wager there are 2 steps that are important: new PR and FCP. In order to avoid flooding the PR with comments, I'd link to the rendered RFC instead of the PR thread.
A game session is handed a u64 during session initialization and told to use it as the base seed for any sort of randomness. When the game is reloaded the same seed is passed. If that actually gives a deterministic result is up to the DLL following the protocol.
That exactly the point : there are two different keywords for two different usage of the trait : `impl` for static dispatch, `dyn` for dynamic dispatch
This looks like a nice solution! The only caveat I think needs to be written here is that if the input and output aren't the same size, then the zip may be shorter than expected. I can't think of a situation where that fallback *wouldn't* be appropriate if you need the arrays to be a different size, but I just wanted to throw out that the compiler won't actually check the sizes for you with `Iterator` code being involved.
On the other hand, if it does get backed out ever the best time to do it is ASAP before several people start integrating it into their codebases. But I understand it won't happen. I don't even have a horse in this race.
Decisions which, in retrospect, were poor happen regardless of governance model. The only way to have a perfect decision making track record is to use a time machine.
While I don't agree per se, I do think this points to how precarious it is to consider any feature stabilized the very moment it's part of a minor release of rust, which is the point at which the wider public only begins to use a new feature. A hypothetical alternative could be to have minor releases be released as a more beta quality product, but still widely usable, and only have features be considered truly stabilized at the time of the next minor release.
While not all that useful, I'm trying to make a maze solver that can solve any square or rectangular \(regular\) maze.
The longer the submitter would've waited, the more the wait would've been used against them.
In light of this now being part of stable Rust, I think Patrick is suggesting we should see how the docs, ecosystem, and usage of the feature work out. &gt; How many people actually use this syntax, though? If it's never used, kind of hard to see it shake out. You're describing the problem of not knowing the feature existed (which is a fair point) but the feature was just introduced, so of course there aren't many people using it. Honestly, I think we have to be realistic that this situation uniquely stems from this feature having syntactic overlap with `impl Trait` in return position, making it harder to stand out, that nobody was excited enough to blog about its impending release, and that the beta channel as a line of defense against changes making it into stable Rust isn't pulling its weight. Beyond that, I think there are diminishing returns as to what we can expect from a process that invites open feedback but somehow didn't receive it. It's not just the responsibility of the maintainers, it's also the community's responsibility What can we do or change so this situation doesn't happen in the future? (On that note: I wonder if we can publish a "beta release notes" in TWIR alongside new version release notes to tee up expectations of what's in stable releases. But having more developers using beta voluntarily would definitely help.)
u/thelearnerofcode could even use [wrapper struct with implemented `FromIterator`](https://play.rust-lang.org/?gist=f71afeb085f77f3aee36fc3fc9cd2200&amp;version=stable&amp;mode=debug) to simplify whole process of data gathering.
... well, I guess I can't argue there.
We can reflect, but I'd like to point out /u/phaazon_ didn't really present this in a "reflective" way. Proposing to undo an accepted RFC just because he didn't get the memo (I don't think he raised anything really new?) is kind of disrespectful of the work the Rust team put in herding cats and reviewing (and responding to reviewers). It would be much better if he could reflect on why he didn't know this was coming (even though Rust publishes the various RFC stages in TWIR and elsewhere, and this had a separate RFC) and what the Rust team could have done differently in that regard. This is all in my humble opinion, and I'm perhaps being a bit more blunt here than I would usually be because I think Rust deserves better. But I think in this case it's warranted, and luckily the Rust team is much more professional than me :) 
I investigated a little bit more. I happens that with the simple vec comparison, the `rangepoints` are always smaller that the first point in the set (I didn't investigate why this the case, but I guess this just how your algorithm works). And this the situation where you need the smallest number of comparison. It's not the situation with you custom comparison that is special and do a lot of comparison, it's the one with the vec comparison that can do very little This explain the difference between the two comparison function. Now, concerning the difference between the number of dimension, I thinks this just a question of how the rangepoints is distributed versus the values in the set. For example, it seems that when the dimension increase the number of time rangepoints.0 is smaller than the first element of the set increase up to some values. 
Hey I made one of [these](https://github.com/IntrepidPig/mexprp)! It's probably the most fun programming project I've done to date. I used the shunting\-yard algorithm like that other guy recommended, but I parse out parentheses first into a tree structure which makes the implementation a lot simpler in my opinion. It's implemented [here](https://github.com/IntrepidPig/mexprp/blob/master/src/term.rs#L276) if you want to check it out.
Then how do you do universal types?
The grumpiness is when I looked over the discussion, there was OVERWHELMING support for conservative impl Trait. The argument position impl Trait was controversial, so I didn't even comment on it because I assumed that they'd rush out the conservative implementation out first and discuss the argument position further. But somehow they both made it in at the same time, which was surprising because usually Rust had a more conservative model of including things.
Well, it depends on your goals. As a "damn we messed up" retrospective, sure, it should have been posted in 6 months, sure. I'd appreciate more retrospectives like this, provided they're respectful of the people behind the original decision-making process (it's easy for this to devolve into I TOLDYA SO arguments). But a retrospective is not the goal of this RFC. Retrospectives usually don't make sense as RFCs (maybe they should?) As a "we shouldn't be doing this, let's switch back", it shouldn't have been posted at all. There was a nightly period when it could have been posted as an rfc, among other ways of rekindling active discussion. And yes, the nightly model isn't perfect, but it's what we've got right now (and can be changed if you have better ideas). If someone feels the nightly period was too short, that's another thing that could have been brought up on the tracking issue as folks were discussing stability. There was an argument to be made for "stabilize existential only for now" (which _was_ made IIRC but I wasn't actively observing so I'm not quite sure on this) which could have been used to delay this and give it time to bake more. The system is designed to give everyone a say. The system is not designed to give everyone a say at _every point of time_. There's a time and place for bringing up dissent, and if you miss that window, I'm sorry, there's nothing that can be done. There's a misconception that in an open communities there is _always_ a way to change _anything_. Which makes the "obvious logical" answer to "when should this be posted" become "six months later" (instead of "not at all, not anymore, sorry, you missed your chance"). But it's not. Open communities have limits too. Openness does not mean [you _must_ be consulted](http://www.ftrain.com/wwic.html). Openness does not mean that folks must listen to you rehash arguments that have been brought up and discussed before. Openness does not mean that things are decision-by-popularity-contest. Nor does it mean that things are decision-by-loudest-person-in-room. And, as is the misconception here, openness does not mean openness across the space-time continuum. So yeah. It wasn't an appropriate time to do it. The appropriate time was a couple months _ago_, not a couple months _after it stabilized_.
When i make every coordinate in the rangepoints the same, the weird counts show up in the vec cmp as well, although to a lesser degree. And rangepoints should have no strange distribution towards the set, my algorithm makes the rangepoints centered very closely (based only on the second coordinate) around a point that is basically iterating trough all my points.
I don't think Rust 2018 having RLS 1.0 support means fully functioning code completion or syntax checking immediately. That's very unrealistic or naive.
The same way you'd ever do them. Put the expression underneath a polymorphic lambda. I'm not sure if Rust can actually do that or not, but there's no reason it couldn't be done.
No that should not matter. Afaik insertion order has no effect on the final btree produced. And when both the btree and range are made based upon the same cmp function, you should always be iterating over points in a sorted order (according to that cmp function). Read my above comment on why the vec cmp is a lot faster (which has been fixed now).
&gt; I think that people are overlooking or discounting the fact that bounds in where clauses are strictly more powerful than those before the argument list. This is in the RFC list as well. :)
I don't know what that means or how it would look like.
...as makes perfect sense, given the stability policy. Now if they had waited 6 months and written a retrospective on why they thought (with experience in the ecosystem) universal `impl Trait` was bad, nobody would have complained.
You can absolutely specify a non elided lifetime
Do you find yourself frequently providing type explicit types for parameters in argument position? I'd love to hear the use case
That wouldn't explain why it's slow for 2 dimensions, gets faster for 3-4 dimensions, and much slower after that again.
SGX is an encrypted memory feature which provides userspace "enclaves" which act as a sort of "reverse sandbox": devices with access to memory, be they kernel extensions on the same CPU or hardware devices with DMA, only see encrypted memory when they attempt to access pages controlled by the enclave. The Rust SGX SDK is a wrapper for the Intel SDK developed by Baidu as part of their "Autopilot" self-driving car program. Yes, SGX has a lot of issues, in fact ["Spectre v4"](https://blogs.technet.microsoft.com/srd/2018/05/21/analysis-and-mitigation-of-speculative-store-bypass-cve-2018-3639/) was just announced today. Regarding these vulnerabilities and SGX, Intel notes: &gt; Recently, Intel published Security Advisories INTEL-SA-00106, INTEL-SA-00135 and INTEL-SA-00115. Applications that use Intel® Software Guard Extensions (Intel® SGX) may be vulnerable to certain methods documented in these Security Advisories until mitigations described in the attached document have been implemented. One thing worth noting is this release wraps the new memory fence instructions which disable speculative execution inside SGX. Enclave developers should be using those to prevent Spectre-style attacks. Another thing worth noting is: even if these attacks can't be mitigated, SGX can still be useful as a purely defense-in-depth measure. I am working on using HSMs for long-term key storage and keeping the short-term PSK used to authenticate to the HSM in SGX. The alternative is a "signing oracle" where compromise of that PSK effectively gives you "sudo" access to a key, short of being able to exfiltrate it. While it's great once the PSK is revoked and rotated it is still secure, this signing oracle hole exists for almost all HSM applications, barring some sort of interactive process which most services using HSMs tend to avoid. SGX provides yet another thing an attacker must do to compromise the short-term PSK. It may not be a bulletproof solution, but I personally think it's better than some key sitting around in main memory. I think there's plenty of other applications where secrets are being handled where this sort of defense-in-depth would be a welcome improvement versus just splattering plaintext around everywhere in unencrypted memory.
That's how I interpeted the phrase too.
No, if you make it continue to 10 dimensions, the total cmp count goes up again over what it was with 2 dimensions (keep in mind that the first measurement seen is 2 dimensions, i'm never comparing points with just 1 coordinate).
What is the type of the closure `|x| { x }`? It could be `Fn(a) -&gt; a` for any value of `a`. Or you might even do a kind of coercion to treat it as the polymorphic identity function, having time `Fn&lt;a&gt;(a) -&gt; a` (if you can make some sense of that syntax I just made up).
Is it still existential in this example? fn func(arg: impl From&lt;impl ToString&gt;)
Usually when there's an odd performance characteristic like this it's one of 3 things: - compiler isn't do it's job as well as it should - the memory is being loaded more efficiently due to cache lines and/or packing - the operations are being vectorized better or are more amenable to compiler optimizations like unrolling or a combination thereof. From my time in university working on optimizing matrix multiplication, at least, that's what I learned. This is an interesting problem so I'd like to look at the assembly but I don't have the time right now, so maybe later.
&gt; Currently, in Rust 2015, we don't really know whether Foo in Box&lt;Foo&gt; is a type or trait. This makes reading code more difficult, IMO. Why do you find it important to know this? As a general point, I personally often find isolated arguments about "code is (locally) harder to read" unconvincing, because I don't see why a programmer needs to be able to deduce the distinction from the code: the times when the distinction matters end up being flagged by the compiler if things are being used incorrectly. For this specific case, I can imagine some things that you might think to be important, but it'd be great to hear what you think.
How Exciting! More breakthroughs! Daily breakthroughs! Finally a programming language for the common man.
The problem with this kind of question (and with this kind of discussion in general) is that "Is this existential? Or is it universal?" is too coarse-grained of a question. It's like asking if "2x3 + 4" is an example of multiplication. There's multiplication in the example... but I would be hard-pressed to call it an example "of" multiplication, because it's not really the dominant feature of it. 
What platform are you on? A good step would be to run under a profiler such as Instruments.app (on macOS) or perf (on Linux), which should help in doing a comparison of where time is being spent. Adding `#[inline(never)]` to `cmp` (and other functions) might help highlight how much time is being spent in each function (but make sure to check that adding that doesn't change the performance characteristics). Additionally, do the trees have the same number of elements? Maybe the removal code is removing more for the higher dimensions.
I'm just confused when it's nested like this, because I need to think "what does it REALLY mean?" which probably means I don't understand what `impl Trait` really means in every position. What does it really mean in the first `impl Fn` and the second `impl Debug`?
Rust has a beta release. Nobody targets it, other than in CI builds in my experience. The tricky part isn't the presence of the beta as you seem to imply. The tricky part is getting people to use it and target it in anger. I certainly don't know how to do it. Another feature was recently stabilized that I wish I had paid more careful attention to. But the problem is basically impossible to solve on both ends.
It helps to understand what an "existential" actually is, in that case. The word comes from logic, and they are the analog of the (constructive) proof of existence. "There exists a foo such that bar". In this case (and very typically when people talk about these things), the "foo" is actually a type: "There exists a type T such that bar". And in this particular case, "bar" is a trait (and in Haskell, it would be a typeclass). So "There exists a type T such that T : Display" or whatnot. That is the *type*. But how do you construct something of an existential type? You create a pair. The first element of the pair is the type T itself. The second part is sometimes called the "witness" because it witnesses the fact that T abides by the "such that" clause. It will typically be the anonymous structure containing all of the callbacks the trait requires. So `impl Debug` just means we have collected together a pair `(T, fmt_methods_struct)` where `T: Debug` and `fmt_methods_struct` is an anonymous struct that contains the `fmt` method. The `impl Fn` part is a bit weird in Rust, I think. I don't really know the semantics, since you don't ever get direct access to the types of lambdas.
Yes, they should. I think this is a non-lexical-lifetimes type of thing that will vanish when that gets hashed out.
Writing blog post about my experience with wasm. Mostly done, need few fixes. Preview here: https://jaroslaw-weber.github.io/blog/rewriting-javascript-into-rust-with-wasm.html
I'm on linux, fedora. You are very much onto something with the average btree size, this code calculates avg btree size and average amount of removed points: https://play.rust-lang.org/?gist=e9be6af862cb7f2d6df612f226835a87&amp;version=undefined&amp;mode=undefined As you can see, btree size is much larger for 2 dimensions, and significantly less points are getting removed. I have no idea why this is.
I don't think looking at the assembly will be necessary, since somehow the btree is much bigger for 2 dimensions than higher dimensions. https://play.rust-lang.org/?gist=e9be6af862cb7f2d6df612f226835a87&amp;version=undefined&amp;mode=undefined
I'd prefer to use a crate like slab (https://docs.rs/slab). Which turns everything into indexes and is quite easy to use. 
I don't think looking at generated assembly will be necessary since we discovered that btree size is much larger for 2 dimensions than 3 and 4. I don't really know why that is or why it rises again after 5 dimensions. https://play.rust-lang.org/?gist=01456c204b0add0703df8f265ffcb669&amp;version=stable&amp;mode=release
Some traits have no functions (marker traits). Don't know if clippy could differentiate them and figure out if a trait is unused or just has no use.
The problem seems to be related to the actual size of the btrees being bigger in 2 dimensions, no idea why. https://play.rust-lang.org/?gist=01456c204b0add0703df8f265ffcb669&amp;version=stable&amp;mode=release
Continuing to read the rust book while streaming. I finished chapter 8 last week and now I’m onto the end of chapter challenges. 
How is this even something people are 'fired up' about? It's a great addition to the language and IMPROVES Rusts learnability, not hampering it... It's much easier to explain to a new user of Rust coming from say Java, that impl Trait in a function signature is like using an interface type, than explaining it through the old syntax with the mental calculus of parsing "fn func&lt;T: Trait&gt;(arg:T){}"... that syntax is daunting for new users imo. (I'm commenting in part to offset the vocal minority that are giving the impression that this is a 'hot issue'. This change is a welcome change that improves language ergonomics, that the majority of Rustaceans are surely pleased with. Aaron was right to shut this 'RFC' down immediately. The Rust process and the core-team are doing a phenomenal job with designing a truly brilliant language. Full respect + ❤️ for impl Trait, and the Rust Team)
Do a newtype wrapper with TankID. I'd then have a vec-like structure that can only be indexed with that ID. That way when you have a bunch of functions working with missile IDs, doodad IDs, etc. you can't mix them up. IDVec&lt;TankID, Tank&gt; You'll probably have to manually clean up a unit upon its destruction, but it's not that inconvenient. You can have it where the function that alters a units HP automatically cleans it up if the HP is &lt;= 0.
I think the recommended connection pool library is [https://github.com/sfackler/r2d2](https://github.com/sfackler/r2d2) which is very easy to use with Rocket and Diesel.
If the binary is part of the same package as your integration tests, it should be automatically built when you run `cargo test`. The only case where it isn't built is if it has required features that are disabled.
Thanks! That makes the ID approach a little better. With that, do you think the ID approach is better than the Rc Refcell one?
It looks like a lot of classes have popped up around making it easier to use the ID approach. If one believes the ID approach is dangerous and should be discouraged, then the proliferation of these classes would be worrisome. However, if one likes the ID approach, then these classes are awesome! I take it you prefer the ID approach?
I didn't realise it was that easy! I migrated over lunch an internal project: that was really quick and painless! Not having to pin things down all over the place will really help tons.
What if you bring together two identical objects of different temperatures? \(T1 \+ T2\) / 2
Working on a custom String implementation with in meme to compression for my sta library
&gt; What is this expressing? You have a second generic type parameter called `PartialOrd`. You don't use it anywhere in the interface to the function, but that's not so unusual. Rust presumably doesn't check the *body* of the function; it probably could and issue a warning if a type parameter is never used. Also, this isn't such a big issue. If you actually try to *use* that function, the compiler will refuse because it won't be able to infer the type of `PartialOrd`, and demand that you specify it... at which point, you'll probably notice you have one parameter too many.
&gt; As you can see, btree size is much larger for 2 dimensions, and significantly less points are getting removed. I have no idea why this is. I think this is likely to be the cause, not some compiler/`BTreeMap` weirdness, so unfortunately you may have to sit down and work through the code. From browsing the code, it may be because the extra dimensions makes the points further apart on average (each additional dimension increases the mean value of the distance between points with each coordinate randomly generated between 0-10 by 50/3), but there's lots of code that is just looking at the first two coordinates. This means when comparing these against larger distances (e.g. in the `range` query) more points are included, if I add a counter to the first line of the `secondpoint` loop (i.e. counting the number of `secondpoint` values seen), the average number per `btree_count_amount` is: - 2: 1.42 - 3: 4.95 - 4: 12.02 - 5: 45.92 - 6: 129.23 - 7: 275.45 - 8: 321.06 You'll have to work out the appropriate adjustments (probably not just looking at the first two coordinates with `firstco` and `secondco`) for the algorithm.
+ 1 for actix-web I migrated a project from Rocket to actix-web about two months ago. Author of the project is super friendly and talented. Felt right at home from day one. I even suggested some additions to the examples. He invited me to create a PR which eventually got merged. Haven't looked back. The framework is super solid, stable, fast and with a nice and experienced little community around it. Check it out on Gitter.
In my opinion, the ID approach is very nice where it works. It does make some things more difficult like deleting/removing things out of order, but overall it's a good tradeoff. Runtime bounds checking ensures it's still completely memory safe too, even though there is the possibility of getting the wrong object if the ids are messed up. It's definitely a tradeoff, but I think it's a pretty good tradeoff.
I see you've black-boxed the fib benchmark. I don't think that's required; the C benchmark employs no such mechanism and you shouldn't need to in any case since you're using the return value.
Nice! :)
Undoing universal impl Trait would be a loss for embedded rust. It works great when heap is not available.
Was it rushed, though? I was keenly waiting for impl Trait in return position and was told that the holdup was stabilizing it in argument position. Personally I don't see the fuss - a programming language cannot be a perfect democracy and besides, I think the feature is cute 
Please, don't. The C++ model can be nice for *big players* (Google, IBM, ...) because it is easier for them to control the language. But in my opinion is very unfair for an open community like Rust.
&gt;It's much easier to explain to a new user of Rust coming from say Java, that impl Trait in a function signature is like using an interface type Well it really isn't. Passing an interface type is akin to passing a trait object in Rust, meaning dynamic dispatch, which is very different from the generic stuff we're talking about when using `impl Trait`. Sure, there's lots of cases where you could use both and choose according to the performance needed etc, but still. Definitely not the same thing. Now I don't have a horse in this race, neither am I really qualified to, so I don't really care about the issue at hand. But you see, I don't think the argument about learnability with this specific example applies, as it just shows a common misconception with using `impl Trait` in relation to interface types in Java/C# etc.
I don’t think so, but that’s the model C++ uses. The amount of passive-aggressiveness in this thread is spectacular.
I guess I’ll put myself into the dissenting camp who feels these ‘id’ helpers / crates are code smell and the proliferation of them is perhaps a sign that something isn’t quite right. 
Defined as C, C++ or Java.
Do I now have to weight in that I am OK with universal impl Trait, to help negate the phrase "everyone hates impl Trait"? This is ridiculous. Let's follow the process which was set up with the best intentions in mind, let's trust the people who have given this years of thought as opposed to one day you thought about that.
welp, guess it's time to pack it up and move to LuaJIT
Because "several months down the line" people will say "should've spoken up before a bunch of code depended on this"
[removed]
So You ported raft to rust. What were the pros and cons compared to Go?
What do you mean?
Oh that look cool! I'll have a look at that after I don't know how to progress with my current approach, and probably end up rewriting everything. Thanks!
I like to just mess around and try my own approach before looking up proper solutions. I find that I end up with more knowledge. But yeah I'll definitely look into existing algorithms afterwards!
Yeah, that was confusing. It seems to be all due to the `print_file` benchmark. I looked at the code a little bit, but I didn't see anything obviously crazy in the Rust code.
Yeah projects like these are really fun. You get to have an output from very early on but you can make it as complex and extensive as you can.
Fuck sorry bois I was hella cooked ahahha
&gt; Another feature was recently stabilized that I wish I had paid more careful attention to. Just out of curiosity, if you don't mind, which one are you concerned about?
Adding `#![feature(nll)]` on nightly does not (at this time) fix this problem.
I think it's quite disappointing that, in a lot of this thread people have said they didn't know this feature was coming/hadn't noticed the rfc/didn't this the feature was going to stabilise and the response is "but there was an rfc for this length of time". It would be far better if people considered that there might be a problem with visibility and clarity in the rfc process rather than dismissing it out of hand. 
&gt; Regardless, I'm not sure that bare Trait is something people are still interested in. Bare trait is a _huge_ source of pitfalls and problems, especially around FFI. a) Any generator handling trait objects (which are not uncommon in FFI, by exporting an opaque type) _cannot_ figure out whether a type is a trait or not without doing typechecking (which means that most just don't handle them). b) The size of `*mut Type` is also not known without resolving `Type`. c) A lot of beginners have to deal with confusing error messages around things they didn't ask for. It may be a pervasive change, but also one that has a very straight-forward and mechanic fix. 
i assume you are putting the 32-element arrays on the heap anyway. in that case instead of saving the information if a field is used for every field of the excess array ([Option&lt;T&gt;; 32]) you could also store how many fields are used, since you will be filling it from the front to the back { data: [T; 32], size: u8 ). now, instead of coding that yourself, which would include unsafe, you could also just use a normal Vec&lt;T&gt; there. a Vec is basically just that: { size: usize, capacity: usize, elements: Box&lt;[T]&gt; }. so a vec stores a dynamic capacity which it needs to check at runtime. it probably won't make too much of a difference and would still allow you to reuse the same allocation every time. now instead of using a default Vec you can use an [arrayvec](https://docs.rs/arrayvec/0.4.7/arrayvec/). for performance considerations its probably smarter to call try_push ()there and work with the result which tells you if its full instead of doing the calculation yourself and then calling .push() which does the same calculations and panics if it fails. also for performance reasons it might be smart to not work on [T; 32] but on [T; 4096/sizeof&lt;T&gt;()] which is sadly not possible in rust.
I could be mistaken, but the quick glance I took at the Julia code made me think that the heavy lifting is done in C and/or blas. The rust code I saw didn't use blas (unless I missed it). So it seems a bit unfair?
From my point of view, Rust allows one to assume that everything goes to stack as much as possible. This even means that sometimes a clone() call can create a new binding without allocating heap. However, if a size of an object is unknown at compile-time, then there will probably be an allocation. For example, &amp;T, &amp;[T], Box, Vec and other containers would probably allocate. But even in this case, compiler can decide not to allocate, e.g. because of small size of an array or short lifetime of a cloned object. Keep in mind that bindings in Rust is not the same as variables in C/C++. This means that even if an objects moves between bindings (e.g. if you move it in and out of function) that doesn't mean the actual memory moves in the heap. Sometimes a move is cheap and don't need allocations (e.g. if this is short-sized immutable object in the stack) and sometimes it requires a memcpy() call (e.g. when you call Vec::push()). I'm not sure if this explanation is really what's going on in the compiler, so take it with a grain of salt. But the point is Rust type system is powerful enough that compiler *can* make such optimizations, compared to other languages with weaker type system such as C/C++.
Thanks! We don't want to restrict support to a single language. I think the best issue to look at is https://github.com/amethyst/amethyst/issues/725 at the moment. If you have questions feel free to visit our Discord server, there's almost always somebody around to reply :)
Ah, I hoped as much, very cool. Thanks, will definitely have a look around sometime!
I also do not understand why actix enforces that an actor must response. Actually, the response is not part of the actor model, and it makes streaming-like applications hard (or obscure) to implement. But maybe I'm missing something, so I'm also interested in the answers :)
The problem is at least in part one of documentation. If sometimes the caller can control the type, and sometimes they can't and if the type of your function doesn't specify this, that means as a user of an API I need to read the source code to know whether I can control the type (or I need to rely on the API implementer documenting which case it is). That seems like a rather large source of frustration.
functional thinking
As far as I'm concerned, unless/until someone comes up with a better alternative, the ID approach is fine.
Consider following code: trait A {} struct B; impl A for B {} struct C; impl A for C {} fn foo(left: Box&lt;A&gt;, right: Box&lt;A&gt;) {} fn bar(left: impl A, right: impl A) {} Code with function `foo` won't compile with `#![no_std]` enabled, while function `bar` will work just fine. You can mix different structs implementing certain trait and call methods from that trait, without using `Box`.
I remember reading/hearing about a similar oddity in an article/talk (can't remember which) about introducing Rust at NPM. Apparently, they found their first Rust attempt to be surprisingly slower than the old Node.js version and tracked it down to Node.js's defaults for I/O (things like buffer sizes) having been fine-tuned to a surprising degree, while Rust's defaults hadn't. They hadn't managed to match Node.js's level of micro-optimization by the end of the talk, but they still considered it a success because they saved so much on not having a ton of unexpected exceptions bogging down their fault-recovery and logging machinery. I suspect something similar is at play with LuaJIT.
I made a \(40 minute\) video that goes over the entire thing in fine detail [https://youtu.be/hfRYhJIwYfo](https://youtu.be/hfRYhJIwYfo) 
Actually, the fib benchmark would tend to over-optimize due to code invariance, to the point of taking a near-zero time to run. The C program used volatile variables to prevent this, and something similar could be done in Rust. Feel free to report potential improvements in the independent repository: https://github.com/Enet4/julia-bench-rs
Those have completely different semantics. Static dispatch ≠ dynamic dispatch.
Yes, the Rust version also uses BLAS. By default, it is indirectly used through `ndarray`, as the more idiomatic and safe approach. By enabling the `direct_blas` Cargo feature, some of the benchmarks will use the BLAS API directly, yielding a performance boost at the expense of unsafe, less elegant code.
It's a library. Porting a library from language X to language Y always has the same gain - allowing projects written in Y to use it.
&gt; EDIT: let me answer my own question, I already get it. It's defining a new generic parameter named "PartialOrd". However, PartialOrd isn't used anywhere in the method, so perhaps a warning would be really helpful here for people making my mistake. Type parameters which are not used in the body of the function can be semantically useful as constraints e.g. you're doing unsafe stuff and telling the compiler about constraints you want statically upheld despite the compiler not being able to see a use for these constraints within your body. Plus as /u/Quxxy noted you'll get an error when *using* the function as you'll be asked for a type for `PartialOrd`.
But it can serve similar purpose. Without `impl Trait` when `#![no_std]` is enabled, you won't be able to call function from trait, when passing different types implementing that trait.
&gt; Why do you find it important to know this? Because I prefer knowing whether `Box&lt;Foo&gt;` is a single implementation (type `Foo`) or an arbitrary implementation at runtime (arbitrary implementation of trait `Foo`). If I haven't seen `Foo` before in the code, or haven't fully internalized it yet, this is a helpful reminder. Is it a type? *"Okay, single implementation, got it."* Is it a trait? *"Oh, this is dynamic. I guess I'll be seeing `Foo` a lot around here. Wonder who implements it and what are the different implementations I can expect as `Box&lt;Foo&gt;`..."* Others may have a different internal monologue when reading code so it may not matter less. :) In the end, this is probably just a personal preference.
Why `std::ptr::NonNull` take a `*mut` pointer ? Why we don't have a `const` alternative ?
Related but not the same. [Amanieu](https://github.com/servo/rust-smallvec/pull/92) stared out some work to zero out the size overhead by using an union, I have a follow up [here](https://github.com/servo/rust-smallvec/pull/94).
Originally I was against that PR because I thought that `&lt;` required a `sub` whereas `==` could be done with `xor` but looking at godbolt it turns out that they all compile to `cmp` anyway. As much as I write about x86_64 I'm still not that good at it, it turns out. Good work.
If the term isn't used by language researchers, can you elaborate a little bit more, why such distinctions are senseless? Are the methodologies one applies with a compiler essentially the same, whether I produce machine code, a platform idenpendent byte code or source code of some programming language? Are there other kind of hierarchies or types of compilers that researches use?
Oh right, I thought `test` was `xor`, not `and`. Actually, I'm surprised that `==` doesn't get compiled down to `and`/`test` since you'd imagine it performs better.
`Vec` + ids is better for performance in many cases. We use `specs` to store our entities and fetch them safely.
You could still abuse this with safe code, since you could write a safe alternative to `String` that produces references with the `'static` lifetime by simply leaking on drop. That's really only an academic point though, since this design still prevents a major footgun.
`test`, `and`, `xor`, `add`, `sub` and `cmp` are all single-cycle instructions. It makes no difference which one you use.
Thank you for your answer, I can't use type alias because of this: https://stackoverflow.com/questions/50005507/how-to-use-structopt-to-parse-an-argument-into-a-vec-without-it-being-treated-as
Not the same thing, `Serializable argument` is dynamically dispatched while `impl Serializable` is statically dispatched.
 fn bar&lt;X: A, Y: A&gt;(left: X, right: Y) {} this is equivalent to your bar function
YES! More breakthroughs! Daily breakthroughs! How Exciting! How Exciting! This is proper socialist reform. Finally a programming language for the common man.
Personally I subscribe to the [`impl Trait`is always existential](https://gist.github.com/pthariensflame/9c0632d31d6c3b2672bcc7d052658e09) theory. Or with (slightly) less jargon, `impl Trait` is more or less semantically equivalent to `dyn Trait`, except that the vtable is required to be constant post-monomorphization. That principle ought to be enough to determine what `impl Trait` should mean in most other contexts. Though mutability does throw a wrench into things; `&amp;mut impl Trait`, if it became valid, would have to have *very* different semantics from `&amp;mut dyn Trait`.
It's called parameterized SQL and the idea is that, rather than having to be substituted into the string, binding the data to the `?` placeholders is deferred to as late as possible and, if the database engine supports it, it happens after the SQL has been parsed into an AST, so there's no escaping to get wrong.
&gt; Or is ? somehow special in strings which I don't realize? Not in Rust strings, but it can be special in the DB's API. `?` is the default/base placeholder in SQLite's [prepared statements API](https://www.sqlite.org/c3ref/bind_blob.html), so you call `sqlite3_prepare_v2` with your query string then use `sqlite3_bind_*` for each parameter, after which you can actually execute the statement. Other DBs may use different placeholders e.g. Postgres/libpq uses `$n` where n is the 1-based index of the parameter.
I'd personally use a `Vec&lt;Rc&lt;Tank&gt;&gt;` in `Game` and `target: Weak&lt;Tank&gt;`/`target: Option&lt;Weak&lt;Tank&gt;&gt;` in `Tank`.
&gt; Isn’t the only difference between the two the fact that SmallVec has to check whether it’s inline or heap-allocated? I would expect that this is only required when growing the vector. In all other operations you can just have a pointer to either the heap or the first element inside the vector, and never have to check anything. &gt; You can see that switching to Vec actually improves speed on many of SmallVec’s own benchmarks1: Does it have a benchmark for moving the `SmallVec` around vs `Vec`? There was an internal post complaining that the whole small memory buffer in the vector must be memcpy-ed around all the time independently of whether the buffer is full or whether the vector is allocating its elements on the heap. It would be nice to have hard numbers on that.
I feel like the solution here is going to be a set of lints that restrict Rust to a subset without the questionable features. For me, that will be: * `impl Trait` in argument position * `match` patterns ignoring references * All the `try`, `catch`, `throw` stuff in the pipeline
Yep. I'm hardcore.
I know :P But my pointer doesn’t segfault (yet)
Best answer! Because we may have exactly this need soon
My original example doesn't involve `unsafe` Rust. Yes, it would have to use a library that internally uses `unsafe` but that's also true of the correct usage of the API. The entire point of Rust is that it's possible to have a 100% safe API around an implementation that can use `unsafe` internally.
It is a well-known gotcha about unbuffered io by default. Common surprise when &lt;scripting language&gt; turns out faster than naive Rust. 
I'm trying to use reqwest to fetch various links, but I'm running into problems with a couple of https sites, getting this error: called `Result::unwrap()` on an `Err` value: ReqwestError(Error { kind: Io(Custom { kind: Other, error: Os { code: -2146762487, kind: Other, message: "A certificate chain processed, but terminated in a root certificate which is not trusted by the trust provider." } }), url: Some("https://google.com/") }) This only seems to happen on Windows (both locally and on appveyor). Is this an OS issue with certificates -- or something to do with reqwest/native-tls, or something else entirely? As in the example, it happens with sites that I'm fairly sure should be working fine, like google, youtube, etc.
If you copy [this code](https://gist.github.com/ce83df0a5301598d97119ff04fc7cd35) into Godbolt and compile it with `-C opt-level=2` then you'll see that it passes a pointer. It looks like it copies the whole thing in debug mode though.
You'll also have to watch the tracking issues, that's where things are decided in the end. E.g. that's where the "should `try` auto-convert its result" discussions happen right now. So, personally I'd be very much for a bot posting RFCs (maybe), FCPs (at least the disposition:merge ones) and tracking issues once they are created.
Your link just links to the playground, you need to click the "share" button to share your exact code.
https://play.rust-lang.org/?gist=176d61ab7e437ac3c35a6382c06653a9&amp;version=stable&amp;mode=release
That's really interesting, I wonder if LLVM does a "real" move because it has to return the data again. One would imagine that it should still be faster to pass a pointer though. I wonder what's going on here.
Coming from the .net world this is one of my bigger annoyances with most other languages SQL parameterization. In ado.net you parameterize with @someParameter and then your parameter is a key value pair where the key is someParameter. It's not the end of the world to have index based parameterization but I can't help but think it is more error prone.
I get that, I was trying to get into a mindset of someone trying to execute their query at all costs.
I don't know why it happens, modifying your godbolt example, this triggers it too: https://godbolt.org/g/WWc6eu So probably your example does not trigger it because LLVM uses whole program optimization to figure out that `noinline` cannot be called from anywhere else and still decides to inline it.
Is the recursive implementation intentionally recursive? I would expect the non-recursive implementation to produce better code.
&gt; A naive implementation might use a modulus, but this would return numbers less than 86 with double the probability to those greater than 86. Does anyone have any additional resources to learn why this is?
Finally, note that you can't actually use SGX without getting a license from Intel: https://software.intel.com/en-us/sgx/commercial-use-license-request (other than the completely insecure debug mode) From what I've heard from companies who attempted to get that license, Intel ignores almost all applicants.
That escalated quickly.
I should say this is really just an example, as the range is scaled down so we can see better that there is a bias. Suppose the RNG generates one byte at a time, so every value is in the range 0..256. If the generated value is in 0..170, the result will be exactly the same, in the 0..170 range. If it is in 170..256, the result after a modulus of 170 will be in the 0..86 range. So there is a clear bias towards the lower values in the range. When the range is small, and the RNG generates larger types like `u32`, this becomes less of an issue. And a division, or the widening multiply method, spread the bias over the entire range instead of the first values. But we still try to do things 'right'.
Documentation of the implementation (probably not the clearest) https://docs.rs/rand/0.5.0/rand/distributions/uniform/struct.UniformInt.html.
I'm not sure that it's just a matter of luck. Probably it comes down to whether or not LLVM can use `fastcc`.
&gt; I use rust-nightly-nix I used to use that one too before I switched. It looks to me like that repo is [going to be deprecated](https://github.com/mozilla/nixpkgs-mozilla/issues/15) sooner or later. &gt; especially in combination with `musl` Mozilla's overlay also supports installing additional targets, so you can use `musl` with it just fine. environment.systemPackages = with pkgs; let nightly = rustChannelOf { date = "2018-05-21"; channel = "nightly"; }; rust = (nightly.rust.override { targets = [ "x86_64-unknown-linux-musl" ]; }); in [ rust ];
I sadly couldn’t agree more…
You can compare any 2 commits as follows: [https://github.com/rust\-lang/rust/compare/98686ca...cb20f68d0](https://github.com/rust-lang/rust/compare/98686ca...cb20f68d0) This assumes that previously you saw something like (rustc 1.28.0-nightly 98686ca 2018-05-20).
I think you’re agreeing with me? This is why bare Trait doesn’t work.
&gt; I believe that LLVM will use a pointer if a value doesn't fit into registers anyway so it's only important if you're moving the SmallVec between data structures often, which I can't imagine you'd do that much It will do the actual "pass some handle to the vector to the callee" as a pointer, but that pointer will either be to a copy of the original data in the caller's stack frame, or be copied into a new value the callee's stack frame.
We’ve been tweaking the process lately, and so at the moment, yes it is.
Oh right, this totally makes sense 🤦‍♂️ Thanks for the explanation
Unfortunately the new `from_elem` looks like it's unsafe: if `clone` panics, the uninitialized `arr` will be dropped, causing destructors to run on uninit values. I suspect a good way to tackle this is to re-use `SmallVec`s existing handling of such things, by filling in the array with a length of 0; maybe something like: let mut arr: A = SmallVec::new(); { let ptr = match arr.data { Inline { ref mut array } =&gt; array.ptr_mut(); Heap(_) =&gt; unreachable!() }; // for ... } arr.len = n; arr Something slightly more robust would be to update `len` progressively inside the loop, so that if a panic does occur the earlier fully-initialized elements are dropped.
You’re right. Source: I was there when this happened.
`and` is not suitable for testing for equality of whole values, `test` is more useful for example to check whether a particular bit is set. And `xor` leaves side effects in register contents. So `cmp` is the correct choice.
&gt; Probably it comes down to whether or not LLVM can use fastcc If that's the issue that would be great. If it depends on whether the caller can be inlined, then I think "luck" is pretty accurate, since inlining heuristics are pretty much unpredictable and subject to change. 
&gt; but that pointer will either be to a copy of the original data in the caller's stack frame, or be copied into a new value the callee's stack frame. Do you know what the heuristic is for LLVM to do one or the other?
Normally, yes. Panics do stack unwinding, which will drop data. However: - There's a mode `-C panic=abort` that you can use when compiling, where all panics are hard aborts (no unwinding at all). - Some errors (OOM when allocating, panic inside a `drop` method, etc), will cause an abort, regardless of settings. - If you leak memory (by introducing a reference-counting cycle or using mem::forget), drop will never be called (on the success or error paths).
Guaranteed is a strong word. This depends on what do you mean by a fatal error. A panic with unwind will drop, an abort won't. That said, aborts are very rare, usually in situations when it's not a good idea to continue, say, out of memory situations.
I guarantee you that there is already code out there that depends on this. That's what stabilization means.
Not only does it produce better code, but it is asymptotically faster too: O(n) instead of O(1.618^(n))! For this particular benchmark, I imagine that they're not trying to benchmark the fastest way to compute the particular values, but rather things like the overhead of function calls, and recursive Fibonacci is a great way to get a lot of them.
For historical curiosity: the very first version of this code is from 2014: [https://github.com/servo/servo/commit/beb9b9bd92b8d38932583ff4b129293d8cc5d565](https://github.com/servo/servo/commit/beb9b9bd92b8d38932583ff4b129293d8cc5d565)
By default, yes, a panic will still cause drops of stack-anchored values to run. However, there are various ways to terminate programs that prevent destructors from being run, from `-C panic=abort` all the way up to pulling the computer's power cord.
Regarding "Logging, behind the log feature" - I wonder what sort of events does `rand` log?
Would you mind to post a full example for `/etc/nixos/configuration.nix`? I wasn't able to make it work :(
Absolutely, so much thanks! The documentation produced by Carol, Steve and the Rust community as a whole is the best I think I’ve ever seen for a language ecosystem. We are very fortunate.
thank you :\) 
I think that is something we should document a bit more... When creating an RNG using the `FromEntropy` trait, and especially when you use `thread_rng` for the first time, we use [`EntropyRng`](https://github.com/rust-lang-nursery/rand/blob/master/src/rngs/entropy.rs#L106). It logs which source of external randomness gets used for seeding at debug level. Usually `OsRng`, alternatively `JitterRng`. `OsRng` logs [when it receives an error from the OS and how it deal with it](https://github.com/rust-lang-nursery/rand/blob/master/src/rngs/os.rs#L120), on Linux whether it [uses the new syscall interface or the old `/dev/urandom`](https://github.com/rust-lang-nursery/rand/blob/master/src/rngs/os.rs#L302) and at trace level every time it is used. The `JitterRng` entropy collector logs when it is used (at trace level), and the result of its timer self-test. The `ReseedingRng` adapter logs when it reseeds an RNG, or after which delay it retries again if reseeding fails.
Then this post does not live up to its promise. 
Thanks boscop, i'm trying different solutions to make multiplatform application, du you think that web\-view is a good starting point to make graphical application ? 
Yeah, I was ambiguous by using "fatal error", thanks for the help!
Ok, thank you!
When the version number of the nightly bumps up, it doesn't necessarily mean that there's anything particularly new; it just means that a new stable release has come out, so a new beta release has been forked off and the nightly version number incremented. In general, features are developed for a long period of time on the nightly releases, under a feature flag because they may be unstable (not that they may crash, although that is possible, but unstable in the sense that they may change in backwards-incompatible ways). Once the team thinks that the feature is ready to commit to keeping the same and not changing any more, the feature can be stabilized, and will then be available to use without a feature flag both in later nightlies, and the next beta release, and the stable release after that. For keeping track of what's going on in the development of the compiler and standard library, it's best to follow [This Week in Rust](https://this-week-in-rust.org/), which collects important things that are happening. The one which covers the version increment in nightly from 1.27 to 1.28 (which corresponds to the release of 1.26 stable) is [the most recent issue](https://this-week-in-rust.org/blog/2018/05/15/this-week-in-rust-234/), but if you want to know what is in this nightly compared to the recently release 1.26, you'd want to read back through the archives until you get to the [1.25 release announcement](https://this-week-in-rust.org/blog/2018/04/03/this-week-in-rust-228/), since after the 1.25 release, all new features landing in nightly would be slated for 1.27 or later.
Can AMD SME/SEV replace Intel SGX? If so, do companies need a license for that as well?
Here's 2 articles by Eric Lippert that go into random numbers and sampling in some detail: * [How much bias is introduced by the remainder technique?](https://ericlippert.com/2013/12/16/how-much-bias-is-introduced-by-the-remainder-technique/) * [Generating random non-uniform data in C#](https://ericlippert.com/2012/02/21/generating-random-non-uniform-data/)
Yes, I was trying to say "previously, we did have that requirement, but due to reasons, have relaxed it to see how it goes." The current requirement is that a bug must be filed; that's fulfilled by https://github.com/rust-lang/book/issues/1291. This hasn't been done yet because... I was working on docs for `impl Trait`: https://rust-lang-nursery.github.io/edition-guide/2018/transitioning/traits/impl-trait.html
https://github.com/rust-lang/rfcs/blob/master/text/1951-expand-impl-trait.md
Isn't `leak()` returning a `&amp;'a mut` reference?
Great catch. I think that it would actually be better to wrap the array in a `ManuallyDrop` so that it just leaks on panic. Progressive updates to `len` inhibit optimisation (see `alloc::vec::SetLenOnDrop` in the stdlib).
General approach to such actor I guess should: 1. Initialize stream in `Actor::started` 2. Implement `StreamHandler` with handler that handles each item, `StreamHandler` has two methods to check if stream ends: `error` called when stream threw error(in most cases stream will stop) and `finished` which is called when stream ends(no more Item). In both handlers you're likely to call `ctx.stop()` as otherwise you actor will continue to leave with finished stream 3. Implement `Supervised` for your actor. Its method restarting should be used to clean-up state for next call of `Actor::started` 
:(
To give a little more detail, the [site /u/K900_ linked to is a collection of accepted RFCs](http://rust-lang.github.io/rfcs/), along with a description of the process. Accepted doesn't mean they will actually land exactly as is, but that implementations that basically follow the RFC will be accepted under feature gate for experimentation, and eventually if it looks like it works out in practice, then it can be stabilized. The process starts by sending a [pull request to the RFCs repo](https://github.com/rust-lang/rfcs/pulls). There will be some amount of discussion and debate, and when it seems like that has settled down and not much new is being added, the appropriate team will call for a final comment period, which lasts for a week, after which the RFC can be accepted, deferred, or rejected (the latter two are basically the same, just with some extra information about whether the team thinks it's not time now versus not likely to ever be accepted). Once accepted, implementation can begin, and [a tracking issue filed against the rust-lang/rust repo](https://github.com/rust-lang/rust/issues?utf8=%E2%9C%93&amp;q=is%3Aopen+label%3AB-RFC-approved+label%3AC-tracking-issue). If you have a specific issue you'd like to track, you can subscribe to the RFC pull request when that is active, and then later subscribe to the tracking issue on the main repository. If you want to follow everything, you could subscribe to the whole RFCs and Rust repositories, but that would be drinking from a firehose. [This Week in Rust](https://this-week-in-rust.org/) generally does a good job of mentioning noteworthy changes in status for RFCs and features.
Honestly, in this sort of case, the semantic difference between Id's and raw pointers seems pretty small. Id's are slightly harder to make your program explode with, 'cause array's are bounds checked. However there's also the danger that your objects may move, for example if the Vec is reallocated. Then all your pointers become invalid.
I'm interested in how you can do that without unsafe block.
Not sure whether this answers what you're asking, but there's several crates that demangle Rust names; the best is probably `rustc-demangle` I think. perf and gdb should know how to demangle rust names themselves now though; they certainly do on my system, using perf 4.16.5. Flamegraph, I don't know about.
Code within an SGX enclave can't call out to the operating system, so Baidu has done a heroic effort of re-implementing half of the entire Rust std library. Really appreciate all the hard work you've put into this. Some small questions: 1. How is one supposed to use the samples in simulation mode? When I compile with the SGX_MODE other than "HW" I am unable to run them. It looks like only the enclave is linked against the simulation libraries but not the app (sgx_urts vs sgs_urts_sim). 2. The samples have Xargo builds as well as plain Cargo builds, what are the Xargo builds needed for? 3. Building with Xargo needs almost the entire SDK source, would it make sense to include the SDK in the Docker image to make it easier for others to create their own Xargo builds? 4. Is it possible to link the untrusted side of your app statically so I don't need to package several .so files along with my app? Again, thanks for all your hard work!
In general, you cannot rely on destructors running. This has led to unsoundness in the past.
Are you wanting something like this? ``` extern crate futures; extern crate hyper; extern crate tokio_core; use futures::{Future, Stream}; use hyper::Client; use tokio_core::reactor::Core; struct A { core: Core, client: Client&lt;hyper::client::HttpConnector&gt;, } impl A { pub fn new() -&gt; A { let core = Core::new().unwrap(); let client = Client::new(&amp;core.handle()); A { core, client } } fn get_future_string(&amp;mut self) -&gt; impl Future&lt;Item = String, Error = hyper::Error&gt; { let url = "http://www.google.com".parse().unwrap(); self.client .get(url) .and_then(|res| res.body().concat2()) .map(|r| std::str::from_utf8(&amp;r).unwrap().to_owned()) } fn get_string(&amp;mut self) -&gt; Result&lt;String, hyper::Error&gt; { let a = self.get_future_string(); self.core.run(a) } } fn main() { let mut a = A::new(); let b = a.get_string(); println!("{:?}", b); } ``` You can't specify a generic as a return error, as that's what the caller specifies. When using futures, you need to keep track of what the `Item` is and what the `Error` is every step of the way. For example: 1. `get(url)` is `impl Future&lt;Item = hyper::Response, Error = hyper::Error&gt;` 2. `and_then(...)` is `impl Future&lt;Item = hyper::Chunk, Error = hyper::Error&gt;` 3. `map(...)` is `impl Future&lt;Item = String, Error = hyper::Error&gt;` So your final return type will be `impl Future&lt;Item = String, Error = hyper::Error&gt;`, which means you will get `Result&lt;String, hyper::Error&gt;` if you run in on a core.
I would just have one struct, and wrap it in two different newtype structs: struct Inner { s: String } struct A(Inner); struct B(Inner); fn a_to_b(a: A) -&gt; B { B(a.0) } Note the lack of references. Not sure you can do it safely with references. HOWEVER, DON'T USE TRANSMUTE! You can do it rather more safely just with pointer casts: fn a_to_b(a: &amp;A) -&gt; &amp;B { unsafe { &amp;*(a as *const A as *const B) } } Transmute is a much *larger* footgun than pointer casts are, so avoid it whenever possible.
I have not found it difficult to obtain enclave signing keys (I've done it twice now on behalf of two different companies), provided you have a secure key storage method for the enclave signing key and a legitimate use case for SGX.
Sorry for asking such an overly broad question, but when I write structs in Rust I am not sure whether to use lifetimes/refs in the fields, or to have owned fields, like struct User { name: String } vs struct User&lt;'a&gt; { name: &amp;'a String } What heuristics do you usually think about for this, when designing new structs? Second, I was wondering why the compiler needs to have the `: 'a` information here: struct Foo&lt;'a, T: 'a&gt; { bar: &amp;'a T } Doesn't this seem redundant? However if I write it without it: struct Foo&lt;'a, T&gt; { bar: &amp;'a T } rustc complains `the parameter type `T` may not live long enough`. This is confusing because I've specified that `bar` has lifetime `'a`. Also it seems kind of restrictive because what if you want a ref as well as an owned value of type `T`? Like struct Foo&lt;'a, T: 'a&gt; { bar: &amp;'a T, baz: T } It's not clear to me what function the `: 'a` is telling the compiler here. Thanks!
which_is_the_fastest is strange benchmark. it does not correlate with techempower in any way.
&gt; and last I heard, the compiler is randomizing field ordering in debug mode to eagerly break code that assumes field order is defined when it is not This is *fantastic*. :-D
Oh Thanks, that makes sense. The only thing I'm not sure about is what to do with the unwrap here. `let url = "http://www.google.com".parse().unwrap();` I guess I could just pass in the string as a parameter but that works here in this example. How would I in general handle a function like this where there are two different Results with errors?
Only if you were expected to clean up a ref-count cycle or similar in your drop code. After the user-written drop code is run, `drop` is recursively called on each element of the struct, so there's no risk of forgetting to manually drop a field.
&gt;also for performance reasons it might be smart to not work on \[T; 32\] but on \[T; 4096/sizeof\&lt;T\&gt;\(\)\] which is sadly not possible in rust. Could you explain why `4096 / size_of&lt;T&gt;`? Also, it is possible if you move the const\-calculation into a const value: [https://play.rust\-lang.org/?gist=82e59c17c7e96abf87c2947fcdab9b01&amp;version=stable&amp;mode=debug](https://play.rust-lang.org/?gist=82e59c17c7e96abf87c2947fcdab9b01&amp;version=stable&amp;mode=debug)
Yeah, this is an old technique which has one flaw: can be ignored easily. The OP suggests to use `'static` in order to prevent easy ignoring.
You are right, but you can also write a O(n) tail-recursive version that gets optimized to similar code as the iterative one.
I haven't fully explored the question, so I'm not really sure what the right answer is yet. Thinking about it I note that the formula can be rewritten `T1+(T2-T1)/2` (the difference between two thermodynamic temperatures is a temperature interval; the difference between two points is a vector). I also did a little bit of reading about translation and scale invariance, but can't say I fully understand it yet.
 use std::rc::Rc; use std::cell::Cell; struct Foo(Cell&lt;Option&lt;Rc&lt;Foo&gt;&gt;&gt;); let foo = Rc::new(Foo(Cell::new(None))); let bar = Rc::clone(&amp;foo); foo.0.replace(Some(bar));
Specs will GC your entity IDs too, which is cool.
Actix is not classical actor system, it has responses and backpressure on message queue. Response is usability feature, you can always use `()` as a response.
I get that it's just defined that way, but I don't see the motivation behind it.
The motivation is that if you implement Copy, implementing Clone is trivial and something you should probably do.
No, clone is not used when deriving copy. The copy is bytewise. It does not really make sense to a have a type that is `Copy` and not `Clonable`. `Clone` means "This type can be duplicated". `Copy` means "This type can be duplicated *by copying bytes*", so it implies the former.
I make the decision based on how the struct is going to be used. If it is a view of some pre-existing data , I'll use references. If it owns its on data, I'll use the owned version. I'm guessing the compiler doesn't actually require the lifetime annotation in a simple case like the example you've got above, but it requires you to add it anyway because these things can get complicated and it's better to have you develop the habit. The reason your struct requires `T` to be defined as `T: 'a` is that T may itself be a reference, and in that case you need to specify that T itself lives as long as the reference to T.
But isn't it expected that the implicit copy in an assignment is the same as calling clone instead? In that case it feels redundant.
I'm taking the plunge of wrapping my head around futures/tokio by adding futures-backed `Stream` interface to alsa-rs's MIDI sequencer API. Adding a basic `Stream` implementation was [simple enough](https://github.com/jswrenn/alsa-rs/commit/790a81ff7e2c104ceebf22ce7e5628f537b7a579#diff-f0a22d0ca5f8f67f38198c46ef26ad68): pub struct InputStream&lt;'a, 'b&gt;(&amp;'b mut Input&lt;'a&gt;) where 'a: 'b; impl&lt;'a, 'b&gt; Stream for InputStream&lt;'a, 'b&gt; where 'a: 'b { type Item = Event&lt;'a&gt;; type Error = Error; fn poll_next(&amp;mut self, cx: &amp;mut Context) -&gt; Result&lt;Async&lt;Option&lt;Self::Item&gt;&gt;&gt; { if self.0.event_input_pending(true)? == 0 { Ok(Async::Pending) } else { unsafe{self.0.event_input_unsafe()} .map(Some).map(Async::Ready) } } } However, ALSA offers a file descriptor polling readiness API (wrapped by alsa-rs [here](https://docs.rs/alsa/0.2.0/alsa/poll/trait.PollDescriptors.html#impl-PollDescriptors-1)). It would be _great_ to take advantage of this API so all of my [recent MIDI work](https://www.reddit.com/r/rust/comments/8fx3d2/whats_everyone_working_on_this_week_182018/dy831gz/) didn't burn CPU time! From the perspective of a library author trying to provide a thin-but-safe-and-ergonomic wrapper around ALSA, what's the right way to expose this in the futures/tokio world? 
That's not satisfactory at all. In my mind clone should return the same as an implicit copy in an assignment so it's redundant. I don't see a use case where one would want to do something different with clone.
Yes, I'm mainly extending. Should have framed it like that.
Those performance improvements are amazing. Thanks a bunch!
In my wall of text, I didn't actually answer your last question. So, I'll do that separately. I wouldn't quite say that people talk about "hierarchies" anymore (as mentioned, language like 'high-level' and 'low-level' seems to have fallen largely by the wayside), but there are definitely different kinds of compilers that people talk about. Some compiler distinctions: - Type-preserving vs type-erasing - Single-pass vs multi-pass - Ahead-of-time vs just-in-time - Whole program vs compositional - Macro expressible vs not (as implied by my larger wall of text) All these distinctions are useful because they are (1) defined precisely and (2) relevant to some particular goal(s) of a research program.
There were two separate RFC's: "limited" for impl trait in return position and "universal" for impl trait in argument position. Both were accepted, and "impl trait in argument position" was highlighted twice in "This Week in Rust": [This Week in Rust #184 (30 May 2017)](https://this-week-in-rust.org/blog/2017/05/30/this-week-in-rust-184/): &gt; **Approved RFCs** &gt; Changes to Rust follow the Rust RFC (request for comments) process. These are the RFCs that were approved for implementation this week: &gt; &gt; * RFC 1951: Finalize syntax and parameter scoping for impl Trait, while expanding it to arguments. &gt; [...] [This Week in Rust #209 (21 Nov 2017)](https://this-week-in-rust.org/blog/2017/11/21/this-week-in-rust-209/): &gt; **Updates from Rust Core** &gt; 110 pull requests were merged in the last week &gt; &gt; * implement impl Trait in argument position (RFC #1951) &gt; [...] Yes, it *could* have been made even clearer. But I think the devs made a pretty good effort. 
&gt; So to me this isn't so much a fundamental difference between stack and heap, and is more about imbuing a destructor with interesting behavior. Indeed; this is a much narrower (and thus more useful) characterization.
It’s all good &lt;3
It's not mentioned here, but is wasm32-unknown-unknown supported by this crate?
`Trait` = `impl Trait` means an implicit monomorphisation cost for the benefit of 5 characters saved.
You are right, as it turns out, in two dimensions, the range is quite small due to the points being very close together, just artificially widening the range everytime makes me treat more points, but that cost is offset by having a much smaller btree, which costs less to traverse. Thank you a lot, pls let me send you money.
&gt; Apart from the C/C++ SDK provided by Intel, Rust SGX SDK is the only recommended SDK listed on Intel SGX's homepage. That's a fairly impressive achievement on its own!
Thank you a lot, although you didn't find the solution, i think you contributed heavily, thank you for that.
Yes, but you will have to use the `stdweb` feature flag.
I'm not seeing the claimed performance improvements unfortunately, in fact I'm seeing the exact opposite. I have a program which generates a lot of random numbers, and security isn't a concern, so I was previously using weak_rng. After updating to .5 in Cargo.toml and changing the one call to weak_rng() to SmallRng::from_entropy() my total program runtime in release mode has increased by 40%. Any ideas what's going on here?
Yes I agree, but in the case of them being identical, Clone is redundant which is what bothers me.
The [`UniformFloat` docs](https://docs.rs/rand/0.5.0/rand/distributions/uniform/struct.UniformFloat.html) say &gt; Currently there is no difference between new and new_inclusive, because the boundaries of a floats range are a bit of a fuzzy concept due to rounding errors. So `Uniform::new_inclusive(0f64, 1f64)` is not actually inclusive, which seems misleading at best and subtly buggy at worst, like if you _really_ wanted to know that `0.0` and `1.0` could be hit for, say, a texture sample, or anywhere else that `[0.0, 1.0]` is completely semantically valid. Would this case be resolved by `Uniform::new(0f64, 1.0 + f64::EPSILON)`, or is there no guarantee on how much is excluded on the top end? Maybe it doesn't really matter and a floating point range should be viewed more as `[~low, ~high]`.
The problem is that the current type system doesn't allow you to express something like "this implementation of `Clone` should apply to all types that are `Copy` and don't implement `Clone` themselves". [This post](http://smallcultfollowing.com/babysteps/blog/2016/09/24/intersection-impls/) explains the problems pretty well, and proposes an extension to the current specialization rules to allow it to work. People are still working on specialization to this day, so something like it might become possible in the future. For now though, you can just `#[derive]` both traits. 
Are there still plans to integrate parts of the crate back into stdlib? It would be convenient for newcomers, and people tend to expect some kind of rudimentary random() function in standard libraries. 
Have you considered defining views for queries that are too complex for the ORM?
I use hugo with a custom version of the “one” theme. You can see the source [here](https://github.com/Vurich/troubles.md) but I don’t think it’s 100% up to date with the live site
Hard to say without more details. Could you share the program? `weak_rng()` and `SmallRng::from_entropy()` should be equivalent, so that is unlikely to be the reason.
Thx very much for your well crafted overview and the reason about why transpiler isn't a good term.
Good stuff! Thanks for sharing. Excited to see this work!
I can't image a case where you'd want `Clone` to do something different from `Copy` either, but that's kinda beside the point. You can just do `#[derive(Copy, Clone)]` to instantly get the right kind of behavior. Anyway, another reason why it can be handy for `Copy` to imply `Clone` is with trait bounds. Imagibe you have some function that takes `T: Clone` as an argument. It would be pretty odd if you couldn't pass `Copy` types into the function despite them being trivially `Clone`-able
Do you have a better idea how to implement `Uniform&lt;f64&gt;`? Note that you can use `Standard`, `Open01` and `OpenClosed01` which might suit you needs.
Sure, it's here https://github.com/smmalis37/FighterSimulator. This is really just a small hobby project, nothing "business critical" or anything important like that, but I figure if I'm seeing such a big perf hit I won't be alone.
`Clone` should be implemented for `Copy` types by default. But you can't do this with current limitations of type system.
Some detailed discussion of leaks here: https://doc.rust-lang.org/nomicon/leaking.html
What's a widening multiply?
You're welcome.
To clarify this point further, let me restate /u/Quxxy's point. Type parameters don't have to be named with boring, single character names like `T`. You can give them long, descriptive names. In your final example, you specified two type parameters: `T` and `PartialOrd`, where `T` implements `Copy`. The fact that there is also a trait named `PartialOrd` (or `Copy` in your first example) is entirely irrelevant at that point in the compilation. But, this is not dangerous. As you saw, you got an error message, and the code didn't compile, run, and blow up. If you manipulated the code to the point that it would compile, but you still left `PartialOrd` floating around as an unused and unconstrained type parameter, you're still going to run into compile errors when you try to call the function, and you'll be confronted with the need to fix the error or else be doomed to adding a useless type parameter to each invocation of the function, which is just not going to be a thing. tagging /u/2_bit_encryption 
Yes you can (since `Box` does all the heavy lifting for you): let _: &amp;'static str = &amp;*Box::leak(Box::new("Hi".to_string()));
There is also the `ManuallyDrop` wrapper you could use for a field, which then doesn't get dropped automatically. This is useful in some `unsafe` scenarios, where dropping an invalid value would result in undefined behaviour.
Yes, if your app needs any kind of GUI elements (i.e. not pure opengl rendering) I'd use web-view. You can also use the [yew](https://github.com/DenisKolodin/yew) frontend framework to write your frontend in Rust, then compile to wasm/asm.js.
Have you tried `from_rng(thread_rng)`? The documentation suggests this if you're initializing a lot of `SmallRng`'s. https://docs.rs/rand/0.5.0/rand/rngs/struct.SmallRng.html#examples
IMO production quality libraries should almost never use `failure::Error`. It is useful for application code and while prototyping libraries. Panic if either the library or its caller has a bug. Return a specific error (usually an enum) if the caller is expected to handle it. 
Also I notice you're using `rayon`. This might be overkill for a hobby project, but you could look into PRNGs designed for parallelism: there are many `xorshift` PRNGs with jump functions. http://prng.di.unimi.it has many. Pract Rand offers a whole library with convenient "create an un-correlated PRNG" functions http://pracrand.sourceforge.net/RNG_multithreading.txt
Not unless you also do one of the above things inside of the drop method. Drop::drop takes a mutable reference to self and then will necessarily drop sub-fields afterwards. You can't really cause memory leaks by messing this up unless you purposefully `mem::forget` the real value and replace it with a placeholder instead, or you introduce a reference-counting leak. There's no way to accidentally cause memory leaks in Drop implementations, you still have to do it purposefully.
Can you give an example? I can kinda see where you might have type parameters that aren't used anywhere in the function body, but perhaps we should have a warning for parameters that aren't used *at all*, including in the parameter/output types.
You can still use ```rust #![feature(rustc_private)] extern crate rand ```
Is rand 0.5.0 running on stable rust?
for example: a multiply that takes two 32bit ints as input and returns a 64bit int
The communication problem was less that people didn't know that impl trait in argument position was a thing, but more that people didn't know it was being stabilized along with impl trait in return position. When I saw announcments about stabilizing impl trait, I just assumed they were talking about it in return position, since that had been around for a while and was well agreed to be a good idea. To be clear, that's definitely my fault for not actually reading stuff more carefully, but the sheer number of people that had the exact same problem indicates that there was possibly something that could have been done differently.
&gt; Maybe LTS releases would be an idea. How would that be different from 1.x? Has any backwards progress been made in terms of stability or portability?
There's some much more spooky stuff going on here, I'd love to discuss with you or any passers-by on my attempt [here](https://github.com/Enet4/julia-bench-rs/pull/3).
Sure, but you the "code depends on it" argument is a whole lot more convincing after the feature has been around for a longer time. Breaking changes are always going to be a problem, but it's way more of a problem if it breaks half of crates.io than if it breaks a few people's master branch on github. It's not a binary thing.
Auto-ref matching? :D
Well said with the `Weak`, I was hoping someone would mention that! I generally like the pattern of having one owner, and the rest being weak references, so we can still control the lifetime.
Interesting, will try that when I get home.
Interesting, will try that when I get home.
Probably [`BufRead::lines`](https://doc.rust-lang.org/stable/std/io/trait.BufRead.html#method.lines) is your best bet currently. `StdinLock` is `BufRead` already, or you can use [`std::io::BufReader`](https://doc.rust-lang.org/stable/std/io/struct.BufReader.html) to buffer any `Read`. Another drastic option is to find a crate that will memmap the file for you if it's really that big. There's also the nightly only [`std::io::Read::chars`](https://doc.rust-lang.org/stable/std/io/trait.Read.html#method.chars), but it's not stable yet because a) on a raw `Read` it's probably useless as it'll be a syscall per byte, b) the proper behavior when encountering non-utf8 data is unclear. How big of an input are you dealing with anyway? If it's on the order of MB, then just allocate the String, it'll make your life easier.
yeah, the clone i would rather avoid, but the pointer cast is a good idea, thank you!
Thanks for attempting to combat the statistical bias that shows up in this type of discussion. I do however think this is worded in a kind of hostile way, and I don't think the implication that anybody who is against universal impl trait just hasn't given it enough thought. There are people have been against it since the very beginning, and you have no way of knowing how much consideration a given person has put into thinking about the feature. Either way, it's kind of irrelevant to whether or not that person's arguments are sound.
i actually have not tried doing that recently, great if rust recently gained the ability! the reason is that having a segmented linked list is only useful for large data structures anyway so you might as well just use full pages and remove some load from the allocator.
I don't know how much I can vouch for it having never used it myself, but a quick search on crates.io got me to [the unicode_reader crate](https://crates.io/crates/unicode_reader), which provides [a clean-looking iterator wrapper]() to get UTF-8 codepoints (or a parse error) out of any byte-producing iterator. Usage should look like: ``` use unicode_reader::CodePoints; use std::env::stdin; for cp in CodePoints::from(stdin().bytes().flatten()).flatten() { // Do something with each valid code point; this will ignore all errors and discard invalid byte sequences silently. // If you want to handle errors differently, replace the two calls to `flatten` with your preferred approach. } ```
Sure, but then the thrust of my same questions and arguments apply in that case too. I think it's important to know what is and is not possible to reach in the boundary cases.
For example, if Tank A is `target`ing Tank B, and we remove Tank B from the central list \(intending to delete him\) but forget to clear Tank A's `target` field, then there's a chance that in the future, we can read Tank A'`s targ`et field and call th`e fire`\(\) function, which shows a little firing graphic. This is a problem solved by `Weak` \(like your example\) and also the ID approach, since they both don't keep the target alive. Just curious, what's the tradeoff with RefCell\&lt;Option vs Option\&lt;RefCell?
Ah, got it, thanks
Well said, I think IDs are only a minor step up from raw pointers. Also, they both share a risk if anything is removed from the middle of the Vec, suddenly all our IDs and pointers are invalid.
Ids are minor step, but you can go further implementing safe api around it.
I quite agree, actually... I think the borrow checker is a force to be reckoned with, and one of the biggest challenges is storing conceptual borrows like this. When people hit this wall, they find the ID solution, or the Rc/RefCell solution, or both. Alas, both have problems. I think the "something that isn't quite" right is that people are so exhausted fighting the borrow checker that they'll take the easiest solution, namely, the ID approach, and when they see that it appeases the borrow checker they think "ah, this must be safe!" when it really isn't. I think the best solution is to have one "owning" Rc\&lt;RefCell\&lt;Tank\&gt;\&gt;, which can be pointed at by multiple Weak\&lt;RefCell\&lt;Tank\&gt;\&gt;. Then there's some potential in making some new pointer types which enforce that... that's the direction to think in, not investing more in this dangerous ID approach.
You can use a trait. call it Location, provide default implementations for all methods you need, require implementations for the x(&amp;self) -&gt; i32 y(&amp;self) -&gt; i32 z(&amp;self) -&gt; i32 methods. implement those for your BlockLocation and ChunkLocation.
maybe something like: https://github.com/Boddlnagg/units 
It's not redundant because it allows things to require Clone, and then work fine on Copy things too. `Option::cloned`, for instance, turns an `Option&lt;&amp;T&gt;` into an `Option&lt;T&gt;` by using Clone. This way that also works for all `Copy` types, and we don't need a separate `Option::copied` method. True, you will never call `clone` yourself on a `Copy` type, but it's useful so that generic code can be reused. It's an annoyance when writing new types, but with `#[derive]`, it's a very minor one.
That's a good point. They aren't the same thing at all. The analogy is more of a quick, first pass (and, ahem, inaccurate) explanation to give on the proverbial new users first day when they're looking at the code and you want to give them a quick reference point without bogging them down with details. "Oh, so that restricts the arguement to be something that conforms to this interface-ish thing you call a 'trait'. Got it. That makes sense..." Then after a month or two, you pull back the curtain for that proverbial user and explain in more accurate detail how the mechanics work and show how impl Trait relates to generics with bounds etc. In regards to learning, Rust has nuances that imo are worth abstracting for the sake of overall simplicity in learning even if those abstractions leave pockets of knowledge that will need clarification at a later date. 
It looks like Simon Sapin has a small crate that can help you consume UTF-8 bytes incrementally, handling all the character boundary bookkeeping for you: https://crates.io/crates/utf-8. I guess most people with use cases like this just use raw bytes though. I'm curious what your use case is.
I'm really struggling with anything other than basic use-cases in `failure` I'm trying to re-work error-handling in `transitfeed` to us it and this basically means just wrapping existing `csv::Error` into a custom error type that stores the filename plus the cause. The ideal is that format!("{}", some_transitfeed_error) would result in: `error parsing test.txt:2 - day of week field was not 0 or 1` instead of `CSV deserialize error: record 1 (line: 2, byte: 12): day of week field was not 0 or 1` This would involve a whole bunch of matching (position is only available for some `csv::Error`) which I doesn't work with `[fail(display)]` I could create a bunch of different enums and match in the library logic but then I still have to call `.kind()` in `[fail(display)]` which also doesn't work. I'm really struggling to find any good documentation or tutorials that can show me other ways to work with failure so any tipes would be appreciated.
Well, the program I’m writing is more of a command line utility, so the input can be potentially endless. Reading lines might be the best solution using no external crates, but I want it to work well even when there are no line breaks in the text.
`T` could be an actual reference, yes. It could also be a type like your `Foo`, which contains actual references but isn't a reference itself. Or for a third option, it could be something like `MutexGuard`, where conceptually it's a borrow of the `Mutex`, but in practice it's got a bunch of unsafe pointers on the inside.
But here's the issue: Try `impl&lt;L&gt; Add&lt;L::Displacement&gt; for L where L: Location`. I get the compiler complaining that 'only traits defined in the current crate can be implemented for a type parameter.'
Where would it be important? Programs are viewed with suspicion when they test for equality of floating-point numbers and a program which works if an input is &lt;1 but not =1 is doing nothing less. Take this example: a sequence of floats in [0,1), when mapped over with `x + 3.0`, results in a sequence of floats in [3,4] (closed on the right because of the higher resolution left of 1 than left of 4). Issues of this sort abound so either you are prepared to handle them by a careful floating-point analysis, and you will probably need more information about the floats coming out of your generator than the documentation gives you, or you are not prepared to handle them and relying on the fact that a float you get is not exactly equal to 1, though it be as close as you like, is treading a dangerous line. I think it would be better to say that the floats lie in a closed range whether either bound is actually generated or not (and programs should be prepared never to get the bounds, just as they should be to never get any other fixed value in the interval, by dint of overwhelming probability). But I've never seen a good application for knowing exactly whether the bound is actually generated, so maybe there is one?
It’s just a small text processing utility, that progressively operates over the input. As the input might be large, I don’t want to allocate everything into a String and then use `.chars` on it, both because it’s worse for memory and speed, but also because I don’t want the program to have to read the entire input before doing any work, doing the work progressively as the input comes in is much better say, if you’re piping a command that takes a while to complete such as `curl` on large pages etc. I guess line buffering could work, but no buffering at all seems more “unix-like”
If you need to support input beyond what allocating by line will get you, you're probably best off copying the implementation of `std::io::Chars`. You know what assumptions you're willing to make and what concessions you can give towards fault tolerance. That said, though; since you're operating on utf-8 input only, I think it's highly unlikely that you'll have to deal with a multi-megabyte line in the first place. And to get into the range where trying to grab that much memory would actually be a problem would require something along the lines of the entirety of English Wikipedia with all of the newlines stripped out. Basically: make a note that this is a potential problem for large inputs but don't worry about it until it proves to be a problem. You're preparing for a worst-case that's an ergonomic nightmare and very unlikely to happen in practice. Here in Rust-land we tend to get scared of unnecessary allocation; does that _need_ to be heap allocated; how can I make this a zero cost abstraction; but in most cases doing it the obvious way will work fine. Unless you're in direct competition with a tool that can do what this would prevent you from doing, take the way way. It's not worth the pain. That said, if you do happen upon a good design for a `Chars` iterator over an opaque source of bytes, please do share, it could be reused in many places.
Nice read :\) I work at a major tech company in their automotive group and am investigating the pros/cons of commodity software \+ validation vs stricter formal languages for systems software. I have a few questions: How did you decide on the four pattern types to illustrate in the paper? \(Mutable aliasing, const, enums, and data races.\) How would you respond to someone who presents an argument that these are not common, or heavily construed examples? How do you address the argument that while other languages might prove more amenable to writing safe code, the penetration of C among developers vs those who know Rust/OCaml/Ada make it difficult to convince business people to pursue other languages? While Rust/OCaml/Ada have stricter syntax and can provide better guarantees than C, do you do any other validation of written code, other than comprehensive regression testing, or do you assume that the language \+ compiler are sufficient? If you write systems\-level code \(operating system or similar\), what rates of unsafe to safe code do you expect, and do you find it tractable to maintain low percentages of unsafe code, given the need to interact with hardware, or for concurrency, etc.? Thanks! Feel free to PM me if you'd prefer to talk offline, and we can exchange via email :\-\)
"Mike Pall is a robot from the future" http://wren.io/performance.html
What do you mean?
Note that `ThreadRng` uses `StdRng`, which is relatively slow to initialize. Alternatively to using `from_rng(&amp;mut thread_rng())`, you could try to only use one `StdRng::from_entropy()` from one thread to initialize the fights before parallelizing the workload. This would require implementing and using `Fight::from_rng(rng: &amp;mut Rng)`.
Nice! In general x1\*T1 \+ x2\*T2 \+ ... \+ xn\*Tn makes sense if x1\+x2\+...\+xn = 1, and I think your trick works: you can rewrite that as T1 plus some linear combination of \(T2\-T1\) up to \(Tn\-T1\).
Yes, that is exactly the reason.
Part of what `NonNull` is doing is making the pointer [covariant over lifetimes](https://doc.rust-lang.org/nomicon/subtyping.html), which `*const` pointers already are. I'm not sure whether that's the whole answer though; there seems to be [more here](https://www.reddit.com/r/rust/comments/7zmfuu/when_should_nonnull_be_used/).
Hopefully better support for RNGs in combination with Rayon will be a feature in the next version. An RNG with streams would be my choice there, like PCG. Jumping is also possible, but to my knowledge not yet implemented in a way easily compatible with Rayon anywhere.
For things like `rng.gen().log()` it is important whether you generate 0 or not.
Yes: https://travis-ci.org/rust-lang-nursery/rand/builds/381995352
Wrap the field in [`ManuallyDrop&lt;T&gt;`](https://doc.rust-lang.org/beta/std/mem/union.ManuallyDrop.html)
`SmallVec` was actually faster for my use case of `Vec`s of size 2-4. However, if I were to rewrite it, I'd just use a slice of length of 4 and just have dummy values for the ones that don't exist. The thing is, I have to precalculate whether those values are there anyway and store the result in an `Arc` to share it between threads. For some reason it's faster than doing it every time (I thought memory access was slow, but it's faster than doing the same calculation over and over)
Well, &gt; As of 2018, the only major language with compile-time memory management is Rust, which is notoriously tough to learn. And then I see: (letregion rho (let ((p (allocate rho 10))) (letregion rho' (let ((p' (allocate rho' 12))) nil)))) Is Rust really THAT hard to learn? ¯\\\_(ツ)_/¯
Views are usually not updatable. Also the promise of ORM is that you don't deal with DB on such a low level. If you do \-\- you now have to be an expert in DB and in how your ORM works \(i.e. what are its limitations\). That kind of defeats the purpose for me.
This buildpack simply downloads a fresh copy of rust (via rustup) and builds your project with the `--release` flag. Once compilation has completed it moves the result from `./target/release/[app name]` to `./RUSTAPP`. Upon release it simply executes `./RUSTAPP`.
Any time!
author here :-P Thanks for your support! * part of the code samples support SGX_MODE=SW such as crypto. I forgot to support SW mode in recent code samples :-( I'll update them soon! * Xargo is our destination -- the sgx runtime should behave like a sysroot and Xargo should be the best. However, xargo does not support rust stable to build sysroot and it's too much unstable now. We can't 100% rely on it :( So we keep cargo and xargo at the same time. * Good idea! I think everyone can do it :) So for now I'm not packing them together. But it's not the first priority. * Yes it is possible. I think only a few tweaks are needed :)
One optimization you can do for the "append" \(or "push\_back"\) is to make the memory write unconditional. So basically, you structure things so that there's \*always\* space for at least one more element in the vector. That way an "append" is an unconditional \(non\-speculated\) write, followed by a check to make sure that there's still space for one more element. But this last check doesn't have any dependencies so it can be basically free in the common case. I've seen \~20&amp;#37; improvements to append in C\+\+, using this trick. This does mean two things 1\) You need to make sure an empty vector has somewhere to write the first element \(unconditionally\) 2\) The vector will grow one element "early" as you fill it up \(so when you reserve a capacity, you should probably reserve an extra element\). For 1\) you can either allocate memory on the heap for empty vectors, but that kinda sucks if nobody ever writes anything to it. So it's better to store at least 1 element "inline" in the struct. Which is exactly what SmallVec already does, so it's pretty well positioned to take advantage of this trick. 
Rewinding the end of the buffer to the nearest codepoint boundary before decoding isn't hard in UTF-8. I agree with being pessimistic and being ready for huge inputs without LFs, or even huge binary inputs with lots of invalid UTF-8.
That's one of the hottest parts of the code, will give that a shot too. Thanks!
\&gt; It's much easier to explain to a new user of Rust coming from say Java, that impl Trait in a function signature is like using an interface type It's worth pointing out that java also has something similar to rust's standard generic syntax, so it should be pretty familiar to people coming from java. It's also worth pointing out that not everybody is familiar with java.
Aside from the fact that this is wrong, which the other people already explained, I think this is a good example of how \`impl Trait\` hurts learnability by introducing misunderstandings.
&gt; plans to add try, catch, and throw The `try` block _concept_ was accepted, along with `?`, in [RFC 243](https://github.com/rust-lang/rfcs/pull/243) back in Feb 2016. The unresolved question of the exact keyword to use was settled by [RFC 2388](https://github.com/rust-lang/rfcs/pull/2388) earlier this month. I know of no concrete plans for `catch`, other than that `try{}` won't be one of the constructs (like `loop{}`) that doesn't need a `;`, so it could potentially be added in future. Most of the discussion I saw is that `match try` is good enough for the foreseeable future, and more information is needed about pain points before even trying to design such a feature to fit well in Rust. `throw` was [raised for 2018, but proved too controversial](https://github.com/rust-lang/rfcs/pull/2426#issuecomment-390503383), so the current disposition is to [reserve `throw` and `fail`](https://github.com/rust-lang/rfcs/pull/2441) so one of them _could_ be added before the next edition [_if_ opinion coalesces over time](https://github.com/rust-lang/rfcs/pull/2441#issuecomment-389450130) that one of them of them is a good fit.
I completely agree. Sure, there was an RFC for a long time that people could have looked at. The problem is that people didn't look at it, so clearly something is wrong. It's definitely a problem if people are not aware that a controversial feature is being added. Instead of blaming the individuals that didn't read the RFCs, we should blame the system and talk about what we could do to change it.
That's not necessarily true, you can have single-cycle operations with different throughputs, and macro-op fusion can elide certain instructions entirely in specific scenarios. [According to WikiChip](https://en.wikichip.org/wiki/macro-operation_fusion), `xor` wouldn't be able to fuse, but the rest would.
as a quick fix, just edit the build.rs of the untrusted app. Replace `println!("cargo:rustc-link-lib=dylib=sgx_urts");` with `println!("cargo:rustc-link-lib=dylib=sgx_urts_sim");` and you'll be fine with `SGX_MODE=SW make`!
Can you implement `Display` manually for the wrapper type? If the `derive(Fail)` forcibly implements `Display` no matter what, you may need to implement `Fail` by hand as well.
If bare `Trait` meant `impl Trait` then `Box&lt;Trait&gt;` would be `Box&lt;impl Trait&gt;` and therefore known to have a single implementation. That seems pretty consistent to me when `Box&lt;Struct&gt;` is also a single implementation. You would need `Box&lt;dyn Trait&gt;` to have multiple implementations. It's true that you wouldn't be able to tell given `Box&lt;T&gt;` if `T` is a trait or a struct, but both are types, and the only effective difference I can think of offhand is whether you get direct field access. Even that would go away if we ever got fields-in-traits. I suppose you wouldn't be able to use other traits unless you knew it was a struct, but that seems like a problem today too with `Box&lt;Trait&gt;` meaning `Box&lt;dyn Trait&gt;`. I guess I'm falling on the side of "bare `Trait` meaning `impl Trait` is ultimately more consistent".
Can you send me the techempower link you are referencing? Yeah, with that page, they have done some weird things with the Sanic framework in Python where they used to disable the access log and debug flags in the main run\(\), but they were removed recently. Once they did that, Sanic tanked in speed.
Yeah, the way the standard library's [`io::Error`](https://doc.rust-lang.org/std/io/struct.Error.html) works is great for library users. They can `unwrap` it if they don't care at all, wrap it with `failure` if they only care a little bit, or match on the variants if they need to handle it properly.
There are some guys in the University of Michigan who implemented a faster API for SGX called [HotCalls](https://github.com/oweisse/hot-calls) it's pretty cool!
It would probably look a lot like this without lifetime elision!
&gt;Is Rust really THAT hard to learn? ¯\_(ツ)_/¯ You have to separate the difficulty of learning Lisp (which is a big hurdle for lots) from the difficulty of learning the language's model of regions and pointers, which, if I say so myself (as the author), is not that great. Let me translate this example to a C-like pseudocode, with explicit types and comments: // define the region rho letregion rho { // allocate size(int) in rho, store the value 10 // in the memory if it succeeds, and return a potentially-null // pointer to the memory. Nullable&lt;int, rho&gt; p = allocate(10, rho); // Define the region rho' letregion rho { // As above, but with the region rho' and the value 12 Nullable&lt;int, rho'&gt; p' = allocate(12, rho'); // Finally, nil. The value of the whole expression is nil nil } } This code is a bit convoluted, I think, because we're not really doing anything (just allocating memory and returning nil). But it's a preamble to this other code further down that demonstrates a program that doesn't typecheck do to a memory safety violation. The program, translated, is: // define the region rho letregion rho { // allocate size(int) in rho, store the value 10 // in the memory if it succeeds, and return a potentially-null // pointer to the memory. Nullable&lt;int, rho&gt; p = allocate(10, rho); // Define the region rho' letregion rho { // As above, but with the region rho' and the value 12 Nullable&lt;int, rho'&gt; p' = allocate(12, rho'); // This fails due to a type error: // the type of the variable p is Nullable&lt;int, rho&gt;, // but the type of the variable p' is Nullable&lt;int, rho'&gt; // they differ in that they are pointers to different regions. p &lt;- p' } } There is also a more complete example of using regions further down, which involves creating a region, allocating a value, testing whether or not it's null (ie, whether the allocation succeeded), and dereferencing the pointer to print the value. The example, in Interim, is: (defun main () i32 (letregion rho (let ((p (allocate rho 10))) (case p ((not-null p') (print "Allocated successfully! Value: ") (println (load p')) 0) (null (println "Out of memory!") -1))))) Translated to the C-like pseudocode, this would be: int main() { letregion rho { Nullable&lt;int, rho&gt; p = allocate(10, rho); case p of { // Case where it's a never-null pointer p : Pointer&lt;int, rho&gt; { printf("Allocated successfully! Value = %i\n", *p); return 0; } // Case where it's null _ { printf("Out of memory!\n"); return -1; } } } } I hope this helps understand the (admittedly sui-generis :) ) code in the README.
Round 15 https://www.techempower.com/benchmarks/#section=data-r15&amp;hw=ph&amp;test=plaintext And this is continuous testing https://tfb-status.techempower.com
Very likely you will never get exactly 0.0f64. Not only is it spectacularly improbable, but the underlying generator might need to be k-dimensional equidistributed (for some k, maybe k=2 for 32-bit generators) for it to happen. There might be many values that missing from the output, mostly on the interior of the interval and I don't see why you should miss 1 any more than any others. When you sample an image, each pixel contributes to a range of sample points, of size 1/(number of pixels). Even if many points are missing from the generator's output, as long as the output fall uniformly into these buckets you aren't going to miss any pixels. You would need to get 1/(number of pixels) very close to the resolution of the generator before you start missing pixels, and you'd probably lose some on the interior too at that point. ------ I tried to extract the function that does the sampling // rand is the number that comes out of the RNG fn f(lo: f64, hi: f64, rand: u64) -&gt; f64 { let scale = hi - lo; let off = lo - scale; let v = f64::from_bits((rand &gt;&gt; 12) | (1023 &lt;&lt; 52)); v * scale + off } It looks like `f(0.0, 1.0 + f64::EPSILON, !0)` is still &lt; 1, but `f(0.0, 1.0 + 2.0*f64::EPSILON, !0)` is 1.0. Whether its inclusive or exclusive depends on the bound (eg those were exclusive, but my example from earlier `f(3.0, 4.0, !0)` is inclusive).
It's a tricky problem! Definitely one worth pondering and discussing though * Reddit only allows two pinned posts, which are used for the weekly questions and what-are-you-working-on. Both are awesome, but if I had to replace one with "rust rfc's" or such then I'd say to keep the questions one, even though I use it a lot less. * There's many rust sub-communities divided by topic, but also by communication medium. Reddit, IRC and GitHub are the big ones I participate in, but there's also the official forum, a Discord, Gitter(?), and probably others. Nobody (apart from possibly steveklabnik) can participate in them all. This is natural but also you end up with artificial divisions... Even in just the gamedev topical-subcommunity, ggez people hang out on IRC and sometimes Gitter, Piston people hang out on the forums, GitHub and sometimes Reddit(?), gfx and amethyst people mostly do Gitter, and there isn't enough cross-pollination. * Rust rfc's are big and long, often wide-reaching, and have months or years of discussion behind them. This makes it hard to keep track of the current state and hard to catch up to the current state unless you follow RFCs in particular, especially since a lot of them are, by nature, not things any specific person is going to be interested in... We all have different interests. So I skim over the RFCs section in TWIR because I'm not going to try to catch up with any of them from zero, except the most important ones. * A little of my own experience... Finding the current state of Rust compilers, tooling and changes is actually pretty hard. There's *lots* of information on working groups, automatic testing, rfc's and such, but I don't know how to find *any* of it from rust-lang.org, and I've tried. You have to ask on IRC. Some better indexes would be nice. This is just my view, n'at, and some of it is actionable and some isn't. Two things come to mind: A nicer entry point to finding out what's *currently* going on with Rust would be nice, with major active RFCs, compiler stats, quickintro docs to the rust dev process, recent reports from tools like homu and crater, etc... but that is hard to keep up to date. Perhaps the final comment period of an RFC should be more widely announced, longer (I don't feel that the development pace of rust is too slow at this point, could stand to be a bit more languid I'm), and include a final summary for those just getting into the discussion recently. That said... This is all engineering, engineering of people and processes along with software and formal systems. It's damn impressive that we've all gone from "what is that?" to random people asking me "oh I've heard of that, how is it?" in just the two years(?) since stabilization, and so much of that is due to awesome community and powerful, flexible ways of engineering that community. Occasional kerfuffles like this are not a failure, of Rust or it's people or it's processes, they're just opportunities for refinement. :-D
Good points for that example. Though I do want to reiterate that it was just one off the top of my head. I think my larger points about knowing what's possible and having naming reflect that still stand. But perhaps it's just navel gazing now.
Since people are talking about the two pinned post limit..could this be done with a subreddit CSS that adds this week in rust in a link overlay? I vaguely remember the Bernie Sanders subreddit to do the same
Just to address a few questions that have come up in other mediums: - No, this release didn't touch [`no_arg_sql_function!`](http://docs.diesel.rs/diesel/macro.no_arg_sql_function.html). I want to unify it with `sql_function!` and deprecate it, but didn't get around to it this release. That will probably happen in 1.4. - Saying that 1.3 will be the last release of `diesel_infer_schema` is not violating semver. It only interacts with `diesel` through public API. If `diesel_infer_schema` 1.3 breaks, it's because we've made a breaking change in `diesel`, which we won't be doing until at least 2.0. Realistically it'll keep working much longer than that. It hasn't had a single line of code changed in a long time. - This release also includes experimental support for [`barrel`](https://github.com/spacekookie/barrel) which unfortunately didn't get included in the release notes due to some confusion about whether that had happened in time for 1.2 or not. Happy to answer any other questions folks might have.
I would use yew or PureScript (because I don't like weak/dynamic typing). The advantage with yew is you can re-use all your Rust types between backend and frontend. The disadvantage is that IE11's engine doesn't support wasm, only asm.js which is slower. With PureScript you could use [this crate](https://github.com/tomhoule/purescript-waterslide-rs) to generate PureScript types from your Rust types, which is what I'm using at my job because we have to support IE11 for our customers.. I think with web-view the RAM usage will be lower than electron, it surely is much lower on windows where it uses IE instead of webkit. AFAIK Electron contains a lot of additional runtime JS code that allocates memory in addition to webkit itself. Btw, if you want to have wasm on Windows, you can use [tether](https://crates.io/crates/tether) which is like web-view but uses Edge instead of IE11's engine on windows. But it requires windows users to have Edge installed (which some (e.g. I) haven't). I think in the future we might have a crate that uses both to support Edge with IE as fallback..
The runtime checks I was concerned about were the RefCell ones, which can cause panics if you're not careful, when the borrow checker would've caught those mistakes at compile time.
So I ripped out code from Chars and made this: https://play.rust-lang.org/?gist=faccf8569c3965168e2a23dab6ab8227&amp;version=stable&amp;mode=debug You should be able to replace `foo.iter()` with `io::stdin().bytes()` and have it work off stdin. If it encounters non-UTF-8 the iterator will just stop; no error will be thrown. I'm not entirely sure what its behavior is if there is an error reading from stdin. Anyway, maybe that gives you some framework to build on and get whatever error handling you'd like. The secret sauce is that utf8_char_width function which I copied out of the standard library (you'll want appropriate attribution and licensing on it).
Without even having read the linked page, this seems pretty straightforward to me, so …
I feel like the problem is not actually the RFC process, but much rather that stabilization happen fairly invisibly \(someone does a PR and someone approves it\). I believe we should have an RFC like process that discusses which things are ready for each new stable version of Rust.
That's still defeating the point of the benchmark: it only does O(n) function calls, not an exponential number.
Would really love one for on my shelf, but 60+ dollars is just too much (for eu)
Perhaps there should've been a `impl&lt;T: Copy&gt; Clone for T`
Could you say more about why you think using `ManuallyDrop` is better than modifying the returned `ArrayVec` in place? The latter approach has a (somewhat small, at this point) chance of having the final copy into the return value elided, by just initializing/mutating the space in the return pointer directly.
No, and I don't know if it's LLVM problem: more investigation is required (and unfortunately I won't be doing it).
Thank you, but I don't need the money. Please donate it to a charity instead. :)
Do you have a charity of your choice?
/r/spacex solved this problem via clever CSS, we could do something similar here.
Hm- maybe we could get one if we get sophisticated enough specialization rules! Right now such an implementation would conflict with properly implementing both `Copy` and `Clone` on generic structures. Specifically, we want to allow people to do: struct Something&lt;T&gt;(T); impl&lt;T: Copy&gt; Copy for Something&lt;T&gt; { } impl&lt;T: Clone&gt; Clone for Something&lt;T&gt; { ... } If we have an `impl&lt;T: Copy&gt; Clone for T`, this causes a conflict if `T` is both `Copy` and `Clone`, as there are two possible implementations that could be used for `Something&lt;T&gt; as Clone`! See this example in the playground: https://play.rust-lang.org/?gist=83665e679d45cf176b6d361ed4bec302&amp;version=stable
Since you said EU, I'm assuming this is with shipping? Is it cheaper on Amazon, maybe?
Isn't this just exactly what mem::forget does
Tracking issues for stabilization use a similar consensus mechanism (rfcbot) to ensure there's consensus among the subteam that a given feature is ready to be stabilized.
It's worth clarifying: every approved RFC results in a formal "tracking issue" where we discuss implementation issues, usage feedback, **and formally propose stabilization**. Just like with RFCs, there's a full team sign-off and "final comment period" on these tracking issues. You can see the [full list here](https://github.com/rust-lang/rust/issues?q=is%3Aopen+is%3Aissue+label%3AC-tracking-issue), but also we post a link to the tracking issue each time an RFC is merged. The easiest way to stay up with a given feature's progression post-RFC is to subscribe to its tracking issue.
Since the arguments to `gen_range` are constant, all the range initialization should be done at compile time. I doubt explicitly using `Distribution` will make a difference.
`std::mem::forget()` consumes its argument and does not return it, where as `ManuallyDrop` can still be used via the `Deref` impl.
NVM looks like a need a linker
Not sure how to send a PR for release notes, there is a typo: "supoprts" in the `sql_function!` paragraph.
Wicker or Wicked? I would also like to see at least one benchmark against Actix.
Why not use a phantom type parameter? enum A { } enum B { } struct C&lt;T&gt; { …, // contents of old A and B marker: std::marker::PhantomData&lt;T&gt;, } impl C&lt;A&gt; { … // methods for just A } impl C&lt;B&gt; { … // methods for just B } impl&lt;T&gt; C&lt;T&gt; { … // methods for both } It's not essential that `A` and `B` be empty enumerations, but it helps to emphasize that they aren't used for their values. You can also do interesting things by applying traits to the parameter.
Ha! wow... great typo by me eh? At the moment, it's not as performant as Actix, but it comes close! There are still some performance gains to be had, and I'd like to shore up my process before I publish benchmarks :) Actix is a great framework, this is just a different way of going about the web server problem.
Can't spell, so I deleted the old one here: https://www.reddit.com/r/rust/comments/8lepo4/wicker_fast_web_servers_in_rust/ in favor of this new one! Sorry about that!
There are a lot of reasons that one version of program could benchmark differently than another. Are you sure you're actually doing equivalent things? I recently wrote the same program in C (not C++) and Rust, and the C version was twice as fast until I switched from using the `rand` crate to libc rand(3) in the Rust version. (Gross.)
For a SmallVec, this approach means there's a `A::Item` stored inline that's never used, which seems moderately expensive for optimizing for small-vectors with short lengths, which I imagine is when they're best. (The element is written to the last slot of the on-stack array, and then the whole thing is immediately copied onto the heap as it grows to make space.)
That's definitely an aggressive first project. I've taught compilers using that book, and I teach Rust, but I've never taught them together. Which immutable data structures do you have in mind? I remember using quite-mutable doubly-linked linked lists for register allocation, but that was pretty late in the game. There are [some crates with immutable data structures](https://crates.io/search?q=immutable), though this could also be a chance to learn about making some of your own. Or figure out how to use other data structures—that could be interesting too. I don't know what best practices are for ASTs in Rust, actually, but you could make factories of some kind to ease the burden. Or you could try allocating in an arena, maybe?
Which code in this is actually yours? It looks like there's a lot of incomprehensible (to me) boilerplate. Or: Do I need to understand AWS to understand the part of this you want feedback on?
Cool. Reminds me of the old BeOS filesystem, which was also cool.
Continuing to clean up some crates. Wrote more documentation for [weak-table](https://crates.io/crates/weak-table) and found a bug as a result. Figured out oldest supported versions for that and [bf](https://crates.io/crates/bf).
Yes, you need to be able to justify one extra elem. But if you have a lot of traffic (e.g. a small stack) the savings on append aren't nothing. 
Unfortunately, I found diesel fall far short of what I need and so have adopted a lower level library
It has everything I wish go had: Generics, Option, Result types and no null values! It is like a low level Scala without all the JVM baggage. A more practical Haskell. Cargo is also a blessing. 
I think you can use diesel as just a raw query engine and parameter binder actually. 
Extract the mantissa and exponent of `low` and `high`, `m_l`, `e_l`, `m_h`, `e_h`. Here the mantissa are signed and include the implicit leading bit. If one bound is exclusive, increment or decrement its mantissa at this point. Let `e = max(e_l, e_h) + 12`. Shift `m_l` and `m_h` so it is as if they had an exponent of `e`. Perform rounding on truncated bits manually. Generate an integer in the inclusive range from `m_l` to `m_h`. Truncate it to 53 bits. The truncation will always be in bounds because the endpoints are truncated to 53 bits. Convert this to a floating point and multiply by 2^(e-64).
I can but I won't. I use a library that is meant to do that rather than a library that includes it as a second rate feature
Thanks for letting me know
I'd love to hear more about how we fell short so I can improve on it. Even if you choose not to use our query builder, I do still think that Diesel has the most ergonomic story for raw SQL + parameter bindings. Did you look into [`sql_query`](http://docs.diesel.rs/diesel/fn.sql_query.html) at all? Even if that's the only piece of Diesel you use, I've found it much easier to use than the alternatives
To update all the helpful folks here (/u/vks_, /u/SodaDowel, /u/pitdicker) I have tried your suggestions, but I'm still seeing a 40-50% perf hit. The main culprit does seem to be the creation of hundreds of thousands of SmallRngs. Using from_entropy is worse than using from_rng(thread_rng), that's what brings it down from a 50% hit to a 40% hit. Using a single rng and implementing a Fight::with_rng initializer showed no difference between just calling thread_rng every time. Additionally, if I switch to SmallRng::from_seed with a hardcoded seed, the perf difference shrinks to about 25% slower when compared to using XorShiftRng::from_seed on v0.4.2. Given these data points it seems there has been some small perf regression in the creation of new XorShiftRngs, and it took me creating hundreds of thousands of them to even notice. Thoughts?
Diesel spent a decent amount of time its query performance, and when it first came out, one of the main draws was that it was more performing than rust-postgres. That, and the fact that it has very ergonomic data fetching and deserialization is why I would choose it. I say this as someone who dislikes ORMs and prefers raw queries too, which is why diesel is a good compromise. It allows you to write those queries with type checking without getting in the way too much. But the ability to do it raw is still there. I suggest you take a second look. 
It is [very strongly suggested](https://doc.rust-lang.org/std/clone/trait.Clone.html#how-can-i-implement-clone) that you implement `Clone` such that this is true. 
One thing among others is that having functionality in a crate instead of std isn't really considered a drawback by most rust folks. 
An active RFC summary could maybe be stickied at the internals forum? I don't think /r/rust is the place for that sort of thing. Maybe buried in the sidebar somewhere, but not on the front page...
Yeah, but for new people coming from another languages, find that not having an easy way to just get input is ridiculous this days, depending on a extern crate isn't an drawback but that funcionality should be ready in the std for everyone, here I have some points why we need it built-in: - No internet for download `text_io` or any other crate. - Overkill using extern crates just for some basic functionality. - New rustaceans may find this not attractive and very unintuitive.
I don't disagree, but I also am not a core member of the rust community.
It appears to have [made the rust parts of rustc faster, at the cost of more time spent optimizing](http://perf.rust-lang.org/compare.html?start=8319ef5b78a10b3a8de4109bb8b0e6d23fbe4de1&amp;end=bdace29de04af4fe9e4317b73c3f7d6418a33de1&amp;stat=instructions%3Au), so I look forward to seeing what lolbench has to say about performance on a broader set of workloads. The paper says "we conclude that having the programmer specify non\-aliasing is a bad idea" in reference to the \`restrict\` keyword in C, which I don't think is germane to rust, since in C it can't be pervasive as it's unchecked. But it's everywhere in Rust, so it can plausibly even help with things like iterating two iterators at once.
One issue I ran into was that using associations with multiple relations to the same type, something like struct Person { name: String, father: Option&lt;Person&gt;, mother: Option&lt;Person&gt;, } could not be expressed with diesel.
I've long been a believer in the importance of This Week In Rust for improving project visibility for the broader community, so I'm happy to try pinning it so that it gets more visibility. Since we're only allowed two pinned posts, it'll take some coordination. The "what are you working on this week thread" gets stale faster than the easy questions thread, so we'll try a schedule where we replace the former after a few days with a pinned TWiR. /u/llogiq is in charge of the current pinned threads, but I don't want to foist any more burden on him. /u/nasa42, I'll make you a mod of /r/rust if you'd like to take responsibility for unsticking the working-on-this-week thread and stickying the TWiR thread every... let's say... Thursday (you're free to continue publishing the thread itself on whatever day you like, looks like you usually publish it on Tuesday/Wednesday). This'll give the other thread (posted Mondays) three days of visibility. Would you like to accept this task?
How does this compare to [Henry Baker's Lively Linear Lisp](http://home.pipeline.com/~hbaker1/LinearLisp.html) ?
Hm... This is an interesting case. We definitely generally support this sort of thing, but I'm not sure what you are looking for us to be able to do here. If there are multiple associations of this kind, `Person::belonging_to(&amp;person)` would be ambiguous to what you're trying to express. There's definitely nothing stopping you from an explicit `people.filter(mother_id.eq(mother.id))`.
&gt; `a = input()` As someone with 15+ years of Python experience, let me say that, for the entire 2.x cycle, `input()` was a **massive** footgun for any novice: &gt;&gt;&gt; foo = 1 &gt;&gt;&gt; bar = input() foo &gt;&gt;&gt; print(bar) 1 &gt;&gt;&gt; baz = input() quux Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;string&gt;", line 1, in &lt;module&gt; NameError: name 'quux' is not defined The actual equivalent to that C++ code would be more like this: # Python 2.x a = int(raw_input()) # Python 3.x a = int(input()) As for the Rust code, here's what you're asking about: use std::io::stdin; fn main() { let mut input = String::new(); stdin().read_line(&amp;mut input); // Remove the trailing newline and parse as a u32 let value: u32 = input.trim().parse().unwrap(); println!("Got value: {}", value); } Note that: 1. You didn't put a big try/except in your Python code, so exclude the `.expect(...)` from your `read_line` call. " unused `std::result::Result`" is just warning, not an error. 2. You probably got tripped up by the fact that `read_line` includes the newline character in the output. That's what `.trim()` or `.trim_right()` is for. (And, yes, it will recognize anything Unicode defines as whitespace.) 3. Thanks to the `.parse()` method, Rust's approach to parsing strings into numbers is actually *easier* than in a language like Java... you just need to provide *some* hint as to what data type you get out. (I used a type annotation on the destination variable. 4. Since you're not catching any exceptions in your Python or C++ examples, use `.unwrap()` so the Rust program will also "blow up safely" if the user doesn't type a number.
`from_seed(thread_rng.gen())` will do different things on rand 0.5 vs 0.4.2. 0.5 changed `SeedableRng` to accept only arrays of bytes. So on 0.4.2 a `[u32; 4]` is generated while on 0.5 a `[u8; 16]`is generated. For `[u8; 16]`, rand calls `next_u32` for each byte which will slow down the seed generation by quite a lot. (I think there's some consideration about improving `gen::&lt;array&gt;` with `Rng::fill`)
The `#[belongs_to]` would simply halt compilation because the trait was implemented twice on the same type. I should probably have wrapped the Person inside a struct enum like `Father(Person)` in hindsight. But in general, everytime a struct contains more than one field which is another table, belongs_to is a problem.
If I understand the idea well, this will not hit `low` exactly either. And if `low` was rounded to nearest during the right shift, it might even generate values outside the range. That being said, if used with rounding towards `high` it may be the most sensible approach indeed.
? &gt; &gt; However, even though we used the best possible restrict annotations, we found an average program speedup of less than 1% on average when using two state-of-the art optimizing compilers that implement the restrict pragma [...] &gt; In this paper we have shown that for large C programs, the impact of using restrict annotations is usually limited. While those annotations can dramatically improve performance of small kernels, larger applications do not signifi- cantly benefit from even optimistic restrict annotations. Since the potential benefits are meager but the potential for introducing errors high, we conclude that programmer-specified non-aliasing is a bad idea in general for improving program performance. I found the full context in the conclusion to be a lot clearer. They only discourage it because the results don't outweigh the risks in C - not an issue in rust. 
It's possible that compiling in release mode with LTO is doing enough optimization here to equalize those, because I can't see a noticeable perf difference if I switch between from_seed or from_rng on 0.5.
Agree but disagree, another issue with this approach is that you may lose data when reading the entire line when you just wanted to read a single word, e.g. In C++ input gets stored and everytime you ask for it with the `operator&gt;&gt;` with `cin`, it pop out the first word until blankspace and parse it to the type of the variable right to the operator, pretty, easy and intuitive. You don't lose input from user if he entered more than you were just expecting because you will use it later if you needed to. Reading line in Rust will lead to losing data if the user preferred to enter it all in one line, unless you use a vector, and collect data and manually check if there is more data than expected, see the point? It's just to much to do for a simple input, that's what I'm asking for, something like `text_io` but officially supported by Rust, ready to use at anytime, without overkill for using an external crate, needing to download more sources, etc. There are many languages that maybe also has this problems, but that doesn't mean that Rust doesn't need to fix that, just because the others didn't fix it yet. 
I was trying to split the difference between the Python and C++ examples you gave. If you're complaining about not everything having a C++-style input scanner in the standard library, you *really* don't have my sympathy, because very few do. I might as well complain that C++ doesn't have Python's `argparse` for parsing `argv` and generating `--help` output built into the standard library.
I was just complaining about something needed, not in C++ style necessarily, it's that I consider it a good example and very good implementation on the logic for receiving input, obviously Rust can do that in some many ways, differing in the C++ style, as an example `text_io` does it very well, letting you to even "format" your input, read input with one line of code, that's what I mean, just including that functionality in the std would be such a good thing in my opinion. 
If Vikrant doesn't show up, I'll do it.
I tried throwing a Uniform in a static as a hack to see if this changed anything, and the perf remains the same.
I was just complaining about something needed, not in C++ style necessarily, it's that I consider it a good example and very good implementation on the logic for receiving input, obviously Rust can do that in some many ways, differing in the C++ style, as an example `text_io` does it very well, letting you to even "format" your input, read input with one line of code, that's what I mean, just including that functionality in the std would be such a good thing in my opinion. Edit: As a side note, that will make so many new programmers that want to start learning Rust more happy and interested, who didn't learnt a language making a bunch of programs that consist in receiving some input from your console and resolve a certain problem based on that data? Now imagine you as a new Rust programmer discover that you need to include an external crate , and need to write like 3 or 5 lines of code extra that doesn't make any sense to you as a new Rust programmer just to receive a single number from `stdin` isn't that intuitive as you may expect. 
As a side note, that will make so many new programmers that want to start learning Rust more happy and interested, who didn't learnt a language making a bunch of programs that consist in receiving some input from your console and resolve a certain problem based on that data? Now imagine you as a new Rust programmer discover that you need to include an external crate , and need to write like 3 or 5 lines of code extra that doesn't make any sense to you as a new Rust programmer just to receive a single number from `stdin`, that isn't intuitive as you may expect. And I understand other crates being outside the std, they are in some way more *optional*, but receiving input as explained above should be some kind of *needed* functionality. 
&gt; Edit: As a side note, that will make so many new programmers that want to start learning Rust more happy and interested, who didn't learnt a language making a bunch of programs that consist in receiving some input from your console and resolve a certain problem based on that data? Now imagine you as a new Rust programmer discover that you need to include an external crate , and need to write like 3 or 5 lines of code extra that doesn't make any sense to you as a new Rust programmer just to receive a single number from stdin, that isn't intuitive as you may expect. Python is much better suited to being a learner's first language than Rust, given the rate at which you need to internalize new concepts to make progress, and I'm not even sure if Python has such a facility in its standard library. (If it does, I've neither needed it enough to search it up, nor found it during curious wanders through the module index.) Either way, when I was a novice, that kind of advanced input scanner facility is the kind of thing that I'm not sure I'd have been experienced enough to grasp why it was so desirable, let alone think to want it without having it brought to my attention by someone else.
Apologies if this is generally discoverable by googling, but I couldn't figure out today if I could use diesel to talk to mssql (SQL server), nor could I figure out if I could just include a different connector library, such as the rust SQL server one that is up on cargo. Any suggestions for using diesel with SQL server? Thanks for working on this project and trying hard to build an excellent open source library that the rest of the community can rely on.
Shame. I doubt though that LLVM could somehow optimize a larger number to compare random output to.
I guess you could think of stable Rust as a pyramid-like process: The lower layers are never changed, only compatible stuff is built on top in future stable releases, and as Rust gets refined more and more, the additions become smaller over time.
I used a lot while learning some language, trying to make a tiny system about anything you can imagine, and you won't need to use an UI for that, a simple program receiving input was enough. Anyways just want to share a suggestion why Rust may need a more intuitive way of receiving input :p
It might merely be the switch of `thread_rng` from Isaac to HC 128, but that's harder to test.
Here are a few things I often write which don't touch stdin... let alone interactively: * Batch scripts which receive all of their input via `argv` and by accessing the filesystem (and, even if they support a `-` input path, they manipulate the input in a way which your proposed scanner would have to be bypassed to allow.) * Web applications which receive all of their input via either a configuration file (using a parser which sits directly on top of the APIs in `std` by *choice*) or an HTTP request I'd argue that what you propose is also quite optional.
Thanks for such a thorough explanation. Yet, I still think that it is unfair to separate the complexity of Lisp and pretend that the proposed solution is much easier than Rust (as a whole language).
&gt; I used a lot while learning some language, trying to make a tiny system about anything you can imagine, and you won't need to use an UI for that, a simple program receiving input was enough. Trust me, I write a *lot* of programs like that. When I was a novice, I wrote them using `INPUT` (QBasic) or `raw_input()` (Python) or what have you, and, even when I got into C++, I was inexperienced enough that I saw the scanner's behaviour as a bug to be worked around by finding ways to flush the input stream before printing the prompt. (Because, otherwise, if someone typed "1 2 3 4 5", I'd get a flood of "Please type another number" because I didn't know any way to peek at the input queue to omit the prompts.) When I started to become more skilled, I either used `optparse` (Python's precursor to `argparse`) so I could incorporate my creations into batch files or shell scripts, or I used a TUI library like [urwid](http://urwid.org/) because I felt that, if I was going to go interactive, raw terminal input was ugly and an inferior input modality. &gt; Anyways just want to share a suggestion why Rust may need a more intuitive way of receiving input :p And, as I mentioned, the bar for getting into the standard library is *very* high and I don't think that clears it. That said, we did recently get the `std::fs::read_to_string` convenience function, so something equivalent to what, in Python 2.7, is `a = raw_input()`, may eventually get added.
There is a neat little site called [rusty-dash](http://rusty-dash.com/), which shows the RFCs which are nearing a decision, and it shows their disposition. I think it would be helpful if something like this was integrated into, and made visible on the main Rust website.
Thanks for explaining. Now I see the difference.
I've misunderstood this feature, but maybe newcomers will find it easier to understand?
https://github.com/wu-lang/wu I'm working on a programming language transpiler for what is to be a typed, Rusty alternative to Lua to use with things like Love2D and Torch or just general embedding. So far everything is coming along very nicely.
But that still wouldn't account for the difference in perf when using a hardcoded seed right? In that case we're not even touching another prng. Or do you think that difference would be explained by the different seed type?
* Accounting for the users internet connection isn't a programming languages responsibility. * Saying it's basic functionality is a bit cheap when the functionality you want is entirely dependent on what you're doing. * The implementation you're suggesting would require that the `input()` function your asking for allocate an entirely new string each call which is wasteful vs the `read_line()` implementation which is "standard". * I can agree that this implementation may not be intuitive to a new rustacean but as I described in my previous point, there are some important lessons to be taken away from the Rust implementation.
With `std::fs::read` we have precedent for small convenience functions for I/O. Maybe we could add a function like this as well? ```rust pub fn read_stdin_line() -&gt; Result&lt;String, io::Error&gt; { let mut s = String::new(); stdin().read_line(&amp;mut s)?; Ok(s) } ``` Then to get `i32` or some other type you could combine it with the existing `str::parse` method: ```rust let a: i32 = read_stdin_line()?.parse()?; ``` I’m not going to work on this today, but feel free to file an issue or a PR.
This is an unpopular opinion, but if the stream of features slows down after 2018, I'd welcome it. When Rust was young (the last three years), it's understandable to introduce large features every six weeks. But after 2018, when async / await and SIMD are done, I don't see that many features that absolutely need to be expressed in the language. It's OK to slow down a bit with new features and focus more on stability, more platform support and compiler optimization (both in compile speed and output) - because most critical / quality-of-life features (that were missing from 1.0) are done by now (or until the end of this year) and I wouldn't expect large, incompatible changes in the language. Of course, nobody can see into the future, but I'll just say that it's unlikely that the Rust syntax changes very dramatically. The problem up until now was not that old Rust code didn't compile anymore - it was that the "best practices" constantly changed. That's what keeps some people from learning Rust - because how do you answer a stackoverflow question "What is the best way to do X?" if the answer will change within a month? C is on the other extreme here - answers from 20 years ago can still be relevant today, but the price for that is that you still have the same problems as 20 years ago. I mostly see the coming changes in libraries, less in the language. Ex. `NonZero` / `Pin`, etc. being stabilized, new libraries popping up and being recommended. So that's more what I'd be looking forward to.
I'm finishing a rewrite of a git commit hash miner from C to Rust. I wrote the original on OSX, and while it works there it is deeply wrong somehow on Linux. I couldn't be bothered to valgrind or rr to find the memory access bugs in there, so I ported it to Rust. It's my first Rust project, and I'm very impressed with the language. I was able to get a simple enough program working quickly, use iterators and pattern matching and other nice things, and trivially multithread it. Next up is reading more of the book, I think.
This is really exciting! Congrats to all the `rand` contributors who've been working towards this over the last year!
Being able to look at the real code always helps, thank you 😀. I made a couple of changes that together make it about 3× faster on my PC. The important ones are in `run_half_tick`. it was now doing three divisions, something quite costly. Using `Uniform::new` does two of them at compile-time. I am not sure changing `is_attacking` to use a mask instead of a modulus is ok, but I am sure you can turn that into something correct. Initialization time seemed not to matter in the end. I moved it out of the nested loops, but with little effect. https://github.com/pitdicker/FighterSimulator/commit/875e2a7798458d0e484762e0be3a5c9aabd22fba
`DangerousOption` crate. `take` manually in `drop`. I'm on mobile. 
Ah, now I see what you mean :D Yeah, I agree with you on that, it's not *technically* correct, but it's mostly good enough for people coming from Java etc. dipping their toes in Rust.
 &gt; For example, if Tank A is targeting Tank B, and we remove Tank B from the central list (intending to delete him) but forget to clear Tank A's target field, then there's a chance that in the future, we can read Tank A's target field and call the fire() function, which shows a little firing graphic. Okay, thanks for the clarification. &gt; This is a problem solved by Weak (like your example) and also the ID approach, since they both don't keep the target alive. As long as IDs are not recycled, that is - in which case there is a chance that the ID will now point at some new valid tank, which would be quite bad. &gt; Just curious, what's the tradeoff with RefCell&lt;Option vs Option&lt;RefCell? It's not really about RefCell&lt;Option vs Option&lt;RefCell as it is about RefCell&lt;Weak vs Weak&lt;RefCell. If you have a `Weak&lt;RefCell&lt;Tank&gt;&gt;`, now the `RefCell` has to be borrowed whenever you access the tank. If you have a `RefCell&lt;Weak&lt;Tank&gt;&gt;`, you borrow the refcell, upgrade the weak reference to a strong (Rc), release the borrowed refcell, then access the tank. By keeping your borrows as short as possible you can easily audit your code to ensure there are no refcell panics. 
https://doc.rust-lang.org/src/core/mem.rs.html#174-176
I actually am using `rand`, and a Mersenne Twister in C++. I didn't do anything beyond measuring the total time that the algorithm took. The things I am doing are equivalent on a high level (using two vtables for dispatching algorithms on runtime vs dynamic trait dispatch in Rust), but I'm quite sure on the low level that the Rust code is something weird. I might check it out in a few days, if I still care by then :-)
If you're using `XorShiftRng::from_seed` with a constant array, then yes there should be no difference except what little there might be in `from_seed`. 
MacOS has had tags for years. It rocks.
I just published [promptly](https://crates.io/crates/promptly) a couple days ago for lack of "an easy way to get input". let a = f64::prompt("Enter a float"); let b = i32:: prompt("Enter an integer"); So while I feel your pain, it is also clear to me that there is no Obviously Right Way to handle input. From your title, I assumed our expectations of reading input were the same, but where your comments discuss reading partial lines with `text_io. I wanted dead simple readline-and-parse. Where text_io panics, I want to print an error and re-prompt. I also wanted to sanely handle escape sequences because all too often I experience weird output when using arrow keys when reading from stdin. The idea evolved a bit further and even included tab-autocomplete functionality when using `PathBuf::prompt(...)`. Given that `std::fs::read_to_string()` was recently added as an ergonomic improvement, I'd bet that `std::io::read_line()` would stand a good chance if it had a champion to drive an RFC for it. It would shorten your example to: let a: u32 = std::io::read_line()?.trim().parse()?; But I suspect anything more complex would get pushback that it is best handled by a crate. I'm convinced that neither `text_io` nor my `promptly` belong in the std lib.
There's a few more things that need added to cover some major use cases, type level ints being a good example. We're getting tantalisingly close to "fully featured enough to supplant c++" but we're not quite there yet.
I'm really not interested in programming to make money. Maybe I'm just a college student who's disconnected from reality, but I want to program because it's fun, and rust is one of the most fun languages.
Hi! You are working also on a rust-related project?
I agree on your final solution. But for small libraries, specially if their are intended to be used in application code, is not bad to use failure.
You can check my crate [input-stream](https://crates.io/crates/input-stream). It will pretty much do what you want I think. The module documentation should help you understand how to use it.
Sure, I'll target to do that every Thursday morning (UTC). Happy and honoured to be a mod!
Fixed, thanks!
(note, reddit doesn't support ```` ```lang ```` syntax, only plain ```` ``` ````)
&gt; You need to make sure an empty vector has somewhere to write the first element (unconditionally) Note that in Rust `let v = Vec::new();` allocates no memory. So while this could apply to SmallVec, that's only the case if the array backing it up does not have zero size.
Lisp is not inherently difficult. Just most programmers are unfamiliar with it.
Why not? Wouldn't this work? impl&lt;T&gt; Clone for T where T: Copy { fn clone(&amp;self) -&gt; Self { *self } } It works when I define my own `Clone` trait: https://play.rust-lang.org/?gist=c98de506b979afdd3b2d6de754360ff9&amp;version=stable&amp;mode=debug
What crates do you use for the graphics part? `glium`? `gfx`? I’d LOVE to bring `luminance` to wasm, yet I’ve never tried to compile it. It uses the `gl` crate internally so I’m not sure it would work directly.
Why is my first reflex to go see the `Cargo.toml`? :D
Sure but the problem is that these FCPs are given zero visibility because they happen in tracking issues in the Rust repo and not the rfc repo.
I’m adding thread-safe context handling to `luminance`, along with OpenGL state tracking (binding, state changing, etc.). I took a complete different approach than `gfx` / `glium` there, so it’s likely I’ll write a long blog post to explain why I did it the way I did. I’ll try to release a new version of `luminance` later but I need to test everything works out as intended by updating (locally) `spectra` and a demo binary with already some scenes to witness everything is okay. Yeah, I wish had a correctly setup unit test suite for `luminance`.
C++ is hardly a good example of easy input. Error handling is absurdly convoluted, and I don't want to count the number of people who got into trouble for mixing `&gt;&gt;` and `getline`.
&gt; the additions become smaller over time Haha, that's unlikely. Once dust is settled with current large features you'll see the new generation - something like reflection, optional garbage collector, contracts, heterogeneous computation or whatever. Look at other languages - the ride never ends.
Yep, the idea is to provide precise guarantees on what *isn't* generated, not to provide arbitrary precision of those that are. Doing the latter is pretty tricky, since you need a loop, and in practice a fully accurate dynamic range of 2^11 (2048) is probably enough.
This is an easy opinion to have, but I'd like to have more complex citations. For example, Ruby and Python, while continuously being worked on, still follow the same semantic model they had 20 years ago. Java has gained some features, but the language model still is fundamentally the same. Yes, languages are huge projects and people expect changes. It's also the place where you can - for better or worse - ship changes to a whole ecosystem very easily. But what the parent says is true: things cool down over time, also, because the cost of proper integration of new features rises.
Btw, reflection and contracts could be done with proc-macros or compiler plugins.. Heterogeneous computing could be done with IPC, right? Not sure there's demand for a garbage collector, aren't arenas enough for those scenarios that could cause cycles? What I would like to see included in a future version of Rust is being able to prove correctness of the code, but first the theoretical groundwork has to be done for that..
&gt; This is an unpopular opinion, but if the stream of features slows down after 2018, I'd welcome it. When Rust was young (the last three years), it's understandable to introduce large features every six weeks. But after 2018, when async / await and SIMD are done, I don't see that many features that absolutely need to be expressed in the language. I have to note that most of the changes you see were discussed in 2017, 2018 is just the implementation year. So we are already in the phase you want - it's just delayed, because implementation now happens. Also, there's _a lot_ of things that just enable new things (e.g. Termination/Result in main). You can totally use the old ways. I understand the want to learn a language once, but barring future improvements because of that is problematic. Many of these changes to address _very serious_ problems. Yes, async/await will make a lot of code look old-fashioned. It will do also improve that code dramatically. Also, in young languages, last years code will always look old-fashioned, even if we wouldn't change the language. Why? Because a lot of practices weren't designed into the language, people will find out their own ways to deal with it. Even in cooled down languages, that happens. Someone comes up with new practices to fix the old ones and in the process makes old code "really old".
Diesel currently only supports Postgres, SQLite, and MySQL. It should be possible to get MSSQL support going with an external crate, but I don't think there are any _public_ 3rd-party adaptors yet.
Interesting! Are you able to say what the use-cases were? In my case they were all financial related, which might have scared Intel.
I don't really know much about SME/SEV unfortunately. I'd love to learn more though.
I am very new to rust and started porting my software renderer written in c and c\+\+ into Rust. I setup a lib with cargo and decide to implement my unit tests as following: * private methods inside the [lib.rs](https://lib.rs) * public methods from lib inside as Example: vec2\_test.rs * Integration\-Tests not yet exist My first language mechanism/ features i used were Structs, Traits for Operator Overloading and writing unit tests for each operation. Inside the Project Context i work with needed package/module visibility. For the first Weeks it feels a little bit sluggish to develop in Rust, but i like it. Some Parts feeling more easier like Traits für Operations\(the naming of functions in plain c was/is horrible\). Its a shame but the first usage about Ownership, Borrowing and Lifetimes were more intuitiv but succesfull. I Hope to understand this Mechanism in Detail...in the next time ...i hope :D. Tips and Hints about possible Design Failures are welcome and regarding the context of software rendering performance as well. So lets rust in piece and happy rusting :\)
As for the second part of your question: https://areweideyet.com/#vim has a lot of information on editors. It does not list anything on debugging, but a quick google search led me to https://rnestler.github.io/debugging-rust-code-with-vim-and-conque-gdb.html In any case, any GDB debugger should work on Rust, so you can just google on that.
This will cause conflicting impls with generics. There's an example in another comment. 
Autocompletion might not work for modules, not included in compilation. Declare it in your main.rs or lib.rs and it should work.
Another VSCode user here. Rust support seems good enough. Not quite typescript level, but still decent. Apart from thing i mentioned [here](https://www.reddit.com/r/rust/comments/8jyqwt/auto_completion_in_vs_code/dzfrt7x), only macroses lack any autocompletion and checks. And maybe once I needed to restart VSCode with rls
Yep, github.com/nebulet/nebulet
Futures is used, so my guess is yes
bad bot 
Thank you, Oxidopamine, for voting on im-dad-bot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
&gt; or am I missing some higher design principle for handling file paths? I think I would question the wisdom of path canonicalization. The fundamental problem with it is that it translates file paths provided by end users into something potentially quite different. For example, if I gave your tool `~/nas` and there was a problem reading a file in that directory, then your error messages would come back with `/m/data/andrew`, which would be quite baffling to me. (`~/nas` is a symbolic link to `/m/data/andrew`.) &gt; url::Url::from_file_path() also happens to require a canonicalized path The documentation does not appear to agree with you. The docs say that it needs an absolute path. Canonicalizing a path is one way of getting an absolute path from a relative one.
&gt; I guess you could think of stable Rust as a pyramid-like process Or an ever-accumulating pile of something.
Wow. The downvoting of disagreement and disapproval... I gave diesel a whole-hearted effort and interacted with the group quite a bit while conducting research. You're generous with your time and care. Please keep up the hard work. The road is long and winding but at least you're not alone in your journey. I'm sure others will find value in what you do. Lack of features, failing to realize the full value proposition of dynamic query building, fatigue from prior query building experiences, etc, lead me to adopt an alternate approach. I have a lot to say about this and would rather do so in a constructive manner. I will get back to you on this, maybe directly by email to you. 
C was an interesting ride to learn during the early K&amp;R and transition to ANSI C89 days. But that was almost 40 years ago and getting the next influx of extensions and stable features meant actually buying a new compiler.
I've been writing a simple exchange. Can be found at \[GitHub\]\([https://github.com/carl\-foster/exchange\-rs](https://github.com/carl-foster/exchange-rs)\). It's been quite fun so far. Had to move away from Rocket as the API so that I could use Diesel.
That's right! I'm using the most recent release of the tokio runtime as well.
http://diesel.rs and documentation aren't presenting it as first class. The front page doesn't mention sql_query at all, not even in the complex queries section. That's misleading because Diesel handles complex queries *with* sql_query. The page markets what it does, not what it doesn't do. This is as expected. Claiming a first-class status, though, is an exaggeration, isn't it?
Please check that you're posting in the right subreddit before posting. r/playrust
My bad! I read the rules and forgot to read the subreddit name clearly, sorry for making such a random post :p
That's a good point. I'd be happy to accept a pull request adding a tab for it to the front page
It shouldn't be hard to add, but I'd want to see a concrete proposal for the API.
I see what you mean. Thanks for pointing that out.
It seems like you didn't actually read the comment you're replying to.
Things in rust fit together fairly well for the most part, i dont think calling it a pile implies that. The pyramid implies order. Perhaps nightly features could be described as a pile😁
Cool! &gt; Wicked Fast Web Servers in Rust Is the title referring to the fast development experience or the performance? (I saw your benchmark against tokio-minihttp in the Readme and while it's cool I don't see that as proof that this is actually fast for non-Hello-World scenarios.) &gt; Composeable subapps: each Thruster app can be added to a parent subapp at any route That sound pretty cool -- I love this property, because it let's you build components by themselves and also allows for more isolated testing. Do you have an example of that? &gt; Multithreading: each Thruster request is by default put into a separate thread That's… surprising, given you also seem to be using futures? &gt; With the help of fuel_line, templates are compiled and checked at runtime and are extremely performant. Did you mean to write "compile time"? Anyway, I'd love to read more about that.
Wrote library to get ip geolocation info from ipinfodb, can be found [here](https://github.com/zxey/ipinfodb-rs).
This is why I largely don't complain about open source. If something can be improved, it's up to each of us to do something about it and not bitch on the sidelines. I've already shit the bed here. I'm going out the way I came.
&gt; I think I would question the wisdom of path canonicalization as something you reach for frequently. I'm OK with a program handling relative paths as-is, if they were given to it directly, and it processed them and reported errors immediately (example: tools like `sort` and `grep`). There's little scope for ambiguity, and as a user it's useful to directly see the path I provided in the error messages. For a program that queues or sends paths, or a program that constructs paths for internal use (like cache directories or intermediate files), I'd rather it use absolute paths. Separated from the original invocation, there's more scope for ambiguity, and I find the explicitness of absolute paths is more generally useful in diagnosis than matching the input with the output. If a program is going to make absolute paths, I'd rather it *not* wind up with a chain of `/foo/bar/../baz/./qux/../../..//path/to/some/other/thing`, since that obscures what's actually going on. I don't care too much about resolving every possible symlink, just the ones that occur before `/../` segments, but I've never seen a path-handling library with method that does that, so I fall back on "canonicalize". &gt; Canonicalizing a path is one way of getting an absolute path from a relative one. In a Python context, there are separate functions to make a path absolute, normalize path delimiters, find a path's real name, and the "resolve" method I linked that does a combination of them all, so I have a rich vocabulary to describe what kind of path manipulation I'm talking about. In a Rust context, `canonicalize` and `join` seem to be the only ways to disambiguate a path, and `join` is itself a pretty ambiguous term, so I was left with "canonicalize" as the only way to describe the precondition of `url::Url::from_file_path`. Now that I play with it, I can see that it only needs an absolutified... absolutized... joined path, but that potentially includes messy `/../` segments so I would still rather canonicalize. &gt; My guess is that if you really want to double down on the path canonicalization, then you'll need to implement the routine you want yourself. Fair enough! Thanks for the input, and thanks for your hard work on regex and ripgrep. :)
The post is down from here, was it cached somewhere?
That's really odd. It was just working a minute ago. I'm hosting it on github pages, so it should be working.
Could someone give me (or point me to) a summary of how Redox does async IO? I'm really curious whether it's similar to epoll/kqueue, or whether it's something new entirely.
Interesting. I hope that, if that is a bug in Tokio, it gets fixed.
good bot
Got the same error.
Got the same error.
I gave a talk about Rust at CodeMesh in 2016: https://www.codemesh.io/codemesh2016/steve-klabnik It was a pretty great conf, overall :)
Shift+K runs a help command (by default `man`, but you can specify it, and e.g. override it on a per-filetype basis) on the word under the cursor. So install your preferred local form of documentation, and you can easily hook it into Vim.
Can confirm, it didn't work, but now it does!
Key management for digital asset/cryptocurrency use cases
Double NVM I'll delete this later, but it looks like it simply won't compile to object code. Instead I used Xargo to get the core and compiler builtins crates, and then used rustc to compile to object code - I'll try using llvm to compile it instead, ah well.
Some of the ideas for GC I have seen floated are more like a way to embed Rust into other languages more easily, utilizing their GC.
Thanks! I actually completely forgot about this feature of vim.
&gt; Not sure there's demand for a garbage collector, aren't arenas enough for those scenarios that could cause cycles? There sorta is demand for integrating with a GC when you're embedded in another language. It'd be nice to say "Ruby is gonna take care of ownership here" and let it be managed then. That kind of thing.
I use RLS with `neovim` along with `LanguageClient` and `deoplete`. My config is [here](https://github.com/phaazon/config/blob/master/archlinux/.config/nvim/init.vim#L34).
Yep, that's what I mean. Thanks!
There you go. :)
Occasionally I find myself using a block to trick the borrow checker. Like for example if I need to borrow a mutable reference to a member of a struct, then call a method on that struct that requires a mutable refernce to itself. If I don't use a dummy lexical block the borrow checker cannot figure out (on 1.26.0, I'm usually on stable) that there is in fact no conflict. For example, this pseudocode: Like this: struct AStruct { v: Vec&lt;Option&lt;Box&lt;T&gt;&gt;; } impl AStruct { fn do_something_else(&amp;mut self) { // mutate self } } fn do_something(s: &amp;mut AStruct) { // Use a block to fool the borrow checker { let elem = s.v[index].as_mut().unwrap(); // do something with elem } s.do_something_else(); } Is there a cleaner way to do this?
Thanks for your configuration, I will take a look. By the way, does deoplete use Racer for completion?
I don’t know, I’ve commented it because I had issues with overlapping features – I still have weird behavior with `deoplete` but I guess it’s because I misconfigured it.
Oh no, they [broke](https://github.com/rust-lang/rust/pull/50782) my [silly Rust program](https://www.reddit.com/r/rust/comments/4iwv4r/whats_the_most_surprising_rust_code_that_actually/d327ypx/).
Unfortunately the way to get unbuffered keyboard input is implementation defined. But, on most unices, the procedure is fairly standard. You'll have to use `libc` of course. Just call `libc::tcgetattr` to get the current termios. Set the proper flags on it (this is a snippet that enables full raw mode, you'll need to read the manpages for each flag since I forget their meaning off the top of my head): let mut stdin_fd = io::stdin().as_raw_fd(); // You'll need the AsRawFd trait. let mut termios = libc::termios { c_iflag: 0, c_oflag: 0, c_cflag: 0, c_lflag: 0, c_cc: [0; 20], c_ispeed: 0, c_ospeed: 0, }; let ret = unsafe { libc::tcgetattr(stdin_fd, &amp;mut termios); } // check ret here for errors! termios.c_iflag &amp;= !(libc::BRKINT | libc::ICRNL | libc::INPCK | libc::ISTRIP | libc::IXON); termios.c_oflag &amp;= !libc::OPOST; termios.c_cflag |= libc::CS8; termios.c_lflag &amp;= !(libc::ICANON | libc::ECHO | libc::IEXTEN | libc::ISIG); termios.c_cc[libc::VMIN] = 0; termios.c_cc[libc::VTIME] = 1; let ret = unsafe { libc::tcsetattr(stdin_fd, libc::TCSAFLUSH, &amp;termios) }; // check ret for errors! Voilà, you're in raw mode on macOS/linux. 
It's using the `event:` scheme :) It used to be a simple per-thread thing where you register with `fevent` and then `read` events from that scheme. But since /u/jackpot51 changed it, it's now just like epoll: instance specific. For a code example, see his [RFC](https://github.com/redox-os/rfcs/pull/10) [(Rendered)](https://github.com/redox-os/rfcs/blob/507649c44cedc2f88a474796bb1ff98bfc7423fa/text/0000-event-overhaul.md).
In the first instance, turn each horn of the match into an block: match x { Some(a) =&gt; { // do something }, None =&gt; { // print error } } This insures that each branch of the match returns `()`. If you need to convert between error types on an `Result`, use the `map_err()` method.
The `tokio::spawn` thing? Yeah definitely, it's quite odd. I haven't actually tried it on a non-redox system but I really doubt it somehow is introduced by that, considering most of the os-specific logic is done by mio :P
This is a compiler bug: [**rust-lang/rust#47358**](https://github.com/rust-lang/rust/issues/47358). The Serde derive is receiving: pub struct Created { #[serde(default = concat!(stringify!(Created), "::", stringify!(value1)))] pub value1: u32, pub value2: u32, } in which the attribute is [not valid Rust syntax](https://play.rust-lang.org/?gist=e6b00ce715813a960b9d1fd20a8f4a4f) and gets ignored.
Working for me
Why not wrap each field in an Option&lt;T&gt;? You would be able to take() values as needed, and the drop() would only clean up whichever fields are Some. I suppose it could be more cumbersome to use the fields, maybe some added memory for the options, and slight runtime cost when dropping.
When [non\-lexical lifetimes](https://rust-lang.github.io/rfcs/2094-nll.html) are implemented, the borrow checker will become smart enough to compile your code without the extra block. \(You can already experiment with this using the "nll" feature in nightly rustc.\) Until then, using a block like this is perfectly idiomatic.
Down for me, don't use GoDaddy.
&gt; most critical / quality-of-life features (that were missing from 1.0) are done by now I'm working a lot with highly generic code and there are still several features that are sorely missed there. Implied bounds, specialization, generic associated types, and integer generics come to mind, but there might be some more that I'm not thinking of right now, and more things might come up as these features are implemented and start to get used. Noone just thinks "I want to add generic associated types to my code!" out of the blue, these features come up after refactoring existing code and discovering "oh, this abstraction is only possible if feature X is added to the language". For example, the need for immovable data had been discussed for a while but the more futures and especially async/generators were used the more obvious this need became. Luckily this time the need was able to be solved without language changes (with `Pin` instead), but that won't always be the case. So, moral of the story here is: people (including me) will keep requesting features for a *long* time, because as features get implemented and used, they start to highlight needs for other features that weren't obviously needed before.
 $ dig lsneff.me ; &lt;&lt;&gt;&gt; DiG 9.8.3-P1 &lt;&lt;&gt;&gt; lsneff.me ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 6468 ;; flags: qr rd ra; QUERY: 1, ANSWER: 5, AUTHORITY: 2, ADDITIONAL: 0 ;; QUESTION SECTION: ;lsneff.me. IN A ;; ANSWER SECTION: lsneff.me. 1654 IN A 184.168.221.53 lsneff.me. 1654 IN A 185.199.108.153 lsneff.me. 1654 IN A 185.199.109.153 lsneff.me. 1654 IN A 185.199.110.153 lsneff.me. 1654 IN A 185.199.111.153 ;; AUTHORITY SECTION: lsneff.me. 3454 IN NS ns21.domaincontrol.com. lsneff.me. 3454 IN NS ns22.domaincontrol.com. ;; Query time: 63 msec ;; SERVER: 2a01:e00::2#53(2a01:e00::2) ;; WHEN: Wed May 23 17:24:49 2018 ;; MSG SIZE rcvd: 162 Still down... Do you have a *.github.io link handy?
Yeah, but that redirects directly to lsneff.me
as a redundant link, it can be found on github: https://github.com/lachlansneff/lachlansneff.github.io/blob/master/_posts/2018-05-20-nebulet-booting-up.md
To anyone who finds this - the binaries are apparently compiled before running any integration tests. I switched the tests from unit tests to integration tests, and was pleasantly surprised that it the binary file was updated automatically.
It solves the problem of having to maintain two copies of the same code. You'll still probably need to mark it `#[repr(C)]` and transmute it.
Hehe... [this](https://github.com/lachlansneff/lachlansneff.github.io/blob/master/_posts/2018-05-20-nebulet-booting-up.md) will do in the mean time then.
Your article says to manually add thruster and futures to Cargo.toml and I noticed must instructions for rust projects say this. Why not do`cargo add thruster futures` instead?
That's probably the difference: everyone I spoke too was making use of remote attestation directly, e.g. for smart contracts, where breaks can quite directly be used to steal money via fraudulent attestations.
cargo-add is an extension so I'd guess most people either don't know about it or don't have it installed.
&gt; A few times in my years following Rust (mostly via this subreddit and TWIR as linked in the subreddit) I've been surprised by what appears to be a sudden stabilization without much debate. I'm confused by this statement. The RFC in question has [205 comments](https://github.com/rust-lang/rfcs/pull/1951) and is from last year. This makes it one of the most-discussed RFCs! And it's not sudden! That said, I'm definitely one of the people that has decided to only selectively follow the RFC process, because of the sheer volume. It's really hard to follow, which might amplify any visibility problems the process already has. Also, I'm generally fine with the recent track record. But, for the reasons outlined above, I'd be very cautious to take too quick action based on this one instance of a problem. While I feel that I can relate to a lot of the sentiment (this is not Rust 1.0 "you can read all RFCs in a couple of hours" anymore), I don't agree with the conclusion that the process is fundamentally flawed (or, even framing it as the project trying to sneak something in). I also appreciate the complaints (I don't like the form it has taken), but I don't think "bad visibility of the discussion" has been the true problem of this RFC. The problem with `impl Trait`, as someone who followed it along it its entirety is that it spanned _most of the lifetime of what we call Rust 1.0 and even some time before_. People that _read_ about universal impl trait back in 2016 might have already _forgotten_ that this was part of this release. This rarely happens. It has been (for many reasons) a very painful endeavor and much more complex then expected. Heck, I was even joking in my courses that the first thing I listed in my "soon" slides by the end of the course (based on Rust 1.6) has never properly arrived. That still means that giving more visibility in every way (e.g. by having someone frequently do a summary of what's going on, in the form of a blog post or a podcast) is probably not a bad thing, but we need more and more people for that!
I’ve found deoplete with racer to be a total pain in the ass. Personally, I’m now using asyncomplete and asyncomplete-lsp with the Rust language server and it’s much better
You can use diesel_cli with the --database-url option. The easiest (but maybe not most effective) way I found to do tests was in a transaction. fn init() -&gt; PgConnection { dotenv::dotenv().ok(); let database_url = std::env::var("TEST_DATABASE_URL").expect("DATABASE_URL must be set"); let db = PgConnection::establish(&amp;database_url) .expect(&amp;format!("Error connecting to {}", database_url)); db.begin_test_transaction().unwrap(); db } 
Totally agree re: TWiR!
Transactions and truncation are the two major ways to do this, both with their pros and cons.
&gt; It's worth clarifying: every approved RFC results in a formal "tracking issue" where we discuss implementation issues, usage feedback, and formally propose stabilization. Just like with RFCs, there's a full team sign-off and "final comment period" on these tracking issues. Another opportunity to raise awareness of this is to add to the release announcements a "What to look forward to" section that includes features in the beta channel. - It raises visibility - But it is very late in the process - Could help raise awareness of testing the beta channel An alternative would be to have a stablization timeline rather than just whats in the beta channel. This catches things earlier at the cost of providing more noise / things less relevant to the post these are being tacked on to (since they are further out in the future).
You managed to get Mozilla to accept this as a GSoC project? That's great. This is quite an interesting project, and I look forward to seeing what progress you make!
I'm sorry, but I don't get it, I'm trying to test controllers that use diesel, so they are just making regular HTTP (mocked) requests to the API, but the API manages a DB, which if it's not cleaned up it will generate conflicts. How does that help?
Yeah, the only thing I down voted was simply for &gt; I can but I won't. Nothing against you picking a different library, just seemed like you were in the thread celebrating a new release to cause drama. Thanks for owning it though! 
`begin_test_transaction()`creates a transaction that will never commit. So it verifies all of your queries would go into the database correctly, then rolls them back without writing them when the db object goes out of scope. So #[test] fn duplicate_account() { let db = init(); let acc1 = Account::new(stuff); create_account(&amp;db, acc1).unwrap(); assert!(create_account(&amp;db, acc1).is_err()); } Creates an account successfully, then attempts to create another which will cause a duplicate primary key. You verify that it's an error, then when the function ends the DB transaction rolls back and is never actually written to the DB. Testing your API endpoints and your library are different things and you should probably segregate them.
The default Rng's in rand are slower (but more secure) than a Mersenne twister, so that might be part of it. However they've recently introduced new algorithms in rand 0.5 that might improve your performance
On nightly we have box syntax
Oh I see, that's very nice, I will have to think about it more tho, because I don't really see how to apply that to a r2d2 diesel Pool, which is the only state passed to the function. So I will dig deep in the APIs, thanks!
nnethercote is awesome. Those compiler perf wins are just amazing
Oh fuck my bad
Well since I want something really adapted to wasm and webgl, I'm not using any crate for that purpose, at least for now. The idea is to have a 3D library that will handle the scene and the heavy computations, hand them back to JS through linear memory, and let JS call WebGL directly. The idea is that crossing the JS to WASM boundary is costly (at the moment), and directly calling the rendering methods in WASM will make you cross that boundary quite a lot per frame (every call to the gl context will need to cross the boundary back to the browser apis, which are not yet directly exposed to wasm (that might change sometime, but not soon enough)). So what I'm doing is something that would rather work like this; each frame: JS gathers operations to apply to the scene and batches them together in the linear memory as a byte array-&gt; JS calls *once* the WASM function that will actually apply the operations one by one to the Rust scene, do the heavy computations like skinned animation and normal computations, update everything that needs to be updated, etc, then hands back the results as pointers to arrays in memory -&gt; JS uses that to call the gl rendering context and render the scene using the usual webgl calls. So the idea is have a scene management/3D computation without the actual rendering, a bit like luminance does rendering without scene management. It might be completely possible to hook both of them together though, but that will need to wait for something usable on my end first. ;)
Great questions, I'll try my best to answer: - I hope both! Performance is still an area that I haven't fully optimized yet. I've been writing a few of my side projects using the framework, so the ergonomics should be good though. - I'm hoping to write another post exploring this more thoroughly, but the basic structure looks like the starter kit found here: https://github.com/trezm/thruster-starter-kit (the sub-app is the connectivity module) - Each request uses tokio's spawn method to add the future work to the thread pool: https://docs.rs/tokio/0.1.6/tokio/executor/fn.spawn.html, although I might be misunderstanding the question - I did mean compile time :), I'll update that in the post. I actually am working on a separate post about how it's written using proc_macros which I'll throw out here soon as well :)
Even though I encourage people to ask questions on SO for greater future discoverability, I acknowledge that there are plenty of people who 1) don't like SO for whatever reason, and/or 2) already have a reddit account and are going to want to post questions here anyway. I figure that having the sticky is an acceptable tradeoff if the alternative is to have the front page flooded with questions (and trying to impose a ban on questions would be met with a lot of backlash and would substantially increase the burden on the modteam). As long as people keep using the sticky to ask questions, and as long as people continue to monitor the sticky to answer those questions, it's doing its purpose of providing help while also keeping the front page relatively unburdened.
We used to do this, it felt a little weird, as it made each release feel like it was part of two releases.
 BitAnd&lt;u8, Target=u8&gt; It's BitAnd&lt;u8, Output=u8&gt; See here: https://play.rust-lang.org/?gist=dfe8302d50ed34ac1fbb8c02e72a7124&amp;version=stable&amp;mode=debug
How exactly `u32` should implement it? Should it assume other bytes are zero?
Maybe check out the traits in the num crate? It has traits related to generic conversion between numbers and the like though I didn't find one with generic masks (only shifts).
Can you give more information? It's impossible to tell.
Oh, I mis-read your original post slightly.
Some reasons I can think of: - no one's found a particular reason to need these (unlikely?) - this contains an implicit conversion from u32 to u8 which we might not want - should we then have `BitAnd&lt;u16, Output=u16&gt; for u32,u64`? Not sure where this ends. As for how you'd do this, you'll need to rely on an external crate `num_traits` to cast. The interface for casting numbers doesn't seem fully stabilized, so it's not in `std`, but `num_traits` is still fully usable. In particular, you want [`AsPrimitive`](https://docs.rs/num-traits/0.2.4/num_traits/cast/trait.AsPrimitive.html). You can then require `T: BitAnd&lt;T, Output = T&gt; + From&lt;u8&gt; + AsPrimitive&lt;u8&gt;`, convert the `u8` into a `T`, apply it with `BitAnd`, then use `as_()` to convert back to a u8. If you do this a lot, a helper trait might be useful: trait AndU8 { fn and_u8(self, mask: u8) -&gt; u8; } impl&lt;T&gt; AndU8 for T where T: From&lt;u8&gt;, T: BitAnd&lt;T&gt;, &lt;T as BitAnd&lt;T&gt;&gt;::Output: AsPrimitive&lt;u8&gt;, { fn and_u8(self, mask: u8) -&gt; u8 { (self &amp; mask.into()).as_() } } Then you could do `x.and_u8(&lt;some value&gt;)` in both regular and generic contexts. Full example using this trait: https://play.rust-lang.org/?gist=3adb644709970debae84489e88842b9f&amp;version=stable
Sounds so easy when you say it. And it probably is. Still can't get it to work. (But I am using `Irc` crate [https://crates.io/crates/irc] ... my stream never starts, although everything compiles just fine) pub struct IrcDriver { pub test_actor: Address&lt;TestActor&gt;, pub client_stream: Option&lt;ClientStream&gt;, } pub enum IrcMessage { Notice(String), Private(String), } impl StreamHandler&lt;IrcClientMessage, IrcError&gt; for IrcDriver { fn handle(&amp;mut self, item: IrcClientMessage, ctx: &amp;mut Context&lt;Self&gt;) { if self.client_stream.is_some() { self.test_actor.send(IrcMessage::Notice("Got something for you!".to_string())); // TODO send actual useful message } } fn started(&amp;mut self, ctx: &amp;mut Context&lt;Self&gt;) { println!("Started IrcStreamHandler!"); let client = IrcClient::new("./conf/config.toml").unwrap(); client.identify().expect("Failed to identity to irc server"); self.client_stream = Some(client.stream()); } fn error(&amp;mut self, err: IrcError, ctx: &amp;mut Context&lt;Self&gt;) -&gt; Running { unimplemented!() } fn finished(&amp;mut self, ctx: &amp;mut Context&lt;Self&gt;) { unimplemented!() } } impl Message for IrcMessage { type Result = Result&lt;testActorAnswer, ()&gt;; } impl Actor for IrcDriver { type Context = Context&lt;Self&gt;; } #[derive(Debug)] pub struct TestActorAnswer { pub message: String } impl TestActorAnswer { fn new(message: String) -&gt; TestActorAnswer { TestActorAnswer{message} } } pub struct TestActor; impl Actor for TestActor { type Context = Context&lt;Self&gt;; fn started(&amp;mut self, ctx: &amp;mut Context&lt;Self&gt;) { println!("TestActor started!"); } } impl Handler&lt;IrcMessage&gt; for TestActor { type Result = Result&lt;TestActorAnswer, ()&gt;; fn handle(&amp;mut self, message: IrcMessage, ctx: &amp;mut Context&lt;Self&gt;) -&gt; Self::Result { match message { Notice(msg) =&gt; { println!("Notice from Irc: {}", msg); Ok(TestActorAnswer::new("notice response".to_string())) }, Private(msg) =&gt; { println!("Private message from Irc: {}", msg); Ok(TestActorAnswer::new("priv msg response".to_string())) }, _ =&gt; { println!("Other message from Irc"); Ok(TestActorAnswer::new("other response".to_string())) } } } } This was done in 10 minutes or so, so probably I have to invest a bit more time than just seeing "ah, look, just 3 easy steps, wonderful!" (and I am well aware that Step 3 is even missing here ;))
You can't destruct type via `&amp;mut`. Before `ManuallyDrop` I usualy used `Option`
My issue with Lisp is that its "lack of syntax" means there are no syntactic markers to aid readability.
Sadly, the deterministic algorithms also perform worse... I'll dive deeper if I still feel like it in a few weeks.
Are your searches always going to be aligned at the byte level of the integer you're passing? If so, perhaps you could just `transmute` into a byte slice and search that using a trait\-based solution like in [this playground](http://play.rust-lang.org/)? Also, your formatting is broken. Did the new Reddit editor mess you up? I forget about that all the time too...
Here's a possibility that requires an unstable feature: https://play.rust-lang.org/?gist=d7169630cea60fa8eed30a68a013937d&amp;version=nightly&amp;mode=debug #![feature(try_from)] use std::ops::BitAnd; use std::convert::TryFrom; fn low_4_bits&lt;T: BitAnd&lt;Output=T&gt;&gt;(x: T) -&gt; u8 where u8: TryFrom&lt;T&gt;, u8: Into&lt;T&gt; { u8::try_from(x &amp; 0xFu8.into()).ok().unwrap() } 
I'm a bit lazy right now, but you can take a look at my IRC actor https://github.com/DoumanAsh/roseline.rs/blob/master/bot/src/irc.rs
Specifically how I start it https://github.com/DoumanAsh/roseline.rs/blob/master/bot/src/irc.rs#L386-L423
Just to clarify: this is a PR **against** `rustc` **proper** that was measured to have improved the `inflate` crate's compilation time by 10&amp;#37;, NOT a PR against `inflate` itself. :\)
The most important thing for Actix is to add your Irc's stream to Actor's context so that it could start reading: `ctx.add_stream(stream);`
That seems to solve it, thanks! It's still a bit of a pain though. Your argument about also having BitAnd\&lt;u16\&gt; etc. is certainly a good point, but one could argue that bitwise and with a single byte is by far the most common scenario.
Probably also important to mention: &gt; This is the old bloated inflate, right? I feel like we should make that more explicit - people actually using the crate today won't see anything, the piles of code liveness (and presumably borrowck too) chokes on, are gone. \- [eddyb on Github](https://github.com/rust-lang/rust/pull/50981#issuecomment-391461224)
Another interesting solution, thanks! Although the extra error\-handling cost is a bit annoying considering it's guaranteed to never fail.
Seems like you answered your own question.
Thanks! Seems to be very fast and accurate.
In release mode, it compiles down to playground::low_4_bits: andb $15, %dil movl %edi, %eax retq , which is doing nothing but returning `x &amp; 0x0F`.
try_from is still unstable???
I don’t want to wrap the fields because that would change the way I access them / add `unwrap` everywhere. Such semantics shouldn’t bleed out everywhere in the code.
"Best practices". Indeed. I wish I new what the best practice around defining and handling errors is supposed to be. `?` I get and like, but I know there is a try trait, I am sure I have seen something somewhere about a try\catch keyword being introduced that looks a bit like an exception handling block, many web posts recommend using `error-chain`, but then there's the `failure` crate which is `intended to replace error management based on std::error::Error with a new system based on lessons learned over the past several years, including those learned from experience with quick-error and error-chain.` Crikey. Sounds like things are in a massive state of flux.
[Apparently](https://github.com/rust-lang/rust/issues/33417) it's waiting on the `!` type to be able to describe infallible conversions.
&gt; I mostly see the coming changes in libraries, less in the language. Ex. NonZero / Pin, etc. being stabilized, While I partly agree with this post, I'll note that these two features both do require language changes to really work properly... and I think the same will be true of lots of other useful library features.
Ah... thanks.
Arenas work really well unless you actually need to reclaim memory before you're done using them... then you actually just straight up want a GC.
How would you represent `(1 + 2) * (3 + 4)`?
How can this be solved, in the presence of generics erasure?
Correct.
There's a general Rust philosophy that the standard arithmetic operators (including bit arithmetic) should only apply if the result and both arguments are all of the same type. This is believed to be an aid to program correctness. One approach would be to make a wrapper type for byte masks and define `BitAnd` for it. Sadly, it apparently needs to appear on the LHS to make it clear to Rust that you are defining an impl in this crate. &lt;https://play.rust-lang.org/?gist=d70520f6c007c132a07234908f6fc3bb&gt; 
You can certainly use Diesel with Rocket.
Note that `i8` doesn't implement `From&lt;u8&gt;`, so this is slightly incomplete. How about just using `T: AsPrimitive&lt;u8&gt;`? impl&lt;T&gt; AndU8 for T where T: AsPrimitive&lt;u8&gt; { fn and_u8(self, mask: u8) -&gt; u8 { self.as_() &amp; mask } }
If there's anything I know about optimization in the real world, it's that even just the act of asking this question is likely to reduce annoyance. Makes it feel like concerns have been heard, and something is being done. Glad this question was asked.
The way we handle this in crates.io is to give the application a connection pool with a size of 1 to force the connection to be shared between tests and server
Mind = blown. Thanks, I haven't thought about that.
Awesome, can't wait for that to hit stable. On a slightly related note \(generic numbers\), is there a generic cast from `&lt;T: Unsigned&gt;` \(from the num crate\) to the corresponding signed value and vice versa? In C\+\+,we hav`e std::make_signed_t&lt;decltype(x)&gt;(`x\). Can Rust do something similar?
Update, didn't work, a lot of tests are integration tests and the transaction is not passing the inserted values to select queries for some reason. I may be wrong and there is a way, but for now it seems I'm going to keep testing in series.
&gt; and the transaction is not passing the inserted values to select queries for some reason. That would imply that you have two separate database connections
Are you compiling rustc in release mode? (jk)
Hmm, I've set the Pool builder to max_size(1) and then derefed the connection and started the test_transaction. This pool is send to every function called and for some reason nothing is stored. Am I doing something wrong?
I agree 😁
A lot of dynamic descisions at runtime. E.g. make all trait objects "sized" by putting boxes around them, but only if they are used in a sized fashion. You pay for all of this with the `dyn` keyword so the overhead is explicit and opt-in. But this isn't thought through at all so maybe there are huge gotchas (e.g. that this would most likely be a breaking change).
Something is definitely wrong, but this isn't enough information to say for sure what it is. Is the repo something you can share? If not, can you reduce the problem down to something you can share? I can probably help more with some more context. (Might also be worth moving this to the Diesel gitter)
I'm in the area and would definitely be interested. Perhaps the google form could allow multiple choice for the possible social media platforms box rather than just one though?
I feel like the Rust libraries are similar. There isn't a library for everything yet, but some of the libraries in the ecosystem are the best I've seen in any ecosystem. `serde` one easily of the best, if not the best, serialization libraries. `rand` 0.5 that was released yesterday is the most extensive randomization library I've seen. `regex` is also great. Cargo is an incredible package manager. I think what gives Rust the edge is the community. So many volunteers come forward and work **together**. The Rust community is an ideal example of the what the free software movement is about.
Changed now. Thanks for the suggestion! :\)
Can anything be done to reduce linking times? Linking takes the most time in my projects (rustc-msvc)..
I guess I was being conservative to implement exactly what the OP had, but if that is equivalent it does sound better. One question though: are we guaranteed to have `((x as u8) &amp; mask) == ((x &amp; (mask as &lt;other type&gt;)) as u8)`? I thought they might differ due to wrapping, but I'm probably misunderstanding some operation.
Have you given lld or gold a shot?
We have a somewhat similar problem in one of our projects at work although not rust but uses a shared db for testing. We just clean up records based on a created_time field which we needed anyway.
What exactly you didn't like in deoplete? Also it seems RLS uses Racer for code completion.
I think the big problem is that when you depend on non-standard crates for basic stuff it make sit a bit harder to learn when there are a few different crates for these basic things; as a new user to a lanugage you expect to be able to look up basic things and say 'How do I do ... in Rust' - when you get 10 answers with different crates it's not easy.
My project isn't at a state where it's super huge, so I haven't noticed. the OS is just a toy :) You can set up a `.cargo/config`, and then set the linker there https://doc.rust-lang.org/stable/cargo/reference/config.html#configuration-keys
I'm curious, how hard is it to find coders who are intimately familiar with Rust and Monero / cryptography?
Well sure, those are the tradeoffs, but as a pro it is a completely safe solution. Depending on the needs those cons may be negligible. 
So just this? [target.$triple] linker = "lld" 
Literally none of the Tari Labs devs know Rust, although I’m certain there will be some contributors to the project who are Rust wizards. Everyone learns together:) Tari Labs *is* hiring, but since it’s based out of Johannesburg, South Africa, it may not suit unless someone is crazy excited about relocation.
i think it should be full path.
HELLO! I live in Logan which is a crisp two hour drive from you but I'd absolutely come!
The fundamentals are the same as always, there's just a lot of boilerplate. All of the things you mention are competing attempts at reducing that boilerplate, but that's pretty much it.
It's still down for me.
The usual problem for things like this is inference. [Same as with indexing by more types](https://internals.rust-lang.org/t/focus-of-rust-evolution/7217/34?u=scottmcm), you don't want \`x &amp; 4\` to say "I don't know what type you meant; please say \`x &amp; 4\_u8\` or \`x &amp; 4\_u32\`".
There's no sign\-casting in num, but we could add that. Perhaps something like: pub trait SignCast { type Signed: Signed; type Unsigned: Unsigned; fn into_signed(self) -&gt; Self::Signed; fn into_unsigned(self) -&gt; Self::Unsigned; } I guess this would just bitcast, so overflows are silent. It's less clear what something like `BigInt` to `BigUint` should do with negative values, since it's not two's complement \-\- just return the magnitude?
I'm glad that you have observed this. Another thing that I have observed with myself is that if someone tells me to do something I find ridiculous, they are much more likely to make me happy and play nice if they give me a good reason for why they are doing it, than if they just hand out orders.
[removed]
Great idea. I was considering it myself but you beat me to it. Just filled out the form.
Still very impressive. Not every codebase gets as much attention by people smart about the compiler as the inflate crate does.
One option if you're using postgres is to create a new database for every test, using a template database. I haven't done any benchmarking, but it's apparently much faster than truncation, and you can actually commit , unlike running all tests in a transaction. Basically you run all your migrations on a DB, let's call it `test_template`, then before every test you run `CREATE DATABASE test_real TEMPLATE test_template` and you use test_real for your tests, dropping test_real after. I've only done this before for a node project, so I don't have any advice for how to implement this in rust specifically. Also no idea if you can do something similar for MySQL if you're using that.
I would love this but would prefer Utah County. I have been thinking I should take the initiative and organize a meetup but you beat me to it. I have a team of 5 besides myself at work learning Rust so they are potential participants as well.
There's a couple of things, but nothing very good. It's currently in a state of constant flux, so it might be confusing for contributors. Once it has applications running on it, it'll be easier for people to add to it! Join [https://gitter.im/nebulet/nebulet](https://gitter.im/nebulet/nebulet) and we'll help you set it up!
Thanks!
Does compiling rustc count? Heh 
That’s already a test case for sure :)
Paths of are interest to the CLI-WG. Please add your thoughts to https://github.com/rust-lang-nursery/cli-wg/issues/10
Oh shit, I think I understand what is going on. Basically I'm starting the test transaction with the Pool, but when some DB error happens the transaction is rolled back, so everything after that is in fact commited and everything from before is lost. My solution was to re-create the server (the pool and the related data) everytime I expect some error, but this is not great and with an unexpected error it leaves residue in the DB. How may I solve that? Or am I completely wrong?
Oh shit, I think I understand what is going on. Basically I'm starting the test transaction with the Pool, but when some DB error happens the transaction is rolled back, so everything after that is in fact commited and everything from before is lost. My solution was to re-create the server (the pool and the related data) everytime I expect some error, but this is not great and with an unexpected error it leaves residue in the DB. How may I solve that? Or am I understanding what is going on wrong.
Thank you for doing this! Great article. You mentioned usecases I haven’t even thought about. The example with the `apply` function is exactly why I *hate* that universal notation. It’s confusing. Your `accumulate` function is also a great example of false assumptions people might have.
&gt; Multithreading: each Thruster request is by default put into a separate thread &gt; Each request uses tokio's spawn method to add the future work to the thread pool These are two different things that cannot both be true. Adding a new Future to the thread pool does not put the request into a separate thread.
Which extensions are you using? There are a few out there.
Can you make `syntax_ext` compile faster, please? It takes so much time when I compile rustc! ;-) Joking aside, would there be any use in compiling crates from the ecosystem and, through profiling, find where most time is spent when compiling such a real-world sample of code?
This doesn't work by itself, because LLD needs different linker flags from the GNU LD, which means you have to pass a `-Z linker-flavor=ld` flag as well. `-Z` means nightly only, sadly, but it is possible to add this flag into the cargo config. In my experience, you also have to specify the system libs directory (at least on Linux) as a linker flag for lld, since it doesn't know where they are.
I think I likely am misunderstanding -- but I thought the Tokio runtime added future to its thread pool, which enables work stealing, which is inherently a multithreaded scheduling concept. I'm mostly going off what I read here: https://tokio.rs/blog/2018-03-tokio-runtime/, please let me know where I'm not making the connection!
This is technically orthogonal to `wasm-pack`, right? Can both of these be used at the same time?
`conv` handles this by having an `ApproxInto` trait that takes an approximation method as a type argument, one of which is `Wrapping`. Conversions to integers implement approximation via `Wrapping`, with a different set of implementations for floating point types. `into_unsigned` really doesn't suggest *how* it's going to do that conversion, and I don't think it's a good idea to have wildly different behaviours behind a single, generic interface. As in, if you have a piece of code written using this that works on normal integers, it's probably going to break the moment you throw a `BigInt` at it.
I started off by wrapping Diesel's functionality as the heart for a very large (30,000LOC) project. Made a generic interface for interacting with Sqlite / Postgresql and performing basic CRUD operations. The things that eventually made me switch to simply using a lower level library were: * The documentation at the time was lacking (though what documentation there was was very high quality, I must say). There were almost no Stack Overflow questions to inform solutions to common problems, so it was IRC or spend a few hours figuring the solution out. * The proc macros for `infer_schema` broke my IDE's `Jump to definition`, `Find Usages`, and intellisense. On top of that, painstakingly defining the schema AGAIN in code seemed frustrating. * Some of the attributes (like `column_name()`) actually didn't work in the version I was using (this was a while ago). If they did work, sometimes the combinations of attributes I needed it to have would conflict with one another for some reason. I actually still use the command line tool to manage database migrations. It's an amazing tool. And most of the problems I originally had with diesel have probably been fixed. I just found it frustrating to be fighting errors and bugs with little documentation for hours, since I realized I could write the actual query I wanted to run in less than a minute.
Ah, thank you! I forgot about this because I have a custom target that sets the linker flavor, so I don't do that step.
You can enable incremental linking on MSVC by passing`-C link-dead-code` to `rustc`. This does result in larger binaries though.
I... wasn't using a mutable reference. `self` has type `Foo` here, not `&amp;mut Foo`. Otherwise the whole pattern on the left hand side wouldn't match.
Yeah, orthogonal. This is intended for folks to clone and have everything set up for building and developing wasm projects ready to go. wasm-pack is for taking an existing project and publishing its wasm binary to npm.
Leaking on panic sounds bad if something is catching the panic so it isn’t causing the process to terminate.
That's a good observation and can greatly help to keep in mind when we are asking for something that seems ridiculous. Are we being allowed to do something that seems ridiculous here?
Hey, in the meantime don't forget you can use the `try_from` crate on stable!
As /u/burntsuhi said, [path\_abs](https://github.com/vitiral/path_abs) should handle your use case perfectly. It attempts to create an "absolute path" with zero sys calls \(although on windows it still requires one sys call to resolve the "root"\). If it doesn't fit your use case, please open a bug!
&gt;Accounting for the users internet connection isn't a programming languages responsibility. Well, to be fair, it would be really nice if there was a way to cache libraries you use often on disk, so you can just use them in new projects without having to re\-download them. Maybe that functionality already exists, but if that's the case, it should be better advertised. For instance, recently, I chose to use C for a small script where I needed to get random numbers because I didn't have access to the internet and as such couldn't have used the rand crate. That's admittedly a rare problem, but I imagine waiting to download common crates is something most would like to avoid. Now to be fair, C\+\+ and other language don't have a specific feature for that either since you just use your OS package manager to download any libraries you use \(and I suppose you could do the same with Rust crates\), but I think Rust should at least consider accounting for that issue since it supplies its own package manager.
Not really, most of the time is taken by LLVM when it optimizes the IR emitted by rustc. The problem is, that IR is huge because rustc doesn't attempt much to reduce it. If I recall correctly, part of the benefit of MIR was that it makes slimming the LLVM IR more feasible.
Thanks! Even though I don't know you, that means a lot.
Is there an advantage to using webpack over parcel?
The [Hawthorne effect](https://en.wikipedia.org/wiki/Hawthorne_effect).
I mean, `rustc` itself is a pretty sizable project. I'm sure the Rust devs don't enjoy waiting an hour on a clean build
The optimization I suggest needs to maintain the invariant that there's always somewhere to write the next element without checking first. So if you were to do this for Vec you'd have to either inline one element, or allocate the array right away.
I implemented [this warning](https://github.com/rust-lang/rust/pull/37946), but it was not merged and I got asked to write an RFC. I couldn't be bothered. Maybe you can help with RFC part? Implementation is basically done and easy to update.
hi there, i've pushed new sample codes and they are now working well with both SGX_MODE=HW and SGX_MODE=SW
It's a _multithreaded pool_, that's very different from sticking each request in a _separate thread_. What you said implies you have the same number of threads as requests, which is not true. `tokio` will be smarter and only make as many threads as would be efficient- some requests will be on the same thread as others.
I'd be happy to set RUST\_TELEMETRY=1 on my shell and send you all the data for every one of my compilations if you supported it!
Just started (experimenting) using rust at work (Orem) would love to go to a lunch meetup (nights are busy)
Thank you for the detailed response. I hope you do give us another look at some point. Our documentation has definitely been lacking in the past. However, as you may have seen, we spent *a lot* of time improving it leading up to the 1.0 release. `infer_schema!` has definitely caused a lot of problems in the past. We actually just deprecated it in the 1.3 release, as Diesel CLI now can automatically regenerate a schema file whenever you run migrations. Any case where `#[column_name]` etc doesn't work is definitely a bug that I'd love to have reported so we can fix it. However, it does sound like you were on a very old version where this was a known problem and has since been resolved. I'm glad you've found Diesel CLI useful. It's a tool we spend a lot of time on polishing. Thanks again for your detailed feedback, it's extremely helpful to hear things like this. :)
First things first, welcome! The most common thing I've seen in response to this question is to... Go learn something else first. Play with Python, Java, C#, then play with C/C++, then come back to Rust. We all love Rust here, but it's not a beginner's language. Can you learn it as a first language? Absolutely. But it will slow down how much you fast you can learn general programming skills. As a brand new programmer you typically don't care if your string is on the stack or the heap, or what errors are and why you always have to handle them. `let x = 2 + 2.5` is a compilation error in Rust. There is a very important reason this fails by default, but you probably don't care when you're in the rapid learning phase. Rust forces you to do it right and it means it will take you a lot longer to build something that 'works'. Unless you have an idea how to structure and troubleshoot programs you will likely get very frustrated, and frustration while trying to do something new is likely to end poorly. Learn how to program in something, learn basic memory management in c/c++, then come be confident in your programming with Rust :)
Correct.
This. Even making it so that RUST_TELEMETRY just stores the data and give us a `cargo telemetry` command would allow us to give the teams more data to work with while being minimally invasive. I for one would use it.
Go with python for machine learning and ai (its mostly same thing). Rust is nice language itself so it worth learning but python have better libraries for ML.
Webpack is more customizable. Which also makes it a bit harder to get started with.
Does it have some pros/cons comparing to [https://github.com/emk/heroku\-buildpack\-rust](https://github.com/emk/heroku-buildpack-rust) ?
It’s pretty standard behaviour. It’s good to avoid it if possible but it’s considered ok.
You can have a vector that has one element as part of the struct, which allows you to avoid the memory allocation on `Vec::new()` while still performing this optimization as /u/ssyivan said. I think that `Vec::new()` not allocating is something that's not worth breaking, but making `Vec` a bit thicker might be worth it, who knows.
As mentioned in the post, there were some changes to range sampling ([Relevant diff](https://github.com/rust-lang-nursery/rand/commit/230af65df6eee438f0f13b271d595eb135fb91b3#diff-153cafbbfe715a7ee14f4c683c2d1ca0)). I'm not sure how you could test that except to implement the changes yourself. (Implementing the 0.4.2 version with `rand = "0.5"` might be easier)
Cool. What about wasm-opt? Does it have to be installed? I wanted to integrate it in my project but seemed like too much work for not big gain.
This is a great article for reminding people that you don't need to prematurely optimise things, and that the std library and the basic implementation is usually the best due. Unless you are working with an algorithm that has a known data structure that makes it better, contiguous arrays/vectors win. I've come up against this w.r.t. sparse vs dense representations for relatively small data sets.
wasm-opt is necessary only to reduce the WASM binary size. I would say this is useful only when deploying to production but it’s good to have a concrete idea of the binary size before deployment. It must not be a strong factor but still, it’s easy to generate a potentially big binary of few mega bytes.
Currently that is: `Block(Constant(1), Add, Constant(2)), Multiply, Block(Constant(3), Add, Constant(4))`
From poking around with `ps` while `rustc` builds are running, I think it's actually `librustc` that's slow. Cargo just hangs at `syntax_ext` for some reason. (repro: look at the running processes when it appears to be compiling `syntax_ext`, they'll probably mostly have an argument like `--crate-name rustc`). This would make much more sense as well, as `librustc` is much larger, 101k LoC versus 7.5k LoC. Not to mention `librustc` is much more macro heavy. It can take my machine as long as a minute to go from single -&gt; multithreaded compilation after touching a file in `librustc`, which I believe indicates that macro expansion/parse is being Real Slow (since everything after parse is multithreaded across CGUs AFAIK). Admittedly this is a laptop so, not the fastest machine. I'd still like for it to go faster though =)
I think the idea was to have some kind of user-account-wide HTTP cache for cargo which could be well-documented (if not default) and set up with as little as a line or two added to `~/.cargo/config`.
Impressive work! I have been using resvg and associated crates a lot to test and make things related to [lyon](https://github.com/nical/lyon), and the experience has been great.
Yep. And the fact that the RFC has already passed into stable (with a lot of people being excited about it) shows that it's what many want. Mistakes happen, sure. But trying to revert an RFC immediately after it stabilizes seems less like a "hey guys, I got the data: we made an error" and more like a "fuck you, I'm very opinionated about this feature I don't like".
I understand you are giving a hypothetical solution, but even that one would damage the community. Rust should be rust; if rust is different for everyone, tutorials, code samples, and projects would all be very confusing.
I've been wanting this feature for months and have been following the process. It was anything but shadowy. I don't like to be harsh as I love Rust and it's welcoming community, but I need to be blunt - you're blaming everyone else for your own inaction instead of learning from it and moving on. Deal with the fact this is stabilized, and pay more attention in the future so you can give criticism when it is appropriate.
They are different things. WASM-pack is for building WASM packages and publishing them to NPM, parcel is for getting a set of packages and bundling them up for use on a web page.
Exactly, make the array point to a single elem to start, then point it to an actual allocation after the first append. On destruction, check that there's a real alloc before freeing.
May need integer type parameters a la C\+\+… I seem to recall that was proposed though?
Pro Tip: Never post on Reddit after a short night. This crate logs WASM panics to console.error, not to console.log! ;-)
Is there a way to make it use fastlink? [https://blogs.msdn.microsoft.com/vcblog/2016/10/05/faster\-c\-build\-cycle\-in\-vs\-15\-with\-debugfastlink/](https://blogs.msdn.microsoft.com/vcblog/2016/10/05/faster-c-build-cycle-in-vs-15-with-debugfastlink/)
&gt; Are we being asked to do something that seems ridiculous here? Oh, not at all! I just came to think of the phenomenon.
You can pass `-C link-arg=/debug:fastlink` to `rustc`.
this is a work of beauty!
Good luck, I hate working on other people's big libraries. Heck, I even hate working on mines :'(
What linker you use shouldn't have an effect on the performance of your resulting executable, afaik. The linker doesn't actually understand what the code is doing or modify any of it. That is the job of the compiler in the previous step. The linker just looks at symbol tables and relocations (placeholder addresses) for functions, and replaces them with the correct memory address where the function actually is located, to link the different functions together correctly into a single binary. That said, some linkers will automatically remove/delete functions that are not used anywhere, others will not, so it could have an effect on the size of your final executable. Also, I think (not sure) the linker is responsible for choosing the order in which to put things into your executable, so there might be a very minimal / negligible effect on performance from certain functions being closer together / further apart in memory. But don't quote me on this.
rustc itself may not be slow, but it can still ruin compile times, by generating too much llvm ir to process down the line. You can't really blame llvm for being slow, the time it spends is roughly proportional to the amount of IR it is given. For example: a simple for loop in Rust will generate [590 lines](https://godbolt.org/g/fH2chr) of IR, but a similar thing in C++ would only take [39 lines](https://godbolt.org/g/fN46WM). Which of the two will spend less time compiling in llvm? And if you go for a functional approach, you'll get even more code, [871 lines](https://godbolt.org/g/LzdVN9). Granted doing so in C++ will also produce [540 lines](https://godbolt.org/g/9iuwbX) of ir, but regular for loops are much more popular in C++.
You probably want [bindgen](https://github.com/rust-lang-nursery/rust-bindgen/blob/master/README.md). It has a [guide](https://rust-lang-nursery.github.io/rust-bindgen/). See also the [ffi](https://doc.rust-lang.org/book/ffi.html) part of the book. 
In my experience bindgen does not always generate nice bindings, sometimes making handwritten bindings preferable.
I think it would be nice for some guarantee to be provided somewhere in the official docs that you can safely transmute between `struct A { /* ... */ }`, `struct B(A)`, `struct C { inner: A }`, `enum D { One(A) }`, and `enum E { One { inner: A } }`. And maybe also `[A; 1]`.
&gt; Also, I think (not sure) the linker is responsible for choosing the order in which to put things into your executable, so there might be a very minimal / negligible effect on performance from certain functions being closer together / further apart in memory. But don't quote me on this. Yes, that's the aspect that I meant, grouping hot code together in pages reduces pagefaults, and putting code on pages in the order that it is executed reduces startup time, afaik.. IIRC someone once said that the pascal/borland linker did (at least) the latter, to get fast startup time..
That's probably because you are still on the old design.
In this case, all variants are proper rust. We are talking about user preferences. I don't mind or care that someone might have a different preference and I think automatic conversion does not cause any trouble in this case. Other than wierd diffs, if your repo doesn't have its own enforced style:)
&gt; Johannesburg :D Did you choose that one on purpose to be ironic/fun?
How would I set up continuous integration for a Cargo project on a Raspberry Pi? I have a new Raspberry Pi 3 running with SSH and VNC access.
Oh, I was referring to webpack[webpack](https://webpack.js.org/), not [wasm-pack](https://github.com/ashleygwilliams/wasm-pack). Webpack and Parcel do similar things, I was just curious if one offers advantages over the other for Rust + WebAssembly bundling
No, the whole topic is about extracting values from a `Drop` type that you own. The function the blog post is looking at is this fn take(self) -&gt; (A, B) { (self.a, self.b) } Which takes ownership of `self` and manually destructures it.
^The linked tweet was tweeted by [@mcypark](https://twitter.com/mcypark) on May 24, 2018 05:19:06 UTC (0 Retweets | 13 Favorites) ------------------------------------------------- I wish that one day I'll write a C++ library that produces error messages at the quality of the [@rustlang ](https://twitter.com/rustlang ) compiler. Well, maybe that's too ambitious. I think I just want to, one day, produce errors where users are pleasantly surprised. \#cpp ------------------------------------------------- ^^• Beep boop I'm a bot • Find out more about me at /r/tweettranscriberbot/ •
That's basically what structural typing is, if I'm not wrong. I can't imagine Rust will ever have something quite that permissive, though. I think it's been discussed and shot down before but I can't remember the reasoning.
Do they need compiler or is interpreter fine? In other words, is performance important? 
&gt; 1 hour &gt; Clean Build I wish 😭😭
This is a cool pattern. What would the `Any` version look like? `HashMap&lt;&amp;'static str, Box&lt;Any&gt;&gt;`?
Thank you, KillTheMule, for voting on tweettranscriberbot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
I don't mean being able to convert between them in safe code, just that you can transmute between them in unsafe code and know that that operation is defined to work and will not break. Those conversions currently work, I can't see a reason to break them, and I can see plenty of reasons not to break them, so how is that permissive? Or is that just because I was not clear?
If you're really heartset on learning rust right away, [Rust by example](https://doc.rust-lang.org/stable/rust-by-example/) can bring you up to speed fairly quickly. It'd probably be in your best interest to learn something else first though, since you Rust can be a frustrating language. If you want Artificial Intelligence or Machine Learning, you might as well try python. 
Sorry, I misread that you wanted the compiler to provide safe conversions for theoretically compatible types. That would be structural typing which I believe has been shot down before. There's a ticket open for elaborating on what can and shouldn't be done with `transmute` but it's been sitting for three years (one year on the linked ticket and at least one more on its predecessor): https://github.com/rust-lang-nursery/nomicon/issues/6
rls\-vscode, which seems like official one.
Best bot.
Argh... good morning! I think webpack doesn't confuse people. Most people use it. The binding features of parcel are now mostly handled through other means, I think?
In a C++ code base I used to work on, we had a data file that we'd use to generate enums and "traits" (structs templated / specialized on the enum / constant value). Haven't thought how I might do that in Rust.
I think I'm missing the joke...:)
I waited 32 years for a decent programming language... it's here :\) \- The Rust Programming Language 
The deepest gold mine on Earth is near Johannesburg. There was [a great documentary](https://www.youtube.com/watch?v=rqv9u4-nUHg) about it.
There is a talk about that, I think on Rust's YouTube channel. Basically, you do this: - Wrap the C API 1-to-1 to Rust, e.g. using `bindgen`. Typically such code is put inside a crate or module called `&lt;lib&gt;_sys`. - When calling C API functions returning booleans, convert them by hand to Rust types with `!= 0`. The reason being that Rust and C handle them differently. On Rust, only the lowest bit of a boolean is checked, so `3 as bool` is `true` and `4 as bool` is `false`. In C, however, everything `!= 0` is `true`. Why? Probably because LLVM and `i1`. - Put a `catch_unwind` in Rust functions being called from FFI code *(callbacks)*, or abort on panics. - Design your nicer Rust-API around the created `&lt;lib&gt;_sys` module/crate. - In case you need C type definitions, such as `c_long`, you can use the crate `cty`. It supports `#![no_std]` and does not provide any C function declarations. Just the basic types.
Is there a general way to get the amount of bytes of a primitive type? Of course, it is obvious how many bytes a `i64` and so on has, but it does not feel "clean" to hardcode this, especially if you switch `i32`to `u64` in the future for some reason.
As long as it is opt in +1 to this idea.
YES
I thought runtime size was what this picture was attempting to illustrate.
If this is a parametrised query, meaning that $1 references the parameter array you give a second argument, try removing the quotes around the $1: PostgreSQL will know its a string and do whatever escaping needs to be done by itself (actually the answer is that if you do a parametric query, it first compiles the query without the string, then execute with the string as argument, which means that security wise quoting is unnecessary).
I never used `error-chain` myself, and with the existence of `failure`, I don't see myself ever using it. In terms of some concrete practical benefits, `failure` is nice because it provides backtraces and a mostly convenience trait object for all types of errors (`failure::Error`), which is nice in CLI applications. I am still unsure of whether `failure` should be used in libraries. My answer is "probably."
&gt; SGX_MODE=SW make It works now, thanks!
wrong sub
Not sure how useful this might be because the keys are constants. It's basically a really high overhead struct. :p
I failed to include assembly... i also failed to tell it's a windows binary... however, compiled \(optimized\) on windows 10, these figure are prettu accurate...
Thanks a lot! I waited for your comment.
I like failure. If you use their `Error` or `Context` object you can supply context, and also supply the underlying cause. If you don't want trait objects, you can just implement `Fail`, and not supply cause or backtrack. Failure gives you the tools to decide how heavy you want your errors to be. The actual size is just 2 pointers, so won't affect the size of your `Result`.
Depending on what you're doing, it might be easier to work with BitVec
Ah ok - Joburg started as a mining town, so I’m so used to it that it never even occurred to me:)
No problem! Anybody who has further problem pls feel free to directly contact rustsgx@gmail.com - I'll respond asap :-D
I had no clue about that boolean behavior in Rust... I just have no reason to play fast and loose with type punning or type casting given Rust's solid type system.
Also, I have tried using `failure` in a real (but niche/low-risk) library to get experience with it: https://docs.rs/imdb-index/0.1.0/imdb_index/struct.Error.html The pattern largely follows the above, but you impl `Fail` instead of `Error`, and make some state that was explicit in the error type, implicit. For example, the `Io` variant is now just a tag, instead of containing the `io::Error` itself. Instead, if callers want that, they need to iterate over the causes of the error and downcast it out.
The idea behind a dynamically typed value HashMap is that a user of a library can insert data into the hashmap the library owns and later on retrieve it again, without the library itself needing to know all possible types the user may provide.
Fun to see this! I did my updated snowflake hash while at Recurse Center last fall, and used JavaScript mostly because I wanted to make an [interactive visualization](http://levien.com/snowflake-explain.html). I half-finished it, maybe some day I'll get back to it. In any case, happy to answer any questions.
I'll preface by saying I have never personally used simd, but my understanding is that those instructions relate to the filling special simd registers with those values, so there is no benefit to making `const` versions of them as one way or another they would have to be put into those registers.
The first way I can think of off the top of my head is to start with an iterator of ints, and then `map` each element to a float using whatever formula. I think that'll lead to different rounding errors than if you e.g. repeatedly added a float to itself. But maybe those tricky details are part of why there's no native floating point range?
Their process is more like the tower of babel..
Right, that's a really good use case actually. I could see this being used in request/middleware stacks to pass data along.
No. The blue circle is made using blur \(aka filters\), which aren't implemented yet.
We have thought about this in passing, but have always been very concerned that even if it’s opt out, people would get upset.
I'd say nothing stops you from having variants containing wrapped errors. I do think you get more type safety that way, but the context method allows you to change the wrapped error types without it being a breaking change.
Furthermore, and as a more minor point, it's completely unnecessary to backslash-escape a single quote in a double-quoted string.
Oh, I misread your comment! I see what you're saying now.
My projects have always been small so far, so I haven't had the need to really use an error library. I tend to keep it simple (even simpler than the already simple solution from burntsushi in this post).
&gt; most critical / quality-of-life features (that were missing from 1.0) are done by now We'll have to agree to disagree. It really depends what you try to get out of the language; for me: - the ability to use and manipulate arrays of any size (and build complex data-structures parameterized by sizes) is very much necessary. - the ability to pre-compute large swaths of data at compile-time, in a way that the optimizer can fully compute a number of queries on this data, leads to real nice improvements; it may even lead to completely remove the actual data from the final binary (all queries have been fully solved at compile-time). The former means value generics and the latter means `const fn` and increased support for `const` types (ideally, I'd like to put a `BTreeMap` in there!) and automatic `const` evaluation at optimization time (if the function is `const`, and the arguments are `const`, then just compute it). And unfortunately, those cannot be emulated with libraries. At least I cannot think of any convenient way to do so! Next to that, I have little to no use for async/futures or embedding =&gt; what's critical very much depends on the use case. 
I would have if it worked, but it doesn't when dealing with paths. See: https://github.com/rust-lang-nursery/failure/issues/183
I just had an idea. To avoid collisions it would be nice if the keys generated their own unique id. No need to provide a string for it. You just use the constant variable. The IDs could probably use a lazy_static AtomicUsize with fetch_add to each get a unique number. pub const HOME: Key&lt;PathBuf&gt; = Key::unique();
So awesome seeing this pinned! Thanks everybody
I'm certainly looking forward to that. This optimization has been blocked on LLVM for so long, it'd be great if it didn't turn out to be a dud!
Correct me if I'm wrong, but to me it seems like the initial LLVM compile (as in when doing an overall "x.py" build of everything) leaves a lot of unnecessary default options enabled in the CMake config. Does the Rust build process actually need all/many/any of the LLVM tool executables? (For example llvm-ar, llvm-as, llvm-bcanalyzer, llvm-cat, e.t.c.) It doesn't seem like it would. If not, wouldn't it be a better idea to set LLVM_BUILD_TOOLS to false by default, and only build the necessary library archives for linking against? 
Right. So which is it? 1. Either the implicit context or the embedded error approach are valid and equally well supported. 2. Context handling is the preferred method of dealing with errors that add additional context to other errors. 3. Context handling is too implicit and folks should prefer embedding error types explicitly. My read of the `failure` documentation is that the intended answer is around (2), but I don't know for sure. I have a rambly issue around the process I went through: https://github.com/rust-lang-nursery/failure/issues/189
Yeah I don't really know. I just know that each approach is possible.
Ah, yup. "I don't know" is exactly the state I'm in. :-)
I tried this, the code itself works in that it doesn't panic but I'm getting nothing from the database with this query. I know the syntax of the SQL is fine because I get the expected result when entering it manually from the Postgres console. This indicates the bug is elsewhere in my code so I'm going to spend the evening sweating over lldb! Cheers for the advice!
[As for the ditching of floating points for ranges due to floating point error, you’d be correct.](https://github.com/rust-lang/rust/issues/17010) 
Sorry for not having any documentation yet besides the example. I just made it work today so I wanted to get some feedback from Reddit before proceeding with polishing it into a first version.
Woohoo! Lifetime elision is already handy enough, but having it available for this will be nice.
[removed]
I was going to suggest that long builds have a plug for the RUST_TELEMETRY functionality if it isn't explicitly defined. Users can set it to false to hide the plug. But I guess this still get people upset somehow. 
Try https://learning-rust.github.io/ . Rust is not something super hard to learn. The issue is most learning materials currently we have still targeting experienced system programmers. 
TL;DR - Pretty sure. But use cases can dictate otherwise. Nonexhaustive errors are probably a decent default. &gt; But I think that adding a new error variant should be a breaking change for people who want to handle all your error variants. The whole point is that the enum is not exhaustive, and that there is specifically no guaranteed way of doing an exhaustive match. Therefore, you're forced to handle a default catchall case (unless you specifically opt into using the undocumented `__Nonexhaustive` variant, in which case, the trade off you're making is very clear). &gt; Say I want to use your library and handle all the ways in which it can fail specifically. There should be very specific use cases that motivate this. If there are good reasons to specifically want to handle not only each error individually but exhaustively, then the person doing the API design work for the library should take that into account and weigh it against the alternatives. &gt; but I think many Rust crates are too reluctant to bump up their major version number. I'd like to note that you've turned a simple API question into a philosophical stance on how version numbers are handled. If I took your bait, the complexity of this conversation would increase by an order of magnitude. :-) For every person that considers "bumping major versions frequently" to be an obvious thing to do, there is at least someone else that considers the *exact opposite* to be the obvious thing to do. It is a contentious issue on which reasonable people can disagree, and moreover, can disagree in subtle and different ways depending on the specific circumstance. Trivially, crates that are commonly used as public dependencies have a **wildly** different calculus for determining when to do a major bump than literally any other crate.
The README is very interesting, but also leaves me wondering: what is the primary, motivating use case? 
My proposal is opt-in, not opt-out.
This is pretty much exactly the approach I arrived at for one of my projects - I had to take a break before getting to concrete implementation, but I have been eyeing `PhantomData`... This makes it easier for when I get back to it; thanks for sharing!
Sure!
If you’re interested, have a look at my [any-cache](https://crates.io/crates/any-cache) crate. A nice example of use case is [warmy](https://crates.io/crates/warmy), in which I use typed keys. However, my keys are a bit more typed than the ones you provide in your crate – `any-cache` gives you any kind of key to any kind of value.
At that point it seems like you're investing increasing amounts of effort to asymptotically approach a typemap.
Sorry that’s exactly what I meant. This kind of subtle difference and possible confusion in messaging is the issue.
&gt; placement new You should check out the boxext crate. https://crates.io/crates/boxext It has a way to get *most likely* placement new semantics. It doesn't guarantee it, so it still fails in debug, but in release I believe it should work.
It would crash neovim if you tried to complete on the same line as a const keyword appeared. I filed an issue and it was never responded to, and updated a bunch of times to no avail. It really affected my workflow, then racer started to sit at 100% cpu on large files, at which point I abandoned ship.
It seems unfortunate that you need to store the actual key name, for a given `Key` this is always fixed. Can something like this serve the same purpose? // Implement this type for a unit struct to define a strongly typed key trait Key { // The value type associated for this key type Type; // They name of the key const KEY_NAME: &amp;'static str; } // Key named "Foo" with associated type u32 struct Foo; impl Key for Foo { type Type = u32; const KEY_NAME: &amp;'static str = "Foo"; } // Gets the value from some keyvalue store // No key is actually passed because K is a unit struct, very efficient fn get_value&lt;K: Key&gt;(key: K) -&gt; K::Type { unimplemented!() } 
Don't hesitate to ping me on them, I love working on error messages. :) (maybe we should create a specific team for error messages...)
You're welcome. ;)
Will this be recorded? I'm always interested in what's happening with Chalk.
I think whether you want to use `Error`, `error-chain`, or `failure` really depends on whether your errors are just simple enums without much extra information, or if they need to be linked to some underlying cause like an IO error. If all your errors are doing is implementing a basic enum for a couple of different kinds of errors, possibly with a small amount of extra data like one or two integers or pointers, then there is no reason to use `error-chain` or `failure`. You should always at least derive `Debug` and implement `Display` for error types. If you are not going to be using `failure`, then you should implement `std::error::Error` manually, which should be pretty simple; this will allow applications using your library who are using `error-chain` or `failure` to use your errors as a cause for their higher level errors. Size guidelines and boxing really depend on how these errors are going to be returned or used. If it's something that will be used in a tight loop and unlikely to be reported to the user but instead the calling code just needs to dispatch on the type of error, then you likely want to make it as minimal as possible, just a bare enum if possible, or an enum with as little associated data as is needed to properly be able to deal with the errors. If it is something that will need to be reported to the user, heavier weight error types are appropriate, since the cost of constructing the error is likely dwarfed by the cost of reporting it. If it's something that may indicate a programming error or for which you need to be able to trace from some lower level IO error, to the higher level logical operation in your library that was related to it, all the way up to the application level logic that triggered that, then using `failure` is the most appropriate. In summary: 1. If it is a small, simple error, just use an enum, possibly with a small amount of extra data. Implement `std::error::Error` yourself. 2. If it's more complex, and needs to be associated with an underlying cause, the it's probably a good idea to use `failure`. a. If it is expected that your errors will be raised often, or bad for it to be expensive to create an error, you probably want to implement `Fail` yourself rather than deriving it to avoid all of the backtrace machinery b. If it's expected that your errors will be rare and might indicate programmer error or need detailed explanation, then `#[derive(Fail)]`
Yep.
Maybe opt-in by `cargo install cargo-telemetry`?
rust = quality :\)
As they should be! The offending piece of code is clearly shown, and a solution is often provided. In my opinion, they're a major part why I don't feel like I'm "fighting the compiler" when I'm writing rust. 
The messages are so good so often that it's jarring when you get a message that isn't helpful and you have to think about the code instead of the compiler saying "You did this. You meant to do *this*. Look, I'll just make the change for you while you get a cup of coffee or something".
Well, in the case of `std::error`, making the `Error` enum non-exhaustive is indeed clearly the right choice, but only because every single function in `std::io` returns the exact same error enum. I wouldn't go so far as to call that a "mistake" but it's certainly not how I would design an API, because it leads to variants like [`WriteZero`](https://github.com/rust-lang/rust/blob/8ccab7eed5f4fc93500fbf242e575073ca70d7cb/src/libstd/io/error.rs#L165) that are completely nonsensical for some operations. In my opinion, each operation should return an error type with only variants that make sense for that operation, but I realize that I'm probably in the minority here, because it usually means you have to handle errors more pretty explicitly (such as by pattern matching) instead of just using the `?` operator everywhere without thinking much about it. It's a tradeoff between ease of use versus higher confidence in the correctness of your code, much like dynamic versus static typing.
The recommended stable way for dealing with constants and SIMD today would be through unions, for example the above constants can be defined like so -- https://play.rust-lang.org/?gist=b7ac5698ab5d5f4ebde80319e7800a94&amp;version=beta&amp;mode=debug
Just use io::Result until there is a specific reason not to. When you have that reason, then you'll know if failure is a good solution.
Oh! that is excellent! For other reasons as well. So type punning in Rust is ok? In C it works usually but is warned as potentially unsafe. 
Link is broken for me. Removing the trailing slash fixes it.
At a glance I don't understand how to utilize this? Does this require annotations? I tried the Getting Started guide and that was no help. I found a gif with a Java example, and it *appears* to not require annotations? At least in that example.
Well that depends on how the entire thing is jiggered. 1. your Rust code calls Python, so you're embedding the cpython runtime in Rust, through the cpython or pyo3 crate. In that case you'd be getting a Python object which you can [call](http://dgrunwald.github.io/rust-cpython/doc/cpython/trait.ObjectProtocol.html#method.call), and whose result you can convert back to rust if necessary 2. your Python code calls Rust, the Rust code knows about Python, same as above except you've built a module, Python gives a Callable object to your module, you call it and deal with the result 3. your Python code calls Rust, but Rust is unaware of Python. In that case you're supposed to expose a C-level interface which Python can access through cffi or ctypes. They'll have to pass around a function pointer [cffi side](http://cffi.readthedocs.io/en/latest/using.html#extern-python-new-style-callbacks), [Rust side](https://doc.rust-lang.org/1.9.0/book/ffi.html#targeting-callbacks-to-rust-objects) ([see also](https://stackoverflow.com/a/26118378/8182118))
This is an issue I'm surprised about as well given it is a feature that I regularly use. Maybe I'll write a post on it, haha!
&gt; In my opinion, each operation should return an error type with only variants that make sense for that operation, but I realize that I'm probably in the minority here, because it usually means you have to handle errors more explicitly than just using the ? operator everywhere without thinking much about it. Yeah I would basically never do this unless there was a compelling use case for it. The juice ain't worth the squeeze.
The key problem with this approach is that if you're dealing with the file system, none of your errors will include file paths, which are _essential_ for CLI applications intended for end users.
Hooray! Where would I be able to find the video once it's uploaded?
To render SVG files? Can you clarify your question? 
Speaking of error messages and gtk-rs, when trying to get it working on windows it would always give a runtime exit code of some huge negative number. Its been reported and closed on the issues board with various solutions, but none of them worked for me. Any idea what I can do?
If you add Debug as a bound for Trait on the trait definition itself, what you are trying to accomplish will work: use std::fmt::Debug; trait Trait: Debug {} #[derive(Debug)] struct Struct; impl Trait for Struct {} fn print_trait_object(trait_object: Box&lt;Trait&gt;) { println!("{:?}", trait_object); } fn main() { let trait_object = Box::new(Struct); print_trait_object(trait_object); }
I don't see it. Here is the visual diff: [https://imgur.com/S397DJc](https://imgur.com/S397DJc)
The meetup page will be updated with the link. I also usually post it to this subreddit.
Type inference takes time, but it shouldn't take much *more* time over plain type *checking* which has to happen regardless- they are very similar processes. In simpler cases like your example, I honestly wouldn't be surprised if type inference were faster to compiler, because it doesn't have to compare the computed type to the user-specified type.
Why using a macro to construct the key and not simply a fn new? I don't get the advantage of the macro here.
For good ergonomics, keys should be declared as constants. Ideally this should be done with the help of `pub const fn new`, but `const fn` is unstable, so this is hidden behind a `nightly` feature flag, and a macro is used as a workaround that works for stable: https://github.com/matklad/typed_key/blob/68c35639a1958fceb1c3e1ed17e6afdff4429ca0/src/lib.rs#L82.
Ha, OK, static initialization, thanks for the quick reply.
Oh wow, neat! Had no idea it was actually a thing.
I wrote my first -sys create just a couple of days ago (not on github yet though), but I didn't really line the advice of "just having the raw ffi bindings in there" since several structs were quite big, and there were a lot of status codes and such. So what I did was to not touch the bindgen-generated FFI and only write "helper functions" such as constructors for the structs, From traits, and wrappers which returned the enums instead of status codes. Still very unsafe, but it made the code a ton cleaner in the other crate where I try to convert it to idiomatic rust. 
Had no idea this was possible, thanks very much.
&gt; Raph Levien, talking about GUI development with Rust via ECS Anyone know what ECS is? Google is not helping. Thanks.
https://raphlinus.github.io/personal/2018/05/08/ecs-ui.html This will probably be the the primary focus. It was posted here a few weeks ago. 
It targets 1.21. I think that it’s mentioned briefly. I’m on mobile or I’d check. Given that the new crate keyword for the edition, many more people will know about it regardless.
I see now that it only show up when I choose "Fit to View". The QtSvg rendering, however, doesn't show it in either mode. The height of the rectangle is dependent on the size of the window. It starts at the bottom of the window and moves up the image as I increase the window size. It manifests itself as making that portion of the camera appear to be matte as opposed to semi-glossy.
Thanks
For a fair mini comparison you shouldn't have used a range in the first example, after all ranges are implemented in Rust code which makes it obvious there will be more code generated than a "primitive loop". Implementing the same code as in the C++ example would result in [61 lines](https://godbolt.org/g/Jc5KR4) of IR and not 590 lines.
That's a very good idea. The compiler can already log runtimes – we'd just need a way for `cargo telemetry` to get them into an archive format and then to send them to a central server.
Windows is... complicated. In my case, I always put dll files into my project folder. Suddenly it just works! :D
Speaking as someone who has never used bindgen, but I would personally still use bindgen unless the API is very small. Instead of had coding the bindings, I'd wrap the low level bindings in a nicer API
[Froggy](https://docs.rs/froggy/0.4.4/froggy/) could be interesting to use for a GUI.
I gave $10 to the fsf.
Interesting. I might just not be in `error-chain`'s target audience then, but I look forward to whatever you come up with. :-) I wonder if I should just pick a pattern I'm happy with that uses `failure` and launch full steam ahead.