Do you know what that might mean?
What is it about SSDs that is relevant to think about for this application?
Well, the error handling principle is the same: Not throwing exceptions, but returning errors. The difference comes from Rust having generics and macros, which make error handling both safer (Go s error type is basically "Box&lt;Any&lt;Error&gt;&gt;") and more convenient (try!, ?, functional approaches).
NLL will make your simplified example work. Or you can clone today: if let Some(existing) = self.records.get(&amp;id).clone() { let derived : Attributes = derive_new(&amp;existing.attributes); let record_key = self.add_rec(Record::new(derived)); record_key } Lexically scoped lifetimes are more of a natural fit for functional-style code. let record_key_res = { self.records.get(&amp;id) .map(|existing| derive_new(&amp;existing.attributes) ) } .map(|derived| { self.add_rec(Record::new(derived)) } ) .ok_or("No such BPID!"); 
&gt; it’s still much faster than the likes of Java, C#, JavaScript, Python and so on. [mfw](https://benchmarksgame.alioth.debian.org/u64q/compare.php?lang=csharpcore&amp;lang2=go) [also](https://benchmarksgame.alioth.debian.org/u64q/go.html) &gt;Hello world server example [Here it is in Rust with more sane error handling](https://play.rust-lang.org/?gist=0a7b7e8c3b7e5d7222b7ea0845b3718c&amp;version=stable). I won't pretend I'm a Rust ninja, but I feel like mine is a lot more readable and concise. When I tried to run the Go code in the post, it just silently died a bunch of times before I realized I forgot to turn off the Rust version and they were on the same port, which just goes to show that part of the added verbosity is in making sure your code isn't terrible. &gt;Here is the corresponding example from above done in Go: ``` func fooDivide(a float32, b float32) (float32, error) { if b == 0 { return 0, errors.New("divide by zero error!") } return a / b, nil } func main() { result, err := fooDivide(5, 4) if err != nil { log.Printf("an error occurred: %v", err) } else { log.Printf("The answer is: 5 / 4 = %f", result) } } ``` That just looks like the terrible `function(err, result) { ... }` pattern from callback-style javascript.
This is far from a usable driver. &gt; OpenCL 2.0 compatible kernel language support with OpenCL 1.2 compatible runtime &gt; Supports offline ahead of time compilation today; during the Beta phase we will add in-process/in-memory compilation &gt; The ROCm platform relies on a few closed source components to provide legacy functionality like HSAIL finalization and debugging/profiling support. These components are only available through the ROCm repositories, and will either be deprecated or become open source components in the future. (last part from [install instructions](https://rocm.github.io/install.html)) and currently requires [kernel modifications](https://github.com/RadeonOpenCompute/ROCK-Kernel-Driver/tree/roc-1.6.0#package-contents). It's a total mess currently.
Ah, I see. I wouldn't call that so much a *bug* - `WaitGroup` is working as designed - as poor ergonomics. You need to ensure that the main function of each thread calls `.done` and that the main thread `.join().unwrap()` everything. A `defer` function like Go has would be helpful. I dunno why stdlib doesn't have one, but it's [easy enough to write your own](https://play.rust-lang.org/?gist=0c0b86f435d1a261a56dbb6ae9eafc34&amp;version=undefined). (Public domain or Unlicense - I doubt that something so simple is copyrightable in the first place.)
I remember getting some error like that due to unsigned integer overflow below 0, causing a requested capacity to become an unreasonably large integer. This was in release mode though, so no debug overflow assertions.
All your variables (including the BTreeMap) are stored in the Random Access Memory (RAM), not on your hard drive. You probably don't have 384 Gb of RAM. Have a look at the amount of RAM you have: https://www.computerhope.com/issues/ch000149.htm If you need to store more data, keep it on the hard drive but beware you will not be able to access it all at once: https://rustbyexample.com/std_misc/file.html
[removed]
This is especially useful with larger, more intricate types. Using `Self::Item` lets you not repeat yourself. Also, any changes to `Item` automatically get propagated.
Having Cargo resort to cache **if you don't have reliable connection** would be great. Or an `--offline` switch, like the one Gradle has.
Thanks! I've actually made it work. I've updated the git repo with a solution. Few questions: 1. Why we need two Box'es? 1. Why we need RefCell? 1. Why do we need to specify the type explicitly? E.g. `Box&lt;RefCell&lt;Box&lt;FnMut(&amp;Painter)&gt;&gt;&gt;`. When I'm removing it - applications segfaults.
384G is quite a reasonable amount of memory to have in a server, It's a normal configuration with 4 banks of 6 DIMMs. With 16G DIMMs that gives OP's figure.
I think the author is missing a few points. Apart from being dead simple, Go wins by compiling *very* fast, making the edit-compile-test cycle quite short. This makes up for the rather weak type system, because tests can be added and run rather quickly. Rust wins (apart from being both safe and fast) by moving complexity into the language that would otherwise plague the programs, and by keeping knowledge about the code local, sometimes even at the expense of brevity.
One explanation is if you're program is compiled as 32 bit, which is unlikely but on many systems limits memory to 2GB max and will start failing well before then due to fragmentation. I doubt that's the case but it should be easy to check the executable. How to check depends on the OS. A bug may lead to a very large allocation, Noctune mentioned an integer underflow could cause that. But safe Rust has some strong protections against that in Debug mode. Try running in Debug mode (ie not release). If you're on nightly you should try stable or a different nightly if that's not an option, since there's could be a bug in nightly. Are you running the executable somewhere that might have a memory limit (within a VM, some kind of container) Do you have some unsafe code? Errors there can fail in very unexpected ways. I've run Rust programs using 100GB+ of memory (vecs of btrees in my case), on a large Linux machine with 384GB of ram. So it's not a limit of Rust nor it's default allocator.
Cool, that makes it more palpable to me 🙂
in the README, "contributting" should be "contributing" I can't wait to see how this project develops!
Are you using any custom allocators (eg. an arena), or a dependency that might? Your program might be running in a container that artificially limits it's memory. It might also be about some other memory than your programs. If it's doing any kernel-level stuff, you might be exhausing memory of something else within a kernel, which leads to `-ENOMEM` that translates to this message.
Not using a 32 bit toolchain by any chance?
I had the same experience with nalgebra. The person I talked to in #rust-gamedev on IRC said it was "obvious", but as someone who doesn't actually know the math and was just struggling to copy the formula from an opengl tutorial it was almost 100% opaque.
How is this different from https://crates.io/crates/collision ?
For development though? 
Supercomputer with 384 GB on a standard node. There are a few 2 TB nodes but the job queue is long. Turns out it can finish when running on one of those, and UGE reports its maximum occupancy was 20G. The mystery deepens.
That's a good thought. I've been bitten by overflow before, so I build with debug assertions enbaled in release mode. Also, every place I'm calling a .reserve or ::with_capacity on a container, the argument is coming directly from a .len on another container. While looking for these kinds of things, though, I noticed the Vecs that are the map values are collected from an iterator that might hypothetically iterate much more than expected for some input. This happens before the assertion that it's &lt; 5. We'll see if .take(6) before .collect has any effect. I'm thinking at this point it's that collect rather than the map insert that's causing the offending allocation.
Rust is a language that includes stability as a feature. I expect that all books written about Rust for a general audience (e.g. not about some specialised niches) should have a shelf life of at least 10 years.
Is this Keras for Rust? If so, woohoo!
True. My bad then. :-)
That might be it. The iterator could have a very high `size_hint` lower bound, causing a single huge allocation instead of slowly growing.
It would be META if you contributed that as a patch. 
Besides having routines to handle physics, something that collision does not do, MGF does collision detection a little differently than most other libraries, using exact continuous algorithms instead of approximate GJK algorithms for moving collisions. Additionally, as far as I'm aware, MGF is the only publicly available source code for accurate moving capsule collisions. 
Definitely not 32-bit or a VM limit; I run larger programs pretty routinely. Nothing unsafe inside the loop that's failing. If it's a bug in Rust, its present in nightlies from April and August. Refactoring to remove nightly features would be painful and not at all guaranteed to address the problem. The latest nightly has incompatible changes to TryFrom that'll take awhile to work out.
Seemed like a sure thing, but no joy.
Great work, and thanks for sharing! That note about borrow checker issue with glium seems completely off the mark to me.
Do you do RNNs? Because, I think this can be a huge blocker? https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h#L1002 Or did you find some way around it?
Keep in mind, that you train in python and deploy TF model to prod in almost any language, which has basic set of bindings (loading of graph works in low level rust bindings, java bindings, haskell bindings, ...)
Well, they do work similarly. I got the impression that he is comparing boilerplate. I think that Rust has lower boilerplate. And most importantly, the boilerplate isn't mixed with other code.
strace may be able to give you a hint. All memory comes from the kernel through system calls so seeing what system call failed is useful. Here's an example command line using "rg": strace -ff -o /tmp/rgtrace rg fred Downloads/ less /tmp/rgtrace* Look for failed mmap, brk, sbrk calls.
What'd you use serde for?
To add to this question, what are the differences and similarities to nphysics and ncollide?
Thanks for the quick answer. I checked in my O'Reilly profile and redownloaded it. :-) Usually, my former ebooks were synced to Dropbox automatically but this time the sync stopped with the 8th early release in January. With manual download I also got the 10th. Yeah. Pretty big pdf / ebook with more than 80 MB each.
When I read the title, I was scared, because I'm working on something with a similar title, but from a different approach. How did you implement the "growing" part of your plants? Is it just random?
Have you thought about how bw-trees might perform on persistent memory? IIRC latency is expected to be 2-5x higher than DRAM, so we can't assume current in-memory DB designs will be appropriate. However, cache-line read/write is the same as DRAM, vs page-based NVMe. 
&gt; 1. Why we need two Box'es? One for creating a trait object (two pointers, one to the struct and one to the vtable), another for having a single pointer that can be passed around as such in C &gt; 2. Why we need RefCell? In this specific case it's a FnMut. You need some kind of mutability then, which RefCell provides you then. Whether FnMut or Fn is the right choice depends on the semantics of your callback. If it is guaranteed to be called at most once at a time (never simultaneous from multiple threads at the same time, you can use FnMut &gt; 3. Why do we need to specify the type explicitly? E.g. Box&lt;RefCell&lt;Box&lt;FnMut(&amp;Painter)&gt;&gt;&gt;. When I'm removing it - applications segfaults. I thought that is should be deduced automatically. If you don't specify it, the inner box will become not a trait object but a box of the actual closure type you passed in (which is unnameable). At a later point, a pointer to that is converted to a box with the trait object and tried to be called/dropped, which then crashes due to the type mismatch (trait object vs. box containing the concrete closure type) &gt; 4. What about overhead? Which overhead? Each piece has some overhead. You can get rid of the overhead of the RefCell by using some more unsafe pointers, but that should be minimal anyway. Measure it if you think it matters :) The other pieces are required as-is, you can't get around them. &gt; 5. How safe is it? How easy can I break it down from the user/safe side? Or is it completely safe outside the implementation? It's safe as long as the semantics of your callback are the same as what the Rust types stand for. E.g. what I wrote above for the FnMut vs. Fn, or the other trait bounds on the closure (Send, Sync, 'static, etc). When wrapping an unsafe C API in safe Rust, you need to make sure that all invariants of the safe Rust code are kept intact. If you do that, it's completely safe (if we ignore bugs in the C code).
Can you please submit the IPFS backend to upstream cargo? I'd love that!
&gt; Only place where it seems weird, is when using glium. Since most of the types there are actually handles to some OpenGL objects, the mutability and borrowing isn't that clear any more. For example, if you assign a texture to FBO, and draw something to it, the texture doesn't get mutably borrowed, while it actually should be. This sounds like a mixing of value mutability and reference mutability. A `&amp;mut T` can have its value changed, and a `&amp;T` can't. However, if `T` uses interior mutability (e.g. `Cell`), then you can "mutate" an immutable reference (although the value behind the reference is the same). This is probably more likely to come up in APIs like this due to them wrapping non-Rusty APIs that are more "exotic".
LoL. I completely missed that. 
Well, nphysics/ncollide is based on nalgebra, while mgf and collision is based on cgmath. It's not a huge difference but it's one of the major ones. Another difference is the focus. MGF aims to be a low-level collision and physics library intended exclusively for video games. It aims to be fast and and accurate and use geometries that are typically used in video games, primarily spheres, capsules, triangles, rectangles, meshes, etc. The physics focus is similarly light weight, with a very general "RigidBody" class that does pretty much everything, and the ability to easily design your own RigidBody types should the performance of a Compound type not fit your needs. Additionally, I believe that MGF shines in a few areas with performance. I have no evidence to back this up, but the bounding volume tree used in ncollide uses boxed point values to point to their children, while the BVH provided by MGF is based on a custom Pool container which should achieve much better performance (that said don't trust me on this until I have evidence, but I am very confident about this) 
Nice! Maybe you can add a quick video showing it in action?
You may have wanted to post this in /r/playrust.
Sure, I gutted a bunch of the display code I was working on to make it a stand alone library, but if you'll bear with me I'll put up a link to some old phone cam footage while I create a better example 
Those videos are beautiful and interesting to watch. Great job!
Not quite, is more a replica of the Python API. There is nothing stopping from building an even higher lvl framework like Keras in the future though.
The NLP library I am building uses custom C++ extension ops, that should be a workaround for now. Is also possible to import graph definitions from Python so that may be an other work around, but haven't experimented with it yet. Is just a matter of waiting though, they are acelerating how fast they add functionality to the C API as they move logic from Python to C++
Buggy unsafe can break things in only places only loosely related to the crash. Sometimes a full debug build (without optimizations basically turned off) will behave differently which might indicate an issue in unsafe. Have you tried using setting RUST_BACKTRACE=1 in the environment to get a stack dump out for the panic. That could at least tell you which allocation is failing, then you could just print the size before to see if any overlarge requests come in. My Goto tool for hard problems in C code is [rr.](http://rr-project.org/) I haven't needed it in Rust yet but I've heard of people gotting it to work.
Very nice! You should crosspost this to /r/proceduralgeneration, they will love it!
This is the subreddit for the Rust programming language. You're looking for /r/playrust
Ooops! Thank you!
Here is a video demonstrating compound capsule shape collisions in action. Apologies for the low quality, this video wasn't intended to represent a "product" "professionally" or anything like that. https://www.youtube.com/watch?v=YoqC1_XaObw&amp;feature=youtu.be
Maybe tagging /u/Elession would help...
You transform between ellipsoids using a Helmert Transform. I touched on it (by which I mean "I don't work in geodesy, but you have to know a little bit about it, so I did a lot of reading") in my RustFest talk. The slides starting [here](https://urschrei.github.io/rustfest/#/8/5) explain how you convert between the WGS84 and Airy 1830 ellipsoids using matrices. Your maths doesn't need to be good – I'm remedially bad at it (please don't tell anyone), you just need to get slightly comfortable with matrix multiplication. Even better, if you **don't** want to implement the transform using matrices, you can do it manually. My [first pass](https://github.com/urschrei/lonlat_bng/blob/master/src/conversions.rs#L273-L373) at WGS84 lon / lat to OSGB36 easting / northing was fully manual. The function is well-commented, and the constants are obviousy-named, so you should be able to figure out exactly what's going on. As for ellipsoid scale, rotation, and translation data, they're all in Proj.4, and there are numerous other free sources available. Getting these data is the least of your problems. Be aware that the Helmert transform (when using `f64`, you shouldn't use `f32`) can introduce horizontal error up to 3 metres, so it's not suitable for high-precision (i.e. surveying or similar) applications. The way Proj.4 deals with this is using "grid shift" parameters – usually supplied by various national geospatial agencies – which use a bilinear transform (basically a rubber sheet transform) to correct the error introduced by the Helmert transform, to give sub-centimetre accuracy. The OSTN15 transform (which is what my `lonlat_bng` crate implements) uses these, but I made it really difficult for myself by deciding that I wanted performance, so I baked them into [a separate crate](https://github.com/urschrei/ostn15_phf) using PHF. They come in a standard NTv2 text-based format, so you can just write a function to look them up. The [OSTN15 documentation](https://www.ordnancesurvey.co.uk/business-and-government/help-and-support/navigation-technology/os-net/formats-for-developers.html), which explains the entire transform, is extremely readable, and ["A Guide to the Coordinate Systems of Great Britain"](https://www.ordnancesurvey.co.uk/docs/support/guide-coordinate-systems-great-britain.pdf) is a phenomenal resource – it's everything you need to know about coordinate system transforms in general, and will answer almost every question you have. A final comment (sorry, this is already way too long): there are a lot of moving parts to getting this right for even a single transform; once you've done that, moving to a more generic implementation should be the fun part. The OSTN15 implementation was what I used to learn Rust, and I should have started with something less ambitious; not fully understanding the problem domain, or the language you're using to solve it results in a really steep learning curve. 
Very nice! Is it an L-system or some other generation technique? Do you plan to release the source?
&gt; It's been long enough that most people should realize the only commonality here is timing of their appearance on the scene and the acceptance that we've got to do something about these newfangled CPUs. Thats a little disingenious. High performance servers, the area Go seems to shine the most, has been made an explicit goal in the Rust roadmap for this year. Many things appearing in the language and around it (`await` syntax, Tokio) are designed to target this specific use case.
&gt; Recompiling a C++ compiler, updating standard library, other libraries and keeping that up parallel to the ones of your Linux distro requires a lot of work. That has been my experience as well. I remember that for some reason the wrong boost library kept getting linked. After struggling for some time I gave up and replaced the version string in the binary by hand.
The first google result, and the first result from a crates.io search (which maybe you should have conducted yourself): https://github.com/mongodb-labs/mongo-rust-driver-prototype
Very intriguing. When you manage to figure it out, consider sharing your findings with the rest of us. At the very least, it might help the next poor sod who faces this issue. Best of luck. 
When you figure it out I hope you let us know what it was. I'm extremely curious.
Is there any chance that you're allocating huge numbers of kennel structures (fds, ... Threads... I can't think of anything) that aren't showing up as process local memory but are still being attributed to your process and causing the oom kill? I can't imagine what that would be, but come to think of it I don't know what os you're running on.
This is the subreddit for a programming language called Rust, not for the game. You probably wanted r/playrust
Ah, that's also great. Thanks for putting time into this and getting it released.
Have you checked ulimit?
Serious question, how do you program without being able to copy-paste from Stackoverflow? Is all of your documentation local?
On the subject of trying to get a backtrace in relevant code... I'm not sure if you can set a breakpoint on an enum getting created - I guess it's just inlined. Assuming this is the right file, there are only a few places where AllocErr::Exhausted gets created. Maybe a breakpoint on them would get you a stack that would shed some light on it? [jemallocator source](https://github.com/alexcrichton/jemallocator/blob/4e2a285389274eaf3699137e04d20c8769c6252a/src/lib.rs#L137) 
I recommend [thijsc/mongo-rust-driver](https://github.com/thijsc/mongo-rust-driver). It is a wrapper around the C driver, and works pretty well.
I have another machine which is able to contact the web, however, on the development machine it is on a separate network. Although I don't have the convenience of Ctrl+Ctrl+V, it's just a slight step removed. It only really makes it really difficult for language ecosystems that kind of assume you're connected to the web.
Thanks! I've heard some issues about this driver. People recommended me to change my db to mySql or PostgresSQL because of some incompatibilities with mongo.
Thanks! I'll give it a try!!
I've wanted to do exactly this for years, c/o a woodworking hobby that branched (lol) off into a very amateur dendrology hobby for a few years there. It's one of those too many "I'd love to, but I'll never have the time" things, but at least I get to see it in action. It looks great!
It seems like you're not really splitting, but incrementally allocating, right? Basically a `Sync` arena?
Yeah, that a solid theory. The number of mapped memory ranges is also limited. I think the default is 65535-ish so if you were really enthusiastic about mmap, you could run out. Maybe the limit is configured to be higher on OP's 2TB nodes and that's why it works there?
A nice read! Regarding nalgebra, I was also really confused by the complexity imposed by everything being generic. In the end, I just wrote a tiny library for personal use, which just has 2/3/4d vectors and matrices, and I have been quite happy using that.
No, you misunderstand: there was a bug in *my* code, which makes a thread panic. `WaitGroup` works indeed as intended. What I'm looking for is some other synchronization primitive that, upon dropping on panic, "poisons" the group and makes the whole program panic.
I have been entirely genuine and I resent the charge. These features have been available in programming languages for ages. Fast servers have been in demand for over half of my entire life. C++, Java, C# and friends are working hard on the problem too, as well operating system developers.
 cat &gt;/dev/null Might get you same effect as any mongodb driver ;)
Are there C++ ops, which do general gradient computation? Can you point me there, it would be very useful? As a matter of C API, I am very skeptical about the speed. The issue is there for at least a year (https://github.com/tensorflow/rust/issues/22), and number of commits to c api is too low. https://github.com/tensorflow/tensorflow/commits/master/tensorflow/c/c_api.h 
Ok, let's set aside what you *think* is going on, because it sounds like you've gotten rather confused. A move is where the raw bits of a value are read from where they're currently stored, and written to some other storage location in memory. After this is done, the original storage location is flagged in the compiler (at compile time; there is no runtime "metadata" associated with this) as being invalid and is no longer accessible. A copy is exactly the same, except the "flag as invalid" step never happens. `Copy` is just implemented on types for which this behaviour is valid and safe. Note that in both cases, there is *nothing* the directly stored bits being read and written. There is no dereferencing going on, no special code being executed. Whether a value is on the stack or the heap is irrelevant to all this. Now, if a value is on the heap, it has to *somehow* be reachable from the stack. This means there needs to be some kind of pointer (or something that fulfils the same purpose) that tracks that heap storage. When you move a `Box&lt;i32&gt;`, you move the `Box&lt;i32&gt;`. Not the thing it points to, just the pointer value directly stored in the `Box&lt;i32&gt;`. This `Box&lt;i32&gt;` might be on the stack, might be on the heap. It doesn't matter. The behaviour is the same either way. The bits stored behind the pointer on the heap don't get moved because they're not part of the `Box`. They're just a thing the `Box` points to. If you were to try and move the *contents* of the `Box`, that'd be different. All of which is to say: there is *nothing* magic or complicated going on here. Rust is doing the absolute simplest thing it possibly can. If you're thinking about "stack metadata" and differences between behaviour on the stack and heap, you're overthinking it.
A Box&lt;T&gt; just compiles down to a pointer to some heap memory; it is the only heap construct in (the safe part of) the language. Any types which are "on the heap", are actually just structures on the stack, which have a member of type Box&lt;T&gt;. Any type with heap allocation cannot be Copy, since Box&lt;T&gt; is not a Copy type.
Super sick!
Thanks, Your Holy Macroness. I really was confusing myself with overthinking simple things. Thanks for showing me the path of light.
The issue here I believe is that f32 doesn't implement eq or ord. Only partialeq and partialord. You are getting a confusing error message because it is conceivable that LinHash implements another trait with an associated constant with a type that does implement ord.
Well, you haven't posted enough code to reproduce, but based on the error, I'd assume it's because you're trying to use `f32` in a context that requires strong equality. That is, it's not confused by what the type is, it's telling you that the type *doesn't support `Eq`*, and it doesn't know what it's supposed to do.
Thanks again!
It looks amazing! Good job!
Why only store a few crates if you can store all of them. My copy from a few months ago takes 7 GB on disk. See https://github.com/tennix/crates-mirror written by a chinese dude to erect a mirror that lives behind the GFW around the chinese intranet. 
I don't really know if your comment violates the third or the fourth rule of this sub.
I'm using nom crate. I wouldn't say it's the best, but its working, and written in rust.
I don't have other associated constants in LinHash though(if I understand what you're saying correctly). This is my code: https://gist.github.com/samrat/e0bebae04da4b6ec93317f8d9d388651. (Line 61 is where I get the error)
Hmm. After reading /u/phenguin 's reply, I guess what I don't understand is *why* Rust expects it to support Eq-- if I use a literal float or a struct field, the compiler doesn't complain.
&gt; what I don't understand is why Rust expects it to support Eq where K: Eq + Hash + Display + Debug + Clone It's *right there*. &gt; if I use a literal float or a struct field, the compiler doesn't complain. Because `==` doesn't use `Eq`, it uses `PartialEq`. You're demanding a stricter set of traits than you actually use.
Ah, that makes sense. Thanks for explanation. I am confused as to why it's not LinHash&lt;K,V&gt;::THRESHOLD though? (ie. why the extra set of colons?)
Because that construct is in expression context, and the compiler needs to tell the difference between `LinHash (less-than) K` (an expression) and `LinHash&lt;K, V&gt;` (a type).
Got it. I don't think I'd had to use generics in an expression context before, but that makes sense. Thank you for your help and patience :)
I think it's clear that Rust has superior performance, if performance is REALLY your primary goal, rather than a trade-off between simple code and performance. The manual memory management vs. garbage collection alone make that clear... to me, at least.
Yeah, I avoided arena as a name, because it doesn't really allocate: you give it an already allocated slice (and if it runs out of storage and just returns `None`). But maybe an `Arena`-based name would be a better name.
&gt; some of my coworkers have occasionally called it "intense" to program in Rust lol :)
Lol :)
Nice! Thanks!
I'd recommend using another database because mongodb has no transactions, which makes data consistency a huge pain. You can even embed json in postgres, though it lacks some useful extensions mongodb's bson supports (e.g. binary data).
It's usually nice to separate the code which receives user input and puts it in the right form (the "glue" code), from the code that actually does the work (your conversion in this case). In this case, you could create a trait which can convert from some arbitrary temperature unit to/from kelvin. Then converting between Celsius and Fahrenheit (or any other combination) is just a case of converting Celsius -&gt; Kelvin then Kelvin -&gt; Fahrenheit. This also means you can break your program up from a single 200-odd line `main()` function to many smaller functions and types. Making things a lot easier to understand and maintain in the long term.
This is a bit of an edge case, but when I was doing FFI and passing strings between languages I didn't know that `std` can do conversions to and from UTF-16, and that you should usually be using an `OsString` anyway. I ended up wasting a good 3 or 4 hours trying out different encoding crates and in the debugger wondering why I'd get gibberish when passing a string into Rust. It'd be nice if this kind of string handling was mentioned (even in passing) on the FFI page, seeing as (unfortunately) not all languages use `char *` for working with strings.
Thanks a lot for your comment. I had to put the "domain knowledge" together myself and google isn't very helpful when it comes to explaining the specific details of reprojecting coordinates (e.g. what steps to do, what to watch out for). For my use-case (cartography), the 3m tolerance is acceptable but of course a better accuracy would be nice. I'll look at the code you've written. Yes, I am familiar with matrices and I use f64 for all the calculations. &gt; Getting this data is the least of your problems. Could you link to any sources listing these transformations for the ellipsoids or where to get them from? Is the OSTN15 specific to the OSGB36 projection or does it apply to the earth in general?
I may be wrong, but my understanding was that it avoids the need for programmers to remember separate rules for things with and without `Drop` implementations. (Things with a `Drop` implementation can run arbitrary code when they're dropped (typically because they reference resources outside of Rust's control), so `rustc` can't reason about the safety of reordering their cleanup.)
When writing Rust, you shouldn't think about mutable vs immutable. Instead you should have in mind that &amp; means "can be shared in multiple places" while &amp;mut means "can one be used once at a time".
Isn't there a risk that the transformation into an intermediary representation may lead to a loss of precision compared to a specialized transformation?
Hmm if you think about it that way, it's way more clear. It never really occured to me before, thanks!
Values can have destructors. These can't just all run at once, but run one at a time, in reverse order of variable declaration. Imagine, instead of your program, the following: struct NoisyDrop(Option&lt;i32&gt;); impl Drop for NoisyDrop { fn drop(&amp;mut self) { if let Some(n) = self.0 { println!("Dropping {}", n); self.0 = None; } } } struct Bump&lt;'a&gt;(&amp;'a mut i32); impl&lt;'a&gt; Drop for Bump&lt;'a&gt; { fn drop(&amp;mut self) { *self.0 += 1; } } fn test(i: &amp;mut i32) { *i += 1; } fn main() { let mut a; let i; { a = NoisyDrop(Some(5)); i = Bump(a.0.as_mut().unwrap()); } test(i.0); println!("i = {}", i.0); } In this example, if `a` gets dropped *before* `i`, then the destructor of `i` would attempt to increase the value of a variable that *no longer exists*.
We still want to take advantage of DRAM for caching frequently accessed state. The current Lru cache is super native and needs to have MFU capabilities added anyway to make it scan resistant. I'm considering trying out a 3 level cache where movement between persistent memory and block-accessed flash is not frequent, and persistent memory can be the only thing present or just turned off, but block-accessed storage requires some word-accessible cache layer enabled. Things only graduate to DRAM when they are frequently accessed. Maybe an ARC-like MFU+LRU for both the DRAM and persistent memory layers would be appropriate, where things that hit the MFU get to be copied to DRAM. I don't know if it makes more sense to use libpmemobj, libpmemblk, or if we should use columnar structures yet. I saw someone just wrote a columnar struct crate that puts different fields in their own field-specific vectors. It would be interesting to build a concurrent vector on top of libvmem or something and measure the different approaches.
* Don't just know the types, know their methods, too * Use [clippy](https://github.com/rust flange nursery/rust-clippy) to get helpful advice to improve your code * The `From`/`Into` (and their fallible counterparts `TryFrom`/`TryInto`) traits are really good stuff * learn about slices (`&amp;[T]`). They have many useful methods. * `Iterator`s also have a slew of great methods. If you are still missing anything, [itertools](https://crates.io/crates/itertools) probably has it. Note that it is often unnecessary to `collect()` an iterator if you further process the values * `std::borrow::Cow` lets you abstract over both borrowed and owned values, which is quite neat. See [my blog on that topic](https://llogiq.github.io/2015/07/09/cow.html)
There's a random component, but the basic idea is to approximate how much each leaf gets light and then grow each branch depending on how much light it received. 
/r/rustservers
There’s a scopeguard crate that implements defer. 
I've spent a year writing and rewriting everything. The final version is most similar to http://algorithmicbotany.org/papers/selforg.sig2009.html I'll release the source once I've decided I can't do anything with it myself.
[There's accuracy and then there's precision.](http://cdn.antarcticglaciers.org/wp-content/uploads/2013/11/precision_accuracy.png) If the transformation is done correctly, the precision of an `f64` is in the sub-centimeter range, which is good enough for surveying - the *precision* is high enough. However, as [another comment](https://www.reddit.com/r/rust/comments/794e3a/proj5_a_proofofconcept_replacement_for_proj4/dozlgz7/) suggests, the *accuracy* of this crate is currently in the meter range (for UTM coordinates). For my use-cases (cartography) this is accurate enough (when you go hiking, you don't care about a few meters of accuracy, just the bigger picture has to match the real world). For surveying (where you usually have a tolerance of 1 - 3 cm at best), this crate is currently useless, but I'm trying to improve it to eventually have this accuracy. A "specialized transformation" is simply not feasible. Think about what you'd have to do in order to support 10 coordinate systems. For each coordinate system, you'd have to write 9 different transformations to each other coordinate system, meaning 90 specialized transformations. This is simply not feasible - even PROJ.4 doesn't do this AFAIK. Yes, there may be a slight bit of precision loss, however, there's little accuracy loss (if implemented correctly). The precision loss is in this case negligible.
I wrote about a few "hidden treasures" here: https://deterministic.space/hidden-treasures-of-the-rust-ecosystems.html 
Hoping to see the source
The original idea was to do a webgame that simulates plants in the server and sends some condensed data to browser that gets used to generate realistic 3d models. It's actually working, but I realized that marketing a webgame that's so slow is way too hard, so I changed directions. Anyway, serde was used server side to serialize the packed plant into msgpack for sending to client, and to save them on the disk.
Nice feedback! I am still planning my app, so mobgodb is not a must have... I'll look into your tips. Thanks!!
I don't actually know if it's touched by the book, but I gotta say I didn't know about slice's [binary search](https://doc.rust-lang.org/stable/std/primitive.slice.html#method.binary_search) function until recently. It looks pretty useful if you want to insert stuff in a list and maintain order.
Slight typo in your [clippy](https://github.com/rust-lang-nursery/rust-clippy) link, there.
The [`mem`](https://static.rust-lang.org/doc/master/std/mem/index.html) module, especialy the [`replace`](https://static.rust-lang.org/doc/master/std/mem/fn.replace.html) function. Also, in the same spirit, the [`Option::take`](https://static.rust-lang.org/doc/master/std/option/enum.Option.html#method.take). Not sure if they're mentioned in a book, but they're really helpful.
[cgmath](https://crates.io/crates/cgmath) is (IMO) much more approachable (and more similar to `glm` that you'll see in C++ OpenGL tutorials). It has a few small annoyances (e.g. needing to `use` a bunch of traits to do just about anything), but I'd certainly recommend it over nalgebra for most people.
This makes me feel so much better about using the texture array approach to approximate bindless texturing! I'm new to graphics programming and one can never really know if one is about to do something clever or truly awful. Hearing someone who knows what they're doing taking a similar approach gives me a lot more confidence.
Looks really good, but it seems like your internode distance increases on older branches, which doesn't happen in Woody plants...
[The Rust FFI Omnibus](http://jakegoulding.com/rust-ffi-omnibus/) saved my life when I started doing FFI work. 
Some other people have already answered why order can matter; because variables that have side effects due to `Drop` implementations need to be dropped in a deterministic order, and it makes it easier to understand if that is consistent even for variables which do not have a `Drop` implementation. However, it is possible, if you have variables which do not implement `Drop`, to have variables be dropped all together when their lifetime ends; since there's no way for you to observe an intermediate state, the compiler will let you do that. You just need to declare them on the same line: let (mut a, i); This may not seem much different than declaring them on different lines, but you can actually set up circular references this way: use std::cell::Cell; struct Ref&lt;'a&gt;(Cell&lt;Option&lt;&amp;'a Ref&lt;'a&gt;&gt;&gt;); fn main() { let (a, b) = (Ref(Cell::new(None)), Ref(Cell::new(None))); a.0.set(Some(&amp;b)); b.0.set(Some(&amp;a)); } 
And std::mem::swap
Hmm, I'm pretty sure it doesn't. Where exactly can you see that?
Hold on a second. That example doesn't have anything to do with drop order. If you swap the `a =` assignment with the `i =` assignment, it doesn't compile *because you're not allowed to have a reference to an uninitialized location.* If the compiler error says otherwise, it's misleading.
If you install Rust with `rustup`, you probably already have all the official docs locally via `rustup doc`. Very useful on planes!
What kind of security-critical project you work on to do this?
I don't think its either. Pointing out that mongodb is a bad idea isn't a meme, it's fact. For example, if someone was like "what's the best way to walk across a highway at night in dark clothing" it wouldn't be a meme to say "I'm pretty sure you can stay at home and run yourself over with your own car". It's also not zealotry about a programming language because mongodb is not a programming language. I think a better response would have been linking to the [numerous](http://www.sarahmei.com/blog/2013/11/11/why-you-should-never-use-mongodb/) [blogs](https://runnable.com/blog/think-before-you-mongo) [on](http://cryto.net/~joepie91/blog/2015/07/19/why-you-should-never-ever-ever-use-mongodb/) [the](http://pastebin.com/raw/FD3xe6Jt) [subject](https://aphyr.com/posts/322-call-me-maybe-mongodb-stale-reads)
What is the `thiscall` ABI used for in practice? I've never heard of it, usually just the `cdecl` ABI.
In msvc, c++ class methods use the `thiscall` ABI.
&gt; Use clippy to get helpful advice to improve your code As long as clippy requires nightly I don't think the general public is going to use it.
Mongodb has put quite some effort into better reliably (e.g. https://jepsen.io/analyses/mongodb-3-4-0-rc3), so things aren't quite as bad as they used to be. But even assuming that they fix all their bugs and it works as specified, I still couldn't recommend mongodb. The big limitation is the lack of transactions. You can atomically update a single document, and that's it. And you can't even pay the performance cost from stuffing more related data into a single document, because then you hit the 16MiB document size limit.
It's msvc specific: https://en.wikipedia.org/wiki/X86_calling_conventions#thiscall and https://msdn.microsoft.com/en-us/library/ek8tkfbw.aspx .
Is there anything that requires "must not implement Drop" besides Copy?
You can use nightly just to lint your project(s) with clippy every now and then, but still use stable for all code you ship.
Maybe it was an illusion, from the way the leaves appear.
&gt; It'd be nice if this kind of string handling was mentioned (even in passing) on the FFI page, seeing as (unfortunately) not all languages use char * for working with strings. It's probably worth opening an issue in the bug tracker if you have the time.
[Here](https://i.redd.it/moxxoeir7iqz.png) is a cheat sheet showing how different types are laid out. When you move a struct, it copies whatever is in the top row, but doesn't do anything to the things that it points to. When you have a `Box&lt;Vec&lt;T&gt;&gt;`, then the `T` part of the `Box` diagram is replaced with the first row of `Vec`'s diagram.
You're right that companies don't want to introduce yet-another-language. But go and rust are mainly used if you don't want to use an interpreted language such as nodejs combined with C/C++. One phrase from the dropbox talk I remember is that they chose rust as they didn't have a C++ culture in the company. It was also said that they were practically forced into using two languages as go didn't meet their requirements for some parts of the software, so they had to rewrite parts of the go code in rust. This combination of go as an easy language and rust as performance language seems to work very well at dropbox, so I see it as a lighthouse project for that combination. 
Damn autocorrect. Thank you.
Or also "do I want aliasing or non-aliasing pointers?" one doesn't need to mutate something to want to use `&amp;mut`.
Cool! A few questions though: * Will it be possible to change the default registry? (Use case: importing a crates.io crate into my intranet registry without patching its Cargo.toml) * Say I depend on package `A` from registry `R`, from where will its un-annotated deps will resolve? Crates.io or `R`
Currently the easiest way: `cargo +nightly clippy` ^(after running `cargo +nightly install clippy`; needs to be done every time you `rustup update`)
I know I haven't used clippy because of that. I'm hoping eventually some of these super helpful tools start to come off of nightly but the compiler warnings are still quite good at least.
Not that I know of, but I never really paid much attention to the "must not implements" to be honest.
My code was 4 random numbers by the way. I highly doubt he got lucky and even then I would've seen the light turned green when he unlocked it. Keep in mind he also did this to my code locked tool cabenet and griefed me. This isn't possible in vanilla game right? 
It looks like you are looking for /r/playrust/ This subreddit is about https://www.rust-lang.org/ the programming language.
Wow that's a cute trick.
My wife recently got interested in learning to program, and now I remember how much knowledge it takes to program from helping her learn. The learning curve is much worse if you don't already have a strong computer background. As much as I would like to start her with Rust there's just too much to learn for someone in her position.
&gt; Will it be possible to change the default registry? (Use case: importing a crates.io crate into my intranet registry without patching its Cargo.toml) You already can. Cargo.config has a `registry.index` member which is the URL for the default registry: [registry] index = "https://crates-io-mirror.mycompany.com" &gt; Say I depend on package A from registry R, from where will its un-annotated deps will resolve? Crates.io or R crates.io (or whatever the default registry is)
[One can `collect` multiple results `Result&lt;T, E&gt;` into a `Result&lt;Container&lt;T&gt;, E&gt;`](https://play.rust-lang.org/?gist=d12b718b855b208a06cf7c13f1230855&amp;version=stable).
Thanks for the answers - I think it would be great to add them to your README in some form. I look forward to seeing where you go with this, good luck.
Wait, so I can already do this? I guess the registry just have to look like what this article suggests, right? No special magic?
No special magic. But I think we would recommend if you want to mirror crates.io, you have two different registries (one a mirror and one with just your private crates). We would be excited about tools which make it easy to set up a mirror which automatically tracks &amp; self-updates crates.io's index &amp; tarball set.
Yeah, that's wise. I'll with it soon when I'll get the chance and report any findings. A mirroring tool a-la-`apt-mirror` will be great
Would you be interested in something like [`euler`](https://github.com/alteous/euler) instead?
Oh I fully agree. If a company doesn't already have a strong investment in C or C++, or is willing to invest for peace of mind/ease of maintenance/ease of recruitment, then using Rust is an easy choice to make. As for Go, I would think its strengths are well known; it's a surprisingly good scripting language (due to its blazing fast compilation step &amp; batteries included approach) and it is crafted for webservices. So there is definitely space for both Go and Rust to coexist; it's just a company may find it difficult to adopt either if they already have a large codebase in another language filling the same "sweet spot".
Thanks for the detailed answer. I will admit I do not know this particular domain at all. I have however followed with care the evolution of C++11, and in particular the introduction of the `ratio` class. The interesting tidbit in `ratio` is that when converting from a quantity expressed in one `ratio` to a quantity expressed in another, rather than "naively" going through a `ratio` of 1, the implementation will instead compute the exact (specialized) ratio necessary to perform the conversion in one step. I was thus wondering if it would be possible to do so here, using meta-programming technics. That is: - describe the transformation in terms of normalization-denormalization, - but combine the normalization &amp; denormalization steps at compile-time. This would leave a single transformation step which can be "optimized" to avoid losing accuracy/precision as much as practical.
My biggest strugle with the "heavily templated" nature if many libraries is that it makes the documentation useless. You never know what concrete type a return value is, because it has 3 layers of nested templates, and the documentation doesn't make any attempt to guide you though those templates and what that can mean. If you've ever asked yourself "What is the return type of this function?" and spent an hour digging through various pages of the focs trying to piece it together, you'll understand my poin. It might be that rust's documentation is inadequate for heavily-templated projects, or it may be that library devs ATM are too clever for their own good, and as rust matures, they'll learn better techniques for readability and documentability.
Neat. I will keep it in mind for my next rust+OGL exploration. I did the same tutorials in Haskell using the [linear](https://hackage.haskell.org/package/linear) package. It was fine having to look through `linear` a bit to figure out how it matched up with GLM. Compared to that experience, looking through nalgebra was very difficult, and cgmath (which is what I eventually used) was also fairly tricky seeming. Part of it might also be rustdoc compared to haddock (i like the haddock format quite a bit more) i suppose.
Probably both at the same time. Also the lack of a REPL, or the ability to easily just query the type of an expression.
It reminds me of a discussion I saw in the "&amp;dyn" RFC, the fact that rust makes templating so easy means everyone immediately goes for it, in cases where using a trait object instead would make the library more useable, readable, and compile faster, while not impacting performance in any meaningful way
I love how it's clear where potential errors are in Rust.
When is it appropriate to use type traits over generics? For example, if I were creating a trait which describes a Tree-like data structure, should the type contained by the tree be represented by a generic or by a type trait? e.g. ``` trait Tree&lt;T: Ord&gt;{ }``` vs ``` trait Tree{type T: Ord;}``` is there a difference? 
Maybe a "pool" name? But this is a little overused too, in the context of thread pools. The implementation looks pretty good, but you might want to take more care about integer boundaries. The `fetch_add`s may silently wrap around, and the subtractions from `self.len` could underflow. Both would require unusual inputs, but unsafe code should be paranoid about safe interfaces. Also consider worst-case races between threads. e.g. if one thread causes a `fetch_add` over the length, then one or more threads add even more before the first has done the following store to fix it.
Finally documented my filesystem Trie. It's not [too sophisticated](https://docs.rs/fs-trie/0.1.4/fs_trie/) but it's the first time I'm touching the cargo doc tooling. Just like anything else cargo, it's polished as fvck.
mem::swap is super useful. If you have a container struct with a bunch of objects in it and want to call a method on one of the objects that takes the entire container, you can't because it would lead to double mutable borrows (both &amp;mut self and indexing the container would give you the same location). You can fix that by using mem:: swap with a dummy value in the container, calling the method on the real object, and then swapping the real object back into the container. Like, this is a /super common pattern/ for me and when I first learned Rust just seems mostly impossible to implement due to borrowing rules. But how to fix this problem with men::swap isn't introduced anywhere.
What's a use case for take? Sound just like std::mem::swap with None
It's a thing the dev tools team is aware of and working towards, the RLS is already riding the trains IIRC.
https://doc.rust-lang.org/stable/src/core/option.rs.html#753-755 Convenience.
I like using `.binary_search().is_ok()` as a faster `.contains()`
lol, I feel bad I didn't guess that
which works, assuming your container only has sorted data
It would be nice to have an easy way of downloading the entire crates.io repo for use in places with no internet.
Yup, and this is super baffling the first time you run across code doing this with no explicit types. Cool but make sure you explain what's going on
Have you considered using a bignum crate for minimizing loss of precision?
This isn't possible. The `ratio` is basically a linear transformation in a 1D coordinate space. A projected coordinate (let's say in UTM) is not defined in a [linear coordinate space](https://www.mathplanet.com/Oldsite/media/27393/coordinateplane01.jpg), rather the coordinate space looks like [this](https://upload.wikimedia.org/wikipedia/commons/7/7a/Transversal_Mercator_0.jpg) (depending on the UTM zone). This is a "warped" coordinate space, which is why any vector math and / or linear ratios are not applicable in the warped coordinate space. You can use a matrix to reproject between two ellipsoids, though, because both are in a linear coordinate space (as if you were looking from a sattelites view onto the ellipsoid). But the projections from UTM -&gt; lat/lon and back are not linear, you have to essentially project Plus, due to how floating-point-precision works, you don't really lose precision here. You'd lose it if you would, for example, add a really big and a really small number. This is, however, not happening in the code. In terms of performance, Rust is [pretty fast already](https://github.com/urschrei/lonlat_bng#benchmark). [This booklet](https://www.ordnancesurvey.co.uk/docs/support/guide-coordinate-systems-great-britain.pdf) does sum it up really well - or [this (wrong) picture](https://cdn.images.express.co.uk/img/dynamic/78/590x/secondary/North-Korea-news-Hawaii-missile-range-how-far-attack-1030023.jpg) vs [this (correct) picture](https://cdn.images.express.co.uk/img/dynamic/78/590x/secondary/North-Korea-news-Hawaii-missile-range-how-far-attack-1030028.png)
Thanks for having a look! &gt; the subtractions from self.len could underflow Argh yes, of course! Thanks for that! &gt; if one thread causes a fetch_add over the length, then one or more threads add even more before the first has done the following store to fix it. That's true, but I figured that would be harmless. I suppose is many, many, many threads do it, it could end up overflowing. I don't really know how to stop a `fetch_add` from overflowing though... I could replace it with a CAS-loop.
Hi, I'm writing this rust library for computing time-expressions like "today + 5days". I'm in the process of writing a nom-based parser which is for handling user-input. It fails (at the linked line) resolving a simple expression. The complete code for everything one must know is in the linked file... I do not understand why this fails and would love to get some help here. 
Cool, thanks for posting! A few questions I couldn't tell from the readme: 1. What algorithms does this library implement? 2. What sort of information does the library return for collision detection queries? Collision points? Normals? Times of impact? Penetration depths? 3. Does this library support non-convex meshes?
f64 is precise enough and bignum would possibly cripple performance. f64 allows for 16 decimal places - if you look at [this chart](https://en.wikipedia.org/wiki/Decimal_degrees), this is more than enough precision. The code doesn't add big and small numbers (which would make a significant impact), only division and multiplication by similar-precise values. So a bignum crate wouldn't help. The coordinate *values* usually range from 10000000.0 meter (10000 km) to 0.00001 degrees. Bignum wouldn't help here - bignum is useful if you have numbers like 100000000000000000000.234567979 or something like that, where you'd use up / use more than the precision bits of an f64.
**Decimal degrees** Decimal degrees (DD) express latitude and longitude geographic coordinates as decimal fractions and are used in many geographic information systems (GIS), web mapping applications such as OpenStreetMap, and GPS devices. Decimal degrees are an alternative to using degrees, minutes, and seconds (DMS). As with latitude and longitude, the values are bounded by ±90° and ±180° respectively. Positive latitudes are north of the equator, negative latitudes are south of the equator. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
This is really exciting! I've been following your posts for the last few weeks, but I wasn't expecting you to have got far enough that a TodoMVC was close to being viable. I'm itching to write my own music player, and this looks like it might soon be a good way to do a cross-platform GUI. Looking forward to it being open sourced!
Can you give some examples for when that pattern is useful? If it is so common, have you considered abstracting it, for example by using a decorator that only allows access to the rest of the vector?
There's a discussion of the differences [in the book here](https://doc.rust-lang.org/nightly/book/second-edition/ch19-03-advanced-traits.html#associated-types).
&gt; Why do Rust needs to drop the values in order, why can't the values be dropped all together when their lifetime ends? Well obviously it can't do it at the same time and _one_ needs to be dropped before the other; this can interleave in theory yes but that would make the data model significantly more complex about guarantees. The Rust compiler could be made smart enough to automatically re-order this when it preserves semantics but that would make understanding when this happens more complex so they just kept it simple. So they just say that: { let a; let b; ... } Is identical to: { let a; { l let b; ... } } And call it good I guess.
Well, I don't think saying "Something something is pure evil." or something like that is constructive any way. You actually cited some sources to confirm your views and opinions. I'm currently reading your linked material, and I knew mongodb wasnt a silverbullet (this would be zealotry in my eyes) but can be a good choice in certain scenarios. Now I'm sceptical a bit. I'm just a bit sad there's no such engine as mongodb in sense of file storage which is reliable. (CouchDB also can store files, but does not cache or keep in memory anything.) Thank you for your input!
Go 1.8 improves on the cost of calling out to C (via cgo), though there still is a toll for crossing the language bridge. "The overhead of calls from Go into C has been reduced by about half." There is a tracking issue, so hopefully it will continue to improve in the future: https://github.com/golang/go/issues/9704 (You may already be aware of this, but others reading this may not).
Doesn't this break down if you see the order in which others are picking up cards? (Btw, [direct link](https://www.youtube.com/watch?v=5kC5k5QBqcc&amp;t=8m9s) to the relevant part of the video.)
The docs are pretty comprehensive so I recommend checking those for more detail. 1. This library implements a number of algorithms, almost entirely "primitive" dynamic collision routines (i.e. moving sphere / triangle, moving capsule / moving capsule). It also implements a variety of useful data structures, such as a BVH tree. 2. It depends on how much information you want and if I forgot to implement a certain algorithm for a pair of types. At the minimum it can return a boolean stating if two object overlap and at most it can return global and local collision points, contact normal, and time of impact. Penetration depth is not calculated because it can be calculated from the provided information. In fact, I'll write a method to do that automatically. 3. Meshes right now are stored as a BVH tree of triangles (with the obvious mesh structure optimization of a list of vertices and then a list of faces) and are generally assumed to be static structures so there is no convexivity requirement. In the future a non-static mesh structure that is required to be convex may be added (so collision can be implemented in GJK). 
And clarity. Swap with None takes a little extra mental effort to process intent.
you're looking for /r/playrust.
That would mean not just the index, but also all packages—or at least the latest version of any crate, and any older versions depended on by the latest versions of other crates. I have no idea how big that would be at present, but if it’s not already gigabytes it will be soon. Then it’ll be terabytes.
Wrong sub, you are looking for /r/playrust. This sub is for the Rust programming language 
That `Result` implements `Iterator` and `FromIterator` has been the cause of so many incomprehensionable error messages for me where I forgot a `?` somewhere. Like I just do `expr.collect()` instead of `expr?.collect()` and it works because it treats the Result like an iterator and then down the line the error message is incomprehensionable.
C++ has exactly the same issue. It's a template problem, rather than a Rust problem.
Yeah absolutely 
Looks like they're looking for /r/playrustlfg. /r/playrust doesn't want "looking for a group" posts either.
I only know about the good old sand-bag trick because of the Entirely Too Many Linked Lists.
Anytime you have a "World", an object in the world, and want the object to be able to modify/access the world it's within. Stuff like ECS are basically the "abstract" way to do this. I use this in my WM. A Workspace has a trait object Layout. The layout has a method that updates all the windows in the Workspace. You can't do `workspace.layout.update(workspace)` because of duplicate &amp;muts, so I swap workspace.layout with a dummy and call it after before swapping back.
But because templates in c++ are a little more unwieldy, aren't supported by type inference etc, library authors in my experience tend to create more usable/discoverable APIs in c++, because otherwise they'd be too much of a pain in the ass to test.
Didn't even know that sub existed
Not the author but I found it recently while looking for something else and it seemed very cool.
The functionality of [cargo-update](https://github.com/nabijaczleweli/cargo-update) should be included with Cargo. :t
Great ideas! I will certainly attempt to break up the code a bit. Also, good thinking on the part of converting to a mutual unit. That will make things a lot easier.
The last time I mirrored an apt repo (17.04 I think) it was about 150-200 GB. I would imagine crates.io is much much less
That impl methods that receive `self` (e.g. `fn foo(&amp;self)`) is just sugar for a regular parameter (e.g. `fn foo(self: &amp;Type)`). This makes it clearer what impls having lifetime parameters (`impl&lt;'a&gt; Type&lt;'a&gt;`) actually means in the context of one of its methods. Like, `fn foo(self: &amp;Type&lt;'a&gt;)`.
I'd approach this by breaking up the `amount_expr` parser into named components (particularly the `next` bit with `do_parse`) so that I can test them independently and work out which step is failing.
CPAN has about 35,000 Perl modules, and my local mirror with only the latest versions takes up about 4 gigs. Crates.io has around 12,000 crates, and I'm sure it's growing faster than CPAN, but I'd be surprised if we hit terabytes anytime soon.
[The work on it has been inactive for some time](https://github.com/rust-lang/rfcs/issues/322), but people do seem to be aware of it. In the meantime if you have a lot of code to evaluate at compile time, you should consider using a procedural macro. It's not exactly what you're looking for, but it can be great in a lot of cases. :)
Full steam on [wlroots-rs](https://github.com/way-cooler/wlroots-rs/), and started receiving some help on my attempt to add [awesomeWM compatibility to Way Cooler](https://github.com/way-cooler/way-cooler/issues/338) which is nice. 
That shouldn't happen. Result implements `IntoIterator`, but not `Iterator` so there has to be a call to `.iter()` or `.into_iter()` first.
Yeah I guess it happened in `for ... in ... then`. I don't like how `for ... in ...` calls `into_iter` itself and that whole trait; I feel you should just explicitly call it and communicate what kind of iterator you want. A lot of types have multiple meaningful ways you can iterate on them and it makes no sense to pick one as the default.
It's actually not inactive; it's in active development, under the name [miri](https://github.com/solson/miri), for an MIR interpreter, which is intended to be used eventually for compile time function evaluation in the compiler after compiling to MIR.
Indeed, looking forward to the output of the impl period. Note to self to contribute to that effort instead of just playing with my own projects X(.
If you're going to cross-post, at least link to [the Stack Overflow question](https://stackoverflow.com/questions/46997109/rust-file-bytes-function-iterates-over-bytes-in-strange-order). Otherwise, people coming *here* won't know about answers posted *there*.
It's actually `hexdump` that's displaying the bytes out of order! By default it displays 2 bytes at a time as little-endian 16 bit numbers. $ hexdump myfile 0000000 6568 6c6c 0a6f 0000006 $ hexdump -C myfile 00000000 68 65 6c 6c 6f 0a |hello.| 00000006 You should check out the [byteorder](https://github.com/BurntSushi/byteorder) crate for reading integers from binary files.
Whoops, sorry bout that. I'll edit the stackoverflow question too
Well, I did not say that mongodb sucks, I just said, you can sometimes lose your data when you use it. (And besides having well cited constructive posts all the time, would be too boring)
Why reupload the [original talk](https://www.youtube.com/watch?v=FMqydRampuo)? I know it's CC-BY, and it's allowed, just seems a little weird. Looking at your channel it looks like you are either archiving talks that you find interesting, or maybe eventually trying to pass 10,000 views so you can monetize other peoples talks?
Thank you, it seems that I still have a lot to learn :) As a followup question, I saw that I could have also implemented `parse_int` as follows fn parse_int(index: usize, vector: &amp;Vec&lt;u8&gt;) -&gt; u32 { let headptr = vector.as_ptr(); let num = unsafe { *(headptr.offset(index as isize) as *const u32) }; return num; } This seems more like how C would "do" it, but is there any way to do this without unsafe code? Is there any way to do this without unsafe code, or is the combination of pointer arithmetic and dereferencing just always going to be unsafe? As a more general question, is unsafe code bad? Would the (presumably slower?) function I had where I was looping over the bytes preferable to having this version which is unsafe? 
`unsafe` is bad when it is uncontained or unjustified. In this case, it leaks unsafe outside of the `parse_int` function: the array could be too short for example (the previous case would panic thanks to bound checks, and at least not access random memory). The byteorder crate mentionned above offer fast and safe methods like `read_u32(&amp;[u8])`, which does this `unsafe` containment.
I find this to be incredibly helpful to learn Rust combinators quickly and start writing functional code where it makes sense: https://jethrogb.github.io/rust-combinators/
Wow, this is a really good way to solve the writer mutable reference in constructor problem I've had before with some crates which didn't take writer by value (not for any good reason since a reference still implements).
Thanks! As the author, pleased to hear that! If someone is excited enough to try to make a new Mles client, the beta3-key mentioned in the blog is still functional, you may smoke your client against the reference server running at mles.io. 
It's also already in the compiler and IIRC kinda works but is unstable.
A function that returns a constant will be in-lined as that value by LLVM ( if it is able to do so ). So i don't really understand your statement. Could you give an example for greatly increasing performance? 
I like to use it for deserialization. [Here](https://github.com/haptics-nri/nri/blob/master/src/bin/render.rs#L60-L82) I've parsed a CSV file into a `BTreeMap&lt;u32, HashMap&lt;u32, (f64, f64)&gt;&gt;` using nested `collect`s. Monstrous? You decide.
FWIW clippy [has a lint for this](https://rust-lang-nursery.github.io/rust-clippy/master/index.html#for_loop_over_result).
Disclaimer: I have not worked with tera. You can edit tera templates before compiling them with regular string operations ( or you could template your template, but i doubt it would be fun ) with one of the Tera functions like [Tera::one_off](https://docs.rs/tera/0.10.10/tera/struct.Tera.html#method.one_off) The real question is , what is the user input suppose to be? Should the user just supply a bare minimum for displaying the test name, and your program will wrap that in a recursive macro you described? Recursive macro's are likely the way to go.
miri or the old evaluator?
Clients as in Java, Python, C#?
How difficult would it be to automatically turn something like this: // BINDGEN: elide-lifetimes // BINDGEN: arg s non-null, null-terminated // BINDGEN: returns nullable, null-terminated const char *strchr(const char *s, char c); Into this: mod sys { extern "C" fn strchr(s: *c_char, c: c_char) -&gt; *c_char; } fn strchr(s: &amp;CStr, c: c_char) -&gt; Option&lt;&amp;CStr&gt; { unsafe { let res = sys::strchr(s.as_ptr(), c); if res.is_null() { None } else { Some(CStr::from_ptr(res)) } } } ?
This is why I like rolling my own maths. I know how it works, and it can be as general/templated as possible. if someone else has *exactly* the same idea on how to organise it as me, I could do a rename to match their library .. but the chances are there will be some differences here and there. 
I'm currently working on a project to provide a changes stream you can use to build replication on, based on an idea by Ashley Williams. It will allow you to continuously follow the changed and download them. I cannot give you a time frame, though.
Sure! CBOR is available as a library in all of those. Moreover, the Rust example client is really just a bare bones example. So any real client would need to have its own implementation, perhaps using the Mles-utils library. 
I'll just add that this is undefined behaviour in C as well, since it breaks the strict aliasing rule. So, please don't do that ;) 
Hmm, the WebSocket proxy mode in the example client is then again a different story, it can be used for real as a proxy for e.g. JavaScript WebSocket Mles clients.
If someone would do the first client, others would follow. I suggest at least a place in the README for "client libraries for other languages". What I'd find attractive, is if the library I could use for (say) Java was the whole thing. The whole pub-sub message server. I.e. totally embedded, yet still interoperble with other nodes elsewhere over IP.
Isn't `char *` permitted to alias with other pointer types?
You can alias pointers to anything with `char *`, yes. But the other way is not allowed, you can't alias `char *` with a pointer to any type you want. I tend to think of it as being able to inspect the bytes of any type, but not being able to cast random bytes into any given type 
Is there a way to also mirror the packages? 
Hey this looks really good. I've just started learning rust this past week, still reading the book, and was wondering what's available similar to nanomsg, mangos, etc, then found this post. Have you ever looked at rsocket.io? It'd be interesting to have that in pure rust.
You can use both `swap` and `replace` for that: let mut temp = vec!(); mem::swap(&amp;mut temp, &amp;mut self.the_real_vec); // do things with temp mem::swap(&amp;mut temp, &amp;mut self.the_real_vec); Or: let mut temp = mem::replace(&amp;mut self.the_real_vec, vec!()); // do things with temp mem::replace(&amp;mut self.the_real_vec, temp); I like the second one better. 
Thanks, and thanks for the rsocket.io pointer! With a quick peek, looks really interesting!
There are 3 things wrong with this unsafe code. I'd advice to avoid it where it's not absolutely necessary. Broken unsafe code undoes a lot of what we like about Rust; predictable failure modes and understandable crashes, if any. This is simple to write in safe Rust, so please do! See the byteorder crate. - It's not valid to dereference a misaligned `*const u32`. It's misaligned whenever the index is not a multiple of u32's alignment (it's 4 on a common platform). - It's not valid to even compute the new pointer, if it's out of bounds of the Vector's allocation - It's not valid to dereference the new pointer, if it's out of bounds of the length of the Vector.
For example, I have an array of the first 1 million prime numbers in a separate module. I am checking if a number belongs to that array a few times and I don't want to create that array at runtime.
Wikipedia is actually pretty good for the [main ones](https://en.wikipedia.org/wiki/Helmert_transformation#Standard_parameters), and you can also have a poke around in the Pyproj source on GH, which is very readable. 
Yeah, terabytes was a little excessive. I expect crates.io to surpass CPAN in 2–3 years (a projection based on [data at modulecounts.com](http://www.modulecounts.com/)), but it should still take many years before we get to terabytes. By this schedule of estimates even npm would only be approaching 100GB now. A quick check on my ~/.cargo/registry/cache/github.com-1ecc6299db9ec823 directory, after removing old versions of libraries, is 300 items and 28MB—with the top seven accounting for over half of the weight, libgit2-sys-0.6.14 winning at 4.4MB (at this stage, most of the large ones are because they include a large C/C++ codebase to compile against); this suggests our average package size is in the same ballpark as CPAN’s. We’ll see how that continues—I imagine we could easily end up with people accidentally including compiled binaries in their published package, which can immediately increase it from the vicinity of 100KB to dozens of megabytes.
That makes a lot of sense, I stand corrected. 
Or: ``` let mut temp = mem::replace(&amp;mut self.the_real_vec, vec!()); // do things with temp self.the_real_vec = temp; ``` if you don't need the temporary Vec back after you're done.
I would be pretty unhappy if a constexpr or macro expanded to contain calculate the first million primes each time i compiled. Cant we have a file generated by a build.rs file?
Even in C I'd do a memcpy relying on the optimizer to avoid aliasing and unaligned related UB.
I'm writing my first rust project and trying to get a feel for the language. In this project I have a lot of classes which are very similar in structure so I made a trait for them to share and a factory to generate them. So far, everything works. However, I would also like to make sure their constructors would also be uniform and defined by the trait (so I can change it there, fix all compilation errors and then know I changed it everywhere). This means I require something like python's abc.abstractclassmethod. However, to my understanding this cannot be done due to this not making sense in terms of vtables. My question is this, what would be the rust way of doing this? I would highly prefer them being uniform so if I do a change I would not be able to miss it in their factory. In case it matters, what I'm making is a small 8086 emulator and the classes I have a lot of are the opcodes. Any help would be appreciated, thank you.
May sound cheesy, but I'm not really able to say. 
It's not quite in https://github.com/rust-lang/rust/pull/45002
Not 100% sure, but in most emulators I've seen the opcodes are created as either a set of structs (where each instance = one opcode) or an enum (where opcode = variant). Both options can be more or less elegantly expressed with macros.
We do have a size limit in place.
You may want to ask /r/playrust
You are looking for /r/playrustserver, right?
This is only true inside the crate that defines a function or if the function is marked inline.
https://stackoverflow.com/questions/28898872/why-is-variable-scope-dependent-on-the-definition-order
You had the right idea
I *think* LLVM is smart enough to remove an array entirely, if the only references to it have been optimized away already. Have you tried it?
I'm really curious about this! Are you accepting contributors, because I'd love to help out?
Hmm, not really examined it. Would be great if LLVM does it by default, And to the people that are concerned with the compile time, wouldn't incremental builds solve this? Not like the array creation module is being changed.
What kind of applications could be build on mles? Are there any? What is it good for?
Ah, nice, I had missed that this PR had been started already. Great news!
There are plenty of other pub/sub libraries already out there. I’ve seen it used in IoT for communication between devices and the hub, but I’m sure there are many other uses as well.
I'm trying to avoid the standard 3000 LOC file in the usual emulators, that means (for me atleast) splitting into many classes and files. I think I'm going to go with your suggestion of a macro. Would you consider a macro such as: ``` macro_rules! make_opcode { ($class:ty, $decoded:ident, $address:ident, $memory:ident, $state:ident) =&gt; { { &lt;$class&gt;::make($decoded, $address, $memory, $state) } }; } ``` idiomatic? The reason I'm doing this is that I remember there being prefixes in 8086 and therefore I'm going to have to change the signature of the opcodes constructors when I decide to implement them. Thank you for your help, I didn't consider a macro.
Just as an exercise, I looked at byteorder's source code and tried to implement something myself by using what I saw in there. Does the following code look "contained" to you? fn parse_u32(index: usize, vector: &amp;Vec&lt;u8&gt;) -&gt; u32 { // Safe? let mut rdr = Cursor::new(vector); rdr.set_position(index as u64); let mut buffer = vec![0; 3]; rdr.read_exact(&amp;mut buffer).expect( "Ran out of bytes to read", ); return unsafe { *(buffer.as_ptr() as *const u32) }; } This seems to panic if I try to read past the end of the array, and should enforce "bounds checking" through use of the `read_exact`
Nope, it's undefined behavior to cast a `*const u8` to `*const u32` and dereference it because it's not guaranteed to be properly aligned. This code would also be slower than necessary because it's allocating and freeing heap memory for the new vector's data on every call. All the IO types use u64 because you can have a file bigger than 4GB on a 32-bit platform.
It works if you clone the vector before moving it into the closure: [Playground](https://play.rust-lang.org/?gist=4a6c1744bf05fd728217efc4f5a33a5a&amp;version=stable) If the vector is large however, it's not advisable to clone the vector for every thread. In that case you could wrap it in an Arc to tie the lifetime to that of the longest living thread. This works since you only want to read from the vector. -&gt; [Playground](https://play.rust-lang.org/?gist=cb1aacaa293735d54a5733be174992f5&amp;version=stable) And lastly, one might think that a simple immutable reference would suffice, since you only want to read. This poses a lifetime problem however, since threads might outlive the main function scope. In that case the vector would be dropped, leaving the references dangling. One way around this would be to ensure that the threads join before the main function returns. Scoped threads in crossbeam do exactly that ([link](https://aturon.github.io/crossbeam-doc/crossbeam/struct.Scope.html#method.spawn))
So it's obvious the author isn't a rust expert. Rather than looking at this as a deficiency, we should look at this as a way to gain empathy. Just not exactly sure what to say, except maybe we could make some guides
Oh. I put the clone statement outside of the for loop. Thanks alot.
How about you come up with a more contrived example.
title is pure clickbait
Sounds like a case for the already existing [`cargo vendor`](https://crates.io/crates/cargo-vendor)?
awesome thanks
First; why only 3 bytes? Don't you need 4 for a u32? Second, you don't need a `Cursor` to get `Read` on a `Vec`: we have `impl&lt;'a&gt; Read for &amp;'a [u8]`, so `&amp;vector[index..]` should implement read itself. Second, instead of `let mut buffer = vec![0; 4];`, you could use a stack-allocated array: `let mut buffer = [0u8; 4];`. `Read::read_exact` takes a `&amp;mut [u8]`, so it doesn't need to be a heap-allocated `Vec`. I think the memory layout for `[u8; 4]` is pretty stable and will keep a 32-bits alignment, but I may be wrong here. [Here is a playground with an updated version.](https://play.rust-lang.org/?gist=d1ada1aac5e113b9f17791f774d5e826&amp;version=stable)
E.g. configuration distribution and IoT telemetry are the first ones mentioned, but for anything really where the distributed pub/sub approach works. You can combine it with clients that handle e.g. database connection for the data and build your own client protocol on the Mles based on your requirements.
Thanks for the idea! I think I'll add a "client library section for other languages" at some point. As the Mles-utils library includes a large part of server and client functionality, I believe it could be possible to do a combined build that would have all the functionality quite easily. And also to use Mles-utils library as a baseline for wrappers that would provide working API for other languages. Contributions are certainly accepted here :)
You should be aiming at all-language Sqlite3 style utility IMO :)
I believe they are assuming they already have the array "hard coded" but would like to have it be optimized away entirely.
That's what I did fory first Rust project too :) https://github.com/jdormit/txtpic
Awesome! A couple of my friends and I had wanted to make one very specific ascii image for the last couple months, so I finally decided to just sit down and make one :D
Hmh... yep, I [did that](http://git.imag-pim.org/kairos-matthias/tree/src/parser.rs?h=parser#n141) and the error is the same still... so I'm not sure how to proceed here.
So you clone it once, but try moving it 5 times (for 0..5).
The code's pretty good overall. A few things that can be improved: - `intensity_to_ascii` could return a `char` instead of a `&amp;str`. This would be cheaper than passing string pointers around. - The `ascii_chars` array could be a static, so that it is only initialized once (maybe the compiler is smart enough to do this anyways, but I'm not sure). - You make new strings by doing `old_string = old_string + some_stuff`. Instead consider using `old_string.extend(some_stuff)`. This avoids doing lots of reallocations in the loop. - The whole for-loop can be rewritten in terms of `imgbuf.pixels().map(/*Stuff*/).collect::&lt;String&gt;()` if the map function returns `char`s.
Thanks for reading it through! I had a feeling there was a better way to extend a string... I 'll give your edits a go. I'm trying to get to a more functional style of programing, so I appreciate the pointer on the for loop.
Another question, kinda late, but what have your experiences been like using an ECS for a project like this? I'm working on something similar, so I'm wondering what specific components and systems you have, and how they work together to create the game, and also what problems and solutions you had while designing it.
I think you are looking for something like https://github.com/redox-os/rusttype which takes a font file outputs rendered text in some way
Instead of forcing people to make the executable for themselves, you could add it to the releases in GitHub.
That's really helpful for understanding error_chain, but is there something even more about fundamental design? Maybe its somewhere in the Rust book and I missed it but the error chain macro sets up your error, errorkinds, etc, but I also see (for example) csv hand rolling its own error and errorkinds. As an example, I want to do specific things with my errors from csv, but I end up with this awkward as hell error wrapper because I don't know why this error model is abstracted the way it is (https://github.com/medwards/transitfeed/blob/csv_crate/src/gtfs/error.rs )
That's a good idea. I've never tried that with an executable. Looks like I'll have to do some reading on cross compiling...
pangocairo
I'm trying to write a really basic program to find the mode of a vector of integers. This is what I have: let v = vec!(1, 3, 8, 2, 10, 5, 4, 2, 2, 1); let mut amounts: HashMap&lt;i32, i32&gt; = HashMap::new(); for &amp;n in v.iter() { if amounts.contains_key(&amp;n) { let amount = amounts.get(&amp;n).unwrap(); // here amounts.insert(n, amount + 1); } else { amounts.insert(n, 1); } } It tells be that, where I labelled *here*, I can't borrow *amounts* as mutable because it's also borrowed as immutable. The problem is, how would I fix this? Like, I know why it happens, but I'm not really sure what to do about it.
What on earth are these instructions doing in my generated, optimized assembly? https://play.rust-lang.org/?gist=53e05914d32f667c69af478506078f57&amp;version=stable In my main function: callq _ZN3std3env4args17haf05397a1d1aab2cE@PLT movdqu 152(%rsp), %xmm0 movups 168(%rsp), %xmm1 movaps %xmm1, 48(%rsp) movdqa %xmm0, 32(%rsp) It calls std::env::args, but then it starts doing weird things with floating point registers that I don't quite get.
Not everyone does this. Cross compilation takes time and infrastructure and while it is a good idea for projects like ripgrep for something this small you can't blame the author for just doing this for their own pleasure to learn the language. No need to be so harsh about it.
I'm not familiar with one, but I think the protocol is quite simple. You should just be able to spin up a web socket connection to your hub. Sorry I can't be more helpful. But the new SignalR bits (I think it's currently called ASP.NET Sockets) should be a nice experience to support. They've got [some specs](https://github.com/aspnet/SignalR/tree/dev/specs) you can have a look at too.
That was honestly a reason why I posted this.
Euclid is also very nice in that regard.
Do you have opinions on the wamp protocol? http://wamp-proto.org
What I did is create '[snippets](https://www.reddit.com/r/zsh/comments/2v1ar7/snippets_for_zsh/)' for zsh. Basically I have a letter or two and hit ctrl+j and it expands 'c' into '/mnt/storage4/complete'. Since you don't need to actually need to type 'cd' it becomes fairly quick "c,ctrl+j,enter". Also it works with other commands.
Thanks for doing this!
The Ion shell would benefit from this by being about to create maps at compile time with a static lifetime, rather than having to create them during runtime. Where would it use that? The map of built-in commands that the shell supports, and the map of colors used by the colors namespace. Although in the meantime, I've created my own structures and am using macros to initialize maps without needing support.
Awesome work!
It looks like it's just realigning some 128-bit values relative to the stack pointer. One move is for packed floats and one is for packed integers but the result is the same, so maybe it's just using them to exploit instruction-level parallelism?
&gt; This would be cheaper than passing string pointers around. Do you mind expanding on this? How is this cheaper if a char and string ptr are WORD-sized? (is that assumption incorrect?)
Ah, ok. That makes sense, thank you!
I'm assuming the cost is in an extra dereference, where you could be just passing a char by value.
Some poking around online indicates that GNU sort's comparison function is locale-dependent. If you need to implement the same comparison function, I'd suggest checking sort's source code and *hoping* that it uses a locale-aware comparison function from `libc`, then using that from Rust. There's nothing in `std` for locale-aware comparisons, as far as I know. Also, as a nitpick: `string::cmp` isn't a thing. There's no type called `string`. You're thinking of `str::cmp` *a.k.a.* `&lt;str as Ord&gt;::cmp`.
Oh I see, so that's why the rust uutils coreutils just uses 'str::cmp'. Thanks for the nitpick, I was aware it was 'str::cmp' but I just typed string out of habit.
And what is that error? It's a failed assertion, but it should tell you what the incorrect value it received was.
The problem with `string` is that you *could* be talking about `String` or `str`, and it's not clear which. In this case it doesn't matter, but sometimes it does.
Oh yay that's right, I was so confused when I just started with Rust.
Great news, this is really encouraging! OTOH, I don't want Rust to get **too** popular: where's my market value as a Rust dev if everyone's jumping on the bandwagon? :)
It would definitely be nice to have automated builds. I had to make a couple of changes to (crowbook)[https://github.com/lise-henry/crowbook] to enable the custom syntax highlighting so I'll first try to get those merged
https://rust.godbolt.org/ and https://play.rust-lang.org/ are your friends here. The best way to tell how smart the compiler is, is to see what it does. As for `#[inline]`, inlining is LLVM's favourite trick, especially with LTO enabled. If it looks like it can be inlined, it probably will be (with `--release` at least). Also, I don't mean to rip on your code, but maybe a pass with `rustfmt` before posting? I usually don't like `rustfmt`'s style, but it's 10x more readable for me in this case.
Huzzah! 
This posting set off some alarm bells for me. Please take care when distributing works like this. Here are some rather disorganized things that come to mind that may or may not be correct. The repository you linked seems to indicate the entire contents of the project are licensed under both the Apache 2.0 license and the MIT license. Both licenses allow you to rework, process, and republish the text, but only if certain conditions are met. One of the conditions is that a copy of the licenses themselves must accompany redistribution. You're generating book formats and publishing them online, where they could be republished and shared again and again. If you don't attach the license to these before sharing, you're probably violating the license you were granted. There is normally a bit of a gray area around source vs binary distribution, but in this case the vast majority of the text in the project repository is being published word-for-word. It's pretty close to just being a source distribution of the book files. There is an additional condition that relates to copyright and copyright notices. The general idea is that all copyright notices that apply to the portion of the work you are distributing must remain attached to the distribution. This project seems a little disorganized on this front. The copy of the MIT license indicates the project is copyright "The Rust Project Developers" and there don't seem to be any individual copyright statements on files. I have seen some projects note that copyright can be determined from version control history. There is a `COPYRIGHT` file in the repository that is an outdated copy of the same file from the github.com/rust-lang/rust project. The one in rust-lang/rust has been updated to clarify some of the confusion around copyright. It seems like the intention in rust-lang/rust is that all copyright is retained by individual contributors and there is no copyright assignment. That may or may not be the intention of the book project. It looks like you've attached a statement that the book is copyright "Steve Klabnik and Carol Nichols". That's probably true, but it is also copyright some other authors you've neglected to mention.
It's surprisingly difficult for thread A to *interrupt* thread B and force it to panic. (Java JVM was designed for that's feature, but it's still a bit ugly in practice.) Google "Posix thread cancellation" if you're not familiar with the difficulties that any Rust implementation is working with. I think your best bet at this point would be to fork `WaitGroup` and add the feature you want. BurntSushi seems like a reasonable guy and I'm sure he'd welcome a contribution.
You're absolutely right, I didn't think of any of that
Well Javascript is fairly widespread, yet many companies are looking for people who know it :)
&gt; https://rust.godbolt.org/ and https://play.rust-lang.org/ are your friends here. The best way to tell what the compiler will do is to see for yourself what it does. Yeah but like I said I don't understand assembly nearly enough to understand what it does. I don't even understand the basic example it gives there of a simple function let alone this complex one. &gt; Also, I don't mean to rip on your code, but maybe a pass with rustfmt before posting? I usually don't like rustfmt's style, but it's 10x more readable for me in this case. Yeah I probably shouldǘ e; I just copied it from the actual source. I'll update it after a format.
gotta get that sweet youtube money
There many different possible approaches, so if you give more context you might get better answers. Do you only want to render one single character (one glyph?), or arbitrary strings of text? Only English text, or could there be text in any language? In a specific font, or whatever’s available? What do you want to do with that bitmap afterwards? Export to PNG, show it on a screen, do more pixel-based processing? What kind of environment does your program run in? Is there a single target machine and OS, or is this a library that other people would use potentially anywhere?
I've never been directly involved in FIRST, but my peripheral knowledge of the subject leads me to believe that they only allowed Java, C++, and LabVIEW in competitions. Is this wrong? or has it changed? But, [Japaric](http://blog.japaric.io/) has lots of good info on embedding Rust on Cortex-M microcontrollers, if that's what FIRST uses.
They allow other languages. Also, the roborio runs embedded Linux, which rustc has a tool chain for, so it shouldmt be an issue.
&gt; If this trait were rust-internal, we could use debug_assert!; assert_eq! will check all Rust user implementations too. [Here's a demo of it working in a user application just fine.](https://play.rust-lang.org/?gist=126dae6d72dcf6c7a3e79972cde240dd&amp;version=stable) Maybe I'm misunderstanding the intent of that comment? Idk. It's really hard to say from this snippet of code whether Rust/LLVM is going to be smart about it or not. Looking at the assembly is the best way, and benchmarking the code is the second best way. I know you've already said you're uncomfortable with assembly, so maybe just build some contrived examples where you spell things out for the compiler and some where you see what the optimizer does and decide if it is actually optimizing it or not. In general, I would say it's better to build strong abstractions now and worry about performance later. Rust is an optimization-friendly language, in general, and LLVM has awesome optimizations, so it's rare that I see abstraction getting in the way of performance, but if it does, then you can solve the performance problems that actually matter. That's just my opinion, you're welcome to disagree.
gotcha, best of luck then! getting Rust into FIRST would be awesome!
HRTB are extremely rare; I think I’ve used one once in my ~4 years with Rust. If you’re using generics over closures you need them, but only in complex situations. They do exist for a reason!
Great! That's awesome to know. :)
Can you prerender the glyph using another program and include the result in your program? I recall that GIMP could export bitmaps as c header files. Anyway, if rendering a single glyph, look into using bitmap fonts which usually can be easily translated into a format of your choice and the logic is simple enough to hand roll. If you want to use an outline font like truetype or opentype, you'll need to parse the file, extract path data, and rasterize that path into the format of your choice. There are small rendering libraries that will handle all this for you (I vaguely recall SDL having something like this). If you want to render lines of text (especially if non-ascii), you'll have to use a full-blown font rendering library like cairo.
No market value as long as there are no jobs :D
&gt; crates.io is not even 3 years old, but already the number of crates has passed CRAN(R) and Hackage(Haskell). :'(
Haskell's goal is to "avoid success at all costs", according to Simon Peyton-Jones. That is definitely not Rust's goal, though; we want success! (And the baggage that comes with it, like the need for backwards compatibility.) So really, crates.io surpassing Hackage should be celebrated by both camps.
Well the point of the quote is avoid "success at all costs" not "avoid success" at all costs But I know what you mean
True, we need Rust to be popular enough that many companies are using it, but not so popular that Rust devs are dime-a-dozen. It's a tricky balancing act. :) 
RETF at it again I see
Check out https://github.com/b4b4r07/enhancd
Honestly, I'd be so happy if Rust devs became a "dime a dozen". That would mean the average programmer would have a *much better* grasp on memory ownership than they currently do, and that would be cause for celebration.
One barrier for me getting Rust into production at work is that there aren't enough people who know Rust.
So I looked at the ASM of: struct Foo(*const [i32]); impl Foo { fn as_slice(&amp;self) -&gt; &amp;[i32] { unsafe { &amp;*self.0 } } fn size_hint(&amp;self) -&gt; (usize, Option&lt;usize&gt;) { let len = self.as_slice().len(); (len, Some(len)) } fn len(&amp;self) -&gt; usize { let (lower, upper) = self.size_hint(); assert_eq!(Some(lower), upper); lower } } fn main() { println!("{:?}", Foo(&amp;[0, 0, 0, 0] as *const [i32]).len()) } And to my pleasure it was the exact same asm as when I removed the whole expression at the bottom and replaced it with a constant `4` But then I decided to add an extra 0 making it five to see if I could spot where the difference there would lie and that was also identical ASM even though the output was now "5" isntead of "4" so I'm really confused now.
Nice! API reference link on crates.io is broken btw.
Entry API to the rescue! *amounts.entry(n).or_insert(0) += 1;
In the end, I did just that: https://github.com/BurntSushi/chan/pull/22 Thanks!
`str` pointers have a size of `mem::size_of::&lt;usize&gt;() * 2`.
I honestly didn't realize that, and I've seen that quote all over the place.
The main difference to other pubsub approaches, like wamp, is that Mles provides a very simple basic pubsub service. Only one message type - and that's it. Authentication and distribution included. Still, the client can build on Mles whatever extended functionalities they may need between other clients. I think we need all kinds of protocol options for users to choose from based on their needs and use cases.
I haven't tried this particular thing yet but, from what I know of scripting things in general, how this kind of thing works under POSIX, and Rust's design philosophy, it's definitely possible. I'm about to go to bed, but my first hypothesis would be that maybe `BufReader` is being to aggressive. See if it works without it and I'll get back to you in the morning if nobody else has solved it in the mean time.
It's not identical for me (release, stable). There is this snippet: .Lcfi0: .cfi_def_cfa_offset 80 movq $4, (%rsp) movq %rsp, %rax movq %rax, 8(%rsp) movq _ZN4core3fmt3num52_$LT$impl$u20$core..fmt..Debug$u20$for$u20$usize$GT$3fmt17h81d74262ac606fbbE@GOTPCREL(%rip), %rax [...] callq _ZN3std2io5stdio6_print17hc2847a6726c4b4a3E@PLT Where `4` gets changed to a `5`. There's also this trick I use to make the assembly output easier to skim: https://play.rust-lang.org/?gist=08787aaf713630a94477cdd11d0be401&amp;version=stable.
There's a simple example of using rusttype to write to an image at https://github.com/PistonDevelopers/imageproc/blob/master/examples/font.rs. Even if draw_text function itself isn't what you're after its source code https://github.com/PistonDevelopers/imageproc/blob/master/src/drawing/text.rs might be useful as a demonstration of using the rusttype API.
I think I spottet a little mistake: &gt; In addition to community, Rust-focused contractors like Integer 32 and Asquera have sprung up. Both contractor-links point to the same address (https://twitter.com/asquera).
That's a fallacy: it assumes the market for capable programmers is too small for everyone. This is false, as Java comfortably demonstrates.
It's a string *slice* which is both a pointer and a length. 
Same for me.
How do I identify beginner-level issues to which I can try to contribute on Github?
The ambiguity is part of the fun
&gt; OTOH, I don't want Rust to get too popular: where's my market value as a Rust dev if everyone's jumping on the bandwagon? :) IMHO if that's the problem we've probably mitigated or solved so many problems plaguing the system software today, that it would be worth the bother :)
Are CRAN/Hackage curated/moderated in any way? crates.io is full of "I made Hello World and published it"-style crates.
A silly and dirty crate to load Linux kernel modules, in an "alternative" way: https://github.com/lucab/modinsert-rs
/r/playrust
Working on my turn-based strategy game [Zemeroth](https://github.com/ozkriff/zemeroth/). Last few weeks I: - [Added a component system - 'Rancör'](https://github.com/ozkriff/zemeroth/pull/141) - [Added boulders](https://github.com/ozkriff/zemeroth/issues/142): https://i.imgur.com/pMXSDAk.png - Implemented some basic abilities - "jump" &amp; "push": https://youtu.be/Egfyd4VX2YU This week I'm going to finally implement [Throwable bombs](https://github.com/ozkriff/zemeroth/issues/69). I'll try to post a new ["monthly" report](https://ozkriff.github.io/) soon. ------ [@ozkriff on twitter](https://twitter.com/ozkriff), [imgur devlog](http://imgur.com/a/SMVqO)
When using generic libraries with application code one should wrap them. Rust's traits can greatly help with this. Wrappers will make your code cleaner, by narrowing provided interface. You also will be able to replace wrapped library relatively easy. 
Interesting, could you at least disclose the industry you work in?
Hackage is not curated at all, I can't speak to CRAN. Stackage builds on top of Hackage and provides a level of curation (confirming that things build together), and has about 2500 packages in its set (https://www.stackage.org/lts-9.11).
&gt; So really, crates.io surpassing Hackage should be celebrated by both camps. I think there are lessons the Haskell community could learn from Rust (particularly with regards to community building), but it's still a bit frustrating in other ways. 
To me this looks like a bad hack to work around the borrow checker. Things would break if you for example accidently called some method that accessed `the_real_vec` field while it's set to the temporary value.
[removed]
Counting the number of packages seems a bit futile, when there can be significant differences in the granularity of packages. By count, npm has more packages than every other package manager put together*, but in part that's because there's a culture of publishing many tiny packages. * this is wild hyperbole 
CRAN has a revision process. Your package has to pass automated checks (including unit tests if you defined some), and if it doesn't, you talk to the maintainers on the mailing list until it does.
&gt; slice: *const [xcb::Window], // this should be a &amp;'a [xcb::Window] but sadly Rust's type system can't express this Wait, rust's type system can express this? Just as you wrote, &amp;'a [...]. Or am I wrong?
I don't really understand the explanation. Can someone try again for me?
Ah, sorry. Yes, the error is: ---- parser::tests::test_amountexpr_next stdout ---- thread 'parser::tests::test_amountexpr_next' panicked at 'assertion failed: `(left == right)` left: `Incomplete(Size(12))`, right: `Done([], (Plus, AmountExpr { amount: Amount(12, Minute), next: None }))`', src/parser.rs:296:8 
&gt; Is this possible in Rust? No. The best approximation available is to use a method... trait Value { fn value() -&gt; i32; } pub enum Int0 {} impl Value for Int0 { #[inline] fn value() -&gt; i32 { 0 } } fn value_of&lt;T: Value&gt;() -&gt; i32 { T::value() } value_of::&lt;Int0&gt;(); ...and then hope the optimiser does what you want.
I just ran a test in rust.godbolt.org, and yes, yes it does optimize this. Thank you! https://i.imgur.com/46JD6Im.png
Last week I put a lot of time on [Sucredb](https://github.com/arthurprs/sucredb). Now it has basic support for Redis hash/set types using CRDTs. This week I plan to write more docs and start work on support for [MULTI/EXEC](https://redis.io/topics/transactions) operations on the same shard key (made possible by Keys hash tags like: `user:{123}`, `friends_set:{123}`, ...), after that Sucredb should be usable in real projects.
How do I get a Unicode code point from a char? The doc says char is a Unicode scalar value, which is not the same.
avoid $ success at all costs
You can already use associated constants though.
I tried it, and it still doesn't work: trait Int { const N: usize; } fn value_of&lt;T: Int&gt;() { let _ = [0; &lt;T as Int&gt;::N]; panic!(); } Gives: error[E0277]: the trait bound `T: Int` is not satisfied --&gt; src/main.rs:6:21 | 6 | let _ = [0; &lt;T as Int&gt;::N]; | ^^^^^^^^^^^^^ the trait `Int` is not implemented for `T` | = help: consider adding a `where T: Int` bound = note: required by `Int::N` 
Take a look at [typenum](https://crates.io/crates/typenum) crate, it's essentially a cool abuse of type system to emulate const generics (which I really hope we will get relatively soon), but unfortunately sometimes it can be quite unergonomic, plus compile times take a small hit.
All your tests pass if you add a `complete!` inside your `amound_expr` parser: ``` named!(amount_expr&lt;AmountExpr&gt;, do_parse!( amount:amount_parser &gt;&gt; opt!(sp) &gt;&gt; o: opt!(complete!(amount_expr_next)) &gt;&gt; (AmountExpr { amount: amount, next: o, }) )); ```
&gt; Here it is in Rust with more sane error handling. I won't pretend I'm a Rust ninja, but I feel like mine is a lot more readable and concise. You are not handling errors, but causing them to terminate your threads. This works out in your case because: 1) You are using expect within threads which isolates your main thread from dying when they are encountered, and 2) Your threads are transient, so having them die off isn't a huge problem. I recommend avoiding .expect() or .unwrap() outside of tests unless you really mean for the program to die leaving a message in the sand. The .expect() works out in this case, but if it were production code and it needed to be refactored for some reason, you might end up seeing .expect() being used in code intended to be long running. For example, if this were refactored to use a static thread pool set to the number of hardware threads and then something like channels to communicate between them (a common performance optimization for web servers), those .expect() calls would become a problem. For your rewrite of the example explicitly, you might keep the .expect() within the thread::spawn section for brevity, but then introduce an Error type and bubble all other errors. 
You very rarely write code for only one specific lifetime. Usually you want to be generic over many lifetimes, such as: fn foo&lt;'a&gt;(x: &amp;'a str) -&gt; &amp;'a str This doesn't declare one function. It declares an *infinite number of functions*, one for every possible lifetime that can exist. Similarly, a line like impl&lt;'a&gt; IntoIterator&lt;Item = &amp;'a T&gt; for &amp;'a SomeStruct implements an *infinite number of types* at the same time; one for every lifetime in existence. In the same vein; this line where for&lt;'a&gt; &amp;'a L: IntoIterator&lt;Item = &amp;'a T&gt; doesn't just make one restriction on the type `L`, as `where` clauses usually do. It makes an *infinite number of restrictions* on `L`, one for every lifetime. It matches the `impl` above, so `SomeStruct` can be used for `L` in this case. You can use the `for&lt;'a&gt;` syntax in some other places too, like in trait objects iirc. But like Steve said, it's very rare. 
This is the reddit for the Rust Programming Language. I suspect you're looking for Rust the game? If so, check: /r/playrust
Yes, definitely, I'm more interested in it becoming a thing that in it becoming _my_ thing :D. The experimental code is here: https://github.com/skade/crates-io-changes-stream It's rough, basically, it currently streams out the history in JSON (not in proper CouchDB changes stream format). I'll document it a little tomorrow. Feel free to ping me on the issue tracker.
You tried using it *generically* in an array length, where you *couldn't* use a method anyway. And yes, that's the one major thing about associated consts left to enable - we can't switch it on right now because rustc isn't lazy enough to avoid cyclic dependencies.
Wow, awesome, thanks for your help! May I give credit to you in the commit message?
Right, but if there's no expressive difference between associated consts and methods, you might as well use the one that works across more versions of Rust :)
&gt; ? with error_chain in Rust Error bubbling is a common pattern in Rust and not exclusive to the error_chain crate.
True, but without it you have to write some extra code for converting errors. 
Ah, sure. Personally I find that sort of thing counter-productive, and would not support anything older than the latest stable (if that) in my own software. Old versions are historical record, riddled with long-since fixed bugs and missing useful features.
And with it, matching against errors becomes a bit more complicated. :). I understand why people choose to use the `error_chain` crate. Your response just made `?` seem exclusive to it. :)
This one looks smart but I'd rather stick with simple and cheap solution, the one with inline trait function serving the const value. Thanks though, I will remember this if I need it later.
As someone who has been trapped on old versions of tools before, I'll support as far back as I reasonably can. *I still use `try!` in some crates.*
If you do end up switching to specs, that would be a huge boon to Rust game development. If there's significant convergence around specs there will also be greater convergence around game tooling (see Amethyst) and general tutorials.
I edited my response, you are right. The post compares code sizes, which error_chain usually reduces, which is why I listed it. But a big benefit of Rust is still not polluting the main code path, the error conversion code is outside of it. 
I don't think it's necessary. Best!
Crates.io has a ton of micropackages in the style of npm. Don't know about CRAN, but Hackage ones are definitely larger on average.
In my experience, Hackage is full of abandoned concept-packages, too. There was a suggestion about having a Rust platform like the Haskell platform with some curated, blessed, crates, but that was heavily criticised. https://internals.rust-lang.org/t/proposal-the-rust-platform/3745
1. https://this-week-in-rust.org has a weekly list 2. the [findwork](https://www.rustaceans.org/findwork) site has a running list 3. many projects have issues marked as 'easy', e.g. [clippy](https://github.com/rust-lang-nursery/rust-clippy)
You're bad at selling, then ;). If Rust becomes very popular, you can sell yourself as being ahead of the pack by years.
If I understand correctly you can just cast it to `u32` using `as`
I've actually wished many times that I could use `for&lt;&gt;` with types, not only lifetimes. This happens when you have a function that wants to take a generic function as argument (not necessarily a function) and you want to call that with different types: // silly example: fn foo&lt;F&gt;(x: u32: f: F) where for&lt;T&gt; F: FnOnce(T) -&gt; () { if x &gt; 0 { f(x as u64) } else { f(x as u32) // arg type differs } }
I'm guessing these errors have to do with Rust the game? The subreddit you're posting to is for Rust the programming language, which has no relation to the game. Perhaps they can help you at /r/playrust?
I implemented the stage editor for my game, Platform Fighter Sandbox. [Youtube Video](https://www.youtube.com/watch?v=bOryyXE1koI&amp;feature=youtu.be) [Github](https://github.com/rukai/PF_Sandbox) Previously I was just modifying json files by hand. This addition is a lot of fun to play with, in part because its a lot simpler than the fighter editor. It makes my game feel like a real sandbox. Note that it currently only supports the GC -&gt; Wii U controller adapter for input.
Is there an idiomatic way to concisely handle error conversions between types defined by 3rd parties? I'm using [Iron](https://docs.rs/iron/0.5.1/iron/), where handlers for routes are defined like e.g.: fn hello_handler(_: &amp;mut Request) -&gt; IronResult&lt;Response&gt; { Ok(Response::with((status::Ok, "hello"))) } where `IronResult` is defined as `type IronResult&lt;T&gt; = Result&lt;T, IronError&gt;;`. When I have more complex handlers, it'd be nice to be able to use something like the `try!` macro, but I don't think I can define `From` on `IronError`, or `Into` on things like `std::io::Error` since they not defined in my module. I don't think I can return from the current function with any of the `unwrap_or_else` type functions either (though if I'm wrong, that would be a good solution here). At the moment I'm using `match` all over the place to handle errors here, but it's making the code very verbose compared to a lot of other functions I've got, e.g.: fn hello_handler(_: &amp;mut Request) -&gt; IronResult&lt;Response&gt; { let x = match some_io_operation() { Ok(value) =&gt; value, Err(io_err) =&gt; // ... rewrap as an IronError here ... }; let y = match parse_some_integer() { Ok(parsed) =&gt; parsed, Err(parse_int_err) =&gt; // ... rewrap as an IronError here ... }; // ... etc. ... Is there some better way I'm missing? 
Totally. :). One of my favorite Rust features is type variants and the respective compiler enforcement around handling them. Regardless of how the type variants are implemented. When that comes to Errors, Rust makes it really easy to move the errors forward, but then still forces you to properly handle them when it does come to doing so. (If you plan on discarding the handling of a variant in a match statement you have to do so explicitly. Otherwise the Compiler has your back and lets you know you've forgotten something!)
Yep, would really love to have this! Wondering if there's an issue open on the RFC issue tracker. Would be interested to see how this would play with monomorphisation. Because you can't specialize `F` because it is called with wit two different argument types…
crates.io
&gt; And yes, that's the one major thing about associated consts left to enable Every time somebody heards on IRC that they can use associated const this "one major thing missing" is the only thing they try to do with them. When somebody mentions associated const and somebody else says "really?!" I start counting deception in 3, 2, 1,... "this doesnt compile". Classic.
What kind of libraries are you interested in? There's not going to be consistent answer for all possible libraries.
A Unicode scalar value *is* a Unicode code point. The only difference is that the code points from 0xD800 to 0xE000 are invalid scalar values, and are therefore also invalid to store in a `char`.
File reading / parsing (e.g. json/csv), data analysis, REST/websocket libraries.
Seconding what Steve said, I've been using Rust for six years and have never had need of this feature. I hear that it's invaluable if you're doing in-depth library design and trying to go above and beyond in making your APIs as ergonomic as possible, but as a library consumer you never need to worry about this.
It is Rust Evangelism *Strike* Force, not Task Force!
Thanks for catching, I filed a PR. https://github.com/jonathandturner/jonathandturner.github.io/pull/2
Yeah, I'd say there are great libraries for those things: https://github.com/BurntSushi/rust-csv https://github.com/serde-rs/json https://github.com/housleyjk/ws-rs or https://github.com/snapview/tokio-tungstenite 
Is more important the work being done over C++ porting from Python (check some ops over here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/cc/gradients) The C API is rarely extended because how it is set up is rarely necessary to make changes to it (basically you work by passing op names and attributes for those operations except for some exceptional things the data structures used remain pretty much all the same).
Wrote a small REST JSON client with automatic serialization/deserialization (missing proper error handling): https://github.com/spietika/restson-rust I'm pretty new to Rust so the main motivation to write this was to familiarize myself with the language.
Well, without someone investing much research, these numbers are currently all we have. libraries.io could be such a project. But, for example, Hackage is also not shy of such things, for example: https://hackage.haskell.org/packages/#cat:Cloud takes up a _substantial_ part of these 11k packages.
&gt; crates.io is full of "I made Hello World and published it"-style crates. or even straight-out name squatters, like [soap](https://crates.io/crates/soap).
If you say "definitely", I'd like to see numbers. We have _huge_ codebases on crates, starting at servo over all the gaming libs, ending at web frameworks of substantial size. Sure, there's a long tail of very small things, but Hackage has that, too.
Nobody mentioned that we did accept an RFC for this feature, and so you *will* be able to do this natively in the future.
Thanks for the response. I did try and call `dd.stdin.flush()` but that didn't seem to do anything. I will have to keep digging on the rust implementation of child process.
Yeah seriously. We lose nothing by having more people use the language. Look how many Java developers there are. They're not getting paid less or anything than others.
Usually [error chain](https://github.com/rust-lang-nursery/error-chain) is recommended here but you can also do it without that too (just do something similar manually without using the macro).
I've been using Rust for a few years now and never ran into needing this. I didn't even know you could do this
You forgot https://github.com/cyderize/rust-websocket which is fantastic. It has both a sync and an async (Tokio based) API.
Isn't that basically what higher kinded types are for?
Link to Integer 32 leads to Asquera.
Templated functions are required for this trick to work and Rust doesn't have them. Generics are compiled in an abstract way that eliminates these types of tricks. Note that you can do this for a Trait or Enum or similar trick to get close enough.
I did not post my subject in the right place? Yet I have /r/rust/ in the link of my subject. But I can try in /r/playrust. It's about error messages on my rust dedicated server. 
I recently implemented a lot of functionality to [Hexe]'s `PieceMap` type and made things perform better: - Last night I made `is_empty` go from ~13ns to 2ns via the `simd` crate, which is nice. - Made use of the memory layout to add efficient rank-wise piece swapping, insertion, and removal. - Added `find` and `rfind`, which use `memchr` and `memrchr` respectively to find the first or last piece and return its square, if any. I've now added more than enough functionality to `PieceMap`, so I'm going to move on to other aspects of the engine such as position representation. [Hexe]: https://github.com/hexe-rs/Hexe
If this is an exercise in learning processes in pipes for Rust, that is fine, otherwise, do you need to exec `dd` at all? You could just do exactly what `dd` is doing, which is writing bytes into a file at a tune-able block size. It would take about 20 lines of code. Then you wouldn't need to muck with processes or pipes, just write a block, then print an update status. That said, I notice a couple issues: 1. You're not executing kill.SIGINFO anywhere on the child process like you are in line 32 of your nodejs script. 2. In line 34 of the rust program, you're telling your loop to take 10 lines of dd output before doing anything... odds are is that it is only proceeding after it gets an EOF from the child process. 
&gt;You are not handling errors, but causing them to terminate your threads That's what the original *and* the Go code did. It's obviously not the best way to do it, but it seemed like brevity was the goal.
I guess right now the workaround for these use-cases is to use build.rs?
Here's a sample usage invocation: nunit-to-html -i input.xml -o output.html --template some_template.html ...where the contents of `some_template.html` might be like: &lt;!DOCTYPE html&gt; &lt;html lang="en"&gt; &lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Document&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;pre&gt; name: {{test-run.name}} date: {{test-run.date}} time: {{test-run.time}} total: {{test-run.total}} errors: {{test-run.errors}} &lt;!-- ... --&gt; nunit_version: {{test-run.environment.nunit_version}} &lt;!-- ... --&gt; {% for test-suite in test-run.test-suite %} * Test suite "{{test-suite.name}}" suite-type: {{test-suite.suite-type}} executed: {{test-suite.executed}} &lt;!-- ... --&gt; &lt;!-- There are more `results` containers inside --&gt; {% endfor %} &lt;/pre&gt; &lt;/body&gt; &lt;/html&gt; Now to answer your question directly: &gt; Should the user just supply a bare minimum for displaying the test name, and your program will wrap that in a recursive macro you described? Ideally, the only requirement for user input should be the three items in the invocation about: input data, output location, and the template to transform the NUnit schema into HTML. There are two options for taking the recursive macro route that I see, as stated in the OP: * If I force users to simply adhere to a pre-built macro, then they couldn't structure the HTML the way they want. Bleh. Styling can get you a long way, but there's no substitute for letting users provide their own HTML, IMO. * If I require a user to provide their own macro file, then they have to have a separate arg for that. Also bleh. A compromise might be that a default macro could be provided, with instructions for overriding it. This might be the best option for now. For my own uses, I would almost prefer to simply convert the entire NUnit structure into a JSON blob, which could be inserted at the beginning of the HTML document with a script tag. That way, at least procedural generation of HTML could be done with JS...but that's already starting to defeat the point of using a templating engine, eh? :P
Me learning Rust (casually) for ~2 months. [Me after reading that](https://i.imgur.com/IppKJ.jpg?fb)
It's definitely this -- as somebody wrote in the [C2 Wiki] said &gt; Any performance problem can be solved by removing a layer of indirection.
Big shout-out to `serde` -- who knew that serialization and deserialization could be so magical? :)
The project sounds awesome and I'd love to help, except 1. I'm not very good with Rust (yet) 2. I know nothing about these robots 3. My days unfortunately also only have 24 hours If you do it, I wish you luck.
This sounds really interesting to me too.
Sure, but a quick jump to the search engine of your choice leads to [similar problems on hackage](https://github.com/haskell/hackage-server/issues/461). I'm not saying name-squatting isn't a problem, but in a discussion like this one, you'd need notable proof that it is more of a problem on crates.io then on hackage.
Don't worry, this is the most obscure feature in the entire language. :P It wasn't even described in the original edition of the book, IIRC. If you aren't trying to do crazy things with types then you can safely ignore this.
[This is how I solved it in Ion](https://github.com/redox-os/ion/blob/fe365337c5620408251f9da8a539b58dbecf0c37/src/builtins/mod.rs#L34:L84)
Hmm.. yeah, I guess I could just implement a macro similar to `try!` for this case. I've not looked at error chain yet, it's been on my todo list for a while, thanks for helping remind me.
This uses the imageproc library but it's super easy to render a character by hand with rusttype. You just need to pass a closure which writes into an arbitrary Buffer/Image per pixel. You have to do a little setup though, like creating and scaling a character first, but the rusttype docs are absolutely perfect. Just in case you want to avoid additional dependencies and only need single characters.
rust-websocket still depends on Hyper 0.10 (not-tokio-based), not 0.11.
&gt; outputs rendered text in some way It takes a closure to draw it. It's really flexible and I like it.
Maybe tunapanel? https://crates.io/crates/tunapanel
If you have an acceptable alternative to panicking, then you could always use something like `{checked,wrapping,saturating,overflowing}_{sub,add,mul}` (see an integer types docs -- sounds like [`usize`](https://doc.rust-lang.org/std/primitive.usize.html#method.checked_add) might be what you were just talking about)? For instance: let i = 0; let y = i.saturating_sub(5); // y is 0! This lets you define fallback behaviors much more easily than simply panicking -- if it's appropriate, you can easily convert something like a `None` being returned from `checked_sub` etc. to a `Result` with `.ok_or("integer overflow!")`. If you still feel like panicking, then make it explicit(!): let i = 0; let y = i.checked_sub(5).expect("no integer overflow"); // panics! 
&gt; I've been bitten by overflow before, so I build with debug assertions enabled in release mode. Whoa, hold the phone! I definitely think this is fine if you're just trying to learn the ropes, but as soon as you try to use Rust in the real world (which I so hope you do!) I would caution that **for most developers, having debug assertions present in released code would be very unexpected**. For instance, let's say you have some really heavy logging that only gets enable in debug mode -- you'd be losing one of the main advantages of using Rust if you enabled debug-gated features for `--release`. It would be totally reasonable for somebody unfamiliar with a codebase they were working with to assume that their debug assertions shouldn't have an impact in production (see [this issue](https://github.com/rust-lang/rust/issues/17081) in the `rustlang` repo for an example of this -- that issue is actually the reason debug code doesn't compile by default in `--release`!) There are definitely alternatives to enabling debug assertions for integer overflows in release mode that would probably fit your use case, including panicking -- see [reply to the GP that I made about checked arithmetic](https://www.reddit.com/r/rust/comments/794irl/how_might_allocation_fail_other_than_being_out_of/dp3gqs4/) that I hope helps! I hope that you don't feel attacked, or anything -- I just don't want to see you keep up a habit that I think might hurt you in the long run. :)
A good, yet another formulation of the sentence that looks like something it isn't.
Thanks for the reply. It is more of an exercise in continuing to learn rust than specifically learning process pipes. Towards the end of the day I did think it might be easier to just bypass dd all together, so I pulled up a copy of the dd source from [here](http://lingrok.org/xref/coreutils/src/dd.c) but a lot of this seems really over my head. Maybe it was just late in the day or maybe it is my lack of experience reading/writing C? Looking at what `child.kill()` does, it doesn't seem like I have the ability to send SIGINFO but only SIGKILL with that method but maybe I am misunderstanding the docs? The node.js `child.kill()` [takes an optional signal argument](https://nodejs.org/dist/latest-v6.x/docs/api/child_process.html#child_process_subprocess_kill_signal), so this isn't just a method for sending SIGKILL but a method for sending sending any signal to a child process (also maybe a misunderstanding of the underlying infrastructure?). My attempt to send this signal is on line 48, and maybe that is a big part of my problem, the child process is isn't interpreting my write as a signal but instead the actual text? I'll try adjusting the read lines, or moving to reading raw data from the BufReader and see if that helps. I had been trying another path for most of the day and switched to this very late after reading a stack overflow answer and I think I had just been too fatigued at that point. Thanks again for the response, I am going to see if I can't make some more progress this evening when I get home from work.
Bit-level fixed packing for hardware stuff like registers, various endianness and bit numberings and other boring things that quickly lead to typos and frustrating bugs... https://github.com/hashmismatch/packed_struct.rs
You can do `#[inline]` and make it a regular parameter, since that will essentially guarantee that it will be const-folded. Rust doesn't have non-const `alloca` on stable though though so you can't use it for the size of an array.
Government, I'm in the armed forces
I'll just do TWiR and perhaps blog about a recent comment.
So if I don't need to use fixed allocations that'll do just fine. Thanks!
If a file contains several definitions for structs/functions etc., should they usually be grouped by the struct they're defined for, or by the trait they're implementing? E.g. which of the following two styles would be preferred: // style A struct Foo ... impl Display for Foo { ... } impl From&lt;String&gt; for Foo { ... } struct Bar { ... } impl Display for Bar { ... } impl From&lt;String&gt; for Bar { ... } vs. // style B struct Foo { ... } struct Bar { ... } impl Display for Foo { ... } impl Display for Bar { ... } impl From&lt;String&gt; for Foo { ... } impl From&lt;String&gt; for Bar { ... } 
You have indeed posted to the /r/rust subreddit, but that one has nothing to do with the Rust game.
The same idea works in Rust, it's just a little more ceremony because Rust requires traits (and, compared to C++14 `auto` closure parameters, doesn't allow for closure syntax). trait Foo { fn do_it&lt;T&gt;(x: T); } struct FooFn { ... } impl Foo for FooFn { fn do_it&lt;T&gt;(x: T) {} } fn foo&lt;F: FooFn&gt;(x: usize, f: F) { if x &gt; 0 { f.do_it(0_f32) } else { f.do_it(0_f64) } }
You can specialize: first `foo` is monomorphised, meaning `F` is statically known, and then the calls to it are specialized, just like other generic functions: once `foo` is specialized, it's not particularly different to `f` being a normal global `template&lt;typename T&gt; void f(T&amp;&amp; x)` instead of a parameter.
## Indigo UI Framework This past week I got some more work done on Indigo, my UI framework prototype for Rust. (See last week [here](https://www.reddit.com/r/rust/comments/7860s7/whats_everyone_working_on_this_week_432017/dosn7et/) This week saw some changes to the style system and how the markup plugin works. Here's what's new: ### Style changes I spent some time simplifying the style system and doing related work in RSX to make things less confusing. Previously you'd declare an RSX node like so: ui!(&lt;StackPanel Alignment={Alignment::Stretch}&gt; &lt;Button Height=400. /&gt; &lt;Button Height=400. /&gt; &lt;/StackPanel /&gt;); and the attributes would get collected and built into a Style (collection of properties) struct of "explicit" properties which was then merged into the "resolved" style (as a result of finding which Style the element's StyleId resolved to). In an effort to simplify things, I did away with the notion of StyleId and with StyleContext (a collection of Styles per component) and made it so that each element has a default style (defined in the Component using the #[component] derived plugin) and an explicit Style (passed into the element using the Style attribute on the RSX node). Every other attribute that isn't named Style gets built into a separate struct and passed as a "props" object to the component. In addition to simplifying Styling, this fixes a long standing issue with RSX where the direct content nodes would be "erased" after the plugin expanded the RSX syntax the initial time. This made invalidation (which is done with subsequent calls to render) have corner cases where the children of components (Say a StackPanel) would disappear. I plan on integrating the props definition into my #[component] derive plugin so that you can define it more naturally via a props! macro inside of the trait impl. Here's some code showing off the implementation of the StackPanel component: #[derive(Debug, Default)] pub struct StackPanelProps { pub children: Vec&lt;ComponentRef&gt;, pub Orientation: Orientation } #[component] impl Component for StackPanel { fn default_style(&amp;self) -&gt; Option&lt;Style&gt; { Some(style! { Layout: Layout::Stack(StackLayout(self.props.Orientation)); }) } fn render(&amp;mut self) -&gt; Vec&lt;NodeRef&lt;Box&lt;Component&gt;&gt;&gt; { let children = self.props.children.clone().into_iter().flat_map(|ele| { ele }).collect(); children } } You'll notice the render() function operates on the children passed as props, collected into a flattened vector of components. Also default_style() uses the Orientation prop and passes that to layout. This lets the component author selectively expose props on their Component to allow more expressiveness in defining their component input. All of this paves the way for more straight forward invalidation of the visual tree with weird issues. ### Drop Shadow support I added support for the DropShadowProperty which hooks into WebRender's box shadow support. It supports multiple shadows per primitive: #[derive(Clone)] pub struct DropShadow { pub blur_radius: f32, pub spread_radius: f32, pub x_offset: f32, pub y_offset: f32, pub shadow_color: webrender_api::ColorF, pub shadow_type: ShadowType } #[derive(Copy, Clone)] pub enum ShadowType { Outset, Inset } and used in a Style: let scroll_viewer_style = style! { ... DropShadow: vec![ DropShadow::new( ShadowType::Outset, 0., 2., 4., 0., ColorF::new(0., 0., 0., 0.2) ), DropShadow::new( ShadowType::Outset, 0., 25., 50., 0., ColorF::new(0., 0., 0., 0.1) ), ]; }; which is then passed to a Component: &lt;ScrollViewer Style={scroll_viewer_style}&gt; ... &lt;/ScrollViewer&gt; I posted this late last week, but [here's a screenshot](https://imgur.com/a/c4Di3) of a work in progress TodoMvc port for Indigo. ### Next week I hope to finish my internal refactoring to make invalidation really rock solid and also perhaps start on an TextBox component. Perhaps even add styled text runs for the TextBlock component.
Rust has templated/generic functions too, and Rust generics are compiled in a very similar way to C++ ones: they're just as concrete. The only difference is Rust requires trait bounds so that the requirements for calling a function are known from the signature, rather than only finding out after monomorphising.
I'll let other folks share their experiences, but one place you may find useful is the FAQ. It goes into more detail as well as letting you skip around in related topics. https://www.rust-lang.org/en-US/faq.html
If by some miracle I manage to find time inbetween midterms I'm planning on getting [quickflux!](https://github.com/benlau/quickflux) and [rust-qt-binding-generator!](https://github.com/KDE/rust-qt-binding-generator) to play nice. I'm curious to see if I can write a flux architecture where all of the store is written in rust with just the UI in QML.
The answers to both of your questions are in the concept of [ownership](https://doc.rust-lang.org/book/second-edition/ch04-01-what-is-ownership.html).
Your questions intersect: understanding how memory management works in Rust is exactly the part that people say is harder than Go. Rust's memory management is not really "manual" in the sense that you're taking it - you do not have to "delete" anything. However, Rust also does not have a garbage collector. Instead, Rust manages memory through a set of rules: * Whenever a variable goes out of scope and can't be used anymore, the memory associated with it is freed. * Ownership and borrowing make sure that nothing can go out of scope and have its memory freed while there is another way to look at that memory. Its this ownership and borrowing system that is very different from other languages, and that many people struggle with as they're first learning Rust. I recommend [into_rust](http://intorust.com/), a series of screencasts which explain how ownership &amp; borrowing works, for more information.
Could you give a link to the RFC?
https://github.com/rust-lang/rfcs/blob/master/text/2000-const-generics.md
Sorta. HRTB, as the name implies is about higher *ranked* types, not higher *kinded* types. They're different things.
Agree to disagree
2\. Only in very rare cases do you need to manually manage memory. When you use a simple value or struct, it goes on the stack, which is automatically freed once the function returns. With more complicated values like `Box`, `Vec`, and `String`, the programmer who wrote those containers specified code that is automatically inserted when the value goes out of scope to free the memory. It's extremely rare to need to do that yourself unless you're writing a container like those or interfacing with another language.
It's not in either edition, actually. It's so marginal we decided to cut it from V2 as well.
I think anything that can fit on a page is probably better suited to a wiki where it can be copied and paste.
Oh, right. I'm still having trouble visualizing what HKTs would allow us to write that we can't right now.
It's a bit low-level, but one instance of HKT is a trait that returns `impl Trait` in one of its methods.
What would be a use-case of this?
http://www.oreilly.com/programming/free/files/why-rust.pdf
I haven't run into a need in Rust, but basically, the higher you move up the generics ladder, the more stuff you can write that abstracts over things. So like, something that works with Rc and Arc both; something that works with Option and Result both, etc.
Thank you!
You're saying that the numbers are comparable (name-squatting can happen in both places), but in doing so you're showing that the numbers don't mean anything.
IIRC a usecase for HKT would be that you could return e.g. `ìmpl Future` or `impl Iterator`, greatly simplifying return types for code that deals with futures/async (so you don't have to return Box&lt;Future&lt;something&gt;&gt;, thereby avoiding an allocation for the Box) and complex (multi-layered) iterators.
&gt; In Rust you need to manually manage your memory. I (think I) understand what a GC does: It throws unnused variables out of the memory. But what does that mean for Rust? The beauty and the innovation of Rust is in the fact that you don't need a GC, but you also don't need to manually manage memory! But therein lies some of the complexity that makes Rust hard to approach. [Here](https://play.rust-lang.org/?gist=f13975316879554957d552246f3dedbe&amp;version=stable) is a contrived example of the kind of error I ran into *constantly* when I first started, for example. It doesn't take long to run into the borrow checker. However, once you get it figured out, it gets easier and Rust becomes very pleasant to write... and once you're done messing with the compiler, it tends to **Just Work**&amp;trade;
Rust does have generics, but not "templates" (I'm assuming you mean C++-style templates here). The `json` crate is fairly basic, if you want to deserialize JSON directly into Rust types, look into [Serde](https://serde.rs/).
When/If you start to make your own Wayland Window Manager (and you plan to use Rust to build it) feel free to pm me and I can offer you some guidance.
I'm saying that the numbers are flawed, and bickering around name-squatting is just as. In the absence of anything else, it's all we have.
I am a noob, delving into lower level things for the first time. Is there any rust tutorial on networking stuff?
In my case, the math was simply incorrect and it shouldn't have been possible for it to overflow at all. I would probably have used checked sub if I expected it to be possible to overflow.
&gt; Does rust have reflection? Not at runtime, but at compile time, you can do things. Eventually very many things, but just a few for now. &gt; What about generics Very much so. &gt; or templates? Rust's generics don't do what templates do, which is "expand the template and then check things." Rust's generics do "Check things and then expand the template".
Why do you need errors to be `Sync`? failure currently requires `Send` but not `Sync`.
I haven't really grasped it very well, but I've read about how it could solve the problem of nesting match statements (by flattening it as function calls I guess) when going through a long chain of `Option` or `Result` types. Wouldn't that be very desirable for rust?
I have not read much about rust but come from C, C++ and Go background. Is rust memory model based on reference counted objects? Different reference counts for shared and exclusive access. Block exclusive access till shared reference count reaches 0
Don't we have `impl Trait` already in nightly? Isn't that just anonymized types? It doesn't look like HKTs to me.
kind of, yes, but it is done at compile time. for cases where it is impossible to prove at compile time you can also move the checks to runtime, but will pay a runtime cost, but that is done rarely. were all objects reference counted then it would just be a gc.
In addition to what others have said, Rust’s macros are more powerful than C/C++ macros, and can do many (if not all) things usually done in C++ by template metaprogramming.
Rust cannot currently express the idea of one member of a struct having a lifetime that depends on another member like with a hypothetical syntax of: pub struct Foo { vec : Vec&lt;i32&gt; : 'a; slice : &amp;'a [i32]; } There is naturally nothing objectionable about this; the type system just can't express this right now. You can however defina type (which internally is unsafe) like: pub struct OwnedAndRef&lt;O, R&gt; { /*some fields omitted */ } Which is created like: impl&lt;O, R&gt; OwnedAndRef&lt;O, R&gt; { fn new&lt;F&gt; (o : O, f : F) -&gt; OwnedAndRef&lt;O, R&gt; where F: FnOnce(&amp;O) -&gt; &amp;R; fn get_owner (&amp;self) -&gt; &amp;O; fn get_ref (&amp;self) -&gt; &amp;R; } Which you can use to abstract this.
OK, let's take your questions one by one. Disclaimer: I am no expert on Rust, I am barely a newbie trying to use it more in my daily life. Also, I will treat as if you have no idea about any of the stuff I am writing about. This is not to insult you, but to make sure that my message gets around correctly. This means: - I will sometimes explain some basic topics that you might already know - I will oversimplify certain parts, to avoid diving into an infinite well of "except when"s By continuing to read, you accept my terms and conditions. Let's start with the first question: Why is Rust significantly harder than Go? Well, the simple answer to that is: Go is designed, from ground up, to be a *simple* language. What does simple mean? Well, simply put, it means: Easy to read, understand and maintain. When you read most Go programs, it is much like reading some boring text with some boilerplate. It might seem dull, but you just need to read it once and you will be able to understand what the program does and how it does that. You will be ready to make changes to it in no time. Go accomplishes this task in expense of expressive language features. Go misses a lot of language features that one might take granted in many other modern languages. Ŕust, on the other hand, is a feature rich language. It has Generics, procedural macros, operator overloading, enum types and an ownership/lifetime mechanism (which I will detail in reply to your next question) that allows you to write your programs in a really concise, expressive way. The downside? The same features that make Rust a great programming language, also makes it harder to read, understand and maintain. Basicaly, while reading a piece of code, what you understand the code does and what the code actually does might differ a lot. You can have some code like: ``` let a = some_function_from_some_library() let b = some_other_function_from_some_other_library() let c = a + b ``` Unless you already have all the necessary knowledge about the functions and their return types, it is impossible to say anything about what this piece of code does. Compare it to some similar code written in Go: ``` a := SomeFunctionFromSomeLibrary() b := SomeOtherFunctionFromSomeOtherLibrary() c := a.AddSomething(b.ToCompatibleType()) ``` Which one is more understandable? Did you understand that there was a type conversion in the Rust version? Probably there was an `Into` trait implemented. While all this functionality and flexibility makes Rust a great language to write DSLs and write concise code, it also makes it a harder language to read, understand and maintain. Now let's come to your second question: First of all, coming from Python, I can say that you don't really need to worry about the performance of Go anyway. If we were discussing this topic in 1992, my approach about garbage collection might have been different, but we are not living in 1992 and Go has an incredibly performant garbage collector which you won't even notice unless your application needs to allocate and deallocate huge amounts of memory. However, just to be clear, let's talk about memory. When we are talking about runtime, there is mainly two types of memory: The heap, and the stack. The stack is the place where your parameters and local variables live. Due to how CPUs work, accessing the stack is really fast, and maybe more importantly, stack basically manages the memory itself. Once a method returns, all the memory it has allocated in the stack is reclaimed automatically. It is like heaven, right? Well, nothing in Computer Science is free. Allocating memory on stack has certain limitations. First of all, the stack is small. While it is possible to let the stack grow very big, generally speaking the stack does not handle growth well (citation needed). The second thing about stack is that since the stack grows linearly, anything you put on stack needs to have a constant size. You cannot put anything that can randomly grow and shrink to stack (like a Python list or map). Therefore, if you have a data structure that needs to be able to change or needs to survive after you leave your method, you need to put that data to somewhere else, namely: The Heap. Heap is kind of the relaxed, little sibling of stack. It does not impose much rules about how it needs to be used. It is basically a huge blob of memory, where you can allocate memory as you want. You can allocate a part of heap anytime you want and it will survive when your method returns. The downside? Heap access is not as fast as stack. Since heap can be anywhere in a computers memory, it is much harder for the CPU to optimize heap access since you might be accessing a completely different part of the heap with the next instructtion. The second thing is, since the memory you allocated in heap is not automatically released when your method returns, you need to take care and release that memory once you are done with it. Now this sounds simple, but this is actually one of the hardest things to do correctly when you have to manage your memory manually. Well, since it is hard for us, humans, to keep track of our memory allocations, we find automated solutions to do it for us. Enter, Garbage Collector. The garbage collector is a classical solution to the memory allocation problem. Basically, in languages with a garbage collector, the runtime of the language tracks every bit of memory allocated and every now and then, stops the execution of the program, goes through the list of allocated memory, checks if it is still being used, and if it is not used, frees it. This is the main source of complaints with garbage collectors. SInce they are not really predictable, they can cause random pauses while your program is running. In that sense, we can say that a garbage colelctor is a brute force solution to the memory management problem: It solves it by removing all control that the developer has over the allocation of memory in the program. Rust has a different (and if you ask me, much more elegant) approach to this issue. Instead of randomly pausing the program and going through the memory, the Rust compiler does a nice trick, it assigns an owner, and a lifetime to a variable. Then it continues on to check that all the access to the variable according to the ownership rules and during the lifetime of the variable. It also inserts a bit of code to release the memory used by that variable once it's lifetime ends. This way, the developer gets back all the control over the memory, without being allowed to make any classical mistakes that comes with manual memory management. More importantly, since the checking is done at the compile time, this safety has no effect on how the program perfoms while running. Bonus: Also, one other limitation with garbage collectors is that they can only help you for managing memory. The developer still needs to manage other resources, like open files, network sockets or locks manually. With the ownership model, it is possible to enforce lifetime guarantees on other types of resources too, making it impossible to have leaking resources around.
&gt; Wouldn't that be very desirable for rust? Everything is a tradeoff. Is adding a new language feature, and a complex one, worth removing some nesting?
It is not based on reference counting at all. You can use library types that have reference counting. The key difference is this: with reference counting, you can make something live longer. Rust's ownership and lifetimes don't let you do this, inherently. That is, they *describe* your program, they don't *prescribe* how your program should work. Thinking about it the way you're posing can be sort of helpful at the start, but the devil is in the details.
FWIW, [over 20% of Rust devs now use it at work](https://blog.rust-lang.org/2017/09/05/Rust-2017-Survey-Results.html). There may not be "Rust jobs" but there are "jobs where Rust gets used, among other things". There also are pure-rust jobs but companies I know with these seem to be focusing on hiring internally; i.e. ramping up existing employees on this. This should improve soon.
&gt; let deserialized: Point = serde_json::from_str(&amp;serialized).unwrap(); That looks interesting. So how does it know it should be Point? I googled "rust docs reflection" and I guess it isn't called reflection. Where can I find docs on reflection so I can write a very poor version of serde_json?
Because I'm sick of writing this code in C#: Order ParseOrder(string content) { if (content == null) { throw new GodDammitException("Seriously, again?! STOP IT."); } ... }
Can I get a link to the docs/manual on the synaxs? or maybe an example? I tried googling reflection and it didn't work out
No, the opposite. Reference counted variables are possible with `Rc` and `Arc` (the latter one is a thread-safe reference counted variable). Here's how it works: There are two areas of memory, the stack and the heap. The stack is faster, but only stores values local to the current function. It grows down (from big memory adresses to smaller ones). When a function ends, the values of the current function are freed. The heap is not limited to the current function, but you have to request and free memory manually (via `malloc` and `free`). The heap grows up (from 0x0000 to bigger memory adresses). If the stack and the heap intersect, you're out of memory. **The memory on the stack has to be known at compile time ('static). The memory allocated on the heap can be dynamically allocated and freed**. If you write `let a = "hello";`, the 5 is put on the stack. If you write `let a = Box::new("hello")`, the a is put on the heap. (Vec and String are wrappers around Box). A Box is the same as a &amp; - it's just a (guaranteed valid) pointer. However, the Box also stores the size of the allocation. Now what's the difference? When a function ends, the variables of the current function (called a "stack frame") are cleared. This is why you cannot return a reference out of a function - the memory is cleared when the function ends, so the reference would point to garbage: // does not work: a is allocated on the stack, reference would point to garbage fn returns_five() -&gt; &amp;u32 { let a = 5; &amp;a } However, remember that I said that a Box is basically the same as a &amp; : fn returns_five() -&gt; Box&lt;u32&gt; { let a = Box::new(5); a } This makes a dynamic allocation on the heap. The memory for 5 is not cleared when the function ends. But when does the memory get freed - do we just allocate and allocate and never free? No. The memory is freed when the Box goes out of scope. The destructor of the Box says "ok, I still hold memory of size X at adress Y". The destructor is guaranteed to run, even on panics. But how can Rust return things by value and not by reference out of a function? Well, Rust knows that the size of a "Box" is 64 / 128 bits (pointer + length) - at compile time. So it allocates these bits on the stack and returns them to the function that calls `returns_five`. This is also why Rust has two string types - str is on the stack, while `String` is on the heap. You cannot modify an str at runtime (because the size has to be known), however you can with a `String`.
You should read into traits in Rust. The closest thing in other languages is protocols in Swift or interfaces in Java. The `Point` type here implements the `Deserialize` trait, which allows it to be constructed from JSON via `serde_json`.
Have you checked out the book? https://doc.rust-lang.org/book/second-edition/ or Rust By Example? https://rustbyexample.com/ You might need to learn about the language more generally before you can understand how those relatively advanced features work. https://doc.rust-lang.org/book/first-edition/procedural-macros.html is the specific page you're looking for.
&gt; Days 9 and 10) Everything I thought I knew about the console APIs was a lie, the fact that [directory listing/TLS/threads in general] worked was just a fluke, oh god I think I'm finding bugs in the posix implementation in X, how am I going to work around this, okay I think it ACTUALLY may work now... repeat this a few times. &gt; Made me smile. ;(
Reflection is done at runtime (when you run the program). This is why you won't find anything on that topic. Generics are here: https://rustbyexample.com/generics.html and here: https://doc.rust-lang.org/book/second-edition/ch10-00-generics.html 
I'll look those up!
I didn't want to read the book and be disappointed. Looks like I won't ;) 
Continuing work on [tarpaulin](https://github.com/xd009642/tarpaulin), currently working on an issue which will let tarpaulin accurately get coverage of itself. So basically sacrificing PIDs to satan in the hopes ptrace will work. I also hope to see some process on tarpaulin HTML reports. I've also got a C and C++ embedded project on the go. Have a tight deadline and don't want to rewrite the HAL/driver code. But once it's done I'll rewrite it in all rust at some point. Working with this [board](http://www.st.com/en/evaluation-tools/32f469idiscovery.html) which is pretty cool
Because with Rust you can instead do: Order parseOrder(content: Option&lt;&amp;str&gt;) { let content = content.expect("ಠ_ಠ"); ... } 
I agree the numbers are flawed. I don't think it's worth bragging about a flawed number, even if it's all we have. (Also, I think I'm more annoyed about name-squatting than you are.)
Serde *generates the code needed for parsing a `Point` at compile time* using macros (see: "serde derive serialize / deserialize"). Since the macro knows that "Point" has an "x" and "y" field and the data type, it generates something like this: impl Point { serialize(&amp;str) -&gt; Point { /* generated parser */ } } This is done at compile time, all you need is to put a `#[derive(Serialize)]` in front of the `struct Point { /* fields */ }`. The `unwrap()` at the end allows you to specify what happens if the parsing fails.
So here `Point` can implement `Deserialize` because `x` and `y` have a type of `i32`, which in turn implements `Deserialize`. The code that generates the implementation can also see the labels of your struct values (`x` and `y`). This is all done at compile-time, unlike reflection which is done at runtime.
Lazy error handling, often used in examples for brevity. Crashes the program if it encounters an error. Don't use unwrap in production, unless you know that it can't go wrong.
FYI, take a look at Conan.io for a C++ package manager
`.unwrao()` essentially aborts the program with a message if the value is `None` (or in other languages, `null`).
I know a good number of academics who primarily work in R, and I've heard a ton of complaints about some CRAN moderators being capricious, nitpicky and rude. I wouldn't be surprised to learn it had reduced the number of packages submitted to CRAN. 
Rust macros are also harder to learn but it's definitely worth it once you get a hang of it.
Being generic over pointer types, e.g. A tree that can use `Box`, `Rc` or `Arc` to store its internal nodes. This requires being able to write something like `Tree&lt;T, Arc&gt;` without giving a parameter to the pointer, since the actual type used there will likely be some internal tree-node type that the user doesn't and shouldn't need to know about.
&gt; So how does it know it should be Point? It specifically says `: Point`. ;-]
No, the equivalent of that in C# would be the (not yet released) nullable reference types, in which case your signature would be: Order ParseOrder(string? content)
Dime-a-dozen is good. Let's not turn into yet another gatekeepery systems community.
When is it generally considered good practice to cast using _as_ ? I was making the average calculating program [as suggested by the book](https://doc.rust-lang.org/book/second-edition/ch08-03-hash-maps.html#summary) and had lots of ~~difficulty~~ _learning experiences_ around juggling integers and floats (u32 and f32). Eventually I opted for u32 input and f32 output, casting using _as_ whenever​ dividing... It doesn't look pretty.
So if I wrote `foo(serde_json::from_str(&amp;serialized).unwrap())` it still should work because it can inference the type from the function params?
Instead in rust you just do `fn parseOrder(content: &amp;str) {` and be assured it's statically impossible for the caller to pass a null pointer.
If `foo` took a concrete (as opposed to generic) type, then correct. For a generic type you may have to use turbofish syntax to specify the type (which could be done here as well to obviate the `: Point` if so desired).
"Why Rust?" is a hard question to cover in its entirety because Rust's greatest strength lies in how it managed to get so much right in a single language. For example: * Like C and C++, it's got predictable performance, with mature code optimizers (from LLVM) and a high performance ceiling. (On "The Benchmark Game" the only place where it's not in the same ballpark as C and C++ is tests where "SIMD support requires a nightly build of Rust" bites it.) * Like C# and other managed languages, it has a comfortable, high-level API with a lot of safety guarantees. (Which means that, once you get used to how the borrow checker thinks, it can even replace dynamic languages like Python in a lot of places.) * Like Haskell, it beats out every other language of noteworthy market share when it comes to how much you can get the compiler to verify for you at compile time... but it stops a little short of Haskell in order to produce a much more familiar language syntax which requires no garbage collector and much less need to un-learn imperative programming and learn functional programming. * Specifically designed so it can not only call C easily, but be *called from* C easily, making it almost unique in its viability for incrementally replacing bits of other programs with Rust wherever you need better performance or stricter compile-time guarantees. * `cargo` is better than `pip`, `npm`, or any other such tool I've tried so far. * The community is the friendliest, most helpful, most polite one I've ever been part of. * A lack of `NULL` and monadic error handling (`Result&lt;T,E&gt;`) make for a powerful combination for ensuring you've handled all possible return values without it being painful to do so. etc. etc. etc.
So if you want to be _evil_. You can get _most_ the power of C++ templates from `macro_rules!` to build literal templates. Within `macro_rules!` you can invoke `stringify!` on a identifier (`type`/`function` invokable name, kind of), and at compile time it'll become a `string`. --- Most of the type variance is handled by generics. Something like `fn foo&lt;X&gt;(arg: X)` will act like a C++ variadic template a compile time. --- If you want limited type level variance, `enum` is also a good choice as it can include _some_ type level reflection. --- If you are feeling super bold you can likely implement function/type/trait level reflection with procedural macros but that'll be slog.
&gt; I agree the numbers are flawed. I don't think it's worth bragging about a flawed number, even if it's all we have. It is flawed, it is not worthless. Hackage is a notable repository with slower growth then crates.io. Hackage exists before 2010, crates.io since 2015. Crates has _notably faster growth_, even in the presence of errors. Getting in the range of Haskell is not about bragging, it is about getting at least some notion of where we are. And that finding has to be communicated to the outside, as unsharp as it may be. No one else has sharper numbers. Arguing that it shouldn't be done because of that helps no one, neither us, nor Haskell, nor interested people trying to get a bearing of Rust. For anyone who needs better numbers, please give money to Andrew Nesbitt, who's working on making these things sharper, through libraries.io. &gt; (Also, I think I'm more annoyed about name-squatting than you are.) I know, but neither your or my annoyance makes the number we discuss sharper or not sharper.
Rocket is great for REST APIs (not sure if WebSockets are supported though): https://rocket.rs
&gt; -Edit- What's with .unwrap() being everywhere? As the others have said, it's lazy error handling. It basically aborts the program and, if you've asked for it, produces a traceback. The difference between that and a C/C++ crash is that, like an uncaught exception in a managed language, Rust aborting will unwind the stack, running any cleanup code types given in their `Drop` implementations. Never use `.unwrap()`... Instead, return the `Result&lt;T, E&gt;` type if it's possible to recover from the error or call `.expect("Nice, greppable message")` if it's something which can't be meaningfully handled. (eg. `let loopback_address = Ipv4Addr::from_str("127.0.0.1").expect("Constant IP was somehow invalid");` )
&gt; In other words what kind of problem is it trying to solve where it make a significant difference compared to other languages For one thing, it’s impossible to write secure C/C++. 
I once needed it to abstract over different implementations of bigints to make sure &amp;T + &amp;T works where T is generic.
I need errors to be Sync so that they can be inside an Arc and Arc&lt;E&gt; can be Send, mostly due to the way rlua works. It's not really avoidable, I logically need the errors to be inside an Arc because Lua can catch them and do stuff to them, and without doing that there's an unavoidable "error consumed" state to deal with in Lua.
In a single thread this isn't a problem. However, imagine you instead passed bar1 to a thread, and then write a new value to _bar2. Now you have two threads that can write to foo.bar1 at the same time. Which one will write first is indeterminate. This would be a classic race condition, something Rust tries to prevent. Instead, Rust makes you set up other code around this to make sure it happens safely and in a deterministic way.
&gt; The obvious consensus is that Rust is (significantly) harder than Go. But when I compare how things are done in both languages (functions, structs, enums, handling strings, etc) I dont see any difference. The syntax is more or less the same and even when it is different, it is not different enough that I would say "Oh wow this looks way harder to grasp". But much more experienced developers than me keep saying that Rust is harder. So there must be a valid point. I just don't understand where/how exactly? How is Rust harder than Go? And why? Are there a few big limitations? Are there many smaller limitations? I have no idea! I wouldn't agree to this statement, but I see where it's coming from. Rust has notably more ramp-up time. It introduces new concepts that are _not_ existing in other programming languages. So it is not a language that is easy to pick up in a week. Go is one. Rust is built for safe and secure construction of large, fast, concurrent codebases. Indeed, the team behind the current rewrite of the Firefox styling engine made two failed attempts to write a parallel on in C++, both attempts had failed. And this idea is baked into the very underpinnings of the language: Ownership, References/Borrowing, Mutability, Send &amp; Sync are all explicit concepts in Rust, which need to be understood. But what Rust makes easy is breaking huge software into pieces that can be reasoned about locally. Currently not working in a piece where concurrency is no concern? You don't have to care about it. Currently working somewhere where concurrency might become a concern? The compiler will warn you about it or not compile your code. Rust is a much richer language when it comes to interplay. For example, every parameter in a Rust function has additional properties: * it's mutability * whether it is a value or a reference (Ownership or Borrowing) * if it is a reference, whether I'm allowed to mutate through it or not * if it is a reference, whether it is points to another piece of data that needs to be alive All this is rather complex at first, but later allows you to form a secure assumption of what is happening around the piece of code you are currently working with. This is a _huge_ gain in software where the context is thousands of line big. Rust itself is a surprisingly simple language at its core, if you grok the 5 things I mentioned above, you are basically in. The problem is that _all_ of these are not present in other widely used languages explicitly and have to be learned.
Looks like you already got good answers to your questions so let me muse a bit and be less than helpful. First, you could make a window manager in python, but it would require you to understand (or learn) how python interacts with C. If you've little experience with C this could be an infuriating project. Second, let me congratulate you on the decision to learn a language that is different than what you normally use. At least for professional programmers, this is an important part of mastery. There are many computational models (paradigms). It's easy to stick to the ones you're familiar with but branching out periodically to gain competence in unfamiliar paradigms does a lot to improve your skills languages you've already mastered. Third, although I would never recommend anyone to *program* in C (due to memory safety issues and undefined behavior), I would recommend *learning* C and some form of assembly language. For better or worse, C has become a lingua franca for communicating between languages. So learning some C lets you work with the foreign function interface (FFI) in your language of choice. Learning some assembly gives you a better sense of how programs are represented at run-time, which is also useful at times. I bring this up because as far as I know, your window manager project will need to use the Rust FFI. Fourth, this project is probably bigger than you realize. If you work on it for a while and fail, then remember to start *smaller* next time. The advice is to repeat until you successfully finish a project. Then you know about the right size given your constraints and skill level. More ambitious things will become possible as you "level up". Good luck!
Here's my attempt at an ELI5: &gt; How do I take care of memory management manually? Is there something like: &gt; &gt; del temporary_integer; "Manual memory management" usually means exactly that... and it's far too easy to make mistakes. The Rust solution is to have the compiler follow each variable through your program and automatically insert the equivalent to your `del temporary_integer;` wherever it falls out of scope. (eg. If you don't return `temporary_integer` or move it into a container like a `Vec`, it'll automatically get freed at the end of the function.) That's also why people say Rust is harder. Everything is obvious at runtime, but you burn CPU time keeping track of everything (that's what a garbage collector does). At compile time, the compiler sometimes just has to turn to you and say "You're doing thing X here and I don't know how to handle that for reason Y. Please clarify/rework it." On the plus side, Rust's rules also prevent certain **really** painful-to-debug kinds of bugs and it's much better than Go at giving you the tools to build things the compiler can prove to be safe. (eg. an [HTTP client](https://hyper.rs/) which won't compile if you try to send headers after beginning the request body.) &gt; Another obvious consensus is that Go seems to be slower due to the Garbage Collector. Languages like Python and Go accomplish the same visible effect of not needing to free memory by stopping your program every so often, inspecting all of the memory to find variables that are no longer reachable from the program, and then freeing them... possibly moving things around to close up holes in the process. Rust doesn't need to do that because it all got figured out at compile time and baked into your compiled binary.
I'm pretty sure you can write a Wayland window manager in Python. You probably have to use ctypes.
In this specific example, the borrow checker is perhaps too conservative. But in general, this rule is there for a good reason. For example, iterator invalidation or resizing a `Vec`: let v = vec![0]; let r = &amp;v[0]; // or mut v.push(1); // this borrows `v` mutably and would invalidate `r` The same rule also enables optimizations around pointer aliasing that are harder to do in languages like C that don't enforce them, though that also doesn't really apply in this small example.
The [motivations](https://github.com/rsocket/rsocket/blob/master/Motivations.md) page for rsocket is a great high level summary of concerns. I like [Aeron](https://github.com/real-logic/Aeron)'s choice of little endian to remove unnecessary field reordering on all popular machines, so rsocket's choice of big endian is grating, especially if intending to use Aeron for the transport. Aside from that, what I've seen of rsocket is good. Nice work on mles, /u/jq-rs
Perhaps the example would be even better with `Vec::clear` :)
Manish has a very good explanation here. https://manishearth.github.io/blog/2015/05/17/the-problem-with-shared-mutability/ Yes, in you case, a sufficiently smart compiler could deduce that one of the references is not written to, but in the general case, it would break the invariants of the language.
There's your problem. Writing `SIGINFO` as text isn't the same as sending a signal. The reason `kill()` doesn't take an argument is because stdlib limits itself to cross-platform APIs when feasible and platforms like Windows don't have a concept of POSIX signals. You'll probably need to feed the PID from calling `Child::id()` into a function from a crate which wraps POSIX-specific APIs like [libc::kill](https://doc.rust-lang.org/libc/x86_64-unknown-linux-gnu/libc/fn.kill.html) (low-level) or [nix::sys::signal::kill](https://docs.rs/nix/0.9.0/nix/sys/signal/fn.kill.html) (higher level).
1. Even forgetting memory management, Go is still much easier to learn than Rust because it is a much smaller language. 2. Go is slower, but not due to garbage collection. Go is just plain slower. It's more on the level of C# or Java than C, C++, or Rust. For what it's worth, writing Rust is a lot more enjoyable for me than writing Go. I won't go into details (no bashing, etc...) but I just do not like the tradeoffs made in the language design for Go. Rust's tradeoffs are much more palatable. Reasonable people can certainly disagree, because it is exactly those tradeoffs that make Go so much easier to learn. To expand on what I mean by "smaller language," I mean that Go either hides or simply does not include several common features of current, common enterprise languages. There's an obvious example here; I'll leave that to someone else. There are, however, other examples. The first one that comes to mind is async io. C# and Rust, for instance, allow the programmer to employ async io. It will involve a library or some special classes, but you can say, "here, read this file and get back to me." In Go, it is not possible to say, "here, read this file and get back to me" in quite the same sense. This is because that is in fact the standard behavior of such calls in the first place, but this behavior is hidden from the programmer in Go.
I *still* don't get it:/ &gt;This doesn't declare one function. It declares an infinite number of functions, one **for every possible lifetime** that can exist. fn foo&lt;'a&gt;(x: &amp;'a str) -&gt; &amp;'a str &gt; The point is that you do not want &amp;L to implement IntoIterator&lt;Item=&amp;T&gt; for one lifetime **but for all potential lifetimes** that L may happen to have. &gt; &gt;.... &gt; In this case, you need to use a Higher Ranked Lifetime Bound: for&lt;'a&gt; will take care of introducing the lifetime name whilst simultaneously signaling to the compiler that the clause using it should be valid **for all possible values of 'a**. I believe this is gibberish for my non native English based brain. These ALL sound like they tell the very same thing. What's the actual difference? What the code does with it? 
FWIW I got the joke and you made me laugh pretty hard
&gt; If I can modify foo bar1 would change? if I modify bar1 foo would change? The borrow checker *also* prevents you from accessing `foo.bar1` through `foo` while the `bar` reference exists. So there's no inconsistency there. Others have posted the explanation on why aliasing mutability is bad in general.
Oh I see i didn't know that wasn't allowed. It makes more sense.
You're looking for https://www.reddit.com/r/playrust, this subreddit is about the Rust programming language.
Look at the sidebar. There's a brown box, read its header.
Hi! I just wanted to add that I really-really like your questions. Especially the section around `del temporary_integer;` given a theory, an observation, and the hunger for more knowledge, all based on your very different previous experiences. Happy learning and keep the questions coming! =)
So, while I love the crate and everything it does, the documentation can be improved. Your examples show how to create structs with various fields and attributes but not how to read from those fields. Once you add that I think this will be a killer crate for those of us who have to deal with bit fields all the time! In addition, I would be interested in seeing if this compiles down to the same code as just doing the bit manipulation manually, and if not how much overhead it adds.
Is there some noob-friendly good tutorial for Rust?
Networking is a pretty complex topic. Here are some places to start: macOS uses [`pf`](http://www.openbsd.org/faq/pf/), which is equivalent to `netfilter/iptables` on Linux and is essentially a software router running on your laptop. [`bpf`](https://man.openbsd.org/bpf.4) is a raw interface thru `/dev/bpf*` to data link layers of the network stack. Both `pf` and `bpf` can be configured to accomplish a wide range of things: filtering/viewing traffic, manipulating packets, etc.
Incredibly difficult to do at scale, sure. Impossible? Clearly not.
Having blessed packages I think would be wrong. But I also think getting rid of name squatting packages and in appropriate would be good. So I guess maybe I am saying a form of curated but not too strict.
Hey @llogiq, watch out for the smileys ! ;)
Oh, the documentation doesn't exist at all. I've just forced myself to at least publish something with a half-interesting README file. I'm also preparing a sensor device library that's a heavy user of packing, so there is a lot of dogfooding going on. The attribute scheme has been rewritten at least three times :) As for the performance: the codegen generates some horrible Rust, but the optimizer seems to do a brilliant job of reducing it down to bit manipulation operations. I've checked the assembler for ARM targets, not much else.
[removed]
Coming from python you will find it pedantic, frustrating and maddeningly unproductive compared to what you are used to. You don’t need to manage memory but you need to be explicit with the compiler about what you are trying to do with it. You will find some conventions weird, especially borrowing, which works like no other language I’ve used. It’s fast and small. The abstractions are zero cost but you will pay a price in the steep learning curve. Be patient and keep your expectations in check until you’ve become comfortable with the core concepts. Good luck!
On the optimization, it's not the generated Rust code that matters, it's the assembly at the end! That's great to hear and I will definitely keep an eye on this for your actual release. No pressure!
With the non-lexical lifetimes changes, will the `bar1` borrow expire immediately (because it's never used)?
I don't think the compiler enforces anything around that so I guess it is up to your preference/what makes sense. The common convention I've seen in a lot of code (&amp; what I use in my code) is that you define a struct with its methods and trait implementations in one file. You usually group by the struct/enum but then if you have a custom trait and you want to keep its implementations in the same file, that's okay too. It's totally okay to put multiple structs/enums together in a file if that's what makes sense for your code. As an example, [here's an enum](https://github.com/sunjay/turtle/blob/45130b96842bdaddeb78188724067cf23aec19be/src/speed.rs#L69) I created with trait implementations in the same file. [Here's a trait](https://github.com/sunjay/turtle/blob/45130b96842bdaddeb78188724067cf23aec19be/src/animation.rs#L13) I created in the same project with implementations in the same file as the trait. The difference is that in the first case it was a trait from an external library (rand) and in the second case it was a trait I was defining myself and I knew there would only be a few implementations. I could have easily split up that second example and put everything in their own files, but I prefer it this way because it's easy to reason about and find the code. It really comes down to how you think is the best, most logical way for you to structure your code. What will help you remember where things are so you can come back to them later? What will make it easier for others (+ you in 6 months) to find things in your code?
In that respect, it doesn't sound much different from C, which is super compact, but tough to design well in
I'm a big fan of anyone learning something new or outside their comfort zone, but I also believe that it's better to play (and mess up in) something with a lot of resources available
Rust follows (somewhat confusingly named) RAII idea. I'll try to illustrate it with an example code snippet: ``` { let v: Vec&lt;i32&gt; = Vec::with_capacity(10); // do some stuff } ``` So what we have here? Basically we are creating a new vector called v (basically same as list in python) that has a block of memory allocated for 10 integers by calling a (static) method of Vec named `with_capacity`. This method in addition to some book keeping etc does ultimately end up calling a special global function called `__rust_allocate` which does the needed memory allocation. Then we are doing some stuff with that vector that is mostly irrelevant for this explanation. More importantly the code snippet is surrounded by braces (like it would be for example in a function definition) which denote a *scope* for the variables declared inside of it. At the end of the scope the compiler inserts a call to special method named `drop` for every variable declared in that scope, so in our case there is an invisible call `v.drop()`. And the `drop` method for Vec deallocates the previously allocated memory by ultimately calling another special global method named `__rust_deallocate`. IF you were to call `__rust_allocate` yourself directly you'd get a "raw pointer". Raw pointers do not have `Drop` implemented so at the end of the scope nothing would happen. So, unless you'd then also manually call `__rust_deallocate` at some point, you'd end up leaking memory. And this is basically the core of "manual" memory management. In Rust we have all these nice libraries in std (and core) so you practically never need to get your hands dirty and actually use those very low level methods. But at the very heart of Rust they still lie and establish the basic building blocks that are then used to create everything. The thing that makes that increasing abstraction possible is the existence of destructors in the form of Drop trait. It is important to note that while drop is commonly associated with memory management, they are not really coupled to each other. You can do literally anything in a drop implementation, memory deallocation just being a typical use. Conversely you could just forgo fancy drop semantics and do C style manual memory management with raw pointers if you wish, although you'd lose Rusts safety benefits by doing so. I brushed over many finer details of Rusts memory management, so the picture is not perfectly accurate. Examples being that Vec itself does not directly manage the memory, and as such does not actually have drop implemented at all. Then there is the whole thing with moving variables, which changes the story quite a bit. Also notably rust does not actually guarantee that destructors (drop) are always called, meaning that you can actually leak memory in safe Rust, although in practice that happens only in quite exceptional situations that you don't typically need to worry about. Some select pieces from the documentation that I was referencing while writing this: https://doc.rust-lang.org/std/ops/trait.Drop.html https://doc.rust-lang.org/1.12.0/book/custom-allocators.html https://doc.rust-lang.org/alloc/ https://doc.rust-lang.org/std/mem/fn.forget.html https://doc.rust-lang.org/src/alloc/vec.rs.html#299 https://doc.rust-lang.org/std/primitive.pointer.html
Also what you need to add you need for every single error case a single exception class in C# If you want to do It right. Each class is normally in 1 file. An Enum in C# cannot contain fields like in Rust so you cann't write something like: enum ErrorKind { NetworkErrorKind(Network::ErrorKind), MyOwnErrorKind(ErrorKind2) }
I have this current unidirectional linked list definition and an implementation method `next_node`. I'm wondering why when accessing `self` without a reference it compiles and works just fine, but when I access it by reference it won't compile with a "can not move out of borrowed content" warning. What's the proper pattern here? I figure accessing self by reference would have better performance implications, and as far as I know it means I don't have to explicitly back ownership later on. ``` // This is the main function struct Node { val: u32, next: Option&lt;Box&lt;Node&gt;&gt;, } impl Node { pub fn next_node(&amp;self){ let target: Option&lt;Box&lt;Node&gt;&gt; = self.next; match target { None =&gt; { println!("Nothing"); }, Some(thing) =&gt; { println!("{}", thing.val); }, } } } fn main() { let node = Box::new(Node {val: 10, next: None::&lt;Box&lt;Node&gt;&gt;}); node.next_node(); } ```
Well you're free to disagree, as long as you're okay with the fact that you're objectively wrong. I mean, in a speech at the 2012 haskell symposium, Simon Peyton-Jones, the guy who coined the term, clarified that `avoid (success at all costs)` was the correct intepretation to the term, and regardless, the community adopted the interpretation.
You can only use `impl Trait` when implementing. You cannot do something like: ``` trait Foo { fn foo() -&gt; impl Iterator; } ``` Which is the kind of things HKT would enable. It's very frustrating not to have when implementing certain kind of patterns (async/iterators scenarios are the most straightforward examples).
They're anonymized types that capture generic parameters. `fn foo&lt;T&gt;(x: T) -&gt; impl Trait` can be thought of as `exists type Foo&lt;T&gt;: Trait; fn foo&lt;T&gt;(x: T) -&gt; Foo&lt;T&gt;`. In short, this means that trait methods which return a different `Foo` based on the implementation need an associated `type Foo&lt;T&gt;: Trait;`. This feature, generic associated types (GATs, formerly ATCs or associated type constructors), is very similar to higher-kinded types.
I'd also add that while Go is a much simpler language, I've found Rust to be much easier to write complex applications with. Go may be easier to learn and get started with, but it's ultimately several decades behind Rust in it's feature set, and so a lot of things that can be succinctly expressed and wrapped up in a generics bow requires a massive undertaking in Go. Plus, the whole Cargo thing is miles ahead of Go's lack of package management and semantic versioning capabilities.
Maybe it would help to put it in English, and take lifetimes out of the equation. Consider the bound `for&lt;M: Meat&gt; Person: WillEat&lt;M&gt;`. This bound means that for any `Meat` `M`, our `Person` `WillEat` the meat `M`. We're not requiring that a person eats any specific meat, or just that they eat a single meat: to meet our bound, `Person` must be willing to eat any possible meat they are given. If we know that this bound is met, we can call `person.eat(Steak)` or `person.eat(Chicken)` -- it doesn't matter what the meat is, they will still eat it. Putting this in terms of lifetimes, we could write `for&lt;'a&gt; Person: Foo&lt;'a&gt;`. In order to meet this bound, `Person` must implement `Foo` for every possible lifetime `'a`. `for&lt;'a&gt; &amp;'a T: IntoIterator&lt;Item = &amp;'a T&gt;` is a very similar bound. It says that, for any lifetime `'a`, a reference to `T` with lifetime `'a` will implement `IntoIterator`, yielding items of type `&amp;'a T`. If that last step wasn't immediately obvious to you, no worries-- it's a pretty complex bound. Still, if you break down the pieces I think it's really not too different from the other two cases.
Comments on my code, writing style, or content welcome.
Sadly, no mention of Redox OS.
It's really interesting that you read "Rust is harder than Go" as "Rust has limitations". It's very much the other way around. Go is very limiting, but some people see the limitations and reduced feature set as a "good thing" and "easier". Rust on the other hand has a rich set of features and tends to leverage all of them. A good example of this is returning an error. In go, if you want to be able to return either a result *or* an error, you will typically just return two values. One value is your result or a nil, the other is a nil or an error. You as the programmer than have to do the right thing in response to these two values and the fact that either of them can be nil. (Only handle the result if it's not a nil. Deal with the error if it's an error and not a nil...) Rust deals with errors by leveraging it's type variant system. (Rust uses the key word 'enum' for this.) You either have Result::Ok(data), or Result::Err(error_value). Ok and Error are variants of the same type (Result), which is what you specify as the return type. The compiler then *forces* you to consider both of these variants. Not dealing with Err causes a compiler error. Not dealing with your err variable in Go causes a runtime problem. 
Maybe someday soon we will see support for this syntax: pub struct Foo { data: Vec&lt;i32&gt;, slice: &amp;'self [i32], } Shouldn't be too hard to convince the compiler to associate a `'self` lifetime with one of it's fields.
Because you can write complex, multithreaded code; because you can fearlessly modify code that was written 18 months ago by a misanthropic dev who thinks documentation is a sign of weakness; because you can chop and change, slice and dice, add features and remove them, combine crates and split them apart; and after all that, be certain - **100% certain**(1) - that there are no memory race conditions, buffer overruns, or dangling references waiting to bite you in the butt. This (IMO) is Rust's major value proposition. (1) Some caveats apply to this statement, but they are minor. 
Passing a single big parameter is different than passing several small parameters in assembly level. You can't compare them. :P Now, if you replace the tuple with a struct, it should be the same because they're equivalent.
Let's take a look at the ASM. This is the only difference I can see between the two versions, which is in how the parameters are moved into the function: normal_params: movups xmm0, xmmword ptr [rsi] movups xmm1, xmmword ptr [rsi + 16] movups xmm2, xmmword ptr [rsi + 32] movaps xmmword ptr [rbp - 240], xmm2 movaps xmmword ptr [rbp - 256], xmm1 movaps xmmword ptr [rbp - 272], xmm0 movups xmm0, xmmword ptr [rdx] movups xmm1, xmmword ptr [rdx + 16] movups xmm2, xmmword ptr [rdx + 32] movaps xmmword ptr [rbp - 192], xmm2 movaps xmmword ptr [rbp - 208], xmm1 movaps xmmword ptr [rbp - 224], xmm0 tuple_params: movups xmm0, xmmword ptr [rsi] movups xmm1, xmmword ptr [rsi + 16] movups xmm2, xmmword ptr [rsi + 32] movaps xmmword ptr [rbp - 240], xmm2 movaps xmmword ptr [rbp - 256], xmm1 movaps xmmword ptr [rbp - 272], xmm0 movups xmm0, xmmword ptr [rsi + 48] movups xmm1, xmmword ptr [rsi + 64] movups xmm2, xmmword ptr [rsi + 80] movaps xmmword ptr [rbp - 192], xmm2 movaps xmmword ptr [rbp - 208], xmm1 movaps xmmword ptr [rbp - 224], xmm0 In the "normal" version, the parameters are coming from two separate registers, but in the tuple version, they are coming from the same register, one after another, because that's how tuples are stored in Rust. I'm not well versed enough in CPU architecture to say why this would be causing a slowdown, but since there are no other differences in the ASM, this must be the cause. Perhaps someone else can explain why this would cause a performance difference.
it's also worth noting that in practice LLVM should inline small functions like that aggressively, making them even faster than as standalone, and probably eliding the difference entirely, making them perform equally. I could be wrong.
i like it—clever solution. Is there any RFC regarding this or you just made it up?
in my real-world case each function is being passed two big complicated structs (of different types), one with multiple generic members. 
[removed]
Right - I would have thought so, too. However, 1) I'm benchmarking in release mode, so those optimizations should have been applied, 2) I just re-ran them with `#[inline]` affixed to each of the functions in questions and the results remained the same. 
It's important to note that the variance on the tuple benchmark puts its lower bound within 0.7% of the normal function call benchmark. I would personally consider this to be noise. If there was an order of magnitude difference, then I might be more concerned.
Thanks for your explanation. I'm somewhat relieved that both points sort of refer to the same problem. Currently I am leaning more towards Rust, because of exactly that reason (of memory management). I already know the easy end of the spectrum (Python) and for my general programming skillset I think Rust would be a more meaningful addition (compared to another "easy"/"I take care of the hard stuff for you" programming language like Go). Also thanks for the Link, I will definitely have a look!
go to /r/playrust, thanks
I've heard it mentioned on here a few times. There's likely an RFC revolving around this, considering that this is a pretty important thing that people have been stressing over since the beginning. I could even use something like this to enhance the Ion shell's performance by quite a bit within loops. Have been pondering just giving up waiting for it though, and using *const pointers. Ion's performance is already ahead of Dash, whilst simultaneously having many more features than Dash, but there's no such thing as being too fast.
When you pass something that's too big for registers, even when it's by-value/move, the compiler passes a register containing a pointer to them sitting on the stack. With the normal params, it sends two registers each with a pointer to where the object is on the stack. With the tuple it sends a single pointer to where they are contiguously on the stack. It is likely that they aren't already contiguous on the stack, so the compiler has to create a local temporary on the stack which is them both combined together into a tuple and then pass a pointer to that. This means in the tuple case it must make a copy of them to put them contiguously in memory, which is probably what accounts for your 10ns.
Ah, you are one of the developers of way-cooler? I was looking through your Github repository today to get a brief overview of the project and Rust in general. I will definitely(!) get back to you with some questions once I get started. For now the #1 question is the correct order of things to take care of. What should i start out with? What is not that important at first? And a more general question: Why did you choose Rust for the WM and how happy are you with the choice? Would you say there'd be an easier way to do it? Besides C of course. Also how easy is it to "talk" to the Wayland C-functions in Rust?
Well, the C and C++ specs are riddled with gaping holes in their specifications which allow for undefined behavior to reign free, and come with absolutely no rules to ensure proper management of memory in a manner that's simple for a static code analysis tool to reason about. Not to mention, their standard libraries are pretty ancient / limited, and there's much contest over what libraries and features to use within a project. It's very chaotic, and extremely difficult to master. You largely have to work with some rather archaic tools, and you may find yourself reaching for debuggers like gdb a lot of the time to figure out why your program is exhibiting this odd behavior 10% of the time -- something that Rust would have caught and pointed out to you at compile time. And yeah, it's significantly easier to write complex, multi-threaded code in Rust. Rust has first class support for strings, threads, collections and more -- which works universally on all platforms supported by Rust. You not only get to have the tools to easily and effectively write multi-threaded code, but you get access to Cargo, which can drastically speed up your development process by providing you with nice APIs for all the efficient data structures and libraries that you need. As a result of this, a single Rust developer can potentially be as efficient as an entire team of C++ developers. Software can be developed much more quickly, using much less resources, in much less time. I do recommend installing `cargo-edit` so that you can just `cargo add &lt;package&gt;` to add a new crate to your Cargo.toml file, automatically.
&gt; the only place where it's not in the same ballpark as C and C++ is tests where it's getting bitten by "SIMD support is API-unstable and requires a nightly build of Rust" Which benchmark(s) does this apply to?
&gt; Also, I will treat as if you have no idea about any of the stuff I am writing about Literally exactly what I need(ed)! And before I start responding: Thanks so much for this exhaustive explanation. This really cleared things up. Especially the whole GC/stack/heap scenario. The fact that Rust is a more complicated language is actually a reason why I am leaning towards it and not Go. I already use a lot of Python so if I want to expand my hobbyist-programming-skillset then Rust is a much more interesting choice because I'll actually learn new concepts instead of simply having a "compilable version of Python" (by that I mean Go - and yes Go is more complicated then Python of course) The big question is: What's the cost/benefit ratio of the added complication introduced by Rust? Will I be super annoyed because I constantly have to fiddle around with memory management and it's all gonna end up in a huge mess? Or is it gonna force me to write cleaner/more efficient code? 
&gt;Ah, you are one of the developers of way-cooler? Really the only developer right now, but I'm getting some useful contributions lately which is nice. &gt;For now the #1 question is the correct order of things to take care of. What should i start out with? What is not that important at first? Well, choose your language and choose what you want your wm to do would be a good start :-). Right now, the only languages that have been used to make "successful" Wayland WMs seem to be mostly written in either C or Rust, but really any language will work. When it comes to starting out, it's like any big project. You gotta celebrate the little victories and keep medium term goals in mind. It gets much easier once you've developed enough for you to actually use it. That's a really cool turning point because once you use it everyday you _have_ to improve. I cannot stress enough how important it is to switch to your wm as soon as it is even halfway usable. &gt;And a more general question: Why did you choose Rust for the WM and how happy are you with the choice? Would you say there'd be an easier way to do it? Besides C of course. I originally was going to do it in C at first, because he's the language everyone uses! Yeah, I wasn't particularly tied to C. The other person on the project (who has since moved on to bigger and better things) wanted to use Rust and I fell in love immediately. I really suggest learning Rust properly before diving in, but you might have a better time then we did. We made a _lot_ of mistakes (lots of bad library design for the wlc wrapper, and of course [this horrible bug. Just don't use unsafe unless you really really need it](http://way-cooler.org/blog/2016/08/14/designing-a-bi-mutable-directional-tree-safely-in-rust.html)). But Rust has been great. If you are a C veteran you might want to use that instead, but if you're not then go with Rust because it's harder to blow your foot off (mostly because you can't get the gun to compile) &gt;Also how easy is it to "talk" to the Wayland C-functions in Rust? You can do so unsafely using wayland-rs, but depending on what framework you use you shouldn't need to do that. We only rarely use unsafe in Way Cooler directly (a quick grep will only show some edge case uses, outside of tests). Check out [wlc.rs](https://github.com/Drakulix/wlc.rs),a *much* better wlc wrapper than what Way Cooler uses. You should be able to get something usable quickly. Feel free to message me with more questions. 
Thank you so much for your input. This is exactly the kind of information I need. As I already stated I prefer Rust because of the fact that it is more complicated and that it is LESS like Python than Go. Even though I must say the Rust syntax is VERY Python-like. Even more so than Go it seems. But one last important question: Would you say that all the added complication introduced by Rust is worth it or will I be super annoyed because I constantly have to fiddle around with memory management instead of writing "actual code". Is it gonna force me to write cleaner/more efficient code? Or is it more like C++ where there are 10 different ways of solving the same problem. (Something I am not a fan of. That's why I like Python :-) ) 
&gt; First , you could make a window manager in python That is true. I even found a WM that is written in Python on Github. However I want to learn Rust not only because I want to write a WM, but because I want to learn a compiled language. And since I already know Python I am leaning a lot more towards Rust than Go, because Rust seems to have more new (to me) concepts than Go. I also considered C++, but after I looked at some string comprehension tutorials I was literally like "F**k this shit" and I gave up. The rust syntax is so Python like, that's why I like it. It's even more Python like than Go, which I was quite surprised about.
Right, but in the benchmark environment the only function being called *is* the function under test. It's pretty certain it would be inlined there anyways, but I'm suggesting that register allocation would be a little different in a real-world app with more stuff happening than a single function in a vacuum. This may be complete hocus pocus, I will readily admit. On my laptop, running your benchmark code just copied and pasted as-is, I get the following results: test tests::it_uses_normal_params_impl ... bench: 618 ns/iter (+/- 544) test tests::it_uses_tuple_params_impl ... bench: 624 ns/iter (+/- 620) and test tests::it_uses_normal_params_impl ... bench: 614 ns/iter (+/- 629) test tests::it_uses_tuple_params_impl ... bench: 629 ns/iter (+/- 515) and test tests::it_uses_normal_params_impl ... bench: 613 ns/iter (+/- 476) test tests::it_uses_tuple_params_impl ... bench: 633 ns/iter (+/- 594) from a couple of runs. The uncertainty I'm seeing is much higher than the numbers you reported in your post. I'm not sure how you saw results that were so much more stable -- maybe since I'm running this on Windows, Windows may just suck. But, from what I'm seeing, there is nothing conclusive here to draw from. No, Rust is not intentionally making tuples more expensive. It does look like LLVM is allocating registers slightly differently with tuples than with normal arguments. When you get into detail this fine-grained, you have to consider such things as how doing floating point division is slower than doing floating point multiplication. My results in this microbenchmark were consistent in showing that the tuple-argument version was the smallest-measurable-amount slower than plain arguments, so there may be a performance delta. It would be interesting to get to the bottom of this, and figure out what the problem is. In this regard, I have dug into the LLVM IR that is being generated a little bit. This is the starat of the normal_fn_params function: define internal void @_ZN10playground16normal_fn_params17h9493a3a6753cebc5E(%"B&lt;f64&gt;"* noalias nocapture sret dereferenceable(24), %"A&lt;f64&gt;"* noalias nocapture dereferenceable(48) %a, %"A&lt;f64&gt;"* noalias nocapture dereferenceable(48) %b) unnamed_addr #1 personality i32 (i32, i32, i64, %"unwind::libunwind::_Unwind_Exception"*, %"unwind::libunwind::_Unwind_Context"*)* @rust_eh_personality !dbg !2041 { start: %personalityslot = alloca { i8*, i32 }, !dbg !2044 %_25 = alloca i8, !dbg !2044 %_24 = alloca i8, !dbg !2044 %_23 = alloca i8, !dbg !2044 %_22 = alloca %"alloc::vec::Vec&lt;f64&gt;", !dbg !2044 %_19 = alloca %"alloc::vec::Drain&lt;f64&gt;", !dbg !2044 %_14 = alloca %"alloc::vec::Drain&lt;f64&gt;", !dbg !2044 %ys1 = alloca %"alloc::vec::Vec&lt;f64&gt;", !dbg !2044 %xs = alloca %"alloc::vec::Vec&lt;f64&gt;", !dbg !2044 %_7 = alloca %"alloc::vec::Drain&lt;f64&gt;", !dbg !2044 %ys = alloca %"alloc::vec::Vec&lt;f64&gt;", !dbg !2044 %zs = alloca %"alloc::vec::Vec&lt;f64&gt;", !dbg !2044 and this is the start of the tuple_fn_params function: define internal void @_ZN10playground15tuple_fn_params17h3a6a3f9e2e5a28f0E(%"B&lt;f64&gt;"* noalias nocapture sret dereferenceable(24), { %"A&lt;f64&gt;", [0 x i8], %"A&lt;f64&gt;", [0 x i8] }* noalias nocapture dereferenceable(96) %arg0) unnamed_addr #1 personality i32 (i32, i32, i64, %"unwind::libunwind::_Unwind_Exception"*, %"unwind::libunwind::_Unwind_Context"*)* @rust_eh_personality !dbg !2062 { start: %personalityslot = alloca { i8*, i32 }, !dbg !2069 %_26 = alloca i8, !dbg !2069 %_25 = alloca i8, !dbg !2069 %_24 = alloca i8, !dbg !2069 %_23 = alloca %"alloc::vec::Vec&lt;f64&gt;", !dbg !2069 %_20 = alloca %"alloc::vec::Drain&lt;f64&gt;", !dbg !2069 %_15 = alloca %"alloc::vec::Drain&lt;f64&gt;", !dbg !2069 %ys1 = alloca %"alloc::vec::Vec&lt;f64&gt;", !dbg !2069 %xs = alloca %"alloc::vec::Vec&lt;f64&gt;", !dbg !2069 %_8 = alloca %"alloc::vec::Drain&lt;f64&gt;", !dbg !2069 %ys = alloca %"alloc::vec::Vec&lt;f64&gt;", !dbg !2069 %zs = alloca %"alloc::vec::Vec&lt;f64&gt;", !dbg !2069 %b = alloca %"A&lt;f64&gt;", !dbg !2069 %a = alloca %"A&lt;f64&gt;", !dbg !2069 Interestingly, the normal_fn_params function is directly assigning the names `%a` and `%b` to the arguments, whereas in the tuple_fn_params version, it preserves the semantics of the argument being a "tuple" even in the LLVM IR. This tuple parameter is named `%arg0`. Then, there are two additional `alloca`s in the tuple function's initialization sequence. Later on in the function, the tuple version is generating `getelementptr` statemetns that access the chunks of the tuple. Really, I think LLVM should be capable of optimizing this to exactly the same assembly as the normal_fn_params version. Since C and C++ have historically had non-existent support for tuples, it looks like LLVM is not optimizing this as well as it should, and I think this could be considered a bug in LLVM. Ideally, `rustc` will be reaching a point in the near future where it may start doing real optimizations of its own in MIR, and it could probably smooth this out for LLVM from its side of the compilation. But, I still don't feel strongly either way. When you're worried about +/- 10ns, you need to be worrying about how floating point division is slower than floating point multiplication, and a host of other things that are just no fun. And, as a point of terminology, "zero cost abstraction" does not mean that the code is zero cost, [it just means that the abstraction shouldn't impose a cost over the optimal implementation of the task it is abstracting.](https://news.ycombinator.com/item?id=12270428) On different CPU architectures, and even AMD vs Intel, one memory model may perform better than the other, and vice versa. A tuple is not really a high-level construct that I would even call "zero cost", it's just choosing a different in-memory representation. Passing a tuple directly to a function seems like something that would "splat" the tuple and make it disappear, but tuples are used for a lot more than function arguments, and the memory model more closely aligns with a struct than with individual parameters, logically. However, structs may get optimized better even than tuples because of how heavily C and C++ uses structs for arguments. Anyways, here's another benchmark you can try: https://play.rust-lang.org/?gist=0f2c8c7c9869229e2f9b6a87e243aefd&amp;version=stable I'm curious what the results are on your computer, since they vary so wildly on mine.
I like the insight into decision-making. Lots of documentation skips over the “why”, and focuses on the “what”. 
Thank you for the kind words! It is a bit of a respect thing. Look at all the exhaustive, great answers I've been getting so far. The least I can do is to state my questions with enough context so that people know where I'm coming from and what it is I need to hear in order for me to understand it. 
Sorry for the low value comment, but I just want to point out that pq in French is short for toilet paper.
Haha I was expecting something like this. I'm leaning towards Rust, because it isn't just a compilable version of Python. I actually want to learn new concepts. I don't have a problem with a steeper learning curve. The only big question is: Will Rust's added complications actually "force" me to write cleaner/better code? Or will I just be constantly fiddling around with memory management at the cost of actually writing "useful" code?
This gives me hope! :-) Thanks!
Wrong sub, boyo
It is fundamentally different. What is the trait for a function that takes either an int or a float in a language without overloading? You could hack together something that makes this example compile but trait based definitions is a trade off, you lose some generality for stronger errors as they can specify the missing trait.
Thanks for your input. So a GC is a runtime memory-cleaner which understandably makes the executable slower. While in Rust's case the compiler is some sort of "compile-time-Garbage-Checker" which tries to handle as much memory management as possible during compilation? This hasn't been worded correctly, but I hope you get the point. So does that mean that Rust programs take (significantly) longer to compile? I really don't mind, I'm just wondering.
Thanks for the input and links. I think i will have to work on my Rust skills a bit more before I dive into the links you attached. But I saved your reply so i can come back to it later. Much appreciated!
That is also what I kept hearing while researching. Everybody seems to be very happy with cargo. Also the "tooling" in Rust seems to be something that people are quite happy with. Even though I'm not 100% sure of what exactly is meant with "tooling". Is it the ecosystem of compiler, debugger, cargo, etc... ?
It's also the name of the PostgreSQL client library (libpq).
Well the problem is that I don't have any experience with compiled languages yet. If Rust is a bit more complicated than Go or others, but this also forces me to write cleaner/better code then this is something I am happy with. However if I have to waste 30% of my coding time with memory management, just so my executable is 3% faster than a comparable Go executable then I'd have to say: "F**k it, I'm using Go.
Yes. All of the tools that you use every day when writing and maintaining projects in Rust. There's no language that parallels Rust level of tooling. From the Cargo build tool, to web services like doc.rs. If you need API docs for any Rust crate, all you have to do is type docs.rs/crate-name to get it's documentation in your web browser. Very simple, and very memorable. If you install 'cargo-edit', you can also simply do a 'cargo add crate' to automatically format and append that crate to your project file. Has a number of other useful features. Using Cargo, you can just do a cargo search to search through crates on Crates.io without opening your web browser, followed by a cargo add to add it.
I will take a look at wlc.rs! And once I am up to speed and getting started, I'll get back to you! Thanks!
ok, but again my real-world example (way to convoluted and involved to put in a playground), I'm getting a difference of 100ns on a function call that takes 500ns. Depends on what you're doing, but 100ns for passing a tuple is a pretty steep cost, IMO. The example is just to show the pattern that's causing a large divergence in real-world code. 
this makes a lot of sense, thank you. 
I already played around wit cargo/cargo-edit a bit and it is very appealing to newcomers like me!
here's what I'm getting on my laptop on several runs of both my benchmarks and yours (denoted with `_repeated`: test tests::it_uses_normal_params_impl ... bench: 260 ns/iter (+/- 6) test tests::it_uses_normal_params_impl_repeated ... bench: 3,910 ns/iter (+/- 69) test tests::it_uses_struct_params_impl_repeated ... bench: 3,975 ns/iter (+/- 208) test tests::it_uses_tuple_params_impl ... bench: 268 ns/iter (+/- 7) test tests::it_uses_tuple_params_impl_repeated ... bench: 4,028 ns/iter (+/- 383) test result: ok. 0 passed; 0 failed; 0 ignored; 5 measured; 0 filtered out test tests::it_uses_normal_params_impl ... bench: 279 ns/iter (+/- 4) test tests::it_uses_normal_params_impl_repeated ... bench: 3,883 ns/iter (+/- 136) test tests::it_uses_struct_params_impl_repeated ... bench: 4,272 ns/iter (+/- 58) test tests::it_uses_tuple_params_impl ... bench: 295 ns/iter (+/- 3) test tests::it_uses_tuple_params_impl_repeated ... bench: 4,278 ns/iter (+/- 82) test result: ok. 0 passed; 0 failed; 0 ignored; 5 measured; 0 filtered out test tests::it_uses_normal_params_impl ... bench: 261 ns/iter (+/- 3) test tests::it_uses_normal_params_impl_repeated ... bench: 3,923 ns/iter (+/- 60) test tests::it_uses_struct_params_impl_repeated ... bench: 3,977 ns/iter (+/- 113) test tests::it_uses_tuple_params_impl ... bench: 265 ns/iter (+/- 15) test tests::it_uses_tuple_params_impl_repeated ... bench: 3,963 ns/iter (+/- 116) test result: ok. 0 passed; 0 failed; 0 ignored; 5 measured; 0 filtered out test tests::it_uses_normal_params_impl ... bench: 263 ns/iter (+/- 8) test tests::it_uses_normal_params_impl_repeated ... bench: 3,925 ns/iter (+/- 315) test tests::it_uses_struct_params_impl_repeated ... bench: 4,024 ns/iter (+/- 156) test tests::it_uses_tuple_params_impl ... bench: 265 ns/iter (+/- 12) test tests::it_uses_tuple_params_impl_repeated ... bench: 4,018 ns/iter (+/- 179) test result: ok. 0 passed; 0 failed; 0 ignored; 5 measured; 0 filtered out
How this differs from the builtin `protoc --decode` and `protoc --decode_raw`?
I would say on almost all of those that all the variants are within the uncertainty / margins of error of each other. It's a good guess that the tuple version *is* marginally slower than the normal parameters version, but scientifically I think the data in these benchmarks isn't all that conclusive.
What point would inheritance have when custom derives exist?
Go is also a fantastic language and has its own challenges and unique concepts (channels &amp; goroutines, struct composition, interfaces). I think it's definitely worth learning if you'll do the types of tasks that Go is designed for, such as networked services and other highly concurrent tasks. Rust is a fantastic, general purpose language, so it's also a good choice.
Then it's likely not the tuple itself but some pessimization it causes, because otherwise the cost of passing the tuple itself should be constant.
The reason you need `self` instead of `&amp;self` is because `target` is an `Option&lt;Box&lt;Node&gt;&gt;` instead of an `&amp;Option&lt;Box&lt;Node&gt;&gt;`. As written, you try to take ownership of `self.next`, which does not work unless the method takes ownership of `self`. What you probably want is to make `target` a reference to `self.next`, i e let target = &amp;self.next; or let target: &amp;Option&lt;Box&lt;Node&gt;&gt; = &amp;self.next; 
Yep, I've hit this bug too. Tokio appears to leak connections in certain situations: https://github.com/tokio-rs/tokio-core/issues/269 For Hyper (which is my use case), seanmonstar has taken the liberty of extricating out tokio-proto and providing a [no-proto](https://github.com/hyperium/hyper/pull/1362) option. I believe the plan is to remove tokio-proto completely at some point. 
You probably saw my comment on the issue, but [I think I figured it out](https://github.com/tokio-rs/tokio-proto/issues/194#issuecomment-340648596).
You shouldn't need to mess with memory management at all, excluding the concept of ownership itself (unless you're using the unsafe superset). Ownership takes care of that for you. So long as you understand how ownership works, both in general and in practice for your application, it will just happen.
Do you plan on writing about those? I'm especially interested on your experience with memcmp.
[removed]
You will possibly be annoyed at first but much less so later on. That's been my experience, and that of countless others
Yep, that's pretty much it. Think of it like Rust simulates the memory management if it were to be run. Anytime it's unable to figure out what happens to memory, that's an error.
Awesome! I ran across this post as well and thought about tracking it in a Rust version too, but have limited time at the moment. If I get some spare cycles maybe I'll keep an eye on yours for fun. I'd be interested to see if you run into any Rust specific issues.
I feel bad only having this comment, but, your website theme is great.
There's not much manual memory management in Rust over Go, or at least not in the sense that other lower level languages like C have. For the most part, the effort in heap memory management comes in the form of deciding who owns a piece of data.
[Here's][impl] my improved implementation regarding `memcmp`. Rust apparently has a specialized `PartialEq` implementation for `&amp;[u8]` that emits a `memcmp` call. I know this by simply seeing the assembly output for the standard non-`simd` version. Sooo, having _a bit_ of experience with low level programming and knowing how to read some assembly, I tried making a faster implementation. I realized that since the buffer is `[u8; 64]`, it can essentially be split perfectly into `[[u8; 16]; 4]`. Because of this, it could _also_ be represented as `[u8x16; 4]`. [`u8x16::load`] allowed me to do exactly that. I made the appropriate changes, commented out the `#[inline]`, and ran: cargo rustc --lib --release -- --emit asm Finding the method with its mangled name is as simple as searching "PieceMap.*eq" as a regular expression. I also saw that LLVM managed to unroll the small loop, which was a small bonus. So while some people may be able to tell if the new output was actually better, the better way is to use concrete numbers and run two benchmarks: one with `simd` enabled and one without. [impl]: https://github.com/hexe-rs/Hexe/blob/dddc77323406686866342e756d0d7aa1b1b921f2/hexe_core/src/piece/map.rs#L60-L73 [`u8x16::load`]: https://docs.rs/simd/0.2.0/simd/struct.u8x16.html#method.load
Thanks. I use Hugo with this theme: https://themes.gohugo.io/after-dark/ Mostly stock.
Never tried those before actually, I'll try them and report back.
Alas, I've seen too much of this fallacy in our industry to make it remotely funny, smiley or not. What gets my goat is that *some* people will ignore the smiley and see their beliefs validated. Your 'just a joke'-rhetoric is a classic gatekeepers' defense.
What issues did you run into with the enum representation? So far my only problem is that `Square`'s doc page is massive due to its 64 variants. But that's not a usability issue.
The original reason was to make memory safe non-garbage collected programming usable, but the reasons have evolved somewhat. Around the end of last year we had a great discussion about what Rust's mojo is which brson documented at [fire flowers](https://brson.github.io/fireflowers).
What issues did you run into with the enum representation? So far my only problem is that `Square`'s doc page is massive due to its 64 variants. But that's not a usability issue.
100% this. I've been able to optimize in weird ways by changing a few bits of Rust code around (not adding or deleting) and seeing that it compiles down to _fewer_ instructions, which is generally faster.
France? I work in Montreal and none of my coworkers mentioned it.
I disagree: both Rust and C++'s generics model is based on monomorphisation, meaning just as much type information (i.e. all of it) is available for code generation. This differs to languages like Haskell, Ocaml and Swift that use dictionary passing, and to Java's erased generics too. The fact that the ad-hoc overloading that the function uses has to be specified in its signature is a smaller difference than this, and indeed, C++'s static_assert/enable_if/concepts shows that gap being bridged (in one direction), whereas implementing a dictionary passing strategy in Rust or C++ would be much harder. &gt; What is the trait for a function that takes either an int or a float in a language without overloading? trait IntOrFloat { fn do_int(x: i32); fn do_float(x: f32); } Or, depending on what's exactly is being done, trait DoNumber { fn do_it&lt;T: Number&gt;(x: T); } You can argue that these aren't traits for the "function"/closure that do this, but that's "just" a syntactic thing, since closures are equivalent to structs with trait impls, similar to C++. You can definitely argue that C++ is nicer for this and I'd agree (at least, when the code doesn't have any errors in it), but it is *possible* in Rust. &gt; you lose some generality I think you lose some convenience (e.g. requiring more trait definitions, and possibly wrapper structs), but I don't think you lose generality.
This is one, I remember commenting how this is a security whole and it needs to be password protected, even for localhost.
Default behavior which is variable on internal types. having this without having to write a compiler addon is nice. 
From France yeah, it's actually short for "papier cul"(Q). 
[The book](https://doc.rust-lang.org/book/second-edition/) is a pretty good place to start.
First off, I would say that any time you learn a new language your code is going to start off "worse" than in a language where you have experience. This is just like in natural languages—when you're starting out you don't know what is natural and so will use constructs that a native speaker would find awkward. I think one of the stumbling blocks you are likely to find coming to Rust from Python is that "there is more than one way to do it". In Python, you have one way to represent strings (well, two in Python 2). In Rust, you will have to deal with many more [different types of strings](http://www.suspectsemantics.com/blog/2016/03/27/string-types-in-rust/), each with its own uses. Keep in mind that, just because there's more than one way to do it doesn't mean that the choices are arbitrary. For a particular bit of code, there is usually a best choice or some choices are much better than others, and so part of the learning curve is understanding the trade-offs between the different options and the right one to use in each case. 
The irony...
Unfortunately some slides are unreadable.
I'd honestly suggest a different name because googling "cargo airplane" is never going to be a pleasant experience.
Congrats on 1.0!
Even if you are doing crazy things with types, you can often ignore it. I don't think I've used it once, and I've abused quite a few types.
Even if you are doing crazy things with types, you can often ignore it. I don't think I've used it once, and I've abused quite a few types.
Websocket: not now, but you can run a websocket in a separate thread, and communicate like you would between threads in Rust. You have to choose a different port though, but you can reverse proxy them together with nginx (that's my current setup) if you need both on like port 80
It depends how you define generality. I would include arbitrary function calls but you think that traits are equivalent (even when the example uses overloading which is obviously impossible). Heck template specialization is a huge pain to replicate and that is only scratching the surface of template meta programming in C++. I don't want Rust to have a non-Rust turning complete meta language but have to admit there are things that templates can do that Rust can't. There is work to get the useful subset converted but there always be a small gap of things that aren't useful or possibly not important enough to prioritize.
Sorry about that... Here are the slides he used for his speech: https://sergio.bz/docs/rocket-china-meetup-slides.pdf
It's not always the wisest thing to do; check out the blog post by /u/Manishearth linked in a sibling thread to learn why. However, it will be possible to do in the future, but there will always be a speed bump. Here's the thing that makes it possible: https://github.com/rust-lang/rust/issues/43038 After this, you can take a single `&amp;mut foo.bar1`, convert it to `&amp;Cell&lt;type of foo.bar1&gt;`. After that, you can have multiple `&amp;Cell&lt;type of foo.bar1&gt;` references by reborrowing it as many times you like. You won't still be able to pass it as the plain type of `foo.bar1` anywhere, because having multiple mutable aliasing references in the absence of `Cell` types is undefined behaviour in Rust.
When I last tried them, neither `qmlrs` nor `qml-rust` worked. However, there is a newer project, the [Rust Qt Binding Generator](https://www.vandenoever.info/blog/2017/09/10/time_for_rust_and_qml.html), which does work and supports QML. You should probably use that. Regarding your question, once you have a binding, you can use any of the QML components you want, they don't require any specific interaction from the back-end, whether C++ or Rust. So you can import the Universal Style controls, build your own from `Rect`s manually, or whatever. 
&gt; I would include arbitrary function calls but you think that traits are equivalent (even when the example uses overloading which is obviously impossible). There's little difference between a function call and using a trait in Rust: `f(x, y)` is equivalent to `f.call((x, y))` aka `Fn::call(&amp;f, (x, y))` (or `FnMut::call_mut` or `FnOnce::call_once`). The overloading for these sort of higher-ranked calls can be handled by calling differently named trait methods: having one name for the two function different calls (i.e. overloading) is just convenience here, since you can get the same behaviour without it by typing a bit more. As I said, Rust does require more syntax (not allowing these higher-ranked calls to *look* like direct function calls) and so is less convenient, but it still can do what C++ can do in this respect: the CPU will end up executing similar sequences of instructions. &gt; Heck template specialization is a huge pain to replicate and that is only scratching the surface of template meta programming in C++. To be clear, I'm only talking about the higher-ranked function calls with a generic function/`operator()`. There's no question that C++'s full templating system is more general/powerful that Rust's full system, but I think the handling of this higher-ranked-ness is semantically similar (but not syntactically).
I also would love to have generic `Fn` traits. I seldom need them, but the few times I've had the need, it was really unsatisfying to bump into the limits of what the type system can do. It should be entirely possible to monomorphise them, as long as the closures wouldn't be trait objects but statically known.
&gt; &gt; Also how easy is it to "talk" to the Wayland C-functions in Rust? &gt; &gt; You can do so unsafely using wayland-rs, but depending on what framework you use you shouldn't need to do that Well, most of [wayland-rs](https://github.com/smithay/wayland-rs) is safe to use, that's the entire point of these libs! Though yes, if you need to pass pointers to or from a C framework, it'll be unsafe, that's for sure... /u/booooomba : to add a little on the excellent answer of u/_Timidger_ I'll add that mostly the landscape of wayland compositors looks like this: - at the lower level, you can build a compositor from scratch using the low level system and wayland libraries. This is enormous and a real lot of work. - at a higher level, you can use a library that abstract some of these for you. They exist in different levels of abstractions, mostly the big ones currently are [wlc](https://github.com/Cloudef/wlc) (but keep in mind that it's kinda being abandonned), libweston (the weston developpers are trying to extract part of weston into a lib), and the soon-ready [wlroots](https://github.com/swaywm/wlroots). All of these are C libraries. If you don't want to have to worry about all the gory details of a compositor, I definitely recommend you use on of these 3. As u/_Timidger_ pointed to you, rust bindings to wlc already exist and can be a good start. On the other hand, if you are interested about some of the low-level stuff that happen in these wayland compositor framework, I'll let you know that u/drakulix (the maintainer of [fireplace](https://github.com/Drakulix/FIREPLACE) ) and myself are working on [smithay](https://github.com/smithay/smithay), and other framework for wayland compositors, but in Rust. ;-)
Agreed. Something like `--offline` would be great
Ah, that makes sense.
It might suggest the option when network attempt is unsuccessful.
Awesome, thanks!
Note that in C there actually exist `malloc` and `free` (delete) functions. This is also why I think you should try a little bit of C - just to understand what Rust protects you from. (I once did a workshop for people to see where/how they can screw up in C, so they would better understand Rust on the next workshop.) &gt; I already know the easy end of the spectrum (Python) and for my general programming skillset I think Rust would be a more meaningful addition I like your approach. :)
something something cargo cult
Thank you! This is a interesting talk.
&gt; Did you understand that there was a type conversion in the Rust version? Probably there was an `Into` trait implemented. I'm perfectly sure that the example you've given doesn't involve `Into` trait. Rust has no implicit conversions besides few obvious things like `Deref` coercions and `&amp;mut` -&gt; `&amp;` conversions. Of course, without knowing types I don't know what `+` operation does. If I know that those libraries were written by reasonable people, I may expect that it's number addition or string concatenation.
I'm pretty sure googling a word starting with a dash will be disappointing, too!
Nothing really special, but I thought that it would be cool to share: I'd plugged a couple of sensors into the RaspberryPi, dropped in [pcf8591] (https://crates.io/crates/pcf8591) and [rumqtt](https://crates.io/crates/rumqtt) crates and it just works! Amazing :)
Hello! I tried to reach you on IRC, please, I have a question about developing RAW files. Why there is the need for `rawloader/data/cameras` and the like, i.e. why is it necessary to supply the various camera attributes by hand? Aren't those in the RAW's TIFF structure? Also, if it's not too complicated, could you please explain in the most basic terms what steps are needed to develop a RAW file? I mean, what other steps there are besides: locate the sensor data (`StripOffsets`), possibly decompress it, find width and height, crop, adjust black and white points (?), adjust white balance, do something about colors (?), debayer, ..?
I'll just repost here what I posted on the board for greater visibility: &gt; I just want to expand here that “airplane” is not the only possible reason for no internet connection. &gt; Here are a few examples: &gt; - Limited data plan, the user wants to do some development offline, without using up data. - In area with bad/no internet connection. Yes, there are still areas like this, especially in the less developed countries. - No internet connection for a set period because ISP decided to do some intrusive maintenance, or there is some other problem on the ISP’s side. (Yes, this does happen from time to time where I live).
&gt; Will Rust's added complications actually "force" me to write cleaner/better code? I believe that yes, they will. Especially when you grasp generics and traits.
I'm one of those academics, and while my more traditional packages got through, my most impactful one (IRkernel, the R kernel for Jupyter) didn't.
&gt; Thanks for your input. So a GC is a runtime memory-cleaner which understandably makes the executable slower. &gt; &gt; While in Rust's case the compiler is some sort of "compile-time-Garbage-Checker" which tries to handle as much memory management as possible during compilation? Yep. Rust has a few different checks it runs, but, just as people tend to lump Firefox's "cycle collector" in with the garbage collector when talking about memory management (even though they run as separate passes), we tend to just refer to all of it as "the borrow checker". That's also one of the reasons people call Rust "harder". If you go beyond the limits of what the borrow checker can figure out, you have to explicitly choose a workaround. (eg. the `Rc&lt;T&gt;` and `Arc&lt;T&gt;` types allow you to wrap a variable in a reference count, similar to what CPython uses. `Rc&lt;T&gt;` being faster but not usable from multiple threads, while `Arc&lt;T&gt;` takes the performance hit of using threadsafe code to manage the count.) &gt; So does that mean that Rust programs take (significantly) longer to compile? I really don't mind, I'm just wondering. Rust compile times are comparable to those of a C++ program using similar language features (and C++ is known to be slow when not using incremental compilation) but they're working on improving that. (At the moment, they've written incremental compilation support for `rustc` and they're in the process of shaking the bugs out so they can turn it on for everyone.) It's actually not the borrow checker that's the major slow part but, rather, various things related to code optimization which haven't yet been tuned up because, for Rust 1.0, the focus was on fixing everything that couldn't be fixed later without breaking people's code. (Rust is made up of two parts: The `rustc`frontend, which translates your code into LLVM IR and the LLVM backend, which optimizes LLVM IR and then translates it into machine code. `rustc` current generates really sloppy LLVM IR and relies on the LLVM optimizers to clean it up... but that takes time.) That said, being really fast to compile was a specific design goal of Go, so you could say that Go intentionally sacrifices runtime performance to get fast compiles as long as it satisfies the Go developers' definitions of "fast enough". (Sort of like how some of the design decisions which make Python convenient to program in also resulted in it being one of the slowest dynamic languages, but it's hugely popular because, with compiled C extensions, it's still perfectly suitable for many purposes.)
&gt; No internet connection for a set period because ISP decided to do some intrusive maintenance, or there is some other problem on the ISP’s side. (Yes, this does happen from time to time where I live). Or because electricity is down entirely, which also happens from time to time. A laptop can work for some time on battery (one of their various advantages) and a company can run on generators, but cargo won't work in that case.
Does this mean, that sometimes tuples ca be faster and sometimes slower?
While I avoid QML because none of the styles feel anywhere near native on Linux, I do have an approach which works for me and may be useful to you: I use [rust-cpython](https://github.com/dgrunwald/rust-cpython) (for stable rust. The [PyO3](https://github.com/PyO3/pyo3) fork is nicer if you're on nightly.) to hook Rust up to either [PyQt](https://en.wikipedia.org/wiki/PyQt) (complete Qt5 support, GPL or paid license) or [PySide](https://wiki.qt.io/PySide) (only Qt4 support is complete so far, LGPL) with Python becoming sort of a "QML for the QWidget API". [setuptools-rust](https://github.com/PyO3/setuptools-rust) can then integrate cargo into Python's `setup.py` build automation.
**PyQt** PyQt is a Python binding of the cross-platform GUI toolkit Qt, implemented as a Python plug-in. PyQt is free software developed by the British firm Riverbank Computing. It is available under similar terms to Qt versions older than 4.5; this means a variety of licenses including GNU General Public License (GPL) and commercial license, but not the GNU Lesser General Public License (LGPL). PyQt supports Microsoft Windows as well as various flavours of Unix, including Linux and macOS. PyQt implements around 440 classes and over 6,000 functions and methods including: a substantial set of GUI widgets classes for accessing SQL databases (ODBC, MySQL, PostgreSQL, Oracle, SQLite) QScintilla, Scintilla-based rich text editor widget data aware widgets that are automatically populated from a database an XML parser SVG support classes for embedding ActiveX controls on Windows (only in commercial version) To automatically generate these bindings, Phil Thompson developed the tool SIP, which is also used in other projects. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Every time I need information on how to use the ip command... googling for ifconfig was so much more pleasant.
Thank you for your clarification, I defend myself under the "I am a newbie" clause of my disclaimer. Thinking more about it, I think I mixed it in my head with the `Copy` trait, which executes implicitly. I remember wonderin about why I was able to transfer the ownership of a variable and continue to use it anyway. If you have any recommendations about how to express it more correctly and clearly, please let me know and I will fix it.
I don't know the complete list, but I remember [`n-body`](https://benchmarksgame.alioth.debian.org/u64q/nbody.html) being mentioned.
Really enjoyed the write-up - thank you!
&gt; The Rust team at Mozilla is spread across three continents, with most hosting meetups/talks. Only three :-)? There are lots of people in North America &amp; Europe, @nrc is in New Zealand, and me &amp; @Manishearth are in Asia. If you count contributors, there are also people from South America.
&gt; If you have any recommendations about how to express it more correctly and clearly, please let me know and I will fix it. Any of the Rust methods could involve a long chain of trait implementations, or be defined inside a procedural macro, which can make it hard to track down where they come from, and hard to change their behavior.
Oh, I didn't know second edition of book is completed, thanks :D
That seems to be a pretty clear bias from people traveling from conference to a conference all the time :) Otherwise working on a plane is a pretty exceptional situation compared to more boring reasons to be offline.
How did you learn to write parsers? Are there good resources?
Thank you very much :)
Interesting. Thanks.
&gt; The borrow checker also protects you from accessing foo.bar1 through foo while the bar1 reference exists. Note that sometimes the compiler is smart enough to understand disjoint accesses so e.g. let m1 = &amp;mut foo.f1; let m2 = &amp;mut foo.f2; will work because the compiler can see that there is no aliasing. However it breaks easily e.g. if you hide `&amp;mut &lt;obj&gt;.&lt;field&gt;` behind functions they're opaque to the borrow checker (maybe by design, IDK) and then Rust will refuse to compile: https://play.rust-lang.org/?gist=7c220c215f26693df5bf5b4eca4493e7&amp;version=stable
`u32` inputs and a `f32` average? Oh my... use `[std::u32::MAX]` as input and be disappointed :)
I'll take the lead in maybe leading people down the wrong path :P. movups xmm0, xmmword ptr [rdx] movups xmm1, xmmword ptr [rdx + 16] movups xmm2, xmmword ptr [rdx + 32] will be faster than movups xmm0, xmmword ptr [rsi + 48] movups xmm1, xmmword ptr [rsi + 64] movups xmm2, xmmword ptr [rsi + 80] because the first instruction doesn't need any addition, and the +16 and +32 could be optimized in hardware.
Yep, rural India’s mobile data coverage isn’t that great between towns and villages
A bit late but I've got this idea to work. I have the user_index only, then changed the FromRequest to this impl&lt;'a, 'r&gt; FromRequest&lt;'a, 'r&gt; for User { type Error = (); fn from_request(request: &amp;'a Request&lt;'r&gt;) -&gt; request::Outcome&lt;User, ()&gt; { request.cookies() .get_private("user_id") .and_then(|cookie| cookie.value().parse().ok()) .map(|id| User(id)) .into_outcome((Status::Unauthorized, ())) } } Then created an error handler that does a redirect: #[error(401)] fn unauthorised(_req: &amp;Request) -&gt; Redirect { Redirect::to("/login") } Then included it in my routes: fn rocket() -&gt; rocket::Rocket { rocket::ignite() /* ... */ .catch(errors![not_found, unauthorised]) } With all this done, attempting to visit the index page with the User parameter will automatically redirect to the login page without having to define a second route.
Not OP, but have written a parser in Rust using https://github.com/Geal/nom. Worth a look, the documentation's great. Helped that the file format I was trying to parse had decent documentation as well (https://github.com/ephtracy/voxel-model/blob/master/MagicaVoxel-file-format-vox.txt).
Yeah, I suppose it means they can often result in an unnecessary copy, but typically it's going to be a negligible difference. If you're going to use a particular tuple type by value in a lot of places, probably better to make it a struct, as keeping the data in the same representation everywhere would avoid copies. Really this is a micro-optimization though.
&gt; I also made a small contribution to assert_cli. Friendly bunch of maintainers. Thank you! If you have any more suggestions or want to chime in on the API design, we are currently trying to settle on something for 1.0 and would value your opinion a lot :)
Or when you're camping in an RV somewhere you have a laptop, power and a lot of downtime but no cell service (or 3G so slow it might as well be voice+SMS only).
Weather as well messes up the internet around here (I guess due to the long distance microwave links)
I made a txt file where I copy and paste interesting Rust-related stuff that I need to look at once I'm more familiar with it. This goes into that file also!! ;-) Thanks again! And one last question: How did you get so familiar with the "under-the-hood" workings of Rust, Go, Python, etc... Is that something that automatically comes with using languages frequently? Something you looked up yourself because it was interesting to you? Or did you actually work on under-the-hood stuff?
I will keep that in the back of my head when trying to cope with my frustration!! :-)
Thx! :)
But also structs can be copied.
Thanks for your input. I actually took a look at fireplace also, but I wasn't aware that sway (which I'm using) has its own set of wayland-libraries. Awesome! So just to make sure I understand this: wlroots is a higher-level C library so it's easier to work with wayland? And wayland-rs is a higher-level rust binding? Does wayland-rs connect (what is the right verb here? connect? bind?) directly to the low-level wayland libraries? Because it seems that: Wayland libraries -&gt; Rust library would be better than having another layer of library in between like so: Wayland libraries -&gt; C higher level library -&gt; Rust library You would agree to this?
&gt; The big question is: What's the cost/benefit ratio of the added complication introduced by Rust? Will I be super annoyed because I constantly have to fiddle around with memory management all the timeand it's all gonna end up in a huge mess? Or is it gonna force me to write cleaner/more efficient code? How much of the coding time in Rust is actually spent/"wasted" on memory management? If it's something like 10%, that's something I could live with, but if it is considerably more than that, then this would be a real disadvantage (for me). Having come to Rust from a GC'd language (C#), I've found that in practice I've not had to worry about memory management any more than I have with C#. I have, however, only done fairly small projects so far (biggest being under 2k lines), so that may be something that only comes into play with larger projects. However, chances are you'll get fairly frustrated with the compiler at first, when it keeps telling you that you've done something wrong because you haven't properly learned what the borrow checker wants yet. I decided to learn Rust specifically because of the borrow checker; it wasn't something I'd seen in other languages. Now that I've spent time with the language, I've found I much prefer it to the others I know. I like how strict the compiler is, because I can be more confident that if my program doesn't work it's because of a logic error on my part. Compile times could be better, though.
Finally got some time to focus on [`uom`](https://github.com/iliekturtles/uom) (type-safe zero-cost dimensional analysis) over the weekend and made big process on my efforts to support non-floating point underlying storage types. I'm down to two failing tests from before the change!
&gt; "there is more than one way to do it" This is exactly what I was afraid of. Since Python is my only language so far it seems very bizarre that choosing a different type of string could have any benefit or disadvantage for your program. I cannot even imagine how this would come to any real speed improvement. Or whatever the benefit of choosing the right type of string is. But I will find out soon! :-)
I'm glad to hear that! It seems the entire concept of ownership is very important and will be vital to me understanding Rust. Thanks!
I will keep that in mind when I get frustrated! :-) Thank you!!
The one thing that sticks out from all the answer I received is that the entire concept of ownership is very important and will be vital to me understanding Rust. It is good to know this before really getting into it!
From my very anecdotal perspective: a healthy medial between consolidation and heavy active development. Due to rust's policy on being very welcoming to newcomers and the relatively young and incomplete library ecosystem, we suffer from a large number of incomplete or immature libraries that rush to claim a solid position in the ecosystem but get abandoned soon after announcement. Luckily, things have died down but these abandoned libraries still form a large portion of crates.io.
I'm leaning towards Rust for similar reasons. I want something that is not like Python at all and Rust seems to have many interesting features. When I did my research two things became pretty apparent: 1. Rust seems to have a over-proportionally frustrating learning curve 2. Rust seems to have a over-proportinally happy user-base. So I really hope (2.) is related to (1.) :-D
I like the name
It's the kind of shallow-but-broadly-useful knowledge that you'll pick up sooner or later if you work with a variety of languages unless you're *really* incurious because the general stuff is included in well-written answers to questions like "Why do memory leaks happen?", "Why do programs crash?", "What do they mean when they say 'managed language'?" or "Why is language X slower than language Y?". There are a few speed bumps but, as long as you're willing to google up explanations friendlier than the Wikipedia articles, you can learn this stuff very quickly. Once you know the general terms it's easy to check Wikipedia for specifics like "CPython (the main Python implementation) uses reference counting with a limited garbage collector to break reference cycles". NOTE: Reference counting is technically a form of garbage collection, but people tend to think of them as separate (ie. "RC and GC") for two reasons: * RC will get confused by reference cycles while other types of GC won't. (If A holds a reference to B and B holds a reference to A, then both are being referenced by something, but it can't tell that the rest of the program can no longer reach them.) * RC may be garbage collectION, but it doesn't require a garbage collectOR, which means it can be easily added to any language as an option. (It just needs some extra code to manage the count and do the final deletion which gets run whenever something acquires or releases a reference to something, such as Rust's `Rc&lt;T&gt;` and `Arc&lt;T&gt;`, or C++'s `shared_ptr`.) One detail which *does* take a little more luck to stumble across without looking for it is that RC, despite its simplicity, can be slower than GC in certain circumstances. (Because all of that fiddling with stored counts doesn't come free, while garbage collection just "lets memory leak" for a while, then sweeps through in one big pass, freeing anything that "leaked".)
Example with `Vec::clear`. fn main() { let mut v = vec![1, 2, 3]; let elem = &amp;mut v[0]; Vec::clear(&amp;mut v); // usually written as v.clear() println!("{}", *elem); // this would read freed value } 
The compiler always has to make a copy if you pass something by value, because the callee owns the arguments and might modify them. And usually LLVM cannot optimize that copy away either. Iff the caller no longer uses that particular value that is passed to the callee, it could omit the copy, but currently this optimization is not implemented. Note that there was a bug until recently where the caller didn't create copies of constants that get passed by value, which was shadowed by the callee always creating copies of its arguments.
&gt; aarch64-linux-gnu-ld Change the linker to `aarch64-linux-gnu-gcc` and linking should work out of the box. All the Linux built-in targets assume that the linker is `gcc`.
I honestly understood it as a joke - as other posters said evidence tends to show it's erroneous. I don't know about the OP's original intent though.
Thanks again for the explanation. I'm hoping that in a few years I will be able to give the same types of explanations! :-)
Ah, thanks! It works now.
Great :) smooth sailing, friend. Have a good time!
FYI, there is *very little* that is impossible to do with Python, and almost all of it is going to be impossible in Go as well.
I don't doubt that. However, as I wrote before, others may feel their sentiment validated, joke or not.
oh i didnt noticed that thanks for saying 
Not normally a fan of JSX syntax, but this looks very good. Nice find. Edit: This seems to be a GUI library for both the web and desktop.
I doubt that said add is the issue. Said "add" is not a separate instruction. There is also no particular benefit for the +16 etc as the address generation unit adders don't really care. I'd expect this to rather be a CPU front-end issue, as the args variant code is (from the top of my head) 5 bytes shorter than the tuple variant. The code parallelizes quite fine so decode might just be the bottleneck. Hard to know without arch into. 
I don't operate in particularly data-starved environments (in any case I like staring out of windows while traveling) but I've enjoyed using my `runner` tool because it keeps a static cache and rebuilds the _offline_ docs for all crates in that cache when a new crate is added. Really just uses Cargo creatively.
It's not really about speed, but rather different requirements. * [`String`](https://doc.rust-lang.org/std/string/struct.String.html) is a resizeable string that *must* be valid UTF-8. * [`str`](https://doc.rust-lang.org/std/primitive.str.html) is a non-resizeable slice of ~~a `String`~~ bytes that also must be valid UTF-8. * [`CString`](https://doc.rust-lang.org/std/ffi/struct.CString.html) is intended for interop over a C interface. You shouldn't see this unless you start calling or being called from a C interface. * [`OsString`](https://doc.rust-lang.org/std/ffi/struct.OsString.html) is because different operating systems have slightly different strings. This is also resizeable. May not be valid UTF-8 or UTF-16. * [`OsStr`](https://doc.rust-lang.org/std/ffi/struct.OsStr.html) is a non-resizeable slice into `OsString`. * [`PathBuf`](https://doc.rust-lang.org/std/path/struct.PathBuf.html) is specifically for file paths, and has functions related to that. This one is resizeable like `String`. Again, this may not be valid UTF-8. * [`Path`](https://doc.rust-lang.org/std/path/struct.Path.html) is a non-resizeable slice into a `PathBuf` and has path-related functions. You can usually convert between these as necessary, though you'll need to handle a failure case.
wayland-rs is a low-level binding to the wayland libraries. It does nothing more than expose a safe interface in top of them. A wayland compositor has a lot of work to do, including direct discussion with the OS to handle user input, to setup a framebuffer, possibly via udev, on top of managing all the wayland clients. But the wayland C libraries _only_ manage the "wayland protocol" part, serializing messages between client and server. They do nothing more, nor does wayland-rs. Wayland compositor libraries like wlc, wlroots, libweston and (hopefully) smithay, on the other hand, abstract all this setting-things-up-with-the-OS for you, as well as providing a much higher-level handling of the clients, so that you can focus on what really makes your compositor what it is, rather than all plumbing job to make things work. Now, different wayland compositor libraries are not all as high-level as others, there is a spectrum, the main tradeoff being that higher-level libraries will often be easier to use, but less flexible, as they make more and more assumptions on how the compositor will behave. Now, if you want to compare directly, using wlroots or WLC would be: wayland C libs -&gt; C higher level lib -&gt; rust ffi bindings -&gt; your rust compositor while using smithay would be wayland C libs -&gt; rust ffi bindings -&gt; higher level rust crate -&gt; your compositor So I'd say the main factor of choice is mostly to look at what you expect from the compositor library to do for you. A rough comparison of how I understand the various libs is (I invite u/_Timidger_ to correct my mistakes if I'm getting things wrong): - WLC is pretty high-level, making it quite easy to get something to work (but is being abandonned) - libweston is quite high-level too, but I'm not sure about what's the status of its extraction from weston - wlroots is a little lower level than WLC, as apparently WLC was not flexible enough for the needs of sway, and afaik it is very near to being usable to make a compositor - smithay aims to remain very low-level at first, and thus being very flexible, and keeps open the possibility to integrate higher-level abstraction in the future, once the bases are solid. It is still very WIP, [see the 0.1 release notes](https://smithay.github.io/smithay-v-0-1.html) if you want more details.
I hope your open sourcing is coming along ... ;P You talked about it last week – no mention this week – suspicious!
It'd be nice to finally have a decent GUI library for Rust. That's one thing that seems to be sorely lacking.
Tbh, I don't really like the GUI libraries in any language that much.
I think when I first saw Rust, and heard it had C++ levels of performance. While I respect C++, I never really liked using it all that much as a language for programming games. I've tried it before, using SFML and OpenGL to do my own engine of sorts. But it felt like a lot of work, and then trying to get certain libraries to compile was annoying to no end. Rust offered cargo, a way to bring in dependencies similar to ones like npm or bundler. It also offered newer language ideas that I'm used to in dynamic languages like ruby or JS, mainly around blocks and iterators. I know C++ has more and more of these added, but it with Rust it just feels intuitive. While I've done code challenges, and have tried things like Piston in the past, I didn't really give rust a full try until this year. After I launched a v2 of a mobile game I built in HTML5, I decided to put some good effort into Rust. Took me a while to get tile map rendering in GFX, but it works now! I then took what I learned and made a game for ludum dare 39, at the end of July. I'm still in progress on improving that game. Both code structure wise, and adding new design.
&gt;&gt; &gt; Also how easy is it to "talk" to the Wayland C-functions in Rust? &gt;&gt; &gt;&gt; You can do so unsafely using wayland-rs, but depending on what framework you use you shouldn't need to do that &gt; &gt;Well, most of [wayland-rs](https://github.com/smithay/wayland-rs) is safe to use, that's the entire point of these libs! &gt; &gt;Though yes, if you need to pass pointers to or from a C framework, it'll be unsafe, that's for sure... Doh! I totally thought wayland-sys when I remembered wayland-rs. My fault because that's the only part of the library I use. Thanks for correcting my mistake :-). 
we could try to summon /u/ncarrillo to makes this happen :D and work together and help him ...
Anything heard from ticki?
Woo, glad to see a user of `kafka-rust`! As a recent contributor, I'd be happy to hear any feedback you might have about it. I also recommend you upgrade to 0.7, as I recently discovered [an unfortunate bug](https://github.com/spicavigo/kafka-rust/commit/d900e2f83b414450239f93febb5e089e3e750750) in the consumer offset committing logic.
Well, it sort of changed a little bit, a double dash `--word` will certainly include `word` now a days.
Excellent overview. Only thing I'll add for u/booooomba is that I'm currently building bindings for wlroots ([wlroots-rs](https://github.com/swaywm/wlroots-rs)). They aren't ready, and there's still a lot of design thought I need to put into it, but the hope is that you can create a compositor using safe Rust with all of the ownership and other rules laid out. Since wlroots is made to be more flexible, more is exposed however. So to extend it, you would need to drop down to unsafe Rust code and really understand what the library is doing. They are doing some [intense documentation work](https://github.com/swaywm/wlroots/pull/354) which will be useful, but it's still difficult to extend. Hopefully though, you should be able to do most things in safe Rust. Smithay, when it's done, won't have that problem. 
Ticki hasn't made any commits anywhere for 3 months. rip.
I think the parsiest thing I wrote for pq was: https://github.com/sevagh/pq/blob/master/stream-delimit/src/varint.rs#L24 I got that straight for the Google Protobuf documentation: https://developers.google.com/protocol-buffers/docs/encoding#varints And some trial and error as well. I'm mostly gluing together various libraries in pq.
Not quite full JSX like that, but /u/ncarrillo has been sharing some work on the subreddit of some UI work, where macros give some of that jsx-like syntax. https://www.reddit.com/r/rust/comments/76o9fi/whats_everyone_working_on_this_week_422017/doge36g/ It's very cool to see this stuff. Right now i'm looking at building custom UI using gfx-rs ontop of my game. Would be nice to hook in something with components figured out :)
After using JSX a LOT in React, it's is by far my favorite way to do UI now. I now understand why people build native desktop/mobile apps using things like React Native and js/css... despite the gotchas and not as good performance, it is that much nicer/quicker to build. 
I believe that's only for the sync part. If you use `["async", "async-ssl"]` you'll only get the Tokio based part.
This is the book I recommend to everyone getting started with rust!
Yeah, I know. Maybe there is a particular reason. Maybe he just takes a break. I don't know.
This is really cool and really close to what I've been doing with my syntax plugin, we even share the same name :P The power I think in going this direction is the ability to use Rust expressions as attribute values / content values. This enables the seamless mixing of declarative markup and Rust code that's analogous to what JSX does for JS. I don't support spread operators or member variables at the moment, so this seems a bit more fleshed out (not to mention the code is nice :)).
Tiny point, you can make a `&amp;str` from any sequence of bytes that's utf-8 encoded, it doesn't have to be from a `String`.
&gt; Even though I'm not 100% sure of what exactly is meant with "tooling". Everyone means something slightly different with this term, it's not just you. It's a grab-bag for anything extra that's helpful, basically.
I do the former.
Of course, my bad. Thanks for the correction.
I assume he is busy with school - he has responded to DMs on Twitter
I appreciate the sentiment, but for my case, "production" is the guy next to me doing bespoke data analysis.
well there is also limn-gui, which does layout and rendering in safe rust: https://github.com/christolliday/limn Currently, this is the most promising library for Rust-based GUIs.
Not yet, it's a heavily desired feature.
Finished a very basic prototype of my IPFS pinning service, you can try it out here if you have something to pin: http://md.alopex.li/ . Nobody's managed to outright break it yet, so I encourage you to try! Hopefully that means I can get back to ggez, but I can't always control where my inspiration leads me...
This turned up something interesting. I'm seeing a lot of mmap(0x7f0f1db7c000, 36864, PROT_NONE, MAP_PRIVATE|MAP_FIXED|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory) madvise(0x7f0f1db7c000, 36864, MADV_DONTNEED) = 0 happening well before entering the loop where things actually crash out. It might explain why it seems to fail on a BTreeMap of a few thousand items if that's just the straw that breaks the camel's back. I still don't know why the allocation it's doing seems to be invisible to accounting by UGE or top.
You might want to take a look at [this](https://gmjosack.github.io/posts/dissecting-cratesio-minimum-mirror/) post. There's also [this post](http://www.integer32.com/2016/10/08/bare-minimum-crates-io-mirror-plus-one.html) that builds on top of it. This is a highly desired feature but not close to being done. But you would definitely be interested in [this post](https://boats.gitlab.io/blog/post/2017-10-28-alternative-registries/) about the new alternative registries feature that was launched earlier this past week.
&gt; posting to /r/rust with a hint of autism
Also a [script to hack around this on airplanes](https://www.reddit.com/r/rust/comments/793evq/using_cargo_on_the_subway_a_script_to_hack/). The timing on this question is spot on :) 
Looks like the author works for Mozilla. Can't wait to see where this is going !
My personal experience with that benchmark is that it's mostly a measure of loop unrolling. If you manually unroll the loops or use crunchy, the Rust version jumps well above all the C/C++ programs.
Oh, he still goes to school?! How old is he?!
&gt; but it stops a little short of Haskell IMO it stops quite a bit more than a little short That's not bad or good, by the way, but it doesn't feel right to say that it stops a little short of Haskell for me. 
I just don't like XML syntax. Would prefer something like Maud. Also I really don't like "modern" JavaScript with all the js to js compilation. Would just rather use Kotlin at that point.
None so far. Any time I pick up my Rust tools, I feel like it takes me 5x longer than it would in NodeJS/C# (not exaggerating) to write, but then it just works and I have confidence it will perform too. It also takes a while to find the right crates sometimes. Rocket was easy enough to find as the (almost) de-facto web framework. I spent a while scratching my head why it was so complex to use Hyper to make a client http request until I came across reqwest. Those things fade with experience. One "rust" issue I didn't resolve through was lifetimes interacting with serde. You can return a Json&lt;T&gt; where T: serde::Serialize from an http endpoint and Rocket will serialize it for you, but I couldn't get that to work with borrows. E.g. returning the entire chain of blocks, the underlying Blockchain struct hands out a borrow, and in my head, that's fine, because Rocket just needs to serialize it. I kept hitting lifetime issues though - I think because the Rocket endpoints are static and I'm missing something about how to handle that. In the end, I serialize it manually and return the string with application/json, which is the same interface from the client point of view, but it irks me that I only did that because I couldn't satisfy the lifetime bit.
I intended to refer more to what's known of its potential. There's a lot that can still be done for Rust, but there are certain things which are off the table if you go for eager evaluation.
I agree strongly with all of this. It would also be useful to be able to bring the desired crates in on a USB drive (in the case where I have Internet access elsewhere but not near my development environment).
That's weird because it is setting that block to PROT_NONE which shouldn't allocate memory at all. NONE is what guard pages are set to: pages that use virtual memory but don't allow access. Looking at the documentation ENOMEM can happen when the number of maps per process is exceeded. That can happen on PROT_NONE because if it splits a map it creates three new maps, the area before, after, and the new PROT_NONE map. Check # cat /proc/sys/vm/max_map_count 65530 My Fedora system seems to have 65530 as the value which should be enough for anyone. :-) But here's at least one link that explains how raising the value solved a problem. I saw one about Oracle DB as well. https://www.novell.com/support/kb/doc.php?id=7000830
Ask him if you want an answer
Grad school.
That's a [very promising project](https://github.com/KDE/rust-qt-binding-generator)! Thanks for the reference.
It's all good! Many people forget that this is the case :)
Could you point this poor JavaScript programmer to some reading material to help me understand your dismay? I put std::u32::MAX in a playground and messed around dividing and multiplying. Nothing obviously wrong jumped out at me. JavaScript encourages glossing over this kind of detail! :P I've seen suggestions of using two integer values to represent the integer component and fractional component - is this necessary for my use case? I just want to be able to return decimal answers accurate to say 3 dp
… jumps well above all the C/C++ programs that don't manually unroll the loops.
Is there any estimation when second edition will get such missing chapters, or it's not planned?
What specifically are you referring to? It depends on the exact thing you're talking about.
Like one above about procedural macros.
I really need to find some time to help working on Ion again.
That'll be in Appendix E; I plan on starting the appendix work soon, but am not sure in what order I'm working on them.
This isn't too specific to RSX, but it's important for Rust GUI in general. I like the data model of React- one-way data flow, a single application data store decoupled from the UI, re-rendering on changes which are triggered by events. However, JSX and virtual DOM are a hack to deal with browser performance, and in the long run they're a dead end. That model has two major problems as a candidate for a Rust GUI library: First, **the virtual DOM model is too low-level.** It compares the structure of the actual document, throwing out any knowledge of which parts are static and which parts may change. JSX bakes this assumption into the API, making it impossible to improve on. For an example of something better, take a look at Glimmer.js- templates are high-level enough that the diffing stage only has to look at the actual state, not the virtual DOM output. This has huge performance and simplicity benefits. Rust has even more reason to go this direction, because it's not tied to the DOM at all, which leads into: Second, **Rust has much better options for modeling a GUI.** It's not stuck with the web stack. Even with Servo/Quantum showing how much room for optimization there is there, the interface is still fixed as a big polymorphic tree that's expensive to modify, hard to control, and filled with evolutionary dead ends for backwards compatibility. A new GUI library has no reason to hold itself to that- it can reimagine layout and styling to be simpler, more special-purpose, and more easily optimized. Applications (as opposed to documents) don't need all the complexity of HTML, or if they do they can constrain it rather than making it the default model for their entire UI. And it can even do this without giving up on WebRender- such a GUI would likely make even better use of WebRender than Servo ever can.
Thanks!
Ticki was probably programming in the womb. 
Is there some intersection between this kind of feature and supporting multiple cargo "repositories". For caching, or private company repositories, or offline work there feels like there might be a common set of features for cargo repository references or lists. Edit: the mentoring instruction writeup by alexcrichton (linked in the op) has more info on this aspect of it, and it seems like it would help cargo along this direction.
Or you are in metro and have no wifi.
No problem :)
&gt; What is the trait for a function that takes either an int or a float in a language without overloading? You just need to bound the function type, or what am I missing? // silly example: fn foo&lt;F&gt;(x: u32: f: F) where for&lt;T: num::Num&gt; F: FnOnce(T) -&gt; () { if x &gt; 0 { f(x as u64) } else { f(x as u32) // arg type differs } }
&gt; I don't want Rust to have a non-Rust turning complete meta language Rust trait system is already turing complete so... that ship has sailed.
I've been dreaming of something like this for the new version of [`cedar`](https://github.com/jtomschroeder/cedar/tree/protoype/layout), as I've transitioned to leveraging a virtual DOM through an embedded browser (via CEF). The difficult part of a 'declarative view' (a la Elm) is finding an elegant syntax.
Comments and criticisms welcome!
Yes, but not above the Fortran program, and from reading the assembly it looks like the Fortran compiler correctly unrolls all the loops.
When you're stuck with C++ ;)
In Java, Maven does this by default. Released dependencies should never change. Why isn't download once the default behavior for cargo? http://maven.40175.n5.nabble.com/force-maven-to-redownload-refresh-quot-released-quot-dependencies-td2259711.html
Well done. One thing you forgot to mention is signing. Commits can be signed by the developer. Cargo could add additional support for verifying commit signatures through GitHub by using the repo link the developer provides.
Yep, Agner Fog states (www.agner.org/optimize/instruction_tables.pdf) that `movaps xmm_reg, m128` and `movaps m128, xmm_reg` take 1 micro-op on all Intel architectures, which means it'll take 1 cycle to issue the memory operation.
Ah yeah totally forgot about that one and I commented it on the other day!
I'd wait for the next nice round number instead, like 10,000.
C++ package manager of the month
Celebrate? 30,000 isn't a power of two, silly. :P (You're still updating that post??)
I'm down! :) 
&gt; (You're still updating that post??) I am addicted to Rust – help me :D
Hey, this is actually mostly my doing from my last year there. Was planning on making it more suitable for public use, but will take a little while. Just for reference this isn't actually tied to C++, we just happen to use it primarily for C++. It knows nothing about language details. It just uses docker to manage/shell out to build environments, and some simple cargo-like logic to handle versioning of pre-built libraries with lockfiles.
Banned! Thank me later (somewhere other than here, where you are banned). :)
If I have a struct with a vector of structs and want a hashmap into that vector (for indexing purposes say), do I really need lifetimes? I have a toy example that is close to what I'm really doing right now (doesn't compile): https://play.rust-lang.org/?gist=4e51514e3cdc7b737b31cea940f1d084&amp;version=stable I tried adding lifetimes but got stuck, and more generally, the private hashmap only contains references to things the struct already owns so it seems weird to me to add lifetimes (feel free to tell me to reread the lifetimes section of the book(s)). In my real work I was going to just have a vector of Rc&lt;T&gt;'s but I thought it was overkill so I'm trying to do it with simple ownership.
Right then, for all you lot out there reading this, we're looking for 20,000 volunteers to be banned. Any takers?
Great library. I don't know much about Kafka but quick/polite feedback and responses to PRs is the #1 most important thing for me when contributing to open-source projects and my patch was merged quickly. I'm already on 0.7 luckily (https://github.com/sevagh/pq/blob/master/Cargo.lock#L235)
I meant specifically the "Rust team at Mozilla" rather than all the Rust teams. Btw, @Manishearth is in the US. He moved a bit ago.
Ah yes, the OSes written in Rust would have been a good fun fact.
Not every 32-bit integer value can be exactly represented with a 32-bit floating point value so you can easily introduce errors. A 64-bit float can exactly represent every 32-bit integer but not every 64-bit integer. [Here](https://stackoverflow.com/questions/3793838/which-is-the-first-integer-that-an-ieee-754-float-is-incapable-of-representing-e) is a brief StackOverflow post on it. That's what (I believe) they were getting at when they said "use `u32::MAX` as input and be disappointed". Converting that value to `f32` won't be what you'd expect. fn main() { println!("{}", std::u32::MAX); println!("{}", std::u32::MAX as f32); } Prints 4294967295 4294967300
This comes up every once in a while -- this here is an older thread, but the info in it should still be relevant: https://www.reddit.com/r/rust/comments/5bx34b/mutex_vs_rwlock/
Power of two numbers are overrated (off by one). Subtract one and you got nice numbers that may even be prime. Like 31 for example!
Ah ha, I did expect that might have been it. Very well explained thank you :) I did get confused when reading into it as it seems floating points can represent bigger numbers than integers (at first counter intuitive) but since you lose accuracy it does make sense. So apart from f32 when f64 would cover all cases. Converting with as seems appropriate?
Clone can have arbitrary complexity. Move is always a memcpy of the thing itself. For Vecs and Strings, move does *far* less copying than Clone; it’s always three words for move, but Clone copies all data and so is proportional to the length of the Vec or string.
It might show that I don't know squat, but... what do you mean by "it's always three words for move"? Moving things from one memory location to another sounds exactly like cloning something plus invalidating the old location. What am I missing?
A `String` is a triple of `(address, capacity, length)` each of which is a `usize`. That's all that needs to actually be moved. The data stored on the heap, to which `address` points, doesn't need to be moved at all.
It's all good! `String` looks like this: struct String { data: Vec&lt;u8&gt;, } So it's the same as a `Vec`. Vec doesn't literally look like this, but is the same as struct Vec&lt;T&gt; { data: *const T, // *const T has the same size as usize len: usize, capacity: usize, } So, moving a Vec or String always copies those three `usizes`. `Clone`, on the other hand, will not only copy these three, but follow the data pointer and also copy all of the data. You might want to read https://doc.rust-lang.org/book/second-edition/ch04-01-what-is-ownership.html, it even has diagrams!
Thanks to /u/frewscxv for putting this together: https://github.com/rust-lang/rfcs/pull/2192
Spooky
Right, thanks!
Ah right, thanks! I did read that chapter (several times over the course of the last 1-2 years I'd say, it was in the old edition, too, wasn't it), but ~~I seem to have forgotten~~ ~~I did not connect the dots~~ ALIENS!
The old edition was not nearly as great as the new one; it's one of the parts we worked hardest on improving :)
`Copy` trait is even less of a problem. It really means just some bytes being copied. All languages do this! I'm not sure what you actually meant. If you meant that Go is more explicit than Rust, then I disagree. Where Rust becomes harder to read (for a newbie) than Go is generics. Not because of implicitness but because of possibly unfamiliar syntax.
Issue 1 isn't fundamental to vdom approaches in general, just to react specifically. I believe infernojs can optimize based on whether the jsx is static or dynamic. It has to use a different babel plugin to do this though. I'm not an expert on inferno internals though so I may be misremembering something I remember reading about their approach. As for issue 2... that may be true, but the web is essentially the only cross-platform stack that actually works well, so I think it's going to be useful to work with for the foreseeable future. But hey, if someone comes up with some awesome innovative approach to UI in rust, I'm all for it. (this project is pretty cool though).
But in this case they were moved, so once passed into the function, the calling function no longer owns them, therefore it's fine for it to pass a reference to its own copy of them on the stack, as it isn't allowed to use that copy again after the call.
`v.iter()` should be `v.into_iter()` The reason is that iter uses `&amp;self` so it can't move data out, so each thing passed into `map` is a reference to the content. `into_iter` consumes the vector, which means you can map over the actual owned values of the vec.
I know that, I meant the copies can be avoided if you always keep them in the same representation, passing tuples and unwrapping them callee-side incurs unnecessary copies in this case, whereas if they'd been passed as a struct it would be less likely someone would decide to unwrap them (member names would make the code clear) so the copies would be avoided.
Awesome, thanks a lot! Also, reading it up it totally makes sense!
If it helps, the gist of ownership is "Who's responsible for cleaning this up?" When the owning variable goes away, the memory gets freed. "Ownership" boils down to "the compiler requires that each piece of memory be tied to one variable so have one variable Things like `Rc&lt;T&gt;` are for situations where, when the compiler asks "When does this get cleaned up?" you have to respond with "It depends." (With `Rc&lt;T&gt;` and `Arc&lt;T&gt;` being "It depends on which of the multiple owners goes away last." `Rc&lt;T&gt;` and `Arc&lt;T&gt;` work around the limitations of compile-time checking by letting you mark specific variables for handling at runtime.) **EDIT:** Oh, and it definitely won't be wasting 30% of your time on manual memory management. I find Rust makes a good replacement for Python in many situations, since the time lost satisfying `rustc` is more than made up for by the time gained not needing to test the program as heavily to reach the same level of quality. Assuming Rust and Python both have equally good libraries available for something, the only place I wouldn't recommend Rust is cases where you're not quite sure what algorithm you're building yet and need to experiment with a prototype against some test data to figure it out. In that case, a buggy implementation doesn't matter as much as rapidly trying out various designs that are "just good enough" to answer the questions they're meant to explore. Once you *do* know what algorithm you need, then you can move the design over to Rust as an efficient way to squeeze out a lot of bugs. (Something I'm saying from first-hand experience, since I'm using a mix of Rust and Python to develop a game launcher which can heuristically guess titles from filenames with high accuracy if you point it at a folder full of games that didn't have installers.)
Very cool. Why Rust, and not say, Python?
My rule of thumb for these things is: if you'd have to `clone` the argument, take it by value and let the caller make the decision. Worst case: the caller needs the data too - in that case you'd `clone` anyway. If the caller doesn't need to keep it, you've saved one allocation and copying large amount of bytes.
&gt; plus invalidating the old location That would be in C++. In Rust, the old location is invalidated compile-time and doesn't make extra instructions (except for rare edge-cases when a bit needs to be flipped).
Issue 1 is fundamental to JSX, not vDOM. You can sort of get past it but it requires a pretty big departure from the code as-written and involves a lot more code analysis. Dealing directly with templates swaps the default case from dynamic to static, and lets you make more guarantees. The web is useful, but it already exists, and is already (in the process of being) implemented in Rust, even. If you're going to do a new framework specifically for the desktop, there are certainly several paradigms around specifying layouts that work just fine across platforms.
Ah I should have searched before posting. Thanks. :D
put me behind the rusty iron bars, your honor.
That was exactly what I was getting at. The "average" of a list of a single `u32` value will not be equal to that value when you store it in a `f32`. Storing the average in a `f64` value avoids this, for the most part (although even there it's possible to get bad results if the sum gets high enough).
Your life will be much easier if you map the strings to indices in the vector, instead of references to `Record`. No need for lifetimes or `Rc&lt;Record&gt;` that way.
If you have any questions about the rust-qt-binding-generator feel free to ask me, I've contributed to it so I'm pretty familiar with all of the internals. Also, /u/vandenoever is really active on this sub so I'm sure he'd be able to help you out, he was extremely helpful for me.
IT IS DONE
Very cool, though I think somewhere on the front page it ought to stress that accepted RFCs are often historical artifacts, which may not 100% reflect the implementation that ultimately arose, may contain references to removed nightly features, and may reference sibling RFCs that never ended up being accepted. And some RFCs predate 1.0, so may be entirely obsolete! Best to think of them as snapshots of development history at various points, rather than documentation that is kept up-to-date.
Just ban -70 000 people instead, so we get 100K subs
Rust is indeed very addicting. I check this subreddit like 15 times a day. Probably most of my time on reddit is spent here.
Yeah OP, get back to us when it's 32767
We considered python, but getting an easy to set up and consistent installation for all devs on their various distros right when python was transitioning to python3 (in various ways across distros) continues to be a cause pain to this day for our other dev tools. With rust we can build lal with the [muslrust](https://github.com/clux/muslrust) image on CI, and tell people to `curl https://artficatory.host/lal/latest.tar | tar xz -C /usr/local` once, then the distribution problem is pretty much gone - no rust installation necessary. We also wrote simple auto-upgrade logic around this, knowing we only ever dealt with a single binary. lal was a bit of an experiment though, honestly. Rust is a language that appealed to our team a lot as we have a lot of C++ for performance reasons, so this tool was a good place to experiment and get some more experience with the language. I was personally sold on it after getting clap ([with this chain](https://github.com/cisco/lal-build-manager/blob/52ffd8fe99670eacf4e420fb0d0cc303b04e93ab/src/main.rs#L240-L522)) and the codegen libraries set up for arg parsing and serialisation. For a low level language, it really feels like a super ergonomic high level one sometimes.
I know right.
In Python I can read/write a memory mapped file as a uniform array of 64 bit ints using [numpy.memmap](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.memmap.html). Is there a way to do this in Rust. Perhaps a combination of [memmap](https://docs.rs/memmap/0.6.0/memmap/struct.MmapOptions.html) and something else? Basically I am looking for the convenience of not needing to convert each entry to and from bytes myself. If no such thing exists, I will end up using [byteorder](https://docs.rs/byteorder/1.1.0/byteorder/).
I've been waiting for nightly to go back to that icon. The whole reason I downloaded it was for this.
&lt;3
Thanks for your reply! Since the application will be a wrapper around pf/bpf libraries, is there a benefit of doing it with Rust? In other words, does it require any performance/processor lift up?
[Something like this is what i would do](https://play.rust-lang.org/?gist=cd4ff62d4a374bd5bcd3fcc7810ebb87&amp;version=stable) 
Worth noting that Chucklefish didn't have that much to do with Stardew Valley's development (ConcernedApe did that by his lonesome over the course of several years), although they did a tonne in getting it published. 
Great post!
This is seriously awesome. I always have a hard time finding rendered RFC text after the RFC is accepted because the original links in the PRs tend to break. This is A+++++ Thank you!
Seems like someone else is celebrating, pretty hard if I'm not mistaken.
Thanks, I did not know about maplit. That's really gonna clean up some of my code.
I'm still waiting for [`defmac`](https://docs.rs/defmac/) to take off. :-) It's just a shorter syntax for defining function-like macros. (examples [1](https://github.com/bluss/petgraph/blob/43a6e55e1c41a6042641dfba25114b367eae7c8f/tests/stable_graph.rs#L303-L309), [2](https://github.com/bluss/petgraph/blob/8fdf8d90a0d993510366d796317ee5e7170f2be0/serialization-tests/tests/serialization.rs#L355-L357)) - Replaces generic functions, without writing out long type signatures — the linked code is an example - Sometimes it's just about how much easier "syntactical variable capture" is to work with than "borrow checked variable capture"
nice list, just maplit stands out a bit, as you can always replace it with [(k,v),..].iter().collect() so i usually just do that instead of adding the dependency.
[`rust-phf`](https://github.com/sfackler/rust-phf) is what I've used for compile-time hashmaps in the past. Nightly only at the moment, but can be useful. 
Oh jesus that feels obvious after you say it
maplit would be more efficient in this case though, right? It feels weird to allocate and populate an array just to iterate through it and populate a map.
C++ only requires the two overloads. You could mostly do it with a custom trait you add to both types though.
The API style looks pretty good in cedar right now.
Ahhhh ok I got it. So I definitely want to be using something like wlroots-rs or smithay. You are a great resource and I really need to "bookmark" you! :-) I will need a few months to be comfortable with Rust and it will surely also take a significant amount of time to get familiar with how Wayland works. If you don't mind, I have a few more questions for you: 1. Where did you get your Wayland knowledge from? If my research is correct, there's not a single book out there. And the online-text by Jan Newmarch is very limited and (for me) not very helpful. The official Wayland docs seem to be alright though. However I am not sure how much background knowledge they cover. 2. Who do you think will be using your library? Is it only people that want to write their own Wayland window managers? 3. And on a personal note: What is your motivation to write a library like smithay? Do you want to build something on top? is it just pure interest? Work? 
I think the issue is read_line https://doc.rust-lang.org/std/io/trait.BufRead.html#method.read_line says &gt; This function will read bytes from the underlying stream until the newline delimiter (the 0xA byte) or EOF is found. Once found, all bytes up to, and including, the delimiter (if found) will be appended to buf
I like your gumption! You have been made a moderator of /r/rust.
I personally might use the `From` impl, e.g. `let f: f64 = 1u32.into()`, but that impl uses `as` behind the scenes, so yes it's fine.
and even that bit is on the stack, not in the thing that's been moved
You should script something up to update the post daily. Write it in like python, but then #REWRITE IT IN RUST
I'm from Montreal too, and have always called it liked that :D
As I said, that optimization is currently not implemented, and you can't perform it just because the value is moved. Consider this (nonsensical, but valid) code: let mut s = String::new(); foo(s, { s = String::new(); s.len()}); bar(s) Here, `s` is moved as an argument to `foo`, but then _before_ `foo` is actually executed `s` is reinitialized, reusing the same stack slot. If `foo` was to modify its argument, and the caller did not make a copy, it would actually modify `s` in the caller's scope, which is pretty bad.
I fail to see the appeal of [either](https://crates.io/crates/either). Shouldn't you use your own enum and have meaningful names for the branches? Do the provided combinators worth the confusion of dealing with what's the meaning of `Left` and what's the meaning of `Right`?
7 * 31 * 151 = 32767. No Mersenne prime for you!
&gt;Prerequisites: docker wat
rust-qt-binding-generator looks rather impressive. Are you active on the IRC? Or do you have a separate forum? I will be asking a lot of questions soon.
Thanks for the info. I was able to correct this by "trimming" the string before I parsed it and unwrapped it.
I’m a huge fan of the blue/green/purple Viridis Fox. 
I think you've proposed a great set of ideas. I think maybe some work could be added into how to avoid confusion or splits in the offerings between crates.io and the new site. As a new user the barrier to entry is already filled with all sorts of new things to grok so whatever is done should be very careful to make the relationship between new site and crates.io apparent in every way possible. I would actually almost prefer it as a layer over crates like an extension. There's a whole bunch of problems with that idea too, but the sentiment is there.
Yeah I was exploring the rationale of calling them Left and Right
&gt; First, the decision to not include namespaces in Rust’s crates.io repository was 1000% correct. I’m learning Elm now, which has a package repo namespaced by username, and I have to sort through eight different packages named “elm-css” to find the one that doesn’t suck. The Rust situation is typically the same. I didn't check css specifically but you could have stuff like `css`, `rusty-css`, `css-rs`, `css-oxide`, etc. The advantage of having so many packages named "elm-css" is that it's easier to find all of them.
The actual code is [(k, v),...].iter().**cloned()**.collect() so it might be less efficient if this cloning does something. I'm not sure it'll be more efficient otherwise, LLVM is pretty good. A quick benchmark shows no difference between the two for integers keys&amp;values.
[removed]
When you have a large scale build system you care more about determinism than having fewer dependencies (and implicitly depend on the state of the system)
Ah apologies I missed that part of your post, that makes sense, thank you
One thing that might help is printing strings in "debug mode", like this: println!("{:?}", f_D); Which prints: "12.0\n"
If `iter()` was what I had to implement, I guess it would simple to just return `records.iter()` but I'm confused about how to implement `next()` using the vector's `iter()`?
3 steps to support iteration: 1. Declare a `struct BucketIterator&lt;K, V&gt;` which just contains a [`std::slice::iter`](https://doc.rust-lang.org/std/slice/struct.Iter.html). 2. Implement the `Iterator` trait. `Item` is `&amp;(K,V)`, `next` is `inner.next()`. 3. Add a method `iter()` to `Bucket` that returns `BucketIterator(records.iter())`.
Regarding the matches crate, the motivating example given is the inability to do something like: if let Foo(_) = x &amp;&amp; y.is_good() { If you want to avoid adding the dependency, you can use the power of tuples: if let (Foo(_), true) = (x, y.is_good()) { Edit: It’s been pointed out to me that my example doesn’t evaluate y.is_good() lazily. I wonder if it would be possible to do tuple matching lazily, but I guess it would be too late to change that in the language.
A brief look at the docs tells me that if you did this with Strings, you'd end up cloning each one individually. I haven't done a proper benchmark (micro benchmarks can be quite difficult), but looking at it with [godbolt](https://godbolt.org/g/M3MX5W) shows quite a bit more code emitted in the case of using an array temporary (compiled with optimizations). Which makes sense, when you consider what the code is doing in terms of semantics. I'd also venture that `&amp;'static str` is a common key type for literal hashmaps. In general, any type that doesn't implement Copy is going to have more code to execute there - at least for Copy types (like i32) it's not *so* bad.
There are three approaches you can use. Take a moment to consider which is most appropriate. 1. Implement your own iterator type which wraps existing iterator types (`std::slice::Iter`, and `std::vec::IntoIter` if you want a consuming iterator). **Advantages:** most flexible, ensures API stability if you need to change internal details. **Disadvantages:** a lot more effort, if you want to do it properly (which involves implementing about ten traits on your iterator wrapper type); and if slices or their iterators add something new, you don’t get it unless you implement a wrapper yourself. 2. Have your `iter()` functions and `IntoIterator` implementations use the standard iterator types directly. **Advantages:** easier, gets you all the other trait implementations on `std::slice::Iter` for free—AsRef, Clone, FusedIterator, ExactSizeIterator, Debug, Send, DoubleEndedIterator, TrustedLen, Sync). **Disadvantages:** if you need to restructure things so that this is no longer an option (e.g. store things in a different type of vector and thus need to map it before presenting it to the user) it’s a breaking change. 3. Implement `Deref&lt;Target = [(K, V)]&gt;` and just treat your `Bucket&lt;K, V&gt;` as a `&amp;[(K, V)]`. (Read-only; implement `DerefMut` if you want to allow mutations of values.) **Advantages:** easy, and lets you simply treat the whole thing as a slice (this is what `Vec&lt;T&gt;` does). **Disadvantages:** there really aren’t any, if it matches your purpose. (If not, it’s useless.) (I haven’t really answered your *question*, but if you decide #1 or #2 is what you need, I’m happy to explain further.)
What do I have to do to get the ban hammer as well? Edit: Here is a Rust meme for you: https://funnypictures3.fjcdn.com/pictures/Rust_fbdc54_6116664.jpg
It's a haskell thing.
Yeah definitely a Haskell thing. Traditionally used as the equivalent of Result but it's also used as a way to say it can be one of two types of values
BANZINGA
no problem at all, I hope it helped!
Platforms? Linux only, I guess? Or any system with docker?
So the govt/army is using Rust? Shouldn't they be on the Rust friends page? 
Niccee, this seems great for those cases where a concise, local `macro_rules` "function" can cut a function body in half. That said, not sure if I come across these cases enough to import a crate for it :) Definitely would use it if was "just there" (like in std or something).
I'd recommend this: https://github.com/TeXitoi/rust-mdo And this: https://github.com/Keats/validator
If you can guarantee that the integers in the file are the same endianness as the current CPU (if it's always read and written locally, or you don't plan on using the program on other architectures), then `byteorder` is superfluous. You could simply cast the byte slice pointer to a `u64` pointer with a fixed-up length: // leaving unsafe because the endianness is undefined // also you could have alignment issues if you passed in an arbitrary byte slice, but a memory map is always // page-aligned; alternately you could assert the alignment of the slice is &gt;= 8 unsafe fn bytes_to_u64(bytes: &amp;[u8]) -&gt; &amp;[u64] { ::std::slice::from_raw_parts(bytes.as_ptr() as *const u64, bytes.len() / 8) } // same unsafe fn bytes_to_mut_u64(bytes: &amp;mut [u8]) -&gt; &amp;mut [u64] { ::std::slice::from_raw_parts_mut(bytes.as_mut_ptr() as *const u64, bytes.len() / 8) } I don't know if numpy fixes up the endianness behind the scenes or not, as it's not stated in the documentation you linked. I would assume not, or else that would be a feature to advertise, wouldn't it? Of course, appending to the file is a little more complicated as you have to lengthen it in a separate step IIRC. Edit: fix code snippet and add note about alignment 
Does it work on Windows too?
Doesn't PE specifically ask you not to share the solutions publicly?
Eh, there's plenty of 'em out anyway, far beyond the first 50. No harm in having some fun playing with optimization.
A vote system would probably be sufficient. If more than half the people who download a crate then go on to say "this crate is useless and shows no sign of getting better" then that's probably a low enough bar to say it really should go.
What's the best crate for glyph rendering currently? It would be great if there is also a cache system.
I'm just a lone developer with an interest in functional programming, I would not say that my actions constitute an endorsement on behalf of the DoD. 
Static (or even better yet: eager) Drop semantics when? ;)
I'd like to try your crate. We are using Wire at work for communication, meetings etc. I was thinking of writing a Bot for Wire, but I'm not really sure what it would do or what value it could add. Do you have a suggestion? What capabilities can Wire Bots have?
Wow, this is exactly what I want, thanks soooooooooo much for your help :)
It’s just a generic sum, with various instances written for it. E.g. you can map inside the right value. Usually, Left is for errors and Right is for the actual value.
The two large ones that I know of are initial commitment and compile times. Learning Rust's lifetime system is rewarding, but not like any other popular languages today. Understanding ownership fully may help you write programs in other languages, but it isn't required for any other mainstream programming. If you're going to learn rust, there will be a period of time where you won't be able to make meaningful programs, and it will seem like the compiler is your worst enemy. You won't be able to write the majority of rust programs until you understand the lifetime system. And while the compiler's errors are very helpful, they're also very confusing if you don't know any of the terms. This one is purely in the learning stage though - once you're an established rust developer, the compiler will be much more a companion. It has helpful error messages if you understand the underlying systems, and you can rely on it ensuring thread safety and all of Rust's other benefits. ---- The second drawback is more during the established stage: rustc's compile times. There are many different factors which contribute to this, but the main product is that projects take a while to compile. While I can use a prototype-see results-prototype cycle in Python or Java, this isn't viable in Rust. If this is your main way of working, it can be incredibly disruptive. That being said, there are other ways of working, and my main cycle in rust is idea-implementation-syntax-check-... I like this strategy for some things, and it's utterly horrible for others. For example: if you're experimenting with different physics constants in a game, it's not feasible to include them as code constants. [tunapanel](https://crates.io/crates/tunapanel) is incredibly useful for that specific use case, but in general you don't get to edit code directly and see the results.
Here are the first two that comes to mind: - Soundness bugs in the compiler make it possible to write safe code with "nasal demon" undefined behavior. Although this is no worse than the default state of affairs in other languages, Rust's big safety promise shouldn't be taken too far. (For example, you really shouldn't build a security sandbox out of the safety guarantees. In my estimation that will *always* be a bad idea.) - Rust has no definite ABI, thus dynamic linking and FFI should only go through C or *maaaaybe* C++ interfaces. Anything else might work or might not.
&gt; Move is always a memcpy of the thing itself Wait, really? Is a move equivalent to a copy (in terms of actual duplication of bits) for a type that implements Copy? I don't know why I had it in my head that moves were copy-less. Though now that I think about it, everything has to be copied into a register to be used at all... I think I was all screwed up on this.
Rust undefined behavior? I never heard of that. Doing a quick search gets me it's still in beta comments
The first thing I did was learn lifetimes in rust. I give it an A+. Although I think 'Lifetime Subtyping' is crazy and hope I will never need it. How slow are the compile times? Like what does it take to make the computer take 10 seconds or a minute or whatever? In one of my C++ projects it took 25+ seconds to compile and it was barely doing anything. I wrote a new version from scratch with significantly less templates and it takes 1.5seconds despite doing much more. Also do people know why RUST build time is slow? Walter Bright (author of D) said when he implement D to be more safe and have scope rules it didn't make build times longer. But scope != lifetime
&gt; (mostly because you can't get the gun to compile) :) mostly because guns are unsafe!
How big is the stack of each coroutine by default?
You can also map within an ‘Option’, which I think is probably easier than using a macro in many cases. 
I mean the realistic time depends on your system - but it's usually fairly long for dependencies, and not _too_ long for the actual project. On my main laptop with a 2.8Ghz i7, a 2.8k-line game UI project takes ~15 minutes to compile Rust/C++ dependencies, and ~65 seconds to compile the project itself. With incremental compilation enabled, that's cut down to about 35 seconds build time. Not insanely long, but long enough to annoy anyone used to using a code-run-code cycle in JavaScript, Python, Scala, etc.
There are a few edge cases of unsoundness which exist and are usually very hard to run into - see https://github.com/rust-lang/rust/issues?q=is%3Aopen+is%3Aissue+label%3AI-unsound. They're all bugs in the compiler with varying levels of being ingrained in how things work - and sometimes the fixes are delayed on architectural changes if the portion of code would need to be overhauled anyways to fix it. Undefined behavior is a very specific portion of those. The main one which comes to mind is no-side-effect infinite loops: these are considered undefined behavior by LLVM, and can be... quite bad. See https://github.com/rust-lang/rust/issues/28728.
You can `map_err` a `Result`, too. Other stuff is missing though like `unwrap_err_or` or just a simple `flip`
These are not equivalent. The first evaluates `y.is_good()` lazily.
&gt; First, the decision to not include namespaces in Rust’s crates.io repository was 1000% correct. As someone who wants namespaces badly, I can only disagree with this. The anecdote by the author "back when I was using elm, I had to decide which elm-css package to use" is not really valid, as in crates.io you need to do such a decision process as well. crates.io without namespaces is right now a race for the name. And whoever is first to think "hey I need to write a crate to solve this issue" can obtain that name. Does that mean that the crate with the "better name" is also better overall? No! In fact compare [reqwest](https://crates.io/crates/reqwest) to [request](https://crates.io/crates/request). Thanks to crates.io having no namespacing you are forced to remember how to write `reqwest`. If you search on crates.io for "request", the `request` will always show up first because its an **exact match**. Of course displaying exact matches first is important, as sometimes you know the crate name but you are too lazy to figure out the url for it. But it also has the disadvantage of giving whomever won that race an unfair advantage.
- either: semantically meaningless, this is not normally a good idea; use a custom enum instead. (I wish we had anonymous sum types.) - lazy_static: definitely indispensable. - maplit: useful, but only if you *actually* need a `HashMap` or whatever; if you merely need to look things up (which I find to be the case most of the time), use phf instead. (However: phf_macros is still nightly only, so if you want to use phf on stable, you’re stuck with using build.rs with phf_codegen.) - try_opt: making `?` support `Try` actually landed [a month ago](https://github.com/rust-lang/rust/pull/42526); it’s currently in beta and so it should arrive on stable in a few weeks with Rust 1.22.0. - exitcode: huh, didn’t know about sysexits.h. Nifty. I’m looking forward to fn main being able to return an exit code. - enum-set: I think bitflags is also worth mentioning here; EnumSet’s advantage is when you may wish to refer to a single member of the enum, or multiple, distinctly at different times (thus, `Weekday` in some places and `EnumSet&lt;Weekday&gt;` in others). bitflags is more suitable for the low-level bit flags (!) case. - antidote: that one sounds *really* useful. Most of the time when I’ve written multithreaded code, PoisonError was simply annoying (the shared data was immutable or all integers or some such thing, so there was no inconsistent state). - matches: I’m still mildly inclined to want the same guard syntax as is used in match branches—`if let PATTERN if GUARD`. But I tend to avoid tiny dependencies that are easily and harmlessly worked around, so I’ve never actually used matches in a real project.
That's platform dependent, to cite the library I use: https://docs.rs/context/2.0.0/context/stack/struct.Stack.html#method.default_size. Indeed, when I played with manual sizes, there were systems that were Ok with 4096 bytes stack, while some others gave stack overflow there. This is probably decided by `boost::context` internally. Anyway, you're probably OK specifying the size manually with some extra. At least on Linux, it'll take the addresses, but the memory will get mapped there only when first used ‒ so it'll actually use only the first pages as they are needed.
Rust had `either` until 2014: https://github.com/rust-lang/rust/issues/9157
It really depends on how a bot can help you in chat w.r.t your job. For example, in my previous workplace, we had a Skype bot in our group which shows debug messages and sends you analysis logs from specific AWS machines when the devs ask for it (which was quicker and easier than ssh'ing into the machines or checking the admin panel, especially during a discussion). In the #servo IRC channel, we have firebot and crowbot which fetch bugs from bugzilla/github (which also play with you). I can't think of anything that makes Wire bots special. If you're using Wire and you need a bot, then you can now write one! Well, Wire allows users to upload assets (files, images, videos, link preview, and all that), so bots can do that too. Maybe start with a funny bot that posts hilarious messages when you ping it?
Okay, here's my list of problems with Rust: # SemVer is a lie Creates can add more exports without updating their major version. Rust's resolution of identifiers actually has a hierarchy which means that something can "slip in between" for instance to compute the length of a Vector in `vec![1,2,3].len()` at the moment `len` here resolves to `std::Vec::len`, however if `std::Vec::len` did not exist then `std::slice::len` would've been used instead because of `Vec&lt;T&gt; : Deref&lt;Target=[T]&gt;` so if `std::Vec::len` was defined _later_ it would change the function called; in this case this is fine because they do the same and in both cases the compiler is no doubt smart enough to optimize them both to simply be inlined to a single memory address but in some cases it can either be an error because the new method has a different signature or worse a silent change in behaviour _without_ an error. You can indeed avoid this by using universal function syntax everywhere but no one use `&lt;Foo as DoubleEndedIterator&gt;::next_back(foo)` when `foo.next_back()` works but yes it is possible that in the future the latter code will suddenly switch to a different function call with entirely different semantics. # Specialization isn't there yet This is actually related to the above but that specialization still isn't there yet and that you can't define more specialized type signatures to have a different implementation that is more optimal is kind of a mess. The proposeals for specialization are also very unsatisfactory # Error propagation is inconvenient Rust does not have normal exceptions and instead communicates this information through normal value returns; while there are _some_ convenient ways to propagate this directly with special syntax and macros it still isn't fully ergonomic, like jumping an error out of a closure for instance is so inconvenient you often just end up not using concise iterator methods because jumping the error out is too hard. (Rust really needs labeled returns like it has labeled loops) # Loops don't return a value Rust claims to be an expression-oriented language but most looping constructs are not capable of returning a value (if there were labeled returns all of this would be solved again). # The inability to define traits on types whose crate you don't own is infuriating Yes, there are reasons for this but if the crate owner did not define the trait then you're stuck with that and how can you expect the crate owner to define a trait which another crate defined which you need? You should at the very least be able to define the trait for the span of your own crate while not exporting it. # No paramatrization over type size Well it's there. There is this _one_ function in the stdlib that has this hidden magic ability that its type paramatrizes over the size of its types and requires that its input and output type have the same size. It says so in the documentation; there is no way this can be indicated in code but it does it and you can't do it yourself and constrain on your function that two arguments must have the same size or a certain size which would definitely be useful in some bit level hackery. # A community full of friendly people I distrust friendly people; they're only friendly because they want you to like them and their opinion isn't worth much as they'll never say it how it is—they're invariably waaay too sensitive to social pressure.
Another thing people haven't mentioned yet is that the crate ecosystem is still maturing. There's lots of good crates out there, but there's no definite answers for many things. In, say, Python, if you're building a web app, you're basically choosing between Flask and Django (and Pyramid if you want to feel hipster), and both can do the job just fine. Rust has like ten different web frameworks, each with its abstractions and its own not-immediately-obvious quirks, and if you ask people which one you should use, you'll get ten different answers. Now, don't get me wrong, this is _OK_. This is how ecosystems grow. In a few years, people will figure out which approach is best and settle on two, maybe three major solutions, but right now the situation is a bit messy.
I'm active on irc as vandenoever. You can raise issues on the kde bug tracker [1]. Or you can ping me on irc or mail (see commit log). https://bugs.kde.org/enter_bug.cgi?product=rust-qt-binding-generator
The generated binding code is platform independent. This is ensured by using Qt types and Rust types.
Neat. Still, it's clearly a Haskell thing :3
I have a suggestion for json_typegen: "ARTSCommonHeader" will currently be converted to type "ArtscommonHeader" and field "artscommon_header" but it should be type "ArtsCommonHeader" and field "arts_common_header". Similarly for other types containing acronyms. (So the last letter of a chain of uppercase letters should be assumed to be not part of the acronym, because it usually isn't). And it would be useful to have an option to turn all `String`s into `&amp;'a str`. Also, do you have any plans to extend this site to also work for xml? I'd really appreciate that! :)
Heh
The two crate's I've written which are both small and quite useful to me are [derive_more](https://github.com/JelteF/derive_more) (especially the From/Into derives) and [defaultmap](https://github.com/JelteF/defaultmap) (coming from Python I really missed this datastructure in the stdlib) 
Every language that is unique in some way has a vocal part of the community that are practically fanatical about it and will insist it is the only true way. For Rust this is type safety and avoiding unsafe code. (for Erlang it is OTP for everything and putting all conditions in function headers). Rust type system can result in convoluted solutions and terrible usability. Libraries that look nice from the outside, but are impossible to debug when something goes wrong or they force you into inefficient solutions. I particularly dislike rusqlite and Tokio. I'm sure there are lots of others but I've gotten better in avoiding libraries like those. Often I simply use the low level -sys crates. Rust is great as a C/C++ replacement. It is not so great at being a Python/Erlang/Ruby replacement. Frameworks/libraries that try to mimic stuff in higher level languages just turn into convoluted messes of closures and macros. 
[removed]
[I have an idea how to fix this!](https://imgs.xkcd.com/comics/standards.png)
[removed]
[removed]
[removed]
I like your list. I'm still new with rust. Is there a simple 4 lined example you can write for error propagation? I don't think I understand how you'd return a error at all from a closure unless you call it directly? But usually closures are functions given to other functions so IDK how they'd know what type of errors you'd return. &gt; The inability to define traits on types whose crate you don't own is infuriating I 1000% agree
This seems like a major issue... :/
Well you can't return errors easily from a closure, that's the problem. Say you have an iterator that returns directory entries. It has to return it in the form of `Result&lt;DirEntry, io::Error&gt;` because I/O could fail due to a variety of reasons at any point so let's say you want to filter inside of a function for filenames that sart with a dot: fn surrounding_function () -&gt; io::Result&lt;SomeType&gt; { let right_entry = dir_entries.find(|dir_entry| ...); ... } What do you do eh, how do you propagate the Err from inside dir_entry to the return of surrounding_function? You can't just use `try!` or `?`. So in practice you tend to just forgo the nice abstractions then and use an old-fashioned `for ... in ...` loop where you can so you basically re-implement `find` yourself. Amongst many other reasons I feel that labeled returns would be a good addition to Rust. Being able to assign a label to _any_ scope and being able to return from that scope immediately. Naturally in a closure that would mean the closure gets the lifetime of that scope but just like with labeled breaks in loops it'd be great if you could skip a return if you will and return to the outer function instead of the inner one.
Omg, big shoutout to dotdash, this PR was amazing https://github.com/rust-lang/rust/pull/45380
Yeah, we only had Linux devs at the time. Mac support is getting there now though, but Windows probably needs work.
There's a rust kernel module example out there, I've been a little surprised kernel modules in Rust aren't more popular.
Ah, tricky questions that you have here! I'll try my best to answer them: 1. Wayland is indeed scarcely documented. Though I believe it has improved recently, even the API docs of the wayland libs were very lacking when I started [wayland-rs](https://github.com/smithay/wayland-rs) two years ago. For me, this has been a long work of digging into the source code of various wayland programs (weston and its example clients, wlc, a little of mesa, sdl, ...) to clearly understand how the libs are to be used and how the protocol works. I've started documenting my findings in [the smithay book](https://smithay.github.io/book/), though still quite WIP at the moment. For now it contains some general information about the wayland protocol, and I plan to add some kind of a detailed tutorial about how to write client and server apps using respectively my wayland-client and wayland-server crates (both part of wayland-rs). 2. Depends on which library you are talking about. wayland-rs is really a general purpose group of libs. For example, currently wayland-client is used to make the wayland backend of [winit](https://github.com/tomaka/winit). I'll not lie: I'm the one who made this backend, for me it was a great way to test wayland-client in the wild, and make sure the lib is actually usable. Mapping wayland API into safe and ergonomic rust was no easy task, but I think I reached a not-too-bad state now, with the 4th design. Smithay on the other hand is indeed pretty specific to writing wayland compositors (or window manager, they are the same in wayland). Though I whish it to remain quite general-purpose in this domain: I found experiments like [motorcar](https://github.com/evil0sheep/motorcar) to be something very interesting, and I want such things to be possible to build on to of it. 3. Ah, the motivation. Big question. :) To be honest, my work on wayland-rs and smithay is driven by a few things, but not money, I'm not remunerated for it. Mostly, 2 years ago my motivation for wayland-rs was that I quite liked rust, I found wayland to be interesting, I saw that mostly nothing existed at the time for wayland+rust, and thus decided that doing these bindings would be an interesting task. And it was indeed! I learned a lot about both wayland and rust by working on wayland-rs. Over the last year, as wayland-rs started to kinda become mature, I started smithay too. Again, mostly driven by curiosity and the fact that I was not particularly thrilled by wlc. At the time I had no real plans to do something with it, though I already hoped some people would want to use it (as nothing drives a lib as well as having user's feedback). Now that it's becoming more than just a draft (we can run apps in smithay's examples after all!), I'm starting to consider some possible ways to experiment a wayland compositor of my own, though it's still quite blurry in my head for now. Hope I answered your questions!
Any idea how to fix this? subtly_homoerotic mention specialization not being good enough and other problems. In your opinion what could make libraries not be convoluted? 
They can't because the language does not have the tools. It is operating under stricter conditions then higher level languages as it does not have a GC. 
I mean like... what's missing? Right now I don't even understand how serde works and how rust converts anything to json. I wonder if there is an obvious solution would help a lot if implemented.
Async/await is a big one. Other than that I do not know. I'm not one for language design.
If SML didn't have it, it was certainly inspired by Haskell.
[removed]
man, of the year at least.
I found the section the quote that you're probably talking about at https://projecteuler.net/about: "I learned so much solving problem XXX so is it okay to publish my solution elsewhere? It appears that you have answered your own question. There is nothing quite like that "Aha!" moment when you finally beat a problem which you have been working on for some time. It is often through the best of intentions in wishing to share our insights so that others can enjoy that moment too. Sadly, however, that will not be the case for your readers. Real learning is an active process and seeing how it is done is a long way from experiencing that epiphany of discovery. Please do not deny others what you have so richly valued yourself." It has now been 10 or 11 years since the original 50 questions were published, and answers and solutions to all of them are now easily searchable, including an answer set linked by some of the Rust documentation, https://github.com/gifnksm/ProjectEulerRust . For that reason, I decided that I wouldn't be denying anyone the epiphany of discovery, but would instead be offering them a way to gain new insights after solving the problems. At least for me, a repo like this would have provided a lot of value, since I've learned to solve problems on my own before looking for other solutions. However, I suppose I could follow a model similar to the forum and require that you post the answer to a given solution to get my corresponding solution. Any thoughts?
Whut? Most of your list is valid critisism. But i don't get : * The problem with Error return values. What pattern can't be solved by '?' and map_error? Are you proposing introducing exceptions? * What would a for-loop do that can't be done with iterator functions? These function names also hint me about what you are doing ( and they have early return ). [like this](https://stackoverflow.com/questions/26368288/how-do-i-stop-iteration-and-return-an-error-when-iteratormap-returns-a-result) 
I couldn't agree less with you. There are plenty of things with two branches that aren't Success and Error, and generalizing that into the stdlib with a single implementation of combinators is so trivially the right way to go that I'm puzzled why Rust chose to be super specific when it comes to Result. If naming is the problem for you, then providing the possibility to alias enum branch names would be one solution.
I haven't done much rust yet, so apologies if this is wrong or the answer is obvious. But in your example, how would you want to propagate the error? The only things I can imagine are "I want the caller to return an error if I find an error before I find the entry I'm looking for", or "... if any/all the directory entries are errors". The first case should be easy to express in the `find` closure I think, and then you can pattern match on `right_entry`. The second case would be easy to express with `any`/`all` as well. What am I not seeing?
Rust _does_ give you a certain kind of freedom to find your solution, but usually, every approach there to a different end. But in general, there's almost no concepts that can't be expressed in two ways. I very much like coding Rust day to day, because once you got the concepts down, it's an incredibly rich language that can go from "I just want it quick and dirty" to "and now I refactor it to something proper" easily.
I don't agree that it is trivially the right thing. I've been writing Rust for a long time and have felt the urge to use a generic either type maybe once. I've written plenty of enums with two variants though!
You can easily check for the error inside of the find closure but how do you propagate it up to the function body. Remember that find must return a `bool`, not an `io::Result&lt;bool&gt;` A way to do it is for instance putting a `let mut found_error = None` above it and assigning `Some(error_found)` to it and checking if that contains it but that's not nearly as convenient as being able to just use `?` like you normally can.
OK now that I'm awake: Can I make the key in the HashMap a reference to the record.id instead of allocating a new string?
The whole point of this is being able to call `foo(0, | x | some_op(x))` like in C++14.
[Chapter 20 of the Rust Book](https://doc.rust-lang.org/book/second-edition/ch20-00-final-project-a-web-server.html) leads you through implementing a single threaded server to a multi-threaded server with thread-pools
If I understand correctly, `find` returns an `Option&lt;T&gt;`, and the closure passed to it should return `bool`. So I think you could have the closure return true if the element is an error or what you're really looking for; then outside the closure (in the caller), just check whether `find` returned `None`, or `Some(Err)` , or `Some(DirectoryEntry)`. Right?
Async IO and N..M threading. ( green threads ). There is a huge history concerning green threads from pre 1.0 . We have Futures-rs which is brilliant abstraction, but the compiler needs more work to give meaningful feedback to new users. On top of this the guys from the Tokio are building a green threading solution that the community desperately wants. But questions remain: * Is Tokio exposing a good and general API? Or will we see a second async framework within a year or two ? * Are generators going to be a widely used construct in the stdlib and in teaching rust? * async macro's &amp; function tags ? They are smart and will do their best, but it's by no means a given that they will succeed in building something for everyone. In fact its very likely this is not even possible. So which use cases should they prioritize , and will these overlap with yours ? 
&gt; Are you proposing introducing exceptions? No, I think labeled returns are more than enough. In fact labeled returns also make `continue`and `break` obsolete. Simply the ability to label _any_ scope. Being able to do something like: `escape: { ... if ... { return 'escape value_to_return; } ... } If `try!` and `?` then had support for the label to return to that would be fine. &gt; What would a for-loop do that can't be done with iterator functions? These function names also hint me about what you are doing A for-loop can do one important thing than a closure fed to an iterator can't: It can return directly to the surrounding function so you can immediately return from the calling function on the first `Err` with a `?` or `try!` &gt; ( and they have early return ). like this The trick of using `&lt;Result as FromIterator&gt;::from_iter` only works if your eventual goal of iterating is to create some kind of collection; it often isn't. Let's hypothetically say I have an iterator that traverses upwards a directory hierarchy and I want to get the first directory which is owned by the root user: fn nearest_uid0 ( current : Dir ) -&gt; io::Result&lt;Option&lt;Dir&gt;&gt; { for dir in current.ancestors() { let dir = dir?; if dir.stats()?.uid == 0 { return Some(dir) } } None } That's basically re-implementing the logic of find. If our functions had no possibility of error I could just use: fn nearest_uid ( current : Dir ) -&gt; Option&lt;Dir&gt; { current.ancestors().find(|dir|dir.stats().uid == 0) } But since it can all be a negative result along the way it is really hard to conveniently propagate that error to the top. However if there were labeled returns: fn nearest_uid ( current : Dir ) -&gt; io::Result&lt;Option&lt;Dir&gt;&gt; 'top: { Ok(current.ancestors().find(|dir|{ let dir = dir?'top; dir.stats()?'top.uid == 0 })) } Already a bit simpler and yes that syntax needs work but you get the idea.
Then you can find the place where the first Error if any occurs yes. but you can't find the place where the first correct result occurs. Basically we are operating on a teritary logic here. There are three cases to discriminate between: 1. Err 2. Ok, and the value is the one we want 3. Ok, but the value is not the one we want It should short-circuit on the first two but keep searching on the latter and the closure can only return true or false so it can't discriminate between the three possible cases. If it some-how returned `Result&lt;bool&gt;` then it could.
&gt; A community full of friendly people Funny you mention that. I'm also getting more skeptical about the Rust community's friendliness, because I see it going against fostering community cohesion and growth. It's a little bit like StackOverflow "community": new poeple come in, get their questions answered and problems solved, and don't have to stick in for any longer than necessary. Because they don't have to work very much to be considered a part of the community, they don't tend to value it very much. I see it especially on the IRC. Dozens of people come and go, but the pool of regulars grows extremely slowly.
I think the closest right now would be using a build script in cargo. (If i'm understanding your question right) http://doc.crates.io/build-script.html
And perhaps more importantly (I should've mentioned that in the post!), you cannot do `||` expressions this way.
I honestly also think it just scares away a lot of people. I see a lot of criticism on the rust community on a lot of places. When you look at Rust contributors the _vast_ majority of contribution seems to come from North America. A lot of people outside NA tend to feel that North Americans take friendliness a bit too far to the point of that it unnerves them. I also think the supposed "exclusivity" message is a pretentious load of it. As usual "inclusive" means "inclusive towards a select few groups while all others can get the sack" the Rust community is shamelessly ageist and defends it. As said the problem with friendly people is that they need social pressure to stand for an ideal and will stand for oppression just as easily if there is social pressure that way. I don't believe people are friendly for altruistic reasons; they're friendly because they want to be liked which means they will go along with the dubiousness of their peers and there's research which corroborates this idea which finds that unfriendly people are more likely to stand up against their superiors and their peers when they do something that's morally wrong: https://nextshark.com/psychology-study-exposes-a-scary-truth-about-people-who-are-nice-all-the-time/
I don't agree that the reason for the convoluted design is fundamentally a consequence of type safety, it's mainly because of the limitations of the Rust type system. The Rust type system is primarily designed for achieving zero cost abstraction and avoiding shared mutation. It lacks support for many high level abstractions which you can find in for example Haskell and Scala. The result is convoluted design solutions which doesn't compose, like for example the error handling. I agree that Rust is superior to C++ (in most ways), but it's not a great high level language.
This is central in the ecosystem - a lot of wrapper crates for c/c++ libraries build the library via build scripts (e.g. using the cmake crate, sometimes create bindings via bindgen on the run) and export them. I'm sure this approach is feasible for other languages, too.