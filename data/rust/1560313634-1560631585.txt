I think the biggest reason is that `let x = y;` returning the success of y binding to x would be potentially confusing, and non-obvious. If someone unfamiliar with rust reads let x = (let y = 3); they'd probably assume `x` has the value `3`, not `true`? In another vein, if I write fn my_func() { let x = 3 } Having an error "expected (), found bool" is probably much less useful than "missing semicolon". With that said, bringing `let` statements closer to expressions has definitely been considered. The most successful effortI know of is [2497: if let chains](https://github.com/rust-lang/rfcs/blob/master/text/2497-if-let-chains.md), which is an experimental eRFC to allow syntax like ` iflet x = y &amp;&amp; let a = b {` to be valid. I don't think it fully promotes `let` to an expression, but it definitely comes closer than the current status. See also the responses to this post: ["Why doesn't let just return a bool?" on internals.rust-lang.org](https://internals.rust-lang.org/t/why-doesnt-let-just-return-a-bool/5665)
Thank you so much for the explanation and the links!
Boy speaking of hacky GitHub shit, I used 805% of the storage and got LFS locked on my account by storing shit tons of financial timeseries on it.
In your Field struct have you considered wrapping each field in the `Option` type for optional fields? I do agree that things can be very verbose. You may benefit from a small macro to deal with repeat implementations of Traits if you decided to go down that route.
&gt; if current_epoch == self.last_epochs[index] &amp; current_epoch &amp;&amp; USIZE_MSB == 0 &amp;&amp; current_epoch != 0 { https://github.com/4lDO2/evc/blob/7910aa4c9a5e805d54685bb389f251aed914f978/src/write.rs#L62 Is this a typo, or am I missing some case where `USIZE_MSB` could be zero? I'm kind of surprised the compiler doesn't get mad about this.
Some time ago I was working on a cargo subcommand to work with git registry: https://github.com/synek317/cargo-git-registry/ Unfortunately, as always, I ran out of time. I think I overcomplicated things a lot there and today I would rather start from scratch than continue.
This is how academic talks should be. Instead we have [a culture of trying to sound intelligent](https://stevenpinker.com/files/pinker/files/why_academics_stink_at_writing.pdf). I have often gone back to my own writing after the fact and found myself doing the same thing—despite being trained under one of the best mathematical writing mentors on the planet. Academia is a vast wasteland of poor communication skills. The hard part for me is, despite my immense appreciation for clear, comprehensible writing, I also love language and really enjoy the aesthetics of the technical idioms of my specialty. Such and such a function “on” some domain “vanishes.” What a great word, vanishes! We have an “onto” function, “the identity,” and “general abstract nonsense,” which refers to something very specific indeed! Things can be “degenerate,” “natural,” “trivial,” “canonical,” “general,” “arbitrary,” “well-behaved,” “pathological,” “regular,” “sharp,” and “strong.” When, as an undergrad just starting to learn advanced mathematics, my professor wrote a formal mathematical definition for “almost every,” I laughed out loud. Even the most sophomoric, immature math student can talk at length about large balls, tiny balls and even hairy balls without so much as a smirk or giggle after a semester of metric spaces. The mathematicians reading this might have to pause briefly to even understand the significance of the previous sentence, so entrenched are we in our language bubble. Half of the jargon we use we don’t even realize is jargon. “If and only if,” and “necessary and sufficient,” while confusing, at least have the same meaning in vernacular English, but we use the words “and” and “or” in a specific technical way without realizing that outside of a mathematical context “and” can sometimes mean “or” and “or” can sometimes mean “and”! And (or?) don’t get me started on xor. But how, when, and with whom we use this language matters. I am careful to be very explicit with my math students about how the words “obvious” and “trivial” are used by mathematicians. To an outsider, being confused by something that is “obvious” or “trivial” just makes you feel dumb. To a mathematician it’s just a normal Tuesday. Well, Tuesday “up to isomorphism.” I don’t know why I went off on this subject. Sometimes I just need to get it out. :) I should make this into a blog post or something.
Not necessarily disagreeing with the assessment, but the Rust ecosystem as a whole does tend to put a much higher emphasis on performance than Python, which makes sense to me. The two languages have different strengths and goals. Also, this seems like behavior specific to certain executors, which could maybe be changed if a sufficient argument was made for it.
I was thinking about implementing the `Default` trait to deal with defaults as it seems like a more or less good case for this. Macros would be a good thing but it's too early for me and I only had a quick look at them. The least I can tell is that it feels like it's inspired from the Scheme syntax-case macros. That said, my main goal is to start from a good design as it feels like Rust unlike other languages doesn't seem to make it easy for bad design to evolve flawlessly. It's a plus and a minus at the same time. In Python I'd be able to implement a really bad design quickly before coming up with something that's better. In Rust, I wouldn't want to wast my time making a bad design work if I'm going to spend twice as much time to make a good design work from a bad design.
Well, that goes on the saved list. This should definitely be incorporated into the upstream as sanxiyn said. This is the kind of quality hard earned knowledge which screams for wiki entry.
Not positive but I’m pretty sure that in the first scenario, the compiler can infer the declaration of a static variable. The second scenario, you’re explicitly creating a local variable, then trying to create a static reference to to. So you’re returning &amp;’a Foo where ‘a is the lifetime of the function.
Interesting! What I'm curious about is if in my first working example, if I call blah, multiple times does it refer to the same location in memory? It seems if I give it constant values it also compiles, but non-constant variables it fails. My guess is the allocator is not allocating each time, and its like auto inferring a static variable.
Yeah my assumption would be, that behind the scenes, it recognizes the value is constant and just returns references to the same memory.
Oh, it should be `if current_epoch == self.last_epochs[index] &amp;&amp; current_epoch &amp; USIZE_MSB == 0 &amp;&amp; current_epoch != 0 {`. Thank you.
I released `0.1.1` with that typo fixed.
Looks like it's [RFC 1414](https://github.com/rust-lang/rfcs/blob/master/text/1414-rvalue_static_promotion.md) in action. A bit off topic, but the rules for what can be promoted seems a bit vague, though. The RFC talks about constexpr, but doesn't specify what it is. (Or maybe I just missed it?) The seemingly obvious definition is const expression, as defined by [the reference](https://doc.rust-lang.org/reference/const_eval.html). This would mean that calling const methods is constant expression. But replacing `100` in your code with `40i32.wrapping_add(60)` breaks it, so this can't be the answer. On the other hand, `i32::max_value()` seems to *not* break it, which is somewhat strange.
Still, the implication is that these people either - don't know the name of the subreddit they're hanging out on well enough to notice that they're submitting to the wrong one, or - submit their stuff to reddit without being regular readers.
Would anyone be able to explain the generated MIR for this? I haven't looked at MIR at all before, it seems to related to something called [promoted constants](https://rust-lang.github.io/rustc-guide/mir/index.html#promoted)? `fn blah() -&gt; &amp;Foo {` `let mut _0: &amp;Foo; // return place` `let mut _1: &amp;Foo;` &amp;#x200B; `bb0: {` `StorageLive(_1); // bb0[0]: scope 0 at src/main.rs:11:5: 11:22` `_1 = &amp;(promoted[0]: Foo); // bb0[1]: scope 0 at src/main.rs:11:5: 11:22` `_0 = _1; // bb0[2]: scope 0 at src/main.rs:11:5: 11:22` `StorageDead(_1); // bb0[3]: scope 0 at src/main.rs:12:1: 12:2` `return; // bb0[4]: scope 0 at src/main.rs:12:2: 12:2` `}` `}` &amp;#x200B; `promoted[0] in blah: Foo = {` `let mut _0: Foo; // return place` `let mut _1: Foo;` &amp;#x200B; `bb0: {` `(_1.0: i32) = const 100i32; // bb0[0]: scope 0 at src/main.rs:11:6: 11:22` `// ty::Const` `// + ty: i32` `// + val: Scalar(Bits { size: 4, bits: 100 })` `// mir::Constant` `// + span: src/main.rs:11:17: 11:20` `// + ty: i32` `// + literal: Const { ty: i32, val: Scalar(Bits { size: 4, bits: 100 }) }` `_0 = move _1; // bb0[1]: scope 0 at src/main.rs:11:5: 11:22` `return; // bb0[2]: scope 0 at src/main.rs:11:5: 11:22` `}` `}`
&gt; It can be read from just fine, but writing is a different story. I don't think `cargo publish` hits the git repository directly, as /u/mgattozzi notes TFA notes: &gt; it's a manual process since you can't just `cargo publish` The normal cargo process is probably to bounce the publication through a specific endpoint on the registry actual rather than directly update the git repo, such that the registry can do its own validation and the like.
&gt; C/C++ macros are not defined as "transformations of text", they operate on the parse tree. No, they operate on the "preprocessor token stream", which is similar to, but not quite the same as, the regular token stream. No trees in sight when you're working with macros. Except for parentheses around the macro arguments.
Use slog. It’s not that complicated and very flexible. Logs should be async. Serializing and writing to disk on the same thread as the rest of your app is terrible.
I think it is currently limited to _literal_ expressions. Technically, it could be expanded to work with all const expressions. I suspect that the reason this hasn't been done yet is that it might not be very obvious when reading code, what is a const expr and what isn't. That could lead to confusing situations when you change one small thing and everything breaks.
Correction, stuck having to develop on ART, which only understands a mix of Java 7 and 8 features. On a proper JVM, against Java 13, its appeal fades away.
Because Android is stuck on a mix of Java 7 and 8 features, and Google rather fosters Kotlin adoption than briging Android J^++ in line with standard Java.
I will add that only Java devs that are InteliJ users (Eclipse/Netbeans support is lagging) and that haven't bothered following up on latest Java improvements.
Kotlin also targets native code via Kotlin/Native. However right now, JetBrains requires you to buy Clion if you want the nice tooling for this use case.
`cargo publish` is just for creating the `.crate` file isn't it? I was referring to adding the registry in a way cargo will understand it after adding the url to `.cargo/config`.
Actually, `wrapping_add` has been `const fn` since [1.33.0](https://blog.rust-lang.org/2019/02/28/Rust-1.33.0.html). Also, replacing `100` with `i32::max_value()` still compiles. Which I said is somewhat strange.
You have to add a line to the top of your main.rs. I think it might be this one &gt;#![feature(proc_macro, conservative_impl_trait)] But I'm not home so i can't double check
I understand what you're trying to say, but you're relating "manly" and "masculine" with something negative though, and it's at that point your argument breaks down. Being masculine is not a bad thing, just like being feminine is not. You can be a "manly man" and still walk with a pride flag in your hand. In fact, that would be even more manly than being afraid to do so imo. Also, as a side note. As someone who went through a decade long depression that almost ended me several times, I can promise you that it wasn't posts talking about my masculinity that helped me get out of it. A lot of people just don't have anyone to talk to about problems in the first place. And even if they do, the problems are usually very hard to solve.
Rust is in full blown 100% hype mode. People who don't know types from the hole in their ass are "getting into rust" right now. Subscriber count is not a useful metric for anything other than "early adopter mind share," which, believe it or not, is not that informative.
What do you mean under “hype”?
I think i32::max_value() returns a compiler constant and that's why it compiles. It probably gets turned into a const fn or directly substituting calls to that function into the constant.
I would say that in the first case, \`&amp;Foo { bar: 100 }\` is automatically inferred to have the right lifetime (i.e. \`'static\`). You can do the same thing at top-level with: &amp;#x200B; const BLAH: &amp;str = "Goob morning!"; // inferred 'static &amp;#x200B; In the second case, you’re explicitly binding \`Foo { bar: 100 }\` to \`f\`, which is local to \`blah()\`. Hence, the lifetime in return position cannot be higher than the lifetime of \`blah\`.
Very interesting insights!
What's the deal with bzip2 compression? I read in another thread, that it's hard to port it (or smth. like that) and I can't imagine why, is it such a huge codebase?
The variable self itself (not what it references) is out of scope when and\_then is executed. Move its value into the closure: ``` .and_then(move |body| { // &lt;- add move here // ... let return_struct = DataRef {field_1_ref: &amp;self.field_1, request_data:data}; ``` [playground here](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=1b68c3a9d82db8bfd5cfc16494aa1a61)
My mistake! I checked, but I'm on mobile and accidentally found the docs for the `num` crate instead of std.
&gt; I guess my tl;dr to you is that it is never as clear-cut as you seem to think it is in human communication. I think we're talking a bit at cross-purposes. So I don't think we're getting here anywhere. But I would prefer if you don't imply what I think, because this certainly isn't the case.
I don't think it's hard. I think the reasoning was that it hasn't been maintained in a while and would make a good example of porting to Rust. It seems to be worked on by the same team as librsvg, the Rust port of libsvg.
That works out perfectly. Thank you!
&gt; I had started looking into what it would take to use rayon to parallelize work, but since IntoParIter is not implemented for std::io::Lines, it would take some fussing to parallelize the work, which may not be worth it at all. Check out https://docs.rs/rayon/1.0.3/rayon/iter/trait.ParallelBridge.html#tymethod.par_bridge.
I think it was a very bad decision to create yet another language people have to learn. There are more than enough languages on earth already. Arguably you could say the same about Rust, but Rust's great advantages are better communicated (or more significant...) than Kotlin's
&gt; It seems to be worked on by the same team as librsvg, the Rust port of libsvg. librsvg is librsvg (the r is for render). It was converted to rust in-place, not ported.
I went through some of the discussions. It is really weird that only a few people mentioned that `let` can potentially be an expression. Though there are quite some discussions on using `let` in ways that are similar to expressions, no one has started a discussion on whether this syntax should be an expression... I was kind of shocked when I saw in a thread discussing implementation one person said "this is going to make `let` *like* an expression in implementation". It really does not need to go into the implementation to see its similarity with an expression...
I'm not too familiar with the underlying library but some things confuse and concern me right off the bat. Why is there a call to this `.destroy()` method instead of having that role be handled by the drop implementation? Why does the `.destroy()` method in leptonic cast an internal raw const ptr to a mut ptr? If you need that ptr to be mut just define it as one in the struct. All the methods seem to be defined as taking &amp;self (except when required by traits) even when they seem to change internal state which seems very wrong.
It's a bunch of work to port something that has organically grown over 20+ years while keeping binary compatibility, regardless of size :)
I've not deeply looked into your code but wouldn't a Trie data structure to store the words naturally lead to a parallelisable algorithm. If you had a trie with 26 base nodes of each character, then you could search all 26 branches in parallel between 3-7 levels (depending on your word length) and reduce the search space by pruning branches that could definitely not be in your target search space.
Hello! Is there a way to tell cargo to build static or dynamic library depending on features passed? I'm trying to write a plugin to some app with conditional compilation. By default it must build as dynamic library and some initialization functions are exposed, and with feature "no-init", library should be static and no initialization methods are exposed.
👍 Have you through fn attributes? ``` #[counterfn(key = "Packet")] fn send_hi(addr: SocketAddr) -&gt; Result&lt;(), io::Error&gt; { ```
Hi. I wrote this book as a result of investigating the low level basics of how “green threads” (or userland threads) really work, and I thought it might be of interest for others as well. It's published it as a gitbbok, but I hope to condense it into an article sometime later. &amp;#x200B; I’m actively asking for feedback, and all feedback is welcome - especially if you spot anything wrong, it will only make it better for the next person reading it. &amp;#x200B; The repository for the book is located here: [https://github.com/cfsamson/book-green-threads-explained](https://github.com/cfsamson/book-green-threads-explained)
Well, a PR to gtk-rs explaining all this process would be very appreciated. Since no gtk-rs developer is using windows, is a bit complicated for us to test on this platform...
I had really bad experience. The current documentation is really shitty.
Note: there is currently no changelog for Rand 0.7.0. It's on the to-do list!
Bikeshed question: Why not use mdbook? I think it is good enough to replace gitbook.
Where would you like to receive feedback?
See [CovenantSQL](https://covenantsql.io)
I don’t see anything about Tokio applications in this. Will this be introduced at some point or does this focus only on std::io?
Well, the tradeoff is that while you can have multiple readers reading without waiting (not even when refreshing), only one writer can write at once, plus that you take up double the space. The technique is explained [here on YouTube](https://youtu.be/s19G6n0UjsM).
Okay, thanks! So, using this method the RwLock will get blocked as long as I'm using the value, right?
But shouldn't the first one go out of scope in any scenario since it was created inside the function?
If you are developing a library API, the futures/streams it exposes should be public named types. `async fn` cannot be used for that before [named impl Trait types](https://github.com/rust-lang/rfcs/pull/2515), or "existential types", are stabilized. Combinator types can help to some extent, but as the full combined type needs to be spelled out, this becomes unwieldy beyond a few simple combinators, and it's currently impossible to incorporate unboxed closures in type signatures for the same reason as with `async fn`s. Also, the library wouldn't want to expose the exact implementation future type with an alias, so the combined type needs to be wrapped into a public structure that needs provide the `Future` impl. Another reason to code futures explicitly is in being able to provide a useful `Debug` implementation. I'm developing a procedural macro and a support library to help code futures with the 0.3 API by deriving an implementation from an enum type definition and state transition traits. Hopefully it will get into a good shape to publish soon.
Yeah, I think I should change "lock-free" to "lock-free when reading".
Thanks!
I'd take a look at https://gitlab.nebulanet.cc/xacrimon/rs-hm-bench. Disclaimer: I'm the author of those benchmarks and the ccl crate.
You probably want a mutexed map or a concurrent map if you care about performance. Take a look at https://gitlab.nebulanet.cc/xacrimon/rs-hm-bench for some options.
\&gt; (tip: think carefully about whether bumping the versions of your dependencies affects your API) Should this be a lint of some sort? A warning if you bump a minor/major version of a public dependency without bumping your own version?
No guides anywhere that I could find.
This is pretty amazing. Thanks for the write up!
Perfectly fine question. Honestly, I started writing an article but soon realized I wanted to explain more than would fit in that format, but I wanted to continue writing while I had time so I chose something I already knew that had a nice editor so I could just continue working. Mdbook would probably be slightly better, fortunately if this is popular enough an people want it, it should be fairly easy to migrate later.
No, because it is recognized as a static value. It's the same thing as with strings literals or integers. `123` lives in the ROM, so it's safe to infer `'static`.
There is a similar crate, [metered](https://docs.rs/metered/0.2.1/metered/), that uses proc macros.
I have a struct, it's supposed to represent an image. The color values should be allocated on the stack because it could be megabytes. So I put a `Box&lt;[u8]&gt;` field on the struct. Does that make sense? If it does, what do I call to allocate it? Will it get freed when the struct is destroyed or do I have to do something extra?
Mostly the code from documentation is enough
I'm running through a rust and wasm tutorial, but really confused as to how extern is used in the two different Hello World examples found in https://rustwasm.github.io/docs/book/game-of-life/hello-world.html and https://rustwasm.github.io/docs/wasm-bindgen/examples/hello-world.html One uses: #[wasm_bindgen] extern "C" { fn alert(s: &amp;str); } While the other omits the "C" #[wasm_bindgen] extern { fn alert(s: &amp;str); } What is the difference and what is the "C" for? Thanks.
&gt;I think we're talking a bit at cross-purposes. So I don't think we're getting here anywhere. We are definitely not getting anywhere. =) &gt;But I would prefer if you don't imply what I think, because this certainly isn't the case. Most of your responses have been telling me what I think, or telling me things like I'm making easy jokes. I would also not prefer this. You might consider re-reading your own posts from the perspective of a third party. I took a brief skim through your post history, and there's a definite recurring theme of hearing but not listening in them, and doing what you say you wish others wouldn't. In any case, I think the possibilities of this particular conversation have been exhausted. If you want to talk more later, feel free to ping me.
Yes, I've found out to be. To OP, I've used the basics and aggregation framework in production. The driver seems stable enough based on my usage of it. It's just a pity that it seems to get no love from Mongo Inc.
If the whole issue could be a compile time error, that’d probably be fine, but I think that would more or less break the design of any code that is trying to hide that the implementation is done using async code. I just think that making a runtime error out of something that *could* work is silly. Especially because the argument is that it would have bad performance. Not running at all during runtime is much worse performance than running slowly.
Thanks for your explanation. Keep up your good work!
&gt;I understand what you're trying to say, but you're relating "manly" and "masculine" with something negative though, and it's at that point your argument breaks down. Not really. You're missing the distinction I have been making between healthy and toxic masculinity. While I'm happy to have a longer discussion about it with you in private, it is probably best to leave that particular issue where it is, for now. It's clearly not something we're going to agree on without a lot more discussion. &gt;Also, as a side note. As someone who went through a decade long depression that almost ended me several times, I can promise you that it wasn't posts talking about my masculinity that helped me get out of it. I'm not sure what you're trying to say here. I don't think curing depression by reading online messages has been something anyone has talked about in this particular discussion. Are you doing ok? &gt;A lot of people just don't have anyone to talk to about problems in the first place. And even if they do, the problems are usually very hard to solve. If a problem is making you consider suicide, it is indeed hard to solve. Do you need someone to talk to?
Wow, nice first contribution to the Rust itself 👏 congrats!
Cool! I am all for these changes that reduce inconsistencies.
Would this allow for a macro to define default values?
Hopefully the combination of [public/private dependencies](https://github.com/rust-lang/rust/issues/44663) and [semverver](https://github.com/rust-dev-tools/rust-semverver) will be able to detect this issue.
Still waiting for variadic arguments without unsafe C-compatible binding. Passing as a slice feels quite awkward.
&gt; The color values should be allocated on the stack I think you meant on the heap. I does make sense, you don't have to allocate it, when you create it it will allocate enough space. But currently stuff you put in a `Box` using `Box::new` has to go through the stack so you might have a stack overflow at that moment. There are workarounds however for example you can use a `Vec` and keep it or call [into_boxed_slice](https://doc.rust-lang.org/std/vec/struct.Vec.html#method.into_boxed_slice) to get a `Box`. Regardless if you store a `Vec` or a `Box` you don't have to deallocate it, when the `Vec` or `Box` is dropped all it's content will be freed. [Here](https://users.rust-lang.org/t/how-to-create-large-objects-directly-in-heap/26405) is a conversation where you have a working implementation.
I have an issue filtering out none options in a vector and putting in the underlying type the issue starts at line 184 I am trying to turn a Vec&lt;Vec&lt;Option&lt;(usize,usize)&gt;&gt;&gt; into a Vec&lt;(usize,usize)&gt; https://gist.github.com/rust-play/8c689d42f741f583e69bc8c55eb3da84 https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=8c689d42f741f583e69bc8c55eb3da84
Btw for anyone curious, the `asm!` macro issue that the author mentions (compiles fine on debug build but not on release build) is here: https://github.com/rust-lang/rust/issues/61429
Yea sorry I meant heap, long day. Thanks for the answer.
According to the [reference](https://doc.rust-lang.org/reference/items/external-blocks.html#abi) they are the same. You can also read more about FFI in the [book](https://doc.rust-lang.org/book/ch19-01-unsafe-rust.html#using-extern-functions-to-call-external-code).
Looks nice, just reading it. The first full example code has an error, line 11 reads \` gt\_switch(&amp;mut ctx, &amp;mut n);\`, but should be \` gt\_switch(&amp;mut ctx);\`.
Oh excellent, I needed exactly this for a proc macro recently and had to go for a less than ideal workaround. Looking forward to this being in stable!
I wrote code for exactly the same problem a while ago in Rust for the 26 English letters only. IIRC my code runs in single digit milliseconds on a list of 270k words. I first compute the frequency of letters of the word into a `[u8; 26]`: fn frequency(str: &amp;str) -&gt; [u8; 26] { let mut frequency = [0; 26]; for byte in str.bytes() { match byte { 97..=122 =&gt; frequency[byte as usize - 97] += 1, _ =&gt; {} } } frequency } and then do a simple zipped iteration over the 26 byte arrays of the word and the query to see if the frequency of the letter in the query is less than or equal to the frequency in the word. My words are stored in a `Vec&lt;&amp;'static String&gt;` using `lazy_static` and I don't use anything like parallelization. I'd love to see how this algorithm performs with your data set if you only need to support English letters. I don't filter by length so it might work even faster for you!
Also interesting, a similar talk by Anders Hejlsberg on the Roslyn (C#) compiler: https://channel9.msdn.com/Blogs/Seth-Juarez/Anders-Hejlsberg-on-Modern-Compiler-Construction
Here is a working [playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=da9e7d7b9ae32a028dfd81b6fd456fd0). I used `flatten` instead of `fiter_map` because you din't map and `flatten` is simpler than `filter(Option::is_some)`. There was also a little borrowing issue with the `forest` variable but I added a `&amp;` and all is good.
I remove the &amp;'static in this version. I'm surprise it also works. [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=5f6bcc4003b26cf7afc1890238b51729](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=5f6bcc4003b26cf7afc1890238b51729) &amp;#x200B; Should it work?
I removed the 'static in this version, but I'm surprise it also works [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=5f6bcc4003b26cf7afc1890238b51729](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=5f6bcc4003b26cf7afc1890238b51729)
There are two ways to write it I can think of immediately: // [1] adjacent_positions .into_iter() .flatten() .flatten() .collect() // [2] adjacent_positions .into_iter() .flatten() .filter_map(std::convert::identity) .collect() Depending on your preference of the level of clarity. Note: You also need to add `&amp;` in front of `forest[new_y][new_y]` to fix a move error.
Good catch. You're right, it's corrected now.
Unfortunately, there isn't support for \`#\[proc\_macro\_attribute\]\` but it is something I will definitely work on starting with maybe another RFC.
I use the \`pin\_mut!\` macro all over the place... which is just a macro that expands to an \`unsafe { }\`
Well, we very clearly have different philosophies on what language evolution should look like, so there's that. And honestly, getting into these discussions with you in the past has been terribly exhausting, so I don't really feel like doing it again. Sorry. Bottom line is that there are plenty of language changes (and std library additions) that are "nice" but not critical, and therefore, I don't bump an MSRV just to get them. Removing small rough edges isn't enough, because the code has already been written, tested and deployed. Whenever I bumped the MSRV of a crate, one of the things I do is go through the release notes for each version of Rust released between the old MSRV and the new MSRV. I then make use of as much of the new stuff as possible. Because I don't value lack-of-churn above everything else. It's also important to me that the ecosystem doesn't stagnate, and that those reading my code get a good picture of what Rust code _should_ look like. I just move more slowly than the bleeding edge. Sometimes I lag more than a year behind for some crates. Part of that is because I can only do so much in a day, and the other part is that I want to decrease churn for users of my crates, if possible.
I'm not missing the distinction, but the use of "manly" as a derogatory word is wrong imo. Manly man should not be a bad thing. Or should all men be "unmanly"? You get the point. I strongly believe that you won't make toxic masculinity (or femininity for that matter) go away by treating those people like you do. Even if it feels good saying that they have a fragile masculinity, a big ego, and so on. It won't change them. It will probably only make them get more defensive. Also, no, I don't need to talk about depression, I got out of it. But yes, I do understand that most societies places weird expectations on men to be able to handle all of their problems alone in silence. But I wouldn't blame that on any toxic masculinity, because it's something that is determined by the society as a whole. A lot of women don't expect men to speak out about their emotional problems either. Anyway, no need to discuss further if you don't want to. Don't want to end up in a never ending discussion.
My question: what should I implement to get rid of the `.expect` in the function `string_to_txt` ?
I mean hype. The normal definition of hype. Google "define hype."
Apologies I didn’t write it clearly, what I meant was that I wanted to make use of attributes in function parameters when processing the syntax with syn in a procedural macro (similar to attrs in [this](https://docs.rs/syn/0.15.35/syn/struct.ItemFn.html but for each parameter rather than for the function itself), I think this will now work after your changes? (once syn is updated to reflect the changes of course).
It's so unintuitive. Like why would they [name a function o!](https://docs.rs/slog/0.6.0/x86_64-apple-darwin/slog/macro.o!.html)? I've never done serious logging but I've never seen drain or o! anywhere else.
According to [the docs](https://serde.rs/convert-error.html), you can do `.map_err(D::Error::custom)?`. Since this is a trait method, make sure to also `use serde::de::Error;`. [playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=dd2747a71421b24b8c4d84dd429b627b)
Ohh, I see. \`Vec&lt;Attribute&gt;\` probably needs to be added into \`[FnArg](https://docs.rs/syn/0.15.35/syn/enum.FnArg.html)\`. I forgot that the \`syn\` crate also needs to be updated. Thank you for your comment.
I still don't get lifetimes. Just never need to use them. Always find a way around it
Great, thank you. I’m not sure who maintains the syn and quote crates, I’d be happy to assist with this change if required.
Great, thank you. I’m not sure who maintains the syn and quote crates, I’d be happy to assist with this change if required.
Thanks, I didn’t understood this syntax. It is now clear
Yep, that's me
That or just watch everything break on a minor version like 0.6. Was well worth it though.
Yeah, correct.
It is kind of strange! Thinking about it, I found one more reason which might or might not have been brought up in the threads: since let introduces a new variable, it really doesn't work well to have it only run sometimes. If I run `(rand::&lt;bool&gt;() &amp;&amp; (let x = 3));`, this... really shouldn't and couldn't introduce the variable `x`. Maybe that fact influences some people's decisions as well? That if let were on expression, it wouldn't make sense to put it in many places that an expression is valid, or at least, doing so wouldn't introduce a new variable binding.
&gt; Honestly, when I read this I feel that work on the language and myself are rather underappreciated... I think what you may be observing with this sentiment is library authors focusing on the experience of using the library vs your attention on implementing the library. For serde currently the minimum supported Rust version is 1.13.0. This is largely because there have been zero language features since 1.13.0 (Nov 2016) for which serde would be a better library if we required the new feature. Similarly the minimum supported Rust version for serde_derive is 1.15.0. Again there have been zero language features since then that would make serde_derive a better library (probably true for most derive macros). In other words the best possible API for a serialization library that uses all the latest stable features is a compatible superset of the best possible API for a serialization library that one could make using only 1.13.0 features. That is not to say that there is no such language feature that would be worth raising the minimum supported compiler version for; there just haven't been any of those in 2.5 years that would benefit this specific library. Among upcoming ones, for example I consider it possible that the best possible API for a serialization library leveraging GAT would look significantly different.
Error handling with rust is so beautiful
Embedded is way smaller than I expected.
With exposing public types, there is one more alternative: using a new type struct as the return value. `hyper` does this - it defines `pub struct ResponseFuture { inner: Box&lt;Future&lt;Item=Response&lt;Body&gt;, Error=::Error&gt; + Send&gt; }` as the type it exposes. Boxing is a cost to pay, but I'd say it's well worth it to avoid having to manually implement futures - and as you said, once we get existential types we can use those. I guess providing `Debug` is a valid reason - I personally wouldn't choose to use the unsafe APIs for the sake of just providing a `Debug` impl, but to each their own, I guess?
I am creating a program for a device that only has a window driver with a c header and the dll file compiled with an old visual studio (2005 or some such). Now I would really like to use relm for this program which means I need to use windows-gnu toolchain. Will I still be able to call into that driver through cffi if I'm using gnu toolchain on windows? This drive asks for a callback. Is there a way to mimic a c function from rust code for callback?
Hi, I'm also interested in this question. I'm guessing there probably exists a site like Leetcode but for becoming familiar with a language instead of algorithms. Anyway, the bulk of the research about deliberate practice comes from Anders Ericsson. The "10,000 hour rule" is a pop-science simplification of that research by Malcolm Gladwell. Ericsson recently published a book detailing that research to a pop audience (properly): Peak: Secrets From the New Science of Expertise. You might enjoy it.
Thank you! I’m sorry if I over react, but as you know lock free is hard and likely to have correctness bugs. It’s incorrectly used as bragging rights, which leads to poor quality and thus harms users. Being more specific doesn’t diminish your work nor dissuade usage.
Like killercup said, metered exists and handles that sort of function wrapper/proc macro angle. It'd be nice to have that support, either by adding them myself or by having metered use the facade macros when it ultimately stored the counters/histograms/etc. &amp;#x200B; Unfortunately, I don't have enough time (or desire, really, since metered already exists) to reinvent that particular wheel... and there's a lot of work thats already gone into metered (and things like tokio-trace, etc) to handle the full spectrum of sync/async code correctly.
&gt; vs your attention on implementing the library. Not really. There are some language improvements that make implementation smoother, but I do think there are plenty of stuff that make the UX for library users more pleasant. When we design language features, it is with the external UX of libraries in mind. &gt; For serde currently the minimum supported Rust version is 1.13.0. As JetBrains's [survey](https://www.jetbrains.com/lp/devecosystem-2019/rust/) just showed, we can be confident in that 1.13.0 is needed by very few people. &gt; Similarly the minimum supported Rust version for serde_derive is 1.15.0. Again there have been zero language features since then that would make serde_derive a better library (probably true for most derive macros). In 1.34.0 there is https://blog.rust-lang.org/2019/04/11/Rust-1.34.0.html#custom-attributes-accept-arbitrary-token-streams which specifically is aimed at improving libraries like `serde_derive`.
Probably it's the same thing again. If you can use static lifetime, than you can use any other lifetime.
Isn't it because i32 is Copy, so it doesn't matter if it's known in compile time?
the lifetimes \`'a\` in your function can also be elided. This works: `fn get_request(self: &amp;Self) -&gt; impl Future&lt;Item=DataRef, Error=hyper::Error&gt; {`
The role of minimum supported Rust version in my libraries (and I think some of BurntSushi's libraries) is: the oldest compiler that can benefit from the current best design of the library. So language additions that don't motivate pivoting to a fundamentally different incompatible API are not a factor in raising the Rust version. In the case of Serde, language additions that would enable a fundamentally better design would be GAT or possibly certain trait object improvements. Observe that supporting alternative attribute syntax does not require us to drop support for old compilers. Even after starting to accept the new syntax, there is no reason to make that a breaking change (serde 2.0) by stopping accepting the old syntax.
If you change the second example to `const f: Foo = Foo { bar: 100 };` then it works. I believe the problem is the keyword `let` explicitly creates a new, locally-scoped variable on the stack, which inherently means it cannot have a static lifetime (at least, not without `unsafe` code)
Exactly. For example, regex makes use of simd features but still supports versions of the compiler that don't have stable simd intrinsics. Same thing with byteorder and 128 bit integers.
https://github.com/pingcap/talent-plan/blob/master/rust/projects/project-5/Cargo.toml
Wow, gfx-rs has existed for a *much* longer time than what I initially thought. (I thought it started in, like 2016 or so) Minor nit: (second paragraph, last sentence) where -&gt; were
This is fantastic! Thanks for writing this :)
&gt; Will I still be able to call into that driver through cffi if I'm using gnu toolchain on windows? As far as I know, C FFI compatibility should work across all toolchains. As for callbacks, this may help: https://doc.rust-lang.org/nomicon/ffi.html#callbacks-from-c-code-to-rust-functions
What does the 'create' keyword do in this context? `create f = Foo { bar: 100 };` Should that be `let` ?
Another interesting thing to note is that this doesn't work for `&amp;mut`. You get "cannot return reference to temporary value", presumably because taking mutable references to statics would be unsafe.
Congratulations on the milestone! And make sure to take some time away once in a while so nobody gets burnt out from contributing. :)
Yes, thank you!
Backend devs often stick to/r/java . Kotlin is fairly simple for java devs. Most of my everyday problems are not language-specific.
Even if lots of embedded folks use Rust, embedded is pretty small in general.
Manually define a new type wrapper for an int with methods for setting and getting the bits you want? The methods could be marked with `#[inline]` or something. Not sure if that's useful for you.
Given Rust's core target market is systems programming, it does surprise me a bit that web development is way ahead of it.
I see, thank you!
Gfx-rs is one of the coolest rust projects ever. I used it for a small graphical application and the gfx_defines macro is absurdly intuitive for setting up your pipeline. Congrats to you people for the 5 years of awesome work!
I was going to use a trie, that was actually my plan. However, when I got into the trenches, I couldn't find a place where it would be beneficial. If I were just calculating string permutations over the n possible solutions, a trie would make sense. I would need to actually profile it to see if there's room for improvement. Now that I think of it, it _may_ turn out to be worth it.
Regarding how small the ecosystem for web development in rust appears to be so far, it is kind of surprising that so many people use it for that purpose. However, web development is a very hot thing right now. About 80% of the applications I've developed at work has been web applications. So I don't find it too surprising that people are trying to make rust work for web development. It would be really nice if I could use it at work for some projects.
Well... based on compilation, you could: fn doit(real: i32, #[cfg(windows)] defaulted: i32) { #[cfg(not(windows))] let defaulted: i32 = 4; // Rest of function. } However you'll note that all callers must supply (or not) the parameter based on some compile-time condition...
I wonder, is there a list / census of projects which lack functional maintainership but are still a source of CVEs and get disparately / independently patched by systems &amp; distributions?
 $ cargo install cargo-asm $ cargo asm
There's also been a trend towards a larger number of smaller services. This architecture makes it easier to try out new technologies for several different reasons.
Thank you very much
You can [try godbolt.org](https://godbolt.org/z/79PhNs) website, just don't forget to pass optimization flags. It can also [show LLVM IR](https://godbolt.org/z/4nWJwR) if you interested.
[https://godbolt.org/](https://godbolt.org/) supports rust
You might want to look at Linux Foundation's Core Infrastructure Initiate and their census: https://www.coreinfrastructure.org/programs/census-project/
Rust was just stabilized for embedded in the last year, otherwise you'd have to use nightly. Embedded tends to be fairly change/risk adverse so I'm not surprised that there isn't a wide adoption. There are no chip vendors actively supporting Rust to my knowledge. Currently the main support is on ARM Cortex processor and many embedded developers target smaller platforms. RISC-V support is coming along. Support for the ESP32 processors would be a boon on the hobbiest side. The number of embedded developers is actually encouraging IMO and will only go up, especially if there is support for safety certification.
I'm going to PM you if that is ok. I have a suspicion about what is at the root of our misunderstanding I'd like to talk to you about.
Wow, the [results table](https://github.com/coreinfrastructure/census/blob/master/results.csv) is bleak. unzip, gzip, cpio, file, patch, rsync are high on the list. So are both procmail and postfix.
`&amp;f` declares a pointer to `f` which is within the stack frame of `blah()`. When `blah()` returns its stack stack will be lost to other calls, or other variables. The [compilier error](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=568ba0ac2cfe335127359cc4c97781f3) points this out stating &gt; returns a reference to data owned by the current function So `&amp;f` will point to garbage. When the lifetimes are properly annotated like `fn blah&lt;'a&gt;() &amp;'a Foo` [see here]https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=cbdf392a8f09d8d4abfc5cf89db06efa) it is still an error because when `blah()` returns the stack space that `Foo{ bar: 100 }` is using disappears. Same as before --- While `&amp;Foo { bar: 100 }` return a `&amp;'_ Foo { bar: 100 }` and the `&amp;'_` is resolved based on the function's signature, which is `&amp;'static`. This is kind of magic, and why I'm not a fan of implicit lifetime erasure because it makes things like this seem _kind of magic_ when the behind scenes rational is pretty straight forward.
Yes that does sound interesting. In the lecture I linked above the speaker addresses the pop science understanding of the 10,000 hour rule by focusing on the difference between practice and deliberate/edge practice. Deliberate practice means repeated exercises which target specific, fine-grained skills and require 1-3 attempts to repeat. The idea is that you can have an expert in an area coach beginners to focus on the few things that are the most important. I think it's kind of like how Josh Waitzkin was taught chess - an expert chess player had him use one piece at a time in a knight vs knight 1v1, before doing the same with each other piece and working up to a full board. There is a way to become an expert by focusing on small parts and turning those into a greater whole. That is what I would like to find for rust or programming in general.
"State of developers who answered Jetbrains Survey about the Rust ecosystem"
There are not much of them. And you will look into documentation when you write a macro first few dozens times anyway.
Not exactly. `A::B` is valid for `$ty` but it is not single `$tt`.
Awesome and really cool to see a screenshot of my rendy-pbr project as the thumbnail ;D
For flags, that's `-O` or `-C opt-level=3`.
You may also find it beneficial to ask on r/rust_gamedev, a subreddit dedicated to game development, as the participants there may have more experience in the particular domain both with Rust and C++.
I think I may have misunderstood what you are trying to achieve so if you could help me understand a bit more about why an `Enum` could be problematic for you I'll see if things click. `Enum` requires you to declare all the fields you need when its created and cannot be extended at runtime. If you needed a new variant you would add it to the `Enum`. &amp;#x200B; Using a struct would also mean having to declare a new struct so the why not use the former.
Yeah building projects I'm interested in is what I've been doing so far. I guess I'm just afraid of building poor habits - the 10,000 hours to be an expert is 10,000 hours of a specific kind of engagement with your subject matter. You can spend 10,000 hours doing something poorly and still be mediocre. The second half of this lecture was a good overview of the pieces you need to be able to engage well with your subject matter and build mastery. Basically it is: read other people's good code, do difficult exercises within your reach that focus on the most important concepts, and incentivize yourself properly to continue. Since I haven't seen many places to do rust exercises, I guess I'll keep working on my own projects, read Programming Rust, read through the crate of the week on the rust blog, and do some research on how the early experts got to be experts without guidance.
Rust beginner here - so happy to see this! We need more exercise-driven Rust courses!
Well, time to pick a small project from high on the list and get oxidizing! :D
That conversation's happening in my head whenever I (ab)use `LD_PRELOAD`
Does this have anything relate to Rust?
Sorry, but I cannot agree with you. I think that main " symptom" of hype is quick and fast rise in popularity, and after hype is gone, decline until certain level, as with Go lang. Rust has completely different pattern - slow but steady growth.
One of the best reads I have come across in a long time! Thank you very much for this.
The end goal is to be able to implement custom field type in plugins loaded at runtime. I'm not sure thought how possible it is at this point. For example, `FileField` vs `ImageField` in both case the data can be stored as `Vec&lt;u8&gt;` but the data handling can be different. The interface thought would be the same trough the trait. So the enum in my example is just storing data a bit like enum used to store Json variants. the Variants are fixed but flexible enough to implement complex types.
If you can do it with a procedural macro that can sort out all the unsafe pinning issues for you, why not? :)
I suggest godbolt.org if you want to look at assembly. It is quite nice at highlighting which parts of the assembly correspond to which parts of the code.
This is cool.
I'm not sure this is possible, but someone else might know more. Would it work for your use case to just always build both a dynamic library and a static library, and have an "init" feature expose initialization methods in both if enabled? You can always specify multiple crate-types with something like `crate-type = ["dylib", "staticlib", "rlib"]`
The tool is written in rust: https://github.com/anderspitman/fibridge-proxy-rs
Thanks for pointing it out. I read the article and I thought it didn't mention anything about Rust.
For me? I'd choose Rust every time. I say that coming from 20+ years of C++ development and only about 4 years of Rust. C++ would "work", assuming you don't screw up. But I'd far rather have the guarantees and syntax that Rust offers.
Yeah sorry for the confusion. I should have added a top-level comment right away.
For clarification, the service discussed in this blog post is written in Rust, MIT licensed, and running in production as part of the open source [iobio](http://iobio.io/) real-time genomics data analysis suite.
Ahh, I'm with you now, thank you! Using a trait does, in fact, seem like a better approach. Previously I mentioned a macro may help but in your case where you have different behaviour depending on the `XXXField`, it may not be as useful. If, for example, you had the `XXXField` implementing the same trait with the same behaviour it may have helped reduce the boilerplate code. This is tricky - I totally echo your feelings about Python and miss the witchcraft I used to get away with. Rust does make you think more and 90% of the time it's with good reason.
If you ask in r/rust what response do you expect? :D
It's fun to bash on it, but we haven't had any problems, and it fits out workflow well. If we were doing it again from scratch, we probably wouldn't do it again, but it's very far down the list of "things I'd like to fix/replace" right now.
OMG!!! Thx, this is so interesting and fundamental!!! I will check this out in the weekend!!! :) :) :) Thanks again, I'm so happy about this fundamental knowledge!!! :-D
I assume that Rust devs did/do use c++, for some reason, while C++ devs may not know about Rust.
The first message greeting you when writing a post is: &gt; **The Rust programming language. For the Rust video game, see /r/playrust** You should have read it :/
lol I actually did something similar as my first rust practise
It kinda feels like Rust is attempting to be an "accessible" systems programming language, in such a way that lots of users may already have a background in the web domain? Is the C++ conversion rate very high?
Thanks pub fn foo() -&gt; i32 { let b = Box::new(13); let a = 14; return 15 + a + \*b; } &amp;#x200B; Became &amp;#x200B; example::foo: mov eax, 42 ret
 I am just starting out and coming from high-level languages like python and java make me confused, whats is the difference between :: and .?
Not all are suitable though e.g. the info-zip project (zip, unzip) boasts about its extremely wide cross-platform compatibility from AOS/VS and X68000 to modern windows and linux, so I'd guess oxidation at the cost of &gt;80% of their supported platform would be undesired.
`::` is used to form qualified names, which includes names for things that Java would call static. For example, the `new` function of `Vec` doesn't rely on an instance of a `Vec` struct so it would be a static function if you were to implement it in Java. So `::` is used to access it. `.` is used to access everything else in a struct or an implementation of it.
Interesting that most use VsCode instead of Atom, I thought it would be opposite odds pretty much
That's true. I used C++ for more than 7 years but now I would suggest Rust for just about anything that C++ would be good for and haven't touched C++ in a long time. I even use Rust at my workplace when it's a solo project.
Please create more content like this! I thoroughly enjoyed reading this. This is the type of content Ive been looking for, but didn't know existed :)
Suggestion: add an ["unreleased"](https://keepachangelog.com/en/1.0.0/#effort) section at the top of the changelog and keep it up-to-date as changes are made (when a PR that does anything noteworthy comes in, require the author to update the changelog). This way you won't forget anything when doing a release.
As a former user of C++ for many many years I would suggest Rust with every fiber of my being for this project of yours, but of course I am certainly biased. As you've alluded to in your post, the productivity benefits of using Cargo instead of something like CMake is quite grand IMHO. Being able to add a crate to your project with a single line is very easy compared to setting up a cross platform build script in CMake or another Build Tool^TM. As for the technical aspects of your post, instead of using TCP I suggest using a library like [laminar](https://github.com/amethyst/laminar) for networking. You can specify which packets need to be reliable or unreliable, ordered or unordered, etc. You could initially set your packets to be reliable, and implement a lock-step system if you really want a turn-based style of gameplay but keep the option to later decide if that's the route you would like to peruse. If you have any other questions about gamedev using Rust then please ask away. I'd be happy to answer any additional questions.
Does it have to do with the [`#[rustc_promotable]`](https://doc.rust-lang.org/nightly/src/core/num/mod.rs.html#275) annotation?
Awesome read, thanks! As someone who comes from a Java background who has been trying to get into systems programming, this was a nice refresher on OS concepts and a great tutorial on how do achieve these concepts in rust. I look forward to similar posts.
Great! I think other users have suggested how you can disassemble the binary but I'll share the point where it clicked for me. Understanding how traits worked went further than expected, in particularly it gives you a large insight into the standard library and the different patterns that are utilised to achieve. For me, breaking down why and how traits are used within the standard library answers a lot of questions (My "Ah hah!" moment) the language presents to programmers. Initially it helped explain why the **Deref** trait was attached to Box and how you got access to contained object. The Traits themselves help explain behaviour of the struct and allows reading the documentation from what I see as a 'Trait' perspective, looking at what Traits are attached to an object to explain how I can use it. Hopefully this insight sheds some light into your own learning experience.
How would I write completion for this thing? Tт To be fair, that is not worse for IDEs, in general, than other forms of conditional compilation.
Thank you very much. That sounds like an excellent learning approach. Will definitely copy that
There are good and bad thing about "witchcraft". You can make things work easily but one day it becomes part a "enterprise" piece of software and things get weird. At least in Rust you can probably get somewhere close to Python in term of flexibility, but to get there, you pretty much chose your own poison.
Embedded is huge (but spread out), and Rust is tiny in embedded. Embedded includes TVs, stereos, microwaves, thermostats, air conditioners, furnaces, routers. Embedded includes innumerable pieces of equipment in factories, cars, utilities, elevators, stop lights, and much more. Embedded doesn't move fast, and C is still the most used embedded language. There's still lots of resistance against moving on to C++, much less Rust. I'm getting into Rust on a personal embedded project, but I don't think I'll be using it professionally any time soon, at my current job or at one of the many other embedded software positions available. Hopefully the ecosystem nature's and I'll be able to say differently in 5-10 years, though!
He's vegetarian too? I should have known. Glad I didn't continue.
I am so happy I am not an American.
I was hoping they’d celebrate 5 years by releasing robust examples of how to use their crate. But alas the project is still not ready for that. :(
Why not a interpreter? Depending in how you build it, it can be very fast: &amp;#x200B; I ask here about kdb+: [http://lambda-the-ultimate.org/node/5075](http://lambda-the-ultimate.org/node/5075) &amp;#x200B; In short, if you support vectorized operations like: [1, 2, 3] + 1 = [2, 3, 4] you can do all this work INSIDE the host (rust). This mean that you can turn fold, map and filter like functions to operate at efficient speed.
This is very nice! I've been through something like this before... when I was trying to compile cairo-rs crate using MSVC. I remember I installed the cairo lib using vcpkg and then added the "%VCPKGDIR%\\installed\\x64-windows\\bin" to my PATH variable.. after that I remember the compiler was complaining about the .lib files... I tried to add the "\\x64-windows\\lib" folder to the PATH variable, but that didn't work. Then I just copied all the .lib files the compiler was complaining about from vcpkg/lib folder to my project's root folder and everything worked. That was enough for me at the time... &amp;#x200B; But hey, I think it would be great if gtk-rs could have this incorporated to the docs! Thanks!
Did you read the post? gfx-hal is not meant to be a crate that you learn by examples. It's a Vulkan-like API that requires good understanding of how the hardware works. wgpu-rs has [examples](https://github.com/gfx-rs/wgpu-rs/tree/master/examples), did you try those?
Really enjoyed the read and how simple its in rust, btw is it useful in rust context? can generators replace this, does it have any specific advantages over generators?
[Seems possible to change the opt level of all dependencies or individual crates on nightly](https://doc.rust-lang.org/nightly/cargo/reference/unstable.html#config-profiles) [Example](https://github.com/rust-lang/rfcs/blob/master/text/2282-profile-dependencies.md)
Like so? https://github.com/badboy/stdshout
I was interested in adding touch support for GTK on Windows, but dropped it due to the pain of building on Windows via either MSYS or MSVC. I'd still be interested if that were less an issue.
I think `vec![0_u8; SSIZE as usize];` can be refactored into `vec![0_u8; SSIZE as usize].into_boxed_slice();` at which point all your issues with vectors reallocating themselves go away because you cannot grow the slices. Cannot edit code for a few days so commenting here instead.
Well, first you should read the subreddit before posting to it. This is the subreddit for the Rust programming language by Mozilla. You're looking for /r/playrust.
&gt; Should be presumed to be: none. Don't assume immature cryptographic libraries from random people on the internet will be safe. Thank you for being honest. Seriously, thanks. Which protocol does this implement? We have proven-secure protocols now, so using one of those compared to rolling your own is pretty much a no-brainer. I'm partial to WireGuard, which is close to what you describe. The entire thing is ~3k LoC of C and there is a nascent Rust implementation. The protocol has been proven to be secure at least to some degree, I admit I didn't dig too deeply into that.
the fact, that we can't use the GNU toolchain but only LLVM is still a massive handicap for adoption in the embedded field. just look for the numbers concerning the used resp. preferred C compilers and the importance of this simple fact becomes more than obvious.
Thanks! It's not really useful since most of what we do here has already been implemented before. Rust already has [generators](https://doc.rust-lang.org/std/ops/trait.Generator.html) if you enable the right features, and we're actually making a generator here. If we expand a bit on this example we can pass in a trait object like `Fn`, `FnMut` or `FnOnce` to our `yield` function and that will basically be a useful generator. &amp;#x200B; If you look at the `trait_object` [branch of the project repo you'll se an example of this](https://github.com/cfsamson/example-greenthreads/tree/trait_objects), however as you will also notice there is an increased friction when it comes to portability. In the concrete example above windows uses an different register for passing first parameter than OSX/Linux so we need more and more conditional compilation to make this work) and since this is done before there are safer and more portable abstractions available so it's smarter to use them. However, that doesn't mean that you *couldn't* build generators like I show here.
These instructions are nice and a good first step, but we need do better. This is why I've opened https://github.com/gtk-rs/gtk/issues/702 and https://github.com/mcgoo/vcpkg-rs/issues/9 in first place. We need automate this in some way, not sure exactly how. I was experimenting with a solution at the time, but my need of this diminished. It would be great to see this advance.
Elision*
The readme now notes this possibility.
Seems it optimized a bit too much! For this small example, maybe try `-C opt-level=1`? It should be slightly less assembly than opt-level 0, but also not enough optimization to completely remove things like allocation.
&gt; Why not a interpreter? I already have a simple interpreter and I'm trying new stuff. My grammar, like K, cannot be parsed at compile time. I'm trying to see whether Huffman compression of all possible ASTs + bytecode VM or even compiling could yield an acceptable solution.
I think you're right. Combined with using [to\_ne\_bytes](https://doc.rust-lang.org/std/primitive.u64.html#method.to_ne_bytes) for the `u64` instead of pointer casting might make this safer without introducing too much complexity. I'll make an issue for it and have a closer look later.
Vulkan also has at least one tutorial. At some point people need to start learning it. At least there is learn-gfx-hal, and it's good to get started.
I very much doubt that he wants to cross-compile the scripting language into Rust, because the long compilation time of rustc+LLVM is a no-go for scripting languages. The usual approach is to *interpret* the language. For that, you need an intermediate representation, for example enum Value { Bool(bool), Number(f64), String(String), List(Vec&lt;Value&gt;), // these must be wrapped in a type with Object(Obj), // interior mutability, because scripting } // languages never have ownership semantics struct Obj { values: HashMap&lt;String, Value&gt;, constructor: Function, }
For most projects I'd say language matters much less than you'd expect. The language you already know usually beats anything shiny, performant and new. Unless you're looking to learn a new language of course. But if we're just looking at capabilities I'm not sure I really follow your choices here. For the client I'd say Java is not really the best choice. It will get the job done but I think either C++ (traditionally the game programmers language), Rust or something like C#/Unity have better choices of game programming frameworks available. For the server I would actually never go with C++, It lives behind a relatively high-latency lowish bandwidth network connection so performance of the server itself won't be that much of an issue. I would look at a language like Python for fast prototyping here, Initially use HTTP/websockets for the connection and only go to raw TCP when you really need it. You're probably better off trying to offload performance critical/high bandwidth stuff to the client (you have 64 computers there against one lonely server) I know of large MMOG type games running on PHP. But of course Rust would be a good choice for the server side too. Once async/await is finished i'd say its a great choice. But also look at languages like Java, Scala, Go or C#, they would also work here.
cargo considers 0.6 to be a major version.
&gt; The last constant RUNTIMEis a pointer to our runtime (yeah, I know, it's not pretty with a mutable global variable but we need it later and we're only setting this variable on runtime initialization). Are you familiar with `lazy_static`? It's purpose is to do exactly this: initialize static values that can't be pre-initialized at compile time safely. I've learned to **always** replace `static mut` with *anything else.* A `Mutex` or `AtomicPtr` (or `crossbeam::atomic::Atomic&lt;Box&lt;Runtime&gt;&gt;`) would help when modifying it later, or a `thread_local!()` if you're not bothering with multiple threads. (Rust like to keep you safe even in environments you don't plan on using.) And as /u/Shnatsel suggested, using `Box&lt;[u8]&gt;` would be preferable to `Vec&lt;u8&gt;` to prevent reallocations. (`Pin&lt;Vec&lt;u8&gt;&gt;` or `Pin&lt;Box&lt;u8&gt;&gt;` would probably work as well.) If the stacks are all going to be the same size, then you could also declare the stack as its own type and even specify the alignment with something like `#[align(16)] struct Stack([u8; DEFAULT_STACK_SIZE]);`. If they're not all the same size, then you might be able to get away with something similar like `#[align(16)] struct Stack([u8]);`, but unsized types are harder to work with in Rust. A good rule of thumb is "when in doubt, encapsulate in a new type! :D" &gt; let mut pos = self.current; &gt; while self.threads[pos].state != State::Ready { &gt; pos += 1; &gt; &gt; if pos == self.threads.len() { &gt; pos = 0; &gt; } &gt; if pos == self.current { &gt; return false; &gt; } &gt; } Rust isn't very good at optimizing out bounds checks, so `Iterator`s can often have better performance and *much* less code. If you're not using a proper queue, I'd probably implement this as let pos = self.threads.iter() .cycle() .drop(self.current) .take(self.threads.len()) .find(|t| t.state == State::Ready);` (`self.threads.len()` might need to pre-calculated if you run into borrowing issues.) This is actually how I implemented my own scheduler in the initial versions of `dumb-exec`. Using a proper circular buffer would reduce it even further. I suppose `self.threads[self.current..].iter().chain(&amp;self.threads[..self.current])` would work too.
This is fairly similar to the "pass an id everywhere" solution, but one alternative is to use a [slog](https://docs.rs/slog/) logger to store the context. The logger would store the id as context, and each task would own a logger with their own context. Then you'd be able to use `info!(log, "....");`. It's similar to `id`, but with automatically adding context to every log. Slog loggers can also be nested, in that you can have a root logger with some context, then create child loggers off of that root and add different key/value pairs for each child.
Just noting that the video OP links in the original post is definitely about cross compiling _into rust_. I agree that interpreting the language is a more sane choice, but I don't think it's what OP was asking about. Or at the very least, it's not what the video he linked is talking about? See timestamps in the video like https://youtu.be/U3upi-y2pCk?t=1081 - the presenter has written a compiler in rust, but the output of that compiler is also definitely rust code.
Yes. Rust's generators and async/await provide *stackless* coroutines. This means they can't be used with ordinary functions as all their locals are packages into one large state machine implemented like an `enum` with a variant for each yield or await point. It's main advantage over generators is that it can be used for arbitrary functions, including ones written in other languages and ones where the stack usage isn't predetermined.
OP, could you possibly clarify the relation of this post with Rust? As far as I can tell, this is a helpful guide for using a browser automation framework - but I can't find if it uses rust internally, or has any direct application to rust programming.
You just magically acquire that prior knowledge then, I guess?
&gt; What’s the point of the generic annotation in `f&lt;‘a&gt;`? Could `‘a` come from an outer scope? Yes, in fact, it must come from an outer scope. (Note: lifetimes are not actually passed around like values, instead they are used to find constraints on a function's inputs and outputs and pass those constraints to a solver which checks you lifetimes) &gt; I also don’t understand why the lifetime annotations need to be repeated in an `impl`: Once to bring it into scope (`impl&lt;'a&gt;`) and once to bind it to the type (`ImportantExcerpt&lt;'a&gt;`) &gt; The lifetime is already apparent from the declaration. But what if you have constraints on the lifetimes? Like this: impl&lt;'a, 'b: 'a, T&gt; SmartPointer&lt;'a, 'b, T&gt; { ... } Then you would need a way to declare the lifetimes, which is what `impl&lt;'a, ...&gt;` does. &gt; I am also confused about lifetime inference &gt; `// fn longest&lt;'a&gt;(x: &amp;'a str, y: &amp;'a str) -&gt; &amp;'a str` This desugaring is wrong. The arguments would actually be desugared to fn longest&lt;'a, 'b&gt;(x: &amp;'a str, y: &amp;'b str) -&gt; &amp;'??? str But it, can't figure out which input the output should be bound to, so it errors out and makes you decide. You can do this by annotating the lifetimes as you showed. The compiler doesn't even look at the body of the function to resolve lifetime errors like this because function signature should carry all of the necessary information to describe the inputs and outputs regardless of what the body contains.
`'static` can be automatically promoted to any lifetime, since it by definition lives longer than any of them. The inference that `Foo { bar: 100 }` lives in static memory is not affected by the return type of the function or really any future usage of that value.
[RFC 2565](https://github.com/rust-lang/rfcs/blob/121bbeff500c3274cea22c7e0ca176274d592646/text/2565-formal-function-parameter-attributes.md): &gt; Allow attributes in formal function parameter position. What does "formal" mean here?
&gt; What’s the point of the generic annotation in `f&lt;‘a&gt;`? Could `'a` come from an outer scope? Yes, yes it can. Mixing lifetime scope, like mixing generic traiting binding can be scoped, and can yield confusing results. &gt; I also don’t understand why the lifetime annotations need to be repeated in an `impl` They don't as of `rust-edition=2018` _correct me if I'm wrong_. This is lot like mixing generics. When `impl&lt;'a&gt;` is declared it is an explicitly lifetime for the `&amp;self`/`&amp;mut self` parameter within that scope. [Here is an example](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=cfe5ff52f9c3ef2b586d113c7e1ba40e). In the example we assert the borrow of the generic `S` must live as long as the type the `KeyValue` that it interacts with, as the lifetime is scoped. [Here you can see what happens if you mix lifetimes](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=59b5675343bfe62c6d7d7eda37eeaa1d) &gt; I am claiming that at least a substantial subset of lifetimes are inferrable, and given how challenging the borrow checker is for programmers new to Rust, it strikes me as a feature the Rust team would want to prioritize. So what’s the story? [It is in the language](https://doc.rust-lang.org/reference/lifetime-elision.html) Edit 1: bad example wrong link
Yeah some of that would be hard to do with rust. Another example is zlib where the main code is still written with K&amp;R C syntax. Another challenge as mentioned earlier by this blog is the fact that C lets the library user specify the memory allocation functions, which is a bit iffy with rust. That said there are rust libraries for zip and several for the underlying deflate/zlib compression now.
You're looking for /r/playrust
You're looking for /r/playrust
I found myself in your position not so long ago while porting a piece of Java code to Rust. A friend gave me a solid advice WWHD (What would Haskell Do). In this case instead of multiple structs and a `Field` trait I would have a `Field` enum: pub enum Field{ IntField(u64), StringField(String), ImageField(Vec&lt;u8&gt;) }
&gt; They don't as of `rust-edition=2018` correct me if I'm wrong. You can do this impl Foo&lt;'_&gt; { ... } impl Trait for Foo&lt;'_&gt; { ... } But if you want to name that lifetime, you must declare it at the `impl`
This is awesome! I almost didn't believe green threads could be made this simple - but here it is.
One alternative is https://github.com/NieDzejkob/enumflags2 - it uses a proc-maco, but it might still be better for autocompletion? I don't know your specific use case, but I rarely notice the slow compilation of `proc_macros` after the first compilation. It's definitely slow for the initial compilation but after that it shouldn't have much of an effect?
Sounds like something the IntelliJ Rust plugin could improve on! I believe it could get access to the syntax tree after macro expansion using something similar to `cargo expand`. Maybe file an issue with them about it?
ah, apologies, am inexperienced with reddit
What I would like to see is a Rust graphics crate *somewhere* with on the order of 50 examples. Comparable to bgfx or maybe the ancient Nehe tutorials. With some depth along the lines of Humus. I would strongly argue that a crate with a Vulkan-like API should have examples of how to use it’s similar-but-unique API. But it you want to gatekeep and say Vulkan is complex and if you can’t figure it out then too bad — well, I guess? Maybe in 5 years Rendy will be awesome and have those examples. That’d be nice. In the meantime gfx-rs is 5 years old and the “shiny examples” look... I’m going to bite my tongue here. Yes Vulkan/gfx-hal is hard and for experts. That’s... fine! Things for experts can still have examples. Easier-to-use things can have examples too. Rust is generally an open and inclusive community. I think the exclusionary gatekeeping is unnecessary and a huge net negative.
Thanks for the comment. I considered working around the `static mut` but if I'm not mistaken a `Mutex` will deadlock since we lock it just to read it in the `yield` function and the lock isn't released since we switch context while holding the lock. There are safer ways though but they all ended up with a pretty big increase in lines of code to parse just to make it safer and I found it taking focus away from the main logic I wanted to convey. The same with the iterators, even though I love them I think the subject matter is complicated enough and wanted to use them carefully here since I know a lot of people have a harder time parsing them. [/u/Shnatsel](https://www.reddit.com/u/Shnatsel/) suggestion is very good though, and the same is making the stack it's own type so I'll probably investigate this a bit closer and consider updating the book with them. However, I have an idea to make second book later with "a better implementation" where I try to make the code safer, faster, support `Fn()` type closures (which is already solved in the `trait_object` branch of the repo) and last but not least dive deeper into some subjects I have planned and all these suggestions will be useful there.
Ok, thanks for highlighting this restriction. I also had a look at one of your [unit tests](https://github.com/c410-f3r/rust/blob/1eaaf440d5173f090d6e937f4b4cffec6c038984/src/test/ui/rfc-2565-param-attrs/param-attrs-allowed.rs) which makes this clear. The examples in the RFC text hints that this will eventually be supported if I am reading it correctly? For my purposes I don’t need support for using procedural macro attributes in function argument position but instead something akin to [helper attributes](https://doc.rust-lang.org/reference/procedural-macros.html#derive-macro-helper-attributes) from custom derive. Is this likely to be supported in the future?
I thought I was ready, got scared on the third page.
&gt; TCP protocol will be used, since the game is slow paced (not sure though). Would you consider using a combination of UDP and TCP? Consider that TCP protocol enforces that every packet must arrive in-order, and how important latency is compared to data loss. For example, with UDP, if the server sends a position update every 0.1s, you can lose one packet and often interpolate on the client side fairly accurately. However, if you use TCP and your client has a 100ms latency, you might introduce delay, since the TCP protocol is trying to ensure that packets are received in-order, and one was lost. You then have to wait for the original packet to be re-sent. In this case, the latest data is much more important than that old data.
It's not an issue! Just remember to always check the sidebar for the subreddit rules before posting.
thanks!
You could learn Vulkan
I think the idea is that gfx is no longer a crate to be used for graphics, it's a crate to be used by crates used for graphics. You shouldn't need that prior knowledge because you shouldn't need to use gfx-hal. Rendy and wgpu-rs are the new crates occupying the space gfx used to occupy. And they don't require that knowledge, I think?
&gt; The methods could be marked with `#[inline]` or something. Sorry for being somewhat off topic but, how do you know when to mark a method with `#[inline]`? From what I've read online, I think I know what it does, but shouldn't the compiler generally take care of inlining methods for you? I always see the inline marker when browsing the std lib code and was wondering what criteria they use to decide "this method should always be inlined"
Thanks, Actually I was really asking about cross-compilation. I am writing an array language, so not really a scripting language but definitely not a systems programming language either. APL and the like are usually interpreted, but the Co-dfns compiler made quite a splash recently by hosting itself and compiling to gpu code. The compiler target is c++. I'm far less ambitious, though :-)
Yeah, and how? By examples I hope.
Thanks, &gt; You'd still be constrained by the borrow checker though, and I'm not sure if you actually want That's precisely what I want for the target but not the source, because I'd like to avoid writing memory management code. Reference counting seems to check those boxes, so that's why I asked.
I’m just tongue in cheek teasing. Every time I played around with sharding and mongo it didn’t do what I expected. But that was a long time ago. I prefer Postgres and redis for storage but to each their own. :)
So does gfx-hal, https://lokathor.github.io/learn-gfx-hal
Probably not to be honest, there are so many concepts you need to learn that you need to learn from a book, tutorial, or class rather than just from examples. Edit: I know this might sound gatekeep-ey but I promise it really is not. There's a reason we don't recommend people to just read some Rust examples if they want to really learn Rust; a reason why the Book exists. Yes, Rust By Example exists too, but it is much more than just a collection of examples; it's pretty much a book on its own, just in a different format. Vulkan/low level GPU programming is very much on the same level of learning Rust, perhaps even more of a burden, as now the memory management and synchronization is *highly* parallel, happening on two different devices which often share only pieces of memory, and in the case of gfx-hal, the Rust borrow checker can't help you to statically guarantee that you don't screw up.
No one said to "just" use examples. Examples are a necessary addition to properly teach concepts.
The old (pre-ll) gfx examples, which are now being ported to wgpu, numbered in the 20s I believe. It would be cool to have the equivalent of Sascha Willems Vulkan examples library, but the things is that if you can understand the Sascha Willems Vulkan examples then you should be able to relatively trivially migrate that knowledge to gfx-hal, and so the priority for most of the gfx team (which, btw, is only a couple of people that contribute regularly) not been on creating examples for something that would basically be repeating what already exists, but rather on improving gfx-hal and wgpu on top of it, and making wgpu more friendly and user-facing. &gt; I think the exclusionary gatekeeping is unnecessary and a huge net negative. I don't think that's what's going on here at all, it's just that the division of labor/prioritization of work is different than you believe it should be... Finally, [learn-gfx-hal](https://lokathor.github.io/learn-gfx-hal) exists, and hopefully, [learn-rendy](https://github.com/termhn/learn-rendy) will soon be made to be more than a surface-level introduction, though as I'm currently working two jobs, trying to find time for it is a hard prospect. There are a couple basic-ish examples in the `rendy` repo (though they are slightly hidden under the `rendy/examples` folder), and [rendy-pbr](https://github.com/termhn/rendy-pbr) is as project I made to serve as a more feature-complete example of actually using rendy in a more production-like environment.
The way I see it, Windows users using `vcpkg` to acquire GTK development libraries is the spiritual analogue of Linux users having to install `libgtk3-dev`. The only reason I have to include that in these instructions is that even consumer-targeting Linux machines are closer to being developer tools than Windows machines; on Linux, development headers are in the same package manager as everything else, while on Windows you have to install and use a different tool. The rest is working around bugs (mostly Microsoft's) so I don't think there's much to automate here. The problem as far as I'm concerned is that Windows is such a second-class citizen in the world of GNOME. The `win32` theme was unmaintained for the entirety of GTK3's lifetime until it was taken out behind the shed. So far as I can tell there are no Windows builds provided of *any* GNOME development tools, most notably Glade. The message is pretty clear to me: if you want to use GTK, get a Linux box.
I mean sure, but in this case they will be built in to whatever the teaching medium of choice is, not just a collection of code in the root git repo.
Prioritization of a small team I completely understand and support. &gt; gfx-hal is not meant to be a crate that you learn by examples. It's a Vulkan-like API that requires good understanding of how the hardware works. Imho, that comes across as pretty exclusionary. I mentioned the lack of examples a year or two ago and got a pretty similar response. Ideally I think the gfx crate would have both robust examples and automated tests that leverage them. https://aras-p.info/blog/2011/06/17/testing-graphics-code-4-years-later/
I don't think it seems exclusionary at all. It's just saying that it's not practical to learn the things you need to understand from examples, and that another medium (possibly accompanied by examples specifically meant to be used with that medium) would be better. See my response to another post https://www.reddit.com/r/rust/comments/bzrgl3/5_year_anniversary_of_gfxrs/eqyqeo7/
&gt; Yes, in fact, it must come from an outer scope. (Note: lifetimes are not actually passed around like values, instead they are used to find constraints on a function's inputs and outputs and pass those constraints to a solver which checks you lifetimes) I don't mean the lifetime itself, I mean the _name_ `'a` we are using to establish the constraint which we have already provided in the annotation of `f`'s argument. Why provide it twice? I speculated that the answer is because the name `'a` in fn foo(x: &amp;'a i32) -&gt; &amp;'a i32; is _still_ ambiguous, which I think is what you and u/valarauca are saying. ~~Do you have an example? Maybe something like this:~~ Edit: No, this example fails because `g` does not capture from its surrounding scope in this case. The example I was trying to make was something like what u/valarauca13 [provided](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=cfe5ff52f9c3ef2b586d113c7e1ba40e). fn foo&lt;'a&gt;(x: &amp;'a i32) -&gt; &amp;'a i32 { // ╰━━╮ The 'a below is the 'a introduced above. fn g(y: &amp;'a i32) -&gt; &amp;'a i32 { // Some stuff } } But that example gives a "use of undeclared lifetime name" error. &gt; Once to bring it into scope (impl&lt;'a&gt;) and once to bind it to the type (ImportantExcerpt&lt;'a&gt;) Both of which are unnecessary, because the compiler already knows about those constraints—they can _only_ be those constraints supplied in the declaration. By which of course I mean that the requirement does not come from logical necessity for the compiler to know how to translate the program. Correct? &gt; But what if you have constraints on the lifetimes? Like this: &gt; &gt; impl&lt;'a, 'b: 'a, T&gt; SmartPointer&lt;'a, 'b, T&gt; { ... } Ah, I would have thought the constraint `&lt;'a, 'b: 'a, T&gt;` would have to be declared in the declaration. I will have to meditate on this. It's strange that you can further constrain the lifetimes in the `impl`. Could you omit the lifetime annotation in the declaration altogether and only have it in the `impl`? &gt; This desugaring is wrong. ... &gt; The compiler doesn't even look at the body of the function to resolve lifetime errors like this because function signature should carry all of the necessary information to describe the inputs and outputs regardless of what the body contains. It's not a desugaring, it's the result of unifying the lifetimes of `x` and `y`, which I understand the compiler does not do. My point is that it could, at least in principle, in which case the programmer supplied annotations would be unnecessary (and presumably less frustrating for the programmer).
Thanks I got the Idea to use filter_map from https://doc.rust-lang.org/rust-by-example/error/iter_result.html#ignore-the-failed-items-with-filter_map I assumed the filter by option is some was also the map
I admit that my reply was harsh, could have been much softer, sorry about that! Thanks /u/termhn for clarifying the message, I hope it's clear now that we don't mean to exclude anybody. If we are talking about robust examples and automated tests in the context of a good graphics library, then we largely get this via the gfx-portability layer: we run and benchmark a large number of Vulkan native applications, especially on Metal, including Dota, Dolphin games, and Sascha Willems examples. Most importantly, we run Vulkan conformance through it, which is the largest test suite you can find for a graphics library. Besides, gfx-hal has had a small testing framework (called `warden`) from early days, which is a soft requirement for contributors to run before making a PR.
&gt; Here is an example. In the example we assert the borrow of the generic S must live as long as the type the KeyValue that it interacts with, as the lifetime is scoped. Here you can see what happens if you mix lifetimes Yes, that's the example I have been trying to construct. This makes sense. &gt; [It is in the language](https://doc.rust-lang.org/reference/lifetime-elision.html) No, that's child's play. The compiler could do much, much more. I'm talking about something like the type inference in Haskell or OCaml. The example from The Book that I quoted in my original post is a great example. The programmer does not have to annotate the lifetimes of `x` and `y` (mathematically speaking, not according to the compiler) because there exists a unique most general unifier of the lifetimes of `x` and `y`—there is only one thing it _could_ be, as the author points out: &gt; “...the lifetime of the reference returned by the longest function is the same as the smaller of the lifetimes of the references passed in. ” Thanks for the help!
Don't confuse 5 years of stability and growth in a wide ecosystem (that is much older) with our 5 years of constant experimentation and API exploration in a small Rust ecosystem that had almost no prior history. Heck, Rust wasn't even 1.0 when we were full-steam rolling! Overall, our evolution and goals appear to be quite different from "bgfx", so comparisons aren't useful. I'll be happy for us to get to a point of having 50 examples. With an API that we are confident about, and with your help. gfx-hal is not an API that needs examples in particular: once you understand Vulkan (which has tons of learning resources online) and you see one example in gfx-hal, you can pretty much port any other. That's a deliberate decision (prioritization!) we made when we decided to stick with Vulkan-like API, because otherwise working on documentation and education would be much more difficult.
I'm working on a CLI tool that involves setting up websockets and an HTTP server. I've got working code, but I'm stuck with an awkward 50 line god function that does all the setup. I'd like to move the websocket config out into a separate function, but I'm stuck because I don't know how to work with closures, specifically returning from a function. The snippet I'd like to extract is this: let shared_styles = std::sync::Arc::new(styles); let initial_html = std::sync::Arc::new(parse_file(&amp;opt.file)); let websocket = ws::Builder::new() .build(move |out: ws::Sender| { let cloned_content = initial_html.clone(); let cloned_styles = shared_styles.clone(); move |_| { let initial_message = Update { content: Some(cloned_content.to_string()), stylesheet: Some(cloned_styles.to_string()), }; let serialized = serde_json::to_string(&amp;initial_message).unwrap(); out.send(ws::Message::text(serialized.to_string())).unwrap(); println!("Connection established"); Ok(()) } }) .unwrap(); [(full code)](https://gist.github.com/adamhammes/c65f946f6a9300f7a403d24a05fda9c1) If I could move the `websocket` construction into a different function it's make my code a lot cleaner. However, the inferred type is weird: ws::WebSocket&lt;[closure@src/main.rs:71:16: 86:10 initial_html:std::sync::Arc&lt;std::string::String&gt;, shared_styles:std::sync::Arc&lt;std::string::String&gt;]&gt; and I'm having trouble finding the right return signature. This is my best effort: fn build_websocket() -&gt; ws::WebSocket&lt;impl FnMut(ws::Sender) -&gt; Fn(ws::Message) -&gt; ws::Result&lt;()&gt;&gt; { // copy paste of code above websocket; } This fails with the error: the size for values of type `(dyn std::ops::Fn(ws::message::Message) -&gt; std::result::Result&lt;(), ws::result::Error&gt; + 'static)` cannot be known at compilation time and I have no idea how to fix that :-( Any ideas?
Ah, so you want global inference for lifetimes. This has been explicitly denied because the lang team wants to be able to locally reason about code. Global inference makes this impossible.
&gt; No, that's child's play. The compiler could do much, much more. I'm talking about something like the type inference in Haskell or OCaml. The example from The Book that I quoted in my original post is a great example. I do not believe that'd be decidable at compile time without GC.
&gt; fn foo(x: &amp;'a i32) -&gt; &amp;'a i32; Ok, this looks reasonable. The only thing I have against it is that it is inconsistent with type parameters (because lifetimes are semantically very similar to type parameters). You could write up an RFC for this, although there may already be a proposal for this. I think I've seen it before, but I don't remember where. &gt; Ah, I would have thought the constraint &lt;'a, 'b: 'a, T&gt; would have to be declared in the declaration. I will have to meditate on this. It's strange that you can further constrain the lifetimes in the impl. Could you omit the lifetime annotation in the declaration altogether and only have it in the impl? You could shove the constraint into a `where` clause, but that seems weird.
You're looking for /r/playrust/
Ah thanks
Oh my gosh, I got it! fn build_websocket( content: Arc&lt;String&gt;, styles: Arc&lt;String&gt;, ) -&gt; ws::WebSocket&lt;impl ws::Factory&lt;Handler = impl ws::Handler&gt;&gt; { This `impl` thing seems pretty nifty :-)
`file`'s [code execution CVEs](https://www.cvedetails.com/product/2869/File-File.html?vendor_id=1661) surprised me. But none have come up in a decade. IIRC, some were due to its reliance on other libraries to parse ELF headers. So I imagine oxidizing `file` may require the same for a few libraries which do some heavy lifting. [afl-fuzz has 4 trophies for file](http://lcamtuf.coredump.cx/afl/), but most of those links are 404s ATM. archive.org says the most recent afl-fuzz trophy for `file` is from [2015](https://web.archive.org/web/20170707133142/https://bugs.gw.com/view.php?id=459) The highest item in the [results table](https://github.com/coreinfrastructure/census/blob/master/results.csv) you linked to, `libexpat1`, is interesting. My day job sometimes requires some XML wrangling.
The `uom` crate from this week looks pretty useful for scientific computing. Will definitely be using it in the future.
Honestly, the deadlock does seem inevitable due to inherent unsafety with the system as it's designed. It would probably take quite a bit of effort to avoid the `static mut` with the way out is now.
&gt; For example, `' '`, `'*'`, and `'|'` are treated specially by the shell, and `':'` is used as a path separator in environment variables. `unf` renames these files, **so you no longer have to be annoyed when your Windows-using friend sends you an irritatingly-named zip file.** Thats funny, considering that `*`, `|`, and `:` are all invalid on windows.
That's amazing. And it makes sense too for how rust works. Since Box will deallocate memory when it leaves scope, the compiler saw malloc() write read free() and turned it into a constant
I'd imagine that reading large portions of the specification would also factor heavily. It isn't exactly light reading, but it (obviously) does go into great depth on exactly how things work in Vulkan.
Never encounter this situation before. But this crate is nice.
 &lt;script&gt; window.location = "https://dev.to/endtest/a-practical-guide-for-finding-elements-with-selenium-4djf"; &lt;/script&gt; Why use aws when the page itself is redirected to `dev.to` ?
bools are smaller than a pointer (1 byte vs. platform pointer size, usually 64 or 32), so you should prefer passing them by value. To compare them, just dereference the &amp;bool. I'll let someone else answer why rustc doesn't do it for you v- I think I prefer being explicit about but I'm not sure of any officially
This gets my vote not only for being great content but for sticking with the std library! Brilliant stuff
Essentially, the `core::future::Future` trait will become the official one. This trait will only expose a single method, `poll()`, and will be used to provide native async/await language-level support support. `futures` 0.3 will drop its own `Future` trait and expose several new traits, `FutureExt` and `TryFutureExt`, that extend the `core::future::Future` trait with common combinators like `map()`, `map_err()`, and so on.
Just to be clear, pointer sizes are either 8 bytes (64 bit) or 4 bytes (32 bit). `bool` is 1 byte (8 bit) I'm pretty sure rustc will do this for you if you don't cast the reference to a `usize`
Sorry, that's exactly what I meant with the sizes. Edited for accuracy
Sorry, spaces are the issue. The others are provided for the sake of example.
I'm uninformed. What's wrong with Discord's ideology?
Definitely.
The compiler _should_ inline them, but might not. I think it doesn't inline non-generic functions across crates, which is where the attribute comes in handy. There's no single rule of thumb for using it, but here's one: a non-generic newtype wrapper that delegates its methods to those of its inner type, especially if you don't know it won't be used across crates. Another way to know you should mark a function inline is by running a benchmark that indirectly involves it, or by looking at the generated code. Or you can guess, but it's hard to guess. There was an article about how disabling `inline` in the Linux kernel actually improved performance.
How does it compare to CurveCP?
You talk about having good faith conversations, but then claim a lack of time and sophistry when confronted with the fact that you're engaging in the same kinds of behaviors you claim to be taking a stand against. You've clearly rubbed a lot of people the wrong way, and not because of a celebration of pride, but because you used it as a platform to personally stereotype "cishet" males. And in doing so, you gave people legitimate reasons to oppose these sorts of things in the future. As it relates to the "Eh, probably not meant to apply to me" attitude, something tells me there's not a clause for that in the CoC. The only part of the CoC that seems relevant to this topic is the following: &gt;In particular, avoid flirting with offensive or sensitive issues, particularly if they're off-topic; this all too often leads to unnecessary fights, hurt feelings, and damaged trust; worse, it can drive people away from the community entirely. I'd argue that your (or anyone's) grievances with "cishet males" apply here.
I think the Noise family of protocols is the standard to beat here (it is what Wireguard uses, for example). What distinguishes this from that?
Ah, thanks!
Where did you get that from? The cargo reference specifically links to here: https://github.com/steveklabnik/semver/blob/master/README.md
VS Code, for most under-the-hood purposes, is a faster and more powerful Atom. VSC is noticeably snappier in my usage, and its intellisense is pretty incredible without bogging down your system (VS) or even chewing through your battery (JetBrains). Don't really use either tbh, I quite like Spacemacs (even if it makes Emacs slow as shit to start up)
Hi, I have a pic running a ryzen 5 2600 + rtx 2070 and I'm only getting upwards of 60fps on max settings. Help?
Hi, this is Devon from the GitHub Sponsors team. :) TLDR we believe that companies should fund open source, and we're actively building tools to make that happen. I 100% agree that (a) companies are the prime beneficiaries of open source and (b) the bulk of funding should come from them. I'd go even further: it's a no brainer for companies to support open source, because it's the infrastructure they depend on to run their business. They just don't have a great suite of tools to do so right now. With that in mind, today's GitHub Sponsors is just an MVP. We have a lot more to build, and in fact **corporate sponsorships is our primary focus this summer**. So you hit the nail right on the head! We started with individual sponsors because it is a key building block to build out more complex sponsorship relationships moving forward. You might enjoy reading the "philosophical FAQ" we recently published: https://github.blog/2019-06-12-faq-with-the-github-sponsors-team We’d love to hear your ideas as we build it out. Send me an email -- devonzuegel@github.com.
sorry to break it to you, parens are valid filenames in unix :P this works fine: &gt; echo hello &gt; \\( &gt; cat \\(
Hi, this is Devon from the GitHub Sponsors team. That's exactly how we've been thinking about it! Responded to the OP here: https://www.reddit.com/r/rust/comments/by9qdi/github_sponsors_maintainer_spotlight_sean_griffin/eqzlb0f
I think the point is dealing with "annoying" filenames, not invalid filenames. Linux (and, I assume, Unix) can have crazy filenames, including embedded newlines (and probably weirder stuff as well). But they're a huge pain to work with in the shell.
The only things which aren't valid in a posix/unix filename are the bytes 0x00 (NUL) and 0x2F (/). TFA is about things which are *annoying*.
On most Linux filesystems, all characters except the path separator ('/') and the null-byte are valid in filenames. This also includes, for example, newlines. The purpose of the the author's tool is to remove characters which need escaping in a shell, I believe, not removing invslid characters (as those cannot be present on the filesystem to begin with).
Yep, that's right! It's for each developer's first year. For example: If Alice joined on 1 June 2019, she'll get the match until 1 June 2020. If Bob joins on 16 Sept 2019, he'll get the match until 16 Sept 2020. We specifically wanted to avoid the issue /u/theZcuber pointed out with the beta limiting how much folks can get. That would've been a huge bummer! Also hi this is Devon, the person from the Sponsors team who designed this policy. :) Happy to answer any other questions/feedback you have.
&gt; bools are smaller than a pointer (1 byte vs. platform pointer size, usually 8 or 4), so you should prefer passing them by value. OTOH there are many generic methods for which you can't do that e.g. ::filter pretty much always gives you an `&amp;T`.
It can inline across crates if you have lto enabled I think, but don't quote me on that.
I'm sorry to hear that's your experience! Increasing opportunities to participate in open source internationally is at the core of the mission for this project. Concretely, we prioritized getting broad geographic representation in the beta, and we put a ton of work into supporting payouts to individuals in every country where GitHub does business, so I'd love to hear about what we can do to improve. My email is devonzuegel@github.com.
I would say that if a function is just one or two bitwise operations then it should probably always be inlined. The compiler will most likely do if anyway. Also the #[inline] attribute is just a hint, the compiler can ignore it if it thinks not inlining would be better. You also have #[inline(always)] which as far as I'm aware is just a stronger hint. Really though, you should benchmark with and without and check the output to make sure it actually has been inlined.
FYI espressif have a fork of llvm which they're building out for esp32 chips, and someone's built a semi-working rust compiler fork for it (it gets to das blinkenlights). espressif staff have also made comments in the forums that they're looking to support and IDF-style embedded framework for rust at some point in the future.
I know that we rehash this every now and again, so I'll drop my somewhat more unique $0.02: Rust's superpower for your situation is Cargo. The ease of dependency management you get with cargo will make up a fair chunk of lost time tweaking C++ to get the dependencies you need for simple things like networking. Getting setup with Rust is super easy: Rustup, VS Code, and the plugins listed on areweideyet. Once you get that I reckon you've got a pretty well working environment to just start doing your thing.
Going to disagree with you about C++ and Rust necessarily being better than Java for building a client. Sure, they might have advantages, but both have strong disadvatanges as well. Java isn't bad for client development if it fits in your constraints! I can't comment on C# - it could very well be nicer than Java in the same space. Agreeing with your comments on the server side of things, though.
It can, but as a crate author you shouldn't be relying on your users to enable LTO. Plus, it's more complicated, as there are two kinds of LTO, one better and slower, and a faster one. I'm not sure what's enabled by default and what's not, but unless told otherwise, `rustc` will prefer the faster one (ThinLTO).
Do I ask dumb questions or something? This(the decision tree used to select the return type, not that every return should be the same) is something that should be standardized across APIs.
I used &amp;[thing, thing, thing].into_iter() that creates a slice::Iter or something. I got the idea after looking at the implementation for args().
The behavior of cargo, as is widely relied upon.
&gt; not removing invalid characters Plus posix filenames are just bytes, there's no notion of characters. Most software just interpret the bytes as UTF8 encoded, there's just no guarantee that it is (which is one of the reason Rust very much splits out strings and "OS strings").
You linked to the documentation of the [futures 0.1 crate](https://docs.rs/futures/0.1.27/futures/), but the [Future trait in std](https://doc.rust-lang.org/beta/std/future/trait.Future.html) is similar but not the same. It *is* the same as the Future trait in the [futures 0.3 crate](https://rust-lang-nursery.github.io/futures-api-docs/0.3.0-alpha.16/futures/) and will work with that. The upcoming async/await stuff is also based on `std::future::Future` or futures 0.3. The major difference is the [whole pinning stuff](https://doc.rust-lang.org/beta/std/pin/), which was neccessary for async/await support. The [poll() method](https://doc.rust-lang.org/beta/std/future/trait.Future.html#tymethod.poll) now does not take a plain `&amp;mut self` but [`Pin&lt;&amp;mut Self&gt;`](https://doc.rust-lang.org/beta/std/pin/struct.Pin.html) (and an extra `Context` argument). I'm still trying to get my head around that myself. The tokio team is working on [converting tokio to std::future::Future](https://github.com/tokio-rs/tokio/commits/std-future) as well.
[Nervos CKB](https://github.com/nervosnetwork/ckb) uses PoW
&gt; 0.0.x is not considered compatible with any other version.
Try asking in /r/playrust
In that same code, you could remove line 3 :)
Thanks for the answer, I am more into TCP cause it was easy to understand, just read/write data in loops. Before posting the topic I've also read quickly about laminar by searching "reliable udp rust" without falling into details. Maybe I have to read more about that specific library or UDP in general. But, for example, if someone will use laminar/any protocol on top of UDP for his server, won't clients have to use laminar with Rust too? Or it won't be hard to process laminar-specific packages on Java client with raw UDP api?
`let x = &amp;true == true` doesn't compile because there's no PartialEq&lt;T&gt; implementation for &amp;T.
That's another part of the change/risk adverse nature of embedded. LLVM is a great compiler and has a much cleaner and easier to certify design than GCC, but GCC has so much "proven in use" that it's hard to convince people to switch.
&gt; The only things which aren't valid in a posix/unix filename are the bytes 0x00 (NUL) and 0x2F (/). Are you sure about the first? I kinda remember that NUL is actually valid, but of course it wreaks havoc with any C code trying to handle it.
&gt; Are you sure about the first? Yes, POSIX filenames (and path names) are specified as NUL-terminated. As per OpenGroup's Definitions: &gt; [a filename is] A sequence of bytes consisting of 1 to {NAME_MAX} bytes used to name a file. The bytes composing the name shall not contain the &lt;NUL&gt; or &lt;slash&gt; characters. In the context of a pathname, each filename shall be followed by a &lt;slash&gt; or a &lt;NUL&gt; character; elsewhere, a filename followed by a &lt;NUL&gt; character forms a string (but not necessarily a character string). *Filesystems* may not have this limitation, and the API may not be quite POSIX-compliant, but POSIX-wise you can't have a NUL in the middle of a filename. Likewise `/` incidentally, IIRC some filesystems do allow it in directory entries.
It's a matter of developer resources. If someone cares enough about using GTK on non-Linux, the GTK maintainers would be happy for any help that could be provided. These things don't happen automatically, someone needs to do the work, and almost all of the current GTK maintainers are exclusively running Linux.
This happened https://github.com/gtk-rs/gtk-rs.github.io/pull/111 :)
This happened https://github.com/gtk-rs/gtk-rs.github.io/pull/111 :)
Ha ok, thanks for clarifying that :) Now I need to find out how to coerce my filesytem into making such a file to bug our admins :D
Honestly, writing a server in C++ can be perfectly fine. The reason it becomes complicated is because people tend to over-optimize, and prematurely. If you use Boost.Asio for the networking code, and are prepared to deal with the cognitive load of understanding that model, you'll likely be just fine. In Rust, you would use the Tokio framework instead, which also comes with a pretty significant cognitive load, albeit with more help from the type system. In C++, asynchronous code must be written using callbacks / completion handlers, while in Rust you will soon have the option to use async/await (in nightly today). Async/await may be easier to follow, or it may be mind-bending, YMMV. I think the main advantage of Rust will be in the area of "general productivity" - Rust solves many problems that you need to be actively aware of in C++, and dealing with Rust's `Result` is a much easier task than C++ exceptions. If you think C++ exceptions are easy, you have not understood C++ exceptions. ;-)
I just read Vulkan spec 3 times. I return to it to check small details. Besides that I experimented a lot.
Always quote your filename strings (or use a more explict language than shell scripts), and you'll be glad of the flexibility of unix filenames over windows.
https://doc.rust-lang.org/edition-guide/rust-2018/ownership-and-lifetimes/inference-in-structs.html The suggested `T: 'a` gets inferred now.
Let's say I have a `String`, coming from another thread. This string is 40 characters long (it's ensured it's always ASCII) and I want to limit it to 30 characters. What's the fastest way to do this?
I think, my dear sir, that it best if you simply move on from this topic; you won't find the conflict you're looking for here. The furor has died down, the communities came through relatively unscathed (we've actually had an increase in people joining specifically because of the stance we've taken), and I've had a shockingly high amount of constructive dialog with people. A few people even changed their minds as a result of the interactions here, which I rarely see on the Internet. (Don't worry, I'm sure we'll have the opportunity for more exciting conversations the next time you pop up to comment on a controversial issue.) That is all. :)
I have a couple of thoughts on this that I hope I can help flesh out. Any solution I propose would probably be based on: [https://github.com/don/cordova-plugin-ble-peripheral](https://github.com/don/cordova-plugin-ble-peripheral) I have used don's cordova ble plugins before (both server and client) and I think covers most things off.
Thanks, I should have caught that when correcting the first comment :) It's fixed now though.
The problem is that LLVM doesn't support many embedded targets, and even for the targets that LLVM forks support, you have to switch \`rustc\`'s LLVM library with that fork, which might be based on a quiet outdated LLVM version, and therefore not work. &amp;#x200B; If you can't generate code for your hardware with rustc, you can't use Rust for embedded programming. It's that simple.
Better look in /r/playrust
Wrong subreddit [r/playrust](https://www.reddit.com/r/playrust/)
Thanks dude
Ossuary implements its own protocol, which is very similar to but much simpler than the Noise/WireGuard protocol. Comparing Ossuary to Noise: 1) Noise is a Real Thing™ used by real cryptographers. Ossuary was scribbled on a napkin by some dummy. Use Noise. 2) The Noise protocol supports multiple modes of operation and swapping and negotiating crypto primitives. Ossuary (intentionally) supports none of that. This leads into: 3) Noise libraries expect you, as a user, to understand the Noise protocol. You pick your curves and hashes, you parse packets off the wire, you poke its state machine in the right direction. The very first thing they do in the Snow example is pass it "Noise_XXpsk3_25519_ChaChaPoly_BLAKE2s". Hoo boy! Ossuary goes the other way, where you don't need to know a single thing about the underlying protocol, and there aren't any big decisions for the user of the API to make. You just use it or you don't. 4) Ossuary is pretty small as Rust libraries go and pretty small as C libraries go, and builds them both from the same source, which is nice. Where I see Ossuary fitting into life is in the cases where you want to do some really simple network communication, it would be swell if it were encrypted, but it's not worth the mental effort of using a *real* crypto library. I literally built it so I can change the playlist on my alarm clock. I figured 6 months of writing a crypto protocol from scratch was easier than using something else.... ;)
Agreed! Java on the client side is still perfectly fine, Minecraft, one of the most popular recent-ish games is made in Java. I think I just gave too many examples of languages that kind of buried the two points I was trying to make. If you're looking for performance i'd try to go closer to the metal on the client first. Not on the server. Thats why I gave Rust and C++ as examples. But yeah, java performance is also perfectly fine. On the client side game-framework availability is important. That's why I brought up C#/Unity there. I'm sure you can find decent game-programming frameworks for any language so no reason not to choose Java if that is your tool of choice.
Will potentially adding \`map()\` to \`core::future::Future\` in the future be a breaking change? Sounds like an operation every library would re-implement?
You can deference a value without taking its ownership.
\&gt; Coverage tools are not very popular in general, and Rust is no exception. Most developers (87%) don't use them, while some others use codecov (12%) and very few use gcov (1%). &amp;#x200B; Most people use tarpaulin / \`cargo-cov\`
Thanks for the suggestion; sounds good to me!
Yes, it would be considered a breaking change. But why would you need to reimplement? Users only need to implement `core::future::Future` with its lone `poll()` method. If you wish to use combinators like `map()`, you only need to add `futures` 0.3 to your dependences in your Cargo manifest, which includes a blanket implementation of `FutureExt` for all futures and `TryFutureExt` for all futures that return a `Result&lt;T, E&gt;`. The idea is to promote async/await as the standard core primitive for working with futures, no combinators necessary, along with regular Rust language features like `?`. If you still want to use combinators, however, adding `futures` 0.3 to your dependences and importing `FutureExt` and `TryFutureExt` into scope will restore the current behavior.
Accessing the keys maybe and adding new entries to the hashmap would cause data collisions for sure, I presume?
I literally mentioned it in my comment.
Adding or removing keys requires changing some things around in the hash map's internals, and those changes aren't thread-safe. That's why you need a mutex to make sure that no other thread tries to read it while the change is incomplete. When all you're doing is atomic get/set on the keys, nothing else needs to be synchronized.
How is this superior to an `Arc&lt;Mutex&lt;HashMap&lt;id, bool&gt;&gt;&gt;`, what is the underlying mechanism in concurrent HashMaps?
See also: - https://stackoverflow.com/questions/40677086/why-isnt-it-possible-to-compare-a-borrowed-integer-to-a-literal-integer - https://www.reddit.com/r/rust/comments/7k84kc/question_about_partialeq_across_references/ - https://github.com/rust-lang/rfcs/issues/1332 In general, I think the reason primitives aren't comparable to their reference types is just a matter of explicitness? That and the fact that passing primitives by value is more efficient and convenient, anyways. If you happen to have a reference in any particular situation, using `*x == y` instead of `x == y` shouldn't be too much of a problem.
:o somehow I completely missed that
It is superior because concurrent hashmaps have the advantage of being able to be modified from multiple threads at the same time. So you could have 5x Thread A that insert elements and at the same time have 5x Thread B reading elements. Without having to wait for the other threads to finish their work before accessing the map.
Note that this chapter will be removed in the next stable release: &gt; The "Advanced Lifetimes" section of Chapter 19 has been removed as compiler improvements have made the constructs in that section even more rare. See the changelog at https://doc.rust-lang.org/beta/book/. Since the book follows the same release trains that rust follows, this hasn't been released in the main book. But the fix will be released!
:O ARE YOU SERIOUS ABOUT THE SECOND 0:56 ????????? :o :O how this is happening. i mean how do you make the hight map terrain in amethyst ??? Please someone give me the source/sample/tutorial for it.
&gt; I'm still trying to get my head around that myself. What part? `Pin&lt;&amp;mut Self&gt;` or `Context`? `self: Pin&lt;&amp;mut Self&gt;` as opposed to `&amp;mut self` for `poll` is basically a promise that the Future will never move anymore before it is eventually dropped. This is what makes it possible to borrow across suspension points. Untested pseudo-example: async fn read_bytes(into: &amp;mut Vec&lt;u8&gt;) -&gt; io::Result&lt;()&gt; { ... } async fn read_vec() -&gt; io::Result&lt;Vec&lt;u8&gt;&gt; { let mut buffer = Vec::new(); read_bytes(&amp;mut buffer).await?; // ^^^^ ^^^^^^ // borrowing, suspension Ok(buffer) } Why? Because `read_vec()` returns a self-referential future and those are only safe if the object does not move. After the first call to `poll` returns, the future object has a valid `Vec&lt;u8&gt;` as data member as well as a valid `&amp;mut Vec&lt;u8&gt;` pointing to it. There is one "relaxation". Some future objects don't care about being moved or not. Those futures implement the `Unpin` trait which opens up a less restricted part of the Pin API. As for `Context`: The old [`poll` method](https://docs.rs/futures/0.1.27/futures/future/trait.Future.html#tymethod.poll) signature hides the fact that there has to be some communication between the reactor and the executor so that the executor knows when to repoll what future. This communication happened through "global state" and, as far as I understand, also makes it more difficult to decouple executors from reactors. In the new futures API, the `Context` object provides access to the executor in form of a `Waker` which the reactor is supposed to use to tell the executor to repoll a future. I hope this helps.
I have read several blogs, and the std::pin documentation, and I did implement std futures manually, etc. But sometimes it feels like "changing stuff around until it works", I need a deeper understanding of the mechanics. And every bit of explanation helps to understand it more, so thanks!
Formal here means roughly the definition of the parameter inside the function declaration. &amp;#x200B; `fn foo(_formal_parameter: i32) {}` &amp;#x200B; While the parameter being passed to the function is called `actual parameter.` &amp;#x200B; `let actual_parameter = 123; foo(actual_parameter);` &amp;#x200B; Source: [`https://stackoverflow.com/questions/18870156/what-is-a-formal-parameter`](https://stackoverflow.com/questions/18870156/what-is-a-formal-parameter)
[Associated type bounds &lt;3](https://github.com/rust-lang/rust/pull/57428)
It looks like you're using characters that have gaps between them, or a terminal that renders lines with gaps between them. You might want to use characters that join together smoothly. You might also look at the Unicode half-block characters, which would let you use roughly square pixels. If you set foreground and background, you can draw two differently colored pixels per character.
Macros are a bit like you said, but they suggested you use them to provide the default implementation for a lot of types without having to type them all by hand, like I did [here](https://gitlab.com/uni-mmf/bypass/blob/master/src/traits/macros.rs). I had never written a macro before that but it really gets the job done pretty easily. On the other points of your post, I definitely recommend you go into the Trait territory since you want so much flexibility. Yes, it wil hinder your performance. You might want to check out [enum_dispatch](https://crates.io/crates/enum_dispatch) at some point, but you'd loose the possibility to add fields at runtime.
[https://en.wikipedia.org/wiki/Parameter\_(computer\_programming)#Parameters\_and\_arguments](https://en.wikipedia.org/wiki/Parameter_(computer_programming)#Parameters_and_arguments) discusses the difference between "formal parameter" (in a function definition) and "actual parameter" (in a function invocation).
&gt;Another challenge as mentioned earlier by this blog is the fact that C lets the library user specify the memory allocation functions, which is a bit iffy with rust. Rust supports custom allocators, and I think providing a malloc and free function would be sufficient to implement one.
Excellent material, easy to understand and digest. Keep up the good work.
Very interesting, derives weren't brought in the original RFC and might be a good thing to discuss it in a new one with some real-world examples. I think this feature will really shine with macro support and I also wish the relaxation of the current restrictions which are very likely to happen with a lot of hard work.
It's more like Discord is a closed product and not aimed at FOSS folk. So they are not ideologically aligned. Some people see that as problematic. Count me among them, though I use Discord for practical reasons - the product works very well.
Why would adding combinator methods be a breaking change? IIRC, adding methods to traits is only a breaking change if you don't provide a default implementation.
You are correct. Still, the previous commenter had asked: &gt; Sounds like an operation every library would re-implement? So I assumed they meant that a default implementation would not be provided. Perhaps I should have clarified that in my response.
This user is the worst kind of spammer, I can't believe there's no site wide ban for him yet, I imagine many subs have put in automod rules to filter out his garbage, this is an attempt to circumvent that.
OP (Endtest) is spamming up tech subs, every day with multiple accounts [1,](https://www.reddit.com/user/boss_scarbos) [2,](https://www.reddit.com/user/dragnea_presedinte) [3,](https://www.reddit.com/user/llupei) [4](https://www.reddit.com/user/wernerklaus), [5](https://www.reddit.com/user/jos_cu_klaus), [6](https://www.reddit.com/user/sa_vina_werner) with focused self promotion spam like this that clearly breaks [reddits self promotion rules](https://www.reddit.com/wiki/selfpromotion) See for yourself. Vote and report accordingly.
I would find a second version, which aimed at being more safe and optimised, valuable. I agree with you, the simplicity of the first version needs to remain. Iterators are wonderful, but for those not used to them, they can be a cryptic nightmare!
Nice work! I opened a pull request concerning key bindings ;).
What do you mean by "limit it", exactly?
Yeah, but how is this accomplished? How does the map avoid Mutex-locks?
I want to throw away the last 10 characters. I don't need them, they are not allowed. Deallocate their memory or shrink the String's size to 30
It depends on the implementation. One type is lockfree, where you use atomic operations to avoid locks alltogether. There is also sharding which is what \`ccl::dhashmap::DHashMap\` does. It splits the elements into multiple chunks with different locks so one thread using chunk 1 wont block another thread from using chunk 2.
`the_string.truncate(30)`
&gt;It depends on the implementation. You implemented it; tell me ;) I have to decide whether I want to use a list or HashMap with a Mutex-lock or an alternative, but to judge this I need to know if, when and how the alternative is performant
Well, as i said it uses sharding. So an element can go into one of n chunks. The chunk the element goes into is dependant on the hash of the key. When you add/remove/whatever an element to the map, it takes a lock on only that chunk allowing other threads to continue working contrary to a Mutex&lt;HashMap&gt; which would lock the whole map. The amount of chunks is determined upon creation with regard to the amount of cpu cores on the machine.
Hm, the Book's example on lifetime inference is misleading. It presents the full function body, saying that a human can easily infer the lifetime while a compiler cannot. But that's not true; the compiler could just as easily infer the lifetime probably. The point is that it doesn't look at the body. And that's not an accident, it's a conscious design choice: the signature of a function must be able to stand on its own, it must not be deduced from the function body. If it were to be deduced, then changing the implementation of a function could secretly change the signature, and that's basically an API compatibility nightmare.
&gt; In C++, asynchronous code must be written using callbacks / completion handlers, while in Rust you will soon have the option to use async/await (in nightly today). C++'s [coroutines](https://en.cppreference.com/w/cpp/language/coroutines) are coming too; Clang and MSVC have experimental support I think. (Will probably take longer than Rust though.)
Ah, thanks. I had only seen it talked about as _parameter_ and _argument_.
As someone writing C++ everyday at work, I'd choose rust over C++ for literally *anything*.
Could you share your code, please? Some time ago I created a solver for Boggle game ([https://en.wikipedia.org/wiki/Boggle](https://en.wikipedia.org/wiki/Boggle)) but in Python. I used Trie structure as @ronniec95 suggests.
Are you sure about `*` and `|`? Certainly also annoying, but invalid? `:` is indeed totally verboten, but amusingly enough *permitted by NTFS*, so it's entirely possible to create a file on a Windows volume that Windows itself cannot access or manipulate. (There are alternate file APIs that can work around this, but almost literally nothing—up to and includeing Explorer itself—uses them.)
From what I understand, yes. Due to the nature of adding virtual connections on top of UDP the clients would also need to have to use laminar to enable those features not present in raw UDP. It would be possible to wrap the client side API in Rust using a Rust to Java FFI. It might just be easier to serialize your structs with Serde with something like serde-json and use TCP after all. It won't be as efficient as using something like serde-bincode on both sides however. In general Serde is really nice for this kind of application.
How can I use Alacritty with MSYS2 bash/zsh/fish and tmux on Windows? Is this even possible?
In your scenario, you _can_ use `filter_map` (and because the items are Options, `flat_map`, too) but the function you pass it needs to be something different than `Option::Some` what you did. So instead of a 2nd `flatten` \/u\/leudz's solution contains, add `.filter_map(std::convert::identity)` (or shorter but less descriptive `.filter_map(|x| x)`). It reads _I have an iterator over Options, now I filter for all Some values I get by just passing along the original Options by mapping the identity_. On the other hand, your `.filter_map(Option::Some)` is a no-op because `Some` is of type `&lt;T&gt; fn(T) -&gt; Option&lt;T&gt;` _always_ returning `Some` on _any_ input of type `T`. Hence, with `filter_map` you filter for all Some values but you also always return Some value. In the end, you filtered for all original values, nothing changed.
Author of [`uom`](https://github.com/iliekturtles/uom) here. I haven't had a chance to review the code but your README is very complete and answers a lot of the questions I would have about any unit library. I like the reduction in scope compared to `uom` as I believe it allows you to side-step a lot of the issues `uom` currently has while still providing the most important features. I also like the easy conversions between units and storage types. I think this is one of the weaknesses of `uom`'s current design and I'm experimenting with changes but I'm not there yet. How is compile time? This is another of `uom`'s weaknesses and I'd expect this crate to be pretty quick.
&gt;riemass Thanks
https://github.com/rust-lang/rfcs/blob/master/text/2592-futures.md
This is a good blog about it too: [https://concurrencyfreaks.blogspot.com/](https://concurrencyfreaks.blogspot.com/)
Closures pretty much require allocation, and that adds overhead. Cancellation is currently achieved by just not polling it anymore, but people are also looking into other approaches since this one doesn't play nice with some use cases.
NTFS is a pretty advanced filesystem, but Windows applications don't care about half of it. Case sensitive filenames, special characters, alternate streams, extended metadata, etc are all ignored by most Windows apps, including explorer.
Relevant quotes from the RFC: &gt; The main alternative model for futures is a callback-based approach, which was attempted for several months before the current approach was discovered. In our experience, the callback approach suffered from several drawbacks in Rust: -It forced allocation almost everywhere, and hence was not compatible with no_std. - It made cancellation extremely difficult to get right, whereas with the proposed model it's just "drop". - Subjectively, the combinator code was quite hairy, while with the task-based model things fell into place quickly and easily.
&gt;Closures pretty much require allocation, and that adds overhead. That's now how Rust closures work. Closures are zero-cost abstractions and simply store the values (or references) they capture inline. As far as I'm aware, one of the reasons Rust futures aren't based on callbacks is that it would require some combinators to deal with thread synchronization themselves (making them not zero-cost), rather than having the runtime deal with it. I highly recommend watching the [Back to the Futures](https://www.youtube.com/watch?v=bcrzfivXpc4) talk from RustConf 2016, it goes into this in way more detail.
I will report him. r/mods, I think boss_scarbos violate our CoC.
Hi, Alex! Thank you so much for your great work! I have adopted your algorithm for the `Moderate Path` in [jsoniter-scala](https://github.com/plokhotnyuk/jsoniter-scala/blob/4685f006524ed64eee27d2d39f1f82f5aaa1b98d/jsoniter-scala-core/src/main/scala/com/github/plokhotnyuk/jsoniter_scala/core/JsonReader.scala#L1354-L1416). What do you think about an idea to still use double-precision float in the `Fast Path` but for limited size of mantissas and exponents? [Here](https://github.com/plokhotnyuk/jsoniter-scala/pull/303/files) are all changes, including commented brute-force checking of all 2^30 mantissas and exponents from -18 to 20 which proves that it works without any errors (comparing with standard Java parsers).
How bad is the performance overhead of a hashmap if I want to store key-value pairs in an amount of 30-300? And is there an alternative to a hash map if I want a structure which allows me to store values with keys, search for values by keys, add new entries?
Thought this would be of our interest since Rust ranked nicely against other languages and the report is based on a sizable sample of 71,281 developers. Direct link to PDF: https://info.hackerrank.com/rs/487-WAY-049/images/HackerRank_2019-2018_Developer-Skills-Report.pdf
Not bad at all. Generally hashmaps are the most efficient structure for this type of task.
Unfortunately, when manually working with pins, you are often in unsafe territory, so "changing stuff around until it works" may result in the stuff breaking in subtle ways, or only in certain uses. You definitely need to understand what you are doing. I feel that the documentation on `std::pin` is not sufficiently detailed yet, and it may need a solid chapter in the nomicon as well.
I would imagine any executor that needs to store futures would box up the closure wouldn't it? Excluding some weird hackery with some macro that generates an enum type of all futures (not sure if this is even possible with impl Types like closures/generators).
I remember when we went through this with JS Promises. For a a while, projects would have their own Promise implementation favorite and it was wasteful and annoying. Once JS async/await landed, that problem all but disappeared.
I actually go through the evolution of this API in a decent amount of detail in https://www.youtube.com/watch?v=9_3krAQtD2k. The discussion of standard library futures, and how they differ, start at [2:24:07](https://www.youtube.com/watch?v=9_3krAQtD2k&amp;t=8647s).
example of how to do this: `logger = logger.new(o!("id" =&gt; my_uuid))`. That would be inside the context of the job processing code. Also note, uuid crate has a slog feature that implements Value which adds convenience to logging uuids.
https://docs.microsoft.com/en-us/windows/desktop/FileIO/naming-a-file &gt; Use any character in the current code page for a name, including Unicode characters and characters in the extended character set (128–255), except for the following: &gt;The following reserved characters: &gt;* &lt; (less than) &gt;* \&gt; (greater than) &gt;* : (colon) &gt;* " (double quote) &gt;* / (forward slash) &gt;* \ (backslash) &gt;* | (vertical bar or pipe) &gt;? (question mark) &gt;* \* (asterisk)
Hmm, that looks aimed at app development, not the embedded side, right? We can still take inspiration for how it solves things though, but the reason this has been so difficult for us is that we're targeting embedded systems.
Interesting report! Before I read it fully, I made a quick search for the word **Rust** and hit the line *f****rust****rated them the most*. LOL! I'm sure I'm not alone here!
Passing by value and reference both take 8 bytes on x86_64 since either way requires pushing onto the stack or using a register. If rust treats function arguments as an adhoc structure they may be able to get around this restriction, but I don't know what the abi does.
Yeah, I was wondering about this - I believe you're correct. Still, I would think passing by value should be preferable to avoid the indirection (an extra load.)
Oh, of course! Somehow I had the brain bug that `map` would be implemented by runtimes such as `tokio`. Thanks for clarifying!
This blog post is about myself exploring some design rationales about public APIs and their private (i.e. in the sense of backend) implementations. Feel free to ask any question / share your feedback about the subject.
I thought the issue was that returning closures would make it hard to know how much space to allocate for a future in advance. So it's not that closures inherently require allocation, but rather that storing them in a collection without knowing their types up front does.
Yeah, anything that's the same size or smaller than a register should be passed by value unless you need to modify it, otherwise you have to do another dereference. That should probably be some kind of compiler warning.
Thanks for the kind response! Compile time is quick. It’s not something I’ve measured or optimized. I work on a high-end desktop (i7-8700). Incremental changes take seconds. A full clean and rebuild including all dependencies is probably 30 seconds — give or take. I think I could clean and rebuild plus tests in less than a minute. Looking at my Travis CI build it’s about 2 minutes for Linux to compile and run tests.
If the keys are strings, technically a [Trie](https://en.wikipedia.org/wiki/Trie) *might* be able to outperform a hashmap, but as xacrimon said, hashmaps are very good performance wise. Lookups are considered constant time in big-O notation, and additions are the more expensive thing -- once a hashmap is a certain amount full it may need to re-bin the existing values. So if your hashmap is load-once-then-use-to-fast-lookup then you really are going to be stretching to optimize it further. However, even re-binning is very fast and it only happens in certain circumstances (generally as the hash map grows siginficantly larger). My approach to optimization is to write it as smartly as you can using good abstractions, such as a hashmap, and then if you're having performance issues, profile and figure out the hot spots and optimize those. Being a rust noob who is just pondering picking it up again, my answer is not about rust specifically but just development in general.
Yeah, embedded companies change tech at a glacial pace.
There is a clippy lint: https://rust-lang.github.io/rust-clippy/master/#trivially_copy_pass_by_ref
Why does the `csv` crate example (https://docs.rs/csv/1.0.2/csv/#example) use `process::exit(1)` at the end of the program? Why not just let it exit normally?
In practice this kind of micro-optimization rarely has any impact. Mainly because the optimizer is very aggressive about inlining, so very often a function call dissolves, pointer dereferencing and aliasing is often collapsed, to the point that you can only barely recognize the original code. And for the call sites that are not inlined, generally they are called so much less frequently than the call sites that are inlined, that these kind of concerns are rarely a first-order concern.
regarding sealed traits, there's a trick you can do with private supertraits to emulate them: https://rust-lang-nursery.github.io/api-guidelines/future-proofing.html#sealed-traits-protect-against-downstream-implementations-c-sealed
VS: Code, IntelliJ/Clion with the plugin, and Vim/Emacs have been the most popular, every time this has been surveyed.
IDEA is ultra-slow, but still the best option at the moment.
A non-zero exit code tells the caller of the program that it failed, this is a convention that's been around for quite a while. In C, you would `return 0;` or `return 1;`, etc. This example was probably written before the `Termination` trait was added to Rust which allows you to return anything that implements that trait such as `()` or `Result&lt;(), T&gt; where T: Debug`. Returning an `Err` will print the value and exit with a 1, so that example could be reduced.
Don't forget the other interesting restriction: &gt; CON, PRN, AUX, NUL, COM1, COM2, COM3, COM4, COM5, COM6, COM7, COM8, COM9, LPT1, LPT2, LPT3, LPT4, LPT5, LPT6, LPT7, LPT8, and LPT9 are all dis-allowed filenames.
I'm using (neo)vim, and you can load it up with as many or as few plugins as you want. There's usually a way to get any missing feature into vim. For Rust, you can use ALE, vim-rust, and vim-racer to get linting, completion, formatting, etc. You can use universal-tags with or without plugins to make jumping around easy. If you're missing the navigator drawer, there's netrw built in and plugins such as nerdtree and dirvish.
I think I misunderstood what Flandoo meant when he said &gt; I'll let someone else answer why rustc doesn't do it for you I know that you can't do `&amp;true == true` because there is no implementation of `PartialEq&lt;T&gt;` for `&amp;T`.
I use IntelliJ and VSC both for Rust and Java and I can say that VSC is booting up much faster (2seconds vs 2minutes). It's even fast enough to use as a pure text editor, so I think there is not much space for improvement for a mainstream product. Concerning productivity features, IntelliJ seems to be still ahead, once it has loaded. How do you know the lag with large directories comes from VSC? Sounds much like a language server issue. Imho VSC scales quite well, for example you can open text files with 100MB in size without trouble.
Is there an open dev board for the nRF52 and rust that you would recommend to potential contributors?
Thank you for a pointer to an article!
As far as I am aware there is no pure Rust IDE for Rust. Although it would be nice to have a more performant IDE written in Rust. So I am quite interested if it would be feasible to create this. It seems like it is now easier to create than a few years ago due to the new Language Server protocol. There seems to be an [implementation](https://github.com/rust-lang/rls) for rust. So IMO it would be awesome if this could be made using OrbTK. &amp;#x200B; Maybe some other people who are more familiar with the RLS and OrbTK can share their experiences?
If you're not familiar with these kinds of issues, here's a great deep-dive: [Fixing Unix/Linux/POSIX Filenames](https://dwheeler.com/essays/fixing-unix-linux-filenames.html).
Ah, I'm adding a "required hardware" section now, but an Adafruit BLE Friend could be used with an STLINK or blackmagicprobe, I'd probably recommend that in terms of simplicity (it already has a USB to UART chip onboard). There's also the closed-source (I think) Nordic adapters. I also designed some open source nRF52 hardware around the Laird BL652 modules, those modules could even be used on a breadboard with some dead-bug soldering.
So by default, main will return `()` and thus normally exit with 1?
In the standard library, there's also `[std::collections::BTreeMap](https://doc.rust-lang.org/std/collections/struct.BTreeMap.html)`, which is probably going to be slower than the `HashMap`, but is really useful if you're interested in _ranges_ of keys, or want to iterate over keys/values in a sorted order. For `HashMap`, also note that the default hashing algorithm used is designed to make it robust out of the box against key-collision attacks, and is less efficient for smaller keys (e.g. integers). If you're not dealing with externally provided keys, and the keys are generally short, using something like the [FNV](https://github.com/servo/rust-fnv) crate might make things faster.
To any authors who might be writing editors/etc, please make your editor fully customizable. I'm picky about the keyboard interface I use. To the point where I'm using Atom and toying with my own brand of somewhat Kakoune editing. With that said, most editors I have to immediately disqualify because they can't do what I believe to be fairly basic things for plugins. Remapping all keys, controlling text input, and some information UI display. So this comment isn't a finger pointing at any editor or anything. I'm just saying, please take the time to consider plugin architecture and letting users customize your product heavily. Thank you :) _(sidenote: I too am excited for XiEditor. I'm using Atom currently while I wait for Xi)_
&gt; But that's not true; the compiler could just as easily infer the lifetime probably. The compiler can infer \*a\* lifetime, but the compiler cannot infer \*which\* version is actually correct. That requires knowing programmer intent.
I think it would be easier to understand if you also gave some example code which shows how a presumed user would call the API. Show the problem a bit more before investigating solutions?
Interesting. I've just never had cause to create files with those names, I guess.
Returning `()` or `Ok(())` from main will exit with 0 for success, and the default return value for functions is `()`. Returning `Err(...)` from main will exit with 1, indicating an error.
&gt; Adafruit BLE Friend This is nRF51 only though, so it only works if you build [your PR](https://github.com/jonas-schievink/rubble/pull/59), and even then the demo doesn't work with the 51 series.
The nRF52840 Dongle is supposedly pretty good, but also out of stock in many places. You might still get lucky though!
I can't seem to find a way to do something, that I have found fairly easy to do in Python. I just would like to open a file that is not in my CWD. This is what I have tried, but it does not work. It says the file does not exist. `let aliasfile = BufReader::new(File::open("~/.config/crow/crowfile").unwrap());` If I change this to just the file name, and run the script in the directory the file is located everything works as it should. But I want to be able to run the script anywhere on the system. I'm very new to this so I apologize if this is a scrub question, it's just been wracking my brain for a few days now.
Only global allocators, not per-library.
Regarding allocation, see [this comment](https://www.reddit.com/r/rust/comments/c056yh/-/er1akf0)
I've never actually met another developer that uses Atom but I've met loads that use VsCode. Not claiming my anecdote is in any way statistically significant, but I didn't realize Atom was considered popular until I saw the last Stack Overflow survey results.
Is the nRM52832 basically the same but with less flash? It looks like there's many of those in stock at Adafruit. I assume you have to take over the BLE peripherals from a nordic supplied BLE stack? Is the smaller storage size any issue with deploying rubble? I've been looking for an excuse to play more with embedded rust and separately work with BLE a bit, so this looks like nice side activity - this looks perfect but I'm trying to minimize the initial setup hassle. https://www.adafruit.com/product/3406
Yeah, absolutely. I meant to clarify that closures don't inherently have to allocate, but in the context of futures, they certainly go hand in hand.
Your problem is that Rust isn't treating `~` as a special character - it's not the file system that converts `~/` to refer to your home directory, but the shell. If you write a program that calls `std::fs::create_dir_all("~/.config/crow")`, you'll end up with a directory named `~` in your current directory, with `.config/crow/` under it. This is what `File::open` is trying to open. Rather than relying on `~/` to expand to the user's home directory, you'll need to do this yourself. The `dirs` crate has a [`home_dir`](https://stebalien.github.io/doc/term/dirs/fn.home_dir.html) function that will return the current user's home directory (if any). You can then use `PathBuf` methods like [`push`](https://doc.rust-lang.org/std/path/struct.PathBuf.html#method.push) to build the rest of your path, and then pass that to `File::open`.
&gt; Is the nRF52832 basically the same but with less flash? Yeah, the 52840 also has a special radio that can do more protocols, but that's not needed if you want to work on Rubble. All chips in the nRF52 series are currently supported by Rubble. We've been using the nRF52810 for most of our development. &gt; I assume you have to take over the BLE peripherals from a nordic supplied BLE stack? We don't flash the Nordic stack, so we can just get the peripherals from the [PAC crate](https://docs.rs/nrf52810-pac). &gt; Is the smaller storage size any issue with deploying rubble? Not at all, even the small nRF52810 still has 24 KiB of RAM, and 192 KiB of flash, which is plenty for working on Rubble with all cranked up to max. A minimal Rubble build uses a few hundred bytes of RAM and around 20 KiB of flash. &gt; this looks perfect but I'm trying to minimize the initial setup hassle Yeah that looks like it could work, but I recommend using it over SWD, since that allows proper debugging (I think you can only *flash* over the built-in USB port). "Proper debugging" meaning that you can do semihosting (which is how panic messages are printed by the Rubble demo), and if needed even use breakpoints. Flashing over GDB might also be more convenient than using their Arduino-style serial bootloader.
&gt; The compiler can infer *a* lifetime, but the compiler cannot infer *which* version is actually correct. That requires knowing programmer intent. You're not wrong, but that's not what inference does. Inference finds *the most general unifier* of the lifetimes, which means in this context the shortest possible lifetime. There is no need to guess programmer intent to do this. As with types, the programmer always has the option of restricting the lifetime further, but (as with types) never has the option of making the lifetime longer without violating the borrow checker's rules. &gt; If it were to be deduced, then changing the implementation of a function could secretly change the signature, and that's basically an API compatibility nightmare. You have to think of it like type inference (because it *is* type inference). It would work the same way APIs work in OCaml and Haskell with respect to types, both of which support stable APIs and separate compilation. Type inference does not eliminate the need for type annotations. What it does is it collaborates with the programmer prove that the resulting program is correctly typed. In public APIs the types are annotated (but may be generic). Type inference proves that the implementation corresponds to those annotations. It goes both ways, so to speak.
&gt; Ah, so you want global inference for lifetimes. This has been explicitly denied because the lang team wants to be able to locally reason about code. Global inference makes this impossible. Do you not already have to reason about lifetimes globally with the borrow checker? You have global proofs of incorrectness but not global proofs of correctness. Meanwhile, the *human* programmer is required to ensure global correctness manually anyway. The question boils down to, do we help the programmer by proving correctness, or do we make the programmer go it alone and only check her work afterward? I appreciate you all taking the time to help me think this through, BTW. You guys are awesome.
Thanks, that's helpful guidance. I've got an stlink already, I think I can get that to use SWD for the nrf.
Not really. Yes we have global proofs of correctness, but these are built up incrementally. This is done by going through each function and proving correctness. If that proof depends on another function, we then prove that function is correct as a sub-problem. In this way we can prove that every function is correct. Thus we have global correctness. But at no point did we do global analysis. Also once we prove a sub-problem (function), we don't look at the body of that function every again (in terms of proving lifetime correctness). This saves on compilation time, and makes Rust code easier to read because we don't need to juggle non-local lifetimes (which would be even more nightmareish to learn than local lifetimes, and local lifetimes are already hard to learn) This method will reject every incorrect program. And there may be some correct programs that it rejects. In those cases we can almost always create a `unsafe` primitive (for example `Rc&lt;T&gt;`) that solves the problem (with `Rc&lt;T&gt;`, the problem being shared ownership, like in graphs). That primitive may use `unsafe` internally, but it exposes a safe interface which can be used to solve whatever problem you have. It is more important to reject wrong programs that it is to accept correct programs. For the latter we can use `unsafe` to build primitives. But for the former, we can't trust any code written using just safe Rust (which defeats the entire purpose of Rust).
My anecdote shall outdo yours! &amp;#x200B; I've been told multiple times to use Atom for Rust, as well as other languages. Vscode is a lot easier though so I can see why I was wrong about that.
&gt; I do not believe that'd be decidable at compile time without GC. Well, my intuition isn't very well-developed. I only started looking at Rust literally three days ago. And the examples in The Book have the sophistication of, "See spot run" (as rightly they should). The reason I think it's decidable is because of my mental model of lifetimes as subsets of code regions. Do you have an intuitive explanation for where this breaks down? BTW, as I wrote to u/YatoRust, I appreciate you all taking the time to help me think this through. You guys are awesome.
What's difficult about large projects and Vim? Do you have `ctrlp` installed? Do you have "go to definition" working? I'm actually using `kakoune` for Rust, and it a really good `kak-lsp` plugin.
It is not decidable because perfect lifetime checking can be reduced to [SAT](https://en.wikipedia.org/wiki/Boolean_satisfiability_problem)
&gt; Cancellation is currently achieved by just not polling it anymore, but people are also looking into other approaches since this one doesn't play nice with some use cases. One of them is io_uring on Linux and IO Completion Ports on Windows, where you give the OS a data structure to write data to upon completion. If a future is dropped, the OS would be writing to an invalidated chunk of memory upon completion, because even cancellation requests are asynchronous and can't be deterministically done in `impl Drop for Future`. How is this going to get solved?
Because the call stack is prenex polymorphic structure (it goes down, it goes up). While a developer can (and likely will) write code that is "_high rank_" (k-rank), this implies the temporal locality (stack frame) is impossible to determine as you need higher order dimensions to quantify its existence. The problem is our hardware doesn't respect these "_high ranks_" (as the stack is first order only). So we'd need GC (and possibly pre-allocation, or allocation probes, implemenation details) to handle these other cases. --- This is purely speculation, I'm not sure I fully followed your link/examples. I just kind of assumed you'd reduce the lifetimes to a polymeric parametric type system so you do some Hindley–Milner-esque magic on it to deduce types (lifetime-constraints).
[Powered by Sequoia PGP](https://gitlab.com/sequoia-pgp/sequoia)
Does the line immediately after the one you linked not “un-ignore” diesel_cli/Cargo.lock?
Is OrbTK ready for usage yet?
LOL!
I don't think it is. It seems like there is currently a rewrite in progress for [version 0.3](https://gitlab.redox-os.org/redox-os/orbtk/milestones/1).
I assume you mean lifetime inference? Reducibility to SAT is a proof that it is [decidable](https://en.wikipedia.org/wiki/Decidability_(logic\)), because SAT is decidable. Transforming the lifetime inference problem into a SAT problem isn't a complexity problem either: it means that SAT is *at least as hard* as lifetime inference, which doesn't tell us much, since SAT is NP-Complete. In other words, knowing that the complexity of our problem is bounded above by an NP-Complete problem doesn't tell us how hard our problem is, only that it can't be harder than SAT. It could be linear (my suspicion) or low polynomial order. I would actually have been surprised if it wasn't reducible to SAT. It is a problem that cries out for a Prolog program to solve it. [Prolog uses Horn clause inference](https://en.wikipedia.org/wiki/SLD_resolution), which is [a subset of SAT](https://en.wikipedia.org/wiki/Horn-satisfiability). It's like the circle of life. :)
You're describing a bottom-up algorithm that reasons globally. Some type inference algorithms work this way as well. It's not clear to me whether or not this is possible for lifetimes, but that might be because I'm naive. It might be "obvious."
I suppose I'm still getting used to working with vim. Having multiple tabs and windows up still doesn't quite feel comfortable yet.
I gather that the [https://www.adafruit.com/product/3406](Adafruit Feather nRF52) and the [https://wiki.makerdiary.com/nrf52840-mdk/](MakerDiary nrf52840-mdk) would work.
&gt; LLVM is a great compiler and has a much cleaner and easier to certify design than GCC This may well be true, but is not consistent with what I thought. Any supporting link?
This will break in the face of multibyte characters. To remove 10 _codepoints_ it seems that the only way to do it without copying is like so: for _ in 0 .. 10 { the_string.pop() } However, if you want to avoid angering /u/Manishearth and the rest of the non-BMP world then you should probably use [`unicode-segmentation`](https://docs.rs/unicode-segmentation/1.3.0/unicode_segmentation/) and truncate at the 31st grapheme: use unicode_segmentation::UnicodeSegmentation; // .grapheme_indices() docs recommends using `true` for extended graphemes definition if let Some((trunc_idx, _)) = the_string.grapheme_indices().nth(31) { the_string.truncate(trunc_idx); }
They said explicitly it’s ASCII. You’re absolutely right otherwise.
I missed that part, I just got so hyped up to remind people how hard strings can be.
It’s cool! You’re not wrong.
It's important to note that when a function defers to a sub problem, after the sub problem is checked it only uses the other function's signature for the rest of the analysis. So there is no global analysis of all function bodies at once, which seems to be what you are suggesting.
Development on xi seems to have slowed down a lot though. Does anybody know why?
Most of the solutions I've seen involve a separate buffer owned by the future so that the buffer's ownership can be moved out of the future before dropping it. Then it can wait for cancellation to complete before dropping the memory.
Most of the solutions I've seen involve a separate buffer owned by the future so that the buffer's ownership can be moved out of the future before dropping it. Then it can wait for cancellation to complete before dropping the memory.
I tried this, but using the Directories crate, as it seemed to be the same, but with the need for less pushing. I have it set up like so `if let Some(home) = BaseDirs::new() { let mut path = PathBuf::from(home.config_dir()); path.push("crow"); path.push("crowfile"); let aliasfile = BufReader::new(File::open(path).unwrap());` But it still seems to only be referring to the named file in the current directory.
What version of lalrpop are you using? I found this link: http://lalrpop.github.io/lalrpop/generate_in_source.html What does your build.rs contain? I presume lalrpop generates the file from there.
What if you move `calculator1.lalrpop` into the `src/` directory?
I'm sorry, I got my terms mixed up, yes it is decidable, but it may take a long time to compile in the worst case.
So we did. But it's worth mentioning anyways
Sure, that’s why I made the second half of my comment.
While we are at it, is there somewhere an Emacs `use-package` config that loads and configures all plugins needed to provide a good experience? I would love to give it another try.
I am using: ``` rustc 1.35.0 (3c235d560 2019-05-20) lalrpop-util = "0.17.0" lalrpop = "0.17.0" regex = "1.1.7" ``` Hmm, it is true that this build.rs file is different from mine. The one in the tutorial is this: &amp;#x200B; extern crate lalrpop; fn main() { lalrpop::process_root().unwrap(); } &amp;#x200B; And the code in the link that you posted looks like this: &amp;#x200B; extern crate lalrpop; fn main() { lalrpop::Configuration::new() .generate_in_source_tree() .process() .unwrap(); } &amp;#x200B; However, even with this code, it is still generating `calculator1.rs` in my crate root directory.
I guess this may help for reference pub fn openalias(alias: String) { let srchterm = "-&lt;&lt;&lt;&gt; ".to_owned() + &amp;alias + ": "; if let Some(home) = BaseDirs::new() { let mut path = PathBuf::from(home.config_dir()); path.push("crow"); path.push("crowfile"); let aliasfile = BufReader::new(File::open(path).unwrap()); for line in aliasfile.lines() { match line { Ok(line) =&gt; if line.starts_with(&amp;srchterm) { let path: Vec&lt;_&gt; = line.split(": ").collect(); let file = &amp;path[1]; Command::new("nvim") .arg(file) .status() .expect("Something went wrong."); println!("{}", file); println!("{:#?}", path); }, Err(e) =&gt; panic!("Error reading file: {}", e) } } } }
:tada: Please try the feature out and if you find bugs, please file issues. :)
:tada: Please try the feature out and if you find bugs, please file issues. :)
try printing the path and checking if its [absolute](https://doc.rust-lang.org/std/path/struct.PathBuf.html#method.is_absolute) println!("{:?} is absolute: {:?}", path, path.is_absolute()); If it returns true, you're probably just targeting the wrong file. Also, I'd use ProjectDirs instead of BaseDirs in your case.
Hmm, I seem to get the same error with this, except now the `calculator1.rs` file is generated into the `src/` directory instead of the crate root. . ├── src │ ├── calculator1.lalrpop │ ├── calculator1.rs │ └── main.rs ├── target │ └── release ├── build.rs ├── Cargo.lock └── Cargo.toml
I'll try that. I'm embarassed to say I didn't understand the ProjectDirs format very well, so I went with BaseDir as it was easier for me to grasp.
Development on xi seems to have slowed down a lot though. Does anybody know why?
iirc some other surveys show that VsCode wins over atom a lot in general for programming.
The linked page says &gt; For each foo.lalrpop file you can simply have `mod foo;` in your source tree. The `lalrpop_mod macro` is not useful in this mode.
Thanks!
By giving ownership of the memory to the thing processing the completion notifications, i.e. the reactor.
Calm down Satan
Boxing isn't necessary. Things like [dynstack](https://guiand.xyz/blog-posts/unboxed-trait-objects.html) can store trait objects without allocations.
[Mine does](github.com/animatedrng/emacs-config)! Admittedly it's an awful unreadable mess written by someone who never properly learned elisp, but it should work. I think there's a section in there for everything related to rust.
See my other comment regarding [dynstack](https://guiand.xyz/blog-posts/unboxed-trait-objects.html).
That's ok! Its really confusing and I'm not sure I'm 100% right. Some platforms (macOS, iOS, Android) use the [reverse DNS notation](https://en.wikipedia.org/wiki/Reverse_domain_name_notation) for internal representation and to create directories (which is basically the inverse of order of a site name: e.g. com.reddit.www). The qualifier would normally be your [TLD](https://en.wikipedia.org/wiki/Top-level_domain) (if you have none, you can make something up for development porpuses or leave it blank). The organization would be your company name (normally as it appears on your domain name, but can be other things, including your own name). The application would be the application name itself. For example, on my phone, the telegram app is org.telegram.messenger. And well, you don't really have to get it right until you want to publish it =p (and feel free to ask any question!)
Thank you that makes sense
Let me flip this around and say where you would *not* want to use Rust: * Client side of a graphical 3D game. Sorry piston, but Unity is way easier to use and UE4’s maturity and graphics quality presently blow it out of the water. * Something with a significant amount of native UI. The libraries just aren’t mature here. * Anything else that depends on a domain-specific or industry standard library that doesn’t have Rust bindings or would be a significant effort to implement them * A preexisting project with old-school management or contractors who will oppose technologies they don’t understand * Work where quality, stability, and maintainability don’t matter - you don’t care if the program crashes and burns if it hits an unexpected edge case. Eg prototyping * Places where a domain-specific language exists and is a better choice * Places where a compilation step doesn’t make sense (eg CI scripts) Some of these are arguable, and I’m pretty sure other people will point out how you can mitigate the weaknesses / obstacles. Also, speaking generally, I suspect in places where logic errors that can’t be represented by the type system are the real problem, Go might be a better choice due to the faster compilation time. However this comes with the caveat that Rust will catch more logic errors than you thought possible at compile-time due to its significantly better type system. I don’t think the use case you’re talking about runs afoul of any of these, and I think a huge benefit you would realize is that you can encode the rules / constraints of your game in the type system to catch those issues at compile-time. Another benefit you’d have is that Rust is just more stable and easier to learn than C++. Yes, you can learn a subset of C++ and work for years with it, but if you actually try to learn the *vast majority* of the C++ language (to the extent you can do the same things as in Rust) it will likely take you years. And Rust is more likely to fight you when you do something dumb, whereas C++ will let you shoot yourself in the foot and you’ll just have to remember not to do that thing in the future. And when it comes to managing dependencies, C++ is a friggin nightmare. Multiple times I’ve seen work blocked due to canker issues, or figuring out how to integrate some library’s build system with a proprietary build system. This can take days, weeks, months, or sometimes it simply just kills the idea entirely. With Rust, you often only need to specify a single line in Cargo.toml On top of that, if you do have to refactor Rust eg to split a library up as it scales, it is enormously easier than an arbitrary C++ library. Again, Rust will have been guiding you to use safe, modular constructs the whole time, and provided you worked with the type system, you probably just need to get it compiling again and won’t have to worry about esoteric bugs creeping in through side channels. If you are a bad C++ programmer, you will hate Rust at first. You won’t understand why it’s stopping you and forcing you to handle a case, or why it isn’t letting you use more global state, or stopping you from modifying a variable when you “know” it’s not going to crash the program *right now*. However as you scale it up, if you keep track of those things it forced you to think about, you’ll find that those were all the things you needed to worry about for scalability or multithreading. If you’re a middling C++ programmer, you’ll probably understand why the compiler is giving you grief, and you’ll suddenly start wanting all the primitives that Rust gives you in C++. And you’ll realize that there’s a whole dimension to C++ that you never realized because C++ never forced you to go past certain humps to do better, but Rust does. And if you’re an expert C++ programmer, you either got there by learning Rust and then going back to C++, or you’re super passionate about the language and wouldn’t be asking. :) Your specific use case sounds pretty reasonable. Also if you already know an analogous language (C++), I recommend checking this site below out. It’s more of a walking overview of the language than going really in-depth on every topic like the book, which I think helps avoid overcomplicating things when you’re first starting. https://learning-rust.github.io/index.html
You can write bad code in any language , C# and Java can be damn fast for polymorphic/ interface calls to a library. C# has had massive improvements lately for low level code and SIMD.. Its far faster to write code and tests in those languages and write the 5% you need fast in something low level after you have measured it and refined your algos. eg https://www.techempower.com/benchmarks/#section=data-r17&amp;hw=ph&amp;test=plaintext your like 0.2% of the best . Look at numpy.. Completely agree to be a better programmer you should know a GP language , a lower level language and a functional language.
I just cloned your repo, moved calculator1.lalrpop into `src` and then was able to `cargo build` no problem.
&gt; Green threads, userland threads, coroutines, goroutines or fibers, they have many names but for simplicity’s sake I’ll refer to them all as green threads from now on. It is confusing to talk about green threads and coroutines as if they are the same thing, when they are enough like each other to be confusing but still pretty different. I was definitely confused by the distinction when I first started learning about this stuff, so it might be nice to just drop "coroutines" from that list. Otherwise, this is a really awesome write up.
I/O completion ports are kind of problematic in this regard: you have to hand off ownership of a buffer to fill, and in the intervening time that buffer isn't usable for anything else. Not only does that complicate ownership, but it's inefficient in that you preallocate buffers for every "sleepy" connection instead of just using a buffer pool to handle active I/O operations. &amp;#x200B; Fortunately you can hack around that in an I/O completion-based model by doing I/O operations of zero bytes, which means you don't have to provide the buffers up-front and get signaled for I/O readiness in a way not too far off from the \*IX mechanisms like epoll. If nothing else, that approach is more efficient from a buffering perspective, and also simplifies the ownership story.
Why would you need `map` at all instead of an async-await world? Things like ``` doOp().map(|result| next(result)) ``` simply become ``` next(doOp().await) ``` Most of these combinators are relicts from a pre-async-await world and actually not that idiomatic anymore with it.
It is indeed called multiple times: - Zero or more times where it returns Pending/NotReady - One time where it returns ready. In the first cases the object on which `poll()` is called is obligated to "remember" the task (handle) and wakeup the task when it would be able to make progress.
Been using vim for more than a decade. It never will. Switched to VSCode as my primary now that remote editing is available. That was the last thing holding me back in console editing.
&gt; Why do you think it wasn't well received? Well, most of the top voted comments are criticizing some details of the post, and also it had waaay more comments than upvotes (well, ~65% of votes were upvotes). I don't think people didn't like it, but considering the rust community and the topic itself, I thought it would be a little different. But yeah, you convinced me it's a win ;) &gt; If you truly don't know what specifically is toxic and would bother LGBT+ people in online interactions Oh no, I wasn't talking about that, the way some act towards LGBT+ people is obviously bad and unfortunately common online. But I was referring to "I think cishet people should reconsider how they speak about the LGBT community", where I thought you were referring to how some cishets (and maybe me) treated you in this thread or something like that and I wanted to know if anything was interpreted as offensive or anything like that. &gt; Cool! I'm curious what your experience so far has been? Have you found many opportunities to help? Maybe I can learn something from how you approach these situations to handle them more delicately. For some context, I live in Brasil, but I've only been able to do small things in the last years: Offline: On university / work: when related topics rise, I discuss it up to a certain point, but some people are just not compassionate enough to care or to even listen about certain topics... I never end up being too close to those, and for some I don't really think they'll ever change their mind, as they make it almost impossible to have any constructive discussion. Most younger people I interact with really don't care about other's sexuality and gender though, and a great fraction is not het. Also, being mostly het never made any LGBT+ offline dislike me (that I know of), even in gay night clubs, which says something really positive about the community. Family is more complicated as they're generally older and more conservative, but when people show homophobic signs I just argue while its still constructive. Also, I try not to argue when I'm mad and keep calm on any discussion, because when you're mad you end up being much more unreasonable. Its usual for me to be with LGBT+ IRL and I've never witnessed any harassment, but I would stand up if it happened. Online: I'm not really active online, I don't really use social networks (although I started using mastodon last week) and on the IM groups I'm on I've never seen a single case of homophobia. Also there's is probably at least one LGBT+ on each group I am. If I ever find other / better ways to help, I'll PM you, but I guess that the biggest advantage (on this cause) I have being mostly cishet is that I can wait for the right moment to speak (when its more effective), and some may listen more to me (as when defending others, you've got more credibility than when defending yourself). &gt; Hopefully everyone can continue with constructive dialogue, and at least in Rust and Amethyst, avoid the extreme polarization that infects many online communities. Yes please! That's actually one of the reasons I'm not really active online.
I purchased a license for CLion (JetBrains) and haven't looked back. It does everything I need, and then some - it's fast too, much slimmer than IntelliJ, and once any plugins you don't use are disabled it flies along.
Thanks for feedback! I've tried it and it does look better now.
That's awesome! Last night I decided to permanently switch to sublime but you might have changed my mind. I'm definitely going to look into CLion!
Does ffpeg/OBS support this codec too?
Well done with your game! I would be super interested in the Rust &lt;-&gt; Unity integration part of it. How hard is it to set up? What is the build process like? Does it limit the number of build targets for Unity? What kinds of types can be passed across the boundary? Are there any performance penalties to the communication? How is communication handled, is it some sort of serialisation and sending of messages or is it C ffi style? Thanks :)
I think it's cargo. If only I could have something similar for C
Atom is on its way out. It’s usage numbers are way smaller than VSC’s in every survey and has been for a while. If you look at the recent commit history of both, Atom looks abandoned. Which only makes sense. Microsoft shouldn’t be making 2 different open source text editors based on Electron.
Because `doOp().map(next)`, which in addition to being 2 characters shorter, doesn't have a monadic bind in the middle of a statement, threatening to have multiple separate `await`s in one line.
Why not a code hosting site like github? It allows people to click around in your code in seconds and suggest a change, instead of downloading and unpacking a tarball. It also allows linking to specific files and code ranges in discussion. Sorry for the offtopic suggestion, just wondering.
Which is really cool but not usable by 99% of Rust projects because it's GPL 3.0.
You're probably looking for r/playrust, this is the subreddit for the programming language named Rust
Wow! Great questions! **How hard is it to set up?** Really easy actually, you just need to expose some Rust functions and then set up the C# bindings for the same functions. Then you create simple structs in rust and you can pass them across from C#. I suggest you start by reading this excellent blog entry: [https://dev.to/living\_syn/calling-rust-from-c-6hk](https://dev.to/living_syn/calling-rust-from-c-6hk) You can build from there. I basically got by with very basic structs and a few strings. Didn't do anything very complex. I used a representation of the board which was a 256 byte array (but interpreted as a u8 struct). It was similar to this representation: [https://www.chessprogramming.org/8x8\_Board](https://www.chessprogramming.org/8x8_Board) **What is the build process like?** Well, lots of unit tests in Rust. So I could Run Cargo Test and get lots of intermediate steps checked, including evaluating and scoring some boards. Then I ... ahem... copied the DLL across into unity by hand... uhm. You have to restart unity at this point (!) for the editor to stop using the DLL so you can replace it. Yay? (Godot doesn't have this issue). I was using a 2017 version of unity, perhaps this is fixed now. Building was as simple as Cargo test, cargo build (by hand) and then jump across to a mac to repeat. The project wasn't long enough to justify setting up a good Continuous Integration setup (perhaps next time!) **Does it limit the number of build targets for Unity?** We were only targetting Windows and Mac and so this was no issue. I imagine it would have been harder on consoles, iOs and Android but I didn't have to worry about that. I found it was easy to add all the different builds to the project and have conditional defines to load the correct ones. **What kinds of types can be passed across the boundary?** I found another tutorial that had some great smart pointers to help with complex types, but from experience I sticked with very easy stuff. Flat Arrays, Strings and some primitive types were all I needed. Basically you can send in a string and have it turned into a board, or the reverse. Or you could provide a board and an array to hold suggested moves and the AI will fill the moves up. I also had the AI hold some static variables to hold all the intermediate state and manage a log file that I used for debugging. **Are there any performance penalties to the communication?** Probably minor ones, but very minor. However the calls across the boundary were very infrequent - in the order of a few calls to start and poll the AI each human turn. I ran this communication in a different thread that I started on the Unity side. **How is communication handled ?** It was straight C FFI. You just set the functions as pub and #\[no\_mangle\] and you can call them straight from C# code with a simple decorator. I hope that this helps!
He tit for tat I say :D
you can get a significant speedups if you use some SIMD techniques. If you can combine this with "regular" parallelism (e.g. rayon) you can get impressive gains.
Haha, daily fun! r/lostRedditors
Thanks a lot.
Part of the major benefit of rust isn't in tricky little data manipulation. It's that the language causes you to think about your data, how it flows through the system, higher level algorithms, and that you can enjoy parallelism and multi-threading with relatively little work compared to other languages. Using the stack over the heap, using proper aliasing, etc etc. It adds up into a faster program through higher level design rather than trying to shoehorn performance in as an afterthought.
https://trac.ffmpeg.org/wiki/Encode/AV1 Code? yes Encoder? no
ffmpeg includes libaom to encode/decode AV1 videos. However, it's currently ultra slow for encoding. There are other libraries currently being worked on that aim to be much faster, but they are not included in ffmpeg as far as I know. I don't know for OBS, I guess it depends on which ffmpeg version it embeds.
Stack switching on Windows requires more than just the callee saved registers due to the nature of how exception handling and `_chkstk` work. See: https://probablydance.com/2013/02/20/handmade-coroutines-for-windows/
It's quite a niche application, so you're mostly on your own. You can try to update `winapi-kmd-rs` for the newer WDK versions. Do you have any specific gripes with it?
&gt; In Python, I could fully execute the GIMP program without scripts. But when I converted to Rust I could not get the file name with quotes set up properly. So, that is why Rust calls the scripts. That should totally be possible.
Thanks! this is exactly what i was looking for1
Could you give more detail on how you "use the stack instead of the heap" (do you mean by using references?) and "proper aliasing" (not sure what aliasing means).
ffi offers CStr `use std::ffi::CStr` What would I need them for typically? So far, I use `std::os::raw::c_char` pointers if I get strings from C to Rust and transform them into Rust-Strings as soon as possible, to get rid of the broken ancient C ascii strings.
`CStr` was designed to handle that initial conversion for you: https://doc.rust-lang.org/nightly/std/ffi/struct.CStr.html#method.from_ptr
I'm happy with sublime text. It's features are not the same as VC code. But it has the most important ones like cargo hints
https://doc.rust-lang.org/book/ch04-01-what-is-ownership.html
Dynamically sized collections (such as `Vec`, `HashMap`, etc.) generally use the heap. You can also allocate single objects on the heap with `Box` (unique ownership) or `Rc` (shared ownership). Everything else is allocated on the stack, and only exists for the duration of the function call (the stack frame). If you can organize your program such that heap allocations are rare, and most of the "working" memory resides on the stack, it will usually be much faster.
/r/playrust
My Bad.
Thanks for the feedback. We'll I used the [Wikipedia definition for this](https://en.wikipedia.org/wiki/Coroutine) (and also the same article on [green threads](https://en.wikipedia.org/wiki/Green_threads)), while I'm hesitant to use Wikipedia as a single source of truth, I still find the differences quite small. I'm open to other viewpoints if you want to provide a little bit more context on what confusion this might cause since there are multiple coroutine implementations out there.
It will be faster in general, but watch for big data structures which don't change size much and are to be passed by value. Those are better on the heap. Rare exception but still...
See also https://medium.com/vimeo-engineering-blog/behind-the-scenes-of-av1-at-vimeo-a2115973314b.
I don‘t understand why the autocompletion is in the positive side? I try it nearly every day with vsc +Rust(rls) or Rust-analyzer. Also with neovim+coc. On Mac and Manjaro Linux. And it only works for simple cases!
I really enjoy it! It boots much faster. The feature I miss the most is crates from vscode
Are you sure about this regarding exception handling on x64? I read that Windows needed some context for this in x86 but I think the way they did exception handling changed since then. The `NT_TIB` stack info members seems to be linked to `_chkstk`. Again, I'm on a bit unknown territory here since I haven't looked at the asm on windows to check the prologue of our Rust code on that platform specifically, but remember that the `f` function does have a prologue and epilogue (it's not `naked`), the prologue should call `_chkstk` and I *think* this might be handled as a part of the prologue for `f` ( [https://docs.microsoft.com/en-us/cpp/build/prolog-and-epilog?view=vs-2019#prolog-code](https://docs.microsoft.com/en-us/cpp/build/prolog-and-epilog?view=vs-2019#prolog-code)). &amp;#x200B; I'm not saying you're wrong, but I'm also not 100% convinced it's needed in this context.
&gt; You can try to update winapi-kmd-rs for the newer WDK versions yeah, was planning to do so, was confirming something wasn't in place already &gt; Do you have any specific gripes with it? Not really, just it used to depend on nightly then and since then a lot of things have been stablised and new features lice proc macros have come as well as rust alloc. Loving the community though, used to never get replies out on other lang subreddits. It appears it is encouraged to come forward and help here.
For storing mappings from keys to values, a \`HashMap\` is a great default option. 300 is a small number of elements, modern CPUs perform around 50,000,000,000 operations per second, so you could do about 160,000,000 operations on each element and still have a runtime of less than a second. If you need to do a lot more operations than that or do it a lot faster, then considering the overhead of the map would be worthwhile, but it still might be the fastest option. &amp;#x200B; To do better than a hashmap, you'll need to take advantage of something specific about your problem rather than thinking about the general key-value problem. &amp;#x200B; For example, for only a few elements (lets say up to 10) it might be be faster to store all the elements in a Vec and search from the start to the end. If there are a lot of elements, but they are sorted, a Vec might still be a good option, and use bisection to find elements. &amp;#x200B; Alternativly, if the the keys are all strings with similar prefixes (like a dictionary of words, where the keys are "walk", "walks", "walking", "walked" etc, which all have "walk" as a prefix) then a [Trie](https://crates.io/keywords/trie) would be an interesting option. &amp;#x200B; Then there are fun options like [Bloom filters](https://llimllib.github.io/bloomfilter-tutorial/) if you have a lot of elements, but you have specific needs like checking for the possibility that a key is set very quickly. &amp;#x200B; But I think the main take-away should be that 300 is a small number, so pick whatever makes the code easiest to read, as that will make it easier to replace with something faster if you need to later (when you know more about the problem you're trying to solve, and can use specific tools to solve it that wouldn't make sense for a general problem).
&gt; Loving the community though, used to never get replies out on other lang subreddits. You're probably asking the _hard_ questions :-). &gt; It appears it is encouraged to come forward and help here. Help _and_ ask for help.
Aliasing - check out this entry from the "Rustonomicon" [https://doc.rust-lang.org/nomicon/aliasing.html](https://doc.rust-lang.org/nomicon/aliasing.html) The very short version - if you have a function that takes two pointers, you can do some great optimizations if you are sure they are always point to different places that don't overlap. In Rust aliased pointers are illegal, so you can do cooler optimizations.
It’s also much faster to run RLS/rust-analyzer with Sublime. I like it for performance mainly since I only do booting once. I miss crates too, I think I’ll take a chance at porting it for Sublime soon
If you have questions, shoot!. It's build with rust/actix/actix-web on top of our symbolic library which we built on top of gimli/goblin/pdb and a lot of other libraries. Both the symbolicator service is entirely open source and we're also contributing our fixes and improvements upstream into the specific crates. For instance we added basic line number information and omap support to the PDB crate recently.
I think what is mainly meant with autocompletion nowadays is symbol display/infobubbles with docs/signatures and so on. So far only IntelliJ Rust has good completion imo (But I use sublime since I really dislike slow / bloated editors)
I have a init() function in a library and want the Rust library to only print to stdout and stderr if this init-function has an integer set to 1 or similiar. How could I accomplish this most efficiently. Can you change the println! and eprintln! macro so they check for the flag? Instinctively, I would implement it this way: fn my_println(s: String) { if global_flag == 1 { println!("{}", s); } }
Have you used rust analyzer? If so how does it compare to rls? That'd be super awesome if you made a port!
Just read the link, it's x64 specific `_chkstk` checks uses the NT_TIB stored at `gs` to expand the stack for large allocations, and Windows will use the same NT_TIB to handle SEH for that thread. If the NT_TIB isn't correct for the executing stack, you either access an invalid page and segfault, or [Windows assumes you are malware and kills you](https://devblogs.microsoft.com/oldnewthing/20080215-00/?p=23443). Now I'm not saying you have to obey Chen and only use the WinFiber API, but you can't just naively switch stacks on Window either.
Yes this things work most of the time. But when it comes to third party crates they sometimes break too. I also use WebStorm with the great rust Plugin. But I also don't like such bloated editors and that's why I keep an eye on all other options.
[https://twitter.com/read\_rust](https://twitter.com/read_rust)
https://twitter.com/rustlang generally retweets either interesting rust stuff, or people asking for help.
/r/playrust
`dav1d` should be the decoder of choice. And yes, it is available via FFmpeg. SVT-AV1 will probably become the encoder of choice soon, at least for the short term future. rav1e's progress is steady but slow(ish). Hopefully, enough attention (and resources) will be given to aspects like psychovisual optimizations and **user-tunable** parameters that will, in the long run, lead the encoder to be the top choice for many.
I was going to say this announcement is probably 6 months to a year too early. But since they are not fully rolling AV1 support yet. They are just testing and publicly pledging support for rav1e, I guess the timing is fine. While its speed is more than passable (for an AV1 encoder), rav1e's quality is not there yet (check my other comment). ----------- Shouldn't we stop posting stuff like this if development-related issues are not discussed? Isn't Rust *big* enough already?
Have you tried [7 Billion Humans](https://store.steampowered.com/app/792100/7_Billion_Humans/)? It’s a game by the same people, with a bit higher-level language, but your program is executed in parallel by multiple workers. Creating a nicer higher-level language that would compile to the game’s one might be an interesting project.
I've bought it but I want to finish world of goo first before I dive into 7 Billion Humans :D It definetly would be a nice project and it crossed my mind before - you could even modify the instruction set a bit as a first step with relative jumps / adress registers etc.
I don't think you can change the behavior of the `println!()` / `eprintln!()` macros, but making new wrappers ones shouldn't be too hard. I imagine this is something like what you're imagining? static PRINTLN_ENABLED: AtomicBool = AtomicBool::new(false); macro_rules! p { ($($arg:tt)*) =&gt; ( if PRINTLN_ENABLED.load(Ordering::SeqCst) { println!($($arg)*) } ); } ([playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=9b79863d1df7e442855deada04caba3a)) Since the global variable can be changed at runtime, it needs some sort of synchronization. Atomics are good for that if you just have one primitive to keep track of - fairly low overhead as I understand it. --- With that said, if you want to get more complicated with this, I would recommend checking out the `log` crate. It has various log macros (`info!()`, `debug!()`, `warn!()`, etc.), all sending messages to the configured global logger which can then handle them as it decides. This could be potentially nicer if you end up needing more than a simple on/off switch or want to coordinate things across multiple crates.
One use case is for Rust-to-C string conversions! If you want to use a string in calling FFI methods multiple times, it can be useful to convert it upfront in safe code using `CString`, and then pass a `CStr` to the FFI crate. This can avoid having to allocate a new null-terminated CString for every single FFI call which needs it.
You could use a function like that. For convenience, you can write a macro that forwards to `println`: ``` macro_rules! my_println { ($( $arg:tt )*) =&gt; { if global_flag == 1 { println!($( $arg )*); } }; } ``` That way, you can use `my_println` exactly as you would `println`: ``` my_println!("Hello, {}", "world"); ```
From my understanding, libaom was really just aiming to be a completely accurate/correct implementation of the encoding algorithm until one steeped in speed came along.
rust-analyzer, the RLS-2.0 [https://github.com/rust-analyzer/rust-analyzer/](https://github.com/rust-analyzer/rust-analyzer/)
Ok I got this up and running! Thanks, I sort of understand this little dirs crate a little more now. I got this opening to the right file path as well after realizing that "path" was printing as a vector, because I had changed the path variable after a reuse later down in the function. Changed that to another name, and it seems to just work now!
As long as the underlying type implements `Copy`, which primitive types like `bool` do.
Is that the way to go? At least for debugging I've seen debug-macros some time, which I presume can be activated with cargo?
Picking a better data structure. &amp;#x200B; Layout the data so you not need to work (aka normalize and denormalize like in relational model). &amp;#x200B; For example, if your data is already sorted you not need to sort it :).
Yeah it's a bummer it's not AGPL3
You're getting downvoted a lot.. Can the down voters please explain why? Are this persons statements incorrect?
[Rust Optimization.md](https://gist.github.com/jFransham/369a86eff00e5f280ed25121454acec1) is an interesting read about optimizations.
&gt; You can deference a value without taking its ownership. So what you are saying is that ou can deference a value without taking its ownership?
Have you considered using convolutional neural networks for your AI, as pioneered by DeepMind?
Ideally, an IDE should be able to defer the actual editing to an actual full-fledged editor and embed it instead of emulate a few behaviors. Like what neovim has been trying to push for at first.
What's the difference between GPL3 and AGPL3 that makes AGPL3 better/easier to work with? More permissive?
Those kinds of structures should generally be passed as references right? Which will compile into pointers?
Yes. References/pointers are usually preferred. As I said, heap is for the rare case that you'd want to move it.
Most likely because of the passive aggressive tone
That's fair. Just hoped that if it was somehow incorrect they'd at least correct s/him. Licensing always confuses me and I've been excited about Sequoia. I'm thankful that /u/rebootyourbrainstem mentioned it, because I wouldn't have thought the license would conflict. This is why I always choose the most open license I can find. I wish more FOSS did. But I can understand why they don't.
I don't believe I see any mechanism in `f.poll_sync_all()` that would "remember" the handle of the first task. It's simply a `tokio_threadpool::blocking` blocking call that wraps the standard libraries sync all function.
&gt; As long as the underlying type implements `Copy`, which primitive types like `bool` do. As long as the underlying type implements `Copy`, which the vast majority of types don't. :) let x = *&amp;true; // :D let y = *&amp;String::new(); // :'(
The IntelliJ's Rust plugin *can* expand macro_rules - you just have to enable it in the plugin's settings. It's still in testing phase, but seems to work really nice.
Sure but we are talking about booleans.
OK, I see. I thought this was handled by the `seh_*` commands LLVM emits in the prologue, but given your explanation I don't see how that can make any sense. Thanks for pointing this out. I'd love to keep this working correctly for Windows as well so I'll see what a minimal change to account for this will look like when I have some time. In the end this could actually be a nice addition.
Makes sense, is the stack very likely to be on the CPU cache, is that the reason its so fast?
The `blocking` call is what can return `NotReady` before calling the sync function. It will check if there is a "blocking thread" available (there's a configurable max), and if not, it joins a queue and returns `NotReady`. Once there is a blocking thread available, the fs function will only be called once.
ICOP is great when you provide the buffer, because under all the nice layers the driver can tell the device (HDD, SDD, NIC, etc.) to DMA the result into your buffer directly. So it's zero-copy as fuck. Of course cancellation in this scheme is problematic because you have to wait for the kernel and the underlying driver to cancel the DMA setup to not accidentally overwrite your stuff. Though, of course, the runtime/language/library/ecosystem should make a fire and forget cancellable abstraction all over this. After all, it doesn't matter if you own the buffer or the runtime, if it gets managed properly. (You pass the buffer to the runtime, it takes ownership, and when you cancel it, the runtime will wait for the cancellation and drop the buffer, if not, when the request is done, the result is in the buffer, you get it back and then when you are done processing it, you drop it.)
So basically every cancellation is just launching a new future that takes ownership of the buffer, waits for ACK of cancel and drops. Sounds like tombstones in distributed storage systems.
I've tried this too. Doesn't work.
Indeed, but there was nothing in the original comment to indicate that the specified behavior is not a universal truth, which is context that my first commented was intended to provide. :)
It's true that it's more likely to be in the CPU cache, but the most important factor is that allocating/freeing on the stack is a single machine instruction, where allocating on the heap involves calling `malloc()` and `free()`, which internally must do a lot of work (comparatively). For example, they must usually take locks in order to support the fact that pointer ownership can be passed between threads, as well as run some algorithm to figure out where the allocation would fit, and if it doesn't fit in any recently freed block, request more memory for the process via the operating system (usually the system call `mmap()` on modern Unix-like systems).
Nah, nobody is ever ready for ASM. You just have to expose yourself to it more and more, and it'll just eat itself into your body :) Permanently :o It's very much like all the low-level black magic stuff that happens in CPUs ( https://www.infoq.com/presentations/click-crash-course-modern-hardware/ ) and during chip manufacturing ( https://www.youtube.com/watch?v=KL-I3-C-KBk ).
Why is the actual content of your post only like 10% of the Website? (without scrolling) https://imgur.com/a/PcmI8JT
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/mSZvpdu.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme)^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20er5l4r5)
Unfortunately not my area of expertise :)
I tried it with coc.nvim, it used quite a bit of RAM (a couple of GBs). I ran back to RLS pretty quickly, which only uses a few hundred MBs.
Do you have any recommendations for resources about how a debugger works? I’m interested to learn how one works under the hood (the article gave a nice overview, btw), but I’m dubious about diving straight into the source for something like GDB.
Not really. One important part is that we did not build a debugger. We take a memory dump post mortem to stack walk it. That’s an operation a debugger does as well, but a debugger does more since it can hook into the execution of the program. We don’t need to do that thus we did not really investigate it much.
Right, just thought there may be some overlap since a big part was reading the debug symbols. Thanks
That's great thank you! We're in the process of upgrading to unity 2019 at work and it would be awesome if we could use some rust with it. Not sure if it'll work that well though since we're also targeting the web... We might be able to get away with having some JavaScript glue between two wasm modules, but it might not be worth it in the end.
Why do the `PartialOrd` implementations on tuples require that the types are also `PartialEq`?
For the symbols we mostly went by reading source code, specifications and even contacting people we think might have domain knowledge. The blog by /u/brucedawson is amazingly useful for instance: https://randomascii.wordpress.com/ The breakpad code and mozilla’s stuff (tecken, socorro) as well as the llvm source code is really useful.
I was able to get it working. The current, working changes are pushed.
Ah, yes. If only I knew how to read. Thanks for so patiently pointing out exactly what the page says haha... &amp;#x200B; It is working with this configuration. But I still don't know why the exact steps in the tutorial don't work. Is it a bug? Someone shouldn't have to go to the Advanced Setup section to get the hello-world equivalent tutorial to work.
Scrolling is natural and fast — that post is long enough that you’re going to be doing it in any case and if the top navigation wasn’t reasonably sized you probably (and Google definitely) would be complaining about touch targets being too small &amp; close together.
Thanks for the guidance! As it turns out the interface I'm working with can only handle one response at a time, so I've had to implement it with an offset option, but that doesn't really matter as far as the relevant code is concerned. Here's how it's implemented now: [GitHub commit](https://github.com/SamHH/bukubrow-host/commit/3f2b5cd7de7eabb8c14ead74c9483f03b13ff138#diff-4ce93534efc34e923ce01e975eb7ed80R162) As you can see I'm serialising each item in a loop, and then once I hit the limit I'm going back and re-serialising everything up-to that index or near it. I'm also moving stuff about between vectors and slices... I don't know but I imagine what I've written isn't terribly efficient. Any immediate advice looking at that? Unsure how to add to/remove from a serialised array with serde as you go.
Makes sense
&gt;Isn't Rust big enough already? For me it will be big enough when it has as many job offers as C++.
The ownership system is orthogonal to linkage (static, dynamic, etc.). It is connected with the type system, i.e. completely a compile-time concern. Bear in mind that Rust does not have a stable ABI, so using dynamic libraries written in Rust from other Rust code comes with a few caveats. For example, both the plugin and the app must be compiled with the same version of the compiler. This can be fine if the app and plugins are distributed together. The best portable approach may be to define the plugin API in C-compatible (unsafe) functions. However, this has the drawback of requiring the plugins to either statically link the standard library (binary size bloat), or avoid the standard library (no_std). Both not entirely satisfactory.
The header .gif on this post is 13.9 MB :O https://i.imgur.com/2fxRhix.png
Unfortunately, an AI like that takes far too many computational resources for a normal person to train, especially as the complexity of the game goes up. Leela Chess Zero only just beat Stockfish and has played over 220 million self play games.
Check out references here https://github.com/starfleetcadet75/rdbg/blob/master/README.md
Oh man, this is perfect, thank you so much!
If you are using Rust 2018 edition (`edition = "2018"` in Cargo.toml), [you need to use the `crate` keyword to reference items exported by your own crate's `lib.rs`.](https://doc.rust-lang.org/edition-guide/rust-2018/module-system/path-clarity.html) For example, `a.rs` needs: use crate::c::C;
Check out the blog for the `crossbeam` crate. There was a post with an absolute boat load of links to the kind of resources you’re looking for.
This is a guess, but can it be related to which edition you are building with? In the 2018 edition, module paths from the current crate are prefixed with with `crate::`. You can try swapping between the 2015 and 2018 version in [this example](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;code=use%20a%3A%3AA%3B%0A%0Amod%20a%20%7B%0A%20%20%20%20pub%20struct%20A%3B%0A%7D%0A%0Amod%20b%20%7B%0A%20%20%20%20use%20a%3A%3AA%3B%0A%7D) to see what I'm referring to.
&gt;This is why I always choose the most open license I can find. I wish more FOSS did. But I can understand why they don't. &amp;#x200B; Most open may add some confusion to what you mean, in context I'm guessing you mean most permissive. Most open could reasonably be interpreted as most copyleft too. Alternatively maybe you mean a license which is compatible with a maximum of other open licenses.
Sidenote: Your license file differs from the license in the readme - namely the copyright line is different.
I became much more happier since I’ve stopped persuading each and every program to use my custom keyboard shortcuts and instead start remapping CapsLock+key on the lower level. That way, I don’t have to use arrow keys at all :-) I’ve got the idea from https://manybutfinite.com/post/home-row-computing/
Ah, thanks, I must have copied it from that org and forgot to change it. I'll be sure to fix that before publishing.
This is a great idea. Actix definitely seems well suited to this task. Keep it up!
https://medium.com/vimeo-engineering-blog/behind-the-scenes-of-av1-at-vimeo-a2115973314b - this states that its quality is in the same ballpark. ---- IDK, I appreciate announcements like these. A Rust project being the *de facto* media encoder is huge.
&gt; Does Rust always share the Heap in Plugins? If heaps are shared between DLL's (or .so's) is platform specific; Linux, Windows, Darwin, FreeBSD, and OpenBSD handle this differently. Rust doesn't attempt to bridge this gap as this could add a massive amount of runtime complexity. Presently Rust doesn't have a _standard_ ABI, so dynamic linking is UB. Generally speaking you cannot (and should not) expect `panic!` to correctly unwind through a dynamic call, or different compiler versions to produce code which will correctly link together for identical symbols. &gt; Is there some form of fat pointer that keeps owned plugin data delete-able by the app? Nope. &gt;Does the ownership system works between dynlibs/app equaly great as in non-dynlib apps? Nope --- Dynamic linking is this _worst of both worlds_ solution that sadly got a new breath of life in the 80's when Sun Microsystems required a way for people not to notice that X11 Client Programs required _all your RAM_ so you could forget running more then 1 at once. Also it has all the legacy cruft of early mainframe segmentation where you'd just dump address `0xBEEF - 0xDEAD` to disk, and call it _code reuse_.
I think the most interesting problem for IDE to solve is providing a language runtime for plugins. Rust, as a language without runtime, can’t solve this. It may find its uses in making actual pixels on the screen light up. Maybe, if you design runtime with zero-cost safe native interop in mind, it could also be a good fit for lowish-level data structures like text buffers. Like, what makes VS Code work s the JavaScript runtime. The mountain of C++ underneath is sort of an implementation detail.
Is actix well documented now? I know actix-web is documented, but the underlying actix wasn't the last time I looked at it.
&gt;Another example is zlib where the main code is still written with K&amp;R C syntax. It even includes a script to turn it into ANSI C :) https://github.com/madler/zlib/blob/master/zlib2ansi
I'd say it was well-documented, except for the lack of high-level explanation of "I have x scenario, what do I do?" A lot of my initial code was based off of simple implementations like actix-redis, and then I found some nicer ways to do it such as `Actor::create` that give you access to the context during construction.
Rust is not really in a fantastic position when it comes to dynamic linking scenarios. FWIW, I wrote a smart pointer ( [ARef](https://docs.rs/reffers/0.5.1/reffers/aref/struct.ARef.html) ) years ago, and transferring ownership over a FFI boundary is one of its use cases. As long as what ever the pointer points to is also transferable, it should be safe. Might be one piece of the puzzle.
/r/playrust
I think the gp was sarcastic, since the AGPL is even less permissive than the GPL itself : \- with the GPL you must release your code to your customers (not necessarily to everyone) if you give them a compiled binary a a GPL product (and your product must be GPL if you use a GPL library in it or any other kind of GPL dependency) \- with AGPL, you must give them the source code even if you don't give them a compiled binary but simply a network service. It was invented to close the «ASP loophole», where a «rogue» company would use GPL software server-side, and still make money out of a proprietary service (think any SaaS provider using a Linux distro server-side, if Linux was AGPL-licensed, they wouldn't be able to use it unless they distributed all their source-code).
Simon Brand has a great talk about this: https://youtu.be/0DDrseUomfU . Despite what the talk title says, it is not very C++ specific.
There's no data structure that can allow you to always have values sorted unless you use [Intelligent Design Sort](http://www.dangermouse.net/esoteric/intelligentdesignsort.html)
BTW if you usually prefer GPL, but worry about excessive "virality" when used with static linking, you can use MPL. IIUC it does not require users of your crate to disclose source codes of their whole application, but demands sharing changes if your code got modified.
I think you misunderstood badly what I'm saying. &amp;#x200B; \[1, 2, 3\] &amp;#x200B; is already sorted.
Agreed.
I have considered this, and Glyn (the founder) really wants me to do it also. Writing an AI chess game requires 1. Interface code, passing boards, etc 2. Given a board, find all possible moves 3. Apply and reverse moves 4. Score boards (partially by trying different moves) and cache results And a bunch of little things. Now, Using the convolutional AI requires all these things, plus also AI awesomeness. So I thought we should start with something traditional and 100% guaranteed to work, then when we have the time and resources we can go do down the fully "human-like" AI path. Glyn and I are hopeful that the existing game's ruleset will become a great test-bed for new AI techniques or research, since it is "harder" than chess and more dynamic.
"Look ma, no copies!" https://github.com/dpc/rdedup/wiki/Rust's-fearless-concurrency-in-rdedup#avoiding-copying
Please post the backtrace you get when executing Rust with `RUST_BACKTRACE=1`.
Great, now when that rare construct will actually occur, we can happily go through internet and have no clue what the fuck is that and ask again instead of having it documented. Very cool.
Struggling understanding the API docs. I'm looking a simple piece of code: let reader = BufReader::new(file); let lines = reader.lines(); To an OO programmer it tells that `BufReader`'s docs should describe the method `lines()`. 1. `rustup doc --std`, search for `bufreader`, there it is, click. 2. Look for the `lines()` method. It's not there. Dead end. Feels like I'm doing something wrong. How.. I mean.. When a method is called, what is a way to look up its docs?
you don't sell source, you sell support, trust, coustomizability. Source code is a liability, not an asset. Sharing is not bad.
I am trying to compile the example code of 'yew', but I can't seem to compile it, not by fault of the code, but because I seem to be missing a dependency: [https://bin.privacytools.io/?ce9a2b2dcf00f120#zsmzl0e1Iwxh2kKTAD2pNdUS0F0MjCyW05pYudIPdLA=](https://bin.privacytools.io/?ce9a2b2dcf00f120#zsmzl0e1Iwxh2kKTAD2pNdUS0F0MjCyW05pYudIPdLA=) &amp;#x200B; I tried to install cargo-web, add wasm32-unknown-emscripten and compile it using nightly, but to no avail. I'm using Pop OS! 19.04 (Ubuntu based) I have cmake and build-essentials installed, is there more I could've missed? &amp;#x200B; Using 'yew' 6.0 with the minimal example code from the documentation: [https://docs.rs/yew/0.6.0/yew/](https://docs.rs/yew/0.6.0/yew/)
Ada not even mentioned? Shame.
It's mentioned.
It is mentioned. But it is a dying sight in aerospace. Right now this industry moving to C++/Linux as well.
Oh, yeah. Missed that.
It's still documented in the reference, and applying lifetimes to types is still there somewhere, I assume? Just this particular usage of `T: 'a` isn't part of the book until a better non-compiling example can be found. We should probably make sure it's still documented somewhere! But the book isn't the _only_ resource.
you may enjoy my talk about simd intrinsics. and or my crate simdeez https://youtu.be/4Gs_CA_vm3o https://github.com/jackmott/simdeez
the primary reason is allocating on the heap requires some work, on the stack requires almost none. just bump stack pointer.
When I launch it?
not to burst your bubble but ``actix-net`` is *the* core library that is the engine behind actix-web 1.0 and a replacement for the actor architecture
In retrospect I could have worded it differently. The first part was unironic though, I think it's a really cool project. Also let me be clear that everyone is free to release their software under whatever license they want.
Thanks for the clarification.
How do you convert the result of an `Iterator.map()` to `Iterator`? `map()` produces `Map` and `Map` implements `Iterator`, but when trying to assign the `Map` to an `Iterator` variable, compiler says expected trait std::iter::Iterator, found struct `std::iter::Map`
You have to assign the `Map` to a `Box&lt;Iterator&gt;`. `Iterator` is a trait, so the size of the concrete type can vary. A `Box` is dynamically allocated and allows us to sidestep the problem of not being able to store a DST on the stack. Alternatively, you could assign to a variable of type `impl Iterator&lt;Item=...&gt;`, as that refers to the concrete type (which you cannot reference without this syntax) and can be allocated statically on the stack since it is not a DST.
How do you properly apply MPL? I've done a little bit of research into popular MPL projects, and they all add the license into every source file, which seems like a lot of bother. Is that required by MPL, or is it just what companies like Mozilla like to do?
I am assuming that the runtime is needed in order to make sure that the plugins are safe/secure? Or is there a different reason why a runtime is needed? I am not that familiar with how plugin functionality is added to applications.
A blog post summarizing what you’ve learned about using `actix` from a high level would be very useful.
There’s a Mastodon account for this, as well
Yes, that is definitely coming soon (probably when I finish the basic bot framework). I'll post the link here when it's done :)
Forth has been debugged in our outer solar system...
IANAL but I just checked this, MPL says this: ``` “Covered Software” means Source Code Form to which the initial Contributor has attached the notice in Exhibit A, the Executable Form of such Source Code Form, and Modifications of such Source Code Form, in each case including portions thereof. ``` How a license is attached to source code is a bit open for interpretation. GPL has a similar situation. MIT is a bit more general. Note that for you as the author of the source code there is no risk. When your users distribute your code they need to make sure it's got the license properly attached. In my opinion putting a license file in the root of a repository is attaching it to the source code form, so I would gladly use a repository set up in such a way. But I'm not a big mega corporation that has to worry about how a judge might interpret the wording in a license.
Thought I'd check the stats on resource use - currently it's using 0% CPU, and has allocated 2GB of ram to itself, but is using only 677MB of that allocation. The IDE has a little resource meter in the bottom right too, which you can click to free resources up (now using 196MB of the allocation). *checked on a multi-crate 8000 LOC project.
Fair point. It just saddens me that it feels complex from my angle, is all I meant :)
Talking specifically about what I do in abi\_stable (a library for Rust-to-Rust ffi,with a focus on creating libraries loadable at program startup),cannot speak to how it would interact with other systems: **Every answer is the about approach I take in abi\_stable(not about Rust itself).** Allocation and deallocation of memory is done through virtual dispatch,since dynamic libraries are not guaranteed to use the same allocator IIRC allocation/deallocation has to happen in the same dynamic library for the same heap data on Windows,which I discovered after deciding to use virtual dispatch to allocate/deallocate. \- Does the ownership system works between dynlibs/app equaly great as in non-dynlib apps? Yes,you can pass `RBox&lt;SomeType&gt;` (a uniquely owned pointer)for example,between abi\_stable-using dynamic libraries and they will be deallocated as soon as its dropped. abi\_stable stores metadata for every type that's reachable from the root module(a struct of function pointers) defined in a static exposed using the `#[export_root_module]` attribute.This is to check that the types are compatible at load-time,giving a detailed error message for what is different between those types(I really need to reduce the verbosity),this has saved me a few times while developing the library. \- I've only tried compiling `cdylib`s which statically link every dependency that's not itself declared as a `cdylib`,I don't know how well it would work to link the standard library of the same Rustc version for every dynamic library at runtime(or how to do it),or to compile the layout checking code as a separate dynamic library.
If BSD (os) was under GPL, Mac OSX would be Open Source (or not existant). So hard to tell what is an "open" license.
Found [this similar issue](https://github.com/koute/stdweb/issues/101), does it help?
The difference is that coroutines can be, and often are, used without a scheduler getting involved. When a coroutine yields, it gives control back to the coroutine which invoked it. Having a coroutine implementation lying around means you can implement the sort of cooperative green threading described in your post trivially, but they are not quite the same thing. I think the fact that most green threading systems are preemptive rather than cooperative adds additional confusion. Cooperative green threading occupies a space sort of half way between coroutines and the most commonly used sorts of M:N threading. What I really take issue with is putting goroutines (preemptive N:M threading) and coroutines in the same category. There is a solid argument to put cooperative green threading in either bucket (there's a runtime it's also cooperative so...), but making it seem like there is one big bucket is just a barrier to understanding. To illustrate the point think about how coroutines and goroutines are used to do IO. Both allow you to perform IO efficiently, but with coroutines you make a series of asynchronous calls and manually juggle promises while with goroutines you can just spin off a worker goroutine and have it make blocking calls until it finishes doing IO and then have it send a result back. The experience of using them is really different because they are different things.
Can you provide some context? It sounds like you're trying to do something that Rust doesn't really support.
I have a function that takes a sequence of strings. Therefore `Iterator`, right? Now I want to get those strings from file and for that do `BufReader::new(file).lines().map(|x| /* ... */)`. `map()` to convert io result to string. That gives me the `Map`, which I would like to pass to the function that wants an iterator. And that doesn’t work. It feels natural to do that in kotlin but I struggle to grasp how to do that in rust.
Lmao sorry
It's unusual that a function would want an Iterator. Can you post the function signature. Most people would write a function that accepts a generic Iterator&lt;item=String&gt; or accepts IntoIterator.
raphlinus shifting focus to GUI toolkit stuff, and some of their more interesting/ambitious experiments have apparently not turned out well; there's some background in the thread below, but specifically the CRDT scheme and collaborative editing, as well as the async plugin framework are mentioned. https://github.com/xi-editor/xi-editor/issues/1187
Looks like you picked up Rust since the last time we talked!
Yes indeed, loving it. Don't find myself wanting to write anything else anymore.
The keyword you are looking for is "simd intrinsics". Google that. I found [this video](https://m.youtube.com/watch?v=4Gs_CA_vm3o) on the first page. Also intrinsics can be found in std::arch
Hey sorry if my question wasn't clear. I'm familiar with the std::arch module and the simd intrinsics rust provides. My question was about calling conventions for functions that are passed simd vectors as arguments - and whether it's possible to pass vector arguments in registers directly, which is an optimization that the `__vectorcall` specifier provides in compilers like clang and msvc.
Oops
No worries, appreciate the reply
Don’t get me wrong this looks really cool, but wouldn’t it be better to make a GPG frontend? It would do the same thing.
I like libsodium, have used it before, and saw that it had a Rust wrapper so reached for it. Having a standalone binary (except on Mac, which is a normal .app bundle) that can easily be carried on a flash drive and not having to install something like GPG was a good part of the motivation for this. Plus, I wanted to use Rust and learn FFI with C++. As for better, my approach is small and simple, and I think I used sodiumoxide's stream encryption properly, but please let me know if you see anything that can be improved: https://github.com/spieglt/Cloaker/blob/master/core/src/lib.rs.
I like Tokio over Rayon since Tokio is multithreaded plus and event loop.
Oh yeah it’s no doubt you did a great job, I was just wondering. Thanks for the reply!
Why use Box if you want to minimize heap allocations?
Thank you!
trying to read Steve's thoughts here: is it massive annoyance, covered up to keep cool on stage? Is it genuine humor? My guess is the annoyance.
You are most welcome kind sir :) Rust can be compiled into a web assembly. I know it is possible but haven't looked into how. Seems likely you could find a way to call it from Unity. If I were researching that I'd start by seeing if there is a way to call standard 3rd party javascript libaries like jquery from Unity. Then I'd learn about how to make Rust offer APIs in the same way JQuery does - join the two together like a good programmer and you're set!
He looks amused and like he's trying to think of a retort before proceeding.
It is! :)
I'm still thinking about this, but I wonder if a similar algorithm works for lifetimes. In the `longest` function example, we (well, the author) reasoned from the bottom up: fn longest(x: &amp;str, y: &amp;str) -&gt; &amp;str { if x.len() &gt; y.len() { x // "innermost" expression } else { y // "innermost" expression } } We look at the return values first, see that both `x` and `y` are among them possible return values, and make a conclusion about the relationship between the lifetimes of x, y, and the return value, namely that the lifetime of the borrow (loan?) that the function returns must be at most the shortest lifetime between that of `x` and that of `y`. Then we proceed to the call site: fn main() { let string1 = String::from("long string is long"); let result; { // Inner most let string2 = String::from("xyz"); result = longest(string1.as_str(), string2.as_str()); // The scope string2 lives in ends; // any loan from string2 must be dropped. } // Outer println!("The longest string is {}", result); // The scope of string1 ends; // any loan from string1 must be dropped. } Again, reasoning from inner to outer scope, we compute a constraint relating the lifetime of `result` to that of `string2` (and `string1`) based on what we have already computed for the `longest` function. That constraint is, the lifetime of `result` must be contained within the lifetime of `string2`. Since `string2` goes out of scope at the end of the block, `result` must drop its reference. In the version above, we instead have the contradictory constraint that the lifetime of `result` must include the outer scope. The theorem prover detects the contradiction and reports an error. Alternatively, imagine that `string2` is instead declared in the outer scope. Then the theorem prover would move out to the outer scope and continue its reasoning, bottom up. The lifetime constraints for `longest` propagate to each call site, but each call site is independent of any other with respect to the consequences of those constraints for the scope of the call site (ignoring recursion). I may try to cook something up in Soufflé this weekend if I have the time.
I always have to pause and think about the meanings of complete and consistent whenever I use them. Complete is too overloaded in mathematics. There's no reason for me to be confused about the word consistent, but because my brain is uncertain about complete, it thinks it should be confused about consistent, too. My brain is ridiculous. But neither complete nor consistent, apparently.
relevant? https://docs.rs/clang/0.2.2/clang/enum.CallingConvention.html
This would loose the property that you can know how the inputs and outputs of a function are related by just looking at the signature. This important property is what makes it significantly easier to understand and validate `unsafe` code, because you have an interface to check against. Also, this doesn't scale as to more complex lifetime situations. I should be more clear about how the current system works. --- Let's take a more complex example, fn foo(a: &amp;Foo, b: &amp;Bar, c: &amp;Bar) -&gt; Result&lt;&amp;Foo, &amp;Bar&gt; { .. } How should we annotate the lifetimes? We have a few ways, each has different semantics fn foo&lt;'a, 'b, 'c&gt;(a: &amp;'a Foo, b: &amp;'b Bar, c: &amp;'c Bar) -&gt; Result&lt;&amp;'a Foo, &amp;'b Bar&gt; { .. } fn foo&lt;'a, 'b, 'c&gt;(a: &amp;'a Foo, b: &amp;'b Bar, c: &amp;'c Bar) -&gt; Result&lt;&amp;'a Foo, &amp;'c Bar&gt; { .. } fn foo&lt;'a, 'bc&gt;(a: &amp;'a Foo, b: &amp;'bc Bar, c: &amp;'bc Bar) -&gt; Result&lt;&amp;'a Foo, &amp;'bc Bar&gt; { .. } Note that there are a few more signatures that we could write, but they would be more restrictive fn foo&lt;'x&gt;(a: &amp;'x Foo, b: &amp;'x Bar, c: &amp;'x Bar) -&gt; Result&lt;&amp;'x Foo, &amp;'x Bar&gt; { .. } fn foo&lt;'x, 'c&gt;(a: &amp;'x Foo, b: &amp;'x Bar, c: &amp;'c Bar) -&gt; Result&lt;&amp;'x Foo, &amp;'x Bar&gt; { .. } fn foo&lt;'x, 'b&gt;(a: &amp;'x Foo, b: &amp;'b Bar, c: &amp;'x Bar) -&gt; Result&lt;&amp;'x Foo, &amp;'x Bar&gt; { .. } Or equivalent to ones from before, for example fn foo&lt;'a, 'b, 'c: 'b&gt;(a: &amp;'a Foo, b: &amp;'b Bar, c: &amp;'c Bar) -&gt; Result&lt;&amp;'a Foo, &amp;'b Bar&gt; { .. } Is the same as the third annotated example. Now, each of these first six examples have different semantics. 1) the output does not depend on `c` 2) the output does not depend on `b` 3) the output depends on all of the variables, but the lifetime of `a` is independent of `b` or `c` 4) the output depends on all of the variables, but the lifetimes `'x` binds all of the variables together. This is more restrictive than 3 5) Same as 1, but binds `a` and `b`'s lifetimes together (too restrictive) 6) Same as 2, but binds `a` and `c`'s lifetimes together (too restrictive) So, we have 6 different variants for just 3 lifetimes. Depending on the function, it depends which one we want. Now, let's say we implement your solution, and it works perfectly. Changing an implementation detail in the function could switch which of the six types that function will fall into. Thus changing the public interface of this function. (Thus it is a breaking change to change implementations without thoroughly checking that *all* lifetime behavior remains the same or is made less restrictive). This is bad. Changing the implementation of a function should have 0 effect on whether a downstream crate even *compiles*. Yes the users may have to work around your change, but it shouldn't break their builds. --- Next, lets make a new function fn bar&lt;'a, 'b&gt;(a: &amp;'a Foo, b: &amp;'b Bar) -&gt; &amp;'b Bar { let x = foo(a, b, b); match x { Ok(_) =&gt; panic!(), Err(bar) =&gt; bar } } Is this function correct (wrt lifetimes). Well, if you picked either 1 or 2, then yes, this function is correct. (Notice how we don't even need to know the bodies of `foo` to know this). If you pick 3-6, then no, because the lifetime of `a` must be tied to the lifetime of `b`, but it isn't in `bar`. To be correct in all cases of `foo` you would need fn bar&lt;'x&gt;(a: &amp;'x Foo, b: &amp;'x Bar) -&gt; &amp;'x Bar { ... } Because that ties the lifetimes of `a` and `b` together, which is compatible with `foo`.
Interesting! do people use tokio for non network applications?
Don't put the decrypted file next to the original one. The encrypted file is most likely going to be stored somewhere permanent while you want the decrypted file to not be available most of the time. Most often people drag&amp;drop the encrypted file directly from that permanent storage. If the software puts the decrypted file into the same directory, it would put it onto the permanent storage. Deletion of files is recoverable in most of the instances and with modern SSDs exposing a virtual layer of blocks, even tools like shred don't help much. Therefore, only decrypt to ramdisks! [pass](https://www.passwordstore.org/) for example decrypts to /dev/shm which in linux is always a ramdisk.
This article feels like it ends right before it gets to its point. It says the use case for embedded is great without really describing what makes it so. People already familiar with rust probably don't need an explanation, but I'd have loved to send an article to my team saying the same things I've been saying. Oh well.
Point taken. I will expand on this in the next part.
&gt; doesn't help anyone Well that's also a little strongly worded. It won't help any companies trying to make profit directly out of it, but there might be other gpl software which would be happy to use it, no?
Thanks. Sorry if that sounded too critical. I also work in aero, and I'm really excited to see more action with Rust! I'll be keeping an eye out for your next post!
Thanks werecat for the feedback! I have updated the mutability annotations. I do have a follow up question for your comment about the `destroy` method. I did use drop implementation in the initial version, but then later ran into a lifetime issue. Basically after you call `api.set_image(img)`, `img` (Pix type) need to out live `api` (TessApi type). Because when you later call `api.get_utf8_text()`, tesseract will access `img`'s memory. I wasn't able to find a way to enforce lifetime check conditionally based on whether `api.set_image` method has been called or not. Do you have any recommendation on how to handle this case?
GPG is bad and no one should use it for anything (yes I'm exaggerating but also it is pretty bad)
Thanks for the link, I'll check it out. I was hoping there was some sort of function annotation that would handle this for native rust functions, but it doesn't appear to be the case.
It seems to be possible to declare an extern function with the vectorcall convention and define in natively as described in [this comment](https://github.com/rust-lang/rust/issues/48088#issuecomment-364466298). Seems very experimental and only works with `#![feature(simd_ffi)]` and `#![feature(abi_vectorcall)]` enabled.
go show some love a lot more insane content will be coming up i hope you guys enjoy this one
You want /r/playrust.
Well that’s just objectively not true
He's one of us now! Remember to join /r/rustjerk ! :D
You can still make money from it, the GPL allows you to e.g. communicate with the library using IPC, which e.g. many git clients do. They can also use the `sq` program. Inconvenient yes, but its possible. "Cost of doing business", as they say.
You too? I don't know why, maybe because the language is elegant and the teenager rebel in me loves the fact that in school I was taught that a garbage collector is the best thing since sliced bread and enables functional programming when clearly that is not entirely true.
Usually, when moving collections around in Rust, you'll want to use a Vec, which you can create from an iterator with [collect](https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.collect). Otherwise you'll have to use some kind of polymorphism, either static (type parameters like `IntoIterator`) or dynamic (trait objects like `Box&lt;dyn Iterator&gt;`.
Unless I'm mistaken, you can't write a function that accepts `impl Iterator&lt;Item=String&gt;`. `impl trait` can only be used in return types.
This post may be of interest: https://www.reddit.com/r/rust/comments/8bjhld/calling_fortran_code_from_rust/ I couldn't find any libaries specifically for rust&lt;-&gt;fortran interaction, but it should be possible to expose a C api from fortran and call that C api from Rust. The post linked above mentions using a module `iso_c_binding` in Fortran to expose a C api - I'm not familiar with it, but hopefully that can help. If that works, then here's some good documentation for the rust side when calling the "c" functions: https://doc.rust-lang.org/nomicon/ffi.html
It's not overkill when it's resources for dealing with *hell itself*. Thank you for this.
More info: https://twitter.com/github/status/1139665072618446850
The main thing about plugins is that they are dynamic and can be added/removed/changed at runtime. While it is possible to dynamically load compiled code as a plugin, by using shared libraries, it much more painful and less flexible than runtime based solutions.
You can actually catch panics... [std::panic::catch_unwind](https://doc.rust-lang.org/std/panic/fn.catch_unwind.html) So while they do not support it in syntax, there's still clearly a mechanism built into it that allows to catch them like exceptions. I am torn on this one honestly, I wrote a thread pool impl and I'm wondering whether user closures should be wrapped in this or not... I am "You fucked up, you go and fix it" kind of guy, but what if some code just can panic but application shouldn't crash and user should be able to handle the panic from inside? I never came up with excuse for that and I'm lazy so I never implemented it, but if the function exists, someone somewhere needs it, right?
In argument position, it's equivalent to generics. See [the current book's chapter on generics](https://doc.rust-lang.org/book/ch10-02-traits.html#traits-as-parameters).
Thanks @daboross. It was helpful. However, my code is fortran77 and I think I better re implement with rust so that I will also understand the algorithm part.
Thank you for your work on this project and for this essay! You inspired me to actively show public support in one of the communities I run even though I knew that it would unfortunately be a bit controversial.
`Iterator` is a trait - many things implement it. The result if map _is_ an iterator - it implements the `Iterator` trait. The error expected trait std::iter::Iterator, found struct `std::iter::Map` is a bit misleading, and I think probably the result of using the `Iterator` as a trait object, rather than as a trait. It's hard to say exactly what's wrong here without also seeing your code, unfortunately. If I had to guess, I think the function is asking for a trait object (`dyn Iterator`) when it really should be generic over any kind of iterator? If you want to take in an iterator - any iterator - into your function, the standard way to do that is to use `impl Iterator`. See [the book's chapter on traits](https://doc.rust-lang.org/book/ch10-02-traits.html#traits-as-parameters). If you write the signature function like this, it should work, and accept any iterator (including the iterator that `map` returns): fn takes_iter(it: impl Iterator&lt;Item=String&gt;) { ... }
Sorry if I got your requirements wrong and that's not what you need. You're trying to make virtual calls happen, although you already know at compile time which version of your printer you want to use. Generics are your friend here: the compiler will monomorphize your code to use the implementation that you really use. pub trait Abstract { fn print(); } mod concrete1 { use super::Abstract; pub struct Printer; impl Abstract for Printer { fn print() { // whatever you need here println!("foo"); } } } mod concrete2 { // ... } fn with_printer&lt;T: Abstract&gt;() { T::print(); } use concrete1::Printer; fn main() { with_printer::&lt;Printer&gt;(); } You give your function a `T: Abstract` type parameter, and can then call `T::print();`. Note how the struct is not even instantiated, and that there's no vtable in action. Of course, if you do have some state, you'll need a static, but you won't need a `dyn Abstract`.
I'm not a lover of a gpg toolset, because of it's than optimal UX, but your statement really lacks explanation.
Well, if they're already using compile-time selection of the implementation by a feature, then why not go one step further? pub trait Abstract { fn print(); } #[cfg(feature = "use1")] mod concrete { pub struct Concrete; use super::Abstract; impl Abstract for Printer { // whatever you need here println!("foo"); } } #[cfg(feature = "use2")] mod concrete { pub struct Concrete; // ... different implementation. } use concrete::Concrete; static mut PRINTER: Mutex&lt;Concrete&gt; = Mutex::new(Concrete::new()); (haven't checked that the code actually compiles, therefore it probably contains errors). In this case, the trait would just ensure that the required API is available, and the concrete struct could be stored directly.
Any update on this?
There are many situations where allocating something on the heap is desirable, even if there is no shared ownership. For example, `Box&lt;T&gt;` can be passed around between functions without `T` changing its address (it can be "pinned"), and it can be passed to other threads. `Box&lt;dyn MyTrait&gt;` can contain any type implementing `MyTrait`, without exposing the concrete type to the user. An finally, it is problematic to allocate very, very large things on the stack, for two reasons: 1) the amount of stack memory available to each thread is limited, and there is no recovery if you run out, and 2) passing very large objects by value involves lots of `memcpy()`ing the object around, which can be slower than passing a single pointer, which is what `Box&lt;T&gt;` is internally.
This is what I came up with as well originally, but I don't like very much the fact that it doesn't enforce having to use a subset of the interface during development. For example, lets say that "Concrete" for "use1" has some extra methods available than "Concrete" for "use2". If "use1" is the most popular configuration that is used during development there is the risk that methods that are only "Concrete" use1 creep into the code and break "use2" compilation. &amp;#x200B; I would like to have something like traits, that enforces a subset of the interface, but with compile time speed. From that perspective Silly-Freak's suggestion of using generics instead might be spot on. Maybe if I wrap the real concrete class using a trait that has a limited interface and just "passes the calls through" it might do the kind of enforcing that I want! Thanks guys, I'll try the concept out! On a more "generic rust question note" , why making a static with a "Mutex&lt;&amp;(dyn Trait)&gt;" is something the compiler doesn't like mentioning \`(dyn Trait + 'static)\` cannot be shared between threads safely"
Thanks Silly-Freak for the answer. I think you are right , generics are the right solution here. I was trying to enforce an interface through the use of traits, but I should probably use generic instead of dyn here.
Hmm, yes... I know some of these words.
You could use CMake if your Fortran routines can't make use of `ISO_C_BINDING` and are relatively simple. For example, if you have the file `libfoo/foo.f`: subroutine foo(a,b,c,r) integer a,b,c,r r = a + b + c 100 return end Then you can write a small CMake build script in `libfoo/CMakeLists.txt`: cmake_minimum_required(VERSION 3.0.2) enable_language(Fortran) if(${CMAKE_Fortran_COMPILER} MATCHES "ifort.*") set(CMAKE_Fortran_FLAGS_RELEASE "${CMAKE_Fortran_FLAGS} -fltconsistency") set(CMAKE_Fortran_FLAGS_DEBUG "${CMAKE_Fortran_FLAGS} -fltconsistency") endif() add_library(foo STATIC foo.f) install(TARGETS foo ARCHIVE DESTINATION ${CMAKE_INSTALL_PREFIX}/lib ) In your `build.rs`: fn main() { let dst = ::cmake::build("libfoo"); println!("cargo:rustc-link-search=native={}", dst.join("lib").display()); println!("cargo:rustc-link-lib=static=foo"); } And finally in `src/main.rs`: use std::os::raw::c_int; extern "C" { fn foo_(a: *const c_int, b: *const c_int, c: *const c_int, r: *mut c_int); } fn main() { let a: c_int = 1; let b: c_int = 2; let c: c_int = 3; let mut r: c_int = 0; unsafe { foo_(&amp;a as *const _, &amp;b as *const _, &amp;c as *const _, &amp;mut r as *mut _); } println!("{} + {} + {} = {}", a, b, c, r); } --- You may have noticed the symbol is exported as `foo_` not `foo`. This where things get... *kind of* messy. Ideally you'd want to make use of CMake's `FortranCInterface`. You would need to add something like include(FortranCInterface) FortranCInterface_HEADER(${CMAKE_CURRENT_SOURCE_DIR}}/foo-mangle.h MACRO_NAMESPACE "F_" SYMBOL_NAMESPACE "f_" SYMBOLS foo ) To your CMake build script and maintain a C header that is in sync with your Fortran routines: #ifndef INCLUDE_FOO_H_ #define INCLUDE_FOO_H_ #include "foo-mangle.h" void f_foo(const int* a, const int* b, const int* c, int* r); #endif INCLUDE_FOO_H_ But the issue here is that the `foo-mangle.h` header defines `f_foo` as a macro that resolves the mangling, so you'll have difficulty generating valid bindings with `bindgen`.
&gt; you could use cmake if your fortran routines In Soviet Russia, your fortran routines could use cmake if **you**! ^(this post was made by a highly intelligent bot using the advanced yakov-smirnoff algorithm... okay, thats not a real algorithm. learn more on my profile.)
Don't put decrypted files on the HDD/SSD because deleting them again so that they can't be recovered is hard. Instead, just put them in RAM only because RAM is designed to lose its content when it loses power.
The crate provides a transaction primitive to allow atomic all-or-nothing operations.
So... It was "The journey of a lifetime"?
Thanks @rjsberry. It is indeed nice explaining the concepts of calling fortran from rust. Appreciate it.
&gt; Therefore, only decrypt to ramdisks! I assume you'd need to take advantage of some OS-specific APIs (if available) to ensure the RAM is never persisted to disk on hibernation or swap.
Yeah, taking this further, operating systems in general are very leaky in terms of what they put onto the disk. Thumbnails they cache, log files that contain relevant information, memory regions swapped to disk. The only good remedy is running the entire OS from a ramdisk per default and only saving relevant data to storage mediums. This is precisely what distributions like Tails do (or any live distro really). However, even with a traditional OS, this is not a sunken cost. It's common to have an encrypted file on an USB stick and decrypt it for usage. If the decryption tool writes the decrypted file to the stick, and if it's only temporarily for editing, it can be recovered from the USB stick later on. Sometimes only the USB stick is accessible to attackers but the HDD isn't.
Have you had a look at this article? https://medium.com/@richardanaya/a-journey-through-rust-lifetimes-5a08782c7091
dyn Trait is not sized, that's the problem. The implementation
Yes I wrote the documentation to get RA to work with sublime and I use it daily. The only drawback being the lack of support of Traits, but the speed is amazing. Also it doesn’t go crazy like RLS sometimes that just straight up starts to eat CPU cores and 10GB of ram alone.
I have the opposite experience here; RLS being feature complete but a slow, resource-eating monster, and rust-analyzer being fast and light, but lacking in features.
Can I teach Cargo to automatically increase the version-number in Cargo.toml with each git-commit I make?
Thanks for this article! The thing that I don’t appreciate in it, is Foobar. How to teach abstract concept (Lifetimes) with abstract names? It would have been much more powerful to take a real example, maybe Pizza { size: i32 } or User { name: str }. Anyway, thanks a lot for taking the time to share the experience!
The UI requiring you to explicitly specify encryption or decryption is unnecessarily error-prone. If you want to have a simple tool, go for encrypting or decrypting automatically based on file extension.
As someone calling Fortran from C and vice versa in complex programs, I wholeheartedly suggest that you ditch Fortran if you can. If you can't, update your code to Fortran 90/2003 and use ISO_C_BINDING for interoperability with C and go on from there to rust.
It's for anything that could be async. Networking is a big one, but if you are waiting for something handled by some other thing, even on your local system (maybe filesystem IO) I'm guessing it could be useful.
You are mistaken. Here's an example. https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=339ec3e314d999c32ed1cc6cd188c11c
DUA can now keep track of the traversed directory tree to allow quick discovery of where most disk space is taken, similar to NCDU. The latter was clearly an inspiration. However, initial disk traversal is multiple times faster, and DUA offers a safe way of marking files for later deletion. That way, you can easily clean your disk once in a while. During development of the \`tui-rs\` based interface I had the greatest of fun and learned a lot about what it means to manage state in immediate mode user interface. Never really satisfied with my solution of passing mutable references, I came up with something I called \`tui-react\`, which essentially allows UI components to have their own more persistent state, while still being immediate mode using render-properties. Another lesson learned is that representing a user interface in the type system makes it safe to work with. This goes as far as 'self-consuming' interfaces, which may possibly decide to go away entirely. Thanks to this work, I finally feel that I have added terminal user interfaces for myself to the wide space between command-line tools, and full-blown self-hosted browser interfaces. And with a growing selection of off-the-mill widgets in TUI, it should get increasingly easier to build them, quickly. Great fun to build, I hope you will like to run it.
https://tokio.rs/docs/overview/
Could the compiler infer a most general signature from the body?
Worst part is, I'm supposed to know the basics of ASM, I've coded quite a bit in it, but the idea of just inserting ASM into rust or cpp is just so fucking weird to me.
Can you post the full command you used?
Well, the message is pretty clear, right? It could say what option is unknown, but I guess that's easy to see here. What are you trying to accomplish using `--error`? That said, that's a pretty old version of the book. You might want to check out https://doc.rust-lang.org/book/index.html.
Awesome! Really one of my favourite Rust tools! :)
I have a function which gets a Vector and has to serialize its data for a socket. I could take the data, modify it and store it to another vector. Assuming I always knew how large the Vector entering the function is, I could use an array alternatively. I'm wondering which is better. In C it's clearly the array, is it just spawns with your stack frame. In Rust, you have to initialize the array, even if you want to only write to it, what presumably takes a long time. The vector on the other hand needs to call the allocater when it's created, what also takes time. So I wonder which operation is cheaper: Vector versus initalized array.
"Cargo build" as in the linked tutorial
Not exactly what you're asking for, but there's [Spacemacs](http://spacemacs.org).
It seems you are passing something after cargo build. Your title makes it seem you're using "cargo build --error" and as far as I know, that's not a valid option.
&gt; Well, the message is pretty clear, right? Is it? &gt; It could say what option is unknown, but I guess that's easy to see here. What are you trying to accomplish using --error? Trying to run the command "cargo build" in the tutorial. "error: Unknown option" is the response I'm getting. &gt; I don't even see that command on that page, maybe you were just mistaken somehow? Isn't "cargo build" the correct command? &gt; That said, that's a pretty old version of the book. You might want to check out https://doc.rust-lang.org/book/index.html. Thanks! But it suggest the same command to build and I am getting the same cryptic error: https://doc.rust-lang.org/book/ch01-03-hello-cargo.html
Give us result of `rustup show` .
Oh, -- was meant as separator between command and error message ... and it doesn't seem possible to change the title now.
$ rustup show . error: Found argument '.' which wasn't expected, or isn't valid in this context USAGE: rustup show [SUBCOMMAND] For more information try --help
&gt; Is it? Ok, you made it sound like you used `cargo build --error`. Seems you didn't, so I retract that statement :) &gt; Trying to run the command "cargo build" in the tutorial. "error: Unknown option" is the response I'm getting. I did, no such thing, all works out. Maybe delete the project and try again? You could have made an error somehow. If the error persists, you should post the exact steps and the output from the console, just start afresh and then copy and paste the whole console session.
Just `rustup show` no full stop
Can you post a full output of your cargo command? cargo build doesn't give me that error even in directories where there's no Cargo.toml file
There is a pretty good PGP implementation in Rust called Sequoia: https://crates.io/crates/sequoia-openpgp
This is awesome! &amp;#x200B; Two quick bug reports: The coloring is difficult to see with light terminals, and you have Aalyzer instead of analyzer. &amp;#x200B; Screenshot: [https://i.imgur.com/iqNqVCp.png](https://i.imgur.com/iqNqVCp.png)
This is literally the full output: $ cargo build error: Unknown option My directory looks exactly as described in https://doc.rust-lang.org/book/ch01-03-hello-cargo.html, auto-generated with `cargo new hello_world`.
$ rustup show Default host: x86_64-apple-darwin installed toolchains -------------------- stable-x86_64-apple-darwin nightly-2019-05-14-x86_64-apple-darwin active toolchain ---------------- stable-x86_64-apple-darwin (default) rustc 1.35.0 (3c235d560 2019-05-20)
You can probably teach git to do this (lookup commit hooks), but given that it is inherently decentral that sounds like a bad-ish solution once more people start working on that project.
Bug report: It says Aalyzer in the video :P
If this runs natively on Windows, that'll be huge too. I don't think NCDU does?
You could use a stack-allocated Vec, like [stackvector](https://crates.io/crates/stackvector).
You're right. Thanks for teaching me something!
I'm not sure it is that much a problem for that kind of object to be more heavyweight than others. I mean, I would be astonished if the extra management (that is not even *extremely* costly anyway -- of course is it still worse than not doing it but modern processors have tons of dedicated predictors for things like virtual dispatch) shows up in profiles. There is a huge difference between e.g. internal string objects used everywhere and a big entity object that *should* be far less instantiated and freed (\*). If an high bandwidth stream of things is needed across module boundaries (and they use different heaps or otherwise incompatible ABI in some dimensions, etc.) then maybe a transfer with a semantic by value would be a good idea (probably serialized) -- and if the size of the data is too huge and a copy is not wanted because of that, then probably the intrinsic amount of work needed to manage its lifetime even with complex schemes will be very small compared to the actual processing anyway. Also the actual heap management is probably way slower than one or even a few pointer calls and other slightly slow things. Even ARC should not be an issue outside of core loops -- and likewise but even more core for simple RC. Apart from that I don't think there is any other solution beyond deleter interfaces or shared heaps (if lifetime really needs to be managed across module boundaries) (\*): and btw that's why I'm not a fan of programming approaches trying to reduce everything to "objects"; "objects" for entity purposes are fundamentally different than "objects" for value purposes, and trying to conflate the two can quickly lead to various mix-up and confusions in designs and a meaningless and counter-productive search for common grounds. I'm fine with both being implemented with partly the same tools, but their difference has to be clear in the head of the programmer, and I have the feeling that education is lacking in that regard...
Are you typing that, or copy-pasting it? Do you have a `.cargo` file in your home directory? It not, my educated guess is that your toolchain is haunted. You can also try from a different user account.
You don't even need file extension, just insert some magic bytes at the start of encrypted files for designation.
Thanks for updating the library, making idiomatic and safe rust bindings is a non-trivial and sometimes complex task. I didn't mention it before but I'm happy to see people creating bindings of OCR libraries for rust. What I would do in this case is make another struct, let's call it `Recognizer`, whose constructor takes and holds on to a `&amp;Pix` and a `&amp;mut TessApi`. That constructor would run `set_image` and this struct would have the corresponding recognition API's on it, instead of `TessApi` having those methods. Now you can only run those recognition methods when your api is in a valid state (has an image loaded and both the api and the image are alive). Now I made some assumptions about how the library works since I didn't want to dig too far deep, so this might not be the best way to handle this, but it should give you some ideas of how you can leverage the type system to your advantage here. A lot of rust bindings around c apis will also employ empty api context struct whose whole job is to initialize and deinitialize the library, and ensures initalization happens by making all the other structs in your library need a reference to that context handle for their constructors. [Rust SDL2](https://github.com/Rust-SDL2/rust-sdl2) is a good example of this, take a look at some of the examples in their examples folder to see it in action. Also you probably want clean up your bindgen output, since it's emitting lots of completely unrelated functions from other c libraries, like scanf. I have found that whitelisting the structs, types, and functions works very well for this, and I wish the documentation talked more about doing this. I've done this previously with libobs bindings and it looked something like this. let obs_match_regex = "(?i)(obs|libobs).*"; let bindings = bindgen::Builder::default() .header("wrapper.h") .whitelist_function(obs_match_regex) .whitelist_type(obs_match_regex) .whitelist_var(obs_match_regex) .rustfmt_bindings(true) .generate() .expect("Unable to generate bindings"); In my case I was a bit lucky in that the library prefixed almost all of its functions and types with `libobs` or `obs` but it looks like your library isn't as lucky. In either case it looks like you have a small list of types to worry about so it shouldn't be too hard. These whitelists will also capture any function that takes one of the whitelisted types as an argument even if the regex didn't capture it for some reason.
It’s learning curb is a little steep, I guess that’s all I have for now .
Copy-pasting. No `.cargo` file but no file like that is mentioned here: https://doc.rust-lang.org/cargo/getting-started/first-steps.html Not sure what different user account means, it is really my first look at rust and I haven't seen anything mentioned about users on those first pages.
I think the biggest issue with rust is that the language is quite complex and has a bigger than usual learning curve. If you take up say go, you can learn it in less than a week. Rust on the other hand requires you to learn the ownership and borrowing model, generics, lifetimes, macros, special operators, pattern matching, etc. Unless you're coming from C++, it seems to take the average developer some time "fighting the borrow checker" before their productivity goes back to normal. Not that I would change it, the benefits you get from this complexity are, in my opinion, worth the time it takes to learn, but the learning curve hinders rust's growth.
The reason why OOP is so popular is that it’s a natural way of thinking. Everybody can understand that a table and a stool are furniture and thus share some properties. Rust not having that makes mapping real-world problems into code much harder.
The main pain point that I currently have is that the async story is only just starting to settle down and we've had some annoying difficulties with different versions of tokio and actix, and libraries that use old versions of one or the other that have never been updated (because it was unstable many such libraries haven't bothered to update while it kept changing).
- Compile times for large projects - Crate ecosystem is still very young, volatile and sparse - Because the community is young and still largely curiosity-driven, some essential crates have inconsistent maintenance since there aren't many maintainer organizations yet - Orphan rules make the glue-package approach (like Java uses) intractable and thus it's difficult to combine different parts of the ecosystem or factor libraries as much as one would like - Related to the above, we're probably still several years or editions away from a good dynamic linking/plugin story (but for good reason, related to the below) - The language's real potential still always seems one unimplemented RFC away (e.g., async, const generics, GATs, unsized rvalues, etc.) - A lot of the features really essential to write libraries of the quality one would like are still unstable (e.g., specialization) &gt; When would you use another language over Rust? Languages are tools that have suitability for particular tasks. Rust is great, but I'm still using Python for prototyping, scripting, non-bottleneck data munging; Java for the ecosystem; Julia; JS/Typescript; and so on.
It’s not an in-memory collection though, it’s a lazy sequence with potentially data. Would Iterator be a proper choice for that?
Is `actix-net` equivalent to the original package, or is it architectured differently?
I think you mean learning curve, but learning curb is a great expression.
You are so right hahaha
What if you type the command instead of using copy-paste? I asked about that file because it will br used even if it's outside of your project directory, like in a parent directory (not sure) or your home. I meant operating system account, you can make more than one on MacOS. That would help pinpoint your issue, since if it works then it's either your installation or your user settings.
OK, I'm inclined to agree with you that this might cause unnecessary confusion. I'll remove coroutines from the list.
Very nice! Can I ask for added keybinds? lowercase L, \`l\` should do the same thing as \`o\`, and \`h\` should do the same thing as \`u\`. This would be consistent with vim-inspired file managers like ranger.
It really depends on how you're using it in your code. If you're just assigning it to a variable with `let`, you don't have to specify the type at all, as the compiler will figure it out. If you're accepting it as the parameter of function or method, I would use `impl IntoIterator` so you can accept types like `Vec` which implement `IntoIterator` but not `Iterator`. If you're returning it from a function or method, I would use `impl Iterator`. And like I said above, using trait objects is also an option if you need that kind of flexibility.
We have traits. We have structs that can define data and functions.. You can write OOP is Rust fine. I usually write OOP in Rust.
Traits are about interface declarations, not unified data access.
The language contains multiple ad-hoc informally-specified narrow-purpose implementations of various halves of Monad.
My main gripe with Rust is my inability to just experiment with it. Often I have a rough idea of what I want to do, and I want to assess its feasibility by writing an equally rough implementation. But that's just not possible, because if I don't do it completely right from the get-go the compiler will scream at me. Ultimately it's a good thing because when I put that final idea into production, I want that solid static checking. But it can be annoying when just bouncing ideas around my head.
- Compile time might be long if you have a big project. - Writing Rust is hard as you need to fight with the compiler. This generally makes the code better, but sometime you just don't need to write the perfect code. - As others have pointed out, the learning curve is steep. More, even if you've worked with Rust for a considerable amount of time, you will still meet advanced topics along the way. - Some libraries might be missing or immature. Whether Rust is great or not depends a lot on what you are trying to do, Rust is not for everything. I know someone who tries to write everything in Rust, which doesn't turn out so well IMO, as he often struggles with very hard ownership problems, or even something that isn't in the language yet. The key is to know when to use Rust to fully take advantage of its features, and when to use less restrictive languages.
GATs?
[Generic Associated Types](https://github.com/rust-lang/rust/issues/44265)
And if the interface includes getters or setters, then it allows unified encapsulated data access too.
It looks like you're talking about lack of inheritance, which is indeed a good thing.
Rust forcing you to have a clear distinction between a table and a fucking stool is absolute win, not sure what you're talking about...
Yes, you have to define a trait for those getters and setters and then write hundreds of duplicated lines of those for every single struct to approach the OOP concept. Another thing is shown in serialization. What you do in OOP is to let the superclass serialize its members first, then add your own to the stream. That's simply not possible with that approach.
Thanks for the guidance! As it turns out the interface I'm working with can only handle one response at a time, so I've had to implement it with an offset option, but that doesn't really matter as far as the relevant code is concerned. Here's how it's implemented now: [GitHub commit](https://github.com/SamHH/bukubrow-host/commit/3f2b5cd7de7eabb8c14ead74c9483f03b13ff138#diff-4ce93534efc34e923ce01e975eb7ed80R162) As you can see I'm serialising each item in a loop, and then once I hit the limit I'm going back and re-serialising everything up-to that index or near it. I'm also moving stuff about between vectors and slices... I don't know but I imagine what I've written isn't terribly efficient. Any immediate advice looking at that? Unsure how to add to/remove from a serialised array with serde as you go.
For unified access, just write your entire project inside fn main.
Personally I've found the language from a design perspective to be pretty close to perfect. As someone who came from C/Cpp, rust seems to have learned from all the mistakes from those languages and made sure to address the problems properly, all while being significantly simpler than the mess that Cpp is. Learning rust was very enjoyable for this reason. However, the stability of the compiler and the development of the tooling (cargo, crates.io, rustup) is still an ongoing process, and the rust team has stated so in their 2019 roadmap that focus will be put into maturing the language. Like most people this is what rust lacks in my opinion. You will also find that the ecosystem is still lacking. Here's a couple things that stick out to me: * Rust does not have a stable ABI, and it's not in the works for the near future. One of the consequences of that is libraries cannot be shipped independently like you might be used to on a Linux system. It also means that you need to ship all your dependencies along with a new version of your binary. Cargo/crates.io does its best to mitigate the negatives, but it's not perfect. * The compiler is heavy. It has to be, because it does so much work, but it still doesn't feel very good. Compile times are higher than what you might be used to from C. You also have to recompile dependencies locally between different versions due to the previous point, which makes compilation seem even slower. In general, it's what's around the language, not the language itself that needs improvement.
You're right that a ramdisk would be safer, though I don't want to disallow decrypting files larger than the RAM in one's system, and I don't think there's a native ramdisk command/API available on Windows. Maybe I can add that as a menu option for future Mac and Linux versions, thanks.
Futures are pervasive in a lot of networking packages and they're downright unusable. I don't think this is going to get a lot better after await syntax shows up. There's no way to avoid it in the crates ecosystems. Tldr: futures are my least favorite feature of rust.
Interesting username, fellow Rustacian.
Magic bytes is a good idea. Maybe I can add that to a future version but still try to decrypt in the absence of magic bytes so it's not a breaking change.
I disagree in that this is rarely a problem, but I agree I that GUI toolkits are unnatural in rust.
&gt; That's pretty much an antipattern when you create some base garbage class to build everything on top of it just to have that shitstain of a project implode the moment you need to change said base class. I'm not saying that OOP with inheritance is the best solution, I'm just saying that it's more natural to think about. Composition instead of inheritance is great in the long run, but needs some mind bending in the beginning of a project. Since you're complaining about my stupid textbook example that's only there to explain the difference: In my real-world project, I'm writing a vector graphics editor. It has points, polygons, freehand strokes, circles, etc. All of those share certain aspects like a position, rotation, background image, etc. There's about ten of those properties every single one of those has. In Rust, I'm now forced to implement 20 accessors for every single object type, and if I change a minor thing there (like going from degrees to radians for the rotation), I have to go back and change that for every single implementation. Then I have to put them all into a single `Vec`, which doesn't work at all in Rust. After a few forum discussions the best solution they had was to put all types into a single enum, with every single accessor having to `match self`. That's quite a few hundreds of lines of code for nothing. (I'm *not* doing it that way now.)
The Java OOP feels to me like it was designed to protect programmers from other programmers working on the same project. It has all these constructs for defining accessibility and interfaces so that users of your classes cannot possibly do something bad with it, and it's your responsibility to make your class *safe* in this manner. In my experience this is just a productivity sink with no real benefit to the end product. You have most of the OOP concepts in rust, you just have to rethink inheritance in terms of traits.
A PGPGP implementation? Thanks, hadn't seen this.
Yes, Java is designed for large enterprise projects for the lowest common denominator in programming prowess. I think it does have benefits when you pick your programmers based on who's the cheapest (which many large companies do). That's not really related to the point I'm trying to make. Other languages like Objective-C are also using OOP with inheritance, but don't have any kind of access protection system.
My personal nemesis is the verbose syntax. Some things like implicit returns are visually hard to spot.
My biggest gripe is that a lot of Rust tends to contain a lot of details. A lot of common parts of the API has far more detail than in other languages. Just look at the Iterator APIs, and then compare it to say Java or JavaScript. The language it’s self has a lot of semantic details. Individually they aren’t difficult. There is just a lot of them. This can sometimes make writing code a chore. It’s quite common to think *’I know I can get this to work’* but cannot remember how.
Yeah sure, sounds about right
A big one I forgot: proc macros are essential for many non trivial projects because a lots of Rust's decisions about design, while well considered, would otherwise require tons of boilerplate. But writing proc macros is still a pain, the debugging experience is bad, and the documentation is not mature or consistent because many of the required libraries are still in flux.
Not to be the nitpicking one, but isn't something implicit the opposite of verbose?
IDEs. RLS seems easily breakable with modules. Issue #1383 has been open for a few months on that. This breaks VS Code and Eclipse. There are probably other issues, but this is the most major one I've hit. Intellij is not using RLS, but it's always behind the language. It also can't compile most projects I've worked on. You need clion to debug though.
Try /r/playrust, buddy.
Postfix async. What a pity!
To add to that: * Many crates have not yet reached version 1.0. This can make them more "scary" to use, as the API could change at any point. * `std::net` is quite lacking. For example, you can't perform a non-blocking connect. There is net2, but it has its own set of issues. Then there's socket2 to address net2's issues, but its API is a bit clunky. It's also not very popular, so it remains to be seen for how long it will stick around. * I think only recently was mirroring of crates.io introduced in a more sane way, though I don't remember exactly in what state. * Async IO is still clunky. MIO seeks to improve this, but it's not zero-cost by any means. It also has some issues with Windows. I ended up having to use my own wrappers for epoll and kqueue, and used [wepoll](https://crates.io/crates/wepoll-binding) for Windows. * No crate namespacing, so you end up with crates like "yorickpeterse-socket2", "billy-bob-socket2", etc. This also means name squatting is more annoying. * This is not an issue with Rust itself, but is worth mentioning: I really can't stand Clippy. More often than not I upgrade it, and it introduces really annoying lints. For example, if you define `len` it will complain if you don't also define `is_empty`. This makes sense for a library, but when writing a self-contained application where you'd never use this it's annoying. Of course you can disable it, but I don't want to sprinkle Clippy directives all over the place. * No (computed) `goto`, or something that is guaranteed to compile down to it. This means interpreters won't be able to make use of them, slowing them down a bit (depending on the hardware)
&gt; Then I have to put them all into a single Vec, which doesn't work at all in Rust. After a few forum discussions the best solution they had was to put all types into a single enum, with every single accessor having to match self. That's quite a few hundreds of lines of code for nothing. Box&lt;dyn Trait&gt; exists for this purpose, and again rust is a bit more flexible in this regard. While using an enum can be a bit of a PITA, it can also be a few hundred times faster than dynamic dispatch, and I'm not sure C++ can quite as easily offer the same approach You have tons of valid points though.
`Box&lt;dyn Trait&gt;` also uses dynamic dispatch, that’s the whole point of it. The reason why I'm not using that one is that I don’t think that I can cast back to the original object. The web-sys API is pretty horrible due to this, because it has to allow casting structs into other structs due to mapping an inheritance-based API.
Yeah that's what I meant, sorry if it was unclear. You have 2 choices with this which is to either use a Box&lt;dyn Trait&gt; or an enum, the latter can be made a bit easier using custom derive. I would suggest you check out [enum_dispatch](https://gitlab.com/antonok/enum_dispatch). It can also do the latter fairly easily, allowing you to cast it back in the original type by a simple match statement.
&gt; Intellij is not using RLS, but it's always behind the language. It also can't compile most projects I've worked on. CLion is my daily driver for Rust and I can’t recall any problems with compiling projects. I regularly use cargo from the run configurations or from the integrated command line without issue. I have found the Rust plugin to be regularly updated and they are always adding better refactoring tools. The “follow references” features has always worked and the syntax highlighting is great. They have a “code-lense”-like feature for displaying types next to variables and it’s awesome. Like most IDE support for Rust, the Jetbrains Rust plugin still fails at supporting any kind of macro-like syntax. It doesn’t autocomplete or give very good info about how to use them. Like I mentioned above, the go-to reference always works so I can easily get to, and read the definition myself to understand how it works.
You can use a [custom derive macro](https://doc.rust-lang.org/book/ch19-06-macros.html#how-to-write-a-custom-derive-macro) to reduce redundant boilerplate. &gt; Then I have to put them all into a single Vec, which doesn't work at all in Rust. I'm not sure what you mean. `Vec&lt;Box&lt;dyn MyTrait&gt;&gt;` is Rust's equivalent to Java's `ArrayList&lt;MyInterface&gt;`
Maybe this has been discussed to death (I'm relatively new to Rust), but can you explain why you think futures are unusable or point to something that does?
&gt; - No (computed) `goto`, or something that is guaranteed to compile down to it. This means interpreters won't be able to make use of them, slowing them down a bit (depending on the hardware) Do you mean interpreters written in Rust, or interpreters of Rust? I'd be curious to know more about why interpreters need `gotos` more than other applications. Is it just because they are more performance-sensitive?
Serialization is already a solved problem in Rust - cf Serde.
Semicolon inconsistency. It's not a big thing, just stupid and annoying
I am a beginner and find the error messages confusing.
`echo $CARGO_FLAGS`
The last price for CA Inc. (Nasdaq: CA) was **$44.44** (as of 12:44 PM EST on Jun 15, 2019) The 52 week high is **$44.49** and 52 week low is **$31.97** Price action (weekly and monthly): **Weekly:** CA made a weekly high of **$44.47** and a low of **$44.02** (for the week ending on Jun 14, 2019) **Monthly:** CA made a monthly high of **$44.40** and a low of **$41.04** (for the month of May 2019) ^^I ^^am ^^a ^^new ^^bot ^^and ^^I'm ^^still ^^improving, ^^you ^^can ^^provide ^^feedback ^^by ^^DMing ^^me ^^your ^^suggestions!
Try using the executable's full path: ~/.cargo/bin/cargo build Your output does not match Cargo's usual error format, so something fishy is going on. Maybe a shell alias, or another script which has higher precedence in your PATH. You can run `which cargo` to see if this is the case. If that doesn't work, I'm guessing your Rust is somehow corrupted, and needs to be reinstalled.
a common pattern is to build a jump table and use that for performance. this requires gotos (or something that compiles down to them).
What a pointless bot, can it be banned?
You can do everything with composition that you can do with inheritance and even more, entire project doesn't break if you change a single class... And when it does, not in a way that's impossible to fix, unlike with inheritance.
* There are quite a few "novel"/"different" things in that people have to learn, which makes on-boarding harder. * Compile times. * Sometimes stuff like `&amp;&amp;str` `String` conversions and similar are getting in a way, when you really don't care because you're writing performance insensitive code. * Couple of years away from all the important RFCs are implemented.
As soon as you're turing-complete, the question is not whether you *can* do something, it's whether it's easy to conceptualize, readable and maintainable. I think that composition gets a yes on the latter two things, not so much on the former.
Still learning but I've found modules to be a considerable pain in the ass. I still don't fully understand how they work and how the Mozilla folks expect me to compartmentalize my code.
You're forced to implement 20 accessors for ever single object type but you won't be crying later that you reused a base class and now everything is broken and you have no clue what's wrong. Boohoo, programming is a job that involves a lot of typing! Tell me something new. As for degrees/radians... Lol... That's probably the most stupid thing I ever heard. What you do in the real world is pick whichever one you want to use, and stick with it for forever. Of course there will be a lot of work if you just switch it for no real reason. The inputs can be whatever the user wants them to be, then converted to "native" units. If this is an issue for you, then here's a better idea - just build two versions of your project, one that uses radians, and another that uses degrees, because multiplying two floats is too much effort for you.
What you're describing is not at all unique (or even fundamental) to OOP. We understand that chairs and tables are both furniture. However there's nothing natural or intuitive about furniture constructing itself.
If you want to go full enterprise-scale Java development, you'd have a SingletonFurnitureFactory class producing them.
Rust definitely optimizes things like zeroing out memory, but in reality the difference will probably be negligible, and a local `Vec` might even get optimized away entirely if LLVM can prove you'll never reallocate it. Still, if you have fixed size data, I'd use an array simply because it makes the invariants clear.
My main problem with rust is not unique to rust; which is that there are many things that still use C++. Libraries are an obvious one, but things like Unreal would be a great example. Then you have things like embedded devices. Nearly all of the SDKs are either C or C++. Get rust onto the esp32 properly and I would wet myself. This is something all new languages face. PHP as a viable replacement for Perl for the web took time. Python as a candidate to replace PHP for the web also took time. Even HTML5 and javascript to replace Flash took a long time until jobs put a bullet into its head. Then you get being in the right place at the right time for things like Python and ML. It could have been matlab, R, or Java, but all of those blew it completely. I work with C++ and programmers of a variety of skill levels. C++ is great in that it doesn't dictate how you do things at all. People try to dictate via CPPCon lectures but there is no certification in C++ and thus nobody can foist their way or the highway type thinking unlike say, Java. But the problem with C++ is that I see programmers of all skill levels (except maybe uber super duper gurus who wrote some of the instruction set at intel) who can't reliably get thread right, require valgrind to keep them out of trouble, or write strange template monster bit shifting code that nobody else can maintain. Rust shows great potential for this solid replacement of C++; but at this point, for many programmers we are facing chicken and the egg of who wants to jump in first with a mega project. Quite simply Rust needs the killer use. Objective-C went through the roof as it was the ticket you needed to punch in order to release iOS apps, as Swift is now. Kotlin is the ticket that could be punched (in 2019) to make Android Apps. People aren't learning these three languages because they want to use them in general. Rust has the disctinct possibility of being a general language people do want, but some killer use case is probably what is needed to get it into general use. If Apple switched iOS over to rust tomorrow from swift, or android from kotlin, then rust would become the defacto high performance language of the next 30 years. So, what is not great about rust is that it does not yet have the super duper killer use case.
Learning the concept of borrowing is a slightly daunting task you have to tackle first, but once you're past that it's ... well, compile times can be huge and unruly.
It is easy to read, conceptualize and most importantly, is very maintainable. If your maitenance involves changing 30 classes at once, consider formatting your hard drive, and starting again, this time, with a project that isn't a complete joke. This is non-issue unless you do everything from start to finish ad-hoc and expect Rust to be perfect and just make it work for you. If you want something that can be just hacked together without giving a single fuck, consider using JavaScript, it was made for people like you.
 let file_size = match entry.metadata { Some(Ok(ref m)) if !m.is_dir() =&gt; m.len(), Logical file size isn't disk space used - you need to take things like sparse files and compression into account to estimate actual disk usage. Have a look at [filesize](https://crates.io/crates/filesize).
The thing that's bothering me right now is the debugging experience. It's easy to build your code and step through it, but hard to tell what's going on. A lot of data structures are either opaque or just hard to make sense of when you look at them in the debugger, and you can't really interrogate your data interactively because there's no repl. Rust encourages a functional style where you write things like let v = foo.iter().map(bar).filter(baz).collect(), but long collections of chained methods are difficult to step through, and it's hard to follow what's going on because you can't see any of the intermediate values.
I don't know if it's directly related to your problem, but you seem to be referring to the version of the book that accompanied Rust 1.3, which was released in September 2015. Many things have changed in the language and tooling since then, and the book itself has been rewritten entirely. You can find the most recent version of the book at [https://doc.rust-lang.org/book/](https://doc.rust-lang.org/book/). Good luck in your efforts learning Rust!
I had a similar problem when I was starting out with Haskell. I felt like the language wanted me to know what I wanted to write before I started writing. Eventually I figured out that you can do exploratory programming in Haskell, and it's not even that hard. It just looks very different. I'm pretty sure the same will turn out to be true of Rust. I just don't know what the Rust version looks like yet.
* To quote Graydon Hoare, creator of Rust, the "cognitive load" that Rust requires of you compared to other languages. It's a complex language and the syntax doesn't always help with that. * No stable ABI. * Very limited GUI toolkit support.
Inconsistency?
&gt;a local Vec might even get optimized away entirely if LLVM can prove you'll never reallocate it Would mean the data lies on the stack and the vector's methods which would normally work on heap-data work on the stack instead?
That _should_ be possible, yes, though it's highly unlikely LLVM is smart enough to figure it out for the general case.
I used Rust on and off since 2016, I understand lifetimes but I don't understand lifetimes as a type parameter, I don't know how to speak that language at all so I just throw in random parameters until it works, or until it takes longer than 15 minutes, when I just throw a RefCell or Rc at it. Also, the fact that the compiler can't always tell you whether you're missing some lifetime parameters, or what you're trying to do is plain impossible.
Thanks for the feedback! I debated this actually, but I don't have alot of experience writing tech articles :P I tried to make the examples as consistant and small as possible, and couldn't really think of good meaningful names for what I was doing.
Missing features like specialization, and long compile times. For my work project, an incremental release build takes 2 minutes! Since my domain requires high peformance computing, I can't use the debug build to test most new features.
lol, I swear this joke kept going through my mind the whole time I wrote this
I think it's closer to a learning cliff
&gt; Many crates have not yet reached version 1.0. This can make them more "scary" to use, as the API could change at any point. I think that importance of 1.0 release for indicating production-readiness is significantly over-exaggerated. We have many pre-1.0 crates which are extensively used across ecosystem without breaking changes (`libc` is the most obvious example), and many crates which have released 1.0 just to make a 2.0 (or even 3.0, 4.0) release shortly thereafter.
Neat! Thanks, I didn't realize how general Tokio was. This reminded me of a cool crate I saw recently: https://github.com/rustasync/runtime
That goes away with practice – at least it did for me. Still I use Python quite often, simply for many really helpful packages, not because I like it.
&gt; This script will find .rs files that are safe to delete (they are not mentioned by any mod &lt;...&gt;;). Project repo: https://github.com/pzmarzly/stale-rs If you instead want something that works on per-function level, see [warnalyzer](https://crates.io/crates/warnalyzer).
Yeah I'm excited about that too! I like your blog post about lifetimes by the way. Planning on finishing reading that today.
Have you tried to excessively clone stuff in order to avoid borrow checker issues? Takes the most of Rust's complexity away.
The syntax, for the love of god. Angle-brackets `&lt;&gt;`, double colons `::`, the lifetime tick `'a`, function return type arrow `(a: A) -&gt; B`, mandatory semicolons, closure syntax `|(a, b)| a + b`, functions being declared with `fn`, the new postfix `.await` syntax, the fact that tons of common things are macros `vec!()`.... I love Rust for its language but Christ almighty I hate every single syntax decision they've made. It's almost as if they are trying to make the ugliest and most un-ergonomic language possible.
Obviously not talking about embedded here, more in the realm of cli apps and server apps. Also, if any web assembly folks are here, i'd love their take on how they think async will affect wasm-bindgen /u/fitzgen Thanks for your feedback!
Thanks!
Is there an rfc for unsized rvalues?
Trait objects right now feel like a second-class citizen. It's ok for the language to be opinionated in not supporting dynamic dispatch, but currently there's a halfway house of special cases and exceptions (I would guess mostly to support the boxed closure use case) and downcasting is a mess (can't downcast to parameterised types, only boxes really work instead of anything implementing Deref and you have to use a third party crate it you don't want everything to be Any)
The Book told me statements end in a semicolon. The Book told me function definitions were statements. The Book does not end function definitions with a semicolon.
&gt;Of course you can disable it, but I don't want to sprinkle Clippy directives all over the place. Definitely not perfect, but if you disable them at the top of your [`main.rs`](https://main.rs) then at least you don't have to clutter your code with them. Maybe in the future we'll be able to enable/disable rules from a configuration file.
I assumed pattern matching would do this if it was able to
But you just said it *might* do so
I've always thought the use of the ? operator in Rust, while being really cool, doesn't really follow how the ? is used in most languages. It's very un-intuitive. Nothing about a question mark makes me think "unpack this or if it errors, pass the error back to the caller"
Yes, it _might_, if you're lucky. It's definitely not a thing you should expect to happen consistently or rely upon in any way.
What does crate::* import?
Not necessarily. For example, syntax highlighting could be used to make all return points stand out.
One word.....rust-Inator
This makes sense if you see Rust as a safer, more functional-oriented C++
Once you get a little more familiar with Rust's rules, they'll start to make sense. I find them to be the most helpful compiler errors out there, but it definitely took me a little while to get there.
To be usable, Futures need to infect an entire application. The asynchronicity stems from the lowest level of an application but if for example you want to read on a socket at the same time as you get user input you need to drive the Future at the highest level of the app. So you need to thread the Future through every layer of abstraction you try to build. Rust Futures also only have yield points in the implementation of Future; any blocking operation immediately brings down all asynchronicity. These things together make it basically impossible to port an application to Futures and require that the lowest level things you use all do Futures-bases I/O. Futures also currently produce inscrutable error messages; the rules for using them in the abstract aren't that complicated but after you've added a few layers of abstraction the types in a single error will fill your screen making it very difficult to figure out what your mistake was.
&gt; I would like to know if one can say which is faster: Invoking the allocator to get X bytes, or setting all X bytes in an array to value Y. Depends on the values of X and Y, but you can run your own benchmarks.
&gt; a lots of Rust's decisions about design, while well considered, would otherwise require tons of boilerplate Can you give some examples?
The `-&gt;` function return type is a godsend to anyone trying to search a code base for functions which return certain types. It's impossible to do this in most programming languages. With Rust a simple search for, say, `-&gt; String` would quickly return every function that returns a string. Imagine how you'd grep for the same thing in C or C++. There's also precedence for the `'label` annotations. It's commonly used in other languages that also support labeling loops and lifetimes. Rust wasn't the first, and chances are it won't be the last.
Yeah and we often feel like a learning turd while the compiler is screaming at us.
Hey, I saw a thread a couple of days ago where someone asked if only some crates can be compiled with in release mode and others in debug. This should speed up compile times.
&gt; A lot of data structures are either opaque or just hard to make sense of when you look at them in the debugger, and you can't really interrogate your data interactively because there's no repl. Look into the `dbg!()` macro. &gt; but long collections of chained methods are difficult to step through Only if you aren't formatting them. Try running `cargo fmt`. &gt; can't see any of the intermediate values. You can insert a intermediate method that logs the value and passes it on. iterator .map(func1) .inspect(|value| debug!("{}", value)) .map(func2)
It's the syntax I like the most in Rust. Except, of course `.await` *shudders*.
Same way you'd compartmentalize code in every other language, honestly.
C++ lets you hide almost everything with abstraction. Rust has some design decisions - most obviously the copy/clone distinction - that make it impossible for APIs to hide some implementation details, thus forcing the API consumer to always worry about them.
I've switched to using rust-analyzer (RLS 2.0), and it seems to be doing better. I've tried IntelliJ IDEA, but syntax highlighting tends to take multiple seconds to load after making changes to a file.
Where does it say function definitions are statements? Because that's just plain wrong.
What's wrong with the Iterator API? It works pretty much how you'd expect it to.
Function definitions are items, not statements. The three things that the compiler knows about are items, which have a body expression which can have 0 or more statements and 0 or 1 tail expressions, statements which are composed by an expression, and expressions which have different types and represent everything you can see inside of let's say a function. I think the only statement that is not composed by an expression is let assignments, but there might be more.
[Yes](https://github.com/rust-lang/rfcs/blob/master/text/1909-unsized-rvalues.md), but it seems like it's far from being implemented.
dbg! and inspect are nice, but that still means that if I'm in the process of stepping through my code and wonder what the current value of some variable is, I have to stop debugging, edit my source file, recompile, start debugging again, and then get back to where I was, which might be the 10000th iteration through some loop. Also, if I'm mapping over a big iterator, that means that my console gets spammed with all the values of that variable, not just the one I care about. Don't get me wrong, I can work with it, but it's not as lightweight as I'd like it to be. The nice thing is that I don't find myself needing the debugger all that often because code that's composed of a bunch of maps filters and folds usutally does what I expect in the first place.
Since you're doing a drag-and-drop UI, why not present the user with a drag target, so that they can choose where the resulting file goes? Maybe a Save button too for good measure, with a standard file picker dialog. Assuming the decrypt location might cause more problems than just the privacy issue mentioned above. It might also cause disk space issues if the input is a very large file on a relatively small disk. Or unnecessary network traffic, if the source directory is a Dropbox folder or something.
Rust has a very poor support for custom allocators. You can use one as an experimental feature, but it's just one allocator for the whole project, so it's not much use anyway. There is a working group, dedicated to adding proper allocator support, to Rust, but it doesn't look like it's going to happen anytime soon.
&gt; compile times for large projects Are they at least comparable to C++, or worse?
Yes
You can fake this by putting the supertype struct into the subtype struct and call serialize on it first. Slightly more boilerplate but not by a lot. You could write a strategy like classy lenses in haskell to remove most of the getter/setter boilerplate. What oop is exactly is kinda fuzzy. If you mean co-data based abstractions than that's slightly harder in rust than in an oop language. But data-based abstractions are simpler and neither solves the expression problem.
[https://doc.rust-lang.org/book/ch03-03-how-functions-work.html#function-bodies-contain-statements-and-expressions](https://doc.rust-lang.org/book/ch03-03-how-functions-work.html#function-bodies-contain-statements-and-expressions) &amp;#x200B; "Function definitions are also statements; the entire preceding example is a statement in itself." &amp;#x200B; Should I submit a PR to delete it? It seems to be there very deliberately...
I haven't been using futures a lot, but can't [block_on](https://rust-lang-nursery.github.io/futures-api-docs/0.3.0-alpha.16/futures/executor/fn.block_on.html) be used as an async -&gt; sync bridge?
Probably worse in absolute terms by SLOC/s, but better in that there are fewer hoops that you have to jump through for good incremental compilation, and definitely better in that you don't spend days at a time fiddling with CMake, which is basically part of having good compilation times in C++. &amp;#x200B; But in the same ballpark (big change means getting a coffee, building something really big from scratch means do something else for an hour).
I'd call it more *explicit* than verbose. Off the top of my head: `fn` vs. `def`, `func`, or `function`; `pub` vs. `public`; `impl` vs. `implements` or `extends`; `Eq` vs. `Equality`.
async -&gt; sync isn't the problem, it's the inability to do sync -&gt; async. Not that I'm saying that's even possible with Rust Futures, but that inability makes them hard to use. If there's a super complicated API but the only wrappers for it are sync, you're hosed.
[https://doc.rust-lang.org/book/ch03-03-how-functions-work.html#function-bodies-contain-statements-and-expressions](https://doc.rust-lang.org/book/ch03-03-how-functions-work.html#function-bodies-contain-statements-and-expressions) &amp;#x200B; &gt;"Function definitions are also statements; the entire preceding example is a statement in itself." &amp;#x200B; So the book is just wrong? It explicitly says it is a statement. It confused me and everyone else in my reading group at work. This is printed in the physical book too...
I would assume so also. I've just never tried it so I can't say for certain. I would be interested in finding out though.
I mean, the maintainers might say that they don't want to be confusing by explaining items (it seems the concept of an "item definition" has been excised entirely from the book). Unfortunately I believe they generally prefer to be wrong and misleading instead of accurate and precise, if there's any chance of the latter being slightly more difficult for a beginner to parse -_- But anyways [here's the file](https://github.com/rust-lang/book/blob/master/src/ch03-03-how-functions-work.md).
Sorry, by `async -&gt; sync` I meant calling async code from sync code. Probably was a poor choice of notation on my side. As far as I understand, block_on allows what you're talking about.
Sure, Rust doesn't have reflection (for good reason), so to avoid writing boilerplate serialization code for *every* field and struct you use proc macros from serde. Same for coproducts, delegation, dealing with the lack of const generics, enum dispatch, etc. These are all things that you could in theory workaround partially by just writing a bunch of code, but in practice that's infeasible so we write proc macros. It's a great way to fill in gaps in the language, especially since some of these are things Rust may never support, and macros in Rust have few of the problems that make them terrible in C-likes, but the ergonomics of making them and (to a lesser extent) working with them aren't great yet.
I had an idea for implementing this and proposed it on the discord. Essentially it's a way to return into alloca'ed buffer and memove from the newly dead stack frame. An llvm expert there said it as a decent idea, but would require a ton of work in llvm.
Why? What is the disadvantage of it?
I see a lot of increased rust usage among cryptocurrency projects, mainly because of the security and performance.
`type cargo` would potentially reveal more, as `which` is (usually) a command itself, and cannot see aliases or functions.
I didn’t say there is anything wrong with it. I said it’s far more detail, and thus can be a little cumbersome. Especially when you have options and results mixed in. You have iterator calls which can do stuff like mapping, or can fail with a result, and which depends on the type of the variable the result will be assigned to. Small things can get complicated with lots of small details. Quickly.
Good catch! I didn't know builtin `which` was a zsh addition, I thought bash had one too.
I agree, it sounds contradictory. What I wanted to say was that it is hard to spot because of verbosity. It is kind of hard to explain for me, the syntax is verbose and condensed at the same time. I overlook such things like the implicit return all the time. I just think that rust is very hard to read that’s all.
IO-bound tasks basically should always use async so long as it's as easy to use as sync. (If you're only doing strictly serial work or it's a one-off script, why pay the async tax?) CPU-bound tasks, however, don't benefit from this style of async task modelling. Instead, they need to use a CPU usage tuned task manager such as rayon if they want well-tuned defaults for their workload. In the perfect future, all code supports being async. But the truth is that sync code is easier to {debug, test, write} (choose any) than async code, so it's still the likely default.
As years go by, Rust loses some of its simplicity. Here are a couple of examples: Match ergonomics. Before, if you matched on a reference, you had to be explicit about references in your cases too. Since the adoption of match ergonomics a few months ago, you can now omit the reference syntax from the cases (see example below). Since the cases *had* to be references too and the compiler could tell you that, it was decided that the shorter (more ergonomic) syntax should be allowed. What was gained in ergonomics was lost in simplicity: there are now new rules about what is a valid Rust program and it can be harder for a beginner to understand why syntax without the ampersands is valid there, but not elsewhere. // Before: match &amp;x { &amp;None =&gt; unimplemented!(), &amp;Some(ref y) =&gt; unimplemented!() } // Now: match &amp;x { None =&gt; unimplemented!(), Some(y) =&gt; unimplemented!() } Non-lexical lifetimes. Lifetimes are tied to their lexical scope; once a variable reaches the end of a block, its lifetime ends. This can cause problems sometimes, requiring awkward manipulation, e.g., creating unnecessary blocks to control the lifetime of a borrow. It was not always pretty, but it was simple to understand. With non-lexical lifetimes, the situtation will be more complex; it will make some of the current contortions unnecessary, but at a cost. Other features such as procedural macros or the async IO (and its `.await` syntax) also make the language more complex, and there are people talking about proposals such as higher-kindered type that scare me outright. Now, these features are typically done in the service of Rust's three core values -- performance, safety, and productivity -- but I feel that Rust is reaching the limit of its complexity budget and that if unchecked, Rust could become a language that's too complex for its own good in a few years, à la C++.
Kotlin accomplished this with `runBlocking` and that's what `block_on` is intended to do: a transition point from sync code into async code.
Not sure how you'd suggest iterators handle that in any other way. It works the same here as in other languages that feature an iterator API. Luckily you are also given a wide variety of generic methods for interacting with optionals and results. Iterators are state machines that generate values in the future, so if you're building an iterator that can error, you also have to handle the errors iteratively as well. The types in a given iterator adapter are dependent on the return type of the previous adapter, so it should be pretty obvious what values you're dealing with at a given point. If you want to log values between adapters, you can insert an `inspect()` adapter to write the value to a log, like so iterator .adapter1() .inspect(|v| debug!("{}", v)) .adapter2()
I think this is a plus. Too many times I’ve been burned by forgetting in Python that the dot operator is completely programmable via getattr, with people doing all sorts of strange behaviors.
- compile times - const generics - every crate is "My First Crate" (lack of ecosystem maturity)
If you want to call a blocking function inside a future, you can wrap the blocking code in [`tokio_threadpool::blocking`][1]. [1]: https://docs.rs/tokio-threadpool/0.1.14/tokio_threadpool/fn.blocking.html
This is why I develop all of my complex architectures with debug and trace logging from the getgo. There's no need to top and rifle through a debugger if you already have the logs to pinpoint the exact location with greater up-front human-readable details than a debugger can provide.
Please remember that you don't have to use futures to the exclusion of traditional concurrency primitives. There's nothing wrong with running your UI and I/O loops on different threads. UIs tend to be event-driven too, though, so futures are a pretty good fit for that.
I wrote a small Python VM/interpreter in Rust, and indeed used pattern matching (on an integer, so it's like C's switch), and it works. However, CPython makes use of `if (next_opcode == &lt;most likely next opcode&gt;) goto foo` at the end of each opcode to speed things up. That's impossible in rust, until we get either gotos or tail-recursion optimization.
Yeah. I think my problems are mostly teething. I just need to get used to the language and develop the appropriate habits.
 * Lack of forward compatibility in compiler version coupled with an *extremely* unstable ecosystem and no tools whatsoever to manage the incompatibilities. To wit: libraries that built against one compiler version are very likely to drop compatibility with that version by using new features as soon as the next compiler version comes out and will no longer build with the first one. The only way to figure this out is by test compilation, which is outlandishly burdensome. If, like me, you understand that the only acceptable way to install software is from your OS's package repository and the only OS with acceptable stability guarantees is Debian Stable, you're trying to build stuff with rustc 1.24 and a negligible fraction of the most recent versions of packages on crates.io are actually usable. Possible solutions to this, in order of my preference (notably *not* how feasible or reasonable they are): * Sloooooooow doooooooown. New compiler versions should introduce forwards-incompatible language changes maybe annually, not every six weeks. Libraries shouldn't track the latest compiler. Broad ecosystem compatibility with the compiler packaged in Debian stable is a bare minimum, but oldstable or RHEL/Centos are better conservative targets. * Produce an extremely stable intermediate representation, and ship that (instead of raw source) from crates.io (call this "the Java approach"). * Tag crates with the oldest version of the compiler they can be built with, or at least the oldest version of the compiler they *were* built with, so at least a compiler-version-respecting dependency resolution procedure is possible. * Stabilize the ABI so I can link against pre-compiled crates in my OS repo instead of relying on crates.io.
Trait implementations, in my experience. If you need a trait that applies to a number of standard types (particularly the different sized ints), it's usually a case of writing the same thing a lot of times. This is worse if you're dealing with arrays, although that should hopefully go away with const generics.
Just IMO, but the name is not great. First `tx` is usually associated with "sending transaction", and I've never seen it used for atomic transactions. Second, `_rs` is a bit redundant in the actually Rust crate name. If I'm importing a crate in Rust, I know I'm writting Rust code, so what's the point? It's OK to keep it in the project name, github repo name, but the crate itself should omit it. I would name it "transaction" or "atomic-ops" or some code name instead.
At the same time, it's much easier to make the case for using a library to a manager or team members of there are stability guarantees.
My understanding is that any practical logging system has to be lossy to some extent, unlike a debugger. Moreover, threading logging through otherwise pure functions makes testing them more tedious. Can you explain how you avoid these issues?
Why would I use async in CLI programs, in particular, programs that principally read and write files?
I disagree, I think async/await syntax will make futures far less painful. I do agree that the futures 0.1 wave was a bit difficult to work with, but I am optimistic for the future.
I find these comments very interesting. We have people complaining that Rust doesn't have enough features yet (or at least not the ones they want) and then people also complaining that it has too many features already. One thing I've learned about language design is you can never win for everyone.
When I'm doing experimental type stuff, I tend to sprinkle `unimplemented!()` around quite liberally. Also lots of `.clone()` and `Rc`. These don't show up in the final product as much, but they do help me experiment rapidly.
Yeah that's a weird one to mention. Rust actually got its iterator API _right_ IMO, after me complaining for so long about how other languages did it wrong.
&gt; The types in a given iterator adapter are dependent on the return type of the previous adapter This isn't really true though. I mean it is, and it isn't. It's a good example of what I am getting at in this comment chain. Consider this code ... let nums : Vec&lt;u32&gt; = vec![1, 2, 3, 4]; let foo = nums.iter().map(|n| Ok(n+1)).collect(); You'll find it doesn't compile with `cannot infer type`. This is because `foo` could be a `Vec&lt;Result&lt;u32, Error&gt;&gt;`, or a `Result&lt;Vec&lt;u32&gt;, SomeError&gt;`. Which one used will also change the behaviour of `map`. Now yes you are right that the return value depends on `map`. But there are multiple maps, and in this case which map used will depend on the type of `foo`. So in practice it's dependent on the value it's set to. In Rust types aren't inferred in one direction. When you have lots of code which is similar, but not the same. It's painful. Because those tiny differences fuck you over. Again, and again. Rust has a lot of that. That's probably my main point here. When you write code that mixes Options and Results you can get a lot of that, and it can be very painful. The iterators have lots of flexibility which gives you again lots of *'similar but not the same'*.
I second this. Especially when dealing with concurrency and timing-related stuff, I find stepping through with a debugger doesn't actually help me as much as I wish it could. I spread `log::trace!` around liberally. Nice thing is that you can turn on trace logging in production / on end user systems, if you need to troubleshoot _outside_ of the development process.
&gt;They have a “code-lense”-like feature for displaying types next to variables and it’s awesome. Woah. Do you know if that's CLion specific? It's not something I've come across with the Rust plugin in Intellij community. &amp;#x200B; I do wish they would pull type info from RLS when they can't infer it though. I keep a copy of VS Code open with RLS integration for checking types that Intellij Rust can't infer which is pretty irritating.
Not sure if I thought that out properly, but can't you just do it with `iter_mut`?
I guess the thinking is you could do things while you wait for files to load, or async open multiple files at once.
Great point, a save dialog would let the user save to a ramdisk. Will add that to the list. For a drag target to work, they'd have to drag the output file before it was actually decrypted or I'd have to keep the whole file in memory, neither of which I want to do.
&gt; Changing an implementation detail in the function could switch which of the six types that function will fall into. Observe that everything you are saying is true of types as well. (Because lifetimes are types.) Yes, changing the implementation could change the lifetimes—or the types—in the signature of the function. &gt; Thus changing the public interface of this function. ...This is bad. Changing the implementation of a function should have 0 effect on whether a downstream crate even compiles. This is the case regardless of whether or not lifetimes are inferred, and it's true of types as well. (Because lifetimes are types.) In languages with type inference, like Haskell or OCaml, public interfaces are always annotated. The type checker still checks that those annotations are respected, though. But you are right, a public API needs to have a contract. Changing the implementation _already_ potentially changes the lifetimes. The only difference is that now the human programmer has to figure it out instead of the compiler. If the lifetimes of the public API is annotated, the compiler will still refuse to compile incompatible implementation code just as it does now. In your examples, presumably only one of the signatures is correct with respect to the body of the function. The programmer doesn't get to chose which one without also writing the implementation to be compatible. Said another way, *if the code is correct*, the (most general) signature of the function is determined by its implementation. (I suspect—haven't found a counterexample yet.) So by the time we borrow-check `bar()` we know the signature of `foo()`. If the signature of `foo()` has already been determined, then we don't have a choice as to what the most general lifetime is. All we can do is artificially restrict the lifetime(s) more than they actually need to be. The programmer *could* choose to artificially restrict the lifetime to be shorter than the longest possible lifetime inferred by the (hypothetical) compiler. The programmer would then have to annotate any such lifetime. If we write a function that *could* work with a `List&lt;T&gt;`, but we only want it to work with a `List&lt;int&gt;`, we would have to specify `List&lt;int&gt;` in the signature. What *would* be a problem is if the most general unifier were not unique. I have tried and failed to come up with such an example—but again, I've only started learning Rust since, what, Tuesday I think? Anyway, the point is I don't really know if lifetime inference is always possible, or if it is feasible (i.e. not exponential), or if it's even useful. Maybe requiring the programmer to work it out themselves in nearly every situation is a good thing and a better design decision. ¯\\\_(ツ)_/¯
I'm not sure what Rust would have to sacrifice in order to guarantee that destructors don't leak, but I feel like I would have preferred that.
I use IntelliJ Rust plug-in and I see all my types next to the variables.
I can definitely see this. For me, it's not so much that those parts of syntax exist in isolation, but rather how "busy" lines of Rust code tend to be. It's really hard to describe it in exact terms, but I often find iteration much uglier than, say, Python list comprehensions.
I agree with you. This would've been less of a problem if there was a layered complexity where you can learn the basics, be productive, and move on to more advanced stuff later. However, Rust offers little opportunity for this. Take \`collect()\` as an example, or async stuff (not ready yet, I know). Even the signatures/public API is incomprehensible for a beginner. Reading tutorials, I was constantly met with "what Y does is roughly equivalent to X". The authors were constantly covering up leaky abstractions with "don't worry about this for now", but in reality you're running into exactly the things you're told not to worry about very quickly. &gt;If you take up say go, you can learn it in less than a week. What I personally like the most about Go is that when people try to achieve X, it's done in 1-3 different ways. When people try to achieve X in C++, it's done in 100 different ways -- people from one project can't easily understand code from another project. Generally, code is written once, but read many times, so code readability is critical in larger projects. It is so fast and easy to read Go. My sense is that Rust is somewhere in between -- there is still a lot of unofficial dialect variance across projects and authors, resulting in less mutual intelligibility than ideal. Without offering any concrete solutions, I think Rust can do a lot better.
&gt;Woah. Do you know if that's CLion specific? It's not something I've come across with the Rust plugin in Intellij community. &amp;#x200B; I called it "code-lense" but I think that is a VSCode term. The rust plugin calls them "Hints". I hope I'm not confusing you with the terms. I'm just talking about how variables will have the type annotated by the IDE if they are not explicitly annotated with a type already. &amp;#x200B; I use the Rust plugin from WebStorm too and the hints works there too. There is a section in the Rust settings called "hints". There are a bunch of checkboxes that are always on by default when I install the plugin. Settings &gt; Languages &amp; Frameworks &gt; Rust &gt; I do wish they would pull type info from RLS when they can't infer it though. I keep a copy of VS Code open with RLS integration for checking types that Intellij Rust can't infer which is pretty irritating. &amp;#x200B; If I'm ever doing msvc toolchain work, I will usually pull up VSCode for debugging. But outside of macros and futures, I've found the type hints to be pretty good.
AFAIK almost all Rust crates make a stability guarantee for pre-1.0 versions, so 0.X.Y will be backwards compatible with 0.X.Z if Z &lt; Y. And many are not intended to release 1.0 until they'll be sure that no breaking changes will be needed in a year or more (some also believe that right now [MSRV](https://github.com/rust-lang/rfcs/pull/2495) change should be counted as a breaking change). So it's not really different from 1.0 vs 2.0. Yes, it's not a requirement of semver spec, but this approach is widely adopted by Rust community, so I think it will be easier to explain this cultural quirk to your colleagues than persuade authors to release 1.0 crates.
Honestly, using custom allocators with standard library collections in C++ is also kind of painful. It's sounds like a very good idea to me to punt on standardising anything related to that in Rust. :-)
Operator overloading is weird and inconsistent. &amp;#x200B; Given your own type, you can easily make a==b work, because std::cmp::Eq takes an &amp;self but can't make a+b work for non-copy types as std::ops::Add takes self.
If you want to link the two I would definitely say do the Fortran -&gt; C -&gt; Rust route. In one of my projects, I need make use of several rather old Fortran routines that no one wants to convert over to C++. In that project I have a small C wrapper file that provides access to the Fortran code and takes into account the Foo_ naming convention for calling the Fortran subroutine/function rather than the Foo name used in the subroutine. It would then have a nicer pure C or C++ function that had a better name that I would then call from an external code. Also, you if you're dealing with matrices you might need to reorder the data so that it's in column-major order. I also generally just made this code it's own library that I would then load into my C++ project. I would imagine you should be able to accomplish something very similar here.
It should not take that long. It takes several seconds for me. &gt; is it getting out of hand with the crates database growing? It has in the past to the point where the history got squashed. Here is the thread about it https://internals.rust-lang.org/t/cargos-crate-index-upcoming-squash-into-one-commit/8440, Note that even at its worst we (The Cargo Team) would consider the behaber you are seeing a bug! If you don't get it figured out here, then please file an issue.
&gt; UIs tend to be event-driven too, though, so futures are a pretty good fit for that. They would be if we had any future-based UI libraries! I think one problem here is that at an OS level, waiting for IO events and waiting for input events can be quite different problems. Even if both are event-driven, combining them can be hard. Pretty much every application with both networking and a UI that I've done has involved a two-threaded setup (with a networking thread and a ui thread), despite the fact that all of those UI frameworks are also event driven.
Actually, the reason that fails isn't because of the types in the iterator. You just didn't specify what to collect into. It could be a Vec, a HashSet, or any other type which implements a compatible `FromIterator` trait.
Everything that's in the namespace in the crate root (`lib.rs` or `main.rs`). Generally, that includes all top-level modules declared as `mod x;`, any types declared in the root file, and anything imported into the root file with `use`.
You basically just want to run `replace_with` on every element of the `Vec`. See this closed RFC for a discussion of the pitfalls of this approach, and why `replace_with` was not added to the standard library: https://github.com/rust-lang/rfcs/pull/1736 And this crate for an implementation of the `replace_with` function: https://crates.io/crates/replace_with Given this function, you can easily map a `Vec` or other data structure in-place.
I'd recommend submitting this as an issue on https://github.com/rust-lang/cargo/issues, with the information you've posted here in the comments. We can potentially help troubleshoot this, but from what you're saying this definitely sounds like a bug in `cargo`.
Looking at your implementation, there is one problem I see, but it is not a safety issue (though that's not to say there isn't also a safety issue...): if the closure panics, then the remaining elements in the `Vec` are not dropped. Leaking memory in this way is OK if the panic comes from a destructor (ie. you're allowed to exacerbate an existing memory leak), but it is usually expected that panics outside of a destructor do not leak memory.
&gt; The reason why I'm not using that one is that I don’t think that I can cast back to the original object. If you have a 'static lifetime, then https://doc.rust-lang.org/std/any/trait.Any.html#method.downcast_ref should be able to help out.
I don't think I've ever had a static lifetime except for test code and constant strings.