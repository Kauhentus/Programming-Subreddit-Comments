I did! The problem I have with that is I also have enums F, G, H that also take X, Y, Z. So what I end up with is match arg { &amp;A::B(C::D(F::X(s))) =&gt; ... &amp;A::B(C::D(F::Y(n))) =&gt; ... &amp;A::B(C::D(F::Z(s))) =&gt; ... &amp;A::B(C::D(G::X(s))) =&gt; ... &amp;A::B(C::D(G::Y(n))) =&gt; ... &amp;A::B(C::D(G::Z(s))) =&gt; ... &amp;A::B(C::D(H::X(s))) =&gt; ... &amp;A::B(C::D(H::Y(n))) =&gt; ... &amp;A::B(C::D(H::Z(s))) =&gt; ... } What I want to do is process the X, Y, Z and if it's X, handle it one way and leave Y, Z to be dealt with another. I'm sorry if I'm explaining that poorly.
Just wanted to say that your stdweb crate is awesome. I started out writing my own wrappers and macros for the emscripten runtime before I found stdweb. Your implementation of a javascript parser in Rust macros is nothing short of a useful pursuit of insanity. 
Thanks! (:
You can combine the conditions on your match arms maybe, if you know they'll give the same type: match arg { &amp;A::B(C::D(F::X(s))) | &amp;A::B(C::D(G::X(s))) | &amp;A::B(C::D(H::X(s))) =&gt; ... ... }
Oh, now that's an idea. I'll give that a go - thank you kindly!
Dom manipulation in rust? Sounds fucking awful.
It would be nice if README included some examples. Hard to tell how to use it without digging into tests...
if you mean the background yes, if you mean the whole page, I think it is scrolling issue.
&gt; Hard to tell how to use it without digging into tests... Agreed. I wrote an auto-clicker in rust, that lets you specify different input events to occur at various intervals. and it *sounds* like pendulum might help manage my event queues and waits, but with the dearth of documentation, I'm not even totally sure it does what I think it does, let alone how to use it.
There is also a [horrible](https://play.rust-lang.org/?gist=7b4cc66a7b7a2a52499f8c9e7ad3423a) way of abusing `SipHash`: let (keys, values) = map .drain() .filter(|tuple| pred(&amp;tuple.1)) .unzip::&lt;_, _, HashSet&lt;_&gt;, Vec&lt;_&gt;&gt;(); map.extend(keys.into_iter().zip(values));
Yay, very glad to see emscripten out of the way. A neat use case for this is non-web targets. For example, I wrote a wasm-to-jvm compiler which means a pure rust lib could run in the JVM.
&gt; I don't know - this makes things brittle in a new way: you must be careful and make sure that the order of receivers in the tuple matches the order of results you're pattern-matching on. Indeed; however that is purely a logic error, much like an indexing error, it cannot cause a misuse of the API as far as I can tell. --- &gt; Let me know which parts you find confusing the most and I'll try to elaborate. The first thing that confused me was the fact that the loop is executed multiple times (I know... it's a loop). I had not expected that, and initially thought that the use of a loop was just to be able to `break`. Now, I have understood that the 1st iteration is a blank shot, but it's unclear whether the second iteration is still a blank shot or not. After triggering the 1st case a second time, and moving to "attempt" mode, case 5 says: &gt; In the next iteration of the loop, all calls to `select::{send,recv}` on cases with index less than start simply fail. On all other cases, a send or receive operation is attempted. Is the just started iteration the "next" iteration (which seems the most efficient), or will the loop run "blank" another time and only the 3rd iteration will actually start selecting? And then there is the dance involved in which it seems that only half the cases are played on each iteration, whose purpose is not fully clear to me. And there's talk of a wait list, whose purpose is never explained, and on which stuff is added/removed. And finally, I seem to keep looping between 11 and 5; I *think* that the breaks may occur in steps 5 and 6 (*attempted*), but it's not made explicit. --- Maybe it would be easier if first the terms were explained (what's the wait list? what's its purpose? why does the loop keep adding (7) and removing (10) stuff from there? why can't the cases be registered once until selection succeeds?). And maybe it would help to address the conditions for a spurious wake-up; that is, when can selection fail after a wake-up? Without the idea of spurious wake-up, it seems bizarre to loop back from 11 to 5. --- Adding to the brittleness: - It's a loop, therefore one must pay attention not to accidentally put unguarded code lest it be executed multiple times for a single successful selection, - In case of dynamic list (`Vec&lt;Rx&gt;`), the list may apparently be modified right after step (10), though the first element must remain; or maybe it'll actually cause the algorithm to blow-up and should be guarded against, - The usage of `send` is quite peculiar, and nigh mandates `if`/`else` (whereas only `if` is required for receiving) because of the need to get ownership back (and put the item back in the local variable). 
I think your code will sometimes pass in the same key twice to `hashmap_swap` which is UB. And indeed I saw this as output: { 2: 2, 4: 32758, 5: 200, 3: 4, 1: 3 } Dangers of using unsafe!
I agree that DOM manipulation is a great use-case for JavaScript. After all, that's what the language is for, right? However, I don't know JavaScript so if it can be done from Rust, that'd be awesome for me personally. The potential speedup is just a bonus.
&gt; However, last thing you want to handle manipulating the Dom would be ownership worries. I don't think this would really come up. The DOM is all garbage collected, so manipulating it from Rust would be very similar to manipulating it from Javascript. Especially given [WebAssembly GC integration](https://github.com/WebAssembly/gc/blob/master/proposals/gc/Overview.md) and [Rust GC integration](https://manishearth.github.io/blog/2016/08/18/gc-support-in-rust-api-design/) to smooth out the rough edges around safety.
Oooooooooooooooooo. I am very happy to be wrong here. I'll dig more into thise links. Thanks /u/Rusky !
[removed]
&gt; what's the best on the entire Web? The best *what*? Your question is unclear, which is not helpful. And then the fact that you apparently ignore capitalization and punctuation, and express yourself in a form of pidgin, does not show much effort; and if YOU do not show much effort, why do you expect anyone else to? --- You are encouraged to edit your original post; make it better, and you may get answers that suit you better.
Rust GC integration is very much stalled right now. Overall if something is GCd ownership wise manipulating it from Rust is harder, not easier. Until something like the GC integration appears. You can build APIs for DOM manipulation that are annoying to use, but nothing straightforward.
Going to preempt this this time by leaving the same comment I had to leave on the last post: **Please** avoid analyzing the personality authoring a post. That's not on topic, and in this case is leading to many personal attacks or insinuations against the author. Feel free to disagree with their technical points. It's fine to discuss thought processes that lead to such conclusions, since it's important to know how to better teach our language, but don't say negative things about people, or discuss their intentions/personality. **Be nice.** Hope this notice isn't necessary this time. 
Yeah, I'm not expecting any of this to happen any time soon- more as part of a full solution to Rust being the "systems language of the web" as Steve likes to call it.
No, the conversation that would have engendered has happened here. I'll make a blog post when I publish to crates.io.
This link: https://github.com/Inner-Heaven/cavity/blob/master/mkfile_bad is dead. Did you forget to commit the file? 
I have no idea what you're trying to accomplish here.
I think the only truly built-in way is to do an unsafe `transmute` or a pointer cast. But like /u/thiez mentioned, I think almost everyone uses the `byteorder` crate.
If you want to do this efficiently: let stdin = io::stdin(); let mut stdin = stdin.lock(); let sum = BufReader::new(stdin).lines() .flat_map(|s| s.split_whitespace()) .filter_map(|s| s.parse().ok()) .sum(); println!("The sum is {}", sum);
Okay on the off chance that you are not a troll (which I assume to be really low): If you want people to help you and actually put in the time to look into and answer your questions, at least have the courtesy to write in a coherent and ordered way. Try to state your problem/question in a brief and organized way, throwing buzz phrases such as 'the art of', 'noble price' or 'award-winning' is not in any way related to your question and should be omitted or at the very least not put in the main part of your topic. Also understand that you are the one asking others to spend their time on your problems, and while there are many people that are glad to do so, that is not something that should be taken for granted, even less so being demanded. 
The simplest way to think about it is probably "references in Rust are just like pointers in C/C++". One of the complications you might be noticing is that Rust _also_ has "pointer types": `*const T` and `*mut T`. Under the hood, both references and pointers are...just pointers really. The differences with `*const` and `*mut` are: 1. They totally circumvent the borrow checker, having no lifetime. 2. As a result, only `unsafe` code is allowed to dereference them. This ties into one of the important properties of Rust's borrow checker and lifetime system: The whole thing happens at compile time. If you look at a compiled Rust binary, there's nothing lifetime-related left there.
Why does everyone want WebAssembly to have GC integration? Why can't languages compiling to WebAssembly be expected to provide their own GC, just like any other low level platform?
Did you install Rust through [rustup.rs](https://rustup.rs/), and have you updated with `rustup update` lately? Because that feature has been stabilized since [Rust 1.21](https://blog.rust-lang.org/2017/10/12/Rust-1.21.html).
Two reasons: First, every language providing its own GC takes a lot of space, so if a language can get away with reusing the JS one it can make smaller apps. (Think stuff that would ordinarily be translated to JS anyway.) Second, even if a language has its own GC, it still needs to integrate with the JS one somehow if it ever wants to talk about DOM objects directly.
&gt; Second, even if a language has its own GC, it still needs to integrate with the JS one somehow if it ever wants to talk about DOM objects directly. I think this can be accounted for by the "tables" feature, if they generalized it to arbitrary JS values. Anything in the table is something the WebAssembly program wants to hold onto. Then you pretty much just need an instruction for calling methods on objects, and passing JS objects to JS functions in the table. I'll grant you that avoiding needing a new GC per language would help in code size, but I'm not sure how significant this is. If we ever get a decent dynamic linking story with imports/exports in WebAssembly, I think browser caching of a GC module could cover the majority of this problem. Plus, GCs aren't *that* big.
It would be neat, but I don't think it can. I think the different designs mean rustfmt can't do that.
Oh, that suppose to be an anchor. Fixed. Thank you.
 fn new(id: usize, receiver: Arc&lt;Mutex&lt;mpsc::Receiver&lt;Job&gt;&gt;&gt;) -&gt; Worker { let thread = thread::spawn(move || { loop { let job = receiver.lock().unwrap().recv().unwrap(); println!("Worker {} got a job; executing.", id); job.call_box(); } }); Worker { id, thread, } } In the [Sending Requests to Threads lesson](https://doc.rust-lang.org/book/second-edition/ch20-05-sending-requests-via-channels.html), this is the code used to handle requests for the thread pool. I'm confused as to how this allows for concurrency. If say thread1 has to do a long computation, wouldn't it still be holding the lock to the Mutex which contains the Receiver holding the next job for a long time as the Mutex wouldn't be dropped until the computation is over? Because thread1 is still holding the lock, no other threads would be able to get the next job as the Mutex won't be unlocked until thread1 has finished its computation. If this is the case, how exactly is concurrency achieved as the jobs would essentially be running in the same manner as a single threaded program?
Even without DOM manipulation, having a working and easy to use wasm target for libraries would be a huge boon. I have a pure rust compiler for a language I'm working on I wouldn't mind embedding in a browser for live syntax checking.
Is Rust compiler smart enough to perform SIMD optimalisation by itself? If so, is this crate more for enforcing and having more fine-grained control over SIMD instructions and is not the only way to achieve SIMD-optimized code in Rust? I'm genuinely curious about this, sorry if it is something obvious.
That's a pretty simplistic way of solving the problem. The GC proposal I linked attempts something much more integrated and thus much more performant.
The compiler (LLVM specifically) can auto-vectorize, but it’s pretty limited.
This line: &gt; let job = receiver.lock().unwrap().recv().unwrap(); is the relevant section. The important thing to know is that `receiver.lock()` creates a temporary value that is dropped at the end of that line. The `Drop` impl for the `MutexGuard` (the type returned from the `lock` call) will unlock the mutex. Written out, the order of execution looks something like this: loop { receiver.lock.lock().unwrap(); let job = receiver.inner.recv().unwrap(); receiver.lock.unlock(); println!( ... }
Features should be additive, so prefer `std`.
It also makes WebAssembly a *much* more complicated spec than it perhaps needs to be. It's not exactly clear to me how it would be more performant anyway, except for languages that benefit from *exactly* JS's GC. Certainly no GC can adequately represent *all* languages. Haskell's GC, for instance, is extremely different than most GCs. I'd much rather optimize so that languages like Haskell can implement their own GC rather than try to make WebAssembly something it need not be. Plus, it's much much easier to port languages if you can keep their existing GCs written in C rather than writing a special cased binder to WebAssembly's GC.
Import groups! Yay! Eliminate all the long preambles at the at the top of files!
Haskell can already implement its own GC. GC integration is a completely orthogonal issue, and it's only "something it need not be" if you consider DOM manipulation from WebAssembly unnecessary.
I want to run an iterator forever. The iterator sometimes return None and sometimes return one or more items. Something like this: for x in myiter().cycle() { if x == 5 { break; } } The problem is that `cycle()` breaks whenever `myiter` returns an empty iterator. The best I've come up with is like: 'outer: loop { for x in myiter() { if x == 5 { break 'outer; } } } ...is there a better way?
Can you explain your design decisions? In particular, I am curious why you preferred to use a large single wheel with sorted lists instead of a hierarchical wheel with unsorted lists?
Interesting writing style. I have no clue what your actual goal with this post it. So I'm gonna read it again but with the beat from Nobody Canna Cross It playing in the background.
I did not say we shouldn’t try to give WebAssembly access to browser APIs. I said we shouldn’t add a GC that we don’t know we need. You can implement DOM manipulation without such GC integration. Adding support for arbitrary JS vals in tables would be sufficient. I just don’t see a need for more than a slightly more powerful version of tables. Ported languages are just not gonna use whatever GC integration there is if they can just compile the one they already use. I think we need to be extremely cautious with WebAssembly. The more things we add before we even know how it’s going to be used in practice, the more likely we are to end up with technical debt that we can never pay off. It just doesn’t make sense to me to add a GC that no one needs and which requires a massive amount of specification which could easily conflict with future ideas for the VM.
Does this work? loop { if let Some(x) = myiter.next() { //... } }
&gt; You can implement DOM manipulation without such GC integration. Adding support for arbitrary JS vals in tables would be sufficient. We'll have to agree to disagree here. The table solution forces every GC value to go through a double indirection, adds the new problem of managing the space in those tables, and risks space leaks from their improper use. In any case, it's still merely a proposal, it's gated on having zero impact on code that doesn't use it, and your assumption that nobody would need or use it is absurd.
Link?
 &gt; Link? [Here you go!](https://upload.wikimedia.org/wikipedia/en/3/39/Wakerlink.jpg) --- ^(I am a bot. | )[^Creator](https://www.reddit.com/user/alienpirate5)^( | Unique string: 8188578c91119503)
How is it different from the recently-announced [faster](https://www.reddit.com/r/rust/comments/7b3959/announcing_faster_01_zerooverhead_crossplatform/)?
This is the library that underlines faster, in fact: https://github.com/AdamNiederer/faster/blob/d60890fe6b47e9108212f5fdb53b7f182efc01e7/Cargo.toml#L10
Seems like the sort of thing you'd want to integrate with Rayon (I think Niko has ultimately planned for that).
https://rust-lang-nursery.github.io/api-guidelines/naming.html#feature-names-are-free-of-placeholder-words-c-feature
I’ll grant you the double indirection problem. But the other issues are just inherent to any manually memory managed environment, which I think WebAssembly ought to be. You could have the same complains about `malloc` And it’s not that I don’t think anyone needs it. I just think we should go about it differently. I know it’s not generally useful to say “no one needs &lt;abstraction&gt; because they can just do it manually.” But in the case of WebAssembly, I think this is the way we ought to approach most problems. Stuff like GCs probably ought to be *libraries* that you import a la dynamic linking, whenever we can support such designs.
This is a troll or a bot, if you look at their comment history, they just ask these 'explain the entire topic X' to me in the simplest form on dozens of subreddits. 
&gt; Stuff like GCs probably ought to be libraries that you import a la dynamic linking For literally the third time, *they are and will remain so under WebAssembly.*
Can't wait for it to be included in the standard library and a new chapter in the book :)
I feel like you must be intentionally ignoring my point... I'm not saying that's not already possible. I'm saying adding a *massive* amount of complexity to the WebAssembly spec for something we can already do anyway is a really dangerous idea that could have really bad repercussions that are hard to predict right now. We're talking about adding a massive amount of semantics to a new language that's meant to be really low level when we don't even have a clear picture on what its real-world problems are in practice.
&gt; new-school dynamic languages &gt; Java Huh?
You are only going to learn rust if it’s interesting to you, nobody can tell you if that’s true. Everyone has different interests. My first real language was C and I still use it regularly. Try it, then you will know. It costs nothing to download, even the book is free. Memory concerns are not trivial and if you are writing high level code which isn’t performance critical you are going to work at a much high level of abstraction and consequently be more productive. For some projects the right answer may be a blend, rust for performance sensitive parts, a high level language for higher level concepts. That’s what I do. Most of my application is Elixir with a couple of rust components for gluing together C/C++/FFI code, handling low latency communication and building a dynamic library with C bindings that can be linked into a customer’s product. Rust for grunt, elixir for the brains, works great. 
It's not something we can do already.
What is `mkfile` used for? I've never touched it.
&gt; being able to at least assist in translating error messages to users eventually (how? unclear but maybe by letting applications map error kinds to translation strings). The backend project at my job has stable, permanent error codes. Using these, each client can explain / translate the error in the best way for that particular use case. We have multiple clients, for people of different ages, and need need to explain errors in different ways. So when it comes to translation, I think just having a map between your IDs and your lang strings is good enough. This way you're not forcing every user of the lib to use the same translation system, either.
Autovectorization (i.e. what the compiler does) typically works best for tight arithmetic loops, meaning array processing, whereas more interesting things like media encoding/decoding do tricky things with SIMD lane widths and fused operations, that would be, at the very least, fragile for a compiler to synthesize, e.g. SSE2's `pmaddwd` ("**p**acked integer **m**ultiple and **add** **w**ord (16 bits) to **d**ouble word (32 bits)).
it's the same as `dd if=/dev/zero of=upload_test bs=file_size count=1` Used for swap and virtual machine disks. In my case I needed it for zfs integration tests to create new pools without touching any hardware.
Just to answer that last point - rustc relies on LLVM for lots of the optimization work. IIRC the code that rustc is quote verbose/heavy and LLVM does surprisingly well to digest it as well as it does.
&gt;Well, since you asked for it: you spelled "colour" wrong. It's conventional to use American English in code.
[HSLuv](http://www.hsluv.org/) also has some nice properties.
It does, but isn't very concise. But `loop { if myiter.next() == Some(5) { break; } }` is a bit better...
Also, faster is licensed as AGPL-3, stdsimd is Apache/MIT.
So this is entirely a casting question: Have: `*ptr u16` (Really a pointer to a pointer to a string) Want: `&amp;'a str` How do I cast this?
Oh, I hadn't quite understood what you're doing. while myiter.next() != Some(5) {} Is my final answer :P
[This is what you're trying to do](https://play.rust-lang.org/?gist=b83c167f289b57db0a4adf0d354a14ba&amp;version=stable). Because the closure has something that uses `self` in it, it borrows the _whole_ of `self`, when it is already borrowed. [Instead, you want something like this](https://play.rust-lang.org/?gist=9cea7280efd041f655d97884430af0e2&amp;version=stable), where you explicitly borrow `vec_2` before hand.
Try to avoid situations like that. Is there a way to not have two borrows into W at once? Maybe pull obj out of the struct and pass it in separately? Maybe Vec doesn't need to be a member, and should just be the result of a method? (I'd need to know the specific situation though...)
Thanks for the input!!
Thanks, didn't know it existed!
&gt; When NLLs get introduced, this should cease to be a problem, and the logic behind 'Yet it should be OK since mutation of the Vec does not affect A' should follow. Are you sure? I recall a limitation about closures that remains with NLL, dunno where I read that though..
Yeah, colourspace seems odd but honestly I just wanted to adapt the package to test my skills :)
In`RandomColor::hue` and `RandomColor::luminosity`, the `match` expression could be simplified to: self.luminosity = match luminosity { "random" =&gt; Some("random"), "bright" =&gt; Some("bright"), "light" =&gt; Some("light"), "dark" =&gt; Some("dark"), _ =&gt; None, } self Writing it that way is more idiomatic in Rust. Like u/Alteous already said, it would also be more idiomatic to use an `enum` instead of optional strings.
oh, ok, I kinda assumed it would.
According to the author, the AGPL license is only temporary: &gt; I intend to switch to MPL-2.0 (headers, included copy, and all) once it has a test suite and I can vouch for it not breaking anything (by 0.2.0 at least) &gt; &gt;I usually use AGPL-3.0 to make sure nobody accidentally links it. Wouldn't want a buggy SIMD wrapper with a ton of unchecked unsafe code finding its way into somebody's system because "it's rust, so it's safe". https://www.reddit.com/r/rust/comments/7b3959/announcing_faster_01_zerooverhead_crossplatform/dpf8gbp/
It seems simillar to what I wanted to do. I want to propose an idea for better ux. My widget trait is defined as pub trait Update { type UpdateMsg; /// Returns UpdateResult::Dirty if widget becomes dirty. fn update(&amp;mut self, msg: Self::UpdateMsg) -&gt; UpdateResult; } /// Holds state (to support trait object in future) pub trait Widget: Update { /// Has style type Props; /// Renderer: Proxt struct fn render(&amp;self, r: &amp;mut Renderer); fn from_props(props: Self::Props) -&gt; Self; fn update_props(&amp;mut self, props: Self::Props) -&gt; UpdateResult {} fn handle_ui_event(&amp;self, event: ...) -&gt; Option&lt;Self::UpdateMsg&gt;; } What I want to propose is UpdateMsg and Update, which can be used to update ui when custom event (e.g Event::ProductsLoaded) occurs. Note: I'm not sure if this will work well because I didn't implemented it yet. But I 'm not sure how can I pass custom events down to users' widget. Perhaps something like pub trait Handler&lt;E: Event&gt;{} impl&lt;E: Event&gt; Handler&lt;E&gt; for AppBar&lt;W&gt; where W: Handle&lt;E&gt; {} can be used, but rust coherence rule may not like this. 
&gt; Lol: because I legitimately laughed when I read that he had implemented both 'a' and 'an' That's what I found hard to believe. I thought it was a detail-oriented, a smart thing to do. Laugh-worthy? As in "LOL i'm pissing my pants, laughing out loud etc."? I don't see it. But all right, no felony here!
And I like props!{} and state!{}. That's really clever.
I don't get what you are trying to do with OwnedSlice; it is "safe" once the requirements are upheld yes and people don't make one in unsafe contexts but your documentation on it is very inaccurate. Is there any particular reasons as to why you are not using `Box&lt;[T]&gt;` to get an owned slice? which by the way is just a Vec that can't grow or shrink whose capacity is always the same as its length which can't change?
How is my documentation inaccurate? The reason I don't use `Box` is documented: I need a type that owns its elements but not their storage.
From the perspective of his heyday, the early nineties :) And it is a surprisingly dynamic language, once you look past the static typing. Bjarne Stroustrup said that Java reminded him much more of Smalltalk than C++
There's also [palette](https://crates.io/crates/palette/) crate.
Well one of the invariants your documentation does not cover is that you need to call `mem::forget` on whatever owns the original slice to not get undefined behaviour by dropping twice. Furthermore if the slice you pass it lives on the stack then it _will_ be dropped twice no matter what because mem::forget works differently on the stack so you need to document other invariants for a stack-based slice. If you for instance get a slice from a Vector and feed this to this function evven if you never reference the slice or the vector the vector will get dropped at some point but the same memory is dropped by your `drop_in_place` inside of this as well so the memory will get dropped twice which is undefined behaviour. To avoid this I recommend that you just transfer ownership properly to the OwnedSlice and make something like this: pub struct OwnedSlice&lt;T&gt; (Box&lt;[T]&gt;); impl&lt;T&gt; OwnedSlice&lt;T&gt; { pub fn from_box(b : Box&lt;T&gt;) -&gt; Self { OwnedSlice(b) } pub fn from_vec(v : Vec&lt;T&gt;) -&gt; Self { OwnedSlice::from_box(vec.into_boxed_slice()) } } /*access code*/ impl&lt;T&gt; Drop for OwnedSlice&lt;T&gt; { fn drop (&amp;mut self) { unsafe { let slice = Box::into_raw(self.0) as &amp;mut [T]; for x in slice { ptr::drop_in_place(x) } } } } You feed the vector or Box into it now and the structrure actually properly owns it which is important because your own OwnedSlice does not own the memory so something else does and that something else will probably attempt to drop it at some point. But I still don't get the purpose of why you would want to drop the elements but not return the memory to the OS. What you're creating here is basically a memory leaker. When it goes out of scope it leaves you with a portion of memory on the heap that is not returned to the allocator that is unreachable but the former contents of that region are dropped so it's just a large swab of uninitialized memory that sits some-where doing nothing that is unreachable.
Thanks this is a nice solution.
Oh wow, someone made a HSL crate in 2015 for some stupid little experiment but never updated it? That's a shame! Yeah, so: If anyone wants to have this crate, ping me with a good story of how you became an expert on color spaces and I'll add you as owner on crates.io and GitHub!
Are there any plans to integrate this stuff more closely into the compiler and have a feature similar to gcc's [function multiversioning](https://gcc.gnu.org/wiki/FunctionMultiVersioning)?
Another solution to the one already mentioned is to destructure on `self`: fn foo(&amp;mut self) { let &amp;mut W { ref mut obj, ref mut vec } = self; obj.do_stuff(|item| { let bar = ... vec.push(bar); }); } 
LLVM has an SVE pass that can vectorize some scalar code, so it is not restricted to loops. The main problem with auto vectorization is that it is not reliable. It's a good thing to have if you don't needed and get for free, but if you do need it then it's not enough.
You can do this with procedural macros already. Parched has one on github that does this.
You would need for&lt;T&gt; for that. You can only monomorphize lambda with a single type but a rayon like library might want to do so for different vector types simultaneously depending on what your host supports at run time.
I'd say this is what the cookbook is for
Could someone please explain to me why NLL is considered by so many people to be such high priority over pretty much everything else? IDK, to me it has always seemed like more of a cosmetic/convenience thing to just get rid of some papercuts and make the language more ergonomic. Sure it will remove some ugliness from rust code, but it somehow never felt necessary or particularly important to me. I am surprised to see it considered much higher priority than, say, SIMD or const generics. SIMD and const generics are things that will enable Rust to do so many new things not currently possible. In the sprint document, NLL is at the top, SIMD is at the very bottom, and const generics are out. :(
Thank you! I had thought the implementation was fine even in this case, but I was just wrong. I corrected the link above.
http://www.arewelearningyet.com/
Yes, agreed; I think there's a LOT of applications. Not even in browsers; even stuff for Node.
I've never tried ML in Rust but TensorFlow has Rust bindings: https://github.com/tensorflow/rust You could also write your ML parts in python and use TF Serving to expose it to your Rust backend: https://www.tensorflow.org/serving/ 
&gt; I have no idea how to even run a wasm file! You can use the JS interface like this: https://developer.mozilla.org/en-US/docs/WebAssembly would work in a browser or node. Small note about the gist; wasm support is now exposed by default, IIRC, so you don't need to do your own build, etc. Does the wasm file show a `start` section? That'd be `main`, basically. 
There was a talk at RustConf 2016 about it.
I think you can avoid comparing by making the swap function unsafe itself, and guaranteeing in `shuffle` that you don't generate the same index twice. That said, you're still collecting stuff into a temporary `Vec', and you're cloning the key which could easily end up being more expensive than the top voted solution. 
To add to this, the valgrind suite doesn't only offer checking for invalid memory access, but also includes profiling tools. I use callgrind and cachegrind to profile my (safe) Rust code.
as I said [over here](https://www.reddit.com/r/golang/comments/7bw5e9/question_to_the_blizzard_engineers_have_you/dpljc3r/): tl;dw Summarizing the video, these people are aware of Rust, and they would love to try it, but they've each personally been too busy to do anything but keep churning out more C++ code. They mention that some smaller teams who are working on less critical infrastructure have had the opportunity to dip their toes in new languages, and they mention that one of these teams is trying out Go for a small, greenfield project, as an example of trying out new languages.
Is it safe to "unsafe" sometimes ? I published my first little [crate](https://crates.io/crates/pretty_toa) today. I used str::from_utf8_unchecked(..) to gain few nanos, because i think it is "safe" (It uses the output from itoa/dtoa which will probably never mess it up). My question is is this a code smell? Especially since I do not mark the whole function as "unsafe". 
Is there a list of all these “arewe__” sires? Lol
I don't have any issues with GC integration itself -- that is a good idea. However, if in the end it gets *easier* to use GC integration than to do proper memory management, then it would create a slippery slope towards GC. I mean Rust already now doesn't implement pointer arithmetic via the Add trait and similar, so that it becomes harder than doing it the safe way. If pointer arithmetic was easier, people would use it more often, resulting in worse code! I'm no absolute GC hater, but I think its presence in Rust should be limited to communication with languages that have GC, and not become an "easier" way to manage memory.
Somebody should register arewemetayet dot com.
Indeed there is! https://wiki.mozilla.org/Areweyet
Experience is bad, since most of the libraries cannot do what they promise to do, or are abandoned. If you want to do simple models (like logistic regression), I suggest writing it by yourself (it is not that hard). If you want to do deep learning, you have to options: a) build model for Tensorflow in Python and load it in Rust b) wait till I stabilize https://crates.io/crates/cntk (or risk API changes)
TF has rust bindings, which cannot do much more that load the model.
You literally just said we can already write GCs in WebAssembly
Why does this work though? What's the underlying logic of the borrow checker?
Overall, this looks really cool! Would stdsimd be able to use [const generics](https://github.com/rust-lang/rfcs/blob/master/text/2000-const-generics.md) for functions like [_mm_shuffle_epi32](https://docs.rs/stdsimd/0.0.3/stdsimd/vendor/fn._mm_shuffle_epi32.html) which have an "immediate" parameter? At a glance, it looks confusing to have an [i32x4::eq](https://docs.rs/stdsimd/0.0.3/stdsimd/simd/struct.i32x4.html#method.eq) be a method which does an element-wise comparison and returns a vector, and also an [i32x4::eq](https://doc.rust-lang.org/nightly/core/cmp/trait.PartialEq.html#tymethod.eq) which does a whole-type comparison and returns a scalar. How does the compiler know which to use? 
Let me start over, since clearly I'm not explaining this well. Right, we can not already GC JS vals using only WebAssembly. My whole point is that it should be possible to do this manually with a GC written in WebAssembly, and that simply using such GCs is much better than adding a type system and GC that could easily constrain the future of WebAssembly and sit at a much higher level than WebAssembly was meant to sit at.
I think the concern is that it's hard-ish to predict where NLL is needed and the errors you get don't do much to lead you in the right direction. So it is a learning barrier that is both frustrating and a letdown in the sense of "oh, I thought the rust compiler was supposed to be smart. I guess not..."
There is literally a hackfest going on between Rust and Gnome right now in Berlin.
&gt; You can use the JS interface like this: https://developer.mozilla.org/en-US/docs/WebAssembly This is basically what the code in the gist does, yes. &gt; wasm support is now exposed by default, IIRC, so you don't need to do your own build, etc. Yep, `global.WebAssembly` is available in node 8.9.0 (which I'm using). &gt; Does the wasm file show a start section? Looks like it: ~/P/hello-wasm  wasm2wat target/wasm32-unknown-unknown/release/hello-wasm.gc.wasm &gt; hello-wasm.wat ~/P/hello-wasm  rg start hello-wasm.wat 23707: (start 137) 23875: (data (i32.const 5120) "slice index starts at ") So here is what I did: ~/P/hello-wasm  curl https://gist.githubusercontent.com/kanaka/3c9caf38bc4da2ecec38f41ba24b77df/raw/92e8fe3f612b055b65d40dfeac7812a05a1ec632/runwasm.js &gt; runwasm.js ~/P/hello-wasm  node ./runwasm.js target/wasm32-unknown-unknown/release/hello-wasm.wasm main 0 RuntimeError: unreachable at wasm-function[12]:1953 at wasm-function[11]:38 at wasm-function[1]:327 at loadWebAssembly.then.instance (/Users/pascal/Projekte/hello-wasm/runwasm.js:68:30) at &lt;anonymous&gt; at process._tickCallback (internal/process/next_tick.js:188:7) at Function.Module.runMain (module.js:678:11) at startup (bootstrap_node.js:187:16) at bootstrap_node.js:608:3 Doing the whole thing in node's REPL gives me this, by the way: ~/P/hello-wasm  node &gt; code = fs.readFileSync("target/wasm32-unknown-unknown/release/hello-wasm.wasm") &lt;Buffer 00 61 73 6d 01 00 00 00 01 ce 01 22 60 03 7f 7f 7f 00 60 03 7f 7f 7f 01 7f 60 01 7f 00 60 01 7f 01 7e 60 02 7f 7f 01 7f 60 00 01 7f 60 00 00 60 04 7f ... &gt; &gt; instance = null; WebAssembly.compile(code.buffer).then(module =&gt; new WebAssembly.Instance(module, {})).then(x =&gt; instance = x) Promise { /* bla bla let's wait a second or two */ } &gt; instance.exports { memory: Memory {}, main: [Function: 1], memcpy: [Function: 87], // … __subdf3: [Function: 146] } &gt; instance.exports.main() RuntimeError: unreachable at wasm-function[12]:1953 at wasm-function[11]:38 at wasm-function[1]:327 at repl:1:18 at ContextifyScript.Script.runInThisContext (vm.js:50:33) at REPLServer.defaultEval (repl.js:240:29) at bound (domain.js:301:14) at REPLServer.runBound [as eval] (domain.js:314:12) at REPLServer.onLine (repl.js:441:10) at emitOne (events.js:121:20) 
Iiiinteresting. So the `start` function is supposed to run on `instantiate`; you don't neccesarily need to call `main()`. https://cdn.rawgit.com/WebAssembly/wabt/fb986fbd/demo/wasm2wat/ is an online version of your wasm2wat; you can have it generate names instead of just that 137; might make it easier to see what that function is.
Ah, good, that's what I expected to happen, too. Maybe `println!` just doesn't work yet? (My wasm2wat is just what a `make install` of https://github.com/WebAssembly/wabt gave me yesterday, btw.)
JPEG and other media encoders/decoders can take massive advantage of vector instructions, particularly floating point ones, to do things like colour space translation and cosine/wavelet transforms in parallel as these operations involve a lot of independent calculations. E.g look at all the assembly trickery [libjpeg-turbo](https://github.com/libjpeg-turbo/libjpeg-turbo/tree/master/simd) does.
https://github.com/cretz/asmble
Yeah, possibly. I haven't had the time to build the PR and try it myself...
(http://www.sciencedirect.com/science/article/pii/S0933365716302950) The bAIsic application, they are referring to in the article, is not written in rust. =] But, in a past few months, I've managed to rewrite most of its core logic in rust, as an overnight experiment. One of the biggest things I took from this experience, is that linear algebra libraries which are available, let's say, for Scala, definitely are not better than what is available in rust-land. In both cases, I end up directly calling into blas/lapack. Regarding performance. It is almost identical, for obvious reasons. Except for one scorer function, which works literally ten times faster, without any optimizations. Because jvm sucks? I dunno. =] And I feel like introducing low-level optimizations in rust going to be easier. On other hand cargo bench not even close to JMH. So, it also going to be difficult to measure performance boosts =]
It's definitely frustrating to hear a "technologist" suggest that focusing on anything other than C++ makes you less hire-able (for Blizzard, I assume).
I think he's also saying more difficult to hire. It's an argument I've had to fight against while trying to introduce F# at a .NET shop What bothered me was him calling these esoteric-- but that's as someone who brings up programming Befunge in job interviews
Doesn't change the fact that this is true. I just checked jobs.ch (job ad portal for Switzerland) There are 0 Rust and over 50 C++/C ads online. 
I think [limn](https://github.com/christolliday/limn) shows promise as a pure Rust GUI framework. But, I'm barely an initiate to Rust.
There's more to machine learning than "simple models" and deep learning, though
&gt; so we should make it possible for those GCs to manage JS vals so that languages who want to integrate with the DOM don't have to turn on a second, unnecessary GC. That would enable incorrectly-implemented WebAssembly GCs to violate the memory safety of the browser itself. Besides, that second GC is already always on. There is no way to load a web page without it. &gt; adding a type system and GC that could easily constrain the future of WebAssembly Go read the proposal again. As I've mentioned, it *explicitly* does not place *any* constraints on WebAssembly programs that don't use it.
That is the primary motivation for GC integration in Rust, but the argument that something should intentionally remain difficult because it's "bad" is silly. The reasoning behind `unsafe` and the lack of an `Add` impl for raw pointers is not that it makes those things harder, it's that it makes those things *obvious* and *easy to find.*
&gt; That would enable incorrectly-implemented WebAssembly GCs to violate the memory safety of the browser itself. The tables solution is one way to avoid this. That may not be the best solution, but I think it should be possible to find a solution that does not violate memory safety. &gt; it explicitly does not place any constraints on WebAssembly programs that don't use it. I'm not talking about existing programs. I'm talking about future design decisions. The existence of a type system and memory model means we've bought into that forever, and if we ever find that a different design would have been preferable, potentially without a GC at all, we can never go back on it. I'd be less against this if it weren't so early in WebAssembly's life. We just don't know what it's real-world problems are going to be in the future.
&gt; a different design would have been preferable, potentially without a GC at all That is impossible, since the DOM already requires GC forever. Literally any design that touches it requires *some* kind of GC integration. &gt; We just don't know what it's real-world problems are going to be in the future. ...which is why it's a proposal and not a spec. Honestly, what do you think you're going to gain by arguing, on /r/rust, against a far-future proposal to add *opt-in* integration with an *existing* type system and memory model that are *already* required to exist forever in the web platform?
Amusingly, I have been told of one AAA studio using Rust (I believe for backend services, but potentially also their client). I forget who it was at Rust Belt Rust who told me this so I won't divulge it just yet in case it was supposed to be in confidence (a tease, I know :P ), but I do happen to have a contact at that company (in a past life I was a video game, er, "journalist", and my former editor is now their director of marketing) so I'm going to reach out and try and validate it independently.
You're right - my frustration seems to have little impact on Swiss job listings :) But seriously, my gripe is with the idea that experience and expertise don't translate and that the minutiae of a particular company's tech stack is important. A company could easily post a job opening dealing in C++ that doesn't require "many years of nothing but C++ experience". In my opinion, that company would find a better person for the job.
&gt; [...] they've each personally been too busy to do anything but keep churning out more C++ code. That and the amount of already existing code bases *(e.g. in-house game engines)* written in C++. tl;dw complete.
I agree that it shows promise, if it keeps getting developed for the next year at least. "Early stage and experimental" means it's simply not ready for a "complex GUI app" today. If you try and use it (I have) you will come to the same realization, i.e. that it really is very early and experimental.
Diversity of tech experience does make you more well rounded and more attractive to standard software companies, but game devs are in super duper deep with C++. It's not a situation where you could pick up C++ through sheer cleverness; you need to know the deep ins-and-outs and be able to understand whatever came before you.
In a year or so I think there's a pretty good chance that we'll have a reasonable GUI story (likely GTK), today I don't think so. I'd write the whole thing in another language (C++, python, ...) or the core logic in Rust and a gui in C/C++/python depending on what I was doing.
there is a Rust-Qt binding generator: https://www.vandenoever.info/blog/2017/09/04/rust_qt_binding_generator.html https://www.vandenoever.info/blog/2017/09/10/time_for_rust_and_qml.html
There seem to be QML rust bindings of some sort, but I've never tried them. For an example of a photo editor I wrote in C++ and QML, check out https://github.com/CarVac/filmulator-gui. Qt Quick has decent canned widgets but I made my own derivative ones for some of them, with everything custom styled.
If there is a C GUI library you like, it is absolutely possible to use it directly from Rust. The Rust community just has really high standards, and wants a GUI library that fits seamlessly in Rust like Qt was designed to do with C++.
Yep, I've done two or three projects with rust for ML. To give the tl;dr: Use Python or R for your data exploration, they still rock for this. Use rust for feature extraction, cleaning, storage, etc - Rust is fast, and makes reasoning about your data a lot simpler due to its type system. Use Python for the machine learning model - so you could store the data in a table with Rust, and then have a separate Python/ Julia service do the ML itself. Basically, in this case, a tiny Python microservice that simply serves your model is all you need. It's gone very well. Rust lacks a few key components that would make feature extraction and the like easier, but I've found ways around that. For example, there is no 'untyped dataframe' in rust... but then again, would we even want one? It's great in Python where there are no types anyways, but it feels like a waste of Rust's power. That said, Utah is pretty cool for a typed dataframe approach: https://docs.rs/utah/0.1.2/utah/
Rust has [gtk bindings](https://github.com/gtk-rs/gtk), so I don't see why you wouldn't be able to build something like Gimp.
Sure. But lot of those things (like probabilistic models) are not properly implemented even in sklearn. (And a lot of stuff from sklearn can be implemented in couple of hours with proper math library). 
Thats not true from my experience. I've yet to see a really good developer that hadn't years of experience in his field. In the business world it's not about beein cutting edge and using fancy new tools. It's about delivering rock solid software as fast as possible. When i write firmware for a uC, it better doesn't have a bug in it once we shipped 1000 boards that can only be upgraded over a hardware debug interface. The problem is, that the vocal minority (cutting edge developers) make you think, that this is how the industrie looks, but reality is, that probably more than 90% of software jobs are business software, firmware or maintaining old systems and software. And in these fields you need to have a lot of experience to write stable and good performing software. I don't think it's possible to be a top c++ developer with less than 5 years of experience besides some edge cases of really talaned ppl and thats like 1 in 1000. May be different where you live or work, that's just what i saw over the last 8 years as an hardware / software engineer. Hope my english is not to bad.
Since I want my apps to feel native on my KDE desktop, that means I have to use the QWidget API and not QML or GTK+. (And, since I came to Rust as a more typesafe and statically-verifiable alternative to Python, thr arrival of a binding generator which requires me to feel comfortable with C++ componentry and build infrastructure didn't really change much.) My approach is to use [rust-cpython](https://github.com/dgrunwald/rust-cpython) to parlay Python into being to the QWidget API what QML is for the Qt Quick ecosystem. It provides a very nice, high-level API (even nicer if you're willing to rely on nightly Rust for the [PyO3](https://github.com/PyO3/pyo3) fork) which allows me to smoothly mix Python and Rust in the same project and [setuptools-rust](https://github.com/PyO3/setuptools-rust) then merges cargo into `setup.py` commands for a seamless "build the Rust library and then copy into place as part of `setup.py build` experience"
&gt; In a year or so I think there's a pretty good chance that we'll have a reasonable GUI story (likely GTK), today I don't think so. GTK is crap on anything non-linux. No way will anything GTK based be a viable solution.
So you could train a model with Python and then make use of it from Rust? Would there be any advantage in doing this?
I've worked on embedded systems in C++, so I know your pain. Deploying an update to fix a bug can be very costly. But, how does in-depth knowledge of C++ inform your verification process? My knowledge of SFINAE, move semantics, and vtables isn't much help when I'm trying to prove that the system works as it should. I agree that becoming a good developer takes time, but I'm not hiring someone who can only sling some C++ code. That just isn't enough to build and deliver a successful project.
Yes, you can do that. Advantage of this, is that you can fiddle with your deep net any way you want, and then you just pack it and execute from anywhere. And I believe, that function call for model evaluation is much better in production that having API HTTP call (less maintenance, less cluttered code with futures, etc.) And if you know what you are doing (naming all relevant ops in network, or knowing default TF names), you can even define network in python, save the structure and train it anywhere, save it and predict outputs anywhere. If you ask why you have to define network in python, it is because only Python API has full graph creation facilities like gradients over recurrent networks, ...
Ah yes, gimp, inkscape, firefox, etc are all crap on anything non-linux? Besides which OP has literally said that just linux is fine... and the likely GTK part comes from the large amount of effort being put into making rust work well with it.
Not really. Still waiting.
Fun fact: this bindings are for GTK 3, and GIMP is still using GTK 2.
&gt;gimp, inkscape They still use GTK 2 which is dead and doesn't support HiDPI. So yes -"crap".
Firefox is not using GTK outside of linux. Gimp/Inkscape absolutely.
For vehicle applications I'd expect the worst case times to almost more important than the average case. It doesn't matter how fast it is on average if every now and then it takes forever and a half and in that time the vehicle crashes.
&gt; Is it safe to "unsafe" sometimes ? Yes, of course. You only have a bit more responsibility: you must "maintain invariants". An invariant is something that must be true about the memory and variables of a program. If it's not, it's "undefined" what the program will do. If the bytes *aren't* valid UTF-8, Bad Things are allowed to happen once you create a `&amp;str` reference to them. Which `itoa` are you using? I would prefer to not touch anything the C standard library does with strings - it's probably correct but Rust is a much better language for avoiding string pitfalls. &gt; code smell? Generally if you use `unsafe` you should limit the amount of code which, if buggy, will break an invariant. Your code is fine: the byte buffers are neatly limited in scope to the functions that create them, so it's easy to see that the `&amp;str` reference you conjure comes from `itoa`/`dtoa`. You should also be careful of conjured lifetimes. Fortunately `from_utf8_unchecked` keeps the same lifetime so this isn't a problem. - On an unrelated note: your functions *do* have an unavoidable heap allocation of `String` - because you put data into a new `String`. It would be more in line with your goal to take the output target as `&amp;mut String`. A potential code smell that I do notice is function names that are not verb or prepositional phrases, consider `fn write_with_thousand_sep(self, out: &amp;mut String)` 
If Qt was designed to seamlessly mold to C++ it failed abjectly. 
Alex commented on the PR that a clean build (i.e., also compiling LLVM) is necessary to build it, so it takes some time… I've actually used it as a benchmark to see how long it takes with `-j2` (50min) and if my MacBook gets loud (nope) ;) Anyway, I have no clue how to compile a library to `.wasm`, but that'd be my next try.
&gt; I want to run an iterator forever. The iterator sometimes return None and sometimes return one or more items. Don't think of it as an iterator then. Iterators don't start producing useful values after `None` - `cycle` saves a copy of the iterator, runs copy A, throws it out when done, restarts with copy B, etc. etc. Because your thing doesn't act like an iterator, iterator methods and `for` aren't very useful with it. I'm going to call it a "generator". I'm pretty sure the loop you want is this one while let x = my_gen() if x != Some(5) { // now x is a loop variable that gets rebound every time the loop iterates }
&gt; it's that it makes those things obvious and easy to find. The entire *point* of Rust is that in so many cases, the *easiest* way of doing something is also the *best* one. Think of `for j in v.iter()` vs `for i in 0..v.len() { unsafe { v.get_unchecked(i)`. Now look at the situation in C++03 (C++11 seriously improved this): `for(vector&lt;int&gt;::const_iterator i = v.begin() ; i!= v.end() ; ++i)` vs `for (size_t i = 0; i &lt; v.count(); i++) v[i]`. Or think of the `unwrap` vs `Result` discussion. `unwrap` is much easier to use than `Result` and this results in a ton of code using `unwrap` instead of `Result`, making many people really sad every time they see `unwrap`, even though unwrap has completely legitimate uses. Or think of C/Go. The classical error handling approach is here to check whether a pointer is null or some error variable is set. I definitely do not want libraries starting to depend on garbage collection just because garbage collection allows them to avoid to think about resource management. Or people coming from GC'd languages learning the GCRust dialect before being introduced to proper Rust. GC integration should only be about talking to languages that only have a GC, and make sure that it is not convenient to use for some other purpose.
Both are working on porting to Gtk 3, Inkscape is almost done and will very likely use Gtk 3 in its next version.
You can have your struct own the HashMap, and then just do `Context { store: store, outer: None }`.
Is there any reason why you want to store a reference to said hashmap instead of just storing the hashmap itself in the context object? If you just store it directly in there the context struct would own the hashmap and there would be no issues.
I think so... But you're right, it certainly works better if it's not a reference :)
It's a unit struct, and it implements Encoder as follows: impl Encoder for TcpCodec { type Item = TcpMessage; type Error = io::Error; fn encode(&amp;mut self, item: Self::Item, dst: &amp;mut BytesMut) -&gt; Result&lt;(), Self::Error&gt; { let required = 1; if required &gt; dst.remaining_mut() { dst.reserve(required); } dst.put_u8(item.opcode.get_code()); dst.extend(item.payload); Ok(()) } }
That's an argument for making it easy and idiomatic to learn and to do things the right way, not for making it hard to do things the "wrong" way. GC is useful in more situations than just "*le sigh*, I must speak to a language that uses GC, time to use a bunch of purposefully-verbose APIs." Those use cases are important enough that they should be just as easy. Further, GC is a perfectly legitimate strategy elsewhere as well. Concurrent data structures, experimentation with an unknown and quickly-changing dataset, etc. Libraries 100% *should* be able to depend on GC for these use cases, and there's no reason to punish them for it.
I'm sure a lot of people would agree and disagree with that statement.
I don't think that this is a dismissal of Rust by the author. Some of his earlier posts may have been, but I think that the discussion following them he realized that some of the things he was complaining about were very active matters of development in the Rust community, and so it wasn't the case that Rust was simply not going in the right direction but just that it wasn't there yet. And he repeats that conclusion here. I think, though, that he substantially overestimates how long it will take Rust to get there. It was a bit under a year ago that he first tried out Rust, I believe. One of his big complaints was that there was no way to do asynchronous networking in the standard library. Of course, it's interesting he'd complain about that for a C replacement because C also has no such thing, but I guess he considers the standard library to also include POSIX. He also, having come from Python and then tried out Go, seems to prefer a "batteries included" approach, to the "have a very focused and stable core standard library along with really good support for package management" that Rust goes with. So, the fact that mio and Tokio exist didn't help out with the lack of built in async support (beside the fact that Tokio was extremely new and still in active development). So even though he had a fairly negative opinion at first, thinking that the priority of the Rust community is in the wrong place, he realized that for a lot of things there was active work being done and it's just not ready. He writes it off as "won't be ready for five years", while I think that an ambitious schedule would put it at about a year from now would be reasonable, and two years at the most. This year's impl period is hitting a lot of papercuts that he encountered, as is the libz blitz and cookbook covering better the "discovery" aspect of the crates ecosystem, and I think with a year or so more of maturing a lot of his issues will probably have solutions and he'll be better able to find solutions that he's looking for. I didn't see much more in the comments that were particularly a dismissal of Rust, just people saying that they don't really think it's ready to use for various reasons yet.
&gt; minutiae of a particular company's tech stack Uh, this is **C++** for a games company. What are you expecting? C++ isn't quite a language to just "pick up". It does require years of experience as it's a beast of a language, with a very high learning curve. Why *should* a company hire someone who doesn't know that? That means they'd have to spend time getting the employee situated with the company + code base itself (which takes time), *and* teach them a difficult language? Come on. Experience and expertise will translate but that's like saying "I have 5 years of Porsche mechanics experience so I can just apply for a BMW position, nevermind that they have a completely different setup and different things to know". Is it the same underlying stuff? Sure, but that doesn't preclude a company from preferring to hire candidates who they *know* are already good within the domain. And C++ is a wide domain. Not a niche thing like rust.
Yeah. The answer doesn't really surprise me. As cool as Rust seems (mostly from an outsider, since I'm a horrible programmer), you can tell that in the current coding environment it's definitely still the new kid and people aren't sure what to do with it yet. It's much easier to just keep doing what you have been doing. I really do hope they get a chance to look at it.
Because that's what was decided at the beginning of the year to focus on. https://blog.rust-lang.org/2017/02/06/roadmap.html https://internals.rust-lang.org/t/setting-our-vision-for-the-2017-cycle/3958?u=aturon
The other side of it is that, as a programmer who codes in Rust, C++ and Python, I can easily find a new job if my current job goes bad on me, and my employer knows it. If you're trying to compete with Google, Mozilla, Facebook, Amazon, Microsoft et al for the grade of talent you need, but you don't have their deep pockets and large pool of employees, you can end up scared that you won't be able to get everything done because your experts want to keep working in F# instead of fixing C# bugs for the release.
&gt; That's an argument for making it easy and idiomatic to learn and to do things the right way, not for making it hard to do things the "wrong" way. The two things go hand in hand. You need to decide for both language constructs how hard or easy they should be. The important thing is the gradient: That the good way of doing something is *easier* than the bad way, not *harder*. &gt; GC is useful in more situations than just "le sigh, I must speak to a language that uses GC, time to use a bunch of purposefully-verbose APIs." A ton of stuff is purposefully verbose. That is no bad thing, but a legitimate tool of language design! Again, think of `unsafe`, or `get_unchecked`, or avoiding `Add` impls for pointer arithmetic. But we are doing circles here. And maybe I haven't expressed myself clearly enough: I definitely don't have something against using garbage collected objects easier than they are now. I just don't want to make them easier than proper memory management. This is not to punish you for using garbage collection, it is to *not* punish you if you want to do the right thing! &gt; Further, GC is a perfectly legitimate strategy elsewhere as well. Sure, if you want your program to scale badly across CPUs and be plagued by GC pauses, then do it!
&gt; That the good way of doing something is *easier* than the bad way, not *harder*. &gt; That's an argument for making it easy and idiomatic to learn and to do things the right way, not for making it hard to do things the "wrong" way. --- &gt; think of `unsafe`, or `get_unchecked`, or avoiding `Add` impls for pointer arithmetic. &gt; The reasoning behind `unsafe` and the lack of an `Add` impl for raw pointers is not that it makes those things harder, it's that it makes those things *obvious* and *easy to find.* --- &gt; Sure, if you want your program to scale badly across CPUs &gt; Concurrent data structures
We can throw the same arguments at each other ad infinitum but I suggest we stop.
I think WASM will make websites even fatter, for the simple reason that optimizing your framework to include only the parts you need is an even bigger overhead for the developer, as it involved recompilation. So why don't ship the entire 200 MB blob if everyone has gigabit fiber! You can already observe this with electron: the entire chromium fork is being shipped, with support for all media codecs, all image formats, etc. that chrome supports, even if your application maybe doesn't need any video playback support or webp. The issue is less about technology, it is more about an attitude that doesn't care about waste and bloat. Bad actors don't get punished for creating wasteful websites, instead they get rewarded if they can deliver a "done" website faster.
I'm surprised there's so much pushback to what I said. I'm calling for companies to value a diversity of knowledge. I never suggested that C++ is trivial or without uses.
You can find my experiments here btw https://github.com/killercup/wasm-experiments
I surveyed the Python fat-UI techs a few weeks ago, and the projects for each have many unresponded issues and pull-requests. Not exactly good signs :-( 
Yes, my point is that autovectorizers don't create good SIMD for that sort of code.
I only said it works best for loops, not that it only works for loops. :) My experience with SVE/scalar code is that optimising a single set of statements is often too localised to get good code: the programmer needs to make slightly more invasive changes to things like order of operations and data layout to be able to get the best SIMD-ification, and an autovectorizer either can't do so without risking changing the program's behaviour, or struggles to do so due to lack of context and information. And, even if the programmer manages to convince the compiler to get it to work, it is, as we both said, very fragile.
&gt; I don't think it's possible to be a top c++ developer with less than 5 years of experience besides some edge cases of really talented people and that's like 1 in 1000. Even 1 in a 1,000 seems a stretch :( It's one thing to learn the language, and practice it enough to be confident (and dangerous...), but there's also the whole environment in which it evolves. Good performance on a modern PC requires understanding a whole lot of interactions between different components (so many devices!), the bottlenecks (disk &gt; network &gt; RAM &gt; cache), stuff like data-dependency (which precludes pre-fetching/parallel execution), ... And all that is changing rapidly. The "best" solution of yesteryear (Robin Hood Hashing with Backward Shifting Deletion rocks) is actually already obsolete (see Matt Kulundis' talk at CppCon 2017 about using SSE intrinsics for hash look-up). Memory is at a premium, yet variable-length encoded integers (protobuf) is slower than zero-overhead decoding (aka transmute, as found in Cap'n'proto and SBE). Talent doesn't suffice, experience is required. I suppose someone programming non-stop (ie, even at home) could go from 0 to expert in 5 years... but I haven't seen it happening yet.
I am afraid that the worst thing about Qt is that it was designed for a very old version of C++. And it is very keen on backward compatibility...
[removed]
Thank you!
Thanks! The last time I tried to use associated types, it did not play well with the Trait Object setup I have going for storing components. I'd like to get rid of them if I can, as it'd enable much more natural patterns like having a `Handler` trait. Right now, I get tripped up when it comes to invoking the handler for the type. Since I'm working with a tree of trait objects, the concrete type information i.e `impl Handler&lt;ClickEvent&gt; for Button` has been lost and all I have as a `&amp;Component`
If you need a C++ developer you are not looking for diversity, you are looking for a c++ dev. I don't care if he can write a rest api in go or has data science knowledge in R if i want him to work on my physics engine. There are jobs, where you look for allrounders but i don't think that's the case when you work at blizzard on a game with one specific task.
I hope rust wins
D'oh! Of course :-)
If your target is primaraly linux, I highly recommend gtk and gtk-rs, a combination I used and have had success with. If your target is hassle-free cross platform development, you might have better luck with writting your GUI in another language and using Rust for the logic. Perhaps Qt/C++ for windows and linux.
I appear to be fighting with the compiler over the matter of a struct which modifies itself. Specifically over the matter of getting a vec of objects, calling a method on them in which they modify themselves, and putting them back, _without_ cloning, copying, referencing or otherwise mucking about with them. Depending on what I try, I either get 'value used after move', or 'cannot borrow as mutable'. This can't be a rare use case, so presumably there's a recommended approach. Does anyone have some advice? Current implementation that returns the correct thing but requires external input and another list: pub struct Foo { pub list: Vec&lt;Bar&gt;, // others, but not relevant } impl Foo { fn construct(mut self) -&gt; Result&lt;Foo&gt; { let mut rl = vec![]; for mut list_item in list_from_elsewhere() { list_item.construct(&amp;self)?; rl.push(list_item); } self.list = rl; Ok(self) } } Approximate desired implementation, which should result in the instance of Foo being modified but not copied, cloned or returned: impl Foo { fn construct(mut self) -&gt; Result&lt;()&gt; { for mut list_item in self.list { list_item.construct(&amp;self)?; } Ok(()) } } 
There's [conrod](https://github.com/PistonDevelopers/conrod), built on the Piston ecosystem. [Guide](https://docs.rs/conrod/0.56.0/conrod/guide/chapter_1/index.html) with screenshots and videos.
Are you compiling with `--release`? Feel free to post the projects to here, not much I can tell you without actually seeing the code.
No I did not run it with —release. Will try that later. The code is here: [client ](https://github.com/zoranzaric/emnify_hose) and [server ](https://github.com/zoranzaric/emnify_filter) 
I don't know enough about network programming to comment on your code anymore than that it looks clean and simple enough. Please do try again with `--release`, it makes a big difference because rust can leverage the power of LLVM in addition to any optimisations `rustc` can do itself.
Thanks a lot. I’ll try adding a struct to deserialize into and running with —release. The Go code is here: [server ](https://github.com/zoranzaric/emnify-filter.go) and [client ](https://github.com/zoranzaric/emnify-hose.go) 
Thanks guys, I really appreciate the feedback! :)
QML with QQuickControls 1.x uses native widgets and looks native on KDE. The 2.x version has the non-native styles which so far mimic the phone OS's. Fortunately both are still shipped with Qt. 
I'm currently writing my final year project (like a diss in other subjects) in Rust. It's on boosting algorithms :) It's been quite fun, Rust seems like a good language to write this sort of stuff in due to high performance. At least for me anyway, I'm sure if you were doing this properly it could be the wrong tool. Others have mentioned Python and I started off by writing my algorithm in Rust and then building a Python GUI to change parameters and so on. The compile times really do kill me though - when you're not entirely sure what you're doing so you're running things quite often having to wait ~30s for a build is a pain in the arse.
Thanks for the detailed reply. I am not sure though that I understand the concept of ‘unsafe{…}’ block.. I mean what is the point having it in your code or even worse, in some dependant crate, but still be able to hide it by not marking the enclosing function as unsafe? How can i ever be sure that my code doesn't depend on un safe code hiding in some dependant crate? &gt; Which itoa are you using? [this](https://crates.io/crates/itoa) one. Seems to have nothing to do with c 
I have a moderately-complex app ([gattii](https://crates.io/crates/gattii)) written using gtk-rs and I want it to be cross-platform. GTK has a really terrible cross-platform story and it doesn't have a good way to track state. So I've been really excited about some of these other libraries cropping up that could replace it and be much easier to use. I hadn't heard of `limn`, but I'm watching [relm](https://github.com/antoyo/relm) and [Indigo](https://nc4rrillo.github.io/) and will likely try to port over my GTK app once either of those matures a little bit and grows a little bit of a community.
We have a few Rust enthusiasts throughout Activision :)
QQuickControls 1.x is unsuitably incomplete compared to the QWidget API and, if I'm going to incorporate a dynamic language into my project using an under-maintained Rust binding, it might as well be the one I'm familiar with, which also happens to be complete enough that I always have the option of writing the entire project in it if performance and maintainability weren't concerns. (Heck, my approach is more or less "Write using rust-cpython and, if it wounds up too bit-rotted before PyO3 starts working on stable Rust, jettison Rust and move everything back to Python+PyQt.)
I really don't get, why the client dies after about 16 000 Requests. Do I have to close connections? thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Io(Error { repr: Os { code: 49, message: "Can\'t assign requested address" } })', src/libcore/result.rs:906:4 note: Run with `RUST_BACKTRACE=1` for a backtrace.
&gt; i'm pissing my pants, laughing out loud etc Not quite I would save LMAO for that, but an audible chuckle did escape my lips. It is a very smart, detailed oriented things to do, just something I think a lot of developers would have overlooked, or couldn't have been bothered with.
&gt; You wouldn’t immediately think to search for “parking lot” if you were looking for concurrency primitives for instance. % cargo esr -o -s concurrency primitives | grep '^(' (1) crossbeam (2) rayon (3) triple_buffer (4) fibers (5) fastq (6) desync (7) thunk (8) carboxyl (9) rculock (10) canal It would have worked if *concurrency* was mentioned anywhere in `parking_lot`'s meta info. It makes the top 10 if we search for "*primitives*" alone. % cargo esr -o -s primitives | grep '^(' (1) tokio-core (2) tokio-io (3) crossbeam (4) bincode (5) rayon (6) ring (7) parking_lot (8) enum_primitive (9) conrod (10) dtoa Any search &amp; ranking tool, official or not, will be as good as the meta info provided by crates. As long as you don't ask for perfection, discoverability is not a hard problem. And everyone can help, simply by submitting trivial PRs that add keywords/categories to their favorite crates.
We'd love to speak with the people using Rust there as part of our annual commercial outreach survey, if they're interested. :)
GTK+3.x is also bad on anything outside GNOME3's Mutter compositor. There are unsolved bug reports of people using a plain window manager with or without a compositor or XFWM4 with its built-in compositor and still suffering from drawing regressions compared to GTK+2.x. GTK+3.x scaling of widgets and dialogs when used under Wayland is done in a fashion that gives you zoomed out rendering which is blurry. Qt5's Wayland backend doesn't do this. Just two data points to show that GTK+3 is only fully supported and smooth under Mutter, hence GNOME3. That is on Linux, on macOS and Windows the backend may be better, since there's a single rendering path.
I agreed with everything else he said, but calling Rust 'esoteric', especially when that word is used to describe languages like Brainfuck more than in its normal meaning in CS, is slightly infuriating, especially when Go (which imho has a much stranger set of design decisions going on) is specifically mentioned as being a realistic language for games.
&gt;GTK is crap on anything non-linux. Sounds good to me!
Ah yes so FWIW wasm has no "print" instruction in the sense that the only option it has for output is to hand it to JS. The standard library, however, doesn't want to assume anything exists, so `println!` is "implemented" but it's just a noop effectively. If you're debugging Rust wasm and can recompile libstd in `src/libstd/sys/wasm/mod.rs` there's a constant `DEBUG` which you can set to `true`, and then you'll be using shims defined in `src/etc/wasm32-shim.js` which do the printing pieces for node at least.
Qt is the way to go if you think that at some point you might want to run this on some commercial embedded device. And it's generally more reliable/predictable than GTK+3.x, in my experience. GTK+4 will break backwards compatibility again and probably lose another batch of applications to the idea of a Qt port. More than a few projects have migrated to Qt out of frustration with GTK's stability guarantees, APIs, and development model.
The reason why go is so much faster and rust is crashing is that the go version transparently shares the clieng across requests which saves it from creating a new connection each time. In rust you can do this by creating a http client and using it for each request. You will get that error if you make a large about &gt; 16k requests to the same destination IP/port from the same ip in less than 60 seconds.
Your client code appears to only spawn 1 request, are you trying to spawn 16,000 clients on one computer? I think what is happening is you are running out of ephemeral ports, Linux supports about 32,000 for each destination IP address. If you are connecting to a server on the same computer that's going to mean a maximum of 16,000 simultaneous connections. If you manage to spawn all your processes before any start cleaning up it's going to crash.
Is there anything reasonably good for all platforms (Windows, macOS, Linux, iOS, Android)? It seems Qt supports all these platforms, but I'm not sure on the quality of the support on each and whether it's worth supporting them all with one library. I want the following, in rough order of preference: 1. Stable 1. Well documented 1. Fast (needs to handle large amounts of formatted text) 1. Custom widgets 1. Native enough feel to not be distracting (me doing some extra work here is fine, such as having different designs per platform) * I want access to unique features per platform as well 1. Can be used for commercial and open source projects, preferably without licensing fees (though I would donate if it makes me money) 1. Supports a nice, modem language (preferably Rust or Go, but I'm willing to sacrifice a bit here) I feel like this shouldn't be as hard to find as it is. I think Qt is what I want, and if I can stick with QML for everything I'll be golden, but it's hard to get an idea of which languages have a pleasant development experience with which platforms.
Do you know this other dataframe? https://github.com/sinhrks/brassfibre
Absolutely, it'll be at least a year probably more. Personally I'll be putting the next 4 months into it full time-ish, then probably part time after that. I'm hoping after those 4 months the core architecture will be mature enough to know whether it is fundamentally sound and worth developing a larger community and set of complex desktop style components for, hence the "experimental" status. The components that exist right now are mostly to validate the architecture. In the meantime I'd say GTK is probably the way to go.
Haven't seen it before but it looks cool. Personally, I try to maintain 'bounded' feature sets, which means I can represent features using structs. So like: struct Features {a: f32, b: f32} So a Vec of those would be somewhat similar to a dataframe
Just thought I'd add a couple more sophisticated examples that I have worked on: - [An old synth editor from a few years back](https://imgur.com/a/htAuo) - [The GUI for my generative audio workstation](https://www.instagram.com/p/BbOTYyWjDiX/) - [A GUI for a spatial audio server](https://imgur.com/2PycCye) (currently under contract) Unfortunately I can't share the code for these as the first is too old for today's conrod and the last two are for commercial work! Hopefully it gives more of an idea anyways.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/2PD3Upp.jpg** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
Author of faster here. The elevator pitch for faster is "bring stdsimd to the masses". In fact, most of it is 1-line wrappers around stdsimd intrinsics with some type magic to make it all look nice. 
I think whatever representation those dataframes have internally they should be able to serialize to a Vec&lt;Row&gt; where Row is something you define as your example. I've opened a PR to brassfibre a couple of days ago but not response and Utah has no action since last December also :( 
Hello experienced Rustaceans! I'm looking for the best, most Rustic, way to destructure and modify a value while satisfying the borrow checker as in the following example: enum Foo { Bar(Box&lt;()&gt;), Baz(Box&lt;()&gt;), } impl Foo { fn change(&amp;mut self) { *self = match self { Foo::Bar(a) =&gt; Foo::Baz(a), Foo::Baz(a) =&gt; Foo::Bar(a), }; std::mem::forget(std::mem::replace(self, self_)); } } Naturally, this doesn't compile because the borrow checker determines that the `match` moves out of `self` while it is borrowed. However, by the end of the function, I'm going to give `self` a new value. Importantly, in this scenario `Foo` doesn't have a default, so I can't just give `self` a temporary value. Currently, I find myself employing this pattern: fn change(&amp;mut self) { use std::mem; let self_ = mem::replace(self, unsafe{mem::uninitialized()}); let self_ = match self_ { Foo::Bar(a) =&gt; Foo::Baz(a), Foo::Baz(a) =&gt; Foo::Bar(a), }; mem::forget(mem::replace(self, self_)); } But obviously I'd prefer to avoid `unsafe` code, and this approach as written is quite unsafe in the event of panics (perhaps a scope guard could fix that). In any case, I would be most grateful if someone could point me toward the best way to handle cases like this.
A wider range of experience results in better development. I regularly encounter situations in which my knowledge of other technologies and solutions help me solve the problems in front of me.
&gt;if in the end it gets *easier* to use GC integration than to do proper memory management, then it would create a slippery slope towards GC. The same is true for `Rc`/`Arc`, and people is still trying to use borrows as much as possible. I don't think that a `Gc&lt;T&gt;` type is going to be different.
&gt; they would love to try it Unfortunately I'm not convinced this is the case, panel had little interest for anything besides C++. They mentioned Go but I wouldn't say any on the panel were excited about it. It's not so surprising: they're invested in C++, both their company (with large C++ code bases) and personally (they know C++ really well). At least IMO, it's hard to enthuse anyone to begin to incremental (!) Rust usage without them trying it out in their own problem domain... and seeing the benefits for themselves.
If the types being boxed are really zero-sized (or you don't mind the allocation), this at least avoids unsafe: fn change(&amp;mut self) { use Foo::*; let x = std::mem::replace( match *self { Bar(ref mut b) | Baz(ref mut b) =&gt; b }, Default::default() ); *self = match *self { Bar(_) =&gt; Baz(x), Baz(_) =&gt; Bar(x) }; }
Ofc it benefits you, but that doesn't change the fact, that you need long time experience for certain jobs like working at a game / engine as the guys at blizzard do. And finding someone with that experience and other skills is most of the time impossible. You can't take avarage c++ devs for these jobs. It doesn't work no matter how mutch experience they may have in other fields, because there is a treshold of skill that is needed. I know that this is different in web development, data science and many other fields because the 'hard' work as in writing all the complex librarys is already done and you need other and way more versatile skillsets to do your job. But if you need someone to do something as specifc and complex as game engine related programming your regular 'i know c# so how hard can c++ be' won't be able to do it. You are also not going to hire a first semester art Student, that just started to learn how shading works, if you want a copy of Mona lisa.
Isn't Relm a different API on top of GTK ?
Every time we bring up GC stuff this argument crops up, and it's plain wrong. We already make `Rc&lt;T&gt;` easy to use. It's not used pervasively. `rust-gc` already makes `Gc&lt;T&gt;` almost as easy to use as `Rc&lt;T&gt;`. It's not used pervasively either. The GC integration plans involve whole program decisions (specifically, about the trace hook -- you can only have one) on how the GC is to work; which is harder to work with than `Rc&lt;T&gt;` and `rust_gc::Gc&lt;T&gt;` (the reason this is still better than rust-gc is that rust-gc sacrifices a lot of performance for this to work). Furthermore, the GC integration plans don't come with a built in `Gc&lt;T&gt;` type; you have to write this yourself. Folks can make crates, but `rust_gc` exists already and again, folks aren't using that.
If `Gc&lt;T&gt;` is as hard/easy to use as `Rc&lt;T&gt;` is today, then I don't have problems with it. My point was that it would be really bad if Gc was made *easier* than proper memory management, like via coercing `T` to `Gc&lt;T&gt;` (making it unneccessary to type `Gc::new(value)` or to even do `use std::gc::Gc`). And no, the answer is not to coerce `T` to `&amp;T` in all cases, it is to leave the hands off of such coercions and keep it explicit what the value is. &gt; Folks can make crates, but rust_gc exists already and again, folks aren't using that. I don't think an argument of the form "crate X exists already so let's integrate it into the standard library!!!" is ever good. If you have a library with a proc macro that adds OOP features to the language it doesn't mean that anyone pointing out that OOP is confusing and just makes code more complicated and bloated can be silenced by pointing to that library. There is an extra non zero cost involved in discovering and importing a crate. Again, I'm generally in favour of having Gc integrated into the language, as long as references stay easier to use, but the argument is wrong.
&gt; My point was that it would be really bad if Gc was made easier than proper memory management Nobody is saying that. &gt; I don't think an argument of the form "crate X exists already so let's integrate it into the standard library!!!" is ever good. That is ... nowhere near what I said? I'm saying that the "Rust GC integration" will not come with a built in Gc type, which makes it harder to use. People can make such a type in a crate, but we already know that having such a type available in a crate doesn't lead to the apocalypse scenario you seemed to be worried about because rust-gc exists.
Rust Qt bindings are supposedly progressing: https://phabricator.kde.org/source/rust-qt-binding-generator/
I hope they *both* win :P
Here's some stuff I've done with gtk-rs: - Font Finder: https://github.com/mmstick/fontfinder - Systemd Manager: https://github.com/mmstick/systemd-manager - TV Renamer: https://github.com/mmstick/tv-renamer It's getting increasingly-good at developing UIs with Rust.
Serious work occurs in languages other than C++. The cult of personality around C++ is so frustrating and counterproductive. I've encountered many people who are skilled in C++ that refused to acknowledge the ideas and innovations of other languages/tools simply because it wasn't the 'C++ way'.
It works because you're _not_ borrowing `self` in the closure.
I've not seen any issues with i3wm.
I don't see your point. Also, &gt; (like probabilistic models) Nah
The associated library for SNARKs, `bellman`, was re-written from C++ (`libsnark`) to Rust (`bellman`), and some preliminary benchmarks indicate that the performance is better, which is awesome considering `libsnark` is highly optimized!
Isn’t one of them working on gfx-rs or maybe I’m misremembering.
Correctness is *not* guaranteed by the language. The freedom of an upstream crate to be *completely wrong* has as least as much impact as its freedom to use `unsafe` Consider WPA-Krack vs Heartbleed. Both are security vulnerabilities that come from simple, understandable bugs. Krack (on Linux) comes from a mistake you can make in safe Rust: zeroing a variable (encryption key) you think won't be used again then accidentally reusing it. Heartbleed comes from a mistake that needs `unsafe`: forgetting how long a slice should be and calculating it incorrectly. The advantage of `unsafe` is it tells you when you need to be cautious, and conversely when a module is free of `unsafe` you know it should only contain the kind of sensible, rational incorrect-behaviour bugs that are possible in Ruby or Java, vs the utterly insane undefined-behaviour bugs that are common in C.
Unfortunately, I don't think you'll find anything that fits that criteria better than Qt if you use C++, and even then, it won't be as native as the system's default UI system. &gt; Why does GUI have to be so hard? I might just build a web thing and be done with it (though "fast" may be a bit of a tall order). I attribute GUI development difficulty to the fact that these systems continuously diverge in small but observable details that users will notice, and it takes a considerable effort to unify or replicate these details under one API in a manner that can be considered acceptable. While I agree that the state of native GUI development is abysmal, I still implore you to use the native GUI of the systems you target or at least a full-fledged GUI library. Not only will it be lighter on your users' systems, it will also improve accessibility for those who need it. (think larger fonts, custom contrast, on-screen readers and so on)
&gt; and it is very keen on backward compatibility and that's a very good thing in my opinion. I'd take an ugly api that is stable over a shiny one that will break every few years.
Thats another topic and another discussion. Here it is about blizzard that has a game engine written in c++ and if you want a job there, you are better good at it and therefore he gives this advice. There is no benefit to hire a avarage c++ dev with experience in diffrent fields instead of an expert c++ dev with experience in what they try to do, because everyone there is hired for his specific task that he is good at. And this is the same for firmware development and other low level software because it requires you to be extermly good at one thing. This is completly different if you work in web dev, business software, data science and other fields. But you "complained" about him saying that you should focus on c++, obviously from the perspective that you are interested in video game development, because that was, what the question was about and there is nothing wrong with his answer. If you look at the video game industry right now, most high tier engines are written in c++ if not all of them. (CryEngine, Frostbite Engine, Source Engine, Unreal Engine) I think unity is written in c#, but unity is not really for AAA Games anyway. 
I know many folks who are mentally, socially and economically very invested in C++, they acknowledge Rust, but they have not tried it. But not trying it was a conscious decision because of the above. The motivations are very complex, but some of it is sunk cost fallacy for sure. If we could merge Rust tooling with C++ tooling it would less discontinuous. Evolve, not exterminate. I'd love to turn every C++ programmer into a Rust programmer. Only gently. 
How do you save to tfrecords? In what format do you save the training data?
Do you think it's reasonable to have a separate GUI implementation for each platform with calls into a common library? That seems like a royal pain... I guess my options at this point are: * write in a language I don't hate in Qt (Go bindings seem to be reasonably stable now) * write a separate GUI for each platform with a common library in a language I like (e.g. Rust); this complicates cross platform development * write a webapp and distribute an application that wraps that up I'm leaning toward the first option, though the last is certainly appealing if I'm going to build a webapp anyway. I'll have to try out a minimal Qt app on all platforms I care about.
Is deterministic compilation something planned for Rust? It was a major point of contention in the last ceremony.
QML is a bit limited in my experience, the latest support with QtQuick Controls 2 is better and it has platform specific support too, but the current direction for QML is unified look and feel vs native to the platform. QWidget is meant to provide native UI but had some other drawbacks iirc. Maybe Qt 6 will address these issues better as a blog post by Qt about it seemed like they were going to rework things to accomodate better. While I haven't tried for native UI, React-Native seems to be working towards this support(else just plain React). Windows and macOS platforms are still probably not quite there vs alternatives, mobile platforms are fairly well done iirc, and Linux I'm not sure if that has a React-Native support yet(there were some initial efforts for GTK or Qt backends I think). React-Native while modern JS based, along with it's JSX format for UI specific code is pretty nice to work with(with Redux and immutable libraries). They handle UI logic in JS which provides nice flexibility(you can get some typing and other needs through things like TypeScript or Flow and linters). The actual UI renders natively on it's own thread and was pleasant to work with. Business logic(code not related to UI) can be in Rust and called, there were two threads on this subreddit a month or so ago about using Rust with React-Native to get the best of both worlds :) Downside I guess is to benefit you have 2x languages and ecosystems to be familiar with.
It is being worked on: https://github.com/rust-lang/rust/issues/34902 But you know, even one little bug can cause you problems :)
Oh, excellent. Last time I mentioned it (a while ago) it didn't seem to be considered a major issue. I guess Zcash prompted more people to care about it.
I already use React for web projects, so that's actually am interesting option. I was trying to avoid electron-esque solutions due to performance issues, but I'll definitely take a look and see how it works.
For network operations, I do think a hashed timer makes more sense since the timers will be very short lived and relatively few. I use a [hierarchical wheel](https://github.com/ben-manes/caffeine/blob/master/caffeine/src/main/java/com/github/benmanes/caffeine/cache/TimerWheel.java) for cache expiration, where the duration and capacity might fluctuate more widely. There I used power-of-two time spans for faster bit manipulations. A sorted list is O(lg k), correct? That cost is paid every time a timer is added or removed, which might be done frequently. An unsorted list is O(1), but requires a full scan on a tick if different durations are commingled like in a hashed wheel. A hierarchical wheel will cascade entries from higher (coarse) wheels to lower (finer) wheels. Since the lowest wheel is fine grained, its okay to expire the entire list together and doesn't need to be sorted. The penalty of cascading is O(k), but less frequent than scheduling so the overall amortized cost should be O(1). I don't see a benefit of sorted lists in this layout, do you?
Are you using `env CARGO_INCREMENTAL=1 cargo build`?
I think that `mem::replace` is the easiest option. You have to tell to the compiler that it is safe to move the in `&amp;mut self`, so you have to be the owner of it If you don't care about an extra allocation, something like this can work ([full example in the playground](https://play.rust-lang.org/?gist=4a3dada24a002aa5d89d7f7d4953b0eb)). fn change(&amp;mut self) { let data = mem::replace(self, Foo::Bar(Box::new(()))); *self = match data { Foo::Bar(a) =&gt; Foo::Baz(a), Foo::Baz(a) =&gt; Foo::Bar(a), }; } If you don't care about unsafe code, you can use `mem::uninitialized`: fn change(&amp;mut self) { let data = mem::replace(self, unsafe { mem::uninitialized() }); *self = match data { Foo::Bar(a) =&gt; Foo::Baz(a), Foo::Baz(a) =&gt; Foo::Bar(a), }; } But this is a bad solution, IMO. Another option is to add a private value in your `enum`, used only for the intermediary value ([playground](https://play.rust-lang.org/?gist=c261a147d48eafdc5a68b40127435a9d&amp;version=stable)): enum Foo { Bar(Box&lt;()&gt;), Baz(Box&lt;()&gt;), Nothing, } impl Foo { fn change(&amp;mut self) { let data = mem::replace(self, Foo::Nothing); *self = match data { Foo::Bar(a) =&gt; Foo::Baz(a), Foo::Baz(a) =&gt; Foo::Bar(a), Foo::Nothing =&gt; unreachable!(), }; } } 
Well, yes. Gimp and inkscape is pretty much trash compared to their commercial alternatives like illustrator, photoshop, sketch.app. I'm pretty sure Firefox uses GTK under the hood only on Linux and *bsd. So yeah, GTK on anything outside of linux and bsd is trash and on linux/bsd it's either only available option or ugly alternative to app that uses Qt.
One example: https://bugzilla.gnome.org/show_bug.cgi?id=757104
Examples: https://bugzilla.redhat.com/show_bug.cgi?id=1370791 https://bugzilla.gnome.org/show_bug.cgi?id=757104
Can't reproduce it on my end, with a Carrizo APU + Mesa 17.2.
It is hard to know without debugging, but I think that the key of the problem is in this loop: do { stream.read(rawBuffer, 0, RAW_BUFFER_SIZE); baos.write(rawBuffer); length -= RAW_BUFFER_SIZE; } while(length &gt; RAW_BUFFER_SIZE); stream.read(rawBuffer, 0, length); baos.write(rawBuffer, 0, length); You are reusing `rawBuffer` in every iteration to read the data in `stream`. However, in the `baos.write` line, you write the full array to `baos`. For example, let's suppose that there are 1500 bytes to be read in `stream`: 1. In `stream.read`, you fill `rawBuffer` with the first 1000 bytes. 2. In `baos.write`, you write these 1000 bytes to `baos`. 3. Then, the next `stream.read` will read only 500 bytes. The last part of `rawBuffer` will be untouched, so it keeps the data in the previous read. 4. In the next `baos.write`, you write again 1000 bytes (the new 500 bytes, mixed with the old one). You have to check the return value of `stream.read` to know how many bytes are valid in `rawBuffer`. A solution can be something like this: } else { baos.write(rawBuffer, 0, offset); int pending = length; while(pending &gt; 0) { int len = stream.read(rawBuffer, 0, Math.min(pending, RAW_BUFFER_SIZE)); if(len == -1) { // ERROR no more data } baos.write(rawBuffer, 0, len); pending -= len; } } 
In case you REALLY need to have references, you should try using Rc&lt;RefCell&lt;MyTypeHere&gt;&gt; instead. Unless you want to go crazy and step into unsafeland with *mut.
[removed]
It’s possible. I‘ve worked with the fine people of reproducible-builds.org (I’m the dev mentioned in that issue) and I’m currently testing every PR for reproducibility in one of my rust projects: https://github.com/kpcyrd/sniffglue/blob/master/ci/reprotest.sh This has all variations of reprotest enabled except +time, because this breaks cargos certificate verification. If you don’t use FFI, it’s also a lot easier to pass the tests.
&gt; some of it is sunk cost fallacy for sure... ...but some of it is probably quite pragmatic. Rust solves a certain class of problems, yes, but those are problems that may not actually be particularly relevant to these people. If you've *already* invested in solving the irritating C++ issues; tooling, packages, static analysis, deployment, compilers, etc. then... the benefits of the rust tooling would probably seem quite limited, and the lack of good tooling (eg. state of RLS) would be a major negative. If the project you're working on (ie. game engines) are already quite solid (when was the last time a blizzard game crashed on you?), and don't have server-side-security requirements... rust's safety features don't really sound like they're *particularly* silver bullet magic. I think rust's intro to the gamedev world isn't going to be through engine programmers picking it up, frankly... I just don't see people accepting it as a valid trade off. More practically, if we get some really good rust libraries which have easy C++ interop, rust can flow in, in the same way it flows into firefox; gently, without C++ programmers having to really argue about it. A great water physics engine with a C api? Sure. I'll use that. Rewrite a small critical network library in rust to ensure its safe, but let that be transparent of the existing library api? totally. Rewrite the engine in rust? Yeah... nah. The absolute worst thing you can do for rust is pitch it to C++ programmers when you can't articulate *exactly* what the benefit they'll get from it is. I think that pitch is probably quite hard for most game developers; pitching easy to use excellent libraries written *in* rust is probably an easier path. 
React-Native should for platforms it works on support native apps rather than contained within Electron sandbox? I haven't done anything personally on desktop so I can't say how well those are for desktop apps vs Qt and other native solutions. On mobile it was pretty good. I'd like a React and Redux like experience built with Rust but it seems that type of development approach isn't well suited with Rust alone from what I've seen? (few attempts at Redux porting that didn't go too well, and some Reactive(eg Rx) libs that also seemed to struggle a bit at translating into Rust) Perhaps that'll change in future :)
&gt; A sorted list is O(lg k), correct? Are you referring to insertion time here? With the current hashed timer wheel, if you guarantee that any duration inserted into the wheel is less than tick_duration * num_slots, you get O(1) insertions and O(1) expirations (O(1) removals will be had either way). If any timeout exceeds that, it will fall back to O(n) (since we are using a pseudo doubly linked list here, no random access so no O(lg n)). So if I built a hierarchical timer wheel on top of multiple of these wheels, I just have to guarantee that for each second, minute, hour, etc, that I configure their corresponding wheel correctly, and I should get those runtimes.
Your desired code does not work because it creates aliasing references `&amp;mut list_item` (that gets passed to method as `&amp;mut self`), and `&amp;self`. However, if you can guarantee that that method does not use `self.list` which you are iterating over, you can then move that list out for iteration, and replace with a dummy value (`Vec::new` does not allocate). You also need to handle errors explicitly to be able to fix everything up before returning. And also there was a bit more of hoop jumping to get around what I think needs NLL to work. fn construct(&amp;mut self) -&gt; Result&lt;()&gt; { let mut temp = ::std::mem::replace(&amp;mut self.list, Vec::new()); let mut outcome = Ok(()); for list_item in &amp;mut temp { if let Err(e) = list_item.construct(&amp;self) { outcome = Err(e.into()); break; } } self.list = temp; outcome }
Yeah, doesn't Qt even have its own string type?
Can I debug tests in Visual Studio Code? I know I can debug Rust unit tests using gdb, but working with gdb gets so tiresome. Is there a way to do this with VSCode?
I think your second solution would drop old value of `*self` (which is `uninitialized()`) on assingment, which would be very much undefined behavior because of contained `Box`. 
P.S. Is it just me or does gdb print values inaccurately when debugging Rust code? It's happened multiple times and makes me think "memory corruption error", but I'm only using safe Rust and some pretty vanilla libraries. Makes it hard to figure out what's wrong in your code....
I see you've never used Inkscape on MacOS. It integrates horrendously since it uses XQuartz.
&gt; when was the last time a blizzard game crashed on you? It seems that you haven't played Overwatch yet ;)
It does, and same for vector, linked list, map, etc. They're generally easier to use than the standard library types, but they exist because they predate the standard library. 
Unity's core is also written in C++.
Heck, I got banned from overwatch due to a bug. They eventually acknowledged it and unbanned me...
Very good catch! I totally overlooked sharing the client!
Also Monero should announce a cerimony in Rust
&gt; when was the last time a blizzard game crashed on you They crash all the time for me, even the one that wasn't written in C++. Granted, I do weird stuff which causes that, so...
You should try seeing issues like these from a GTK+ developer side. GTK+ is decades old, has a million lines of C... and has basically 4 people working on it. Most of them are volunteers and I don't think it's very unreasonable to want to spend your own spare time with different issues than installing another WM and testing things with it (been there, done that). And the overlap of people who know enough about X, GDK and are willing to reverse-engineer the black window background problem? Apparently empty. Similar stories apply for the non-linux issue. I've heard the gtk3 Windows story is better than gtk2 (if that means anything) these days but again it's a manpower problem. Nobody maintains gtk on mac, a minimal set of people on Windows. Sometimes there's a burst of activity (like when the gtk3 win32 theme was written) but if nobody keeps maintaining that work it just bitrots (... like what happened to the win32 theme).
It is practically impossible to teach good programming to students that have had a prior exposure to Go: as potential programmers they are mentally mutilated beyond hope of regeneration.
&gt; they know C++ really well There is no such person on the planet.
&gt; it was designed for a very old version of C++ It was designed for a current C++ standard and compiler support at that time.
Indeed. I actually tried to tinker with the code in the jpeg-decoder crate to see if I could get the compiler to auto-vectorize some of the colour space transformations, but to no avail.
For those who, like me, have no idea what this is about: &gt; tiny is an IRC client written in Rust. 
Ah, I should've mentioned that in the title -- sorry ;-(
GIMP and Inkscape are both already decent in a mixed-DPI Windows environment. In the GIMP, “100%” image scaling means physical pixels, so it’s half the size on a 2× display to what it is on a 1× display, but the actual *UI* is all fine on 1× and 2×. And Inkscape crashes if I make its window too large (e.g. maximise it) on my external 1× displays, but apart from that, it does a good job of mixed-DPI support.
On the contrary, this is an optimising compiler that can strip unused stuff out, inline functions where it will reduce size, *&amp;c.*, processing what will hopefully be most or all of your code base (except for those annoying tracking snippets that marketing insist you put it…) in one go. This will take more *compilation* time, but *way* less developer time. (Because it will do for you something that requires very careful selection of tools and quite a bit of trouble in JavaScript.) By contrast, JavaScript libraries are almost all “let’s add this entire new library, even though we use only a small part of it” because JavaScript is a menace to trace dead code on. (ES6 modules improve the situation a *little* as you can remove modules that aren’t imported at all—so long as (a) you deal with a library that is written in such a way that you can take advantage of that (few do, thus far), and (b) you do full-program optimisation (few do).
In C, we can link a binary blob as follows: a.c: #include &lt;stdio.h&gt; extern char _binary_hoge_txt_start[]; extern char _binary_hoge_txt_end[]; int main(){ char *p = _binary_hoge_txt_start; while(p &lt; _binary_hoge_txt_end){ printf("%c",*p); p++; } } then % echo "hoge" &gt; hoge.txt % objcopy --readonly-text -I binary -O elf64-x86-64 -B i386 hoge.txt hoge.o % gcc a.c hoge.o % ./a.out hoge Is there any way to do this in rust? It seems "extern" is for calling a function..
Y'all, there's a lot of trash talking of other projects in this thread. I know the rules only explicitly call out the need for civility and respectfulness when discussing other *programming languages*, but I think the spirit of them suggests that that is also the right attitude to have towards widget toolkits, GUI applications, and anything else.
Ah, I see. That's kind of a "no, you can't do that" answer, but it does solve most of it. Final working version (with error_chain to deal with the outcome bit): fn construct(&amp;mut self) -&gt; Result&lt;()&gt; { let mut temp = ::std::mem::replace(&amp;mut self.repos, Vec::new()); for list_item in &amp;mut temp { list_item.construct(self)?; } self.list = temp; Ok(()) } I would still like to eliminate lines 2 and 6, but I think that's now just aesthetics rather than allocations, so I'm a lot less concerned. Thanks!
&gt; * An old synth editor from a few years back &gt; * The GUI for my generative audio workstation &gt; * A GUI for a spatial audio server (currently under contract) Are all three projects using conrod?
Note that your solution would leave `self.list` empty if any of the `.construct` calls returned an error, instead of leaving it in partially processed state.
Yes - the second is a long-term personal project I've used for some installation and live performance stuff, the third is some contract work for an upcoming exhibition.
If you just want contents of an external file to be available as a string constant, you can use [`include_str`](https://doc.rust-lang.org/std/macro.include_str.html) macro. There is also [`include_bytes`](https://doc.rust-lang.org/std/macro.include_bytes.html), which will give you a `&amp;[u8]` instead of an `&amp;str`.
&gt;good job of mixed-DPI support On Windows 10? X11 doesn't support per-monitor DPI. Also, controls look tiny on linux.
The problem I have with this is more about *safety* than *beauty*. When whether a method call takes ownership of a pointer is documented in comments only, it's bound to lead to issues pretty regularly. There are certain rules, and a certain uniformity, to Qt, so normally you should get used to which does what, but I'd still prefer if it was documented in the types rather than the comments. Of course, prior to C++11, it would have meant `std::auto_ptr`, which is a mixed blessing :x
All three projects looks super clean! Great work. I have been investigating Rust GUI libraries and bindings to existing libs for a while now without landing in a decision, until now. I absolutely must try some conrod experiments. Thanks for sharing!
ah, I found the solution: create a library ar rcs libhoge.a hoge.o a.rs: #[link(name="hoge", kind="static")] extern { static _binary_hoge_txt_start : u8; static _binary_hoge_txt_end : u8; } fn main(){ let mut p : *const u8 = unsafe{&amp;_binary_hoge_txt_start as *const _}; let end : *const u8 = unsafe{&amp;_binary_hoge_txt_end as *const _}; while p &lt;= end { unsafe{ print!("{}", *p as char); } p = unsafe{p.offset(1)}; } } then % rustc -L. a.rs % ./a hoge Thanks jDomantas for the suggestion. but I guess sometimes including a binary is too big to compile.
Actually, there wasn't even a C++ standard when it was designed (C++ was created in '83, but only got standardized in '98). Which is why it Qt has its own `QString`, ... the standard library didn't exist yet. This is problematic because standard types are *vocabulary types*, the building blocks which allow libraries to be mixed and matched freely, but since Qt doesn't use the standard types, you're kinda stuck in Qt-land. It has a certain uniformity, which is its own appeal, but it also mean you're missing out.
Thanks for the kind words! I should really give myself a week off to spend on updating and fleshing out the guide at some point, but I always find myself too busy with my own projects/work. Feel free to open issues at the repo if you get stuck on anything or just need some advice and I'll do my best answer!
I don't believe anything about the way we currently write GUI software is reasonable. However, you work with what you have. Developing specifically for each platform is the only way I know of that provides a truly native GUI experience for users. That said, Qt is more than often good enough (especially when you don't have the resources). If I was a single developer and I wanted to do cross-platform development, Qt/C++/Rust would be my first and immediate choice, at least for the time being.
But that doesn't make sense, because the payload starts to differ around 1400 bytes in, rather than after 1024 bytes.
I use the Native Debug extension for this.
Sometimes, yes. It has basic Rust support, but there's always more to do.
I am a single developer and will be for the foreseeable future for projects like this. At work I do mostly web/backend stuff, but I have lots of ideas for cross platform apps (there's a notable lack of apps that work on all platforms I care about with syncing between them). I really want to use Rust for this because I have run into far too many concurrency bugs that I want the compiler on my side to double check my work. I can keep all the GUI stuff separated for the most part, so C++ &amp;. Rust may be the way to go here. Would you have C++ or Rust be the entry point? If C++, then has there been any work in getting Rust srtucts or generics to work from C++ or would I just use a C API?
This seems like an appropriate place to ask my question. I am a computer science student, in my junior year. My professor for my object-orientated class has decided that we would be programming in Rust, rather than the normal C++. Now, I am not a strong programmer by any means. I struggle with the homework, mostly the syntax. I am probably the worst in the world, but I do appreciate it, and I want to improve. And the way I have decided I want to improve is by designing a game. I want it to be a table-top kind of simulator, but a focus on cards. Such as Magic the Gathering, or others. But I have no idea of how to go about this. I would *rather not* use Piston, as I kind of want to learn as much as I can, from the bottom up. However, if it is a better idea to learn a resource given to me, rather than develop my own, I would do so. But anyways, I ask this subreddit's users for their advice on how to approach such a thing. Thanks in advanced.
It's probably better to make your own thread, and maybe one on /r/rust_gamedev too.
I've put probably 50 hours into Overwatch, and I've honestly never had it crash. My desktop is total overkill, so there's no real chance of the game ever running out of any resource, but it has been stable for me, at least. I still think most people underestimate the value Rust could bring to the game industry.
I will do so. Thank you.
Ultimately this will panic if any of it doesn't work; partially empty is functionally as bad as entirely empty (i.e. 'halt completely') for this use case, so I'm just passing the error up the chain and killing everything if any step fails. It's not very graceful, I admit, and your version is definitely better for a situation where failure might be recoverable or acceptable.
The story of Qt 4 begins in 2005, so C++03 already existed then, but I doubt that it was fully implemented in all major compilers. That's why Qt does not use std containers, exceptions, templates (heavily), etc. &gt;which allow libraries to be mixed and matched freely That's why I like Rust. Everyone relies on std types. On the C++ land a lot of libraries still using `const char *` and even `std::string` is mostly useless, because it doesn't know anything about encoding.
https://gitter.im/rust-lang/rust might not be the best place but who knows, if you still having issues. For me It looks more active than IRC. 
C++ is much easier than Rust, so does nobody know Rust really well either?
Have you tried [`rust-gdb`](https://github.com/rust-lang/rust/blob/master/src/etc/rust-gdb)? It adds more detailed pretty-printer support including `Vec`, `String` and their corresponding slices, as well as automatic dereferencing IIRC.
Define "easier"; C++ is just a far larger language with large parts of no longer in use any more but kept for backwards compatibility. Kind of how no one understands the Xorg codebase any more with large portions of it just sitting there not being used.
Hmm, after setting up the config file it now just closes after flashing something I can't read onto the screen. No panics, no error reports, just immediately closes.
Recent versions of QT have moved on to c++11 at least
I could use a recommendation for a data structure, and maybe a library that implements it? Feels like a try, but I'm not so sure (I googled and read a bit about trees in rust, and it all looked a bit too complicated for what I have in mind). I need to save up pairs`(a, b)` of `u64`'s, where `a&lt;=b` (that is, ranges). For two pairs `(a,b)` and `(x,y)`, the following is always true: Either one is a subrange of the other (i.e. `a&lt;=x` and ` y&lt;=b` or vice versa), or the ranges are disjoint (i.e. `b&lt;x` or `y&lt;a`). This seems to lead to a tree-structure via the inclusion relationship. I'll commonly need to do the following: Given a number `l`, which ranges contain `l`? Also, delete those ranges containing `l` without loosing any others, and then regenerate the structure after recomputing the ranges that contain `l`. I'll also need to do this not only for one number `l`, but several at once (most of the time, the candidates for `l` will form a range themselves). To elaborate that last point, suppose I have a range `A` containing the (disjoint) ranges `x, y, z`, and suppose `l` is contained in `z`. Then I need to identify `z` and `A` as the containing ones, and recompute them, after recomputation, `A` will still contain `x, y, z`, but both `A, z` might change up to the point that `z` does not exist anymore. The inclusion relationship is expected to be pretty shallow most of the time, I'd guess 2 depth most of the time, 3 max. If anyone uses `vim`, this is about saving the data of folds. Thanks for any pointers. Right now I am thinking about struct folding { folds: Vec&lt;Fold&gt; } struct Fold { range : [u64;2] subranges: Vec&lt;Fold&gt; } but I have no idea if that will be a good way to do it. So thanks for any pointers :) 
In my opinion, you should not aim for an ambitious target like tabletop simulator while new to programming. It will take several years to polish your knowledge and skills to be able to create a project as complex as this. I believe you should try simple projects first, maybe make clones of early games like Pong, Breakout, Asteroids. You will learn a lot, and you will get immediate results at every step of the way (Display a blank window -&gt; draw a sprite -&gt; Control the sprite -&gt; Add interactions/collisions).
I will take your advice with gratitude. Do you know of any resources that would help me get started?
Sorry about that. Can you try `RUST_BACKTRACE=1 tiny 2&gt;tiny_stderr; cat tiny_stderr` and tell me what it prints after it closes?
It often hangs with a black screen after a match for me. You can still use the menu, so not exactly a crash. But I had this once: https://us.battle.net/forums/en/overwatch/topic/20744354236
Did you know that this week in rust has a sentence that explains what Rust is, to give their post context, every week. You can do that too. It's a good idea, you never know how far away a link to your post can travel. &gt; Hello and welcome to another issue of This Week in Rust! Rust is a systems language pursuing the trifecta: safety, concurrency, and speed. This is a weekly summary of its progress and community. 
The bindings are not terribly idiomatic (and the Tensorflow API is generally quite poor), but they definitely get the job done as long as you're happy defining and fitting your model in Python.
I think most people dealing with a large enough C++ project would rather have it written in Rust, but rewriting large codebases, from any language to any language, is a massive effort that most of the time isn't worth the time/money. That's why the Rust community should focus on growing the ecosystem until we reach a level of maturity that allows people to stop deciding to write *new* projects in C++ and opting for Rust instead.
 thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error { repr: Os { code: 2, message: "No such file or directory" } }', /checkout/src/libcore/result.rs:906:4 stack backtrace: [insert libstd unwinding here] 10: &lt;core::result::Result&lt;T, E&gt;&gt;::unwrap at /checkout/src/libcore/result.rs:772 11: tiny::logger::Logger::new at src/logger.rs:52 12: tiny::Tiny::run at src/lib.rs:202 13: tiny::run at src/lib.rs:106 14: tiny::main at bin/main.rs:4 15: __rust_maybe_catch_panic at /checkout/src/libpanic_unwind/lib.rs:99 16: std::rt::lang_start at /checkout/src/libstd/panicking.rs:459 at /checkout/src/libstd/panic.rs:361 at /checkout/src/libstd/rt.rs:59 17: main 18: __libc_start_main 19: _start 
And GTK is named for GIMP and not the other way around.
I did some reading and it seems things are progressing in the "React everywhere and reasonably fast" category. In particular, here are a few resources I found that seem particularly interesting: * [Building a mobile app in Rust and React Native](https://medium.com/@marekkotewicz/building-a-mobile-app-in-rust-and-react-native-part-1-project-setup-b8dbcf3f539f) * [React Native in Qt (Linux only for now it seems)](https://github.com/status-im/react-native-desktop) * [React Native for macOS](https://github.com/ptmt/react-native-macos/issues) * [React Native for Windows](https://github.com/Microsoft/react-native-windows) * [Code sharing between React Native and React Web apps](https://medium.com/the-many/code-sharing-between-react-native-and-react-web-apps-b1e1de22fc53) It seems that all the pieces are there, I'm just not sure how hard it would be up build an app targeting all platforms with enough code sharing to make it worth it. It seems theoretically possible to build a single app for all platforms with a common library between them written in Rust, but it's very early days it seems. Maybe I'll play with this and see if I can put together a template project to compile for all the things (WebAssembly for React &lt;-&gt; Rust bridge seems particularly interesting).
Thanks for the stack trace -- and good catch. I created an issue for this on github.
Pretty much.
Yay for the Curry-Howard Isomorphism!
My distro (void linux) uses libressl, and the install failed when openssl was not found.
Is there no way to install OpenSSL on Void Linux? tiny uses [`native_tls`](https://crates.io/crates/native-tls) which uses openssl on Linux. It seems like we don't have bindings for libressl yet so I think the best we can do about this (unless it's ABI compatible with openssl) is to disable TLS features with a compile-time flag. I created an issue about this on Github.
I just checked, openssl is not in the repos.
/r/playrust
r/playrust
But that requires *running* the compiler in the first place. As I've explained above on the example of electron, nobody in the js ecosystem is running compilers (or even linkers) if the only reward is to cut down on bloat.
There's also http://arewemetayet.com/
Yes! I always hate when status reports or release announcements from projects don't include something like that, nor a link to a main page which describes it. It's frequently the case that there's something of enough interest in such an announcement to submit it to a more general news aggregator like /r/programming or [Hacker News](https://news.ycombinator.com/news) or https://lobste.rs/, but people who come into it without context then have no idea what it's talking about. I think that all progress announcements or release announcements should have at least a sentence or two about what the project is about, and a link to find out more. Regulars can easily scan over it to get to the meat of the announcement, while it helps provide context to people who haven't heard of the project.
The openssl crate works with libressl a lot of the time, but the versions are finicky. I think openssl-0.9.19 worked with libressl up to 2.6.1, or something? 
While certainly interesting, note that according to /u/petertodd there was some flaw in the ceremony - in PRNG, if I remember correctly.
It's been considered important for a while now, given that Debian strongly encourages packages to be able to be built reproducibly.
Based on this thread, [filed a ticket with `native_tls`](https://github.com/sfackler/rust-native-tls/issues/60) to see if it would be possible for `native_tls` to support LibreSSL explicitly.
Monero doesn't need trusted setup and so it's superior in that regard.
Do you handle compiling C++ dependencies manually, as a separate step from using Rust tooling? I've been wanting to investigate Rust for it's build system (to escape the insanity that is C/C++ cross platform build and meta-build systems). If cargo could simplify that I would be estatic, but I don't know how realistic that is.
Yes they are. Most serious JavaScript apps are using toolchains built around tools like webpack and babel these days (there are others, I just name those two as the most popular). Many websites are using such tooling as well, though it is far from most. As for the rest, the normal, smaller websites, WASM doesn’t tend to have a great deal to offer them. WASM will shine on bigger projects that are already using fancy tooling.
Tokio uses mio, mio is a low level wrapper around epoll, kqueue, etc. If you're just starting I would suggest you use tokio, or even a higher level crate such as hyper for HTTP.
You should know that the team acknowledges Tokio is a bit hard at the moment, and is planning on some ergonomic improvements plus a total re-write of the docs in the near future.
In my case, I just want Rust on the web as an even stricter alternative to TypeScript. (I do DOM manipulation almost exclusively because I focus heavily on progressive enhancement of sites that work as well as feasibly possible with JavaScript disabled and consider the browser too heavy to be an acceptable platform for applications which don't already have an inherent need for a network-connected hypertext renderer, such as webmail, browser-based RSS readers, Wikipedia, etc.)
There is the `take_mut` create which basically encapsulates the pattern you've been using: it provides `take(&amp;mut T, Fn(T) -&gt; T)`. It aborts on panic in the closure, to avoid an uninitialized value left in the reference.
I'll definitely take "Rust is cool" and "someday, I hope". Like other people have pointed out, C++ is definitely the standard here. Also, if folks like Mike Acton are a good indicator (1), it's a very particular kind of C++ rather than what other C++ experts generally write. That is to say, that it's very much its own beast. That's not to say we can't make in-roads in the future. Like the Chucklefish folks mention (2), they had the opportunity to use Rust because the project was from scratch/green field. I'd imagine for AAA games, writing from scratch is exceedingly rare, which is where the Blizzard devs are coming from. Overall, my take away is pretty positive, as it's what I'd expect but also a kind take on it (rather than "no way! C++ for life!") 1: https://www.youtube.com/watch?v=rX0ItVEVjHc 2: https://www.reddit.com/r/rust/comments/78bowa/hey_this_is_kyren_from_chucklefish_we_make_and/
(It's a joke.)
Is eddyb working professionally full time in Rust?
mio is just an abstraction on top of epoll/kqueue/iocp. A scalable way to poll for nonblocking sockets/file descriptors. Using mio means you are on your own with regards to structuring your program to work with nonblocking sockets. I like to bash Tokio every chance I get. It's a framework that makes programs unreadable, un-debuggable messes. People jumped on that ship way too fast.
Do webpack and babel recompile electron? I can't recall that they do. Correct me if I'm wrong, but those tools are just text processors plus minification tools. They don't ship with a linker / compiler toolchain, so they are not comparable. Also, how is asm.js being exposed currently to javascript developers? From what I saw, through npm packages that include a gigantic asm.js file inside their "source code", so you are downloading precompiled asm.js files already. Probably wasm will be exposed the same way to js developers: npm will download wasm files.
Maybe there should be an FAQ sticky post with this because it seems to come up a lot.
That's the Radeon stack. Maybe I should avoid Intel GPUs in next machine purchase.
Of course, and that's why I asked if GTK+3 is officially supported and tested only with Mutter. It's not a big surprise, and I'm offended or anything, but this resulted in silence. The issues above and what you mentioned are why Qt is so popular. It may not be 100% native, and neither is GTK+3, but their macOS and Windows support is good enough that even big commercial projects use it. If you look at it from the issues you outlined, we should be happy that Qt has a commercial business model and therefore allocates development resources to those. With all of that being said, let's please not forget that the GNOME developers have been refactoring GTK+ again and are improving the "million lines of C", so I get what you're saying and feel their pain. However, it's fair to report in a friendly way and express our frustration as to regressions introduced in 3.x and still not fixed after years of it being stable.
I'm not sure how it's handled in general, but I only have one real C++ dependency, rocksdb. I don't handle it myself, it's all done by https://github.com/spacejam/rust-rocksdb, and building the C++ code is handled by a build script in that crate. I believe compiling the code is handled by https://crates.io/crates/cc, though checking with the rust-rocksdb repository for details is probably a good idea. It's all statically linked, and keeping track of the rocksdb version is done via a git submodule :/. It works well, but it's all geared around exposing C++ code as a rust crate.
Maybe it is my C background but I found mio much easier to use than tokio. In my experience tokio was poorly documented, especially when it comes to how you are meant to best use the API.
Amusingly, it seems that future Intel laptops will be shipping with Radeon graphics. The main issue with Intel's graphics is that they decided to do their own thing, rather than doing as Nouveau and AMD in developing drivers with the Gallium3D architecture in Mesa. As for NVIDIA, people often have problems with newer Linux distributions because the proprietary driver breaks between Xorg / Linux releases. The open source driver provides a much better desktop experience, but it's lacking in a number of ways due to being the result of reverse engineering, rather than an official project from NVIDIA's own developers. Personally, I've always purchased AMD hardware exclusively, so I've never really had any issues with Linux. The open source driver is developed by AMD and has great gaming performance. Doesn't require anything proprietary, and it just works between rolling release updates.
Yep, those links all look familiar, you might want to investigate they have the platform support for whatever your app plans to do. Nice find with the Linux Qt fork from Canonical! Glad it's seeing active development :) You did miss this one that I had in mind, [ReactXP](https://github.com/Microsoft/reactxp), it's an abstraction over a few to get iOS, Android, Windows, macOS to be more similar and code sharing. I haven't tried it personally but if I get around to doing React-Native again it's probably something I'd look into!
&gt; I like to bash Tokio every chance I get. It's a framework that makes programs unreadable, un-debuggable messes. People jumped on that ship way too fast. If this is at all defensible, the comment is pertinent to the discussion and offers more insight (despite going into no detail) than other critical comments in this thread.
There is a slightly better pattern for the enabler. template &lt;class T, class = decltype(std::declval&lt;const T&amp;&gt;().print())&gt; using Printable = T; void do_then_print(Printable&lt;T&gt; t) { t.print(); }
Yeah. A game written in C++ not crashing due to safety issues is a testament to the engineers working on the game. Not C++.
I'd generally agree that you should set small achievable goals. But also, the _ambition_ to do something can be an incredible driving force. While it's unlikely they would ever finish the project, the journey can be tremendously educational even if it ends up being a failure.
&gt; Amusingly, it seems that future Intel laptops will be shipping with Radeon graphics. The main issue with Intel's graphics is that they decided to do their own thing, rather than doing as Nouveau and AMD in developing drivers with the Gallium3D architecture in Mesa. Intel drivers used to be good, but they've been pretty chaotic in stability for the last two years. &gt; As for NVIDIA, people often have problems with newer Linux distributions because the proprietary driver breaks between Xorg / Linux releases. The open source driver provides a much better desktop experience, but it's lacking in a number of ways due to being the result of reverse engineering, rather than an official project from NVIDIA's own developers. I think if I owned an Nvidia card, I would stick with LTS kernels, which I believe should be fine. Otherwise, as I said, I'd get an incentive to use FreeBSD. &gt; Personally, I've always purchased AMD hardware exclusively, so I've never really had any issues with Linux. The open source driver is developed by AMD and has great gaming performance. Doesn't require anything proprietary, and it just works between rolling release updates. As I said, maybe an APU laptop will be the right choice. Intel has lost my trust. But if atomic modesetting is the primary source of GPU hangs and fences will make it worse, then I fear that once those are introduced in the Radeon stack the same bugs might surface. To have an experience free of GPU hang, this is what I have to pass to the kernel right now: video=SVIDEO-1:d i915.semaphores=1 i915.enable_rc6=0 i915.enable_psr=0 intel_iommu=igfx_off The video= and semaphores=1 seem to be the most important flags. Up until kernel 4.1 (before atomic modesetting got in), I never once thought about bugs in the Intel graphics stack. It was just working. Oh and let's remember that the generic modesetting DDX for Xorg tears heavily and that's why we have to stay on xf86-video-intel for its support of TearFree=TRUE. All of those issues and Nvidia's stack pretty much replacing all of it from the kernel up to Xorg and libgl and vdpau (I use VAAPI with Intel, to be clear) made me question whether betting on Nvidia would be wise. I mean, their mobile chips are low power and CUDA seems to be the only supported accelerator in all the GPU accel projects. And their desktop GPUs are also low power. It's funny, but I'm serious considering it at this point.
While I agree he could have shown a nicer enabler, you can't argue that scaling concept requirements is a pain in C++, whereas in Rust it directly maps to the trait's specification.
&gt; anything other than C++ makes you less hire-able It's better to compare supply and demand. You only need one job, after all. 
To be fair tokio was the only solution to problem it was trying to solve and it was developed by...well...names everyone in /r/rust knows. It is still is. I feel like tokio was developed upside-down — started with tokio-service and then everything down the stack which made the rest of its stack hard to use. That's just how I see it. When I started to yse tokio-service it took care of 90% of my problems, but other 10% were nearly impossible to do without dropping tokio-service. Decision to make it all very single-threaded-ish was weird too. 
&gt; Thats not true from my experience. It may be the case that C++ developers need 5 years to reach the top, but I think that the fact companies have such an emphasis on integrating into their existing tech stack (rather than picking the right tool) is mostly due to entrenched interests, not sound decision-making. 
Mostly its just a cost thing and in switzerland also a staff thing. Short term cost is way lower if you don't change the tech stack and thats what mid level management is intrested in, making them look good and swap jobs to a higher position if possible. Changing the tech stack and causing big investments can really hurt that plan. One reason why i only work for Startups or in positions where i can choose my tools. And there are nearly no really good devs and to find devs for exotic languages like rust or go is close to impossible.
They do work; see https://crates.io/crates/typenum. But that's not nearly as nice as having something built into the language, for a number of reasons.
Well AFAIK typenum doesn't use associated consts.
It doesn't, but they would be trivial to add. I just haven't had the time to muck with it since they've been stabilized.
Nobody would own the store so you cannot have a reference to it. As other said either you use the store directly or you get the store as a mutable borrow argument.
Similarly "upside-down", was trying to ensure that `AsyncRead` works with `Read`. While noble, the fact that the task system is so very hidden has crippled implicit learnability of the whole system. Rather than not imposing that hurdle, only through meticulous extensive documentation, and very diligent students, will they be able to overcome that hurdle. 
Owning the store is probably simpler and faster here.
I agree that mio is simpler to use on my side too. I am waiting for async/await for tokio to be much simpler.
This sure looks like a bug to me. The Rust issue tracker is a better place to ask.
I love it! As a newcomer and very casual IRC user this is perfect. Took me a bit to figure out the keybindings but it's super easy to use and looks very clean.
They are a much bigger problem in C++, one of Rust's main competitors.
This is just a known side effect of const generics not being implemented properly yet.
Same question here. Last year talks are on YouTube though.
I can confirm this ;)
E1138 THAT’S NO MOON
Out of curiosity, what would you write for someone that made you laugh out loud? And for something that's really funnier?
This is exactly what I was looking for. Thanks!
Interesting, I'll have to keep an eye on it. I have projects occupying me until the end of the month, but after that I'll want to dive into UI stuff for the holidays. If I can get a CI system that can run builds on several platforms (Android, Linux and Windows to start, bonus points for Web) with a nontrivial app using Rust in some way, then I'll write up a blog post and post it here. Who knows if I'll actually get it done...
In this instance, yes. But who knows what he might need in the future.
The point is that tools like webpack and babel provide functionality not fundamentally different from what you get with rustc's emscripten backend. For example, merging together the files comprising each compilation unit, performing a *very* limited form of dead code elimination, providing optimization of some form (in this case, minification), translating between languages (eg. compiling ES6 down to ES3), and performing "code generation" in some cases (best seen with CSS toolchains, where the equivalent to ES6-&gt;ES3 compilation is to generate a block of vendor-prefixed rules from a standard rule that's not supported by all target browsers.)
Won't it also make using mio simpler? 
&gt; Decision to make it all very single-threaded-ish was weird too. I agree with everything except this. If you mean they should abstract away threads or make everything thread safe means you are taking on fixed costs (mutex, atomic variables) that you can not get rid of and will cost you dearly in any benchmark. 
It wouldn't actually work for array lengths, which can't depend on generics yet.
I am not sure you need Rust or C++ for any of this. Pick one of the lowest barrier to entry engines you can find. Time box yourself to 1-2 hours for getting hello world running in an engine. If it clicks, use it. I think [Lua Love](http://love2d.org/) is an excellent, zero friction engine. The language and the engine don't matter. The ideas matter. Once honed, you can then project them into Rust if need be.
Is there a list of scenarios that non-lexical lifetimes while make easier? Niko's NLL blog post series lists 3 but I've read that there are other scenarios like `vec.push(vec.len())`.
That was the previous ceremony, and it wasn't a PRNG issue, he had other issues with things like deterministic builds and dependencies etc. As a consequence of the brittle nature of the previous ceremony, it was difficult to give participants flexibility in their software/hardware choices. That's what the new ceremony is trying to fix, in addition to being really large in the number of participants.
I'd employ this simplification: https://gist.github.com/teaearlgraycold/d24d1c79557d583865facf896f74ff10/revisions
One of my biggest gripes with the language is its treatment of Null.
There is also [`relm`](https://github.com/antoyo/relm), which is a fancy async thingy on top of `gtk-rs`.
Well, what am I supposed to do with my 16 cores now? What is this? Node.js? I now know that you can run multiple reactors getting events from for the same to, but it's not explained in docs at all. I think you misunderstood me here. I'm talking about model that nginx uses without reuse socket option.
Yes you're right. It just looked odd to me to propose something that complicated to someone who is just starting getting his head around ownership.
Probably. I was referring to futures-await in particular which if I'm not wrong if more tokio oriented.
I've been working on my compiler. (Is the name Nova available?) I just saw this when checking Reddit after getting a Main module fully compiled! Source: module Main (main) main : (Int, Ptr (Ptr Int8)) -&gt; Int main (n, _) = n (The type declaration is parsed but isn't actually used anywhere yet.) Output: ; ModuleID = 'Main' source_filename = "Main" @_0 = external global i64 ({ i64, i8** })* define i64 @main(i64, i8**) { entry: store i64 ({ i64, i8** })* @_1, i64 ({ i64, i8** })** @_0 %argn = insertvalue { i64, i8** } undef, i64 %0, 0 %argv = insertvalue { i64, i8** } %argn, i8** %1, 1 %fun = load i64 ({ i64, i8** })*, i64 ({ i64, i8** })** @_0 %init = call i64 %fun({ i64, i8** } %argv) ret i64 %init } define i64 @_1({ i64, i8** }) { _entry: %_5 = extractvalue { i64, i8** } %0, 0 br label %_3 _2: ; preds = %_3 %_6 = phi i64 [ %_5, %_3 ] ret i64 %_6 _3: ; preds = %_entry br label %_2 _4: ; No predecessors! unreachable } Nobody said it had to be optimized. I was concerned about values being declared but not constant, so I just allocated a global and filled it in in the initializer. LLVM's optomizer takes care of everything at the moment, though it still keeps the global as a pointer to be set at runtime. LLVM is currently trolling me as the assembly output seems to completely neglect the _0 pointer. There's probably something simple I'm overlooking to fix that. The language itself still has a ways to go. Custom data types are currently being parsed, but aren't used yet. Complex refutable pattern matches are completely broken at the moment as they're handled in an extremely naive way. Oh, and no builtin functions exist at the moment (at least in the backend), which is why no arithmetic is being shown. Finally, none of the static checks (other than basic typechecking) are being done yet. The language should be usable without them, but otherwise, it's Haskell syntax with C unsafety. I should mention that functions are curried by default and I'm rather proud of my closure implementation. Namely, function application is statically overloaded. That said, 2 functions only unify if they capture the same types (and in the same order, but that shouldn't be too hard a fix). So ya. I've been building a compiler in rust over the past while. It has been really hard so far and I don't think the worst is over yet, but I've gotten to a good point.
You're looking for: https://github.com/rust-lang/rust/issues/44168
C.A.R Hoare's 'Billion Dollar Mistake'
Ah, that's too bad. At least there's generic-array to hold us over.
How do I solve this more cleanly: (1) let map_id = if let Some(map_id) = renderer.current_active_map { map_id } else { return None; }; In this case I want to early-return from the function if the map_id is not present. Not sure how to do this better. (2) I have different types of vertex definitions (using glium). All implement T: IntoVertexBufferSource. But now I have this problem: pub enum VertexBufferType { UtmLineVertBuf(VertexBuffer&lt;UtmLineVert&gt;), UtmAreaVertBuf(VertexBuffer&lt;UtmAreaVert&gt;), UtmImageVertBuf(VertexBuffer&lt;UtmImageVert&gt;), UiBackgroundVertBuf(VertexBuffer&lt;UiBackgroundVert&gt;), UiImageVertBuf(VertexBuffer&lt;UiImageVert&gt;), } impl&lt;'a&gt; ::glium::vertex::IntoVerticesSource&lt;'a&gt; for VertexBufferType { fn into_vertices_source(self) -&gt; ::glium::vertex::VerticesSource&lt;'a&gt; { use self::VertexBufferType::*; match self { UtmLineVertBuf(buf) =&gt; buf.into_vertices_source(), UtmAreaVertBuf(buf) =&gt; buf.into_vertices_source(), UtmImageVertBuf(buf) =&gt; buf.into_vertices_source(), UiBackgroundVertBuf(buf) =&gt; buf.into_vertices_source(), UiImageVertBuf(buf) =&gt; buf.into_vertices_source(), } } } Is there any way that I can say: All variants of this enum implement this trait, so just run the `into_vertices_source` function for all variants? (3) How can I easily convert to errors using error-chain with several modules? I have an error nested in AppError -&gt; LayerError module. Now I have this situation: fn something() -&gt; Result&lt;(), AppError&gt; { Err(AppError::from(LayerError::from_kind(LayerErrorKind::NoGeometry))) } I hate error-chains "ErrorKind", it only gets in the way. I have to do three imports into the module (AppError, LayerError, LayerErrorKind) in order to do the conversion correctly. It sucks. Is there any way to do this more efficiently? I end up with imports like this use types::errors::node_graph::ErrorKind as NodeGraphErrorKind; use types::errors::layer::Error as LayerError; use types::errors::layer::ErrorKind as LayerErrorKind; use types::errors::database::Error as DatabaseError; use types::errors::database::ErrorKind as DatabaseErrorKind; use types::errors::Error as AppError; use types::errors::ErrorKind as AppErrorKind; I hate this. It clutters all the imports, just so Rust can do the from conversion of errors.
They said anything that will make it into rust will not be tied to Tokio. So I'm assuming it will be usable with just mio. Thus making Tokio redundant. 
This was in the old thread only for a very short time, so I'll repost here hoping to get some ideas: I could use a recommendation for a data structure, and maybe a library that implements it? Feels like a tree, but I'm not so sure (I googled and read a bit about trees in rust, and it all looked a bit too complicated for what I have in mind). I need to save up pairs`(a, b)` of `u64`'s, where `a&lt;=b` (that is, ranges). For two pairs `(a,b)` and `(x,y)`, the following is always true: Either one is a subrange of the other (i.e. `a&lt;=x` and ` y&lt;=b` or vice versa), or the ranges are disjoint (i.e. `b&lt;x` or `y&lt;a`). This seems to lead to a tree-structure via the inclusion relationship. I'll commonly need to do the following: * Given a number `l`, which ranges contain `l`? Also, delete those ranges containing `l` without loosing any others, and then regenerate the structure after recomputing the ranges that contain `l`. To elaborate that last point, suppose I have a range `A` containing the (disjoint) ranges `x, y, z`, and suppose `l` is contained in `z`. Then I need to identify `z` and `A` as the containing ones, and recompute them. After recomputation, `A` will still contain `x, y, z`, but both `A, z` might change up to the point that `z` does not exist anymore. * Given a number `l`, are there ranges bordering on `l`, and of what type are they? Determine if `l` need to be within a range (that's an external function I can define), and subsume this range if it is of the same type as one of the others, possibly even joining them. I'll also need to do these things not only for one number `l`, but several at once (most of the time, the candidates for `l` will form a range themselves). * Traverse the data structure from bottom to top, that is, if a range `A` contains a range `x`, the transversal needs to put `x` before `A`. Most other ordering considerations aren't that important, I think. If I had to prioritise, the second point is the most important, the first the least important, the last can probably be done recursively somewhat easily. The inclusion relationship is expected to be pretty shallow most of the time, I'd guess 2 depth most of the time, 3 max. If anyone uses vim, this is about saving and manipulating the data of folds. Thanks for any pointers. Right now I am thinking about struct folding { folds: Vec&lt;Fold&gt; } struct Fold { type: FoldType; range : [u64;2] subranges: Option&lt;Vec&lt;Fold&gt;&gt; } where `FoldType` is an enum I'll yet have to define. But I have no idea if that will be a good way to do it. I hope I've put all that I need in here :) So thanks for any pointers :) 
So I'm writing bindings to a c library and there's a function that has the (simplified) signature: void f(t** object_ptr_ptr) Using rust-bindgen this generates a rust function: fn f (object_ptr_ptr: &amp;mut &amp;mut t) Basically `f` takes the `object_ptr_ptr` and overwrites `object_ptr` with a pointer to a newly allocated `t`. However I can't create a `&amp;mut &amp;mut t` from rust because there isn't a constructor and the generated `t` has private fields (`_unused: [u8; 0]`. What's the best way to be able to create a `&amp;mut &amp;mut t`?
Looks great,- can key bindings be changed? 
Looks interesting, I'll check it out!
I'm developing a crate to build levenshtein automata.
Glad you liked it. Key bindings are hard-coded at the moment ;-(
Cow?
No problems,- hasta la vista baby! (Just kidding)
Are you sure that you're binding on "0.0.0.0:8080" and not on "127.0.0.1:8080"?
I'm continuing work on my [nanovg rewrite](https://github.com/Lisoph/nanovg-rs/tree/rewrite). A lot of the API is already finished, but I still have to export things like font rendering and some auxiliary functions. The point of the rewrite is to have a new, more rust-idiomatic API on top of the nanovg FFI.
Bewere it might exists: http://blog.burntsushi.net/transducers/#levenshtein-automata https://github.com/BurntSushi/fst
Either /u/rozaliev 's answer or you're being blocked by a firewall.
I thank them individually in the [changelog](https://github.com/Geal/nom/blob/master/CHANGELOG.md)
Right, but it could, and associated consts do nothing to fix the problem of having to explicitly define all types. At least typenum gives you an implicit if unwieldy way of doing it.
Thx. I figured it out. That was the exact problem.
Thanks. Yes I am aware of this implementation. I am trying to build a faster one, that produce DFAs with less states.
I just jumped the Rust train about two weeks ago, and I'm already loving it! I've been fiddling around with lots of tiny test programs, but my main beef is now a re-write of a 3D game engine I wrote a few years ago in C++. I'm taking ideas from my old code, as well as from [LibGDX](https://github.com/libgdx/libgdx) which I have used quite extensively for years. I've learned quite a bit about the language doing this engine. I just finished the mesh-related functionality, using configurable mesh structures. Now I can do this: let mesh = Mesh::with(&amp;[ VertexAttrib::Position, VertexAttrib::TexCoord0, VertexAttrib::Color ]); ... and this will reflect my actual mesh data. The idea is pretty much the same as in LibGDX, but the syntax and structure got a lot simpler. I also got a basic batch renderer up and running, although it doesn't do much at the moment besides drawing quads. So, this week I'll try to get the batch renderer a bit further, introduce a 3D sprite struct and functionality and just overall get something usable done.
I have finished main parts of morq last week and published the repo: https://morq.rs/ https://github.com/afshinm/morq I'm going to continue my work this week and it would be great if you guys send me you feedback / comments.
Yes, the data structure that you want is basically a tree where each node is an interval, and has any number of child trees - your definition of `Fold` is basically that. But I don't know any existing crates implementing this (doesn't really mean anything - I barely use other crates). However, if you don't need the first operation, you can get away without a tree (if I understood your requirements correctly). Here's a data structure that would allow performing second and third operation quickly (I assume you need that. If you don't, just store a plain `Vec&lt;Interval&gt;`, it will be much simpler): Maintain two copies of the interval list - first one ordered by interval start point, end the second ordered by end point. Basically something like `BTreeMap&lt;(u64, u64), FoldType&gt;` where in the first one keys are `(start, end)`, and in the second one `(end, start)`. You can perform insertions and deletions in these in logarithmic time. Now to answer the question "what intervals border on point `x`?". All intervals that start at that point will form a consecutive segment in first list, which you can get with `BTreeMap::range`. Same for the second list, but by endpoint. But I don't really get what you mean by &gt; Determine if `l` need to be within a range (that's an external function I can define), and subsume this range if it is of the same type as one of the others, possibly even joining them. Now traversing bottom to top. If interval `a` contains interval `b`, then starting point of `a` will not be after start point of `b`. If the starting points are the same, then endpoint of `a` will be after end point of `b` (otherwise both intervals will be the same). So you just need to sort intervals by decreasing starting point, and then by increasing end point. You can have this by also maintaining a third `BTreeMap` that would keep your intervals in this order. I might have completely misunderstood what you need, but maybe this will be helpful. If you have any more questions about data structures feel free to ask me :)
If you don't know the layout and contents of a type you cannot create an instance. Either you'll have to get correct layout, or you'll have to have the C library generate an instance for you.
The way nodejs handles threading is exactly the way it should be done. What keeps you from horizontally scaling your application?
Two notes: `lengthOf` and `empty` should take an `IntoIterator` rather than an `Iterator` (so you don't have to `.iter()` in order to use it), and Rust's style guide recommends `snake_case` for method names rather than `camelCase`, so multiword pseudo-methods like `lengthOf` should be `length_of` instead. Otherwise, this looks really nice! Good job. This is just personal taste, but could you make this use spaces instead of dots? I know that dots make it look more like rspec but since it's a Rust macro there's no reason to constrain the syntax.
 &gt;When I started to yse tokio-service it took care of 90% of my problems, but other 10% were nearly impossible to do without dropping tokio-service. A case that happens when using frameworks. They tend to make not that difficult stuff easier, hard stuff they make even harder. 
Yay, I'm in there too! :) Although mistyped: drbgn -&gt; dbrgn. In my projects, I also mention contributors in the changelog. I'm not sure an explicit contributors file still makes sense nowadays, since you can simply link to [this](https://github.com/Geal/nom/graphs/contributors) instead, which also shows how much was contributed, during which timeframe.
Thanks! I will update those asserts. I don't think it is possible to add space though. That is somehow the limitation of the macros. See: https://stackoverflow.com/a/46667666/375966
A good part of Rust is that its package ecosystem and strong guarantees about safety (which allow you to have peace-of-mind when plugging libraries together) mean that less has to be rewritten on each project. A lot of C/C++ projects reinvent the wheel a whole bunch because it's easier than dealing with package management in those languages. Rust simply has less of those problems.
I am working more on my [alice-rs](https://github.com/cbourjau/alice-rs) project: Analyzing the public data of the CERN based ALICE experiment with rust and minimal external dependencies (only plain [ROOT6](https://root.cern.ch/); no ALICE software). Currently I am working on dropping that ROOT6 dependencies as well. The published binary format is self contained as such as it contains its own schema description. I managed to write a nom based parser for that schema description and am now working on auto-generating the nom-parsers to read the actual data based on these schemata. I think [quote](https://github.com/dtolnay/quote) should be the right tool for that job? Any other ROOT users out here?
That's exactly the kind of answer I was hoping for, thanks a lot! The more I think about it, the better I like that design of keeping those 2 lists. I'll ponder it some more, but I guess I'll just give this a shot tonight. Is there a reason you used a tuple instead of an array here? I'm also struggling of `Ord` does what I want on a tuple or 2-array here, I've been clicking in circles for some time, could you point me to the relevant part in the docs? Just for clarification: &gt; But I don't really get what you mean by &gt;&gt;Determine if l need to be within a range (that's an external function I can define), and subsume this range if it is of the same type as one of the others, possibly even joining them. Say I have ranges (1,5) and (7,10) of type SHELL and NODE resp. Now if I change line 6 to be of type NODE, I'd need to delete the second fold and create a new one, namely (6,10). If the range (1,5) would have been of type NODE as well, I'd have to delete both of them and create (1,10). Thanks again for your help :)
Yeah, it looks cool but so many dots... In Rspec you have either ".to" or ".not_to" maybe you can use that to remove a dot there. Also, Rspec now prefers to do expect(xx.len()).to.be.equal(2) rather than expect(xx).to.be.lenght_of(2) Because len is just one property of many you may want to check so rather just use equal everywhere. BTW, would be posible that expect(xx).to.be.equal(yy) be shortened to expect(xx).to.eq(yy) ?
Hi! I need to do a project for one of my exams at UNI, and I decided to create a simple WM (as it's basically what the course is all about). I found the x11 crate, containing the xlib module. I got this [window_system.rs](https://i.imgur.com/cmbjAYv.png) and this [main.rs](https://i.imgur.com/R9gPTtJ.png) but when I run cargo build I get [this](https://i.imgur.com/CXU5ERq.png) Any idea? 
Thanks for your comments jcarres. Yes, I'm going to add `should` and simple `expect(x).to.eq(x)`. Remember, these are just a "representation" of the asserts classes. 
I just kind of expected that tuples would be ordered lexicographically. I don't see a mention of that in the docs, but I can see from source of `Ord` and `PartialOrd` impls. The part you clarified would indeed be easily doable - from first list you can get first interval that starts after the point, and from the second you can get the last one that ends before the point.
can you send a PR to fix that? :D That contributors list generated by github is especially nice because it gives a good overview of the project: - how many active maintainers (for most project, only 1 and overworked) - do contributors come back for a second contribution? (most do not, and that's perfectly fine, but we want regular contributors too) - is it easy to contribute small fixes?
How would `should` work? Should is discouraged in Rspec now (I know I come back to Rspec all the time but they have been doing this for many years so I guess they got something really good by now)
Nice work /u/nick29581 !!! It seems that had I been asked this question one week later I would have answered very differently. &gt; you can use git-fmt which This looks so cool. Our common workflow is to submit PRs to a branch with new commits rebased on top of master, so it would actually be great for us to have something like `cargo fmt --git-branch` that applies formatting to the diff all commits in a branch that are not in the master branch. I guess that most people using github have a similar workflow, so integreting something like this into `cargo fmt` might make it way nicer to use than clang format. &gt; So not perfect, but not bad for a moderately-sized project. Yes I'm sorry. What I meant here wasn't bugs as in it destroys the formatting or the app crashes but more like as in "the formatting doesn't work as I expect it to be". For me the biggest bug is that the tool output is not unique. That is, if I give the tool two copies of the same file with slightly different manual-made formatting, `rustfmt` rarely outputs the same file out twice. This completely defeats the purpose of `rustfmt` but also makes it is easier for us to tweak the formatting by manually changing the input file a bit, than by opening a bug report. If this was fixed you would probably start seeing many more bug reports popping out because people wouldn't be able to manually fiddle with the input formatting to solve them. 
For instance: foo.should.be.a(String); I'm not sure if it's good idea though.
I would stick to the `expect` syntax. Else you just created a flamewar between those who favor one or the other :P
This is very interesting and one of the things I always wanted to learn, to see how LLVM and other related things work. Do you have any book suggestion to read more? 
(1) This: renderer.current_active_map.map(|map_id| { ... }) Would evaluate to `Option::None` if the value is not present, but I suspect you want it to be more in the flow of a larger function, in which case what you are doing is the best I know. But please break it up into several lines ;). (2) No, what you are doing is what describes how this is done. The lack of delegating abstractions (e.g. have this newtype or variant automatically implement a trait) is a painpoint for rust right now seeing as composition is so prevalent. This could possibly be patched with a procedural macro, but I'm not sure how to make that ergonomic. This is generally worked around with macros in my experience. (3) Have you seen [Linking Errors](https://docs.rs/error-chain/0.11.0/error_chain/#linking-errors)? If you linked the errors of any crate or subsystem you depend on it would permit you to write: fn something() -&gt; Result&lt;(), AppError&gt; { do_something_for_another_layer().map_err(Into::into) }
You could be missing the feature flags necessary to trigger [metadeps](https://github.com/joshtriplett/metadeps) to locate linker flags with pkg-config. The feature flags are listed here: https://github.com/Daggerbot/x11-rs/blob/master/x11/Cargo.toml#L38 - probably `xlib`. This would be done by adding something like this to your Cargo.toml for the `x11` dependency: x11 = {version = "2.16.0", features = ["xlib"]}
I actually found that the Rust compiler is surprisingly approachable on this regard. The compile cycle is orchestrated by the driver crate, and the librustc README has a high-level overview: https://github.com/rust-lang/rust/blob/master/src/librustc/README.md This plus the official LLVM language reference guide got me along pretty well so far: https://llvm.org/docs/LangRef.html
I want to expand `thanks.rust-lang.org` to more things than just rustc, but haven't had the time yet.
I confess I’ve never seen the purpose of interfaces like this; I prefer the standard `assert!()` *et al.* with perhaps an extra crate or two of particular helpers (e.g. float-cmp for float comparison). But I looked through what this did, and one thing stood out: `.to.not.be.a(…)`. I was curious how that could be done, because I’d expect it to be a compile-time test, which would thus coerce the type (thereby ruining tests of what it will infer by default, for example), and block negative tests (because you need to check “does this fail to compile”. [The approach employed is `std::any::TypeId`](https://github.com/afshinm/morq/blob/286654ffe5c7ca6f301c5aa50e15a9006e57fd2a/src/asserts/type_match.rs#L14-L34) which had never even occurred to me for that purpose, neatly sidestepping the issues I had in mind, though constraining you to `'static` types.
Have you seen https://github.com/cfrancia/spectral ? It looks very similar and does not use macros.
Not Rust-related, but I used to work on a compiler for a Python-like static language. The compiler itself is written in Python and compiles to LLVM IR; there's not a lot of documentation, but it might be a good way to study how this could work (and a bit smaller than rustc). https://github.com/djc/runa/
Last week, I split a new crate [imap-proto](https://github.com/djc/imap-proto) out of my larger [tokio-imap](https://github.com/djc/tokio-imap) crate at the suggestion of rust-imap contributors. This week, we're continuing to iterate on both imap-proto and tokio-imap.
&gt; Wayland provides a smoother experience, the motto is "every frame is perfect" "smooth" is a BAD description: in fact "every frame is perfect" can lead to jerkiness when window resizing for example, so you should replace "smoother" by "tear free". &gt; less use of CPU and memory and consequently it saves battery Depends totally on the implementation, I think that these savings are most likely to occur an embedded system than on a normal PC. &gt; Wayland let desktop environments implement a lot of things impossible on X11 And vice versa: X11 provide remote window display by default, Wayland provide nothing for remote display so you have to add it on top it, which means compatibility issue, etc. 
During the Rust+GNOME Hackfest in Berlin, I improve the `gir` code generator that is used by `gtk-rs` to support the generation of asynchronous methods. The code is not finished yet, but [the PR](https://github.com/gtk-rs/gir/pull/490) is already working.
That's only when you have naked expression fragments (since expressions can contain spaces), you could make your grammar be something similar to `expect ( $expression:expr ) to be ( $type:ty )`, which would be accepted by the compiler. If you had `expect $expression:expr to be $type:ty` then if you pass `expect return to be u32` then the parser doesn't know whether to parse it as `expect (return) to be u32` or `expect (return to) be u32`, since both are valid.
Ugh I'm sorry but these styles of tests really irk me. Why so English-centric, and why is that a good interface for a developer? Developers understand code! Normal asserts with boolean conditions are easy to read and write - they're already in the language you're currently writing in.
You can just remove the `.` from your macro rules; they’re never immediately following an expression, because of the parentheses you habitually use. With that minor tweak and removing parentheses on things like `ok ( )`, you can end up with things like this: expect (x) to not be empty; expect (y) to be a (Vec&lt;i32&gt;); expect (z) to be ok; expect ("hello") to equal ("hello") and not be an (i32); (I’d contemplate replacing the parentheses around *types* with angle brackets, especially if the dots remain.)
One thing that interfaces like this get you is improved error messages. Contrasting a failing test `assert_eq!(x.is_empty())` with `morq!(expect(x).to.be.empty(););`, the former can at best say “expected x.is_empty() to be true” while the latter can say “expected x to be empty”, because it has more knowledge of the semantics being expressed. It seems like a small thing, but it can be a bigger deal on more complex or extensive tests. (I say this as someone who prefers the native way, not really seeing the practical value of the improved error messages, because no one should ever experience them, and if they have then they probably don’t help *much* anyway.)
Right now it does not and it is pretty counterintuitive.
On my machine, --release is 13-14 faster than debug. Really make use if it
Can't you just use `requires`?: template &lt;typename T&gt; void do_then_print(T t) requires t.print() { t.print(); }
yeah that really helped!
Once you get to chaining assertions (on the roadmap), using `IntoIterator` instead of `Iterator` could be problematic because it would consume the object.
While I appreciate people working to make testing better, I think I prefer the brevity of [spectral](https://crates.io/crates/spectral) or [galvanic](https://crates.io/crates/galvanic-assert).
You need to map it manually to a method: fn assoc_const() -&gt; V { Self::AssocConst } Note that associated constants are `const`, but when accessing them through trait objects that cannot be true.
It's a little hard to tell from your question: Do you have a C constructor for this type? What sort of pointer can your C code give you? You can cast a `*mut` to an `&amp;mut` with unsafe code, so I suspect the solution here will be just taking an extra reference on top of what C has given you and then casting it.
Thanks Chris. As I mentioned earlier what I see is just a grammar change and not the internal parts and asserts. I can see why you prefer the simple `assert!()` and that's why I have added this to the roadmap. I will write a simple macro for that purpose. And yes, that `a(...)` / `an(...)` is very cool. 
Oh I see. Sorry I'm not a expert in macros. 
You just use `&amp;x` instead, more syntactically lightweight and readable than `x.iter()` IMO
You mean `c(&amp;self)`, right? But that doesn't compile anyway. Try the playground link. "the trait `Desc` cannot be made into an object ... note: the trait cannot contain associated consts like `NAME`". If I'm understanding that error correctly, it's not just that you can't access the associated consts from a trait object; you can't even make a trait object at all.
You could make another wrapper trait that doesn't have any associated consts, though: https://play.rust-lang.org/?gist=4271322e61668333fedec0479d5d3152&amp;version=stable
The compiler prevents you from make a trait object out of a trait with associated constants and yes, something along those lines would be already nice. Intuitively one would expect the associated const behave like the associated funcs, so they would get proxified the same way it is done for the methods. That sort of clashes with the fact `const` is what it is elsewhere... Maybe it is time to ask for associated `static` behaving like this? 
What are the thoughts on RLS integration? I used to use ST3 exclusively, but RLS support drew me to other editors. I see an issue on the repo from 10 months ago, but no comments. Just curious :)
I use a tool which outputs markdown into a [CONTRIBUTORS.MD](https://github.com/kbknapp/clap-rs/blob/master/CONTRIBUTORS.md). Link is at the bottom of the file. I also have a [`just`](https://github.com/kbknapp/clap-rs/blob/master/justfile) recipe which invokes the tool.
Have you tried the LSP package? If so, what is your experience with it?
The amount of boilerplate involved makes you just give up on using associated consts and just create the methods returning the values as static directly. =/
The compiler is complaining here because once the `new()` function returns, the `HashMap` would be destroyed and any references would be invalid. Making the `Context` own the `HashMap` removes that issue by giving the `HashMap` a place to be stored after the `new()` function returns.
Another week working on [`uom`](https://github.com/iliekturtles/uom) (type-safe zero-cost dimensional analysis). This week I got tests compiling for all primitive storage types. Not sure what to do about other types like `BigInt` and `Rational` as I'm using `quickcheck` and the `Arbitrary` trait isn't defined for these types.
What about the question at 12:30min regarding IS-A relationships? 
If you're only using them for the trait object, there's definitely no advantage to the indirection.
Nice list, shame I haven't heard of it earlier. Somehow I knew bors would be the best
I have. Couldn't get it to work for autocompletion. It starts and I can see LSP messages in debug log but doesn't seem to complete anything. (I'm talking about https://github.com/tomv564/LSP, there's another one I haven't tried)
I just finished my character-oriented ngram crate, [ngrammatic](https://crates.io/crates/ngrammatic), modelled somewhat after the python [ngram module](https://pythonhosted.org/ngram/). I also hope to get my command line toml mutator utility working, at which point I'll publish on github. The idea is for a command line utility to allow specifying an element in a toml file using JsonPath or XPath, and query or set the value of that element. That would allow bash/powershell scripting to work with toml files.
Your chicken is insecure, you should probably do something about that
`Arc::new(&amp;chicken)`?
You could always have the guts in Rust and the UI in another language. But as others have said, there are rust bindings for GTK, we just don't have a 100% rust library at this level yet.
Seriously pretty "mood" 
You need to really think about ownership here. Who *owns* the store? Is it the caller? In this case, yes you do need a reference, but you also need an object that lives long enough. If it's the `Context` structure, then either use a plain object (no reference), or a `Box`.
I think that’s a great idea, since people seem to post that type of post quite frequently!
Looks really great bro
Very cool!
Another thing, I think is that the NLL RFC was accepted a while ago, while the const generics RFC (that was accepted) is pretty recent. Similarly, people have been arguing about SIMD for years but this approach is fairly recent. Also stdsimd doesn't really need core team support, almost anyone can help port intrinsics. There's PRs into it all the time.
Another good resource to at least read some rust is the [cookbook](https://rust-lang-nursery.github.io/rust-cookbook/). I have seen nice patterns and standard library types I might have otherwise missed.
But it looks like it's only causing a detour for calling createmove_hook(), not for calling the actual function in the DLL at address 0x10111790. When the game calls the DLL function, it won't call closure_for_createmove, right? Wouldn't it have to be CreateMoveDetour.initialize(fn_ptrs.addy, closure_for_createmove) to cause a detour for calling the DLL function?
Oh no, my arch enemy is back! :O cc /u/antoyo
I guess trait objects are somewhat second class citizens in Rust, which is a bit sad. I believe it should be pretty straight forward to resolve (just another entry in the vtable), but I guess there is just no one pushing hard enough to make it happen. Same goes for functions that don't take `&amp;self` or `&amp;mut self`.
Does RLS provided anything except refactoring? Also, `racer` works better in Sublime rather than in VS Code for me.
You might be interested in vulkano, gfx, and amethyst crates
&gt; The amount of boilerplate involved makes you just give up on using associated consts and just create the methods returning the values as static directly Which is basically why everybody agrees that associated consts are close to useless.
&gt; You mean c(&amp;self), right? No, I don't. I mean `fn c() -&gt; Type`. I don't recall whether you can provide a default impl for it or not, but the method does not need to take the object as argument because it doesn't access the object, only an associated const of it. 
Added CPU temperature to [systemstat](https://github.com/myfreeweb/systemstat) (please contribute cross-platform support for all the things here!), MPRIS music display/control to [unixbar](https://github.com/myfreeweb/unixbar). And some progress with running Servo on FreeBSD — namely, [it sandboxes](https://github.com/servo/servo/issues/11625#issuecomment-342243384)!
Yeah, you're right. It seems really obvious to me now lol
Hi Kbknapp, thats a great question. There have certainly been some conversations about it here: https://github.com/rust-lang/rust-enhanced/issues/134 and here https://github.com/rust-lang/rust-enhanced/issues/210 There are still discussions as to how this would be implemented in Sublime, please get involved in these conversations, I would love to hear your thoughts on it 
Good call, I've gone through a few of those. It is always a bit different reading a finished product compared to taking a rough idea and figuring out how to make it better.
I keep the a separate contributors file because I do a lot of offline work and don't like to assume constant internet connections. Of course if someone has a connection they have access to more metrics and nicer lists ;)
First, good job doing it in Rust. What I will complain about is a critic of the concept that you implemented, not your work. Please, please don't use such DSL in Rust, or anywhere in any programming language. I used and still sometimes use libraries/frameworks like that and they are terrible. In Scala they look like: something must be equal 5 which looks a bit better than with dots, but still sucks. The whole thing is a complete nonsense. First: We are doing programming, not poem writing. Natural language is not better or more readable for what are we doing, otherwise we would be wrtting all our code like that. Second, these DSL grammars are always extremently clumsy. Why is it `lenghtOf`, and not `lenght.of` or `lenght of`? Why is it `.empty()` and not just `.empty` etc. Discoverability of such things is poor, debugging is poor. And all of these in the name of absurd notion of these being somehow "friendlier" or more clean. For whom? Generally, one of many many things I dislike in Scala (which I work a lot with at my dayjob) is a general entusiasm about DSLs. DSL have a terrible cost: instead of having to master one language, they make you forever deal with tens of languages developed by people who though they are being smart. It's a nightmare. If you're thinking about developing your own DSL every, think twice. It really needs to be important, or improve things tenfolds, to justify introducing everyone to yet another mini-language. OK. Enough of my rambling. I'm getting back to work. 
I see in the gif it still has that horrible style of jamming errors in the code. I've never noped out of an editor faster. It makes my skin crawl.
Would like to see a Rust solution which can automatically do all of that.
It's a bit difficult because reddit only lets you pin 2 threads at a time. I do see a lot of these kind of questions in the beginner thread.
Something I've been contemplating is the need for reusable assertions. One I see coming up in a couple of places: - `assert_cli` has stdout/stderr assertions. - I plan to also do filesystem assertions, so I'll be wanting file content assertions In both cases, I need string predicates that are useful for assertions. It'd be great if the same API is used in both cases so people don't have to relearn how to do substring assertions depending on the subject of their assertion.
More accurate completions and gotodef than racer (when it works).
There's no c constrictor. All the c library needs is for me to pass a pointer to a null pointer which will then get overwritten by a pointer to a new instance of the object allocated by the library. Come to think of it maybe this would work: &amp;mut std::ptr::null() as *mut *mut t
Though you'll need to create a local variable storing the inner pointer first, right, so that you can use it afterwards? Also you might want to use `null_mut`. (You might not need any explicit casts then, since references will coerce into pointers.)
Write one :)
I had to reinstall rust (and nightly) and also rustfmt. For some reason it keeps creating backups though the default is overwrite. Is there a workaround?
It needs to take self for the trait to be object-safe. Try removing the `&amp;self` from one of the methods in my playground link and you'll see what I mean.
Perhaps that can be worked around by making one "master" pinned thread which links to the other weekly threads, with a title concisely summarising all of them? I've seen other subreddits do something like this. One pinned thread with "beginner resources" or "read this". We could have a single pinned "weekly discussions" thread, containing links to the various weekly threads (such as the beginner questions thread, the "what is everyone working on" thread, the idea proposed here, etc). That would also keep the second pinned thread slot available for important announcements and stuff.
Let's do this. The opportunity for learning would be great.
I've started learning Rust with a small project, a [CLI tool](https://github.com/videah/compromise) that prevents Twitter from compressing images. It's nothing special, small but useful enough to learn the basics in a meaningful way.
&gt; I believe it should be pretty straight forward to resolve (just another entry in the vtable), but I guess there is just no one pushing hard enough to make it happen. This isn't true. consts have to be available at compile time, you can't use a vtable constructed at runtime to perform compile time computation.
Well, it doesn't work at all for me, so I can't compare. But I though that RLS uses racer underneath.
It's optional. But yeah, it's horrible.
There are two things that could be done: 1. include them in the bound, like associated types: `Box&lt;Foo&lt;A = 100&gt;&gt;`. 2. Allow you to add `where Self: Sized` to the associated const, like you can on functions.
it needs to take the object as an argument if it wants to be dynamically dispatched. How else would we be able to figure out which vtable?
I don't agree.
I certainly have a lot of fun making code idiomatic Rust. It also might help people discover some new crates.
Yup, I just realized what tokio can and cannot too late.
I'll hopefully fix the last bug in the stemmer for [elasticlunr-rs](https://github.com/mattico/elasticlunr-rs), to do my part in adding search to mdBook.
At some point I imagine associated consts will be mostly superceded by `const fn` in traits. When used at compile-time, they give you the advantages of associated consts, but they can still be used through a vtable at runtime.
It also provides type information on hover and docs on items
&gt; The way nodejs handles threading is exactly the way it should be done. I'm 100% not having any threads is not the right way to handle threads. &gt; What keeps you from horizontally scaling your application? AWS bill. /thread
I think associated consts are super useful for describing properties of things that will only be resolved at compile-time. However, I agree with you that they don't transition well to runtime-resolved structures like trait objects. Eventually, I hope they will mostly be replaced by `const fn` trait methods.
Indeed. 
Can you show me something that can be built with associated consts that cannot be done without them? Every single person I've talked with say they tried to use them to create an array of a particular size, which isn't allowed, and never looked back at them.
Multithreading is one of the reasons why Rust even exists - because it is hard to get right. I mean if you have multiple cores on a single server then by all means - use them. But use them by spawning multiple processes, not multiple threads. To be fair: I should‘ve mentioned that single threaded things are only great for IO-heavy processes. Computationally intensive tasks should benefit from multithreading and thus not be run with node (obviosly). 
I'm continuing to work on rust support in lldb ([my WIP branch](https://github.com/tromey/lldb/commits/rust)). Currently it can parse the DWARF for most types. This week I plan to handle non-C-like enums; and then probably start on the expression parser.
&gt; I mean if you have multiple cores on a single server then by all means - use them. But use them by spawning multiple processes, not multiple threads. And what will that get me? Nginx does it to allow zero-downtime configuration changes and updates. Well, it will let me handle crashes as well, but then I shouldn't have any with rust, right? From linux/bsd kernel point of view, a thread is almost equal to a process. In terms of scheduling, they're equal. I think I will lose more than I gain by adapting that model. &gt; To be fair: I should‘ve mentioned that single threaded things are only great for IO-heavy processes. Computationally intensive tasks should benefit from multithreading and thus not be run with node (obviosly). So threading in nodejs is perfect as long as it' just a front-end to backed written in proper language that has threads? 
You completely ignored my first point. This discussion leads to nothing. 
It is. You telling me to use rust in a way I would use C because multi-threading is hard to get right, which would eliminate 1/3 of rust.
Nginx uses worker processes, too, by the way. 
Right now the biggest application of compile-time constants, constant generic parameters, aren't part of the language. Associated consts will be a lot more useful once those are in.
Did I say otherwise? &gt;"Nginx does it to allow zero-downtime configuration changes and updates."
I don't have a concrete suggestion but you might check out the interval set structures in [this Scala library](https://github.com/rklaehn/intervalset) for inspiration.
/u/desiringmachines said that he thinks they are usefull, but yet you are saying that they are useless :/ What is it? If they are useful, what can they do? They can't be used in constant expressions, they can't be used in virtual dispatch, ... I am really interested in knowing what they can do today that a simple function returning a constant cannot. 
Have an issue I plan to close on [tarpaulin](https://github.com/xd009642/tarpaulin), then I'll continue my work on working out condition coverage details. Last week I did a check on what VectorCast does in terms of condition coverage because the issue of side-effects in conditions is a difficult issue. Unfortunately, VectorCast gets coverage by injecting it's own code at compile time and seems to rely on stubbing and only working on unit tests to sidestep the side effect issue. Whereas I plan on doing it on a crates entire test suite without altering the behaviour of the test to enable integration and unit tests to be included... So yeah condition coverage is hard and being the first open source tool to include it means I can't learn from others work as easily 
Unless the optimizer shits the bed, there's no difference in code generation between a function returning a constant and a constant. So that leaves frontend restrictions as the difference. As far as I know binding expressions (match arms, `if let`) and array lengths are the only contexts that require a compile-time constant value.
 &gt; consts have to be available at compile time, you can't use a vtable constructed at runtime to perform compile time computation. Hmm, this is not the way I use associated consts, maybe there's some other programming pattern you're referring to? I use it like this: pub trait Foo { const BAR: &amp;'static str; fn print_bar(&amp;self) { println!("{}", BAR); } } ...in this case it would work to have `BAR` in the vtable. &gt; What is the implementation of Trait::static_method? Without a self argument you don't know what to dynamically dispatch to. A trait object is a fat pointer, data and vtable. For `Trait::static_method` you skip the data pointer and keep the vtable pointer. I believe this is how it works in Delphi (not sure how/if it works in other languages). 
Dear Dr. renozyx &gt; "smooth" is a BAD description: in fact "every frame is perfect" can lead to jerkiness when window resizing for example, so you should replace "smoother" by "tear free". Smooth is OK, "tear free" is just a more tech term. &gt; Depends totally on the implementation, I think that these savings are most likely to occur an embedded system than on a normal PC. No, it can save energy on laptops too and the fact it depends on implementations is what makes Wayland so better than X in saving energy. Wayland implementations can be written in modern languages and technologies. &gt; And vice versa: X11 provide remote window display by default, Wayland provide nothing for remote display so you have to add it on top it, which means compatibility issue, etc. LOL, the fact X11 had network transparency doesn't mean Wayland can't use network, it just mean that Wayland cares of something more specific and who implement Wayland is free to decide how to use it over a network. It's a feature and improve security. It's how modern software is designed: do one thing and do it well.
(1) Yeah, this pattern is not uncommon. I wrote an [inner](http://docs.rs/inner) crate that makes it look less ugly a while ago. Hopefully we'll get support for using `?` with `Option`. Until then, you can also have your function return a `Result&lt;X, ()&gt;` instead of `Option&lt;X&gt;` if that makes things more ergonomic. (2) This is what you have to do if you're using enums. Consider generics or trait objects to see if that's a more suitable approach (hard to tell without knowing more about the program). For (3) I would typically rather do: use types::errors::{layer, database}; ...and then say `layer::Error` instead of `LayerError`. 
One disadvantage is that it would hide the posts more, so people are less likely to see them and post in them.
In many ways it is a "papercut", but I think the idea for this year's roadmap was that Rust in some ways is dying from a thousand papercuts---small problems that lead new users to get frustrated and give up. Thus the focus on ergonomics, to try to fix these papercuts and make it more pleasant for people to stick around and actually use Rust.
This is the subreddit for the Rust programming language. You're looking for /r/playrust.
neat
We could just do a regular post every week and everybody upvote it to the top???
Hmm, `BTreeMap` feels super clunky because `insert` returns `None` if everything went ok. Is this how I am supposed to do it: let old_card = self.Folds.get([start, end]) if (old_card.is_some) { return Err(Box::new(Error::new(ErrorKind::Other, "Fold already in foldlist!")))) } else { let _ = self.Folds.insert([start, end], card); let _ = self.FoldsInv.insert([end, start], card) } I played a bit on the methods of `Option`, but that just got nastier and nastier for the same reason (ignore the error type please :)).
Yes, I agree. However, that is the only major disadvantage that I can think of, while there are plenty of advantages. I am sure people would still find it fairly easily, if the sidebar specifically points them to it. Also, it is still pinned and all. I am also sure most people would expand and at least glance at a pinned thread titled something like "weekly discussions about common topics (including beginner questions)", even out of curiosity if nothing else.
&gt; A trait object is a fat pointer, data and vtable. For Trait::static_method you skip the data pointer and keep the vtable pointer. The static method doesn't take a trait object as an argument, it has no way to access the vtable pointer. `Trait::CONST` has the same problem.
So is Firefox Quantum a separate release channel? Will these improvements be in the next regular Firefox release? 
I'm migrating nickel.rs to hyper 0.11.x. I have a very rough PR at https://github.com/nickel-org/nickel.rs/pull/410. Some discussion of the process is also at https://github.com/nickel-org/nickel.rs/issues/402.
From what I understand (I'm not a mozilla dev, just reading the posts), it is part of the regular Firefox release. The one that comes out tomorrow, actually. :) I believe a lot of this has been part of Firefox Nightly for a while now, so it has been tested a fair amount.
I'm sharing this with the Rust community because it's a perfect example of the pitfalls of garbage collection.
What GUI library would you recommend for this project? I decided to write a kernel in Rust, already started but I feel that I should take a break and get to know Rust better before diving deeper. Thanks!
I also recall that they are smaller (memory-footprint-wise). I remember writing an experimental library that could conditionally be compiled to use either STL or Qt for its strings and vecs (and it had a fair amount of them, since it was a log message processing library), using lots of `#ifdef`s to choose one or the other. The (stripped) binaries from the Qt build were much smaller than the ones from the STL build.
Based on what I know about the Reddit ranking algorithm, it would be gone after 3 days no matter how much it's upvoted.
i think its just for marketing firefox as "fast agian and going to get faster"
I personally think that `const MY_STR: &amp;str;` is much clearer than `fn my_str() -&gt; &amp;'static str;`, and the `const` version eliminates the possibility of the user trying to do something side-effect-y internally. This can be important for areas where side effects can be dangerous, such as a potential panic during an unsafe section which is not panic-safe.
 if let Some(old_card) = self.Folds.get(&amp;[start, end]) { return Err(Box::new( Error::new(ErrorKind::Other, "Fold already in foldlist!"), )); } Is a bit nicer.
Ah, that's lexical lifetimes at work for you. NLL will help here certainly. In the mean time I recommend this workaround: https://play.rust-lang.org/?gist=54d3389aed9ee878370cff403f864324&amp;version=stable It's probably better to have your commands own the underlying command data anyways, makes them much easier to pass around.
If what you are developing is primarily a GUI application (i.e not a GUI frontend for something that also has a CLI or other interface), I would suggest to have the entry point be in the same language you are writing the GUI in. So since you are going to be using Qt, that would be C++. Having the main function in C++ means that you can easily initialise Qt and set up your GUI, in a well supported and reliable way, just as if you were writing a C++ GUI app (which, effectively, you still are, you are just linking it against a Rust library). You can write your logic in Rust, compile it as a library, and expose a C API. You can then easily call that from C++ and link it into your C++ GUI app. I think this would be the most elegant solution. Feel free to disagree (and if you do, tell me why, I want to see what your arguments/opinions are and learn about new ideas :) ).
No it's just a codename for Firefox 57.
The point is that you _could_ write this: pub trait Foo { const BAR: &amp;'static str; fn print_bar() { println!("{}", Self::BAR); } } Note that `print_bar` does not take `&amp;self`, but it can still resolve `Bar`.
Unfortunately this is #[no_std] and the data is a 512 byte flash page, so that probably won't work here. But thanks, at least I understand why now!
CloudFlare is writing their own crypto? That's got to be nerve-wracking. It was mentioned briefly in one of the links in that article. I definitely found this article to be interesting. I feel like the optimal solution would be to keep a long lived buffer for each goroutine, and use that as a scratch space, rather than continuously allocating a same-sized chunk of memory over and over. Unfortunately, I don't think Go slices have a `clear()` function like Rust vectors have, which means you could create a heartbleed-style leak by accident.
I'm a bit concerned about how tenuously on-topic this is, but I'll refrain from removing it as no flame war erupts. :P
It wouldn't be *immune*, but it would make it easy to deterministically solve this issue without having to run the program over and over to optimize the GC collection cycle knob, and you could probably eliminate all recurring allocations entirely in relation to the crypto stuff.
I can fix your problem if I inline your receive function: https://play.rust-lang.org/?gist=4f475377ad79e08197d2ba2f9b0666dd&amp;version=stable
Oooh. Didn't think of that! I'll try it on the real codebase (tockloader-proto).
That's odd, I really like it.
Rust is a language that very heavily relies on function inlining by the compiler to achieve good performance. The LLVM compiler backend is really good at that (and many other optimisations that greatly speed up your program), but it is done only if you compile with `--release`. In Rust, the difference between debug and release builds is *massive* -- even more extreme than in C++, where it is already quite big. Debug binaries are incredibly slow, due to how verbose Rust as a language (and its stdlib) is, relying on the compiler to simplify and optimise everything away. This only happens in release mode, but the optimisations make your program difficult to debug, which is why debug mode is the default. :) In some cases, rustc has been known to flood LLVM with actual *gigabytes* of intermediate code, especially for crates that use lots of macros and generics. This is also one of the big reasons why Rust compile times are so slow, but it is being worked on. It doesn't have to be that way, and there are WIP improvements to rustc that will reduce the amount of spam it sends LLVM's way.
Rust can still have contention on multithreaded allocation/deallocation. Any allocator can overloaded by lots of small requests. It isn't a binary question whether GC is harmful or not. 
Well, it's a set of new technologies they're using to change the core architecture of the browser. They didn't just push a normal update and slap a new name on it, this project has been going on for at least the better part of the year.
"Quantum" is a broad term for a collection of related initiatives that the Firefox team is currently working on: https://wiki.mozilla.org/Quantum The first of those initiatives will be making it into Firefox 57, which releases tomorrow. If you're using Firefox Beta or Firefox Nightly, you've been using Quantum components for months now.
Cookbook maintainer here! Most of the examples in the cookbook were contributed by rust newcomers who learned on the job with help of some mentoring and code review with great success. Anyone is welcome! TL;DR contributing to cookbook is a great way to help community and learn some rust along the way!
&gt; I would suggest to have the entry point be in the same language you are writing the GUI in That seems fair. After some browsing, I think the right choice for me is React Native since it abstracts over the particulars of each platform (and I use ReactJS elsewhere) without being something silly like embedding an entire browser to do a UI. I think I want to support web at some point in the future, but I'm not 100% sure. &gt; it would be nice if you could write/blog about it That's definitely something I'm interested in doing. However, I feel like I need a certain amount of familiarity with whatever path I choose before I feel qualified to write a blog post about it (I hate unhelpful blogs). What I'm building will most likely be open source, but I want the option to switch it to proprietary if I want, so whatever I choose will not be license encumbered (e.g. statically linking Qt).
malloc implementations still have flags and knobs you have to fiddle with. Some in a memory allocator I use: * caches to avoid contention: per-thread vs per-cpu (a flag) and size (a knob) * returning unneeded memory to the OS via `madvise(MADV_DONTNEED)`: after every so many bytes freed or at a constant rate in a background thread (flag), the actual number (a knob) * size class thresholds * be huge page-aware or not, that is if it should avoid punching `MADV_DONTNEED` holes in blocks of 4 MiB (a flag) and probably many others I'm not thinking of.
Will we need to enable any of the features like we do in nightly? Quantum CSS/Quantum Render? Will these be turned on in release?
You don't need to tweak knobs if you're not making allocations, for starters, and for seconds, tweaking your memory allocator is far more niche than tweaking your garbage collector. It's not out of the question, of course, but I've never personally been involved in a project that was tweaking the behavior of malloc beyond choosing to use gemalloc or not.
You can either store the return value which is the hook interface, or in the case of `static_detours`, use the static object to retrieve the current instance and call it, in the referenced example: ``` CreateMoveDetour.get().unwrap().call(&lt;parameters&gt;); ```
Just Git as handcrafted lists often get out of date. GitHub can then show contributors like this: https://github.com/jhasse/ears/graphs/contributors
CSS is default tomorrow and webrender is still behind flags in nightly.
When did you try it? I send some fixes to LSP a few months ago that made the autocompletion work with RLS, but now that I tried it, it had some hiccups again, probably for an unrelated reason. (https://github.com/tomv564/LSP/issues/194)
┌( ಠ‿ಠ)┘
&gt; It's certainly possible to avoid allocations in Go, unlike some garbage collected languages, but it's not as easy to avoid as it is with Rust. I definitely agree it's possible to avoid allocations in Go. I think it's possible to do object pooling in any general-purpose language, and I know Go in particular has [sync.Pool](https://golang.org/pkg/sync/#Pool) in the standard library. I also know Go has the ability to compose a one-allocation structs from other structs and arrays, unlike say Java. So I think Go's not too bad in terms of ability to avoid memory allocations. I'm curious why you say it's easier in Rust than in Go. Is this a difference in the language, in the commonly-used libraries, or both? I know a decent amount of Rust and a little bit of Go (probably not as much Go as you). In any case, I think this is consistent with my general feeling that any differences are of degree, not of kind. Even GC vs not is not as dramatic as it may seem. It may be more niche to tweak malloc knobs than to tweak GC knobs, but it's still pretty common where I work, and we derive substantial benefit from doing so.
&gt; CloudFlare is writing their own crypto? Cloudflare has in the past employed some really good cryptographers, and they do plenty of quality crypto work. 
They're one of the few companies that I believe are justified in writing their own. They have the resources to attract and then hire skilled cryptographers, and the company lives and dies on security and performance. Nerve wracking is still accurate, but I wasn't dismissive of it. When most individuals or companies write their own crypto, I have some very serious reservations.
The actual site: https://turbo.fish/
Yeah. I think something like JUnit Hamcrest matchers are actually fairly ideal, because you get the benefits of smart error messages generated by the matcher objects, but the matcher objects are composed as normal value types, rather then chained in a english centric overly verbose way. assert_that(x, any_of(is_empty(), has_length(1)));
&gt; For `Trait::static_method` you skip the data pointer and keep the vtable pointer. Let's say you were to write an RFC making static methods object-safe by putting them in the vtable, taking only the vtable pointer. I'm not an expert but that seems possible to me. Why would you say this is worth adding to the language? The obvious alternative is to add `&amp;self` to the methods in question. One word is passed in unnecessarily. It's hard for me to imagine where that's a significant performance cost. There are probably other things that would help performance more. For example, in this case there are both `name()` and `features()`; if you need them both, calling them individually means two vtable dispatches. You could avoid that by having one method that returns a struct with both.
Correct me if I'm wrong, but wasn't Mozilla's sponsorship of Rust specifically for Quantum? If so, it's been in the works for at least like 5 years
I'm not sure about the solution to whether to make this a weekly thread, but here's a problem in the "how do I make this rustic?" vein. This is a lot of code so hang in there. Imagine there's a game where players control units that can be given different orders telling them how to move each turn. I'd like to structure my program like this: struct Orders { delta_x: i32, delta_y: i32 } impl Orders { fn carry_out(&amp;self, unit: &amp;mut Unit) { unit.x += self.delta_x; unit.y += self.delta_y; } } struct Unit { x: i32, y: i32, orders: Orders } fn main() { let orders = Orders{ delta_x: -1, delta_y: 1 }; let mut unit = Unit{ x: 100, y: 100, orders: orders }; unit.orders.carry_out(&amp;mut unit); } But of course this doesn't work because unit gets borrowed once to access `orders` and gets borrowed again to have its location affected by the orders. And so I've come up with a different approach, which is basically to store all the units in a HashMap and then pass around the keys to the HashMap rather than the actual units. I throw in a `UnitManager` to handle generating those keys: use std::collections::HashMap; struct Orders { delta_x: i32, delta_y: i32, target_unit_id: u64 } impl Orders { fn carry_out(&amp;self, units: &amp;mut UnitManager) { if let Some(unit) = units.get_mut(self.target_unit_id) { unit.x += self.delta_x; unit.y += self.delta_y; } } } struct Unit { id: u64, x: i32, y: i32 } struct UnitManager { next_unit_id: u64, units: HashMap&lt;u64,Unit&gt; } impl UnitManager { fn new() -&gt; Self { Self{ next_unit_id: 0, units: HashMap::new() } } fn new_unit(&amp;mut self, x: i32, y: i32) -&gt; &amp;Unit { let unit_id = self.next_unit_id; self.next_unit_id += 1; let unit = Unit{ id: unit_id, x: x, y: y }; self.units.insert(unit_id, unit); self.units.get(&amp;unit_id).unwrap() } fn get_mut(&amp;mut self, unit_id: u64) -&gt; Option&lt;&amp;mut Unit&gt; { self.units.get_mut(&amp;unit_id) } } fn main() { let mut units = UnitManager::new(); let unit_id = units.new_unit(100, 100).id; let orders = Orders{ delta_x: -1, delta_y: -1, target_unit_id: unit_id }; orders.carry_out(&amp;mut units); } This works, but it feels weird to me. It seems like by making a HashMap and passing around IDs to look things up in it, I'm basically simulating the heap. Instead of pointers, it's u64's. In fact, this is more or less how Emscripten implements the heap---as a vector where IDs are used rather than memory addresses. This makes me wonder whether there's some other facility in the Rust language that I should be using instead. If nothing was every going to get mutated then I guess I could probably find a way to use `&amp;Unit` instead of `u64` but since the whole point is to mutate the units those shared immutable references would get in the way. Or maybe this really is the right approach, but I hate paying the runtime cost of working with the HashMap if I don't have to. Recommendations?
Essentially the same as C, it's just spelled funny: let mut my_ptr = null_mut(); troublesome_fn(&amp;mut my_ptr as _); 
Biggest issue for me is power usage for Firefox is crazy.
tokio-core is the glue between mio and futures; async/await are syntax sugar to make building futures easier. Tokio will continue to be necessary.
Something in that site smells *fishy*
You are misunderstanding the problem. A static method is called &lt;Type&gt;::method_name() This is a significant distinction from normal methods! You do not need a value of the type to call this function. So a "dynamically dispatched" static method would be called Trait::method_name() It does not take any arguments, so it does not have access to a vtable! The entire point of a static method - that it needs no arguments of the receiver type - makes it incompatible with dynamic dispatch. This isn't just that the feature wouldn't be useful, as you have tried to argue, but that the feature is fundamentally impossible.
Not a Go programmer, but most GCd languages have [off](https://github.com/xerial/larray) [heap](http://www.mapdb.org/) [collections](http://kotek.net/blog/3G_map). [This](https://github.com/glycerine/offheap) looks like the canonical one for Go. Another technique is store your data in an array of primitives. Then the garbage collector won't have a huge tree to crawl. Hiding from the allocator or the garbage collector for performance is an age old technique.
The speed of movement reminds me of testufo.com. Shows the monitor's ghosting but very smooth @ 120 Hz with strobed backlight
Implement `BitAnd`, `BitOr` and `BitXor` and you can make it nicer still: assert(x, is_empty() | has_length(1)); If you wanted, you could further make `is_empty` a constant and drop its parentheses, but I don’t think that’s a good idea.
The LLVM Kaleidoscope tutorial is a good start: https://llvm.org/docs/tutorial/index.html
&gt; I'm curious why you say it's easier in Rust than in Go. Is this a difference in the language, in the commonly-used libraries, or both? In my experience with Go, you choose between statically guaranteeing correctness, or statically guaranteeing performance. Go implicitly makes a lot more allocations than Rust does, so it can be easier to trip up, and Go does not support stack allocated arrays/slices AFAIK. &gt; One thing that comes to my mind is arena allocation but AFAIK custom allocator stuff is still under development. You can use a [TypedArena](https://crates.io/crates/typed-arena) today in Rust on stable, no problem, without needing a full-blown custom allocator, but mostly I was talking about avoiding heap allocations altogether. If the data you're dealing with has a known, static size, it's a lot more efficient to just keep everything on the stack. You don't deal with allocator contention in highly threaded scenarios, allocation is simply bumping the stack pointer, and deallocation is equally as cheap. Go stuffs a ton of stuff on the stack, which is great compared to Java, but if I'm right about it not supporting stack-allocated arrays,, then that would be unfortunate for this particular application.
 &lt;!doctype html&gt; &lt;meta charset="utf-8"&gt; &lt;meta http-equiv="x-ua-compatible" content="ie=edge"&gt; &lt;meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"&gt; &lt;meta http-equiv="Content-Security-Policy" content="default-src 'self'"&gt; &lt;title&gt;::&lt;​usize&gt;&lt;/title&gt; &lt;link rel="stylesheet" href="normalize.css"&gt; &lt;link rel="stylesheet" href="turbofish.css"&gt; &lt;t&gt;&lt;​usize&gt;&lt;/t&gt; &lt;u&gt;&lt;​usize&gt;&lt;/u&gt; &lt;r&gt;&lt;​usize&gt;&lt;/r&gt; &lt;b&gt;&lt;​usize&gt;&lt;/b&gt; &lt;o&gt;&lt;​usize&gt;&lt;/o&gt; &lt;f&gt;&lt;usize&gt;&lt;/f&gt; &lt;i&gt;&lt;usize​&gt;&lt;/i&gt; &lt;s&gt;&lt;usize​&gt;&lt;/s&gt; &lt;h&gt;&lt;​usize&gt;&lt;/h&gt; 
offheap just means the GC isn't dealing with it. If you're still malloc-ing and free-ing that "offheap" memory continuously, that's super expensive in a tight loop, which is what they were seeing here. Their loop was generating a lot of garbage, which is objects that were allocated in each iteration, and then ready for deallocation at the end of the iteration. I'm suggesting that Rust makes it easier and safer to have each thread do a single long-lived allocation at the beginning, and then exist in a state of non-allocation forever after that, and to do so without worrying about leaking the contents of a previous iteration's memory thanks to the `clear()` function on Vec, which keeps the allocation but resets the size to zero, so you can't access any of memory unless and until you choose to overwrite it with fresh data. Rust also supports stack arrays, which might be even better for this application, since there would be no heap allocations at all, just a stack pointer to bump up and down as needed, which is super cheap.
The HotSpot JVM stores objects on the stack based on the results of escape analysis too. It's a common misconception that it doesn't. The difference is that HotSpot does not rely as heavily on escape analysis, because it has a generational garbage collector with a bump allocator in the nursery.
Huh, I suffered from this misconception and did not realize Java did escape analysis, but it is so obvious in retrospect that I should have. Thanks for the insight!
This is destined to become the [rick roll](https://www.youtube.com/watch?v=dQw4w9WgXcQ) of the rust reddit
[Probably more familiar.](https://turbo.fish/::%3CVec%3C_%3E%3E)
Yeah. I pretty much link to this in release notes. And call out special thanks to people who contributed during a release cycle. 
You are correct, they see Rust as a way to greatly reduce the odds/amount of security bugs in Firefox, which has to run untrusted code as fast as possible. It has taken time for rust to mature as it has for the new technologies built on it. Some rust code has already made its way into Firefox some time ago, but this is the first major architecture change to make it into stable.
I'm sad I can't do an Okfish, as it prominent in futures-await based code: `Ok::&lt;(), ()&gt;(())`
Thanks! That's a cool approach I haven't thought of.
Not specifically so, but it has led to a number of the components, via Servo, that are part of 57 and will make up future improvements in Quantum.
Yeah that's a good point.
The worst thing about it is it moves your code. If it was more like a pop up or something I could hover over I wouldn't mind at all. 
The same index where Swift is going downhill big time or Scratch is only marginally less popular than Ruby or Go ? Yeah, can't trust this.
Static methods are already callable in contexts in which you statically know the specific implementation of the trait and may not have an instance of it. (Such as a function which is monomophized via `T: Trait`.) I believe diwic is saying that they could also be callable in contexts in which you have a trait object and thus a vtable pointer, called via `trait.method_name()`. I think you're wrong in saying this is impossible. I'm asking about desirability; I'm not making a claim either way.
Ooh, that sounds interesting. Were you thinking of just selected crates, anything or crates.io?
I hated it at first, but grew to really like it. It's handy to have the error message literally right next to the erroneous code. 
It *is* a regular release, just with a new name to emphasize that they're finally implementing all of the stuff from Servo that they've been working on for years. It's actually been possible to move to the Beta of this for a while.
Is tarpaulin ready for prime ? Does it miss features compared to kcov ?
My passion for shilling Rust is second only to my antipathy for TIOBE. :P If only the vast armies gormless managers didn't give a damn about it...
When you are editing code it is common for it not to compile since you're not done writing it yet. So it keeps jamming errors and messes with formatting while you are writing if you made the mistake of saving it too fast. I can not fathom how anyone can write code that way.
The [PYPL index](http://pypl.github.io/PYPL.html) is much more accurate in my personal experience. Rust is not bad, around Haskell and Lua but growing faster than them. Sounds about right.
That means async/await is tied to Tokio, which is the opposite of what was said by the core developers.
As cramertj said, they are an effective way of expressing intent.
Tarpaulin is nearly feature complete with kcov it lacks it's own html report format (in progress) and a few niche options. But it let's you upload to codecov.io or coveralls.io and works for the majority of projects. Also, it uses Syntex syntax to remove lines that aren't really coverable i.e #derive statements and to make sure unused templated code is included. So for some projects it actually gives more accurate results. I'd say it depends on your project but it's easy to use and still actively worked on so in my probably board opinion I'd say it's worth using even if you just raise an issue android hold back on using it until I close that issue
Yep, it's like self-references in a struct with a vector by using usize indices. Does feel like overhead incurred to keep a compiler happy. In this case, shared pointers would be less expensive, however.
I rate this site turbo out of fish. (My comment is even linked on it!)
Both Tiobe and PYPL are basically buzz indices of varying quality and should be read as such. They both just measure how often people search for something and try to extrapolate some meaningful data. Unless someone does some serious surveys (and people answer right![1]), we're never going to have a good assessment. [1] The number of clients I have that would answer "Are you using Ruby?" with "No" and later turned out to have substantial Ruby codebases in Ops or small Web tools written on Rails is amazing.
It's a common misconception because it used to be true for a long time. IIRC escape analysic was introduced in Java 7.
Old Firefox or the current version?
I installed it last week, i think. 
Why? Also, why is this comment even upvoted? I don't like the tone of the other comments but they at least try to make a point. This answer doesn't really add anything to the table.
Yeah that was a leftover from my various tries, thanks. Still feels weird not to have a method that does "insert if key not there, otherwise just return None/Error". I feel dirty discarding return values that way...
Thanks, but this looks much too complicated for what I want from it :)
The parent claimed that "everybody agrees" on something. Both my comment and the upvotes act to demonstrate that this claim is not accurate. Sorry you don't like the tone of my comments. I'm trying to convey accurate information about how dynamic dispatch in Rust works.
That does look _barely_ useful. Do you think that stabilizing them in the language was worth it? In the IRC channels we get people asking about how to use them / what are they for _every week_, because they try for a while to use them in array lengths without success. The docs and the error messages doesn't seem to help them. Then there is people like the OP, that after managing to use them, end up needing to access the value via a trait object and have to add a trait method for it anyways. I don't know. Personally, the "clearer intend" associated consts bring has come at a very high cost. I really can't believe people where ok with stabilizing it without at least having rock solid docs and error messages for it.
`rust-lang` and `rust-lang-nursery` crates to start, but we'll see after that...
Awesome ! I'll try to integrate it to tantivy. I opened a ticket, but there was some concern about the support of multithreading. (https://github.com/xd009642/tarpaulin/issues/35) No problem there?
I mean, specifically for Servo and eventually Firefox. "Quantum" is a project that was created much after that and is a packaging of many improvements, one of which is Stylo. So no, it's not been in the works for five years. Servo's been in the works for that long (longer even) but Firefox users don't get benefits from Servo. The stylo project has been in the works for one and a half years, and the overall oxidation project has been in the works since ~Rust 1.0 (2.5 years).
This is so cool. Is there any known timeline to have such significant boost on android?
For years now I've heard about firefox being fast again, but as someone who enjoys having a significant number of tabs and browser windows open I've been disappointed each time.
Hi, I'm trying to parse user input and instead of panicking if it's not a number, I would just like to continue looping until a number is provided. I was wondering if this code is the correct way to handle the situation or if there is a better way! loop { let mut option_choice = String::new(); println!("Select option"); println!("1.Option 1"); println!("2.Option 2"); println!("3.ect"); io::stdin().read_line(&amp;mut option_choice).expect("Couldn't parse input"); match option_choice.trim().parse().unwrap_or(4) { 1 =&gt; do_something(), 2 =&gt; do_something_else(), 3 =&gt; break, 4 =&gt; { println("Wrong input"); continue; }, _ =&gt; continue } } I wanted to do something like: match option_choice.trim().parse().unwrap_or({println!("Not a number"); 5}) But the print message showed even if the input was a number. Thanks for any advice!
&gt; but unfortunately the Go team is against this change. Do you know why? I'm a noob in GCs, but I can't think of any negatives.
His experience with C++ matches mine well, an expert can write very safe, performant and high level (I wouldn't say beautiful though) code in modern C++, but it takes a big effort and you're basically never gonna have a development team consisting of only C++ experts. And to become an C++ expert you have to learn hundreds of idioms and rules (just look at the [C++ Core Guidelines](https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md)). Rust on the other hand has sensible, safe defaults and actually encourages you to write good code, the language is an order of magnitude lower less complex than C++ and the compiler is much more picky. I haven't been part of a larger team using Rust, but I expect it to be much easier to maintain a high quality level of the code over time. It would be interesting to read a report from someone who has been part of a medium size Rust project over a longer time (a year at least). I guess there aren't many so far, maybe Servo is one. This could be a great argument when trying to sell Rust over C++ in a company.
&gt; I'm curious why you say it's easier in Rust than in Go. Is this a difference in the language, in the commonly-used libraries, or both? In Go you have interfaces, and escape analysis which allocate automatically, whereas in Rust you only allocate if you manually box or return a trait object.
Bump allocator is dependent on generational garbage collector, which in turn have it own set of ups and downs. Besides I'm far more interested in ROC[1] proposal - this fits Go goroutine model better. 1. https://docs.google.com/document/d/1gCsFxXamW8RRvOe5hECz98Ftk-tcRRJcDFANj2VwCB0/mobilebasic
It was functional within the restrictions of Sublime when the plugin was originally written. However I now believe Sublime can show small floating windows with HTML formatting. This would be the ideal way to show messages in my opinion. 
Stylo (Quantum CSS) isn't in Firefox 57 on mobile but I think the target is 58...maybe...no idea about Webrender on mobile though. You can download Firefox nightly on mobile and switch on Stylo though.
I'm trying to write a function that converts a Vec of things into a comma separated string, the closest I've got is: use std::string::ToString; use std::iter::IntoIterator; fn as_csv&lt;V: ToString, C: IntoIterator&lt;Item=V&gt;&gt;(c: &amp;C) -&gt; String { c.into_iter().map(|v| v.to_string()).collect::&lt;Vec&lt;String&gt;&gt;().join(",") } fn main() { let x = vec![1,2,3]; as_csv(&amp;x); } This fails to compile with: error[E0507]: cannot move out of borrowed content I'm not sure that using `into_iter()` is even really what I want here, ideally I'd just want to use `iter()`, but I don't think there's a trait for that, so not sure what the best approach for the above is (I don't want to convert the collection into an iterator, I just want to iterate over it without modifying the original).
If you like fluent assertions but this particular style doesn't jibe too well with you, you can also try [the spectral crate](http://docs.rs/spectral) which resembles the Java's Truth library more.
That only affects accurate measuring of hits per line something kcov doesn't provide. Just use the --no-count option and there's no issue :)
Not sure if this is exactly as you want, and I hope someone wil recheck my asnwer because I don't know much about rust, but this works for me: use std::string::ToString; fn as_csv&lt;V: ToString&gt;(c: &amp;Vec&lt;V&gt;) -&gt; String { c.iter().map(|v| v.to_string()).collect::&lt;Vec&lt;String&gt;&gt;().join(",") } fn main() { let x = vec![1,2,3]; let bob: String = as_csv(&amp;x); println!("{}", bob); println!("{:?}", x); } 
I don't know much about prior changes and claims, but Firefox 57 feels noticeably faster than the current Chrome builds to me. That wasn't the case prior to 57. There are some big changes in this version, perhaps even the biggest changes of any Firefox version? 
Not quite. Mozilla sponsored Rust and Servo together, to investigate better programming techniques and applying them in a better implementation of a browser. The success of Rust and Servo is that is has proven to be so good that some components got integrated into Firefox proper. Quantum as an initiative was born later.
bird eating Ok seeds same bird (top down) looking down into a hole afterwards i guess
I believe the purpose is also to get much better performance by integrating parts of Servo; the ease of concurrency in Rust has already led to visible improvements.
Moving the update part into its own function (not `self`) works. It is a bit similar to /u/Xaeroxe answer: https://play.rust-lang.org/?gist=ec28d2d7c2560b0320c45bc8e5a06acf&amp;version=nightly 
Thanks, that put me on the right track! With the help of clippy, I started with your implementation above, then made it more generic using: fn as_csv&lt;T: ToString&gt;(values: &amp;[T]) -&gt; String { values.iter().map(|v| v.to_string()).collect::&lt;Vec&lt;String&gt;&gt;().join(",") } 
Associated constants are quite useful; one benefit is to be able use them inside other const expressions. Another is the compile-time guarantee that things will be optimized; I don't want to have to rely on the compiler's optimizer to do value propagation. Finally, I feel they'll be quite useful once we have const generics.
I think you ran into this: https://users.rust-lang.org/t/cannot-borrow-self-as-mutable-more-than-once-at-a-time/8536/2
&gt; Kotlin 0.8 % (+0.9 %) Wat.
This is so awesome and I really want to fall in love with this language! The learning curve with Rust is steeper then I’m used too, but I think it will pay off in the end.
I don't understand https://github.com/jplatte/turbo.fish/blob/master/src/turbofish.rs#L21
I just wanted to start out by saying that i love posts like these and i think this is useful feedback for people working on various things in the community as well as for newer people trying to understand certain concepts. &gt;As was serde, although I found it hard to find what I wanted from the serde docs; renaming fields for instance, or other ways to modify the output, were hard to learn about. I'd love some prominent examples just showing the different ways you can control the output (perhaps I just missed them?). Just curious, where were you reading the docs? The website https://serde.rs describes most of those things fairly well, but i imagine if you were looking through docs.rs or something it would be hard to figure out. I don't personally have too much experience with tokio and the like (though if you haven't seen they are going through a major docs and ergonomics reform), so i can't give much insight there but from a quick glance your code looks pretty well done on the client side and in general is pretty easy to follow.
For me Chrome is crazy, Firefox is very good with energy
Relative increase? 
You don't think the world runs on visual basic, Delphi and assembly?
&gt; switch on Stylo in about:config Thanks! Maybe it is a placebo but it really feels much faster (scrolling in particular).
You can disable it. When using LSP, it draws a red box around any errors it finds, and gives details in a dedicated pane below, which auto-hides: https://www.dropbox.com/s/3onik5dhcijgvs6/Screenshot%202017-11-14%2011.30.05.png?dl=0 If you hover over the error, it gives you details and code actions (which mostly don't work yet, but you get the idea): https://www.dropbox.com/s/eljxt4a3b6f3bey/Screenshot%202017-11-14%2011.30.21.png?dl=0
Now lets hope SeaMonkey also follows the suite, its the only real webBrowser left after Netscape, while Firefox is a Chromes Btich.
&gt; Rust can still have contention on multithreaded allocation/deallocation. Yes, however: * Rust will not generally do (lots) of (implicit) allocations * Rust defaults to jemalloc which AFAIK has - thread-specific allocation cache (0 contention when hitting the caches) - multiple arenas to reduce lock contentions (it also has support for thread-bound arenas but that's currently disabled by default as it leads to efficiency issues) - granular (sub-arena) mutex e.g. if one thread hits an arena for a large object and an other hits the same arena for a small object they should use different locks
Looks like C++ trying to beet Rust in this area too with the [spaceship operator](http://open-std.org/JTC1/SC22/WG21/docs/papers/2017/p0515r0.pdf).
Scala rose to visibility only after the advent of SalaJS, so whats happening to Rust that its rising NOW, please let us know so that i can head that way, it took me two years to realize i should be moving to ScalaJS and i am so happy from what i found.
Yep, confirmed using Wayback.
I think title-wise its not clear that the Attributes section contains everything I want. I needed defaults and a custom Deserialize, so I googled and unfortunately ended up in the custom Deserializer section. There's nothing outright wrong about the section names, and if I had paid attention I would have found the right sections, but the names don't help you find the right section when you don't know the serde terminology yet and end up somewhere random in the docs.
Sure, this is an entirely acceptable way. In an application, I would probably avoid panicking on IO error. Printing an error message and exiting would be my preferred tactic.
Perhaps the recent work around the wasm target might induce a similar effect for Rust. There's also ongoing work in the embedded space to make Rust viable on more boards. More targets bring in more devs.
Does that do a copy? 
True, an *expert* can *by definition* write safe C++ code. But given the current state of software security, either the expert's aren't doing that (having a bad day, perhaps?) or there is a *serious* shortage of actual C++ experts per the above definition.
&gt; Go has such a simple story for this that I get frustrated with having to remember how to use `mod x`, `extern crate x` and `use x`. I'm not a Go developer, could you expand how Go approaches this problem?
It's a buzz index. Swift is currently not advertised by Apple much and Scratch is currently heavily pushed by Microsoft for programming for children. That might not be apparent in the circles we are usually in.
Alright, thank you very much for your answer!
Is there something like this for Visual Code?
&gt; I guess there aren't many so far, maybe Servo is one. Larger deployed systems written in Rust: Dropbox, 1aim, Wire, Sentry, Chef Habitat, Skylight, Parity, Maidsafe, Clever Cloud, Appsignal. All of them have good feedback, some of them are publicly visible. And these are just the ones I get out of my head. DropBox gives a few talks about them, but I definitely see that we should probably start - sorry, that might sting a little - to write whitepapers.
hey, that whitepaper idea sounds good, I'll look into it. We're already going around giving talks about how Rust was nice for Clever Cloud
A lot of that will change and become simpler with the upcoming module system changes.
You could use entry api: https://play.rust-lang.org/?gist=51852bc4a3a53fa59bc1ab6117245ff9&amp;version=stable
Tried the beta on Linux. It used more memory than Chrome.
I looked at entry, but did not realize it would work this way, nice. I'll still need to ignore the return value on line 22, but that'll be ok. Thanks!
Chrome was an scam to destroy FF and it succeeded when FF copied it BS features
Please get in touch with the community team, the topic is frequently coming up.
You get `cannot move out of borrowed content` because calling `into_iter` tries to consume `C`, but you only have `&amp;C`. You can change the function to // I removed the `&amp;` here v fn as_csv&lt;V: ToString, C: IntoIterator&lt;Item=V&gt;&gt;(c: C) -&gt; String { c.into_iter().map(|v| v.to_string()).collect::&lt;Vec&lt;String&gt;&gt;().join(",") } Now to the function you pass `&amp;Vec&lt;i32&gt;`, which implements `IntoIterator&lt;Item=&amp;i32&gt;`, and `&amp;i32` implements `ToString` (because `i32: ToString`, and `&amp;T` implements `ToString` if `T` implements `ToString`), so everything works out. 
Go files have a `package name` header, and all files in the same folder with the same header are parts of the same program. If you have a library in `$GOPATH/src/helloWorld` for example, you can just `import "helloWorld". `go get` is used like Cargo for Rust, except it downloads to your `$GOPATH/src` folder, so to import a library (from Github for example), it's just `import github.com/username/repo`
It's [magic](https://github.com/jplatte/turbo.fish/commit/687713da4d6f064c26d76ae50651c41577583ecf)!
After consulting a link in this thread, I came up with this: https://play.rust-lang.org/?gist=f53ae3e3dedeb71259713f103d2ba7b0&amp;version=stable Please have a look if it suits your needs.
The lectures seem to be in german, which narrows down the target audience. Wouldn't be a bad idea to add that to the title, if you can still edit it!
Damn, I wanted to add it, but forgot. Sadly, I can't edit.
Yea, i would definitely agree there. The attributes section is only an obvious place to look if you know the terminology and are used to using them in other contexts. They definitely don't stand out at a first glance. 
/r/playrust
As of a month ago
[removed]
I have been roughly following along (though there were several iterations so I don't have a clear memory of where it ended up), and am excited by this :)
[str::replace](https://doc.rust-lang.org/std/primitive.str.html#method.replace) allocates and returns a new String clone. This seems like a silly way to do it though.
Thanks! As another commenter has already replied, it did not click that the place to look was in the Attributes section (though with the benefit of hindsight it now seems obvious!). My gut reaction was to jump straight to the "Examples" section, assuming that I would be able to pick up enough quite quickly from glancing at a few examples of how to add attributes, but usage of attributes therein was scarce :)
master.rs:214 let (code, body) = match req.method() { &amp;Method::Head | &amp;Method::Get =&gt; ( you can do let (code, body) = match *req.method() { Method::Head | Method::Get =&gt; ( I also don't like response_str(), because you don't differentiate the errors from the Ok methods. `call` should return a `Result&lt;Self::Future, Error&gt;`. This way you aren't batching the errors together with the Ok-s. Have you run clippy over the code?
Wrong Rust
Regarding your question as to why Rust only uses one core: Tokio's Core reactor is both an Executor and an event loop. So you can spawn futures onto the same loop that is running epoll. While that is a nice option, it also means that all your futures are going to be executed on a single thread. So it may make more sense to spawn your futures onto a different Executor like a futures-cpupool, so your event loop is less blocked by CPU work and can dispatch more work onto the different threads of the thread pool. This may not always be fully possible, as especially hyper is not all too thread safe. However with the channels provided by the futures crate you can at least send the results of the single threaded computation over to the thread pool to do additional computation there. 
This playlist was basically my entry to rust, so thank you /u/DebuggingPanda. I watched all of it. Especially the lesson on Send + Sync traits and Arc / Rc / Cell is very well explained. The last lecture about performance isn't that necessary, basically just "remember CPU caches and don't use linked lists". The videos about Git aren't particularly important if you know Git already, you can skip those, too. The "community" video is about where to get help (IRC, Forums, GitHub, Reddit), so if you found this comment, you don't need to watch that video. The FFI video is little more than the FFI web page + I tend to auto-generate bindings nowadays. The other videos are solid material however, slides &amp; programs / tests / solutions here: https://github.com/LukasKalbertodt/programmieren-in-rust
How to implement height for width allocation for my custom gtk-rs widget? It looks like I need to implement virtual methods (http://gtk-rs.org/docs/gtk/struct.Widget.html#height-for-width-geometry-management--geometry-management), but I am not sure how to do that.
Wondering if the extra buzz Rust is getting recently is due to Firefox Quantum, or if it's more distributed than that.
Sounds like what Java was trying(and failed) to do
Maybe /u/kibwen can help out? :)
Too bad it is in German...
Well, the title of the series gives it away, anyway.
&lt;3
The only downside is the death of XUL addons...
Well, given that I have 4 talks and 2 trainings lined up until the end of the year and are booked for 2 more talks for the next year already and neither me nor most of the conferences/clients have anything to do with Firefox... probably more. 
Just confirmed, it isn't possible, sadly.
I think this myth of 'sufficiently smart C++ programmer' needs to die along the same lines as the 'sufficiently smart compiler'
&gt; I get frustrated with having to remember how to use mod x, extern crate x and use x http://manishearth.github.io/blog/2017/05/14/mentally-modelling-modules/
&gt; Associated constants are quite useful; Do you have any links to source code examples? &gt; one benefit is to be able use them inside other const expressions Can't one use a `const fn` returning a constant instead? &gt; Finally, I feel they'll be quite useful once we have const generics. /u/cramert was talking in another comment about allowing trait methods to be `const fn`, if that is the case we basically have two language features for doing more or less the same thing. 
That's good to hear!
&gt; I've been disappointed each time. Are you sure you've been using 57? &gt; it should be note I'm writing this comment using ff on android. Ah, you need to wait for 58 or 59; you should try out beta or nightly.
You should file a bug; it should use less. (It doesn't use one process per tab for this reason, instead multiplexing N tabs over M processes, and therefore should use less memory.)
I wonder whether libservo could be used to build a TUI browser...
Thanks for this - I did have a small play with futures-cpupool in the watcher, but I suspected that the master isn't doing enough work to really benefit from using it, and so I didn't bother. If I have time, I'll have a go at trying to maximise my use of futures-cpupool in the master and re-benchmarking to see what sort of difference that might make :)
Being German, too bad the audio level is so low. :(
&gt; Smooth is OK, "tear free" is just a more tech term. Except that Wayland doesn't guarantee smooth resizing, if you resize your window more rapidly than the applications can send updates, there are only two choices: - the compositor updates the window with the previous content which means that the window is temporarily ugly but the 'frame' resizing is smooth (X11) - the compositor wait until the application finish sending data which means that "every frame is perfect" but resizing is laggy/jerky (Wayland) &gt; LOL, the fact X11 had network transparency doesn't mean Wayland can't use network Users don't care about "theoretical" things, currently if you use X11 you have remote display of windows, if you have only pure Wayland you don't have this and you need to add something on top of it to have again this (nice) feature. 
What's alternative to `wait()` on futures that doesn't block I'd just like to run the future and return nothing, not wait for it nor return the future. Like flipping the switch on and leaving fn log_stuff(){ let f = lazy(|| { //expensive io stuff ok::&lt;u32, u32&gt;(1) }); //what can I do with f that doesn't block return; }
Unfortunately, quite a lot of the world still runs on visual basic, Delphi and assembly.
WebAssembly being supported in all browsers is huge. That’s written in C C++, which compiles on LLVM. This bodes very well for Rust.
Visual Basic yes, but Delphi, like, where even *are* these people? I refuse to believe that Delphi is more popular than MUMPS, which barely even registers on TIOBE.
Firefox is fast... For about an hour of being opened without being closed. If left opened for extended periods of time (I keep various sites open to check BTC prices) the app starts locking up and freezing. It's happened regular ly enough to me I think I'm going to switch back to Chrome. It's sad because it's really fast in the beginning and I really dislike Chrome as of late. However locking up out of no where all the time after being left open for long periods of time is unacceptable
But in PYPL rust is [pretty much flat](http://pypl.github.io/PYPL.html?country=US) since May 1st 2014. That doesn't seem right to me. Not dissing it -- it is hard to come up with reasonable trends when the numbers are so small (and rust's numbers *are* still small, but climbing). I am hopeful that the communities' efforts to stabilize core parts of the language/ecosystem pay off with some increasing popularity. I see this jump in TIOBE as a *potential* indication that it may be happening!
This blog post brought to you by the "how many times can you say 'fearless concurrency' and keep a straight face" cabal." Seriously though, I now appreciate that term a lot more. One thing that cropped up in the review of this post was that I didn't have examples of bugs Rust prevented. Because I couldn't think of any concrete ones. Because Rust's safety doesn't work that way, it prevents your concurrency bugs before you realize you had them, by making sure you don't paint yourself into a corner. "Fearless concurrency" really is the best way of putting this; the benefit was not that it prevented concrete bugs, but that it let us fearlessly and aggressively write code knowing that it would be concurrency bug free.
wat Even with flat earth and fake moon landing, this is one of the more obscure conspiracy theories I've ever seen.
I'm now using the final release and it's using less than Chrome. All good.
Quote w.r.t. Stylo: "It replaces approximately 160,000 lines of C++ with 85,000 lines of Rust.". That sounds fantastic. What are the largest contributors to this reduction?
&gt; True, an expert can by definition write safe C++ code. Then depending on which expert you ask, such an expert does not exists. For example, if you ask Joe, a C++ expert in writing Qt throw-away application code with 20 years of experience that read Effective C++11 yesterday and thinks that `std::unique_ptr` and `shared_ptr` are the answer to all of C++ safety issues, then yeah, such an expert will tell you that writing safe C++ code is not only possible, but also easy if you follow modern practices. OTOH if by expert you mean a C++ committee member writing fundamental libraries used by way to many developers, the answer is completely different. For example, I know range-v3 internally pretty well, and if I had to make an estimate, I would say that ~30-50% of its LOC are actually layers and layers of abstractions to ensure safety. Stuff like [`dangling`](https://github.com/ericniebler/range-v3/blob/master/include/range/v3/utility/dangling.hpp) to track iterators that might not be pointing to a valid range, [`static_const`](https://github.com/ericniebler/range-v3/blob/master/include/range/v3/utility/static_const.hpp) to avoid ODR issues, box, polymorphic cast, scope exit, a full type-checked generics emulation layer, a contract-programming layer, and the list goes on and on. Yet range-v3 is thread-unsafe _by design_, and all these safety layers are barely enough! Bugfixes for index out of bounds, reads of uninitialized memory, integer overflow, and other kinds of undefined behavior still land every now and then. So if you were to ask the kind of world class expert that writes range-v3, the answer might be that is probably impossible to write safe C++ code. 
I have commented on that issue now, if there's any need for help or any issues found let me know and I'll try my best to sort it out :)
F E A R L E S S C O N C U R R E N C Y
As far as I remember, Go sandboxes all projects into the same folder and allows you to call any publicly available fns or interfaces from anywhere inside it. It won't allow you to compile things outside toyland tho.
You might want to use a gist for this. While I haven't fully reviewed your code yet one thing you can try is to see if you can restructure your code so that you're only borrowing the portions of the structure you need. I.e. rather than self.function() see if you can use function(self.member).
Gecko uses a lot of C++ macros to deal with boilerplate and handwritten parsing/animation/computation code. We use custom derives for parsing/animation/computation and mako templates for boilerplate. Mako templates aren't Rust specific, but they work pretty well. But custom derives should be a major contributor.
I can imagine a lot of things contributing. Classes/templating/inheritance can get pretty verbose. Rust can get verbose signatures, but defining traits and data methods is pretty succinct. Rust's threading is more succinct than pthreads. Rust then generally has a lot of higher level constructs that just chop away lot's of fiddling. Like match, if lets, Result/Options, how rust does iterators and closures. Rust benefits a lot from a syntax that could be planned to incorporate a lot of modern quality of life features. Also let's not forget the joys of refactoring. A lot of that 160,000 lines in some part was code that existed to merely to deal with how other code was written. If you could just chunk all the important bits, throw away all the legacy stuff, you cut down a lot of code. Lastly, I'm not sure what all those line counts include, could also have years of regression/unit-tests in that c++ code base, header files, and dependencies. The Rust line count might not be counting crate dependencies.
When asking questions it would be nice if you reduced the example that causes the error - then it will be easier for others to review it and explain what's going on. I reduced your example, and it still gives the same error: use std::collections::HashMap; struct Config; struct Sources; struct Builder { config: Config, bundles: HashMap&lt;String, Sources&gt;, } impl Builder { fn merge_sources(&amp;mut self, sources: &amp;Sources) { // free all memory that is used by hashmap self.bundles.clear(); // uh oh, we are looking at freed memory! println!("{:?}", sources); } fn merge_bundle(&amp;mut self, bundle_key: &amp;str) { let sources = self.bundles.get(bundle_key).unwrap(); self.merge_sources(sources); } } Now, to answer your questions: 1. Calling `self.bundles.get(bundle_key)` (`HashMap::get` to be precise) requires an `&amp;HashMap&lt;_, _&gt;`. So by calling it you use an immutable reference to that hashmap (`self.bundles`), which means that some part of `self` is now borrowed. This borrow will last for as long as you keep the returned value. 2. Calling `self.merge_sources(sources)` mutably borrows `self`, because `Builder::merge_sources` takes `&amp;mut self`. At this point `sources` is still in scope, so the previous borrow of `self` is still live. Thus you get the error - you cannot have a mutable and immutable borrow at the same time. If you could, consider what would happen if you had this: impl Builder { fn merge_sources(&amp;mut self, sources: &amp;Sources) { // free all memory that is used by hashmap self.bundles.clear(); // uh oh, we are looking at freed memory! println!("{:?}", sources); } fn merge_bundle(&amp;mut self, bundle_key: &amp;str) { let sources = self.bundles.get(bundle_key).unwrap(); self.merge_sources(sources); } } When checking `merge_bundle` the compiler doesn't look that you don't do such stuff in other functions. It sees that `merge_sources` takes `&amp;mut self`, and that's it. 3. I don't really know what's the best way to avoid this. One way to do this would be to inline contents of `merge_sources` and `merge_source` inside `merge_bundle` - then the compiler could check all code at the same time and see that you use `self.bundles` immutably, and `self.config` mutably, but don't create multiple mutable references to one thing. However, this will result in pretty large functions, and the resulting code will not be very nice. Another way would be making `merge_sources` and `merge_source` into standalone functions that wouldn't take `&amp;mut self`, but instead `&amp;mut Config` - then compiler would be able to see that the function cannot mutate `&amp;self.bundles`, because it simply doesn't get the reference to that. The code would look something like this: impl Builder { fn merge_bundle(&amp;mut self, bundle_key: &amp;str) { let sources = self.bundles.get(bundle_key).unwrap(); merge_sources(&amp;mut self.config, sources); } } fn merge_sources(config: &amp;mut Config, sources: &amp;Sources) { // do the stuff... }
We're not counting tests or dependencies there, but we're not counting tests or dependencies in C++ either; there are plenty of more general purpose bits of code in Firefox not included in that count.
How much does this matter? There are languages that are unpopular but good, popular but bad, unpopular and bad, and popular and good. 
I think he makes a lot of interesting points and generally characterizes the current situation pretty well. Of course I do disagree with his last paragraph :P Personally garbage collection conceptually bothers me enough that I still prefer the borrow checker, even if it's slightly harder to use. Why should I burn CPU cycles on memory management when I have the choice to just write code that doesn't need GC?
You can't edit post titles on Reddit, sadly
It is? I never noticed and no one ever told me :/ Sorry!
Quoting *The Book of Mozilla*, 11:14: &gt; The Beast adopted new raiment and studied the ways of Time and Space and Light and the Flow of energy through the Universe. From its studies, the Beast fashioned new structures from oxidised metal and proclaimed their glories. And the Beast’s followers rejoiced, finding renewed purpose in these teachings. (From `about:mozilla` in 57)
Where is the `.set_parallel(true)` call?
That's a pretty soft generalization. Keep in mind that visual basic and VB.net are separate, and if you are putting stock in their ratings, then even more of the world runs on scratch and still more runs on R and Matlab. It would also mean object Pascal is a rising star and about to overtake php.
When will Quantum be merged into the main Firefox distribution?
You could also keep `merge_sources` associated with `Builder`: impl Builder { fn merge_sources(config: &amp;mut Config, sources: &amp;Sources) { // ... } fn merge_bundle(&amp;mut self, bundle_key: &amp;str) { let sources = self.bundles.get(bundle_key).unwrap(); Self::merge_sources(&amp;mut self.config, sources); } } 
My current design checks at runtime whether the unit_id points to a valid unit, but mutability checks are done statically. Under shared pointers it seems I'd know I had a valid unit, but mutability checks would have to happen at runtime via RefCell. Is that correct? I think I will stick with the current design since there are some other conditions that I need to enforce, like having a consistent index from unit_id's to locations and from locations to unit_id's.
This morning, I believe.
1. Because `HashMap::get` returns `Option&lt;&amp;V&gt;` where V is the value type, you're immutably borrowing something that the hashmap owns and storing this borrowed value in `sources`. 2. `self` is borrowed mutably when you call `self.merge_sources`, which takes a mutable reference to self (`&amp;mut self`). 3. I think if you borrow at a finer level of granularity then the error will be resolved. `merge_sources` takes `&amp;mut self`, but only uses `self.config` from that borrow. In `merge_bundle` `sources` is holding on to a reference from `self.bundles`. So if you sent `self.config` to `merge_sources` instead of borrowing all of `self` then I think the compiler would see that the borrows of `self.config` and `self.bundles` don't conflict, and you wouldn't get the error. I haven't tried it, though.
The fact that you can't use them for array lengths is a bug, and should be fixed along with the arrival of const generics.
Except Qt does break API constantly. It's just that they keep supporting ancient C++ versions because lots of companies are stuck with ancient codebases that require that.
What is the Rust limitation that requires the RTLD_NODELETE thing? 
Really? I have over a thousand tabs open in Firefox at the moment and Firefox has been able to handle this well for years. But for me chromium always chokes on more than 50 tabs due to running out of memory.
Rust's TLS registers some stuff with pthread which is not cleaned up on dylib unload. So when the interpreter finally shuts down the main thread exists and pthread calls into unloaded code and crashes.
&gt; True, an expert can by definition write safe C++ code. I thought that too, when I was young and naive (aka: just fresh out of school). I dove headlong in C++ (back in 2008), followed the C++0x development, learned and experimented, played around with all the cool kids (preprocessor programming, template meta-programming, ...), etc... I assumed that as my experience grew, I would after some time reach a point where crashes would be a thing of the past and I could intuitively navigate the C++ seas. I was wrong. I did become somewhat of an expert on C++ (at least, up to C++11; I must admit having only incidentally followed the development of C++14 and C++17). What I had failed to anticipate, however, is that the better you get, the more difficult the challenges you tackle. In retrospect, it seems obvious! However, it means that the experts will not write safe code. They will simply move on to more and more complex tasks, combining functional (complex domains) and technical (distributed multi-threaded servers) difficulties, up until the point where they are challenged enough. And by the law of trade-offs, it means they'll be challenged enough NOT to write perfectly safe code. 
I guess it will really depend how good GCs get. Working in low-latency environments, GCs are simply not realistic. The one example of a low-latency Java application I know was one where *everything* was pre-allocated and there was NO garbage collection cycle for day-long runs (none, zero, zilch, nada); at this point, though, you're fighting the language more than anything else. So, in the mean time, I really like the value proposition of Rust :)
Expanding on /u/pm_me_good_usernames, Quantum is the process of gradually integrating parts of Servo into Firefox, and other assorted overhaul. Firefox 57 is the first version to benefit from that process, but it isn't over. More goodies will come in the upcoming releases.
There is the `let x: () = foo;` to force a compile error to figure out the type of `foo` approach. It's not great though.
Oh! I did not know this was in the work and that a consensus had emerged as to how to deal with this. Nice to know that we may be saying goodbye to one more soundness hole.
I'm skeptical of how good a GC can be to be honest. I think the GC is conceptually broken in regards to latency and there just isn't a good way to fix that.
Truly a great example of *successful* yak shaving ;p
I ended up using a `tokio_core` event loop. Like in an example on [hyper](https://hyper.rs/guides/client/basic/) . But instead of using `run()` which polls to completion, I used `turn()` which run at least once and has a timeout.
The second value is not just `&lt;` though: if you open the file in a hex editor, you will see the string actually includes the bytes: `3C E2 80 8B`, which is a zero-width space. I think this somehow removes the need to escape the bracket in HTML, as seen in the rest of the commit.
&gt; (where’s my select(2), again?) Which languages have a standardized API for `select`? (Pardon my ignorance. From the context, I'm assuming Go does?)
&gt; I think the GC is conceptually broken in regards to latency and there just isn't a good way to fix that. Unclear. As in most everything, it's a trade-off. Most GCs today optimize throughput at the expense of latency however there's no reason it ought to be so and the focus on micro-services architecture has actually shone the light on the need for lower latency prompting efforts to be renewed in that domain. I think some interesting prospects are: - Java's G1: auto-tuning GC? Given how hellish it is to configure a GC well, especially on a moving target, a GC which can be given "targets" and react live to changing workloads sound really appealing, - Nim's GC: Nim was originally developed by Araq, a game developer, needless to say near-realtime is a very real concern there (60/120 fps targets!), and the Nim GC is extremely controllable; it can run in either of 3 modes: automatic (with configurable max collection time), controlled (the user calls collections explicitly, each time passing a max time) and just disabled (which leaks, obviously), - Go's ROC proposal: I worked on a C++ application which used a per-request arena scheme, where all transient memory is just allocated by a bump allocator during the request, and the whole area is reset after the reply. It's extremely effective, though incredibly error-prone in C++ of course. It's also essentially what Herb Sutters demonstrated with his `deferred_ptr`.
That's why I think the 'definition' is flawed, so I wrote the above strawman argument.
It's twice the speed and less memory now. Might be worth checking out again.
I'm admittedly not well versed in the inner workings of GC mostly because I largely turned my nose up at it. When I say "conceptually broken" I guess what I really meant was "conceptually flawed". I just dislike the idea of using processing power to manage memory automatically in any way. A program should have ample opportunity to know when it's done with a piece of memory and when it ought to delete it. I don't care for the idea of a separate process calculating my program's memory usage at run-time. I want those resources for my code so my program can keep doing what it was made to do. Perhaps you can fix latency. Even then you'll still have problems with scaling up the amount of objects in use by a process. I guess what I'm getting at is yes it's a trade-off, but only if you consider the programmer's time spent in the equation. For the program itself, most of the time GC is a detriment. I consider my code to be my art, so I'll go to great lengths to make it work as best for the computer as possible. I want to be proud of the software I made. Big fat disclaimer: I rarely work in commercial contexts currently, so time and money are less meaningful to me in software design, because the only time I'm working with is my own. It's easy to sacrifice that, but maybe I'll change my mind when I'm dealing with other coders that I need to get work done quickly. 
Thank you for the clippy shootout. I'd probably be even more happy if you included the caveat that it runs on nightly only (for now, we're working on getting it on stable for some time now). Apart from that, I'd like to add [patterns](https://github.com/rust-unofficial/patterns).
Misread the code, `context.cause` will return the inner context. Verified this by writing code.
Delphi is quite heavily used in teaching and still quite popular in Germany. It's unsurprising, given that it's based on PASCAL, which was made for teaching. Also note that Delphi is still actively developed. MUMPS is and always was a fringe language which I'd not expect to have ever built a large Q&amp;A culture outside of the reference manual.
Can I get fearless parallelism instead?
I was hoping you'd link to the Rust bug for this...
I'd like to ask a follow-up question if you don't mind :) I'll be regenerating this structure somewhat often, and i saw that `BtreeMap::clear` just allocates a new one. Should I just iterate over it and remove the items? Moreover, is there a way to have it preallocate some space? Memory usage isn't really a concern as long as I don't leak memory, since I have a lot, but will have that thing running for long times uninterrupted.
I think this one is the best description of the issue: https://github.com/rust-lang/rust/issues/28794
As a production user of Go I also know about the disadvantages of this - mainly that everything always fetches HEAD and vendoring dependencies is very hard. Granted, that has nothing to do with the import syntax, rather the lack of a `Cargo.toml` or similar in Go. My gripe with the import syntax is that your publishing place is backed into your import path. While that is great for third party libraries you use, it is not super great for your own library that you eventually want to export to GitHub. The mental model is hard for me to adjust to.
Part of my incredulity, aside from having never seen anyone use it and never seen any job applications asking about it (and I've used RPG professionally!), is that getting a copy of the compiler costs [$1,200](https://www.embarcadero.com/app-development-tools-store/delphi) (and that's only for the cheapest tier!). I just don't see a future for an ecosystem with such an extreme barrier to entry, not when every other toolchain on earth (including manager-favorites Java and PHP) can be used for free.
&gt;Scala rose to visibility only after the advent of ScalaJS What makes you say that? I would've guessed Scala got big with the Big Data&amp;trade; revolution.
It's a tradeoff between latency and throughput. [This recent presentation](https://www.youtube.com/watch?v=VCeHkcwfF9Q) about a low latency GC for Java is interesting. Basically they achieve sub-millisecond pauses even for huge heaps, but pay up to 30% in overall performance loss and about 15-20% in memory overhead. There is also Zing for Java which I think achieves similar latency with better performance by patching the Linux kernel. But yes, if you want both low latency and minimal performance/memory overhead I don't think GC is a viable option.
German here: As fgilcher says in your sibling post, I had both Delphi and TurboPascal in school (around 2010 IIRC), although only for half a (school) year. In grades 11 and 12 we used Java (ugh.).
&gt;How much does this matter? There are languages that are unpopular but good, popular but bad, unpopular and bad, and popular and good. I'd make a bet that languages which are popular have better ecosystems, regardless of whether or not they're "good" -- look at `npm`, for example. Rust has a great foundation, and growing popularity would mean more and more mature tools for everything, which is great.
As I understand the distinction, Rayon `join` and `spawn` give you concurrency, and Rayon iterators give you parallelism. It's all fearless!
[You can do the turbofish bit of it though.](https://turbo.fish/::%3C(),%20E%3E)
I tripped over a couple of the same things with hyper: &gt; First, unlike clients in Tokio, which take a handle, and work created is explicitly given to a Core to run, servers by default don't do this at all. It turns out that they internally manage their own core! I filed an [issue](https://github.com/hyperium/hyper/issues/1075) requesting that hyper servers reuse existing cores. &gt; I did find myself wishing that there was a built in static file handler available to plug in (Go has one in their http lib which was most convenient), but hyper_staticfile served me well and was easy to plug in. I have an unpublished [http-entity](https://github.com/scottlamb/http-entity) crate and `http-file` crate based on it. A few advantages over http-staticfile: * you can specify a separate thread pool for I/O rather than blocking the async reactor thread on disk I/O. * conditional GETs * byte range serving I really should publish it. It works. Mostly it needs a name, maybe a continuous integration setup, maybe a pass over the docs and a simpler example. (The example sort of turned into a benchmark of several modes by accident, oops.) Name-wise, I felt nervous about claiming something super-generic like hyper-file or http-file. Suggestions welcome! &gt; It wasn't as clear to me how to handle errors [on the server side], though admittedly I did not really put much time into trying, so perhaps I missed a fairly obvious nice approach. See [issue](https://github.com/hyperium/hyper/issues/1128) about hyper redesigning the server errors. You returning an error to hyper means "drop the connection abruptly"; they're hoping to make this more clear in the redesign. If you are trying to do user-friendly errors to clients, you probably should be returning ok with HTTP status 4xx (client error) or 5xx (server error) instead.
IMO, C is more to blame than C++ for most security problems, but you may be right that even a very experienced C++ programmers using the latest C++ guidelines and language/stdlib features will write unsafe code sometimes.
I suppose that by main distribution you mean the Release channel? (As opposed to Beta or Nightly.) Firefox Quantum, a.k.a. Firefox 57, moved to Release a few hours ago.
Well, I don't see how the compiler currently being messy, slow and sub-optimal is "awesome", but as I said, it is being worked on, and that *is* "awesome". :) Maybe that's what you meant. :) What I described are problems with the compiler, amplified by the design of the language (notice how generics are used *everywhere* in Rust, even `Option` and `Result` being some of the most important types in the language). Debug binaries really don't have to be this slow and release compile times don't have to be as long as they are either. It is an indication that the current compiler still has a long way to go and get better. Even LLVM's optimiser was designed with C/C++ in mind and does not yet properly support some additional performance optimisations that are relevant to Rust/Fortran but not to C/C++. This is another known area for improvement. Rust programs will get a further big performance boost when LLVM learns how to properly do these additional optimisations. There is a lot of unused potential with Rust. But yes, the Rust language itself is awesome. Glad you are liking it. :)
&gt; Users don't care about "theoretical" things, currently if you use X11 you have remote display of windows, if you have only pure Wayland you don't have this and you need to add something on top of it to have again this (nice) feature. What are you talking about? Wayland is just a protocol between apps/clients and compositor/wm, you can use it with everything you want, there is no need to integrate network transparency in Wayland. Or are you talking about apps for remote desktop controlling that doesn't support Wayland yet? That is a totally different thing and there is nothing wrong in Wayland design!
True, maybe it is school students driving those rankings, given how high Logo is as well.
Go doesn't, and referring to `select(2)` here is a bit of red herring. What ESR wants, and what he found in Go, is the ability to select over the standard library's channels.
[removed]
Right, but you could just as easily have a `smart_assert!(x.is_empty())` which knew about `is_empty()`, or the more interesting case like `smart_assert!(x == some_big_collection)` which shows a diff of the two collections if the assertion fails rather than just printing them out and making you manually pick out the differences. The question is, what's the value of having the syntax be some custom `morq!(expect(foo).to.be.bar())` rather than `smart_assert!(bar(foo))`.
Nothing can surpass the long arrow operator.
Ah gotcha. Thanks.
[removed]
I imagine a lot of it is simply due to rewriting the thing. This is a codebase that's grown incrementally over the course of 25 years. There's a lot of artifacts resulting from its Netscape Navigator heritage, dating from before C++ was standardized. Standard libraries took a surprisingly long time to mature. There were a lot of implementation specific incompatibilities between the C++ compilers on Solaris, Mac OS classic, Windows, and Linux, all of which Mozilla had to support. (probably stuff like HP-UX and Irix as well? I don't know.) As a result, Firefox has its own builtin STL, much of which is redundant. It contains, for instance, 3 implementations of `std::vector`, 4 hash table implementations, and more linked list variants than you can shake a stick at. Exceptions used to be spectacularly slow, and Mozilla took the (then) pragmatic approach of disabling exceptions entirely at compile time. So Firefox is significantly more verbose than it would be if it used a more modern error handling strategy. One of the things they did was implement their own option type, (they call theirs 'maybe') which is nice, but since they've rolled their own, it's still extra lines of code.
&gt; The main one being that we would likely be depending on highly unstable interfaces in the Rust compiler and we’d prefer to wait for all of this to stabilize first. What features are you looking to use?
Aah, I should probably have perused the http-staticfile code; I had assumed it used a cpu pool under the hood I think. Come to think of it, my main issue was that I could not return Results from my future because the server expects a specific type back. However, I could have used it within my future chains just fine I think to propagate errors and allow me to handle them all in one go at the end, which I think was all I wanted to do really! 
Parallelism is is a form of concurrency, and is mostly that they mean when they say concurrency.
No, the CSS engine was merged a while ago. It was flipped on for the 57 release (which started as a nightly a couple months ago), that's all. Quantum is kinda a vague term, it both describes a set of projects (some of which have not been enabled yet, like Webrender) and is also a way of talking about the 57 release but emphasizing that it's pretty new. I don't think we'll be calling it quantum for every release from here on. But maybe.
In a highly theoretical world what I would be interested in is a way to auto generate C bindings out of generic code and also emit meta information to later bind higher level Python bindings from So the transformation would go from this: struct Foo { } impl Foo { #[expose(default)] fn new() -&gt; Foo { ... } } via extern unsafe fn something_foo_new() -&gt; *mut SomethingFoo { ... } and struct SomethingFoo; typedef struct SomethingFoo SomethingFoo; SomethingFoo *something_foo_new(void); as well as maybe a JSON output of all functions generated and how they fit together. So what you need is I think compiler plugins that can generate C bindings automatically (and monomorphize generic functions into C types) and then spit out enough meta information so you can automatically generate high level C++, Python, Go, whatever wrappers around them.
No, that's not the distinction. Rayon gives you parallelism, but parallelism is one way of having concurrency. Concurrency can also be attained by green threading for example.
It already is. Or, *part* of it; integrating Stylo is the first step of Quantum. But that first step is already part of the main Firefox release distribution.
I fixed it with a callback in the end. That'll have to do until we get NLL. https://github.com/thejpster/tockloader-proto-rs/commit/f9d7d4e3853db8c4bf3d9348f2f99bca2decc3e3
For what it's worth...*I* got the joke. :)
Another Redditor here had a similar problem -- I hope that [my reply](https://www.reddit.com/r/rust/comments/76r6yf/need_some_help_understanding_behavior_of_mut_self/doh6p8f/?context=2) going into a little more detail about what others have suggested helps! :) If you have any questions, feel free to reply here.
big 👌 to committing to the meme
[Go 1.8 garbage collection](https://golang.org/doc/go1.8#gc) is supposed to typically have under 100 µs stop-the-world times. I don't have much personal experience with it, but as far as I know the stop-the-world times really are imperceptible in most situations. It's an impressive achievement that means I don't laugh at the idea of GC as much anymore. My understanding though is that there's no GC that performs well with over 50% heap occupancy. Put another way, garbage collection doubles your RAM usage. Presumably it noticeably decreases the effectiveness of L1/L2/L3 memory caches as well. [1] These are significant enough drawbacks that I don't think Rust's approach will be obsoleted or confined to extreme-low-latency niches any time soon. [1] I think it's a bit hard to verify this cache effect given that I'm not aware of a high-performance language that lets you just toggle between a well-performing GC (in particular, not the Boehm conservative GC) and manual memory management. So I think any comparison is apples to oranges.
There's a starter edition: https://www.embarcadero.com/products/delphi/starter
Goodness, I assumed that it wasn't actually preforming a replacement. Good catch!
Sweet! I guess I was confused, because there are so many Firefox variants, and the Quantum release even had its own website, so I wasn’t sure if this was an experimental fork or what. Glad to see servo/Rust in the main distribution now!
it is cool, use remacs
In your opinion, does your milksnake bring all the developers to the yard? If not I can teach you, but I have to charge.
&gt; I'd make a bet that languages which are popular have better ecosystems, regardless of whether or not they're "good" I'm sure they're correlated but I have no idea how strong the correlation is. &gt;Rust has a great foundation, and growing popularity would mean more and more mature tools for everything, which is great. True! 
Regardless of where the ranking "actually" is relative to other languages, trending up means that I am more likely to use rust for my job in the future and will have access to better and better libraries (or at least more libraries). This is a good thing IMO.
I'd like to deploy my rust program to a server that does not have rustc. What are the caveats I need to be aware of? Do I just need to run `cargo build --target my_target_triple` on my development machine and copy the release folder over?
I read that as RTLD_NOODLE which makes way more sense in the context of milkshakes
&gt; but parallelism is one way of having concurrency Is it right? I always read parallelism to mean simultaneous and independent execution, while concurrency always implying interleaved execution while competing for shared resources. So how parallelism has any bearing on attaining concurrency?
I do agree that Rayon's [work stealing] technique employs both concurrency _and_ parallelism, but that it's fine to refer to it all just as concurrency.
[This SO answer](https://stackoverflow.com/questions/38334994/how-to-read-a-c-struct-from-a-binary-file-in-rust) shows the basic technique of reading POD (plain old data) structs. 
Awesome! Well done to everyone involved. I dearly wish I could participate in one myself. Sounds like fun. 
From the recent posts I’ve seen, Eric Raymond seems to have a measured opinion on Rust. I’m new to Rust and have previously only used Haskell, so his criticisms are tough for me to weigh accurately. Does his 5-year horizon to real maturity seem accurate? Are the criticisms legitimate?
They're different kinds of things. The best way I've seen it phrased is that concurrency is a matter of the problem space -- "I want to be able to run multiple routines such that they do not just run sequentially and instead seem to run together". This can be solved by actually running them on two different execution thingies (parallelism), or by interleaving them. Concurrency means "multiple threads of execution make progress together" and makes no comment on how they make progress. Parallelism further clarifies that they make progress simultaneously, not just by interleaving. I believe in the Go community concurrency is sometimes used to mean what you say it means (i.e. "concurrency - parallelism" in my definition) but this is not a standard way of looking at it. 
[Am I doing this right?](https://turbo.fish/::%3Cstd::basic_string%3Cchar,%20Allocator=std::allocator%3CT%3E%20%3E%20%3E)
&gt; how many times can you say 'fearless concurrency' and keep a straight face I don't think it should even count if you don't stylize it as ***FEARLESS CONCURRENCY***
Do y'all contribute back upstream to crates? I love well constructed ecosystems like that.
Yep. Many of the crates are servo maintained (servo existed before the crates ecosystem so we have created a lot of the foundational crates) but we have zero forked crates in stylo and contribute upstream all the time, and only one or maybe two in servo that had to fork after extensive discussion with the authors (our needs were not the target needs)
I believe that if you look at *parser* in crates.io, you'll have lot of examples. The [`Read` trait](https://doc.rust-lang.org/std/io/trait.Read.html) already provides binary level reading in general. Then you can wrap it however you like.
I'm currently doing that, except my "prerequisites" aren't really blockers, I'm just stubborn and like building tools. For example, I want to build a sweet multiplayer, so: 1. Building a multiplayer game is hard, so I'll start on something easy 1. I get distracted a lot on projects, so I'll build something with my coworkers 1. I need a way to collaborate on the design, but I'm trying to rid myself of Google Docs, 1. So I looked at OwnCloud/NextCloud, but I refuse to set up PHP (I *hate* PHP with a passion) on my server 1. So I started writing a simple LibreOffice Online front-end in Rust 1. But I needed some way to keep track of my to-do items (the project's huge yo) 1. So I evaluated several and settled on Task Warrior and set it up to sync 1. There's a mobile app, but it's buggy and written with electron, which is silly, and doesn't even work on Windows (I want my coworkers to help me on that game, and most are Windows people) 1. So I looked around for a cross platform GUI framework to do it myself and found React Native So yeah, now I'm playing with React Native so I can learn to build a GUI app for my to-do list mobile app (and Windows app) so I can manage my to-do items for developing my Google Docs replacement so I can collaborate with my coworkers on a game to learn game development to build the game I want to build. It's a bit ridiculous, but I've convinced myself it's not insane (it is, but I'm really good at doublethink), so yeah, I think I have the next 20 years of my life accounted for :) BTW, this is mostly a joke, but I really *am* learning React Native to build a to-do list front-end that doesn't suck for mobile, and yes, the above was my actual process for arriving at this current project over the last couple weeks...
You should, it's a pretty great name :)
I've been working on a scheme interpreter for my research. It's my first experience with writing a real interpreter, but it's been going pretty well. I got a REPL going today. I'm working on error handling now, and I plan to focus on other improvements to the interpreter this week.
Hahaha, I know this process well 😁 . Sometimes it goes super well, and you end up learning a *lot* of useful things. Other times, it's a massive waste of time and you end up behind for completing important things. I think a key ability to learn is recognising when this is occurring and being able to make a value judgement call on whether to continue or not. How applicable the knowledge is elsewhere and the like.
Heck, C++ didn't even have a boolean type when Netscape Navigator was around!
Yeah, I'm working on a serious project as well and I make sure to budget my time more appropriately, but these exploratory projects tend to take on a life of their own sometimes :)
Clickable link: https://github.com/arosspope/eliza-rs
It has been a very, very long ride but we finally have the nightly Rust compiler and Cargo running on Redox! This has required a large amount of work in porting software and implementing features, but it is almost ready for general use.
&gt; Therefore: eventually we will have GC techniques with low enough latency overhead to be usable in kernels and low-level firmware, and those will ship in language implementations. I've seen a lot of people believe this and insist on this, and not much evidence. I would be very surprised if this occurs in &lt;30 years (for non-toy projects). Interesting to speculate about though.
For what it's worth... I didn't. What is going on with this comment? 
In the beginning, Mozilla created a web browser. The web browser was full of undefined behavior, segmentation faults, and null pointers; and chaos was over the architecture. Then Mozilla said, "Let there be Fearless Concurrency"; and there was Fearless Concurrency. Mozilla saw that Fearless Concurrency was good; and Mozilla separated the Fearless Concurrency from the chaos. Mozilla called the Fearless Concurrency "Rust", and the chaos Mozilla called "C++".
Congratulations! Just out of curiosity what kind of changes (if any) need to be made to Rust code currently to make it redox compatible?
Looks like the [Tessel project has been active recently](https://github.com/tessel), but not tessel-rust. Maybe they just got more traction with people interested in JavaScript?
Here is a diff showing all changes: https://github.com/rust-lang/rust/compare/master...redox-os:compile-redox-stage-0
Firefox 57 review - **Overall:** Excellent browsing experience. Very fast and snappy. Chrome is sometimes a little faster to load things, or at least _visually_, but Firefox seems faster to load them _functionally_ (that is, I can interact with them, and scroll, etc), which is more important to me. I am normally a Chrome user, but today I have exclusively used Firefox, and I gotta say, this is the first day in a loooong time that I've considered switching. I'll spend the rest of the week using it, and decide then. **Other:** I use some extensions. uBlock Origin, uMatrix, and Tree Style Tabs. All work with 57, so, great success :)
I'm learning how to use the language, since I really only became aware of it with the new Firefox release. I've just finished chapter two of the Rust language doc, and it seems interesting so far. Some of the syntax doesn't yet quite make sense, but I'm sure it'll come together as I finish the language doc.
No, it does not.
Double check for some small typos. You can peek at [oXeRun](https://gitlab.com/cardoe/oxerun) for another example of doing that. It’s absolutely basic due to time. 
For me it's a matter of not having to duplicate code. In my D-Bus bindings I now need to have two separate traits (`arg::Arg` and `arg::RefArg`) - one for generic/monomorph, one for trait objects. I would have saved a lot of time and effort if the static methods in `arg::Arg` were callable from a trait object. (And I need the method to be static because they are also called without data.)
I can't find anything. It just prints out can't find the crate for `core`
I’m on mobile and not in front of my machine but try not having .json after the target name or try adding it. That error is familiar to me. 
&gt; we have zero forked crates in stylo *cough* servo_arc, hashglobe *cough*! I guess those aren't crates, exactly...
I've tried it both ways. Same error.
&gt; The static method doesn't take a trait object as an argument, it has no way to access the vtable pointer. Trait::CONST has the same problem. Yes, that's what I'm proposing to change - for the dynamic dispatch version of `Trait::static_method`. The static method should take a vtable parameter. If it makes things easier to understand, this could be stated explicitly, like this: pub trait Trait { fn static_method(Self) { /* "Self" means vtable pointer */ } } A `Trait::static_method` called via dynamic dispatch would access to the vtable of `Self` runtime, so it can call other static methods and associated constants. 
so sad that vimfx extension has to go. And I don't think WebExtension can completely replace that (since there's no way to intercept input at browser level, not per-page level)
Holy cow! This is so exciting!!
What's the purpose behind changing the crate types to dylib and rlib? 
async/await is tied to futures, which is a zero-cost abstraction that isn't specific to tokio.
&gt; Multithreading is one of the reasons why Rust even exists - because it is hard to get right. &gt; I mean if you have multiple cores on a single server then by all means - use them. But use them by spawning multiple processes, not multiple threads. Those two statements are at odds. Rust is strong on spawning multiple threads, and can do cool stuff with them (check Servo and Firefox 57). If I wanted to spawn new processes instead of new threads, I probably wouldn't bother with Rust.
Cargo is pretty heavily focussed on building your source-code, at the expense of generic build-system functionality like gathering and/or pre-processing resources. If making a thing requires compiling Rust code *and* setting up a file hierarchy *and* putting the right files in the right places, you probably want to use some other automation system to invoke Cargo and do those other things. It could be a platform-specific system like RPM, an industrial-strength cross-platform tool like CMake, or just a simple shell-script or Python script, it's up to you.
Does this mean Redox is now self hosted?
This looks like a very nice challenge! I haven't found an official documentation on the format. Are you familiar with [this](https://cran.r-project.org/web/packages/sas7bdat/vignettes/sas7bdat.pdf) specification, is it accurate/up to date? I like having a python reference but being able to read a spec helps too.
Check https://doc.rust-lang.org/std/macro.include_bytes.html
Redox has no dynamic linking: https://github.com/redox-os/redox/issues/927
At least C programmers have some semblance of knowledge about the eldritch horrors they're getting themselves into. Many a C++ programmer thinks they can abstract away the unholy entities nibbling on the remains of their sanity if they manage to just spell the arcane incantations right.
You could use a [build script](http://doc.crates.io/build-script.html). You can get the build directory path with `std::env::var("OUT_DIR")`.
As far as I know standard library doesn't leak memory just like that. If `BTreeMap::clear` didn't properly clean everything up it would be pretty useless. 
Self-hosting means the developers run Redox on their laptops rather than in a VM I think :)
Thats as good a spec I found. There are a few implementations out there. The Python one is the only one that can read binary comoressed files though so it's the best reference. 
&gt; s as good a spec I fou Right. And for the fst conversion, you're expecting a pure rust solution (is this possible?) or something like https://github.com/rustr/rustr to bridge the two.
fst is coded in C++ and I believe is just a binary format also no spec. Hmm, rustr would not ideal. If we cant decode fst then can we nust call the C++ code from the fst package for the save? The author of fst is real nice, could perhaps ask on github how to use the fst C++ codes
Actually any method is fine as long as its fast. I think the benchmark we will try to hit is being able to process a 11G sas file in 32 chunks 8 minutes (inclusive of saving as fst)
Self-hosting usually refers to being able to start a tool chain from a previous version of the thing your building. Often used about compilers when they are able to build their own code. In this case, I take to mean being able to build the next version of Redox from within Redox. https://en.m.wikipedia.org/wiki/Self-hosting
**Self-hosting** Self-hosting is the use of a computer program as part of the toolchain or operating system that produces new versions of that same program—for example, a compiler that can compile its own source code. Self-hosting software is commonplace on personal computers and larger systems. Other programs that are typically self-hosting include kernels, assemblers, command-line interpreters and revision control software. If a system is so new that no software has been written for it, then software is developed on another self-hosting system, often using a cross compiler, and placed on a storage device that the new system can read. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
I (and others) usually use the [byteorder](https://crates.io/crates/byteorder) crate for reading binary data. I've actually dealt with reading an IFF-like format before in [my project](https://github.com/crumblingstatue/rgmk/blob/5e31eee9741ed7b43254ed5c3881b280e13d4d24/src/serde/mod.rs), if you want to take a look.
Remind me to add an option to next RustFests dietary restrictions field labeled "I'm killercup, I take milkshakes as a side to noodles".
Took less time than anticipated predicted. https://youtu.be/eH5JgMlNE8o
Thanks for the insight!
😢
Isn't that C++20... maybe? He mentioned concepts, and how they *might* be in C++20, compared to what you can do in Rust today.
Nom is an amazing library and I highly recommend it for all your parsing needs. https://github.com/Geal/nom
WTF are fuzz bugs?! Seriously, this term "fuzzing" has exploded but nowhere can I find wtf it actually means!
I don't think there is a standard way yet. Relevant: https://github.com/rust-lang/cargo/issues/2729
You got three answers with three different solutions: An external build script (such as CMake), a build script that's used by Cargo, and `include_bytes!`. Which one is best depends mostly on how much processing these image files need. If there's no processing, you just want to include a file in the application, use `include_bytes!` for binary files (such as images) and `include_str!` for text files to be read as strings. If you want to do some light processing (maybe creating lower-res images from a high-res source), use a `build.rs` script. This is a normal rust program, where you can use Rust libraries and/or call external commands. If the processing is complex or time-consuming, so that you want the Make-like functionality of only re-running those tasks that are outdated, you may want to use an external tool. Plain Make, CMake, or really just a script in any scripting language that processed all the data file, and finally runs `cargo build` at the end. In your case, there seems to be no processing involved, so go with `include_bytes!`. 
Vimium managed to replace almost everything I used VimFx for, and I believe they're working on supporting the API's needed by VimFx: https://bugzilla.mozilla.org/show_bug.cgi?id=1215061
https://en.wikipedia.org/wiki/Fuzzing https://testing.googleblog.com/2016/12/announcing-oss-fuzz-continuous-fuzzing.html
Fuzzing is basically when you apply random inputs to find bugs. https://en.m.wikipedia.org/wiki/Fuzzing
**Fuzzing** Fuzzing or fuzz testing is an automated software testing technique that involves providing invalid, unexpected, or random data as inputs to a computer program. The program is then monitored for exceptions such as crashes, or failing built-in code assertions or for finding potential memory leaks. Typically, fuzzers are used to test programs that take structured inputs. This structure is specified, e.g., in a file format or protocol and distinguishes valid from invalid input. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Yep, you're right - it's dynamic borrowing. Not unsafe, but takes some extra thinking.
I wasn't clear, sorry. I did not think `BTreeMap::clear` would leak memory, that was just a general comment that I don't mind memory usage as long as it doesn't really get out of hand. The real question was about reusing the `BTreeMap` I'm carrying around, because I'll be regenerating the whole structure quite some times I think, but I'm afraid that without reusage the allocating will hurt performance too much. Also when making a new `BTreeMap`, I'll have a pretty good idea about the size required most of the time, so I was thinking of something like `BTreeMap::new_with_capacity`. No matter, I can do benchmarking when things work :)
Make sure you have any required fields in your target .json. IIRC `"target-c-int-width"` is a recent addition that the blog post doesn't mention yet. Not sure if there are any others.
Awesome, thank you so much! I promise not to slurp my noodle shake during the talks. (Do you think for breakfast there's an option to get Nutella, with some croissants on the side?)
As of today, Rust is used on hundreds of millions of desktops via Firefox. For a while now Rust has been used on hundreds of millions of desktops via Dropbox's client, and processes untold reams of data daily as the core of Dropbox's storage engine. So it's certainly met some baseline levels of maturity if massive companies are already willing to bet on it via integrating it into their core products. There are still certainly places where Rust shows its youth: crates.io only has 12,000 packages (I'd say 20,000 is the lower bound for a well-populated package repo); the compiler still needs elbow grease (compilation times are higher than they ought to be); tooling is still being developed (RLS should have a 1.0 release this year); and certain important aspects of the language are still only available in unstable (e.g. SIMD, which hopefully gets remedied soon). But for a whole lot of purposes it's perfectly usable.
That ain't canon: https://en.wikipedia.org/wiki/The_Book_of_Mozilla#The_Book_of_Mozilla.2C_12:10
[removed]
I'm talking about current implementation of Wayland: if you want to have remote display on Wayland, currently your only option is to add something on top of Wayland X11 or VNC or whatever. Currently pure wayland implementation doesn't offer remote display, X11 implementations do. That is a significant difference between X11 and Wayland, that's all I'm saying.
I’ve seen it in the wild quite a bit. Some popular consumer chat apps are written in Delphi, and gobs and gobs of financial software.
Watched the first vid on the bus today, and the audio level was fine, I was even kind of happy about the decent quality. Thank you for them :) Maybe someone should organize an event in Osnabruck ...
Wow, that is really nice to know that it could be supported in the future. I tried Vimium but it still far from VimFx, especially since I use &lt;C-[&gt; instead of &lt;esc&gt; to get out of focus/search bar/address bar. 
Yeah, that's a relic of the Chrome API not supporting that, which led to Vimium reimplementing its own address bar. Once the WebExtension API's are expanded, hopefully the VimFx will still have the intention to port it.
Given that this Servo code replaces an existing code base, couldn't we get a "guestimate" by looking at how many unsolved bug reports are now closed because their associated previous (presumably C++) code has been replaced?
Why is there a Safari icon?
Which you today need Nightly to do, with `std::sync::mpsc::Select`. 
I view parallelism as statistically independent execution, and concurrency as the decomposability of ordered or partially ordered computations.
Is "HolyJit" supposed to sound like "Holy shit!" or is it just me?
You sort of answered yourself: simultaneous and independent execution implies interleaved* execution while competing for shared resources. *parallelism is a superset of interleaving
It’s supposed to be a generic browser icon
I don't know BTreeMap well enough to say how it manages unused capacity, but it's quite possible that it always deallocates unused tree nodes. 
Thanks! AIUI, the dll file is modified when the detour is enabled, and restored when the detour is disabled. Is it restored in drop() too? What happens when I get a BSOD in the middle of the detour being enabled (or just power off)? So drop() won't be called and the dll will be detouring into nonexistent code from now on?
Not very generic if it immediately reminds you of a certain browser. Now, a fox curling around a blue ball, *that* would be generic!
/r/playrust
I wish that if Redox will implement dynamic linking, it will be completely optional. 
Looks like you're encountering [WouldBlock](https://doc.rust-lang.org/std/io/enum.ErrorKind.html#variant.WouldBlock) which is a normal thing to get when using asynchronous I/O. You should just retry the operation on the next event loop iteration if the error you get is of this kind.
But it's reading to the string all right, which is what I am confused with.
Well done! Switched one of my projects to that. One thing I've noticed that `link-dead-code` has no effect anymore, no good.
`read_to_string` probably tries to read everything from the socket, until it's closed. It might work for synchronous/blocking I/O, but `EWOULDBLOCK` means that the current data was consumed and you should try again later. Maybe see https://github.com/carllerche/mio/blob/master/test/test_echo_server.rs for an example.
Good job! Does this means you have to maintain a patched version of Cargo and rustc? 
“You can't prove a negative.” When people ask for a bug that Rust prevented, it's hard to show it, because there was never a bug.
Can't we just make Doge the generic sign for "Browser"? Servo already does it well: Doge + Rust -&gt; Servo.
I have added an http interface to my project to learn rust: https://github.com/abhijat/system_snapshot_server It uses iron to show the running processes on a linux machine over http in JSON form. I am looking into error-chain now to translate errors into a more uniform-ish interface through the program. Will work on adding disk and network usage endpoints next, probably.
Is there a reason that the upcoming events listed are always at least 2 days in the future at the time of publishing (seems to be the same in previous issues of TWiR)? Especially since a lot of people decide on their meetup attendance last-minute from my experience, this prevents people from finding events happening this evening like e.g. [the Rust Hack and Learn in Berlin](https://www.meetup.com/opentechschool-berlin/events/244893450/).
- "How about your favourite food? What would that be?" - ["Uhh, Milksnake!"](https://youtu.be/dUbx5TAKSdg?t=77)
I have a directory override to nightly, so that should work. I'm running it in the WSL, would that make a difference?
is the app written in Rust? :P
I'm the author of this. I wrote it very specifically for my [other ongoing project](https://github.com/maghoff/sausagewiki), so it is quite limited. But it might still be of use to others. Also, it could serve as a starting point for something [better and more general](https://github.com/rust-lang/cargo/issues/4657) in cargo proper.
It's because we publish the newsletter ~24 hours after we post it here (in case we find any issues with the post - wrong link, misspellings etc.). So, if the post is published on 14 November, and newsletter is published on 15 November, it makes sense to list events which happen on or after 16 November. (We list events for at least next 14 days, so every event is guaranteed to be listed at least once, as long as it is mentioned in the [calendar](https://calendar.google.com/calendar/embed?src=apd9vmbc22egenmtu5l6c5jbfc@group.calendar.google.com) in advance).
wait what? seriously?
Burn the heretic!
...and can you add a link to the sources?
Why is this page so slow? http://output.jsbin.com/surane/quiet 
sadly enough, no :(
Are you hiring in Germany or Scandinavia?
It is close. There is still work to do in the network stack to improve performance and reliability so that the crate index can be fetched in a reasonable time, and work to port some C dependencies in the build environment.
You should contact http://www.integer32.com/
The changes will be pushed upstream over time.
I have decided to create an open-source repo for this app to make it grow. https://github.com/TheUberCatman/crates-io-android
Ah okay, that makes sense. Still feels somewhat suboptimal :/
Continuing work on my resource management game. I got a preliminary tech tree showing: [Imgur](https://i.imgur.com/2S0Yld5.gifv). This week i plan to fade the rest of the game under a dark screen, so you can better tell it is paused. As well as start working on the actual tech to research.
With network sockets, you almost **never** want `read_to_string()`. Sync or async I/O, it will try reading until EOF which in case of sockets means until the connection is closed, just like /u/WellMakeItSomehow said. What you might wanna do is read chunk by chunk with a buffer of N bytes or read until a certain condition is hit which in the case of an echo server could be a newline received.
[palette](https://crates.io/crates/palette/) is excellent for messing with colour spaces
What /u/sjustinas is correct. An important thing to note regarding reading from NonBlocking sockets is that you shouldn't use the "blocking" API calls. Most of the `Read` API is blocking sockets. you can use them, but the API assumes that the source is going to block until more data is available or it gets an `EOF`. It doesn't expect the `EAGAIN` or `EWOULDBLOCK` results back from a socket. This Trait : http://rust-doc.s3-website-us-east-1.amazonaws.com/tokio/master/tokio_proto/io/trait.TryRead.html used to be part of Mio, but was moved to Tokio. 
[ponylang](https://tutorial.ponylang.org/gotchas/garbage-collection.html) GC looks interesting. An actor only gets garbage collected when it's done, which means the performance is deterministic AND concurrent.
You said "Wayland provides nothing" and not current Wayland implementations. As I said, not being network transparent is a good point, not a missing feature. From the user point of view nothing will change because of X -&gt; Wayland transition. Does Wayland care of less things than X? It's a good thing, compositors will implement features and eventually new protocols appear where convenient. There is nothing wrong in Wayland design. So what's your point? Useless speech.
Hey, I was also trying to get a simple `mio` server working last night! Maybe my basic attempt will help you ([here](https://github.com/jaemk/mini_http/blob/cb0301c92ebe1c2d6f284c877561bb1e92206b29/src/main.rs)). I was running into the same issue at first. Like others have said, you need to be handling the `io::ErrorKind::WouldBlock` case when you are incrementally `read`ing and `write`ing from your socket. I think the addition of an `Socket::Listener` and `Socket::Stream` also make it a little easier to understand. If you run my example, you can `curl localhost:3000` and have your headers echoed back. Side note: tomaka's [PR](https://github.com/tomaka/rouille/pull/144) for converting `rouille`s internals from `tiny_http` to `mio` was a big help for my understanding after reading through `mio`s docs. 
The main thing in my mind is to be aware of what you link against. By default your binary will dynamically link at least libc so you'll have to have all those dependencies on the host machine. For instance, running `ldd` on one my local binaries shows linux-vdso.so.1 (0x00007ffde0ffe000) libdl.so.2 =&gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fcbe71c7000) librt.so.1 =&gt; /lib/x86_64-linux-gnu/librt.so.1 (0x00007fcbe6fbf000) libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fcbe6da2000) libgcc_s.so.1 =&gt; /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fcbe6b8b000) libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007fcbe67ee000) /lib64/ld-linux-x86-64.so.2 (0x00007fcbe7636000) You'd have to do some work to statically link everything if you want to be able to just copy your release binary to a server, though it's possible.
You don't have to copy the entire folder over, only the binary found in the folder. It should be noted that if you don't have the same libraries on both machines the program might not (won't) work. If you need to deploy to an environment with different libraries you should look into musl.
It needs webrender to be fast. Webrender is not in firefox 57 yet, or at least not enabled by default.
I'm excited too, can't wait to build my personal apps for redox and eventually use redox in the long run.
Thanks for sharing!
Having written `cargo` handling tools myself and seeing your reading of the `Cargo.lock`, you might be interested in `cargo metadata` which makes reading of the dependency data a lot easier, and you might not need to access `github` at all.
The implication is fine but I don't agree with the conclusion. _Implication_ doesn't equate to _containment_.
Of course, that makes sense. Thanks.
I guess I can use a `while-let` and use a buffer to read until data is available. Thanks.
Isn't panicking at runtime on overflow/NaN basically free, if implemented as a branch that's hinted unlikely? It seems no more expensive then any of the other approaches proposed, and is (IMO) the preferred behavior. Yet, panicking was dismissed as being too expensive...
Oh. Thanks. I tried starting with tokio, but figured I have very little idea about the basics which was making tokio more complex for me. Also, the purpose of the different crates is kinda unclear to. Like I totally expected this trait to be part of tokio_core rather than proto.
Hey thanks man. I have been searching for some examples on using mio.
Nom comes with lots and lots of linked examples and implementations to parse various stuff. It has a very steep learning curve if you are new to rust, though, as the error messages produced by the compiler can be quite ugly. Parsers implemented in Nom are stupid-fast and can be built from small pieces, individually tested. I highly recommend looking at it.
If the code hasn't changed. I'd investigate if events are coming into your socket that you aren't expecting. If you're getting spurious data, it could easily explain why the socket keeps waking up and processing frequently. Your code only handles the happy path. I'd recommend handling all errors, bad data, etc. Have you tried injecting logging into the loops that you suspect to see what is waking them? In your loop you're handling one message type with the if-let. Maybe there are others.. 
Sounds interesting.
Has any OS had mandatory dynamic linking? Other than the necessity to "dynamically link" into the kernel via the syscall interface
After I turned on `gfx.webrender.enabled` in nightly I see only small improvement, it went from 8fps to 14fps (when chrome is able to provide a solid 60). So I guess webrender not finished yet.
I put data files in a data/ directory next to src/ which I just ship completely for packaging. I access the file by using ../data from the binary path, see https://github.com/jhasse/rust-opengl-test/blob/master/src/font/face.rs
are you me?
It's just a reference to the [common misconception that concurrency **is** parallelism](https://talks.golang.org/2012/waza.slide#8). Think of it this way: * Concurrency means *the ability to stop and start a set of tasks*. You can think of it as being a pause and play button for programs on your computer. It doesn't define how many things you can hit "play" on at a time -- just the fact that the buttons are there. * Parallelism is where *two or more things can literally run at the same time*. It means *you have play and pause buttons* **and** *that you can hit "play" on more than one thing at a time*. Most people think of parallelism when they hear "concurrency", because it has, in many usages, become a buzzword synonym. The distinction gets lost in many places, especially those where somebody is trying to sell something to you. A humorous example I've heard often is that *humans are concurrent and not parallel* -- if you've ever heard people joke that "Humans are terrible multitaskers", it's because...well, we switch rapidly between tasks, but we really don't do more than one at a time. I can make progress on more than one to-do given an hour, but that doesn't mean I did them simultaneously. How I wish that sometimes I were capable of parallelism -- two keyboards please! :)
It is the browser icon from the Moka icon theme - https://snwh.org/moka
I'm now logging the message when it's not `Text` but it's not logging any messages so I'm not receiving any non-`Text` messages (as expected). It's not even receiving any `Text` messages, no client is connected. What's weird is that the high CPU usage happens only after the program is running for some time (like a minute), not immediately. Whenever I restart it, it runs at low CPU usage for a while..
Can you run `procmon` when it does that?
That's an hour long. Shortcut to relevant timestamp?
&gt; what is the long-term intention or roadmap of the company regarding rust on microcontrollers? The company hasn't been a company since 2015 but Tessel lives on as an open source project. Rust on Tessel is usable if you have some embedded Linux experience, but hasn't quite gotten over the ease-of-use hurdles needed to attract a self-sustaining community like the JS side has. I'm pretty sure the steering committee would still love for Tessel to have first-class Rust support, but hasn't had the resources or experienced contributors needed to get it there. The main barrier right now is that it requires a custom uclibc-based toolchain. In the repo you linked, you'll find various tools and instructions for using a glibc-based libstd anyway (mostly works, but unsafe), distributing uclibc libstd binaries for Rust stable (not kept up to date), and an attempt to sidestep the install process by building in a Docker container on a VM or cloud server. Nowadays I'd use [xargo](https://github.com/japaric/xargo) to build a custom libstd with nightly, and that's probably where you'll find the most up-to-date documentation for toolchain setup the moment, even if it's not Tessel-specific. The path forward is updating the OS to LEDE, which would be compatible with the `mipsel-unknown-linux-musl` target you can get with `rustup`. (You'd still need a cross-gcc for linking, which is easy on a Linux host, but harder on other platforms). I [started on this](https://github.com/tessel/openwrt-tessel/pull/72), in May, but it's currently blocked by a wireless driver regression that I haven't had time to investigate.