Hm, there are some neat ideas here but there may be more useful questions to ask about our bots. bors purpose is not at all to make PRs land faster. bors purpose is to keep the build from breaking. I might even expect bors to make landing PRs slower, so that it didn't appear to make the integration process slower is kinda surprising really. A more interesting question to ask about bors is whether it is successful at keeping the build from breaking, and whether it succeeds enough to be worth the effort of slow integration times. Another interesting question might be whether and how bouncing off bors discourages contributors.
D'oh! I had meant to come back and write those docs before anyone noticed. :P 
In cases like this, it is best to embrace the async nature of what you're doing and program that way. I would spawn a thread, then use something like `mio` to create an event loop that uses `TTYPort`'s internal FD to watch for read/write notifications. When you get notification of something to read, then you read as much as you can, and can pass the data along via a future, a callback, etc. The method isn't important, it's up to you. When the program wants to write something, we append to a buffer that we control (and protected by a mutex). Next, wake up the event loop somehow (a pipe is a nice way to do this but there might be something more ergonomic). When we've awoken due to activity on our pipe, we drain the pipe, acquire the lock on the buffer, and write as much as we can to the serial port. If there's still some data left over, we await a write notification from `select()`, and adjust our buffer accordingly. Biggest benefit of this approach: the objects used to communicate with the serial port are thread-confined, making it impossible to cheat or use incorrectly.
From my servo new-contributor experience, the discouraging part of submitting to bors is the likelihood of being rejected for merge due to some intermittently-failing test. On the other hand, having the centralized CI testing is really satisfying in that it doesn't matter to merging whether it runs on someone else's random machine, just whether CI likes it. For my work on WebGL, which varies a lot between HW, that's been *really* nice. I think the best improvement to bors interaction would be to ruthlessly disable intermittent tests. It would probably be a big win on the time-to-merge metrics, too. Another possible improvement without ruthlessly disabling intermittents, would be to allow any submitter whose PR was r+ to request a retry if their code hasn't changed, even if they don't have review privs
Hi Carol &amp; Steve, would like to suggest to add a pinned post about the resources to learn Rust to /r/rust . This might be easier to newcomers to find good resources to learn Rust :)
In addition to Gankros benchmark that was linked upthread, [this benchmark](http://aras-p.info/blog/2016/08/09/More-Hash-Function-Tests/) made recently the rounds around here too. The most interesting thing there imho was the differences between different platforms.
That'd be great, but I'm not a mod here! You'll have to ask one of them :) There is a "Learn" link in the sidebar?
For those curious into the numbers, the output of `perf record -g` can be found here: http://downloads.yorickpeterse.com/files/reddit-5doyo9-perf.data. SHA512 (for the paranoid): 27ec98356f8e26f062dc629d4e43dd06be0c054be29b6acf1898795eb4ffc24f773f273039edf1259bc341219f7a51bcafcfcdfe93b80e9fc3339f627845eb8c Copy-paste from the ncurses UI: Samples: 10K of event 'cycles:u', Event count (approx.): 6186199556 Children Self Command Shared Object Symbol + 99,74% 0,01% playground playground [.] __rust_maybe_catch_panic + 61,31% 0,00% playground libc-2.24.so [.] __clone + 61,31% 0,00% playground libpthread-2.24.so [.] start_thread + 61,28% 0,00% playground playground [.] std::sys::thread::Thread::new::thread_start + 61,28% 0,00% playground playground [.] &lt;F as alloc::boxed::FnBox&lt;A&gt;&gt;::call_box + 61,25% 38,49% playground playground [.] std::panicking::try::do_call + 38,48% 0,00% playground libc-2.24.so [.] __libc_start_main + 38,48% 0,00% playground playground [.] _start + 38,47% 29,54% playground playground [.] playground::main + 38,47% 0,00% playground playground [.] std::rt::lang_start + 31,46% 31,46% playground libc-2.24.so [.] __memmove_avx_unaligned_erms + 23,71% 0,00% playground playground [.] &lt;alloc::raw_vec::RawVec&lt;T&gt;&gt;::double + 23,70% 0,01% playground playground [.] je_arena_ralloc + 23,70% 0,00% playground playground [.] rallocx + 23,65% 0,00% playground playground [.] je_huge_ralloc 0,17% 0,17% playground [kernel.vmlinux] [k] page_fault 0,10% 0,01% playground playground [.] je_huge_dalloc 0,09% 0,00% playground playground [.] je_arena_chunk_dalloc_huge 0,09% 0,03% playground playground [.] arena_maybe_purge_ratio 0,06% 0,00% playground playground [.] huge_ralloc_no_move_expand 0,06% 0,00% playground playground [.] je_huge_ralloc_no_move 0,06% 0,01% playground playground [.] je_arena_chunk_ralloc_huge_expand 0,04% 0,01% playground playground [.] chunk_recycle 0,04% 0,00% playground playground [.] arena_unstash_purged.isra.37 0,03% 0,01% playground playground [.] je_chunk_alloc_wrapper 0,03% 0,03% playground [kernel.vmlinux] [k] apic_timer_interrupt 0,03% 0,00% playground playground [.] je_chunk_alloc_cache 0,03% 0,00% playground playground [.] chunk_record 0,03% 0,00% playground playground [.] je_chunk_dalloc_arena 0,03% 0,01% playground playground [.] arena_stash_dirty 0,03% 0,00% playground playground [.] je_tcache_alloc_small_hard 0,03% 0,00% playground playground [.] je_arena_tcache_fill_small 0,02% 0,00% playground playground [.] je_tcache_cleanup
I love these blog posts and am envious of projects with pretty pictures to show off in their updates. I wish rustc contained a renderer...
It depends on how complicated the architecture is really. There was quite a lot of trouble with AVR due to its "weirdness". The are a lot of assumptions within LLVM that can make code generation tricky for more exotic MCUs. The story for 32-bit chips is quite good though. You can probably write a backend and get something compiling within a full weeks work, but it will likely take longer to get to know all of the LLVM internals. Normally the easiest way to start is to copy-paste an existing backend and modify it. 
For this who don't know, Graydon here is the one who originally conceived of Bors, after spending all of his time each week manually performing the task that Bors now does automatically. :) I remember the sheer chaos that a broken build used to result in, and the complete and total lack of chaos since then is astounding. Bors was a silver bullet. FWIW I agree that these are the wrong metrics to focus on if one is to attempt to gauge the usefulness of the bots, but the post is still interesting for what it does reveal. I'm in agreement with brson that it's surprising that Bors didn't end up making the integration cycle slower, since I recall that being a concern at the very outset ("what do you mean we have to run an integration test on *every single* merge??" (and indeed we did end up adding rollup functionality to Bors eventually)).
They slot in between AVRs and Cortex-Ms in terms of weight class for me. For when I don't want the headache of an ARM but need more power than an ATMega. The new FRAM MSP430s are a great alternative to MCU + EEPROM, as well.
OK, I wrote something: &lt;https://github.com/nikomatsakis/rayon/blob/master/src/par_iter/README.md#what-on-earth-is-producercallback&gt; =)
This will also be handy for people using fast connections but slow network file systems. Installing the documentation can take a very long time because there are so many files.
The Q&amp;A thread would be a better place for this question. Sorry üòê 
Agreed that they're not pretty, but I think they're easy to remember at least. If you have a better idea on names I'd be glad to hear! I just wanted something short and easily derived from standard macro names. I've tried prefixing and suffixing (like `sprintln` or `kprintln`), but these names were not pleasant to type and I wanted to avoid name clashes (many prefixes have their own meaning already). And in the end, `xformat` or `formatx` is just as arbitrary soup of characters as `fomat` in my eyes.
Are you planning on contributing back the performance improvements into the compiler?
&gt;nor do we have to manually allocate/deallocate memory ourselves such as in C and **C++** RAII is a thing in C++
Can you show the whole error?
I used Rust primarily because I wanted to keep memory usage low (millions of ballots), and make it run fast, and it paid off! In the process I also learnt that in the way of bignum libraries, Ramp and GMP are much faster than the standard `num` crate. In the end this didn't really matter as I was able to drastically cut down the number of multiplications I needed to do, and the size of the numbers involved (thanks rounding!).
Really nice, especially the inline conditionals and loops. And I think the names are cute :)
This ticks an alarming number of my nerd boxes.
Is [this](https://play.rust-lang.org/?gist=a4732a0b3844b14204bdb6580630f51e) what you want?
&gt; pretty quickly. Depends on how often methods are added. Maybe instead of doing it for every change, decide based on Crater run. If too many projects implemented conflicting method, put it into extension trait. If not so many, just change it.
Thank you for the comprehensive answer!
Amazing, what you did there is most likely one of the best things you could do for a democracy. I wonder how many people would realize that.
Here it is Compiling tin_can_telephone v0.1.0 (file:///home/kibbles/Projects/rust/tin_can_telephone) error: cannot borrow immutable borrowed content as mutable --&gt; src/server.rs:86:21 | 86 | clients.get(&amp;id) | ^ error: aborting due to previous error error: Could not compile `tin_can_telephone`. And the source code is [here](https://github.com/colindjk/tin_can_telephone/blob/master/src/server.rs). (Side note, if I put the 'get.(&amp;id)' on a new line, the error still points to the clients variable, if that means anything. Thanks!
&gt; Can you use a thread pool and divide the call frames roughly equally between them, then join the resulting Vecs in an Vec&lt;Vec&lt;T&gt;&gt;, rather than dividing each call frame across multiple threads? I have attempted this using crossbeam's Treiber stack and its Chase Lev deque, both cases were not faster than the serial version (regardless of the number of threads). And you understand the question correctly, the idea is to scan all frames faster; somehow.
This is really neat! Any interest talking about it in a future [Bay Area Rust](http://www.meetup.com/rust-bay-area/)? We can handle remote speakers if you aren't local. It sounds like a great demonstration of building a data pipeline to solve interesting real world problems. 
/r/playrust
I've actually used the `s` prefix at the beginning and even tried to think about it as "super" too! But when I've looked at the actual code, I couldn't stop myself from thinking it's some kind of C code ‚Äì `sprintf` is used in C for printing to a buffer. So maybe a `k`? Now I think I'm looking at the kernel code. Also, the names like `sprintln` are kind of heavy on my eyes ‚Äì they have both prefix and suffix and thus require some mental effort to parse. But of course, it's a personal preference. &gt; Omitting a letter makes it look too much like a typo... That can be something positive! (or I'm just trying to rationalize current choice) The fact it's a typo means that when skimming through code, you don't even notice there's a typo! And that's good, because eg. `pintln` is not a macro that does something else (like `kprintf` or `sprintf`), but actually just another syntax for `println`. The fun thing is that most of `print` invocations are illegal `pint` invocations and vice versa! The only exception are single literals, which work the same way in both macros (except of "{{", sadly). So, when the proper macro scoping comes to Rust (thanks for mentioning that!), I'd probably just change the names to be exactly the same as original, so it can be a drop-in replacement (it would just require simple logic "if there's a comma, use `std` version). Anyway, thanks for the opinion! But I think I'll stay with the current naming, as it looks like some other commenters seem to like it. You're free to publish a crate with wrappers though :)
[removed]
Anyway, this still doesn't fix the problem because you will have - glob imports being broken when new types are added to a module - adding an trait impl of an existing trait (which people may have imported) to an existing type can cause this same kind of conflict Ultimately it's impossible to completely plug up these kinds of breakages in any language with a static type system without freezing the stdlib.
Try [these docs](https://manishearth.github.io/rust-internals-docs/std/index.html) instead, they're updated very regularly.
Small apps that are meant to be used by a single user at once, and dealing with a known amount of data, eg: apps to extract information from SSL certs, apps to update a "console" via REST calls, etc... Copied from my Ask HN thread that got zero traction: &gt; I tend to use Python as my weapon of choice, and love how much, most of the time, I can ignore that it runs on a computer and focus on my problem domain, but I run into two issues: &gt; &gt; ‚Ä¢ performance: I've used different solutions to this, sometimes better native library, sometimes pypy, sometimes C for the whole thing, sometimes a different strategy, etc... but I have started to learn Rust to address this space and it seems to be the perfect fit. &gt; &gt; ‚Ä¢ distributing to other users / dependencies: When I need to have other people use my scripts, then I run into the issue of having to help them install packages, and often on platform I don't know (different versions of Windows, etc...). This is where I am wondering if I Haskell would have and advantage over Rust. 
Could you give the errors you're getting, or provide the complete code on the Rust Playground? Either one will make it easier to help.
I use Haskell at work and Rust in my spare time. I find I often prefer Rust for many of my side projects because I love the language. I do like Haskell and find it easier to express certain things in it rather than Rust. I find I'm a bit more productive with Rust. However, given your comment on HN I would probably recommend Rust and the musl target if you want to distribute static binaries that can be run elsewhere. Getting Haskell to run well on windows is a huge pain. That being said if you'd like to use a mix of both I'm the author of the [Curryrs library](https://github.com/mgattozzi/curryrs) which is working on making FFI between the two easier. From your answer though that doesn't seem like the most ideal.
I am not an experienced Haskell programmer, so take my comment with a grain of salt. However, I feel like Rust is even better to spot errors at compile time than Haskell. Also, I think that debugging a Haskell program is a nightmare (have you ever tried `Trace`ing a Haskell lazy function :) ?).
The breaking change happened when the algorithm was cracked. Pushing the update helps the compiler catch dangerously broken code.
I've just noticed that the `pintln!` is not captured by `cargo test`.
[removed]
&gt; Getting Haskell to run well on windows is a huge pain. Ah! I hadn't realised that, that makes it kinda useless for me. Thanks!
I'm looking for answer from somebody who's given Rust a really good chance, and thought these people are probably hanging out here!
Totally, but the ones here are more likely to have had a positive experience, whereas the ones who aren't here may have solid reasons for why they gave it a shot and went back to Haskell.
You could always go with F#, if you want something that kinda looks like haskell :P
Wrong link, I believe... but nice Hello World
Having once visited a Mozilla meetup in SF a few years ago, I would love to! It'd have to be remote though 'cause I'm in Sydney for the foreseeable future :)
I think it would be a shame if you kept the "remove the r" scheme. Honestly I really like the idea of these macros but I couldn't bring myself to use them as-is because of the naming. They read like total nonsense to me even knowing their origin. I'd have to rename them every time I used them. If you needed a scheme based on the `std` ones I think shortening without the vowels would read much better: `wrt!`, `wrtln!`, `fmt!`, `prnt!`, `prntln!`. Even though the `ln` variants get a little "soupy" they still evoke their purpose. But may I suggest not basing it of the `std` names and going more semantic with `out!` and `outln!` for `stdout`, `err!` and `errln!` for `stderr`, then `fmt!` for formatting because it's such a common shortening anyway. I'm less sure about generic writing, but `wrt!` and `wrtln!` strike me as reasonable shortenings, with `buf_write!` and `buf_writeln!` for longer but clearer names.
You outlined precisely the issues I have with Haskell. I love the language itself, and truly think a language like it is the future of high-level programming. But the path they're taking Haskell as a community isn't sustainable in my opinion, for exactly the reasons you listed.
I've always just assumed that newspapers and human rights organizations double check these computations after every election, but now that I come to think of it, I've never actually heard of anyone doing it before... 
Thank you for pointing this out, I forgot I created this crate with plans to move ZDT1 out of simple_ea and implement the rest of the ZDT test suite. I've updated the link
Ok thank you!
This code seams a bit strange. You pass ```dom``` as mutable but never mutate it. You mix the code for finding a node and inserting in one function. You should create a find function that takes a name and returns a node. than you call something like: ```find("head").insert(&amp;mut dom)``` That would simplify things and at the same time create a reusable find function.
it's mutated if the head is found if found { insert(&amp;handle, &amp;mut dom); } &gt; You mix the code for finding a node and inserting in one function. it calls itself recursively, not sure how to rewrite that without upsetting the borrowck
Exactly. If you separate, find and insert, no mutation needed
Heatseeker is very fast, but I suspect that most of the fuzzy finders out there that compile to native code are fast enough in the [loper-os](http://www.loper-os.org/?p=300) sense. Even Selecta (written in Ruby) is a lot faster than it was when I started working on Heatseeker. I think you're right that the main selling point is actually portability, since Heatseeker is the only fuzzy finder I'm aware of that runs on Windows--not just PowerShell, but even cmd.exe. (There's no way to integrate with the actual cmd.exe prompt like you can with zsh and PowerShell, but you can invoke hs directly or [use it from within vim](https://cloud.githubusercontent.com/assets/3725049/8273517/2a2f9afa-1826-11e5-9e1e-a15e84751bd0.gif) and it will just work.) I've also made an effort to try to nail down some things that are weirdly hard to do in POSIX terminals, such as SIGWINCH handling and arrow key support. I've found that the Windows terminal is actually significantly easier to program against! One other subtle selling point of Heatseeker: a Linux binary that actually works. I use the Rust toolchain's musl support to ship a statically linked `hs` binary. I've run this binary on truly ancient versions of Linux (think RHEL5) without any problems, so there's really no need for Linux users to compile from source or install through a distro-specific package manager. It's interesting to compare the static and dynamic versions of hs to see how many libraries the latter requires: ~/src/heatseeker # ldd target/x86_64-unknown-linux-musl/release/hs not a dynamic executable ~/src/heatseeker # ldd target/release/hs linux-vdso.so.1 =&gt; (0x00007ffd44ff7000) libdl.so.2 =&gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f2ceb38d000) libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f2ceb170000) libgcc_s.so.1 =&gt; /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f2ceaf59000) libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f2ceab90000) /lib64/ld-linux-x86-64.so.2 (0x0000561c16566000) 
This looks like a lot of work that `rustup` does for you these days. I used to build [Heatseeker](https://github.com/rschmitt/heatseeker)'s musl binary against a manually installed musl build using custom build scripts, but now I can just install the musl target automatically: rustup target add x86_64-unknown-linux-musl and then compile: cargo build --release --target=x86_64-unknown-linux-musl I also don't recommend trying to cross-compile from OS X; as you've already discovered, it doesn't really work due to linker issues.
I realize this is tangential, but... &gt; In a particular use case of [rust-mmap](https://github.com/rbranson/rust-mmap) To my knowledge, the `mmap` crate isn't maintained. You should use the [`memmap`](https://crates.io/crates/memmap) crate instead.
&gt; I've also made an effort to try to nail down some things that are weirdly hard to do in POSIX terminals, such as SIGWINCH handling and arrow key support. I've found that the Windows terminal is actually significantly easier to program against! Now I want a library to handle those terminal things in a portable manner..
&gt; So you don't like the "removing the r" scheme and you propose to remove all the vowels instead ;) I prefer removing all the vovwels too. Do you know that, there are [languages that don't write down vowels](https://en.wikipedia.org/wiki/Abjad), but no language omitting consonants while writing vowels. 
Oops! Thanks for this, I've removed the reference to C++ in the article now.
ndarray should provide what's necessary for dealing with a matrix efficiently.
Thanks for your comment. I assume you're referring to the way I'm doing my fake shadows? For those I'm using whatever interpolation is being done between vertex colors by default... whatever that is. I assume it's being done in sRGB. Don't worry, though. It's a temporary solution just to help with visibility while I figure out what I actually want. I'll keep your advice in mind. :)
Thanks. :) Rustc might not offer as many obviously pretty things to demo, but I imagine there would be a ton of possible visualisations of its internals that could be both useful and aesthetically appealing. I'm thinking of things like performance graphs, visually mapping between progressive representations of a Rust program (source through MIR, LLVM, machine code), or tracking invalidation of cached artefacts in incremental compilation. The biggest question in my mind is whether they'd be useful enough to justify spending time on them. I.e. would they be helpful learning or diagnostic tools, or just be nice to look at?
This works fine for a pure Rust project, but I think the OP has the added complication of additional C dependencies.
Thanks, I had looked them, but didn't understand enough of them, at first, to get the pq-sys crate link statically. They were of great help, though, as I was able to transition from cargo-culting to understanding.
As someone brand spanking new to Rust (and lower-level programming in general, I've been working with PHP/JavaScript), I just want to see if I get it. A lifetime explicitly declares that borrowed values in the same lifetime have to remain accessible and unchanged for the same amount of program time. By forcing developers to handle this task, rather than an automated process, Rust saves on resources and is guaranteed to be memory safe at compile time. Close? Way off base?
AVR has 16-bit pointers and so load/stores are done with 16-bit addresses. Almost every other instruction deals with 8-bit values. If you set the target pointer width to 16, LLVM then assumes that `i16` is a permissible, legal type. If we have a 32-bit add operation, LLVM will expand it into two 16-bit add operations. What we want for it to do is to then expand those two 16-bit adds, into four 8-bit adds, which is then directly representable with the `add rd, rr` instruction. What it does instead is stop at 16-bits and gives up because "i16" is a legal type. It doesn't understand that it's only legal for memory operations. &gt; Approximately how long did it take you to implement AVR backend? I picked up the project a few years after it went stale ([here](https://sourceforge.net/projects/avr-llvm/) it is on SourceForge). I don't know how much time was used on it previously, but it was able to create assembly programs for a some fairly complex programs. It took about 4 days to add basic ELF object file support, took 10 hours to add tests for every single possible instruction. 
It probably wouldn't be too much different to be honest, if you look at [an example](https://github.com/avr-llvm/llvm/blob/avr-support/test/CodeGen/AVR/add.ll) LLVM IR file, it's not much higher-level than C. There's an LLVM project named DragonEgg (which has been basically stale for 6+ months now, still works with LLVM ~3.4) that will basically convert LLVM IR into GCC IR, allowing GCC backends to be used with LLVM. Assuming that esp8266 actually has a GCC backend, which it no doubt does.
If portability is so inportant, why not just write it in mruby? It's basically ruby that you can compile to executable form.
**How do I convince my thesis advisor that using Rust instead of C++ is "safe"?** Currently I have a core algorithm in my .NET program that I need to make really stupid fast, which initially lead me to write it in C++, export simple C interface and call it via FFI (C++/CLI doesn't feel like the right tool, since the algorithm is generic enough to be used outside of the .NET app). This though has lead me to consider using Rust instead of C++, since they'd both export the same interface, and I might learn something new in the process if I used Rust. I haven't tried talking to him yet, but he's quite conservative in terms of tools, and picks ones that are best suited for the job, even if they're ugly in other ways. I can already hear the _nobody except for a few researchers at mozilla and a few small companies are using it_ argument.
&gt; I haven't tried talking to him yet, I think you are worrying for no reason. Just do it. If you try and fail and he has specific reasons to reject your idea then you can try to come up with things to counter his argument. Until that it is just useless worry. 
Off the top of my head, runtime errors you don't get in Rust: uninitialized record fields, record field accessor functions and ADT, less non total functions, no exceptions, incomplete pattern matching. So for my day to day programming these seem to matter more than the IO safety you're getting from Haskell. But this certainly also depends on your use cases.
Good observation, I'm on OSX so I'm using clang by default.
Thanks, I'm not sure how I missed that when reading docs. After changing to `weak_rng()` my performance is on par with C++. I guess this is what I'm going to use in my subsequent experiments.
I didn't do anything fancy, I just used `time` and later followed instructions on this blog: http://carol-nichols.com/2015/12/09/rust-profiling-on-osx-cpu-time/ Though after switching to `weak_rng()` as u/zzyzzyxx suggested my performance is much better, so it probably doesn't make sense to dig deeper into this code.. anyway, thanks!
SML in 2020! ;)
You mean implementing `Fn`? [That's already in there](https://github.com/matthiasbeyer/filters/blob/master/src/impl_traits.rs) for building with unstable rust! :-)
Yeah.. Because with rust you have a lot of different compilers and lots of choice.. Don't get me wrong... I really like haskell and looking at rust but your argument seems... meah... EDIT: I prefer to have a language and community around a standard implementation (Haskell GHC, Rust Compiler, Python CPython, Go etc) than a mess. I came from Common Lisp to Haskel because of that! 
&gt; I'd probably just write a DSL in F# that compiles to C That's an insanely bad idea. Generated C code, once it gets complicated, can easily invoke UB and be infeasible to audit for it. At least have your DSL target an actual compiler backend, since the ways to invoke UB in those are better documented.
`unwrap` is same as incomplete pattern matching. Ghc can detect incomplete pattern matching or uninitialized record fields, emit warning and treat it as error. I am not sure what is difference between exceptions and panic or what do you mean by record field accessor functions and ADT.
/r/playrust
I am currently tying to parse a Vec&lt;str&gt; into a Vec&lt;char&gt; but it wouldn't work. for x in arr_cor{ let elem: char = x.parse().ok().unwrap(); result.push(elem); } This is my code, it says "trait 'char: std::str::FromStr' not satisfied" as an error. 
`vec.reserve(8); // now capacity of 16 because vector empty and 8 will be multiplied with 2` Reserve 8 returns capacity 8. rustc 1.13.0 (2c6933acc 2016-11-07) capacity vec:0 capacity vec after reserve(8):8 capacity vec after clear:8 capacity vec after clear and shrink_to_fit:0 vec:[] 
C++ code used c-style arrays. I've tried switching to ndarray as u/staticassert suggested and the performance is roughly the same... but it's on par with C++ now so I guess I'll stick with it (as it looks cleaner than `Vec&lt;Vec&lt;...&gt;&gt;`). 
Actual internal algorithm (unless someone has updated it since I wrote it): reserve(cur_cap, target_cap) -&gt; new_cap: if cur_cap &gt;= target_cap { cur_cap } else { max(cur_cap * 2, target_cap) } (but with a lot more very careful overflow/oom reasoning) reserve_exact just simplifies the `else` branch to `target_cap` Edit: oh and there's some special logic so that you go from 0 to 4. https://doc.rust-lang.org/src/alloc/up/src/liballoc/raw_vec.rs.html#201-241
AFAIK it also doesn't work on windows.
It's weird that they haven't seem to run their own test programs, which reveal that their understanding of reserve behaviour is incorrect. e.g. let mut vec: Vec&lt;i32&gt; = Vec::with_capacity(5); vec.reserve(4); // still capacity of 5 println!("capacity of empty vec with initial capacity of 5 after reserve(4):{}", vec.capacity()); vec.reserve(6); // now capacity of 12 (6 * 2) println!("capacity of empty vec with initial capacity of 5 after reserve(6):{}", vec.capacity()); Prints 5, 10. (they say it prints 5, 12) Also note that `reserve` is *relative to the current len*. If you call `reserve(100)` you are saying "I'm expecting to insert 100 more elements, make sure that I have space for that". This is why push/insert have historically been implemented to invoke `reserve(1)`.
The `memmap` crate *absolutely* works on Windows. If it didn't, I'd have lots of angry users knocking down my door. :-)
To give one more example: Œª let val = 40 in let val = val + 20 in print val Evaluation killed! There are perhaps some options to give to the compiler to warn about it but Rust will give at least a warning in a similar case.
Okay so I do admit that I posted a bit too quickly because I was getting frustrated. I found that bit on the internet but I'm not happy with taking a byte slice out of a string when the size of a char might not be the same every time, and taking a substring of chars and not bytes is proving more difficult than I thought it would be! I am new to coding rust but I will get it right
The fixed almost all of the exponential blow ups in the latest releases. This being said git has won completely. Which makes me sad because patch theory is awesome. 
Yeah I think my problem is less that there's one compiler and more that the one compiler that's available is slow, has huge memory usage, and doesn't support cross compiling - at least not for a great deal of hackage code. The emphasis is more on new language features and less on performance and portability. Turns out that balance wasn't what I needed for my projects. 
In the future you can use a tool like cachegrind to measure cache misses. You're likely to see a nice drop in cache misses after switching to ndarray.
"The general public" is not really a good measurement stick, anyway. Subtle algorithms won't be accessible to the general public, no matter how you write them, because "the general public" isn't trained to deal with complicated, exact arithmetic.
&gt;&gt; For projects where performance and memory usage is not an issue I will use Python, not Haskell ;)
This I can not disagree with, it's the same when you want some performance metrics, with the annoying dependencies! I'm not well versed on exception handling so don't know if better ways are possible these days, might be. 
Why are you multiplying slices on line 22?
This looks wrong: add = add + self.data[i * left_num_cols + k] + rhs.data[k * right_num_cols + j]; (Hint: "k" is looping over the columns, not the rows)
`RangeToGeneric` is the name for two *different* but related things: * The trait `RangeToGeneric`, which is used as a trait bound on *types*, as in `G: RangeToGeneric`. * The "trait object" `RangeToGeneric`, which is a type, not a trait, as in `r: RangeToGeneric`. The former is used to specify things about your types without specifying the type exactly. The latter does specify the type exactly, and refers to an "unsized trait object". A trait object is an unsized type. This means it can only be used behind so-called "fat pointers". Other examples of types that need to be behind fat pointers are slices (`[T]`) and some strings (`str`). These are unsized because the size is not known at compile time. A fat pointer consists of a normal pointer and one auxiliary element. In the case of strings and slices, this is their length. In the case of trait objects, this is a "vtable". The pointer points to *some type* `T: RangeToGeneric`, and the vtable is a special piece of memory that tells you how to use `T` as a `RangeToGeneric` if you've forgotten what type it is. So asking for `r: RangeToGeneric` is like asking for an `[i32]`. You can't accept an `[i32]` because you don't know how long it is. Similarly, you can't accept an arbitrary `RangeToGeneric` because you don't know how big it is or how to call any of its methods. Instead you need some kind of pointer, which will be magically converted to a fat pointer: `&amp;RangeToGeneric` or `Box&lt;RangeToGeneric&gt;` are the obvious choices. The downside of this is that you're not getting monomorphisation - the tool that makes each function specialized to the types it accepts - and instead have to be indirect through a pointer. This is normally a bad thing, and since `Box&lt;RangeToGeneric&gt;` can implement `RangeToGeneric` normally the choice is left to the caller and use generics in the function.
&gt; Generated C code, once it gets complicated, can easily invoke UB I would have said that generated code has a higher chance of *not* having UB in it, compared to hand-made code. And at least, you can prove the subset of C you are compiling to doesn't. 
Thanks :).
Thanks. Yeah, I have looked at what you've pointed at before, but I haven't really had a strong desire to want to move other pieces around while this project is so hot.
IIRC the main reason is that signatures for the operations needed by functors/applicatives/monads are different for different types. Consider the monadic bind operation for [`Option`](https://doc.rust-lang.org/std/option/enum.Option.html#method.and_then) vs the one for [`Iterator`](https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.flat_map). The former takes an `FnOnce` and returns `Option` while the latter takes an `FnMut` and returns a new type wrapping the old one. While it may be possible, I don't immediately see a good way to unify those without giving up safe mutability and/or moving everything to heap and accepting the extra indirections.
Well, I know that I can forbid the crate, but I would like the ability to turn it on passively when running `cargo test` so that others don't have to remember or get a warning.
Thanks for all your help guys after reviewing how adding matrices works with different rows/cols, and making more test cases I got my program to work without bugs. 
You're hint helped a lot thank you!
The range as parameter thing is exactly for that goal. To make the library easier to user, because working with ranges is easy and intuitive. I just didn't know if was possible so I did this experiment. My goal is to write a parser combinator library that takes a Vec of parsers and ranges specifying how many parsers should be applied to the input buffer. So you could write something like this: ```buffer.many(1.., Vec![p1, p2, p3])``` Where p1, p2 and p3 are parsers. And that would appy one or more parsers to the input buffer. Its still just a concept in my head, but it looks interesting.
Tnx for all the Info, have a nice day Sir.
The idea of having ranges as function parameters is a nice feature to have. Good to know its on the way. I'll keep an eye on it, maybe it will get into to stable soon. Regardless, this was a fun and instructing exercise.
&gt; LLVM IR file, it's not much higher-level than C. Seems lower level to me... :) &gt; There's an LLVM project named DragonEgg (which has been basically stale for 6+ months now, still works with LLVM ~3.4) that will basically convert LLVM IR into GCC IR, allowing GCC backends to be used with LLVM. Wow, that might be good idea too!
I agree. It's exactly because of this I don't like using glob imports (maybe except enums and within crate). &gt; in any language with a static type system without freezing the stdlib Is this your opinion or mathematically proven thing? I can't believe it's impossible (although, the consequences might be worse than breakage).
There are pretty straightforward answers to your questions: 1. You can use `flat_map` to flatten any `Iterable` results of your mapping closure into a single vector: [flat_map](https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.flat_map). The `map` function does not do any flattening itself. 2. This code constructs a new, **owned** string using `String::new()` which you populate and return directly, passing along ownership of that string. 
2) as far as I can tell you are putting it on the heap and so passing it out is perfectly fine. just an fyi, lots of ways the hyper client request into string you are doing can fail which has nothing to do with the hyper side. Getting a bunch of text from the web is an easy fail when converting to a string. i had this issue when web scraping a largish website full of text.
Aha thank you - I think the point I was missing for #1 is that it acts like a stream of `Vec&lt;CsvTable&gt;` which is why when using `.and_then()` collects them to `Vec&lt;Vec&lt;CsvTable&gt;&gt;`. And for #2 when you say it's an owned string being created, do you mean that it's created on the heap? Thanks!
I understand that all strings in rust are heap allocated, except static strings. By "owned" I mean that you have full control over the string - you can mutate it, loan out mutable or immutable references, or drop/free the memory it holds. You can also pass along ownership of the string e.g. by passing it to or returning it from a function, as you do with the line `Ok(buf)`. The inverse is "borrowed". E.g. If I have a function like: ``` fn foo(buf: &amp;mut String) { /* mutate buf */ } ``` I can mutate the string and hand out mutable and immutable references to it, but (most importantly) I cannot drop it. Only the owner (whoever gave me the reference to the string) has control over the string's lifetime. If I rewrite the function like so: ``` fn foo(buf: String) { /* do stuff with buf */ } ``` Then I am taking ownership of `buf`. When this function exits, the string will be automatically dropped, unless I pass ownership along e.g. by passing it to another function or returning it. *Edit: Added some notes about borrowed strings.*
Sometime ago I did create this page http://daily-rust.github.io/2016/09/01/videos.html to add videos about Rust. This site will be specific for that. Don't have many videos, but I will add with time. Any contribution will be welcome =)
This just gets me a reference to it, right? Doesn't create a copy?
&gt; &amp;[T] If it created a copy, it wouldn't attach a reference to it.
Damn, I need to get back to regularly working in rust. 
Are you referring to things like HTTPS and needing cookies to access data? If my needs for this need to expand I plan to just have it parse HTML over stdin. That way it can be piped after `curl` or something which is made to interface with downloading documents. Also if there are errors they will print out to console which is all I'm intending right now.
As a project with a firm historical bent towards the autotools, it is interesting to note recent interest in meson (a front end to ninja) among GNOME developers.
I worked on a patch for LLVM, it grew quite large though so I stopped. The current workaround is to define 16-bit 'pseudo instructions' for all of the instructions that work on 8-bits. The LLVM instruction selector will then match 16-bit operations with these and then we can set up a [pass](https://github.com/avr-llvm/llvm/blob/avr-support/lib/Target/AVR/AVRExpandPseudoInsts.cpp) that expands the pseudo instructions into the 8-bit instructions. The downside is that leaves us with a ~1500 line file which basically duplicates functionality that can normally be done by the generic LLVM instruction selector.
I am very well aware of that. However, I don't see how generated code would be more prone to that than hand-made code. It is easy to check for overflow systematically in generated code, and very tedious by hand.
nope. i mean get a few foreign characters and tada! not able to go into a string, failure! it's a pain man. i ended up using String::from_utf8_lossy to fix my issues.
If you're on Linux, [fac](https://github.com/droundy/fac) is really nice.
The problem is that the HTML processing chain you're using doesn't handle encodings, as far as I can see - `hyper` gives you a `Read` (which returns a byte stream in general) while `scraper` only accepts `&amp;str` as input (`html5ever`, which it wraps, has an interface for bytes). So you have several options: * handle arbitrary encodings correctly -&gt; use byte parser, and provide header encoding to it if present * handle all ASCII-compatible encodings while throwing out non-ASCII characters -&gt; use String::from_utf8_lossy * handle only UTF-8 -&gt; use read_to_string as now
&gt; Is this your opinion or mathematically proven thing? I meant "any currently existing language" (that I can think of). Languages with any of the following features are out: - Glob imports - The ability to add extension methods when you have static dispatch - C++ is out because of the way templates work - Certain kinds of overloading - Coercions mixed with generics
I was about to say that `cargo` is all I need... but then you went to `bazel`. Yes, better `bazel` support would be great!
Is there a data structure in Rust that behaves mostly like a Vec, but does not relocate elements when growing dynamically, and doesn't keep all elements in continous order? I still want good cache behaviour, so continous elements in discontinuous slices or something similar is what I'm looking for. Something with fairly good random access and good iteration speed that doesn't choke when growing beyond capacity. 
Thanks for the clarification!
I'd be interested in seeing this as a CLI app like `pup`: https://github.com/ericchiang/pup
Prodbg uses tundra and the author added Rust support to it: https://github.com/emoon/ProDBG https://github.com/deplinenoise/tundra
Open an issue, please.
I wasn't aware of that syntax. Interesting, thank you.
–¢–µ–±–µ –≤ /r/playrust.
Link? What was it criticised for?
Just in case: it's standard range syntax. Ranges can be closed, eg `2..10`, or open on either or both ends, eg `5..`, `..1` or `..`. When applied to slices/vecs an open end means the range extends to the start or ending of the slice/vec.
Whats the advantage of writing let a = (|&amp;a: &amp;usize|{ a &gt; 5 }).and_not(|&amp;a: &amp;usize| a &lt; 20).or(|&amp;a: &amp;usize| a == 10); instead of let a = (|&amp;a: &amp;usize| ((a &gt; 5) &amp;&amp; !(a &lt; 20) ) || a == 10) ? I personally find the latter one more readable.
And here I am, just started studying at Lund's University... Great to see Rust getting even more traction though!
Yeah, I started out trying to use nom. But the whole lack of docs and crypt compiler error messages, put me off. Nom take macros to the extreme, it creates a whole DSL (Domain Specific Language) for parsing, but lack documentation to back it up. For a person that is already familiar with parser combinators and can read macro code like they read a novel, than nom is awesome. For a new Rustacean like me, nom looks like hieroglyphs. Thats the whole reason I thought about this project. I thought: "Is it possible to create an interface to nom that is ergonomic, non-verbose, and relies only on idiomatic non-macro rust?"
Looking at the course material, this seems based on the ostep.org book. Is that correct?
Of course the latter one is way more readable. That's basically the problem with examples: You can't get verbose. For the sake, imagine you have functions you want to build into a filter: `let filter = some_function.and_not(some_other).or(some_else);`. Of course, this can get really really complex. Also, you can build filters dynamically (for example from user input). You _can_ do this with `&amp;&amp;` and `||` and so on, but it is way simpler to build with filters, as you can pass around trait objects. 
How would one get the size of a file using fs metadata (u64) and send it over a TcpStream (u8)?
Thank you for explanation!
Trying to get the iterator equivalent of this: let i2: Vec&lt;_&gt;; let c0: HashSet&lt;_&gt; = func_filter.collect(); let c1: HashSet&lt;_&gt; = cls_filter.collect(); i2 = c0.intersection(&amp;c1).collect(); This is what I came with, but it's not filtering correctly: i2 = func_filter.filter_map(|n0| cls_filter.find(|n1| *n1 == n0)).collect() func_filter and cls_filter are both iterators which return string references based on some filter/mapping stuff, from two different collections. I want to iterator over one and add it to the final (i2) result if it's also in the other. 
Thanks for sharing, something I definitely want to run through at some point. Just a quick question about code styling if I may. https://github.com/m-decoster/iron-example/blob/master/src/model.rs#L16 is it standard rust fmt to indent a too long method/param name set that way?
Would a GCC backend for LLVM be possible? [The opposite](http://dragonegg.llvm.org/) exists...
I think the [CBE](https://github.com/JuliaComputing/llvm-cbe) is the go-to for portability to other targets. So, yes, indirectly there's a GCC backend. EDIT: I see that this is hinted at in the article: "Another route to getting Rust (and other languages) compiling is to cross-compile them to C."
You can call map_err() on the future, to change the type to the type that it is expecting. The other option is to just pass reader into core.run, which is less picky about the type of future it runs. The only reason to call handle.spawn is to schedule another future for it. Not knowing the rest of your code, I don't know if that matters or not. 
The challenge I had when I tried to make a project with Iron was unit testing. I am used to starting and stopping webservers in other languages - a common test I like to run is to start a new webserver with a random unused port, call GET and POST to various paths to test the API, and then close the webserver when the test is complete. I got stuck when trying to do this with Iron, is there some part of the documentation I am missing?
You should be able to do this: you can use any `ToSocketAddr` to create the Iron webserver, so I suppose `(IpAddr(IpAddrV4(127,0,0,1)), some_random_u16)` should work. You should be able to close it by closing the socket, but [there seems to be/have been an issue with that](https://github.com/hyperium/hyper/issues/338).
Lack of higher kinded types.
Thanks! I guess a lot of what I found lacking were * small errors, such as referring to hyper imports instead of iron imports in code examples * assumptions in the documentation that the person using the library knows how they would implement something in another framework * the design philosophy part explains how stuff like middleware works, and combined with the examples one can figure out how to use it, but I think some more examples would be useful: now only GET requests are covered. I figured out POST requests, but what about UPDATE and DELETE requests? * When should my handler return `Ok`, when should it return `Err`? Actually, I think some more examples or tutorials is what Iron needs, not necessarily better documentation, though it wouldn't hurt. I am considering contributing to the documentation, if I find the time and a good place to start.
Indeed, patterns all the way, nice!
I'm also not too happy with some messages (the one you get when the closure arguments have the wrong type is pretty unreadable), but this is high on the list for the core developers. The new output format for errors introduced recently is already a big win, and also makes it easier to make individual messages more understandable.
I'm getting different ordering of election between this program and the official results, at least for Tasmania. **Official**: Eric Abetz, Anne Urquhart, Peter Whish-Wilson, Jacqui Lambie, Stephen Parry, Helen Polley, Jonathon Duniam, Carol Brown, David Bushby, Lisa Singh, Catryna Bilyk, Nick McKim **aus_senate result**: Eric Abetz, Anne Urquhart, Stephen Parry, Helen Polley, Jonathan Duniam, Carol Brown, Peter Whish-Wilson, Jacqui Lambie, David Bushby, Catryna Bilyk, Nick McKim, Lisa Singh An Australian friend tells me this is probably because everyone up to (and including) Jacqui Lambie were elected prior to any eliminations, and your code might be treating Parry as elected before Whish-Wilson because he had more votes after distributing Abetz's surplus. (Whereas the law [specifically Commonwealth Electoral Act 1918 s 273(8)] requires those above quota on first preferences to be elected before distributing any surpluses).
The problem is that the `find` function only ever moves forward through the iterator you're calling it on, so once you move past a given element `find` will never see it again. (The implementation is very simple, [it's just a `for` loop](https://doc.rust-lang.org/beta/src/core/up/src/libcore/iter/iterator.rs.html#1454).) And once you reach the end of an iterator, `find` will usually keep returning `false` no matter what you give it, since there aren't any elements left for it to look at. If you want an intersection, you'll have to build a real HashSet out of at least one of your iterators. If you knew ahead of time that both iterators were sorted, then you could do some clever coding to get the intersection by just walking forwards, and you could do it without building a HashSet first. But I don't think there's a simple method anywhere for doing that.
Unless both iterators are sorted I think you'll need to collect into at least one set, but then you can collect directly into another vector without the second set. Something like let i: Vec&lt;_&gt; = cls_filter.filter_map(|n| c0.get(n)).collect()
The performance of `deque` is not that good, most notably because it's quite difficult to find the right unroll factor and C++ implementations tend to have small factors. Since for a Vec-like interface push-front is useless, I would instead recommend implementing a different strategy: exponential growth of the buffers. That is: [0] -&gt; [e] [1] -&gt; [e] [2] -&gt; [e] * 2 [3] -&gt; [e] * 4 [4] -&gt; [e] * 8 ... *Note: the latest buffer might be incomplete.* That is, each new buffer is twice the size of the previous, except for the first 2. The reason of this exception is making indexing easy: the bucket number is indicated by the leading bit (which is easily obtaining with some bit twiddling). - 0: bucket 0 - 1: bucket 1 - 2..4: bucket 2 - 4..8: bucket 3 - 8..16: bucket 4 You can use [`usize::leading_zeros`](https://doc.rust-lang.org/std/primitive.usize.html#method.leading_zeros) to get to it quickly: let bucket = std::mem::size_of::&lt;usize&gt;() * 8 - index.leading_zeros(); let index_in_bucket = if bucket == 0 { 1 } else { index - (1usize &lt;&lt; (bucket - 1)) };
The same friend also pointed to http://results.aec.gov.au/20499/Website/External/SenateDopDownload-20499.zip as a possible parseable candidate list. (The Senate candidates start on row 996).
You shouldn't worry! Lund seems to me to be one of the best places to study computer science in Scandinavia. I would have studied there if I could :-/ 
No, not really. That solved all my problems. :D I don't know why I didn't go with owned Strings to begin with. Thanks! Still don't really know what I'm doing. The more amazed I am that my program is running by just making the compiler stop complaining. I have yet to read a lot but as usual I'm impatient and have to start programming right away.
Oh duh - my mistake. I'll edit to clarify.
I actually like just diving into it and then understanding problems as they come up. One of the things that should be understood about lifetimes is that they don't change the semantics of the program. They're a type system feature, so they can't possibly make something that didn't work work.
It'd be super cool if this also somehow contributed to Redox OS. Maybe giving students task to write some small tool for it? BTW, I can't help myself, but I read [HKT](https://en.wikipedia.org/wiki/Kind_(type_theory)) instead of KTH... :D
Sadly, I will not be a part of this course at all as I have already finished the previous iteration of the course :( But I can't wait to hear from the younger students how it turned out! KTH is an abbreviation for Kungliga Tekniska H√∂gskolan which translates to The Royal Institute of Technology (which is why it's often written as Royal Instititute of Technology, KTH)
Wow, sounds even worse than the old IX1304/IX1303. Any idea if IS1350 is being changed as well?
In case anyone else was wondering: `distcc` wasn't particularly appropriate to Mozilla's requirements because while there was a lot of code in common, relatively little could be built in parallel, which is what `distcc` is designed for. Think of it this way: `sccache` is a distributed `ccache`, while `distcc` is a distributed `make -j`.
I wouldn't use borrowing in this case. If you deal someone a card you add it to their hand and remove it from the deck, so just a transfer of ownership.
Which of the string's characters are you trying to extract?
Surely that removes the ability to encapsulate the struct's data members though?
Coming from C, I can't get my head around what 'transfer of ownership' means. I think it means a copy, but also setting the original value copied from to a default value so it can't be accessed? Not too sure. Would be a shame if rust's borrow checker forced me to make a copy where it could easily be avoided, especially as Card could be a much more heavy weight struct.
'Interior mutability' seems interesting, I'll have a look into it, bit of a beginner at the moment. Thanks!:P
Lund is rad! I studied abroad there for a year and had a blast. I highly recommend the functional programming class taught in Haskell, if you have the chance to take it.
Not meant as an excuse but an explanation: the blog is a personal one, not the top-level official Mozilla PR approved channels. That's why the theme may not be as portable.
In this case it would likely be a copy. However, semantically, it makes sense to move it into another hand. That said you could totally do this with indices into the vector, which is probably the typical solution and the cheapest.
The first call to `.deal_card()` makes a mutable reference, then that reference ends, and it creates an immutable reference and returns it. If you take the function body and just put it in place of calls, it's possible to see fn asd(deck: &amp;mut Deck) { let hand = //create hand here if hand.top &gt;= 52 { return None; } hand.top += 1; // &amp;mut hand, immediately end borrow - return Some(&amp;(hand.cards[hand.top - 1])); // * &amp;hand.cards[n], and return it to caller, // | borrow continues to exist // | if hand.top &gt;= 52 { // | * &amp;hand, immediately end borrow, allowed because immutable - return None; // | } // | hand.top += 1; // | * &amp;mut hand, NOT ALLOWED, an immutable borrow exists already return Some(&amp;(hand.cards[hand.top - 1])); // | // | if hand.top &gt;= 52 { // | return None; // | } // | hand.top += 1; // | return Some(&amp;(hand.cards[hand.top - 1])); // | } // - borrow of &amp;hand.cards[n] ends, as the scope ends When dealing with objects, borrowing one field borrows out the whole object. Then, you make a second call to `.deal_card()`, but the first reference is still in place. You can either have one mutable reference, or any number of immutable references, but not both. I think you can go around this by using `Cell&lt;usize&gt;` in place of `top`. That way `deal_card` could take `&amp;self` instead of `&amp;mut self`, which will allow calling it multiple times. I haven't thought about the Hand struct at all, this only solves the function. Also, depending on what you're making, it's quite possible that `Card` will implement the trait `Copy`. `Copy` means that the object can be copied bit-by-bit and still be valid. This means that you could just make your hand a `Vec&lt;Card&gt;` and not bother with lifetimes. The Card will just be copied when returned, instead of having to use a reference. [Here is an example of it being used.](https://is.gd/evzGDb)
Oh, could you hide the 'constructor' in the module? That'd let you encapsulate it nicely
Have you tried combine? https://github.com/Marwes/combine
If you're mixing C with Rust, you could use the [gcc](https://docs.rs/crate/gcc) to have Cargo drive the C compiler.
But a 'move' is essentially still just a copy, you can't 'move' memory unless you're talking about reassigning references
A "transfer of ownership" always involves some level of copying (though it may just be copying a pointer), and may involve setting a marker so that it won't be accessed if there is no way to statically determine that it won't be accessed. To list a few examples: # Transferring ownership of a Handle into a vector, and then back out again // A handle is four bytes in size. struct Handle(u32); // Construct the handle. // This will involve a stack allocation of four bytes. let h = Handle(42); // Construct the vector. // This involves a stack allocation of three pointers in size. let mut v = Vec::new(); // Transfers ownership of the handle from the `h` variable to the vector. // This will involve copying the data from the stack to the vector's heap allocation. // The stack slot is not written over by a default value, // and in this example there is no side-car flag, either, // but if I tried to access `h` I would get a compile-time error. // `h` is no longer considered initialized. // It also involves incrementing the vec's length member, // and potentially adjusting the capacity and heap allocating // (in this case, it will, because the capacity is 0 right now). v.push(h); // This would not compile if it was uncommented. //println!("{}", h.0); // This will, and it'll print 42. // It will perform a bounds check on the array, // to make sure that a value actually exists there. println!("{}", &amp;v[0].0); // Transfer ownership from `v` to `k`. // In this case, the transfer of ownership is performed by a copy // and decrementing the length of the vec. let k = v.pop(); // This will compile, and will print 42. println!("{}", k.0); // This still won't compile. //println!("{}", h.0); // This will compile, but will cause the program to crash. // The bounds check fails. The memory formerly occupied by // the handle is no longer considered initialized. // Ideally, it wouldn't compile, but halting problem. //println!("{}", &amp;v[0].0); // Transfer ownership from k to m. // In this case, it's just a copy. Nothing else happens at runtime. let m = k; // This won't compile. `k` is no longer considered initialized. //println!("{}", k.0); // This prints 42. println!("{}", m.0); 
To clarify, it IS the Rust logo, but it's the Rust logo being shown in an image viewer in Redox.
yes, moves are memcopys, but nothing more. It's a shallow copy, not a deep one.
To me 'move' means reassigning references and I assume the optimizer to do so where possible. Wether a move/copy is just a compiler hint or also something the optimizer is informed about I don't know.
Storing indices is probably the best way to go. But there is another way: http://play.integer32.com/?gist=74d26ae48028e12e774d57dfbe1000f7&amp;version=stable The trick i'm using here is to "Freeze" the deck by making a new struct that contains a non-mutable reference to it. As long as the "Frozen" struct is around, we can't mutate the underlying deck, so it is safe to give out borrows that live for as long as the frozen struct does (not just for the length of the mutable borrow taken when you call the `deal` method). I don't know how well this strategy holds up in larger implementations, but it does express the constraints in a nice way: Once you start dealing cards from the deck, you can no longer move things around inside of it. Ideally, we'd be able to freeze only the top n elements of the deck, and maintain the ability to re-shuffle the unrevealed cards (since we know that there are no references into that section of the array), but freezing the whole deck is not a bad compromise. 
Yeap, but only updated videos will be available on this page. Old videos will be removed over time or market with a tag.
That's the default ;) If a tuple struct's members aren't pub, it can only be constructed within the same module.
* https://github.com/cdunn2001/compile-times
Assuming that your `Card` struct is something like struct Card { value: CardValue, suit: CardSuit } and `CardValue` and `CardSuit` are 13- and 4-variant enums, copying around a `Card` is actually 4x less memory to copy than copying around pointers to cards.
What do you expect to have happen if you do: let hand = // create hand hand.cards.push(deck.deal_card().unwrap()); deck.shuffle(); ? Give you a lifetime error? Shuffle the remaining cards? Shuffle the hand back into the deck and re-deal?
It is *possible*, but may require you to write your own `View`s - though it might still be less painful than using ncurses directly. As a bonus, it may also work on windows using a different backend. To be fair, it may not be quite ready for prime time (it may benefit from having more view types built-in), but I still encourage you to try it a bit and see if it could be a good fit.
Wrong sub, dude.
That's not silly at all lol
I see.
EDIT: whoops, replied to wrong person sry.
EDIT: turns out I started replying to /u/oconnor663 and halfway my reply changed whom I'm addressing. Start by reading his and continue here :p In the same vein it's also slower than C because the compiler **will** insert bound checks for indexing that it cannot prove will not fall out of bounds. Of course in C this may result in a security issue which you will have to manually paper over (thus resulting in code that is again equally fast as C but you're just doing it manually). In some cases you just don't care, C makes this 'don't care' mindset really easy whereas Rust forces you to deal with safety (or suffer inconvenience). I think this is a good tradeoff but nevertheless it is a tradeoff and technically makes Rust 'slower' (however little) which is the price paid for correctness. That said all this is irrelevant really: it is trivial to write slow code in Rust just as you can write slow code in C. The trick is that Rust makes it safer to write convenient code that is also decently fast. But this has nothing to do with "as fast as possible" software. To write software that is "as fast as possible" you need to ditch these nice conveniences and think in terms of datastructures and algorithms. For example you'll want to organize your data SOA (struct of arrays) instead of the classic AOS (array of structs) to allow you to maximally use SIMD to process your data. Consider leveraging your GPU and specialzed algorithms to really get going. None of these have anything to do with Rust specifically. What I'm trying to say is: Rust makes it easy to get code that is fast and safe while still being convenient. That doesn't mean "idiomatic" Rust is the fastest possible code. In some ways you can argue Rust is faster than C but that's basically meaningless when talking about "as fast as possible".
I didn't know about `std::process`. This seems exactly what I need now. Thanks for pointing this out. Unfortunately I can't really use `cargo test` as I need a standalone executable. 
&gt; For projects where performance and memory usage is not an issue, do you find Haskell more productive than Rust? As a data point, I am a Haskeller, and tried Rust twice for two distinct projects. The first project involved parsing a large (oval format for those interested) XML. I had a Haskell program that worked very well but was using a lot of memory. I went for several Rust XML libraries, and selected a pair (can't remember which). Just the parsing was an order or two of magnitude slower than my whole Haskell program. I tried fiddling with the compilation options (release compilation), but this didn't help much, so I abandoned. As a comparison, my Haskell XML parsing is the expat binding for streaming events, and a custom combinator library on top of parsec, so it's not anything particularly fast. The second involved parsing some specific files. I looked at `nom`, as I thought it was inspired by parsec-like libraries. I found out it was a giant pile of macros, and found it really hard to understand. Then I tried to write my own simple parser combinators only to realize I didn't know how to write that kind of code in Rust. As i am used to Haskell and only tried beginning in Rust, I am obviously much more productive in Haskell. However I would like to comment on the premise of this post : while memory performance in Haskell can't ever match something like Rust or C, CPU performance is sufficient (like in Java, not like in Python) for idiomatic code, and can sometimes (and sometimes not) be made C-like with only a tiny bit of efforts. While I am certain I will write more Rust in the future, that will not be motivated by speed concerns.
There isn't a canonical way to do it yet, since it's an area with ongoing work. Rust 1.12 added some new APIs to Cargo to allow for caching, and I can see that the solution here uses them. So there's still work to be done, but there's awareness that it's needed, and fortunately there's been enough groundwork done to allow something like this to pop up :)
Fork dependencies to your own repos, point your deps in Cargo.toml to those repos.
For those who couldn't participate, will there be a post-event writeup? (Maybe part of This Week In Rust?)
You'll want to repost this in /r/playrust
You'll want to repost this in /r/playrust
Which, coincidentally, is about when I started working at Mozilla. :)
They're two different problems, really. `distcc` is built for a workload where you want to distribute the compile from one machine to a set of workers, where `ccache` is for caching builds from the same source. The Firefox build actually parallelizes *really* well nowadays, some of my colleagues in our Toronto office have setup an icecream cluster to get faster builds there: https://groups.google.com/forum/#!topic/mozilla.dev.platform/-5Ktzq74KTc However, for our CI infrastructure, it's much easier to simply spin up more instances to build on rather than try to distribute the compilation from individual instances. We've talked about trying this at some point (running less-powerful instances for the builds, having high-powered instances setup as distcc nodes), but it adds a fair bit of complexity.
I have: `Iterator&lt;Item = Result&lt;T, E&gt;&gt;` (ie an iterator over results) where `T: IntoIterator&lt;Item = InnerT&gt;` (where the item itself can be into iterated over). How do I `flat_map` this iterator into an `Iterator&lt;Item = Result&lt;InnerT, E&gt;&gt;` (flat map the ok value of an iterator over results returning an iterator over the 'inner' results preserving the err value)? Specifically I don't just want the `InnerT` (`flat_map`ing the result gets me that), I want to preserve the error, a `Result&lt;InnerT, E&gt;`.
Author here; While vendoring for eternity wasn't a design goal of this project, it should serve reasonably well in that capacity. I'm currently mirroring the entirety of the Cargo ecosystem on my laptop and that is consuming 5.4GB for the index and the crates. Additionally, cargo-cacher supports being a caching server that will only cached the requested crates, which may make it more reasonable for your use case. What that could look like is that you could have your build servers point at the cache server so that, whenever your devs push code to build (beyond their specific dev environment), the crates would then be cached.
Great project For min/max there is `std::cmp::{min, max}` as functions
It's not specified, but it currently has similar packing rules to C. Every enum takes as few bytes as possible to represent all discriminant values, every member is aligned to a multiple of its size, and fields may be reordered to reduce padding. If you're worried about the cost of copying large structs, you can put them behind pointers with a `Box&lt;Card&gt;`.
This doesn't really explain the why, but mutability is a property of the variable binding, not the type (note, `&amp;mut _` is not a variable, different kind of mutability). Read more about it in [the book](https://doc.rust-lang.org/nightly/book/variable-bindings.html#mutability) (more answers via [google](https://www.google.be/search?q=rust+why+default+immutable)). Really it has nothing to do with the type system per s√©, but a convention to reduce mistakes. `Cell&lt;_&gt;` is a way to mutate data behind a shared reference (again different kind of mutability than mutable variable bindings). If this answer isn't satisfactory, I think what you want to know is _why isn't mutability part of the type (not to be confused with `&amp;mut _`)_. which I cannot answer. 
This involves a copy, which is obviously not a big issue with a lightweight struct like Card but could become an issue with other stack-allocated structs. In addition, a re shuffle of the deck would then result in mass copies from all 'hands' back to the deck, meaning i'd need to keep track of all the hands that own some cards of the deck.
The main motivation for this is that I have yet to find a solid (stable, well documented, working(!)) open-source TSQL lexer/parser/analyzer and while I love MIT myself I'd prefer that this stay open so others can benefit from it and craft tools that others then benefit from. That said I completely understand the need for a more liberal option (large enterprises simply will not touch something they cannot own/have a license to in-house), but if that is the case I feel a contract bound by money is not out of line seeing as something has to help further the project (and hopefully the infrastructure behind it at that point). If anyone has input on how to improve this whole situation I am all ears!
cargo test can run without the tests being inside the executable by doing an integration test, unless I'm misunderstanding what it is you're looking for.
Do read the mutability chapter from the book (linked in grandparent) near the end they talk about the different kinds of mutability. `Cell&lt;T&gt;` only stores one copy, and if you're using it with simple num types (`i32`, `f32` etc) then there's basically no overhead (besides being inconvenient to use). What I think is happening is that adding explicit mutability to fields merely restricts some use cases. So it's not done from a usability perspective. Talking about your original problem, as long as you borrow the members directly (and not through an abstraction like an accessor method) borrowck understands what you are trying to do. This works just fine: [playground](https://play.rust-lang.org/?gist=0afa04c61a8b2ee8b59ab24c8242f886&amp;version=stable&amp;backtrace=0) struct Foo { a: i32, b: i32, } fn main() { let mut foo = Foo { a: 42, b: 13 }; let a_ref = &amp;foo.a; let a_ref2 = &amp;foo.a; let b_mut = &amp;mut foo.b; let a_ref3 = &amp;foo.a; } If you really must use an abstraction in front of the members, then yes, you'll need runtime borrowck or just use unsafe.
 If you have a collection of arrays then you can [`flat_map`](https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.flat_map) and `collect`. Something like this will work: arrays_list.iter().flat_map(|s| s.iter()).collect()
Exactly. There's been at least one issue in the RFCs repository suggesting the ability to have methods which only partially borrow `self`, but nothing has come of it so far.
The resulting executable will most of the time run on a system where I can't install the Rust ecosystem (like with rustup). So there is no `cargo`. Also the test program will test various other programs written in other programming languages. Possible use cases are to find a commit that causes a bug (like `git-bisect`) or testing the programming homework of students.
Note that _arrays_ in Rust are different from _vectors_. An array is a fixed length stack-based set of things. `let x = [1,2,3,4]` is an array. While you can read from it and mutate individual elements, you cannot grow it. A vector is heap based and has variable length (and is growable), e.g. `let x = vec![1,2,3,4]`. You can append elements to this if you need. If you want to append an array or a vector to another vector, use `extend_from_slice`: let mut x = vec![1, 2, 3, 4]; let y = [5, 6, 7, 8]; // or vec![5, 6, 7, 8] x.extend_from_slice(&amp;y); If you want to create a new vector from two arrays, you can just make an empty vector and `extend_from_slice` twice. It is good practice to create the new vector with `Vec::with_capacity(n)` if you know that it is eventually going to hold `n` elements. As Steve mentioned below, you can also just use `SliceConcatExt`. This will create a new vector from any combination of array, vector, or slice. let x = [1, 2, 3, 4]; // or vec![1, 2, 3, 4] let y = [5, 6, 7, 8]; // or vec![5, 6, 7, 8] let concatenated = x.concat(&amp;y); If you want to create a fixed-length array from two smaller fixed length arrays, something like the following would work: let x = [1, 2, 3, 4]; let y = [5, 6, 7, 8]; let mut z = [0; 8]; // zero-filled array of length 8 for (ref mut to, from) in z.iter_mut().zip(x.iter().chain(y.iter())) { *to = from } You can just use a regular for loop. In general you don't see this pattern often (concatenation is usually done producing vectors).
I'd argue that flatmap is a relatively complex function, especially compared to size(). Not saying I understand the signature üòÅ
Your comparison isn't really fair as the java example is a very simple, non-generic function. I searched for a flatmap equivalent in Java and found this: &lt;R&gt; Stream&lt;R&gt; flatMap(Function&lt;? super T,? extends Stream&lt;? extends R&gt;&gt; mapper) [From the official docs.](https://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html) As someone who doesn't know Java, it looks incomprehensible to me. So it is not really Rust's fault, it's just that a flat map is rather complicated function. 
&gt; public int size() Yeah... that's not a flatmap function. That comparison is strained. Here's [the `len` function](https://doc.rust-lang.org/std/vec/struct.Vec.html#method.len) in Rust: fn len(&amp;self) -&gt; usize which isn't so bad either.
I had a really great time when I interned a few years ago. It's a great environment, and of course working on a great project (Rust). I definitely recommend anyone who thinks they *might* be interested to apply!
Have a look at the examples directory in rayon's repo: https://github.com/nikomatsakis/rayon/tree/master/rayon-demo/src
Say I can have an internship as long as I want, how long would an internship at Mozilla be at most ?
Does PhD level mean you have a PhD, or can you apply if you're currently in a PhD program? 
I wonder if there would be value in adding something like an `Arrow` and/or `UnsafeArrow` trait which could be implemented for pointers (or anything else) to give you the `ptr-&gt;item` syntax.
Well if the element type is just `usize` then you can copy the value: fn update_or_delete(c: &amp;mut Vec&lt;usize&gt;, i: usize, t: usize) { let value = { let element = c.get_mut(i).unwrap(); *element -= t; *element }; if value == 0 { c.remove(i); } } If you can't copy the element in the list then your solution seems best.
It really blows my mind how often people don't realize what subreddit they're in when they submit...
For this specific internship I think remote candidates are allowed too? Or have been in the past? In general I don't think that's the case. /u/larsberg would know
Although those are only for integers. You can `use std::f64;` and then in your code write `f64::max(x, y)`
No, not really.
Thousands of ten year old business applications, I guess.
The significance is the number of gnome people who are interested in moving towards rust is significant, this blog post is just a n' = n + 1 really.
If you don't mind dynamic dispatch, use: v.into_iter().flat_map(|el| -&gt; Box&lt;Iterator&lt;Item=_&gt;&gt; { match el { Ok(v) =&gt; Box::new(v.into_iter().map(Ok)), Err(e) =&gt; Box::new(std::iter::once(Err(e))), } }) If you don't mind allocations, it can be even a bit simpler: v.into_iter().flat_map(|el| match el { Ok(v) =&gt; v.into_iter().map(Ok).collect(), Err(e) =&gt; vec![Err(e)], }) 
I found an old hard drive one day that had a full suite of server apps and client stuff for a bank written in QBasic... It was terrible.
I have a friend that recently got hired to do some work for a horitcultural company and there entire system runs off a database written in basic.
Well, not every project is so concerned with optimizing perf/memory that every single memory allocation needs to be micromanaged. There are millions of projects based on JVM and Node.js out there, after all.
In case he can't copy the element, just use `to_delete: bool` since the index is just a `usize`, no need to fumble with options. fn update_or_delete(c: &amp;mut Vec&lt;usize&gt;, i: usize, t: usize) { if { let element = c.get_mut(i).unwrap(); *element -= t; *element == 0 } { c.remove(i); } }
Once "impl trait" (https://github.com/rust-lang/rfcs/blob/master/text/1522-conservative-impl-trait.md) stabilizes, the signature should become a tiny bit more readable, something like: fn flat_map&lt;U, F&gt;(self, f: F) -&gt; impl Iterator&lt;Item=U::Item&gt; where F: FnMut(Self::Item) -&gt; U, U: IntoIterator Everything there gives you important information about how the arguments and return values relate. 
This would be a breaking change. :(
Is this rolling admission, or are all applications considered equally (as long as they are submitted before January 31)? Swamped with too many projects until the end of the year. :)
That may be, but I don't see how it relates to my claim that the current state of lifetime [non-]inference on structs encourages learners to draw and solidify incorrect assumptions based on how elegantly inference is used elsewhere.
Yep. There is a really positive vibe around Rust in GNOME community currently. As a GNOME user and wannabe developer and Rust learner, I'm really excited.
Right, in this case Card will be 2 bytes, while a pointer will be either 4 or 8 bytes, depending on the architecture.
This is a very interesting idea, I'll have to give this some more thought for my own code in the future. 
Good point, I suppose we'll probably have to wait for Rust 2.0.
What sorts of things do you typically work on, or are you interested in? I'm finding Rust is great even at small things I would have typically used Python for. Usually it's not too much more work, but you get ridiculous performance, and small portable binaries. Even better than what you get, may be what you don't get...that worrying feeling in the back of your mind that something will blow up at runtime unexpectedly.
To expand a little bit, the only case I can see where it'd be quite useful to check in the lockfile with the library it when building a staticlib that you intend to distribute as a compiled object.
Reader view is great. Not only does it give you text that isn't for ants, it puts a great big 600x600 smiley in the middle. :) :) :)
I read your clock.rs file and you might want to look at https://hoverbear.org/2016/10/12/rust-state-machine-pattern/ for inspiration. Currently there are silent errors or default outputs when calling clock methods in the wrong state. You could enforce that at compile time. Note that I didn't read the rest of your crate so this might be way overkill! It's a good read nonetheless. 
We wanted to vendor the crate sources without having them in tarballs for two main reasons: * Most VCS do not do very well at storing binary files * Having the crate source in our VCS makes it possible to browse and link to in our code searching tools and from crash reports etc. 
Really cool - I started doing the same thing but you've added in some nice automation. Just so you know it doesn't build under windows. It depends on libssh2-sys v0.1.41 which requires cmake :-(
&gt; The other alternative would be to decorate structs, traits and whatnot, but this would be a bit more cumbersome It's perhaps more cumbersome, but decorating structs et al seems more Rusty, based on successful Rust libraries so far.
thanks for the kind words! Yea, I don't develop on windows so I hadn't checked that. It looks like `libssh2-sys` is a dependency of `cargo`, so you should be able to build it on any system that cargo will bulid on, but I don't have a windows machine to test that on.
Well thats still exciting none the less. Didn't mean my comment to sound critical as it did, it just felt like there was something bigger here I was overlooking. Thanks for clarifying.
Same as my response to another comment: &gt; Well thats still exciting none the less. Didn't mean my comment to sound critical as it did, it just felt like there was something bigger here I was overlooking. Thanks for clarifying. 
 error[E0499]: cannot borrow `b` as mutable more than once at a time But shouldn't the borrow of `b` drop when `intern` returns, allowing a subsequent invocation?
which line is line 43 from parser_set.rs? if you could upload all of your code to a GitHub repo, it would be easier for us to help you as well, if you're willing. As someone else mentioned, Ruru could be helpful to you, but there's no reason you shouldn't be able to do things manually, if you want. If you want to figure out where the issue is yourself, you could try adding println! statements, or you could just try removing lines of code until it stops crashing, and then you'd have a good idea of what's causing the crash, perhaps.
As a further comment, you were basically requesting a mutable borrow/reference of `self` that lasted for the lifetime of the object that `self` was a reference to. This has consequences, like not being able to re-borrow it.
That was actually my first design :) But then I thought about another requirement: efficient reverse lookups. With the separate vector, that becomes dirt easy: fn lookup(&amp;'a self, ix: usize) -&gt; Option&lt;&amp;'a str&gt; { self.interned.get(ix).map(|s| s.as_str()) } I agree absolutely with avoiding transmute--that's why I came here to try to find a way to express this structure within safe Rust! Thanks for helping.
Quick Basic was a pretty decent language, http://galileo.phys.virginia.edu/classes/551.jvn.fall01/SORTS.BAS and a [video](https://www.youtube.com/watch?v=leNaS9eJWqo) of it running.
This is a really nice tool, love it! However, I'm not really sure about having so many symbols as syntax markers, like SBT. It could easily get out of control (depends on the complexity of the system) and lead to a non-user-friendly state.
I'm really glad to found out that my problem is already being worked on (or at least added to the rfcs): https://github.com/rust-lang/rfcs/blob/db66dd2039b10b759084079827d85aba1c26256c/active/0000-seme-regions.md, so in the future the natural way to write this case can pass the borrowck.
Yep, in that specific case it sounds like the additional cost would be a major one, so no use going with the simple solution.
To add to the comment: often you compile multiple crates in parallel so it doesn't matter. But sometimes you encounter crates that out of some reason take more time, and you have to wait for one single core to churn on that crate.
You have `&amp;'a mut self` in the signature, which means that the compiler will equate the lifetime of the `Basic` and the lifetime of the borrow made when calling `intern`. And rightfully so - in a second method call (to whatever method) you could remove a `String` from the interned vector and fail to remove the reference from `map`, leading to a dangling slice. Considering your additional requirement of lookup by index, in these cases the way to go is either your version with `unsafe` (and to take care to hold up your invariants), or to use Rc. Although I'm having problems getting the lookup to work: https://is.gd/P2wTbd (the `String::from` in get() should obviously not be there) - there may be an impl of `Borrow` missing for this case.
I would run gdb with ruby (for example: `gdb --args ruby test.rb`), and then set the breakpoint inside the module before running.
That is only about LLVM codegen itself, the last part of the compilation process.
I guess I'm not really contributing anything more than the _lifetimes are actually just types and our structs are type constructors_. I suppose there's no reason why the compiler couldn't infer struct lifetimes in most cases, but it isn't something I've personally found bothersome.
You can type check things independently, but what modicum of performance the compiler has is heavily reliant on caches (like, orders of magnitude differences in compile times). For instance, ridiculous things like proving `i32: Sized` is required to verify the creation of a `Vec&lt;i32&gt;`. If these threads operated completely independently, they'd be forced to reprove all of these things independently, dampening the obvious parallelization wins. If you don't want them doing that, then you need to synchronize every cache access, which presumably makes it slower. There's also the problem that, hilariously (considering the pillars of the language), the Rust compiler is massively reliant on non-thread-safe types. Most notably RefCell. This is an artifact of the compiler being built on the old `Gc&lt;T&gt;` type, which was essentially `Rc&lt;Refcell&lt;T&gt;&gt;`.
I'm not if I understand the concept of toolchain anymore with rustup. I thought a "toolchain" is a combination of a release channel, date (or absence thereof, signalling "the most recent one") and a target triple, like this: `nightly-x86_64-unknown-linux-musl`. However I seem to be able to set the "default toolchain", in the sense that I can set the channel and date, but if I'm trying to set the full specifier, I get: root@4089e546c867:/ganbare# rustup default nightly-x86_64-unknown-linux-musl info: syncing channel updates for 'nightly-x86_64-unknown-linux-musl' error: target not found: 'x86_64-unknown-linux-musl' Even though rustup shows that I have that toolchain installed; `rustup show`: installed targets for active toolchain -------------------------------------- x86_64-unknown-linux-gnu x86_64-unknown-linux-musl active toolchain ---------------- nightly-x86_64-unknown-linux-gnu (default) rustc 1.15.0-nightly (fc2373c5a 2016-11-20) How do I change the target to `x86_64-unknown-linux-musl`? Edit: Also, this confuses me: root@4089e546c867:/ganbare# rustup run nightly-x86_64-unknown-linux-musl cargo install diesel_cli error: toolchain 'nightly-x86_64-unknown-linux-musl' is not installed Say what?! It JUST SAID in the output of `rustup show` that it IS installed?! Edit: So, rustup run x86_64-unknown-linux-musl cargo install diesel_cli apparently works. If this isn't a bug, it's at least a very confusing UI.
A different stupid question: I heard a group is currently writing on a rustc backend just for webassembly IIRC. Could one extend it with x86 and arm7 instead of using LLVM?
That's a lot of nice improvements, you've been busy. Coincidentally, I'm working on a [tool](https://github.com/philipc/ddbug) to do a similar thing to the `-Zprint-type-sizes` option, but it uses the info from DWARF instead (using the [gimli](https://github.com/gimli-rs/gimli) parser). It's in a very early state (I expect it has lots of bugs and deficiencies), but an example of its output is: struct core::fmt::Formatter size: 96 members: 0[4] flags: u32 4[4] fill: char 8[1] align: enum core::fmt::rt::v1::Alignment 9[7] &lt;padding&gt; 16[16] width: union core::option::Option&lt;usize&gt; 32[16] precision: union core::option::Option&lt;usize&gt; 48[16] buf: struct core::fmt::&amp;mut Write 64[16] curarg: struct core::slice::Iter&lt;core::fmt::ArgumentV1&gt; 80[16] args: struct &amp;[core::fmt::ArgumentV1] Major things that I haven't started on include adding options to filter the output, and compare two binaries.
&gt; Although I'm having problems getting the lookup to work I encountered the same thing. IIRC; there is a Borrow from `Rc&lt;T&gt;` to T and one from String to `&amp;str`, but nothing from `Rc&lt;String&gt;` to `&amp;str`, and no "transitive rule" (A -&gt; B and B -&gt; C implies A -&gt; C). `Rc&lt;str&gt;` could be an option, if there's a way to make one? Last time I checked one couldn't construct one, but that was long ago.
The problem seems to basically be that self references don't work. This is a known weakness of the borrow checker, and IMO you're better off abandoning a false sense of security now than locking yourself into a broken model that doesn't actually offer any more safety. 
For some reason it really bothers me to see `///comment` without a space. You might want to `cargo install rustfmt &amp;&amp; cargo fmt` to just have everything formatted automatically. Regarding actual code, there are a few small bits I found: * The extra block and indentation [here](https://github.com/bytebuddha/specs_engine/blob/master/src/clock.rs#L47-L55) can be left out, you can just write it like a normal if + else-if chain (`else if let ...` on one line) * [Here](https://github.com/bytebuddha/specs_engine/blob/master/src/clock.rs#L61-L68) I'd use [expect](https://doc.rust-lang.org/std/option/enum.Option.html#method.expect) instead of if let + panic in the else branch. Just for brevity. * Isn't maintaining [those line numbers](https://github.com/bytebuddha/specs_engine/blob/master/src/machine.rs#L75) a pain in the ass? Isn't this exactly what the `RUST_BACKTRACE` environment variable is for, finding out where a panic came from? * The thing with the leading underscores in variable names confuses me.. Do they have any kind of meaning?
&gt; Once "impl trait" stabilizes Actually, it's not even implemented to the extent that would be required for this yet, as `flat_map` is a trait function. The "conservative" in that RFCs title is there because for now it's only meant to be available to normal, non-trait impl blocks and non-method functions. `impl Trait` in trait impl blocks is blocked on [ATCs](https://github.com/rust-lang/rfcs/pull/1598), which AFAIK are not implemented yet and are still being discussed, although a road towards implementation does seem to be emerging.
can sscache used for speeding up rust compilation?
And then you define `struct I32(i32)` and the compiler is back to having to prove `I32: Sized` to create a `Vec&lt;Ii32&gt;`. Besides, if these proofs are cached then you will save one trivial proof for each built-in type per compilation; I strongly doubt that this would make a measurable difference in performance, but it would mean introducing extra special cases in the compiler.
In my experience the steps are: 1. Profile 2. Take out all the memory allocations you can 3. Make memory access cache friendly by understanding prefetching. 4. Structure for concurrency including pipeline and fork join concurrency. Don't forget that typical allocators block other threads. Jemalloc is a general purpose solution, although it still needs to get memory from the OS. There are OS specific functions to do thread local allocation, which can be huge for utilizing all cpus. 5. Use SIMD instructions. These frequently mean the architecture needs to make them possible. I take the view of architecture for speed but worrying about it only once I have things working. 
I would ask: is there anything special about the general "algorithm" or compiler architecture/model that would make compilation slower for rust? Is it that some features are inherently slow to have in the language/grammar? If so it would be nice to understand when certain code pattern are pathological cases. Moreover, what would the equivalent clang frontend vs llvm backend costs be? Are they comparable (as in possible to compare) to rust?
&gt; If you don't want them doing that, then you need to synchronize every cache access, which presumably makes it slower. But isn't this already being done every time an `Rc&lt;Refcell&lt;T&gt;&gt;` is dropped? `xsub`/`xadd` is triggered which has the same semantics as `lock` or `mfence`, a full memory barrier. 
`Rc` is not atomic, that's `Arc`. The only thing to worry about with that is cache misses due to the indirection (since the refcount is stored in the allocation).
I'm not trying to make it out to be anything. I'm genuinely unaware of the difficulty.
Oh thanks TIL. 
Which trait objects do you have in mind?
I think a big part is, that rusts static analyzer is massive. Rust catches many runtime errors at compile time, therefore it analyzes the source code much more than a C compiler or a C++ compiler. I think thats the part causing it being slow. Also, rust is a much newer language and (I think) the rust frontend is spitting out pretty unoptimized LLVM IR, which the backend must crunch through.
I never did much with Quick Basic, I was a pro with TI Basic however.
It doesn't feel like the static analysis is the big cost. Cargo-check is a very fast alternative to rebuilding a binary as it only performs the type checks.
Highly recommend watching C++ talks by Herb Sutter and Scott Myers. Anything on cache locality / parallelism.
The language server is just the compiler (sort of). The real blocking issue on named/default value args seem to be how to parse them. The suggested `fn( my_name: val)` collides with type ascription. You can likely *hack* something in that allows named/default args with the Macro1.1 system. But I then I doubt the language server will work nicely with it since it is your own compile time extension. The real issue is default/optional parameters add a decent amount of overhead to function calling. You end up [branching for each argument](https://play.rust-lang.org/?gist=d69fcfda649bb2779c8218b85737db1e&amp;version=stable&amp;backtrace=0), and your type signature becomes very ugly.
But that's also why I think it's a good idea to sidestep the issue completely with tooling like this. The language server would just have to store the names of arguments as seen from function being called, and then make them available for the editor to display. The language itself isn't impacted at all this way. This doesn't give you default/optional arguments though, so that will still require language updates.
If we can build multiple crates in parallel (which we usually can), there aren't any gains in doing this.
Delighted that my weird use case ([#36799](https://github.com/rust-lang/rust/issues/36799)) was the indirect catalyst for some of the improvements, and a benchmark.
I keep hitting this unreachable running it against a custom x86_64 target (-mmx,-sse,+soft-float). https://github.com/philipc/panopticon/blob/master/lib/src/amd64/mod.rs#L2185
If you have any questions/complaints/requests for gimli, don't hesitate to reach out to me :) Edit: nvm hi philipc :)
Another perhaps crazy option is to use lua thrift.
Well the expect's are actually unreachable code so i guess keeping the expect's would be best, but i should add a better error message.
Last time I wanted `Rc&lt;String&gt;` as a map key I had to wrap it in a newtype and impl `Borrow&lt;str&gt;` for that.
&gt; The real blocking issue on named/default value args seem to be how to parse them. I think, the real issue is that having named parameters suddenly makes changing parameter name a **breaking change**. 
Which is another nice thing about this; the names are never official API, but are just extracted by the language server as a sort of guidance.
&gt; Concurrency - (core count * 0.9)x This is a good point worth focusing on. Architecting your compiler for more threaded parallelism will optimistically get you a 7x speedup on a beefy 8 core dev machine. The fact that it's not a truly embarrassingly parallel task (due to redundant type checker proofs) probably dampens this a lot to, maybe, 2-3x. A big win no doubt, but rustc really needs to improve by multiple orders of magnitude. Parallelization is also probably a super high-hanging fruit.
Missing key-bindings. They are: Z,X,Space,Left,Right,Down. 
[How to Write a Git Commit Message](http://chris.beams.io/posts/git-commit/) :)
I'm starting to get into traits and boxed traits a bit more and I'm having a bit of trouble understanding how I would clone a boxed trait. Here's a simplified version of what I'm trying to do: http://play.integer32.com/?gist=a0c198a2c8104804e9be9d37896b9957&amp;version=stable and I'm wondering (in that example) how I would clone the FooSet. I've tried several things and none of them worked, but since I'm not yet very familiar with this stuff, I'm having a hard time finding a way to describe my attempts properly. Any help would be greatly appreciated. Edit: After a bit more searching I did find this: https://users.rust-lang.org/t/solved-is-it-possible-to-clone-a-boxed-trait-object/1714/6 I don't _love_ that solution, but if that's the best way to go, I guess that's the way I'm going. 
While it's awesome that fedora has rust compiler included now, as it opens the door for natural inclusion of other software written in Rust, I see no reason to use Fedora-provided `rustc` over `rustup` as a Rust developer. I am a Fedora user.
Yeah, the [panopticon](https://github.com/das-labor/panopticon) disassembler doesn't have complete x86 support yet. I'll probably disable disassembly by default (currently I only use it to find out the call graph, which I want so that I can determine which functions can panic).
Hey, you changed the problem specification by using `Err(iter::once("three"))` in place of `Err("three")`! Shenanigans! But it does look good, nice job :)
Oh sorry they are halfway down the README. Perhaps I should move them up?
Guilty. I apologise. I get a bit lazy on personal projects. I will take this on board though :)
It's rolling.
You can rework it a little bit if you really wanted to have `Err("three")` while maintaining your iterator implementation. One option is to pass in the function you want to use to create the iterator from the error. Something [like this](https://is.gd/G4FjlA).
No, you just make named parameters opt-in. func add(x: i32, y: i32) -&gt; i32 Wouldn't be callable with named args. func add(x: i32, to y: i32) -&gt; i32 Would be callable as `add(3, to: 4)` or `add(3, 4)`. Potentially there could be an attribute for "don't allow nameless calls", but in general you want to support both so that APIs can be improved to use naming backwards-compatibly. func add(x: i32, _ y: i32) -&gt; i32 Would be a convenience for "use the same name externally and internally". 
I would add "streamline hot paths" as item 2b. Lots of basic stuff like inlining, putting fast tests in to short-circuit slow paths where possible, stuff like that. That's a lot of what I've been doing, and it's typically much easier than 3, 4 and 5.
A lot of it has to do with crates being the unit of compilation, because they tend to be big. E.g. a Rust program might have 10 crates and a similar-sized C++ program might have 100 source files (and in both cases there will be some dependencies so some units depende on others) so you'll be able to get many more concurrent compiler invocations happening in the latter case. Also, Rust's dependency tracking is currently crude. If you touch a crate, every other crate that depends on it will rebuild. Incremental compilation should help a lot with that.
Yeah that's what I ended up doing, but I went a bit further and just assumed the error case will never be an iterator (which I think is fair): [playground](https://play.rust-lang.org/?gist=24caf35bef1c37d331e1b9f0fa29a12a&amp;version=stable&amp;backtrace=1). Essentially this hardcodes the error to be a `Once` iterator (which I looked up the implementation for, it's just an `Option&lt;T&gt;` where the next is just a `.take()`).
I'm using ruru right now at work for optimizing a very hot loop in a ruby application, and has worked out pretty well for me so far! Thermite too: https://github.com/malept/thermite
That isn't representative of OCaml's performance. I've submitted programs several times faster than those in the past only to have them subjectively "de-optimised" by the author. Furthermore, the problems are too simple to be of practical interest and the methodology is flawed (e.g. half of the benchmarks simply print trivially reducible compile time constants).
There's a caveat with using `f64::max`. In [`std::cmp`](https://doc.rust-lang.org/std/cmp/fn.min.html), pub fn max&lt;T&gt;(v1: T, v2: T) -&gt; T where T: Ord The issue here is that floats don't implement `Ord` because of the difficulty with dealing with NaN. But [`std::f64::max`](https://doc.rust-lang.org/stable/std/primitive.f64.html#method.max) says &gt; If one of the arguments is NaN, then the other argument is returned. And `std::f64::min` says the same. So they "solve" this problem by making float min/max inconsistent with each other (instead of, say, saying that NaN is always greater or always smaller than other floats), while the min/max in `std::cmp` are always consistent.
Also, type ascription could use another syntax (unfortunately `::` is already taken and `:&gt;` feels like some sort of coercion, but there's other operators available)
There is no alternative. Take a look [here](https://doc.rust-lang.org/1.5.0/book/lifetimes.html).
Great article! Interesting to peel back the inner workings of a data structure that we take for granted :)
Hey, not a lot is going on. I have multiple projects going on and this one is a lower priority because I don't completely understand Tokio and the library is still being developed. For now I'm just using mio directly but even that doesn't have much progress. If you want to take a shot at it, go for it!
I don't get it. Who/what is the borrower that ends up holding the mutable reference to self?
For comedic value, but testing is good too: [Serious code must have serious tests...](https://docs.rs/boolinator/*/src/boolinator/.cargo/registry/src/github.com-1ecc6299db9ec823/boolinator-2.4.0/src/lib.rs.html#162)
This; /// /// `**callback macro**` is shorthand for; /// /// ```rust,no_run /// { fn c(x: &amp;ObjectType) { x.object_function() } c } /// ``` /// It's purpose is to run a function associated with the /// object passed in as `x` /// /// + `ObjectType` can be a trait type /// + `object_function` can be a function in that trait /// /// # Example /// ```rust /// trait Command { /// fn run(&amp;self) {} /// } /// struct Test { x: i32 } /// impl Test -&gt; Test { /// fn new() { Test{ x: 42} } /// } /// impl Command for Test { /// println!("{:?}", self.x); /// } /// fn main() { /// let t = Test::new(); /// let macro_test = callback!(Command::run); /// println!("{:?}", macro_test(&amp;t)) /// } /// ``` macro_rules! callback { ($t:ident :: $m:ident) =&gt; {{ fn __f(x: &amp;$t) { x.$m() } __f }} } It should hopefully be explained in the comments. It's become something of a pattern for me in relation to making a game.
The function is requesting a mutable borrow that lasts for the lifetime of the object. Regardless of whether it is necessary or not *right now* to have the borrow last that long, no matter what you change inside the function, you will not break any code that uses the function. If the lifetime of the reference only depended on what you did with it inside the function, this would make it easy to accidentally break external code in ways that could be complex to resolve. At least, that's how I understand the reasoning.
When doing debug builds of rustc-benchmarks more than half of them only spend 10--20% of the time in LLVM. Also, the amount of process-level parallelism is typically lower in Rust than in a language like C++, because the unit of compilation is typically much larger.
That description helps my mental model of lifetimes, but maybe I'm abusing terms and it's not actually helpful for anyone else :) There's a section of the [Rustonomicon](https://doc.rust-lang.org/nomicon/subtyping.html) all about how lifetimes behave as types, but it does require a bit of background knowledge. Back to your original point about eliding lifetimes for structures, it actually looks like there's been very little discussion about it, which is interesting. There is this [one little section](https://github.com/rust-lang/rfcs/blob/master/text/0141-lifetime-elision.md#lifetime-elision-in-structs) at the end of the original elision RFC for functions and impls about elliding lifetimes for structs too, that doesn't seem to have lead anywhere.
What's the difference between that one and this? curl -sSf https://static.rust-lang.org/rustup.sh | sh
&gt; Thermite too I'm very happy to hear this! Let me know if there's anything missing that you need for your integration (preferably in GitHub).
Flat context seems easy to handle in most languages; pass along a handle to the resource rather than obtain a handle in the called function. Structural context is more complicated. The contagious annotations (tainted, sensitive) could use something like `fn Taint&lt;T&gt;::map&lt;U, F&gt;(self, f: F) -&gt; Taint&lt;U&gt; where F: Fn(T) -&gt; U`, which handles most taint propogation but requires the user to enforce that the closure doesn't cheat by mutating a cell.
This is sort of a bad example because of the types involved. Instead consider a type like `Foo&lt;T: PartialEq&gt;` and a type like `Option&lt;X&gt;`, where `X` is a local generic type that is bound `X: PartialEq`. The compiler doesn't precompute that `Foo&lt;Option&lt;X&gt;&gt;` is a valid type - the number of hypothetical, potential types to check is unbounded. So it lazily proves, as it checks your creation of a `Foo&lt;Option&lt;X&gt;&gt;`, that this would be a valid type (because `Option&lt;X&gt;` implements `PartialEq`). And of course this intertwined with inferring that your expression is a `Foo&lt;Option&lt;X&gt;&gt;` in the first place.
That macro looks like a no-op in that case; `Command::run` already gives you the method you asked for.
&gt; Now, how would one e.g. implement a `try!` macro in C++? Exceptions! _(\*ducks\*)_ But seriously, textual macros decoding to an [immediately-invoked function expression](https://en.wikipedia.org/wiki/Immediately-invoked_function_expression). _(\*ducks again\*)_
Is there a reason it's not a part of cargo? 
Sounds great! I'm currently doing a contract that involves some face detection stuff but resorted to C++ due to the lack of options in rust. 1. At the moment, I don't think there are any pure-rust CV libs. 2. All the usual rust benefits: - High performance - CV is generally quite intensive - The ability to provide a modern API for users - Super simple package management and distribution via cargo - Great portability - all the popular desktop OSs, mobile OSs, embedded, etc. - Memory safety by default. 3. The `leaf` framework became quite renown throughout the rust ML community for a while, I imagine trying to name it this would cause a bit of confusion, especially as there is still the possibility someone may pick up the old leaf ML project. Normally it's fine to approach the previous owner if it looks like the crate has been abandoned, though in this case I'd probably recommend just going with something else to avoid confusion. Good luck!
It does yes, but the purpose of this all is to store a pointer to a function in a map or array for use later with *any* object of the right type.
I've always liked this example, from [a blog post from Felix Klock](https://blog.rust-lang.org/2015/04/17/Enums-match-mutation-and-moves.html), since it's just so neat and makes great use of several Rust features to make what could be a fairly complex set of if/else clauses come out very cleanly. fn num_to_ordinal_expr(x: u32) -&gt; String { format!("{}{}", x, match (x % 10, x % 100) { (1, 1) | (1, 21...91) =&gt; "st", (2, 2) | (2, 22...92) =&gt; "nd", (3, 3) | (3, 23...93) =&gt; "rd", _ =&gt; "th" }) } 
I think thats about as close as you get to justifyably writing assert_eq!(true, true) in a test. The silliness fits the absurdity.
To paraphrase Cameron Zwarich pwning me one time: OIBITs[1] in Rust are modeling coeffects. Send, Sync, and the unloved brother UnwindSafe. All of these specify under what environments a piece of data's operations can be safely evaluated. You could similarly make a `PublicSafe` OIBIT, with functions that leak info to the outside world marked as requiring `T: PublicSafe` (and secret data in turn not conforming to this). [1]: You'll never get me to abandon this atrocious acronym.
mine is: let x = 0; let () = x; I use this if I want to quickly know what type is x. The compiler will tell me: = note: expected type `{integer}` = note: found type `()`
Glad we have rustfmt: |m| { (2..m).fold(vec![], |mut p, n| { if p.iter().all(|k| n % k != 0) { p.push(n) } p }) }; 
It must have changed, I don't see any `&amp;mut Read` uses except for the pretty printer.
Can i sort a Hashmap after the value of the keys? I have something like this: {"apple": 2, "banana": 0, "tomato": 3} And i need the keys in the correct order based on their value. (banana -&gt; apple -&gt; tomato)
I love this blog. I was just looking at it the other day hoping for a new post. Too bad it was one found in rust!
Paging /u/tomaka17, in case he'd like to take a crack at it, or knows someone who would.
[std::variant](http://en.cppreference.com/w/cpp/utility/variant) is maybe safer than regular union. However, it is a C++17 feature and the code remains more verbose compared to Rust. 
That is nice. I would move the test for the 1x values into its own line though: fn num_to_ordinal_expr(x: u32) -&gt; String { format!("{}{}", x, match (x % 10, x % 100) { (_, 4...20) =&gt; "th", (1, _) =&gt; "st", (2, _) =&gt; "nd", (3, _) =&gt; "rd", _ =&gt; "th", }) } 
I am from C++. To observe the deduced type from the compiler, there are different options that I am aware of: * [typeid](http://en.cppreference.com/w/cpp/language/typeid) * creating error based on a template * [Boost.TypeIndex](http://www.boost.org/doc/libs/1_62_0/doc/html/boost_typeindex.html) Rust seems simpler! 
&gt; ascription could use another syntax (unfortunately :: is already taken and :&gt; feels like some sort of coercion, but there's other operators available) They could use `~` for type ascription. Or another sigil. 
I used the "Time Profiler" template of an app called Instruments on OS X. I ran it from the command line like this instruments -t "Time Profiler" -D ~/traces/conrod_glutin_glium_screenshot ./target/release/examples/glutin_glium
[removed]
Btw if you're on nightly you can abuse `std::intrinsics::type_name` (for debugging pls do not ship in production) which gives you the exact same representation of a type the compiler prints in error messages and whatnot.
If your "oneliner" has curly braces then it shouldn't be a one liner
Unfortunately I surrendered and just switched to linux, but if you ever find one please tell me! Sorry for the late reply.
/u/Gankro's "hacking generativity onto Rust" (section 6.3 [here](https://github.com/Gankro/thesis/raw/master/thesis.pdf)) and bluss's [implementation](https://github.com/bluss/indexing). This is some of the most horrifying code I've ever seen, up there with [Duff's device](https://en.wikipedia.org/wiki/Duff's_device) in C and [partitioning lists](https://www.stavros.io/posts/brilliant-or-insane-code/?repost=true) in Python. Basically, you can use lifetime-based black magic fuckery to move bound checks to compile-time. Rust: [you had my curiosity, but now you have my attention.](https://www.youtube.com/watch?v=Yyfdcoocex8)
While that is a useful example and valid code, having both a lifetime and a variable called 'a' might confuse inexperienced Rustaceans.
I have zero knowledge of big software optimization, but isn't 1% - 2% considered as premature optimization? Is that one percent such a big deal? 
It's also justifiable to write that when you're testing the test framework itself.
 let kw = if on { "on" } else { "off" };
This one is much easier to understand.
&gt; That is very dumb. A useless limitation, yes. _Very_ dumb, I don't think so... I've seen way worse. Keep in mind this API is more than 20 years old and has never been broken, along with all its siblings. So there's that, it's limited but trustable. If you had to handle more than 2 billion objects in Java (or any language for that matter), you'd also be better off by _not_ use the standard library data structure, which are optimized for the general case, which might cover up to a few million items... beyond that, you'd choose a structure appropriate for your application domain, HPC, storage, reporting... Also, keep in mind that anything that implements .size() in Java will have it's own structure and the objects it holds allocated individually _on the heap_, which will likely represent at least 16 bytes per object (with references and structure overhead), which would take at least 2Bx16b = 32GB in RAM. I don't even have have half of that on my workstation, and I have an OS to run... So all in all, that int isn't worth getting upset with. edit: a letter + another thought
&gt; When is it appropriate to add where clauses to a struct declaration vs its constructor? So, unless you've done something to make the literal syntax not work ([maybe like this](http://words.steveklabnik.com/structure-literals-vs-constructors-in-rust)), if you only put the bounds on `new` and not on the struct itself, then someone could create a struct that doesn't have those bounds. So I'd never put them on only a constructor. Remember, "constructors" are just a convention in Rust, they're not special. So it's not like this is something that we specifically decided to allow for a reason; it falls out of the fact that a constructor is just another associated function.
Thanks, in my specific case the members are fully private. Take this example: [playground](https://play.rust-lang.org/?gist=24caf35bef1c37d331e1b9f0fa29a12a&amp;version=stable&amp;backtrace=1). Notice how the bounds are on the impl containing the constructor and the iterator impl, why shouldn't I also put them on the struct?
[This comment](https://github.com/rust-lang/rust/blob/b1363a73ede57ae595f3a1be2bb75d308ba4f7f6/src/test/compile-fail/const-err.rs#L13-L14) makes me think that you can't actually make this a compiler error: &gt; these errors are not actually "const_err", they occur in trans/consts and are unconditional warnings that can't be denied or allowed
What about code like this: let a = [1;1]; let b = 1; let c = a[b]; It doesn't have compile-time warning, but it seems reasonable that it should.
If you increase the mir-opt-level you'll probably get it, because of the const propagation pass.
You can take some solace in the fact that it's a really nice library, with a nice tutorial. I was able to get some simple graphics stuff going relatively painlessly with glium, with no prior opengl experience after failing to make any sense of piston. Are you planning on ever revisiting it, or are you all-in on Vulkan from here on?
Hashmaps are unordered. You'd need to collect the map into a vector and sort that. https://play.rust-lang.org/?gist=c488e6010280711f1f2f4bb824e2b04c&amp;version=stable&amp;backtrace=0
Awesome, thanks! 
Try `-Zmir-opt-level=3` - this is likely a temporary measure until we can fine-tune it to do as much as possible without impacting compile-times.
Vulkan is so much cleaner than OpenGL. It also has the advantage that it's more straight-forward than OpenGL, which is half high-level and half low-level. I'm not against revisiting it, but it's really low priority for me. 
There's a direct relationship though -- `rustup.rs` is the successor to `rustup.sh`.
I have only used OpenCV, and not other libraries, but while the lib is powerful I see a big potential for rusty improvement, mainly: * Type safety. OpenCV has a really terrible architecture: undocumented exceptions coming from inside the library can be really frustrating. Another example, when performing various operations it's hard to keep track of what the pixel format is. You can load an RGB image, extract the foreground unknowingly getting a greyscale image (it makes sense for this algorithm, but it's not made apparent from the signature), and with no compile-time checks save to a file it as if it were RBG, getting some random trash. * Easy package management - getting OpenCV to properly compile on my machine was far from trivial I think it would be great to build on top of the [nd-array](http://bluss.github.io/rust-ndarray/master/ndarray/index.html) project, since CV is going to require lots of addition/subtraction/other matrix-ey operations anyway.
How can I disable features of dependencies recursively in `Cargo.toml`? (I'd like to disable all "openssl" features within my project)
Are you bluss? I went looking but I couldn't figure out your Reddit username.
Yes!
Dude, you single-handedly made my capstone project work out. I was `lilred` on IRC, with those endless list of silly questions. I've never had another experience like that, before or after. #cpp is grumpy, #web is confused, #haskell is only interested in theory - only #rust is safe harbor for obstinate newbies. I'm so happy I picked Rust for my capstone project. I'd love to get back into it and continue my OS once I graduate in April.
Thank you,t hat's awesome to hear! No doubt there are many helpful people in the rust irc network.
So now that LLVM has added AVR support, when does this filter down to Rust? http://lists.llvm.org/pipermail/llvm-dev/2016-November/107177.html
 for i in 12..buffer.len() { let prediction = coefficients.iter() .zip(&amp;buffer[i - 12..i]) .map(|(&amp;c, &amp;s)| c * s as i64) .sum::&lt;i64&gt;() &gt;&gt; qlp_shift; let delta = buffer[i]; buffer[i] = prediction as i32 + delta; } This is [a snippet](https://github.com/ruuda/claxon/blob/8fc094f/src/subframe.rs#L527-L534) from [my FLAC decoder](https://github.com/ruuda/claxon). I like it because it demonstrates the cost of abstraction in Rust very well. It contains non-trivial things such as iterators and closures being constructed, potentially out of bounds array indexing, and it uses high-level concepts such as map and sum. Yet, when you look at the generated instructions, it is a very tight loop with some moves, 12 multiplications and adds, and a shift. Not a single instruction is redundant. I couldn‚Äôt have written it better by hand.
This is such a bad warning, it is similar to gcc's POD warning in style where it generates code that literally makes your program crash. I never understood the point of this. When the compiler knows that my program is invalid and it cannot work then why the hell does it compile? Why is this not an error?
It's legal and safe to panic.
As always, the issue is more nuanced - see https://github.com/rust-lang/rfcs/pull/1229 for more details on this stance. Half of it is backwards compatibility (since these warnings do not exist in a vacuum, but rather arise from optimizations), and the other half is that code being objectively bad is not a certainty without potentially unbounded context.
You can do this with termion
If you're OK doing things manually, https://crates.io/crates/term exposes a lot of things related to handling terminals: https://stebalien.github.io/doc/term/term/trait.Terminal.html, is compatible with both Windows and UNIX systems, and supports all terminals that ncurses does (via terminfo). To my knowledge it's still the only pure-Rust crate that does this correctly. For features not wrapped yet, you can also get raw access to the Win32 Console or underlying fd with access to the raw terminfo database. You might also inspect https://crates.io/crates/termbox.
In fact he is: https://github.com/tomaka/vulkano
cheers
That's great! I was looking for a library like this a few days ago. Also for communicating with an Arduino.
Having spent my day at work dealing with serial ports, this could be pretty handy. 
Was this deleted, or is there a caching issue making me sad?
A WordPress 404 page‚Ä¶ which happens to include a link to the post in the "Recent Posts" widget. :-)
Works fine for me
some of you should have [received pull requests](https://twitter.com/gcouprie/status/802091790253887488) to upgrade to nom 2.0 by now. I'll write soon about how testing your releases with projects using your library is a great QA practice :)
ok, while we wait for wordpress to get its act together, I exported it to a [gist](https://gist.github.com/Geal/3d127ec38a80f23d434730876a91c1b2). But the links are not there, and you won't enjoy the gifs :(
The main problem of the blob is that it is limited to a specific operating system version on a specific hardware architecture. For example i have some sparc64 servers which seems to be totally out of reach for Rust. This reminds me the fate of the Rebol language which never took off because it was limited to specific platforms and were not even up to date.
Congrats! It's really good to see 2.0 libraries in the Rust ecosystem. :)
I stumbled upon the same issue. I found this video: https://www.youtube.com/watch?v=7D9GE3-o54o, but i'm still not sure how to apply it.
Thanks! That looks basically like the suggestion in the first comment on the blog, the corresponding code of the video is: http://silverwingedseraph.net/rust-using-enums-and-match-expressionsstatements It's a &amp;mut State -&gt; State function, but could as well be State -&gt; State here because of how the function works (it always replaces the State anyway). Which is what I actually tried first and which has another set of problems (see https://coaxion.net/blog/2016/11/writing-gstreamer-elements-in-rust-part-3-parsing-data-from-untrusted-sources-like-its-2016/#comment-61079 and reply) Or maybe I'm misunderstanding?
Thanks, I didn't know there's a RFC about that :) That's basically like take_mut (https://crates.io/crates/take_mut), right? That is another option but it has other (and same) problems. See https://coaxion.net/blog/2016/11/writing-gstreamer-elements-in-rust-part-3-parsing-data-from-untrusted-sources-like-its-2016/#comment-61079
I didn't see that one, but that's actually what I tried first :) It has the problem that you always need to provide a new state (which can be quite verbose) and possibly has problems with moving parts of the state into the new one if it's borrowed in one of your (possibly nested) matches. Conceptually it's however my preferred solution, having a State-&gt;State function just seems right See https://coaxion.net/blog/2016/11/writing-gstreamer-elements-in-rust-part-3-parsing-data-from-untrusted-sources-like-its-2016/#comment-61079
Thank you for your reply. If i got it right, there is no plan and more support will come as soon as someone step up to do the job. Will the bootstrap blob will be always necessary ?
That's like asking whether a bootstrap blob will be necessary for gcc. The Rust compiler is written in Rust so, just as with a C compiler written in C, you first need to add support for cross-compiling from an already-supported platform. The only reason some compilers (eg. GHC for Haskell) don't need this is that they can output C to be compiled on a C compiler you already have on the target. (Early versions of the Rust compiler were written in OCaml.)
Rust allows for cross compiling. So you might only have Linux, Mac and Windows binary builds but you can use them to build it for other archs. We have Linux (ARM/x86/x86_64/mips/mipsel/powerpc), Windows (32 &amp; x64). OSX. iOS/Android/Webasm (as build targets). BSDs/Solaris (but maybe only on x86/x86_64, than again I'm not sure if anyone has tried any others, maybe a PowerPC/ARM BSD works...). Having said that, other CPU architectures are limited currently. Particularly embedded, but it's being worked on. Rust's compiler toolchain is based on LLVM, so that's the first piece that needs support for a given chip. LLVM just had AVR support added to 4.0-dev this month. Rust has out of tree support currently via the rust-avr fork but it is in the process of updating to the LLVM 4.0-dev (previously it was using a patched AVR-LLVM). Then it needs to get up to date with the Rust trunk and get it upstreamed. The other thing that is required is 'bare metal' support. Basically the std library needs a OS for things like having a file system. So while we have many cpu archs via Linux, currently there is limited support without an OS. But you can write code without using std. And get non-OS ARM working. AVR should be the same (although a bit harder as you need the out of tree fork). There is also a tool called xargo for helping embedded programming. [PIC support seems unlikely any time soon](https://github.com/woodrowbarlow/MPRS8) due to a lack of LLVM backend. Xtensa also lacks a LLVM port (ESP8266, etc...). RISC-V is [getting merged into LLVM](http://lists.llvm.org/pipermail/llvm-dev/2016-October/105768.html), but I haven't heard of any on working on a Rust port. A Linux-RISC-V port would probably be fairly quick but bare metal RISC-V would be in the same boat as AVR/ARM. There is the [llvm-cbe](https://github.com/JuliaComputing/llvm-cbe) that would let any arch supported by a c compiler work in theory.
EVERY language relies on a binary blob from an untrusted origin... it's just that the "untrusted blob" is usually a copy of GCC that you've learned to take on faith. The only way to avoid relying on an "untrusted blob" is to re-walk the chain of progress back up from the beginning, building your own ICs and OS and writing your own assembler in raw machine code. Otherwise, you're trusting an "untrusted" GCC on an "untrusted" OS in a machine full of processors running "untrusted" firmware. (It's not out of the question for a modern hard drive to have more ARM computing power in it than the original Raspberry Pi and the potential for installing an exploit within the flash memory in the hard drive controller has been demonstrated.) (Also, LLVM doesn't take C++, it takes an intermediate language called LLVM IR. The C/C++ compiler offered as part of the LLVM ecosystem is called Clang (or llvm-clang) and uses the LLVM core as a backend library in the same way that Rust does.)
But a closure does not need braces: `map(|on| if on { "on" } else { "off" })`
Yes, unwinding is always a big concern when doing unsafe stuff (i.e. what `take_mut` does). The current status of the RFC is quite good, with a second closure that provides a default value (or e.g. panics) in the case of unwinding. Still, using a closure at all introduces more nesting. However, when writing Rust, we often trade off nice-looking syntax for safety and _compile-time correct code_. I believe this is just another case where the trade-off looks a bit worse on the syntax side, but provides much better ergonomics overall.
You don't need to trust every binary on the internet to use Rust. You only need to trust one.
&gt; Now that i realised that Rust isn't build from c++, i'm less tempted to try it. Not sure if I should be sharing this as it is someone's WIP, but you might be interested in [mrustc](https://github.com/thepowersgang/mrustc) - an alternative rust compiler written in C++. Compiling rustc is one of the medium-term plans. It is very much a WIP however, and I don't believe it has any official support other than thepowersgang's time and amazing skills.
No, i was just considering playing with Rust to see if the language was fun enough for me to use. But actually there is no way i can do that easily and i don't plan to port a language that i'm not even sure to like. I do not trust Firefox that much but more than any other modern browser. But the Firefox binary i use was compiled by myself on the system of my choice :-)
Sorry, that was a bad example but if you have a closure that needs braces then it shouldn't be a one-liner
Have you seen this article in which someone put a whole lot of thought in to the best way to do state machines in Rust? It really does get pretty verbose, though. https://hoverbear.org/2016/10/12/rust-state-machine-pattern/
What /u/aturon said is correct. For undergraduate internships, they must be done on-sight. Our graduate research assistant program (for eligible PhD candidates with an existing research relationship with Mozilla Research) are available for remote work in countries where we may legally employ students.
I don't think #[cfg(unix)] is correct, I have never heard of /dev/serial in BSD land, `#[cfg(target_os = "linux")]` may be better.
I dare say Rust is one of the most portable and ported programming languages there is, and intentionally so. For something with more ports (but not necessarily more _portable_) one needs to look to the big languages like C/C++, Java. Rust has binary builds of the compiler for [these platforms](https://github.com/rust-lang-nursery/rustup.rs#other-installation-methods) (more than indicated in the op), and there are more coming. Soon there will binary compilers for Linux for three varieties of mips, three varieties of powerpc and s390x as well. That's just platforms the compiler runs on and has binary builds for. We distribute binaries of std for even more. Rust is currently being ported to MSP430, the AVR, the Fuchsia operating system, and others. You mentioned wanting SPARC support, and Rust doesn't have it today. This is only because nobody has been motivated to do it. Someone should. You also mentioned not wanting to bootstrap from binaries, but there is no other way. All self-hosted languages bootstrap from a binary, even C. One must trust somebody to provide those binaries. In Rust, today one must trust the Rust project, but there are other entities creating their own bootstrap chains, Fedora probably being the most advanced. So at some point you'll be able to trust Fedora if you don't trust us. It sounds like you are quite convinced not to use Rust at this point. Maybe you'll come back and check it out in a year and think differently. 
&gt; I though that the bootstrap tool was generating C++ code to be compiled on GCC or LLVM after seeing a requirement page somewhere. Well, *technically* the compiler is generating LLVM bitcode to be compiled by the LLVM. Clang does the same thing - it generates LLVM bitcode from C++ code. 
well, in case you're motivated to look again in the future, and someone has added sparc as a host, you could use the untrusted blob to emit llvm IR, have the option to inspect that, and then compile it down to machine code with the trusted LLVM from your OS? not sure if that would be good enough for you, but it's an option at least
&gt; why should i have the same trust in the Rust binary than the one provided by my operating system ? So you don't trust the Rust developers but you trust your operating system vendor? And what operating system do you use, if I may ask? Because, you know, if it's Debian, [then you already have Rust in your repository](https://packages.debian.org/sid/rustc). Or [Ubuntu](http://packages.ubuntu.com/yakkety/rustc). Or [Fedora](https://apps.fedoraproject.org/packages/rust). Or [Arch](https://www.archlinux.org/packages/community/x86_64/rust/). Or [FreeBSD](https://svnweb.freebsd.org/ports/head/lang/rust/). Or [CentOS](http://dl.fedoraproject.org/pub/epel/7/x86_64/r/rust-1.11.0-3.el7.2.x86_64.rpm). Or [Mageia](http://distrib-coffee.ipsl.jussieu.fr/pub/linux/Mageia/distrib/5/i586/media/core/release/rust-0.10-4.mga5.i586.rpm). Or [OpenSUSE](http://download.opensuse.org/distribution/leap/42.2/repo/oss/suse/x86_64/rust-1.10.0-1.4.x86_64.rpm). I can keep on going if you want; there are a lot of Linux distributions out there.
this is the really humbling part. People went a lot farther than I could imagine. The usual conversation on IRC is a bit like this: "Hi, how do I do &lt;simple thing X&gt;? I'm stuck on this. -Here you go: &lt;solution&gt;. By the way, what are you working on? -oh, just &lt;extremely advanced project I wouldn't be able to do&gt;, no big deal"
There's a bunch of flat map stuff described in the Error Handling chapter of Rust Lang that's (hopefully) helping me grope my way towards idiomatic Rust. [Error Handling](https://doc.rust-lang.org/book/error-handling.html)
Ah, I thought these were part of the same program. Thanks.
I think you probably do need a new target, yes, so you can have something that pretends to be target_os(unix) throughout. FWIW I'd name that based on cygwin, as that's fundamentally what MSYS2 is, just with a different library name for the ABI part perhaps. Or see what LLVM calls the target triple already, assuming LLVM is even there.
&gt;but without luck, it still fails to find the symbols needed (openpty, fork, and whatnot), so I'm assuming it's not even trying to link to that. If it's just failing to link because it can't find the appropriate symbols, can't you just figure out which library provides those symbols and tell rustc to link it? o_0 Also, delete Rust's bundled mingw component, because it may be interfering with your cygwin/msys versions.
Loading dlls at runtime is not weird, I think [libloading](https://crates.io/crates/libloading) is the easiest way to do it in Rust. To statically link to any library you can write a [build script](http://doc.crates.io/build-script.html). There are probably other ways but I'm not sure.
I remember trying it out, but it failed with conflicting symbols or something on that line.
You definitely can link to Windows APIs from cygwin/msys, but maybe the winapi crates would need modification to allow such an environment. But to use cygwin APIs, I think you really need to be using that entire libc for the process, which is why I think you need this to be a new target. (edit: for example, my [ssh-pageant](https://github.com/cuviper/ssh-pageant) program is cygwin-native, but uses APIs like `SendMessage` to talk to Pageant.)
Yeah, the problem would be making crates confused if you changed the `target_os`, because Cygwin stuff is both `unix` and `windows`. I wouldn't mind having to build a new target that would work with cygwin, I also wouldn't mind having to write my own bindings for the few functions I need, but I have no clue how to build that kind of target or if it would work at all.
Yeah, tried that, stuff segfaults, it can find the symbols but I guess there's some initialization magic that needs to happen for them to work.
I hadn't heard of this project before, but was looking for something like this! Very cool! Also side note, your changelog link 404s. 
Whoops! Thanks, that should be fixed. I knew I forgot something haha.
Honestly I haven't thought about image. What I like in OpenCV is that the Python bindings directly interface with Numpy allowing easily performing arbitrary mathematical operations. For this reason I instantly thought of nd-array, since it aims to provide something similar to numpy. Now I see imageproc probably offers enough to make it the winning option. I suspect it's possible to convert between image and nd-array containers using their low level (unsafe) constructors, so perhaps that shouldn't be considered an issue anyway
Since you don't seem to be too [concerned with memory](https://cgit.freedesktop.org/gstreamer/gst-plugins-good/tree/gst/flv/gstflvdemux.h#n55), how about making `Header` and friends `Copy`-able to make your first code snippet just work: match self.state { ... State::HaveHeader {header, to_skip: 0 } =&gt; { self.state = State::Streaming {header: header, ...}; }, }
As for the web app security issue, seems like a combination of [row level security](https://www.postgresql.org/docs/9.5/static/ddl-rowsecurity.html) and creating a database user per application user could offload some of the permission checking to the database. I have no idea what the scaling implications are of having a db user per app user. 
here: view-source:https://unhandledexpression.com/feed/
well, a name conflict in something that can be namespaced, like a trait, is easier to manage that the `error!` macro conflict between nom 1.0 and log :p
Neat. It might be worth adding to the readme the motivation for doing a rust implementation and a usage screenshot for those of us not familiar with `tldr`. (It prints examples of a command with single line explanations. e.g. I'll probably use `tldr find` instead of `man find` next time I'm using `find`.) 
Maybe so, but these things are orthogonal, this is how you can make the same algorithms go faster.
crates.io (and cargo) provides enormous benefits, and is a very big reason why rust has gotten so popular so fast. A newbie, even new to making software in general can effortlessly benefit from the works of so many different people, and keep it all up to date with a single command and editing a single file. And you don't have to wait for repository maintainers to make the next release or manually install and set up tons of different dependencies. The downside is similar to github. Even though github is awesome, free and easy. What if it isn't always that way. You also encounter other issues too. You are relying on an outside party in the process of compiling your code, which can have [tons of issues](http://www.theregister.co.uk/2016/03/23/npm_left_pad_chaos/). If what you have created is suitable for release to the public, go for it, as long as it is appropriately licensed.
I tried to convert to an Iterator using the `-&gt; impl Trait` feature, but it created errors that I didn't want to figure out at the time. Creating a wrapping struct would be nice, but then I thought, "How often is someone going to be using this in performance critical code?" It would be nice to make the library more idiomatic, but I just wanted to get it out there first. :P Edit: There is a cross platform serial library [here](https://crates.io/crates/serial).
&gt; you can't (t)rust them. really like what you did there
&gt;We changed from an ad-hoc IPC to the much more standard D-Bus! Eugh. Yuck. Keep D-Bus well away from anything you write. Next thing you'll be celebrating becoming a systemd module or something.
Out of curiosity, what issues do you have with D-Bus? From your systemd comment I assume it has something in common with the complaints lobbed at systemd. I haven't read much discussion about it, it simply seemed to fit our needs better that what we were doing before, so I'd like to hear others' thoughts 
Such as?
All the existing methods of Unix IPC.
DBus uses "Unix IPC" (UDS) but adds a standard serialization and RPC format. Entirely different problem domains.
Unlike that guy, I like D-Bus, so I'm just mentioning an alternative because I know it, not because I think it might fit your use case better: [√òMQ](https://en.m.wikipedia.org/wiki/ZeroMQ)
We already have a [fork](https://github.com/avr-rust/rust) to add AVR support. Once LLVM is updated to [4.0](https://github.com/rust-lang/rust/issues/37609), it will then be trivial to enable AVR support in-tree.
In a slightly different vein, we have https://capnproto.org/ :)
Oh yeah, the keyboard driven resize makes a lot of sense for touchpads:) Another useful step is to adjust the default split to whatever you most frequently use. I found that I want a 2/3-1/3 split much more frequently than an even split.
Rust only does unchecked integer overflow in release mode.
I doubt I can improve libstd, but I went ahead and implented `Rc&lt;str&gt;` and `Rc&lt;[T]&gt;` for [my own version of Rc](https://docs.rs/reffers/0.4.0/reffers/rc/index.html) in the [reffers](https://crates.io/crates/reffers) crate. I hope it becomes useful :-)
well, you can, but it is very limited. I'd much rather have a simple solution that does not try to manage the loop for you, and a more powerful one based on futures and tokio.
Cargo can work just fine with git(hub) [dependencies](http://doc.crates.io/specifying-dependencies.html#specifying-dependencies-from-git-repositories): myawesomelib = { git = "https://github.com/coolguy/myawesomelib.git", branch = "stable" } I'm not sure if there are major 'technical' reasons why you should publish to crates.io. It's more of a community thing where people can publish because they want to be part of a community.
now that Rust has hit 1.0, how hard is it to fix bugs in core data structures like this? do you have to make a new one and depreciate the old, or can you make some subtle internal changes to data structures and assume that programmers are interacting with it abstractly enough that you won't break anything?
[It does, yes](https://is.gd/BuLyyy). I don't enough about this stuff to know wether that's an issue though.
The motivation was simply "me too", since there are already multiple implementations in different languages. But I'll add a screenshot, good point :)
This was just me wanting to write down my thoughts, ergo the unfocused dialogue, but if people end up agreeing with it I'll probably push this through the RFC process.
I'm trying to modify an entry in a 2D vec array that's part of a structure. The code below doesn't work - what am I doing wrong? https://play.rust-lang.org/?gist=9c8e894a5eecf82da2a8ef9cc2c0404b&amp;version=stable&amp;backtrace=0 Thanks! 
Yes, but it works out in this specific case: Base64 encodes every chunk of at most 3 input bytes into 4 characters. Given an input of `n` bytes then your output will contain `(n * 4) / 3 round up to multiple of 4` characters. It's that ceiling operator combined with the input always being a positive integer that makes it equivalent to (and avoid potential overflow): `((n + 2) / 3) * 4` Or in general: `(n * x) / y round up to multiple of x` ‚¨Ñ `((n + y - 1) / y) * x` iff n, x, y are positive integers and y != 0. So yeah this definitely doesn't work in every case, but it does work under these specific circumstances. EDIT: This is an extension of 'divide two positive integers `x / y` rounding up': which is `(x + y - 1) / y` or `((x - 1) / y) + 1` (follows directly from algebra on the first solution, extract `y / y`. However you should use the first one as it gracefully handles `x = 0`).
What does the flat_map do?
https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.flat_map The code `(0..4).flat_map(|x| (0..x))` gives an iterator over the values `0, 0, 1, 0, 1, 2` by chaining the sequences `0..0 == []`, `0..1 == [0]`, `0..2 == [0, 1]` and `0..3 == [0, 1, 2]`.
I believe pcwalton's opinion is relevant, so I will quote: &gt; It is important to have a sensible IPC system for Linux. I see a lot of resistance to this, with claims that Unix sockets and POSIX are good enough. As the maintainer of Rust's ipc-channel, which wraps all this stuff, I strongly disagree. https://news.ycombinator.com/item?id=12802333
The [traverse](https://github.com/reem/rust-traverse) crate provides an `InternalIterator` trait.
This is somewhat of a tangent, but I would argue that we need both push and pull iterators if we want to be as general as possible. The problem with pull iterators is that they can't do diverging flows without either iterating twice or caching the result. Ie. I can't take a single iterator and feed that to multiple sinks. Push iterators have the opposite problem; they can do diverging flow fine, but not converging flow (eg. `zip`). So they are not really equivalent, and neither of them are stronger than the other.
Erik Meijer has some great talks about the theory underlying push-based vs pull-based iteration. The ‚Äúinternal iteration‚Äù proposed here is called a ‚Äúcold observable‚Äù in Rx terms.
Have you also reviewed every single line of source running on your computing? 
I just didn't want to stray further from Rust than necessary; those changes seemed distracting more than enlightening for a non-Haskell audience. :)
What I think is interesting is that `std::iter::Iterator` is a widely used internal iterator trait. Methods `all, any, find, position, rposition, fold` all are internal iteration. Iterator adaptors like map and chain already support composing them, to some extent. It's weird, but the cat is out of the bag. `find` and `fold` and so on are methods that any iterator can provide a special case implementation for.
Right that's a hard part because it requires compromising with the expressiveness of enums. Result simply has the wrong semantics, too wrong to be acceptable IMO. I think that macro control flow akin to `try!` is not a one-off for Result and is a recurring pattern in Rust. This is another enum with the same pattern.
It's one hard optimizer problem. You have a general segmented iteration to turn into a collection of simple loops. Internal iteration allows you to unravel it in straight-line code. [My draft rfc has some motivation and links too.](https://github.com/bluss/rfcs/blob/composable-internal-iteration/text/0000-composable-internal-iteration.md) Note that ndarray and Rust have both merged improvements along these ideas. Every optimizer battles with this and it's not been solved yet. Here's a paper about a different kind of iterator for segmented iteration http://lafstern.org/matt/segmented.pdf
For chain, the optimizer is almost there when it comes to solving it. But what about nested chains and n-dimensional arbitrary strided arrays :-P? Internal iteration can be quite powerful. [ndarray uses it](https://github.com/bluss/rust-ndarray/blob/86c583ba07b6e872d772c753a5c343d4b8f8cf8a/src/iterators.rs#L95-L96) in Iterator::fold, for example.
I don't think it's all that different to `binary_search` returning `Result&lt;usize, usize&gt;`. YMMV.
Sounds odd. Were you using `cargo bench`?
The github repo actually seems to contain a lot of resources, judging from its Readme.md. Mentioned are (among others) "example parsers" and "reference documentation". Going a step further and checking out the reference documentation (which is nom's rustdoc output), it immediately links to several guides, including topics like "how to write parsers" and "error management" (related to those pesky and cryptic compile-time errors). I'm sure nom, as any non-trivial project, requires some time getting used to, in addition to understanding the basics of parsing as well, especially if you're also not 100% comfortable with rust itself yet. But if you check out the documentation that can be found from the repo, and follow it up with questions on nom's irc channel and rust's #rust-beginners and #rust channels, I'm sure you have a good shot at getting a better understanding of this library (and rust in general)! ;)
Nope I didn't. I haven't used it before but trying out now. Iterator code is the same as before. Oddly enough it was actually slower when I ran the bench before I figured out I needed to return the value for the bencher to work on the loop. Loop is three times as fast as before, now runs in +/- 13 ms. So using bencher only impacted the the loop timings. I'm not sure what to make of that, if it's possibly to be that much faster why wouldn't it have been when I just had it in the main function with some timing code. I realize it's not a very scientific way to check but I did ran the code a whole bunch of times to check for variance and given the fact that iterator bencher is giving me the exact same results I'd say the timing itself wasn't unreliable.
Sorry, I just had to...
My title was a desperate attempt to be funny (actually, I stole the pun from IRC).
Off-topic but related thoughts: I would love a terminal emulator written in Rust that can run on X and Wayland by drawing via OpenGL like mpv vo=gl. weston-terminal is limited in compatibility and functionality like vte and Enlightenment's terminal is buggy and slow. xterm, urxvt, mlterm and st are the most compatible in that order, if you have "serious" terminal use, and there's no (ignoring the abandoned st wayland port) Wayland port of either of those. This is a chance for Rust to conquer the terminal emulator space. For portability and reliability it can not use Qt or GTK though, just glfw at most.
Yes, this was a mistake in the article. Luckily the benchmark was not affected. Thank you :).
It was just pointed out to me that the manual loop in the article had its `x`s and `y`s mixed up; my benchmark fixed this but I imagine you one, being copied straight from the article, did not. That might be causing the odd timings.
Ok good point. I went for `Result&lt;T, T&gt;` first and then saw that they could be simpler with `Result&lt;T, E&gt;`. So the exploration code needs an exploration adaptor, huh?
They're not the same kind of software. Tor utilizes an "onion router", which is a specific routing technology with specific guarantees. This is a VPN, which is not designed for anonymity and doesn't include surveillance in it's threat model. Benefits include not needing to rely on Tor, and not running a Tor node. I'd use this if I wanted to communicate between virtual machines across many different services.
Okay... That was weird and nsfw üòú 
So you're right on both counts. This is just a message router, but it actually is for sex toys. I've reversed engineered a ton of hardware and would like one program to control all of it though (for doing ui experimentation), while also seeing what I can do with things like generic control messages. 
Using Rust gives a whole new meaning to safe (sex) cyber dildonics, I guess. No more malware that messes with you(r machine).
I was just thinking today I should get around to learning rust. This may have been the sex joke that tipped the scales.
Yeah I had noticed that, it was actually the reason I was curious about testing it since I wondered if that wasn't the reason the manual loop was faster for you. Thing is it makes half the instructions completely pointless and optimized it all away, so that code runs many order of magnitudes faster. My first bencher test was still flawed though, figured out I was actually testing different types (i32 opposed to u64) since I hadn't annotated the manual loop while I was forced to do so for the iterator one by the compiler. &amp;nbsp; I haven't been able to recreate the initial result of ~41ms for the manual loop in benchmark functions. The only way I get that result if the code is inlined into fn main. If I separate it into a function it gives the same result that I now get in the benchmarks: ~27ms &amp;nbsp; black_box didn't seem to matter, but just to make sure the results are consistent I've used it and also put everything into separate functions to make sure the right types were used: https://gist.github.com/anonymous/16224e4f9d749ce4bd99b8e489fce152 &amp;nbsp; test iterator_i32 ... bench: 76,896,965 ns/iter (+/- 963,031) test iterator_i64 ... bench: 48,974,679 ns/iter (+/- 815,132) test iterator_u32 ... bench: 48,817,044 ns/iter (+/- 718,244) test iterator_u64 ... bench: 34,882,466 ns/iter (+/- 596,859) test loop_i32 ... bench: 13,374,573 ns/iter (+/- 339,575) test loop_i64 ... bench: 64,204,792 ns/iter (+/- 802,579) test loop_u32 ... bench: 8,625,181 ns/iter (+/- 174,210) test loop_u64 ... bench: 27,192,387 ns/iter (+/- 524,541) &amp;nbsp; Two things stand out to me: - Iterator is faster with 64 bit integers than 32 bit integers. - For some reason the manual loop is very slow with i64. 
Can't `collect` be viewed as an eager `all!()` and tell the compiler to fuse the entire iterator chain? Or a `take_by(n)` could still be viewed as a smaller collect. Isn't it only when we want to process element by element that poison our own caches?
So, yeah. Look at it this way: What is the impact of a software fault when that software is managing a sexual situation? While it sounds funny, that's actually a serious question these days, as there are multiple software controlled toys out there, and have been for over a decade. The software controlling those toys currently is... not good. Not good at all. I'm actually part of a toy security auditing group at http://internetofdon.gs (yes, no chance for a stupid domain goes unused in this area of research), and some of the stuff we're finding is beyond scary. The biggest problem is trying to convince people they want a rusty buttplug, though.
Ok, for background, my about page might help a little more than my github org page: http://kyle.machul.is/about I've been working in sex and technology for 12 years now. Started off in 2004 running a website at https://slashdong.org (now https://metafetish.com, as having a domain name that makes fun of slashdot stopped being a relevant joke most people get a few years ago and having to answer questions about why I had a domain name about mutilating genitalia was getting old). At that time, I was working in educational robotics, and was trying to figure out how to get people in the 18-35 age range interested in learning robotics/programming/engineering. Figured "hey, why not use computer controlled sex toys as a basis for that?", building off the ideas of sex used in advertising. It worked pretty well. Ended up teaching workshops on building your own vibrator with an arduino, giving lectures around the world on the topic, etc. Since then, the interest has involved more in the HCI direction. The question of "How do we make intimate encounters over technology mediated systems feel natural?" is something that few people have even gotten close to answering. There's a lot of work in things like hug jackets, remote hand holding, etc, but I'm kind of working in the logical extreme of the situation, just to see what I can come up with and how it adds to the discourse. Also, this is sort of a simplified version of haptics programming. I work with larger scale haptics devices a lot (Phantom Omni, Novint Falcon, etc), but those are fairly niche products and fields. This is an area of haptics that everyone understands pretty quickly. The problem this project is addressing is that I have a bunch of different drivers for a bunch of different hardware (I also do a lot of work in reverse engineering of bluetooth/usb/etc devices), but if I want to build an application that just issues a command that's like "run vibration at a certain speed", I have no way to take that generic command and turn it into a hardware specific message. On top of that, I need to be able to hand the software to end users that have absolutely no interest in programming or building interfaces, so they can use it on whatever hardware they have and trust it not to crash/leak info/etc. This project is the core of that, building the hardware connection management and message routing system. Hope that somehow answers the question. This is something I could write a novel on. :)
Have you seen [the Rust YouTube channel](https://www.youtube.com/channel/UCaYhcUwRBNscFNUKTjgPFiA)? It has ~20 more videos.
What does "deep down in stall 15" mean?
"Deep down I'm still 15"
Someone on the IRC told me about it. If I have some free time left I'll integrate those into the repo :)
Most internet connected bondage gear is currently self-made, and most of it is... scary. For some reason the BDSM community LOVES pascal/Delphi. I'm not kidding. There's still a ton of work in the ethics areas too, though that's branching outside of the programming discussion here. In terms of non-bondage BDSM stuff, there's a lot of extremely old/weird/unsafe network control systems for Electrostimulation (think TENS except for sex). Control of those systems is part of this project (for instance, https://buttplug.io/hardware/erostek-et312/, where we've even cracked and reversed the on-board firmware).
Ah my bad, I made some assumptions: * git dependencies are 'less bad' than path dependencies * path dependencies require the user to manually install the dependency at the specified path * git dependencies can be fetched automatically * thus conclusion: git dependencies aren't as bad as path dependencies from the perspective of using cargo * if crates.io has any reason to reject git dependencies * surely it would also ban path dependencies as they are strictly worse (assuming I understand path dependencies as stated above) * but path dependencies aren't banned =&gt; so why would git deps be banned? That's the reasoning I went through after I couldn't find the exact requirements but I could find tangentially related restrictions and I tried to derive restrictions on git deps based on that. With all that said, where _are_ the crate restrictions published? Google queries like "rust crates.io crate restrictions" don't result in a neat place where the restrictions are explained.
Do you have something more concrete to fill me in with? These are high level transformations like `fold(f, start, a ++ b)` =&gt; `fold(f, fold(f, start, a), b)` and so on, not sure if MIR can attack them at that level.
Lol, that was good mobile phone typo
You're missing an important point http://doc.crates.io/specifying-dependencies.html &gt; However, crates that use dependencies specified with only a path are not permitted on crates.io. If we wanted to publish our hello_world crate, we would need to publish a version of hello_utils to crates.io and specify its version in the dependencies line as well: As for why `git` dependencies don't work, it's because that host could disappear someday. crates.io really focuses on reproducability, and in order for that to work, you have to make sure you're not relying on dependent stuff. &gt; where are the crate restrictions published? "Crate restrictions" is a bit too broad. What specifically do you mean here? What can and can't be published to crates.io? (Cargo's docs in general need some work. Someday...)
I mean, at the very least, a cloud connected dildo that can make network requests and could be hacked to get control of it could [contribute to a DDoS](http://www.theregister.co.uk/2016/10/21/dyn_dns_ddos_explained/)...
/r/playrust... !!
Thank you so much for this! I've really needed serial enumeration and serial-rs hasn't integrated their PR yet.
&gt; Buttplug is a framework for hooking up hardware to interfaces, where hardware usually means sex toys, but could honestly be just about anything. I can't wait to try to pitch using *buttplug* at the office when starting up our next IoT project. On a serious note, this looks very cool, nice work!
I'll try to look at helping you later. RemindMe! 1 month
For an example of using termion, my project (very early development) [mutxt](https://github.com/moosingin3space/mutxt) shows how I make a simple text editor. 
I will be messaging you on [**2016-12-26 21:33:04 UTC**](http://www.wolframalpha.com/input/?i=2016-12-26 21:33:04 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/rust/comments/5es4n7/serialenumerate_a_rust_crate_to_enumerate_serial/dagrffl) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/rust/comments/5es4n7/serialenumerate_a_rust_crate_to_enumerate_serial/dagrffl]%0A%0ARemindMe! 1 month) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! dagrh33) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
Again, show your work. How is your handcrafted IPC on top of Unix mechanisms simpler than re-using D-Bus, which already exists, and as /u/Iprefervim says, fits Way Cooler's needs? What mechanisms will you use and how will you solve the problems Way Cooler needs solved with its IPC?
Oh... cool. Misunderstood the purpose of rpgffi.
Thank you! I'll add a special category for the RustConf 2016 :) The one by Felix Klock is already on my list, I'll add that as soon as I have some more free time.
I'm not sure whether you're suggesting an optimizer change or a change to `collect`. If the later, yes. If the former, I'm not quite grasping what that would entail. The `next` calls already get entirely inlined.
Thanks for the heads up. Line drawing characters seem to not work with all Unicode truetype/opentype fonts while they do in xterm. Weird, as st actually displays the most glyphs correctly from plan9port `font/sample`.
Init itself can be really simple, see for example [sinit](http://git.suckless.org/sinit/tree/sinit.c); less than 100 lines of basic C code. Here is the main init shell script, [rc.init](http://git.2f30.org/fs/file/bin/rc.init.html), that can be used with sinit. I'd say you'd probably at minimum want to understand how they work. Of course you can design your init system in a completely different manner, but it still needs to the same basic tasks; set up/shut down the system and reap children.
Wouldn't Pascal/FreePascal/Delphi be less scary than C or C++ because pointer arithmetic is limited (dunno exactly how `$POINTERMATH` works or what it enables) and people don't use null-terminated `char *`s for strings?
You know, this is actually a pretty good point. My distance from having seen Pascal/Delphi is enough to have basically made me completely ignorant on the subject. I tend to find those writing the software a bit below what I'd consider a "safe" experience level to being doing crazy physical interaction stuff, but maybe I shouldn't blame the implementation language as much. :)
lol slashdong. I still find it funny.
If the version is only allowed to occur at the top, why don't you just put a do_parse! in the parse_glsl function and have it specifically look for a version, and then many0! of the glsl_alt (that would not include the version number function)? Very interesting, and nice to see Nom 2.0 already being used. Nom is such a cool project.
Ah, weird. I guess I must have had a different problem than I thought I had. It may have been complicated by generics? Like, maybe it was a generic struct and the SomeTrait depended on the generic?
I think the default lifetime is 'static Box&lt;SomeTrait + 'static&gt; You probably have to explicitly set a different lifetime.
&gt; Maybe if there's no break/continue/return (99% of loops?) you can get away with a naive desugar. Remember that the proposed rule to use all applies to simple loops. This restriction may not be too bad. We can improve the heuristics over time, anyway.
This is awesome! This combines some some of my favorite things: Sexuality, software development, and Rust! I was really disappointed when Dilduino died, so its nice to see some cool open source projects in this area. üòÑ Wonderful idea /u/qdot76367!
Holy sh*t you should be an IDE plugin for compilation visualization. Thanks that was awesome. Seriously turn this into another blog post on how iterators are compiled.
Same with TCO. Hopefully MIR becomes internally stable so it can be opened up to have extended internal passes. 
It's working fine for me? http://github.com/qdot/systray-rs
Would it be possible to release the new hasher as a crate on the crates.io ecosystem? It would certainly be a nice thing to have available for use.
Rust's monomorphisation and value types look to make a lot of that paper unnecessary complexity. There are also some ideas I don't think matter in practice due to LLVM being fairly decent, such as *5.1 Fusing the Stepper*. Most of the rest of the gains are from copious quantities of specialization. The current approach seems a lot cleaner. 
Outstanding. Literally gods work.
Sorry /u/pcwalton, I disagree. Maybe Linux needs 'a sensible IPC system', but D-Bus is not it.
Nice to see you post it here :) Thanks for the tool as always.
Nice! I hardcoded the rotations purely because that was the first thing that came to mind.
For compile-time integers (a.k.a type-level integers), until there is proper support in the compiler, see [typenum](https://github.com/paholg/typenum). This library implements them with traits.
Haha it took quite a bit longer :P
Another thought: For existing projects you may prefer to fix your style issues as you make changes, rather than all at once. Something like this would be a start: if [ "$TRAVIS_PULL_REQUEST" != "false" ]; then cargo fmt -- --write-mode=diff $(git diff --name-only "$TRAVIS_COMMIT" "$TRAVIS_BRANCH" | grep \.rs$) else cargo fmt -- --write-mode=diff $(git show --format= --name-only "$TRAVIS_COMMIT_RANGE" | sort -u | grep \.rs$) || true fi Bonus: The `|| true` makes merge/commit style violations informational only, which is handy for pull request oriented projects.
Oh I see, the wording makes it sound like path dependencies are allowed, iff they specify a version. Thanks for taking your time to explain! And yes I meant what are the "soft" and "hard" restrictions for publishing a crate on crates.io. Eg. a soft restriction would be that material uploaded should be relevant to the rust ecosystem (eg. no uploading porn) but is enforced by human intervention, a hard restriction is something that crates.io outright rejects automatically without human intervention (eg. crates with path or git dependencies are rejected, crates larger than 10MB are rejected). Somewhere all such restrictions are listed. Thanks again for your excellent work :)
Wow that's amazing! The other day was my first time ever using Travis CI, so I haven't dug _too_ deep into it yet, but those environment variable that it provides are fantastic! After I play around with it a bit and fully grok it, would you be okay with me adding it to the article and attributing it to you?
Go for it! btw, I noticed that rustfmt has a `--file-lines` parameter that accepts a JSON string. I couldn't solve it in the few minutes I thought about it, but if you could get file names + modified lined numbers out of git diff/show (it may involve parsing the diff block headers), it's possible you could fail for style violations only within the specific code that was changed.
Ah! Yeah, so here's what's happening: - `cargo install rustfmt || true` tries and fails to install rustfmt to `$HOME/.cargo/bin`, but takes its time doing so - Travis caches `$HOME/.cargo` -- that's just crate downloads, git checkouts, and binary installs, not build objects for projects or installs - `cargo install --force rustfmt` means building rustfmt every time, so it doesn't benefit from the cached binary *or* avoid the build time... though it does mean you get an up-to-date rustfmt :-) A more correct way to solve this: test -x $HOME/.cargo/bin/rustfmt || cargo install rustfmt Another option is to install `cargo-update`, then use it to *update* rustfmt. :-) test -x $HOME/.cargo/bin/cargo-install-update || cargo install cargo-update test -x $HOME/.cargo/bin/rustfmt || cargo install rustfmt cargo install-update rustfmt (Unfortunately, `cargo-update` doesn't fall back to install if the package isn't installed.)
/r/jobbit
I personally also find it annoying how there doesn't seem to be a way to make libinput feel like the Xorg synaptics driver, for touchpads. For some reason it just doesn't expose the settings I'd like to change..
Shouldn't this rather be part of rustc? The diagnostics and the proposed fix happens there, why not just emit a patch file on demand that can be applied if needed. This seems to be a bit hacky IMHO.
&gt; This seems to be a bit hacky IMHO. Absolutely, I hacked this together just to see whether it could be done. I don't think it should be part of rustc but rather part of the tooling for IDE integration. (It should be no problem to offer these suggestions in a language server plugin.) 
Yes you are right. RLS seems to be more appropriate for that kind of functionality.
Why is this slower than the internal code? It looks fairly optimal to me...
This is elegant! Two thoughts: 1. I kind of wonder whether we should use the name "generator" for this somehow. It seems too good to pass up: this basically *is* the Rust equivalent of a Python-like generator. 2. Thinking ahead to a possible future in which the compiler automatically calls `fold_while` in `for` loops in lieu of `next` if it feels it's profitable to do so, is there anything we can do about the compiler's inability to reason about termination of `break`/`continue`/early `return`?
Is there any way to be generic over ownership? I declared a helper trait to provide an extension method, and only need immutable reference to it for the lifetime of a function call. However, in order to support calling the extension method for refs, mut refs and owned values, I had to do the following impls: pub trait IntoIp { fn into_ip(self) -&gt; IpAddr; } impl&lt;'a, 'b, 'c&gt; IntoIp for Request&lt;'a, 'b, 'c&gt; { fn into_ip(self) -&gt; IpAddr { self.request.remote_addr.ip() } } impl&lt;'r, 'a, 'b, 'c&gt; IntoIp for &amp;'r mut Request&lt;'a, 'b, 'c&gt; { fn into_ip(self) -&gt; IpAddr { self.request.remote_addr.ip() } } impl&lt;'r, 'a, 'b, 'c&gt; IntoIp for &amp;'r Request&lt;'a, 'b, 'c&gt; { fn into_ip(self) -&gt; IpAddr { self.request.remote_addr.ip() } } Is there a preferred way to do just one impl? There's all kind of helper traits, like `AsRef`, `Deref` etc., which should I use to get the easy way out?
In C++, all types are sized. In Rust, there's a concept of an "unsized type", which is a type that represents a family of related types of different sizes. The two big examples are slices (`[u8]` can be a `[u8; 3]` or a `[u8; 1000]` or a piece of a dynamic allocation) and trait objects (`Display` could be `i32` or `String` or lots of things). Because, like C++, Rust has value semantics, unsized types can't be used directly. They need to be behind some sort of indirection, like `&amp;[u8]` or `Box&lt;Display&gt;`. A pointer to an unsized type is sized. One difference with how Rust and C++ handle virtual dispatch is where the vtable is stored. In C++, the layout is like this: struct base_vtable { void (*do_a_thing)(); }; base_vtable __base_vtable { base::do_a_thing }; base_vtable __derived_vtable { derived::do_a_thing }; class base { base_vtable* vptr = __base_vtable; int actual_member; virtual void do_a_thing(); }; class derived : base { base_vtable* vptr = __derived_vtable; int actual_member; // Implicit, inherited from base int other_member; void do_a_thing(); }; In Rust, instead of being stored as part of the object, the vptr is stored with the pointer, and the language natively supports "fat pointers" that are something like struct TraitObject { ptr: *mut Something, vptr: *const Vtable } &gt; Why are there no type-level integers except in the built-in arrays? WIP. Rust's generics are a bit more complicated than C++ templates, thanks to how Rust's are based on interfaces and C++'s are duck-typed.
No but maybe more than you.
Packet reassembly and fragmentation *are* application-level functionality according to the current definitions. I wouldn't be totally opposed to some good very cross-platform very non-Linux-specific fragmentation and resassembly protocols being adopted, but the kernel definitely shouldn't just be putting HTTP-specific details into the kernel, which people often seem to want. That is, application-level protocol details are policy, not mechanism. New mechanisms both in the kernel and in the transport protocols to make those policies easier to implement over existing transports? Absolutely fantastic, totally support that. New transports that are Linux-specific? Yuck. Integrating policy into the kernel directly? Yuck.
To be clear (me being new to rust and all) once LLVM 4.0 is added to the main rust project, then avr will become just another target I can install via rustup?
You don't understand. I shouldn't have to do packet fragmentation and reassembly *at all*. It has nothing to do with HTTP. It is a dumb workaround for the random limitations of Unix IPC mechanisms. Compare the code for Mach, a proper IPC system: https://github.com/servo/ipc-channel/blob/master/src/platform/macos/mod.rs#L560
Sure but that's not really what I'm asking: if I have `let e = some * complicated ^ expression`, is there no way for me to say `type MyType = typeof(e)`? This way you can get all sorts of complicated implicit resolution and yet still easily refer to whatever the result of that mess was easily.
Doesn't seem so, [but typeof is reserved, so it's in the cards](https://github.com/rust-lang/rust/issues/3228)
I feel like the rules setup in rust allow for the amazing works that are made. Then again I never got very into C++ (yet)... just curious, what draw backs do you think are most notable for C++ vs rust?
Oh, then it connects to a lot of things I don't know! I like the name fold while, something mundane that connects well to what it is doing. I'm looking for an incremental improvement; I would not propose something as revolutionary as changing the for loop desugaring. Using closures in the for loop is a nonstarter, isn't it? It has so many limitations w.r.t ownership and borrowing rules.
I love it! With a warm filesystem cache, it can count a 1 million line project in about a second. I love fast tools like this that are only bottlenecked by the filesystem.
Oh cool, I didn't know that serial-rs wanted to add serial enumeration. As for your questions: A) I just return a `Vec` of `String`s. It's not exactly "nice", but you can just put it straight into serial-rs. B) I hadn't really planned on integrating it, but I'm not opposed to it. From your pull request it looks like dcuddeback wants to add this functionality anyway.
Had I had foresight I would have chosen [a much more obvious example](https://godbolt.org/g/fA4Fcp) where the downsides are far more apparent and intrinsic. In the case of the example I did choose, though, the problem basically boils down to the fact that the code is much too branchy, the principal cost being that [it stops LLVM doing this](https://godbolt.org/g/jI2qIC). You can compare the "intrinsic" complexity (from the point of view of the compiler) [by compiling for size](https://godbolt.org/g/NFNxy9), where it's obvious the straight loop results in simple code.
The basic idea of this, as far as I understand, is that typechecking bounded generics creates constraints, and the unification of those constraints create nonsense types. These would normally be uninhabitable, but null is a member of every object type in Java, so when you check that the RHS of an assignment matches the unified type of the expression, you get a witness of a type that shouldn't have a witness. This means you end up being able to convince the typechecker that arbitrary types are subtypes of each other. So the problem isn't that there are casts, it's that the type system thinks that all the implicit casts it put in are upcasts when they in fact are not. I think the big problem is that the typechecker never checks that the types that are generated during unification are valid types - it only checks that the assignments are valid. Since we have a universal subtype in Java, this becomes an issue. If anyone knows more about this than me, please correct my inaccuracies!
What would it mean for a room to have direction 'Left' enabled? E.g. I am inside this room: Room { name: "Starting Room", description: "The starting room. It seem really boring here", items: [], monsters: [], exits: [Exits::Left]` } I go left. Where am I now? Your rooms do not link to one another, so how do you decide what going in a direction means?
Seeing so many redundant specializations of so many iterator methods is actually what prompted me to write this!
I've been bashing my head trying to fix this, so I've decided to just head on over to using Ruru/Thermite now and see how far I can get. 
The thing is, even if you have type `Object` you still can't have `&lt;? super Integer&gt;` extend `String`. But `null` is what makes it work. So you can have type `Object` in Rust and it would still not type check this program. There is no trick in Rust that lets you instantiate that type and call a method on it.
I would really like `3 * 4.5` to work, though.
Watch out, the sum adds up to 41654127500, which doesn't fit into a i32 or u32
"Hey Tim, I can't get the CNC router to hook up to my new laptop" "Did you install Buttplug?" "..."
I would use the `Borrow` trait as it's implemented for `T`, `&amp;T` and `&amp;mut T`: use std::borrow::Borrow; impl&lt;'a, 'b, 'c, T: Borrow&lt;Request&lt;'a, 'b, 'c&gt;&gt;&gt; IntoIp for T { fn into_ip(self) -&gt; IpAddr { self.borrow().request.remote_addr.ip() } } The `AsRef` and `Borrow` look very similar, but `AsRef` is not implemented for `T`, just `&amp;T` and `&amp;mut T`. So if you want to be generic over ownership, use `Borrow`. However, I might suggest not consuming ownership at all, but instead doing something like: // Slightly longer name that makes it easier to register when skimming docs pub trait ToIpAddr { fn to_ip_addr(&amp;self) -&gt; IpAddr; } impl&lt;'a, 'b, 'c, T: Borrow&lt;Request&lt;'a, 'b, 'c&gt;&gt;&gt; ToIpAddr for T { fn to_ip_addr(&amp;self) -&gt; IpAddr { self.borrow().request.remote_addr.ip() } } It doesn't make sense to have your trait method take `self` by-value if it's not necessary.
Take a look at https://tokio-rs.github.io/tokio-timer/tokio_timer/index.html That should at least help to answer the timeout question. You would use a future that tries to read from your IO, and a timeout future and then select on both. Almost like in C.
It's easier to interoperate with c in c++ so there's far less incentive to write a wrapper. In rust you already have to write a wrapper so you might as well write a good one. Disadvantages may come when "small" changes to the c api require drastic changes to the rust wrapper. In that case there is a lot of work for both the maintainer and the user that simply wouldn't exist if using the c api directly. Perhaps it would be clever for rust to keep its current ffi so as too encourage people to continue to write safe wrappers instead of the easier direct usage of unsafe APIs.
This is exactly the kind of explanation I was looking for, thank you!
Out of curiosity, why did you choose `wlc` instead of `libweston` for the protocol implementation?
It's... Interesting. But also a good way for a user to *not* learn what caused a problem and why. Hmm...
Thanks for pointing me in that direction. I [think](https://gist.github.com/robojeb/18546cfc2941b4cb1a93630896db2caa) that I have something that should work but it only ever waits for IO, it never hits the timeout. I know that the error handling is dreadfully poor in this but I am just trying to understand futures. 
Hm, aren't you using blocking I/O there? The futures run in the same thread, so if the I/O blocks....
Is there any way to have `match` pattern bindings to be by-ref in the guards, but by-value in the matched branch, or does this need non-lexical lifetimes or something to work? match pi { Some(pi) if pi.item_type == "question" =&gt; { ... Ok(Some(pi, ...)) }, Some(pi) if pi.item_type == "exercise" =&gt; unimplemented!(), // FIXME Some(pi) if pi.item_type == "word" =&gt; unimplemented!(), Some(_) =&gt; Err(ErrorKind::DatabaseOdd.to_err()), None =&gt; Ok(None), } For testing the equality, it would suffice for `pi` to be by shared reference, but when the right branch has found, `pi` needs to be moved in. Is this possible?
Btw. I already solved the problem by refactoring a bit, but it's uglier and involves an `.unwrap` in a place that's impossible to fail (proved earlier by the match). It would be nice to have an elegant solution.
hmmm, that makes sense. I have no idea how to make the IO non-blocking though. I kinda thought that the futures crate read_until would handle that somehow, but that now seems optimistic. 
In addition to the fact that this is not a JVM issue, the article *does not* say that having null makes a type system inherently unsound. You can use null in Java and Scala to provide invalid support for typing judgments because of other features of those languages' type systems. Of course, Rust unsafe blocks are essentially considered 'untyped'; if you include actions permitted in `unsafe` Rust's type system is definitely unsound - in fact that's the whole point of having `unsafe`!
Rust provides a nice safety angle here. No corner condition segfault that takes down your remote-control vibrator right on the verge of climax. Also less worry about hackers injecting an attack that exploits an out-of-bounds array access to execute arbitrary code on your fucking machine...
&gt; The trick is that null is part of every type. I haven't actually checked what they do in this situation, but in general terms it may be vaguely worth noting that using recent Java's ["type use annotations"](https://docs.oracle.com/javase/tutorial/java/annotations/type_annotations.html), you can, with the aid of augmented type checkers (a [small one built into Eclipse](http://help.eclipse.org/luna/index.jsp?topic=%2Forg.eclipse.jdt.doc.user%2Ftasks%2Ftask-using_null_type_annotations.htm), and [a big fancy one in the Checker Framework](http://types.cs.washington.edu/checker-framework/current/checker-framework-manual.html#nullness-checker)) exclude nulls statically. This shouldn't be confused with various older null annotations, java 8 added a [new more powerful kind of annotation](http://docs.oracle.com/javase/8/docs/api/java/lang/annotation/ElementType.html#TYPE_USE). This newfangled stuff hasn't caught on much in the Java community yet, but it exists and is usable today. Perhaps of particular interest: you can also do [linear typing](http://types.cs.washington.edu/checker-framework/current/checker-framework-manual.html#linear-checker)! 
That's a good point.
I'm back. No idea what to do yet.
Does this also mean that Haskell's type system is unsound, because `bottom` is a member of every type? 
Thanks for making the project sound like a Charlie Stross novel. :)
Kushal is one of the cpython core devs. I think it's pretty great that respected folks from other communities are joining Rust, and I think it's been a large part of Rust's success in the past by bringing in different perspectives and also helping Rust gain traction in those communities.
While I always appreciate seeing more cases of C/C++ things getting Rust competitors, I really hope you fully understood the magnitude of what [TADS](http://tads.org/) has accomplished when you chose to compete with it rather than trying to port part of it to Rust. (Bad text parsers are the [bane](http://tvtropes.org/pmwiki/pmwiki.php/Main/YouCantGetYeFlask) of the IF world and TADS is the gold standard on that front. Not only does it allow for very natural clarifying conversations, it's also paired with an advanced world model that allows the engine to infer that, if the developer defined light and dark states for a room based on the presence of a light source, putting the light source in a box and closing it should darken the room.) TADS also beats out Inform 7 (the other big name) in that it's got HTML TADS, which lets you do illustrated Interactive Fiction with ambient sounds and things like "just click on the direction to go there or on the object's name to pick it up" while still gracefully degrading to plain old text on a non-HTML-capable TADS runtime and both of them have runtimes for all major platforms. (Yes, I did a [comparison](http://blog.ssokolow.com/archives/2014/08/27/recommendations-for-an-interactive-fiction-engine/) of existing open-source options. How did you guess? :P)
I'm not the author but IIRC Way Cooler started before libweston was usable. 
Thanks.
Why do you think i am judging Rust ? **I** told that **i** trusted more the compilers i use on **my** own systems because **i** know better the people who are working on it than the people that work on Rust. It doesn't matter here to say which they are because i speak about trusting people and not technical merits. If you are thirsty of truth i am using OpenBSD. I am suspicious of nothing toward Rust, it's just that **i** am not confident about using a blob from unknown people whenever they say *"you can trust us"*. BTW, the original matter was portability : i can't use Rust because there is not easy way to compile Rust from a well established existing and universal compiler AKA C or C++. This is **my** choice even if i am wrong.
My operating system of choice has an amd64 port but it is build from a blob that comes from a source that i do not trust. I don't want to use a blob in this case and this is my choice. I respect the point of vue of the Rust community but it seems that a part of this community can't respect mine.
Thank you for providing this solution. 
Asking in general is hard, because there is no specific reason why rust should be worse. It could be a sub-optimal implementation of the benchmark, a compiler bug, or a real problem, and its hard to know without looking at a specific benchmark very carefully and trying to optimize it.
Technically i can read C++ code. Technically i can also read assembly if i really want to know what the bootstrap do but i just don't want to spend time to do it. I just wanted to play a bit with Rust for 5 minutes ...
I wrote a wrapper for the [blorb](https://github.com/thefarwind/blorb-rs) filetype, so I can write a port of glulxe. And for something completely different, I've been having fun writing a port of rust's result type to java, because I'm fed up with having no good way to handle checked exceptions inside lambda functions in java.
&gt; In C++, all types are sized. Not true! C++ does have a notion of unsized types, they are called "incomplete types". The only examples are `void` and any type that has been forward declared but not yet defined (e.g. `class foo;`). They behave exactly as one would expect: - They cannot be instantiated directly, only used via a pointer: `void*` or `std::unique_ptr&lt;foo&gt;` - Pointer arithmetics is illegal: `foo* p; ++p;` doesn't compile - Even `std::vector&lt;foo&gt;` works with some precautions (at least with modern compilers)
Adding some logging to PlanetKit using `slog`, and retrofitting an ECS using `specs`. If I find the time this week, that is, which is looking decreasingly likely.
Point. I'm so used to having no time that I've grown used to rolling my code kata into some new practical hobby project that I can afford to slow down the delivery of.
Adding interprocedural analysis to Panopticon. Hopefully it will track function arguments from the caller into callee and allow Panopticon to disassemble dynamically linked ELF files.
I'm a novice, so I may have made a mistake, but I think this is what you're looking for. * https://doc.rust-lang.org/book/deref-coercions.html * https://github.com/rust-lang/rust/blob/master/src/libcollections/vec.rs#L1475
The reason Scala has null is for good integration into existing Java code, not because of the JVM.
Thanks for the explanation.... So does that mean `null` in `constrain` is kinda dynamically typed having an unbound type parameter which gets resolved into a concrete type at runtime when `upcast` is called? What is the solution here? To me, it looks like if Java had type inference like Rust (and static resolution of types), then having the `upcast` call in `coerce` will resolve the type of `constrain` to `Constrain&lt;U,U super T&gt;`, which would make the call `Unsound.&lt;Integer,String&gt;coerce` illegal since `String super Integer` is illegal. Is that right?
I'm working on an interpreter for a language that looks like rust and python had a kid. It'll be interesting to see how far I can get
I've started work on a program for automatically grading Linux assessments. It uses a master/slave pseudo terminal pair to collect the output of commands that my students run. This week I plan to get the program to read a configuration file with test questions and answers, then compare terminal output with the expected answers.
It is worth to note that Scala 3 (Dotty) has a sound typesystem (http://scala-lang.org/blog/2016/02/03/essence-of-scala.html) based on the DOT calculus. In fact, it has been formalized and proved by fully formal methods: https://github.com/TiarkRompf/minidot (One of the major goals of the Dotty project was to address unsoundness in the language)
Should be `Refl :: Equal x x` and `Witness` is unused. IIUC the basic difference is that you can only impose bounds through values. The datatype `Equal x y` doesn't tell you `x == y`; you can only learn that by looking at values and imposing equality through type unification. 
We have `uninitialized`.
`c.display(r);` doesn't seem to me safe to call in parallel... The best way to use parallelism is to use "pure" functions (https://en.wikipedia.org/wiki/Pure_function)... If you can't use them, you have to be very careful about what you do in parallel...
Why did you abandoned it?
there are not many bounded combinators, but you can already use [many_m_n](http://rust.unhandledexpression.com/nom/macro.many_m_n.html) and [fold_many_m_n](http://rust.unhandledexpression.com/nom/macro.fold_many_m_n.html)
I'm curious about this micro-bench as well. There are a few programs were rust uses 5 to 10 times more memory. That is a lot. As for speed, rust is on par for half the programs, and about twice as slow in the other half. That too is pretty bad. However I've observed that in many larger programs, rust beats both popular c and c++ equivalent programs. A couple examples: https://medium.com/@raphlinus/inside-the-fastest-font-renderer-in-the-world-75ae5270c445#.jko7pavdx http://blog.burntsushi.net/ripgrep/ There are others as well. Rust regex is on par or is even faster then everything else, and webrender (part of the servo browser project) is by far the fastest renderer for css there is. I think the main reason this non-trivial programs are so fast has nothing to do with rust performance in itself. I think the main reason is... Rust is a modern language with modern syntax and features. This allows for developers to more easily implement complicated algorithms and data structures. Most of the time, algorithm + data structures easy beat optimization. For instance rust regex uses some advanced finite automata stuff. Some guy build a text editor in rust - called xi - using ropes (a fancy data structure for string data). In his demonstration, he can open 300mb c++ files in like half a second on his laptop, and also scroll smoothly through the file. There is hardly any text editor that can do that sort of stuff. Sure you can build this sort of cutting edge software in c or c++, but its just harder. I think you will over time we will see more and more, cutting edge, high performance software coming out, and blowing c/c++ out of the water. When you provide zero cost abstractions and ergonomics to developers, you will be surprised at the sort of stuff they can build. I already am, if you look at some of the stuff the community is building with rust, you might as well.
http://hannobraun.de/vndf/news/von-neumann-defense-force-is-on-hold/
Rust blog posts lead to more rust blog posts - I love that the 'Fixing Python Performance With Rust' post is what inspired this person to try it out.
Thanks for this info. I will take a look at `mio::Evented`
My guess is that some people who understand that ownership and lifetime being integrated deeply into a language are experienced programmers who think deeply about why software is difficult to create, which pulls some very strong programmers to invest heavily in Rust. 
&gt; Im not building a text parser, just a simple text game. Nothing complicated about it. I feel your post is out of place here? I understood "text game" to mean "a game which communicates with the player via narrative" (because I don't count character-cell graphics as "text")... and I've only ever seen that done as Interactive Fiction. Did you mean something more like NetHack and Rogue? (What everyone I talk to refers to as an "ncurses-based game", "roguelike", "terminal game", "game with character-cell graphics", etc.) &gt; and whats TADS? TADS = Text Adventure Design System. It's an engine for building text-based games where it'll say something like "You see a flask lying on the ground" and you can type "take flask". (Or "get flask" or "take the flask" or whatever else. TADS is smart that way.) &gt; Wait what? What does any of this have to do with C or C++ Like any other kind of game, "text games" are often written in C or C++. It's always nice to see Rust used where, in the past, C or C++ might have been used instead.
Please post about it when you've created a repo for the glulxe port :)
You really need two grids - the previous time step's state, and the new state you are computing - for the game of life, which then makes it embarrassingly parallel.
They are exactly the same. The first one used to be faster, but since rustc 1.9 they are the same.
The problem here is that calculating the adjacent cells requires passing the grid as an argument while iterating over it. No worky-worky. It's a design issue, I'll move to a version with two buffers like suggested below. But what I really wanted to say is the mandel-rust looks great and I'll definitely be checking it out later, thanks!
You're right. Currently I have a Cell struct that holds the previous and current state, plus the logic to get the adjacent cells. I'll move those out to their own grids and it should make things easier since I'll never have to mutate more than one at a time. Tomorrow though! Thanks a lot.
I'm fairly certain that display() is the bottleneck here, since it's a call to the SDL. This is just the first program I had that was a candidate for parallelization, which is what I really wanted to try. I could profile but again, I don't really care. Thanks for your help!
Yup, that's what I'll change it to. Was thinking too object-oriented with my cell struct.
I'm not sure I follow the connection between the formatting infrastructure and memory usage? It is expressly designed to not require allocation in most cases.
&gt; additional optimization techniques that the Rust versions are not, such as SIMD Is SIMD provided by the current stable release of Rust ? &gt; It's also not known how the applications are being compiled. The [compile and run command line is shown](http://benchmarksgame.alioth.debian.org/u64q/program.php?test=nbody&amp;lang=gcc&amp;id=4#log) for every program ! 
There are even more ways, like `"Hello".to_owned()` or `format!("Hello")` (though the latter will be a smidgen slower). The thing you probably really want is `Cow::Borrowed("Hello")` üòé
You want to post in /r/playrust. This is a subreddit about the Rust programming language http://rust-lang.org/
I was also curious why it shows that rust is 2x slower than C for some tests. So I downloaded the source code for `n-body` test and run it on my machine. And I got completely different results. Rust was on par with clang and lost only 10% to gcc (not 2x slower!). Maybe they use older version of rust or maybe they have some other issues, but in any case don't trust those numbers.
Thank you guys - much appreciated.
Neat! Do you have any public benchmark results for various lengths, maybe with comparisons to other hashes? :)
I just looked up the pronounciation...yeeeaaaa I wouldn't have gotten it right either...
Anyone know if maidsafe hires remote outside the UK? Or sponsors non-UK citizens to work in the UK?
Publicly available smhasher results would be great too! Sorry to be so demanding ;) This looks like it could be a great hash to use for all manner of things.
I'm using rayon for the first time. For the divide-and-conquer example, the docs use quicksort, and joins two mutating closures which run quicksort recursively on the two halves. My question is, if my function recurses on three problems instead of two, and returns an allocated object instead of working in-place, is rayon::join still the right solution? For example, if I have a function ` fn foo(obj) -&gt; Bar { let (a, b, c) = (foo(obj.first), foo(obj.second), foo(obj.third)); combine(a, b, c) } ` should I join as following: ` let (a, (b, c)) = join(|| foo(obj.first), join(|| foo(obj.second), || foo(obj.third))); `? (Can't figure out how to get newlines in the code blocks)
Lookup hashlife for ever faster performance.
Hm. I could swear that the `Formatter` stuff would allocate, which is why non-specialized `str::to_string()` was worse than `into_owned()`. My bad.
The problem there was formatting infrastructure added a pile of overhead beyond just `string = allocate(str.len()); memcpy(str, string)` (pardon the fake syntax), due things like virtual calls and having to check the various formatter flags (none of which are active in the `to_string` case, but they still have to be examined), and possibly not being able to pre-reserve space in the final `String` as directly as a single `with_capacity` call.
Which is funny, because Polish has predictable pronunciation rules. I didn't know how to pronounce it, but I read it correctly the first time.
&gt; The separation of the two target pointer cases seems like an unnecessarily microoptimisation: the compiler should be able to emit an efficient sequence for the platform given just the target_pointer_width = "64" case, and, the diffuse function ends up doing more expensive things for 32-bit platforms anyway (the 64-bit multiplications). It is actually not an optimization. It is an observation that I cannot rely on any particular memory layout u64 might have, when it is emulating 64-bit arithmetic. &gt; The technique used in the code doesn't particularly seem like it's worth labelling as a SIMD read: it is chunking the data into larger segments, but not necessarily actually using any of the SIMD functionality in the hardware. It also seems like the 4-way pipeline could/should be wider if one is expecting/hoping for vectorisation, e.g. even the most basic x86-64 CPU supports 8 128-bit registers (i.e. can store 16 u64s) as does 32-bit ARM NEON, and new CPUs are only getting more space: AArch64 NEON has 32 128-bit registers, x86 AVX has 16 256-bit ones, and AVX512 has 32 512-bit ones (yes, 2KB of vector registers). You're right the wording is wrong. &gt; Factoring in splitting the stream across multiple cores, a basic modern Intel CPU (like the i5-5250 in the current macbook air) might benefit from being able to process 2 (cores) * 4 (number of u64s in a AVX register) * 8 (half the number of registers) = 64 independent streams. I'm assuming using 4 threads wouldn't help since the 2 extra are hyperthreads, not physical cores, and I'm also assuming that one would be using half the registers for temporaries/constants, but an optimised implementation might be able to get away with fewer. :/ That's not quite true. Evidently, there 4 streams is optimal, because there are usually four pipelines in the core. &gt; The ^= operator is used on a primitive local variable, and even in debug mode, rustc emits a direct xor: for a function that xors an argument x with a mutable local variable a and returns a, the IR for a ^= x and a = a ^ x is identical. Yup, you're right. This is a trace from when I used modular addition instead of XOR for combining (mixing in) the new values. So I used `Wrapping&lt;u64&gt;`, which is a new type, which might emit instructions telling LLVM that the variable is aliased. This is, of course, irrelevant now. You're spot-on on the two last points. --- Thanks for the comments!
https://upload.wikimedia.org/wikipedia/commons/6/65/Pl-Wroc%C5%82aw-2.ogg that ≈Ç is pronounced closer to w
&gt; It is an observation that I cannot rely on any particular memory layout u64 might have, when it is emulating 64-bit arithmetic. The in-memory layout is completely irrelevant for arithmetic: for all we know, x86 registers have their bits all shuffled up. As long as the bytes end up in semantically the right places when loading/storing, it's all fine, and the load you're doing is just manually doing the last step of putting the bytes in the right order. At least, it would be if it was correct: now that I look at it more closely, it seems to be putting the same data in the high and low halves of the `u64`. If the CPU is big-endian, one needs to basically do `(ptr[0] &lt;&lt; 56) | (ptr[1] &lt;&lt; 48) | ... | (ptr[7] &lt;&lt; 0)`, which can be done with the `u64` operation directly, or can be done via `(ptr.load_le_u32() &lt;&lt; 32) | ptr.offset(4).load_le_u32()` as it is currently attempting to do. Also, note that on a little-endian platform the 32-bit version needs to be `(ptr.offset(4).load_le_u32() &lt;&lt; 32) | ptr.load_le_u32()`, that is, the `u32` that's later in memory needs to go in the high half, opposite to the big-endian version. Just using `u64` directly for everything seems like the simplest way to get correct and efficient code. &gt; That's not quite true. Evidently, there 4 streams is optimal, because there are usually four pipelines in the core. No, you're missing my point, partly because I was a little confusing: there's two things, hardware streams (i.e. what you're calling the pipelines) and the semantic "independent processing" streams of u64s (i.e. currently the u64s at indices 0, 4, 8, ... are one of these stream, and those at 1, 5, 9 are another). These do not have be the same. But you're correct that I didn't explicitly touch on the hardware streams in my consideration. Assuming that one can run 4 of the relevant SIMD operations in parallel per core, you still want to have 2 (cores) * 4 (number of u64s per SIMD register) * 4 (pipelines) = 32 separate accumulators (some of which end up sharing registers and instructions). That is, the CPU could theoretically process 32 independent streams of u64s, because the instructions shoved down the pipeline can process more than one u64 at a time. The current algorithm only allows for 4 independent streams, and so doesn't benefit from threading or from SIMD as much as it could. Additionally, processing even more data can still be beneficial for some algorithms: the loop can be unrolled further.
Only works for predictable, coherent configurations. I'm just generating large random grids and using pretty colors.
You may wish to note that recent versions of Firefox include some Rust code, so compiling them requires using the compiler you are suspicious of.
Why do I only sometimes need to dereference with "*" a ref like when using iter().enumerate()? Also, the compiler just says type mismatch and doesn't suggest the dereference...
I believe like "crustaceans" except without the hard C.
Will do!
How does one structure the repository for a webapp with the server side in Rust and the client side in Javascript? How does one get `cargo` and whatever Javascript package manager the cool kids use (`npm`? `bower`? `yarn`? `jspm`?) to get along? I haven't seen any examples. I'm basically new to frontend development. My [moonfire-nvr](https://github.com/scottlamb/moonfire-nvr/tree/rust) will eventually be a mixed Rust/Javascript webapp. The Rust port (from C++) is on the `rust` branch; the Javascript part doesn't exist yet.
I echo your sentiments on Pest. If you are looking for examples on `process!`, have a look at Tera: https://github.com/Keats/tera/blob/master/src/parser.rs
If you are looking for assistance at some point come to the gitter channel https://gitter.im/dragostis/pest, /u/dragostis is extremely helpful! You can also checkout my project tsqlust: https://github.com/Phrohdoh/tsqlust I am working on building an AST currently so it is not very solid but can hopefully offer insight.
Thanks for posting this. Having an end-to-end working example will be really helpful going forward. I did a quick search but I should have done a reverse deps search on crates.io as well.
I'm pretty sure they do hire people remotely outside of UK, see https://maidsafe.net/company.html 
Oh! I was thinking of doing this too! I figured I could make a bot to manage the TODO list my wife always gives me with automatic notifications on what I have left to do for the day haha.
I'd probably just use subdirectories or maybe git submodules and treat them mostly as if they were separate repos. Like root \_client \_package.json \_src \_test \_server \_Cargo.toml \_src \_test Each of those can have their own builds, like cargo and yarn, and you can always have a master build for the entire project which coordinates the others. Start simple - make it complex if you need it later.
There must always be some number of dereferences until you get at a value. Some of the dereferences are implicit, like when you call `ref.method()`, because the language has [deref coercions](https://doc.rust-lang.org/book/deref-coercions.html) built in. Sometimes they're less obvious because an operation is implemented for references directly, like the [`Add` trait](https://doc.rust-lang.org/std/ops/trait.Add.html). That trait has (among many others) impl Add&lt;usize&gt; for usize impl&lt;'a&gt; Add&lt;usize&gt; for &amp;'a usize impl&lt;'a&gt; Add&lt;&amp;'a usize&gt; for usize impl&lt;'a, 'b&gt; Add&lt;&amp;'a usize&gt; for &amp;'b usize which allows you to write `a + b` with `a` and `b` as any combination of `usize` and `&amp;usize`, as opposed to needing `a + *b`, for instance. Pretty much whenever you attempt an operation that isn't supported with deref coercions or implemented on references directly, or when the operation might be ambiguous, then you have have to explicitly dereference.
Yup! I didn't want to talk about monads since not everyone is familiar with the term and getting into that kind of talk makes eyes glaze over sometimes but that's basically what was going on here.
Wouldn't there be a fair amount of interdependency between the two? At the least, my Rust code will provide a (JSON-based) API that my Javascript stuff will use, so i probably want to keep them in one repository to version them together. But also the Rust code will serve the Javascript files: * I suppose for debug builds I just need to have the Rust code serve files from the Javascript directory. I don't want to have to recompile my Rust code when I make Javascript changes. * For release builds, ideally I could just create a binary that has all the files to serve embedded into it. I suppose I could just have the Javascript build kick off before the Rust one (with a master build in `make` or some such) so that the compiled Javascript files are ready to embed by the time the Rust build starts. The build wouldn't be as parallel as with one build system but maybe worrying about that is unnecessary/unrealistic. I also need to have some list of files to embed into the Rust binary. I'm imagining some `build.rs` that generates this automatically, but maybe a simple manual list would be fine depending on how many resources I have / how stable my dependencies' filenames are. Ehh, maybe the answer will become more apparent to me when I get my hands dirty and start actually writing the Javascript part. I'm definitely curious to see examples, though. I'd be surprised to learn no one is serving Javascript stuff from Rust.
curious about this: &gt;If you flip any bit in the input, the probability for any bit in the output to be flipped is 0.5. So, if I take some file F and flip a bit to make F', there's a 50% chance that seahash(F) == seahash(F') ? If you flip 2 bits and make F'', is there a 25% chance that seahash(F) == seahash(F'')? I'm curious what the best use of this is... is it good for detecting corruption? Seems like only a 50% chance of a collision with a flipped bit isn't enough for that purpose, but I'm not very familiar with non-cryptographic hashes and their properties.
Calling Rust from C is as much `unsafe` as C is `unsafe` in Rust code, and when you call Rust functions you do have to uphold unsafe contracts. It's just that C doesn't track these unsafe contracts; if it did you'd have to write `unsafe { call_callback(c_callback); }`. 
Started working on a CHIP-8 interpreter, never really done anything even remotely similar to this before and being new to programming, interested in emulation and how things work at lower levels it's a fun project. Not sure if I'll actually be able to get it running properly, but it's a great learning experience. 
No, you read it wrong. So, say you have some string `n` and you flip the `b`'th bit. Call this string `n'`. If we compare `hash(n)` and `hash(n')` in some bit, there is no correlation. That is, there is 50% probability that said bit differs in the two hashes. Every hash function tries to achieve this emperically (it's called the strict avalanche criterion), but SeaHash actually proves it mathematically.
&gt; Wouldn't there be a fair amount of interdependency between the two? There's interdependency in the sense that each side has an expectation for what the other is going to send/receive, but there's not necessarily interdependency as far as actual code goes. If you wanted that, I'd create a third directory which contains the client/server API definition for each to reference. &gt; my Rust code will provide a (JSON-based) API that my Javascript stuff will use There's a serialization boundary where any given request has to go over the network. That's the natural separation point. Each side only deals with what it expects to receive and sends whatever it wants over the wire, letting the other side accept or reject it, and handling errors as appropriate. &gt; so i probably want to keep them in one repository to version them together It can sometimes be convenient to do that. But it can sometimes be necessary to change and/or deploy the client separately from the server. The two often have different methods and requirements. For example, servers often need to have minimal downtime and lots of backwards compatibility for old clients, while client updates often need to wait for CDN changes to propagate and caches to expire. &gt; But also the Rust code will serve the Javascript files That's usually a runtime dependency, not a build time one. As long as the files are available the place the Rust program expects _when it is executed_, it doesn't matter how those files are generated or stored in a repository. &gt; For release builds, ideally I could just create a binary that has all the files to serve embedded into it Why? I don't see any substantial benefit to that. Sure, you can copy a single binary to server to run it, but it's roughly the same level of effort to create an archive file with the JS and the binary together, copy that to the server, and extract before running. You could also conceivably change the JS files without changing the server even in release. Why change the server when you only have a client bug?
Yes, it is better now. Did you compare your function against Golang's one? In practice, not "theoretical".
You can also use match rather than if/else if/else.
Nice looking code! Maybe PreProc could use a TryFrom(&amp;str) impl?
Is this intended to compete with SipHash?
Those `fn`s are `extern`, so unsafe to use. From my understanding, if you use them from Rust, you're required to use `unsafe`.
The ‚Äú≈Ç‚Äù is pronounced exactly [w], the whole city name is (using IPA approximation) /Ààvr…îtÕ°s.wav/ (like vrots-wav, with ‚Äúo‚Äù like in ‚Äúnot‚Äù and ‚Äúa‚Äù like in ‚Äúcar‚Äù or like ‚Äúu‚Äù in ‚Äústuck‚Äù, or similar depending on your English dialect, and ‚Äúc‚Äù being one sound /tÕ°s/, like ‚Äúzz‚Äù in ‚Äúpizza‚Äù). Some much older speakers, especially from the east, might pronounce it /Ààvr…îtÕ°s.…´av/, so like vrots-love, but with a ‚Äúdark l‚Äù like in Scottish Gaelic ‚Äúloch‚Äù or like ‚Äú–ª‚Äù in Russian. But you won‚Äôt hear this pronunciation anywhere except for the eastern border and very old TV and theatre.
Serverless? Like this is some sort of peer to peer client side thing?
Your straightforward arguments that result in obviously good answers rather than ad-hoc OK ones, as well as the small square visualizations, remind me very strongly of the PCG random paper. So the obvious question is whether you've read [the PCG paper](http://www.pcg-random.org/paper.html). In particular, it seems like your `diffuse` function could be modified to be inspired directly by it. x = x.wrapping_mul(0x6eed4e9da4d94a4b); x ^= (x &gt;&gt; 32) &gt;&gt; (x &gt;&gt; 60); x = x.wrapping_mul(0x6eed4e9da4d94a4b); This replaces the final `x ^= x &gt;&gt; 32` with a variable-length shift; both of these are normally the same latency so the speed should be the same, assuming the two fixed-width shifts can be done in parallel. The finalizer can just be x ^= (x &gt;&gt; 32) &gt;&gt; (x &gt;&gt; 60); though a short finalizer is obviously only a sensible trade-off for short hashes, where constant overhead matters. Note that splitting the lines evenly doesn't work ideally, since the first two lines alone only really randomize half of the bits, and not perfectly. However, the first three lines together are significantly more effective than the four lines they replace, which is why the finalizer does not need to be as long. The reason this variant is stronger is that the top four upper bits instantly end up strongly affecting the lower bits - particularly the lowest 44. The prime is chosen not to have many gaps, so that no matter where the changed bit is it must affect at least one of the top bits, and that bit cannot be exclusively the bottommost (which causes the smallest shift). So the multiply makes it affect the top bits, and then those bits thoroughly mix the bottom bits. The second round then completely scrambles all but the lowest bits, which are then again randomized. [Your blog](https://ticki.github.io/blog/designing-a-good-non-cryptographic-hash-function/) describes your use of avalanche diagrams. This makes sense, of course, but it's too weak a test. You don't just want your graph to be independent over the whole space of values; you want it to be independent over pathological (albeit reasonable) ranges of values. The most pathological range of values is one where very few bits are initially set - namely, for values roughly equal to the seed. This is pathological because the shift down no longer mixes entropy, which means that the multiplied changed bit can end up aliasing itself. In the case of the PCG-inspired variant, when the toggled bit is particularly high and no low bit were originally set, the bottommost bits might not get toggled. However, even in the most pathological case I could think of, the lack of aliasing artefacts was far more important than the minor upset to the lowest few bits. Here's a Unicode heat map if you can handle it: ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñë‚ñí ‚ñí‚ñí‚ñë‚ñà‚ñà‚ñì‚ñë‚ñë‚ñì‚ñà‚ñì‚ñí ‚ñà‚ñì‚ñà‚ñí‚ñí‚ñë‚ñà‚ñà‚ñì‚ñë‚ñë‚ñì‚ñà‚ñì‚ñí ‚ñà‚ñì‚ñà ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñë‚ñë ‚ñì‚ñë‚ñí‚ñà‚ñë‚ñë‚ñí‚ñí‚ñà ‚ñë‚ñë‚ñì‚ñà‚ñë‚ñë‚ñí‚ñì‚ñí‚ñà‚ñë‚ñë‚ñí‚ñí‚ñà ‚ñë‚ñë‚ñì‚ñà‚ñë‚ñë ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñë‚ñë ‚ñë‚ñà ‚ñí‚ñì‚ñí‚ñì‚ñì‚ñì‚ñí‚ñí‚ñà ‚ñí‚ñà‚ñí‚ñí ‚ñí‚ñì‚ñí‚ñì‚ñì‚ñì‚ñí‚ñí‚ñà ‚ñí‚ñà ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñë‚ñë ‚ñì‚ñì‚ñë ‚ñì‚ñë‚ñí‚ñà ‚ñë‚ñì‚ñí‚ñì ‚ñì‚ñë‚ñí‚ñí‚ñí‚ñà‚ñì‚ñë‚ñí‚ñà ‚ñë‚ñì‚ñí‚ñì ‚ñì‚ñë ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñë‚ñë‚ñë‚ñë ‚ñì‚ñë ‚ñë‚ñà‚ñì‚ñà‚ñë‚ñí‚ñì‚ñë‚ñì‚ñà‚ñë‚ñà‚ñë‚ñí‚ñí‚ñë‚ñì‚ñà‚ñì‚ñà‚ñë‚ñí‚ñì‚ñë‚ñì‚ñà‚ñë‚ñà ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñë‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñë‚ñë‚ñë ‚ñí‚ñë‚ñí‚ñà‚ñì‚ñë‚ñë‚ñë‚ñì ‚ñì‚ñë‚ñì‚ñà‚ñë ‚ñí‚ñí‚ñë‚ñì‚ñí‚ñì‚ñë‚ñë‚ñì ‚ñì‚ñë‚ñì‚ñà‚ñë ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñë‚ñë‚ñë ‚ñë‚ñì‚ñë ‚ñë‚ñë‚ñì‚ñì‚ñë ‚ñì‚ñí‚ñà ‚ñë‚ñì‚ñí‚ñí‚ñë‚ñë‚ñí‚ñë‚ñë‚ñì‚ñë ‚ñì‚ñí‚ñà ‚ñë‚ñì ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñë ‚ñì‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñì ‚ñí‚ñí‚ñë ‚ñì‚ñà‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë ‚ñí‚ñí‚ñë ‚ñì‚ñà ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñë‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñë ‚ñì‚ñí‚ñì ‚ñà ‚ñí‚ñë ‚ñì‚ñà‚ñë‚ñà‚ñì‚ñì‚ñë‚ñí‚ñí‚ñí‚ñí‚ñà‚ñë‚ñí‚ñí‚ñà‚ñì‚ñà‚ñë‚ñà‚ñì‚ñì‚ñë ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñë‚ñë‚ñë ‚ñì‚ñì‚ñì‚ñà ‚ñì‚ñì‚ñë ‚ñë‚ñë‚ñë‚ñà‚ñì ‚ñà‚ñí‚ñí‚ñì ‚ñí‚ñì‚ñí‚ñí‚ñë‚ñì‚ñë‚ñë‚ñà‚ñì ‚ñà ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñë‚ñë ‚ñë‚ñí‚ñë ‚ñì‚ñì‚ñë‚ñí ‚ñë ‚ñë‚ñì‚ñà‚ñà‚ñë‚ñí‚ñí‚ñí‚ñë‚ñë‚ñí‚ñë‚ñí‚ñí‚ñí‚ñà‚ñë‚ñì‚ñà‚ñà‚ñë ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñë ‚ñë‚ñà‚ñë‚ñà‚ñë‚ñà‚ñí‚ñì‚ñà‚ñì‚ñà‚ñì‚ñì‚ñì‚ñë‚ñí‚ñì‚ñë‚ñí‚ñí‚ñë‚ñë‚ñí‚ñì‚ñì‚ñí‚ñí‚ñë‚ñì‚ñì‚ñë‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñë ‚ñì‚ñì‚ñà‚ñà ‚ñì‚ñí‚ñë ‚ñë‚ñë‚ñà ‚ñë‚ñí‚ñí‚ñë‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñë‚ñë‚ñí‚ñí ‚ñë‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñì‚ñì‚ñë ‚ñí‚ñë ‚ñí‚ñì‚ñí‚ñë‚ñà‚ñà‚ñì‚ñí‚ñë ‚ñì‚ñë‚ñí‚ñì‚ñì‚ñí‚ñí‚ñë‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñë‚ñà‚ñì‚ñë ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñì‚ñì‚ñà‚ñí‚ñì‚ñë‚ñì‚ñí‚ñà‚ñà‚ñà‚ñí‚ñë ‚ñë‚ñà‚ñì‚ñì‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñì‚ñí‚ñí‚ñí ‚ñì‚ñà ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñì‚ñí ‚ñë‚ñí‚ñë‚ñë‚ñà‚ñë‚ñì‚ñë‚ñì‚ñà‚ñë‚ñí‚ñë‚ñà‚ñì ‚ñí‚ñí‚ñë‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñë‚ñë‚ñí‚ñí ‚ñì‚ñà ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñì‚ñí ‚ñì‚ñì‚ñë‚ñì‚ñí‚ñí‚ñì‚ñë‚ñë‚ñí‚ñë‚ñì‚ñí‚ñí‚ñà‚ñë‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñë‚ñí‚ñí‚ñí‚ñì‚ñì ‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñë‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñë‚ñí‚ñì‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñë ‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñì‚ñí‚ñí‚ñë‚ñí‚ñì‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñì‚ñì‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñì ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñë‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñì‚ñí‚ñí‚ñí‚ñí ‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñì ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí ‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñë‚ñí‚ñí‚ñí‚ñì‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí PCG-inspired SeaHash Actually random (If you're about ask why didn't I make an image, which is actually usable, obviously you're just not cool enough.) Note the top-left of the PCG-inspired version is slightly weighted. In contrast, much of the variant you're using is just arbitrarily weighted all over! Further, the PCG-inspired version is much more local in how weak it is. For example, generating unique values to hash using Python like x = random.randrange(2**64) x &amp;= ~((1 &lt;&lt; 32) - 1) doesn't visibly generate weaknesses for the PCG-inspired variant, but is just as pathological for SeaHash. And generating unique values to hash like # ~4 bits set x = random.randrange(2**64) x &amp;= random.randrange(2**64) x &amp;= random.randrange(2**64) x &amp;= random.randrange(2**64) shows nearly no bias for the PCG-inspired variant, but SeaHash is noticeably worse than on uniformly distributed data. On unbiased data, the PCG-inspired variant is actually so good that the you might even think the finalizer is unneeded with the plain avalanche analysis. The same is not true for SeaHash, who's `diffuse` must run at least twice even on good data for the avalanche function to work well. 
You're probably looking for /r/playrust.
oh okay thank you!
Thanks, good and well written advise. I'll keep this in the back of my head next time I encounter an Option.
Wish there was some way to do this: fn foo(a: u64, b: u64) -&gt; u64 { a + b } register!(foo); And register would be a compiler plugin which would see if all of the arguments and the return type implement FromStr and vice versa and automatically do the conversion. Compiler plugins are complicated but it shouldn't [be so difficult for this particular task](http://manishearth.github.io/rust-internals-docs/syntax/ast/struct.FnDecl.html)
The return value of `iter()` implements `Iterator&lt;&amp;i32&gt;`. You can use `iter().cloned()` to get an implementation of `Iterator&lt;i32&gt;` back.
The problem is that the `iter()` method returns an iterator over `&amp;u32`, not `u32`. You can use `.iter().cloned()` to get the desired behaviour: https://doc.rust-lang.org/core/iter/trait.Iterator.html#method.cloned This will clone the individual elements to produce an iterator over `T` from an iterator over `&amp;T`, which for `Copy` types, like u32, is essentially zero overhead.
Well, there's multiple ways you can approach this depending on what you want. Do you want to iterate over references to u32? Do you really want it to be boxed? Should it be lifetime constrained? Here's multiple solutions depending on what you want: use std::collections::HashSet; use std::collections::hash_set::{IntoIter, Iter}; use std::iter::Cloned; struct Foo { set: HashSet&lt;u32&gt; } impl Foo { // Boxed, iterates over u32, consumes Foo fn into_iter_boxed(self) -&gt; Box&lt;Iterator&lt;Item=u32&gt;&gt; { Box::new(self.set.into_iter()) } // Iterates over u32, consumes Foo fn into_iter(self) -&gt; IntoIter&lt;u32&gt; { self.set.into_iter() } // Iterates over &amp;u32, can't outlive Foo fn iter(&amp;self) -&gt; Iter&lt;u32&gt; { self.set.iter() } // Iterates over u32, can't outlive Foo fn iter_cloned(&amp;self) -&gt; Cloned&lt;Iter&lt;u32&gt;&gt; { self.set.iter().cloned() } // Boxed, iterates over &amp;u32, can't outlive Foo fn iter_boxed&lt;'a&gt;(&amp;'a self) -&gt; Box&lt;Iterator&lt;Item=&amp;'a u32&gt; + 'a&gt; { Box::new(self.set.iter()) } // Boxed, iterates over u32, can't outlive Foo fn iter_boxed_cloned&lt;'a&gt;(&amp;'a self) -&gt; Box&lt;Iterator&lt;Item=u32&gt; + 'a&gt; { Box::new(self.set.iter().cloned()) } } fn main() { } [Playground](https://is.gd/jkqEof) There's no advantages to the boxed versions btw, so I would advise against using any of those solutions.
I have answered this before in an email, so I'll just copy paste my response below: &gt;Back when we were deciding how we were going to implement Way Cooler, the main things that turned us off of libweston compared to wlc were: 1) The massive api, as we were going to have to make the rust bindings and maintain them it seemed to go with the considerably smaller wlc. It also helped that wlc has been wrapped by others before (so it was do-able). 2) Lack of documentation. In general, wayland is not particularly well documented (especially coming from Rust). Weston, and libweston by extension, follows this trend to a certain extent, so it would take more effort for us to understand what's going on. In contrast, wlc had a reference compositor that you could run and served as a target for rust-wlc. When the example, translated to Rust (it's only a hundred lines or so) worked, we knew we could begin work on Way Cooler. 3) Is more hands off than we wanted. This is a negative, because at the beginning all we wanted was to get up and running so we can solidify the ideas behind Way Cooler. wlc works almost exclusively through callbacks, and it seems libweston would have been a bit more work to get up and running. This is biting us to a certain extent now, but we can work around it. 4) Outside of weston, there wasn't much indication that window managers, tiling in particular, were using it. wlc, in contrast, was used by sway and orbment (both tiling wms), so features that would benefit a tiling window manager were more likely to make it in. &gt;Looking back now, there are a few features of libweston that we are sorely missing: 1) A stable abi. wlc has only recently switched over to a stable versioning stream, and it's starting out at 0.0 (currently, they are at 0.0.7). Even now, they have VERY bad stability. There was an issue just after we announced Way Cooler where people using it with the latest version of wlc (which was basically most, since it seems everyone is running Arch these days...) which had broken the abi without us noticing and caused absolute mayhem. libweston is versioned in such a way that it has a stable abi within certain versions and would have been much easier to target. 2) libweston works better with xwayland, or so others have told me. I have not tested this myself, but it makes sense since it's part of the reference implementation. 3) It's the reference implementation, as you said ;). This means it'll be at the fore-front of new features and we won't have to wait for downstream libs to update (for example, wlc still isn't compliant fully with version 6 of the xdg-shell protocol, which is what e.g weston-terminal uses. We use weston-terminal as a litmus test to see if things work, and until wlc updates to it properly it will be broken with certain later versions of weston unfortunately). 4) To use protocols, we have to use the raw bindings provided by the wayland-sys crate. Because the memory model of Wayland is a little orthogonal to Rust's, it has to make use of the wl_resource metadata field, which wlc already uses for internal book-keeping. This is unfortunate, and something that will hurt more as protocols become standardized and we are forced to implement them. &gt;Ultimately, I wouldn't have done it any differently since this was an ambitious project from the start. Both I and the other developer don't have much time to develop on it (we both go to school and have full-time jobs), and wlc was simply easier to use. 
Does it have anything to do with monads in category theory?
Ah, thanks for that, I should have looked there.
sure, cargo build scripts are just rust files which are compiled and run by cargo at build time. Cargo sets a few [environment variables](http://doc.crates.io/environment-variables.html) when the script is called which you can read with [std::env](https://doc.rust-lang.org/std/env/index.html) vars or so. 
This seems like as good a place to ask as any: when is it ever a good idea to use non-cryptographic hash functions? They're a convenient [attack vector](http://emboss.github.io/blog/2012/12/14/breaking-murmur-hash-flooding-dos-reloaded/) when used in hash tables with third-party data, they're not useful for signing binaries, etc.
Don't forget about the [monad laws](https://wiki.haskell.org/Monad_laws). It's possible to have an impl of `Monad` in Haskell that doesn't actually satisfy the monad laws. (I feel like there are real examples of this too, but I've since forgotten them.)
The only objection I have is that method chaining messes with debuggers. Specifically, point-free style messes with your call stack and precludes breakpoints. (There is no technical reason why this ought to be the case, it's just that most debuggers aren't made with functional programming in mind.) Also, spoiler: `and_then` is just Haskell's `bind`, and a version of it exists for lots of useful data structures. E: clarified.
Just had this issue with Travis on a pull request that I just submitted to Ion.
&gt; SipHash is a cryptographic hash function used in many hash table implementations No it's not! It's not a cryptographic hash function. It's a cryptographic MAC algorithm. If I knew the seed of your SipHash black box, I could generate collisions fast. SipHash is not a cryptographic hash function, it is a keyed hash function designed such that the key is never leaked. &gt; Sounds like a dubious assumption to make if it's not backed by theory. You're right, but I simply haven't gotten time to apply cryptoanalysis yet. &gt; This is usually done to demonstrate data integrity, particularly in the presence of potential adversaries, no? So you need a secure, functionally one-way hash function (AKA a cryptographic hash function), otherwise an adversary can break your hash function and pass off their data as your own. You're thinking of a very specific situation, where some malicious party might have access to it. But many situation aren't like that, and you just need to secure against redundancies. &gt; What's wrong with a simple parity function? Collisions, performance, quality.
&gt; I can come up with hypothetical situations where non-cryptographic hash functions are not attack vectors, but they all rely on the absence of an adversary, in which case a parity function is going to cover most use cases. The exception might be hash tables without buckets, but I'm doubtful that hash tables are often the best way to deal with non-adversarial (i.e. first-party) data. They're very much not hypothetitcal. I bet that 90% of the time, noone other than the user themself is giving the input. For example, if you have a ECS for a game engine, the entities might be in a hash table, but that doesn't make it vulnarble given that they're spawned locally.
Yup! As was mentioned in the comments it acts like a monad or at least a functor meaning it's using fmap. Curious if you have an example of it messing with a debugger like that at all? Maybe I'll just need to fire up gdb to compare the two versions.
Well I learned something new. That would make the code more succinct for sure. I could then turn the result into an Option easily. Thanks!
It's the same number of instructions. x ^ (x &gt;&gt; 32) &gt;&gt; (x &gt;&gt; 60) 1 2 3 4 vs x ^ (x &gt;&gt; 32) 1 2 ... x ^ (x &gt;&gt; 32) 3 4 The assembly seems to be the same length, and the thoroughput should be equal. The main downside is that it uses the functional units for a tiny bit longer, which might hurt ILP. &gt; I generated mine through SVG That's a surprisingly simple way of doing it. I like it! The diagram is exactly the same as your diagram, but with unicode shade characters, and some biasing to enhance the atrocious bit-depth. It was easy enough to steal your SVG code, though, so [here it is in your much nicer but less cyberpunk image format](https://cdn.rawgit.com/Veedrac/09f74771ac76b11c8c2be54467ea5bcc/raw/bdd3c02f043a42e2a1adaa167c567e3b83402b5b/diagram.svg). You can also have [the code used to generate it](https://gist.github.com/Veedrac/67b8c488300076560f426bab01043cf7). &gt; Actually, the finalizer is still needed, because of length padding. Otherwise, it would be local in case of appended zeros. Yes, that makes sense. In that case you'd want a proper finalizer, probably like x ^= (x &gt;&gt; 32) &gt;&gt; (x &gt;&gt; 60); x = x.wrapping_mul(0x6eed4e9da4d94a4b); x ^= (x &gt;&gt; 32) &gt;&gt; (x &gt;&gt; 60); 
Basically, point-free style means that you can't use breakpoints at the call site :0
Ah I see. In that case I think I'd still structure it as above with separate client/server directories (so the dependency management systems are totally separate), then use `include_bytes!` in the Rust Rust build to import whatever artifact(s) the JS build produced. I _might_ want to separate things so that the builds don't strictly depend on each other by copying/linking the output of the JS build to some resources directory for the Rust build, rather than directly having `include_bytes!` have the path to the JS output file(s).
Been a rough week or two for cargo :(
crates.io does what I do: put both at the top level: https://github.com/rust-lang/crates.io `yarn` is going to be the closest thing to Cargo, so I suggest giving it a try: it is new though, so you might run into bugs. `npm` + `bower` is the most standard thing.
And I'm not claiming to be a benchmark website. I didn't give you exact versions and numbers, because you are not supposed to believe my word for it, you should double check the results yourself. What numbers did *you* get?
Ahh, Thanks, so it's "servers-that-I-control... -less" not a confusing bit of naming at all... Then again it's probably less confusing than the name for Amazons version... Woo for terrible names.
&gt;If I knew the seed of your SipHash black box, I could generate collisions fast. SipHash is not a cryptographic hash function, it is a keyed hash function designed such that the key is never leaked. That's interesting (and new to me), do you have more reading on the subject? &gt;&gt; What's wrong with a simple parity function? &gt;Collisions, performance, quality. Can you unpack? In particular, I'm under the impression that parity functions and CRCs have best-in-class performance for hash-ish functions. 
Just read the PCG paper. That was ... surprisingly good. Probably the best CS paper I've read so far. Anyway, I replaced the diffusion function with essentially your idea. Look at 3.0.0 on crates.io.
It seems that rust itself cannot do it, but I've found this example that make it work. I haven't tried it tho. https://github.com/klutzy/rust-windows/tree/master/examples
I recommend the siphash paper. As they say, it is not collision resistant.
&gt; The real function signature is definied in a trait. I figured that i probably need a trait object to get polymorphic iterator return types. Normally you use associated types for this. If the consumer wants it boxed, they can still box it themselves, but if they can work with a concrete type the associated version makes things more efficient.
&gt; Do all uses of a non-cryptographic hash function result in an attack vector? Compile-time hashtables, for one, are where it's not an attack vector. In general when input that can reach the hashtable only come from trusted sources.
I believe Aeson is doesn't follow the laws. But being able to use `do` sugar is worth the price.
&gt; That's until someone running a web server uses ripgrep on user-uploaded (read: adversarial) data in a CRON job, it fails to exit, and the rest of the CRON job goes unexecuted. I'm sure you can see how this could lead to a denial of service attack or worse. There are other things to worry about. For example, a key reason why ripgrep is fast on even [codebases with large `.gitignore` files](https://github.com/mysql/mysql-server/blob/5.7/.gitignore) is because of optimizations like that hashmap. But end user data could easily be composed of an even bigger `.gitignore` file that defeats all optimizations (including the hashmap). You could even write a file so big that it causes the underlying regex implementation to switch to a slower matching engine. (Note that this isn't especially trivial to do, since there's a hard upper-bound on how big the compiled regex is.) There are other things, like if you don't cap the size of the user data, then searching it could take a sufficiently long time such that shared resources become unavailable to other processes that need it.
&gt; I'm asking how the set of use cases isn't vanishingly rare. By analogy, there exist hypothetical situations when rewriting C code into assembler for performance is a reasonable proposition, but in the general case it's almost always a bad idea. I still think "vanishingly rare" is a bit strong, but FWIW, I find this formulation far less objectionable. :-)
&gt; What numbers did you get? http://benchmarksgame.alioth.debian.org/u64q/measurements.php?lang=rust
Some amazing stuff in this thread. Love this community.
Good luck with your project! I too am beginning work on an emulator and the tooling that goes with it. They are a huge project but even if you don't finish it, what you learn on the journey is pretty epic.
Do you have a specific example? I have generally found the syntax of Rust to be the easier part.
Thanks! I've learned a ton already, consider myself a fairly new programmer and Rust is the first language I've really put effort into. :) Good luck with your project! 
Just an idea, that game of life is amenable to SIMD parallelization as well.
Ok, If you want to play this game, here is my numbers: http://dpaste.com/3QSKY7P See the difference?
That file doesn't exist on Fedora 25 -- instead it's `ca-bundle.crt`.
In this case, every letter has a predictable pronunciation except for the final `w` which is devoiced into [f] because of its word-final position. But I'm sure *[vrotswav] would still be understood
&gt; play this game It's just basic information we need. &gt; See the difference? You're using a different version of gcc, on Darwin, on a MacBook.
Did Khronos express any interest in your project?
Cool. :-) Its always awesome to see adoption by people making tools for other programmers. My dayjob is a Bugsnag client.
&gt; It is actually not an optimization. It is an observation that I cannot rely on any particular memory layout u64 might have, when it is emulating 64-bit arithmetic. I see that you added a comment to this effect, but it looks like you possibly didn't catch my mention of a bug in the sibling comment. Have you tried running that code? The 32-bit code should work even on a 64-bit machine, but I strongly suspect it doesn't. It should presumably also work when run in a MIPS/big-endian ARM QEMU instance (or real big-endian hardware, of course). Also, two other points: - this library has a huge amount of unsafe code with seemingly little effort to factor it out nicely or even a comment justifying it, and, given the ease with which `read_u64` hid the bug mentioned above, all this code seems dangerous, - there's nothing that guarantees that the start of the input is appropriately aligned for loading a `u64`, and so the code may crash on platforms which are strict about this.
I think your answer is apt to confuse people: * First, as I argue in another comment, the term "cryptographic hash function" in actual practice confuses the hell out of people. * The term "keyed hash function" is another confusing term, because it doesn't say anything about whether the key is secret or public. Some applications of "keyed" hash functions use public random keys, others use secret keys. * The property of SipHash that its authors claim makes it suitable for hash tables isn't the MAC property (message authentication code), but rather the PRF property (pseudorandom function; note that all PRFs are MACs, but not all MACs are PRFs). I don't think that good MACs are automatically good hash table hash functions, because the truncation of a good MAC's output to the hash table's size is not automatically a good MAC as well. PRFs, on the other hand, do support truncation in this way. * While using a secret key is a good strategy for protecting against hash table multicollision attacks, it's neither a necessary nor a sufficient avenue for such protection. A collision-resistant public hash function would be sufficient; and a secret keyed hash function could well be insufficient if the attacker can learn enough from interacting with it.
The two answers illustrate the dichotomy between programming as a means to maximize income and programming as a means to solve problems. Whether Rust is important to learn really depends on whether your focus is on maximizing employment prospects or wanting to build the best programs you can. Paul Graham has written on this phenomenon, back before Python was a mainstream language: http://paulgraham.com/pypar.html (Though the things that made Python interesting are mostly at odds with the things that make Rust interesting, the general principle applies.)
Yes! But small nitpick: `and_then` is actually more like `(&gt;&gt;=)` than `fmap`. `map` is the `fmap` equivalent in Rust.
It's probably a difficult scenario to represent in the test suite, but feel free to file a bug asking. I'm guessing cargo doesn't test SSL at all because it requires seeing up a server.
Simple and useful, I like it!
I have updated the `LICENSE` and `Other` sections of the README. Hopefully these changes make the intentions a bit more clear while not seeming so nefarious. I should have worded this better from the very start. Please take another look if something about this previously turned you away!
Why do standard library classes typically not implement any traits? ie. Why don't HashMap and BTreeMap implement a Map trait?
What issues are you referring to about debugging fp things? I'm new to Rust and want to know more (written some code but no gdb-style debugging yet). Is it things like not being able to breakpoint single-line closures (it can't tell if you want bp on the `.and_then()` or the closure passed into it?) or something more?
In my experience, even optimized CRC implementations are slower than medium-speed hash functions, such as SipHash. (i.e. Rust's SipHash implementation was twice as fast as the CRC64 implementation of the [crc crate](https://crates.io/crates/crc) last time I checked). If you look [here](https://github.com/rurban/smhasher), MetroHash is up to 15 times faster than a non-hardware supported CRC. 
They did at one point but support was spotty at best. The consensus is that generic collections traits need Higher Kinded Types to work right.
Why is the other post deleted and this one just contains a smiley? Did I miss something?
Mostly the breakpoint stuff. I've edited to clarify.
Couldn't help but notice the *entirety* of the crate's source code looks like this: mod disk; mod cache; No, the `disk` and `cache` modules do not exist. If this is a "hey, look at this project" post, you're *maybe* just a smidge premature. If you're trying to draw attention to it to get people helping, a post *about* it, rather than a nearly empty source repository would have been better.
Because you can do `writeln!(io::stderr(), "Hello {)", world);`
Looking at the commits, the current work seems to be focused at the [specification document](https://github.com/ticki/tfs/blob/master/specification.tex) complete. I agree that it is nowhere close to the ready-to-look-at state, but it doesn't seem to be completely vacant at the least.
I'd say it's vital if you plan on doing any kind of systems programming work in the future.
That looks very promising, thanks.
It's a reasonable argument for sure. I guess it's thought printing to stdout is common enough to be special-cased. I think this crate is useful for situations where that assumption is proven wrong.
Author here. There is a reason I haven't posted it yet. It's incomplete. edit: Also, the main crate is essentially empty, but if you look at the other components, they're complete. Still, the project as a whole is not mature enough for posting it here. Following components are complete: - SeaHash - PLRU caching - LZ4 But my main focus is the specification, which is only half way through. When the spec is done, I can begin to work on the main crate.
If LZ4 is complete, do you have a good estimate as to the performance hit of using whole-disk compression? I use a HDD on both my main computers anyway so it would have to be pretty significant to affect me personally, but I'm interested to hear what the tradeoff is here.
I have a situation that i think is described in [RFC 1522](https://github.com/rust-lang/rfcs/blob/master/text/1522-conservative-impl-trait.md#motivation). &gt; while you can write &gt; &gt; fn produce_iter_dynamic() -&gt; Box&lt;Iterator&lt;u8&gt;&gt; &gt; &gt;you cannot write something like &gt; &gt; fn produce_iter_static() -&gt; Iterator&lt;u8&gt;
https://en.m.wikipedia.org/wiki/Flash_file_system
I've noticed as I've been learning Rust, that many of my "fights with the borrow checker" involve for loops. After my last battle I decided to try and pin down the details, and then write a blog post to help others. Hopefully the second version of the Rust book will make this unnecessary.
&gt; Better file monitoring Please make it scalable, real-time and low overhead so that it's used extensively and also consider extensive use of metadata for tagging files and querying that in real-time later. Basically, please make something that is as good as BeFS or better. No current file indexing solution is as fast and seamlessly real-time on current hardware as BeFS on 1990s disks and cpus, nor is any of it integrated coherently as it was on BeOS. These days we have media managers storing stuff in separate databases and then file indexers accessing that rather than a common db accessible to everyone the same way via the filesystem. It changed from storing it in the fs to hooking the indexer into the app databases which promotes silos and non-portability. On BeOS you copied your folders and everything was contained. I miss the days that I could search for a media file with more than its filename without importing it all into a media collection manager and then tagging it private in that app. Anyway, I'll stop ranting.
Ffs 
Also: https://en.m.wikipedia.org/wiki/Unix_File_System, which is also called Fast File System (FFS for short).
Oh flashing hell
Yes, the algorithm is still stable. It must be, otherwise we'd break the API. Since it must be stable, the only algorithms we can even consider are: * Merge sort * BlockSort/WikiSort/GrailSort * TimSort At the moment, Rust has a well written but rudimentary merge sort. It works very well on random arrays (it's almost as fast as C++'s introsort), but has terrible performance on structured (presorted or mostly sorted) arrays. Also, it allocates an array of length 2*N. So if you want to sort 10 MB of data, you'll have to use 30 MB of memory. That is just silly. :) My algorithm is just a tweaked version with a few tricks to perform better on structured arrays. It allocates an array of length N, which is not great, but tolerable. Using O(1) memory would be ideal, though. That is exactly the goal of BlockSort/WikiSort/GrailSort. However, these algorithms are very complicated and perform poorly. I mean, just look at [this](https://github.com/BonzaiThePenguin/WikiSort/blob/master/WikiSort.cpp). They can compete with `std::stable_sort` in C++, but are a far cry from performing as well as `std::sort`. If you think you can implement a fast stable sort that uses O(1) memory... good luck. I'd say that's pretty much impossible. Btw, Rust's default sort is stable but still easily beats `std::stable_sort`. That's good. :) The third option is TimSort. Again, this is yet another variant of merge sort. TimSort performs extremelly well on structured arrays. However, I've tested implementations of it in Python, Java, C++, D, C, and Rust. All of them will disappoint you on random arrays. Originally, the algorithm was designed for Python and then copied verbatim into other languages. My hypothesis is that this approach is very misguided. TimSort is obviously designed with the assumption that CPU cycles in Python runtime are cheap, while calls to user-defined comparison functions are expensive. If you copy TimSort verbatim into Rust, two things will happen. First, you will implement some ideas that improve the performance in Python, and make it worse in Rust. Second, you will miss a lot of Rust-specific optimizations. The end result is that pretty much all TimSort implementations in lower level languages suck. Right now I'm trying to incorporate only the useful ideas from TimSort into my algorithm. We'll see how that goes, but I'm optimistic. **Update:** Turns out I was right. The algorithm now allocates space for just N/2 elements and is even faster: [benchmark](https://gist.github.com/stjepang/fa8818f10284144c9cb2d15805e4223e)
Hi, I wanted to write a blog post for people coming from C - we usually use pointers for a lot of things. I don't have a blog and I don't write these things often, so I just wrote it as a markdown document instead. I'll be happy for a comment if you found it useful, or if you found typos, some common patterns I've forgotten, etc.
Ah this is awesome! I'd love to see more guides like this. For my own edification, what common pattern in C leads to using `Rc&lt;RefCell&lt;T&gt;&gt;`? I've been writing Rust for a long time now, and I still don't think I've had the occasion to use `Rc&lt;RefCell&lt;T&gt;&gt;`. (Certainly, I've used `Arc&lt;Mutex&lt;T&gt;&gt;` though.) I understand where it's useful in Rust-land (I've just never worked on those sorts of problems), but I'm curious about the bridge from C to Rust that made it prominent enough for you to include it in this guide and write an abstraction for. Thanks again for writing this!
You're welcome! :-) &gt; For my own edification, what common pattern in C leads to using Rc&lt;RefCell&lt;T&gt;&gt;? Object oriented programming in general; at least as I'm used to it, includes pointers in all directions, with possible mutation. This includes callbacks as well (hmm, maybe I should add something about callbacks in particular?). 
You want /r/playrust I believe. Please check the side bar before you submit to subreddit :)
this seems very likely. OP might also try using `trim` on their input string.
Holy $#!7 How wonderful.
&gt; Updates from Rust Core &gt; &gt; ... &gt; &gt; Implement break with value (RFC #1624) \o/
Agree, but reminds me how I hate that VSCode doesn't allow even simple variables (like $HOME) in its config files still (as far as I know). I move between development machines a lot and this always hurts.
`^_^`
reddit.com/r/playrust
Thanks! Your crates have been really useful as a foundation for experimenting with this. :) It might be nice to get the mappings into a crate though naming the crate might be difficult. 
I would avoid recommending `Rc` and `RefCell` that much. You need to know about them (so they should be in the guide), but very often you can avoid them with borrows. Also, please recommend `Cell` before `RefCell`. You also recommend `&amp;` for "function call arguments and local variables", but it's an extremely common pattern to use them to return borrows out of one of the arguments (which aren't "local").
rips is based on libpnet (thanks /u/mrmonday !) which does not support async io (yet). There is [an issue](https://github.com/libpnet/libpnet/issues/229) though.
Implement a string tokenizer as an `Iterator`, without using any vectors or recursion. That's a really good practice problem for solidifying your Rust knowledge. You have to create a structure to contain the string, and other variables for keeping track of the state of the Iterator. Then you have to implement the `Iterator` trait for that structure to return the next token in the list. Going further, you can use bit-twiddling techniques to compress all your boolean values into a single u8 and save some precious CPU cycles/registers. If you're not familiar with bit-twiddling, it goes like this, where FLAG is a value of a single bit (1, 2, 4, 8, 16, 32, 64, and 128), and variable is the u8 that you are using to keep track of your boolean states: variable &amp; FLAG == 0 // FLAG is disabled variable &amp; FLAG != 0 // FLAG is enabled variable |= FLAG // enable FLAG variable &amp;= 255 ^ FLAG // disable FLAG variable ^= FLAG // flip the state of the FLAG You can often times save some CPU cycles by combining multiple similar boolean operations, like so: variable |= (FLAG1 + FLAG2) // enables both at the same time variable &amp; (FLAG1 + FLAG2 + FLAG3) == 0 // check if all three are disabled The reason why this is useful is because the compiler treats booleans as 8-bit integers even though they are logically 1-bit values. It's not smart enough to condense multiple booleans into a single byte. You waste a decent amount of memory/registers using the boolean primitive in excess.
 &gt; I would avoid recommending Rc and RefCell that much. You need to know about them (so they should be in the guide), but very often you can avoid them with borrows. Hmm, I think this is what the guide currently says? It starts by recommending references (i e, borrows) whenever practical (in the first section), and later it talks about the risks of using `Rc&lt;RefCell&lt;T&gt;&gt;`. &gt; You also recommend &amp; for "function call arguments and local variables", but it's an extremely common pattern to use them to return borrows out of one of the arguments (which aren't "local"). You mean like this: impl Foo { fn get_bar(&amp;self) -&gt; &amp;Bar { &amp;self.bar } } Yes, this is common enough to be mentioned somewhere. I'll see if I can find out a good place to put it.
The string tokenizer idea is a good one. In particular, do a zero-copy parse of a borrowed string, and be unicode aware. It's pretty basic but it gets a lot of very important ideas and idioms across.
Thank you, sounds like a good exercise. I'll give it a shot.
With the linux network stack, you can make this works by putting each NIC in a different network namespace. The problem is to reach the "main" namespace, where your applications are listening. You'll need to connect each NIC namespace to the main namespace with veth pairs, assign a private IP to the main namespace, and configure source nat and destination nat with iptables in each NIC namespace to reach it. Networking can be fun indeed!
Nice! I'm definitely going to give this a try over the weekend. The wife leaves for a work trip so I can tear down our home network and set up some things without hearing that Netflix is dead :P 
&gt; Sometimes you actually want to return *Foo Wouldn't you usually return a `Foo` and expect the caller to box it up if that's what they want? With the caveat of `Box&lt;Trait&gt;`, which I believe is the only option (assuming you can't return an `&amp;Trait`)
This is relevant to my interests and the new version of the book's interests... we want to write a chapter about what to do instead of some common object-oriented patterns in Rust. Can you expand a bit here, or link me to, more examples where you need to have pointers in all directions? For instance, why not change the Bicycle/Wheel example so that the Bicycle provides the Wheel with the information it needs instead of the Wheel needing to know what Bicycle it's attached to? struct Wheel { diameter: i32, } struct Bicycle { wheels: [Wheel; 2], size: i32, } impl Bicycle { pub fn inflate(&amp;mut self) { self.size += 1; for w in self.wheels.iter_mut() { w.adjust_diameter(self.size); } } } impl Wheel { pub fn adjust_diameter(&amp;mut self, bicycle_size: i32) { self.diameter = bicycle_size / 2; } } Basically I'm struggling to understand why object oriented devs think in terms of what you have, so that we can explain how to think in terms of something like this instead.
I'm trying to learn tokio by implementing tokio-amqp. Couldn't find a single working example of protocol implementation under tokio-rs org. So far, I'm stuck. Should I implement Future or Stream, or use Framed? Very confusing.
You probably want to post that in /r/playrust.
I wonder how the memory consumption was measured. Does it include the memory used by shared libraries? As Rust typically compiles all dependencies statically into the library that might explain the huge memory footprint.
To learn rust I will port Peter Norvig's Sudoku solver to it. 
I'm super super excited about rips :) Ever since faern first joined #robigalia I've been excited to integrate their work to provide our networking stack.
It's interesting that while Rust lags behind in terms of throughput here (only ~80% of what Crystal delivers), it has the lowest max latency (3.73ms), with the second lowest being jetty (6.47ms) and all others going from 9ms upward. I'd still be interested in knowing why the throughput is only 80% of Crystal's, while consuming 3x as much CPU.
[No panic slicing!!](https://github.com/rust-lang/rust/pull/36340)
&gt; You haven't shown there's anything wrong. I've shown that numbers on the website are misleading and do not actually represent the real world performance that users will get across different platforms and devices. &gt; Telling people to "do the tests on their own hardware" does not give that, it gives "one point of data". 1+1 is two, not one. Besides, this data point they will get is at least relevant and valid for their setup, and not some other guy's computer.
Rust has `rustfmt` too.
I'd recommend Rust over Vala for creating GTK applications. The GTK-rs project supports GTK 3.22 really well, and it's easier to work with, in addition to gaining the support of the Rust ecosystem which Vala cannot tap. I'm not sure what you mean by error syntax being difficult to build for Rust. All you really need is just an enum with possible error states, returning the error state when there's an error, and when you get an error you can just handle that error in a match.
&gt; I wonder how the memory consumption was measured. What is the right way to measure this? I usually use `top` or activity monitor in OSX, but these don't always seem to give numbers that feel right to me. What exactly is being included/excluded there?
Multithreading. There's no doubt some suboptimal code paths and areas that the compiler could optimize in the future, but Hyper does internally use multiple threads for some speed/latency gains.
This works a lot better on Atom, where you can also get Clippy lints each time you save.
What about "RxFS"? 
I don't think those are really comparable examples. Hyper (from my understanding) is intended to be a low-level HTTP interface-- something you'd use through a framework or a higher-level system. Much of the Rust example is taken up with things like setting the content-length header and spinning up a handler thread per CPU. That sort of functionality could easily be abstracted away in a higher-level library.
It also makes the ABI much smaller‚Äîwith allocation of Foo confined to the implementor side, the fields and size of Foo can be changed freely without having to recompile any calling code.
Why not continue the venerated objc tradition and call it NSRust? ;-)
what exactly is to verbose in your opinion? I find the Rust version quite tidy especially the match/switch statement in comparison. In Swift you need all those case and return statements which i find quite noisy. You can even shorten the e.g. `Error::NoLocalPart` like you use it in Swift `.noLocalPart` just `use Error::*;` before the match [Rust Playground](https://is.gd/tzD44C) Thanks for the great inside in your experience!
Are you not aware of the ? operator for error handling? Your Rust is doing a handful of silly things like using a match instead of ? and converting a Result into a boolean even though you could have just used the .is_ok() method.
&gt; what exactly is to verbose in your opinion? I've to implement the `From` trait and the user-facing Display representation of the error, besides of the `Error` trait.
This sort of stuff is just crazy impressive to me. High level operations transformed into fast code is something that not many programming languages can do. In particular, it is really impressive how well rust handles things like lambdas. It is super impressive that the rust+llvm can remove that layer of indirection. That is something that other higher level language really struggle with.
You have in mind that this was the first version, just to compare with the syntax of other languages. I accept pull request for a better usage of syntax. Thanks!
I have to wonder how this would fair if they used the Futures http client instead of hyper for this.
I think it was a contrived example to show that cyclic ownership is common. I'm not sure recommending Rc and Cells is the best way to get idiomatic translations. Ideally the relationships should be rethought to see if they can easily be made acyclic. That said I don't think the post was trying to suggest idiomatic translation from c to rust but rather just more straightforward translation.
You guys are the fucking best. Added to the queue too. I will definitely send code reviews here. Thanks guys!
&gt; I also love how we can write high-level code and get crazy great perf...except when we don't. To be fair, this was one of the success stories of my optimisation journey. I also hit quite a few cases where code was unexpectedly expensive ([#37538](https://github.com/rust-lang/rust/issues/37538), [#37573](https://github.com/rust-lang/rust/pull/37573)).
Pass closures directly to functions where possible, instead of putting them in variables, they'll get infered from the caller's bounds.
Yeah, that's nice, but it's a double edge sword, because now your privacy technique is coupled with heap allocation. :-( That's always been my least favorite part of the technique anyway.
It's a tricky problem in general. Imagine we have this slightly more complicated situation: trait LaunchNukes { fn foo(&amp;self) { println!("Launch nukes"); } } struct Dubious; impl LaunchNukes for Dubious {} impl Dubious { fn foo(&amp;self) { println!("Cure cancer"); } } fn main() { let baz1 = |bar: &amp;Dubious| bar.foo(); let baz2 = |bar: &amp;LaunchNukes| bar.foo(); baz1(&amp;Dubious); baz2(&amp;Dubious); } So, will we launch nukes or cure cancer? Note that both `baz1` and `baz2` are valid interpretations of `|bar| bar.foo()`. Best not to guess incorrectly, for it might have potentially dubious consequences.
You have to be careful to make sure that your iterator is "fused" if you have a second while loop which continues where the first one left off. (A non-fused iterator does not guarantee that it will return `None` more than once at the end of the iteration)
There were several submissions of this link which the spam filter caught - yours was one of them, likely because your account is fairly new and hasn't had a lot of activity. I could approve it, but there's no sense in having duplicates, and this post has picked up some momentum now. If you have problems with things not showing up in future, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Frust) and we'll make sure it's visible for you. Sorry you had problems this time around, hopefully it doesn't happen again!
It doesn't have to be fused, and it doesn't have to return None. In this scenario there's typically a break condition contained within the loop and a check after the loop to verify that whatever the first loop was looking for was found.
That's not a released version of hyper. See what it will likely look like when hyper releases 0.10: https://github.com/hyperium/hyper/blob/tokio/examples/hello.rs
Excellent, why are they called schemes? I browsed through the source a few times but I had skipped over the schemes even though I see there's some super interesting stuff there. Shouldn't they be called something like system services or something?
More classic projects is just re-implementing utilities in the core utilities suite, such as basename, tail, head, wc and ls.
Alex going back and forth with bors bot was fun
be cool, take it easy. https://github.com/tokio-rs/tokio-minihttp/issues/9 . https://www.techempower.com/benchmarks/#section=data-r12&amp;hw=ph&amp;test=fortune should be the right battle.
I'm pretty sure all production-grade malloc implementations do that? Willing to be wrong, though.
Thank you for the work that went into this! I've been using gtk-rs and found it very ergonomic, powerful, and complete; and I've had great success with it despite never having used GTK before. 
Your brain gets used to reading it as a unit after a while. Just like Haskell's use of `'` as a valid character in identifiers (allowing mathematical-style `someVar'` as a modified `someVar`), Rust's use of `'` as a lifetime marker can break your mental parser (or lexer) for a while, until you get used to reading it.
Pertinent to Rust: &gt; Out of the bunch, Rust has impressed me the most. They have delivered on much of what we set out to deliver with Midori, but actually shipped it (whereas we did not). My hat goes off to that team, seriously, because I know first hand what hard, hard, hard work this level of type system hacking is. This underscores one of the important facts about Rust: Most of it isn't _new_ at all. Cyclone had these ideas. Midori had these ideas. Safe systems programming isn't a new idea, nor are most of the ideas that make Rust safe. Rust's particular cocktail of features is unique, and we do a couple of things in a new way, but overall it's built on mostly established research. What Rust did is take these ideas and package them with usability in mind. Being accessible was as important as being safe. Most research languages exist to test out ideas and see how they work; Rust existed _to be used_. IMO this was a big part of its success.
Yeeeeeaaaaaah! btw, I reckon the "most of it isn't new at all" message is an important one to go with the 2017 goals.
Wrote this yesterday. It's a small and (hopefully) straightforward explanation of an API nicety that arrived in Rust 1.12.0. Yes, the type in the title is wrong, but it reads better than "The 'Into&lt;Option&lt;T&gt;&gt;' Trick."
I believe each one corresponds to the scheme (aka protocol) portion of a URL, out at least that's where the name comes from.
And you can also then demonstrate the safety benefits of Rust by showing how a function like fn tokenize&lt;'a&gt;(input: &amp;'a str) -&gt; impl Iterator&lt;Item=&amp;'a str&gt; can't be misused, thanks to the lifetime annotations.
Trying to understand tokio-rs. 1. Are there any tutorial/getting started guide? 2. most of the sample apps are failing to build. e.g mini-http server
I switched all of my projects to error-chain and haven't looked back. I recommend it to my friends and co-workers. 
AFAIR, it's possible to compile to LLVM byte code and use LLVM's PGO capabilities. A bit cumbersome, though.
I've been learning lisp this semester and to see so many of its concepts in languages like Haskell and Rust just blows my mind. I'm not surprised this is old stuff. If anything it just reaffirms how good some of these ideas are
Syntactic sugar for function signatures would be a really nice idea. Not just for String but maybe folks can think of a more general usable addition, which does not make everything too complex.
I think the reference counting + cell combo is the only way to do it. I suppose you can use unsafe code as well to get it to work if you can reasonably guarantee no use after free. The whole point of the borrow checker is to prevent multiple mutable copies of an object. It seems like a hard problem to get us to a better solution than the ones that already exist. Hi
&gt; monomorphize So there will be two `set_read_timeout` functions in the binary in the example? One for `u32` and one for `Option&lt;u32&gt;`?
Please don't use this. It makes APIs awful and horribly bloats up binaries. Push the Rust developers to either: * add proper support for default values. * add proper support for T -&gt; Option&lt;T&gt; coercions. (Both of these keep the signature simple and clear, and only produce codegen in the caller) Into&lt;Option&lt;T&gt;&gt; for T wasn't accepted because the devs thought this was a good idea. It was accepted because the implementation was obviously fine and harmless.
How do i test it? I just created a VM and added the ISO as CD medium. During boot i've seen some fast scrolling hex numbers and now it's just a blank screen and VirtualBox using up 100% CPU. Any special settings to do?
Thank you, I actually checked it out last night and did not find Rust amongst the supported languages. 
That's a lot of steps, and I think rust can do better. I guess the long term goal for the [rust language service](https://github.com/jonathandturner/rls) is to include it in the standard rust install, like cargo? That should remove 8 of the 14 setup steps if I counted correctly?
`From` and `Into` conversion isn't implicit, it's very much explicit. You can make your functions accept `Into&lt;T&gt;` instead of just `T`, and then explicitly call `.into()` to convert it, or you can call `.into()` when passing an argument to a function, but it's still explicit. `as` also has nothing to do with `From` and `Into` - it's used to perform casts that are (AFAIK) implemented at the language level, and described [here](https://doc.rust-lang.org/book/casting-between-types.html). `From&lt;T&gt;` or `Into&lt;T&gt;` is not transitive. Your type either implements `Into&lt;Something&gt;`, or it doesn't; if it doesn't, your code will explode.
I think your example with the contacts file shows the unfortunate issue with file errors, that you don't see the name of the file, if it's a named one. You might not always be able to display a meaningful name for files, but it would be nice if the source has a meaningful name, that it would be somehow kept and displayed in the error case.
more like quickerror += 1 ;)
I'm using VB 5.0.24 on Debian and it just stays blank.
Nevermind, I see you linked to Rust within excercism. The main page didn't offer it and I couldn't find it somehere else, so thanks! 
u can try some of the https://www.reddit.com/r/dailyprogrammer/ challenges, https://www.codingame.com also supports rust. if u like a bit more mathy problems https://projecteuler.net/ has many problems which u can solve using rust
I am going to use it for my next project for sure! Sadly last time I actually tried it, but later I have chosen to drop it in favour of no error handling at all ^^(being lazy).
Thank you, I'll check them out.
There doesn't have to be two full functions, though. There can be a (possibly inlined) shim that converts u32 to Option&lt;u32&gt; and then passes the value to `set_read_timeout`.
Nice write-up. Especially as most content on available is intended for beginners or people coming from higher-level languages.
&gt; The first thing I do - really, the very first - when starting any project in Rust is deciding the error handling strategy. [takes a look at most recent personal personal project. Finds that I've been using `Result&lt;_, String&gt;` everywhere as a placeholder for deciding error handling strategies later. Blushes.]
This may have something to do with your host CPU - what model is it?
Yes, the kernel rewrite is merged
[removed]
Thank you for the example! I will think on it :)
/r/playrust
I've just tried it in VirtualBox 5.1.10 - it booted, but got stuck at `DHCP MAC: .... Current IP: ....`. I removed the network card and tried again, this time it panicked: `ethernetd: failed to open network: No such device`. EDIT: getting the same panic with a card other than Intel 82540EM.
I think the redox book explains it better than I could: https://doc.redox-os.org/book/design/urls_schemes_resources.html
Just FYI, while we wait for templates to land in `cargo`, you can `cargo install cargo-template` and use this command to start with /u/brson 's quickstart.rs template: `cargo template quickstart &lt;myappname&gt;`. The repo with the template is https://github.com/rusttemplates/quickstart if you want to see what will make up the resulting project.
Could you post some more details here: https://github.com/redox-os/redox/issues/764 ?
At the moment, it needs about 512 MB. QEMU uses 256 as the default. EDIT: Apparently it is using more than 512 MB. I will have to identify the exact amount required - due to loading the entire uncompressed filesystem image into memory, and having ample free space in that filesystem image, I had thought usage would be around 400 MB. Apparently that is not the case. EDIT 2: Usage is about 520 MB after boot
This is expected and the boot should continue due to the ISO using a ramdisk
Intel(R) Core(TM) i7-4910MQ CPU @ 2.90GHz
Aha! Thank you both. Turns out there were a bunch of `null`s in there, `trim` didn't seem to work but `trim_matches('\u{0}')` did the trick!
Thanks!
Oh, it must be the DHCP MAC issue I'm getting stuck at which other users are reporting.
This is a great reply, but there's also one related thing: one of the few implicit coercions is through `Deref`: https://doc.rust-lang.org/stable/book/deref-coercions.html
Graph search algorithms, doubly-linked lists, undirected cyclic graphs. I don't have any "real-world" ones in mind, but those patterns are rather difficult in today's Rust because they require unsafety AFAIK. 
1. More docs are forthcoming, last I heard a new release was close. 2. I imagine this is due to said refactoring, I imagine they'll be updated after. Not sure why a Cargo.lock wasn't checked in.
I added to the release notes - most issues have been caused by giving it less than 1024 MB of memory. I will improve menory footprint in future releases.
I gave it 1024 MB. Tried bumping it to 2048 MB as well, but still gets stuck at DHCP.
What host OS, VirtualBox version, and CPU model do you have?
You might want to take a look at [The builder pattern](https://aturon.github.io/ownership/builders.html). I found that article from [an issue in derive-builder repo](https://github.com/colin-kiegel/rust-derive-builder/issues/2), which by the way is also a nifty crate.
Could I run a machine purely on Redox?
Yes, you can. We do have a limited number of drivers at the moment though.
Congratulations. I really look forward to Redox's continued development.
There is also [this one](https://github.com/rust-unofficial/patterns/blob/master/patterns/builder.md) which is more up to date. EDIT: Actually it's not. In my memories there was more to it.
Arch Linux, VirtualBox 5.1.10, Intel i7-5600U
Oh yes, `match` can work like `switch`, just better. But it still needs that guarantee that it always matches something. As for value being thrown away, it's more like it gets _moved_ out of the function. E.g `fn s()-&gt;String {"hello".to_string() }` - the string _value_ is created, and then moved to the caller. (C++ has started doing this with 'move constructors' - most useful) 
Remember, zero cost abstraction ... If you need the name, you can store is somewhere along the file "handle", but if you don't, you don't have to pay for the overhead
[Updated](https://gitlab.com/fastchemail/fastchemail-rs/commit/19bd260a2210728b32730c421ce8553c466f61c0)
I'm fairly certain that will cause a build error because you didn't map the error: asciiutils::check_ascii(address).map_err(Error::NoAscii)?;
Thank you for taking the time to write this post! I think this is exactly the kind of wisdom that makes intuitive sense to people who've watched Rust error handling strategies evolve and participated in RFCs around `?` or w/e, but that is really hard to acquire for someone who's just looking at contemporary Rust and not deeply wired into the community. A write-up like this that motivates the lib, spells out a simple starting point and gives a perspective for more "advanced" use is invaluable for "fundamental" libs/frameworks.
Yeeeeah, success (OSX 10.12.1). Nice! I can post my settings here if anyone is interested. Edit: did I write I'm impressed? But I am.
&gt; quickerror += crate::From(1) No implicit casts.
You're right - and in addition, at least one pointer direction needs to be refcelled, or otherwise you get a chicken-and-egg problem when trying to create it initially. 
I think that this is proof that default and keyword arguments have to be seriously discussed soon before we get into the situation where there's a half-dozen mediocre workarounds.
Yes yes yes yes yessssss!
tl;dr: mutable references are preferred.
Ooh, neat!
Any ideas of why it currently has such a high memory footprint?
So because I had zero context for this I looked at that author's twitter profile which includes: &gt; Author of Inferno ‚Äì a blazing fast React-like JavaScript UI library https://github.com/trueadm/inferno 
Wondering why is the brand new OS has a GUI at this stage? It does give hope and attracts many users. But I feel if networking and fs is more polished, redox can be quickly adopted for servers as it can claim most secure against hackers. Mainly due to rust and the fact hackers have to relearn from scratch.
If you use another command watch you can chain it as cargo watch check And every time you save it'll run the check automatically. It's a nice way to constantly check your assumptions as you go.
&gt; Thanks clippy! Thank you for including clippy in this post. I'm looking forward to the next 23.
Yep. The bonus is that the `u32` version can then strip out all code related to handling the `None` case, since the optimizer ends up seeing something like let timeout = Some(timeout).unwrap_or(0); , which it can remove completely.
Which (conceptually) is gonna be pulled into cargo soonish! https://github.com/rust-lang/cargo/pull/3296
Yeah, avoiding the monomorphization with implicit `into` would be nice. For now, you can always create a private function that performs the actual operation and takes the converted type, and have the existing function simply do the conversion and then can this private function. It's a little more verbose, but it ensures the only thing that gets duplicated is the bit of conversion code.
There was a substantial discussion in the following RFCs thread, which covered keyword arguments, optional arguments, and variable arity functions: https://github.com/rust-lang/rfcs/issues/323 It is currently postponed, but it provides a useful starting point to see what designs have been proposed, what issues considered, and where a new RFC may go from here.
&gt; It makes APIs awful and horribly bloats up binaries. I am willing to accept that this is not everyone's cup of tea, but this claim seems hyperbolic. Yes, this pattern may result in monomorphization of large functions. If this becomes a problem (which I don't imagine it would, in most cases), it is easy enough to overcome with a private function that takes in the converted parameter and then does the work of the original function, such that the original function only has to do the conversion and then call the private function. I agree that default values or implicit `T -&gt; Option&lt;T&gt;` coercion would be nice (and this may be part of the focus on productivity for the next year of Rust), but until such things exist I don't think it is terribly unreasonable to use this pattern. Particularly when use of this pattern would be compatible with either such addition in the future.
Just to add another example, I just looked through Diesel's code base. We use `RefCell` alone twice and `Rc&lt;RefCell&gt;` once. All cases are for prepared statement caching and interior mutability on non-copy types. Edit: And actually our use of `Rc&lt;RefCell&gt;` in particular is kind of interesting. We have a struct like this: #[derive(Clone)] pub struct StatementUse { statement: Rc&lt;RefCell&lt;Statement&gt;&gt;, } which is what comes out of our prepared statement cache on SQLite. Makes sense, it's in a cache, needs to be `Clone`. But why not just return the `Rc` directly? Well basically it's because we need a separate `Drop` impl for each use, to reset the underlying prepared statement. In practice, the refcount never exceeds 1 as best I can tell, but that's not proveable to the type system without dropping a lot of our abstractions internally. We're talking about major IO so it's not like the `Rc` is a bottleneck.
Note that the [serde json builders](http://kbknapp.github.io/doapi-rs/docs/serde/json/builder/index.html) use consuming builders, I'm not sure why. I find using them somewhat annoying.
AFAIR a good number of (probably fake) accounts were deleted from reddit that day.
I'm partial to the latter, which I use extensively in [aster](https://github.com/serde-rs/aster). In my case, I want to transfer ownership of everything I built to the caller to avoid excessive copies for a large AST. The main place it gets annoying is that if we need to do some complex logic around some chain, then we need to re-assign the builder. I found that to be pretty rare though.
At the end of last year Reddit recalculated subscriber numbers based on new metrics, specifically by removing subscriptions attributed to deleted accounts (which they had previously not been doing). The older the subreddit, the more this affected their subscriber numbers (see e.g. redditmetrics.com/r/haskell ).
Thank you
Per [Appveyor](https://github.com/appveyor/ci/issues/1194) the cache isn't used on pull requests. I merged the appveyor config changes into master and it [appears to work.](https://github.com/appveyor/ci/issues/1194#issuecomment-264248401)
The boot appears to work, and gets as far as &gt;DHCP: MAC ... but pressing functions keys doesn't seem to do anything. On my keyboard I need to press "fn + F2" and redox doesn't seem to recognize that. I'm going to try to build it for myself, so I can change the keybindings.
If you want to encode type state in builder generics (e.g. `fn method(self) -&gt; MyBuilder&lt;State&gt;`), consuming self is the only way to return a different type.
100% agree that it's pretty specialized.
We are working on this issue here: https://github.com/redox-os/redox/issues/764
Ah, so nice! I often use a use a script to watch for changes in a file I'm working on, and run `make` or similar whenever it changes. When I press Ctrl + S in, say, Gimp, and the wheels start turning ‚Äì Gimp is invoked to convert the file to a standard image format, Inkscape renders a drawing where this image is included, saving it as a PDF, which LaTeX uses in a figure and produce the final document (Evince automatically updates its view for me to see) ‚Äì something happens in my body‚Ä¶ Edit: I'm told I should use more commas when writing in English. Done.
[This page](https://www.reddit.com/r/rust/about/traffic) may be interesting to people.
Here is the blog post where it was announced: http://www.redox-os.org/news/this-summer-in-redox-15/
Thank you!
I don't agree; gofmt doesn't even try to wrap long lines, which is a basic feature.
What is your question here?
Also TL;DR: `&amp;mut` is preferred because you can often cache or clone partially built objects. With just `mut`, you always have to build from the start of a builder chain.
Pretty sure OP's question is "I'm getting a 'time limit exceeded' error, how can I make this faster?". That problem has a limit of 0.635s.
I think the timescales here probably depend on how rapidly the most-desired features of the language can be selected and implemented. It's not hard to keep the hype train going if new stuff is always just around the corner. But it gets pretty 'disillusion'-like if that stuff is always vaporware.
Yes, and comments would be like "what if Bell labs' Unix succeeded? .. then this world would be much better place".
I'm not sure it's my friends server and we want more people to play with us 
We're still in the early adopter stage. I'm in NYC (decent size tech hub) and a large chunk of developers I talk to recognize Rust as a language but don't really know much else. If Tokio winds up rallying the entire community behind one network stack like Rails did for Ruby, that'd be pretty good. Goals 2017 hype.
Working on [snowpatch](https://github.com/ruscur/snowpatch) as we gear up for v0.2 which will actually be useful!
No, but as you suggested, a required user parameter solves that problem. If I'm understanding the Rails example correctly, you were saying that the required-parameter solution causes a problem in the special cases where you actually want universal access. I was suggesting a superuser as a solution to that secondary problem, though I don't know if it's ergonomic in Rails to get the framework to pass the superuser when making its cross-project queries.
They're consuming builders because we want to avoid copies when you finally construct the object. Do you have an example for when they're being annoying?
Hey, I worked on this for the exact same reason! It was super fun, and surprisingly easy once you get into the swing of things. The spec is very nice and easy to follow. I was hoping that I would build on it and work on an NES emulator afterwards, and then move on and on... but I never got around to finishing the last 10 or so op codes for Chip-8 :(.
SIMD is one of the things I'm surprised has been taking so long to make it into stable Rust considering Servo has been all about using it for such a long time now. SIMD has always been the red headed stepchild in compilers/languages in terms of integration though due to it's complexity and lack of portability. I just hope when SIMD development settles down it feels like using the rest of the language rather than a weird bolt on.
Any laptops you know work with it? I'm shopping for laptops now for other reasons and I figure I might as well buy one that can boot Redox.
Will &lt;T&gt; ... &amp;mut T type generate noalias flag in the future?
This looks interesting, sadly he can't talk more about it. The timing for me is nice, as I was evaluating options for what to explore on the rust+web domain. I was messing around with rust+typescript+vuejs. JS is a mental torture and endless pain to me - something that typescript seems to helps out. But now the possibility (someday) of using Rust? Nice. Sadly this may take a long time still and I have no experience with these js frameworks but I have some hope that some Rust+Web wizard will come with something (public) like these frameworks for Rust. To do some experimenting, I tried to use rust-webplatform on windows and had no success. Haven't investigated much though. Anybody got rust-todomvc working on windows? 
how i can improve the performance of this code? i got time limit exceeded. i think it is because iterator is kinda slow.
Congratulations, this is exciting.
Thank you very much, Graydon.
I've developed on a similar platform (beaglebone black), which at least has a similar gpu. Never did rust graphics on it, but did use egl accelerated Qt5 with fair performance. Do you know what is currently doing the gl rendering in your case? (mesa typically provides the software renderer). On the graphics driver: I used yocto (via poky) and the meta-ti layer for the sgx driver. If the driver is getting built &amp; loaded, but the graphics don't appear to be accelerated, I'd take a look at how Qt5's "eglfs" platform (this is what I used) works. Personally, I'd prefer getting the GPU working, but the alternate on linux is to try using the DRM api (direct rendering manager), and manually rendering (ie: write your own software renderer). There is also the fbdev api, but I'd recommend avoiding that one if possible (for DRM drivers it's a wrapper around the DRM api, but is missing some features like vsync, which is required to avoid tearing). It looks like there is a WIP crate for drm: https://crates.io/crates/drm-rs , but I've got no idea how complete it is.
LLVM Software Rendering I have attempted to build the driver and made progress but not the complete package. I'll take a look at those other things.
hyper and reqwest builders use consuming self. Using mutable references turns a chain into 3 statements: first to hold the builder, second to chain all your stuff, and then 3rd to finally consume. I'd rather keep the chain in 1 statement. 
SIMD would be amazing. I know it exists, but it is hard to use efficiently right now. I do large n-body simulations and had to switch back to C after I couldn't get the performance good enough :(
&gt; loop { &gt; match iter.next() { &gt; Some(x) =&gt; { &gt; println!("{}", x); &gt; }, &gt; None =&gt; break, &gt; } &gt; } This is better written thus: while let Some(x) = iter.next() { println!("{}", x); }
I've been interested on Rust web rendering lately. My idea is to have something similar to vDOM on the rust side that can generate a tree of changes. The tree of changes can be serialized into a simple binary format that is read and applied from the JS side. One advantage of this is that it works with Rust compiled to WASM but it would also work with a native rust app communicating with the browser over websockets on a low latency network (localhost on my case). That's the general idea, but it can be extended to support more complex components implemented on the JS side instead of just DOM nodes. Another idea I have is to avoid creating the new vDOM and performing the diff, and instead create the changes tree as you go. The idea is similar to the immutable/persistent data structures, when you "modify" your vDOM changes are stored on a new tree with pointers to the things that didn't change. What do you think? Is anyone else playing with similar ideas?
Why would consuming builder prevent you from using clone?
Beware the borrowck. It bites. And don't get too `unsafe` early on. (seriously though, you're looking for /r/playrust)
ü§ò
There's a few orthogonal concepts at work here: SIMD is a broad categorization of instructions behaving a certain way. Specifically **S**ingle **I**nstructions that work on **M**ultiple **D**atas. There are different arches implementing SIMD behaviour: SSE, AVX, NEON. They're pretty much independent implementations of SIMD and aren't really inter-compatible. Further more each SIMD arch is versioned and has newer versions implement more features. Here's how it works in C/C++: You specify which SIMD arch and version you wish to target (there may be restrictions based on the compiler infrastructure, eg. SSE, AVX are only available for x86 targets and NEON is only available for ARM targets). You use the provided intrinsics which generate code, no compile errors. Whether this code is actually run on a CPU that supports the specific SIMD arch version is punted. You will get an illegal instruction trap when you decide to run it on older CPUs. The question is how does Rust want to organize its SIMD support? For the low level access to intrinsics I think the C/C++ way is the way to go. Just flip some features and you get access to their intrinsics (the compiler checks that you only get SSE, AVX on x86 targets and NEON on ARM targets). It generates the code you tell it to no questions asked. Stick as close as possible to the vendor specification (ie. C APIs). Further questions are around how this functionality should be exposed on a higher level. Here one suggestion is to try to create a higher level abstraction over all SIMD architectures taking a common denominator approach. I... don't like this. I can see the value (no need to implement your features in a platform specific way but still getting the value of SIMD) but this is trying to abstract things that are totally not the same even though they have superficial likeness. The better way to go is to have a generic function (not using SIMD) which can then have special implementations for specific SIMD arches. This doesn't solve the entire problem of *when* it should use the SIMD optimized implementation as not all features of a single SIMD arch may be available on older CPUs. There is the approach of dynamic dispatch: initialize function pointers at the start of your program to the generic implementation then use cpuid and overwrite them with specialized SIMD implementations as needed. This approach suffers from dynamic dispatch however there are some features that can reduce this at load time (the binary loader replaces the function calls at call site, what I think is called "ifunc" on elf targets, mentioned by alexcrichton). The other approach is static dispatch: just build different binaries and select the right one at installation time, or in case of dynamic libraries, load the right one compiled with the available SIMD arch version. The discussion revolves around how the above can be exposed to Rust programs. Did I get that right?
&gt; What if, tomorrow, a very popular Rust library is published with a wrong version number, can it break lots of other projects? You might as well ask "what if, tomorrow, a very popular Rust library is published with code that deletes all your files, can it delete all my files?" Well, aside from the fact that popular libraries have *already* been published with "a wrong version number" and broken projects. Although not everyone agrees on what *constitutes* a "wrong version number", so it's even *less* clear cut. Unless you can find a way to remove people from the equation, yes, people can break things.
Hi all, For maps and sets, Rust std lib provides both hash and b-tree based implementations (which is a bit surprising coming from other langs that juste expose "map" or "dict"). What are typical use cases where we want to use one rather than the other ?
Why could not he play with the rust compiler with his friends? It's very fun to challenge the borrow checker in team.
cargo does not (yet?) enforce the semantic versioning when publishing a package elm does this: https://github.com/elm-lang/elm-package/#version-rules Elm does this by checking if the types of the public API have changed. ~~I don't think it can detect changes like switching from `pub fn foo(f: Option&lt;i32&gt;) {}` to `pub fn foo&lt;T: Into&lt;Option&lt;i32&gt;&gt;&gt;(f: T) {}` as non breaking though.~~ It would still be nice to have smth like this in cargo though. For your exact problem I would say push the Cargo.lock file into your repo aswell. So you get reproducible builds.
For anyone up on the discussion, is anyone looking at ISPC from Intel ? It uses llvm and does an exceptional job at vectorization. It uses a 'varying' keyword to specify that a variable represents a SIMD lane. I think it would make a lot of waves if rust was able to approach the same vectorization capabilities. I've written things in ISPC that are significantly faster than what I had in C++. If that were possible with Rust, many people could justify switching just based on the fact that it would be the language that would allow them to get the best performance (ISPC compiles to object files that need to be linked and few people seem to know about it).
&gt; I don't think it can detect changes like switching from `pub fn foo(f: Option&lt;i32&gt;) {}` to `pub fn foo&lt;T: Into&lt;Option&lt;i32&gt;&gt;&gt;(f: T) {}` That is a breaking change. It breaks this, because it can't infer the type: foo(Default::default());
We're committed to helping developers solve problems! As part of our commitment we give away one of our eBooks every day.
It's chasing an ever-changing target. You never have a stable set of simd (or other cpu-specific) instructions. Whatever you do, it has to be flexible enough to adapt to new implementations while still giving you the option to fall back to something workable. In practice it will never be transparent or automatic for the programmer. 
That seems reasonable. However, how do you get the arr[x][y] syntaxis? I thought a vector of vectors would have to be accessed like this: arr[[x], y]?
You really do want the ability to explicitly specify simd operations in your code, though. For some code you know you need it - and you also typically need to control memory access patterns in those cases so you're not making the caches unhappy. I think this ties in with the future need for specifying threading (in an OpenMP manner) by the way; in all these cases you want the programmer to be able to specify how to actually interpret a section of code.
Post your configuration, CPU model, OS here: https://github.com/redox-os/redox/issues/767
why wouldn't this work? unless you're just curious about how that code had been designed. https://is.gd/usyGev
Did you know that [`_mm_crc32_u16`](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_crc&amp;expand=1220) doesn't use vectors at all? Yet, it was introduced as part of the Streaming **SIMD** Extensions 4.2 instruction set. This is the language that vendors use, so it is the language that I use. The goal is to make it possible for folks to use SSE, AVX, NEON, etc., vendor intrinsics on stable Rust. High level APIs come later.
Unreal engine 4 seems to do it at compile time with "microsoft specific SSE?" FQuat FRotator::Quaternion() const { SCOPE_CYCLE_COUNTER(STAT_MathConvertRotatorToQuat); DiagnosticCheckNaN(); #if PLATFORM_ENABLE_VECTORINTRINSICS const VectorRegister Angles = MakeVectorRegister(Pitch, Yaw, Roll, 0.0f); const VectorRegister HalfAngles = VectorMultiply(Angles, GlobalVectorConstants::DEG_TO_RAD_HALF); .. where /** * Multiplies two vectors (component-wise) and returns the result. * * @param Vec1 1st vector * @param Vec2 2nd vector * @return VectorRegister( Vec1.x*Vec2.x, Vec1.y*Vec2.y, Vec1.z*Vec2.z, Vec1.w*Vec2.w ) */ #define VectorMultiply( Vec1, Vec2 ) _mm_mul_ps( Vec1, Vec2 )
Hey u/arkham, as you can see I'm still going at it. Actually that does help. And it's pretty neat too. That means this could scale to n-dimensional vectors. Since the code itself is parameterized with dimensions for the grid, makes me wonder why they didn't include macros to create this n-dimensional vectors in the first place.
Why would I read this instead of the official docs?
Can you provide some more info on the consequences of increasing the program's recursion limit? What is it by default and how was the new value of 1024 chosen? Are there any performance penalties for doing this? If not, why is the default value not high enough to accommodate error-chain? Would it be possible to change error-chain's internals in a way that doesn't require recursion?
Could anyone explain how this is different from The Rust Book?
My senior project for school has been a zero knowledge backup system written in Rust with libsodium (via rust_sodium). It's pretty ugly code wise (hey, it's my first project in Rust!) and I didn't get the netcode working, so it's just a local backup system right now. This week is building a presentation for it. 
Brilliant, thanks!
Follow up now that I've actually read everything: The high level API is being (rightfully) punted. Focus is on getting a low level API worked out. The low level API deals with: * SIMD type representations and the differences between vendors and type safety. * Getting some/all the intrinsics defined (c&amp;p from clang? but clang isn't perfect either). * Special case intrinsics that need compile time arguments (shuffle and friends). * Dealing with how the arch and extensions are gated.
Try `println!("{}", target.len())` just before `let user_id_vector = ...`, it seems that not all the information is reaching `post_decode`. I imagine that the error is not in the code you have supplied.
Maybe when my code stops sucking, I'll code said crate and submit it to the community :)
Unfortunately, that proposal has precisely the same problem as just using `__m128i` everywhere: you'll need to cast them to concrete types for the higher level APIs. The higher level APIs will need to define different impls for, say, `Add for u32x4` and `Add for i64x2`. If those two types are aliases, then that's a no-go.
There's a typo in your main blurb about the book at the bottom. "Ruse" instead of "Rust"
&gt; Yes, but why can't the higher level APIs provide newtypes? And now you have to cast every time you want to dip back into the lower level APIs... That was the initial motivation for putting better types on the Intel intrinsics, so that folks could move more freely between the APIs. &gt; Only if you desire to go down the abstraction layer will you need to 'cast' which seems reasonable. This is in fact precisely the point at which you disagree with a lot of folks in that thread. (I don't happen to strongly disagree with you by the way, but their arguments seem somewhat persuasive.) &gt; The fact is that at the lowest level these are the same types, even though Intel themselves pretend there is a difference between them it is only loosely enforced at the API level. The other fact is that Intel's API is the oddball of the bunch. The Arm NEON APIs all have types like `u8x16`, and even Clang/LLVM convert Intel's APIs into more structured types in their builtins/intrinsics.
This is all fair enough. I've no idea what might be the right trade-off to make. Seems like an incredibly daunting task where there just isn't a way to satisfy everyone :/
Hey everyone. I'm having a bit of trouble figuring out how to resolve an error. I'm trying to use an enum as an error type. One of the variants contains an associated type of a trait. What I'd like to do is write a converter from T::Error into my enum type, but when I do, I get an error saying "conflicting implementation in crate `core`". I've written up an example on play rust: https://is.gd/aFIBDy (uncomment the From&lt;T::Error&gt; implementation to see the error) Any help much appreciated!
You're right, `target` is about 20 bytes shorter than it's meant to be. The error is definitely somewhere else. Cheers!
&gt; Depends on what you mean by good. Of course lower-level interfaces are likely to offer potentially higher performance. This is true for parallelization and vectorisation. That's what I mean by good. For HPC, higher performance is the whole point. Of course you need to balance it out with development time which is why a "close enough" OpenMP abstraction would be very welcome. With that said, compilers are quite good at teasing out SIMD opportunities in common code patterns for C and Fortran today, and I bet you could get the same "for free" benefits for Rust as well given time. &gt; The problem with running MPI on that much hardware is that hardware failure becomes very likely, and MPI does not deal with that very well. If one node fails, it will take down all other nodes. The MPI libraries implemented for that scale machines do handle this situation reasonably well (periodic checkpointing is a good idea). But this is indeed one of the issues that needs to be solved for exa-scale machines of course. And no other library is even close to solving all those scaling issues either; whenever exa-scale hardware becomes a reality we will need some serious rethink of the software stack as well. &gt; I don't think that is true, the API are network communication primitives, which MPI uses internally. Of course MPI is the most commonly used API for inter-node communication and hence the best supported one. For Peta-scale hardware this is (unfortunately) literally true. They implement bespoke interconnect hardware architectures, then tend to expose that only through an MPI-compatible library for the user side. So if you wanted to, say, implement ZeroMQ, you'd have to do it either with slow, inefficient posix network primitives that never get anywhere close to the real system latency, or through the fast vendor-supplied MPI implementation. 
&lt;3
The Cargo.lock file ensures that anyone who builds the project will build with the same version of libraries that the developer published the application with. So only if you run Cargo update might you encounter API breakage, although very rare without also upping the versions of libraries in your Cargo.toml file.
Thanks! Fixed now.
Pushed an update with some of these comments addressed. Thanks!
Pushed an update now that briefly mentions Box. Thanks for the feedback!
Not affiliated with the author, but for free I downloaded it and took a quick looksie. Here are a free quick reasons: It appears to want to supplement the official docs rather than replace them. It actually references the online docs several times. It is written much more casually than the official docs which can be boon to some people. It has learning exercises to help you get a deeper understanding of the content. 
Did your thinking result in any useful, Rusty pattern? If so I want to hear about it :-) I think it comes down to needing pointers in both directions, because of the need to make method calls in both directions. Which makes the obvious follow-up question "do you really need to call in both directions", which in turn is related to our "event loop" story, i e "how does the program get to know about the fan failure event in the first place". I think it's possible to make things work without pointers in both directions, but then you need to send in those pointers/references in every method call instead, which might not scale very well with the complexity of the program. But this is just my thoughts rather than actual experience, so take it with a grain of salt.
There is the unsafe [ptr::copy](https://doc.rust-lang.org/std/ptr/fn.copy.html), but I wouldn't be so sure that the version you're currently using is that slow, it looks like it could be optimised fairly well by the compiler. Have you measured it? (Make sure you run with optimisations, i e `cargo release`) 
Thank you for this summary. The thread is a bit overwhelming at this point :)
&gt; What if, tomorrow, a very popular Rust library is published with a wrong version number, can it break lots of other projects? **No.** There are two bits and pieces that matter here: 1. The published libraries are immutable 2. `cargo` creates a `Cargo.lock` file which references the exact version of all your dependencies Therefore, any sane project will *commit* the `Cargo.lock` file, guaranteeing that everyone on a given checkout of the project uses the exact same dependencies (all other factors such as platform/OS being equal). In case of update, a person or script will call `cargo update` which will modify the `Cargo.lock`, commit the change, and push it. Providing that either: - the updater is first compiling and testing the changes locally - or there is a continuous integration setup that validates pull requests before integrating them then a busted update will provide *some* angst but will be intercepted before it reaches your project's master branch where everyone is using the *current* dependencies which work just fine.
Presumably yes, once [this LLVM bug](https://github.com/rust-lang/rust/issues/31681) is fixed.