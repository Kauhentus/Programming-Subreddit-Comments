It's not really a lot. I just posted it because it's a base 10 overflow and humanz seem to celebrate those. As comparison: - [`c++`: 585_000](https://stackoverflow.com/questions/tagged/c%2b%2b) - [`java`: 1_459_000](https://stackoverflow.com/questions/tagged/java) - [`js`: 1_682_000](https://stackoverflow.com/questions/tagged/javascript) - [`swift`: 206_000](https://stackoverflow.com/questions/tagged/swift) - [`go`: 32_000](https://stackoverflow.com/questions/tagged/go) - [`julia-lang`: 4,400](https://stackoverflow.com/questions/tagged/julia-lang) - [`haskell`: 38,000](https://stackoverflow.com/questions/tagged/haskell) - [`prolog`: 9,600](https://stackoverflow.com/questions/tagged/prolog) Rust is more present on GitHub than on StackOverflow. There was a nice chart about this posted a few months back. It listed all languages where one axis was the number of SO questions and the other axis the amount of code on GitHub. Rust was way unbalanced in comparison to other languages -- in the GitHub direction. People discussed reasons for this fact already then. If I manage to find this thread, I'll link it here later.
I think it just means that a good programmer can make high quality software in any programming language even if it's not the most efficient language for them. I'd say Jonathan Blow is a good programmer and I think he's a great video game designer. But as a language designer, he seems unwilling to consider viewpoints other than his own. 
\&gt;he seems unwilling to consider viewpoints other than his own. &amp;#x200B; no argument there! &amp;#x200B;
 Yes, but it's not about the object itself. shared_ptr may allocate some memory to do housekeeping and weak_ptr's will prevent it's deallocation. It's not a gotcha in the std, it's how reference counting pointers work ([rust does this too](https://github.com/rust-lang/rust/blob/master/src/liballoc/rc.rs#L317)). If for some reason this behavior is not acceptable, you have to use something else, like reference linked pointers. 
Thanks! I think one one third of questions for Go is actually pretty decent, considering Go is older, quite popular and has a broader target audience than Rust. It's impressive how quickly Swift became popular though. 
A quick plug for the [underlying RustConf talk](https://www.youtube.com/watch?v=aKLntZcp27M): as someone familiar with rust but unfamiliar with ECS I found it to be a nice intro to "what is an ECS and what design constraints lead to designing/needing something like that?" 
The new aggregator class is pretty interesting. It says data must be queued on everything before aggregate() is called. How would that work if I wanted to write a custom muxxer for something like mpegts? Where I would probably want to packetize and send out video, but would only have audio and/or metadata some of the time.
Hyakuu's referring to the standard's recommendation that `make_shared` put the control block and the object in the same allocation. See the first note [here](https://en.cppreference.com/w/cpp/memory/shared_ptr/make_shared). This is how Rust always does it, right?
Every now and then I consider whether to answer questions about Rust on SO. But I really hate that just about anything you do there will get edited by someone else, usually Shepmaster. If this were SO your last sentence would probably get edited to "It would be interesting to see if there is an upwards trend." While such a change may be considered for the better, now it's someone else's words with your name under it. I much prefer reddit (or IRC, but my RasPi running Irssi died so I don't hang there anymore), where others cannot change your words. I'm happy to help people - on my terms, in my words. You can't have that on SO.
A segfault also gives you a backtrace. 
You must be talking about another video.
&gt; Hyakuu's referring to the standard's recommendation that make_shared put the control block and the object in the same allocation. I know, but's that irrelevant to my point. weak_ptr, *just like shared_ptr*, might prevent *some* memory being freed when you want to, which can be a problem, and is enough to justify jon's implementation (and that he isn't "just plain wrong"). &gt; This is how Rust's Arc/Rc always do it, right? Rust does shared allocation too.
Ggez has a new version on its development branch that is quite different. You might want to consider using it instead of version 0.4.
I find it kind of funny that the Rustaceans here can understand the zillion rules that Rust has but completely miss Jon Blow's point.
That's a huge accomplishment for the rust community! First Debian, now Ubuntu... Congratulations and thanks for the sublime work, /u/burntsushi! 
Agreed. I had some intuition of the theory behind ECS, but this talk goes into a lot more of the specifics to leave you with a feeling like you are actually ready to implememt stuff in a ECS.
I risk getting buried for this on this sub, but the c/c++-style naming is one of the few things I don't care for in Rust. Vec would be an example, but also I will never understand going with snake_case: it's just objectively more keystrokes than camelCase, and the underscore key is out of the ergonomic center of the keyboard. I suppose these kind of things are just because that's what's familiar to the systems programming community. However I think this is one area where other languages have innovated in good directions.
&gt; Doesn't cargo look for subcommands in your CARGO_HOME? Cargo looks for `cargo-foo` in your `$PATH` when you execute `cargo foo`.
I think Swift is so popular because of the massive iOS ecosystem.
&gt; He talks about having something that tracks the address of all weak pointers in order to null them. I've never seen anyone do this, but I suspect he would be bullish on the performance hit from double indirection, or the memory overhead of zombie allocations. It's called reference linked smart pointers, they aren't really used (for [a lot of reasons](https://stackoverflow.com/questions/14291744/why-doesnt-stdshared-ptr-use-reference-linking)), but sometimes you need them to deal with weak pointers keeping some memory allocated, [in cases where memory is very limited](https://en.wikipedia.org/wiki/PlayStation_3). &gt; The phrase "turn off the borrow checker" keeps coming up. I don't know if he believes that's literally what happened No he doesn't. He means defeating brwck by making a system that allows you to get a valid handle to invalid (stale) data. It's like saying that `void*` turns of type checking. 
Fedora has been shipping it for a while, also
Shrug, broadcast is a very domain specific requirement, and I don't mind the claim that Julia is an excellent language for numerical computation style work. It's only problem is that Python still has better libraries.
`snake_case` is a lot more readable than `camelCase`, and it's also useful to have this style distinction available as a notational convention instead of just using `camelCase` everywhere.
Okay, so first I click on 'commits' https://github.com/rust-lang/rust/pull/53981/commits and then the SHA of the commit: https://github.com/rust-lang/rust/pull/53981/commits/28745a6e190a8c61ba2f08b03ea8afed620c9735 This says ```text Implement initializer() for FileDesc in order to avoid constantly zeroing memory when it's not needed. master (#53981) ``` Note that it says `master`, and only `master`. This means that it's currently in nightly. Nightly Rust is 1.31. So, unless something funny happens, that's the release it will be in, on December 6th. What if something was in a release? Well https://github.com/rust-lang/rust/pull/52731/commits/3bca170bc7543da8ddb1b550a824ad0f4cbaf395 ```text introduce, but do not use, `free_region_relation` computation This duplicates, effectively, existing code in the universal regions computation. master (#52731) 1.29.0 ``` So, this code is in 1.29. Stuff that's in beta but not in stable will say `beta` as well. You can get a good approximation of this by looking at the date, but around releases it can be a bit tricky, as we branch beta before the actual release, so there can be a few days of leeway. It's only stable releases that we commit to the strong schedule around. Does that make sense?
Swift is actually not that much more popular in terms of GitHub projects. Take a look at [this graph](http://sogrady-media.redmonk.com/sogrady/files/2018/08/lang.rank_.618-1.png) (from [this post](https://redmonk.com/sogrady/2018/08/10/language-rankings-6-18/) I posted in another comment too). While Swift has way more questions on SO, it seems to have roughly the same number of code on GitHub. Broadly speaking, it seems that most languages above the line are the the ones people are often forced to use, while the stuff below the line is the hip new stuff. Of course this isn't true for every language there.
I think it's more about being able to differentiate on glance; I see a UserAccount and know it's not some local_variable, but rather a struct
I just call my physics-and-graphics vectors `Vec2` or `Vec3` as appropriate.
&gt; Swift is actually not that much more popular in terms of GitHub projects. [...] While Swift has way more questions on SO [...] This might be explained that Swift is mostly used commercially, while Rust might be attracting more people who use it for side projects they publish on GitHub.
It's not just C or C++; I fought for this style because it's the standard in Ruby, for example. I find it significantly more readable. YMMV.
This is a really good summary. Thanks for writing this for folks who can’t watch the whole thing but may be interested in getting this gist. I believe that you are right in understanding Jon doesn’t literally believe the borrow checker was turned off. His argument does seem to be that the borrow checker has effectively just been confused into complying because the ECS he is discussing (from he Rust Conf talk) effectively implements its own memory allocator. I would like to know whether more experienced Rust programmers agree with Jon’s argument. On the surface, it seems agreeable, though I think he downplays some of the benefits Rust offers over languages like C++ in this case. In particular, the borrow checker still helps us, I think, by ensuring the underlying semantics of our custom allocator are correct because we aren’t using unsafe code. Furthermore, using sum types like Option give us stronger guarantees about how memory lookups will be handled. I suspect this doesn’t come at a big performance cost. It’s worth noting that Jon’s personal opinions come or strongly here as well as in most of his “rants” like this. Jon is a very experienced developer who knows how to use sharp tools safely, so his opinion of tools like Rust’s borrow checker is influenced by his belief that it’s not something that would add value to his work. I don’t think he would disagree that it’s a good thing for most developers, as he said in his comment about net program correctness.
Not really, you have to run the program under a debugger, which then might not trigger the segfault, or wait a long time if it’s not deterministic (eg due to a data race), or compiling with debug information might prevent from reproducing it in which case the backtrace might suck, or... or...
Hi, author here. This is the review I've been working on. I got part-way through my review, got the crazy idea for the macro implementation, then polished up my review a little. I'd still like to expand the review to evaluate the considerations against some of the higher level proposals, but I got tired of sitting on this. 
I think you missed some of the comments in this post, the hour long video is NOT an hour long rant. It was done on twitch and he spent the time to answer his viewers questions &gt;jessfulwood: The video itself is an hour long, full of long pauses, lots of chat, digressions, bald statements of opinion ("Vec" is a bad name for a vector, apparently). The last 20 minutes is just random, mostly off-topic questions from viewers. If it was boiled down to a blog post it could be read in 5 minutes (but would probably take longer than an hour to write). I think this form of creating and consuming media probably only makes sense when you view it as Twitch-like entertainment. Probably lots of the viewers are gamers first and foremost who want to know a bit more about the development process. &amp;#x200B;
&gt; It's important to note that you still have the bug when "freeing" references (and reusing the same heap space), because entity referencing the component only knows it index. It is totally obivious to any internal state changes of the referenced object, or even memory overwrites. The rustconf talk is worth watching. It addresses this by "generational indices"; each index has a generation as well as an array offset attached to it. IIRC the generation is incremented every time something is deleted, so you can at detect this sort of use-after-free.
It's not that much but that might not be a bad thing. It could instead be an indication of the fact that we have a lot of places to discuss Rust code: this subreddit, the [forum](https://users.rust-lang.org/), IRC, Github, etc. It could also be an indication that we have a lot of good documentation and guides out there, and that the questions that people have can be answered using those. I certainly don't see the number of Rust questions on SO per day as a metric to strive towards.
He is right to some extent though - `size_t EntityIndex` is a reference in the abstract sense, but one that the borrow checker does not enforce ownership semantics of any kind on.
If you're lucky.
Sorry I should have mentioned I knew about the first part. I was more wondering about the process of making a release. &amp;#x200B; So if I understand correctly, as of right now the next 1.30 release will be what's in the \`beta\` branch. But I guess some commits can still be added to it? As for my commit, I understand that it'll automatically be in \`beta\` once 1.30 is shipped, and so it will be in 1.31 ?
this is the best time to start. Essentially everything is summarized in a few minutes
&gt; If you use cargo to make your projects you already have a crate, it's just not published. Beyond that, if you compiled any rust code, you have a crate. `rustc` takes a crate root as its argument to compile code. Slightly confusingly, crates.io takes *packages*, which contain one or more crates. Cargo gives you packages, not crates.
Well the implication of the rust-talk was that the borrow checker is why this works, which is not the case. That type-systems can help you is not what is being debated here.
Next step: `rg` is mandatory for POSIX compliance :-D
One indeed has to get used to it. It's really important to understand that SO is primarily an open collection of knowledge and not a communication platform (like a forum). Similar to a wiki that can be improved and extended by anyone. And yes, many people on SO (including me) are perfectionists and edit posts for minor improvements, too, because we think it's worth it for the thousands of future visitors. I dislike getting distracted by typos when I'm researching something. But this is actually a lot like open source development: the goal is to build something awesome together. Everyone can help and improve things. And yes, it hurts when someone rewrites a part of your code :P Btw, I'm not trying to convince you, but simply explain why things are the way they are. If you prefer other means of communication, that's totally fine. SO has fairly strong opinions on how certain things should be -- which don't suite everyone.
It's a better architecture relative to the OOP one Catherine was using before. Her point is not that other languages cannot implement a similar pattern but that Rust made it much harder to implement the 'bad' OOP architecture she was using before.
Great thank you!
You're quite welcome :)
Hey Steve, I think your example is poorly chosen because it *does* compile with NLL, e.g. [see here](https://play.rust-lang.org/?gist=6d0ae777af23c1471db546920104de74&amp;version=nightly&amp;mode=debug&amp;edition=2015). I assume that NLL will get stabilized at some point in the future, and people who live in that distant future will try out your examples and be confused; they don't get any errors. Have you considered adding a simple `println!("{}", y);` at the end?
snake_case sucks to type, camelCase sucks to read. That's a saying I've seen repeated a number of times, and with which I agree. camelCase is nice when I'm writing dumb 40 character Java identifiers and IntelliJ autocompletes everything from the upper case characters. snake_case is more readable, and if I may be a bit facetious, has less of an impact in languages that don't encourage outrageously long names. It also depends on where you are since the underscore placement can vary a lot between countries. I personally don't find it too bad on French keyboards, but memories of UK qwerty tell me it's more of a pain on those. Note, I don't believe Rust syntax is perfect, and my personal preference would have been to stay closer to its ML-like roots. But, I also know that C++ users are a major part of the language's intended audience.
&gt; "Vec" is a bad name for a vector, apparently Vectors had established mathematical meaning; since Blow does a lot of graphics he prefers to have a distinction between that mathematical meaning and the term used for a contiguous sequence of structures. &gt; his own language Jai, which has zero interest in memory safety or correctness That's not really fair. He cares about those, he just doesn't care to the exclusion of everything else. &gt; He does state at one point that he has never written Rust beyond hello-world, and it shows. I think he should write some more before he makes any more "rants". I don't think he would change his mind about it, but it might stop him wasting his time making videos that will be easily dismissed by the community. I think most of what he said was correct. The only thing he said that seemed factually incorrect was stuff about the internals of shared/weak pointers, and that error didn't particularly change his overall point. 
&gt; games programming really does seem to be it's own little world. _His approach to games_ is its own little world, in a way, too. Plenty of professional well-received games are developed nothing like his talks seem to indicate. I'm just as guilty of sometimes saying "games" when I mean "some games that I know about personally," and I think that's true for developers in just about any niche of the wider CS industry. :) &gt; I think this form of creating and consuming media probably only makes sense when you view it as Twitch-like entertainment. Jon Blow's a celebrity developer. Putting his name, face, and voice out there is what he does and what his fans want, almost more so than they want him to make new games. :p
&gt; However, his comments about loss of performance compared to using a hash table (or vector) might be true although it would be interesting to see some benchmarks. It'd be really hard to benchmark this well since it depends on the scale of things. Shared pointers make it much harder to allocate contiguously and efficiently, but that's harder to see when you don't have a large system and the corresponding stochasticity and memory pressure.
He meant it in the sense that these are being bypassed. When you're using indices as substitute pointers you get neither of these.
To each their own I suppose. I never found camelCase to have readability problems.
I basically watched the first half hour and then stopped, yeah. That doesn't change my reaction.
I don't think there is an existing list of everything, so here's my quick summary of what I know * `cargo` - Package Manager, Build Tool * `clippy-preview` - Linter * `lldb` - LLVM's debugger * `rls-preview` - Rust Language Server, used for IDE integration * `rust-src` - source code for Rust * `rust-std-*` - standard library * `rustc-*` - Rust Compiler * `rustfmt-preview` - Code formatter * `rust-docs` - Documentation The ones with preview in their names are still unstable (not yet 1.0).
This is /r/rust, a sub for a programming language named rust. /r/playrust is the sub for the game Rust.
This feels disingenuous. You're picking at minutia and speech patterns that don't represent the video as a whole. His overall point seemed solid to me.
Ah that's true. I'll make a revision, thanks :)
Could this be a bug with the new `dyn` trait object syntax? If I switch it to the old syntax, removing the `dyn`, it compiles just fine.
I think the big difference between SO and wiki's / open source development is that on SO you have your username next to your contributions. Changes on a wiki or in some code don't feel like someone else changing my words, because my name isn't next to it. I have deleted answers on SO because I disagreed with unnecessary edits that were made. I think too much on SO is done with the hypothetical 'future visitor' in mind, rather than bringing someone in need of help together with others offering help. But even if we accept the the goal of building the goal of building 'an open collection of knowledge' as the most important thing, think of this: unnecessary 'perfectionist' edits *are* putting off potential answerers. For every `n` minor grammar issues you fix, someone leaves. Is that in the best interest of the open collection of knowledge? Voting down an answer on SO supposedly costs 1 reputation. Perhaps a similar cost should apply to edits to other peoples questions and answers, to discourage unnecessary edits. tl;dr I hate the SO culture.
This is a [known issue](https://github.com/rust-lang/book/issues/1459). It was patched [here](https://github.com/rust-lang/book/pull/1467) But I guess the web version hasn't updated?
Memory safety bugs don't always happen, so the crashes are not predictable. Since indexing is bounds checked in Rust, a bad index will always panic.
My basic arguments I would give in response are: * The borrow checker is still there. No matter what your solution is, it won't contain dangling pointers, segmentation violations, double frees, data races, etc. * If you like crashes, when looking up a component that's `None`, it's pretty convenient to safely panic immediately with a useful error message. * The problem of reaching the wrong component with the same index value is just outside the scope of the borrow checker and in the domain of logic errors that are your responsibility. * Reference counted pointers and weak pointers aren't as expensive as he suggested, but if that's your solution, rust can do that too. * It's always the case that you could devise a solution in an unsafe language where these things aren't a problem. * Using array indexes instead of pointers/references is always an option to resolve borrow checker issues. Instead of thinking of it as turning the borrow checker off, you can think of it as structuring your program in such a way that safety issues are no longer a concern. The point of the talk was that this is probably a better way to structure the program anyway. But overall I don't think he's very wrong about anything. It does come down to the more nebulous debate on whether the friction costs outweigh the saved debugging costs down the line. I'll say that I don't fight much with the borrow checker that much anymore. I fought with it initially because I was trying to write crazy C code with pointers everywhere. It *probably* saves time in the long run to get feedback very quickly that this isn't a good design. But it doesn't *feel* this way. It feels frustrating and like you can't get anything done.
Ok cool thanks for your help.
Yeah I mean I understand where criticisms of Blow come from: he can come off as arrogant, and while speaking from an assumed authority he sometimes makes mistakes, which people love to point out since he's always telling everyone what they're wrong about. But he's a hugely productive programmer, and he's managed to finish multiple complex, commercially successful software products. I wonder how many of his critics can claim that? I think there is room in the world for people who's fault is that they are a bit too convinced they are correct, and use that conviction to produce something of value. We certainly have enough people who are very careful to avoid any mistake worthy of criticism, and never manage to do anything of note.
&gt; In particular, the borrow checker still helps us, I think, by ensuring the underlying semantics of our custom allocator are correct because we aren’t using unsafe code. I think this is the wrong angle, and Jon points that out when he says things like "the way these games are structured, these vecs will never go away while you're using them." The borrow checker is not completely gone just because you're using (generational) indices. It still provides two major benefits: 1) When you temporarily convert an index to a pointer, it will track both that pointer and any pointers to store behind it. The `Vec`s will never go away, but they may be resized (if you actually use `std::vector`/`Vec`, which you may very well do at least early on). The `Vec`s will never go away, but you may unintentionally store a pointer in them to something that will. 2) At a larger scale, when you're figuring out how to actually process the `Vec`s' contents, the borrow checker will still prevent data races (due to parallelism) and iterator invalidation (due to inserting/removing elements at the wrong time). This is great, and is heavily taken advantage of by the Specs project she mentions. You also see Unity's new ECS caring a lot about this, but they handle it all with debug-mode runtime checks. 
BTW is Option&lt;NonNull&lt;T&gt;&gt; guaranteed to have the same representation at `*mut T` or only the same size? And if it's guaranteed, does that guarantee also extend to references?
The point is about make_shared specifically, which allocates both the control block and the object in one large block for efficiency, which AFAIK means that the object's memory can't be freed until the control block is no longer needed. (The object's destructor is called though.)
I’ve actually never met the lingo “lint” until I entered the Javascript scene. I still wonder where it first came from. 
If you made the indices have a lifetime parameter, it would be completely fixed I think, but then it would completely defeat the purpose.
Rust is "pass by move", and a move is a copy if the type is Copy.
&gt; absolute worst thing that could happen No, the absolute worst thing is crashing and finding out your save state was corrupted because the programmer thought dealing with memory correctness issues was too hard and the program *didn't* crash when it was creating the save state.
Is this meant as a response to recent [Jonathan Blow video](https://www.reddit.com/r/rust/comments/9fqget/jonathan_blow_entity_systems_and_the_rust_borrow/)? I think he was talking about how you can use array indexes instead of references and borrow checker can't check what you are doing. If you do something wrong you will have out of bound error or get reference to the wrong element at runtime instead of compile time error from borrow checker.
There are alternatives to indices-into-a-Vec that have also been explored. There's the obvious `Rc&lt;(Ref)Cell&lt;T&gt;&gt;`, and there's the less-obvious arena allocators that hand out `&amp;'arena Cell&lt;T&gt;`s. We can improve the ergonomics of `Cell` by things like field projection (`&amp;Cell&lt;Struct&gt;` -&gt; `&amp;Cell&lt;Field&gt;`), and we can make the implementation of `Rc` fancier (tracing GC integration), but the fundamental problems, solved by the Vec approach, remain. &gt; Because the current solution is just to abandon the borrow checker. This is wrong. The borrow checker doesn't check the indices anymore, but it *does* check the `Option&lt;&amp;T&gt;`s you convert them to, and it does still play a role in parallelizing your systems. It's really not much different from `Rc&lt;T&gt;` "abandoning" the borrow checker.
Definitely. You basically would need a program size similar to that of a complete game to compare the alternative implementations. I would assume that something like `slotmap` to be faster than `Rc`, but it would be interesting to see some performance numbers.
Very excited for this. serde's recursion has bitten me in a production system. I imagine most people using serde don't know whether it's a problem for them or not. Thanks u/dtolnay!
Some of the projects at https://www.servoexperiments.com/ might demonstrate differences. I haven't looked recently though.
scalaz was a frequent subject of conversation early in Rust's development - namely, if Rust's type system is ever powerful enough to implement it then Rust has gone beyond the type system complexity limit. I still agree with that. Rust is already too complex.
&gt; I think a weak\_ptr can prevent memory deallocation if the pointed object was created with make\_shared instead of new. Does it at least desteuct the object, even if it mist leave the whole allocation in order to keep the control block alive? I was surprised at his list-of-weakptrs description, but your point clarified that a lot.
Disagree, the capacity to understand isn't the problem people have, it is the non-obviousness of a name implying one thing and meaning another. If &gt; a List isn't actually a list but it's called a list I would expect complaints. I know because I am a data scientist, who programs in python mostly, and when learning a little c++ was bitten by thinking of a linalg vector. I am still not sure what a vector is supposed to mean in c++ but mentally I have mapped it to a stack. If this is what a c++ vector (&amp; rust Vec) actually are why not just call it "stack" and have push and pop methods instead grotesque emplace_back garbage? A stack is a familiar cs structure a vector is not, it is a math structure.
Aye! Greetings fellow nixer! 😄
Not directly. I've been meaning to write a post like this for a while, and the discussion around the blow video made me finally write it. However, I haven't watched the video yet, and so this isn't really about what he said. If I wanted it to be about what he said, I would have linked to it. I'd have to watch it first though :)
&gt; BTW is Option&lt;NonNull&lt;T&gt;&gt; guaranteed to have the same representation at *mut T or only the same size? I'm not sure what distinction you're drawing here, can you elaborate? &gt; And if it's guaranteed, does that guarantee also extend to references? `size_of::&lt;Option&lt;&amp;T&gt;&gt; == size_of::&lt;&amp;T&gt;` is, yes. 
I agree with the sentiment but I have not seen such aggressive editing for languages such as Haskell like I have seen for Rust (e.g. casing conventions). If you want to make a change, just do it, please don't make passive-aggressive comments suggesting the changes. Moreover, how does it matter in the slightest if I wrote it as Github instead of GitHub? But now I get a notification so I have to go double-check the edits that were made. Its a QA site not company documentation...
Hmm, I think the example error message is referring to the earlier example.
Ah crap. I’ll fix it soon.
That's true. Probably would make a good sequel to the blog post :-) 
I'm saying it's an 'anti-pattern' specifically in Rust. I don't consider it an anti-pattern in C/C++. The reason I believe so is that Rust is supposed to offer these memory guarantees for us and has facilities for handling it for us. C/C++ that isn't really the case -- sure you can use the std::unique_ptr, shared_ptr, and weak_ptr but these have other tradeoffs and they still don't offer as strong of a guarantee as rust does. 
Can't be done. Julia's dynamically typed. Rust has no runtime type reflection. Also, considering I do most of my work in a language that lets you do this, I have to say: It's an overrated feature and is useful for exploratory data manipulation and not much else.
Incidentally, I've been working on a doubly-linked list, backed by a vec, using generational indicies. In my current tests, it is either roughly as fast, or way, way faster, than a malloc-backed list, which would be similar to the Rc solution, given that each rc has its own allocation.
In C++ or Rust, yes, the object is dropped when the last strong reference goes out of scope, and the memory is freed when the last of all strong and weak references goes out of scope.
He did acknowledge in the video that it can be positive that somebody is nudged toward thinking about better solutions to a problem, which should be part of the cost/benefit analysis.
1. There can be different representations having the same size. For example since references must be aligned, a compiler could choose to store the address divided by the alignment. Or it could choose a different invalid pointer to represent null. In C/C++ different pointers are allowed to have different sizes and representations, which is why you need to use static casts instead of reinterpret casts in many cases. Or it could pass one as a register to a function, while passing the other as pointer or in a different kind of register. Which might cause problems in FFI when using `Option&lt;T&amp;&gt;` instead of `*const T` as parameter type. 2. In the current implementation of the compiler all of those properties may hold. The question is if the specification guarantees that the property will hold for all future versions of the compiler (and alternative implementations). 
A stack is an abstraction that multiple different representations may implement, and in particular, while a vector may be an ideal choice to *represent* a stack, a vector has many other properties that are not part of the definition of a stack. Chief among them include constant time random access to any element in the vector. There are other properties, such as amortized append operations ("dynamic array", "growable array"), that I believe are common to every vector implementation I'm aware of. In Rust and C++, vectors also specifically provide the guarantee that elements are unboxed and stored contiguous in memory. I can't recall any implementation *named* "vector" that doesn't have that property though. A more practical way to look at this discussion is that it's not productive. A `Vec` is a `Vec` and it isn't going to get renamed.
This is a great pun and you need more upvotes 
Well I think that's why he labeled it a "rant" and "commentary" about the subject rather than something more self-important.
He has a small cult following from braid. I think it's fed into his ego a little bit and I think it went to his head. Don't get me wrong. Smart guy. Excellent game developer, but technically, I feel he is somewhat overrated. 
I was writing a very long reply here, but I don't think either of us is going to change their mind, so let's agree to disagree. I'm still interested to find out what "URLO" means, the only meaning I could find is "scream".
I’m not 100% on 1, but I think it’s true. For 2, it’s specified, yes. 
Would you be interested in collaborating to creat a Rust-cryptofin community on reddit as well as being a mentor for me? I am very passionate about joining the crypto/blockchain community. I could use your mentorship. Btw im in the Bay Area if you are as well since you said you work at BitGO.
Not bad, but honestly I feel this discussions are fairly lacking without including the ASM that is generated by in the Value/Reference case as that demonstrates the difference and why in some cases it is important. 
Using things that specifically opt-out of compile time guarantees like indexing, or even, say, `RefCell` or `Cell`, aren't and likely never will be anti-patterns in general. Anti patterns manifest when opting out of those compile time guarantees is unwise, which will vary on a case by case basis. The great thing about the borrow checker is that you can very selectively move compile time guarantees to runtime guarantees in a specific part of your code that doesn't necessarily bleed into everything else.
&gt; Can't be done. Julia's dynamically typed. Rust has no runtime type reflection. I am not sure it runtime reflection is necessary; traits should be able to provide the necessary customization points no?
I really don't think it's that bad. He acknowledges that he agrees with a lot of the RustConf talk, and that he's going to pick on something subtle. Given that he's an expert in his field, I actually _prefer_ to hear him dive into something specific and nitpicky, because it means I'm getting lots of details that I might not've heard from anyone else. Here's how I heard his main point: **What's Jonathan reacting to?** I think there are two distinct things, which come back to back in Catherine's talk. - "Nobody [uses raw pointers], because it's wildly unsafe. If you keep internal pointers, they will become invalidated, and your game will crash." - "This is important, because we _have_ to do this in a lot of places in Rust, but it's the best idea." **Why does he feel strongly about those things?** I think it has to do with a lot of talking points he hears on the internet about Rust, especially since he's building another compiles-to-native-code language right now. He probably feels like he has to correct people frequently on certain points, and so they stick out to him when he hears them. In particular, he probably has a lot of conversations like this: - Internet: You should be using Rust for this game. - Jonathan: I don't want to, because Rust adds friction for me in XYZ ways. - Internet: Sure, but it also solves problems ABC. - Jonathan: Yes, but ABC isn't a problem for me. DEF is the problem I really have, and if paying the cost of XYZ doesn't get me any closer to solving DEF, then I don't want to pay that cost. - Internet: [Conflates ABC and DEF, without understanding real objection.] Having that sort of conversation is frustrating for anyone, and he's probably had it a hundred times. **What are his corrections?** To the first point, it sounds like he's saying that, while it's true that nobody uses raw pointers, it's _not_ because it causes unsafe crashes. To him, unsafe crashes aren't so different from safe panics or aborts or whatever. The world stops and tells you that you've made a big mistake, and you know you have to go fix it. As long as it's not causing subtle logic bugs and silently corrupting the game state, he's happy. Instead, the real problem with raw pointers for Jonathan is the cases where bugs _don't_ cause crashes, because that means you might be reading/writing to some other random object that wound up in the same memory. Those are the ones that really suck up his time as a developer. To second point, it sounds like he's saying that, while the borrow checker does force you to solve the unsafe crashes problem, it _doesn't_ force you to solve the subtle logic bugs problem. He [gets into this part around 31m20s](https://youtu.be/4t1K66dMhWk?t=31m20s). What the borrow checker forces you to do, is to replace your raw pointers with something like `Vec&lt;Option&lt;Component&gt;&gt;`. That guarantees that you won't have unsafe crashes anymore, yes. But you can still write to the wrong object! If you happen to hold index 5, and some other part of your program deleted the object at index 5 and then allocated something else there, you're still going to have those subtle bugs that you had with raw pointers. The real solution is the generational indexing scheme that Catherine goes on to describe. And the central point that Jonathan is making here, is that **the borrow checker doesn't force you to do that**. Instead, we use them because we're experienced developers and we understand the problem that they're solving. So the specific claim that "in Rust you have to write this game correctly" is wrong. Rust is perfectly happy to let you write the bad version that's just `Vec&lt;Option&lt;Component&gt;&gt;`. **How would I respond to his corrections?** Of course, probably the most obvious response is that for much (most?) of the programming world (including multiplayer games!), unsafe crashes are a very different beast from safe panics and aborts, and they end up causing all sorts of security problems and expensive incidents. Even if you believe you don't have to worry about security, it's very nice to live in an ecosystem where you can use the same libraries as folks who do worry about it. I'd add that while I think Jonathan is correct about this particular case, there are plenty of other common cases where the borrow checker does prevent tricky logic bugs. Mutating some object you weren't supposed to mutate is a sweeping category of bugs. Resizing a collection while you iterate over it is another big one. I'd also emphasize part of what Jonathan points out himself, which is that unsafe crashes can turn into freaky heisenbugs, if the memory you're writing to just happens to live long enough. Having a lot of your unsafe crashes come in the form of compiler errors or checked panics can make them more reliable. In this particular example, at least the `Vec&lt;Option&lt;Component&gt;&gt;` is guaranteed to panic when you try to unwrap a None, while a broken C++ pointer equivalent might not crash at all in the oops-that-object-is-deallocated case.
I generally don't have the same issues that you are experiencing. The difference between debug and release comes down largely to the optimizations enabled through the LLVM. By default, a debug build is built with `-O0`, which means no optimizations. For an intensive workload such as yours, that may be leaving a large number of things on the table. I'd recommend changing the optimization level for your debug build from 0 to 1. It shouldn't add too much to the compile time, but also can give you most of the optimizations that you were looking for. To do that, update your `Cargo.toml` file, and add an entry to override the optimization level in the `[profile.dev]` section (add it yourself). Here's the link to the (Cargo manifest documentation)[https://doc.rust-lang.org/cargo/reference/manifest.html#the-profile-sections].
/u/fitzgen made an [arena with generational indexes](https://github.com/fitzgen/generational-arena) for rust. I wonder if this library solves the problem that Jonathan Blow describes in the video. 
&gt; Incidentally, this code is safe. Raw pointers are allowed to alias. And we have no &amp;T or &amp;mut Ts here. So we’re good. I got told that you can never alias mut stuff even if you use pointers because it was UB, why is there all this conflicting info?
At this stage I'm more responding to the characterization from someone else than anything Jon said! :)
Actix, as mentioned by the author somewhere, heap allocates everything it passes around as well. For a framework focused on message passing, this is fine, but it won't be as performant as something like Rayon, which avoids heap-allocation and virtual dispatch.
&gt; I get that outdated indices could return garbage data or crash the program or something like that, but could it be exploited in a way that a crafty adversary could use to run arbitrary code? Yeah it's definitely worth clarifying. Without the `unsafe` keyword, you _cannot_ cause undefined behavior, especially not something like having an attacker execute arbitrary code in your process. Important caveats of course: This assumes you're not calling into libraries that have `unsafe` bugs of their own, and it assumes the compiler doesn't have any big soundness holes. (There are some soundness holes still waiting to be fixed in the compiler, but I believe they all require wacky code that no one would ever accidentally write, so they're not considered security issues in practice.) In these particular cases, as Jonathan is pointing out, you could write a Rust program with a bug where you keep a stale index around, and then later end up writing to some other object you didn't mean to. But it's always going to be an object of the same type, so the consequences are always a well-defined logic bug in your program, rather than arbitrary memory corruption. If you try to write out of the bounds of the vec, your program will always panic, and if you try to inspect the fields of some `Option&lt;_&gt;` type that's been set to `None`, your program will always panic.
You can’t alias &amp;mut T. *mut T has no requirements.
&gt; A stack is an abstraction that multiple different representations may implement, and in particular, while a vector may be an obvious choice to *represent* a stack, a vector has many other properties that are not part of the definition of a stack. Agreed. I was being too simplistic. &gt; A more practical way to look at this complaint is that it's not productive. Disagree. Naming is important for understanding and if you never look at your (rust's) mistakes with respect to naming you may end up with c++. &gt;A Vec is a Vec and it isn't going to get renamed. That may be but it's still a crap name. &amp;#x200B;
His overall point was one that could have been presented in a much more constructive way, instead of in opposition to someone else's talk. This video is one giant "well, actually" and I'm disappointed that the community isn't recognizing it as such. This isn't a minutia thing, he spends quite a bit of time taking apart a claim that he himself isn't sure if Catherine made (she didn't!).
Jonathan [says](https://youtu.be/4t1K66dMhWk?t=2m10s) "Rust is a language that, I think, has more of a reason to exist than most new programming languages." If we end up summarizing that as "Rust is bad", how is he going to feel about engaging in discussions about Rust in the future?
Fair enough lol.
Just paid for a version of Clion mostly for the Rust support, so keep up the good work!
If he was reacting to a general sentiment he's seen it would be much more constructive if posted that way. Instead he nitpicks a specific person's talk, misinterpreting a lot of it. The whole "crash" nitpick is absurd, the term is used pretty loosely and can include memory safety nonsense like overwriting random objects that doesn't end up in an abort but does cause problems. This is a talk. Like most media, nuance will be missed, especially when there's a time/length limit. Give the speaker the benefit of the doubt and move on. Similarly, Catherine never claims the borrow checker solves all the problems, she claims that it drives you to a better solution _in her specific examples_. She never gives generational indices as something handed to her on the borrow checker platter, she does that for indexing and then observes that indexing is kinda broken. (The original version of her talk actually had more here but it was shortened for length, and _that shouldn't matter_ because you can give speakers the benefit of the doubt) Jonathan comes off as someone _looking_ for things to nitpick. I don't disagree that there's a valid point buried within that video. I think that the nature of the video makes it a major asshole move, and it's basically a giant "well, actually".
Not entirely sure but I think the only UB tou can get because of aliasing `* mut T` comes from data races, if `T` is a plain old data type. The reason why it's UB for `&amp;mut T` is to enable optimizations based on this non-aliasing. And of course, in safe code aliasing mutable references is forbidden because it can create UB for types with invariants to hold, the classic example being iterator invalidation when pushing to a vector.
The word "crash" is one that can mean different things. It's often used as a short form for memory safety issues (which it was in Catherine's talk). On the other end of the spectrum it can also be used for unintentional but coded aborts of the program (e.g. panics or uncaught exceptions) He's nitpicking terminology.
Hey, I want to thank you for Ripgrep. It really saved my teams backside. We thought our data pipeline had some discrepancies, we were able to use Ripgrep to quickly search for unique identifiers. I'm talking 100 to 250+ GiB files. Ripgrep literally ripped through them.
&gt; I think rust-analysis is related to rls but I'm not sure. It is, yes. RLS works by reading the analysis data.
I somehow agree on this one. This library definitely sounds like it makes a few design decisions that make it preferable for things like embedded systems. For those supporting CBOR might be an additional benefit.
He doesn't think Catherine's approach is sub optimal. He says multiple times that it is good. What he disagrees with is the the thesis that rust's borrow checker is helpful for solving the problem.
&gt; If &gt; &gt;&gt; a List isn't actually a list &gt; &gt; but it's called a list I would expect complaints. &gt; &gt; I know because I am a data scientist, who programs in python mostly Case in point: [Python’s list isn’t actually a list but a resizable array.](https://github.com/python/cpython/blob/master/Include/listobject.h#L25) Like a “vector” in C++ and Rust.
if blow never engaged with rust in the future that'd be cool
In the reference-to-usize example, you added 1 to what looks like a reference. If that code compiles I assume it just implicitly dereferences it but it could still be confusing/misleading, because in C adding an integer to a pointer would just give you a new pointer to a different place
counterpoint: [blow's games are bad](https://www.youtube.com/watch?v=xSXofLK5hFQ)
#### [Soulja Boy Provides His Thoughts On Braid](https://www.youtube.com/watch?v=xSXofLK5hFQ) ##### 347,524 views &amp;nbsp;👍3,676 👎1,471 *** Description: soulja boy is an incredible genius. witness and shed a tear as heavenly, poetic words come from his mouth regarding the electronic video game masterp... *chafenhimer, Published on Feb 17, 2009* *** ^(Beep Boop. I'm a bot! This content was auto-generated to provide Youtube details. Respond 'delete' to delete this.) ^(|) [^(Opt Out)](http://np.reddit.com/r/YTubeInfoBot/wiki/index) ^(|) [^(More Info)](http://np.reddit.com/r/YTubeInfoBot/)
&gt; I think the core of the discussion is whether this property is really valuable or not. For Jonathan, it sounds like it's not, since he doesn't really have to deal with evil input from the internet. But of course for most of the people who are interested in Rust, it's extremely valuable. Jonathan has pretty publicly stated that he doesn't see memory safety as a pressing problem in the game world. On the other hand, it's not as if game assets haven't been used for remote execution attacks already. https://blog.perfect.blue/P90_Rush_B 
delete
&gt; I got told that you can never alias mut stuff It's UB to mutably alias *immutable* stuff, e.g. transmuting a `&amp;` to a `&amp;mut` (see [UnsafeCell](https://doc.rust-lang.org/std/cell/struct.UnsafeCell.html).
That's not how I interpreted it. He wasn't attacking her talk, and mostly said he liked the way she did things. His clarifications about the point you're saying he was "taking apart" didn't feel like that to me either.
Matter of opinion I suppose. Braid wasn't for me, but it certainly seems to have brought Soulja Boy a lot of joy. I did enjoy Witness quite a bit. Either way I can respect how much work it takes to bring a video game to market.
I *believe* it would require for type inference to be supported across function boundaries since broadcast functions allow for late decision making on how to swizzle the values. Macros can be pretty crazy though. Maybe I'm wrong. I still sincerely question the overall value outside of futzing around with poorly described types. The sorts of situations where you actually end up working with this sort of thing in anything resembling a productive manner are linalg libraries where you're likely pre-building support in for mixed matrix/vector operations. Aside from that, you end up with unreadable morasses just as confusing as long chains of zip/fold/map
https://cl.ly/5b25df0cf7db Does this work for you? (you need to click Configure) The options appear to be working fine for me.
That's ... good?
Thanks for the kind words. :-) At files that big, I'd expect grep and ripgrep to perform similarly though, since they're blocked on I/O. Did you try grep too? Or perhaps the parallelism from ripgrep helped here.
&gt;Disclaimer: I basically only contribute to the Rust tag on SO and have very limited knowledge of other sub communities For what it's worth, I used to be pretty active writing answers on the Rust and Haskell tags on stackoverflow, and the obsessive closing of questions and editing of questions and answers (often without actually improving them) only happened on the Rust side. A few months ago, one of the usual suspects rephrased an answer that I had written more than 2 years ago, which was accepted by the asker and had gotten \~20 upvotes, without really improving it in any way. I honestly don't understand the mindset behind this sort of thing. The people on the Haskell tag seemed to be primarily focussed on actually helping each other and less obsessed with leaving their mark everywhere. &amp;#x200B; I've basically stopped writing answers due to this behaviour. Anyway, this is just meant to point out that a) this sort of thing is not universal on SO and you can have well-maintained tags without it and b) it's really off-putting to some people.
You don't see any difference between a leak of 1 MB and a leak of 10 bytes?
Fair point I guess lispers would have a legitimate gripe against python. 
&gt; I'm disappointed that the community isn't recognizing it as such I'm with you, thanks for piping up about it. It's just terribly tiring to disagree with people like Blow, for obvious reasons, as this entire thread proves. Plus, I stopped listening to anything Blow had to say a very long time ago (from my recollection, it was useless and vitriolic), which prevents me from non-ignorantly discussing the OP.
Did you mean this weekend? :) 
I think people need to be more aware that the borrow checker's job is to perform *static* analysis. Yes, it's very sophisticated and works in situations where you might not expect it to. But it's not Turing complete, so as soon as your ownership patterns *really* depend on runtime behavior, the borrow checker can't help you almost by definition.
Hello all, another question relating to the multithreaded simple web server in Chapter 20 of the book. I wanted to expand on it so that it would perform the graceful shutdown when I press Ctrl+C on the terminal, where it runs the drop function which sends the termination signals to all the threads and joins them before shutting down. I thought I could use the ctrlc crate to do this, but I'm having trouble figuring it out. I tried having it run std::process:exit but that just does the same thing as pressing Ctrl+C. I get compiler errors when I try to drop the TcpListener or ThreadPool because it can't move out of captured outer variable in an Fn closure. Am I missing something obvious?
ETA: 2048
I'm just going to post it basically unedited, otherwise I'll never get it out. Thank you for bothering me about it because I SHOULD have already done it.
Is Swift really using pass-by-reference there? I seem to recall that it actually passes a reference by value. The difference being that with pass-by-reference you cannot only modify the object that is being referenced, but also make the variable that you passed in point to a different object. For example Java is pass-by-value but only passes references around, while C++ is one of the few language that actually support pass-by-reference.
Thanks so much just saw this landed in stable! 😁
fack, now my integration test to ensure rust is working in ubuntu guests will always pass. gotta find a new cargo cult to worship
Can you use rayon for this? Or cross-beam? I thought those were more-so for arrays and stuff, and not really connections. Also, if the heap-allocation thing with actix is true, does that mean that using the cpu thread pool should be more efficient?
Well I don't really know how to make the decision though because I don't understand any of the variables. Like, are actix actors more efficient and scalable than the built in thread-pool? Another commenter mentioned rayon, and I'm not really sure if that's applicable here, or if any other solutions are out there which may be even better than the two listed. I'd love to make a pros an cons list, but I don't think I have enough information to properly make that decision yet, which is why I'm posting here asking for insight.
If he engaged more, maybe he would change his mind about the language?
&gt; A more practical way to look at this complaint is that it's not productive. A `Vec` is a `Vec` and it isn't going to get renamed. Blow isn't talking about what Rust should change, though, he's talking about what decisions he'd like to make for *his* language.
Is it easy to define a kind of serialization to your format, e.g. csv to v11 table?
I'm pretty sure it does. At [45:55] he says that generational indices do not make the problem go away since you can forget to use it and just use a regular index instead. Since this crate doesn't allow indexing by anything other than generational index, problem solved. I would guess `specs` does this too, but I didn't use it, and it was not shown in the original RustConf talk. He also says something regarding entity component systems while saying generational indices are not a real solution, but I didn't understand what he means since this can be used without ECS. It is just a custom allocator as he mentioned several times.
I just had a second son born, and outside of work I mostly change diapers and try to nap. But I would be happy to join, refer to whoever I might know, and if I find time even answer questions and point to good materials. Can't commit to anything else though.
&gt; A Vec is a Vec and it isn't going to get renamed. Wrong. I am renaming Vec in all my code from now on `macro_rules! list { Vec::new(); }` &amp;#x200B;
fd is a fast alternative to find too: https://github.com/sharkdp/fd
My goodness. I wasn't even talking about what Blow said. I was responding to the complaints _in this thread_.
Of course, but in a game it's the difference between running out of memory in 5 minutes compared to 2h. Both are unacceptable no matter what, the advantage with the 1MB leak is that it is that it is easier to track down. 
To keep things simple, I would just pickup a framework like actix or tokio, although both will probably change in the future as async-await gets added to the language. Using a thread-pool natively would allow you to avoid allocations, but to honest for this kind of work, I probably wouldn't bother as the ergonomic benefits are tremendous. Both actix and tokio are backed by crossbeam as the thread-pool because it can be configured to be FIFO. For cache coherency reasons, Rayon is backed by a LIFO queue.
Does something like for line in csv::open(filename) { table.push(line?); } count as easy? (It might be doable iterator-style, I'd have to look into it.) The tables have to use their own serialization formats so it can't be a 1-liner.
Conference talks are short and concise, which this video... isn't
Please note there’s still a chance it might not be part of Ubuntu 18.10 or Debian buster: It got promoted to Debian testing due to a bug, not all build dependencies are in Debian testing/Ubuntu yet. The list you want to look at is https://qa.debian.org/developer.php?login=pkg-rust-maintainers@lists.alioth.debian.org For example clap is only in Debian sid right now, but needed to build ripgrep, so it might get removed by the release team.
Hi u/burntsushi Rust newbie here. I'm working on my first open source project in Rust which has a great deal to do with file I/O. In my program, to speed up things, I'm considering writing a mode which loads entire files into memory before performing any processing on it to avoid slower read from disk (as chunks). Essentially, I'll find how much system memory is free, and consume up to 80% of it by reading file(s) into it. But I'm confused if this approach is prone to breakage, or if it is a good or a bad approach. All I want is very fast I/O, so I'm doing the speed vs memory tradeoff here. Can you give a little idea of how `ripgrep` speeds up file reads. Any comments on my approach will be helpful as well.
The package is complicated since the name is very generic and already in use since 1995 by fdclone. This is problematic due to Debian policies and it’s generally recommended to use longer names and check for clashes in advance.
Sorry, I'm a bit confused by your reply. &gt;To keep things simple, I would just pickup a framework like actix or tokio The first way in my post seems pretty simple, as it's just wrapping a diesel query in a closure. The actix actor method, as demonstrated in the original post, seems much more complex, which is why I originally made this post. &gt;Using a thread-pool natively would allow you to avoid allocations, but to honest for this kind of work, I probably wouldn't bother as the ergonomic benefits are tremendous. I interpreted this as "the first way in your original post avoids allocations but I wouldn't do that because it isn't ergonomic" But the first way is definitely more ergonomic than the second way, and it's only adding a couple lines of code over not using a thread pool at all, so I'm unsure of why you wouldn't recommend it or consider it unergonomic. &gt;Both actix and tokio are backed by crossbeam as the thread-pool because it can be configured to be FIFO. For cache coherency reasons, Rayon is backed by a LIFO queue. Okay, so actix/tokio use cross-beam as a thread pool and Rayon just uses a queue (which .. doesn't sound multi-threaded..?) In any case, is there much of a difference between using cross-beam as a thread pool versus what actix provides with its cpu pool implementation? Should I prefer to leverage one over the other?
Hiya! So I will say at the outset that I strongly caution you to avoid any kind of logic that starts with, "look at how much memory the system has free, and then use X amount based on that." Unless you have a really specialized situation, it's almost always best to just let the OS deal with that. In terms of ripgrep, it has three different strategies for reading file I/O: 1. Plain buffered reading. This is essentially equivalent to using `std::io::BufReader` around a `File`. This is a tried and true method and it is difficult to beat it. ripgrep doesn't literally use `std::io::BufReader`, but only because it needs a more flexible buffer implementation, and not because `std::io::BufReader` isn't fast enough or something. But conceptually speaking, it's just a fixed sized buffer. The purpose of using a buffer is to amortize the number of times you make `read` syscalls, which can be expensive relative to a simple search task. In this case, you don't really need to care about available memory because you're probably using a small fixed for the buffer (ripgrep uses 8KB IIRC). 2. Memory map the file using the [`memmap`](https://docs.rs/memmap) crate. This is a nice way to get a `&amp;[u8]` that behaves as if the entire file was available immediately in memory. The OS transparently manages file I/O for you. No syscalls. Memory maps can be a touch faster when searching large files that are already cached in your operating system's I/O cache. But memory mapping doesn't work in all cases. For example, you can't memory map stdin and you can't memory map certain types of special files (e.g., `/proc/cpuinfo`). So generally, memory maps are an optimization and not something you can rely on. 3. Read the entire file into memory. ripgrep only does when then absolutely necessary. For example, when searching for a pattern that can match across multiple lines with `--multiline` mode enabled, then since the regex engine can't do incremental searching, ripgrep needs to read the entire file into memory before handing it to the regex engine. Why doesn't ripgrep just use a memory map here? Well, if it can, then it does, but it can't in every circumstance. For example, if transcoding needs to happen to convert data into a searchable format (e.g., UTF-16 -&gt; UTF-8) _and_ multiline search is requested, then ripgrep has no choice but to read the entire file on to the heap. TL;DR - If you can manage with a `std::io::BufReader`, then just use that and be happy. :-)
I figured sylvestre knew about that but that it got fixed? Assumptions...
Thank you. A follow-up question though. How will `rg` handle a situation where it needs to read the entire file into memory, but enough memory is not available, and memory mapped I/O is also not possible.
I think Sylvestre is more familiar with the Ubuntu processes than I am, I really hope it’s going to be part of 18.10. We‘re still working on some of the transitive dependencies and I would like to raise some awareness to keep dependencies updated (ripgrep is perfect in this regard, but some crates along the way are not). A lot of the time we spend on debian-rust are related to outdated dependencies. :)
`edited Sep 14 '18 at 3:15 by Shepmaster`
You got your () and [] reversed.
I didn't ask the question, but I learned something from the answer. Thanks for writing this up. 
That's fine! I'm happy to get any replies at all because I'm so new to all of this. Sorry if now I'm starting to sound repetitive, but could you explain why actors are superior to a thread pool if you want to support the most amount of requests per second? Earlier you said that actix heap allocates each actor as opposed to the thread pool so I feel like I'm getting mixed messages here. Shouldn't the threadpool be more scalable? This part is kind of confusing for me. If you have, say, 8 cores on your machine, and a thread pool of 8 threads being maximally utilized for incoming connections, how exactly does actix beat that? Are actors just better at context-switching or something? And if so, are there any metrics on this? Like, are they comparable (within ~10% performance?), or is it the case where actors are just completely superior and something that you would eventually migrate towards if performance is the ideal?
Use `enumerate()` and you should be good, here a solution that works: https://play.rust-lang.org/?gist=56501015bfa509a7e6818d5f649f28f8&amp;version=stable&amp;mode=debug&amp;edition=2015
Can I just say though that I think two letter command names are a horrible idea for anything that is not as fundamental as `cd`; even `ls` could be longer. Or when people name their tool `rename` and assume not someone else had the same bright idea.
Actors are typically just an abstraction over a threadpool. The extra boilerplate they add to your code also makes it easier to logically abstract your code into smaller, more logically defined pieces, which will be easier to extend and maintain in the future. In terms of performance, actors will be the same as you manipulating connections yourself, they should be no faster or slower. If you look at the Tech empower benchmark for actix, you'll notice there is also an "actix-raw". That's where the author avoided all heap-allocations and dynamic dispatch to get max performance, at a big loss to ergonomics. Trying to sum this up, I only meant that the actor pattern makes it faster and easier to write maintainable code, but in terms of "raw" performance, actors can be just as fast as direct threadpool manipulation, but typically aren't to maintain good ergonomics and a sound API.
Fixed.
Yeah, I hear ya. If there's something I can do to help out and reduce the workload---even if it's a social problem---then please reach out. I may ask questions though to get a more concrete understanding of the process y'all go through. :)
Great, thanks very much!
[Here](https://play.rust-lang.org/?gist=893d503f1c1588f1829310d504913fd4&amp;version=stable&amp;mode=debug&amp;edition=2015) is a much worse solution that avoids `impl trait`! :p
Last I checked `Vector`s in most languages have a static length determined by a type parameter. `Vec`s on the other hand are just dynamic arrays.
If you open SO on mobile it displays not the original author but the last person to edit for each post. Right now the first 4 posts when opening the link are all 'by Shepmaster' :D Try it in responsive design mode, requires a reload or SO won't serve the mobile version.
I'd go further and say that on a semantic level, Rust always moves, but `Copy` types are copied before and the copy is moved ;). Why am I saying so? The copy action is completely local to the caller, the receiver does _not_ know if the value was copied before (it may not even know the value is `Copy`), but it knows that it is moved in.
My personal pet peeve: `umount` instead of `unmount`.
From my limited knowledge of how Rocket works (I can't speak for any other frameworks), when the router is trying to call a handler, it's going to check the type of the input that it received against what is expected, if it receives a negative value and a u64 is expected, it's not going to hit that route and will probably just go to 404. If I'm correct about all of that there'll have to be a runtime check of some sort. 
#You have terminal ligma
Wow. That's surprising. Can you share more details? I understand if you can't though. E.g. what was the input? Which version of grep?
I'm sure what people mean when they say it is that they can resort to passing around raw pointers, in which case you do effectively turn off the borrow checker.
So? I was not and will not be participating in a debate on the quality of the name Vec.
Dynamic dispatch is usually not a problem, and you can absolutely inject some `UserService` into your endpoints this way. When talking about DI in Rust though I tend to link to this [sample project](https://github.com/KodrAus/rust-web-app#dependency-injection) (it also happens to use Rocket). It's structured in a way that makes individual services easy to compose without necessarily knowing their concrete type. As an example of a service that can be mocked, we have a [`GetCustomerWithOrdersQuery`](https://github.com/KodrAus/rust-web-app/blob/master/src/domain/customers/queries/get_customer_with_orders.rs#L33-L69). This service [is resolved](https://github.com/KodrAus/rust-web-app/blob/master/src/app/customers.rs#L13-L14) through a piece of managed Rocket state. It's like a service locator, if you're familiar with that pattern, without some of its global-state problems.
If you're going as far to call this "a major asshole move", can you actually clarify what was bad about it? Surely the fact he's responding to small details in a specific talk is appropriate given he mentions it's that talk that got him thinking about this. As to your specifics about what he misinterpreted: &gt; Catherine never claims the borrow checker solves all the problems, He didn't say she did. &gt; she claims that it drives you to a better solution in her specific examples. Which is what he was responding to (as oconnor663 clarified for you). &gt; She never gives generational indices as something handed to her on the borrow checker platter, Which he never claimed, and in fact which he explicitly stated. Your response comes across as much more of a hostile misinterpretation than his does. He had *primarily* positive things to say about Catherine's talk, and made sure to point them out when appropriate to avoid exactly this kind of misunderstanding. You seem to be giving no such leniency.
I want to come back and read this more closely later, but I've read through a couple functions already and I find that your commenting style really helped me understand the intent of each section of the code. One thought I had while reading this though, and this isn't specific to this crate, but I have noticed that functions/methods written in Rust tend to be longer that what is commonly considered best practice in other languages. Also, individual files tend to get very large, partially because the unit tests and documentation sit inside the same file as the production code. Do you find that this is not a problem? Do you have any tips for working around these challenges? Would it be a bad idea to split the `impl IndexList` across several files, each containing the implementation, documentation, and testing for a single (or small group of related) method?
I haven't used Rocket (I stick to stable Rust) but Rust is no different than any other language when it comes to how you implement URL routing and request parameter processing. First, the routing layer will match the stuff it can efficiently match in order to narrow things down (eg. Python's Django framework defines URL paths as regular expressions). For stuff that requires more complex processing (eg. GET query parameters), you resort to some form of `raw_value.parse&lt;desired_type&gt;()` check. If you can overload it to dispatch differently depending on different types, then you trial-and-error until you find a match or run out of candidates.
One of the (few) places where linked lists have desirable performance is their O(1) concatenation/`append` (for instance, [rayon uses this to make `collect`/`extend` efficient](https://github.com/rayon-rs/rayon/blob/8ba64ab93068004770d002116066aa7d91d8401a/src/iter/collect/mod.rs#L136-L149)). I imagine this encoding doesn't give that?
All I/O on a server is just serializing and deserializing streams of data, so yes, it is done at runtime. Rocket, in particular, looks to see if the input matches the route, and if not, tries the next route in line that matches the criteria, otherwise returns an error page.
So, this holds a tail pointer, so we’re able to append in constant time. The issue is when the vector is full, and needs to reallocate; that’s gonna grow with the number of nodes. So I guess it’s not truly O(1) in every case, though it can be.
Thanks! Splitting test modules out into more files can help. And once the “put docs in a file” feature is stable, that can help too. I mostly just don’t worry about it and fold in my editor if it becomes truly annoying.
The borrow checker never interacts with them in the first place. So it’s not the same thing.
Can you be more specific about the problem you're trying to solve? For the example you wrote, I'd just use `array.join(" ")`
Precisely. Using raw pointers circumvents the borrow checker.
“Circumvents” != “turn off”
I agree that’s how the casual phrasing is meant, but I think the framing is harmful because it gives you an incorrect mental model of what unsafe blocks do.
Another way of looking at it is when a Copy type lvalue is moved out of, a copy is left behind. The only gotcha is that this means Copy lvalues can be moved out of in more situations than other types.
That's on the side of nitpicking about terminology though. Pragmatically, people are going to refer to the technique as turning it off, since that's how they see it in their mind.
I think the distinction matters, because it’s ultimately misleading.
I have no idea what most of those words mean, but does it solve sokoban? More seriously, this project would benefit enormously from documentation and some examples.
UB can't be rescued by any means, including whatever kind of "checking" you have in mind. If it could then it would not be ***undefined*** behavior by definition. The words mean something, they aren't just made up.
"I think that the nature of the video makes it a major asshole move" This is going too far.
Mind if I ask why an array? Your format string clearly expects 2 arguments, so why not pass the 2 elements manually?
`Box&lt;dyn Any&gt;` provides runtime type reflection, basically.
An interesting fact: in Don Knuth's implementation of TeX, he did not use pointers because not all implementations of Pascal supported them. Instead, he uses arrays and indices.
No problem!
Thank you so much for posting this. I thought it was only me feeling this way, but I avoid SO for the very same reason: My question gets edited (usually by the same individual you mentioned) and sometimes even the title changes to something that might technically be more correct, but does not reflect my original thinking anymore (which was the reason I couldn't find an answer the first place). 
Did anyone figure out what was the problem with slotmap that Catherine mentioned during her talk? She mentioned it in passing and without going into details, and I had no idea what she meant.
Neat! I didn't know about that.
Ah, I haven’t implemented that yet, but yes, that’d be O(n) for sure. Great point!
I edited my post to clarify it a bit more.
I edited my post to clarify it a bit more.
Consider these two functions: fn add_one(value: u32) -&gt; Option&lt;u32&gt; { value.checked_add(1) } fn add_one_again(value: String) -&gt; Option&lt;String&gt; { let input = match value.parse::&lt;u32&gt;().ok()?; return input.checked_add(1).map(|value| value.to_string()); } If you try to call the first one with "hello", it will fail at compile time. Same as if you try to call it with None, Vec::new(), or "3". When you call the second one, it will fail the same way with None or Vec::new(), but this time the compiler will accept both "hello" and "3". If you give it "hello", it will fail at runtime instead. The route handler works like that second example. The code is structured something like let route: String = get_request_route(); if let Ok(value) = route.parse::&lt;TypeThatFirstRouteRequires&gt;() { send_response(first_route(value)); } else if let Ok(value) = route.parse::&lt;TypeThatSecondRouteRequires&gt;() { send_response(second_route(value)); } else { send_response(BAD_ROUTE); } The client can send any string at all, but only some strings can be parsed into whatever type each route requires. If one route doesn't work, then it will try another, and if none work it gives an error response.
`println!("{} {}", array[0], array[1])`?
Gotcha, thanks.
Thanks for the very detailed explanation!
I did, he's responding to small details that arise from what is basically a misinterpretation of a couple minor points in the talk. And again, it's a giant "well, actually". I'm really not here to argue this. I've said my piece.
If it's type-based routing, then the server can't use hash tables. There's no hash function that will distinguish "3", "4", "12", and all other strings that make a valid u8 from "-1", "hello", "300", and "iAmAHacker". At least not one that is faster than just trying to parse it. If it's getting to be an issue because you have a bunch of different strings for routes (like, for example, reddit.com/u/{username}), then what you'd probably do is tell your generic router to just match any string after the /u/, and then in your own code that the router hands off to you can put your hash table in.
Intent is a complicated thing, and it doesn't have to be there for this move to be bad.
Looking at how the common code which handles the ones of those included in the standard library is implemented (`format_args!` and `fmt::Arguments`), I don't think that's what you're looking for. It's very heavily focused on the task of ensuring that a heterogeneous list of inputs is valid at compile time and things inspired by it seem to follow that same focus. Instead, I'd look for a simple [templating engine](https://crates.io/keywords/template) that operates at runtime. For example, a [Rust implementation](https://crates.io/crates/mustache) of [Mustache](https://mustache.github.io/mustache.5.html) templates.
So then why are you saying you can’t do something that for all practical purposes you actually can do?
But you can use raw pointers and the other unsafe superpowers to undermine code that the borrow checker does check. I.e. you can use unsafe blocks to effectively lie to the borrow checker and cause it to accept code it shouldn’t. How is it unreasonable to call that ‘turning it off’?
&gt; For example, you can't memory map stdin If stdin is a pipe (like from `cat` or whatever) then you can't, but if stdin is in fact a file (like with `&lt;` redirection, when the target isn't a fifo or anything weird) you can memory map stdin. The `os_pipe` crate will let you get your hands on the `File` you need to make it happen.
I'm really glad to see somebody taking at least named arguments seriously. They're probably the last thing I miss coming from Python.
Thanks!
&gt; she claims that it drives you to a better solution *in her specific examples*. I think Jon mention this stated that. Also points out that at least for game development, this "better solution" is already known in the game programming community across other languages therefore he states that the borrow checker would not provide additional benefit at least for a game programmer &amp;#x200B; &gt; Jonathan comes off as someone *looking* for things to nitpick. He also made note of this at 50 minutes because he and I quote (from earlier posts): If you put the programmer through pain... daily friction in order to solve this problem that doesn't exist, that is a negative. It's a net cost. That may be balanced by net gains elsewhere; for example, maybe the borrow checker finds some other bugs that would be more costly to find. Or maybe there are performance benefits. Then maybe those can balance it out to be a net gain. I don't know; I don't know enough to firmly bet a lot of money in one direction or the other. I'm not convinced in net productivity \[..\] If you're gonna say: 'my metric is not how much it costs or how long it takes to make it, my metric is the overall quality of the final program, how bug-free is the final programming,' *then* I'm willing to bet that the borrow checker is a net gain, probably. \[..\] **I just want to decrease friction, that's where I come from on these matter.** &amp;#x200B; At 39 minutes: Of course I'm designing a language that is different from rust. I disagree with the overall goals of rust being the right thing for **game development which is why this is on my mind**. \[..\] Even though I don't want to program using a borrow checker \[..\] I still want to know that if a good language design for games does not have a borrow checker. **If I'm going to be a good engineer, I have to know the cost-benefit analysis of I should be vaguely aware that I'm giving that thing up.** &amp;#x200B; So essentially he was just "researching" to determine the cost-benefit. 
I think if we're calling indexes an anti pattern, we might as well call unsafe code an anti pattern. Which, well, yeah, it kind of is. I certainly wouldn't use unsafe code when safe code would do the job. And likewise I wouldn't use a Vec of objects when regular references would suffice. But people reach for these things precisely because they're in a situation where the regular pattern doesn't work. I don't think anti pattern is quite the right term here. Maybe "not the first pattern I'd try" or something.
I'm not talking about his treatment of the language or community. I'm talking about his treatment of a specific person's talk.
I'm assuming the goal is to support a variable number of arguments at runtime.
In that case, he'll have to do it manually.
Could you show us your code for the Ctrl+C gracious shutdown?
...or, as I suggested, use a simple templating engine.
To be honest, a websocket server would be enough, and responsive (to health changes without reloading) to boot! Combine that with Service Workers, and you have an almost full-offline mostly-client-based status page, which updates when it's back on internet!
`use Vec as ArrayList;`
With opt-level=2, it actually is the same as mem::transmute, it's just that the instructions are in a different order.
Here is the corresponding [issue](https://github.com/rustsim/nalgebra.org/issues/11)
&gt; `{ "method": "log", "params": { "format": "one {} two {}", "params": ["hello", "world"] } }` **"format": "one {} two {}"** You can't do this with `format!()` as it only allows constant format strings, (not even, I'm pretty sure it specifically requires a string *literal*.) You have no option but to do this at runtime.
Steve's whole point here is that it shouldn't be thought of that way in the first place.
You have to pervert code to pull that off, replacing borrows by pointers. That would be *subverting* the borrow checker. It's still on, and still present.
Could you detail the memory representation of your clauses? Is the content inlined with the header like in minisat? What are your heuristics for learned clause deletion and restarts? Do you use pre-processing? Anyway, congrats on having a working CDCL solver! Writing those things is not that easy!
Yeah, I have no problems reading camelCase, but with snake_case I sometimes parse the underscore-connected words as arguments (OCaml is my primary language, where snake_case is also the norm, and where arguments are curried without enveloping parens). Those underscore gaps are too spacious. I've tried fixing this with font changes, which can help but has it's own difficulties (like becoming dependent upon that font without always having it available).
Whether or not memory safety is important on the client side, it seems rather obvious that it's important on the server, given that servers also handle lots of potentially personal information about users.
&gt;This video is one giant "well, actually" and I'm disappointed that the community isn't recognizing it as such. They are. They just don't think that a "well actually" is a bad thing, because they're not SJWs.
Clap is using the [builder pattern](https://doc.rust-lang.org/1.0.0/style/ownership/builders.html), in this specific case, a [consuming builder](https://doc.rust-lang.org/1.0.0/style/ownership/builders.html#consuming-builders:)
I think Qt implements weak pointers like that. Or did at some point.
To be clear, the http protocol is text based and untyped, so everything you send to the server is a string. You simply can’t send a number, negative or not. This string can obviously be parsed to anything in Rust, but you would use a dedicated parser for each type, and it should return an error if there is a minus sign where you expect a digit. 
RCE's on the client side are extremely important, there's still players running games with admin rights or your _just_ need to combine the bug with a local exploit and "nice, we got a botnet". Given that many games have public lists of IPs, it's a really nice way to skim info or place your favourite bitcoin miner. Blackhat had a nice overview of the amount of juicy attack vectors (it's a little dated, e.g. most games don't require admin rights anymore). https://media.blackhat.com/eu-13/briefings/Ferrante/bh-eu-13-multiplayer-online-games-ferrante-slides.pdf
It's very similar to [Generational Arena](https://github.com/fitzgen/generational-arena) published earlier this week. Any chance of reducing code duplication?
Bypassing the borrow checker and turning it off are not the same thing.
Thank you so much!
&gt;I doubt that many people these days would even have seen the actual assistant Yes, certainly there are nobody who can remember back to the pre-historic year of 2006.
You forgot to post the URL of your repository.
That one pisses me off _so_ much, its just so fucking unnecessary. 
Thank you for pointing this out. I've fixed the case of nalgebra.org, and updated the URLs to insecure contents on nphysics.org and ncollide.org.
Still too simplistic, though. It also solves it via `resize()`, which is completely unlike a stack.
&gt; First Debian, now Ubuntu... I understand your enthusiasm, but, the packages in Ubuntu *are* the exact same Debian packages. They were previously imported as source from Debian. So it's not surprising they show up in Ubuntu.
&gt; Just needed ripgrep on a 2GB file, after grep returned "memory exhausted" I've been using *grep* for 20 years now and never ran into memory problems. However, I also didn't use it on Windows.
Yeah, weirdly it wasn't displaying when I searched for 'rust' in the app. :( Cheers.
Ok, fine. Someone who says “turn off the borrow checker” when they mean “subvert the borrow checker” is guilty of insufficient semantic rigor. Is that really all the post is trying to accomplish, correcting a lack of semantic rigor when colloquially discussing unsafe? Given that it doesn’t mention the possibility of subversion or try to draw the distinction, I don’t get that impression.
Maybe you consider it nitpicking, I consiser it it lowering the chance that someone will genuinely believe there is an off switch for the borrow checker and will waste time looking for it. Communicating is hard, being picky about words can help a lot in certain cases. Even if you don't appear to gain anything from it.
Ok, fine, please allow me to amend my complaint to say that I think the post should explicitly mention that it’s possible to subvert the borrow checker (accidentally or on purpose) even if you can’t turn it off. I think it’s important for the intended audience to not walk away with the impression that the borrow checker can’t be subverted via unsafe - i.e. the semantic distinction this thread is drawing between ‘turning off’ and ‘subverting’ needs to be present in the post.
If you run a game as root you're deserve what you get. 
Interesting, could you try grep with -c?
To add elements to the end of an Hlist, you want to use [the `Add` impl of `HCons`](https://docs.rs/frunk_core/0.2.0/frunk_core/hlist/struct.HCons.html#impl-Add%3CRHS%3E). You can then use its `Output` type to get the type of the new Hlist. This works: use std::ops::Add; pub fn append&lt;U&gt;(self, elem: Holder&lt;U&gt;) -&gt; HolderOfHolders&lt;&lt;T as Add&lt;Holder&lt;U&gt;&gt;&gt;::Output&gt; where T: Add&lt;Holder&lt;U&gt;&gt;, T::Output: HList, { HolderOfHolders { holders: self.holders + elem, } } Buuut it's really not very nice. I worked with frunk's hlists, too, and it results in really a lot of strange bounds and line noise. For example `T::Output: HList` seems fairly unnecessary since every `Output` of the addition returns a hlist. But the compiler isn't smart enough. On the other hand, I think `frunk` can also be improved. For example, the `T: Hlist` bound should imply `Add&lt;T&gt;` for any `T`. Ah, that's probably the problem: we can't say `trait HList: for&lt;T&gt; Add&lt;T&gt;` yet. Sigh.
Undermining something by ignoring it's rules is not turning it off and thats not what words mean! You're really really *really* stretching it here. Lying to the borrow checker is not under any reasonable definition "turning something off". By that logic you might as well lie to the police to turn the law off. After all, you can use deception to effectively lie to the police and cause them to accept actions they shouldn't.
I acked the semantic point and amended my complaint in other replies on the thread.
He uses this phrase ("turning off the borrow checker") as a "mental shortcut" - what he really means is "bypassing the borrow checker" or something similar. Which IS possible, as it turns out ;)
But inthe example i was providing at the time, I obtained the *mut from a &amp;mut is this ub?
didn't crash this time, but both: grep -c -o -i "&lt;Record&gt;" WeeklyExtract.xml | wc -l grep -c -o -i "&lt;Record&gt;" WeeklyExtract.xml returned "1"
O_CREAT
Interesting concept, struct of arrays.
It becomes a bit more clear if you change the edition to 2018, do you get NLL (the solution to many lifetime problems) and better error messages (presumably). The problem is that you're borrowing push node immutably, then capturing that reference. That reference has to live long enough for any use of that closure. Then, you borrow mutably, but push node was already borrowed immutably. The way I'd solve this problem would be to add a parameter to the closure, a reference to push node. Essentially, a similar signature to any other method push node implements. I find that it's really natural to try and capture everything when you're writing closures, but often you just need to pass them something instead. An alternate solution would be to use Rcs and Refcells, but this is typically a bad idea unless you truly have no other option.
The first should return 1 regardless, the second is a bit of a mystery. Maybe try to lose the -o, though I don't think it should really do anything in this case
Such a set would have over 1.1e602 elements. What *are* pou trying to do that you think you need such a set? Because you can't compute that in the lifetime of the universe.
I have an assignment in high performance computing course where I gotta parallelize the generation of primes. And then calculate the sum of all the subsets in the generated list of primes. Both to be done parallelly. So if I generate 2000 primes, I gotta calculate the sum of all those 1.1e602 subsets. I know its crazy, but that's the situation.
Modern computers are unbeliveably fast. I would imagine that in a typical web app, request routing is not the bottleneck as long as you are not doing something really weird. Many apps do fine using dynamic languages, and those will be glacially slow compared to decently optimized rust. I do not know the details of rocket's routing in particular, but likely you'd need thousands or tens of thousands of routes before it even becomes noticeable compared to time spent with IO
Just for posterity: Compiler Explorer shows the slice stuff to be *really* fast. .LCPI2_0: .byte 1 .byte 2 .byte 3 .byte 4 .byte 5 .byte 6 .byte 7 .byte 8 .byte 9 .byte 10 .byte 11 .byte 12 .byte 13 .byte 14 .byte 15 .byte 16 myFuncSlice: movaps xmm0, xmmword ptr [rip + .LCPI2_0] movups xmmword ptr [rdi], xmm0 mov dword ptr [rdi + 16], 336794129 mov al, 1 ret This is basically an unrolled `memcpy()` (16 + 4 bytes). [Full Example](https://rust.godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(j:1,lang:rust,source:'%23%5Bno_mangle%5D%0Apub+extern+%22C%22+fn+myFunc(mut+ptr:+*mut+u8)+-%3E+u8+%7B%0A++++let+x:+%5Bu8%3B+20%5D+%3D+%5B%0A++++++++1,+2,+3,+4,+5,+6,+7,+8,+9,+10,+11,+12,+13,+14,+15,+16,+17,+18,+19,+20,%0A++++%5D%3B%0A++++for+n+in+x.iter()+%7B%0A++++++++println!!(%22%3E%3E+%7B%7D%22,+n)%3B%0A%0A++++++++unsafe+%7B%0A++++++++++++*ptr+%3D+*n%3B%0A++++++++++++ptr+%3D+ptr.offset(1)%3B%0A++++++++%7D%0A++++%7D%0A++++return+1%3B%0A%7D%0A%0A%23%5Bno_mangle%5D%0Apub+extern+%22C%22+fn+myFuncSlice(mut+ptr:+*mut+u8)+-%3E+u8+%7B%0A++++use+std::slice%3B%0A%0A++++let+x:+%5Bu8%3B+20%5D+%3D+%5B%0A++++++++1,+2,+3,+4,+5,+6,+7,+8,+9,+10,+11,+12,+13,+14,+15,+16,+17,+18,+19,+20,%0A++++%5D%3B%0A++++let+slice_tmp+%3D+unsafe+%7B+slice::from_raw_parts_mut(ptr,+20)+%7D%3B%0A++++slice_tmp.copy_from_slice(%26x)%3B%0A++++return+1%3B%0A%7D'),l:'5',n:'0',o:'Rust+source+%231',t:'0')),k:50,l:'4',n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:r1280,filters:(b:'0',binary:'1',commentOnly:'0',demangle:'0',directives:'0',execute:'1',intel:'0',trim:'1'),lang:rust,libs:!(),options:'-O',source:1),l:'5',n:'0',o:'rustc+1.28.0+(Editor+%231,+Compiler+%231)+Rust',t:'0')),k:50,l:'4',n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4)
"It's almost as though your homework implied an algorithm for computing the sum without generating all the subsets, using something like dynamic programming." -your Professor, probably
Thanks a lot. I tried changing feed so it takes `&amp;mut self`, but then that still causes 2 mutable borrows to occur simultaneously. Your solution worked like a charm, will now try to figure out if I can create this nice API for the wrapper.
Wait, what? Why didn't I think of that?
Probably both. But learn rust if you're doing your own projects, learn c++ to get hired by someone else.
Dangling indexes are the same kind of bugs as dangling pointers. You're just as likely to corrupt the game state in Rust with indexes into a Vec than you would in C++ with pointers into a vector. Sure, Rust gives us memory safety, but the application state would be just as broken.
Having experienced how some other language went through this, I love your enthusiasm, but I feel this proposal does _way_ too much, and the text lacks good, basic examples. It feels like it goes on a very wild tangent early on. Maybe everyone knows earlier precedents, but to me it feels that a lot of context is missing there. Not meant to put you down, but maybe some useful hints to make the proposal more readable to a wider audience.
Thanks :) Varisat inlines the clause header with the clause literals and keeps all clauses in a consecutive chunk of memory, so that 32bit indices can be used to reference clauses even on 64-bit systems. This is implemented in [varisat/src/clause/alloc.rs](https://gitlab.com/jix/varisat/blob/master/varisat/src/clause/alloc.rs). I'm using luby series restarts for now, but will replace it with a more adaptive strategy. For clause deletion I'm using Chanseok Oh's three tiered strategy. As far as I can tell it's not more complicated but much more effective than other strategies. There is no pre- or inprocessing yet, but I plan to add that too.
Which rule do you claim I am breaking? I am merely pointing out that on the mobile view, SO contributes questions that have been edited to the most recent editor, which means that on mobile view, it *looks* like Shepmaster is single-handedly responsible for most of the questions there. If you agree with me that this is kind of ridiculous, then perhaps you should petition SO to fix their mobile view.
Also, fd already means **file descriptor**.
It is possible to get memory exhaustion errors, but they typically occur in both grep and ripgrep in the same way. For example, if you have a modest amount of memory and do something like `grep -a /dev/sdaX` in a big drive, then all you need is a contiguous stretch of bytes without a `\n` to exhaust available memory. (This is where the `--null-data` flag proves its usefulness.)
I’d need to see that example.
Relevant discussion: [Is Rust more approachable than C and C++, or do writers just make it seem that way?](https://www.reddit.com/r/rust/comments/9884j3/is_rust_more_approachable_than_c_and_c_or_do/)
the first one doesn’t make any sense, it’ll always return 1
Borrow does not compose like that. That would require a blanket impl, but that would collide with other impls. You could make your own wrapper around `Arc&lt;Thing&gt;` and impl `Borrow&lt;u32&gt;` for that.
I spent 3 hours because I made a typo in the actual implementation, and wanted to ask what this did exactly. My function's signiture was: pub fn set_low_callback&lt;F&gt;(&amp;'node mut self, callback: F) -&gt; Result&lt;()&gt; where F: 'node + FnMut(&amp;mut PushNode) { That playground example wouldn't compile with that extra 'node lifetime specifier to self so I was getting really difficult to understand errors, and everything works magically after removing it. More generally, I'm having a lot of difficulty understanding lifetimes beyond toy examples. Any suggestions on how to understand the concept, or learning resources? Thanks a lot for your help - everything works now :)
Perhaps make a type called "Power-Set" that takes in a "Set". Then, implement the methods like "Contains" on the Power-Set to check if both elements of element you are asking if the Power-Set contains are in the original set. Zero-Copy. Zero additional memory usage (can take a reference to the original set). No need to compute anything.
But if you name the commands based on what they actually do you end up with Powershell.
Yeah, I had tweeted about this while I worked on it; fitzgen’s crate was useful to compare against! And I got the free list idea from his crate. They’re fundamentally different structures though; the only real duplication is Index. That might be too small for a shared crate.
Ugh, I hate this "pass-by-value" vs "pass-by-reference" distinction, it derives from so many misconceptions. To begin with, it is not even a property of the language, it's simply an optional feature that some languages support when defining a function: VB with its `ByRef`keyword, Pascal with `var`, C# with `ref` and C++ with reference parameters. Java, Javascript, C, Rust and most other languages do not support any form of pass by reference, and I'm not aware of any languages that *always* pass by reference. Any language with any form of indirection (pointers, references, objects, arrays, whatever) or in other words, every useful language aside from maybe Haskell, allows defining functions with side effects which are visible to the caller *because* of the combination of indirection and interior mutability, and there will be people who call that "pass-by-reference".
&gt; Sure, Rust gives us memory safety, but the application state would be just as broken. That's the point; it eliminates one entire category of bugs. Nobody is claiming it eliminates all bugs.
I don't think the CoC is meant as a way to avoid criticism for what other's believe is bad behavior (not taking a side here on whether it is or isn't, just pointing out that it is disingenuous to invoke the CoC as a way to shutdown what appears to be legitimate critique).
That's a cheap cop-out. Explain that to people that have 0 of our knowledge and the support document states: "please click this button". Especially after the main platform has basically recommended running all admin for years. We have fucked up as industry, badly, at the cost of users. We should own that. 
Catherine in the RustConf talk mentions that she plans on publishing a blog post soon-- does anyone know where I can find it?
Learn C first. Then maybe C++ basics (staring with RAII). Then after getting hired you can learn required C++ features.
Ah, excuse me, you speak for yourself. 'We' have not fucked up in the slightest. You may have, or maybe you like taking responsibility for the failures of Microsoft and lazy game developers while not actually having any responsibility yourself. But I've seen a lot more software saying 'never run this as root!' than I have software saying 'please run this as root trust me :)'. 
"Legitimate critique" is what was expressed in [this comment tree](https://www.reddit.com/r/rust/comments/9fqzv9/we_have_reached_10k_rust_questions_on/e5yvj9f/) to which I replied in an objective fashion. The two comments at the root of this comment tree are not constructive critique as I understand it. If my mention of the CoC seems to harsch to you, I'm sorry. As I said, I just wanted to point out that repeated negative comments (the ones without constructive critique) can make persons sad or make them feel like their contribution is unwelcome. And we don't want to evoke those feelings, right? 
I believe \[this\]([https://github.com/jlao/cuckoo-hash-map-rs](https://github.com/jlao/cuckoo-hash-map-rs)) is the repo OP is referring to.
It just went up [here](https://kyren.github.io/2018/09/14/rustconf-talk.html).
Hey, thanks, your answer is exactly what I needed! &amp;#x200B; Now I'm going to try to figure out why it works.
&gt; And we don't want to evoke those feelings, right? I depends on the reason. I don't agree that everyone has a responsibility to make everyone else "feel good" no matter the issue. That's just not a legitimate position in reality. You seem to be wanting to use the CoC as a weapon rather than as a shield. I don't find that appropriate.
I wouldn't rule out the chance to get hired to use Rust. If you're passionate about it, then you may very well end up in a company that's also passionate about it.
Pardon my drive-by suggestion: Yeah, this sounds like you might want to consider finding an analytical algorithm instead of an iterative one for this. I definitely don't know any particulars, though, so take me with a grain of salt!
Wow, this is really meaty followup post!
I keep seeing this Generational Indices idea in the Rust community and I think its very cool! Seems to work better in idiomatic Rust than the traditional way of doing linked lists.
Thank you. This is one of those things like strong/static/weak/dynamic typing. People get into all sorts of semantic arguments about them when nobody actually agrees on precise definitions anyway.
I started learning Rust before I had a good understanding of RAII or the C/C++ memory model, and that gave me a lot of headaches (e.g. in understanding lifetimes and the difference between `&amp;str` and `String`). I would recommend learning at least C++ structs and classes, pointers (including shared and unique pointers), and pass-by-{reference, value}. Other than that, I don't know very much C++ at all, and I feel pretty productive with Rust. Specifically, the best way to learn a language is to write software in it, and I find it a lot easier to do so in Rust. It's easy to get help from the friendly community, and the ergonomics of Cargo cannot really be overstated. (Updating your `Cargo.toml` is immensely more straightforward than figuring out CMake or trying to fix linker errors from gcc.) So I'd say it's worth it to learn the basics of C++, but don't worry about topics like templates and/or in-depth object orientation.
It's because reqwest depends on an old version of hyper-tls which depends on and old version of the openssl crate that doesn't support openssl 1.1.1.
OK, so I had to make a couple of adjustments in the code to make everything work, so I'm putting it here if someone else reads this post. Also, as I wrote in the original post, I'm far from expert on this, so my solutions might not be the best ones. * For the first function(`HolderOfHolders::new`), the one I said I didn't have any problems with writing. If I put it into the `impl&lt;T&gt; HolderOfHolders&lt;T&gt;`, that seems to confuse the compiler and make it ask me to annotate the type when calling this function. For example, it doesn't allow me to do something like this: let x = Holder { a: 5 }; let l = HolderOfHolders::new(x); It seems that the compiler doesn't know to what to exactly the T should be inferred here. The workaround for this is to put the `::new()` into a separate impl: `impl HolderOfHolders&lt;HNil&gt;`. &amp;#x200B; * One other change I had to make to the function `append` that u/[DebuggingPanda](https://www.reddit.com/user/DebuggingPanda) wrote here (maybe I misunderstood something about his version of the function) was to change the return type to the following: &amp;#8203; -&gt; HolderOfHolders&lt;&lt;T as Add&lt;HCons&lt;Holder&lt;U&gt;, HNil&gt;&gt;&gt;::Output&gt; where T: Add&lt;HCons&lt;Holder&lt;U&gt;, HNil&gt;&gt;, T::Output: HList, and then also in the body of that function: `holders: self.holders + hlist![elem]` This is because the original version of the function would receive `Holder&lt;T&gt;` and would be constrained to work only when it could `Add` that `Holder&lt;T&gt;` to `self` (which is a `HList`). As far as I can see, that addition is not defined, so this function actually has impossible constraints (although compiler doesn't realize this), making the definition of the function itself valid, but it will never be actually defined for any `self`/`Holder` combo it receives in arguments. Instead, `Holder&lt;T&gt;` is now wrapped inside `HCons`, and adding `HCons` to the `HList` is defined. &amp;#x200B; &amp;#x200B;
I edited my original post to add my code
No. It's 2018, there is absolutely no need to learn C first. Or even worse, C first and then C-but-with-classes as lots of colleges still do.
Why would you want to do that?
py-spy seems to be inspired by https://github.com/rbspy/rbspy I have no clue how this works, so would it make sense to have something like this for code written in Rust?
I had heard talk of ECS before and wanted to learn more to see how they can apply outside of games but this keynote is what really made them jell for me. I'm now seeing potential in both the liquid template engine and cobalt static site generator for using ECSs Where an ECS can help me - Better organizing data and functionality. In cobalt, I've been struggling with the architecture, not feeling like there is a good OOP solution. I was thinking at one point that an in-memory database could be a nice approach except for the serialization / deserialization overhead. I feel like an ECS gives me those benefits - More flexible code base. In Liquid, the state (context) sometimes needs to carry plugin-specific information. The flexible ECS that was shown would allow for this. 
Would it be possible to implement a derive with this kind of syntax: #[derive(Table, Clone, Debug)] #[table(kind="consistent", domain=EXAMPLE)] struct Ship { #[storage(VecCol)] name: String, #[storage(VecCol)] cargo: u8, }
See the thread /u/jkleo1 linked. In this case, it's entity reference coordination problem in game programming.
Pretty sure there's an interview with Dennis Ritchie(?) where, upon being asked if he would go back in time and fix anything in Unix, his response was "I would spell CREATE correctly". :-p
&gt; And this is a good thing. For fuck sake, don't try to hold my hand, and let me decide if it is a good thing or not. I'm seeing this shot all over the internet now, and find it stupid. 
I've been refreshing for days waiting for this to be posted!
Thank you! When I watched your keynote my first thought was "wow I really wish I could have watched this ten years ago". I highly recommend it to anyone else getting ninto game dev!
Got it. Thanks!
Ok, thanks for the feedback.
Hi all. A quick question: is there a way to have a pattern guard in `if let`, like it's possible in `match` arms? It can be convenient sometimes, but I'm not sure if the syntax allows it.
Why do you think she writes this sentence for you? Others may need the hand holding, or interpret the sentence as something else (her opinion, maybe?). So unless you are sure that Mrs. West wrote this sentence to offend you, keep it cool.
How you came to conclusion that I think it's for me? It's for all readers. Readers should come up with their own conclusion. She should have said "*I think* this is a good thing", and not try to persuade readers to think that's a good thing without thinking for themselves. 
If you read the top of the post this was written for a talk and was not meant to be seen by anyone else. Cut the author some slack for something so minor. 
I think it's rather obvious she's only stating her opinion. It would be quite redundant to prefix everything one says by "I think".
Doh! Yes, that is the one. I've edited the post to include a link.
Or you can add this piece of magic to the dependencies section of your Cargo.toml for the final binary project: openssl = {version = "0.10", features = ["vendored"]} There's a note about also using openssl-probe you should probably read: https://crates.io/crates/openssl
It's not entirely correct that every input must have something queued before aggregate() is called. There are three exceptions, all of which can be relevant for what you're asking for here. 1) In live mode (e.g. when capturing from a camera or something else that produces data in real-time and can't simply be paused/stalled without dropping data), aggregate() is called when "it's time". That means that whenever data is "too late", you simply wouldn't have data for those inputs at that time. In live mode in GStreamer, scheduling is generally kind of deadline-based. 2) For audio (and video and other non-sparse streams), the assumption is that data is contiguous. If there are gaps, they would be signalled with the GAP event. GAP events currently cause an empty buffer to be queued up on the aggregator input, and then aggregate() is called with that empty buffer 3) Sparse streams (metadata, subtitles, ...) need special handling that is currently not implemented yet but there are plans, see https://bugzilla.gnome.org/show_bug.cgi?id=791202 . That's currently a blocker for various muxers that would need support for sparse streams, so it's likely going to be solved soonish. Feel free to ask more questions here, on the GStreamer mailing list or in any other way if you have further questions, especially with the implementation of an actual muxer.
Well, I don't think it would be helpful. The macro generates a module with items that have a uniform naming scheme, there's not really a `struct Ship` anywhere. I do prefer not having the indentation that using a macro implies, and using a more native syntax tho.
There was a great talk about exactly this: [Stop Teaching C](https://www.youtube.com/watch?v=YnWhqhNdYyk)
&gt; ls could be longer Shorter! :P alias l="tree --dirsfirst -ChFL 1" alias l2="tree --dirsfirst -ChFL 2" … I don't understand how people tolerate typing more than one character for directory listing…
Well let's put it like this: if everyone named their command 2 letters it would quickly consume the entire namespace and collisions would happen very quickly.
Is it currently planned to support this syntax? struct Foo { x: impl std::fmt::Debug } or will it always be necessary to write struct Foo&lt;T: std::fmt::Debug&gt; { x: T }
I had the same problem today, and solved it by linking against OpenSSL 1.0, as suggested in https://bbs.archlinux.org/viewtopic.php?id=225658 . In this case, the command I ran was `cargo clean` and then `PKG_CONFIG_PATH=/usr/lib/openssl-1.0/pkgconfig cargo build`. 
Learn both. They will show insights into the others strengths and weaknesses. 
Hi everyone, I'm working on porting some logic from C to Rust and wanted to make sure I'm on the right track. My C program memory maps a sorted, fixed-length file so I can quickly search if it contains a given ID. I go about it like so: p-&gt;row = (myStruct*)mmap(NULL, p-&gt;size, PROT_READ, MAP_SHARED, p-&gt;fd, 0); This lets me treat the memory as if it were an array of `myStruct` objects, which is convenient for the binary search. I was able to replicate the functionality in rust by accessing the data as `&amp;[u8]` and manually calculating the offset into the data like so: fn contains(&amp;mut self, input: &amp;[u8]) -&gt; bool { if self.len == 0 { return false; } let mut min = 0i64; let mut max = (self.len - 1) as i64; let data = self.map.as_ref(); let inlen = input.len(); while min &lt;= max { let mid = ((max + min) / 2) as usize; let pos = (mid * 16) as usize; match &amp;input[..inlen].cmp(&amp;data[pos..pos+inlen]) { Ordering::Less =&gt; max = mid as i64 - 1, Ordering::Greater =&gt; min = mid as i64 + 1, Ordering::Equal =&gt; return true, } } false } This works and is very fast, but my question is whether there is a "rusty" way to treat this memory as an array of size `myStruct` like I can in C. It seems like `chunks`, `exact_chunks`, and `binary_search` might get me what I want but I don't understand how I can get random access into them. &amp;nbsp; Thanks! 
I sent you a message about planning mentorship, were you able to receive it? I understand you have a child so totally asking for the smallest amount of time possible.
I think I got distracted and forgot to reply. Sure, please count me in and I will do what I can to help. 
I really loved the talk and the SQL metaphor in the blog post was another "aha moment" for me. As a web developer, I don't know how much this maps, but to optimize things from SQL, the next step is often using a cache layer like redis which has different small primitives. All the "writes" are sent to both layers and as many reads as possible are only done on the cache. So it's like making manual customized index/query-strategies. Often, it feels like the custom strategies are tantalizingly structured in away that given X queries that need to be fast, and Y tables, the denormalized indexing/etc could be deterministically figured out. I'm sure there's research about this but I've never seen a "layperson/webdev" description or technology of how to do this. It seems like if it happened, gamedev would be the place it would be "discovered"...
… and I want Vec for my giant text trigram weight vectors. :)
Wrap the pool in an Rc&lt;Option&lt;T&gt;&gt;. Then you can use Option's `take` method to drop the pool. Once the pool is dropped, I think the rest shouldn't matter. (Rc because signal handlers don't run in a separate thread, they stop execution of the current code, and go into the signal handler.) I'm not sure what is safe to do in a signal handler.
Sure, if you provide the vector operations and use it with appropriate types I'm certain Blow would by happy if it was newtype'd to `Vec`. But the default type has none of these operations, so it's a little like if `Box` was called `Scalar`. Sure, that's one way to store a scalar value, but that's not what any of the interface `Box` provides is *for*.
Trying to learn C++ or trying to teach C++ while pretending the C part isn't important is a __huge__ mistake. Omniviral is right, learn C first. But really, learn C, C++ and Rust. 
The length is not the issue, the generic nature is. Why does "imagemagick" ship a "convert" command and postgres as "createuser" command?
Having to preface everything with *I think* if it's a conclusion I come to instead a cold, hard fact would be terrible. I use *I think* when I am uncertain about about my conclusion. If you really insist on the phrase "I think" being so important, then *I think* you should have said the following: *I think* she should have said "I think this is a good thing", and not try to persuade readers to think that's a good thing without thinking for themselves. -------- I don't think she was saying she knows better than the world - she was just sharing a conclusion from her experience. 
Well that's the second part of my post eh.
The reason the "cache layer" doesn't really apply here is that the reasons to have a cache over an SQL server don't apply here. Why does someone use a memcached/whatever in front of an SQL server? Well, there are a few use cases: 1) Cache servers let you scale the *handling of network-received requests* over arbitrarily many CPUs. The crucial part here is that the CPU cost of processing even a `SELECT 1` request is significant, and the cache layer amortizes that cost over many servers. Every layer of the TCP/IP stack, encryption/decryption, syscall boundaries (even more thanks to meltdown et al) and request parsing all require CPU cycles. 2) You can effectively extend the RAM of the SQL database server by keeping more data in memory on the cache servers, reducing persistent storage pressure, reducing latency and making response times more consistent. 3) You can cache denormalized results from a normalized database—in other words, the results of possibly expensive JOIN queries, especially ones that contain GROUP BY clauses and aggregate functions. Now, the transparent CPU cache already does 1) and 2) for you. And ultimately, you have a hard limitation in the topology of your CPU, and any external extension of that would be five to six orders of magnitude slower (going from nanoseconds to hundreds of microseconds or several milliseconds for latency.) As for 3), The core performance considerations here are that you are able to maximize your utilization of CPU pipelining, cache locality and prefetching, and memory bandwidth. The cache system is already there but it requires care to make the most of it. A streaming inlined UPDATE query on an in-memory database is an example that could achieve this: it would effectively just iterate over one or more arrays that are linearly laid out in memory, and if multiple such UPDATE queries are prevented from accessing the same arrays (i.e. table columns) at the same time, they can operate independently on each CPU core's cache without disturbing each other, which allows you to use multicore CPUs to their full extent by processing multiple systems in parallel. This kind of "zipper-like" processing is well-supported by today's predictive CPU caches, which is what the struct-of-arrays approach in data-oriented design is all about. It's also the primary type of JOIN-like query that you would be running every frame as your systems update your components—more discriminating `SELECT` queries would actually be a comparative rarity, though they would show up quite a bit in AI processing specifically. This is because of the nature of games as real-time simulations where the entire world is updated every frame. The only use-case that is really not supported out of the box here is the aggregation query (GROUP BY), which could benefit from an automatically generated cache layer in some cases, but to be honest I'm not convinced that it would be necessary, and invalidating might be more expensive than just recomputing values in virtually all cases.
I would recommend learning Rust for the same reason. Rust will give you lots of headaches trying to get it to compile, and that's the fastest way to learn good practices. I would have been terrified writing C++ before I learnt Rust. Now I feel like I have some idea of what's safe and not safe to do 
While I was watching I kept thinking “Entity sounds like data model entity I used in database class” (of course there are differences) and was actually thinking “How about just using an in-memory database to manage all data? How much overhead can there be?” 
What you see as insufficient semantic rigor, another person could experience as irritation that the compiler isn't doing what they thought it should be doing. It also lets lazy non-rustaceans believe things like "unsafe is rust's way of acknowledging that it puts too much burden on the programmer" (or something to that effect). From the post: &gt;Why does this matter? Well, a lot of people think that as soon as you drop into unsafe, you’re missing the point of Rust, and that you lose all of its guarantees.
The keynote was great, helped me understand some of the stuff I was wrestling with better. Glad there's a blog post too! 
Would be nice if we had something like Haskell's Coercible constraints to make this safe. 
Thanks for the feedback. I do appreciate it. I'm writing a second draft, and you comment does influence it. I was already working on making the intro more concise (talking about named &amp; default args the happy path of builders, e.g. no validation or data transformation). Judging from your comment though, I need to revisit how to treat the historical context.
I'm trying to statically link grpcio, using rust-musl-builder: [https://github.com/emk/rust-musl-builder/](https://github.com/emk/rust-musl-builder/) &amp;#x200B; I haven't been able to get it working though. If anyone is able to help, [https://github.com/emk/rust-musl-builder/issues/53](https://github.com/emk/rust-musl-builder/issues/53)
Scala. Generally speaking it feels like adding this to Rust will be an uphill battle because of all the cruft the language has accumulated already. I'd try focusing on a minimal proposal, to not get bogged down on the more elaborate parts. If there are parts which make a lot of sense on top of your "barebone" proposal, I think it will become easily apparent to people after they used the barebone one. You would basically gain allies which would help you push things, instead of having this large proposal and having to explain it solely from your own experience.
An additional note is to mark the Rust version of `MyStruct` with `#[repr(C)]` for guaranteed-compatible alignment and field ordering.
Shame that Rust doesn't have have a `Get` trait for the `get` method. If it had, you could use it instead of having separate implementations for `HashMap` and `Vec`...
Given my experience with Rust on Stack Overflow... How many of those are valid for post-1.0 Rust?
Being over-terrified of unsafe is better than not being terrified enough of it. It doesn’t happen “as soon as you drop into” it, sure, but you really can lose all of Rusts guarantees if you do the wrong thing in an unsafe block. Obscuring this fact for PR reasons or whatever helps nobody.
It does require a literal, yes.
Yeah, things are not yet at a point where development of this sort of thing would be ergonomic. We have Rust-compiled-to-asm.js code in production for over a year now, but back when I wrote it stdweb was the only choice and it sent all data there and back in JSON, which was prohibitively slow, so I ended up rolling my own iterator-like interface between JS and Rust where JS called a function and got an i32 back. This was fast enough, but not really ergonomic or generic. Our use case was optimizing a relatively isolated hot codepath in JS, and that worked well. But writing an application in pure WASM would probably be a massive pain at this point, the related tooling needs more time to bake.
It's a syntax difference. If the functions were all `fn do_something(&amp;mut self, args: Args)` instead of `fn do_something(mut self, args: Args) -&gt; Self` then to call it multiple times on the same value you would need to do `let mut value = Thing::new(); value.do_something(args);` instead of being able to put it all in a single expression that assigns to an immutable variable like `let value = Thing::new().do_something(args);`.
Just because you can produce programs with performant UIs with managed languages doesn't mean that managed languages haven't contributed to sluggish UIs. Also, yes he can be a douche at times.
In either case you're operating on top of a graphics abstraction that is doing most of the heavy lifting. It's easier perhaps to shoot yourself in the foot with a GC, although you're really just trading one footgun for another.
The important part of the error you're seeing is "musl-g++ is not a full path and was not found in the PATH". Have you googled that? It leads to a discussion that might be relevant: https://github.com/rust-lang/cargo/issues/3359
It typically means the client and server are coded in the same language. In this case, I'm guessing it means the application logic and UI are coded in Rust.
Hmmm that seems kinda hacky. Is there a different approach than the one I was taking that would be better/easier?
Yes, you can make a tuple out of the thing you were matching and the boolean condition you wanted to use as a guard, and you can match the boolean to `true`. https://play.rust-lang.org/?gist=74ab13f8a304241bd3c4ea67ee7c6ecb&amp;version=stable&amp;mode=debug&amp;edition=2015 let my_option = Some(5i32); let my_condition = false; if let (Some(_), true) = (my_option, my_condition) { println!("yep it's a match"); } else { println!("nope no match"); }
I can imagine a scheme where the generational index (the EntityId) in one vec would correspond to a different physical index in a different vec, so you could compact them.
Correct, thanks for clarifying!
Oh, that's clever! Doesn't give the full power of guards - no matched things in the condition - but still very useful. Thank you.
The file just consists of 16-byte structures (`char[16]`) packed back to back with no header or footer. 
Generally you can make different stores for different components, for example using Vec for things which all or almost all entities have like position or velocity, and you can use sparse storage like a BTreeMap for things which are rarer so as not to waste space. If you want a worked example of the ideas in this talk, you can check out https://github.com/kyren/simplecs Keep in mind though that that repository is not for people to really use, it was just a way of explaining what I was talking about. My preferred formulation of ecs is “just a data structure”, and I didn’t think there was a good example of such a data focused api, but the truth is that you should just use specs. Managing and scheduling systems like specs does lets you not have to rely on e.g. RwLock and do a bunch of other fancy stuff, and specs approach to querying will just be vastly faster. Only for educational purposes and to show how to do ECS in simple(ish) safe code.
More likely clippy failed to build against the version of nightly the job was using.
AFAIK, the first linter was called \`lint\` — it was a linter for C.
&gt; no matched things in the condition Ooh good point. Yeah I guess if you need the whole shebang, it's gotta be nested if's.
It makes total sense to use a different type of entity map depending on the type of access that will happen, thanks! I ran some benchmarks for fun, and Vec is way better than a conventional map. Simple benchmark, accessing each key in a map running 5 tests test bench_btreemap ... bench: 661 ns/iter (+/- 33) test bench_fnvhashmap ... bench: 448 ns/iter (+/- 9) test bench_hashmap ... bench: 1,028 ns/iter (+/- 42) test bench_indexmap ... bench: 1,132 ns/iter (+/- 37) test bench_vec ... bench: 13 ns/iter (+/- 0) 
His dismissal of the borrow checker being unable to prevent writes to the wrong index reminds me of how rust beginners don't realize that the borrow checker does not prevent _race conditions_, but rather only data races. Blow simply needs to learn more about rust and exactly what it brings to the table before opining on the subject.
It seems that this code almost optimizes away! I tried it on a godbolt, and the code became 2 movs, presumably to move the argument and a ret.
Regarding blocking, doesn't that depend on the hardware? Some chips have hardware level I2C and others you have to bit-bang it. I imagine you could work out some async scheme with hardware level ones but probably not with a bit-banged one. But addressing your question of whether or not you can use multiple sensors, the answer is yes. Each sensor has a different I2C address but they're all connected on the same bus. To talk to a particular sensor, you write out the device address on the bus so only that sensor will respond. This means you can only read one at a time for each I2C bus, but it should happen fast enough that this isn't a problem. If a chip has multiple I2C buses, then you could potentially read sensor values in parallel. I haven't tried that before so I don't know if that's entirely true but it seems logical enough.
Now that it's in rust stable, is it guaranteed to continue working in rust stable? I'd be happy to just run clippy in my stable travis build, rather than nightly.
This is a wonderful and informative piece of writing, I very much appreciate the effort that must have gone into putting it together!
&gt; SQLite lets you prepare statements, so you could compile all of your statements beforehand to SQLite's bytecode, but you're talking about compiling a query to native code, right? I've never heard of this! What a crazy idea. Folks have been doing it from at least as far back as 2011. http://www.vldb.org/pvldb/vol4/p539-neumann.pdf It goes very fast (HyPer is generally viewed as one of the fastest databases out there).
Yeah, I'd googled for quite a while without finding that :\\ looks like it may contain a fix. Thanks.
Impressive speed-ups across the board from 0.6, and congratulations on surpassing lucene in your benchmarks! :) Do you have an idea why it doesn’t perform as well for ``+"the who" +uk``?
Something really weird is happening to the nightly. The 2018-09-16 nightly was supposed to fix RLS &amp; clippy. But it only got... half-published. If you do `rustup toolchain add nightly-2018-09-16` *it does exist*, but somehow it didn't get promoted the static. I've been staring [at the tool that makes releases](https://github.com/rust-lang/rust-central-station/blob/9334355e08d3dc8ae180d81a4ccb7c84ce96d137/promote-release/src/main.rs#L153-L155) - it appears that the first step ("archive" here means "by date") runs, but the third doesn't. In between are docs, so I started looking at docs. The weirdest part is https://static.rust-lang.org/doc/nightly/index.html is updated, but https://doc.rust-lang.org/nightly/ isn't! When someone from the release team gets online my amateurish investigation won't probably matter, but I'm at least learning about the release process.
Excellent question! Yes. I crafted the test to highlight something Lucene is good at. (If you know such example, please let me know so I can add them to the benchmark) In tantivy, this query is executed as the intersection of two queries. The phrase query "the who" and the term query uk. The phrase query is implemented as the intersection of its terms, post filtered by the condition that the phrase query match. In contrast, recent version of Lucene makes it possible to break down queries into to phases. The first phases trims down most of the resultset, can be computed reasonably fast, but contains false positive. The second phase filter these false positive away. So for this specific query: lucene effectively computes the intersection (the, who, uk), and then, out of these results, postfilters those who match the phrase query "the who". In other words, checking the positions of "the" and "who' ONLY happens on documents that contain the, who, and uk whereas in tantivy, it can sometime happen on documents that do not contain uk. 
Because it's never mentioned once in the blog post or the slides: ECS stands for [Entity Component System](https://en.wikipedia.org/wiki/Entity–component–system)
Hey, I'm currently working on enhancing slotmap with an interface that supports this. The current idea is to allow two different kinds of "secondary maps" (the name I'm currently settling on, that allows you to use the `Key` from one `SlotMap` on multiple maps), one that works as normal and a sparse one that uses a `HashMap` for storage.
Yep, Clippy will always be available in beta or stable.
Great article! I really like the idea! &amp;#x200B; But... I don't believe that vec of indices is such a great solution in dynamic environment like game. The problems I see: \- you have to manually delete entities (== you can sometimes forget about it or you can delete entity when some1 else is still using it (and logically you shouldn't delete it)) \- you have dangling indices \- you can try to access invalid index \- with generational indices and Vec&lt;Option&lt;...&gt;&gt;, 'deleted' entity still consumes as much memory as 'created' entity. It means that without some cleaning (garbage collector!), your game will use us much memory, as during the peak &amp;#x200B; It seams to me like a pointer implementation in rust, that fights and defeats ownership and borrow checker. Unless I'm missing something important. I personally use vec of indices in my apps but in a very static way - I fill the vec at some point and then I only perform reads, no add/update/remove. Still it is possible to access not existing index, but that's the only issue and I can logically provide that it never occurs.
https://github.com/rust-lang/rust-mode Requires emacs 25+ which you might have to compile yourself on older systems. 
&gt;How you came to conclusion that I think it's for me? Because if you don't, why are you offended? &gt;It's for all readers. &gt;Readers should come up with their own conclusion. So now you think you get to tell everybody what's good for them? &gt;She should have said "*I think* this is a good thing", and not try to persuade readers to think that's a good thing without thinking for themselves. I doubt that any reader says it's a good thing because /u/kyrenn says so. Stating her opinion doesn't preclude anyone from thinking.
I wonder if they ever get rid of silly coupling between unstable features and nightly builds that causes this breakage.
Can this splice in `O(1)` ?
I'm going to be pushing a new product into production hopefully within the next month which uses Tantivy, so hearing about these improvements is fantastic. Keep up the amazing work!
I wonder what algorithm or data structure Lucene uses for this. I'd expect something bloom filter except the queries are not known in advance, and a bloom filter is expensive to build.
Awesome! Please communicate about it if you can, and ping me if you encounter a problem or have a feature request!
Can't share it just yet, but I'll be sure to provide feedback if we come across anything once we start working on the search module!
There is no bloom filter involved. It's simply a vanilla inverted index, with skip pointers. The skip pointers also point into a list of positions. Bloom filters based search engine are very rare but are possible. The only occurence I know is Microsoft's bitfunnel. 
Probably not, since it's unclear what the first thing should mean. Should it mean the second thing? If so, is angle bracket notation required when using it? If so, it's different from every other usage of impl Trait. If not, then two instances of Foo can refer to different concrete types which is confusing. Alternatively there's another reasonable syntax for what Foo should mean; that concretely it's specific type that is determined by every construction and assignment of x. A possible use case is if Foo is a wrapper for some complicated Iterator or Future that was built with combinators that you don't want to write out the full type of. But then it could get confusing if there are multiple places where Foo is constructed. I'm not sure what happens when a function that returns impl Trait has more than one path that it can return, but my guess is that it isn't allowed.
This community should give up on Rust's "safety" snake oil and make the unsafe Rust a first class citizen. Trade-offs which Rust made for basically preventing segfaults bugs are hilarious and not worth it.
&gt; Regarding blocking, doesn't that depend on the hardware? Some chips have hardware level I2C and others you have to bit-bang it. I imagine you could work out some async scheme with hardware level ones but probably not with a bit-banged one. Oh, this is just what I've seen mentioned for the current Rust efforts at making embedded dev with Rust more accessible. I think it was related to the HAL, presently I believe the i2c offered is blocking, and they're they've got a github issue about how to approach supporting async. I think there is also some other exploration of using futures-rs for DMA transfers? So maybe async for i2c would utilize futures-rs as well if the developer has success with that? &gt; but they're all connected on the same bus. To talk to a particular sensor, you write out the device address on the bus so only that sensor will respond. This means you can only read one at a time for each I2C bus, but it should happen fast enough that this isn't a problem Good to know :) I guess I'd like to have some sort of event loop going to provide some listeners that respond data from the sensors, if I've got several outputting data along that bus, do I just have an iterator/loop that empties a buffer with a match statement to identify which I2C address the data came from? and then at some point optionally send data back before reading it again? I've only done a little embedded dev back in 2016 with RS485 bus reading/writing packets as a middleman device highjacking/imitating communication between a server/client(hardware product and touchpad interface). I didn't have much fun with all the mistakes I was making and other troubles with C, it's how I discovered Rust, I've had an interest in returning to embedded dev again and learning more about it, and I think as the embedded projects develop further as they have been since, it'll be a much nicer experience :) 
Are 0.6 indexes compatible? Either way awesome work, pleasure building on top of your work. 
Master reqwest should be fixed. You can do: reqwest = { git = "https://github.com/seanmonstar/reqwest" }
Have you ever checked against https://github.com/powturbo/TurboPFor ?
Are there projects to build higher level applications, akin to Elastic or Solr, on top of Tantivy?
only https://github.com/hntd187/Toshi for now https://www.reddit.com/r/rust/comments/925ixh/toshi_a_full_text_search_engine_based_on_tantivy/
No.
Do you mean the backing vector (into which the generational indices point) should be shared across all instances of `List&lt;T&gt;`, for each `T`? As you say, I think the synchronization is potentially problematic for the performance of that, and one probably would probably benefit from alternate/more complicated schemes (e.g. a true pointer-based one, or at least a `Vec&lt;Vec&lt;T&gt;&gt;` to minimise the amount of copying required).
No. They are not compatible. I will really not care with backward compatibility before 1.0. If you want to write an index format converter however, it is not that difficult. A friend even converts lucene indexes on the fly. Check the struct class called serializer.
Ah, but you can (and some builders do) use `&amp;mut self` and return `&amp;mut Self`. The key point is that the last method must actually build some other type otherwise it will be a reference that doesn't live longer than the chained expression.
That's a complicated question... Do you mean: have I considered using the turboPFor codec in tantivy or do you mean : have I benchmarked their codec+intersection algorithm? For the first part yes and no. I know the repo. I don't plan to investigate Pfor families of compression algorithm because I'm happy with the simplicity of SIMD Bitpacking. If there is any improvement I suspect it would be marginal. For the codec decompression + intersection... AFAIK there is no search index using it. Even then, it would be great to integrate it in the benchmark even if it is just intersection without any scoring / docfreq. It would be an excellent target for raw power.
Toshi is the only maintained server project based on Tantivy. It is not distributed yet.
Do you have any links to this workgroup? 😊
Sure, crashes are better than logic errors, but safe crashes (i.e. panics) are much better than unsafe crashes, as unsafe crashes can manifest as logic errors or even security flaws. Rust pushes you away from unsafe crashes, it's then your responsibility to design things towards safe crashes, which the ECS talk does with generational indices
I use Emacs 26+, so It shouldn't be a problem, thank you!
1: A quick look at the source suggests that it uses a few non-stabilised features, though nothing that's critical. So the answer is possibly: because the author used unstable features because they could and wanted to. I think you've overthinking the "uses nightly" thing. The only real difference between the stable compiler and the nightly compiler is that the stable compiler has access to unstable features turned off. Those features are either new features that are in testing, *or* which the core team isn't ready to make a commitment on yet, *or* because they're implementation details the core team is unlikely to *ever* stabilise because they don't want to let people accidentally write code that can only ever work on `rustc` (as opposed to alternative implementations like `mrustc`). Racer seems like a case of the first, RLS is probably a case of the third. As an additional point: this isn't a big deal for toolchain components, anyway. For other code it's frowned upon because unstable features can change at any time. But when you're working on toolchain components, you can coordinate those components with changes in the compiler itself, and the stability thing isn't really an issue. 2: RLS is racer + other stuff. I believe racer is used for autocompletion suggestions, with RLS leveraging the compiler to extract type information. Racer can work on incomplete or invalid code, but the bits of RLS that use the compiler *can't*. 3: It's additional lints. The compiler contains lints for things that are almost certainly wrong or invalid. Clippy contains lints that are subjective or have high false-positive rates. Or another way: the compiler lints are canon, clippy is fanon. 4: Because `clippy` and `racer` weren't started by the core team and are only now being integrated into the toolchain installer which *also* didn't exist until relatively recently? It's not like the entire toolchain sprang fully-formed from Graydon Hoare's forehead one summer morning. Things take time and don't all happen in coordination.
1. Well, there are still quite a few things that only compile on nightly. Rust's development is pretty fast in comparison to other languages. But we never want to break code that was once working. So there are stable features (almost everything) and unstable features (new stuff that we haven't completely figured out yet). We don't want to promise that a feature will stay in that form forever, if we are not completely happy with it. That's why all features have a similar life: idea, proposed, discussed, implemented, repeat(discussed, slightly modified and enhanced,) and finally stabilized when the majority of people thinks we can support this feature forever. And yes, it's really sad that some things still require nightly. But that's the minority of libraries and tools. Most things compile on stable. 2. Racer is a tool for auto-completions, as you already found out. RLS has more features (like linting) and is supposed to eventually have all features one would expect an IDE to have. It sometimes uses Racer for auto-completions, but not always. AFAIK it first tries to ask the Rust compiler for completions (those completions are more exact/correct) and only if `rustc` cannot return the completions in time, racer is used (which is fast, but makes more mistakes). I'm also using Sublime as my main editor and I'd recommend you to use RLS and not Racer directly. I have currently installed `RustEnhanced` and `LSP` as plugins. You can read about some of the setup [here](https://github.com/rust-lang/rust-enhanced#rls-support). I'm not 100% satisfied, but most of the time it's already pretty awesome. 3. Clippy emits even more warnings than `rustc`. And in particular, Clippy has warnings that might incorrectly trigger (false positives). This is unacceptable for the Rust compiler (the normal warnings), but it's sometimes useful (if you know that false positives can occur). So yeah, clippy = more warnings. 4. Many of these tools started as a simple crate someone wrote. The ones that were really useful get more popular and at some point, the Rust team might decide to distribute them via `rustup`. But we don't just distribute everything via `rustup`, only the stuff the Rust team committed to. And don't worry about your questions, they are not ridiculous! I hope my answers are clear enough. But just ask ahead if something is still unclear!
Static/dynamic typing are well-defined concepts. Strong/weak typing are not.
Thanks for sharing. After some googling, I don't see anyone else saying the `touch src/main.rs` is needed for clippy to run in Travis. Did you do any testing in Travis without that? 
Usually it goes slightly further than "in the same language", it also means that the same code can be run on the client and the server. Think rendering the HTML first on the server and reuse the rendering code on the client when parts have to be rerendered because of user interaction
Thanks for your input. Just one last question: What do YOU actually use? All of the tools I mentioned? Only some? And if so why?
I just wanna say as a new rustacean who hasn't ever used `unsafe {}`, I fully appreciate the clarification both in this post and in the comments of the Jonathan Blow video. I'd thought that `unsafe {}` literally and completely turned off the borrow checker, but I now see I was mistaken :o)
Thanks so much for your exhaustive answers. It's so helpful to understand what's going behind the scenes. It makes me feel kind of uncomfortable if I don't know what's happening in the background. I don't need to know every detail, but at least a firm grasp of what is going on. But one more question: Is RLS really based on racer? I don't think that racer was installed as a dependency when I installed rls-preview. But I might just not remember that correctly. Also your answer makes it sound like I have to choose between racer OR rls. Is that correct?
That's pretty cool, though I'm a little concerned about all that open flame in an enclosed space. You might want to submit it to the subreddit for Rust the game also: /r/playrust
/r/playrust
Yeah, RLS currently runs on top of racer. You didn't have to install racer because you probably installed rls through rustup, which ships precompiled binaries. But if I remember correctly my reading from the rls issue tracker, the long term plan is to replace racer. I believe I remember (may not be the exact words) that racer was described as some sort of very clever hack that allowed getting a basic level of completion working quite early and with decent performance, but is by nature limited in what it can provide. You probably noticed that the completions offered by RLS are lacking when your code reaches a sufficient level of complexity. I think the long term plan to provide better completion is to eventually rely directly on compiler internals in place of racer.
Glad to hear it!
&gt; It makes me feel kind of uncomfortable if I don't know what's happening in the background. I know that feeling very well ;-) &gt; Is RLS really based on racer? I don't think that racer was installed as a dependency when I installed rls-preview. Yep, [the README](https://github.com/rust-lang-nursery/rls) says so. I think that a version of Racer is somehow integrated into RLS, so it automatically gets installed when you install RLS. &gt; Also your answer makes it sound like I have to choose between racer OR rls. Is that correct? Yes and no. You are of course free to install both things on your system. I was referring to "what to use in Sublime". It would be kind of useless/bad to get completions from both sources. So in Sublime you probably only want to enable one of those things. And as I said, I wouldn't use [`RustAutoComplete`](https://packagecontrol.io/packages/RustAutoComplete), but [`LSP`](https://packagecontrol.io/packages/LSP) together with [`Rust Enhanced`](https://packagecontrol.io/packages/Rust%20Enhanced). But that's my personal preference from my experience with testing both packages. 
Given that your base is obviously vulnerable to path exploitation what are your thoughts on the recent RFC regarding [new Path and PathBuf API's (https://internals.rust-lang.org/t/pre-rfc-additional-path-handling-utilities/8405)](https://internals.rust-lang.org/t/pre-rfc-additional-path-handling-utilities/8405) in Rust?
It's not a priority (lots of work to do distributed on top of it). It's like rocksdb for search, and you can build a mysql/cassandra/elasticsearch/etc on top of it. 
Ah yes, I currently use RustAutoComplete. But I'll look into LSP and RustEnhanced. Thanks !!
For me personally ... * Visual Stduio Code, with RLS extension. * That means I'm using RLS underneath. * Rustfmt * Sometimes Clippy (now it's on stable it's probably more often) * Sometimes Make for odd jobs. I'd personally recommend looking at IntelliJ. It uses it's own alternative to RLS, and it's a lot better.
Hi there, this submission was caught by reddit's spam filter and never reached the subreddit. If I were to approve it now it would be old enough that nobody would get a chance to see it. I welcome you to resubmit it, and message me when you do so so that I can make sure it doesn't get filtered again.
Maybe instead of trying to drop it directly from the signal handler you can have a `terminate` method that is called on drop as well as from the signal handler. This method would probably need a shared reference (`&amp;self`) so that you do not require ownership in the signal handler closure and that in turn would probably require both interior mutability (so some kind of `Cell`) as well as some kind of synchronization (like a `Mutex`) so that it can be correctly called multiple times, possibly from multiple threads simultaneously. Another option might be to interrupt the receive loop instead of the thread pool so that it stops taking new jobs and then the pool drops as it goes out of scope anyway.
Was gonna say -- I don't know how weak pointers are implemented in C++ in practice, but I can very easily imagine an implementation that is a hell of a lot more efficient and safe than what he describes. And probably make the shared pointer return a Maybe&lt;Object&gt; or whatever when you dereference it, depending on how this works in the given language.
Hi everyone, I'm coming from C++, so I might expect something that's simply not part of Rust, but anyway... I'm looking for a way to read a file bytewise similar to how you can do it with an i(f)stream in C++. Specifically, I need methods similar to peek(), get(), tellg() and seekg(). In Rust though, I've found those functions scattered over many different types (The seek trait, the Peekable iterator and the Bytes iterator), which appear to be exclusive to each other. At least, I can't find a peekable iterator over bytes which lets me seek in a file. Finally, I can't find any method similar to tellg() at all. Is there an easy way to achieve this? Is it maybe viable to construct a peekable bytes object every time it is needed, or is the overhead too large? Thanks in advance for any help!
Nice example!
Great to hear that! I'm really happy that I am able to help quite a few people with this lecture. I wish my videos were useful to everyone and not only to German-speaking programmers. If you don't mind answering: what country are you from? Germany? And how good do you think your German is? :)
`File` implements the [`Seek` trait](https://doc.rust-lang.org/std/io/trait.Seek.html#method.seek), allowing you to do the same things as `seekg()`/`tellg()` (just call `seek(SeekFrom::Current(0))` if you specifically need the current position). For `peek()`, you can't _actually_ do that on files, so you'll probably want to read a byte and then seek back one byte - that should be easy to add as a utility function. For efficiency, you might want to wrap your file in a [`BufReader`](https://doc.rust-lang.org/std/io/struct.BufReader.html), which will be more efficient if you're jumping around a lot. Also, why do you need to do something like this at all? It sounds like you might want a parser. Do you want a parser?
Oh, I didn't notice you get the position from the seek call... My bad. But yes, ultimately my goal is to build a parser for certain filetypes. Your answer sounds like there is a better way to do that than what I'm suggesting here?
I would highly recommend VSCode for Rust development. I use Sublime Text as my main editor for most languages (my day job is web dev), but the VSCode integration for Rust is soooo much smoother than the Sublime Text Rust integration. With VSCode you shouldn't have to install anything manually (except rust itself), the editor plugin will handle installing everything.
The JWT spec does require the payload be JSON. However, a JWE can have an arbitrary payload. You might consider using a JOSE library such as [biscuit](https://github.com/lawliet89/biscuit) which supports JWS/JWE in addition to JWT
`seek` returns the position in the file (after seeking), and can be called with relative and 0 to get the current position as a way to implement tellg. `Iterator` is designed to deal with one-time iteration, so it's not a great match for this. You could write a wrapper around `File` to implement `seek` (passthrough to `File::seek`), tell (wrapper around `File::seek(relative, 0)`), `get` (`Read::read` into a buffer of 1 byte), and `peek` (`read` followed by `seek(relative, -1)`).
Generally it is a good idea to wrap a File in BufReader to reduce the number of syscalls. Especially if you read the file in small portions, like byte by byte, it will have a very poor performance without buffer. In C++ ifstream is buffered by default, but in Rust you need to use BufReader.
I think the most important topic to use Rust in a basic level is understand move semantics, once I understood this a lot of errors starting to make sense. Now I just need to go deeper with lifetimes, which is my biggest roadblock right now.
Might wanna wait on NLL for the lifetimes thing.
The most popular runners-up according to crates.io are: * [combine](https://crates.io/crates/combine) if you want another option for using parser combinators. * [pest](https://crates.io/crates/pest) and [peg](https://crates.io/crates/peg) if you want a [PEG](https://en.wikipedia.org/wiki/Parsing_expression_grammar) parser. * [lalrpop](https://crates.io/crates/lalrpop) if you want an LR(1) or LALR(1) parser.
FWIW, this *does* get used all the time in real games, so these problems are definitely solvable (or at least not so bad, I guess). - Manually deleting entities is really what you *want* in a game where their lifespan is completely dynamic and dependent on gameplay. Assuming that as a starting point is the only way to solve the other problems: - Generational indices solve the dangling index problem. If an entity has been deleted, anything holding a handle to it will find out when it tries to access it. This is often expected, and a place to make a gameplay-level decision! - Invalid indices *not* due to deleted entities are not really an issue, because you can either a) just not make up new indices and try to access them or b) enforce that by making the index a newtype with a private constructor, used only when you *actually* create an entity. - Using less memory for entities when you can is... not really useful in a game. If you don't have enough memory for the peak usage, the game won't work anyway. Further, entities are nowhere near the biggest use of memory in a game- that honor usually belongs to assets like images or level data. And finally, you can use something other than a `Vec&lt;Option&lt;T&gt;&gt;` for less-common components, as described [in this comment](https://www.reddit.com/r/rust/comments/9g1kpy/rustconf_2018_closing_keynote_blog_post/e61vxtk/).
use std::vec::Vec as NotAVec;
I'd watch a Netflix original series like this.
Just learned this on the last (14th) video of 3b1b's [Essence of Linear Algebra](https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab).
What I like a lot in C++ is that files are not read into memory completely, but streamed instead. Is there a possibility to do this here as well? The examples I checked out for nom, for instance, all work by reading the whole file into a string and handing it over to the framework.
&gt;Thanks for the reply! It clarifies a lot and makes indices vec more friendly :) &gt; &gt; &gt; &gt;I think I will try this in my next project to see if it is really convenient. 
Thanks, especially foreclarifying the nomenclature! I'll give biscuit a try. 
Or just enable it via \`#\[feature(nll)\]\`
/r/playrust
I usually take the core dump (or windows crashdump) and feed it to the debugger after the fact. I've even had them shipped to different computers at times. Why do you need to run it in the debugger? With binutils you can split the binary into one file with the binary and one file with the debug information, just like the default for MSVC. I'm not had the need to do it so I don't know if it is possible with llvm, but I'd be surprised if it was not.
This is something I came up with after thinking how on the vulnerability discussed in [Auditing popular crates](https://www.reddit.com/r/rust/comments/8zpp5f/auditing_popular_crates_how_a_oneline_unsafe_has/) post and also [the one I've recently discovered in Claxon](https://github.com/ruuda/claxon/issues/10) (assigned [RUSTSEC-2018-0004](https://rustsec.org/advisories/RUSTSEC-2018-0004.html)) could be prevented.
I'm very interested in 'discovering' a different mindset. So far i've had that while learning Java, C, JS, Elm, Elixer, and now Rust (in that order). I've only ever poked at C++ while learning C. Now that i feel comfortable with Rust i'm somewhat disinterested in learning C++ because i get the impression it has little left to teach me about programming in general. But that might just be my filter bubble talking. So could you expand a little on C++ strengths? 
Very nice, and thanks for contributing back to imap-proto!
BTW - there is a third python solution: from itertools import tee squares = (x * x for x in range(10)) squares_1, squares_2 = tee(squares) print(min(squares_1)) print(max(squares_2)) 
Correct, I observer this behaviour if I run clippy fater compilation.
Hey, thanks) Actually, it's probably not that necessary if you first run clippy and then try to compile.. Probably I am a little bit paranoid. I am looking forward for @[kickliter](https://www.reddit.com/user/kickliter) reply. 
My first thought was, that this is a special-cased `copy`, but it's actually a buffering cache. You may as well use a list.
Ah, neat, I didn't know about `tee`! The [`tee` documentation](https://docs.python.org/3/library/itertools.html#itertools.tee) warns, though: &gt;This itertool may require significant auxiliary storage (depending on how much temporary data needs to be stored). In general, if one iterator uses most or all of the data before another iterator starts, it is faster to use `list()` instead of `tee()`. It looks like `tee` stores the whole sequence in memory in a deque, so I probably wouldn't count this as a streaming solution. BTW, while I was trying to look up `tee`, I ran across [this SO question](https://stackoverflow.com/questions/10999039/tee-function-from-itertools-library) which is SUCH a perfect use case for ownership!
I've noticed the same behavior on my local machine.
Thanks, I ended up just using vigil (which does indeed use rocket internally), and it's exactly what I needed, but its nice to know I don't need a fancy web server to do simple things online.
NLL=non-lexical lifetimes It changes up how some lifetimes work.
What? Nightly is by definition unstable. It's a fact of life that nightly will break sometimes. My post was more wondering if there was something specific that I need to change, as is often the case with nightly breakages.
Wow, thank you! Got around to find this: https://en.wikipedia.org/wiki/Lint_(software)
Thanks for you input. That's exactly what I dont want. Haha. I want to know what vscode installs so i actually understand what's going on in the background. But just in general. I keep hearing very good things about vscode. Why is it so popular? I tried Atom once and compared to Sublime it felt waaaaaaaaaay to slow. Isn't vscode built on Electron as well? There must be a reason why so many people like it, I just dont know why exactly. :-)
I think this has less to do with ownership rather than mutability, because the problem is that min and max mutates the iterator in such a way that it become empty and the next call fail as illustrated [in this example](https://play.rust-lang.org/?gist=72adfb350e7f76168f49608e66a16c22&amp;version=stable&amp;mode=debug&amp;edition=2015).
Yeah, "universal" is the more commonly accepted term.
Hey Catherine, I really enjoyed the talk and the blog post! I gotta ask though about dynamic typing (AnyMap) + the component registry, because I just don't get it. I'm going to use quotes from the blog post to try and guide us through my confusion. &gt; More so, every “system” (for us, this is still just a fancy name for plain functions) depends on all of the types that go into our game state, which may be quite large. This makes sense, because we pass a borrow of the whole `GameState` to every system. But if we keep passing a borrow of `GameState` (or now, `ECS`) to every system, and the game state contains the components' AnyMap, it seems to me that every system still depends on all the types in the game state, no? &gt; Say you get a new feature request for your game, say you need a new crazy special monster that has some kind of counter inside it. Every time you kill the monster, it copies itself into two and decrements the counter,duplicating like the heads on a hydra. This means you might need a new component type, say EnemyDuplicationLevel or something. With dynamic typing, you can add this component without “disturbing” your other systems, because without importing the new module, they can’t possibly “see” that the ECS has such a component anyway. Okay, there's a bit to unpack here. First, is there anything obvious that I'm missing that would make adding a `EnemyDuplicationLevelComponent: EntryMap&lt;EnemyDuplicationLevel&gt;` field to the original game state impossible? Next, how does the addition of an extra field in the game state "disturb" the existing systems? If they accept a borrow to the whole game state, won't they keep on working as they did before? &gt; In ECS implementations like specs, there’s a step where you “register” a type with your ECS, which inserts an entry into some AnyMap or equivalent to an AnyMap. Using a component type that is unregistered is usually an error. This seems very similar to trying to use a field that does not exist in the `GameState` struct, except that the error occurs at run-time rather than compile-time. &gt; Then, we’ll tie them together in one big global constant with lazy_static! At the beginning of the dynamic typing section, you mentioned that the biggest problem we hadn't addressed was that everything was global: &gt; The biggest problem we haven’t addressed from earlier is still that everything is kind of global still. I can't unify those two statements, can you help me? (I'm almost done with the newbie questions, I promise!) &gt; I actually like the idea of a “type registry”, and something like this is necessary as soon as you want to use this sort of dynamic typing with AnyMap, and the two patterns go together well to limit the problem of “everything depending on everything else”. I'm still fuzzy on how listing the fields explicitly in a struct -- and thus having the full benefit of static type checking -- makes everything depend on everything else and having the same information in an AnyMap gets rid of the problem. &gt; I have barely talked about functions or systems! The reason for this is, I don’t like introducing this concept by talking about behavior, I really think that thinking just about how we describe our state is a much more useful way to approach this. Maybe it's my own daftness, but _could we_ see a simple system that would show how dynamic typing and the registry offer an advanage over fields in a struct? I'm looking forward to being illuminated, and looking forward to Spell Bound! 
The main answer to your question is to use IntelliJ with the Rust plugin. RLS does not yet have as rich or stable a feature set.
Are you specifying a 32-bit version of mysqlclient.lib to the linker? IIRC, the msvc linker will ignore .lib files that are not the same as the target architecture
The forth solution would be making a Squares class implementing __getitem__ accepting indexes, since all squares can be trivially computed from the index. It could also implement slicing and upper and lower bounds. They would be trivially copyable due to a low amount of state.
I heard a lot of good things about the IntelliJ Plugin. Does it just "replace" RLS or also things like clippy, racer, rustfmt, etc. ?
Since we no longer care about iteration and just want the output of that specific example, the fifth solution can be `print("0\n81")`
Ooh, I'd love to know what your use cases are! Could you describe what would you use that for? Preferably in that thread, but Reddit works too.
Why not implement this in a separate crate instead of a first, experimental implementation going into std?
Yes, I am
How slow is the motivating example if you just start by copying the input vector? Or, phrased another way, when you say "prohibitively slow" do you mean 10x too slow or 10% too slow?
Right now I'm looking for feedback on whether this is a good idea and/or feasible. I haven't really considered implementation yet. Also, I do not feel I am qualified to write production-quality unsafe code.
I also think that is the best way to go. Create a crate, let it mature, and once people are generally happy with it consider adding it to the standard library.
Actually, cloning out the input slice is something I haven't considered. It's in a small but frequent allocation in a hot codepath so I assumed it would be prohibitively slow, but perhaps I could reuse allocation using a vector? Hmm. "Prohibitively slow" means 10% or 20% hit on the end-to-end DEFLATE decoding, of which RLE is just one part. I have not benchmarked RLE in isolation, admittedly.
Regarding virtualenv: the dependency management part (requirements.txt) isn't needed because rust doesn't do "global" installation of packages. You just list the packages (and versions) that your crate requires in Cargo.toml and cargo compiles them specifically for your project. In terms of isolating compiler versions, [rustup](https://rustup.rs) (usually the best way to get rust installed on your system and keep it up to date, even if your OS has its own package) has a [little-known (?) feature](https://github.com/rust-lang-nursery/rustup.rs#the-toolchain-file) that handles it. 
There is [a macro solution](https://crates.io/crates/if_chain). 
Urgh... thanks for pointing that out, you're totally right. I did not know about `impl&lt;'a, I: Iterator + ?Sized&gt; Iterator for &amp;'a mut I`. I used your example as inspiration to write this program, which successfully compiles: fn main() { let mut squares = (0..10).map(|x| x * x); println!("{:?}", (&amp;mut squares).min()); println!("{:?}", squares.max()); } Is it just conventional that people work directly with `I` instead of with `&amp;mut I`?
I think it may not work for 32 bit. I know I used to build and smoke-test a 32 bit version, but I was using [vcpkg](https://github.com/Microsoft/vcpkg/) to install the connector library and it dropped support for 32 bit builds of libmysql. The diesel CI does not do any 32 bit builds on windows. You could try the [diesel gitter channel](https://gitter.im/diesel-rs/diesel) if nobody here has a definitive answer.
Apparently there is already an issue from [June](https://github.com/rust-lang/book/issues/1392), that itself refers to another issue from [November](https://github.com/rust-lang/book/issues/994) that was closed because "returning from a loop is covered in [the newest features appendix](https://doc.rust-lang.org/nightly/book/second-edition/appendix-07-newest-features.html#returning-from-loops)," which doesn't exist, and shouldn't even cover loop labels because they are not a "new feature" as evidenced by them being in the 1st edition. I will add to the June issue that information about `continue` is also missing.
Labels are not, but break *with a value* is. Thanks, I’ll look closer tomorrow.
Oops, I meant a specific section. Edited.
Hi! I want to define struct Foo&lt;T&gt;. Then I want to write `impl&lt;T,U&gt; From&lt;Foo&lt;U&gt;&gt; for Foo&lt;T&gt;` `where T: From&lt;U&gt; {}` When I try this it fails with a "conflicting implementations of trait" error. I found a post from 2016 where someone asked my exact question and the answer was you can't do that. [https://users.rust-lang.org/t/conflicting-implementations-of-trait-std-convert-from/6427](https://users.rust-lang.org/t/conflicting-implementations-of-trait-std-convert-from/6427) I feel like this pattern is pretty common. I can write a function that performs the conversion. But I can't implement a trait that does it. Is it still not possible in 2018? Is there an idiomatic way to achieve what I'm trying? Thanks! &amp;#x200B;
yikes.
Hi I’m the maintainer of toshi. It’s a pretty rough project right now but it has most of its basic functionality there. I’m beginning to design and implement the distribution of toshi soon. All the pieces are there to handle it, I just think about coordination and consistency. Always interested in people’s thoughts about how this should look. 
&gt; Also, I do not feel I am qualified to write production-quality unsafe code. You know that thing that non-native English speakers do sometimes, where they write a long, beautiful, perfectly worded post and then, at the very end, they're like "apologies for my bad English." I feel like that's what you just did, but with unsafe code :)
Right now, I don't think it can be done. About a fortnight ago, there was [this thread](https://www.reddit.com/r/rust/comments/9ce1t8/blanket_from_impl_woes/) with the same problem - you might find something useful there. You might also want to take a look at the [tracking issue](https://github.com/rust-lang/rust/issues/31844) for specialization - note that specialization can actually be used on nightly now (behind a feature gate), but only for traits impls that use `default fn`.
How large is a typical `repeating_fragment_len`? Could it be practical to allocate something like a 4096 byte array on the stack, and assume the slice you need to copy can fit in that?
Ah hah. This makes sense. And I learned a few keywords that will help me with future searches. Thank you!
Is reading/writting one byte at a time really a great option? it seems something like the below _may_ be better since an extend_from_slice would have larger segments for memcpy: pub fn decode_rle(buffer: &amp;mut Vec&lt;u8&gt;, repeating_fragment_len: usize, num_bytes_to_fill: usize) { //clone the fragment let mut fragment = vec![0;repeating_fragment_len]; fragment[..repeating_fragment_len].clone_from_slice(&amp;buffer[buffer.len()-repeating_fragment_len..]); //allocate required memory so there aren't re-allocations buffer.reserve(num_bytes_to_fill); //repeat extension with full blocks - does nothing when num_bytes_to_fill is zero //panics when repeating_fragment_len is 0 //writes up to repeating_fragment_len*floor(num_bytes_to_fill/repeating_fragment_len) // == num_bytes_to_fill - (num_bytes_to_fill % repeating_fragment_len) for _count in 0..num_bytes_to_fill/repeating_fragment_len { buffer.extend_from_slice(fragment.as_slice()); } // write the remain bytes to the buffer // num_bytes_to_fill - floor(num_bytes_to_fill/repeating_fragment_len) // == num_bytes_to_fill % repeating_fragment_len buffer.extend_from_slice(&amp;fragment[0..num_bytes_to_fill % repeating_fragment_len]); } [rust playground link](https://play.rust-lang.org/?gist=1523b8371d21f9a44b7375e59c351325&amp;version=stable&amp;mode=debug&amp;edition=2015) PS: obviously needs fuzzing and checking to make sure the math works out - but should either be an extend and NOP (when num_bytes_to_fill==0) or a panic (when repeating_fragment_len==0) PPS: could remove the panic by using the expensive checked_div right at the top to turn the panic into a NOP
I have a feeling it's related to this code but I'm not sure: &amp;#x200B; [https://github.com/rust-lang/cargo/blob/502f0ae72ba3b64f96dab8a00b33e93240e68de4/src/cargo/core/compiler/fingerprint.rs#L779-L800](https://github.com/rust-lang/cargo/blob/502f0ae72ba3b64f96dab8a00b33e93240e68de4/src/cargo/core/compiler/fingerprint.rs#L779-L800)
What I meant when I said "everything depends on everything else" is more if you look at things at the level of module to module dependency. For example, imagine that you add a data type to your GameState for something very very particular, just as an example we'll say NPC pathfinding. You make an AStar searching module, and a bunch of new types to do pathfinding and add those to your GameState inside some kind of NPC struct. Since every system has complete access to GameState, every system has now a new "dependency" on AStar, because systems depend on GameState, and GameState depends on AStar, by what we said above. This "dependency" is only arguably in a literal sense at the module-level, as in if you define some physics system that does a simple `position += velocity * dt` or something, that must import GameState, which in turn must import AStar, either directly or indirectly. This is far less important in Rust than it is in something like C++, where module dependency actually influences the way things are compiled, where every time you changed anything that was directly or indirectly owned by GameState, every one of your systems would recompile, however it's not entirely unimportant either. In one sense, this is important because without dynamic typing it becomes very very hard to write something like an ECS data structure as a library (I think you'd either need macros or very fancy type-level lists and HKT or something?). What I meant when I mentioned this in the keynote and in the blog post though was more direct: that all systems have "access" to all of the data, and adding a field to GameState necessarily gives all systems access to that new field. There's obviously nothing stopping you from adding more types into GameState, the problem only arises if you're uncomfortable with giving every system access to all such fields. In a literal sense, all systems have "access" to all the fields because they have access to GameState, but the point is not to make it *impossible* to access them as much as it is to provide a simple declarative speed bump. Say you had an ECS type that used AnyMap. If you write a physics system, and it only imports the component types `Position` and `Velocity`, then by definition it is only able to get `Position` and `Velocity` component storage out of the ECS type, because it must name the types in order to get the correct storage out of the AnyMap. We know that it can't access a `Navigation` component, because it is not imported. When storing these fields directly, *all* systems can simply access component records without this extra import step. It's up to you whether or not this is important, probably quite a lot of people wouldn't think of this as a problem worth thinking about. Often times though, especially in large projects, you start to think very hard about modules in terms of their dependency graphs, and introducing module dependencies can *feel* very scary. In practical terms, module -&gt; module dependencies mean that you maybe cannot necessarily move a subset of functionality into a separate crate, but other than that it's mostly just an organizational concern. Dynamic typing simply allows for you to have a dependency graph other than one with all systems depending on all data types.
Learn C++ first, you will appreciate Rust so much more after that. ;-)
Hey Currently working with rust and Blockchain too. Right now I'm building a debugger for programming languages that work with Ethereum. This crypto Reddit community sounds interesting! Would love to get involved if you two are going for it.
Very nice job 👍
There's a pattern (I've used it once, so that makes it a pattern) that I call "conceptual `split_mut`". In this case a `Vec&lt;T&gt;` could be split into a `&amp;[T]`, for its contents, and some struct with `next: *mut T, capacity: usize, len: &amp;mut usize`. 
An RFC can always be submitted...
I always love to see progress on your great project! The results are just spectacular considering the current state of Qt in Rust-Land. Can you say something about how engaged the community is in the project? I can see some commits from other people in the log and it looks quite healthy to some extend. But i feel like the overall engagement in the (G)UI department – apart from GTK and Webfrontend – is quite low currently. I am just asking for it because i have seen so many projects die in this field and i really want yours to be healthy. Unfortunately one project i really like from its brilliant approach [qmetaobject](https://github.com/woboq/qmetaobject-rs) has a rough time currently – nothing really happens after the initial creation and i don't feel like its getting much attention. Glad to see your project flourish. 
Just a note for parsers... I've used [Cursor](https://doc.rust-lang.org/std/io/struct.Cursor.html) with the [Byteorder](https://crates.io/crates/byteorder)'s extended read bytes trait for quick and dirty binary parsing.
Could someone please explain the general strategy (strategies?) to make good use of vectorized instructions on streaming data? Streams feel inherently serial in nature.
Thanks you for your kind words. I agree that the state of Rust for applications that have a command-line or web UI is most active. Graphical frontends are not so widely seen yet. And that is a shame because Rust has a lot to offer for those applications too. Firefox is an exception of course. Big applications like Thunderbird and KMail could also benefit from using Rust. To me the experience of writing this email viewer was very pleasant. Admittedly, it's not all that complex. It supports two different backends, has no caching, no updates and no support for sending mail. But it does support saving attachments, searching in a folder and is responsive because it does processing in a work thread. If you look at most of the approaches to bring GUI frameworks to Rust, you'll see that all of them are pretty complex. They want to work existing frameworks into Rust or build a new GUI framework. Both things are hard. So I've taken the lazy approach and decided to just allow communication between Rust and Qt according to the model in the particular program. This avoids a lot of nearly impossible work of wrangling an existing API from a less safe language into the constraints of Rust. That being said, the Qt and QML APIs are pretty good too. qmetaobject-rs is indeed a brilliant approach. It's active. The main author has already indicated interest in adapting the mail viewer from this blog to work with qmetaobject-rs. You've just given it attention which is good. Other than that people should just go out and try to write a nice simple app in either of these projects or even try to compare different UIs on the same application core and see how well they work. 
&gt; Streams feel inherently serial in nature. You load N values from the stream and process N values at a time.
I'd love to hear from people trying to get this program to run. So far I've had two reports of people that read their mail with it. 
&gt; Also, I do not feel I am qualified to write production-quality unsafe code. I think there will be enough people willing to review.
Tantivy can compile to wasm. I made a POC a few month ago. Someone is actually working on packaging that in a usable form.
From the code it looks looks that this is hash of metadata. 
I read the links you put in the reply and about Cargo. It's absolutely amazing, thank you for the insight!
IntelliJ internally is sort-of an alternative Rust compiler fronted, written in Kotlin. It provides all IDE features using this "compiler", and does not depend on rls/clippy/rustfmt or racer. You might still want to use clippy and rustfmt separately though. IntelliJ has only some lints at the moment. It has it's own formatting engine as well, which works pretty differently from rustfmt: if rustfmt rewrites the whole source in one true way, IntelliJ's formatter just makes sure that nothing is "wrong" with formatting, leaving the overall style to you. In other words, rustfmt breaks long lines for you, while IntelliJ generally leaves line breaks to the programmer. 
Roughly. We plan to. But this is still a preview, the eventual plan may change.
Thank you for your answer, I've checked the header and can't see anything like this. Moreover, the header is used in an 32bits target compilation when used in a C++ env. 
No. Pointing out what other people did wrong is easy (and makes you look smart). Actually doing everything right is hard.
Not sure about the typical length, but the maximum length is u16::MAX_VALUE, bytes so 65k. If they're small typically, perhaps a SmallVec would help.
&gt;there are a couple of unsafe vulns this would prevent" is probably not good enough for that. &amp;#x200B; Wow! If the bar is anything it should be, "This fixes a couple of unsafe vulnerabilities". IF that doesn't meet the bar for getting into the stdlib, then, it sounds like the bar is ill-defined.
extend_from_slice() is what I wanted to eventually get to, yes. There are some examples like this on the post itself, albeit only with fixed-capacity views. Your implementation is probably slow because of an extra allocation. Using SmallVec instead of Vec or reusing an existing heap allocation might help. Also, try using `copy_from_slice()` (which is a guaranteed memcpy) instead of clone_from_slice(), just in case.
Could you point me to a code example that's used it? If the struct allows copying into the underlying capacity using something that optimizes to memcpy(), I'm interested.
Not the OP, but that code shows a bit of a cognitive bias on my own thinking. I'm so used to operations being sequential that even when a 'loop with +=' comes in I still think it's sequential. I wonder how crazy for general performance it would be if programming languages could prove more things commutative and apply SIMD transparently.
This is not "the stdlib has some vulnerabilities which this fixes". This is "it is possible to misuse unsafe a certain way and mess up, adding this provides a way for folks to opt in to a safer way of using this". &amp;#x200B; There are millions of ways you can mess up with unsafe. This is just one rather specific one. It would need to be pretty common to meet the bar but AIUI that hasn't been demonstrated.
Is there a way to write the signature of `FixedCapVec::extend_from_slice` safely while also allowing the code you've written? I would have thought that the only way to allow you to extend from a slice of the same Vec is by making `extend_from_slice` take `&amp;self` and use `UnsafeCell`.
QT rust projects unfortunately inherit C++ build complexity issues. qmetaobject does not build on my mac and I have no idea why. I've had the same problem with a bunch of other qt/qml projects.
To be fair Rust's documentation is pretty good and the compiler errors are very useful. Most of the time i can fix my code by following the compiler's advice, and if not i mostly refer to documentation or Rust's forum. I never really had to go to SO for anything Rust related so far.
&gt; since otherwise you would have a coexisting `&amp;` and `&amp;mut` to the same Vec To the same Vec - yes. To the same memory region - no, this is statically ruled out by Vec, because you can only read from the initialized part of the Vec and only append to the uninitialized part. Think of it as a `slice.split_at()` except it splits Vec into initialized and uninitialized part and allows appending to the uninitialized part as if it were a Vec in itself. Which, by the way, sounds like a yet another way to implement this.
Yes, it's safe, but I'm saying that there's no signature that you could write for `extend_from_slice` that proves to the compiler that it's safe (in current Rust, I think I've seen proposals that make explicit which fields you're mutably borrowing and which you're immutably borrowing)
Hi, so I have a question about the this code snippet: https://play.rust-lang.org/?gist=2c6c8424f8c9eb314ab3f64d32ebf621&amp;version=stable&amp;mode=debug&amp;edition=2015 I do not understand why the borrow checker forbids the first, but allows the second. In my opinion they are equivalent. So whats the reason for this behaviour?
Ah. Hmm. I admit I'm out of my depth here. Could you bring this up on the thread so someone more competent could take a look at it?
&gt; I wonder how crazy for general performance it would be if programming languages could prove more things commutative and apply SIMD transparently. Not crazy at all, that's pretty much what every SPMD language like ISPC, CUDA, OpenMP, ... does
There's a comment in the SSE2 implementation that gives a general description of the algorithm: https://github.com/BurntSushi/rust-memchr/blob/37d38c5e27963aac2ec948e343da5b1f12ecdd0d/src/x86/sse2.rs#L19-L106
`v[e1] = e2` is equal to `*std::ops::IndexMut::index_mut(&amp;mut v, e1) = e2`, which in your case is equal to `*IndexMut::index_mut(&amp;mut v, Vec::len(&amp;v) - 1) = p[0]`. Arguments are evaluated left-to-right, so you cannot call `v.len()` because you've already borrowed it mutably as the first parameter of `index_mut`. There are some people trying to get this to work with more special casing in the borrow checker. I'm not entirely convinced that this is needed, but if you wait long enough it might be that newer versions of the Rust compiler will accept this code.
Didn't suggest to implement iter for getitem.
I usually want to know what it installed and how. But in this case not so much! It's using the Rust Language Server, and possibly Racer (but maybe the RLS installs/includes racer itself, not sure. I believe it installs these via rustup (if you're not using rustup then you should be - nothing else is nearly as well supported for installing Rust). VSCode is built on electron, but it's not slow! It is a *little* slower than sublime (e.g. when starting up, and when working with huge 100mb+ files), but you don't get typing lag like you do with atom. It's just better engineered than atom I think. Why do people like it? Easy customisation is the main one I think (those web technologies have advantages!), which means that there are a lot of good plugins and extensions for it. Plus they regularly release new features (every month - unlike sublime!).
Wow, it's easy to miss, but check out the [ifunc trick](https://github.com/BurntSushi/rust-memchr/pull/35/files#diff-daf0eb40e188009669f64f1913003e3d). I don't understand how it works, but it's really cool. One only pays for the runtime cpu detection check once, as runtime cpu detection can be expensive when called in tight loop (like memchr often is). Definitely interested in copying this approach for the SIMD hashing library I'm working on, as hashing can also be in a tight loop (though less so, so I'll need to benchmark). I love looking at rust PRs. I think I learn something new every time &lt;3
Every rust QT project inherits one of the worst things about C++: build complexity. I have yet to be able to build a single one of these projects on macos. 
&gt; Uses 2 flat usize vectors (triangle edges and halfedge neighbors) Would one be able to easily use reuse the half-edge data-structure here ? That's a widely-useful thing.
Yeah! It's actually really simple once you break it down. Basically, you write a normal function that does CPU feature detection and dispatches based on that: Then in your public API method, instead of just calling `detect_then_dispatch`, you stuff it into a shared global mutable function pointer and call that instead: fn memchr(needle: u8, haystack: &amp;[u8]) -&gt; Option&lt;usize&gt; { static mut FN: fn(u8, &amp;[u8]) -&gt; Option&lt;usize&gt; = detect_then_dispatch; fn detect_then_dispatch(...) -&gt; ... { ... } unsafe { let fun = relaxed_atomic_load(FN); fun(needle, haystack) } } Once you have that working, now all you need to do is write over `FN` from inside of `detect_then_dispatch` with the actual function you want to call: fn detect_then_dispatch(needle: u8, haystack: &amp;[u8]) -&gt; Option&lt;usize&gt; { let fun = if is_x86_feature_detected("wat") { wat::memchr } else { fallback::memchr }; relaxed_atomic_store(FN, fun); fun(needle, haystack) } And presto! The next time `memchr` is called, it will continue loading the function from `FN`, which now has the correct CPU optimized routine and will no longer branch on feature detection. This does of course give up inlining, so if that's important, you may need to figure something else out! In memchr's case, the routines are so large that they probably wouldn't be inlined anyway. And on modern CPUs, you'll likely get the AVX2 routine, and unless your entire program is compiled with AVX2 support, I don't think it can be inlined anyway. I knew about the idea from gcc, but /u/acrichto's code helped too. :-) https://github.com/alexcrichton/cfg-specialize/blob/3d51fe72f277f165a74a50f82d5d847128755da1/cfg-specialize-macros/src/lib.rs#L224-L273
&gt;but should either be an extend and NOP (when num\_bytes\_to\_fill==0) or a panic (when repeating\_fragment\_len==0) You can statically handle those cases via [https://doc.rust-lang.org/stable/std/num/struct.NonZeroUsize.html](https://doc.rust-lang.org/stable/std/num/struct.NonZeroUsize.html).
If you tell what fails I can help to get it to compile.
I don't have answer for you regarding I2C and futures-rs, though it seems possible to do. &gt; if I've got several outputting data along that bus, do I just have an iterator/loop that empties a buffer with a match statement to identify which I2C address the data came from? and then at some point optionally send data back before reading it again? I2C sensors are typically doing their own thing (sensing whatever it is they do - light, temperature, distance, etc.) until your microcontroller begins communication with them. Each sensor has documentation on how to communicate with them over I2C, but typically you'll write out their address and perhaps a command code in the format specified by I2C and the sensor documentation, and the sensor will reply with the data you requested. So if you have 3 sensors, you'll have their addresses known ahead of time and you'll loop 3 times, sending out the I2C read request over the bus and retrieving the response from each sensor. If you have a _lot_ of sensors, you could throw another microcontroller into the mix whose only job is to collect data from all the sensors, then your main microcontroller can speak to that one to get all the data in one read. For a concrete example, consider the data sheet for this light sensor, a TSL4531: [https://ams.com/documents/20143/36005/TSL4531_DS000182_3-00.pdf](https://ams.com/documents/20143/36005/TSL4531_DS000182_3-00.pdf) Go to page 11 and read about how it uses I2C. You can then read the [source code](https://github.com/SuperHouse/esp-open-rtos/blob/master/extras/tsl4531/tsl4531.c) for an example I2C driver for this sensor (I wrote this). To see the nitty-gritty details of I2C, I'd also recommend reading the `i2c_slave_read` and `i2c_slave_write` functions [here](https://github.com/SuperHouse/esp-open-rtos/blob/a8c60e096093e9e9f4a60b885676adc2cf5b790a/extras/i2c/i2c.c#L429-L484). It eventually boils down to the microcontroller setting the SDA and SCL pins high and low. From a simplified point of view, that's pretty much it!
Thanks. I've updated the playground and linked below in case others are interested. Further experimentation revealed this can also be resolved by enabling non lexical lifetimes (`#![feature(nll)]`). https://play.rust-lang.org/?gist=891a16cf587fd9b33b0d62b3b1b6ecea&amp;version=stable&amp;mode=debug&amp;edition=2015
I was traveling the first part of last week and never ended up posting, however I did finally release [`uom`](https://github.com/iliekturtles/uom) (type-safe zero-cost dimensional analysis) v0.20.0 which includes the long discussed thermodynamic temperature vs. temperature interval implementations as well as some feature hygiene fixes in macro generated code. This most recent weekend I release v0.20.1 in order to fix breakage introduced in [#52841](https://github.com/rust-lang/rust/pull/52841) that changes macro path lookup in nightly. Coming up I think my plan is to review documentation.
Well, using that same argument you can dismiss the advantages of Rust. In both C++ and Rust you essentially just generate machine code. Both abstract over the same end-result. So, sure you can write memory safe programs in C++, but I think you would agree that doing it in C++ requires more discipline. I don't think non-managed languages do anything to technically force you to make things more performant. But they confront you with more of the underlying systems by causing bugs/crashes or memory leaks if you program in complete ignorance of them. Whereas many managed languages allow you to be a little more "sloppy" with less dire consequences. Well that and the heavy emphasis on towering OOP abstraction layers in many of the managed language communities. So I see the disadvantage of managed languages being a lot similar to the advantages of Rust. Based on the interfacing of humans with the language more than just technical merit. Btw. not trying to say that this is what JB was talking about, this is just how I see managed languages. 
I'm working with the following code to iterate over directory entries: [https://gist.github.com/rust-play/fd84f0da0a96ce73982ded06a4ff2aa0](https://gist.github.com/rust-play/fd84f0da0a96ce73982ded06a4ff2aa0) The above code compiles, however I feel like the semantics are a little strange. I feel like something below is a little more intuitive—or it's possible that I would just like to confirm there's not some way I'm missing to make the for loop variation of this work. To be specific, the below code will not compile citing the use of moved value \`dir\_iter\` as can be seen in the screenshot. [https://gist.github.com/rust-play/e386ea872cf348096efb676fc590bb83](https://gist.github.com/rust-play/e386ea872cf348096efb676fc590bb83) [https://i.imgur.com/3i4KLiI.png](https://i.imgur.com/3i4KLiI.png) So I'd just like to confirm—is there anything obvious I'm missing that will make the above compile without having to resort to the while let? Thanks!
wow `extend_from_slice` to `copy_from_slice` increased performance by 70%. I'll have a look at removing `%` and using SmallVec. as /u/kaesos suggested, we can also use the type system to avoid the divide by zero panic.
Not sure I'm following — can you elaborate?
Having a general DCEL implementation would be great (\`Geo\` wants it for lots of things). Spade actually has one too, I've just never gotten around to asking for it to be extracted out.
If you mean DCEL — maybe! Those vectors expose the same thing conceptually, just in the form of two number arrays — I adopted this from the JS version because it's way more efficient there, but perhaps I could refactor this to be more convenient or abstract it better in Rust. Will look into it!
The webrender based projects have a similar problem on macos. They build, but the coordinate system is wrong (in the same way for all of the projects) for some reason.
A 2^16 byte stack buffer is large but not unreasonably large. If your problem is the cost of zeroing things then it might not help you here (I don't know how well the optimizer can notice that you're not reading the zeros). But if your problem is the cost of allocation, it might help.
Yes. There are certain workloads when GC work. Precisely, when most allocated objects die young. If it is not true, then GC does not work well. Malloc/free works (almost) always.
Why do you need to peek? Are you trying to avoid printing out the last entry, or some other reason? In any case, I don't think you've missed anything obvious about getting that code to compile.
Not sure if this counts as an easy question, but I assume the answer is. If this is not the right place for a question like this I'm happy to take it wherever it is supposed to be. Ahead of time, thank you for taking the time to read this and answer. I want to design a program that has multiple "backends" or storage engines. Let's say for this example we want to store objects which have an ID and a name. We want to store them in 3 different backends with the ability to add more later, similiar to how many enterprise web apps support multiple databases: a local File backed backend, Postgres, and MongoDB. I'm simply using these three because they will have 3 different concerns. The user will have a config file that looks something like this: ```toml [backend] name = "file" [backend.config] filename = "/somedir/myobjects.json" ``` Where name will tell us which backend to load and somewhere in the code we will have a function which is a mapping of these human names to backend implementations. The `backend.config` section contains backend specific configuration that will be different for each backend. So above is a file backend configuration but one for say MongoDB would be: ```toml [backend] name = "mongodb" [backend.config] host = "localhost" port = 27017 dbname = "example" collection = "objects" ``` The implementation detail of how the backend works will be hidden behind a trait. The trait will be fairly simple so as an example: ```rust pub struct Object { id: String, name: String, } pub trait BackendTrait { // Connect to a database, open any files etc. fn init(&amp;self) -&gt; Result&lt;(), SomeErrorType&gt; fn add(&amp;self, object: Object) -&gt; Result&lt;(), SomeErrorType&gt; fn get(&amp;self, id: String) -&gt; Result&lt;Object, SomeErrorType&gt; fn remove(&amp;self, id: String) -&gt; Result&lt;(), SomeErrorType&gt; } ``` What is the best way to design these backend implementation structs and their config structs to avoid dynamic dispatch? Is it even possible to do so? I've read quite a bit about Trait objects and doing something like: ```rust fn load_backend(name: String, config: What would this type be?) -&gt; Option&lt;Box&lt;Backend&gt;&gt; { if config.name == "file" { return Some(FileBackend{}); } None } ``` Will pay a, steep in rust world, performance penalty for dynamic dispatch. I've also read that you could get around this using an enum: ```rust pub enum LoadedBackend { File(FileBackend), Mongo(MongoBackend), Postgres(PostgresBackend), } pub struct Backend { loaded: LoadedBackend } impl BackendTrait for Backend { // Connect to a database, open any files etc. fn init(&amp;self) -&gt; Result&lt;(), SomeErrorType&gt; { match self.loaded { File(back) =&gt; back.init(), Mongo(back) =&gt; back.init(), Postgres(back) =&gt; back.init(), } } // repeating the same match from above for each of these functions fn add(&amp;self, object: Object) -&gt; Result&lt;(), SomeErrorType&gt; { Ok(()) } fn get(&amp;self, id: String) -&gt; Result&lt;Object, SomeErrorType&gt; { Ok(Object{id: "test", name: "example"}) } fn remove(&amp;self, id: String) -&gt; Result&lt;(), SomeErrorType&gt; { Ok(()) } } ``` This of course has the downside of a lot of boiler plate. For handling the configuration loading and conversion piece I feel like I have a good answer using serde and enum of configuration structs. However for how to avoid dynamic dispatch once the implementation is loaded I feel like I don't have a good solution. If you made it this far thank you so much for reading. 
I have a multi project workspace, and I want to modify the release profile for a single subproject. I cant do this in its Cargo.toml, as (according to the docs) only the profiles in the root Cargo.toml are considered. How do I accomplish this?
Did some stuff with [x801](https://gitlab.com/nagakawa/rdx801/rdx801); not sure where to go next. No GUI or that kind of stuff yet. Still trying to figure out how to organise the codebase.
One thing having rules does is weeding out people who are against having rules. There's a specific type of person that will explode and/or run away and slam the door when faced with a code of conduct, and that correlates pretty highly with being a jerk. 
&gt; Support for Windows (#1374) is next on the list. We are looking forward to getting this in-tree ASAP. Thanks @zacps for making this happen! Woohoo! 
Are there even any large projects that don't have CoCs? I thought that is pretty much the standard nowadays? Are there "standardized" CoCs that you can just use if you create a new project? Like the MIT or GPL licenses for example?
I'm around to answer questions!
Awesome! Glad to see this finally hapenning, Alacritty seems like a perfectly usable terminal emulator now :) Good job to everyone involved !
Ya that would be nice. How can we connect outside this thread?
I think the [Contributor Covenant](https://www.contributor-covenant.org/) counts as that. I personally chose the Fuchsia CoC, which is a lightly tweaked version of the Chromium one, but that had more to do with the circles I was running in.
This might be a weird question, but - how's Alacritty on VRAM? 
Good for him! I wonder if that will lead to any changes in attitude in the c-programming space. 
We support wide characters, but there's currently an issue where the definitions in the Rust `unicode-width` crate don't match those returned by `wcwidth`. Emoji mostly work, but there's still a couple of bugs to resolve. It's possible to use without a mouse. All of the new scrollback actions (page up/down/clear/etc) can be bound to keys.
Lines of scrollback won't affect VRAM usage. The affect on VRAM within a single process is almost entirely dictated by how many unique glyphs you've displayed over time.
Wide character and emoji support still needs a bit of work. There's open PRs to improve this situation though and it's definitely planned to work on this. These are a few issues you could take a look at for tracking this: [1049](https://github.com/jwilm/alacritty/pull/1049), [153](https://github.com/jwilm/alacritty/issues/153) and [1295](https://github.com/jwilm/alacritty/issues/1295). It's definitely possible to use it without a mouse, just like every other terminal emulator. However there currently are no facilities which help out with using the keyboard exclusively (like a vim-like visual selection mode). 
\&gt; Does the CoC even/really matter? Are there people that are assholes, then you point them to the CoC and all of a sudden they start behaving super nicely? I can't imagine that being the case. Of course not, but it sends a clear message that they're not welcome, stopping enablers in their tracks and making those who are attacked feel safer.
* there's [some alternate registry support](https://boats.gitlab.io/blog/post/2017-10-28-alternative-registries/), maybe check how it progressed in the last year * https://github.com/rust-lang/crates.io/blob/master/docs/MIRROR.md has some information for running mirrors, but it looks like this is for an actual mirror (for latency/connection performances) rather than an in-house crates.io with partial/selective mirroring (as you seem to be looking for) * Maybe inquire with integer32, they had [a post about enterprise setups](https://www.integer32.com/2017/03/21/crates-io-for-the-enterprise.html) a year back, no idea whether they've done anything (http://www.integer32.com/2016/10/08/bare-minimum-crates-io-mirror-plus-one.html may also be of interest) 
Not all projects. A lot of the "old school", "hardcore", especially Unix-related stuff, still doesn't respect them. See Suckless, for example, or the kernel itself. 
Awesome! That's worth noting as a feature itself, imo
I wouldn't call Rust an 'old' language. It's all about using the right tool for the job! :)
GNU still doesn't have one and, if I have been reading the threads correctly (&lt;- dubious), probably won't.
Totally agree, but by my understanding, Linux, and Linus by extension, are massive players in c-programming space. I was therefore wondering if a shift in how Linux handles itself would have any wider impacts on the community around c programming. 
I figured out why the rustup-managed lldb does not work and I sent a patch to rustup; hopefully that will go in soon. I'll announce everything when it is finally working. Meanwhile I have been working on landing a patch to fix up the DWARF that rustc emits for enums. This week I plan to spend going down the rabbit hole of figuring out why this fails in travis but not locally. Feeling lost.
Nope, it's cultural. Haskell has no CoC and is probably the nicest community of all, apocryphically that's due to #haskell on freenode successfully establishing [these principles](https://wiki.haskell.org/IRC_channel#Principles): &gt; To maintain the friendly, open culture, the following is required: &gt; * Low to zero tolerance for ridiculing questions. Insulting new users is unacceptable. New Haskell users should feel entirely comfortable asking questions. &gt; * Helpful answers should be encouraged with name++ karma points, in public, as a reward for providing a good answer. &gt; * Avoid getting frustrated by negative comments and ambiguous questions. Approach them by asking for details (i.e. Socratic questioning), rather than challenging the competence of the writer (ad hominem). As the channel grows, we see a diverse range of people with different programming backgrounds getting accustomed to Haskell. Be patient and take satisfaction from spreading knowledge. Note that this isn't about "ops bannhammering people who don't adhere to it" but a core set of people acting such. Newcomers then liked the atmosphere, witnessed how more abraisive people got dealt with and the attitude -- and the creativity involved as what's actually happening can't be summed up in neat points -- became a force of its own. In philosophy, that's called the normative force of the factual. You can't teach a community aikidoing a troll [as gloriously as this](https://gist.github.com/quchen/5280339) just by drawing up a document. [Some more thoughts along those lines](https://rakudo.party/post/On-Troll-Hugging-Hole-Digging-and-Improving-Open-Source-Communities).
Very probably an advanced use case, but how is Alacritty's typeface support? Does it support ligatures? I'd like to use Fira Code in the terminal ^.^
You can take a look at [this](https://github.com/jwilm/alacritty/issues/50) issue for tracking of ligatures. Currently Alacritty doesn't support it but I'd love so see this happen.
We don't have support for ligatures, yet. We have it on our long-term roadmap to try out harfbuzz for this purpose.
It provides a basis for the project to say “we told you up front what the rules were” to make it easier to confront or remove toxic people. It has been applied in the Rust community and people have been effectively banned, though that application of the CoC is definitely arbitrary and biased. For instance, a while ago ESR wrote a post baiting the Rust community into thinking he was going to give them a big PR boost, and a lot of people bent over backwards to explain why he wasn’t automatically subject to the CoC based on his long documented history of being in flagrant violation of it, to the point of threatening mod actions against people who pointed out that history and asked for the CoC to be enforced. It wasn’t until his (entirely predictable) follow-up post flaming Rust that it was agreed that maybe he shouldn’t be welcome in the community. We’re human after all.
Thanks. Let me try your suggestion and get back
To make things abundantly clear, I don't believe that a mere CoC will change things, which I think is what you're getting at. Like with most behavior-related policies, a lot of this comes down to enforcement and how much mind share the code of conduct has in the community. You can have a CoC that encourages people to be welcoming and warm (as opposed to hostile), but it's up to community members to live it and promote that culture. I think the Rust community has done a fantastic job of promoting its values of compassionate collaboration. For instance, there have been some places where I wonder if moderators here on the Rust subreddit decided not to intervene so as to not be smothering, but by and large people who violate it have been promptly called out -- non-constructive criticism, inflammatory comments and posts, etc. get downvoted and it's obvious that it's not welcome in the community. To your point about theoretical jerks: &gt; Are there people that are assholes, then you point them to the CoC and all of a sudden they start behaving super nicely? Usually, if somebody insists, they get pushed out of this community. Plainly and simply, they don't stick around because they don't feel that they belong...which is fine, if the conflict is our CoC.
I already included the link in the OP ("If you examine the CoC changes"), but definitely an interesting link!
I have a question about the stl file format, but don’t know where to ask it. My easy question is "where do I ask it"
/u/i_am_jwilm, any plans to add daemon mode like urxvtd?
Personally, I've switched to having a dedicated job for clippy. Part of this was from when I was building clippy, to reduce my worst-case build time. Part of this is to make the errors in each of the builds stand out more due to less output per job. See https://crate-ci.github.io/pr/clippy.html
I think you're grossly overestimating the cost of dynamic dispatch here. Your method is going to perform IO! It has to talk to another process or to the file system, write data, etc. The cost of a single dynamic dispatch is completely negligible in comparison. Don't worry too much about performance until you have profiled your solution, the bottlenecks often aren't where you expected.
This may be a bit esoteric, but is there a plan to allow disabling altscreen? That’s the one thing that’s been keeping me on st. Some applications (such as vim) support disabling it, but others do not, so it’s handy to have the terminal disallow it outright.
OK that's how I see it too. I - as a hobbyist - am a minority here and I feel very welcome. So shout out to all the Reddit Rustaceans! :-) 
Any plans on using other backends, e.g. gfx-rs?
Neat! As someone who spends a lot of time in the terminal, I'm always looking to improve my terminal experience. I wasn't too familiar with Alacritty before this post, but checking the GitHub page it seems it's trying to be extremely fast (and is GPU accelerated!). I know it makes sense to benchmark scrolling in this post since the headline feature is scrollback, but to me one of the more pressing performance issues in terms of how it "feels" is input latency, tab switching, etc. Rather than dealing with gobs of data; dealing with a smaller amount of data really fast. When alacritty advertises itself as extremely fast; what are the kinds of measurements being used?
Will input lag (https://github.com/jwilm/alacritty/issues/673) be fixed any time soon?
This is amazing. I love the effort. Now we just need a 3D Delaunay triangulation implementation...
There is [an issue](https://github.com/jwilm/alacritty/issues/241) on github to track this. However I wouldn't expect to see something like this anytime soon. Currently there's not a lot of data Alacritty uses which can be shared between multiple terminal instances, so the use of a daemon mode is very limited. So if there is a way to make this beneficial, it will probably be added. But the benefit isn't quite straight forward with Alacritty's current structure.
It would definitely be possible, but what exactly would be the benefit here? A lot of applications like less, vim, tmux, weechat, etc. make use of the alternate screen buffer. Is the goal to have access to the normal scrollback history?
There is no need to couple the availability of unstable features (which is less about bugginess and more about unwillingness to commit to not making breaking changes) to nightly builds of the compiler. For example whenever they publish a stable compiler, they could build a second compiler from the same source code with unstable features enabled. Or they could just use a compiler flag (there is even an internal-use-only environment variable which can enable feature flags in the stable compiler). I consider this coupling a bad idea, and would prefer better tooling to control unstable features through cargo using a single compiler.
Awesome ! Is there any plans to support fallback fonts ? I'm waiting for this to use alacritty with [icons-in-terminal](https://github.com/sebastiencs/icons-in-terminal)
One of my primary workflows involves suspending the foreground application (i.e. &lt;c-z&gt;) and possibly referencing some text that was on the screen. This isn’t possible when an application used altscreen. It also helps keep flow because the shell prompt always appears at the very bottom of the terminal.
The main measurements being used are these presented in the benchmark. OpenGL + Rust help accelerate the throughput of the terminal emulator, they don't necessarily help much with the latency. We currently do not have any benchmarks outside of the ones presented in this blog post, there have been others who have compared the latency of terminal emulators though (see [here](https://lwn.net/Articles/751763/)). If you look at these benchmarks, Alacritty definitely doesn't perform poorly when it comes to latency. And there's probably still some stuff that can be done to improve it. So while we do look out to make sure that latency stays as low as possible, since we don't currently have a benchmark for it, it's not directly tracked. We have issues like [this one](https://github.com/jwilm/alacritty/issues/673) to keep track of potential latency issues and I'd definitely enjoy it if Alacritty would also come out on top when it comes to latency.
Great blog post. Hits all the high notes: clear intro, simple documentation, solid benchmarking methodology, welcome policy updates, tantalizing future direcitons, and a section for thanking contributors. This ought to be a template for all Regarding benchmarking, can you talk more about vtebench? In particular it's always a valid concern that any benchmarking tool written by someone with skin in the game will favor metrics that their own tool most excels at. Are there any places where you feel that vtebench has blind spots? Are there any aspects of alacritty's performance that you aren't yet satisfied with? Has anyone else ever written any sort of benchmarking tool for terminal emulators?
Does the glyph cache live in process memory? I mean, where do the +40MB compared to urxvtd go?
There's also https://github.com/w0rp/ale, which seemed okay.
The input latency shouldn't be noticeably bad, the issue you've linked is just used to track what's already good and make it even better. There [this article](https://lwn.net/Articles/751763/) which compares the latency of different terminal emulators and compared to the alternatives, Alacritty seems to perform fairly well. If you're experiencing serious input lag, you're probably running into a bug. I'll happily help troubleshoot if you want to open an issue on github. If you're just asking for when the latency of Alacritty will be ever further improved, then I can't answer that at this point. There are other issues which currently have a higher priority than improving the terminal latency, so it could take some time until this issue is resolved.
Do you know a good strategy of updating a delaunay triangulation when each point moves by a small amount? e.g. if the average triangulation edge is length 1, and each point moves by .1 or less. I think ideally, you would just move each point to its new location, keeping the existing edges, then just adjust edges as needed from that point. However, I don't know any library that supports that feature, and that still doesn't give the best order to do these updates. Maybe there's a way to move all the points, keeping all the old edges, then just somehow say to check and fix if the conditions are still valid? Basically I'm working on animating physics objects with their triangulation/voronoi shown, and want the cheapest way to get the next frame. Maybe it'll just be to recompute entirely, but was hoping for something cleaner. Thanks for the help! 
Yes! The font configuration has actually been a point we've wanted to improve for a long time now. There's [this RFC](https://github.com/jwilm/alacritty/issues/957) which proposes a better way to specify font configurations outside of the system font configuration. It's probably going to take a while until this proposal will be made into a reality, but improving the font configuration and allowing for nicer font fallback is planned!
I'm not aware of any terminal benchmarking tool which is similar to vtebench. The issue with benchmarking terminals is always that it's not necessarily straight-forward. With a terminal emulator like `Hyper` for example, the frontend is completely disconnected from the command itself. So the `time` command will return instantly, but you'll see output flying around for a long, long time after that. The goal of vtebench for Alacritty was mainly to track improvements within Alacritty. So it's important for us that it tracks the things Alacritty cares about properly. One area where Alacritty currently is not benchmarked in is terminal latency. Based on benchmarks other people have made, it looks fairly well, however this is one point where a tool like `vtebench` could still see some improvements. Also we're always open to accept benchmarks that maybe don't put Alacritty in the best light. So if you have some area where you feel like Alacritty is performing slowly, please let us know. The primary goal of these metrics is to benchmark Alacritty against itself, so having good metrics to track improvements is always nice.
Awesome work, thank you very much! I've been looking forward to scrollback support. We have recently added a "--show-output" option in [hyperfine](https://github.com/sharkdp/hyperfine) which allows us to do some basic benchmarks of terminal emulator speed: hyperfine --warmup 3 --show-output "find /usr/include" I get the following results: terminator: Time (mean ± σ): 403.8 ms ± 12.5 ms [User: 63.7 ms, System: 142.9 ms] Range (min … max): 379.4 ms … 412.5 ms kitty: Time (mean ± σ): 320.9 ms ± 9.6 ms [User: 78.4 ms, System: 159.5 ms] Range (min … max): 303.0 ms … 332.8 ms alacritty: Time (mean ± σ): 219.5 ms ± 5.5 ms [User: 65.9 ms, System: 152.2 ms] Range (min … max): 212.9 ms … 232.0 ms To get a proper relative comparison, we would need to subtract the time that "find" takes without printing to a terminal: Time (mean ± σ): 65.3 ms ± 1.5 ms [User: 20.9 ms, System: 43.9 ms] Range (min … max): 63.5 ms … 70.4 ms So in this case, alacritty is a factor of ~2 faster than my current terminal (terminator) and a factor of 1.7 faster than kitty.
Generally it seems like a bit of an odd thing to disable a terminal feature like this when applications make such heavy use for it. I'd probably look at alternative ways to do this first (xterm provides the scrollback history of the normal screen in the alt screen for example). However if there's no better way to do this and somebody has the time to look into it, I don't see a reason to outright block it. Your best bet is to just open an issue on github if you're interested. Once it's tracked, it's much easier to see how many people are actually interested and if it's an issue that needs to be addressed.
&gt; talk more about vtebench Sure! We needed a tool which could exercise various common scenarios in a terminal such as scrolling through a lot of text, scrolling through a lot of text in `tmux` or `vim`, or simply having a lot of random screen updates such as a `tmux` pane or `vim` with side-by-side splits. These scenarios cover a wide-majority of applications. With these benchmarks available from vtebench, we were able to iterate on performance in key areas leading up to release. There are two areas that we could use another benchmark in which would be the scrolling benchmarks, but with content that fills the screen, and random screen updates within a scrolling region. The latter is unlikely to reveal anything new, but the former may have some interesting results. &gt; Are there any aspects of alacritty's performance that you aren't yet satisfied with A few. There's an input latency issue that a few people have mentioned today. This isn't something I personally notice, and for how many users Alacritty has, not many people complain about. There's also an issue on macOS where, at large enough terminal size with small font size, render performance suddenly drops. We have a patch that fixes this on some systems but causes even more problems on MBPs with a dedicated GPU. It may be that we need to consider Metal or something on this platform long term since their OpenGL driver seems to just be bad :(. &gt; Has anyone else ever written any sort of benchmarking tool for terminal emulators? Not that I'm aware of, but I also haven't done a thorough search. Most of the benchmarks I've seen are around `cat`ing a large file or similar.
How is macos support? Tried it but some characters dont show up in terminal and some show up like "&lt;00e2&gt;&lt;0082&gt;&lt;00ac&gt;". Might nordic keyboard layout have something to do with this issue? Been looking into this project for a while now, nice work!
Hey, I'm @chrisduerr and added that feature for the purpose of benchmarking Alacritty! Hyperfine is a great tool and I've been looking into using it for setting up automated tests for Alacritty. It's really nice to have a simple way to do warmup runs and then get an average, instead of having to run `time` multiple times and put it into some massive excel sheet. So thanks for your work on Hyperfine! It's a neat tool.
I’m always looking for a better emulator! How is the battery usage? Does the GPU acceleration hurt battery life?
This is really huge, honestly. However, what the input latency issue? Has there been improvements regarding input latency?
i love this project. i have been using alacritty as a daily driver since the week it was usable (which must be about a year now?), and i love it. However, after using kitty for the last week, I noticed some features that would be lovely to integrate into alacritty (or perhaps making alacritty usable as a library so people can make alternate UI layout drivers), such as windowing, tabs, and custom terminal control codes, so that you can do stuff like modifying the clipboard as long as the terminal being used is capable of reading those characters (even over SSH) or displaying images without w3m and rendering at terminal rates. With alacritty's performance and data bandwith, displaying images would actually be much more usable than kitty, and kitty's python layer on top of the C layer is a bit slow on startup, so I think alacritty has the potential to be a much more modern terminal now that it seems almost all of the basics are in. 
Crossbeam channels have better performance and, IMO, design, such as unfailable sends. Also a bounded channel would be a good idea as an option. 
I just scribbled this morning. Didn't care about performance. &amp;#x200B; Thanks for your feedback. I'll update the snippet.
Unicode support still needs some work when it comes to things like colored emojis. The keyboard shortcuts are also constantly improving because of the great work from the winit/glutin developers. For some keys there are also bindings you can add to the configuration file to fix them up, you'll probably find those in github issues. Generally I'd say macOS support isn't worse than linux. I'm not a macOS user, but I'd say macOS just has different issues, instead of being a second tier citizen.
This may be a locale issue. Can you check whether `LC_CTYPE` matches the value in your regular terminal?
I don't really run a laptop on battery much myself, but I haven't noticed any reduction in battery usage myself. There are reports that Alacritty's battery usage on macOS isn't optimal, since it doesn't use the integrated graphic's card (see [this issue](https://github.com/jwilm/alacritty/issues/210)), however this is currently being investigated in an open PR to update to the latest glutin version.
This will depend on what you're doing. Because we use the GPU to draw, you'll have less demands on your CPU cores which can sometimes improve battery life. We also don't draw constantly like a video game; Alacritty only requests a draw if input has changed, and even there, we lean on v-sync to prevent unnecessary draws.
The short answer is D3, and the long answer is server-side D3 provided by Node.js, bundled as a middleman extension, and included into the markdown via ERB templating. I was thinking about writing a post about the charts. Would you be interested in that?
There are plans to split up Alacritty into a library and a frontend. You can take a look at the current efforts [here](https://github.com/jwilm/alacritty/pull/1023). People have already built amazing PoCs with Alacritty as a backend (like an asciinema clone!), so hopefully this will allow splitting up things for the different usecases people might have. There are always some complications with things like image rendering, since the only standard right now is sixel, which isn't optimal. And other things like a UI configuration menu or tabs might not be appropriate for mainline Alacritty itself. However there definitely will be some features that other terminal emulators already have which will be added to Alacritty in the future! If there are any specific things that aren't tracked on github right now, feel free to open new issues so we can see how many people care about them.
On the project homepage you wrote: &gt; Features like GUI-based configuration, tabs and scrollback are unnecessary. The latter features are better provided by a terminal multiplexer like tmux. What made you change your mind? 
My goal with Alacritty was initially to just improve my own experience with `tmux` because I didn't like how existing terminal emulators performed. After launching, the project received a lot of feedback, and I was exposed to new ideas (new to me, anyway) like tiling WMs which ultimately allowed me to abandon `tmux` at the cost of one feature in particular: scrollback. Now, Alacritty can serve both types of users equally well.
Can you use the git repo directly? [dependencies] rand = { git = “https://github.com/rust-random/rand”, branch = “master” }
Eric Scott Raymond. The sibling comment linked his Wikipedia page but the relevant bit to the CoC question is that he’s an open racist, misogynist, and homophobe with a lengthy history of provoking flame wars. I personally think that such an open established history in violation of the CoC *should* bar a person from the project (to the extent it can be enforced) even if they haven’t explicitly been awful in a project-specific context, for the same reason I don’t want a serial killer who only murders at night babysitting my kids during the day. The reddit mods are in charge here, the github admins over there and the forum admins over there. They have the powers granted to mods/admins on the respective platforms. I think they’re all basically the same people from the community team, but I haven’t looked recently. I’m certainly not an insider, but I’m personally aware of on the order of 5 people banned.
Are there plans for RtL support (like Arabic and Hebrew)?
Note: you actually want associative operations, not necessarily commutative (though it can help as well).
Yes, I use that pattern. It's what the Linux console does. I wonder whether it might be possible by running with a hacked terminfo definition that removes the relevant definition.
What I'm trying to do here is to check if a certain value in a matrix are equal to the if statement and then return the position of that element. I can do this with a for loop, but I was wondering if it was possible using iterators. When I try to use the code below, it keeps saying that it cannot move either i or j out of captured variable in an 'FnMut' closure. Why isn't it possible to use these values in the if statement, but I can return them. Additionally, is there a way to do this block of code without using any for loops? input.iter() .enumerate() .flat_map(|(i, row)| { row.iter() .enumerate() .filter_map(move |(j, col)| { if *col == max_values_row[i] &amp;&amp; *col == min_values_column[j] { Some((i, j)) } else { None } }) }) .collect(); &amp;#x200B;
I am using ale linter for my python stuff and JavaScript. However, I haven't yet found a way to lint rust from ale page. But, will look into it. Thanks and cheers!
You're not wrong, but even static/dynamic typing can depend on context. Rust is about as statically typed as you can get, but still allows you to opt-in to dynamic typing via `dyn Any`.
Thanks for the insight! I never really paid attention to stuff like this and I don't think I really will in the future, but it's interesting nonetheless. 
This isn't something I personally plan to invest time into, but I'm also not against it as long as it doesn't conflict with the project's primary values.
I'm actually having noticeable lag in Alacritty compared to some other terminals (st, konsole). I'll open an issue.
I don't have a good suggestion for this unfortunately, but I'd guess that rebuilding the whole triangulation from scratch isn't much more expensive than updating all the points in an existing one, so I'd try that first and see if it's good enough.
`rayon::spawn` does something very similar I think.
Not really practical if you use a crate that has another crate as a dependency. You would need to modify the entire dependency tree to grab each dependency from git.
We've seen some of our positions show up on [https://rustjobs.rs/](https://rustjobs.rs/j). I don't know who's maintaining it.
Yes but everyone calls that pass by reference for some reason
Yes! I'm always interested in a bit of data-viz. 
Will you add binaries to the repos so users won't have to compile it themselves? Like in ripgrep: https://github.com/BurntSushi/ripgrep/releases Right now it's a big barrier in usage.
It's not a bug exactly, it's just that they might be working with different unicode versions which might specify two different widths for the same glyph, or there might be new glyphs mentioned in a newer version.
We are planning on it!
Thanks. I don't have a hard requirement, just kinda a personal project for me (how high can I go?), so it's not a big deal.
The pattern's original is [`Vec::split_at_mut`](https://doc.rust-lang.org/src/core/slice/mod.rs.html#873-883). Your proposal can fit this pattern if `as_fixed_capacity` returns `(&amp;[T], FixedCapacityVec&lt;T&gt;)`. Since the `&amp;[T]` would be the owner of the contents of the `Vec`, `FixedCapacityVec` couldn't implement `Index&lt;Range&lt;usize&gt;, Output=[T]&gt;`.
Thanks! There have been a lot of people mentioning this, hopefully we can both fix existing bugs and improve the existing latency!
oh yeah, I just wanted to mention it since that's what i appreciate most from the subreddit (their suggestions and feedback). Code looks good. Made me realize that storing boxed functions is a lot easier than I thought (I hadn't had much need for it yet).
Wow that's lovey. Thanks for the info! Didn't realize it was already in the works. Are you a maintainer? You seem well informed.
I'm chrisduerr on github, so I try my best to help out when possible. :)
Thank you so much! How's hiring going so far? I'm wondering whether people are taken aback by having Rust in a job description, or whether they're looking forward to it.
You can use source replacement, described here: https://doc.rust-lang.org/cargo/reference/source-replacement.html The `cargo-vendor` tool will grab your specific dependencies into `vendor/`, suitable for the "directory sources" form of replacement.
Last week I released a new version of [tarpaulin](https://github.com/xd009642/tarpaulin) no new features but some bug fixes and dependency updates (which also moved it to a nightly only project). This week I hope to start looking at some stuff in rust-lang/rust and maybe fix some more tarpaulin things. Might be too busy with other stuff
Congratulations on the release. I've been using Juniper recently and have found it satisfyingly functional and low-fuss, so, thanks!
Doesn't the example fail the borrowchecker in the same way as the original? You have to have a mutable reference to the buffer as well as a reference to the slice, which the compiler doesn't know won't alias. Perhaps there should be a method split_at_mut(buf: &amp;mut FixedCapVec&lt;T&gt;, pos: size) -&gt; (&amp;mut [T], &amp;mut FixedCapVec&lt;T&gt;)
Got off to a flying start - needed to write a small agent that collects system performance data and reports them to influx DB to be viewed in Graphana. DB integration only took 1/2 cup of coffee - I feel that Rust has given me new superpowers :-)
I for one would be super interested in reading your blog post. Your previous one about ECS UIs was cool and I'd be curious to see how that plays into your windows ui
Sure! Although I think we can all agree Electron is definitely not the right tool for a terminal emulator.
I've been using alacritty as my daily driver for a good 6 months. Fantastic program. My only comment is be careful of featuritis. Everybody is gonna want all the things that everything else has. You don't have to do that. Keep the core principle of the thing intact: a tight, fast program that displays characters.
This isn't something any TE can implement efficiently today. [notty](https://github.com/withoutboats/notty) aims to address this issue (among others) at the protocol level and enable TEs to implement panes efficiently.
That's definitely the goal! This why we're making efforts to separate Alacritty's core into a library, so that other people can provide different frontends if they care about different things than the Alacritty project itself, but still want to make use of some of its benefits.
Oh, that's really cool! That's a very different approach than I've taken. I'm specifically interested in leveraging the web browser as an SVG renderer as to avoid scaling issues between desktop/mobile/etc. As such, my tooling simply _emits_ SVG rather than rasterizing it.
That sounds like an excellent idea! Something similar [has just been suggested](https://www.reddit.com/r/rust/comments/9gdcyp/prerfc_fixedcapacity_view_of_vec_may_reduce_use/e65cb2b/), but your way is better.
I'm completing refactoring of [CDRS v2](https://github.com/AlexPikalov/cdrs) (Cassandra driver). The refactoring is related to a problem of proper connection pooling in multi node Cassandra clusters. Initially it wasn't quite clear for me what should be a solution that would guaranty proper load balancing along with connection pooling. After some consideration we've figured out that the solution that could provide such load balancing guarantees is following -- instead of returning a plain connection a load balancer should return r2d2 Pool of such connection. Further next available connection will be used by the driver to actually perform a request. This refactoring allows CDRS v2 to be used in multi thread applications.
It might covers much more than you're interested in, but a guy on youtube who posts live-coding sessions in Rust put up a video talking about his full development setup which includes vim. [Here's the link](https://www.youtube.com/watch?v=ycMiMDHopNc)
Bz
It works fine with Visual Studio 2017 and C++/C. That's very interesting.
This is interesting! I've been using a tiling WM for a long time, and once I started using Alacritty, I was "forced" to use tmux in order to get my scrollback buffer. But once I started doing this, I _loved_ it. When every terminal is always a tmux session, I can always reattach to any of them, even when I'm remote. So for example, when I ssh into my home machine from work (or vice versa), I can always hop back into any terminal that's open because they are all in tmux sessions. I can't imagine going back now!
I've always liked having tighter horizontal spacing :)
urxvt would be a good one to compare with. I stopped using Terminator because VTE was just ridiculously slow on a full-screen terminal while urxvt barely even seemed to notice.
&gt; Alacritty had trouble calculating the cell width for some fonts, so the offset could be used to correct that problem. :D So, a kludge-turned-feature? I love that =D It's not just 'nice to have', it allowed me to go from 14pt font to 16pt without sacrificing rows and columns. I'd say it's more of a hidden gem.
You could try searching or asking on StackOverflow under [the `stl-format` tag](https://stackoverflow.com/questions/tagged/stl-format). It's low-activity but if nothing else it's possible your question might have been asked and answered already.
Same here. In my experience, ALE does a great job of "just working" with compiler/linter binaries in your path.
https://crates.io/crates/tokio-zmq/reverse_dependencies is the best way right now. I agree that it would be great to eg. have a mailing list to be able to reach your users. Part of the reason why I always create a gitter channel for my project, and mention it all over readme and documentation: this way some people will join, and it's possible to get some feedback sometimes.
memory mapping will also give you a kind of "streaming" on large files. On Linux you can add calls to madvice() ( C FFI ) to both enable sequential read ahead, and free pages that you have already parsed. On spinning disks, this will also give you better IO performance, since the page requests utilise NDQ ( Native Device Queues ) - and data will be fetched with less time wasted on moving the heads around.
Here are the plugins I'm using: - [YouCompleteMe](https://github.com/Valloric/YouCompleteMe) for autocompletions and jump to definition - [ALE](https://github.com/w0rp/ale) for linting with cargo - [vim-rust](https://github.com/rust-lang/rust.vim/), which I use pretty much exclusively for `rustfmt` integration I think RLS supersedes this functionality, but I like this setup because YCM and ALE are generally useful for a variety of programming languages.
Ah OK. Do you agree that I'm not actually casting a function pointer here though?
Unfortunately I've just realised a function with that exact signature isn't possible; since presuming FixedCapVec is a struct with a ptr, len, and cap that is conceptually a vec with an extra invariant, then such a method must compute a new ptr, len, and cap which it can't just create a &amp;mut to of the correct lifetime. It works for slice::split_at_mut because slices are fat pointers that keep track of their length One solution I can think of is to make FixedCapVec take a lifetime parameter, making it conceptually a &amp;mut Vec with an extra invariant instead, replacing each &amp;mut FixedCapVec&lt;T&gt; above with FixedCapVec&lt;'a, T&gt;
Pretty sure you want /r/playrust. 
Tell me something, do you think your recoil scripts could help with "[Crate Squatting"](https://internals.rust-lang.org/t/crates-io-squatting/8031)?
Thanks! I'll try to edit it to remove the personal hand-wringing about directions for the project, and focus on the progress update and some technical details.
You're capturing `max_values_row` en `min_values_column` in your `move` closure. Try your original code, but insert these two lines before it: let max_values_row = &amp;max_values_row; let min_values_column = &amp;min_values_column; Also you can change your second closure to `move |(j, &amp;col)| { if col == ...` if `col: Copy` (which I suspect is the case, if it is a number).
Yes! I missed that what is being cast is a pointer-to-function-pointer and not the function-pointer itself. I'm quite sure this is fine and gcc [doesn't complain](https://godbolt.org/z/-_ZvfJ).
Does anyone audit that repo? If not, what's the point of having a mirror? To make things faster? 
i3-gaps on /r/unixporn got to you didn’t it? 
In the US you can find a few companies. In europe there are very, very few rust jobs. You might want to take a look [here](https://www.rust-lang.org/en-US/friends.html), but keep in mind that most of these only use rust in some specific component or internal tool.
Wow I didnt know that. That probably takes a while :-)
I’m using it with tmux everyday (without scrollback) and I’m loving it ! Thank you !
Nice, great. I did find a different instance of UB: https://github.com/BurntSushi/rust-memchr/pull/36 I was lucky and got a seg fault on a 32 bit machine.
Thanks for that.
Continuing to work on my game. Had a lot of headaches with `gilrs` on Windows 7. Ended up using sdl2 and finally got things (2 xbox 360 controllers at the same time) working nicely last night! * `gilrs` depends on the `winapi` crate for xinput. * `winapi` uses Windows 8 (xinput1\_4) which is not really backwards compatible with Windows 7. * Consequently I couldn't get multiple controllers to function simultaneously. Otherwise I would have really liked to work with `gilrs`. It looks like a solid library.
Why does the `push()` method of `std::path::PathBuf` not return the mutated `PathBuf`? It seems like it would be convenient to be able to chain `.push()` calls. Is it something to do with taking ownership of the value?
This is actually implemented in `specs` in form of `DenseVecStorage` and it's the preferred general storage: It's only slightly slower than straight `Vec` based `VecStorage` when it's filled and faster/uses usually less memory when it's only partially filled.
This explains it perfectly: http://www.ameyalokare.com/rust/2017/10/12/rust-str-vs-String.html
That's an interesting mockup. I was thinking of keeping a mutable borrow of a Vec in the struct, dereferencing it into a mutable slice for most function implementations so we know it won't reallocate, and updating the length of the parent Vec on drop. This would require FixedCapacityVec to also have an immutable field that stores the branch-off point from the parent Vec so that it could convert its modified length back into the parent Vec length and update it properly.
I really love this project, thank you so much for all your hard work. When (if ever) will I be able to `cargo install alacritty`? Or are you just name-squatting to prevent confusion? All of alacritty's benchmarks are oriented around throughput, but I'm rather curious about power consumption, or benchmarks (like CPU time used) that would be indicative of power consumption. Do you/the project have any plans for that?
I think the more important part is that this is something easy to get wrong with unsafe, *that has a simple safe abstraction*. The two ways to reduce the amount of unsafe code are: * Make the borrow checker better at reasoning * Introduce more safe abstractions Because this is something that has come up multiple times I think it is a good candidate to expand the amount code that doesn't need unsafe.
Honestly, vim does not make installing plugins easy. I've moved to Atom with the vim and rls plugins, and I didn't need to open a single config file or leave the editor to install everything.
A `String` is a standard library type that owns and manages a heap-allocated buffer of UTF-8 encoded data. This buffer is able to be mutated and reallocated so that text can be added or removed from the `String`'s buffer. And when the `String` type goes out of scope, the data that it manages goes away and is no longer accessible. Meanwhile a`&amp;str` is known as a "string slice". It's a non-owning reference to UTF-8 encoded data that lives *somewhere* in the program. That *somewhere* could be on the stack, on the heap, or in static memory. And since the `&amp;str` doesn't own the UTF-8 data that it points to, you have limited to no ability to modify that data through the reference.
&gt; and, IMO, design, such as unfailable sends Author of `crossbeam-channel` here. Plenty of people have complained about sends not detecting dropped receivers anymore. There are good arguments both ways: for fallible and infallible sends. The main obstacle to fallible sends is probably the fact that it's difficult to model `select!` that properly handle the case when a send fails due to the channel being closed. Now I wonder whether we could return back to fallible sends and model `select!` like this: ``` select! { res = recv(r) =&gt; match res { Ok(msg) =&gt; println!("received {}", msg), Err(_) =&gt; println!("channel closed"), } (res, r) = recv(rs) =&gt; match res { Ok(msg) =&gt; println!("received {} from {:?}", msg, r), Err(_) =&gt; println!("channel {:?} is closed", r), } res = send(s, foo) =&gt; match msg { Ok(()) =&gt; println!("sent message"), Err(msg) =&gt; println!("couldn't send {}", msg), } (res, s) = send(ss, foo) =&gt; match msg { Ok(()) =&gt; println!("sent message into {:?}", s), Err(msg) =&gt; println!("couldn't send {} into {:?}", msg, s), } } ``` I'm feeling tempted to try something like that. What do you think? :)
Sorry of this seems noob, but what do you mean when you said you didn’t like how well terminals “perform”? 
Chapter 4 of the book covers this in depth.
There are some crates that have been around since `crates.io` went online and have never contained any code. Take [a](https://crates.io/crates/a) for example. It has had 1,325 downloads since Nov 2014. That puts some kind of upper bound on the frequency of bot downloads, pretty close to averaging one per day actually. More than that then you probably have real users :)
I'm working on a project that allows you to describe the high level of a REST service in a TOML file, and then a full API for it is then generated. I have a lot of APIs I need bindings for, including a lot of fairly unknown / unpopular services. APIs for the types of companies you'd never heard of. I don't fancy writing them all by hand.
&gt; made me want to write an x11 window manager I had this feeling once. I spent the next three years of my life building one, with two very costly false starts. It was a dark time. It has a happy ending though. I've been using the result of that for over 6 years now!
Hey, thanks for a great tool. I use it for most of my own Rust projects. Regarding that update: I don't know if you specifically follow your own versioning policy, if so the next would not be useful to you. But if you don't, the [RFC 1105](https://github.com/rust-lang/rfcs/blob/master/text/1105-api-evolution.md) has some versioning guidelines/policy. In your update I noticed you didn't bump the major version of tarpaulin when requiring nightly, which this RFC suggest you "have" to. Also, I basically copy-paste your Travis config from the README, but currently I haven't gotten it working correctly. Could it be that `# cargo tarpaulin --out Xml` shoudl actually be `# cargo +nightly tarpaulin --out Xml` or is it that your README assuem nightly is the default toolchain?
Tmux is definitely super powerful! But I think for people that do not remote into other machines a lot, there's still reason to go without. I personally appreciate the simplicity a lot of not having to bother with tmux. It's cool though to see that sometimes being forced into a specific workflow can actually show some new insights you wouldn't have discovered on your own.
hell yeah
&gt; source replacement is not appropriate for situations such as patching a dependency or a private registry. Cargo supports patching dependencies through the usage of the `[replace]` key, and private registry support is planned for a future version of Cargo.
Cool! This implementation is probably better than mine since many methods on vec can just be implemented by delegating to the underlying buffer after doing any checks necessary. Though your issue does demonstrate that keeping the non-reallocation invariant could be tricky
I use Alacritty + XMonad right now and it works great. Glad to hear you've been introduced to the world of tiling window managers :p
very nice .. [rustine](https://duckduckgo.com/?q=rustine&amp;atb=v119-3__&amp;iax=images&amp;ia=images)
&gt; Must vte sucks at emoji and east Asian languages. Not to mention dependent vowels...
Making Alacritty available on crates.io has definitely been planned. It's just that there are some dependencies which can't easily be uploaded to crates.io, so that's been holding us back for now. Regarding different benchmarks it's always interesting to see what other people come up with. I don't think battery consumption would be necessarily a priority of Alacritty when the alternative is a performance improvement, however it's impossible to keep track of things without a way to measure them. So even with things that might not be #1 priority for Alacritty, it would be nice to track them just to make informed trade-offs. If you have any ideas of terminal benchmarks that could maybe even easily be automated, please feel free to open an issue on github. It's always appreciated and nobody's going to bite. :)
URxvt has some very interesting dirty tricks to improve performance. Alacritty's speedup in scrolling speed with this PR was because we looked into the way URxvt handles this. Even though URxvt might not be the fastest when rendering and it sometimes skips steps completely, it definitely has some tricks in its back that people can learn from. :)
no, I didn't use unsafe. I've updated the code a bit: #[macro_use] extern crate criterion; //extern crate smallvec; //use smallvec::SmallVec; use criterion::Criterion; fn extend_rle(buffer: &amp;mut Vec&lt;u8&gt;, repeating_fragment_len: usize, num_bytes_to_fill: usize) { //clone the fragment let mut fragment = vec![0;repeating_fragment_len]; fragment.extend_from_slice(&amp;buffer[buffer.len()-repeating_fragment_len..]); //allocate required memory so there aren't re-allocations buffer.reserve(num_bytes_to_fill); // calculate number of full reads, and the bytes in the incomplete read let repeats = num_bytes_to_fill/repeating_fragment_len; let remainder = num_bytes_to_fill - ( repeats * repeating_fragment_len); //repeat extension with full blocks - does nothing when num_bytes_to_fill is zero //panics when repeating_fragment_len is 0 //writes up to repeating_fragment_len*floor(num_bytes_to_fill/repeating_fragment_len) // == num_bytes_to_fill - (num_bytes_to_fill % repeating_fragment_len) for _count in 0..repeats { buffer.extend_from_slice(fragment.as_slice()); } // write the remain bytes to the buffer // num_bytes_to_fill - floor(num_bytes_to_fill/repeating_fragment_len) // == num_bytes_to_fill % repeating_fragment_len buffer.extend_from_slice(&amp;fragment[0..remainder]); } fn copy_rle(buffer: &amp;mut Vec&lt;u8&gt;, repeating_fragment_len: usize, num_bytes_to_fill: usize) { //clone the fragment let mut fragment = vec![0;repeating_fragment_len]; fragment.copy_from_slice(&amp;buffer[buffer.len()-repeating_fragment_len..]); //allocate required memory so there aren't re-allocations buffer.reserve(num_bytes_to_fill); // calculate number of full reads, and the bytes in the incomplete read let repeats = num_bytes_to_fill/repeating_fragment_len; let remainder = num_bytes_to_fill - ( repeats * repeating_fragment_len); //repeat extension with full blocks - does nothing when num_bytes_to_fill is zero //panics when repeating_fragment_len is 0 //writes up to repeating_fragment_len*floor(num_bytes_to_fill/repeating_fragment_len) // == num_bytes_to_fill - (num_bytes_to_fill % repeating_fragment_len) for _count in 0..repeats { buffer.extend_from_slice(fragment.as_slice()); } // write the remain bytes to the buffer // num_bytes_to_fill - floor(num_bytes_to_fill/repeating_fragment_len) // == num_bytes_to_fill % repeating_fragment_len buffer.extend_from_slice(&amp;fragment[0..remainder]); } fn blog_func(buffer: &amp;mut Vec&lt;u8&gt;, repeating_fragment_len: usize, num_bytes_to_fill: usize) { buffer.reserve(num_bytes_to_fill); // allocate required memory immediately, it's faster this way for _ in 0..num_bytes_to_fill { // byte_to_copy variable is needed because buffer.push(buffer[i]) doesn't compile let byte_to_copy = buffer[buffer.len() - repeating_fragment_len]; buffer.push(byte_to_copy); } } fn extend_harness() { let mut v = vec![1u8, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]; let repeat_len = 5; let extend_len = 15; extend_rle(&amp;mut v, repeat_len, extend_len); } fn copy_harness() { let mut v = vec![1u8, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]; let repeat_len = 5; let extend_len = 15; copy_rle(&amp;mut v, repeat_len, extend_len); } fn blog_harness() { let mut v = vec![1u8, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]; let repeat_len = 5; let extend_len = 15; blog_func(&amp;mut v, repeat_len, extend_len); } fn criterion_benchmark(c: &amp;mut Criterion) { c.bench_function("extend", |b| b.iter(|| extend_harness())); c.bench_function("copy", |b| b.iter(|| copy_harness())); c.bench_function("blog", |b| b.iter(|| blog_harness())); } criterion_group!(benches, criterion_benchmark); criterion_main!(benches); So with this, extend_rle takes ~150ns, copy_rle takes ~79ns and the blog version takes ~76ns. But increasing `extend_len` to 5000 gives extend_rle time of ~7.5us, and copy time of ~2us on my machine, hence the speedup you quoted ( and the blog_rle is about ~8us).
With linux terminals like URxvt for example performance isn't even that much of a problem. However making use of macOS terminals without any Metal support can lead to some slow behavior in applications like vim. I believe this was part of the initial motivation for how other terminals perform and I've seen a lot of people on github praise how Alacritty has resolved this exacty issue. With linux terminals like URxvt this might be less of an issue. Alacritty is still faster, but the margin is definitely smaller. However Alacritty is not only speed, it also has a few other things to offer AND it's still super fast. I'd always just recommend to give it a try. :)
You [certainly can](https://www.shallowsky.com/linux/noaltscreen.html). Edit to remove/mangle `rmcup`.
I am unsure about Elasticsearch memory consumption, so I am not comfortable talking about it. I can talk about tantivy. To give an idea, searching a pre-build Index for the English wikipedia without stored document would work perfectly with 3GB of RAM. That's the size of the index. For indexing it consumes the budget of RAM you give it. If you need a very high throughput and use many threads for indexing, you might give it 1 or 2GB. For smaller usage, &lt;500MB should be ok. Merging will also require more RAM. Ideally you would want to have indexing on a different server. 
Costly false starts? You’ve hooked me for a story like an old grizzled sailor :)
https://github.com/BurntSushi/wingo "maintenance mode" basically means "it keeps working for my wife and I." :-)
Your quote doesn't seem to be what they're asking for here, but this earlier bit is: &gt; Cargo supports the ability to replace one source with another to express strategies along the lines of mirrors or vendoring dependencies.
Hah. The year was 2009, and my WM of choice back then was Openbox. I had experimented with others, particularly xmonad, because I really liked tiling. (I can't remember what state awesome and i3 were in back then. It's fuzzy, but I do remember trying i3 at one point.) But I also really liked to keep the tradition "stacking" mode in a lot of cases too. So my first foray into X was to provide a "tiling" layer that sat on top of Openbox called PyTyle: https://github.com/BurntSushi/pytyle1 (It was initially released on sourceforge!) I used that for a bit, but once I moved on to grad school and got a stable income, I could afford to have more monitors. I had been using 2, but I wanted 3, and the new AMD graphics cards with Eyefinity support would give it to me. There was a major problem with it though: Openbox's workspaces weren't "per monitor," so every time I switched to a different workspace, it would switch workspaces across all three of my monitors. I dealt with this when using two monitors, but it was too much for three. I really really wanted to have independent workspaces that I could switch to and fro from monitor to monitor. Nowadays, I understand that this is a more common feature (it's even on Macs AIUI), but it wasn't back then. So I started maintaining a patch set on top of Openbox called [Openbox Multihead](https://github.com/BurntSushi/openbox-multihead), which basically was one giant hack to make Openbox's workspace independent like I described above. It worked for a time, and I can't now recall what the specific issue was, but I remember having a helluva time keeping the state managed inside the WM because it just wasn't built to handle independent workspaces. Not to mention that having multiple workspaces visible at the same time violates constraints imposed by the [EWMH](https://standards.freedesktop.org/wm-spec/wm-spec-1.3.html). And in particular, they were violated in a way that caused most extant pagers and toolbars to stop working. I consider this false start #1. False start #2 is when I started to write my own WM, but in Python. I called in [pyndow](https://github.com/BurntSushi/pyndow). It never got into a fully functioning state. IIRC, I had a lot of trouble structuring the code, and I ran into performance problems. It might sound weird, but when you're building a non-compositing WM, it actually needs to be fairly snappy in order to respond to events generated by users, otherwise it's pretty easy to notice the lag when, say, resizing or moving a window. pyndow was and _will be_ the last big project that I attempt in my free time in a unityped language, that's for damn sure. I did not want to write a WM in C. It seemed like a giant pain in the ass. So I picked Go instead. This was right around the time that Go 1 was released. I liked the idea of having a pure Go WM, so I built up the entire [X client stack](https://github.com/BurntSushi/xgb) in pure Go, in addition to [convenience routines](https://github.com/BurntSushi/xgbutil) that implement ICCCM, EWMH and various other things, such as maintaining X's event loop. This laid the ground work for [Wingo](https://github.com/BurntSushi/wingo), which is what I use today. It has pretty much just worked for me since I built it, with maybe an average of at most one bug per year that I personally stumble across and fix. The WM is a hybrid stacking (like Openbox) and auto tiling (like Xmonad), with per monitor workspaces and the ability to dynamically create workspaces on the fly. It's kind of crappy and non-modern in various ways, but it works.
If you have to push multiple things at once, it's more idiomatic to call `.extend()` (this is true for many container types, not just `PathBuf`): let mut path = PathBuf::from("/home"); path.extend(&amp;["user", "app_data"]); println!("{}", path.display());
Wow, I was curious and googled this, and I am shocked to discover [no less than 5 terminal emulators built on Electron](https://www.google.com/search?q=terminal+emulator+electron).
Wow are access times faster for retrieving an element from a vector? How can you get faster than base address + index = location?
Thanks, I will go back and read that chapter. I am reading the book now. Almost finished. Seems like I need to read it again though.
If you ask me, `Vec` isn't spelled "vector", so I'm happy not conflating the two concepts.
Rust doesn't actually call it a vector though, it only alludes.
Doesn't it depend on what you want to happen? Sometimes you want to delete a thing but keep it around to do cleanup later. Sometimes you want to delete it, but allow others access, just signalling them that it should be deleted. Sometimes you need it gone and everything that references it. Sometimes you need it gone and nothing that references it. And many times you need multiples of these at the same time!
This is a good question because performance can mean a number of things. For me, it's a combination of being able to process a large amount of data in very little time (tailing a production log, for instance), no drawing lag in a busy tmux/vim session, and 60 FPS at all times. You may have experienced it before where your terminal keeps drawing after you stop sending commands or after you've requested it to stop tailing a log. Alacritty solves these problems.
CPU benchmarks would actually be really nice to have, and it's not something I've considered before this. Regarding `cargo install`, this will likely happen at some point, but there's some non-crates.io dependencies preventing this right now.
Honestly, I do miss a number of those features from tmux, and I still use it occasionally when I need them. tmux for me has almost exclusively become the multiplexer of the remote world. It's become a signal for when I'm connected to another server vs running a local shell. The problem that drove me away was actually my aggressive use of panes, and this let to perf problems. This was improved by Alacritty, but it still isn't the same as native. Simply leaning on i3 for splits/tabs/etc, and tmux for remote multiplexing has become a happy medium. tmux really is a wonderful tool, and it's exciting to hear that people like yourself have embraced in part due to Alacritty. Thanks for sharing!
I'm writing a FUSE library at https://github.com/jmillikin/fuse.rs, with the goal of covering all FUSE protocol versions in a unified request/response API. So far it's going pretty well, last weekend it got far enough to start building on Travis. This is the first Rust code I've ever written so I was expecting some struggles, but the Rust part has gone very smoothly (FUSE itself is ... not smooth).
[Yep!](https://github.com/jwilm/alacritty/issues/673)
What is an Impl Day? (I'm not in the area then but I'm curious.)
Great advance
I have been waiting for this since you first announced Alacritty! I've just spent the last 30 minutes installing Alacritty and tweaking my config, and I love it. Thank you!
We audit for licences, but primary purposes are: - reliancy against failures in crates.io. Besides, relying on crates.io introduces a circular dependency in critical, customer-facing infrastructure. - easier integration with our internal build and deployment systems. They're _highly_ opinionated and reliable.
Nice, but the complexity of the algorithm is missing. From a quick read, it seems to be in O(n^(2)).
No. If you're returning a reference, it has to be to something that already exists. If your code *did* compile, you'd be returning a pointer to a value that stops existing as soon as the function returns.
rustfmt on save is a game changer for me in vim.
thanks, I have fixed it using a new trait. trait AsRefType&lt;'a&gt; { type Output: 'a; fn as_ref_type(&amp;'a self) -&gt; Self::Output; } impl&lt;'a&gt; AsRefType&lt;'a&gt; for Foo { type Output = FooRef&lt;'a&gt;; fn as_ref_type(&amp;'a self) -&gt; Self::Output { FooRef { a: &amp;self.a, b: &amp;self.b, } } } I am wondering whether there are some traits defined in `std` instead of one defined by myself?. I think this is a common usage scenario.
I'd look into the relationship between `Path` and `PathBuf` for an example of how you might implement this kind of pattern.
I‘m using alacritty for irc since quite a while since it properly supports the urgent-hint while xfce4-terminal doesn’t. Awesome work! What’s mostly missing in my workflow is being able to click links, I have to manually copy and paste them into a browser. I got used to it, but I would still like to just ctrl+click them.
I should have updated the example Travis to be nightly only. And yeah I probably should have upped the minor number but as cargo doesn't publish lock files by default everyone wants going to be forced to nightly with rust 1.29 anyway regardless of tarpaulin changes
I wonder what the performance difference is between dynamic dispatch through a trait object and manual dynamic dispatch by matching on an enum as in the code above... wouldn't be surprised if it was about the same.
I have a similar "problem" with my first crate. I wrote it as a simple way to work around an ownership problem I had and it's one of my most popular crates now (for some reason). https://crates.io/crates/mrsc I sort of see how it is probably used, but nothing was ever published and I usually remove it from my own projects during a refactoring as well. I've considered redesigned the api a couple of times (I learned a lot since I published this), but it seems to work for whoever is using it.
What version of Iterm did you use for the benchmark? Iterm2 just released GPU rendering into stable in v3.2.
No, it’s O(n log n) in average.
Frankly, I've been using Rust since 1.0 and I still don't know what `for&lt;'a&gt;` does, besides make me reconsider my design decisions. (Don't tell me what it does, or else I won't be able to use that joke anymore!)
`barh` emits SVG too. The main problem is a lack of a good text shaping library (like harfbuzz) for Rust.
I haven't been following Rust 2018 that closely. How are they depreciating `extern crate` and what are they replacing it with? [The book](https://doc.rust-lang.org/book/2018-edition/ch02-00-guessing-game-tutorial.html) still has the old syntax.
They are replacing it with.. Nothing. The thing is that it's effectively duplicating whatever is in Cargo.toml, and the only thing it's being used for is the `#[macro_use]` attribute, which in the future will be unnecessary since macros are gonna be imported with `use` statements.
https://stackoverflow.com/jobs/developer-jobs-using-rust
I don't understand what you're trying to do here. Why do you want a reference to a struct containing refs to the elements of another struct? I would probably implement `as_ref_type` as a method of `Foo` and call it a day. I'm not seeing what the trait adds. &lt;https://play.rust-lang.org/?gist=d62585f837a5ab2a25a2c3c60bc5446a&gt;
I am currently using zeromq for message passing between Rust and Java using Apache Camel for background rendition processing. I'd love to convert the rust part into async and use `tokio-zmq`. Is there any reason that the crate is licensed as GPL instead of the standard MIT/Apache-2.0? This alone would make it very hard to use in any project currently, but would understand if this is something you wouldn't want to change. 
Thanks for the reply, I think I understand how it works now. Referencing those variables instead of capturing them also makes a lot more sense.
No wonder I couldn't find anything... Thank you!
I found out it use unsafe code to do this. Under the hood,`PathBuf` use `Buf` and `Path` use `Slice`: #[derive(Clone, Hash)] pub struct Buf { pub inner: Vec&lt;u8&gt; } pub struct Slice { pub inner: [u8] } impl Slice { fn from_u8_slice(s: &amp;[u8]) -&gt; &amp;Slice { unsafe { mem::transmute(s) } } } impl AsRef&lt;Slice&gt; for Buf { fn as_ref(&amp;self) -&gt; &amp;Slice { Slice::from_u8_slice(&amp;self.inner) } } But I can not define `FooRef` as : struct FooRef { a: [u8], //error since only the last field can be dynamically sized type b: str, }
Pretty sure it's not maintained currently. I've never seen any open jobs there and recently trying to post a new position I received no response whatsoever.
At the moment it's less than a day to run `cargo check`, and a few days to run `cargo test` on all of them. This is on a pretty big AWS machine.
I imagine it depends on the number of choices in the `enum`, but in the 'store data to some backend' scenario described above I suspect one would need advanced statistics to even be able to detect the difference.
Thanks for posting this link (&amp; also the link in that comment - excellent resource)
You can use this trick to return one reference only. Do you really need to return two? Why not implement `AsRef` twice for different parameters?
Ok makes sense, thanks.
because there are `Bar`, `Bee`, `Boo`... also implement `as_ref_type` to `FooRef` like: struct Bar { a: Vec&lt;u8&gt;, b: String, other_field:..., } struct Bee { val:Vec&lt;u8&gt;, // serialized form. } I want handle them all: fn do_something&lt;'a, T&gt;(foo: &amp;'a T) where T : AsRefType&lt;'a, Output=FooRef&lt;'a&gt;&gt; + 'a { let foo_ref = foo.as_ref_type(); //... use foo_ref here.. }
I have an `Image` data structure and I want to make a custom reference on it that corresponds to a subset of the `Image` (like string slices but 2d): struct ImageView&lt;'a&gt; { image: &amp;'a Image, // position, width and height } What I'd like to have is an implicit conversion from `&amp;Image` to `ImageView` like with `&amp;String` and `&amp;str`. Apparently this is done with the `AsRef` trait, but it doesn't seem to work with lifetimes. Is there a way to do that?
You can't do this. Reference coercion happens through `Deref`, but `Deref` can only return references, not custom types.
Feature request: add a [Voronoi diagram](https://en.wikipedia.org/wiki/Voronoi_diagram) generating function!
Indeed, all modules have an implicit `use std::prelude::v1::*;` as shown in [the docs](https://doc.rust-lang.org/std/prelude/index.html). Where have you seen examples putting it in explicitly?
You probably want: &amp;#x200B; impl&lt;'a&gt; Into&lt;FooRef&lt;'a&gt;&gt; for Foo&lt;'a&gt; { fn into(self) -&gt; FooRef&lt;'a&gt; { FooRef { a: &amp;self.a, b: &amp;self.b, } } } &amp;#x200B;
You need to have `qtdeclarative5-dev` installed. https://packages.ubuntu.com/search?searchon=contents&amp;keywords=Qt5QuickConfig.cmake&amp;mode=exactfilename&amp;suite=bionic&amp;arch=any To hook up your solver to the GUI, write a `binding.json` that has the values that you want to send between Rust and Qt. I'm guessing that your model is not a list or a tree, so you should use `"type": "Object"`. If you use Qt Widgets, you can communicate with your rust code via functions you specify in `binding.json` or via the types and events on the data objects you specify there. The Rust Qt Binding Generator has a demo user interface that was written with Qt Widgets in the folder `demo/`. https://www.vandenoever.info/blog/2017/09/04/rust_qt_binding_generator.html 
Yep, mentioned it at the end of the post :)
[There you go](https://github.com/search?l=toml&amp;q=tokio-zmq&amp;type=Code). Searches for TOML files with "tokio-zmq" inside them. Usually you can't access this search via the UI, and you have to change the URL manually to search for toml files. Seems there are no github projects using the crate by OP. But the method works, e.g. it is finding some users for [my lewton crate](https://github.com/search?l=toml&amp;q=lewton&amp;type=Code).
https://doc.rust-lang.org/std/io/struct.Cursor.html
If you work for consultant companies you could very well push for Rust usage at clients. Granted that most consultancy work tends to involve CRUD webapps, but there's still use cases there for Rust (robustness, lower overhead -&gt; lower costs).
Supposing `tokens` is not a slice, but an owned variable, would `clone()` still be needed?
&gt; Rust 2018 Is that a version of Rust?
Thank you so much for this, I had thought of exactly that but never managed to produce a request that got decent results. Turns out all I had to do was edit the URL... Github really needs some work done on their search UI.
Hello there! I'm starting to migrate code over to using `futures-preview` and I wonder if there's a way to type alias the function pointer for a function that returns a Future. According to the docs (and trying it,) you can't directly use `impl Future` because `method 'poll' has a non-standard 'self' type`. With that in mind, I'd like to basically figure out the signature of the following function: ```rust fn some_function&lt;'a&gt;(some_string: String) -&gt; FutureObj&lt;'a, Result&lt;String, ()&gt;&gt; { FutureObj::new(Box::new(future::ok(some_string))) } ``` I would think it would be ```rust type FutureGenerator = for&lt;'a&gt; fn(String) -&gt; FutureObj&lt;'a, T&gt;; ``` but this gives the error: ``` return type references lifetime `'a`, which is not constrained by the fn input types ``` Which begs the question: What is the correct type annotation for the above function, `some_function`? Thanks! 
You could imagine rearranging the computation so that the `tokens` `Vec` is processed in reverse and the final element is `pop`ped rather than indexed (since `pop` takes ownership of the value rather than borrowing it). But that’s unlikely to be more efficient in practice.
Okay, so why is this used? I am a total noob but I think Rust yelled at me saying it wasn't used so I removed it.
`$ locale` `LANG=` `LC_COLLATE="C"` `LC_CTYPE="C"` `LC_MESSAGES="C"` `LC_MONETARY="C"` `LC_NUMERIC="C"` `LC_TIME="C"` `LC_ALL=` &amp;#x200B; It's weird... also when typed echo $LC\_CTYPE the output is empty
Yes, but this is true for a _lot_ of things with unsafe. crates.io has _tons_ of these useful abstractions already that crop up multiple times, the way I see it if this were sufficient reason to include things in the stdlib the stdlib would more than double in size. I'm not arguing against this abstraction, I'm arguing against this abstraction being in the stdlib. Things like smallvec aren't in the stdlib either, and those are used _all over the place_ for an optimization that needs some tricky unsafe in its implementation.
Is there a full list of dependencies for `rust_qt_binding_generator`? I already found out I also need to install `libqt5svg5-dev` and now it can't find `QQuickStyle`.
That looks like it's writing into uninitialized memory and panics Benchmarking copy: Warming up for 3.0000 sthread 'main' panicked at 'assertion failed: `(left == right)` left: `0`, right: `5`: destination and source slices have different lengths', libcore/slice/mod.rs:1654:9 stack backtrace: 0: std::sys::unix::backtrace::tracing::imp::unwind_backtrace 1: std::sys_common::backtrace::print 2: std::panicking::default_hook::{{closure}} 3: std::panicking::default_hook 4: std::panicking::rust_panic_with_hook 5: std::panicking::continue_panic_fmt 6: rust_begin_unwind 7: core::panicking::panic_fmt 8: mine::copy_harness 9: criterion::Bencher::iter 10: criterion::routine::Routine::sample 11: criterion::analysis::common 12: &lt;criterion::benchmark::Benchmark as criterion::benchmark::BenchmarkDefinition&gt;::run 13: mine::main 14: std::rt::lang_start::{{closure}} 15: std::panicking::try::do_call 16: __rust_maybe_catch_panic 17: std::rt::lang_start_internal 18: main error: bench failed 
There is a DockerFile which lists all of them. If you just compile rust_qt_binding_generator, you only need QtCore. If you want to compile the demo, you need quite a few (optional) qt modules. https://cgit.kde.org/rust-qt-binding-generator.git/tree/docker/Dockerfile
Interesting. Could you put the benchmarking code in a git repository somewhere? It'd be interesting to toy with it.
Glad to have been of help!
In the code sample in question, they're using the Write and Seek traits, which they're getting from std::io::prelude. If you weren't using those (or Read or BufRead), that would be why it was unused.
Paging /u/japaric ... I would also like to know what has the lowest threshold to get up and running.
The best embedded experiences I've had (and not just Rust+embedded) is the nRF52DK + [Tock](https://www.tockos.org)
Sorry, I didn't notice the @ mention. I don't know exactly how Clippy works, but it appears to only lint code that hasn't been compiled yet or is dirty.
The [discovery book](https://rust-embedded.github.io/discovery/README.html) is a great resource to get started with embedded programming in Rust. The book works through several examples using the STM32F3DISCOVERY, so I'd recommend that board. 
It's possible to accept an iterator as a function argument but more idiomatic to take a slice. Also, a Vec of something is an unusual choice for a hash map key, perhaps there's a better data structure to represent this. I find I use hash maps far less for organizing data on Rust because of its powerful type system. Just some stray thoughts, hope they are of use. 
IIRC it's called an "edition", and as far as I understand it's like a specific cutoff point that can be used for (backwards?) compatibility reasons.
Why won't you just use proxy? 
First, welcome! :) It's always good to have new people on this subreddit. You might find that this community is different from other programming language communities in a good way -- it's because we take the subreddit rules and the community's [code of conduct](https://www.rust-lang.org/en-US/conduct.html) seriously. I think you'll love it here! Second, I'm excited to see you making your first steps with Rust! Let us know if you have any questions here -- every week we create a new post so that easy questions can be handled in comments. [Here](https://www.reddit.com/r/rust/comments/9ghwuv/hey_rustaceans_got_an_easy_question_ask_here/) is this week's. If you feel it's more complicated or you're just more comfortable with posting a question by itself, feel free to post something else here!
Thanks! I would definitely look into easy question thread. I wanted to introduce my project to the more experience devs out there.
You don't have to run the program under a debugger. For as long as I've used Linux you've been able to toggle having all programs core dump if they segfault. These days with systemd it's built-in, and when a program crashes I just `coredumpctl gdb` to open gdb with the lastest core dump.
Oh, no, it's that thing again! :p I can't seem to wrap my head around it. (At least they changed the name from _epoch_, which made it even more cryptic.) Does it, at least in this context, correspond to some normal version number?
I'm going with the STM32F3Discovery. It's cheap, readily available and. has a ready to go f3. crate on crates.uo and has some neat sensors on board and 8 led’s to blink to my heart’s content. All good for learning
Having been part of a number of communities that have gone through this process, here is my refined take: A code-of-conduct is only as good as its enforcement. It always bothers me when opposition to them is framed as being opposed to the stated goals of the document, which is almost universally not the case. There are legitimate concerns with how they've been used in practice, and to completely cast those aside will only strengthen any opposition to them. A less discussed component of these codes-of-conduct is their connection to American identity politics, which redefine many well-understood terms in far more ambiguous ways. If seemingly unambiguous terms such as "racism/sexism/discrimination" are going to used in more ambiguous ways, then they should be defined in terms that will be globally understood.
Alright, I got `rust_qt_binding_generator` to compile and I could compile `mailmodel`. Running it gives me this error, however: QQmlApplicationEngine failed to load component qrc:/main.qml:73 Expected token `;' qrc:/main.qml:74 Expected token `;' qrc:/main.qml:75 Expected token `;' qrc:/main.qml:108 Expected token `;' qrc:/main.qml:109 Unexpected token `identifier' qrc:/main.qml:116 Expected token `;' qrc:/main.qml:117 Unexpected token `identifier' I couldn't figure out which dependency is missing to compile the other demos and I'm running out of space if I use the dockerfile. Maybe I'll battle the build system some other time.
You might be interested in a WIP parser for POSIX shell scripts that I wrote, it supports most standard shell syntax apart from `case`. I was going to write a POSIX shell JIT at some point but then a company was rude enough to hire me and I didn't have the spare time to do it any more. https://github.com/jFransham/shell_parse
I see, thanks!
Hi there ! I am trying to implement a parser in rust, and I have trouble with borrowing. I understand the error and why it occurs, but I can't figure out a clean work around. Could someone help me figure it out ? `self.filter.on_char()` doesn't mut anything but `self.process_slice()` does. ```rust impl&lt;T: Filter&gt; Parser for ParseEngine&lt;T&gt; { fn parse(&amp;mut self, s: &amp;str) { let mut last_begining = 0; for (i, c, ctype) in s.chars().enumerate() .filter_map(|(i, c)| { match self.filter.on_char(c) { CharType::Discard =&gt; None, ctype =&gt; Some((i, c, ctype)), } }) { self.process_slice(&amp;s, &amp;mut last_begining , i, ctype); } ... } } and have the following error: ```rust 164 | for (i, c, ctype) in s.chars().enumerate() | __________________________________- 165 | | .filter_map(|(i, c)| { | | -------- immutable borrow occurs here 166 | | match self.filter.on_char(c) { | | ---- first borrow occurs due to use of `self` in closure 167 | | CharType::Discard =&gt; None, 168 | | ctype =&gt; Some((i, c, ctype)), 169 | | } 170 | | }) | |__________________- borrow used here in later iteration of loop 171 | { 172 | / self.process_slice(&amp;s, 173 | | &amp;mut last_begining , 174 | | i, 175 | | ctype); | |_____________________________________________^ mutable borrow occurs here ```
I noticed that the author used the word "racism" and "sexism", but the sources he quoted used the words "reinforcing systemic oppression." The author went on to criticize the sources as though they had said "racism" and "sexism", which strikes me as either 1) so unfamiliar with what the sources believe that it's not very responsible to make a video about them before he gets someone to explain it better to him or 2) bad faith.
I've been improving compatiblity of my [MegaZeux recreation](https://github.com/jdm/mzxplay) to the point where the title sequences of several older games almost match the original. I'll be continuing to implement missing Robotic commands and hopefully start messing with compiling to wasm to achieve my ultimate goal of MegaZeux in the browser.
Looks great! Would surely go through the code. I struggled with the parser and wrote a basic parsing method. This would help alot! Thanks
You can achieve this on stable cargo today. If you add the following to your `.cargo/config`: [source.crates-io-mirror] registry = "http://your-in-house-git-server/rust/crates-io-mirror.git" [source.crates-io] replace-with = "crates-io-mirror" The cargo will use that git repo in place of the public one on GitHub. We have a script that syncs our in-house repo with the public one, but re-writes all the S3 links in the public repo to a local caching artefactory server. This means that all crates we use get archived on the in-house server (useful for tedious escrow processes etc. see /u/thramp's answer for more uses).
Yes, you can use the [take](https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.take) method, which returns a Take, which in turn implements ExactSizeIterator
what an interesting [name for a shell!](https://github.com/coder543/rush) I don't have a README because my project was still a few features short of being really usable.
What do you think about this: http://danluu.com/term-latency/ It's hard for me to believe that a native application has worse latency than a browser, but maybe I'm crazy.
While the whole video drips of bad faith arguments and cherry picking and appeals to meaningless, unactionable virtues like "freedom", it does present, in my opinion, one good point: introducing a code of conduct has led to people being excluded or to feel unwelcome. On its own, that's not strong enough... it's logical to assume that a community cannot truly welcome everyone equally. What I would like to see is similar emperical evidence showing how many new people have been drawn in specifically because of the change in community direction and feel. A net benefit, especially if it includes a growth in perspectives and ideas, would really put the nail in the coffin (pardon my death threat) in these arguments. Does anyone know of any solid, relevant studies etc?
Sorry if this has been answered already, but any chance of implementing a scrollbar? I don't want to be confused if nothing's showing up but it turns out I'm scrolled up a bit.
Would it be possible to support multiple tabs natively? Not everyone can run a tiling WM.
How well does alacritty perform with software rendering? My OS (Qubes) doesn't support hardware accelerated graphics.
What version of Qt are you using that gives these errors? 
I'm personally not interested in writing code that can be included in a project without people having access to it. I'd prefer any derivative works to be made freely available.
I'd like to add to this that I'm looking for something really cheap but still capable. I bought an Arduino Uno for $11 (incl. shipping) hoping to use Rust with it with [ruduino](https://github.com/avr-rust/ruduino) but I couldn't get [Rust with AVR](https://github.com/avr-rust/rust) support working ([issue](https://github.com/avr-rust/rust/issues/110)). I'm trying to do some basic stuff using analog inputs, but price is relevant (I'm trying to replicate a product that exists already, but cheaper). I can of course just use C++, but where's the fun in that? Everything in this thread seems to be $20+, which is not prohibitively expensive per se but I'd rather be as cheap as possible (the other hardware components will be around $30-40 and I'd like to keep total cost under $50 if I can).
I have a function that returns a long Future definiton. i.e. crate fn request( &amp;mut self, ) -&gt; Result&lt;impl Future&lt;Item = response::Response, Error = error::Error&gt;, error::Error&gt; { How can I rip out the `Result&lt;impl Future ... &gt;` into something more passable? I want to use this in multiple places. I've tried using `type` but I cannot seem to find a way to get it to work.
5.9.5
If it's an owned vec, then he can use `.into_iter()` no?
You could a custom `Iterator`, either as a dumb wrapper on your `FlatMap&lt;CartesianProduct&lt;..&gt;, ..&gt;` or as a fully custom one with `fn next` doing everything the separate types would. Keep in mind that `ExactSizeIterator` is supposed to know its intermediate size too. You could just start with a field for the total size that you decrement in `next`.
Fundamentally, it’s a flag that tells the compiler you want a specific set of features. No flag means the default set. Versions of the compiler support both sets of features. It’s purely additive, so it does support a “normal” version number: 1.x.
old.reddit.com doesn't render this code properly. Until they fix it, post it using four prefixed spaces instead of triple '`': fn train(&amp;mut self, tokens: &amp;[T]) -&gt; &amp;Self { for list in tokens.windows(self.order + 1) { let children = self .graph .entry(list[..self.order - 1].to_vec()) .or_insert(HashMap::new()); children.entry(list[self.order]); } self } struct MarkovChain&lt;T&gt; where T: Token, { order: usize, graph: HashMap&lt;Vec&lt;T&gt;, HashMap&lt;T, usize&gt;&gt;, } 
I believe on linux the consensus is that this is better provided by the WM or a tool like `tmux`. However on macOS there seems to be a way to allow the system to handle most of it for you (I guess similar to WM in linux), so that might be added eventually.
And note that some (all?) of those features would be breaking changes to existing code, which is why the new edition is an opt-in.
The glutin update to resolve issues with DPI changes is high on our priority list and is hopefully going to land soon! Properly resizing DPI when switching monitor is something a lot of users have been waiting for for a long time now.
Ligatures aren't a super high priority right now, however they have been considered and I'd personally love to have Alacritty with ligature support. It would probably make use of something like `harfbuzz`, but since it's not super high priority, but a lot of work, it will probably take some time.
It might not be cool to talk about C++ here, but if you know it, it's probably easiest to just say a &amp;str is basically a char * while a String is basically a std::string. 
I don't think this has ever been requested. Rendering a scrollbar would add some complexity to Alacritty and I don't think a lot of people would make use of it, however I can definitely see a benefit in being able to tell if there's a scrollback history or if the viewport is currently not at the bottom of the scrollback buffer. We've already got ways to render rectangles at arbitrary locations, so it's not completely implausible. However I'd certainly like to investigate alternatives, when the goal is just to see where in the scrollback buffer the current history is (without actually using the mouse to interact with it). I wouldn't expect this to land anytime soon and I'm not sure if it would support mouse interaction, clicking, a trough and everything there is to it. But if you're interested in this, please open an issue on github explaining your usecase for a scrollbar and I'm sure we'll be able to figure something out.
I'm not sure, I've never tested it on any real computer. However I did try writing some automated benchmarks with software rendering and our high throughput benchmark got bottlenecked by rendering there. Your best bet is to just give it a shot. But Alacritty isn't really optimized for that kind of usecase.
Would it be possible to provide a CPU rendering backend that used SIMD acceleration? I am assuming that the bottleneck is font rendering (nothing else makes sense), and there is a very fast CPU-based font renderer. That said, I need to file a feature request with Qubes for supporting Intel graphics virtualization.
I don't know what you mean by killer feature. If you mean the SIGINT signal handling, I have added basic signal handler which would send the SIGINT signal to running command.
The following Google query should give you all Rust files that reference your crate, but I don't get any results back other than your repository: https://www.google.com/search?q=allintext%3A+use+AROUND%283%29+%28"crate%3A%3Atokio_zmq"+OR+tokio_zmq%29+ext%3Ars
I started writing a shell in Rust called Rush too https://github.com/zethra/rush I've pretty much abandoned the project but feel free to use any of my code as references if it helps. 
Note the URLs. The one you linked is from a specific version of Rust. It’s now called “the first edition.” http://doc.rust-lang.org/book explains the various versions and what they’re for. Does that help?
That's not surprising. Lol
I don't understand the signature. Why aren't you just taking in `self` and returning `Self` (instead of `&amp;mut self` and `&amp;self`) ? 
In the second scenario I proposed I believe the answer is "no", but that's a VERY significant amount of work and I don't expect anyone to do it. In the first scenario where you would just be building your C++ bindings with the gcc crate it would still require an external install of the Qt dependency.
"Killer feature" is a kinda marketing term to mean something special that you can't (easily) find elsewhere. Something to set it apart.
This should be fixed now. I was using 'let' instead of 'var' and that's only possible since Qt 5.10. 
Since one of the main selling points of Alacritty is providing a simple GPU-accelerated terminal emulator, I don't think something like this is in the scope of the mainline Alacritty version. However there has been some effort to separate the core of Alacritty into a library, so the rendering can be easily replaced. This would allow writing a separate frontend which could make use of software rendering. It will probably take some time until Alacritty is properly separated, even though there's already a semi-working PR. And it would probably require some effort to implement a software-rendering based frontend. However this is probably the way to go to resolve this kind of problem.
This sounds great, thanks! 
The `MIRROR.md` sounds pretty great, thanks
&gt; const fn parameter no allowed &gt; &gt; Why not? `const` fn arguments would require `const` generics which are not fully implemented yet. Maybe someday. 
okay
Yes, that is a "killer" feature :-D. I like your response the best. You don't need to prove anything for this to be worthy of note!
You can tell me if you'd like 
So "the first edition" docs where completely rewritten? As a beginner I think it's a pity that most of it is disappeared: I find the contents of the first edition quite informative despite I read the some chapters of the current version. Anyway, now I have found it, so I can consider this a lucky day!
Yes, the first edition book was thrown out, and the second edition was re-written from scratch. They didn’t disappear! It’s still distributed, just explicitly as “first edition”.
God, i so want a tty that'll fucking work transparently with Wayland and KMS (kernel mode setting) for applications that say they support KMS. The problem is that KMS needs a wayland compositor for the applications to not shit themselves when comparing running the application in gnome etc. All the quirks stuff in hooked there, all the permissions model. You can't even run a graphical application more than once because the permission model is 'first app to adquire read lock over some gpu kernel file, gotta return on close (doesn't happen in retroarch for instance) and 'change resolution? Get ready to crash because of this'. 
In \*const or \*mut, the const/mut keyword refers to the pointed-at thing, not the pointer itself. This is a pointer, which can be changed, which points to data which cannot be changed: `let mut ptr: *const u8` This is a pointer, which cannot be changed, which points to data which can be changed: `let ptr: *mut u8` 
Is it better to heap allocate late (in the associated data) or heap allocate smaller structures earlier? As an example: enum Expression { Binary(BinaryExpression), Unary(Box&lt;UnaryExpression&gt;), } struct BinaryExpression { left: Box&lt;Expression&gt;, operator: Operator, right: Box:&lt;Expression&gt;, } struct UnaryExpression { operator: Operator, argument: Expression, prefix: bool, } `BinaryExpression` would force the user to heap allocate their expression early in construction while `UnaryExpression`would heap allocated both the `operator` and `prefix` values that don't really need to be. I can see an argument for either, so I am wondering if there is a idiomatic guideline or best practice around this?
Sometimes you'll see `&amp;mut` methods that return `&amp;mut Self` to allow chaining. Often for builders to allow syntax like Whatever::build() .one() .two() .finish()
Not offtopic. Just like Linus' letter, I thought it had its place here. Sad this opinion isn't shared...
`const` in rust means a compile-time constant. Not a merely immutable runtime variable. 
What an excellent way to interpret the question. I \_think\_ they were looking to know if there is something this shell excels at beyond what other shells may offer. Your interpretation of a feature which KILLs is on point though.
This response is so great and accurate. Literally.
Well, as mentioned above, it was more of an learning based project. I used several resources to read about the unix process management and was very much attracted towards rust. Felt that a simple shell might help me understand the concepts for both. Never thought about any "Killer" feature to introduce.
Yeah. Not the most critical feature, but a nice-to-have imo. [It looks like there's already an issue for it](https://github.com/jwilm/alacritty/issues/775) so I commented.
I would like for one of these rush shells to succeed so my shell will share the name of my favorite band.
Perhaps the topic warrants some discussion, but I don't think an archived youtube video by Bryan Lunduke is the way to start it. Also it's probably best kept on the Rust forums rather than reddit, which discourages people from other subreddits from joining in.
Lol, going to listen them right now!
I would go with the blupill too ! Even though you should go for the fixed version the "black pill" : it has the same mcu but has some board issues fixed.
&gt; The f103 has 16, the f030 has 8 Oh, got it. So if I put the little pin things on the sides, the A1-A16 are the analog inputs? Just guessing... I'm a little confused.
related question, does *const have some actual restrictions that *mut doesn't? because as far as I can tell, you can just cast *const into *mut, no?
Note: the project posted here is a shell (like bash, zsh), not a terminal emulator (like alacritty, gnome-terminal, urxvt, ...)
On stm32 (and other mcus) the pins are organized into blocks of 16 known as gpio ports, the ports are labeled A through whatever depending on how many pins they have. So A1-16 doesn't mean it's analog it means it's on GPIO port A. To know which pins can be used for what function you should look at the datasheets or pinout diagrams or stm32cube
That's exactly what I wanted to achieve, but I forgot the `mut` part.
There's one subtle, but sometimes important difference. Lifetimes have a subtyping relationship and `*const T` is covariant over `T`, `*mut T` is invariant. See the [rustonomicon chapter](https://doc.rust-lang.org/nomicon/subtyping.html). Apart from that, they are only minor guard rails so you don't inadvertently mutate through the wrong pointer.
[You need to explicitly cast it.](https://play.rust-lang.org/?gist=ccea1fb33251ba65df3611ce434b33cd&amp;version=stable&amp;mode=debug&amp;edition=2015(
I have to admit that I almost never used Rust forums so it didn't seem like the place to put it. Learning from other projects experiences is always a good thing. But I have to admit that I'm an idealist for hoping that it wouldn't get too heated...
But then it has to be allocated onto the heap and returned. It’s more work than my original.
Yep! Based on your question I thought you were looking for simplicity, not performance. I found a good write up on some options here: https://tokio.rs/docs/going-deeper/returning/, specifically the section "Custom types" might be interesting to you. Alternatively, if you're using the futures preview, you could use `FutureObj`, although that might incur the same cost given the description: &gt; A custom trait object for polling futures, roughly akin to Box&lt;dyn Future&lt;Output = T&gt; + Send + 'a&gt;.
It's pretty easy to set up on raspbery pi as well. 
Thanks very much. I’m not totally against the Box approach, it just seems like a short commons of Rust if I’m unable to find an alternative that has both simplicity and no heap allocation. I’ll take a look at some of the new futures stuff too. I think it’s moved on since I first wrote my code.
I think that's because the original iterator could be smaller and thus it wouldn't return as many as the take.
You already posted here: https://www.reddit.com/r/rust/comments/9gwchp/const_fn_parameter_no_allowed/ Try to keep them together. Better yet, post in the questions thread: https://www.reddit.com/r/rust/comments/9ghwuv/hey_rustaceans_got_an_easy_question_ask_here/
This looks like a great idea; thanks for making it! :) 
Thanks for your interest. The purpose is to see what screen resolutions are available for your display, then you can change your screen resolution to one of these. A bit like [SwitchResX](http://www.madrau.com) but on the command line and free. I've improved the README, you can check it out at [https://github.com/bn3t/screenresolution-rs](https://github.com/bn3t/screenresolution-rs).
Well, as far as i know the default tty is at the distro pleasure on linux. At least for mine `ps l` seems to indicate the tty on a new 'ctrl+alt+fkey' is named `-bash` (different for some reason than one open inside gnome shell that is `bash`). If you're confused about my complaint and expect that what i want simply cannot be done without running gnome, well graphical applications *already* run on KMS without gnome (retroarch is one) directly from the shell. The problem as I mentioned is that it's a disaster zone of broken GPU permissions and missing tweak libraries like libinput, which are designed to slot into a wayland compositor. I was thinking 'what if the shell was optionally a wayland compositor?'. 
Wait, you mean crates.io doesn't automatically reject uploads that break semver guarantees?
Currently I use ncm2 for auto complete, just started using ultisnips for snippets and also ale. To compile i open neovim's terminal emulator and just 'cargo run' or whatever i need. I can send you a link to my init.vim, if you are okay with reading one really fat config
I will tell you both even more: https://github.com/adamwiggins/rush
It doesn't. But I would totally like to see `cargo publish` refusing to publish something that breaks semver compat by default. 
If you're dealing with lists of something, you might want to consider a [trie](https://en.wikipedia.org/wiki/Trie) data structure. It tends to be more efficient in this case. There is a [QP-trie implementation in Rust](Assuming you can keep the input around) that also lets you save memory by using slices as keys instead of requiring you to clone each key. It will be a much better fit for your use case.
Wow, thanks for this. Just found out my derive\_more crate is being used by racer since this July :O [https://crates.io/crates/derive\_more/reverse\_dependencies](https://crates.io/crates/derive_more/reverse_dependencies) Downloads obviously shot up like crazy around that time (see bottom of the page): [https://crates.io/crates/derive\_more](https://crates.io/crates/derive_more)
you can use `Path::join` but it will probably make more allocations than you expect.
Last I checked there were three versions on the official page: first edition, second edition, and a running version in development on top of the second edition. I recommend starting with the second edition, then learn some strange details from the first, then look at what's new in the running variant.
That's cool, I wasn't trying to be mean about it or anything. It's more than I've done in Rust. 
For amusement, I check the CI configurations on these tools and see how they dogfood it. They call their CI check \`semververver\`. Hah.
I think you mean inmutable by default. 
Ah, that makes a lot more sense. Good comparison! 
This would require a way more opinionated stance on what constitutes a "public" item than would be healthy
This post makes me extremely happy. :D &gt; among the sites of its peers, only Ruby is similarly localized. Given that several prominent Rustaceans like Steve Klabnik and Carol Nichols came from the Ruby community, it would not be unreasonable to guess that they brought this globally inclusive view with them. Not just that, I actually looked at how Ruby's website did it in order to bring it to our site. This has... issues. But it's served us fairly well. Additionally, while there's so much more I could say about this post, one thing: &gt; my naive Rust was ~32% faster than my carefully implemented C. *This* is a metric I'm more interested in, far more interested than classic benchmarks, where various experts eke every last little bit out of an example. What's the *average* case here? It is, of course, much much harder to get numbers for these kinds of things...
&gt; it also means that the same code can be run on the client and the server. My bad. I'm a B2B web programmer by trade, and, in day-to-day conversation, the "DRY" aspect is usually implied by saying they're coded in the same language. You're definitely right that the most important feature of "isomorphic" apps is that they run the same code, not just the same language.
well, you don't have to use gnome-shell, use Weston or sway or whatever :)
Hi all! This is another post about how we use Rust in our commercial product. This time I wanted to share how we do FFI between Rust and C#, some of the challenges involved and how we try work around them. &amp;#x200B; I'm keen to hear about how other folks do FFI with managed languages!
What's the controversy about what to consider public?
Really nice post. I like how they highlighted one of my favorite features `include_str!()` which lets you make the easiest quine ever `println!(include_str!(concat("../", file!()))`
I see, thanks! 
&gt;my naive Rust was \~32% faster than my carefully implemented C. The corresponding quote from his video presentation\[1\] is even more interesting: "So that was a surprise. (...) I've done zero profiling on this. I've made absolutely zero attempt to make this thing faster. None. **In fact, you can pretty reasonably argue that I've gone out of my way to make it slower**. And we are 40% faster than my C." &amp;#x200B; 1. [https://www.youtube.com/watch?v=aWbGPMxs0AM](https://www.youtube.com/watch?v=aWbGPMxs0AM) quote is at about 1:05:00.
As pointed out by /u/cerebellum42, I left out a crucial part of the definition. In an isomorphic application, the code is shared between the client and server, and can execute on either or both. I agree that it's not what's meant by isomorphism in, say, mathematics or functional programming.
You can "just cast it" because you're already in unsafe code. It's 100% up to you to maintain invariants. The type system is trying to help you, but you have flipped the little red switch from "safe" to "FOOTGUN ACTIVATED". `\*const` has a few restrictions -- specifically and exactly preventing your code, as you posted, from compiling. Because the first parameter of `strcpy` requires `\*mut`. I mean, we're getting to the point of tautologies, here. The system is doing what it's supposed to do.
(If you're going by strict rules, this is considered a "cheating" quine because it reads its own source (just as compile time instead of runtime))
Huh, I have his video talking about Rust and Statemaps saved to watch later, but now I'm even more interested in it, especially since I've been spending time with both c++ and rust (since I'm starting to work on my system level programming skills I decided to be insane and pick up both so I could compare and contrast and use things from each to get different perspectives).
Excellent summary of the strength of Rust's error handling. This is the article to which I will refer people when that subject comes up in the future.
Relevant links: * [https://doc.rust-lang.org/nomicon/hrtb.html](https://doc.rust-lang.org/nomicon/hrtb.html) * [https://stackoverflow.com/questions/35592750/how-does-for-syntax-differ-from-a-regular-lifetime-bound](https://stackoverflow.com/questions/35592750/how-does-for-syntax-differ-from-a-regular-lifetime-bound) The stackoverflow answer is quite good, but the short (and obtuse) answer is when using a `&amp;'a`-reference in a `fn` definition the `'a` lifetime is provided by the _caller_ of the function, while when using a `&lt;for 'r&gt; &amp;'r`-reference the `'r` lifetime is provided by _the function itself_. The place where can come up "easily" is when dealing with closures taking references, as you can return a closure that takes a borrow to their argument, but the closure is being passed around outside of the scope they were defined in. This leaves you with no lifetime in scope you can tie the closure's references to, hence you use `for&lt;&gt;`. My explanation doesn't make it justice, and whenever I have to use them (I have _maybe_ twice) I just try a simple attempt, and if that doesn't work I re-read the two links above until it clicks.
Why do you like this combo? I’m a complete noob and have no idea what either of those are or why they go together well.
Sure, have a look at https://github.com/pantsman0/benchmark-repeating-append
You probably want /r/playrust
And a tty (Like mgetty) isn't a shell either. A shell is spawned by a tty and a terminal emulator, but they're not the same. This is more akin to bash vs zsh, not Getty vs alacritty.
As long as it's representable as a stream of tokens where each token can be converted to a byte slice, you're good.
After some thinking I've realized that `resize_with()` is the easiest to optimize, and it (at least in theory) it could rival `slice.iter_mut()` in terms of performance. Looking forward to trying it.
However, there's value in discussing this in an RFC to reach ecosystem-wide consensus.
/r/lostredditors 
Added to this week's issue!
&gt; Get in the habit of running rustc on short programs. Cargo is terrific, but I personally have found it very valuable to write short Rust programs to understand a particular idea — especially when you want to understand optional or new features of the compiler. https://play.rust-lang.org/ was *born* for this, and I use it often.
Thinking that there will be ecosystem-wide consensus on this seems pretty optimistic to me
Some people (like me) aren't that successful with book learning though. I agree that some degree of book learning, especially about ownership/borrowing, is absolutely necessary for Rust. But personally I learn by doing, and find book learning somewhat dry and unmotivating. That said, I admit Rust wasnt that conducive to winging it, especially when I just jumped into shared state without really understanding some core rust concepts. Coming from a dynamic language background made that worse, and no amount of reading about rust helped me here. I think just reading and writing a lot of actual rust code helped a lot more than reading a book or blog posts, it just took a while for it all to sink in. 
Thanks!
because `Foo` need to allocate memory, if I have an instance of `Bee`, I want convert it to `FooRef` directly, instead of creating `Foo` first and then get `FooRef`.
&gt; (the old one -- I don't know if v2 has been released yet, it's been a while since I touched Rust) It is, yeah. Already working on the version after that :) The first edition was completely thrown out for the second one, partially for reasons that you mention.
Thanks mate. Sure, if you can, do share me the link.
I've looked at that tool too, and it seems super interesting. One thing that's unfortunately not covered is semver compatibility checking for networking services or parsers. It's extremely difficult to detect breaking changes or regressions in those types of applications in a semver context just by analyzing the source code. This tool seems to enforce semver only on the crate API, and anything else you must still check by hand.
I'm not sure I understand. The underlying data of the `Foo` must be stored somewhere. Are you trying to avoid the time overhead of a separate allocation?
What other languages do you know? I've jumped into Rust from high-level languages like Python, it took me maybe a month to become reasonably confident with. Writing idiomatic code and tuning for performance took more time. I don't see learning Rust taking a year, you should be fairly proficient in it in a couple of months. C++ has a lot of complexity, and while it's not that hard to pick up C++ to the point where you get something working, it's very hard to get something in it working reliably. In fact, this is one of the motivations behind Rust; you can read more on the experiences of former C++ users with Rust [here](https://www.rust-lang.org/en-US/whitepapers.html). The only example I of C++ use in real-time reliability-critical systems that can think of is F-35, and it's been stuck in debugging for countless years now. If you want to pick up a language other than Rust for use in real-time reliability-critical systems, I'd suggest Ada. That's what aircraft autopilots are programmed in, and that stuff works.
Will the 2nd edition be obsoleted soon? I have the O'Reilly book with me and seriously considering getting your book.
This look cool, and I'm probably going to put a spotlight on my own ignorance here, but can someone explain the point to me please? The focus of this terminal emulator seems to be speed above all else. A laudable effort to be sure, but what is a "fast" terminal emulator, and in what ways might I notice the speed difference? I don't think I've ever noticed a delay caused by terminal speed.
Current list of Rust-based shells named "Rush": [rush](https://github.com/psinghal20/rush) [rush](https://github.com/zethra/rush) [rush](https://github.com/adamwiggins/rush) [rush](https://github.com/coder543/rush)
After digging a little further, I think I've found an answer to my question -- so for anyone else who had the same question I did: terminal speed matters if you have a large window on a high-resolution monitor which needs to repaint often. Imagine running a console-based text editor in a maximized window on a 4k monitor, you might need to repaint a *lot* of characters after every keystroke. That's where the speed will help.
Specifying `default-features = false` under the `noise` dependency in Cargo.toml works for me: [dependencies.noise] git = "https://github.com/Razaekel/noise-rs/" branch = "develop" default-features = false (using the git repo you gave since the noise crate on crates.io still has image as a required dependency)
I know it is a not a very helpful answer, I would recommend learn as you go. I am from OOP PHP background, and I am too facing a lot of challenges. But I have seen improvement after I learned the basics, understanding what the compiler error messages are actually trying to tell. You are from OOP background, keep following it, and fix the problems that are in your programming methodology. Who knows you will discover a different Rust programming pattern in future :)
Was reading this *excellent* blog post while chatting with a friend online. Friend: "In other news, I started learning Rust." Me: "Nice! I'm reading an interesting article as we speak. &lt;http://dtrace.org/blogs/bmc/2018/09/18/falling-in-love-with-rust/&gt;" Friend: "I am reading the exact same article at exactly the same time." Huh.
&gt;Written on September 18, 2019 What I do is just define my own TryFrom that matches the signature. In the future I'll replace my definition with `use std::convert::TryFrom` and everything will magically work. 
Here you go https://github.com/Anfid/dotfiles/blob/master/.config/nvim/init.vim Plugins that might interest you should be in the end of plugins list and setup is in the end of file
Marking as a duplicate of https://www.reddit.com/r/rust/comments/9gzldv/bryan_cantrill_the_observation_deck_falling_in/
Huh. Didn't realize that it was a relatively young project (2.5 years). NixOS had it packaged for over a year.
First, I'm relieved that I didn't miss the mark too badly on Ruby being an influence on the Rust website! I knew I was being a tad presumptuous, but hoped that you would take it as praise were I wrong... Second (and as long as I have you), thank you for everything you've done to make Rust welcoming -- both as a technology and as a community. It has clearly been a ton of work, but it has really paid off; on behalf of all newcomers, thank you! Finally, in terms of understanding the performance vs. C: I have some ideas of what's going on here, but I need some more time to really dig in; stay tuned!
Yes - technically `inmutable`, but I believe for beginners it's just easier to understand `constant`.
Excellent point and I should have mentioned it! I plan to update the blog entry with resource suggestions from the community, so I'll definitely add that to the list. And speaking personally, I should probably get used to it myself -- I just can't quite get out of the habit of creating new little source files to understand particular language or systems concepts, having kept basically all of them over the years...
[removed]
https://github.com/DanielKeep/cargo-script is also a wonderful tool for testing stuff quickly, and it can deal with dependencies that aren't in the Playground set.
I have a little [alias in my dotfiles](https://github.com/oconnor663/dotfiles/blob/5f95151feb38b5277b276624fee6441dd4a8b339/zshrc#L123-L131) that I call `newrust`. It runs `cargo new` in a random temp directory, and pops open the `main.rs` file in my editor. I find that I use this _constantly_ to test out little examples, about as much as I would use the REPL in Python. Often if I want to put together a small example for a reddit comment, I'll get it going with `newrust` first (with the benefits of my regular text editor and all it's plugins), then I'll paste it into the Playground to make a sharable link when it's working.
GHC Haskell has a [`TypeError`](http://hackage.haskell.org/package/base-4.11.1.0/docs/GHC-TypeLits.html#t:TypeError) builtin for this purpose. If trait resolution ends in a `TypeError` constraint, then it automatically fails with a type error. Can this solution work in Rust?
So you cannot use a type alias for impl trait, and trait aliases do not exist yet - see [rfc1733](https://github.com/rust-lang/rfcs/pull/1733). However as the RFC says, there is already a way to create a trait alias: trait Foo: Future&lt;Item=T, Error=Error&gt; {} impl&lt;T: Future&lt;Item=T, Error=Error&gt;&gt; for T {} // Now you can use Foo fn request() -&gt; impl Foo { ... }
Nix has almost everything packaged, is NixOS different?
I was going through stale tabs and I noticed you hadn't gotten a reply. You wrote: &gt; If std would be marked red lenamber wrote: &gt; Trusted Unsafe Dependencies means all dependencies with unsafe code were trusted (by an appropriate review process) → light green color
Thanks mate
Isn't ! going to be equivalent to enum {}? I feel this must be exploitable to resolve this? For example, add enum std::Bang {}. Let Into&lt;T&gt; lead to TryInto&lt;T, Bang&gt;. When ! is stable, alias Bang = !, and deprecate it a few releases later?
I was really just going on how he introduced himself -- certainly not trying to slight anyone!
I'm really not feeling the motivation behind the new proposal to remove turbofish.
https://doc.rust-lang.org/std/primitive.str.html#method.split_at_mut is what you are looking for, I think.
Thanks for catching this. My comment was meant in this sense: https://github.com/serde-rs/serde/blob/v1.0.79/serde/Cargo.toml#L4
The core dump only tells you the state of the stack when a program crashed. That’s useful to know what is wrong (some variable doesn’t have the value it should), but it won’t tell you how you got there (what line of code set it to that value). With gdb you can put watchpoints on memory like variables and get notified every time a line of code modified that memory. 
It is helpful to figure out what features are you missing from OOP. In my personal experience, I was mostly missing interfaces, because I wanted to write plugins. The "trait objects" are basically those interfaces. I have not missed multiple inheritance, because I was already doing without it just fine with composition. Here is a worthwhile read that explores these trade-offs in-depth: https://doc.rust-lang.org/book/second-edition/ch17-00-oop.html
Good to hear. I hadn't heard of the OReilly book before, and I just picked one up. From what I've heard it's also really good. I'll take another look at the Rust Book v2 and see if I can break through some of the issues I was having. :D
I have a Ruby background. In my opinion there is an (anti)pattern in OOP, that many programmers seem to follow: Deeply nested inheritance hierarchies. I was no exception to this. Code structured this way does not map well to Rust. The Ruby community seems to prefer composition over inheritance nowadays. I think this is the way to go in any programming language. If you adopt this style of writing your code, you'll find it easy to translate it to Rust. Composition using traits and types is highly idiomatic. As an example, suppose you have some code that is the same for several "classes" (in Rust structs or enums). Instead of creating a common base class containing the shared code, create a mixin (in Rust trait) containing the code and implement it for each "class". Another example: Suppose you have an OOP hierarchy with one Base and Several Children. You want to iterate over a list of children (different classes) and use some methods from the Base class, overwritten for each Child. It is not possible to extract the behavior into one concrete struct/enum and include it as a member in each Child. Translating this to Rust is a two-step process: 1. You need to solve the memory layout problem. In most OOP languages, Objects are boxed by default. This gives them a constant size, no matter of the real "class" of the Object (the size of a pointer). This allows them to be stored in contiguous memory, eg. arrays. In Rust structs and enums are not boxed by default. This means, that the different children in the example have potentially different sizes. You can overcome this by boxing the children (who would have guessed). 2. You need virtual dispatch to be able to call the methods of the children (Declared in the Base trait, but implemented for each Child). Again, in most OOP languages virtual dispatch is the default. In Rust it is not. You get virtual dispatch with trait objects. Final solution: Vec&lt;Box&lt;dyn BaseTrait&gt;&gt;. But if possible prefer extracting the behavior into a type and store this type as a member of all Children. Iterate over each instance of the newly created type from the Children and call the method on it directly. I hope that helps a little!
Yes, fair -- I misunderstood him. I changed the blog entry to read "one of the authors of Serde." David, hopefully that's still accurate from your perspective, and Eric, if you're out there, my apologies!
FWIW `cargo script` is absolutely stellar for that: just create .rs files and `cargo script` them, if you need deps you can add a `// cargo-deps:` (or a more complete cargo manifest) comment at the top. It can also be used to eval simple expressions straight on the CLI.
It seems that the "Black pill" is compatible with the "Blue pill" Rust support crate ? Is it true ?
In bare metal ? I mean without an OS ? Do you have any link to provide ? I'm also interested.
Sorry for being a nitpicker, thanks. :)
That was my first though actually. I also think it's the best to start with, but the footprint is quite high and I know that in few weeks I'd like to change to a smaller one. I was wondering how easy it would be to go from this board to another. I guess if I stick to STM32 (which includes Nucleo-F042K6) it should be fine in any case.
Another question: Did anyone have played with an HiFive1 board ? Any thoughts ? I won't use that one now, but eventually in some weeks. Thanks
I was doing some reading/learning/experimenting with Linux kernel modules loading/unloading, and put together a small Rust library to play with relevant syscalls without libkmod: [https://crates.io/crates/likemod](https://crates.io/crates/likemod)
I work for a company in the autonomous vehicle segment developing middleware and we use Rust as much as we can. Time constraints occasionally mean we build with C++ or C first because sometimes building the scaffolding over a mix of tech (in one case, it's NvMedia, CUDA, and EGL streams) needs to be a separate step from building the final and safer implementation. Overall our goal is to build everything we can in Rust and provide C apis for public interfaces. The awesome folks at Polysync are using Rust as well. &amp;#x200B; Anyway, feel free to DM me if you like. I'm happy to talk shop!
And don't forget `Cow&lt;str&gt;`, which is a convenient way of saying either `&amp;str` or `String`.
This is primarily the case that the Rust compiler can't prove the sub-slices as non-overlapping in the general case. In some cases, it might be very apparent (as in yours), but proving the general case is hard to impossible. &amp;#x200B; As /u/Darsstar writes, \_splitting\_ is the operation Rust uses there, because that makes clear that the two parts are not overlapping.
...and, to answer the literal question... &gt; I was curious if this was primarily a technical limitation, or whether there is technical reasons why this behavior shouldn't be supported / work? It's a technical limitation. The compiler can't be sure it's safe, so it errs on the side of safety. Functions like `str::split_at_mut()` and `Vec::split_at_mut()` contain manually-audited `unsafe` code. (Basically, telling the compiler "I know you can't verify this, but trust us.")
I've downloaded a static website application from crates.io, and compiled it on my local machine. How do I actually start the application on Linux please?