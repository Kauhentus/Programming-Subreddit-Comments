&gt; &gt; &gt; The best I can hope for is that Rust comes up with a novel way to handle inheritance to make it conducive to good design in the same way it came up with a novel way to handle pointers and enforce their correct usage. Lemonade from lemons, so to speak. I fully agree. :)
Thanks to eddyb for all his tireless work on getting this merged and landed. 🌟
&gt; Rust-compiled binaries link at the very least against libgcc_s_dw2-1.dll, which requires a MinGW or Rust installation to be present. Oh snap, I completely forgot about that. Some quick googling reveals that this *probably* means any attempt to distribute pre-compiled Rust executables for Windows^1 requires you to also make libgcc source code available. That's unfortunate. --- 1: Not sure it it links to this for 64-bit; I think Rust uses native exceptions for Win64.
thanks!
Pythonic syntax is C syntax with whitespace. If you wanted a real change it would be something like Haskell.
At least there's less clutter to look at which is aesthetically pleasing. It is not huge issue for me, but all else being equal I prefer uncluttered syntax. I am not sure that haskel syntax makes sense for (mostly) imperative language.
That's all true but one would hope you could also achieve more concise code since C++ isn't known for terseness, and compiler theory advanced considerably regarding things like type inference and coercion.
You can just bundle libgcc_s_dw2-1.dll with your binary, - it's covered by the GCC Runtime Library Exception.
Thanks!
You can have things like scroll areas. The scroll position is strictly UI only state, and the application shouldn't have to concern itself with keeping track of it. But it does mean that you have to have a way of identifying widgets across frames. Some persistent ID.
I tried rustc -C link-args=-static-libgcc hello.rs and that didn't remove the dependency. Also, from what I've read, using static libgcc breaks unwinding across shared library boundaries. Is this not a concern for Rust, or does this mean you would have to statically link *everything*?
Rust would never add a type system feature that allows memory safety to be broken in safe code. As you say, that goes against the whole point of Rust, and so you're attacking a problem that isn't there. If we do get "inheritance", it will be a form that is not memory-unsafe and any memory safety holes will be regarded as serious bugs that needs fixing (and will be fixed, even if it's not backwards compatible); just like memory safety bugs in other features in Rust. &gt; Oh really? Has it? I don't know if you're being sarcastic, or just don't know. I'll assume good faith (the latter). The following are all serious memory-safety problems in C++: - iterator invalidation - dangling references - buffer overruns - use after move/use after free And, yes, these are even problems in the lastest C++11/C++14 standards, despite what some people seem to think. (I'll note that these are all impossible in safe Rust.)
I am not really on board with whole fully functional shtick. Some functional programming is good but I don't think it is suitable for everything. Also, haskell isn't really system language with whole garbage control and all. Rust was supposed to be C++ done right, but it seems to be mostly "C++ done less wrong." Which is nice but not what it was supposed to be IMHO.
&gt; Rust was supposed to be C++ done right, but it seems to be mostly "C++ done less wrong." Originally, back in the mists of time, Rust was not aiming to be a C++ competitor at all; it was much much closer to OCaml, and has slowly morphed into a low-level, zero-cost-abstraction language. A (semi)reasonable description now would be "C++ done *memory safe*" (although D takes that crown more closely, modulo not being completely memory safe, AIUI), but really Rust is a different language that is targeting a similar space in a different way; it just so happens that C++ has had a lot of experience in this space and so has a pile of good ideas (hence the similarity in some areas), but Rust is not a rewrite of C++. If you're interested in syntax and not so much in semantics, then maybe you might like [Nimrod](http://nimrod-lang.org/). &gt; Which is nice but not what it was supposed to be IMHO. Who's dictating what was "supposed" to happen?
Thanks for clarification. Nimrod looks cool but seems to be targeting different space, being garbage controlled. Also less people seem to be developing it.
I was only commenting on the syntax.
Yes, we currently store all state that is specifically unique to each widget (i.e. the cursor position in a text box or what element of a widget was previously clicked/highlighted) within a Vector in the UIContext. State is set, retrieved and updated using a unique identifier. My above comments were specifically in regards to variables *controlled* via widgets, not all variables involved in a widget's state.
&gt;&gt;I daydream about writing a compiler-compiler that adds significant syntactic sugar to the language I think they've got some sugar on hold till its stable? but they also want something easier to learn. battle between concise/explicit
I think I never really deeply understood how first-class continuations worked until I saw presentations of CEK machines. And they discuss the safe-for-space property briefly, a topic I became quite familiar with in my GC studies. One of the main things I should note about the Felleisen Flatt monograph is that it spends more time discussing operational models of dynamic semantics than discussing static type systems (though both topics are treated in the text).
Thank you for your detailed answer! I'm a little bit confused regarding the external dependencies. For instance I'm using docopt.rs (via cargo) to do the argument parsing. My understanding was that this will be compiled into one binary anyway, no? 
If "fn" is ok to you, why not "f"? It's even shorter to type! Come to think of it, every reserved word should be just a single letter! And if there's not enough letters, why not use extra letters from other character sets, like Cyrillic or Chinese. Rust is not English, so that shouldn't be a problem at all!
In the Piston project, we have been looking for a xpath library to use for the Tiled format.
I believe pure-Rust dependencies will either be compiled into the main executable by default, yes. If you're using any Rust wrappers for external libraries (like ncurses or zlib, etc.) then you'll need to figure out which of those you need and ship them with your binary.
I read through it and even learned a bit about how collision detection works! Very nice!
We're gonna do that? Are we removing `Gc&lt;T&gt;` altogether or merely *uses of* `Gc&lt;T&gt;`? Granted, `Gc&lt;T&gt;` was/is not truly a tracing gabbage collected pointer, but I thought the plan was to make it a tracing one someday, not remove it altogather, right? 
&gt; I think I never really deeply understood how first-class continuations worked Ah. Neither have I - but I also haven't really tried. Learning Scheme is still on my to-do-at-some-point list. But I'll keep this in mind when the topic grabs my attention again. (It's not very relevant to the current Rust design-work, I think.)
I think rust just links normally just like C would.
So the plan *is* removing it altogether then. I won't miss something that is basically "Rc with automatic cycle detection" (I remember `@` being this, guess `Gc&lt;T&gt;` is no different). 
ah, thanks! Makes sense.
I don't speak authoritatively, but I am personally in favour of removing `Gc`. It does not have any form of cycle detection or an intelligent cycle collector, it just frees (in a non-robust way) all the `Gc`s left as a task exits.
Just make different tradeoffs than the person who did it first.
&gt; not remove it alltogether, right? See my comment over here: https://news.ycombinator.com/item?id=8313744 (with the small inaccuracy that there are two places left, I thought the AST was the last one)
I imagine things would be far terser with manual memory management? But Rust is blazing a new path with lifetime management and I suppose finding safe ways to make that occupy less space takes time.
In Ruby, which has similar closure syntax, you can drop the `||`s if you'd like: irb(main):006:0&gt; f = lambda {|| puts "Hello" } =&gt; #&lt;Proc:0x007fb0e6a18968@(irb):5 (lambda)&gt; irb(main):006:0&gt; f = lambda { puts "Hello" } =&gt; #&lt;Proc:0x007fb0e6a2bc70@(irb):6 (lambda)&gt; 
&gt; edit: Not sure why the downvotes. 1. Reddit does downvote fuzzing, they may not be real 2. This subreddit is weird sometimes, I wouldn't worry about it. 3. This could be percieved as something that's either bikesheddy or "C++ vs Rust," in a combative way. I don't think it is, but it could be seen that way. &gt; Succinctness is strength. Rust tends to be 'explicit over implicit.' 
Echoing others: wow, great job :) Hopefully all Rust projects can live up to this kind of docs.
/u/rustcvswvj ^(6 hours ago) *(deleted during me replying)* &gt; Can you tell a bit more about the context? &gt; It seems `Gc&lt;T&gt;` usages are replaced with a custom smart pointer. Is that a temporary replacement or what is the advantage? AFAICT, an immutable affine AST is quite close to being maximally efficient for most operations, especially the in-place folding which can eventually have zero allocations. Many (folding-based) passes have almost been halved in run time just from that. If anyone come up with something entirely superior to `P&lt;T&gt;`, I would love to see it :). My **personal** opinion is that GC has no place in a compiler (less so in the Rust compiler). Even then, `@T`, currently under the guise of `Gc&lt;T&gt;`, is a terrible refcounting implementation (probably ridden with bugs, given that removing it from the AST stopped Travis from crashing while testing the compiler). The core devs I've spoken to agree that there is no point in keeping the broken `Gc&lt;T&gt;/@T` implementation around, it would have to be rewritten entirely for an actual GC. I hope a decision will be taken in this week's meeting about it.
Quick note: You spelled "Miscellaneous" wrong in the table of contents. Question: Is your collision detection machine and compiler-deterministic? That is, if I were to use it on two different computers and start with a common state, will it result in the same things happening? See maybe also &lt;https://code.google.com/p/box2d/wiki/FAQ#Is_Box2D_deterministic?&gt;.
I really like the idea (don't know about lenses), however coming from C++ I must warn that the term *view* through me off. From [Boost.Fusion](http://www.boost.org/doc/libs/1_56_0/libs/fusion/doc/html/fusion/view.html): &gt; Views are sequences that do not actually contain data, but instead impart an alternative presentation over the data from one or more underlying sequences. Views are proxies. They provide an efficient yet purely functional way to work on potentially expensive sequence operations. Views are inherently lazy. Their elements are only computed on demand only when the elements of the underlying sequence(s) are actually accessed. Views' lazy nature make them very cheap to copy and be passed around by value. Examples of "views" in this meaning: - single_view - filter_view - joint_view - zip_view - transform_view - reverse_view - nview - repetitive_view - flatten_view Which is a type of views I am really hoping Rust will get (someday). Not that I have any better name to propose; I lack imagination ;/
Yeah I'm not a "names" guy. I mostly care about the abstract ideas themselves. If anyone has a better name than a View, I'm more than happy to hear them out. Other possible names include: * Item * Entry * Bucket * Siterator * Standerator * Lens * Zipper * Cursor * Pointer * Portal * Handle * Index * Search * SearchResult But obviously almost all of these mean other things in other contexts. It's also not clear if Views will remain their own thing, or get merged in with some other related concepts like my proposal for Cursors (reference owning iterators to support bidirectionality and insertion/removal). This is just a wide open development space due to Rust's type/ownership model. 
Yes, the strict ownership/aliasing model of Rust brings up challenges in new places :)
Why not link libc statically too?
Disagree strongly on the "let Linux users compile" part. Linux has a high quotient of compile-ready users, but assuming all Linux users are at that level is a disservice. How will Linux beginners get anywhere if they're always assumed to be advanced techies? And what about smart cities that adopt open platforms and standards to improve security and avoid waste, like Munich? Lots of people there are now using Linux who may not know how to compile. What you should do, instead, is pick at least one of the most popular Linuxes and compile for that. Debian or Ubuntu are good first choices, Red Hat or Arch would be my next guesses. However, the odds that a Red Hat or Arch user are unable to compile are lower than an Ubuntu user, so the usual choice is to compile for Ubuntu (by extension Linux Mint).
I would be very wary of muddying the definitions of lenses and zippers. 'View' seems fine to me. Naming things is always a minefield though. :(
My understanding is that glibc has issues being statically linked. Don't remember why, specifically.
Almost there, just need: * inheritance * C for-loop * unified strings * renaming "slices" * rename enums * immutable struct members
Inheritance in some form is coming, it's a matter of which proposal. I'm on team "composition over inheritance," but there's good reasons for adding some form for cases when it's legitimate. You're probably never getting a C style for loop. I'm not sure what "unified strings" means. "slices" would be renamed-to what? Why do enums need renamed? Unsure how immutable struct members would even work. (of course, all this subject to the RFC process. I'm just describing the temperature as I currently see it)
&gt; C for-loop Why? A vast majority of C-style for loops take the form `(i = value; i &lt; other_value; i += something)`, which is handled by the `range` variants. All other forms can be handled by the appropriate `while` loop.
&gt; C for-loop oh god no why? my list of things i want: * yield for easy writing of generators (C#/Python like) * keyword arguments both don’t need to be in 1.0, but would greatly influence how people write APIs, so i’d like to have them from the beginning.
I have tried getting into Rust three times so far; in September of last year and again in January. I am now trying once more as of last night after reading the first few chapters of the guide. It seems like much of the ugliness that bothered me the last two tries is gone, and there is also a pretty new package manager that looks very promising. The guide is also fantastic and the documentation in general has improved much in the last year. Great job everyone!
I submitted a PR to fix those, thanks!
That's so much horseshit. Spoken like a first-year programmer. "I've got no experience in programming, why would the `range` and variants ever be insufficient!!! No, don't tell me, I don't want to know!". No doubt you've not followed a single discussion on the issue. I know, I know, you wouldn't *dare* get close to an Operating System if your life depended on it.
It seems unlikely that a well done Rust 1.0 can be released before the end of 2015, and even that might be optimistic.
I've been working on one: https://github.com/rust-lang/rust/pull/17138 It's taking a while because I want to make sure I get it right.
Please no inheritance, it's not that useful that often* and you have to change the whole system how you handle "objects". * especially if you have ADT, like Rust has with its enums
Rust _could_ infer type signatures for functions, but even in programming langauges with full-program inference, it's considered a best practice to fill in the types. This is because a small change in one place can break something somewher else when an inferred type changed. Rust is attempting to be a _safe_ langauge. Certain kinds of implicit behaviors can cause safety issues. For example, as you say, casting: casting a number to a different size can cause an {under,over}flow where one wouldn't have happened before. Stuff like that.
1) Standard iterators are just as efficient as C's for loop the majority of the time. 2) You can get a macro for it here: http://www.rust-ci.org/huonw/cfor/doc/cfor/
Why?
Well, obviously it can be released tomorrow if no quality standards are applied. Thing is, there's a lot that needs to be done for a well done release: 1. Implement lots of major planned work that deeply changes the language (the things mentioned in the article) 2. Figure out what to do regarding huge things not designed yet (inheritance, integer overflow, async I/O, garbage collection, etc.) and possibly implement those 3. Once the major language features are all in, months of experience are needed to figure out the incompatible tweaks that need to be made 4. At least core libraries need to be stable, and API design depends on language features, so these will need a further stabilization period of months 5. Once beta/RC is announced, a wider audience will start using the language, probably unearthing issues, missing things, parts that are hard to learn; hence more time to fix those 
Thank you! Much better.
Use [`enumerate`](http://doc.rust-lang.org/std/iter/trait.Iterator.html#tymethod.enumerate): let foo = vec![1, 35, 64, 36, 26]; for (i, item) in foo.iter().enumerate() { println!("The {}th item is {}", i+1, item); }
 $ git log --grep '\[breaking-change]' in a Rust checkout [brings up](https://github.com/rust-lang/rust/commit/467bea04fa1d5fd894d64b2b2901d94260301631) Additionally, if you used the I/O path extension methods `stat`, `lstat`, `exists`, `is_file`, or `is_dir`, note that these methods have been moved to the the `std::io::fs::PathExtensions` trait. This breaks code like: fn is_it_there() -&gt; bool { Path::new("/foo/bar/baz").exists() } Change this code to: use std::io::fs::PathExtensions; fn is_it_there() -&gt; bool { Path::new("/foo/bar/baz").exists() } 
Python. With braces. ;D
Cool! Does this compile down to something like a simple C for-loop?
We actually [already have it][1] (a more limited form than the 'final' one, though). [1]: http://play.rust-lang.org/?code=fn%20duplicate%3CT%3E%28x%3A%20%26T%29%20-%3E%20T%20%0A%20%20%20%20%20%20%20%20where%20T%3A%20Clone%20{%0A%20%20%20%20x.clone%28%29%0A}%0A%0Afn%20main%28%29%20{%0A%20%20%20%20let%20x%20%3D%20vec![1i%2C%202%2C%203]%3B%0A%20%20%20%20let%20y%20%3D%20duplicate%28%26x%29%3B%0A%20%20%20%20println!%28%22{}%20{}%22%2C%20x%2C%20y%29%3B%0A}
FWIW, most of the "What's left to do" features there have significant portions implemented, and the goal is to get them into a backwards compatible state, not completely finish them.
Does the current "Rust Guide" (0.12.0-pre-nightly) reflect the changes in version 1.0 ? I'm specifically interested in the pointers/ownership sections. I'd like to convert some of my experimental code to rust as a way to learn the language, but I've been waiting for the complex types to settle down so I can make best use of my limited time.
It always is current as of HEAD. As changes land, it will be updated to reflect the changes.
I really like the design sensibility that was conveyed in this post. One of Rust's great strengths is being 'a powerful language built on a few core ideas', but this message could be missed by more casual observers due to the language's long, iterative development process.
Now my code won't compile at all. *sigh*
In C# you can add a constraint on constructor with new(), but you can't add a constraint on a constructor that takes parameters. Are these constraints possible with Rust? Rust doesn't really have constructors so I guess this isn't really possible. 
I like you Rust people. You seem to have a good sense of humor :) (Referring here to the "for Turings sake" of rule #6, but also to other stuff I've seen here, like the way you apply CSS to mess with usernames. Good show!) In fact, I like Mozilla as a whole. You are among my favorite people on the internet.
&lt;3. To be clear, criticism is absolutely fine. It just needs to be better than "lolsux." I'm a big fan of criticism.
I agree though. I remember the idea of Rust 1.0 being a syntax guarantee, with freedom for the base libraries to change. Maybe this has changed. Having the base libraries change from under your feet is only slightly less disruptive than having the language itself change. I'm really excited about 1.0, although I have been a bit less happy with the "easy-to-use" unwrap ! sigil/method. Was that decided in? It feels so counter the Rust policy of making unsafe code harder to write. I lost track of these changes when most of the discussion went over to discuss.rust-lang.org, the low-level noise is too much for me.
Rust's constraints are normally written in the form of traits, so you just have a trait with a 'constructor' of the appropriate signature. E.g. `T: new()` in C# could be `T: std::default::Default` in Rust, similarly `T: new(float)` would be possibly by defining a trait with function `fn make(x: f32) -&gt; Self`. This is a more structured approach to overloading rather than the adhoc "duck typing" of `new(...)`.
Inheritance is extremely useful in game development, where you have varied and complex types of entity all over the place. While it does work with enums (I have tried), it is quite noisy and causes lots of code bloat even more than inheritance. It's a dirty hack and just makes the problem worse. That said, inheritance is by no means a perfect solution either. Some sort of solution needs to be made that suits Rust. I'm wondering if associated types could help. Edit: I'd like to politely request that people please take note of rule #7 in the sidebar.
That's great. The current XPath implementation is mostly complete. The biggest missing piece is most of the functions from the standard library. However, you should be able to compile and evaluate an XPath right now. Feel free to file bugs with specific functions or functionality that is missing. Let me know if I can help you get started with the code in any way!
&gt; I remember the idea of Rust 1.0 being a syntax guarantee, with freedom for the base libraries to change. Maybe this has changed. 1.0 will have the language be stable, but also a certain set of libraries. They'll have the stability markers just like they do now, so you know exactly what kind of stability you're opting into. https://github.com/rust-lang/rfcs/pull/236 is the RFC with the `!`, and, as you can see, hasn't been merged, so the decision hasn't been made.
Unless you use musl libc, ucLibc, or dietlibc. Then you can static link easy peasy.
[#7283](https://github.com/rust-lang/rust/issues/7283) is related.
The main reasons for inheritance are fast access to common fields among types, and thin polymorphic pointers. For example, the DOM in Servo needs generic access to all the Node and Element fields for all the various pieces of a document, without going through virtual accessor methods or giant `match`es. There are several ideas floating around that avoid simply plopping inheritance into the language and instead try to reuse existing concepts along with some new orthogonal features instead.
IDEs can be useful, but they should not be required. (thanks huon)
In some way this is only a guess, but I think the `!` sugar for `.unwrap()` will not be added. It was mainly necessary in conjunction with a suggested convention for error handling that made unwrapping extremely common. This suggestion has since been revised, so that unwrapping is again the exception.
Just refactored the RFC based on the comments, settling on the most minimal enum-based variant. If more is desired it can always be extended later. I've also renamed View to Entry at the recommendation of aturon.
&gt;IDEs can be useful, but they should be required. You probably meant to say shouldn't. But anyway it's 2014. We all have quad core monsters on our desks. How exactly would requiring IDEs hurt? What exactly are we using all that processing power for? Please don't get this wrong, I don't want to antagonize anyone but I am trying to understand why everyone is pretending it's still 1990s.
You'll probably need to specify the environment variable `LD_LIBRARY_PATH=path/to/servo/rust/lib` when you run that `rustc`.
Should be fixed by [PR 17192](https://github.com/rust-lang/rust/pull/17192)
Yes, you'll have to statically link everything (and refrain from loading Rust crates dynamically), if you want unwinding to work. Also, rustc -C link-args=-static-libgcc doesn't work anymore because an explicit link dependency on libgcc_s was added a while ago. If you really want to link statically, you'll have to ask rustc for linker command line (-Z print-link-args), and change it manually.
That worked fine after I added /usr/local/lib to LD_LIBRARY_PATH. It may not conform to Arch Linux packaging guidelines, but I'm happy that I only have to maintain one version of rust.
I'm doing the same thing, actually: the last version of Rust that I had on this laptop was 0.8. After the above post (and the regular slew of anti-Go blog posts from elsewhere), I figured it might be worth it to try wading back into Rust again. The newest version is far more pleasant than the other two versions that I tried in the past and the documentation is also really nice now. It's not quite a systems version of SML (a guy can dream, can't he?), but I could *very* easily see myself using it now. Super excited for 1.0!
This is excellent. I'm recently taking a second pass at Rust and really like what I see. I came here to whine that [slice notation](https://github.com/rust-lang/rfcs/pull/198) wasn't mentioned and found on a second look that it was merged 4 days ago. That PR thread made several mentions to HKT as though it is an assumed thing that will happen. I don't see any open RFC for it and the SP doesn't mention it, so I assume this is well-past 1.0? 
Yeah, I worry that there is still a perception out there that Rust is a ever-changing kaleidoscope of mismatched ideas, where as in reality the language has been homing in on a solid set of core semantics for quite some time now. This is a message that deserves to be told more widely. Thankfully niko is setting a great example for us all!
&gt; Yes, doubling the size of every pointer in a data structure Why would that be every pointer? Assuming a type `D` as follows data D = C1 *Common Bool Int | C2 *Common String ... then you have regular pointers `*D` to model the tree. Do you object in principle to the use of enums because their constructors need to be encoded somehow?
While I personally do not see why anyone would want to keep the current `Gc&lt;T&gt;`, I still think an RFC is needed before the action gets taken. Of course, a decision to submit an RFC (that is guaranteed to be accepted) is fine. ;) 
A way to clear the stack frame / registers is the only concrete feature mentioned in the post, and that part is a possible LLVM feature. Rust already exposes `volatile_load` and `volatile_store` intrinsics, along with optimized versions for copying memory, copying non-overlapping memory and setting memory. It's possible to allocate a type's sensitive data via `Box&lt;T&gt;` and then zero it with `volatile_set_memory` in the destructor. Doing it without a dynamic allocation and zeroing any registers touching it is a pipe dream and the complexity needed in the compiler stack and libraries would be enormous. It would introduce plenty of security flaws due to the complexity. Keep in mind that it's a low-level language and libraries have a lot of freedom in building safe abstractions for low-level code. For example, types like `Vec&lt;T&gt;` leave around the data during a reallocation that's not in-place (Rust allows *all* types to be moved via a shallow `memcpy`).
That's fine if your software is popular enough to attract packaging wizards. Until then, not offering compilation for at least one major distro is a rude gesture towards Linux people. Especially as most cross platform tools now do so (at last).
`Gc` is technically an implementation detail of the compiler with few - if any - uses outside that, but feel free to submit an RFC.
&gt; which would have to use not *Common but Common Why? A value can contain a mixture of values and pointers, can it not? &gt; If you use an enum of pointers, you have the fat pointer problem. I think this is true only in the case where your enum contains just a single pointer in each variant.
That example is a pretty strong indication that the brace should move down for functions: fn frob&lt;T&gt;(argument: T) where T: Copy { ... } I'm excited :-)
I've got a quick question about the slice notation. It makes a lot of sense to me, but does anyone know why the syntax for `.as_slice()` is `foo[]` rather than `foo[..]`? The latter seems far more consistent to me when compared to the other three. My second point is more minor, but it's also potentially confusing to someone used to C types (like me). EDIT: looks like there are a lot of people in support of `foo[..]` in the github comments, but no discussion in favour of `foo[]` that I could see.
Which one of those evokes decadence to you? They're all very "real-world", practical sorts of terms to me (for lack of better words). 
Very cool. I guess .take() or similar should return the key as well.
Rationale was `.as_slice` is so common that they wanted to keep the sugar short.
The best way to counter this is with a very clear blog post - as soon as the syntax is stable.
I can understand where they're coming from in that case, but I disagree very strongly. Consistency is *far* more important than saving two taps of the same key. Rust already gets flak for consistency issues (`Vec`, `[T]`, `String`, `&amp;str`, etc.), I don't think adding another one here is a good idea at all. Consistency makes a language easier to learn and read for newcomers, and a lack of consistency without *very* strong reasoning will drive people away quickly.
I think you might misunderstand my post. I wasn't arguing that the feature mentioned in the post is the correct way to solve the problem. The reason I wrote the post was to discuss the actual threat since a lot of people have been arguing that all bets are off in the case of a compromised computer. Personally I believe that a feature such as proposed in the blog post doesn't solve the problem fully, since (for example) a processes registers will be saved whenever the operating system does a context switch. I think the best way to solve the problem would be with help from the operating system. If an OS has a syscall that allows a process to ask it to scrub all of its memory when the process exits, then an architecture could be created where subprocesses can be spawned to handle ephemeral keys. Whenever such a subprocess exits, the OS kernel could scrub all of its memory including process control blocks, thread control blocks, registers and so forth. This would need no compiler support, but it would need support from the operating system and from the programmer.
A follow up to that question is how to make static binaries. You won't achieve good compatibility if you distribute "Linux" binaries with dynamic linking (unless you choose a distro like Ubuntu and target only it..). The best practice on Linux is to distribute source with build instructions and let the distros package it themselves. edit: apparently Rust does static linking by default.
As someone who prefers the `[]` notation, here’s my reasoning: don’t view `[]` as a slice from the start to the end, but as a way gaining a slice/view into something. I think that `.as_slice` and the other `.slice_*` methods are quite separate, because I like to consider `.as_slice()` (`[]`) as a way of converting between types, rather than a way of taking a slice from the start to the end. The ranged slice syntax/methods have a quite different use case from `.as_slice`/`[]`: most of the time when reading `.as_slice`, I don’t think ‘take a slice from the start to the end of the string’ but usually ‘get a slice view into the string’. For that reason, I actually *prefer* the syntax to be separate and less consistent than from the ranged slicing syntax. So I don’t see the syntax inconsistent at all, except maybe for it being *too* similar to the ranged one.
In "Road to Rust 1.0" Niko also mentions that more blog posts about the whole ownership/borrowing thing will come because it is a very important concept central to Rust.
Perhaps if it was a more distinct syntax I would agree with you. As it stands now, it's similar enough that it seems inconsistent, because the similarity lends itself to the idea of slicing from the start to the end. I personally like the analogue of slicing from beginning to end. Would there be problems with supporting both, so that you could use the one more suitable to the current context?
It seems that Gfx-rs uses many of them, as indicated in [this comment](http://www.reddit.com/r/rust/comments/2gcofl/conrod_gui_update_new_improved_api/ckj7t5b). 
Those are the macros which deal with the AST which was only recently moved away from `Gc` :).
`./mach env` in the Servo root directory prints out the environment variables needed to use that install of Rust.
&gt; the `black_box` function TIL the stdlib provides a function to avoid optimising stuff away. That's a neat one, thanks.
Tuple indexing is available, it's merged. I just think it's weird to swallow the key by value and never give it back :-)
Just FYI, the nightly is still pointing at https://github.com/rust-lang/rust/commit/21d1f4d7c0e637d8ae073798e666b880d31781b1 (which is before the fix landed), it'll be a while yet (another 24h?) before the next nightly builds with the fix for this.
There's no reason to store the common fields behind yet another pointer when the whole point of this exercise is to make accessing them fast. The only case that an enum does not need a tag is when there are exactly two variants, one of which is empty and the other is exactly one non-nullable pointer.
Please, show some gratitude to dbaupp, who not only took time out of their day to search for something you could have found yourself, but presented it in a fashion that required no extra effort on your behalf. While your *sigh* probably isn't directed at them, I'm sure they'd value a "thanks" rather than an entirely negative response.
It was just a comment. Rust is still in development and everyone knows that. I'm merely reflecting some of the frustrations that come with joined a project still in development. If I had a problem with anything, I'd have abandoned rust a long time ago. And yes, I am very thankful for the answer.
Oh yes, macros operate on ASTs. Why didn't I realize this? ;) The gfx_macros code didn't explicitly use `Gc&lt;T&gt;`, but the library code it calls did. 
Features wouldn't have 6 weeks of development before being shipped, there would be a release every 6 weeks where features that *are* ready are shipped.
I've been bitten by this too. Anyone know what time the nightly updates at?
Of course, but anyway 6 weeks of feedback by the community before freezing forever new features seem far too short for me. Only a few power users will ever try them.
21.8 megabytes for those too lazy to get out their calculator.
It's been shown that HKT will give us many benefits in many areas. However, it won't require a breaking change and thus can be added post 1.0. That's why you don't see any RFCs for it yet.
I'd be really interested in seeing a longer example of how to use this and the benefits it provides.
Thanks guy who wasn't too lazy to get out his calculator.
And that's more memory than the computer I had running Turbo Pascal.
I would prefer a much longer development cycle as well, but it occurs to me that frequent releases might help adoption by putting Rust in the limelight more often.
You could fit MS-DOS 3.3 alongside Turbo Pascal 4.0[0] in a 1.44 MB floppy. Having the power of doing systems programming and control the whole PC. Being self-compiled has nothing to do with it. [0] Later versions already required a few floppies.
Great! Do you mind if I translate this post (and maybe others when they will be published) into Russian? It probably could bring more people into the community :)
+1
Pascal doesn't have a borrow checker, or generic types, or traits, or multiple pointer types, or AST macros, ... In addition, Turbo Pascal was a single pass compiler, which allows extremely little room for optimization. You had to basically emit a sequence of instructions for every statement, and that was it. There's no room for optimizing a function call, much less a whole program. Everything had to map 1:1 to assembly. The Rust compiler generates several stages of AST with many passes before even passing it off to LLVM, which then optimizes further and emits code which is dramatically more efficient than any compiler typical of the 80s. In the 80s, the main concern for compilers were memory usage (those symbol tables were the main blocker with compiling large projects - if they got too big, they wouldn't fit in memory and your compile would fail), speed (CPUs were really slow, nobody wants to wait an hour to compile a few thousand lines of code!), and size (it has to fit on a computer with a few megabytes of disk space at best), rather than the quality of generated machine code, or the richness of language features. Priorities have changed dramatically since Turbo Pascal, the size of the compiler binary is not really the important factor. I wouldn't call it bloat for this reason. Bloat is when you have parts which are unnecessary, and I don't think rustc has a 547:1 ratio of bloat:normal code.
I wasn't aware of that. It makes sense - CPUs have been x86_64 for over a decade except in low end models (Celeron, Atom), and OS X has tight control over the software shipped on their systems (unlike Windows, where many users unwittingly install and many OEMs ship 32bit windows on systems with 64bit CPUs).
Indeed. The tp4 compile would run in a computer with 256k of ram. The IDE too, just not both at the same time.
I'm on mobile so sorry for lack of links, but the github wiki has links to papers that have driven design decisions for rust (IIRC). The developers of rust like to say that they aren't doing anything new (another reason why it's called rust!), for example the borrow checker is based off a language called cyclone. If you want to look more into how the borrow checker or the type checking passes work, ask around in IRC where to find the doc.rs files for those modules. Good luck with the thesis! Edit: [got it!] ( https://github.com/rust-lang/rust/wiki/Note-research)
While I think your claim, “priorities have changed,” is fair. It’s notable that the Modula-2 compiler is 4M. Modulia-3 is 8M. Niklaus Wirth, author of Pascal and Modula-2, continues to stress the importance of implementations that minimize space and time complexity. Whenever I’m waiting on a program to compile or a humongous runtime to download, I think of him. Aside: I’ve been experimenting with Swift recently and it’s evident that they’ve tried to replicate Wirth’s approach. This is especially useful for interactivity. Apple has an application, “Swift Playground,” that imitates a REPL by constantly recompiling your code. It’s neat.
FEM solver, 'cause science! \o/
Mostly experiments related to compilation. However, I’ve been gearing up to port http://stat.ethz.ch/R-manual/R-patched/library/stats/html/00Index.html. It’s ambitious, but I think it’d expose a lot of interesting problems in the language, compiler, and ecosystem.
In all seriousness though, the scientific community is littered with horrible c (and other) code "because it's fast"; written by scientists/engineers who have very little formal education in programming methods. It turns out that while stuff works, you don't want to see the results that valgrind produces. Anyways, I see Rust as an optimal middle ground for us and hope that more people will produce libraries for various scientific fields. (hint: I need a pure rust lapack replacement)
This is just a silly (but I think very useful) thing I made in a few hours. The error reporting is terrible, the code is terrible, etc. I'll work on make it more user-friendly in the future.
If garbage collection isn’t a problem, there’s a forthcoming LLVM backend for Idris.
My main project are nice Rust [bindings](https://github.com/SiegeLord/RustAllegro) for the [Allegro](http://liballeg.org/) C game programming library that I also maintain. Otherwise, the next most important project is [RustGnuplot](https://github.com/SiegeLord/RustGnuplot), which is a Rust wrapper/controller over [gnuplot](http://www.gnuplot.info/). When time allows I also work on a [yet-another configuration file format](https://github.com/SiegeLord/SLRConfig), which is a pre-requsite for me to start a new game in Rust. Even more rarely I work on my expression-template based [linear algebra library](https://github.com/SiegeLord/RustAlgebloat).
Me too. I think this is the clearest and most honesty summary of Rust’s mission I’ve read. It left the impression that they’ve refocused from experimentation into implementation. It’s exciting.
[d3cap](https://github.com/jfager/d3cap), a network activity visualizer, for playing around with pcap and d3, though the d3 part got boring a while ago. Sporadically working on a cli, wifi header parsing, and vague plans for trying to use Piston for the viz layer. 
I think curly braces (and square brackets) work already.
okay need is too strong of word, I'd LIKE a pure rust replacement, but a nice wrapper would be okay as well. EDIT: I feel I should explain myself better after thinking on it more. For me it's like including Fortran code in a C project, it works, sometimes really well. Yet it still splits your development environment into multiple compilers and tricks with linking for no necessary benefit besides that you know the code your including works. IMO, and I could be wrong, but you'd loose the benefits of rust without well designed bindings to the C parts.
I am thinking that if the feature is explicitly tagged as experimental (feature gated), then it could be present in a release without any commitment.
An application server.
Your comment is being evaluated relative to the (potential, expected) comments on this submission, whereas the submission is evaluated relative to other submissions in /r/rust. It's hard to compare the two. It's not too weird, it happens regularly.
oh oh, do a compile time comparison next. It was pretty mind blowing how fast TP4 would compile my junk on a 4.77Mhz CPU. There'd probably be a few orders of magnitude of difference as well. 
Thank you :) Static analysis is also an area of interest for me. Did you end up making any decisions based on this or have you had any headway? :)
Kind of new to Reddit as well, but I expected that all those interested enough in the link to view its comments would have found the link itself worthy of an upvote. I guess that's only true for other subs.
The vector would need to have type `Vec&lt;Box&lt;ExampleTrait&gt;&gt;`. You can then make one like so: `vec![box ExampleThing1::new() as Box&lt;ExampleTrait&gt;, box ExampleThing2::new() as Box&lt;ExampleTrait&gt;]`. This casting using `as` is a limited form of dynamically sized types - it only works for boxes and references at the moment, not `Rc` or `Gc` or other types.
In my experience the comments are often more interesting than the link itself. There are minimal content link (like this one) where the comments have at least some discussion. There are bad links (bad titles, flamebait, inappropriate for the subreddit, etc) where the comments explain why it is bad. There are even good links but with comments that are just better (interesting discussion, better links in comments, etc). On average comments are worth more than the links itself.
good point. but future avenue for getting keys back soon opening: entries? post 1.0? ;-)) 
Hmm maybe this can motivate getting proper language support. Nice (:
Maaaaaaaaaaybe.
:D
Eek! Curly braces on a new line?
I'm doing a free-cell clone of my favorite commandline version: http://www.reddit.com/r/commandline/comments/2f7nz6/source_for_a_cli_freecell/ It's pretty simple so it's allowing to me to focus on learning the language rather then solve a difficult problem while stumbling over language idiosyncrasies. 
Thank you very much for your reply! I read about boxes and they are exactly what I was looking for. However, this code doesn't seem to compile: fn main() { trait ExampleTrait { fn example_method(&amp;self); } struct ExampleStruct { things: Vec&lt;Box&lt;ExampleTrait&gt;&gt; } struct ExampleThing1 { just_some_random_data: int } struct ExampleThing2 { other_random_data: f64 } impl ExampleTrait for ExampleThing1 { fn example_method(&amp;self) { //do one thing } } impl ExampleTrait for ExampleThing2 { fn example_method(&amp;self) { //do another thing } } impl ExampleThing1 { fn new() -&gt; ExampleThing1 { ExampleThing1 { just_some_random_data: 1} } } impl ExampleThing2 { fn new() -&gt; ExampleThing2 { ExampleThing2 { other_random_data: 1.0} } } impl ExampleStruct { fn new() -&gt; ExampleStruct { ExampleStruct { things: vec![box ExampleThing1::new() as Box&lt;ExampleTrait&gt;, box ExampleThing2::new() as Box&lt;ExampleTrait&gt;] } } } } The error is: main.rs:8:29: 8:41 error: explicit lifetime bound required main.rs:8 things: Vec&lt;Box&lt;ExampleTrait&gt;&gt; Any ideas on how I could fix this?
curious, why not julia? it seems to be gathering momentum in the scientific community, and have some nice language design behind it.
Building a simple raytracer because it's fun.
The focus is on semantics, not speed. But that doesn't mean nobody is working on speed, either. But there's not a huge effort right now.
I'm gently pushing for making keys more accessible for the sake of Sets, because that's all they know. The basic Set operations are quite weak because of how maps treat Keys, but I don't have any concrete use cases where keys aren't completely disposable/indistinguishable. If you can provide a real scenario where you really care, I'd love to have it. For now though, treating keys as ignorable junk *does* actually simplify things. Also passing a value to Entry might not actually be a thing in a future where we get the proposed Borrow&lt;T&gt; and ToOwned&lt;T&gt; traits. It could be a reference that gets cloned into a value on demand (which is farily reasonable for the Entry usecase -- you're basically expecting to have a Key already be there, in which case you don't need a key at all).
Libraries.
That was basically the motivation. The idea was to be able to do processing at some point in *some other thread* rather than *this other specific thread*. I had a similar system using channels here: https://github.com/reem/rust-actor where you have long-running workers that receive requests over a channel, but that also has a blocking interface.
[julia has a perfectly usable ffi](http://julia.readthedocs.org/en/latest/manual/calling-c-and-fortran-code/)
And if we want a comparison that would actually make Rust look *good*, we could look at the efficiency of generated machine code.
It would be nice if you can publish it. I am currently learning Rust and using it to do my algos.
You aren't very observant, are you.
Will compile speed get priority once 1.0 is out?
Everyone wants things to compile quickly. I don't think that we've made any priority decisions after the 1.0 event horizon, everything is focused on getting a good 1.0 out the door.
Thankyou. One other question is what order of improvements we could hope for. 20%? 2x? 3x? 10x?
It's kind of freaking me out. 
Thanks for getting those! I knew they were online, except I didn't want to go through the docs on mobile... I misworded, I meant that someone should find you those links, and I'm sure they'd give them to OP if OP asked on IRC
An RTS: http://imgur.com/a/kmGTc (Screenshots of the map generator)
That sounds really interesting, do you have a github repo/blog where you post updates/anything like that?
I think /u/lelarentaka meant *writing* libraries, not using them. How easy is it to call Julia from other languages?
[doable](http://julia.readthedocs.org/en/latest/manual/embedding/), though certainly not as easy as in rust.
It seems like rather than having a declaration followed by an implementation, it would be more convenient to have the macro take the function body too and include the definition in its expansion. Or would that make it too difficult to reexport the macro wrappers?
They do. `macro_rules!` is a syntax extension (or works like it).
When I'm not working on school work or some of the non rust projects I'm doing, I'm working on making a better alarm clock in Rust. I'll be straight up honest here, I suck at waking up in the mornings. I somehow manage to shut off every alarm clock on my computer and the physical clock radio across the room all without ever really being awake. After I wake up again later, I often don't recall my alarms going off and sometimes I vaguely remember the short trip I made to cross the room to shut off the alarms, but it still isn't helping me wake up on time. So as a programmer, I'm going to try to solve some of my issues with programming! I'm designing and writing a better alarm clock desktop application (since a website dies with the browser which is FAR to easy to disable/forget to set) that I hope is cross platform and can become the leading alarm application. I've tried several others but they all have shortcomings that just irk me. Some let you play music from your library to wake up to, but there is no shuffle option? Why can't I wake up to a different tune every day? Others have issues that they don't allow you to set weekly rules. I'm wanting something like an optional calendar integration (perhaps google calendar) so that you can set recurring schedules for waking up and on those rough days where you forgot to set the alarm, it will know that you meant to wake up at the same time you've always gotten up. Have a vacation? Exceptions are easy to add in the calendar. Gaah I just have so many ideas for a good alarm app but I just don't have the time to work on it since I'm on my last year of college :/ Eventually when I get enough done to show it off I'll post about it here. Also, I will eventually get back to work on [Oxidize](https://github.com/jroweboy/oxidize), which was my first ever Rust project. Its a web framework I made in Rust and was a really fun way to learn Rust and learn different problems regarding designing web frameworks.
I wouldn't mind if the answer was "no". Right now compilation is slow, but bearable. 
Oh man, I was secretly hoping nobody would ask questions. I created a github repo from some source files as a quick-and-dirty backup about twenty days ago: https://github.com/jswrenn/rustylog It's incomplete all around, so it should be interpreted mostly as a doodle. This is a project for a compiler design course that I'll eventually finish at my college. I had been previously been working with SML, which is a fantastic language for representing mathematical ideas. Algebraic datatypes and pattern matching made it a great language to write my notes for this project in. However, I wanted the speed and familiarity of your average curly-brace language (i.e., C/C++/Java). I quickly ran into the road block that without pattern matching, certain things which are very terse in idiomatic SML are incredibly verbose in idiomatic Java or C++. Then, I discovered Rust this summer, and have totally fallen in love. The tokenizer was the first thing I wrote, and it's very elegant. I'm sure I'm still pretty bad at writing Rust code, but the fact that I could make my code look that pretty despite my inexperience is a testament Rust's design. I'll do a write-up of both Rust and implementing logical languages when I finish my implementation!
Sweet! I'll keep an eye on the repo, looks really interesting!
If compile speed is an issue, you can always try building on an RAMDisk. It's easy to do it in Linux (presumably Mac as well), and AMD provides RAMDisk software for Windows that's free with a size limit of 4GB. You can set it up so the RAMDisk saves to an image on shutdown and restores on startup. I used to build on a RAMDisk when I was freelancing Android apps. If you think `rustc` is slow, you should try `javac &amp;&amp; dex`. 
Compilation is not IO bound.
Strange. I would expect it to be. What's the most time-consuming stage then?
Yeah, an `examples` and/or `tests` directory would be very helpful.
[GLTF](https://github.com/KhronosGroup/glTF) [loader](https://github.com/kvark/scene-rs/tree/master/src/gltf) for [gfx-rs](https://github.com/gfx-rs/gfx-rs)
Merged! \o/
No breaking changes but it will call for some API changes in the standard library - once Option and Result are proper functors and monads they don't need the ad-hoc map and bind methods.
I think a name other than `macro_rules` would be more friendly (just `macro!` perhaps).
That said, this is already tangibly useful to me. Great stuff! :)
Common Lisp's macros are the best, Rust should just copy them. 
Realistically, you'd just need something that works with a clean API. It doesn't matter if it works via a FFI or it's suboptimal in pure Rust. The only thing you're tying yourself and users to is the type signature. The type signature is worth contemplating. There's two ways to go about it. You could go the way of Python/R/Matlab and make a multidimensional array datatype. This is a struct that has a pointer to the first element, and a tuple with the shape of the multidimensional array. The thing could be complex valued m by n by k by j by i. The other way is what armadillo and theano do. Every operation on the data is put into a graph. This graph is optimized and the function that operates on real data is the return value. There's tons of interesting optimizations that are available here. You could use DP to optimize the order of dot product associations. You could simplify expressions given what you infer about the inputs from previous operations like symmetry and bandedness. If you make a compiler/macro monstrosity you can later make it emit to different backends.
&gt;Reddit does downvote fuzzing, they may not be real Not anymore. 
Are the object files produced by EWARM compatible with those produced by LLVM? If yes I guess there's nothing preventing you from linking them together.
That's disingenuous IMO, because you either know what `foo` does or you don't. It doesn't matter whether it's a function or macro. Personally I don't think macros need to be indicated in some special way. The great thing about macros (at least good macros, I haven't experimented enough with Rust's to determine if they're anywhere near as good as Common Lisp's yet) is that you don't have to know what they do exactly when you read the code. 
Rust reads each source file (and the metadata of each library) once, there's no header rereading or anything like that. The majority of the time is spent doing optimisation (and the next slowest is type checking, iirc). You can see the breakdown by passing `-Z time-passes` to `rustc`.
No it doesn't. The votes move up and down slightly because people have voted. Vote fuzzing was never visible upon page reload either, and it never affected the totals, only the upvote and downvote counts. You can't see those anymore, so it's impossible for vote fuzzing to occur, because it would be invisible. 
In a few reloads I saw all of these for the OP (in the order in which they appeared, collapsing repeats): Points | % upvoted | votes ------|---------|----- 22 | 71 | 52 22 | 70 | 56 24 | 73 | 52 25 | 73 | 55 20 | 69 | 52 21 | 70 | 53 
Not really possible with syntax and more complex type system. But I agree that macros are great.
Trying to write example for this, but I keep hitting unboxed-closure-related ICEs :/
You maintain Allegro these days? Wow! I credit it for getting my systems programming start back in the day :)
This was over a period of seconds, and the `votes` column is the total number of votes; I cannot believe that many people upvoted/downvoted (or removed their vote) this 2 day old post in that time.
To add to that, I see similar changes on [a 2 week old post](http://www.reddit.com/r/rust/comments/2f9yp1/introducing_libpnet_lowlevel_networking_using_rust/), so there's definitely some factor that's not just people voting that changes the vote counts.
&gt; Personally I don't think macros need to be indicated in some special way. The great thing about macros (at least good macros, I haven't experimented enough with Rust's to determine if they're anywhere near as good as Common Lisp's yet) is that you don't have to know what they do exactly when you read the code. The 'problem' is Rust macros are flexible, and allow arbitrary token sequences as arguments (the only restriction is balanced `()`, `[]` and `{}`), so it would likely complicate parsing significantly, at the very least, it would require some form of name resolution to work out if some invocation needs proper parsing or not. E.g. https://github.com/huonw/brainfuck_macros takes stuff that is definitely not valid Rust code.
I'm building a fuzzy string matching library. Developing in rust is fun.
A text editor. Right now I have a development environment where vi is too memory hungry, so I figured I could kill two birds with one stone. Learn Rust, and have a decent editing experience in the end, hopefully. I love love love working with iterators in particular. Fun stuff.
Servo's got a list of [interesting projects](https://github.com/servo/servo/issues?q=is%3Aopen+is%3Aissue+label%3AB-interesting-project) if you want something teeth-sinkable.
I want to develop a tiling windows manager, but first I have to get the xcb bindings working. I've spent a good part of the last week porting some old 0.7 code to the latest nightly, with limited success. But I did get a port of tinywm working using libx11 bindgs. 
I would eventually like to see macro_rules! and procedural macros unified, or at least make procedural macros more like lisp quasiquoting-style macros.
I've been having a lot of fun building data structures lately. My largest project to date has been [Turbine](https://github.com/polyfractal/Turbine/), a port of LMAX-Disruptor to Rust. It's basically channels on steroids for low-latency message passing. Currently working on a library which implements a variety of sketches and approximate algorithms (e.g. HyperLogLog, PCSA, Count-Min, Bloom filters, wavelets, etc).
Coming from someone who used go for a while. In my mind using match and Option is far less tedious than using and if after every funcron call. Its also "optional" with calls like unwrap()
Linux has a very short release cycle and an OS should be very stable, so how does it work? In practice, features aren't developed between one Linux release and the next, but the development might span many releases, and is merged when it is done (also: things may be merged in small chunks one release, then a bit more in another, etc). And since things are *always* backwards compatible in Linux, it isn't too much trouble to select a given version to be "LTS" and upgrade only when you need a newer feature. I suspect that many "serious" Rust users won't really use the absolute newest version.
I'm switching from Python to Julia at the moment for a lot of my scientific work actually but, for some things a compiled version of a program is better.
Won't type changes in the standard library be "backwards incompatible"? (an example is the applicative-monad proposal of Haskell, that had first to change a lot of code in Hackage to conform to the new constraints)
Yea I realize that a clean API should work just fine no matter what the library is actually compiled in. The graph ideas for matrix operations sound interesting, I believe I read a paper recently on it, but I should look into it more.
Why not with Wayland? :)
I just want a tool that takes the source code and pretty prints it. That way one could hook it on Emacs after saving. I don't even care about the conventions..
I understand how it works, but Firefox's aggressive release cycle has made things difficult for Debian, for example, and I fear Rust won't warrant the same special treatment that Firefox received.
So you're saying we're also getting around 547 Turbo Pascals of benefit?
This is a pretty popular subreddit. 
Ok, a few random unpopular posts from 2 months ago: - http://www.reddit.com/r/rust/comments/2aur8x/using_macros_to_parse_file_formats/ - http://www.reddit.com/r/rust/comments/2atpxl/rustpy_calling_python_from_rust/ - http://www.reddit.com/r/rust/comments/2au5w9/if_rust_becomes_a_language_in_which_libraries_to/ Repeated reloading see their score changing significantly (e.g. the first ranged from 2 to 6) without the total number of votes changing much; there's no way you can tell me there still is &gt;4 people voting and then *unvoting* on those.
That should really be a read-macro or equivalent (i.e. not really a macro at all). There's a very significant difference between operating on the character or token stream during parsing (what that is or should be doing) and operating on the AST after parsing (which is what something like `println` should probably be doing). Reader macros allow you to do things like #s(SELECT FirstName, LastName FROM Students WHERE id = #id) and have your SQL read macro turn that into something else while recognising the `#id` and grabbing the value of `id` and putting it into the string, etc. etc. etc. That happens during parsing. That's an entirely different thing to normal macros, and I don't like that Rust conflates them. 
Okay that's a fair point. That's certainly not what they said when they announced the changes to voting, and it's certainly not how it worked in the past. That's really quite very odd. Like really, really, really, really odd. I've not seen it on any other subreddit, either, but I might just be blind. Not that much anyway. It's still there when I turn off subreddit styling in RES, though, and I didn't think subreddits could use anything but custom CSS. 
Well the big win is that in rust you HAVE to explicitly consider each possible match arm so you can't forget to handle a possible error in go you can write val, err := func_call(); //use val without checking err in rust you HAVE to write let val = match func_call() { Ok(v) =&gt; v, Err(e) =&gt; //do something with the error or explicitly ignore it with an empty block } Also, unwrap is highly discouraged.
link? 
Actually, the go compiler will complain about err being unused (unless you have multiple variables called err, which can be a problem)
https://github.com/aturon/rfcs/blob/slice-notation/active/0000-slice-notation.md &gt;Make the coercion extensible, via a trait. This is opening pandora's box, however: the mechanism could likely be (ab)used to run arbitrary code during coercion, so that any invocation foo(a, b, c) might involve running code to pre-process each of the arguments. While we may eventually want such user-extensible coercions, it is a big step to take with a lot of potential downside when reasoning about code, so we should pursue more conservative solutions first. I personally think that this is the best way to go. Arguments that people can abuse it are not really fair IMO, especially in a language with macros with support for [this sort of thing](https://github.com/huonw/brainfuck_macros). C++ supports operator overloading and conversion overloading, and I have never seen an abuse of conversion overloading (except by myself for shits and giggles in some intentionally hard-to-understand non-production code) but I've seen a LOAD of abuse of operator overloading, even in relatively well-received libraries like `boost::spirit` where I personally think it obscures meaning more than it clarifies anything. TL;DR on that point: Rust has operator overloading, which is way more ripe for abuse. --- I personally think that using `[]` to mean `as_slice` and `[m?..n?]` to mean `slice{,_from,_to}` is quite confusing as well. I also think that having `..` *and* `...` and having one mean inclusive and the other exclusive is *really* ripe for confusion. I also think that this all seems pretty damn special-cased. Perhaps it can be extended in a way that makes it nicer to extend to more types? I'd quite like to be able to do `mat[4..5,1,1..3,...]` with a user-defined type, for example. It doesn't look like that would be possible with this, whereas it's pretty much trivial to support that sort of thing in Python. 
I think it's very much intentional for error checking in Go to feel like error checking in C. 
I know it's pretty bike-sheddy, but I just can't get over how ugly it is to write `foo.iter().enumerate()` instead of `enumerate(foo)`. Or indeed `foo.iter()` instead of `iter(foo)` or just `foo`. 
Rust's compiler is already quite fast. The compilation speed issues are primarily library design / implementation and language design issues. The compiler's code generation isn't awful, but 90% of the time in an optimized build is spent in LLVM passes and linking. It's not much different than Boost Spirit causing ridiculously slow compile time in Clang.
There would be a 15-30% compile-time improvement if exception support was disabled by default. There are other potential compromises but I don't expect any miracles from the compiler itself. An overhaul of the standard libraries would do a lot more than compiler work.
If you're using Julia for scientific computing tasks, the chances that you'll want to use any other language go way down.
Sounds interesting. Is there a public repo? Btw. please *don’t* call your shell ``rush``. Thanks ``;-)``
The extension you mention is a backwards compatible generalisation.
Looks nicer with code like: for x in xs.iter() .map(...) .filter(...) { foo(x, y, z); } Compare with: for x in xs.iter() .map(...) .filter(...) { foo(x, y, z); } 
I always felt like Option arguments should have been, well, optional. Glad I can finally make this the case.
&gt; I'd quite like to be able to do mat[4..5,1,1..3,...] with a user-defined type, for example. In Python this works because the parentheses around tuples are optional when it’s unambiguous, and `a:b` inside an indexing expression is shorthand for `slice(a,b)`. Getting this in Rust in the same manner as Python would be nice, but would require a number of changes (some backwards-incompatible): 1. Add a new primitive range type, `T..U`. In an expression, this can be of the form `a..b`, `a..`, `..b` or `..`, and has similar patterns (which would now be unambiguous due to range patterns using `...` instead). This could also implement `Iterator`, removing the need for `range`, `count` and friends. 2. Add multidispatch, the RFC for which has already been accepted. 3. *Remove* the newly-added `Slice[Mut]` traits, favouring `Index&lt;T..T&gt;` instead. 4. Implement `Index&lt;uint..uint&gt;` (along with the existing `Index&lt;uint&gt;`) for `&amp;[T]` &amp;c. 4. Allow tuples to have their parentheses omitted when unambiguous. This means that `foo[1..2,3..4]` would be a valid expression, indexing `foo` with a value of type `(int..int, int..int)`. Unfortunately, that’s never going to happen (too many breaking changes), so it’d be best to make some kind of special case for slice expressions.
Actually that isn't necessary in the slightest. Just let the `[...]` operator take a variable number of arguments.
Is it not possible to ignore the error if I'm not interested in the success value? This is almost always the case for calls that return `()` on success, like the write method of Writer trait.
yes, but that will create a warning, since `Result` is marked as a return value which must be used by its caller.
Somewhat off the topic of *Rust code*, but your woes of waking up piqued my interest. I used to have the exact same problem. Tried lots of things to no avail, but I finally got a sunrise simulation alarm clock and it has worked out really well for me. It turns out that most of the time my room is really dark (blinds closed, etc.), so there's very little natural light in the morning. The sunrise simulation slowly raises the brightness of the room for about half an hour until the alarm goes off, which triggers your body's normal reaction to slowly bring you out of deep sleep. It really has worked wonders for me. I may wake up feeling tired or well rested depending on how long I slept for, but I *always* wake up and no longer play those silly alarm clock games just to get up on time. I think I'm also making more efficient use of my sleep time as well, since I don't have to set multiple alarms starting 1-2 hours in advance of the time I really need to be up. [This is the clock that I got](http://www.amazon.com/Philips-HF3520-Wake-Up-Colored-Simulation/dp/B0093162RM/), though that's quite pricey. I got it on Amazon for $120, so maybe you can find it cheaper somewhere else. TL;DR: Offtopic sleep talk. Now back to your regularly scheduled Rust banter...
good catch(its been a while since i touched go code)...but you still get the problem of being able to write val, _ := func_call(); which doesn't seem like an issue until you see code that looks more like left, rght, _, _, _ = get_directions(); 
Mmh not completly on topic but why did they not use the common lisp, dylan exeption handling system? That seams to be best of class and can be implemented without much overhead. Had this been considered?
Verbosity of defining inner functions. Why can't we capture types from the outer scope?
I mentioned this in a comment thread yesterday, and there's a bit of discussion in the github comments, but since it's being posted properly here I might as well say it again: I'm personally in favour of having `foo[..]` as the full-slice syntax rather than `foo[]`. **Pros:** it's internally consistent with the rest of the accessing/slicing syntax. * `[i]` = index with `i` * `[i..]` = slice from `i` to end * `[i..j]` = slice from `i` to `j` * `[..j]` = slice from start to `j` That's all consistent. But `[]`? There are no clues as to what that would do. No number, so it can't be indexing. No `..` so it doesn't *look* like slicing. Now the C part of my brain thinks it might be a type declaration - but that's the wrong language. A typo then, a forgotten index? Seeing this in code, that's what I would assume. `[..]` however, that gives clues. You can immediately tell it's a slice, and it's pretty easy to guess that it goes from the start to the end. It doesn't look like a typo either. Other languages, like python, use this style of full-slicing and it seems to work well. **Cons:** I'm just going off the two pros I've heard, I don't really agree with them. 1. Time saving. It's true that `.as_slice()` is used fairly often, so concise syntax is good, but I can't imagine that two extra '`.`'s is going to slow anything down dramatically. 2. Full slicing is somehow different to partial slicing and so should be differentiated. I can see the argument behind this, but I don't really agree with it. Full slicing is just a special case of partial slicing conceptually, so why not make the syntax match? I know that full slicing gets used in Rust in situations unrelated to partial slicing (due to the type system), but I think it's better to have syntactic consistency in this case. If that argument were to be taken though, I'd feel like `foo[]` is too similar to the slicing and indexing notation and would cause confusion, like I mentioned above.
Thanks! Nice explanation of Rust's concurrency.
Why do you think it's ugly? I don't think it's too bad, and I'm coming from Haskell where it would look more like the latter. It also avoids namespacing issues by not hogging global names.
Like `cfor!`?
Go [doesn't have warnings](https://golang.org/doc/faq). Only compile-time errors. It's one thing that pisses me off about the language. Yes, warnings are bad. Yes, I'd want to eliminate warnings for production code. But, God-dammit, warnings are essential for development - so you can experiment, try things out, debug. --- Yes, I know, a bunch of n00bs are gonna tell me I'm wrong and warnings are pure evil. Sigh.
The main reason I think it's ugly is that I dislike most object-orientation in general. I generally prefer generic functions that operate on data no matter what it is. def enumerate(iterable, start=0): i = start for x in iterable: yield i, x i += 1 That's pretty much Python's `enumerate` function in a nutshell. In reality it's implemented in C for performance reasons of course. Anything that obeys the iterator protocol in Python can be enumerated with `enumerate`. Now I know that in Rust the `Iterator` trait gives you an enumerate method as long if you implement it. But to me that feels wrong. `zip` might be a better example. Doing `foo.zip(bar)` is as weird to me as `foo.add(bar)`. `zip(foo, bar)` feels more clear in its meaning. It's a symmetric operation, why `foo.zip(bar)`. I also think that `foo.iter().zip(bar.iter())` is *far* less clear than `zip(foo, bar)`. In response to the global namespace issue, I don't see why `enumerate` would be in a global namespace in Rust when everything else is in `std::alpha::beta::gamma::delta::epsilon::zeta::eta::theta`.
I also think it looks nicer with some of the `where` syntax that was linked on here recently.
It's not apparent to me why [..] isn't in the RFC. It seems to have been discussed favorably and I'm surprised it's not in.
But what is meant when you have one element? Was it the start or end point?
Yep, for example: impl&lt;T, A&gt; Signal&lt;T, A&gt; where T: Send, A: Send + Action&lt;T&gt;, { pub fn spawn(mut self) -&gt; Signal&lt;T, Receiver&lt;T&gt;&gt; { let (tx, rx) = channel(); spawn(proc() { for msg in self { if tx.send_opt(msg).is_err() { break; } } }); Signal(rx) } }
I guess I wasn't so clear there. I meant using the same syntax for sub-slices (just without indices) for a full slice, rather than the `..`/`:` syntax in particular. I hadn't heard that name though, that's fantastic.
Those are certainly valid points, and your example is on-point. Your point about global namespacing is valid, I think, but I honestly prefer the trait implementation currently. I can make my own functions called `iter` and `zip` and not have the potential for clashes. For such standard functions, I much prefer having their names less clash-able. I personally would prefer `foo.iter().enumerate()` to `iter::enumerate(foo)`, in part because it's more consistent and doesn't depend on your importing.
I have a high opinion of Walter Bright and he is against warnings in the compiler. Currently, I cannot find a good citation/link, unfortunately. However, as far as I understand he thinks warnings belongs into static analysis tools, but not into the compiler. Thus he is not against warnings per se. I believe his main argument is that warnings slow down the compiler. edit: [citation of a citation](http://forum.dlang.org/thread/yzxwyzknvyxdqjuwjuyw@forum.dlang.org#post-mailman.2350.1382231712.1719.digitalmars-d:40puremagic.com) :)
and `[::-1]` is the martian smiley.
I dont think my poor brain can handle the frustration of getting wayland all setup with rust. Sounds like a project for the future though.
My personal preference is for `[..]` too - seems consistent. The reason we went with `[]` was mostly because it will be the syntax used to avoid `as_slice`, where today you write `some_vec.as_slice()`, you will be able to write `some_vec[]`. Since that is a common operation, we wanted it to be as concise as possible. If we implement some other sugar for this (there are two cross-borrowing RFC PRs in the works which could influence this), it might then be better to change to `[..]` or remove the `as_slice` sugar completely.
I've migrated both my C and Rust style to it. It's easier to read. Clarity above compressing lines.
The namespacing issue is less important to me than the fact that `a.iter().zip(b.iter())` is hideous compared to `zip(a, b)` or even `iter::zip(a, b)`. I can understand that you might not agree though, which is why there are multiple languages, right? :) I wish more languages had two namespaces like Common Lisp. A function namespace and a variable namespace. It's a little annoying calling variables `lst` or `L`, `iterable` or `it`, etc. when good names like `list` and `iter` exist, just because they're used for functions. Sure, it means you have to explicitly mark when you're trying to use a variable as a function, but I prefer this (in a Python-like syntax): list = list(1, 2, 3) the_list_function = #'list to this: C has something a bit like this in that the namespace of `struct` types is different to the general namespace, so you can write this: struct foo { int n; }; int main() { struct foo foo; return 0; } Incidentally, this is one of the many reasons why I don't like `typedef struct { } foo;` - it clogs the global namespace with the only thing in the language that doesn't have to be there. &gt;I personally would prefer foo.iter().enumerate() to iter::enumerate(foo), in part because it's more consistent and doesn't depend on your importing. As a final note, I mean no offense here, but I must say that I find the argument that you shouldn't have free functions even in a language with excellent namespacing because they might conflict a bit disingenuous. That's like saying you shouldn't have `std::vector&lt;T&gt;` in C++ because someone might want to create a `gl::vector&lt;T, N&gt;` and they might be using `using namespace {std,gl};`. It pretty much defeats the purpose of namespacing if you build a library around the assumption that people will just import everything into one global namespace, doesn't it? 
A parser generator syntax extension for Rust. You give it a BNF grammar, annotated with Rust-coded actions to perform for particular expressions (similar to Boost Spirit), and it produces a fast Earley parser. It uses a custom type inference algorithm to ensure that all actions are safe and statically typed. It will eventually include decent error reporting for end users. It supports any token type and lexer that implements my Lexer trait, including lexers based on Rust token trees, so you can even use it to write syntax extension parsers. No release yet - I'm still working on the code generation phase. When it's ready I'll post to this sub.
Here you go: https://github.com/reem/rust-event-emitter An asynchronous event emitter based on rust-event. This currently ICEs because of unboxed closures, but should otherwise be fine.
Looking forward to the benchmarks!
I'll be using Rust in my Operating Systems class!
&gt; Since that is a common operation, we wanted it to be as concise as possible. I'd say that readability is more important than conciseness; `[]` is substantially less readable than `[..]` (especially to people unfamiliar with the language); and this operation isn't so common that an extra two characters will be a big deal. When it comes to something like the nine-chracter `.unwrap()` or the fifteen-character `.as_mut_slice()`, verbosity is definitely something to worry about; here, perhaps not so much.
To expand on what /u/editor_of_the_beast said ... The issue here is a admittedly a little subtle. Usually you don't need a lifetime parameter for a struct that holds a box because the box *owns* what it points to. So, for example, a `Box&lt;int&gt;` would be perfectly fine as data member type in your struct because an `int` is obviously not restricted in its lifetime. You can hold on to an `int` as long as you like. But in your case, you have an type-erased/"abstract" box. It's a box that can store anything that implements `ExampleTrait`. But that does not tell us how long we can hold onto such a thing because we don't know its type and because any kind of lifetime-restricted type would have a lifetime parameter attatched to it. Here's an example: You can implement the trait `ExampleTrait` for all sorts of types including types like struct Limited&lt;'l&gt; { some_ref: &amp;'l int, } which carry a reference to some borrowed value. Now, to make this memory-safe, Rust forces us to use an explicit lifetime parameter here so that the compiler can check at compile-time that we don't create dangling references. To put it differently: We don't want to be able to hold on to some reference in case the object/value it referred to already vanished. So, references are lifetime-restricted and are not allowed to outlive what they point to. In this instance the lifetime parameter `'l` restricts the lifetime of the reference `some_ref` which in turn restricts the lifetime of a value of type `Limited&lt;'l&gt;`. The compiler makes sure that there exists a lifetime `'l` so that the `int` we refer to via `some_ref` lives *at least* as long as `'l` and that a value of type `Limited&lt;'l&gt;` lives *at most* as long as `'l` because this has to be satisfied in order to avoid dangling pointers. So far so good. But if you box such a `Limited&lt;'l&gt;` into a `Box&lt;ExampleTrait&gt;` we lost *all* the type information *including* possible lifetime restrictions. Usually -- within functions -- the lifetime properties are deduced/retained in which case we don't need to worry about that. But if you define custom types to hold a reference or such a "type-erased" box the compiler really needs to know how long the reference or box can be used safely. That's why they have to be explicitly annotated with a lifetime parameter. In your case, `ExampleThing1` and `ExampleThing2` are `'static` in the sense that it's okay to hold on to a value/object of that type until the program ends. So, one way to go would be to define `ExampleStruct` like this: struct ExampleStruct { things: Vec&lt;Box&lt;ExampleTrait + 'static&gt;&gt; } But this might be unnecessarily constraining. Maybe you want to also be able to store some boxed values that have limited lifetimes. As long as their lifetimes are not shorter than the lifetime of your ExampleStruct object, we are fine. So, a more flexible approach would be to write it like this: struct ExampleStruct&lt;'x&gt; { things: Vec&lt;Box&lt;ExampleTrait + 'x&gt;&gt; } Now, you can still use `'static` as the parameter `'x`. But you don't have to. And if you don't need it to be `'static` you can let the compiler figure out another "value" for `'x` (deduction).
Nice world generation! I'm seconding the interest in seeing where it goes.
Exactly. `'static` is special and refers to "as long as the program runs". It's the lifetime of static variables and string literals. The string literal `"Hello world"` is of type `&amp;'static str`, so, it's a string slice that refers to some "static memory location" where an UTF-8 encoded, immutable string is stored until the program exits. All the other letters are just placeholders with arbitrary names. Typically `'a` is used if you only need a single one. It's just a placeholder for some lifetime the compiler has to figure out.
Me too, and thank you as well.
why were they removed? something related to dynamic scoping?
Just something. You have code like this: match some_option { Some A =&gt; Some (something with a), Some B =&gt; Some (something with b), ..., None =&gt; None } It might be cleaner to write it using [map](http://doc.rust-lang.org/std/option/type.Option.html#method.map): some_option.map(|v| match v { A =&gt; something with a, B =&gt; something with b, ... }) The key part of Option's map is that it maps Some to Some and None to None.
I already responded to your deleted version of this comment.
I tought they do need compiler support. I do know that they need it in the dylan compiler and dylan had a macro system just a powerful ad that of rust. You need to be able to keep your stack and then jump to some error handler system that can rewrite your stack and restart on a restart point or unroll your stack. Seams like something the compiler needs to be involved.
I am not an expert, that's just what I remember from when they were removed.
The 'error handling system' can be a function/closure with the right signature stored in [local data](http://doc.rust-lang.org/master/std/local_data/), which is called when an error occurs.
To complement the other answers, check this out: http://www.hoverbear.org/2014/08/12/Option-Monads-in-Rust/
Thank you for maintaining Allegro, it is quite used here in my university.
The easiest way to ignore the result and silence the warning is probably like this: let _ = out.write(...)
Oh, I certainly agree with you about the zip example! I guess you're right about the namespacing issue. I'm not really convinced one syntax is better than the other; visually though, I definitely prefer your style. From what I'm seeing, there doesn't seem to be a trait for `.iter()`, only the `Iterator` trait itself. If such a trait were possible, it seems as though functions of the style you're describing would be easily implemented, but I don't know enough to say if such a trait would be sensible, practical or even possible.
I understand these arguments, but I disagree with the conclusion. Rust already gets *a lot* of crap from outside about its "brevity" - people are even avoiding the language because of `fn`. Maybe I'm an idealist, but being inconsistent for *two characters* seems like a very silly trade off to me. It takes me literally half a second to type `..`, it'd probably take me longer than that just to remember the preceding variable name. I also don't agree that it makes it harder to read at all. &gt; consistency to what other languages do may well weigh less than having a short syntax for a common case. My issue isn't so much consistency with other languages (though that helps beginners) as it is consistency *within* the language. As I described above, `[]` doesn't fit in with the other syntax, whereas `[..]` does.
Even if it is super-common, I don't feel that the extra half second it takes to type `..` is worth messing up consistency. I've *never* heard anyone say "I don't like X language, one of the keywords is two characters too long." I certainly have heard people say "I don't like Y language, it has these weird corner cases that make it harder to use."
Anybody who uses this for the class is violating the honor code that they have to confirm whenever they submit an assignment. Anyways, I doubt that it's all that hard to find the answers already.
Why not call the project 'Tetanus'? :)
(note: I forgot the second bit. Oops: I prefer this: # call the list function (actually the constructor # of an object in python, but why not pretend it's # a function? it basically is), and store it in a variable # called list. This doesn't shadow the list function. list = list(1, 2, 3) # the #' means basically "treat this as a symbol # referring to a function, not referring to a variable the_list_function = #'list to this: lst = list(1, 2, 3) the_list_function = list I find myself more often having generic variables called `list` or `vector` or some other (semi-)reserved word in most code I write than writing the names of functions other than when calling them. The only real difference is that you'd write `map(#'int, str.split())` instead of `map(int, string.split())` (or `s` or `strng` or `strg` or whatever awful abbreviation). I should stop ranting about this one really, it's not that big an issue, and it's certainly not really relevant to iterators, haha.) &gt;From what I'm seeing, there doesn't seem to be a trait for .iter(), only the Iterator trait itself. If such a trait were possible, it seems as though functions of the style you're describing would be easily implemented, but I don't know enough to say if such a trait would be sensible, practical or even possible. In Python, the `__iter__()` function is used to implicitly grab the iterator of a function when you want to iterate over it. `__iter__()` should return an iterator object. You can make the object itself an iterator object by making `__iter__(self)` return `self` and then defining `__next__()` to return the next item each time it's called: class A: def __init__(self): self.i = 0 def __iter__(self): return self def __next__(self): i += 1 if i &lt; 10: return i else: raise StopIteration Or you can make `__iter__()` return a generator object. Generator objects are resumable functions, and can be used to implement coroutines along with an expanded iterator protocol: class A: def __init__(self): self.i = 0 def __iter__(self): i += 1 yield i Or you can make `__iter__()` return a separate iterator object. class A: def __init__(self): self.i = 0 def __iter__(self): return range(1, 10) The advantage I've seen listed of having the explicit `iter()` (other than the general "explicit is good" idea) is that sometimes you want to iterate over an object in different ways. You can still do that in the Python model, by having methods that return new iterator objects. You can still have the main object implement __iter__ itself, or you can just leave that unimplemented and require people to use one of the iterators you provide. But for things that can really only be iterated over in one way, I think that leaving off the `iter()` should be possible. 
If you declare that [] is an operator that can take an optional range (with optional end points, of which at least one must be present), then there is no inconsistency.
It's a tricky one in that it appears a lot of Rust compile-time magic is related to specific trait definitions. It is almost as if certain traits are like keywords in that they are special and have side-effects on code compilation. Is there a list of standard traits that directly affect compilation magic?
I wouldn't call it 'compilation magic,' exactly. There is some syntax that desugars to a function call, and that function call is provided by a trait. I don't think we have a complete list anywhere, no.
&gt; with optional end points, of which at least one must be present That's basically saying "It's consistent, if we define consistent = exactly how it is." My point is that for a slice, it is more consistent to always have the `..` than to only have it sometimes. It's not about consistency to the spec (because it'll always be consistent to that), it's about consistency between the various forms of the syntax. Requiring at least one endpoint when using `..` makes things less internally consistent.
Various text-related things. Right now I have a prototype hyphenator (Liang's algorithm) and TrueType font renderer. In both cases performance is comparable to highly tuned C. I plan to open source both of these soon.
I can see where you're coming from with the separate namespace for functions and variables. But coming from Haskell, I'm slightly uncomfortable with it, because I'm used to there being no real useful distinction. In a language like Rust it could make sense though. Python's take on it is interesting. It seems slightly complex to me, but I guess it isn't *that* much more so than Rust's. I believe in "explicit is good," especially so in a systems language like Rust. While it might be nice to be able to leave off `.iter()` when it's unambiguous, do you know if there would be a way to do this in Rust without special-casing the `.iter()` function on a language level?
Thanks. I didn't know this.
That looks like the callback based code in node.js. Probably when rust get HKTs this will be simpler.
Ideally, or from what I've been proposing, it shouldn't require any syntax changes (so something more in-line with Scala in terms of not having a kind syntax, just kind inference, but also in-line with Haskell in terms of not having special syntax and more powerful inference.)
&gt; That's basically saying "It's consistent, if we define consistent = exactly how it is." No, it's saying "Here's a definition that is simple enough. If the inconsistency was so big, it would be more complex".
An rsync-like file copying tool that's designed to be good at copying selectively. I'll open source it soon...
Thanks for the clarification of a Box&lt;int&gt; vs. Box&lt;ExampleTrait&gt;. I'm definitely not fully clear on the semantics of trait objects, so that was useful to know. 
&gt; someone who is should simply propose removing the automatic selection of 'static in return values with unspecified lifetimes What you suggest has already been agreed upon and implemented. (Or rather, we went further than just removing inference of `'static` on return values with unspecified lifetimes.) I think the most relevant document here is this: [RFC 39: Lifetime Elision](https://github.com/rust-lang/rfcs/blob/master/active/0039-lifetime-elision.md) or for a concrete illustration: http://is.gd/5Exg5Y
Good to hear that it helped. :) The short version is really `Box&lt;int&gt;` is implicitly a box we can hold on to forever (since the dynamic pointee type is known to be `int` and known to be `'static`). But you might want to store other things inside a `Box&lt;SomeTrait&gt;` that are not necessarily allowed to live until the program ends. This is where another lifetime parameter comes in. To be honest, before I saw this thread, I didn't know about it either. You live and learn. But it just makes sense that the Rust compiler asks for an explicit lifetime here because otherwise we would only be able to put `'static` types into type-erased boxes which seems pretty restricting.
I somehow doubt so, I would rather think that they eschewed ADT for simplicity's sake, much like they eschewed generics.
It will complain, but you can still check `err` and "forget" to return, then use `val`...
The problem of conditions is *global state*. When you invoke a condition handler, this condition handler appears "magically"; this has several undesirable consequences: - debugging: who setup such a stupid handler ? when ? - documentation: I am calling this function, what are the possible condition handlers I would need to set ? - ... Global state is mystifying at the best of times, so it's best to do *without*.
We can declare whatever we may like. In practice, when people are first exposed to the language, they will instinctively feel that `vek[]`, `vek[1]` and `vek[1..]` *all* do something substantially different to one another. Given the existence of array indexing (and whatever future array operations may be added to the language), similar things need to look similar, in order to prevent "death by a thousand papercuts" for people who are trying to learn Rust.
I fully agree with you, in the trade-off between 2 more characters and consistency, I'll pick consistency as well. Certainly with less characters Rust has greater chances to win code golf, but should it really be its primary purpose?
&gt; I don't think we have a complete list anywhere, no. Any chance you'll have time to whip one up? It would really help to have a guide of all the syntax desugaring we can hook in. 
I agree. Especially stuff like unused functions/variables/arguments/imports I feel are very good as warnings and extremely frustrating as errors. I'm not so sure on an ignored `Result` variable, however. I think the costs of just sticking an `unwrap()` on there are small enough that it's probably worth making it an error.
A CP Solver, drawing inspiration from both Choco and Google's OR-Tools. This is going quite slowly, but I hope to compete in the minizinc competition next year. I have a firm suspicion that Rust can obliterate the competition with relatively simple composition. Something I want to do, but don't yet know enough C to be able to do, is to write a PL/Rust language extension for Postgres, with a clean typesafe API, and enforcing safe semantics so that it can be considered a trusted language. It would be the only static/compiled language that is also trusted. 
One problem global event buses can have is if different entities need to respond to an event in a specific order. This could be done by having each fire an event to the next, but then you end up with a million events. [This article](http://www.code-experience.com/avoiding-event-chains-in-single-page-applications/) discusses this in the context of in-browser JavaScript: Event -&gt; Model A updates -&gt; Event -&gt; Model B updates (given data from A) versus Event -&gt; [Model B asks Model A to update first Model A updates, Model B updates (given data from A)]
Indeed. It seems like the outer braces, the inner braces and the braces at invocation are all arbitrary and not related to each other. Very interesting. (And undocumented).
I know it's a syntax extension, but I'm not sure what is the implications and how it is related with braces. Should probably investigate further.
I'm not sure why brevity around here is considered better than readability. Rust already gets tons of flack for being too dense and hard to read, with inconsistent and overuse/abuse of abbreviations. That said, I'm not sure what is going to be more readable in this particular debate in what I lovingly call Rust symbol soup.
What? Definitely not? It's like streaming computations while handling errors in a safe, concise and understandable way.
Nice to hear that the Rust bindings are being maintained by the actual developers. I wrote up a set of Go [bindings](http://www.github.com/dradtke/go-allegro), but Rust is definitely going to be bigger in the game development space.
To these who are too lazy to look: **Kinds** * Send * Sized * Copy * Sync **Special Operators** * Drop - can't be invoked regularly, invoked by drop glue * Deref * DerefMut – autoderef * Fn * FnMut * FnOnce – overloaded call. arguments passed as a tuple * Index * IndexMut ‒ pair, called by a[x] = b **Ordinary Operators** * Add * Sub * Mul * Div * Rem * BitXor * BitAnd * BitOr * Eq * Ord * Shl * Shr ‒ binary * Neg * Not - unary **Other** * Iterator ‒ used in for loops
Is this with rendering?
Could you link to your proposal?
Of all of my projects, the one receiving the most of my time is currently [Rocket](https://github.com/DiamondLovesYou/rust-rocket.git). It's a (WIP) project builder (but doesn't itself fetch deps outside of sources; it leaves that to Cargo). The 'build crates' are written in Rust itself, which Rocket uses to compile a stable dylib API for its own use; Rocket handles the build infrastructure, the build crates handle the build process. It's highly modular, and allows arbitrary overrides of Routes (resource build pipelines, in Rocket speak), Yards (a single step in a Route), Trains (the resources themselves), various codegen options, tools+tool args (ie overriding clang exe path, or a clang cmd argument), including overriding any part of a dep's build process. Cross-compilation is a goal, and I plan to use 'platform build crates' (literally just build crates) to that end. It also caches pretty heavily, so an inconsequential change can't snowball and cause a whole project to rebuild. Lastly, I intend to implement Rust proper's build with Rocket such that Rust is built with Rocket. I also work on a fork of [Rust](https://github.com/DiamondLovesYou/rust) which can target PNaCl, and a corresponding PPAPI wrapper which goes along with it: [rust-ppapi](https://github.com/DiamondLovesYou/rust-ppapi). In fact, it was these two projects that inspired Rocket, due to the dissimilar (PExe's are basically bitcode) build process vs native targets.
If speed is a problem you can also use `rustc` with the `--no-trans` option. This skips the code generation but is helpful if you just want to check that everything compiles, would love to have something like this for cargo…
I haven't published my RFC yet. However, I do have some blog posts written (but not yet published) going into more detail on what I have in mind.
I don't think you need to worry about the "fn is too short!" people. I think they're a small but noisy group of whiners who probably weren't going to use Rust anyway.
Note: You can add more inner braces for invocation in the patterns, and stuff like that. Macros and syntax extensions need a LOT of docs, I know :)
Thanks! That lint is mainly something that we can turn on if required and audit the various transmutes used. I recently [added another, more complicated lint](https://github.com/servo/servo/pull/3374), that ensures that `JS&lt;T&gt;` doesn't go on the stack in places it shouldn't go. It's in a sense part 2 of our [usage of the type system (among other things) to interact safely with the SpiderMonkey GC](https://blog.mozilla.org/research/2014/08/26/javascript-servos-only-garbage-collector/), giving even more compile-time safety guarantees about the abstractions we use around JS-Managed values. Aside from a few intentional-ish loopholes, this makes it nearly impossible to (unintentionally) incorrectly use JS objects in Servo's DOM. I really enjoy that the language gives you the power to define your own safety guarantees with more granularity than `unsafe` vs `safe` blocks :D (The fact that the language has the `unsafe` vs `safe` distinction is in itself awesome) Working with libsyntax is like building a house with an electron gun, though.
Well, it's been changing. It was mostly just math operators, but now we've got array stuff too.
It passed, then it stopped passing, then it got fixed, then there was [cake](https://twitter.com/zii/status/461944670075895808), then it stopped passing, then it was fixed again, then it stopped passing and has been that way for a few months. ... I think. I don't really work with layout (yet). Perhaps /u/mbrubeck can give a better idea of the reasons why it doesn't pass right now.
Not in the forseeable future, and maybe not ever. It's more likely that Servo will see use in new, different products, where its strengths (parallel performance, memory safety, etc.) outweigh its immaturity relative to Gecko/Blink/WebKit.
Haven't reached the rendering part yet, but that's the plan. Currently figuring out the passes/techniques/material thing.
As many have said, I'm very much in favor of the new slice notation. Coming from Python, this is one thing I sorely missed. As for the `[..]` vs `[]` argument. I'm in favor of the `[..]` because not only is it consistent, but it also implies that you are slicing something, and not just referring to the slice in general (which is different from the whole slice which `[..]` would be referring to)...if that makes any sense. Very much akin to Python's `[:]` operator as others have mentioned. Speaking of Python, it would **awesome** if we also got the negative slice notation (i.e. `[-1]` referring to the last value in the slice) which could then be combined with the other notations as well. I also feel that `...` vs `..` is a bad idea. It is **super** easy to accidentally type, or overlook a single `.` character especially when it's preceded by two of the same. It also doesn't give you anything that special, inclusive vs exclusive? I could simply add/subtract a single index if I wanted to change that, it doesn't really need a whole new operator. 
Why is the non-general form non-orthogonal?
What I'm getting at is that the details and efficiency of the library aren't as important in the beginning as the abstractions are. Numpy, R, Matlab, Theano, Eigen, Armadilliol, et al are nice abstractions. They're married to those abstractions though. How they are implemented is the job of the backend people.
This is awesome! Thanks for writing this up! Nightlies just recently became compatible with basically all current linux distros by default, so [downloading libstdc++](https://github.com/emk/heroku-buildpack-rust/blob/b54e8ecc66fe1b6f628e0042461f8ddc99b827c7/bin/compile#L39-L48) should no longer be necessary. The nightly distribution now statically links libstdc++, and it's also compatible with a very old glibc (old kernels). Also, I'm curious why you recommend storing nightlies/cargo in a pers\onal S3 bucket? Is this to ensure that you always build against the same version of rust or cargo, or was this something you found necessary for Heroku? And lastly, sorry about the `GIT_SSL_NO_VERIFY`! I tried to look into that the other day but came up empty handed, I'll see if I can't look into this again later! I can't wait to see what awesome Rust-based apps pop up on Heroku!
Why does Rust need libstdc++?
It leans heavily on my generic Trie implementation, which is also used in Iron's mounting middleware. https://github.com/michaelsproul/rust-generic-trie
Thank you for looking into the SSL issues! And I'm glad to hear that libstd++ is no longer necessary. I'll be keeping it around for backwards compatibility for a while, though. I keep copies of Rust and Cargo in an S3 bucket so that my blog post will still work next week. :-) But more importantly, it's a cultural thing: If I deploy an application to Heroku today, I should be able to make a one-line bug fix in six months and redeploy it without having to update the compiler and all the supporting libraries. Now, once Rust stabilizes a bit, perhaps people will just be able to put the compiler version number into their Cargo.toml files, and the buildpack will be able to grab the appropriate binaries from the Rust download site. But for now, I encourage people to cache their binaries somewhere. The upside of this extra effort is that you can deploy production Rust applications to Heroku today, and you don't need to worry about the language or a library changing at the worst possible moment. Yes, I have been burned by these issues many times before with other languages. Anyway, thank you for your interest! And if anyone wants to use Rust on Heroku, please feel free to get in touch.
The compiler needs it to run because `rustc` links against LLVM, which is a C++ library. It is not required to run a normal Rust binary (unless that binary links against some C++ library, of course).
map_or is my favourite
I don't like to put string slices into structs if I don't have to. I prefer to make them `String` with `.into_string()` so that the struct owns them, then take slices off of that. But it looks like to fix your existing code, you just have to update the return type of `Connection::new` to add the lifetime parameter `'a` to `Connection`: pub fn new&lt;'a&gt;(...) -&gt; Result&lt;Connection&lt;'a&gt;, io::IoError&gt; Also, you can `use std::io::IoResult` so you can change the return type to `IoResult&lt;Connection&lt;'a&gt;&gt;` which is just a type alias for the above.
Result&lt;Connection&lt;'a&gt;, io::IoError&gt; There is a missing lifetime parameter.
I don't know. Do you think 21.6 megabytes of your hard drive is worth the extensive type, memory, and thread safety, and heavy optimization provided?
I work at Heroku and used to work on Rust, so if anyone needs any help that Heroku might be able to provide, drop me a line anytime :) (work address: tjc at heroku)
I really enjoyed the article for how you approached Rust, however there is one small nitpick if I may from a few paragraphs in: &gt; ... Rust uses several kinds of pointers ... The reason Rust is safe is not because of its pointers (there are actually 2 compiler defined pointer types now, `&amp;` and `&amp;mut`), but because of its ownership guarantees. I would reword the sentence to something like "Rust's strong guarantees about memory ownership allow the compiler to prove when memory is safe and unsafe to use." Thanks for the fun read!
Unfortunately, I recently had to switch jobs and dropped this particular project in the hustle. Though I am interested in picking it back up when I have some free time. Might wait for the 1.0 beta then make a push to get it into a usable form.
Awesome write up! Only small nitpick is how the code is formatted like no space after `:` or `*` and having `-&gt;` aligned. Other than that, it was a great read!
Yeah, I'm still a newbie to Rust formatting. Hopefully soon we'll have IDE support that just does the right thing for me.
It is entirely possible that I'm not understanding something right. If I have 2 functions `from_db(int) -&gt; Result&lt;String, DBError&gt;` and `from_service(String) -&gt; Result&lt;MyStruct, ServiceError&gt;`, how will the code for a function calling these in sequence and returning appropriate `Result` look like without repeated match?
Why isn't this caught by tests and why has it been allowed to regress?
There's some work to do in order to suport Results w/ different types
ahhhhh i promise i won't judge on the cleanliness I am just curious on your strategy...especially in regards to the GUI
Yay! Time to fix up tons of APIs!
Yeah, but why aren't there any discussions/proposals about what exactly "making them stable" entails? It seems that every other corner of the language is being constantly re-examined, just not macros.
It was, but I think it was a conscious decision to not fix it (perhaps it was broken by a more important patch or something? Not clear on the timeline here)
The basic strategy is to have two executables: a line editor that exposes a socket, and a client that is not much more than a glorified key_sequence -&gt; editor_command hash. I don't have plans to make a GUI, proper. Just a vi-esque terminal application. Likely using a wrapper around some basic termios functions.
a toy virtual machine and assembly language, based on the 2006 icfp programming contest, [the cult of the bound variable](http://www.boundvariable.org/). Right now it's command line only but i am thinking of adding some sort of visualization of the vm's state in the future.
There's some work being done on cutting down the time spent in LLVM at the cost of slightly less efficient generated code: https://github.com/rust-lang/rust/pull/16367 This should improve compilation times for some larger, monolithic crates such as librustc. 
I'm currently working on a package manager. Not getting very far though, as it is a bit of a learn as you go project.
The biggest problem I have with this and the [lifetime guide](http://static.rust-lang.org/doc/master/guide-lifetimes.html) that links from your link, is the gap between the conceptual explanation for lifetimes and the syntax we have to use. It's hard to go from *"the compiler knows when to free something as long as you give it a hint"* to *"here's how you tell the compiler how long things should live."* [Here](https://www.reddit.com/r/rust/comments/2g1pla/confused_by_lifetime_syntax/) is another example where the syntax, not the concept, bit me.
I've just recalled that macros are feature gated, so they can freely introduce breaking changes to them after 1.0. And before 1.0 there are many more important things to discuss and propose.
TBH I might use it if it were in rust or in vimscript, but…
I have a follow-up question, I hope you don't mind if I ask you directly: if, for example, ExampleThing1 had another method which ExampleTrait didn't have, would it somehow be possible to call this method on an ExampleThing1 instance which is inside ExampleStruct?
There's also the [in-progress ownership guide](https://github.com/rust-lang/rust/pull/17138) which replaces the lifetimes guide ;)
When I tried it, it actually looks like Cargo by default looks through parent directories for a Cargo.toml. Putting `au filetype rust setlocal makeprg=cargo\ build` in my .vimrc seems to be enough to get it to integrate properly. EDIT: Never mind, I see the problem. When in a folder other than the one containing Cargo.toml, the relative path of the error file is off and it can end up trying to open the wrong file location.
Will macro behaviour in Rust match that of CL or Scheme?
I haven't worked with a language without shared state before. Is my setup of various tasks sensical?
I'm curious about this myself honestly. Simply downcasting from Box&lt;ExampleTrait&gt; to Box&lt;ExampleThing1&gt; doesn't work. rustc says it's a non-scalar cast. Work was put into the compiler just to allow Box's of traits, because the focus of Rust is more on compile time type safety. I wonder if the Any type could be of... any... help. hehe. Edit for brief rant: This has always been something that's bothered about OO though. If I want to pass around base class pointers (trait objects in Rust), the only way to add a method to a derived class and use it polymorphically is by adding it to the base class as well. I think you need RTTI to downcast properly, and no one wants that. Anyway, I'm curious what some smart people have to say about this because I'd like to see what the good way to do that is.
As one specific example, I don't always understand lifetimes and closures. A regular closures can accept a reference without an explicit lifetime: struct A; let foo = |a:&amp;A| -&gt; bool {true}; // compiles fine But when I put this in a struct, I'm required to specific a lifetime: struct Foo { handler: |&amp;A| -&gt; bool } l.rs:4:14: 4:26 error: explicit lifetime bound required l.rs:4 handler: |&amp;A| -&gt; bool ^~~~~~~~~~~~ And I'm not sure why. I don't believe any of the current guides help explain this (but if they do, I will be very happy to be wrong)
No not this PR.
Clarification: part of the q is if I got the basix right. Am I grokking it?
Just a minor point: I believe values are *technically* also `'static`. This comes up when you have to annotate a generic struct like `struct MyIterator&lt;'a, T:'a&gt; { ... }`. This ensures the `T`'s you provide have an adequate lifetime, where values are simply treated as `'static`.
Start with the minimal program in both languages (use `#![no_std]` in Rust) and compare the compile-time. Build up code from there, and Rust will continue compiling faster as long as the comparison is apples to apples. The problem isn't the compiler, it's the standard libraries.
&gt; Hope someone finds this useful. :) Since Windows XP, most of the new APIs are COM based. It is surely useful, specially with WinRT.
&gt; It's |&amp;A|:'static -&gt; bool I know what it means for a variable to have a static lifetime (the variable will be around for the entire duration of the execution). But I'm not sure I understand what it means for a *type* to have a static lifetime (which is how I parse the `'static` in your suggestion). Can you elaborate? 
“Easier to read” is an extremely dubious claim, refuted by many (including myself). As it is, curly brace on the same line is the official style for the Rust compiler and standard library.
As I understand it, all depending libraries must be recompiled because of Rust's process of monomorphization. This means that every generic function is specialized to a specific type. If a library is changed it probably changes something generic, and monomorphization has to kick in again.
The (better|bigger) your libraries the less you need to write.
I think you said that perfectly. The concept makes perfect sense, its the syntax I'm supposed to use, and where, that trips me up. 
The thing, of course, is that the amount of space now available in the average programmer's permanent storage (HDDs, floppies, etc) is way, way more than 547 times the amount of space available when Turbo Pascal was released.
not all functions are generic, at least for me most are not
You are probably using generic functions though. There might be another reason, but I know that rust has to be wary of the libraries in use during compilation for monomorphization. I just extended that logic to recompilation. If there is another answer, I would love to know :)
I think you mean a [repl](http://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop), in which case there is an [open issue #9898](https://github.com/rust-lang/rust/issues/9898) and a closed with more discussion [here at #1120](https://github.com/rust-lang/rust/issues/1120). on the IRC channel, you can query `rusti` or `rustilite`, but im unsure of where their source is.
What's the problem with putting string slices in structs?
rusti just compiles and runs a program in the same sandbox as [play.rust-lang.org](http://play.rust-lang.org) It isn't a repl because there is no "l" part: It just reads an expression, compiles, runs, and prints it. 
A Repl is a highly desirable feature. We used to have one, but it never really worked for any length of time. Seeing it resurrected would be a pretty sweet achievement, but I wouldn't want to work on it until 1.0, since at least the core language should have settled down by then.
yes i was wondering specifically the source, i know how it works. But you are right, `rusti` is not a true REPL, but its what we have at this moment.
I don't know if the IRC part is posted, but the sandbox is https://github.com/rust-lang/rust-playpen
It all depends on the usage of the struct. In many cases, the struct lives longer than the `String` the slice was from, so it might as well take over ownership.
I'm impressed, and a little bit repulsed. Good job!
I'm so glad this came in earlier rather then post-1.0, even though it's backwards-compatible. We can have much nicer interfaces (and internals too!) in gfx-rs now. Thanks /u/pcwalton!
(Boxed) Closures capture references to values in scope. Therefore, if any of the "closed over" values ever went out of scope, or otherwise became invalidated, your closure would become nonsense. Therefore, closures must have a lifetime. 'static for a closure basically states that it doesn't close over anything, and therefore will always be valid. Other types may have lifetimes for the same basic reason: if they hold onto something by reference, they can't live longer than that reference. If a type owns all of its contents by-value, then it has a 'static lifetime. More generally, a non-composite value (integer, bool, etc) is 'static, and a composite type has a lifetime equal to the minimum of all the lifetimes it contains.
sounds like a feature improvement to me, maybe those crates could be saved in some intermediate state that could be used to "monomorphize" it into the appropriate final code? 
No. There is no other way for a good reason. When extending types with new traits you have to bring the trait into the scope explicitly so code that was written without knowledge about new traits does not change its semantics.
Ah! Ok. The "doesn't close over anything" part was a key bit for my understanding, and was a great way to explain it. That is of course exactly what I want in my original example, but I didn't know that was a thing I could specify with a closure. Thanks. I humble suggest that this little bit of information make it into a future Rust Guide
"Stack based dynamic drop flags" is the best option to me. C++ semantics of destroying object near } is clear and simple. Dynamic drop flag will be rarely used in practice (I guess), and when used, can be optimized away by LLVM.
What's the general strategy for a compiled language to implement a repl? Either you don't compile and then you need interpreter implementations of all core functionality, or you do, but then you would usually have to wait until a complete function is written, which means not doing things like writing 2+2. What about inlining and optimization?
To make the array mutable, you would do let mut arr = [MyObject::new(), ..100]; unless I'm misunderstanding the question. The elements will be mutable as well. A method on `Vec` that does something similar is [from_elem](http://doc.rust-lang.org/std/vec/struct.Vec.html#method.from_elem), though it also requires the value to be `Clone` instead of just `Copy`.
So basically the same reason C++ templates are defined in headers?
You're right, that actually does make the elements mutable. I guess I just misunderstood the language. Now what if I want *arr* to be immutable? That is, I still want the slice owned by *arr* to be mutable, but don't want to be able to assign *arr* different value?
I'm not quite sure I follow. I assume you mean you want the items to be mutable, but the container immutable? In this case, I'd say the container _is_ the elements it's owning. As far as I know there is no indirection involved in fixed-size arrays (but I could be wrong). Maybe you can give an example of the behavior you'd like, or what you'd like to prohibit?
I think, since Rust is based on LLVM, one can simply use its ExecutionEngine and get the interpreter with inlining and optimization for free. Even the header file for ExecutionEngine is in src/rustllvm/rustllvm.h. What's missing are its method bindings in src/librustc_llvm/lib.rs.
I know it breaks some common C++ idioms, but I don't think that early dropping is really an issue in Rust. The niche served by scope guard objects like C++11's mutexes are covered in Rust by auto-dereffing proxy types which will definitely not be dropped early. Other uses, like deallocating memory, closing files or closing DB connections, I can't really think of a situation where you wouldn't want it to happen as soon as possible.
This is what I meant: let mut a = [0i, 1]; a[0] = 2; // This is okay a = [3i, 4]; // But I want this line to fail at compile time. I want to disallow reassigning *a*. Is there a way to do this?
How long have you been using Rust?
Not that I know of, but the `a = [3i, 4]` would have the same effect as changing the items individually. Is there a reason you want to prohibit this?
Rust's type system doesn't differentiate between assignment mutability and internal mutability. The only way to accomplish this is to implement your own data structures that use unsafe code to internally mutate while still appearing immutable externally. This is how the standard library's concurrency primitives are implemented, for instance, but it is probably not worth it for what you're trying to do here.
Your other option for 2) is to use `Vec::from_fn`: #[deriving(Show)] struct Foo(uint); impl Foo { fn new(i: uint) -&gt; Foo { Foo(i) } } fn main() { let vec = Vec::from_fn(5, Foo::new); println!("{}", vec); }
I initially worried about things like this, but no longer. With Rust's lifetime analysis (the borrow checker), you are guaranteed that usage of the array is still safe. The bigger question you have to ask yourself is why do you want disallow reassigning it here?
What about it breaking the order they're released? Could matter for APIs that manipulate global state, old style OpenGL for example has lots of begin/end calls where order/nesting matters. Also could you link to something about what these proxies you're talking about are? Still a newb.
How large (LOC)? thanx
Rust mutexes are a wrapper around a piece of data. The data cannot be accessed except through a proxy object that can only be acquired by locking the mutex.
Ah, I've implemented that before in C++. But that just moves the problem to the proxy object doesn't it? You still care about its order of destruction. Or does the lifetime machinery somehow rescue you?
Mutable shared state is certainly allowed, via types like `Cell`, `RefCell`, `Mutex` and `RWLock`. It's not the natural way to handle things, though (the non-concurrent ones particularly).
The proxy's destructor won't run until after its last use, even if it is moved up a bit.
It never actually worked as a proper REPL. It was a hack and recompiled / re-evaluated the entire input up to that point with every new line.
&gt; Rust mutexes are a wrapper around a piece of data. The high-level `Mutex&lt;T&gt;` wrapper is... but not the low-level type it and other concurrent data structures build on. Rust is a systems language and ignoring the needs of low level code would make it useless.
But this works fine, no? A JIT interpreter also has a compile step. Perhaps what would be more desirable is to cache what was already compiled (perhaps just store the LLVM IR so that the unchanged parts of the program don't have to be reparsed). Edit: oh, another thing would be how to keep mutable state. Like, set a variable a to 2, see the value of a+2, then set it to 3, etc. I guess operating like this would be an extension to Rust, but it seems very useful.
Yeah, it will only be used when you ask for it via conditional dropping. You can call `drop` explicitly to avoid that as a micro-optimization. The proposal here will make it harder to write correct low-level code. Rust is behind C++ when it comes to writing clear low-level code, and this would make the gap wider.
I 100% agree with this. Adding a drop() call feels like you're adding free() to the language. 
Do you realise that the `drop` call is not compulsory, and is very different to `free`? (Maybe you could explain what you mean by this analogy in more detail?)
Don't the benchmarks game regex-dna and pi-digits tasks show programs that use libraries? 
For a REPL you also need a dynamic top-level environment.
Right but if proxy destructors can be moved past one another that's a problem for more resources than just mutexes I think. There's no guarantee that two proxy objects won't have a dependency on one another that the compiler doesn't know about. The dependency may only be in an external system.
Not so much talking about the standard libs - just things like gfx-rs which need associated items badly.
Generating code dynamically is not difficult and isn't a significant part of implementing a proper REPL. Evaluating code bit by bit while building up state is what makes this a very difficult project.
New to rust here, one question though. The HTTP implementations mentioned in the post, are those like net/http in GoLang and Ruby? If they are, won't it be a problem going forward with the community being split on two different implementations which will be trying to solve the same problem without making clear what makes them different from each other? Sorry if I sound so inexperienced in this, because I am.
If rust is to someday have a REPL, CTFE could piggy back on that work.
Right now there are several conflicting http implementations (rust-http, hyper, teepee, etc.), but as the dust settles it's likely only one or two will rise to be widely used. Ideally, higher-level abstractions will be generic over the backend used. For instance, it's definitely on my todolist for Iron to abstract away the backend and allow downstream users to choose.
&gt; Spawning and scheduling threads is and will always be expensive, whereas low-level bindings over kqueue and epoll are cheap and will remain so. The one thing that would IMHO really pull Rust ahead of the pack here would be an automatic CPS transformation à la [async / await](http://msdn.microsoft.com/en-us/library/hh191443.aspx). I think Rust's borrow checker would be an excellent fit for this; there's a proposal to add `async` and `await` to C++17 but there is a scary amount of room for user error regarding the lifetimes of implicitly captured variables (including references and pointers).
I agree, but the devs have said that async/await is not going to happen for 1.0. I do hope that it happens later.
[gfx-rs](https://github.com/gfx-rs/gfx-rs) (one of Conrod's dependencies) is also in active development so you can expect `cargo update`s to introduce failing deps or other errors occasionally (i.e. right now) while the Piston ecosystem keeps maturing. These breakages aren't normally too substantial though, and generally get fixed within a few hours.
The same mentality is used with Haskell, but there are still competing http implementations there. I think this is somewhat wasted effort if the api has been done so many times already in rust and other languages. It would be cool if one of the http projects gets a mention or is adopted... even if isn't all that fair for other implementations, it will focus effort of libraries built on top of http.
I also find it easier to read. It's a matter of taste I guess. I don't think the rust community should impose a certain style, just use whatever you want when starting your own projects and follow the conventions that are already in place when contributing to existing projects.
You could post here once you reach milestones (e.g. if you get to HTTP 1.1 post "tiny-http has reached HTTP 1.1 confromance" to /r/rust). We can't know about it if you don't tell us. :) If you don't like promoting it directly, you could write some articles/blog-posts about interesting features/implementation details (e.g. the lazy request decoding sounds interesting). Or, even just "introduction to tiny-http".
Part-time 3 months.
I didn't know about it or I would have mentioned it as an alternative. I'd be happy to hear more and for you to continue working on it - the more ideas we have in this space the better the end result will be. I'm thinking of starting up a "this week in rust web dev" blog or something along those lines, where I could talk about growing projects or changes to give people a better idea of what's going on to combat this sort of problem.
Many mainstream languages I know have multiple mature http server implementations and I think this is not a problem as long as abstraction layers like Rack/WSGI/Ring/Servlet etc are present. I believe Rust will have its own HTTP and SQL database abstraction layers one day. 
Eventually I think one implementation will come out as the most used, likely because it is used in whichever framework captures the highest market share. That might take a while though.
&gt; pub fn make_hotp(secret: &amp;[Ascii], counter: i64) -&gt; Option&lt;u32&gt; { Any particular reason that that is a signed integer?
I want to get excited about Rust web dev, but there is a very real sense in which I don't really *feel* Rust's niche here; at least nowhere near to the same degree that I do in the general programming ecosystem. Rust's main asset is that it has the performance of C/C++, but without any of the safety issues (along with being in general a more modern, ergonomic language). However, the main bottleneck in the vast majority of web applications is either network IO or the database; as such, Rust's speed guarantees aren't really that meaningful. If optimum execution speed isn't really an issue because most of the time is spent waiting for the network and/or database anyway, then I don't really see what Rust has to offer over Haskell or F#, especially considering that they come with the green threading issue pre-solved and have mature ecosystems. I can certainly see Rust's use in lower-level protocols, for example in real time games, and god knows I'd love to have a modern OpenSSL written in Rust which would make me feel safer (for whatever that's worth); but on the actual web app tier of abstraction, I have difficulty seeing what Rust brings to the table.
Mainly because `Timespec.sec` is an `i64`, and this function is used for TOTP as well. Although come to think of it, it's not like it will ever be negative. I'll make it a u64 instead. Thanks!
Rust offers much more than just low-level safety: - You are sure that all the modules and functions that you use exist at compile-time (which is not the case in node, PHP and others) - You are sure that none of the variables that you use are null (for example you know that `database.query()` or `parse_json()` won't simply return null and make your server return a 500 error) - All possible errors must be handled explicitely ; if you want to read Json, you need to explicitely handle the case where the input is not valid Json - The Rust macros system can make sure that you have the right number of parameters, for example writing `get("/", function(id) { ... })` in node works (and `id` is always null) while the equivalent code in Rust wouldn't even compile. etc. The simple fact to use Rust almost saves you from writing tests. The guarantees of Rust are almost enough to be sure that you don't have any bugs. The only things that can't be checked in Rust that I can think of are the SQL queries (writing a wrong table name or column name won't be detected). 
You're picking the worst players in the game (scripting languages) and saying Rust beats them. Their users didn't choose those languages because they're good; they chose them because they're easy. I want to know what Rust brings over, for example, Yesod or Haste (Haskell), which has a much stronger type system and even covers the exception case you mention at the end: database interactions are strongly typed (along with the URLs in your application!) Oh, and if that weren't enough, you can also write your client code in the same language and it will be seamlessly compiled to JavaScript, so you can share your functions and data types and call effortlessly between client and server, rather than fighting over serialization and protocols. Tell me how Rust competes with *that*.
If anyone's planning to suggest that Rust's simpler to use and learn than Haskell, I'll note that OCaml also offers a similar strongly-typed web framework, and I'd consider OCaml's learning curve no steeper than Rust's.
I don't see any reason why you would want an interpreter for a language like Rust. The whole language is oriented around the traditional batch compilation workflow and static typing. Certainly it is doable, there are interpreters for C code.
No, he was picking some of the most used players in the game (PHP and NodeJS). I think most web developers don't have a clue what Yesod or Haste are. Also, you are comparing a complete web framework like Yesod with a bare language that hasn't reached 1.0 like Rust. It doesn't seem fair. Give it some time, let people develop web frameworks, and then you can compare. I'm sure many people treated Michael Snoyman as crazy for having decided to use Haskell for a web development framework in 2010. Surely someone told him at that time: "Tell me how Haskell competes with Ruby on Rails".
Has it been considered making it an error if different branches do result in different drop-semantics? Suppose you have a function fn do_something&lt;T&gt;(x: T) { /* ... */ } so then do something like this fn foo() { // ... if foobar { do_something(x); } } will result in an error and need this fn foo() { // ... if foobar { do_something(x); } else { drop(x); } } to be fixed?
Rust's paradigm is less exotic than haskell's one. Also, while haskell is effectively used as a production language, it's culture isn't purely focused toward prod applications. This means that new features are coming all the time, and that a big part of the community is from academia. While this situation has its advantages, this is a problem for someone who only want to build things without relying on innovative and less understood concepts. So while rust might be as complex as haskell, its culture is more familiar to mainstream developers. A Rust framework should allow for more predictable performances (compared to garbage collected languages). For companies serving a lot of users, being able to scale down the number of servers is a good thing. I don't know if it was justified but the stackoverflow team [mentionned](http://highscalability.com/blog/2014/7/21/stackoverflow-update-560m-pageviews-a-month-25-servers-and-i.html) that they try to avoid relying on the garbage collector. Using rust, you can avoid it completely. I don't see how bad it would be for rust to have a nice web ecosystem, and I certainly intend to try to write a web app with it.
Perhaps this is a silly question, but has the rust web-dev community settled in on the term "middleware" now? I was really hoping it'd switch back to "filter." The move in other communities struck me as odd. While it may seem like bikeshedding (and maybe it is), I've had a few conversations with Rubyists over the years where I mention that they're called filters in other areas (notably the Java ecoystem) and the proverbial lightbulb turns on. Insert obligatory note about anecdotes and data. But it's hard to argue that "middleware" is a meaningful term, IMHO. Having said that, if there's first mover's advantage here and that's the term that's been settled on, that's fine as well.
&gt; we can't know about it if you don't tell us. :) Quoting to emphasize this. You gotta talk about what you're doing. This goes for basically every project. In every language ecosystem, there's only one or two projects that people will actively pay attention to, and even then, not as much as you think. I will tweet and share and promote basically any link about Rust. On a separate note, having alternates is very healthy. I'd be sad if Teepee was the only HTTP library we have. Diversity and choice is strength.
Swanky!
It's the runtime/stdlib. It was already discussed [here](http://www.reddit.com/r/rust/comments/1p2d5o/is_there_a_way_to_escape_the_rust_runtime/).
I don't think I understand what you want to do, sorry.
I'm trying to create a vector of different objects which all implement a trait so that I can iterate over the vector and call methods from that shared trait, but I also need to be able to access methods of the objects inside this vector which are not part of the shared trait. 
My bikeshed leans toward something like "filter" as well. Middleware already has an [established meaning](http://en.wikipedia.org/wiki/Middleware), though it's usage in stuff like Express and other web libraries/frameworks seems analogous.
(That discussion is now a little out of date, e.g. there's now the official `core` library. See [the 'unsafe guide'](http://doc.rust-lang.org/master/guide-unsafe.html#avoiding-the-standard-library) for more details.)
&gt; the whole point of my original post was that Rust as a language offers nothing that Haskell/F#/OCaml don't except very fast execution Rust is not a functional language. So if your preference is to not use functional languages, then that would be an example of something it offers over Haskell/F#/OCaml. &gt; if they can't see the advantage of C# over those languages, I highly doubt they'll comprehend why Rust is any better. Eh? I'll use Python over C# any day for the simple reason that I don't want to be thrown into Microsoft's world.
&gt; To the first point, I don't think PHP/Node/Rails devs are Rust's target audience at all; if they can't see the advantage of C# over those languages, I highly doubt they'll comprehend why Rust is any better. Well, obviously the issue of types is it's own discussion. But the advantage of Node (w/ Hapi, Sails, Express, etc) or Rails is development speed and agility. With modest familiarity, these things are like greased lightning.
If your set of different objects is known in advance, and you don't need to expand it dynamically, you can have an enum: enum ExampleEnum { Example1(Type1), Example2(Type2), ... } Vec&lt;ExampleEnum&gt; The side benefit would be that no dispatch is performed on calls, since the compiler knows exact types.
If the method is not part of the shared trait, it can't be called. Why can't it be added to the shared trait?
I'm not sure how to break this to you, but while the language is in such state of fast change (please don't say otherwise), the number of people involved in the development of the language itself (and frameworks/infrastruture) will be much much larger than the number of users of the language. Which company would do large-scale development when, to cover a security issue they found on the language, they have to go back and redevelop significant parts of their web app? [yay talk like a pirate day!]
I don't know how much *unnecessary* information gcc leaves in the executable, so this ~~may be~~ likely is unfair cheating, but when compiling with optimizations and stripping symbols and other data we can get even lower. Step | Size ---|--- rustc bare.rs | 7.4K rustc --opt-level=3 -Z lto bare.rs | 6.9K strip -s bare | 4.7K sstrip bare | 2.8K gzexe bare | 2.0K Yeah, I know. Totally pointless! :) Note that using `-Os`, `strip -s` and `sstrip` on the C executable brought it down to 2.2K. Also, this kind of *optimization* is usually pretty dumb/unnecessary, even if it only takes all debugging information from you (not saying that's the only effect). `gzexe` decompresses the executable at runtime, which increases the startup time of your executable, which is likely not intended.
Rust bystander here, so apologies if it makes no sense: why not define the trait (ExampleTrait), then create your structs and create impls for each struct? Then your vec would actually be a Vec&lt;Box&lt;ExampleTrait&gt;&gt; and you don't need downcasting. Does that not work in Rust?
Ok, this is definitely the most convincing reply to me regarding Rust's niche in webdev: I absolutely agree that Rust is head and shoulders above any other imperative language that comes to mind. If someone isn't familiar with or doesn't like functional programming, then you're right, none of the alternatives I've mentioned are viable.
On this note of dependencies, how are generic functions distributed? For example, when you use a generic function in the standard library, does it get compiled into your program from source? Linking against compiled generics doesn't really make sense, because you can't monomorphise something that's already compiled. Is there any resource that explains more how generics work?
Well, I'm trying to have getters and setters for each object inside the vector. Should I add getters and setters for everything into the shared trait?
Yeah, it works, but I can't figure out how to access the data of the individual objects inside the vector (they are all different structs with different fields)
Why not mathjax? Should be pretty much complete an polished. 
This seems to work? http://pastebin.mozilla.org/6550917 $ ./bunch Example trait impl for ExampleThing 1 Example inherent method for ExampleThing 1 Example trait impl for ExampleThing 2 
This seems to be quite a hefty dependency (in terms of space, display lag) for an occasional couple of equations... is there no way to render LaTeX to SVG or something and serve that instead? It'll take a lot of equations to make up for 350 KB - 1MB of fonts + JS.
Mathjax was tried in [this PR](https://github.com/rust-lang/rust/pull/16991). The last post in this PR says "we might want to stop to consider evaluating https://github.com/Khan/KaTeX as a significantly more lightweight and performant solution than mathjax".
Are you sure you need to be accessing the individual structs' data? We run into this issue in Haskell too, and it's almost always a symptom of a deeper problem with the design. Put simply, you should almost never be putting different types into a list/vector unless you're only ever going to be using them through the typeclass. You say you need to access the data - are you doing the same eventual thing with the data, such that you could pull the action out into the shared trait?
I definitely might be doing everything wrong, I'm still quite new to programming in general, not just rust - I've mainly been doing very simple things in Python and Java. What I'm trying to create is a simple entity-component system (like here: http://entity-systems.wikidot.com/). I'm specifically trying to modify components inside a collection, and since the different components hold very different data, I assumed that having all the possible actions in the shared trait would not be a good thing. 
drop may already exist, but you wouldnt ever need to call it currently. Under this proposal you *do* to force the lifetime scope and prevent unexpected drops before a scope end. Thats what free does, effectively. If I wanted manual memory allocation and release, I would use c. 
&gt; "eh_personality" lol. what is this Edit: fould about it here: http://doc.rust-lang.org/master/std/rt/unwind/
That stuff is loaded once, then cached
It stands for exception handl{er,ing?} personality. LLVM calls it eh_personality I think, and Rust calls it the same.
Clueless and curious here. Which one do you have in mind? Ocsigen?
I am not exactly a fan of this change. I agree that most uses of Drop are probably about releasing memory (the most common resource, bar none) and that in general lifetime analysis may give the compiler the necessary information to release *early*. However, the other types of resources: locks, files, ... are not nearly as freely disposed of; and actually might matter. Scope-based release provides a simple contract. It may not be fully explicit, but it requires much less second-guessing that any attempt by the compiler to divine where it is best to drop. I would thus invoke the Principle of Least Astonishment together with the "Premature Optimization is the..." guideline: unless there is a very clear gain to be had, let's make it easy to figure out what's going on. *(Note: of course, the stack-based drop flag for conditional moves seems a great idea to shave off memory)*
Abstraction later would be good, but even Haskell doesn't have that yet properly.
To be even clearer: it is loaded only once and then re-used for *all* pages. Still the question remains: how many SVG images would fit, and how many are there in the doc to begin with ? On the other hand, making complexity notations pretty might encourage people to document them...
So, I'm from the Netherlands. Mosly playing the (official) UK servers. GMT+1 timezone. Playing thoughout the day. Just starting up, so no real preference
I think it will be easier to find survival buddies over at /r/playrust. This is the subreddit for the Rust programming language. Good luck out there in the wilderness :)
This is the Rust programming language subreddit. What you want is /r/playrust. Have fun!
Rust megathread! Bay area, Pacific time, I'm available whenever. I'm more of a crafter than a fighter.
&gt; Although they are libraries, having them doesn't imply very much. … one of the most important concerns is how much is already done. If they aren't, you're writing it yourself. Likewise, if `regex` and `big integers` are not already done, you're writing them yourself. The concise language issue with libraries is more about how code composition is done, rather than whether there's a `web dev` library.
I found OCaml much easier than rust. Lifetime errors make me feel like I'm 13 and fighting C++ compiler errors for the first time again. :)
It'd be nice if there was a general way of embedding diagrams and other notations in doc comments. For example, .dot files to display graphs.
Yes, I know about Mono. I stand by what I said.
This is the first alternative mentioned in the RFC. It was apparently found to be too painful in actual usage.
Ideally, the relevant types would depend on one another, lifetime-wise. Not only would this ensure that the compiler drops them in the correct order, but it would prevent the programmer from accidentally dropping them in the wrong order, as well.
If any kind of static drop semantics (aside from unbalance drops being an error, which was found too painful) are implemented (which seems to be several people's desire), this contract will be broken in at least some cases. I think doing so in a consistent way is better than having corner cases. Also, the RAII property that objects will be cleaned up regardless of how the scope is terminated is still maintained.
http://www.reddit.com/r/rust/comments/2a721y/a_safe_way_to_reuse_the_same_code_for_immutable/cj8pebl is a pretty insightful comment in a thread (all of which is worth reading) on this topic. In short the changes to the language would be nontrivial, but it would be an interesting avenue of investigation.
hmm, I remember that I needed to get used to writing "x: int" instead of "int x" for a function parameter but that passed somewhat quickly.
At least on wikipedia, server-side rendering has proven to be a weak solution. In particular it produces garbage results for hidpi displays. However it is a more robust solution for noscript users, I suppose.
&gt; all of which share a trait but also have individually different methods that I need to access. Sorry to be blunt but your design smells. Don't go there and try to solve the problem you described here. Think over your design. It will be better in the long run.
Static drop semantics by default with a hidden local boolean to handle code that's explicitly performing a conditional drop is the only proposal I support. The code can avoid the dynamic flag by explicitly calling `drop` as a micro-optimization.
It is only loaded on pages with equations. If someone were to write a Rust LaTeX renderer it should be considered, but that's a lot of work, and presumably the fonts would still be required (or the SVGs would be large).
We would prefer the docs to work locally (without an internet connection) as much as possible and KaTeX can be easily included, while MathJax is much larger. It seems reasonable to extend this support in future to allow alternate renderers, but KaTeX works for the most common things.
&gt; Should I add getters and setters for everything into the shared trait? Well, the code that iterates over your vector expects a certain interface to work with. If the getters/setters are not part of that interface you shouldn't call them. If you want to call them then you should add them to the trait. Rust is a static typed language and very type safe. If you need to circumvent/trick the type system to achieve something then that's a sign that there's something not right. About your entity/component system: You usually process each component type for itself. So you would first iterate over a Vec&lt;HealthComponent&gt; then a Vec&lt;InventoryComponent&gt;, etc.
Actually results of that study shows that underscore style is more readable (and I don't agree).
Thanks for the advice! I just managed to get everything working with my current (bad) design, but I'm only doing this to learn, so I'll probably start over tomorrow and try to structure everything better. I'm not entirely sure how I would do it if I didn't have all of my components in one collection, though. My understanding is most likely completely wrong, but right now it's very easy for all of my systems to access any component they need. If I had all the components in separate collections, it seems I would need to pass in a lot of arguments to each system and when creating new components it would be very hard to maintain. How should I be doing this?
By posting this I'm necessarily not agreeing with everything he says, but it is interesting to see the perspective of an experienced game developer.
Just wondering what happens if you reply to an bot-deleted comment, don't mind me.
I didn't watch this because of mobile, but Blow is at least loosely aware of Rust, I've talked with him about it before, and he's tweeted about it a few times.
This honestly made me question if I want to continue building a game engine in Rust. So far I've been mostly researching and playing with the language and relevant libraries but some of the things he mentioned about Rust I have noticed as well. Great talk, worth watching the whole thing.
I stopped watching when he criticized RAII and confused it with OO-oriented approach as in C++.
Summary of the first 15 minutes or so: It would be nice to use a language other than the behemoth of C+ for gamedev. However, 'Big idea' languages (i.e. ones that have a strong priority for some particular feature/behaviour) are not appropriate for high performance games. There's three languages that are close: Go, D and Rust. - Go has GC and is a 'big idea' language in terms of concurrency. - D has (optional) GC and is too close to C++ to be worth it now. - Rust "cares too much about safety" "(probably too high friction)". "Rust is very concerned about never letting you do unsafe things to the point of being a big idea language". "I assume it will be an environment I don't want to program in because friction will be too high". He also talks about how Rust is new and the ideas need to prove themselves. (The Rust stuff is about 9:00-12:00.) He then starts talking about how he wants to build a cleaned up C, that allows you to build non-'dogmatic' high-level things on top of it.
Is he developing his game in C++ though? It's like he never heard of unique_ptr.
He does appear to be focused C++'s implementation of RAII. And even then, it's unfocused, he's complaining about needing to implement copy constructors and move constructors and iterators... none of which seem directly relevant to RAII. It seems that Rust's RAII-style handling of mutex-protected data is an example of something where RAII is actually really useful; there's actually no way to access the contained data unsynchronised. He also says "the reason RAII exists because of exceptions", which doesn't seem reasonable, e.g. it allows you to avoid the `goto cleanup;` pattern required to handle early `return`, and also avoid having to manually do the clean up. (And goes on about how 'RAII is bad because exceptions are bad'.)
What he wants from the language and how it relates to Rust: - "no god damn header files" - check - refactorability - we have a rich type system that helps, so check? - no dereference operator - for member access - check, but we still need to deref some things manually - ownership over some pointers + errors at compile time - duh, check - syntax improvements for unique_ptr&lt;T&gt; to focus on T - our Box type is shorter but not quite there yet - optional types - check, beauty of algebraic data types - concurrency guarantees - AFAIK we don't catch deadlocks statically, but everything else is safe - "fewer / no implicit type conversions" - check - "named argument passing" - missing - serialization with per-member markup - rather doable - "The language spec says the compiler just does everything (no wacky tools on different OSes)" - we're doing all of the compiling with one command with multiple target support (if I'm not mistaken), so check? - "Permissive license" - check - nested comment blocks (/* /* */ */) - check (thx /u/dbaupp) --- - hot code reload / atomic deploy - interesting, missing, but probably too late to be done at the language level? - multiple return types - check (destructuring tuples) [thx /u/RFDaemoniac] - not having exceptions - we have Result and Option, unwinding only happens in critical cases, so check? [thx /u/RFDaemoniac] I think that's most of them from the 2nd half of the vid.
thanks. you saved me from downloading 200MB with my 256k**b**ps connection
&gt; nested code blocks (/* /* */ */) - missing (Do you mean nested comments? [They do work][1], just have poor highlighting support.) [1]: http://play.rust-lang.org/?code=fn%20main%28%29%20{%0A%20%20%20%20%2F*%20%2F*%20*%2F%20this%20would%20be%20a%20syntax%20error%20without%20nesting%20*%2F%0A%20%20%20%20%0A%20%20%20%20println!%28%22all%20ok!%22%29%3B%0A}%0A&amp;run=1
Exactly, also, if he wants to keep most of his resources online throughout the lifecycle of his application, he can just omit RAII and manually cleanup or something because here it's optional. I still think long-standing objects with RAII would be more comfortable.
Yes I agree, it's better than C++ for what we need it for certainly, but I think I may stick with Unity for a bit longer to see if a language really does come of this before committing a massive amount of effort.
I can't presume to talk for Blow but it seems like Rust meets the majority of his needs / wish list with the exception of a few notable things. I get the impression that he would enjoy Rust if it wasn't so dogmatic about safety, maybe via making them compile time warnings instead of errors and being less strict about the borrowing mechanics. He is a huge fan of the language getting out of your way and being as frictionless as possible. I also get the impression from him from other talks that he would be a fan of Rust's traits as he has mentioned in the past that he prefers component based architectures over deep inheritance. 
Yep. He takes the idea of RAII and hates on it because of C++'s overly verbose requirements for using it.
He also starts talking about memory ownership around 53:00, you put a `!` on a pointer to denote that it should be freed automatically (i.e. `T*!` is a short notation for `Box&lt;T&gt;`), but doesn't have copy constructors or ownership moving (so he says...), meaning you can have two owning pointers to the same memory, leading to double frees etc (problems he describes as tolerable and not-that-hard-to-fix). He then describes how you can use a debug allocator to detect freeing freed memory. Clearly this doesn't handle use-after free though; he then describes how you can overwrite freed memory with a 0xDEADBEEF-style canary, and then have the debugger hook into it to give you more info; it seems like this would get in the way of high-performance allocators and require debug builds to actually detect any problems. On the "so he says", he says that putting the ownership^1 in the language allows the compiler to statically check things more and thus give more errors about freeing/useing freed memory. Sounds rather similar to Rust's ownership! ^1 One specific example of ownership: this only models unique ownership of normal memory allocations.
For what it's worth, I'm perfectly fine with that solution. I wrote this RFC because the weight seemed to be behind completely static drops, but I felt the manner used to achieve it in the static drops RFC was less than ideal.
a game can't fail. it fails cert. therefore *any* runtime test for failure is an un-necasery waste of CPU cycles, in a game. It has to avoid failure by design. games use debug/release builds to handle this sort of thing. In a debug build you might have bounds-check everywhere, then lose it in release. you're right about sequential access but there's plenty of indexed data structures to deal with 
RAII is rather uninteresting to us, because it is rarely used. In Rust, it seems a bit un-necessary, as well as causing problems, such as the recent "drop" patches. I for one, would much rather see simple macros, such as ``` with_open_file("some.file", "rw") { // do something with file. } ```
`Box&lt;[Vector3]&gt;` in Rust (this has benefits like sized deallocation, which is faster), I'd guess there's something similar in the C++ stdlib; if not, it doesn't seem so hard to implement.
So apparently I haven't been paying attention. I'll be moving this thread to /r/playrust as some of you already have suggested. 
Indeed. It's far more common to put resources in "manager" objects. 
Seems like he's really criticising the interaction of RAII with Exception Handling which is fair enough. Many game developers simply never use Exceptions. He should be happy with Rust's approach to error handling, which is dissimilar to exceptions. i.e. Rusts' RAII would not invoke the hazard he's talking about.
I haven't ever seen bounds checks to be more than a few percent of the most tight inner loops that do nothing but array indexing, and that's when I deliberately went out of my way to not use iterators.
Thats because the struct he shows is using a performance optimization commonly used in AAA game development. The structure he shows vs the structure you show, he will be able to get much better performance in his version. The example in this case is rather simplistic in this case but the point still stands. From engine lead at Insomniac Games: http://www.slideshare.net/cellperformance/data-oriented-design-and-c See: Structure of Arrays vs Array of Structures. See: Data Oriented Design
The absolute worst I've seen, other than missed vectorisation, is the [overall ~15% improvement from removing bounds checks in `reverse`](http://www.reddit.com/r/rust/comments/2fenlg/benchmark_improvement_fannkuchredux/ck8swcx) (`default` vs. `doener`). `reverse` does *two* look-ups inside a tiny loop (best case, 6 or 7 instructions), meaning the 4 additional instructions due to bounds checking are very significant.
That's assuming you miss the branch, and I assume that the Rust bounds checking annotates the branch to default to success, rather than failure. As long as you don't do anything to screw up speculative execution (stores/loads/whatever), then you only get the cost of the check and branch instruction (two cycles?).
It can get in the way of other optimisations like vectorisation.
&gt; Forgive me for digging into this, but isn't this a micro optimisation? Since Vec's length should be in the same cache line as the pointer, you get one branch more with no fetches, right? game engines run on platforms with really bad CPUs sometimes. a games machine is maximum graphics with a minimal cut price CPU. We're used to the Zero Cost aspect of C/C++. You only ask for something you NEED. There's nothing going on you didn't ask for. You know a correctly designed program NEVER does out of bounds indexing, it never Divides by Zero, it never Overflows.. etc... - So its the job of your Debug Build to have extra checks to track down any mistakes you made. The runtime never, ever needs these checks, end of. you test for reasons other than correctness, (i.e does it look right?, is it fun? does it conform to platform UI guidelines?), so a debug/release model is fine
Yeah, I figured it was something like this. Game development doesn't care about safety as much as we do in Web browsers (and as much as Web apps, databases, kernels, systems, etc.) do. I've been serious about suggesting we have a mode whereby the borrow and region checks are just turned off. It would be pretty easy to do that, and the libraries and ecosystem would all Just Work. I'd rather not spend a lot of effort to do that *now*, though—we have work to do on the safe part of Rust. Moreover, by and large, Rust users like me value safety, even when working on projects where safety isn't paramount (like sprocketnes in my case), because the up front cost to learn the system pays dividends in productivity when you don't have to reach for the debugger to debug random memory errors. Yeah, sometimes the debugger doesn't cost too much time—but you can never beat "the compiler told you exactly where the problem is" for speed of development. :)
&gt; syntax improvements for unique_ptr&lt;T&gt; to focus on T - our Box type is shorter but not quite there yet &gt; hehe Rust used to have ~T, ~[T], @T :) 
I'm not assuming, I'm reporting my own finding: I find rust is slower to write than C++. This isn't a 'comfort zone issue' - I've been looking into rust for 1year+. To try put my finger on it:- [1] Rusts *safety* insists that everything is correct at every step. it forces you to do more work up-front. 'rapid iteration': you can skip both efficiency and correctness whilst you're focusing on other things i.e *design*. Then you debug/optimize *once you're happy with design*. [2]Another thing that can make it feel less productive than C++ for me is that it pushes more naming/lookup work on you. maybe i'm feeling the lack of conversion constructors and so on. (i know tweaks are coming for strings..). The fact that trait bounds are *compulsory* is a big offender here. Forthcoming C++ concepts will give me the best of both - adhoc when I want it, traits when i want it. 
&gt; some ideas to fix .. imagine an 'std::unsafe::Vec' that provided [] without bounds checking, and so on. Well, at least immutable slices already provide an `unsafe_get` method.
I don't really see how it would Just Work given that Rust has much stricter aliasing semantics than C--in the absence of the borrow checker, I think it's a lot more difficult to write correct Rust than correct C.
Still, it's a kind of problem you solve *once* and not for every class where you do the exact same thing. So, instead of relying on std::vector you could have your own non-groable wrapper around T* that "feels" responsible for freeing it. It should still compose and not make you write multiple structs that each get their own copy/move ctors and dtors.
So this is interesting, because I haven't found that Rust forces me to do work up front that wasn't necessary to have a functioning design in the first place. Rust forces you to make choices like reference counting versus unique ownership, but those are really fundamental choices that you can't really get around in C++ either. If you don't make the right choice (for example, using `unique_ptr` where you should have used `shared_ptr`), your prototype won't work at all, and I can't see how non-working software can help. I can certainly see how using a GC'd language for prototyping can be a good idea, but not using C++ for prototyping. C++ forces all the same decisions on you that Rust does. There are missing borrow check features that can slow you down, but usually that's imprecision in the borrow checker that we know how to, and plan to, fix (SEME regions and nested calls, for example). The ownership and borrowing *discipline* doesn't seem to slow me down over C++.
[1] rust vs C++: safety &gt; performance - C++ is more compact than Rust for maximum performant code. [2] rust vs C# :it prioritises safety and performance over productivity - so it might not be as productive as C#. Its possible a lot of tweaks post 1.0 might improve productivity IMO the 'perfect language for games' would prioritize (i) performance(ii)high-productivity(iii)safety in the same package, in that order.
Of course I don't yet know exactly how you'd approach this, but I find the idea of making more and more safety optional a little concerning. It would be really disappointing for this sort of code to start flowing through the rust ecosystem, I have a feeling a lot of early starters might turn off the checkers just because they can't be bothered to deal with them and learn. Coming from c++ I certainly found it jarring at first, but after giving it a good month I find production at *least* as easy, if not moreso thanks to the great practise it encourages. This learning process that comes with rust is probably the most valuable thing I've taken away from the language.
/u/xgalaxy does have a [point](https://www.reddit.com/r/rust/comments/2gwi11/jonathan_blow_ideas_about_a_new_programming/ckn90ge), though.
I don't see how that's particularly related. You can still use RAII with data with optimised representations.
If you can come up with a nice mesh implementation supporting vertices and indices being allocated in one block, be my guest. So far, I was thinking of something like this struct Mesh { unique_ptr&lt;void&gt; raw_data; slice&lt;Vector3&gt; vertices; // referring to a raw_data portion slice&lt;int&gt; indices; // referring to a raw_data portion }; Not that pretty.
Well, you can have `Mesh` expose an RAII interface with the internals manually managing everything. I think your point is it is not good idea to try to use RAII everywhere, but no-one was suggesting that.
You probably meant compilable or mostly working, not correct. It is in fact harder to write correct C than correct Rust.
I think what he calls "friction" is just getting used to the language. Typing is not the slowest part of programming, thinking is. When he changes something in his code, the rust compiler spits out an error message that forces him to change something somewhere else he didn't expect to have to change, and this annoys him. As you learn the language, your expectations change, and I find that I am no longer annoyed by these error messages, both because I don't write as much code that has errors and also because when I do get an error, it is caused by either a typo or an underlying design flaw, so I appreciate the error message.
The point is that RAII *does* get a bit noisy if you *also* want *even more* control over memory layouts because then you can't easily compose your types using other abstractions (like vectors).
It's easier to invoke undefined behaviour (i.e. have incorrect Rust) in Rust some types have more restrictions. You can mostly avoid this via the `*mut` and `*const` pointer types.
how can you say it prioritizes safety over productivity? Safety comes largely from behaving like a high level language in terms of general memory management, just without the penalty of garbage collection, and I also fail to see how it impacts performance as well, since all of the memory management is done at compile time. Premature optimization is the cause of a great many software problems, so you shouldn't be worried about it taking an extra line of code vs C++ to achieve optimal performance until you've proven with data that that section of code is the bottleneck. Until then, enjoy writing in a language that feels high level (and therefore easy to be productive in) while still reaping the benefits of fully native code execution. All of your reasoning seems to be based on misconceptions, but maybe I'm just missing some data.
The issue is design difference as Mr Blow mentioned. It'd be to not be forced into a specific memory management option in the language and be rather optional, this isn't rusts design goal, so it might not draw much of a gamedev following due to this.
I think Jon may have noted your post with displeasure: https://twitter.com/Jonathan_Blow/status/513167455695286272
It seems like he really wanted an unsafe pointer type, so I made one for him! I'm sure this probably exists already, but screw it, it was easy and fun to make. https://github.com/TyOverby/unsafe_pointer
&gt; &gt; &gt; Imagine there's a library that only works with safety off. Now you have to turn safety off in your own library that depends on it in order to get a compile to happen. And everyone downstream writing application code would have to as well. Could that be avoided somehow, while still making borrow checking optional? Borrow checking and region checking are both intraprocedural, so that wouldn't be a problem. However, it might be possible for people to make APIs you can't call outside of a safe setting without transmuting stuff. But we already have `unsafe` for that, so I don't think that would be an issue.
Could there be a some way of tracking double frees if debugging is enabled?
By sticking an Rc&lt;&gt; in there during debug mode, you could do basically anything. 
I think ASan/Valgrind are as good as you can reasonably do here.
I watched this whole talk. He has a lot of good ideas, and I love to hear him talk. If I were to summarize the difference between his goals as a game developer, and the goals of rust, I think it would come down to one thing: **Security**. In the world of application development, such as writing a browser. You don't want *any* memory errors. One extremely rare memory error, and it's game over, you have an exploit, and you've failed. In his world, memory errors are just minor inconveniences that he wants to be able to track down in a quick and timely fashion. You can certainly argue that rust's strong memory safety does lead to higher productivity by getting rid of "heisenbugs" that are so hard to track down. Rust also seems to solve his problem of making concurrency easier to work with, which he didn't seem to have a solution for. But perhaps he is right about rust being "unproven". I'm hopeful that eventually it'll prove itself.
I once disassembled a Mono program and it was somewhat disappointing to see how often "ms" appeared in the bytecode.
Looks that way, yes. And this response is totally understandible given that I wrote the initial comment after having watched only the first half of his talk.
I believe that generics are stored in the compiled library as ASTs. When you use a generic, the AST gets pulled out, the parameters substituted, and the result compiled into your binary (probably).
That's a valid point. I forgot that mathjax is such a beast. 
In D, this is implemented with the scope keyword. If you do: bar = something_that_needs_cleanup(); scope(exit) do_cleanup(); if the execution reaches the scope statement, do_cleanup will be executed when leaving the current function, regardless of if any exceptions were thrown. in addition to scope(exit), there's also scope(failure) and scope(success). In practice, the mechanism is sublime, as it makes doing robust error handling that correctly cleans everything regardless of where exceptions happen very simple.
This looks nice. It is great to see nickel.rs progressing. BTW Would it make sense to write like this mustachewrapper::render(response,"examples/assets/template.tpl", &amp;data); Instead of this response.render("examples/assets/template.tpl", &amp;data); That could allow the user to easily use a template engine of his own choice.
I'm wondering if lack of an advanced dev-environment / IDE is the problem here, and not the language itself. Looks to me, people dislike writing lifetimes manually. If there was tool that would automatically insert lifetimes.. ok never mind..
How can scripting languages be the "worst" players in the game since they are the most used? Bringing static safety to mainstream web programming would be excellent, if people *actually use* Rust to build web apps. Otherwise we should stick to [Ur/Web](http://www.impredicative.com/ur/) since it seems to have the most number of static features.
&gt; he's complaining about needing to implement copy constructors and move constructors and iterators... none of which seem directly relevant to RAII. Of course, copy ctors and assignment operators are involved. You have to at least explicitly delete them if you write your own class that manages something. Providing a destructor is not sufficient to make a struct non-copyable and non-assignable in C++. That's why we have the [Rule of three](https://en.wikipedia.org/wiki/Rule_of_three). This is not really beginner-friendly. It happens that you forget to implement some of it in which case the compiler generates defaults that do the wrong thing. And exceptions *do* make RAII more important. But I agree, RAII is also useful for other things including getting rid of gotos.
With the struct Mesh { Vector3 *! vertices; int *! indices; @join vertices }; syntax, the compiler would be aware of this and require all allocations/deallocations to be at once.
That's still missing the point, which was to move from `Box&lt;T&gt;` to `Box&lt;*&gt; T`.
Well, what should I say? When I saw the other inheritance proposals, my reaction was either "Why? Just why?" or "This is quite flexible and preferable, but ..." However, when I saw this, my first reaction was one word. "Yes." 
I think the comment I replied to was edited. As dbaupp mentions, it originally implied that camel case can be harder to read. Maybe the person changed their mind. I tend to agree with the study, we're conditioned from a young age to read things with some kind of spacing between the words; so_to_me_personally_this_reads_more_naturally than SomethingLikeThisWhichMakesMeFocusHarder. Plus I dislike being forced into certain capitalization for acronyms, e.g. Http, Dns, Simd, Os, etc. It just feels wrong. :) Anyways, in the grand scheme of things it's not a big deal and I just go with the style the project chooses. :)
Oops, it seems I was off-the-mark (I haven't written a nontrivial line of C++ ever)... but iterators *are* completely unrelated. Either way, it's still *entirely* C++ focused; I imagine Jonathan might feel less negative toward RAII if his experience wasn't with a system that is so error prone and verbose. &gt; And exceptions do make RAII more important. Yes, definitely, but it's horrible logic to use "exceptions are bad" as evidence that "RAII is bad".
This is also explicitly spoken about in the talk with some sort of "allocated with" decorator
Fascinating, because I have been working on this exact same kind of problem. At first, my Token structs all had slices into the original text that I'm tokenizing. But the explicit lifetimes became too much of a burden to be worth it. They trickled through many of the functions for working with Tokens and became painful. In fact, at one point I tried to implement a trait so I could use the + operator to combine tokens, but the explicit lifetime parameters disqualified my functions from being valid trait implementations. (I think I've heard talk of plans to improve this). I concluded that things feel much more lightweight and my code can evolve faster when I avoid named lifetime parameters in structs when possible. My Tokens are just as happy with a starting and ending integer index into the original text, which I could use at some point down the road to take a slice if needed. I'd love to hear any conclusions/thoughts others have reached on this matter. EDIT: Let me add, however, that the very fact that Rust can do this, as the article's author points out, is extremely valuable. You have to carefully consider when to use it, but it is something that other languages don't even offer.
If the value implements drop you do not run the destructor. Add a pointer read to `free`?
Wow this is a really good point. Am I missing something here or will doing this be able to warn you about a ton of memory errors. Why is this not already done in C/C++?
It is/can be, e.g. [AddressSanitizer](http://clang.llvm.org/docs/AddressSanitizer.html) supports detecting double frees, and (not quite the same, but along these lines) the BDW garbage collector [can be used as a leak detector](http://www.hboehm.info/gc/leak.html).
As a Rust newbie, how would you implement this structure in Rust, which Jon uses an example in his talk? struct Mesh { /*const*/ int nVerts; Vector3* /*const*/ pVerts; /*const*/ int nIndices; int* /*const*/ pIndices; }; Mesh* NewMesh(int nVerts, int nIndices) { size_t size = sizeof(Mesh) + sizeof(Vector3) * nVerts + sizeof(int) * nIndices; uint8_t* buf = (uint8_t*)malloc(size); memclear(buf, size); Mesh* mesh = (Mesh*)buf; buf += sizeof(Mesh); mesh-&gt;pVerts = (Vector3*)buf; buf += sizeof(Vector3) * nVerts; mesh-&gt;pIndices = (int*)buf; buf += sizeof(int) * nIndices; mesh-&gt;nVerts = nVerts; mesh-&gt;nIndices = nIndices; return mesh; } void FreeMesh(Mesh* mesh) { free(mesh); } What I think I want would start with something like this: struct Slice&lt;'lifetime, T&gt; { size : int, data : ??? &amp;'lifetime mut [T] ??? }; struct Mesh { vertices : Slice&lt;'self, Vector3&gt;, indices : Slice&lt;'self, int&gt; }; with `'self` being 'the lifetime of this object'. But I don't think that exists, so you have to add a useless lifetime parameter to Mesh to represent its own lifetime, which then has to be repeated everywhere Mesh is used.
OK, so I'll say something more than just "yes". ;) This intergrates with existing features so well that a programmer may not even be aware that he/she can do inheritance in Rust. All he/she sees is that he/she can define fields in traits (which is more consistent with other associated items), override and explicitly call default implementations in derived traits (which makes much sense and is a welcomed feature on its own). And this is a good thing. It is better than then "extending structs/enums" proposals, as it doesn't introduce a new dispatch method and a new hierarchy other than trait-based dispatch/trait hierarchy. And it is better than the "low level building blocks" proposals in that this RFC's components are easily useful in day-to-day programming and are likely to be requested by programmers in the future anyway. I'd expect, a "low level building blocks" proposal can still be implemented as a library and cover the use cases not addressed by this RFC, if any. 
Yes, people always say "lifetimes are how long objects live!" But never give a true to life example like this. I think that I will give this article to people who are confused about why explicit lifetime parameters exist. (queue someone linking me to new lifetime guide which i have not read because its not out yet)
The problem with the C++ version is that you are *explicitly* allowing the, in this case, incorrect behavior. By taking a const reference you're saying that you don't manage the life time of the passed object and are perfectly fine if the object is a temporary and dies with the scope, which is clearly not the case. If you want to communicate to the user that you don't manage the object but it should not be allowed for the object to die right after, you should take a non-const reference (non-const references don't bind to rvalues). Even more safe would be to pass in a std::shared_ptr and make sure that each token bumps the counter (return vector of tuple&lt;ptr, iter, iter&gt;). 
I don't think a lifetime-based approach will work at the moment (that is, I don't believe we have the sort of power required to reason about self references in a useful way). The original C can easily be translated into Rust via unsafe code, and it would look fairly similar. use std::rt::heap; use std::{mem, ptr, raw}; struct Mesh_ { nVerts: uint, pVerts: *mut Vector3, nIndices: uint, pIndices: *mut uint } pub struct Mesh { data: *const Mesh_ } impl Mesh { pub fn new(nVerts: uint, nIndices: uint) -&gt; Mesh { // (the alignment may not be precisely right, depending on // Vector3) let size = mem::size_of::&lt;Mesh_&gt;() + nVerts * mem::size_of::&lt;Vector3&gt;() + nIndices * mem::size_of::&lt;int&gt;(); unsafe { let mut buf = heap::allocate(size, mem::align_of::&lt;Mesh_&gt;()); ptr::set_memory(buf, 0, size); let mesh = buf as *mut Mesh_; buf = buf.offset(mem::size_of::&lt;Mesh_&gt;()); (*mesh).nVerts = nVerts; (*mesh).pVerts = buf as *mut _; buf = buf.offset(nVerts * mem::size_of::&lt;Vector3&gt;()); (*mesh).nIndices = nIndices; (*mesh).pIndices = buf as *mut _; Mesh { data: buf } } } /// View the contained data as slices. pub fn as_mut&lt;'a&gt;(&amp;'a mut self) -&gt; (&amp;'a mut [Vector3], &amp;'a mut [int]) { unsafe { (mem::transmute(raw::Slice { data: (*self.data).pVerts, len: (*self.data).nVerts }), mem::transmute(raw::Slice { data: (*self.data).pIndices, len: (*self.data).nIndices })) } } } impl Drop for Mesh { fn drop(&amp;mut self) { unsafe { let size = mem::size_of::&lt;Mesh_&gt;() + (*self.data).nVerts * mem::size_of::&lt;Vector3&gt;() + (*self.data).nIndices * mem::size_of::&lt;int&gt;(); heap::deallocate(self.data, size, mem::align_of::&lt;Mesh_&gt;()); } } } I provided the wrapper struct and the `as_mut` method as an example of how one can still build safe abstractions on top of these sort of optimised types. (Other than those and using a more efficient allocation protocol, it is literally a straight translation of the C.)
I find productivity slows down when dealing with lots of Option / Result types. We need HKTs. And the borrow checker sometimes behaves unexpectedly, ie. working on a wrapped value via a match block will work, but trying to do the same via `map` or `and_then` will give unexpected errors.
I try to avoid C++ but those solutions sound miraculously more complex than his Rust solution. Why use shared_ptr aka reference counting when one is able to just keep track of lifetimes and not worry about any of that crazy stuff.
Valgrind is great but prohibitively slow for games; ASan is much better in that regard though.
&gt; I try to avoid C++ but those solutions sound miraculously more complex than his Rust solution. Drop the const in *tokenize_string2(const string &amp;text);* and the code *tokenize_string2(get_input_string())* will no longer compile. Taking a *std::shared_ptr* is definitely more verbose but it's not that bad. Just take *std::shared_ptr&lt;std::string&gt; text* and emplace_back std::make_tuple(text, iter, iter). &gt; Why use shared_ptr aka reference counting when one is able to just keep track of lifetimes and not worry about any of that crazy stuff. Well, because we don't have this fancy stuff in C++ :).
There are a few really great things about this proposal. * Most of the proposal just feels like a set of useful enhancements to the approach I would have taken to achieve "inheritance" in current Rust. * Most of the machinery for achieving high-performance C++-like single inheritance has turned out looking like a set of scary performance hacks for expert programmers (the `Vtable&lt;T,Tr&gt;` stuff, `repr(fixed)`, etc). This is a very good thing, since these features *are* scary performance hacks to fulfil a very narrow niche; there's no need for them to appear inviting to the average Rust programmer. * Those scary-performance-hack features don't change the core language at all (in contrast to, say, the `Extend`/`Cast` proposal, or the proposal which would have utterly changed the semantics of structs and enums). There's been a lot of feamongering about the language "being ruined, just so that the Servo developers can have OOP", so this can only be a good thing. * The "upcasting" and "trait field" features work well for both single inheritance and multiple inheritance, preserving decent performance in both cases. Multiple inheritance can be a very powerful and intuitive tool when it's used wisely, so this is a good thing. (I also think the proposal smoothes over some of C++'s awkward problems w/r/t the diamond problem and virtual inheritance, but I'm not certain.) One niggling worry is that field composition seems a bit tacked on or underexplained. I'm not clear on why it's a part of this proposal. Also, I'm not sure why the type needs to be part of the "field specification" syntax (ie, why can't `a: uint =&gt; self.b` just be `a =&gt; self.b`?)
The suggestion over on /r/programming was to disable the rvalue reference version, that is: vector&lt;pair&lt;const char *,const char *&gt;&gt; tokenize_string2(string &amp;&amp;text) = delete; But all of these work-arounds are missing the point: safety is not the default in C++, you have to opt-in by remembering certain incantations to disable certain types of bad behaviour (or opt-in by choosing less efficient code). And, anyway, all these work-arounds miss cases, e.g. std::vector&lt;std::pair&lt;char*, char*&gt;&gt; foo() { auto s = some_string(); return tokenize_string2(s); } &gt; tuple&lt;ptr, iter, iter&gt; This is safer, but is still problematic: the `ptr` is not connected to the iterators and they can be freely separated. (It's also significantly less efficient, due to the atomic reference counts of `shared_ptr`.)
This looks nice and simple. I like the similarity with associated traits. I am not fully convinced that field composition sugar is necessary.
http://doc.rust-lang.org/rustuv/ ? 
I did try to make it an acceptable local dependency, [as documented here](https://github.com/rust-lang/rust/issues/16300) Highlights include &gt; Holy shit mathjax is a 30,000 file 60MB monstrosity &gt; "29,000 files and 12.5MB" of png fonts &gt; Okay so I sliced Mathjax down to 400 files clocking in at 2MB. But it was still deemed unacceptably large.
Ah, cool!
Actually, you don't have to delete them if you write a move-constructor of move-assignment-operator, because in this case they are implicitly deleted. However, I am starting to wonder if maybe the issue is not a misuse of RAII. If you cleanly separate concerns, then your class: - either is a technical class focusing on RAII, and only that - or a business class not implementing any RAII at all, only functionality As an example, should your business class use `unique_ptr` under the hood then it is implicitly a "move-only" class: - copy-constructor and copy-assignment operator are implicitly deleted - move-constructor, move-assignment operator and destructor are implicitly defined and do the right thing See: *hands free*! Oh, but you wanted deep-copying, so you are thinking of adding a copy-constructor ? Don't. That would be violating the separations of concerns. Instead you are going to create a new dedicated pointer type *once* that will take care of making a deep copy of what it points to, that is, you are going to create a `Pimpl&lt;T&gt;` class. And then anytime you need it, just use the `Pimpl` class. The *Rule of Three* is old (C++03), in C++11, think **Rule of Zero**.
I had exactly the same reaction; there are of course shadowy zones, but all in all I really like the flexibility offered here. The ability to use bits and pieces for other purposes is a strength that should not be ~~overestimated~~ underestimated.
I suppose you might be able to make a nice rust macro for 'several contiguous vectors'.
Field composition syntax is basically there to allow defining a macro for defining an element in the inheritance hierachy that looks more like defining a class in other languages. (Not needing to mention parent "class" fields by using composition syntax to emmbedd them) I need to amend the RFC with an example for this. You're right though, as the RFC is right now it does not really have a clear purpose. Its an artifact of writing everything down without a plan :P About the associated field syntax: Its consistent with how trait impls work today: You also have to give types for the functions you implement. (And its required for inherent associated fields)
&gt; All of your reasoning seems to be based on misconceptions, I'm reporting my findings after 15 years of gamedev in C/C++ and looking into rust for ~1year+. &gt;Premature optimization is the cause of a great many software problems, back on the xbox360/ps3 you needed lots of tricks to avoid branches. Very fidly due to in-order CPUs; I don't think other cpus are as bad, but activisions' recent big release 'Destiny' still has to run on those... and yet games still have areas of coding where productivity is prized: they frequently embed Lua for scripting, and there's tools development surrounding an engine which isn't shipped to the end user, so performance isn't so critical. It would be amazing if one language could handle the full gamut of use-cases. Rust looked closer when it had sigils ~ and @. @ made 'gc too easy to use' - which is a valid criticism for fast,safe code, but made it look like rust could have done the job of Lua aswell. Jonathan Blow mentions the subjective distaste toward unique_ptr&lt;T&gt; ... he'd have preferred Rust in its' original form, I think
s/overestimated/underestimated ? Actually it feels more like "some fairly obvious extensions to traits that *also* supports inheritance". For one, I always expected someone would one day file a proposal for "overridable and explicitly callable default implementations". But I only see it as a convenient feature. "Associated fields" is also a natural extension, though I didn't see why we wanted them because they were bare fields and seemed to be a way to "uniformly leak implementation details", until *now*. The fact that they are bare fields is an *advantage* for the DOM/AST use cases. 
&gt; s/overestimated/underestimated ? Yes of course! Edited in :x 
Hoewver, ``` string &amp;text = get_input_string(); return tokenize_string2(text); ``` will still compile
Except you're usually supposed to be able to compile debug and opt builds simultaneously from the same code, while that would not necessarily be true with Rust's safety flags. As soon as you break it, you stop getting any guarantees because you can't build any longer.
Memory errors in games can be disastrous though, especially online games, and present the same security issues as any program, at least when running in PC.
Only with Visual C++ :)
Overall, I like this too. However, two things come to my mind: 1) Instead of the override keyword, maybe there could be "impl ParentTrait for ChildTrait"? The hopefully obvious gotcha here would be that the impl would not have to implement all methods of the parent trait. When we do "impl FooTrait for BarStruct", there is also sort of overriding going if the trait has a default implementation for some method, so this feels somewhat intuitive. 2) After the big inheritance thread a couple days ago, I started thinking about composition, and thought along the "embedding a struct with common fields as a field in your struct" route mentioned in the RFC. More specifically, I thought about imitating the way use statements work with modules. It's maybe best to just show how I imagine the code could look like: struct Position { pub x: i32, pub y: i32 } struct PositionedElement { pub use position::{x,y}, position: Position, ... } You can use the type PositionedElement like it had the fields x and y itself. You can also control which elements you want to `pub use`. The resulting PositionedElement::x would be nothing but a shortcut to PositionedElement::position::x. (As a sidenote, wildcard importing might be useful here.) If the `pub` in the `pub use` were skipped, it would cause the shortcuts to be visible only to the impl of PositionedElement. This would allow fields to work like `protected` in C++ and friends, though it would be the inheriting struct that decides if the fields are exposed or not. If Position had non-pub fields, aliasing them would not work. The embedded struct that is used as superclass could have `private` fields this way. No change to current situation in that sense. This could be extended to methods too: impl Position { pub fn translate(&amp;mut self, x_offset: i32, y_offset: i32) { ... } } impl PositionedElement { pub use Position::translate; pub fn new() -&gt; PositionedElement { ... } ... } The pub-used function would appear as method of PositionedElement to the outside user of the struct. You could translate the PositionedElement just like you translate Position. The non-public usage would work similarly as with fields. Edit: You could also pub use a method in a trait implementation too, so as a side effect you could get multiple alternative default implementations for a method, depending on what kind of struct you embed into the "derived" struct.
Now that you mention your macro use-case, it occurs to me that field composition would remove a lot of repetition w/r/t field definition and access, even in handwritten code. To wit, this code... struct ParentFields { a: uint, b: f32 } trait Parent { f: ParentFields, fn print_a(&amp;self) { println!("{}", self.f.a) } } impl Parent for ParentFields { f: ParentFields =&gt; self } struct ChildFields { p: ParentFields, c: int, d: f64 } trait Child { f: ChildFields, fn print_a_and_c(&amp;self) { println!("{}", self.f.p.a); println!("{}", self.f.c); } } impl Child for ChildFields { f: ChildFields =&gt; self } ...could become... struct ParentFields { a: uint, b: f32 } trait Parent { ..ParentFields fn print_a(&amp;self) { println!("{}", self.a) } } impl Parent for ParentFields { f: ParentFields =&gt; self } struct ChildFields { ..ParentFields, c: int, d: f64 } trait Child { ..ChildFields fn print_a_and_c(&amp;self) { println!("{}", self.a); println!("{}", self.c); } } impl Child for ChildFields { f: ChildFields =&gt; self } Note that all fields can be accessed as `self.x` rather than `self.f.x` or `self.f.p.x`, and the full listing of each struct's content only needs to be written once.
Can you explain what Win32 **COM** is?
You could say the exact same thing about C++.
Thanks!
&gt;&gt; All of your reasoning seems to be based on misconceptions, &gt;I'm reporting my findings after 15 years of gamedev in C/C++ and looking into rust for ~1year+. I'm not questioning your résumé, merely how general your "findings" are. So general that there are no specific examples to be rebutted. &gt;&gt;Premature optimization is the cause of a great many software problems, &gt;back on the xbox360/ps3 you needed lots of tricks to avoid branches. Very fidly due to in-order CPUs; I don't think other cpus are as bad, but activisions' recent big release 'Destiny' still has to run on those... Yes, stream processing and the related stream coding techniques were extremely important on the PS3, and less so on the 360, but how does this topic apply to Rust? I don't see how you *wouldn't* be able to write stream code in Rust just as easily as you can in C++, this just seems like a diversionary topic. No one is forcing you to use if-statements (which cause branching), and even optional types can be unwrapped unconditionally, if you're so inclined.
Heh yeah, while keywords can be precious, no need to avoid them at all costs.
Yes. I think this point is incredibly important. It's not just that language-guaranteed memory safety allows existing practices to be verified for safety. It's that guaranteed safety allows you to do things without a second thought which would otherwise need to be discarded as too dangerous. In the C++ codebase at work I've also chosen to pass all data objects around as copy-on-write atomically-reference-counted boxes with value semantics. Performance isn't paramount, so this tradeoff of performance for safety and peace of mind makes sense. But with Rust I could have my cake and eat it too.
&gt; "RAII is bad because exceptions are bad" Quote (39:33): &gt; The reason RAII exists is because of exceptions. Exceptions are silly at best, and really horribly damaging at worst. I really don't think "RAII is bad because exceptions are bad" is bad interpretation of what he is saying there. There's some later discussion about how exceptions + RAII make reasoning about control flow harder, but this still isn't evidence that RAII is bad, just that the combination of RAII &amp; exceptions is. &gt; "iterators are relevant to RAII." Quote (32:20) from the RAII section: &gt; now it's got to have a constructor and a destructor; and it's gotta have a copy constructor so I can assign it by value, and that's kinda slow sometimes so I better put in a move constructor also. And maybe you should have an iterator blah blah blah An off-hand comment sure, but why on earth bring it up at all when talking about RAII? &gt; And he's exactly right Yes, I agree the quote I took there was bad, because on the first run through I missed the 'originally' he said. However, the theme of my quote is definitely implied throughout that section: historically, RAII exists because of exceptions, but it still seems like very sensible behaviour even without exceptions.
Yes, `override` is a fairly standard keyword after all. I don't think many programmers would complain if they cannot use this as an identifier. ;) 
&gt; "named argument passing" - missing Nothing macros can't [fix](http://www.reddit.com/r/rust/comments/2gl0xj/keyworddefault_arguments_using_macros/).
&gt; stream processing... were extremely important on the PS3, and less so on the 360 I'm not talking about that.. I'm talking about the pain of the in-order processor generally. This hazard is equal on PS3 PPE and the Xbox 360 cores. Both have a very similar pipeline and hazards ... they're derived from the same powerPC core. &gt; No one is forcing you to use if-statements (which cause branching), rust by default has failure tests for safety: e.g. bounds checked arrays. These might introduce hidden branches, and in turn pipeline hazards. These CPU's need branchless code for freedom to re-order instructions. And even with OOOE on a decent cpu, you're making the hardware work harder. In C++ vector operator[] doesn't do bounds checks by default, but you could add bounds check to a debug build; that's the correct approach for games IMO. 
http://www.unknowncheats.me/forum/1042681-post67.html This is what happens when you think games don't have to worry about security.
games are usually sold on closed platforms - consoles , and now app-stores. game engines doesn't expose as much to the internet as a web-browser does. (a web browser is completely general.. practically an OS within an OS - it's the most extreme case of an online application) 
In the RFC about error conventions (take 3), it has been proposed to "let it crash" because out of bounds errors are considered bugs. See https://github.com/aturon/rfcs/blob/error-conventions-3/active/0000-error-conventions.md The motivation is that an out of bounds error tells you something is wrong with the algorithm, in which case there is no point in continuing the task. Therefore "let it crash".
&gt; This isn't a 'comfort zone issue' - I've been looking into rust for 1year+. That time alone doesn't mean anything. What matters is how much Rust code you have written. I've also been looking into Rust for 1year+, and in that time I've written 5 libaries, 2 complete games and a small scientific model. It's not just the number of lines of code, it's converting 8 different designs into Rust code and learning to see what works (1 project is just insufficient, unless you refactored it a few times). Indeed, initially I was fighting with Rust a lot... but after awhile, I got the model Rust was going for and it's been relatively easy going since then. I'll grant you that it might take a lot of effort to learn 'Effective Rust' (somebody should write a book on that).
And mutable slices provide `unsafe_set` and `unsafe_ref` (the latter returns a mutable reference).
Well, there is Steam. Also, security issues on your phone is sometimes *worse* than on PC.
This is jam-packed with a lot of our ideas for how to make collections better in Rust. Some of the big-ticket items include: * Remove basically all of the collection traits for now. They're awkward and don't actually support generic programming since there is e.g. no generic way to get an iterator. We'd rather just remove them while we wait for a stronger type system, in case people start to depend on them, even if they're marked experimental. * Normalize a lot of concrete APIs. Toss a lot of duplicated functionality like "swap" and "insert" in favour of a more minimal API surface-area. More commonly used collections like Vec and HashMap also have developed a massive suite of methods that other collections don't provide. We smooth this out a bit. * A solution to the equiv problem by introducing `Borrow` trait that abstracts over `T` -&gt; `&amp;T`, `String` -&gt; `&amp;str`, and `Vec&lt;T&gt;` -&gt; `&amp;[T]`; coupled with a `ToOwned` trait that abstracts over the opposite direction. Currently some active discussion on what precise form this takes. Currently a double-dispatch form is being favoured that should allow `find` on a `HashMap&lt;String, T&gt;` to take either a `&amp;str` or `&amp;String`. * A Copy on Write (Cow) smart pointer type based on the above traits. * Making iterators a bit easier to use with an `IntoIterator`trait. All iterators implement this trait as a no-op, while collections implement it in the following way: `into_iterator(vec)` = `vec.into_iter()`, `into_iterator(&amp;vec)` = `vec.iter()`, and `into_iterator(&amp;mut vec)` = `vec.iter_mut()`. All interfaces and language items should then move to taking an IntoIterator instead of an Iterator. This allows handling of iterators and collections uniformly as iterable. It then also becomes possible to write `for x in &amp;vec {}`. * Abstracting over direct or delayed production of an element with the `ByNeed` trait. Both `T` and `|| -&gt; T` would implement `ByNeed&lt;T&gt;`, allowing the caller to decide whether a value should have a delayed computation, without the need for different methods or implementations. `Predicate` similarly abstracts over the pattern of passing an element to search with or, a closure that performs a comparison. The `Predicate` implementation for a bare `&amp;T` would just be equality. * Implement `Deref` for `String` and `Vec` as an alternative to calling `.to_slice()`. In addition to `&amp;*vec` being much more concise, all the methods on slices would automatically be "forwarded" to Vec via the `.` operator's auto-deref properties. * Make more collection methods yield lazy iterators. This allows the caller to decide what to do with the result of the computation. It can be collected into a collection of their choice, directly looped over, or simply queried for certain properties without the guaranteed expense of computing the full result upfront or allocating a new collection.
A lot of people seem to miss his point about the type safe system in Rust with regards to things like ownership of pointers. Yes Rust does that, but at a cost to the programmer. I enjoyed learning Rust but use Go for many things now, mainly because of the type system. Of course it's GC just makes it unsuitable for many applications like games/realtime/OSes/drivers/lowlevel libs/etc... And lack of C ABI limits it too. Also maybe Rust could auto-thread much more. With modern development practices (namely thorough automated testing) and things like the sanitizer libraries (asan, etc...). Guaranteeing memory safety seems less important than it did in the past, when bad memory/thread safety could mean hours in a debugger (largely reduced with good testing practices) or bugs that only showup at runtime (largely reduced by the use of the various memory/thread sanitizers and not sharing memory between threads). Rust's type system might be guaranteed to be memory safe, but I have found it to be a massive pain in the butts. Having to deal with lifetime specifiers, borrowing and so on. Trying to find a way to turn some slice type into some other type by going through 6 chaining functions. It also seems fairly difficult to learn the underlying concepts (I think a lot of the documentation is written by compiler authors, could do with some definitions for things like what it means to be 'boxed').
Ah, DST. Now for something off-topic, is DST support complete now? 
There are two versions of the array access APIs. The *get* methods will yield Options as expected. These should be your go to API if you're uncertain about your search. However the *indexing* methods that are sugared to `foo[i]` will simply fail as a convenience for `foo.get(i).unwrap()`. Basically, this is because when indexing it is often the case that there is no correct behaviour to perform when an out-of-bounds index occurs. 
The best C++ solution I've seen so far was [posted on Hacker News](https://news.ycombinator.com/item?id=8344149): Use `pair&lt;size_t,size_t&gt;` and use it to index into the original string. This makes the underlying hackery obvious to maintenance programmers. Another good solution is to put all token-processing code into a single class, and give that class responsibility for owning the buffer. In this particular case, I really do want to avoid allocating `string` or `shared_ptr` objects for each token in the input. Still, Rust does allow to write the ugly, low-level text processing code that I actually want to write, and I find it pretty amazing that I can write it safely.
Now I understand, your "It also ignores things like libraries" remark was really about *batteries included* and doesn't have much connection with the (ir)relevance of the code-size of tiny benchmarks game programs. The (ir)relevance of the code-size of tiny programs is to do with the fact that they are tiny programs. Tiny programs don't show how well or how badly composition works in large programs.
&gt; I'm not entirely sure how I would do it if I didn't have all of my components in one collection, You usually have one collection for each component type. You could achieve this by having some kind manager who keeps track of all the collections. Then in your system you just ask the manager to give the collection for a certain component type. It helps when you think of a Entity/Component system as a relational database. 
Perhaps compile-time hashing for the lookup string could be done, passing the pre-compiled hash to the runtime function. Might not make much difference though, unless the strings are being written to disk rather than displayed.
&gt; With modern development practices (namely thorough automated testing) and things like the sanitizer libraries (asan, etc...). Guaranteeing memory safety seems less important than it did in the past, when bad memory/thread safety could mean hours in a debugger (largely reduced with good testing practices) or bugs that only showup at runtime (largely reduced by the use of the various memory/thread sanitizers and not sharing memory between threads). This is not the case for many applications such as Web browsers, where use-after-free still appears again and again and again. I suspect most games are full of use-after-free, and it doesn't matter for them because nobody is trying to break them.
&gt; he believes RAII raises ambient complexity Ok answering this precisely: *exceptions* definitely raise complexity by requiring more careful handling (i.e. essentially everything has to go via RAII to be correct), but I don't believe that RAII by itself raises it. Assuming there's no exceptions, if you've got an API that is worse when written with RAII... why is it written with RAII at all? If there's no exceptions, RAII is entirely opt-in and can be used when appropriate. That is, he dislikes exceptions, and the boilerplate of C++ (in particular of C++ RAII), but the direct hating on RAII as a standalone concept is somewhat misguided.
It still has bugs, and I think a few minor parts are still not implemented, but the main feature, unsized types, is implemented.
&gt; it doesn't matter for them because nobody is trying to break them. I wonder if we will start to see more people attacking games with the rise in esports (i.e. real money connected with winning games).
Very interesting. Still, I will not ever touch that language because of its syntax.
You can already do that if you really want to: fn handler (_request: &amp;Request, response: &amp;mut Response) { let mut data = HashMap::&lt;&amp;'static str, &amp;'static str&gt;::new(); data.insert("name", "UserName"); let template = mustache::compile_str("Hello {{ name }}!"); template.render(response.origin, &amp;data); } The downside is that you'd have to compile the template in every request since they're not stored anywhere with this approach. Once there are any alternatives to mustache we could rather easily support those too, even with the current design.
Is there a way we could possibly automate this process using attributes like he showed in the talk? Seems challenging.
You might want to take a look at the Haskell IO manager. http://www.johantibell.com/files/hask17ape-sullivan.pdf http://haskell.cs.yale.edu/wp-content/uploads/2013/08/hask035-voellmy.pdf I think the benchmarks are somewhat old, but Haskell's Warp server has out-performed nginx. Regardless of the benchmark results, the following is a great read, especially if one is looking to optimize performance. http://aosabook.org/en/posa/warp.html &gt; nice asynchronous An oxymoron? I'm still pretty new to Haskell (and Rust), but one of my favorite things about Haskell is the synchronous programming style without having to worry about blocking.
Also, please have a look at : https://github.com/carllerche/nix-rust which is a low level wrapper around unix sys APIs such as epoll. The higher level, high performance IO library on top of that is : https://github.com/carllerche/mio Hackathons are awesome, and I am all for hacking on IO APIs, but your guidelines are right on point with those of mio. I would recommend against making more APIs that do the same thing if it can be avoided. 
When did you last look at it? If you think the code in the article is horrible, then fair enough. The syntax has been getting a lot better lately though, and improvements are ongoing (e.g. the new slicing syntax that is on the way, if not just added by now). 
That's pretty interesting. Does this have any sort of effect on closed/open source licensing? Although if you look at .Net assemblies that can be disassembled so easily maybe this isn't a real issue. EDIT: I mean legally. Technically, it's actually an issue.
Why not just add a mode where you don't need to wrap * pointers in unsafe {} and possibly implicitly convert to &amp;? It would make it more clear that a developer doesn't care about memory safety in a particular crate, and make it so the crate's interface doesn't lie about lifetimes to other crates in which unsafe mode might not be enabled.
I see a Rust post every other day, so I have a bit of idea how its syntax look like. As a c++ programmer, lifetime of objects is all I think about. Moving more of this lifetime dependency to compile time is a very good idea. Yet, syntax just looks horrible to me, especially the single quote thing. Obligatory xkcd: http://xkcd.com/859/
i've written an HTML rust source browser https://github.com/dobkeratops/rustfind , and got some 3d stuff going on android https://github.com/dobkeratops/android_rust_gl . So I probably haven't written as much as you but I don't think I'm exactly a novice. I haven't written more because.. my response to it remains ambiguous. I have more code in C++ to throw away, It has not convinced me its 100% worth doing, and moving away from C++ distances me from 'real production code'. Dealing with android added additional discomfort (fringe language seems a step too far combined with a messy build system on an unusual platform) There's things I still miss from C++; in some ways you have to do more work naming/navigating, and I find that off-putting. This is orthogonal to safety, and a result of other design motivations in rust. I like Julias' approach to organising code, shame its' yet another GC'd language. C++ environments deal with the overloading hazards, C++ overloading IMO leverages the type system to reduce naming work. Between templates &amp; overloading, C++ expresses vector maths types very well, IMO. I've done things in Rust using indirection traits.. I realise this is in flux - but I still prefer the ad-hoc approach. I'm not sure I like the deep namespacing by default (e.g. sometimes the type is sufficient, but you have it namespaces under its filename *aswell*) In the time I've been looking at rust C++ has finally gained polymorphic lambdas which is a nice draw back to it, and there's more advancements promised (modules,concepts) I basically agree with a lot of what Jonathan Blow is saying - some of rusts' core pillars aren't to do with things that aren't our biggest problems, whilst it does get some of the big frustrations of C++ out of the way like headers. Rust also lost a couple of features I originally liked,in the time i've been using it. ~, do 
Rule-of-zero was kind of my point when I said that owning types can be composed. But rule-of-three it's still important to be aware of the rule-of-three *if* you have to go deeper since a C++ compiler still generates default operations that might do the wrong thing.
his point was that Box&lt;T&gt; 'hides the type' in parentheses. ~T makes the type itself as prominent as *T or &amp;T. The sigils 'melt away' for first impression reading; you see more meaningful words and sigils/symbols for the common structure.
yes exactly. you have to change code a lot, and quickly, in response to designers changing ideas, ideas evolving. design is an iterative,evolutionary process, not pre-planned.
Yeah, I agree that the lone single quote is a bit jarring. I've gotten used to it, though. Still, I don't see a major issue. Is this: vector&lt;pair&lt;const char *,const char *&gt;&gt; tokenize_string2(const string &amp;text); really *so* much better than fn tokenize_string3&lt;'a&gt;(text: &amp;'a str) -&gt; Vec&lt;Token&lt;'a&gt;&gt; that you're willing to completely ignore a language because of it? Many cases of explicit lifetime have been rendered unnecessary (87% of those that were in libstd) with the [lifetime elision RFC](https://github.com/aturon/rfcs/blob/7a459c935ad3fe8e4eb1af111c64dd55413906ef/active/0000-lifetime-elision.md) a few months back.
I like this proposal. Reading through it I was wondering why not use macros instead of some of the suggested syntax additions? Of course, I'd like to see the macro system expanded generally and not just in this case. Just like dynamic languages that allow to write arbitrary code everywhere and not just inside functions. This would look like this: struct Parent { x: uint; } struct Child { a: uint; embed!(Parent); } trait Bar { boo: uint; } impl Bar for Parent { bind!(boo, x); } Also, properties. I saw the comment on the RFC pull request but I still remain unconvinced regarding the limitation to not allow mapping getters to a trait field. In higher level code it is more important to support the uniform access syntax than expose lower level cost of a a function call. This can be added in a later RFC as a backwards compatible extension and doesn't have to be part of this RFC but I don't understand why is this ruled out entirely. 
do notation might have been a bit like that, e.g. `do with_open_file("some.file","rw") |f|{ ... do something with an open file 'f' }` but you can still do the same thing, just with more nesting.
 vector&lt;pair&lt;const char *,const char *&gt;&gt; tokenize_string2(const string &amp;text); should be replaced by vector&lt;Token&gt; tokenize_string2(const string &amp;text); for examples to be equal. Also yes, I will simply ignore the language only because of the syntax. 
I've been working with data oriented design for several months and the definitive type I use for vectors in C++ is std::vector. Would you mind explaining how this is somehow magically less data oriented? You still have a structure of arrays, you still have linear access patterns. Assuming you're referring to what sellibitze described, that isn't very data oriented under my understanding. I don't see how that could increase your cache locality, or create any kind of performance boost whatsoever. Cache lines are ~64 bytes long, not several megabytes. In all likeliness, it won't matter whether your arrays are bunched together or not. In fact, by allocating them together like that, it increases the cost of copying the data over should you need to resize the array. CPU cache line preloading works linearly, sure, but if you randomly pull from the other array then it won't look linear. It will look like a random memory access several megabytes away from the array you've been linearly processing. You can have the CPU preload from both arrays at once, but that isn't dependent on them being located spatially near each other. The only possible improvement I can see from this is not having to call malloc() twice.
So you can't handle prime notation in math, either, eg `a` and `a'`?
One thing worth thinking about is whether Rust would be better off with an asynchronous programming model (like NodeJS) or synchronous programming model (like Go). Synchronous I/O is very nice to program with, but it requires you to have a stack for every task, and it might not be possible to make that sufficiently lightweight. Asynchronous I/O is often criticized for resulting in "callback hell", but good languages have many ways of making it just as good as synchronous I/O. In Haskell, I/O is built on monads anyway, so you already essentially have callbacks, and you don't have to change your programming style at all. In C# 5, you can write code that looks synchronous, but if you decorate it with async/await, the compiler will automatically transform it into an asynchronous state machine for you behind the scenes. (See https://mail.mozilla.org/pipermail/rust-dev/2013-November/006577.html for a discussion about this model with respect to Rust.) At some point, both models look quite similar syntactically, and the real question is whether the code runs faster with lightweight threads with their own stacks, or whether it's more efficient to use callbacks and essentially move the stack frames into the heap in the form of closures for the continuations. Also, the way you call asynchronous tasks differs somewhat between the models. With synchronous I/O, you generally spawn threads and then communicate with them using channels or something. With asynchronous I/O, you can have each task encapsulated in its own object, which you can compose with other tasks, etc. I personally prefer the latter model, since it lends itself to a more functional style. That was pretty information-dense -- let me know if there's anything I can clarify.
Ah, it's `unsafe_mut_ref`. Yes.
Nope. http://i.imgur.com/YKASMdz.png
I'll be honest. As a Rust newbie, the whole `vtable: Vtable&lt;Element, HTMLVideoElement&gt;`, looks inferior to C++ syntax. It probably is the best design performance wise, but the syntax really looks cumbersome and hard to do. I get that might be the point, and yes macros exist, but in my humble opinion that is way too much boiler plate. 
Thanks. I'll see if we can use mio as a starting point. Wrapping up epoll/kqueue is definitely the boring part of the project.
It's very hard to have a pretend-synchronous API without garbage collection and split stacks, neither of which is tenable for rust.
Not scalable to more than one core.
Yeah, thats one of the main complexities, which is why I'm also considering a design in the Vtable Alternatives section that basically just amounts to a tiny bit of language magic and putting `#[inline_vtable]` on both structs and traits.
Quicksilver is a library which provides approximate algorithms and sketches for Rust. I had some old code laying around which implemented PCSA (cardinality estimation algo), but it was a year old and no longer compiling. I cleaned it to compile under current nightly, then added HyperLogLog. HLL is a superior algorithm for cardinality estimation. HLL can estimate cardinality of billions of items with minimal memory overhead (E.g. 3.2kb of memory provides estimates that are +/- 2.43% of the true cardinality). HLL works by simple bit twiddling, so the algorithm is very fast. On my Macbook Air, I routinely hit 200m ops/s single-threaded. In the future I would like to continue adding other approximate algos like Bloom filters, Count-Min, Frugal Streaming, Stream Summary, wavelets, etc.
maybe macros could help, some sort of macro for a series of contiguous arrays. a template/generic wouldn't let you name the components. 
No, C++ doesn't complain at compilation time at all, it just segfaults when you get to a certain point in the program
It is a micro-optimization, but it's one that could give you an easy performance win in hot spots of your engine for no real effort. If you had a simple flag that you could just pass to the compiler that that would just get rid of all bounds checks (as well as per-function flags to do it in a more targetted way for apps that care more about security) then you could get a really simple win out of it (and enable other optimizations like vectorization). There's tons of scenarios where you run an algorithm where you bounce around an array for a while and after you've tested it long enough you know it works and you could be able to turn it off. For a browser you'd never do this, but for a client game on a console you'd totally just turn it off all over the place in your shipping build (you have tens of thousands of QA hours to catch these things, and there's no real attack vector). For server binaries maybe you'd leave it on. 
perf works fine. See kcov from the issue you linked.
I haven't thought about it too much, but it *might* be possible to get much of the sugar without actual compiler support by using a macro. Yeah, it's annoying how Linux doesn't have a good async I/O story. From what I can tell, AIO might work for some kinds of file I/O, but it sounds like it silently reverts to synchronous operation for some things, which is rather bad.
Slightly off topic, but does anyone know what the status of debugging under gdb on Windows is? Last I tried (a week or two ago), symbol names were still not correct and line number references weren't working.
Yes, this actually seems to be his most important idea. I know gamedevs care about memory layout a lot due to cache misses, allocator overhead, limited memory on consoles, etc. But could someone in the know explain specifically why in the mesh examples even the three large arrays must be jointly allocated? (But not jointly with other meshes?)
I believe this is a mistake in the design of Rust's macros. Macros should have access to type info. Nemerle for instance allows to write macros that affect different stages of compilation. I believe macros really need to be rethought from scratch after the language stabilizes.
I think taking his point to be purely syntactic, as opposed to partially semantic, is again not the point he was trying to make. It's not that `Box` *visibly* hides the type from being too large, but that `Box` is generic over the indirection instead of the type. The `Box` doesn't care what the type is, so why are we wrapping the type in it? He goes on to argue that this kind of separation is not only visually cleaner, but with it it is easier to make the compiler do certain types of more intelligent operations. For example, imagine if you could do: struct X { HeapAllocation alloc; int box&lt;*, alloc&gt; X double box&lt;[], alloc&gt; Y double box&lt;[12], alloc&gt; Z } vs struct X { heap_allocation alloc; box&lt;* int, alloc&gt; X box&lt;[] double, alloc&gt; Y box&lt;[12] double, alloc&gt; Z } Not only is the first one easier to read, it makes it clear that `box` only affects the storage of the type. The second would be harder to do with a normal type-system due to the intermix of static and runtime evaluation, but in the first the separation makes it much easier for the compiler to help, as the compiler knows far more about what you are trying to do. In the second, the compiler could pass every allocation and dereference attempt to the `box` at compile time, and the `box` can statically compile the behavior of `heap_allocation` to be exactly what would be written when done manually. The dereferences can be used to insure safety and the allocations can make sure `alloc` is sized appropriately. (I assume these compile-time calls would operate on instances, not globally.) 
That's true at the extremes but completely false in moderation. Straight, one-pass iterators are a good example of this; they are extremely generic and work on almost any container. Python has proven that for many cases they are more than adequate to have code that can operate on many different types. Encoding mutability on top of this makes a ton of sense. Having `set` be generic is definitely a good idea, if only because there are a few types of set in common use. The same goes for maps. C++ maybe went a bit far, but it's not a mistake to have tried.
Yeah we'll explore a sugary macro once the basic design is done.
I've written plenty of code of this nature, from things embedded in servers to generic libraries, in a variety of languages. You have some pretty vague requirements that conflict with each other. You might want to pick which battles you actually want to win first, and decide what you're willing to trade off. For starters, as many others have said, take a look at Carl's work on mio. Don't reinvent the wheel. For low latency, you want blocking I/O. Round-tripping through an event loop when a call would block and making two system calls is higher latency than making one and blocking. Some more info on this here: http://www.mailinator.com/tymaPaulMultithreaded.pdf see also https://www.usenix.org/legacy/events/hotos03/tech/full_papers/vonbehren/vonbehren_html/index.html For managing lots and lots of what I'll call "sleepy" connections, you want an event loop. They're great for things like chat servers where you have large numbers of mostly-idle connections, but worse at latency and throughput than blocking I/O, especially if you use system calls like sendfile() that keep I/O out of userspace. You can combine both models into something which sits somewhere in the middle ala Java's Netty: having one thread monitor for I/O events and perform I/O operations, then dispatching those events to a thread pool. In a Netty-like system, you may need to register new IO objects with the monitor thread(s). If these threads are blocked in a system call, they will need to be woken up. A pipe (at least on *IX) is the usual answer to this, however this adds system call overhead. Netty does some "interesting" tricks to allow these registrations to take place in a lock-free/wait-free manner so long as the monitor thread hasn't actually gone into a system call: https://github.com/netty/netty/blob/master/transport/src/main/java/io/netty/channel/nio/NioEventLoop.java#L309 In general, try to make everything you can as lock-free/wait-free as possible. Don't make the same mistakes as Java NIO and clog the whole mess up with locks.
The experimental desynces a lot
Out of curiosity, what do people use the library for?
I've just bumped into Rust via this talk, does Rust provide good concurrency abstraction across the CPU and GPU? As I think the ideal game programming language would allow for concurrency across the CPU and GPU as well as the ability to write shaders in the same language.
Having collections that are consistent with each other is extremely important for language usability, at least for me. Most languages do a good job here, but there are standouts both positive and negative. Erlang is unusable for me because of this, even though it has lots of other unique and great things about it (and sadly nobody in the community has written their own collections library that I could find). I've also always been particularly fond of the c++ standard library for its collections and algorithms. Clojure is another example of a well designed collection library. Scala takes the decent java collections and without straying too far, adds functional programming and immutable collections on top to become awesome. It's good to see rust putting in effort here before 1.0, and I'm interested in trying the language soon.
All esports competitions I've seen that offer large cash prizes are LAN events. Much lower risk.
Out of curiosity, which version are you using? The oxidize step was renamed to better reflect that's it's just running rustc a while ago, I believe. Any ways, it's well-known that small changes can take a long time to build, but it helps if you use a non-optimized build AFAIK.
badamson@406c8f2a3866 ~/g/r/s/libcollections&gt; rustc --version rustc 0.12.0-pre (5d335c94b 2014-09-20 08:05:35 +0000)
Question, would the non-optimized flags be passed to make?
The compiler is bootstrapped with itself and compiles 3 times: once with a snapshot (stage0), then with the result of that (stage1), and then the librustc library is created with the result of stage1 (stage2). The librustc library is the largest crate in the distribution, and depends on most of the others, and thus is definitely the slowest. The makefiles default to doing the full bootstrap, and are forced to assume any change need to start the build at stage0 (or else the change won't necessarily be reflected in the final output properly). There's a few tricks around this: - The easiest shortcut to get a `rustc` for testing is `make rustc-stage1` which just builds the minimum useful `rustc` and standard libraries (just compiling librustc once); the resulting rust is in `$BUILD_DIR/x86_64-apple-darwin/stage1/bin` and you may need to add `.../stage1/lib` to your dynamic linker path (whatever the equivalent of `LD_LIBRARY_PATH` is on OSX). - You can also invoke `make` with an argument `RUSTFLAGS='-Z no-opt'` to avoid spending the time on optimisation, but this causes the resulting `rustc` to be much larger and slower (i.e. the full bootstrap is definitely slower overall, and maybe even `rustc-stage1` is slower). - Another choice is `RUSTFLAGS='-C codegen-units=4'` which parallelises the optimisation step across 4 threads, trading off some final performance (not nearly as much as `no-opt` though) for a faster bootstrap.
&gt; This is not the case for many applications such as Web browsers, where use-after-free still appears again and again and again. I suspect most games are full of use-after-free, and it doesn't matter for them because nobody is trying to break them. Are they being developed using the various modern practices such as sanitizers and lots of testing? Firefox is an old codebase and Chrome was originally based on KHTML. In any case browsers are fairly specialized, they put a much higher emphasis on safety than games since they are a shared common attack target against millions of people.
I have started on exactly that. I have been toying with building a macro to POC the construction of the await state machine. I think I need to go full-fledged syntax plugin, however. I plan to use Mio and reem's event library. It is towards the end of a rather large roadmap, so expect something around the beginning of Oct. 
The video recordings of this year's Strange Loop talks are being posted on this channel: https://www.youtube.com/channel/UC_QIfHvN9auy2CoOdSfMWDw I don't see Chris's talk there yet, but it ought to be there within a few days.
The purpose of grouping allocations isn't really to improve cache hits, it's to reduce the number of calls to the allocator (since this is usually expensive) and reduce the amount of fragmentation (e.g. due to allocation header words, or rounding up the allocation size or whatever).
I don't think a lot of work has been done in that area. GPU APIs are notoriously unsafe which makes things challenging. Once lower level APIs become more commonplace though - like Mantle - we would have more leeway to create safe abstractions without sacrificing performance so much.
First of all, I am *not* a lawyer. This means that, for anything like this, where I haven't been told by a lawyer what the answer is, I assume the worst situation that seems plausible. I'd never considered the ramifications of this, but I would assume that it has the same properties as C++ templates. For example, I would assume that you cannot have dynamically-linked LGPL libraries in Rust that use generics, since it is impossible to substitute a replacement for the library at runtime. --- I did a little digging. You *can* do it with LGPL3, but you need to explicitly allow it (through the clause in section 3). There are also alternative licenses that are more amenable to this sort of thing; for example, the Eigen library moved from LGPL3 to MPL2 (presumably to simplify things0.
libgit2 uses it (not the rust bindings) to implement an SSH transport layer for updating git repositories. I've also used it to script controlling other servers in the past as opposed to driving the `ssh` command line itself. I had to get it building as a dependency for libgit2, and I figured I may as well write some interesting rust bindings while I was at it!
I don't know if this is the right place, but I happened to see this talk and there was a part of it I was slightly concerned with. Sure, static typing is powerful and can potentially eliminate lots of types of errors including security ones. But the message Chris was trying to deliver was that Rust could eliminate _all_ security issues. The audience was understandably incredulous at this claim and offered plenty of counter points. From talking to people afterwards a lot of people were turned away from Rust because of this. I think we need to be careful about overstating these kinds of things or risk alienating a wider audience.
Ah, I've been using 64-bit ever since support was merged, hadn't even thought to check 32-bit.
If a struct field has the same name as an associated trait field, then it could be nice to bind them by default. (Unless overridden) trait Foo { a: uint } struct MyStruct { a: uint } impl Foo for MyStruct { // a is bound to self.a by default } EDIT: I hope that the name collisions can somehow be handled.
My main beef with Rust for game development is the lacking C++ interoperability. The inability to interface with major C++ powerhouses like Bullet Physics (see limitations of https://github.com/bjz/bullet-rs for instance) is a deal-breaker.
Let me clarify myself - I was not making a ""rust y u no full of magic" comment but rather I was thinking about a concrete design (I just forgot to mention it in the post...). I'm talking about a language that's called "Nemerle" which is also based on functional concepts from ML like Rust. Nemerle provides powerful macros with different stages of compilation and it does give you access to type info. Another Thing I dislike about Rust's macros is the required ! in the invocation (compared to Nemerle where it looks exactly like a regular function call). 
No return value for write()?
The [underlying functions](https://github.com/kmcallister/syscall.rs/blob/8385f4c9c6ad4d5192ae7e04aefd3b92e1582f8e/src/platform/linux-x86_64/mod.rs) (and the macro) do return the kernel's response, it is just being ignored (the trailing `;`) here.
The issue people have is not having a short macro to print stuff, it's reusing the `println!` macros in a way that can lead to surprising behaviour (e.g. a beginner trying `let format_string = "foo {} baz"; println!(format_string, "bar");` will (silently) get something unexpected, today it is just an error). Using a completely different macro also allows fancier behaviour, in particular [a macro that prints a representation of the expression as well as its result](https://github.com/rust-lang/rust/issues/12015#issuecomment-33977445) (reproduced/updated here): macro_rules! dump { ($($a:expr),*) =&gt; { println!(concat!($(stringify!($a), " = {}, "),*), $($a),*); } } [playpen][pp] [pp]: http://play.rust-lang.org/?run=1&amp;code=%23!%5Bfeature%28macro_rules%29%5D%0Amacro_rules!%20dump%20{%0A%20%20%20%20%28%24%28%24a%3Aexpr%29%2C*%29%20%3D%3E%20{%0A%20%20%20%20%20%20%20%20println!%28concat!%28%24%28stringify!%28%24a%29%2C%20%22%20%3D%20{}%2C%20%22%29%2C*%29%2C%20%24%28%24a%29%2C*%29%3B%0A%20%20%20%20}%0A}%0A%0Afn%20main%28%29%20{%0A%20%20%20%20let%20x%20%3D%201u%3B%0A%20%20%20%20let%20y%20%3D%202%3B%0A%20%20%20%20dump!%28x%2C%20y%2C%20x%20%2B%20y%29%3B%0A} 
I am taking a look at it, thanks for sharing!
Here is a github issue. I cannot tell you how close they are. https://github.com/rust-lang/rust/issues/16091
Ok, I understand the concern now, thanks for explaining. A different macro will make this a bit more tedious to work with, but nonetheless is beneficial and I at least understand the concerns properly now. Thanks for explaining (and for the macro). 
Well, even though the Rule of Zero only surfaced with C++11, especially as it is backed up (in part) by the language Standard, its principle can be retro-actively applied to C++03. Most people did not use it in C++03, which is unfortunate, but it is more a matter of ignorance than refusal.
Well, in C++11 *some* of those operations are disabled by default; but once again C++ is held back in the name of backward compatibility... The disabled operations: - if a class has a user-declared copy-constructor, copy-assignment operator or destructor, then the move-constructor and move-assignment operator generations are disabled - if a class has a user-declared move-constructor or move-assignment operator, then the copy-constructor, copy-assignment operator, move-constructor, move-assignment operator and destructor generations are disabled This already helps support safe programming, although I wish it had been backported to C++03 classes.
Yeah, I know. But unfortunately, last time I checked, modern compiler didn't even warn about potential rule-of-three issues unlike what has been suggested in the C++ standard proposals about implicitly generating special member functions. The rules they introduced in C++11 to make it backwards compatible to C++03 are already marked as deprecated which kind of made me expect to see compiler warnings in those cases.
&gt; Do alternative debuggers exist. There is also [lldb](http://lldb.llvm.org/) which works better than gdb on Mac OSX but isn't very stable on other platforms. So if you are using Mac OS, I'd recommend using lldb. Newer versions of LLDB work significantly better than older ones. What doesn't work at all so far is calling functions/methods from the debugger (any debugger). Regarding other tools: rustc generates pretty good DWARF debuginfo via LLVM. If a tool can use that (i.e. profilers) that should work. The story is a different one Windows though.
Sadly, debuginfo on Windows is largely untested and doesn't have reliable support yet. But it's definitely on the agenda for the next months. 
This library alone is not quite enough to write a program without any dependencies. **A)** If you want to use `argc` and `argv` you will have to write some assembly code: #[no_mangle] #[no_split_stack] // start function for x86_64 linux pub unsafe extern fn _start() { asm!("pop %rdi"); asm!("mov %rsp,%rsi"); asm!("push %rdi"); asm!("call main"); asm!("mov %rax,%rdi"); asm!("mov $$60,%rax"); asm!("syscall"); } #[no_mangle] #[no_split_stack] // pub because rustc might think that it's unused otherwise pub unsafe extern fn main(argc: i32, argv: *const *const u8) -&gt; i32 { // ... } **B)** If you don't add the `#[no_split_stack]` attribute to all of your functions, you have to write some more assembly in the function above or the stack overflow check will crash your program. This is normally done by glibc on linux. See also [this](https://github.com/rust-lang/rust/pull/17037). **C)** Only very small programs only need syscalls that don't take any pointers to structures. If you want to do any kind of network programming, you have to use the `bind` syscall which looks like this: int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen); The layout of `sockaddr` and the type of `socklen_t` depend not only on the operating system but also on the architecture. You will have to wrap them yourself. **D)** Syscalls that exist on one OS don't have to exist on another one, can have a different name, and can work completely differently. What you want is something that abstracts these things away and gives you a common interface so that you can write portable programs, that is, POSIX.
For your first example, you can actually write everything in one string: asm!(" pop %rdi mov %rsp,%rsi push %rdi call main mov %rax, %rdi mov $$60, %rax syscall "); (Theoretically one could use the `syscall!(...)` macro for the last part, not that it adds that much value in this case.)
Whoops, I had no idea these existed. Nice work :) What kind of stuff are you using Rust for at OpenDNS?
The obvious use case would be to interact with the OS directly without any dependency on a c library ('pure rust' libraries for hardware drivers, say).
This is basically why I gave up on trying to contribute code... on a netbook-class CPU a bootstrap and running the testsuite takes on the order of hours. Sucks all of the joy out of it. (Hopefully you have more powerful hardware though, judging by the target spec!)
Yes. In fact, I wrote a benchmarking tool in Rust a while ago that used syscalls instead of libc.
&gt; Rust already gets lots of flack for looking bad, adding this for something as "simple" as inheritance will only add to that, and that will harm the language in the long run. The thing is, those parts of the proposal *aren't essential for inheritance at all* - not even close. Instead, they just provide some machinery (which is highly advanced and fiddly by nature) which allows the programmer to (1) elect to switch off some of the flexibility of multiple inheritance, as a performance optimization, and (2) if the prior optimization is enabled, more precisely specify the layout of their structs, as a further performance optimization. This won't be remotely necessary for 99%+ of object-oriented Rust projects. It's a hack which allows idiomatic Rust to impeccably match the performance of idiomatic C++ in those very, very few cases where it really matters. Your last paragraph is quite a good point... but at the same time, given the long history of newbie programmers jumping to all of the wrong conclusions when they're first exposed to OOP, I think it might be prudent to tread on the side of caution. The last thing we need is for a "fixed layout is faster, so you should always use it!" meme to pop up.
I don't have a link for his Strange Loop talk, but Chris will be talking this Tuesday at the [SF Bay Area Rust meetup](http://www.meetup.com/Rust-Bay-Area/events/194567252/), which will be live streamed from this [link](https://air.mozilla.org/bay-area-rust-meetup-september-2014/).
Ah yes, I am still disappointed about that too :/ But then I was quite disappointed about C++11 in general; getting new things (move semantics!) was cool, leaving most if not all of the existing issues untouched in the name of backward compatibility, less so.
&gt; Are they being developed using the various modern practices such as sanitizers and lots of testing? Yes, the new parts of Firefox are written in modern C++, we use ASan/Valgrind, and we still find lots of use-after-free. It's unavoidable in C++.
 uint32_t basic_cpuid[MAX_CPUID_LEVEL][4]; is basic_cpuid: [[uint32_t, ..4u], ..MAX_CPUID_LEVEL], or basic_cpuid: [[u32, ..4u], ..MAX_CPUID_LEVEL],
Can we get a version of this macro in the language? Bonus points if it is cleaned up a little and can differentiate between debug and release builds.
Whoops! I can't read C apparently... Fixed, thanks!
I did some work on an async Rust IO library with support for timers, sockets and channels half a year ago. Code here: https://github.com/Matthias247/revbio Feel free to look around and take what you need. I felt too limited by the language (no suitable and sane way for real I/O completion callbacks) and then stopped working on it. 
Can Rust's dynamically sized types and borrow checking be used to reduce the allocations like he talks about around the 1:05 mark? I.e. if you create a struct that contains an array of positions, one of indices, and one of UVs as plain old arrays (DSTs), can the compiler know that they all have the same lifetime and create a single allocation for all three?
&gt; Are there plans of adding this kind of syntax (or anything else) to ease the returning of iterators? This has been discussed before, see [RFC #105](https://github.com/rust-lang/rfcs/pull/105) ([rendered](https://github.com/aturon/rfcs/blob/62fe495d35f279a24eaef9b61dbe491145c2fb82/0000-abstract-return-types.md)).
This has irked me too. If you're willing to use trait objects, then you can [try this technique](http://stackoverflow.com/questions/25436356/how-to-loop-over-boxed-iterator/25436951#25436951). I use it in [quickcheck](https://github.com/BurntSushi/quickcheck/blob/master/src/arbitrary.rs#L47-L67) because it's the only sane way I could come up with to write instances of `Arbitrary`. The extra trait could be avoided if `Box&lt;Iterator+'static&gt;` implemented `Iterator`. (I'm not necessarily advocating that it should. If the Rust devs are planning a different solution, then it makes sense not to encourage this.)
This is something I whipped up after seeing the link to Jonathan Blow's "Ideas about a new programming languages for games" and [a linked HN comment](https://news.ycombinator.com/item?id=8040577). Essentially, it reduces the friction when all you want is a bunch of public functions / public fields in a struct. Edit: also worth noting: I find it really impressive that you can make a syntax extension that does something useful in less than 100 lines of code (75 not counting whitespace).
So one could implement a libc in Rust using this? Could one gain anything from doing that?
I realise that these aren't essential for inheritance. But that's not how it would look to someone outside the Rust community - after all, it's in the name of the RFC.
I look forward to `impl Iterator&lt;T&gt;`. That would probably solve things for you.
What was the result of that discussion? I can't actually find where / why it was closed.
Some other alternatives you can use right now: Write a macro: macro_rules! odds(() =&gt; (count(1u, 1).filter(|i| i % 2 != 0)) ) Macro's can be used as a function that has no explicit return type. In that way you never have to figure out the return type, it's just left to Rust to deal with. As a bonus, you can have different *types* of iterators yielded contextually, which is another weakness of the current Iterator system. Macros are harder to share between crates though, so while this isn't a great solution for public APIs, it's pretty good for private ones. Use a type alias: pub type Odds = Filter&lt;'static, uint, Counter&lt;uint&gt;&gt;; fn odds() -&gt; Odds { count(1u, 1).filter(|i| i % 2 != 0) } *You* still have to figure out the type, but the API stays nice and friendly to end users. It also permits you to *change* the type to something more complex without users having to change their code. In contrast to macros, this is a great solution for public APIs, and a weak solution for private ones.
Now up @ https://www.youtube.com/watch?v=jVoFws7rp88
The proposed RFC was [closed pending further revision](https://github.com/rust-lang/rfcs/pull/105#issuecomment-45357561), so it looks like it will back in an evolved form in the future.
[rust-ci.org](http://rust-ci.org/) will host rustdoc-generated documentation for you, the steps for autoupload are listed if you add a project.
Read the docs isn't Python specific, it's based on Sphinx which is intended to be a generic documentation system. That said, support for including automatically generated documentation from languages other than Python is pretty limited. There's a domain system for including documentation for other languages: http://sphinx-doc.org/domains.html. There's also Breathe which provides a way to include Doxygen generated documentation in Sphinx: http://breathe.readthedocs.org/en/latest/. It should be possible to include rustdoc based documentation by first generating json from rustdoc and creating a Sphinx extension to display this data.
This is based on an early idea for inheritance that never got an RFC. Essentially, a trait can inherit from a struct to specify that its implementors must begin with that struct's fields. It uses `match` for downcasting on trait objects in general, and uses RFC PR #9 (fat objects) for thin pointers.
The slides are also available at http://chrismorgan.info/media/misc/fast-secure-safe-the-web-that-can-still-be.svg.
For the lazy, here's the linked article from the SQL slide. http://www.more-magic.net/posts/structurally-fixing-injection-bugs.html
Here are a few helpful links: https://github.com/rsdn/nemerle/wiki/Macros-tutorial https://github.com/rsdn/nemerle/wiki/Nemerle-macros-intro Here's a simple example from the above link: macro for (init, cond, change, body) { &lt;[ $init; def loop () : void { if ($cond) { $body; $change; loop() } else () }; loop () ]&gt; } Here's the usage: for (mutable i = 0, i &lt; 10, i++, printf ("%d", i)) 
if we're finding a need to write macros for this purpose, that's a pretty strong argument that rust should infer most-general function types, haskell style
He also brings up multiple return types and not having exceptions.
I wonder how it will cope with associated types, something like `-&gt; impl Iterator where Iterator::A = int` ? Associated types don't look awesome for this particular case.
&gt; vector&lt;Vector3&gt; vertices; If you do this (using STL) anywhere I will come and shoot you. Primarily because the damn thing is nearly impossible to customize the allocators for.
tl;dw?
The Cap'n proto serialization suite exposes RPCs, and is all about sandstorm, the potentially awesome user-centric server application server. Cap'n proto rust codegen is pretty good. You already know rust.
This is also very important if we ever want to be able to return closures from a function
Or that we could investigate other approaches, e.g. [#105](https://github.com/rust-lang/rfcs/pull/105).
yeah but whats the better boost to society overall: (i)cheaper game-dev cycle, more productive creators (85% or 99% good enough , not 100%, that last 1% usually taking more time..) (ii)ability to pay professional players to spectate (as opposed to just playing against friends more light-heartedly) I think the former, we get more real content. We still get professional players on LANs where security can be tighter, if you're totally desperate for that. whats basically going on here is making programming harder because of the threat of hacking (look how it discourages Jon Blow from using Rust),... but if you make something not worth hacking you defuse that threat, and both programmer &amp; hacker are freed up to do something more directly constructive and everyone wins 
I'm not too sure, but is there really a need for `Chunk` to hold references to the `Perlin` instances? Would it not be simpler to just pass references of the `Perlin` instances (or a reference to a struct that contains the `Perlin` instances) by argument when calling `chunk.regenerate()` ?
I really only chose the name to have something to differentiate it from the other 5+ proposals in less than 4 words :P
Why would Iterator use an associated type instead of a type param?
I'd assume it depends on the lifetime of the instances from a modelling perspective. If they could vary over time and `regenerate()` works on any instance, this makes sense. However if there should only ever be 1 Perlin instance for the lifetime of a chunk, associated with the chunk, then it does not. Whether or not this is the case, it would solve the OPs original problem. However I'd be interested in hearing how to go about doing things if you did in fact need to keep a borrowed reference for the entire lifetime of the struct.
&gt; It should be possible to include rustdoc based documentation by first generating json from rustdoc and creating a Sphinx extension to display this data. If rustdoc is usable as a library, it might even be possible to build a sphinx extension calling rustdoc via FFI and converting the data to reStructuredText, without having to go through a JSON intermediate serialization.
Also, saying “bugs of type X simply can not exist” sounds very magical and hand-wavy. “Such bugs are detected and rejected at compile-time” might be better.
I've definitely encountered the same problem. 1. I want to return iterators, because they're idiomatic, they give the caller lots of choices, and they perform well. 2. But if I do want to return an iterator, I spend 1 minute writing the code and 5 minutes trying to figure out the type. This is especially bad when building iterators from `map`, `filter`, etc. For example, the most frustrating piece of this code by far was getting the return type right: static TOKENIZER_REGEX: Regex = regex!(r"(\w+['’]?)|(\W+)"); /// An iterator which iterates over the `Token`s produced by `tokenize`. /// Figuring out this type declaration was rather difficult. pub type Tokenize&lt;'t&gt; = Map&lt;'t, Captures&lt;'t&gt;, Token&lt;'t&gt;, FindCaptures&lt;'static, 't&gt;&gt;; pub fn tokenize&lt;'t&gt;(text: &amp;'t str) -&gt; Tokenize&lt;'t&gt; { TOKENIZER_REGEX.captures_iter(text).map(|cap| { if cap.pos(1).is_some() { Word(cap.at(1)) } else { Other(cap.at(2)) } }) } I would love `impl Iterator&lt;T&gt;`.
I think the short answer is: * no because it makes the language more confronting to newcomers * no because it forces allocator decisions into the core language * no because it favors box allocation compared to Arc, Rc, Gc et al * no because it forces different semantics for box allocation compared to Arc, Rc, Gc et al. There's a huge discussion in the comments here: https://github.com/rust-lang/rfcs/pull/59
I totally agree that newtype should be seriously considered, especially if you're exposing the internals of an implementation. However typedefs are great for "throwaway" iterators like the Values and Keys helper iterators on maps. 
Then are there any plans on replacing '&amp;'?
An input/output type parameter distinction. I believe it's detailed in the Motivation section of [the Associated Items RFC](https://github.com/rust-lang/rfcs/blob/8490e0e9778e2e224b3fc955570d824856a7423d/active/0059-associated-items.md).
I used `vector&lt;complex&lt;double&gt;&gt;` with a custom allocator that calls the allocation functions of the FFTW library for SSE-aligned memory. Sometimes it's the right thing to do. And sometimes it's not. But for some reason I tend to manage to come up with `std::`container-based data structures without it being inefficient w.r.t. memory layout. &lt;sarcasm&gt;Unbelievable!&lt;/sarcasm&gt; There is no reason to unconditionally shoot people based on this especially if Jon B holds back with his "memory optimization" until 1:10:00 in the talk and looks at examples like these struct Mesh { int num_positions; Vector3 !* positions; int num_indices; int !* indices; }; before that. So, *of course* I'm suggesting `std::vector` in such cases because `T*!` without any `@joint` isn't really better! I'd say it's worse because you have to define and deal with the length separately. Jonathan Blow could have saved himself a *lot* of negative feedback by presenting "*good* C++" code somewhere in the first 70 minutes instead of presenting code that *looks* like he does not know how to use C++ effectively. Being somewhat successful in the game industry does not make you immune to people doubting your C++ skills. Rightfully so. We all know it's possible to come up with cool programs and games if you stay in the C subset. But if he complains about having to do lots of things that remind him of filling out tax forms, then *maybe* you don't know what "effective" use of C++ looks like. *Maybe*. And unfortunately, this is the only impression I get after 70 minutes in. It gets better after that. Another thing that bothered me -- I first thought it was a typo -- he writes `unique_ptr&lt;T&gt;*` everywhere. I would shoot you if you ever wrote that nonsense. He does not seem to notice. But what he "wanted" to write is `unique_ptr&lt;T[]&gt;` instead. Note the missing asterisk at the end. I still can't help but think he overestimates his knowledge about effective C++ use.
&gt; Ah, so the problem is not just the narrow one the ability to use a different allocator for std::vector specifically (as I interpreted originally), but rather a pile of extra design badness around C++ allocators? Whatever the C++ allocator design is, it does not solve a real problem. &gt; Hm, this seems sensible to me; I don't see any fundamental reason why the allocator used by the vector should be same as that used by the elements. If they have any allocation internally, they can expose customising that allocator too. What most people want is: I have a 4MB slab of memory here, subsystem X, please allocate all your crap into that one. If you run over that space, there is 2MB of extra space but warn me so i can fix/tweak. If I shutdown the system I free up the whole block and be gone.
Just for the record: I would pretty much shoot Blow for most of his suggestions since he pretty much dismissed Rust due to his stance on "no big picture approaches".
I, too, think he is too presimistic about Rust. But to be fair, I was kind of worrying about the same thing. Freezing and alias-free mutable references seem very restricting at first and made me wonder whether I could really express all the things I wanted to do without using too much `unsafe` everywhere.
I have to agree with you. Allocators being forced to be something stateless seems like a bad idea. But I think you are exaggerating the variance across different STL implementations. I know `std::string` implementations differ a great deal (SSO versus COW). But is there more apart from smaller differences in `sizeof`?
/me sighs~ And here's the thing the ffi guide shouldn't do: #[link(name = "snappy")] extern { ... } because: ...which will compile if snappy is installed: Which is isn't. So it won't. ...unless you're using a unix system, in which case it might be an apt-get / rpm install away. Or a mac, in which case maybe brew or fink or port might install the right version, or on windows, where you might be able to hunt down a DLL and drop it into your system folder if you can find one which is pre built. Also, don't worry because if you try moving that binary to another system it won't work either unless the other system also happens to have exactly the same version of the same library installed in the same place (and trust me, if you're using a mac, being installed in the same place really is an issue). Don't get me wrong, the libcpuid binding is really cool! ...but it makes me a bit :| to see so many c bindings out there that just assume that the library is installed, and there's no support in cargo for testing that. Welcome to good old C style 'but it compiles on my machine... good luck'. I'd argue that using DynamicLibrary (for example like this -&gt; https://github.com/shadowmint/rust-dl-example/blob/master/src/bin.rs) or manually building a dependency and linking to it statically (like this -&gt; https://github.com/shadowmint/rust-watch/blob/master/Makefile) is what we should be encouraging c bindings to do. The ffi interface *is* really great, but the guide makes seem as though it's a nice solved problem, when it really isn't. Repeatable cross platform builds are important darn it! :) (...at least to me...)
&gt; But I think you are exaggerating the variance across different STL implementations. The question is completely irrelevant. Nobody uses STL in games because when you have two years to ship a title and your most widespread library is platform dependent and different for every single target, you replace it with something that's the same everywhere. There might be some game studios that use the STL but right now I could not point you to one that does.
Interesting. So does this bring any advantage or is it just a proof of concept?
It might be nice, from a syntax point of view, if `Box&lt;SomeType&gt;` were lighter/focused more on `SomeType`, without cramming more things into the language. Not sure how that would be accomplished, but `~SomeType` is at least subjectively nice that way. :P
Okay, after rereading this again, this seems like something that ["non-lexical lifetimes"](https://www.reddit.com/r/rust/comments/2ecg69/work_starting_on_nonlexical_lifetimes/) (or something mention in that thread) might make possible. 
Ah, but that's where the misinterpretation comes in. With Haskell's type system, it's often possible to get a lot of functionality checking in the type system. Of course the quote that we always use is "if it compiles, is *usually* works," but somehow that's never the quote people attack. But even so, it still parallels the current Rust situation, doesn't it? Someone making a claim that the language can do more than it can in reality will cause public perception problems down the track.
This might be possible with the recent changes to the syntax extension API that allow macro expanders to be stateful objects instead of just functions, but since I don't think macro expansion order is well defined, there may be the possibility of the macro invocation for Foo being evaluated before the invocations on Bar, Baz, and Qux, which would fail due to the internal state not yet containing the results of the other macros. I'm not sure how you would be able to get this to reliably work in the general case.
Well, by doing that you certainly have more control. I'm not surprized that this industry tends to hang on to their own implementations of similar things.
Yes, you are right, it does not end the borrow properly. You could probably mediate this by introducing a new scope with somthing like `{app.load()}`. Or just let `load()` take you app by value and do it like this `let mut app = App::new().load();`. Then it does not get borrowed in the first place.
Thanks Chris! It's nice to see you live ;) I wonder about the reference you made close to the beginning, something about another talk devoted to Rust. Do you have a link to this one as well?
I'm sort of sad the [Ocsigen project](http://ocsigen.org/) is not mentioned. It's like Ur, but integrated in OCaml as a library+syntax extension, and with client programming too. Most of the precepts could be used almost directly in Rust (but by avoiding some of the early mistakes).
Just a note that if you want to quickly determine the type of an expression, you can do let () = { /* my expression here */ } The type will be spat out by the compiler error. e.g. fn main() { let mut foo = [1u, 2, 3, 4]; let () = foo.as_mut_slice().iter().map(|x| x+1).filter(|x| x%2 == 0).take(1); } // &lt;anon&gt;:4:9: 4:11 error: mismatched types: expected `core::iter::Take&lt;core::iter::Filter&lt;'_,uint,core::iter::Map&lt;'_,&amp;uint,uint,core::slice::Items&lt;'_,uint&gt;&gt;&gt;&gt;`, found `()` (expected struct core::iter::Take, found ()) // &lt;anon&gt;:4 let () = foo.as_mut_slice().iter().map(|x| x+1).filter(|x| x%2 == 0).take(1); Edit: of course lifetimes are another problem, although most of them will be 'static of the lifetime of the original source.
You're right of course, but I admit coming from Java, the fact address-of (basically a poorly-understood concept, coming from interpreted or mixed languages) is conflated with borrowing. And there are several types of borrowing too. Rust is a great language, but it's a bit on a divide where the docs and the community mix language between semantics (I mean, intention) and implementation (i.e. details). Maybe they are both important, and maybe they are important at different levels to different people, but they are confusing to newbies. 
I really enjoyed the talk. No talk is perfect, but you nailed quite a few points very well.
Hi Nikita, nickels author here! Just wanted to let you know that I think it's super cool what you've built and that you used nickel for that. I've been planning to put more love into Nickels website and it would be cool to point people to paste-rs as a real world example if you don't mind.
I'm interested, what several types of borrowing are you talking about? In my mental model borrowing is barely a separate concept and it's not *conflated* with pointers (addresses), it *is* about pointers and their aliasing. So far I managed to live with this viewpoint quite happily.
This is basically what jonathan blow says too; he suggested SomeType *! for his hypothetical language over the unique_ptr&lt;SomeType&gt; of C++11. maybe it would be possible to have a general purpose system as it is now (the virtue being extendability and versatility) but still have more convenient syntactic shortcuts for the most common types ...(still mapping onto generic types, much like there are traits for operators) e.g. ?T == Option&lt;T&gt;.. etc. I did like rusts original syntax where you had ~T= Box&lt;T&gt; and ~[T]=Vec&lt;T&gt;, and would have similarly liked @T for refcounting or gc. Then again [T] being repurposed for slices is also good thing. I wonder if something like operator overloading could be done to completely generalise it, given rusts' syntax more clearly separates types and expressions. 
Right now, proof of concept. It allows a completely freestanding Rust implementation, and even a complete userspace written in Rust. However, if C compatibility were dropped, or at least minimized, it might be possible to write a safer libc for Rust, with lifetimes, etc.
No plans, but there is an RFC - https://github.com/rust-lang/rfcs/pull/248. TBH, I'm not even sure if it is a good idea, but it does have some advantages.
Cool. Is there any other language (except assembler) that has no hard libc dependency? I guess under Windows programs have often only a Win32 API dependency and don't use libc stuff, but when ever is the whole userspace written in something other than C or assembler?
i also miss it. recursive data structures aren’t exactly uncommon.
They do seem to be pretty uncommon, especially when not wrapped in Vecs, Rcs, etc. Many current Rust repositories don't even have a single use of Box.
I figure the obvious mechanism for the reintroduction of `~` and similar is a general provision for user-defined sigils as sugar for boxing operations. So, you'd write use '@' = GC; and magically rustc would know to desugar `@foo` into `box(GC) foo` and `@T` into whatever type `box(GC) foo` returned (and maybe even enable pattern matching, though of course `box` is already generic enough for that). It might also work out for the handful of ASCII line noise that's still unambiguous if used in unary-operator-like position (maybe `#`, `%`, `\`, who knows what else), and of course there's always room for shenanigans like the `☃` sigil for frozen data or mathy notation or whatever in "literate"-style programs that are read wildly more often than edited. I think this addresses your concerns since sigils would have to be declared locally to be in scope and no standard library type or allocation scheme is favored (unless we put sigils into the prelude, which seems orthogonal). I don't really expect anyone to subscribe to this since everybody hated sigils and Rust getting rid of them was always received well on HN or wherever, but the prospect of that feature being a thing always makes me gnash my teeth when someone proposes using up another possible sigil character for, say, a new macro syntax or whatever. ;)
Defining recursive data structures is different than writing them, which is when you'd have to type out box.
some shortcuts might help.. OptBox&lt;T&gt; = Option&lt;Box&lt;T&gt;&gt; // nullable pointer VecBox&lt;T&gt; = Vec&lt;Box&lt;T&gt;&gt; // vector of owned pointers a few characters but seems to matter more because of the nesting. Thats why ~ was nice, IMO. it composed very well. ~[~T] for vec-of-owned-pointers, Option&lt;~T&gt; for a nullable pointer.
In which cases should I **not** use Cap'n proto?
Cap'n Proto's arena-style allocation means that it works best as a write-once format. It might not be the best choice if you need to do a lot of in-place mutation of data. For example, when you resize a list or a string you leave behind a chunk of unused memory. However, such unused chunks are zeroed out and will be deflated if you use Cap'n Proto's custom compression scheme, and you can always manually "collect" any such wasted memory by copying into a fresh message.
Do you think Cap'n proto could be used for real time games? As far as I can see it manages serialization and networking. Do I have control of how I send the data? For example think of a game were I update the position of some object, but I am only interesting in the latest position message. TCP enforces messages in order afaik which would not be ideal. But sometimes I need something reliable maybe if I buy something ingame I want to make sure that the message will arrive. Is this possible or did I misunderstand the purpose of Cap'n proto?
&gt; the allocation of the objects contained in the vector are performed by the classe's[sic] new operator, not by the allocator defined on the collection. The allocation of the memory used by std::vector is done through the allocator and no other memory allocation is performed by vector. The initialization of an instance in the vector is done through the class's new operator, yes, via [placement new](http://www.parashift.com/c++-faq/placement-new.html), which does not allocate memory, but essentially calls the constructor for the class using preallocated memory the vector received from the allocator as the address for the instance. EASTL's vector works the same way. &gt; Aside from that, you cannot trust the STL at all because depending on which platform you target the behavior of the thing is completely different. It cannot be completely different or it will not conform to the standard, in which case you have bigger problems. Usually the complaints game programmers have are: there is no guarantee that vector won't pre-reserve space on construction (annoying if you are going to immediately throw it away), and, pre-C++11 allocators were hard to customize in ways suitable for game programming. C++11 improved allocators, for example, by allowing stateful allocators. EASTL was written pre-C++11 and at least some of the motivations for writing it have since been fixed.
&gt; Is this possible or did I misunderstand the purpose of Cap'n proto? Yes and no. I haven't used Cap'n Proto so take the following with a grain of salt. Cap'n Proto is designed to provide no-translation binary serialization of data. That is, there is no intermediary step when it comes to moving data across the wire. It allows you to define your data structures in a way that they can be fed through the Cap'n API for writing to any stream, whether TCP, UDP, or other. Above that, there appears to be an optional mechanism RPC handling. Since Cap'n Proto would often be utilized in applications where low overhead is a concern, having an integrated solution for RPCs is a major feature. In my naive opinion, I think Cap'n Proto would be ideal for real-time games and applications. Writing custom serialization protocols is often the chosen path for professional game development studios, but such an enormous cost is likely obviated by Cap'n Proto.
You have control. In capnproto-rust, you read from any std::io::Reader and write to any std::io::Writer.
Will it be filmed?
Actually, now that I think about it, there are problems using typedefs there as well. The documentation page a typedef gives you no indication of what methods it actually implements: http://doc.rust-lang.org/std/collections/hashmap/type.Values.html
How about when `T` is unsized (like a trait object)?
That or T is large and you need to move it around a lot. Moving ~T is a cheap operation, of course.
I agree. Rust makes it so easy to define new algebraic data types, and ~ made it a lot neater.
not paying 333tl for a js conf mate, could get you a pint if you fancy meeting up and shooting the shit
The cost of a heap allocation can be surprisingly expensive (partly due to the cost of the allocation, and also poorer cache behaviour), so it's worth measuring before using.
Yes, that is a situation where `Box` is always required, however trait objects are relatively rare, and vectors of them rare.
I agree a straight Vec&lt;T&gt; is usually what you want, but it has its uses. Some cases where people still want to use link lists (lots of re-ordering, or moving objects between collections), the vector-of-owned objects is a nice alternative. variable size case, e.g. Vector-of-trait objects or c++ vtable-based objects:- They are indeed rarer, sorting by type is what you usually want; but there are valid use of vtables: when the number of potential types exceeds the number of instances. or maybe functionality of a framework extended by DLL/so's, or code where the versatility is more important than the performance.
agree - one use case of ~T was when different enum variants had different sizes... the odd sized objects could have extra data in a ~T, keeping the majority of the variants the same size eg enum Yada { Foo(x,y,z), Bar(x,y,~(a,b,c,d,e)) // saves Foo from being padded } writing Box there isn't a killer, but it was nicer before
wait wait, it's not the conference that I'll be speaking at. It's the meetup on Friday. I think this is free for everyone.
I have no idea.
Now that you're familiar with the basics, it is important to practice your knowledge on a lot of diverse projects. The most important thing to realize about Rust is that while it is marketed as a systems programming language, it can be used for anything where speed and correctness in needed. You can apply Rust to anything that you find interesting; - As far as systems programming goes, [take a look at Julia Evan's blog](http://jvns.ca), where she talks a lot about how to implement an [operating system kernel](http://en.wikipedia.org/wiki/Kernel_(operating_system)) in Rust and a ton of other, low-level stuff. - If you're more into game development, take a look at the [Piston game engine](https://github.com/PistonDevelopers/piston) and see where you can help! - Perhaps you're more into web development? Take a look at [Iron](https://github.com/iron/iron) and learn how it is implemented down to [rust-http](https://github.com/chris-morgan/rust-http) - I know that Rust runs on some ARM dev boards, which could be used for robotics projects - are you interested in real-time embedded development? Rust is very good for that. - Looking for something else?. Take a look [at this](http://www.rust-ci.org). - Have an idea of your own that you wish to be fast, safe and concurrent, while practising your Rust skills on a real project? Don't be afraid to start on it and when you get suck, hop over to #rust at irc.mozilla.org and we'll be happy to help!
You can bundle the dylib or whatever with your crate (effectively windows style distribute with DLL)
offer still stands if you don't feel like talking about "how it's a swell idea to have the same code running on client and server [you know](http://www.youtube.com/watch?v=d6AnXi2N_do)"
Macro expansion is currently linear from start to end in a single thread. You could totally write a syntax extension that does this. Something like: #[auto(foo)] trait Foo { ... } #[auto(foo)] trait Foo2 { ... } #[auto(foo)] trait ManyFoo : Foo + Foo2 {} struct Bar; #[auto(foo)] impl Bar { } #[autoimpl(foo)] mod impls {} Note youd have to mark all the traits, and modify a target (eg mod) to inject the impls. Im pretty sure it would work. Fwiw, look how this is implemented with the modify and decorate sharing state similar idea: https://github.com/shadowmint/rust-fixture/blob/master/src/lib.rs
Good point, although honestly I'd consider that more of a bug in Rustdoc.
Unicorns are also pretty rare, but I'm thinking it's less because people don't want unicorns, and more because it's hard to glue the horn on.
The reason float does not implement Eq is because of NaN (multiple binary representations of NaN, and NaN != NaN), so you'd have to handle it somehow. Wrap it as: enum myFloat { Real(float), NaN } and implement Eq for it. EDIT: Actually, thinking on it, you ~~might~~ will need to implement some more traits for lessthan/greaterthen. There's no strict ordering to floats when NaNs come into play.
Practically speaking the order seems to be the same as the way files are loaded. eg. mod foo; mod bar; &lt;-- foo.rs loaded before bar.rs I'm not sure if that's by design or 'just the way it happens to work for now' though. There's no way to do a 'multipass' parse of the code as far as I know. You'll probably have to ensure that the definitions are all loaded before anything tries to use them. 
`HashMap` with `float` keys seems smelly. Are you certain that's what you want?
Similarly, if the underpinnings of the web interest you, there are many interesting ways to contribute to the web browser [Servo](https://github.com/servo/servo/).
Hang on, Rust has a `long` type?
How could I've forgotten about Servo? // feels ashamed :-) 
Well, Rust doesn't have a `float` or `double` type either :p (though it does have a `Float` trait in the `std`library). I'm guessing the OP meant `f32` or `f64` and `i32` or `i64`. 
Heh, yeah, I was using "float" to represent all floating points, "long" to represent all integers. Just shorthand instead of writing out all the combinations. Sorry for the confusion!
Ah, interesting. Thanks! Is this is dependent on your application's idea of equality? E.g. why doesn't Rust provide a wrapper which provides this functionality (and how do other languages get around this problem)?
I have no idea :) If a user indexes a field: `"myfield": 0.2`, I want to be able to find the bitvector associated with `0.2`. In other languages, I'd just chuck that into a hashmap and call it a day. Clearly those other languages are doing some magic behind the scenes to make it work, which is why I'm unsure how to do it in Rust :)
Other languages tend to ignore corner cases like `NaN`s and call that acceptable. A case to think about: do you expect to get the same answer if you look up `0.3` as you do if you look up `0.2+0.1`?
Is there a good primer on floating point somewhere that I can read? I don't understand how `0.3` is different from `0.2+0.1`, but I imagine it'll take some reading to understand the difference.
Do hashmaps have to implement eq for the values you store? That doesn't seem right but I've never used it. The [docs](http://doc.rust-lang.org/std/collections/hashmap/struct.HashMap.html) say that just the keys have to implement it. EDIT: Using floats as keys brings up a lot of questions. Hashmaps generally work by hashing the binary value of whatever you pass in. With floats, its possible (and actually VERY likely) that a binary representation of 0.2 doesn't exactly equal 0.2. Or that you might have two instances of "0.2" but they have different binary representations, and would thus point to different values. Java [converts the double into a long during the hash process](http://stackoverflow.com/questions/9650798/hash-a-double-in-java). Your best bet might be to emulate something similar for your needs. 
Short version: what every programmer should know about floating point: http://floating-point-gui.de/ And a much more in-depth version: http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html
Excellent, thank you very much!
By the way, these issues are not unique to Rust. All modern languages have them, because of the standardization of hardware on IEEE floating point and modern languages tend to run on modern hardware. Nowadays, you only see deviation from this behavior as compromise forced by engineering constraints, or because the designers didn't know what they were doing. 
I think Rust is a great language for hardware tinkering on single-board linux computers like the Raspberry Pi. I've written some code for this here: http://github.com/npryce/rusty-pi.
So I was looking through the source because it's nice to see a clean example using one of the few rust web frameworks out there. I came upon [line 55 of controller.rs](https://github.com/Indiv0/paste-rs/blob/master/src/controllers.rs#L55) and almost chuckled out loud. Thanks for the joy ;)
Did you read the sidebar? "Anything whatsoever related to the Rust programming language: an open-source systems programming language from Mozilla, emphasizing safety, concurrency, and speed."
&gt; No sorry i didnt know where els to post it could u tell me where 
/r/playrust. I believe it's written right above the submit button you used to post this rant.
http://www.reddit.com/r/playrust
this is really interesting, dynamic code/jit would be a really nice feature for interactive development; something I've really come to appreciate in lisps
Sorry, not familiar with which subreddit that would be. Maybe even the general gaming subreddit?
or that bullet's limitation was that it was written with only a c++ api in mind: https://github.com/bulletphysics/bullet3/issues/130 
nice presentation! As a python guy: The major point I see in this presentation is full right is the speed of execution thing. I'm sure it's important for big enough company like twitter but for 90% of programming I'm sure it is not really relevant. The code injection is good enough but it is more about how much the template processor is clever enough than really the language is dynamic or typesafe. I can be wrong of course.
/r/playrust
How is a data race not a logic error?
After reading parts of those links (still working through the content, it has been very informative so far, thanks!)...it seems that what I actually want is "Limited-Precision Decimal" or "fixed-point" rather than a true floating-point. It looks like there isn't a library that provides a `Decimal`, so I might have found myself a new weekend project :)
Short version: That ``10 / 50`` is exactly representable is more a coincidence than anything else. Because we use base ``10``, every fraction that is composed of ``2`` and ``5`` (as in ``1 / 400 == 1 / ( (2 * 2) * (2 * 5) * (2 * 5)) == 0.0025`` ), are representable, but others (as in ``1 / 63 == 1 / ( 7 * 3 * 3) == 0.01587301587...`` ) are not, because ``10 == 5 * 2``. If we used base ``21``, every fraction composed of 7 and 3 would be representable. (e.g. ``1 / 63``, as above), because ``21 == 7 * 3``. The computer uses base ``2`` for internal storage, so every fraction not composable of twos is not represantable exactly.
He wants to have the floats as keys
It's not only about speed, it is also about type safety. (You want to have compile-time errors rather than run-time errors. The compiler can do a lot of work for you that would be impossible in dynamically typed languages like Python.)
If you do this, consider wrapping an existing library like [libfixmath](https://en.wikipedia.org/wiki/Libfixmath). Edit: There are some [existing bindings for GMP](https://github.com/thestinger/rust-gmp), which has an "Mpf" which may suffice for your needs.
I don't see any reason why the rules for NaN couldn't be incorporated in the implementation of Eq for floats. The reason float doesn't implement Eq is because testing floats for equality is pretty much always a bad idea due to many numbers not being exactly representable, eg try "0.2 + 0.1 == 0.3" in Python and you get False. The recommended way to compare floats for equality is something like: abs(x - y) &lt; eps where eps is some very small number like 1e-10
Note that that can have problems, e.g. if you're working with very small numbers then `1e-10` may be *larger* than `x` and `y` (and `|x| + |y|`), and so everything compares equal; or, if you're working with very big numbers, the only way to get `|x - y| &lt; eps` may be for `x == y`. In particular, relative equality, taking into account the scale of `x` and `y`, is often more desirable, e.g. `abs(x - y) &lt; eps * y`. (Of course, sometimes one does want absolute approximate equality, rather than relative... it should be decided in a case-by-case manner.)
Oh it's certainly possible, but it's not the place of the language to decide that 10 decimal places should be enough for anybody.
Thanks Josh, would love to contribute. 
I think you misunderstood my comment, I'm definitely not saying Rust should implement that in the Eq implementation. I was trying to show why an Eq implementation is a bad idea and that if OP is actually wanting to compare floats he should do something like that.
There's the `std::rt::task::BlockedTasks` iterator that yields `std::rt::task::BlockedTask`, where you can call `destroy()` to safely kill the task, but I'm not sure where you'd get an instance of the iterator. I searched the source, but I couldn't find a single function that returned `BlockedTasks`. It might just be a stubbed API that someone meant to fill out but it was forgotten about. If you don't mind a nonconventional approach, you can have a sentinel task that spawns the others with `native::task::new()`, which returns the handle to the new task. Then you can do a looping iteration through the task handles, calling `is_destroyed()` on them. If one is destroyed, then destroy the others and exit the loop, letting the program end normally.
Thanks this seems like a good example for syntax extensions. Do you know of any other resources for syntax extensions?
Floats don't implemented `Eq` because they have unordered values. It isn't due to the language trying to discourage equality comparisons. They do implement `PartialEq` which is tied to the `==` and `!=` operators.
It's not possible to kill a task. The task needs to check if it should keep going, and avoid blocking by using non-blocking / asynchronous I/O. Rust doesn't provide the necessary tools to do something like this in the standard libraries yet.
June was Tom, July was me talking about Rust, right? I guess he wins. Curses! :wink:
Yes, it is absolutely possible to start the Rust runtime from another stack (it essentially requires calling an init function and a shutdown function) and thus use the entire standard library and all libraries based on it, but with the impending removal of libgreen there will be very little runtime left, and most Rust features will work without any initialization at all.
Just had an idea for the meetup I am giving in a month, write some Rust code and get it running in Python!
[There are Rust bindings for Freetype, used by Conrod](https://github.com/PistonDevelopers/freetype-rs)
Awesome to see Rust being used instead of C for performance reasons! &gt; As Benjamin Franklin said, "those who would give up memory safety for performance deserve neither." Wouldn't Ben's original quote be closer to "those who would give up essential performance for a little temporary memory safety deserve neither performance nor safety"? :)
See also separate meeting [Meeting-inheritance-2014-09-23.md](https://github.com/rust-lang/meeting-minutes/blob/master/Meeting-inheritance-2014-09-23.md)
Please do. :) Coeuvre is working on a [texture packer](https://github.com/Coeuvre/texture-packer) intended to be used in a high level font library. The ideal thing would be a back-end agnostic text library, since everything else in Piston is back-end agnostic. Then we can make Conrod a pure Rust library, which would be awesome, because then we can reexport it back in Piston, if we want to.
I don't think replacing unique_ptr with Box type even counts at all as syntactically it has all of the obtuseness of unique_ptr that he talks about doesn't it?
Thanks, will do!
I like this proposal very much - in java, many people create Builders just to have something akin to keyword args. It *looks* like it could be added in a backwards-compatible way, though, so it probably can wait after 1.0 lands. How would this interact with anonymous functions, e.g. `|x, y| { x+y }`? Is `|x = 1, y| { x + y }` permissible under the proposed change?
What are the arguments against having default arguments? As a non-experienced programmer, they seems to me a very neat thing, but I realize many experienced programmers don't like them. Are they really this bad for the readability of the code?
On a similar note, a while back I wrote a wrapper library in Rust that wrapped a Python library. Sadly, I could never get rust-bindgen to create a rust binding for Python.h so I ended up writing a c dynamic library that I would call from rust... which is not very elegant. Also the code has just rotted away on my github since schools back in session and I just don't have time anymore :( But in case you care to see something that is partially related to using python to call rust code [the library link is here](https://github.com/jroweboy/jinja2-c)
Theoretically, this could work: let incr = |x = 1, y, z = 3| { x + y + z }; incr(,2,) But it is kinda ugly.
The idea of named parameters is not only to shorten the parameter list, but also to add the names to the call site to aid understanding. Or (taking an example from java) can you infer from the code what `Graphics.copyArea(0, 0, 200, 300, 1, 1)` does?
I don't have a problem with requiring a certain order as long as the compiler returns an easy to follow message to that effect (though others may see it differntly). Apart from keeping the call unambiguous, this would help make the code more canonical.
I don't argue with this. Maybe there should be an option to make a function require **mandatory** named parameters?
Honest question, would this really be considered an unconventional approach? It was my first idea - but to be fair, I'm a Haskell guy and I used Go a bit a while back, and those are two languages where "just create another task" is completely idiomatic.
Isn't arity overloading basically having an array as a parameter?
Agreed!
 let incr = |x = 1, y = 1| { x + y }; // how would you call this with y = 2, x default? incr(x, 2)?
Destroying the blocked task would be something to do when one of the other tasks catches a failure state. But manually spawning the tasks with the other method makes you call `task.run(||)` which returns the failure message if it does fail, which is much more explicit. Now that I know of the other method, I should revisit one of my older Rust projects that relied on handling unexpected failures without exiting the task.
Normally you would use `spawn()` which does not give you a handle to the created task, making you manage it using task channels or other synchronization structures. This encourages concurrent code that is agnostic of the underlying threading model, which might be easier to reason about. However, with the green threading model being moved out of the language, code that is willfully ignorant of the threading model might be at a disadvantage. I couldn't say. This is mostly conjecture anyways.
I wonder if it is worth submitting this as an LLVM bug.
Something that could so simply be solved by coding a C-style `for` loop has become so complex. Sigh. Remind me why C-style `for` loops have gone out of fashion again? I've yet to hear a strong argument.
We already had that in regard to C with Mesa and Modula-2 back in the day, but they weren't the language UNIX was using. Back then, C wasn't the performance monster with 30 years of research into compiler optimizations. So here we are now, hoping that languages like Rust bring back the lost safety languages from Xerox PARC, ETHZ, IFIP, DoD have once provided to the world.
What if you have a variable `x` in scope at the call site?
What does using Fiddle "cost" in terms of performance vs a "real" Ruby C extension? Is it even possible to write ruby extensions without Fiddle (or some sort of C trampoline)? By using Fiddle, do I get jruby/rubinius compatibility for free?
My issue with it is that not all functions have a sensible order. What if you have a function to connect to a server that accepts a port (defaulted to 80) and a retry count (default to 3), and a timeout (defaulted to 10s). What order should I put them in? If I want to change the timeout, I shouldn't have to specify the port or the retry count, or any other combination. Personally I believe in this case you should be using a config struct or similar and that this would be bad API design (named parameters make more sense), but it's just an example. I could live with an order too, I was just pointing out a potential sticking point :) &gt; Apart from keeping the call unambiguous On a more theoretical note, my feeling is that you start gaining ambiguity as soon as you use default arguments. I'm yet to come across an example of a function with optional arguments that would not be improved (ambiguity-wise) by optional *named* arguments, instead of unnamed ones.
I see you pop up a lot on these sorts of discussions but don't remember seeing your thoughts on Ada at any point, are you able to point me to somewhere you've shared them, or are you able to here? Do you a think a more readily available compiler would have changed its community adoption? Edit: Apologies for the off-topic
As long as you keep the original order until the first defaulted argument and start naming arguments at call site, it should be fairly straight-forward.
Not that I know, the plan is to make as much of the standard library as possible independent of the runtime, and to make runtimeless rust very easy to achieve/use. Removing the runtime entirely would mean losing much of the i/o subsystem on top of tasks and anything task-related (including backtracing)
That's fair enough :)
This is awesome! waiting for part two.. Just created a gist, consuming the same shared object from Python https://gist.github.com/aravindavk/2b4298eeb2d8f949224b
I'm personally of the opinion that explicit is almost always better than implicit. It can be a pain in the ass having to look up documentation to find out what the default value of a function is. It can also just adds complexity and confusion - see `theypsilon`'s comment on the RFC. Although this is not an issue with the feature per se, it can encourage bad API design. Take a method `.split(sep: char = ?, count = ?)` that splits a string. The `count` argument isn't so bad, because there's a sensible default - as many as possible. The `sep` argument is a problem for me. I've used libraries where it's newlines, or spaces, or all whitespace. Which one? I have to go look it up. Having to supply the separator every time takes literally 1 second, so the potential game from the ability to leave it off is minimal. My personal feeling is that if you've got a function where it's a real hassle to have to write out all the arguments, perhaps a configuration struct is a better idea. Of course, these are just issues with design decisions *allowed* by optional arguments, but maybe they'll give you some insight into some opinions against them.
OK, that's a problem. :-)
There are a few comments from me describing both Ada's safety and why it failed to become mainstream (IMHO) at HN. https://news.ycombinator.com/item?id=8330165 Nowadays it seems to be rising slowly in some niche markets, from the talks I enjoyed at FOSDEM, but I doubt it will ever become mainstream. I do like the language, being foremost a fan of Niklaus Wirth influenced languages. One of the reasons I jumped into C++, was it gave me some of Pascals safety coupled with C's portability. Until the portability need appeared, I was quite happy doing all my coding in Turbo Pascal, even system programming stuff. My C code always followed USCD Pascal style code, with ADTs and accessor functions with some Eiffel influence using asserts for DBC. 
I [wrote a blog post](http://harkablog.com/calling-rust-from-c-and-python.html) on this not too long ago. The sticking point for me was not getting rust going, but trying to figure out how to get the runtime bootstrapped while still being able to provide a return value to python. If you'd like to use my post as a jumping off point, feel free. If you figure out how to get the runtime to let you return arbitrary values, can you report back? I'll probably take another crack at it myself soon. I've just been busy lately.
I know it's also about type safety. Until now, I found that the price of type safety is too big for the return on investment. The large majority of error I'm confronted are either: - are as simple that they are able to be catched by some kind of lint tool - are some kind of logical mistake which makes the application misbehave but would be very hard to formalized (as far as I know) however I clearly see the benefit of rust over c for example I have still to be convinced of the benefit over python or ruby but all hope is not lost
Nope, arity in this case is just the number of arguments a function takes. Arity overloading could refer to a subset of the ad-hoc overloading, or the use of variadic templates, in C++. In other languages you do get an array, but those are usually dynamically typed languages (JS) or have weak static-ish typing (Java). Passing `&amp;[&amp;Any]` to a function is not really acceptable in Rust, and is less flexible (not only less efficient) than proper variadic generics. In Rust, it might be soon possible to have multiple impls of `Fn` traits for a single type, which could lead to some abuse (but it's clunky and if the standard library doesn't do it, we're not doomed).
I think this is a good way to go. In C++ I always felt like I was lacking a way to tell the compiler that I want to use default values for a and c, but a custom value for b. 
Look at [NanoVG](https://github.com/KevinKelley/nanovg-rs). This is a Rust wrapper for a small, hardware-accelerated vector-graphics library.
I would argue that thanks to C pointers, people will associate `*T` with danger.
Java used to have a way to do it and took it out when it turned out to be dangerous and nearly impossible to use without abusing. I kind of doubt it will ever be added. But maybe strcat knows differently.
I've been using Zinc for a Gimbal that I'm building in my sporadic spare time (jumping out of planes is kinda time consuming :)). http://www.hydrocodedesign.com/2014/08/05/building-a-gimbal-in-rust-introduction/ While I haven't had too much time to work on it recently, Zinc has made some major improvements that I'm eager to try out.
Awesome, thank you for your thoughts and the link.
Well, there are no virtual struct and no struct inheritance in Rust (however there is a current discrussion about adding both of them in some form). Maybe you can use struct that contains common attributes and enum for the others: struct Node { attr1: Type1; attr2: Type2; ...; more: ElementSpecifics; } enum ElementSpecifics { AdmonitionElement, TextElement(String), ... }
Fiddle is a library (written by Aaron Patterson, IIRC) that basically gives you access to a C pointer, wrapped in a Ruby object. Ruby's FFI is a whole different library, and writing a 'real' cext is different than that, too. I'm unaware of the performance differences. Unsure about JRuby/Rubinius, but maybe?
C# requires optional arguments to come after required arguments.
I'm not an iOS Dev so I couldn't say. [Here's the discussion on the removal of libgreen.][1] [1]: https://github.com/rust-lang/meeting-minutes/blob/master/workweek-2014-08-18/threading-model.md
except that then you have to look up the config struct and you can't just tell from reading the function signature. If you don't like the "sep" default option that's just an example, maybe it would be better to force you to always supply the sep - thats a question of API design and doesn't really affect the case for default arguments (there are a million ways to design a crappy API with or without default args)
If you don't have optional arguments then you need to specify `count` each time. That gives you the same problem as with `sep` in your example: You have to go and look up what value the function takes for "as many matches as possible". It could be `0` or `-1` or maybe something else.
And that's why I put the disclaimer at the bottom :) You're right that these are API design problems. I'm not saying I 100% agree with these arguments, just that those are some that people hold.
&gt; My personal feeling is that if you've got a function where it's a real hassle to have to write out all the arguments, perhaps a configuration struct is a better idea. Or the builder pattern. http://blog.piston.rs/2014/09/14/conrod-api-overhaul/
That is very nice! It certainly reads cleanly. Am I right in thinking that it is conceptually similar to creating a struct filled with default arguments and manually changing them, just with methods instead of member modifications (thus allowing a lot more flexibility)? &gt; Removes the need for the old enums that were necessary to handle defaults, etc. This is the one reservation I have about that style. It's moving information about the operation of the program (GUI elements in this case) from data to code. I'm used to the notion that pushing as much into data as possible is a good idea, like the myriad of Haskell's DSLs. But having not used this pattern yet, I can offer no real practical criticism, just thoughts.
Which IMHO is as it should be.
This is one way to do it. The biggest weakness it has is that ASTs usually accept only particular node types as children, so you don't have type safety with the child nodes. For this reason, I might suggest instead doing it like this: struct Element&lt;T&gt; { /* ..., */ more: T } impl&lt;T&gt; Element&lt;T&gt; { fn new(more: T) -&gt; Element&lt;T&gt; { Element { /* ..., */ more: more } } } struct Admonition; struct Text { text: String } // Now you can type-safely create AST elements: struct TextList { children: Vec&lt;Element&lt;Text&gt;&gt; } let admonition = Element::new(Admonition); let text = Element::new(Text { text: "foo".to_string() }); let textlist = Element::new(TextList { children: vec!(text) }); **Edit:** Just rechecked the OP and I guess you want generic list methods? The problem there is that then you either have to allow all children into each node, or use a generic that lets you specify precisely what types are allowed. I'd still do the latter with generics given the choice, and the former (in the rare situation that it was really what I wanted) using an enum containing all the possible node types. **Edit 2:** This is the more idiomatic-Rust way to do exactly what you asked for in the OP (run at http://is.gd/bUufLp): use std::default::Default; #[deriving(Show, Default)] struct Element&lt;T&gt; { /* ..., */ more: T } #[deriving(Show, Default)] struct Admonition; #[deriving(Show, Default)] struct Text { text: String } #[deriving(Show, Default)] struct List&lt;T, C&gt; { more: T, children: Vec&lt;C&gt; } impl&lt;T, C&gt; Element&lt;List&lt;T, C&gt;&gt; { pub fn add_child(&amp;mut self, child: C) { self.more.children.push(child) } } type TextElement = Element&lt;Text&gt;; type AdmonitionElement = Element&lt;List&lt;Admonition, TextElement&gt;&gt;; fn main() { let mut admonition: AdmonitionElement = Default::default(); admonition.add_child(Default::default()); println!("{}", admonition); } **Edit 3**: And the even more "evil" way (run at http://is.gd/Y4zrqf): use std::default::Default; #[deriving(Show, Default)] struct Element&lt;T&gt; { /* ..., */ more: T } impl&lt;T&gt; Deref&lt;T&gt; for Element&lt;T&gt; { fn deref(&amp;self) -&gt; &amp;T { &amp;self.more } } impl&lt;T&gt; DerefMut&lt;T&gt; for Element&lt;T&gt; { fn deref_mut(&amp;mut self) -&gt; &amp;mut T { &amp;mut self.more } } type TextElement = Element&lt;String&gt;; type AdmonitionElement = Element&lt;Vec&lt;TextElement&gt;&gt;; fn main() { let mut admonition: AdmonitionElement = Default::default(); admonition.push(Default::default()); println!("{}", admonition); } (The slightly saner way of using this third option is to have some attribute, ```children```, on the struct of type ```T``` that you use for ```more```, that by convention is always of type ```Vec&lt;T&gt;```, and use ```.children.push``` instead of ```.push```. But it's still magic, and therefore evil :)). **Edit 4**: If you're wondering how Rust does it, check out https://github.com/rust-lang/rust/blob/master/src/libsyntax/ast.rs.
That's one way to implement it, yes. That also means that you can remove your second objection: at the end of the day, you end up with a struct filled with options, so you could also just create that struct and use it if you preferred.
Java still has `Thread.interrupt()`, which will awaken a thread blocking for I/O or one that's watching the `interrupted()` flag in a loop. There's `BlockedTask::awaken()` but getting a `BlockedTask` instance seems difficult, unless you block it yourself with `BlockedTask::block(Box&lt;Task&gt;)`, which returns an instance of `BlockedTask`.
&gt; there are no virtual struct and no struct inheritance in Rust [this is wrong since april 20](https://github.com/rust-lang/rust/commit/fc2815a5cc703523c9a9aa2f2982e667e58a0402?short_path=5b95312#diff-5b9531258e65a8733242b78f81019edd)
I was not going to post this here until she commented on type systems. I can understand what shes getting at, but I disagree. Rust is able to prevent data races and many other things at *compile time*, which is the big boon towards statically types languages. I would love to see more discussion :O
Julia is very aware of Rust, having written a tiny kernel in it: http://jvns.ca/blog/categories/kernel/ Her point is not that types are not useful. I will just pull out two sentences of the post: &gt; I love using static type checking, and I would love to see evidence that statically typed programs actually have less bugs. &gt; &gt; We could talk about this in a more nuanced way! Types do help, but they aren't a panacea. For example, Rust can prevent data races, but it cannot prevent race conditions.
at least when i last updated the nightlies two days ago, it was there and worked as advertized.
I've never used it myself, but it sounds like a job for the [std::select!](http://doc.rust-lang.org/std/macro.select!.html) macro. Or, if the receive channels are not statically defined but built at run-time, I guess you could manually populate the [std::comm::Select](http://doc.rust-lang.org/std/comm/struct.Select.html) structure.
I know that Julia is aware of Rust, and I know what she said, I did not say the she says static type systems are not useful. I was pointing out that through Rust's type system, one can remove a class of bugs from their programs. Race conditions are unfixable, they are a fact of life if you want any sort of usefulness in your code. 
```Thread.interrupt()``` doesn't interrupt normal Java code, and when it does it's implemented as a checked exception (still better than the thankfully-departed ```Thread.stop()```). You can explicitly check for the bit, but most people don't, especially not in the sort of performance-critical tight loops where it would be useful. As someone who actually tried to use interrupt properly, I also frequently ran into situations where an interrupt is swallowed by a catchall exception handler, which unsets the interrupt bit and makes it impossible to tell whether the thread has been interrupted. This isn't just a common occurrence, it's actively encouraged by features like Java 7's ```try-with-resources```, which will swallow them and just advises you not to throw them! And they don't even work consistently on blocking I/O; for any I/O that goes through a ```InputStreamReader```, for example, it doesn't fire (or if it does, it's not identifiable as an ```InterruptedException```). Unless you're using ```java.nio```, you may not get any exception at all in those cases, and when you do, what you get is not an ```InterruptedException``` but a ```ClosedByInterruptException```, which you have to catch explicitly. Your thread also quickly becomes polluted with interrupt checking logic. (I'm not saying Rust has a better solution here--except that it doesn't have catchable exceptions in safe code--but my experience convinced me that unless you control your entire stack it's basically impossible to handle thread interruptions sanely, and if you do control your entire stack you can come up with a custom method for dealing with this situation).
This is a common pattern in javascript world and not in Python community and it requires the same effort in both languages. Why then? Probably because javascript didn't have default arguments until ECMAScript 6. So when they have both possibilities, people (python programmers) prefer default arguments. All the benefits of the builder pattern listed in the [post](http://www.reddit.com/r/rust/comments/2gcofl/conrod_gui_update_new_improved_api/ckhw6fx) are also benefits of the default argument construction. But implementing this pattern comes with a price, **verbosity** in the implementation. This downside was cited by the author of conrod [here]( http://www.reddit.com/r/rust/comments/2gcofl/conrod_gui_update_new_improved_api/ckhw6fx) . However this disadvantage is not present in the default arguments solution. IMHO default arguments are a very "organic" way of growing APIs, with no need of overengineering at the begining of the design process because adding default arguments don't intriduce backward incompatibilites to the API. It also makes APIs more concise by avoiding specialized methods (like in the `split` example).
i don’t like it. instead of having to look at one single function documentation with all the argument types and defaults documented, you now have to remember or look up all the builder methods and distinguish them from the normal methods. also you have to create the object and then modify it instead of creating it once (possibly immutably). default arguments that are specifiable using keywords are self-documenting, easier to use, and have no downsides. (e.g. the ones python uses, except with types here)
config structs have to be defined and are another thing to remember or look up. default arguments are just there, in the function signature, just like function types. pretty much perfect.
Then you have to pass `Some(5)` when you want five matches. That's not very user-friendly.
I would like to propose a slightly different syntax. Instead of `:`, which is used for types and slightly ambiguous we can use `=&gt;`
or just incr(y =&gt; 2) no having to do underlines and commas
Let me see if I understand the problem right: You want to evaluate a bunch of boolean functions. If any one of the functions returns `false`, then you short-circuit and say that the whole set is `false`. You want to spawn a thread for each of those tasks, and have each one `fail!` if any of the others returns `false` before it finishes, or else have all of them run to completion and return `true`. Is that about right?
How's this: https://gist.github.com/Thiez/a139bfaefbd44865872a
This is awesome. I struggle with experimentation with the module system every time to get a library exactly how I want it. This clears up a lot of details that were fuzzy to me including re-exporting. Sidenote: Cool to see people using Ghost. It's popping up more and more.
&gt; I was pointing out that through Rust's type system, one can remove a class of bugs from their programs. Right. But many times, static typing proponents over-reach on this claim, which is basically how I read this post. Static types are nice, but they can't solve all bugs. And some data on how much they solve and in what situations would be supremely helpful. I've seen contradictory studies on this topic.
&gt; But many times, static typing proponents over-reach on this claim Yeah, I believe this is what Julia was talking about. Someone over in the /r/programming discussion remarked [that at least one of the static type talks at Strange Loop was condescending](https://www.reddit.com/r/programming/comments/2hbpwf/julia_evans_on_strange_loop_2014/ckreqvi), so perhaps it was this vibe that Julia picked up on.
Shouldn't you be able to do std::rt::init(argc, argv) on init, and std::rt::cleanup() on cleanup? Those don't jump anywhere, they're ordinary functions.
Please ignore it, that was added by nrc without any such feature being approved.
Selecting channels like /u/suridaj mentioned is probably the best bet. The channels can be used to signal both completion and failure, providing you the ability to exit when all threads are done, or short circuit if one fails. An alternative can be built with a busy-spin method that will burn CPU: - Create an Arc&lt;AtomicBool&gt; to signal for short-circuit failure - Create an Arc&lt;AtomicInt&gt; to signal for completion of all threads - Spawn all of constraints in their own thread, giving them a clone of both Arcs - In your main thread, start a loop which checks if `AtomicBool == false || AtomicInt == # of threads`. If true, exit loop and do whatever needs to be done. Otherwise, keep looping You could add a small sleep or a yield so you don't chew up all the CPU on one core - In your constraint threads, evaluate the particular constraint. If the constraint is false, set the AtomicBool to false. If the constraint is increment the AtomicInt Technically you could use a regular Bool and unsafe memory access to it (since the constraints only write `false` its ok if they race on it), but the contention will be low anyway so an AtomicBool is fine. But yeah, use channels :)
Super useful, I struggled with exactly these things. I'd copy this verbatim in the Guide! :)
I'm used to the Python way of handling this which I've had zero issue using. I.e. key word/default args must come after positional args and are optional. Initially I liked the `foo(x,y=1) -&gt; int` being called `foo(x,_);` if you simply think the default is fine. But if you take that out to some number of kwargs (such as `foo(x,_,_,_,_);` it becomes silly and almost as bad, or worse than `foo(0, 20, -1, 20.2, 0, 0);` In addition it would also force you to memorize the order, which kind of defeats the purpose anyways. (i.e. `foo(x,_,z=2,_,_,a=20);` I loved having the ability to extend APIs with kwargs without having to write new functions which break backwards compatibility. It's also nice having the options such as `foo(x,y=1,z=2);` could be just as easily called `foo(x,z=2);` or `foo(x,z=2,y=1);` 
I forked your gist and added an implementation that uses CFFI instead of ctypes (as that's the preferred mechanism for calling C from PyPy). It's similar to the Ruby original, and different from yours, in that the structs are allocated on the Rust side instead of the Python side -- I couldn't get your way to work with CFFI for reasons that aren't really clear. Might be worth further investigation.
`use std::io::fs:: PathExtensions` to bring the `exists()` method in-scope.
In rust for trait methods to work you need the trait to be in scope. If you look at the docs page `exists()` is from `PathExtensions`, which you will have to `use`. Those docs are autogenerated via rustdoc (code in src/librustdoc) and autouploaded, so no need to manually fix things (the text in the docs can be fixed by editing the relevant doc comments).
Interesting. That surprises me; it wasn’t meant to produce any effect like that. The talk as a whole appears to have been more controversial than I had thought. Can you go into more detail? I’d like to improve my content and method of delivery.
I really like this proposal. Coming from Python this lets you reduce the surface area of APIs tremendously, making them easier to understand and use and conceptually simpler. An API with keyword args, fewer methods, and sane defaults leads to more user-friendliness while still allowing for more specific use cases.
I'm not sure how that's user-unfriendly? It's no more difficult to type `Some(5)` than it is `num = 5`.
It seems that you are using a "very old" version of Rust. The `syntax::ptr::P` pointer [was introduced 11 days ago](https://github.com/rust-lang/rust/pull/13316#event-165172094). Try using a nigthly version.
&gt; It can be a pain in the ass having to look up documentation to find out what the default value of a function is. If you care what the default value is then you should be setting it yourself. 
Code is rarely write-only. I agree with you completely, but what about if I'm going through someone else's code?