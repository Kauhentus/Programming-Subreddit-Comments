Faster will be migrating from `stdsimd` to this ASAP, although users of the crate shouldn't notice. It should be a feature in 0.5.0, along with a huge iterator overhaul, vectorized strides, speedups, more ops, and a super fancy tutorial/marketing website. 
Very cool, thanks
LLVM can sometimes vectorize a polyfill when I don't have an explicit algorithm for an intrinsic (I'm seeing this on a lot with NEON). Most if not all of the x86 stuff is explicitly vectorized at this point. There are a few situations where no intrinsic exists (byte &amp; word gathers especially) where LLVM sometimes finds a vectorized solution for the specific situation, but not in the general case.
&gt; What I'm wondering is if autovectorization is the next step, or if it's further down the road? Autovectorization should already work (it's an optimization pass when lowering IR to x86 in LLVM if I'm not mistaken); I'm pretty sure it's entirely out of rustc's control, actually. &gt; I thought faster was based on another simd crate and the only major downside I remember for it was that it required nightly. So if this means that when this gets stabilized, faster can be stabilized, it means that stable will have certain tasks can be parallelized trivially with rayon + faster. Faster is based on `coresimd`/`stdsimd`. This pull request effectively `pub use`s it in `core`. We're on a pretty old version of `coresimd` so porting will be an nontrivial effort, but faster should be ready for stable before this is. I'm super excited to start melting VRMs on stable. &gt; At that point the only thing missing from the assisted parallelism story on stable would be a gpu_iter, although that might not be practically feasible due to the overhead of passing stuff off and returning it from the gpu. Once proc macros land, I'm on it.
Rust Hack and Learn happens every other week on Wednesday evenings at 7pm at Mozilla Berlin. Come hang out if you're interested in Rust!
&gt; I know this doesn't mean autovectorization now but I thought this was a blocker for it. What I'm wondering is if autovectorization is the next step, or if it's further down the road? I'm not familiar with llvm but I'd expect the backend already has some support. So I could imagine it might be pretty easy, or maybe this is totally orthogonal. To a first approximation, they are orthogonal. This is somewhat muddled territory. If you go look at LLVM's development history, then you will see that they are moving more and more towards autovectorization. They will even remove compiler intrinsics in favor of autovectorization. Basically, the compiler becomes smart enough to recognize certain patterns well enough, that writing explicit intrinsics is no longer required. The point of this effort is to provide functions that are defined by *vendors*, where the desired guarantee is "if you call this function, then you will get this specific CPU instruction." Sometimes these are implemented via the use of [explicit LLVM intrinsics for these instructions](https://github.com/rust-lang-nursery/stdsimd/blob/02a835998ba9b91df7363e7b3a6d8c5759878cf4/coresimd/x86/sse2.rs#L577) and other times they are [implemented in terms of LLVM's portable vector API](https://github.com/rust-lang-nursery/stdsimd/blob/02a835998ba9b91df7363e7b3a6d8c5759878cf4/coresimd/x86/sse2.rs#L604). The latter cases are interesting in that we are essentially reversing LLVM's portable API to provide an API that matches what the vendor specifies (where a vendor is Intel or ARM or whatever). Based on the above, it might seem like there exists a path where everything is autovectorized. Proving a negative is hard, but at least given the current state of things, this doesn't really make sense to me. Some vendor specific operations, like [PCMPESTRI](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=pcmpestri&amp;expand=810), are quite complicated, and it's hard to imagine its full range of functionality being exposed via autovectorization in a reliable way. I would posit that these are precisely the cases where vendor intrinsics are here to stay. Separately from all that, there exists a desire to have a portable vector API that provides high level operations. This is ideally what crates like `faster` would use as a bedrock, while perhaps dipping into the vendor specific APIs when necessary. The point of this portable API is that you get strongly typed vectors with convenient operations like "add" or "div" that compile down to the best possible code regardless of what platform you're targeting. Nightly Rust now [provides a portable API](https://doc.rust-lang.org/nightly/std/simd/), but it hasn't gone through an RFC yet. To summarize: * Vendor intrinsics are the lowest layer possible and mirror vendor specific definitions. That is, when you endeavor to use a vendor intrinsic, you are simultaneously endeavoring to write *platform specific code*. This is analogous to dipping into the `std::os` module. * A portable vector API provides facilities for working with vectors directly, but in a way that is platform agnostic. You can use vectors and perform elementary operations on them without ever caring about platform support. Even if your platform doesn't support vector operations at all, LLVM will know about that and implement the operations using standard instructions. The downside of this approach is that your limited to whatever the platform abstraction gives you. This is analogous to using `std::fs` instead of `std::os`. That is, `std` provides a platform agnostic API for access to the file system. * Autovectorization is a process instituted by the compiler that (ideally) turns code that doesn't use explicit vectors or vendor intrinsics into code that does use them, specific to the platform you're targeting. Generally speaking, the programmer needn't be aware that this is even happening, although of course, you will find that many programmers tweak their code in an attempt to get better results out of the vectorizer. To a first approximation, if you see the use of `xmm` or `ymm` registers in your generated code (on x86), and you didn't explicitly write vectorized code, then you are the happy customer of autovectorization! (This is not as simple as it sounds. If you call `libc::memchr` for example, you're likely to see vectorized code, but that's because `libc::memchr` is probably explicitly vectorized, at least, on Linux for GNU libc.) My analogy probably breaks down at this point... :-) In other words, these three things are all orthogonal, but related. I don't think any of them really block the other.
You don't need to make the fields public, so you can assume they won't be manually modified after the struct is initialized. And you don't have to write the new function yourself, obviously. :P
[`try!` is stable](https://doc.rust-lang.org/stable/std/macro.try.html), and so is the question mark operator `?`.
thank you
I read that as "low-level regrettable code generator". Makes sense to me, I have seen plenty enough regrettable low-level code
rust mode for vim, this is spacevim, still no rust layer for spacevim yet
Will do another english version vscode demo.
My OS is Archlinux, Spacemacs with rust and autocomplete layers enabled, Spacevim with rust mode, Sublime with rustEnhenced and Rls, Idea and VScode with stock rust plugin. No extra configure or hacking ,just following mainstream config docs under the web. 
And I'd exclude Clojure because it is a dying language. See how easy this is?
usually with things like this, there just hasn't been a "champion" willing to put in the effort to build out a full RFC and take it through the process. I personally don't know why post build scripts aren't a feature yet. when you talk about moving files around, I'm not sure what you could be doing. But, for environment variables, you can already inject their values directly using the `env!` macro, no need to edit files in some complicated way.
You are spot-on with the issues you listed. I also have been annoyed by pretty much your list of issues.
Hi there, author here. This is a Haskell _library_ for parsing Rust code into Haskell data types. The use case is for _Haskellers_ who want to generate/analyze Rust code for some other use-case. Pandoc, on the other hand, it a Haskell _application_. Its target audience is not Haskell developers, but people who want to convert documents.
&gt; How do macros in Rust impact the debugging experience? `compile_error` lets you provide better error messages when your crate/lib uses macros: https://www.reddit.com/r/rust/comments/80u1e2/shoutout_to_nom_and_compile_error/
You didn't mention that `build.rs` also generate Rust code, can control what dependicies can cause rereun of `build.rs` and many more functionality. Also one paragraph for dependecy management is too small. Every dependency has version contraint and may has options requirements, dependency of dependecy does the same, so you has huge graph of dependecy with different versions of the same crate and you need resolve this dependency problem. Plus of course this all functionality should works offline. And in fact it is not clear what is the point to duplicate `cargo` functionality?
Helpful compiler errors are nice. I'm more concerned about having the usual row-based debugger (at least row-based) in the major IDE's -- A sensible callstack, variable values shown in context and watch, stepping that works as expected, etc. In most C++ projects I've worked on (game development, embedded work, app development), the vast majority of code I've ever written is code that I've stepped through in the debugger to verify expected behavior.. and if I can reproduce an error with the debugger and get the code to stop close to or prior to the problem, the bug is often nearly solved/fixed (and so it's common to put debug-only assertions everywhere for this purpose as well, so bugs get fixed easily to improve code quality as assertions fail without ever having to be tracked). Macros tend to badly throw a wrench into my most productive way to work (can lower productivity and quality). Anyway - I'm very concerned to not see interest in the debugging aspect, since working on teams that have broken the debugging workflow is very frustrating. Rust has great debugging support, so everyone ought to try working with it for a while to see what that's all about.
Hi, 5 cents into this topic. Keep in mind that Vulkan is a spec, developed by Khronos Group. It meens that you should keep this link handy https://www.khronos.org/vulkan/ Especially https://www.khronos.org/registry/vulkan/specs/1.0/refguide/Vulkan-1.0-web.pdf and https://www.khronos.org/registry/vulkan/ because no matter what rust library for Vulkan you use, you'd have a moment when part of the functionality is not ported to rust (there is no binding for rust) and you'd have to implement it yourself by calling C functions from the native library. Where to get signatures for those functions? In spec :) I hope it helps.
Note: I am not the author of this!
Nice work! What you think about this project: https://github.com/GGist/bip-rs ?
No? Clojure's *macros* aren't explicitly considered experimental or pre-production; they're ingrained tightly into the language. The proposed exclusion of Scala has as its basis something relevant to the discussion at hand; unless you can provide reason to believe that Clojure's putative moribundity has been caused by macros, you're proposing to exclude it based on an irrelevancy.
bip-rs is another cool project, but since it's using futures it's not easily usable for synapse. If a functional client does come out of the project it'll be interesting to compare it.
&gt; One thing which multiple people have told me (over a time span of several years, actually) is that *Rust is Special* in that everyone uses crates for everything. Thus there is no point in having any sort of Rust support, the only true way is to blindly call Cargo and have it do everything exactly the way it wants to. The advice to just use Cargo is not that "*Rust is special*". It's because duplicating everything Cargo does would be a huge amount of effort for little to no gain. People tend to react poorly to others trying to build Rust in not-Cargo because it just serves to fragment the ecosystem (exceptions like embedded or to-the-metal projects aside, which might not be doable entirely with Cargo). &gt; The script currently only works for itoa, because crates.io does not seem to provide a web API for queries and the entire site is created with JavaScript so you can't even do web scraping easily. ... but if it's generated from JavaScript, then it's probably [generating the page from structured data which has to come from *somewhere*](https://i.imgur.com/N9Vg5lj.png)... Also, crates aren't built from the repository. They're built from `.crate` archives uploaded to `crates.io`. A crate's repo might not be publicly accessible, or might not be in Git, or might not have a direct mapping from version number to commit. &gt; [...] these scripts are not in general possible to convert automatically. So... run them? You can't just *not* run them, considering that some projects use them for code generation. I've used them in the past for generating resources to feed to the linker. 
If you have never made games before, then you'll be trying to learn rust, vulkan and how to build games all at the same time, which is too much. Concentrate on learning one thing at a time it's much more efficient that way.
:( That's unfortunate.
What's wrong with git?
That is such a loaded question I don't even know where to begin. Regardless this isn't the best place to discuss pijul vs git as that isn't what this post is showcasing. I didn't write pijul. I have never used pijul. I am not part of pijul or rustsync.
Postprocessing the binary seems like legitimate reason enough. Hooking into packaging systems like wix (MSI windows installers), rpmbuild/debuild/opkg-util (linux installers), assembling flatpaks, etc.
So, I know SIMD instructions are cool stuff liked packed adds and such, good for matrix math to my understanding... Will the most common usage be linear algebra, math libraries, 3D rendering, AI, stuff like that? Or is SIMD used in other areas, and why? Is there a benefit to using specialized SIMD stuff these days instead of doing CUDA stuff, or is they for completely different use cases? I had a lot of fun playing with SIMD stuff in assembly in the past, but that was just playing around, nothing serious or professional. Just wanted to test them out for reducing a 4x3 matrix into echelon form, and wanted to see if I could use SIMD and beat the gcc compiler with hand written assembly. 
It sounds cool, but the last time I looked at it they wouldn't use PGP keys (https://pijul.org/manual/signing_patches.html) and even pushing didn't support SSH keys (https://discourse.pijul.org/t/how-to-make-ssh-auth-work-with-pijul/114)
A terse intro to Rust.
Another Vulkan binding is the [ash](https://crates.io/crates/ash) crate, it is an unsafe library that is very very close to the Vulkan C API.
I'll continue working on getting my Ironman and Hulk bots to smash other super heros, and perhaps avoid doing a weird shuffling dance. Watch my antics on Twitch. https://www.twitch.tv/mortoray
[mutagen](https://github.com/llogiq/mutagen) now is on crates.io and has a new test runner, which will be published later this week. Apart from that, I'll be implementing the opportunistic mutations I blogged about recently, and fix a few things. Also TWiR, and perhaps I'll get to implementing a CAS benchmark for Rust's AtomicUsize.
That's right, it's just a library at the moment, I'll add a CLI soon. Thanks for the suggestion!
Versus CUDA: what you don’t seem to have considered are that GPU numerics are (traditionally) floating point rather than integer. And also GPU hardware is more varied than CPU hardware...
rustsync uses more a modern hash function, for instance (blake2 instead of md4), and the protocol is meant to be defined by serde. The goal of this library is to make it easy to write other protocols that require fast file synchronisation, rather than to reimplement rsync. I wrote it because we needed it in Pijul, to synchronise logs (for instance in the pull and push commands).
Do you have a link?
&gt; Intro classes for computer science rely on graphics to keep newcomers interested and provide immediate feedback. ... really ? When I did my cursus at no point any course did any graphics, it was all in project-time. Courses were always focused on algorithms, data structures, language semantics, etc.
Just released version 6.0.0 of [BrewStillery](https://github.com/MonkeyLog/BrewStillery). This was a massive rewrite with cleaning up code, breaking up large functions, and adding a crap ton of testing. If anyone wants to give it a read through, any ideas for improvement are welcome. Cheers!
&gt; linear algebra, math libraries, 3D rendering, AI, stuff like that Sort of, but this generalizes to anything that has an inner loop doing a small number of operations on each element of a slice of primitive types. `stdsimd`’s docs show encoding a byte string in hexadecimal. Another example is the `regex` crate’s specialized code for "find the next occurrence of this specific byte". `encoding-rs` has "find the next non-ASCII byte", for decoding ASCII-compatible encodings like UTF-8.
Git is great in many ways, but doesn't really know how to merge changes correctly. Even worse, it doesn't know how to "unmerge" changes. It does try to merge branches, though, but doesn't give you any reliable guarantee on the result. For the unmerging part, it doesn't even try and you have to rebase your changes yourself, still without strong guarantees. Pijul uses a patch algebra to do these things, where patches have several nice properties: - Any two patches A and B either commute, or one of them explicitly references the other. This means that repositories behave as *sets of patches* rather than as a linear history. - Patches are associative: applying C on top of a repository that has A and B will yield the same result as applying B and then C on top of a repository that has A. Git could have this, but unfortunately, it doesn't. - Patches have inverse patches. We also have a sane internal representation of conflicts, so that all conflicts don't have to be solved immediately after merging. I should add that Pijul is still quite unstable, and at this particular moment is in the middle of a giant protocol change.
&gt; Or is SIMD used in other areas, and why? Is there a benefit to using specialized SIMD stuff these days instead of doing CUDA stuff [...]? SIMD is reasonably versatile if you have a lot (but not a Google-lot) of primitive data to process. I've had pretty good results [counting bytes and characters](https://github.com/AdamNiederer/bytecount/commit/e807753b9f4e16eec5b2c4845eb5373f1e286292), [filtering colors](https://github.com/AdamNiederer/bgrx-to-grayscale-benchmark.rs/commit/0d3aa7c9e94466e2ffb33e326466d8ff17f2d8ce), and computing [hamming distances](https://github.com/AdamNiederer/hamming/commit/283d590b47f276102d48ef3ced8fa72006119033), and [TCP checksums](https://github.com/AdamNiederer/smoltcp/commit/765b1647bf53a0850314b9271c2e8f4006982c1d). Most of these functions wouldn't be worth offloading to the GPU (unless you're Google) because of synchronization latency, driver overhead, context switches (unless Mesa has a way around that?), etc.
... well, my introduction in school was learning emacs and bash. Only a few people had programming experience, most had none at all. To each his own :p 
Not really. 3rd party subcommands could handle post-build scripts in the root crate, but not its dependencies.
Ha, yeah. Computer science is still young-ish and inconsistent... Some professors have wildly different goals for the same class. (We all use Vim, though. XD )
LLVM is written in a heavily object-oriented way. There's no naive porting to Rust.
If it's about the community, then you should not just look at the question (anyone can come in and ask questions), but also at the response to it. And the response here is mostly the entirely reasonable "you're in way over your head, why not contribute to rustc or Cretonne instead?"
But couldn't all that be done by a specialized cargo tool? E.g. "cargo msi"?
Neat post, thanks! Incidentally, generally you don’t implement Into yourself; you implement From, which then implies Into.
I'm working on the [implementation](https://github.com/TatriX/realworld-rust-rocket) of the ["The mother of all demo apps"!](https://github.com/gothinkster/realworld) It shows how one can build a Medium.com clone with Rust, [Rocket](http://rocket.rs/) and [Diesel](http://diesel.rs/). It's mostly finished but still it needs more work to be done.
`println!` (well, `format!`) is basically a language item anyway. It just delegates to an intrinsic. There's currently no way to parse a string literal at compile time in pure Rust, or even specify that a macro argument must be a string literal.
It's referring to: https://doc.rust-lang.org/nightly/std/simd/index.html The `std::simd` module is as unstable Rust as you can get. Feel free to experiment with it and feel issues / give feedback here: https://github.com/rust-lang-nursery/stdsimd But don't expect the code you wrote last week with it to compile this week with it.
&gt; Faster will be migrating from stdsimd to this ASAP, I don't think this will be a good idea until the portable packed SIMD vector types settles down. You can control which version of `stdsimd` people use, but you won't be able to control which Rust version people use and things will break.
Unless you have a pointer to the node where you want to splice stored somewhere already. In that case the cost is zero.
I agree: needing O(1) splice above everything else is a pretty weird requirement to have. I just mentioned it because that's the only situation I can think of for wanting a doubly-linked list. The moment I not only need O(1) but also something else, like reasonable iteration speed, there are many data-structures that are better than doubly-linked lists. 
There seems to already be an rfc pr which go closed: https://github.com/rust-lang/rfcs/pull/1777 Or is this something else ?
Nothing would stop such a tool to work on dependencies. Especially code signing _is_ usually a toplevel concern. Also, of all the the things you mention here, I _do_ want to know that the dependencies to postbuild things. The problem is that usually, all these tasks lead to weird interactions between them.
Of course, post build stuff could be handled by a separate script, however my point here is that, since we already have a "build", the DOTADIW philosophy has already been broken in this regards. I'd even say that, in order to respect DOTADIW here, since cargo is already providing external script hooks, it should probably give more options than pre_build (e.g. post build and possibly even allow for some hooks during the build process)
While I see the appeal, atomic reads/writes are expensive. I can't imagine that a doubly-linked lists with its node pointers stored in atomic variables performs well in practice.
&gt; To a first approximation, if you see the use of xmm or ymm registers in your generated code (on x86) That's not a good approximation. Nowadays, all single-value floating point operations are typically performed in these registers as well. The old x87 instruction set is pretty much unused.
Yes, as I've dug into the code of LLVM I've come to realize that. As it stands right now, I'm working through the code and trying to come up with a reasonable plan of how to proceed.
I agree. The consensus seems to be that this is more to bite off than one can chew. This is the kind of insight I was hoping to get by asking the question and I thank the community for the feedback. That being said, I think I'm still going to toy with the idea and see if I can perhaps make some headway. Perhaps create a crate to have Rust versions of all the internal IR classes/structs and a crate to parse IR into that and re-serialize as bit code. Then, see where that leads.
I mean, it's pretty obvious he's the author since this is on his website.
easy to solve, just say Faster requires Rust version xxx or later, like any other library using specific functionality
I'm researching how to simplify the [`select_loop!`](https://docs.rs/crossbeam-channel/0.1.2/crossbeam_channel/macro.select_loop.html) macro in [`crossbeam-channel`](https://docs.rs/crossbeam-channel). The idea is to make the following changes: * Get rid of the silly implicit loop and rename the macro to `select!`. * Unify cases [`would_block`](https://docs.rs/crossbeam-channel/0.1.2/crossbeam_channel/macro.select_loop.html#stop-if-all-operations-would-block) and [`timed_out`](https://docs.rs/crossbeam-channel/0.1.2/crossbeam_channel/macro.select_loop.html#selection-with-a-timeout) into a single `default` case (like in Go). * Change case `disconnected` to `closed` accepting a `Sender` or `Receiver` as the argument. * Use `compile_error!` to improve the error messages. This is a huge improvement, but selection would still be a little bit complicated. One remaining problem is that you have to remember writing `closed` cases. Usually, users would prefer to panic by default if any channel in a `select!` is closed. --------- One idea is to strip down the whole API (even methods on `Sender` and `Receiver`) by following the philosophy of Go channels. Overall, this would make things quite a bit simpler: impl&lt;T&gt; Receiver&lt;T&gt; { // Receives a message. // // Returns `None` if the channel is closed. fn recv(&amp;self) -&gt; Option&lt;T&gt;; } impl&lt;T&gt; Sender&lt;T&gt; { // There is no error (Yay! - no more `.ok()` and `.unwrap()` boilerplate). // // Panics if the channel was *explicitly* closed. fn send(&amp;self); // Only senders can close the channel. // // Closing is also automatic: droping all senders closes the channel. // However, dropping all receivers does *not* close the channel. fn close(&amp;self) -&gt; bool; } // A demonstration of the new selection macro. // // Much simpler! Cases `would_block`, and `timed_out` are unified into `default`. // There is no `disconnected` case anymore, but `recv` receives an `Option&lt;T&gt;` indicating // whether the channel is closed. select! { recv(receiver, msg) =&gt; { match msg { Some(msg) =&gt; println!("received {:?}", msg), None =&gt; println!("the channel is closed"), } } // This panics if `sender` was previously *explicitly* closed. send(sender, "hello") =&gt; { println!("sent a hello"); } default(Duration::from_secs(10)) =&gt; { println!("timed out"); } } // Non-blocking receive is pretty easy to do using the new macro. // This is very similar to the Go syntax. select! { recv(receiver, msg) =&gt; println!("got {}", msg), default =&gt; {} } The downside of this approach is that dropping all receivers does **not** close the channel as is the case for `std::sync::mpsc`. This idea is called the principle of *unidirectionality*, meaning the receiving side cannot give any kind of signal to the sending side. Similarly in Go, only the sender side can close the channel, which is a deliberate design decision. Russ Cox explains the decision [here](https://github.com/golang/go/issues/14601#issuecomment-191421028) and [here](https://github.com/golang/go/issues/11344#issuecomment-117862884). ------------ In summary, there are two ways forward and I'm torn between them: 1. Keep the `std::sync::mpsc`-like design and implement a more complicated `select!` with explicit `closed` cases. 2. Take a more opinionated stance and implement fully unidirectional `chan`/Go-like channels with simpler `select!`. Can you help me decide? Reply here or in [the GitHub issue](https://github.com/crossbeam-rs/crossbeam-channel/issues/39#issuecomment-370294595). Or, if you have any other thoughts or comments on `crossbeam-channel`, let me know! :)
`stdsimd` also provides a couple of CUDA intrinsics. The TL;DR: is that CUDA and SIMD are orthogonal to each other.
I've no doubt that varargs can be implemented, but I think the needed reconciliation is just as much psychological; that adding this feature to the language can actually carry its weight compared to the complexity it adds to both the language and the compiler. I have no idea how much work would be required, but I think the Rust team has made the right call in just implementing some common cases as macros and then move their efforts to other areas of the language, for now at least. And yeah, the RFC states the [default arguments](https://github.com/rust-lang/rfcs/blob/master/text/2091-inline-semantic.md#default-function-arguments) as an alternative, but historically it's not been possible to reach consensus on a design for them, making the point moot.
This rfc is in the same spirit: https://github.com/rust-lang/rfcs/pull/2136
There are plenty of crates for that already though.
Right. After I made that comment, I looked it up and saw that that was the case.
&gt; but it doesn't show the documentation. That's true. Or just "No description available" for definitely known methods (like `.unwrap()`). It would be really helpful if documentation was available inline. 
Say I need to strip/rename some symbols of the generated binaries for my crate before it's used for "reasons". I can do this as a post build step independent from cargo. If I upload it to crates.io, my users will need to manually do this as well or hopefully something will just fail (worst case, their build will succeed and they might get undefined behavior). If cargo would support post build scripts, they wouldn't even have to care about any of this.
I don't understand something, I think that the `cfg!` macro was a compile-time condition [as the documentation say](https://doc.rust-lang.org/std/macro.cfg.html). But it seems to be a runtime evaluation. Is the `cfg!` doc wrong or miss-documented ?
I'm pretty sure all 64 bit reads/writes are atomic in x86_64. You just need to worry about memory barriers. They perform extremely well in practice. Certainly better than anything with a mutex.
I think a more careful read of [static CPU feature detection](https://doc.rust-lang.org/nightly/std/arch/#static-cpu-feature-detection) and [dynamic CPU feature detection](https://doc.rust-lang.org/nightly/std/arch/#dynamic-cpu-feature-detection) will answer your question? If not, feel free to ask more!
Haskell devs are too lazy for that.
You could use [https://github.com/casey/just](just) and define a build recipe like so: ``` build: cargo build # extra steps go here ```
My most recent use-case was taking an .so produced by a workspace subproject and putting it into another subdirectory to be found by the application. So, basically to make `cargo run` work.
&gt; And in fact it is not clear what is the point to duplicate cargo functionality? I'm not a build systems expert, but for one thing: if you're compiling 10 files from C, 10 from Rust, and 10 from some other language, and have four CPU cores, one single master controlling what files go to which core seems like a good thing. Having a single master for multiple languages also seems to be a good thing when distributing compilation across multiple machines. 
License? Windows support?
I am a little bit confused. You are talking about `cargo install` scenario here, and not about invoking post-build scripts for **dependencies**, right? That is, do your users use you crate by `cargo install`ing it, and *not* by adding it as a dependency into Cargo.toml? If that's the case, than I think it's a slightly orthogonal problem of `cargo install` not being general enough. And that's a tough problem indeed... Here's an issue for discussing the general problem of packaging Rust applications: https://github.com/rust-lang-nursery/cli-wg/issues/8
&gt; You are talking about cargo install scenario here, and not about invoking post-build scripts for dependencies, right? No. &gt; That is, do your users use you crate by cargo installing it, and not by adding it as a dependency into Cargo.toml? My users add my crate as a `[dependencies]` in their `Cargo.toml`.
[this comment](https://github.com/rust-lang/rfcs/pull/1777#issuecomment-300963785) is important. That RFC was very short and underspecified, and that's the main issue. A lot of code members have mixed feelings about post build, but it's hard to discuss something abstract in much detail. If you have a library, the prebuild script is obvious. It does things to prepare the library's own code for compilation. But, what does a post build script have access to? In a binary, it would be sensible to give it easy access to the final output, but does a library's post build script run after the library is done building, or after the entire application is done building? What's the point of allowing your dependencies to execute arbitrary code in the postbuild step? I am not anyone important in the Rust community, but I think a well-specified RFC that attaches postbuild scripts only to [[bin]] target blocks in the Cargo.toml file would have a greater chance of success. It would be clear that lib dependencies don't need to run postbuild, and it would be clear that each postbuild scripts runs when the bin target it is attached to is completely built. Making such a fundamental change to cargo requires a well-written RFC and someone who is willing to champion it -- to respectfully answer any concerns that are presented and provide clear reasoning. That RFC provides a wealth of information for someone looking to prepare another attempt. It also points out that cargo subcommands work for this, although I consider that to be more cumbersome than a postbuild script, since anyone who wants to develop on your project is going to have to explicitly install and use your subcommand.
This would be actually a very cool idea to implement. An online website for groups to discuss RFCs using the features you mentioned. 
Hm, fascinating! I was under impression that you can't do anything meaningful with intermediate steps of build-process, because the rlib format is not stable... Is the code for your crate available anywhere? I am afraid I might be misunderstanding what exactly does "renaming symbols" means in this context? Is your crate a `cdylib`? 
SIMD instructions can also do parallel comparisons in a single instruction. For example, [Adaptive Radix Tree](https://db.in.tum.de/~leis/papers/ART.pdf) (page 4, under **Node16**) uses them to find a specific value in a byte array. This is faster than running a for loop or doing a binary search: &gt; This node type is used for storing between 5 and 16 child pointers. Like the Node4, the keys and pointers are stored in separate arrays at corresponding positions, but both arrays have space for 16 entries. A key can be found efficiently with binary search or, on modern hardware, with parallel comparisons using SIMD instructions.
In a language where compile-time and run-time are not so clearly delimited, one could argue that this isn't terribly different from a function call. And the advantages for language extensibility are huge. You can literally build a full-spectrum dependent type system in Racket's macro system.
I get an error from that as well because I am pushing type string not type str 
Got it after converting. let temp: &amp;str = &amp;*input; Thanks!
Even better, thanks!
Honestly, the case outlined here is an argument for a `cargo-osx-bundle` or similar. Code signing happens after all deps are linked together anyways so that step only ever needs to be done on the app. Likewise it's probably complex enough that it wants some input parameters (codesigning identity etc.).
Will this be used in Servo/Firefox also?
See edits to OP
See edits to OP
See edits to OP
See edits to OP
See edits to OP
See edits to OP
See edits to OP
See edits to OP
You're right. The Rust community seems to be awesome overall. I'll be definitely looking more into Rust. 
See edits to OP
Lexical lifetimes strike again! :D First, I simplified your code a bit to a smaller example that still shows the same problem ([playground](https://play.rust-lang.org/?gist=ee773e5fe0e80b2cd6f7724ebc1ce27b&amp;version=nightly )): struct Container { id: u64, children: Vec&lt;Container&gt;, } fn find&lt;'a&gt;(id: u64, container: &amp;'a mut Container) -&gt; &amp;'a mut Container { for child in &amp;mut container.children { if child.id == id { return child; } } container } I made `find` function do just one step - the recursion has nothing to do with your problem here, so I removed it to reduce the clutter. Now `find` returns reference to the first child with given id, and the original container if no such child exists. So let's take a look at the types. We have a container that is `&amp;'a mut Container`. Note the lifetime `'a` - lifetimes declared on functions must live for at least as long as the function is running, which is going to be important here. We have to return `&amp;'a mut Container` - a reference with the same lifetime. When we borrow `container.children` for the for loop, the lifetime of that borrow will be some lifetime `'b` such that `'a: 'b` (`'a` outlives `'b`, because derived data obviously cannot outlive the thing that owns it). Yielded references also have that lifetime, so `child` has the type `&amp;'b mut Container`. Because we sometimes return `child`, it must live for at least the lifetime `'a` - our function signature promises a reference that has this lifetime. So we also have `'b: 'a` (`'b` outlives `'a`), and thus conclude that `'a = 'b`. So we do know that `&amp;mut container.children` is borrowed for lifetime `'a`, which goes at least to the end of the function. Therefore the compiler marks this: 10 | for child in &amp;mut container.children { | ------------------ first mutable borrow occurs here ... 16 | } | - first borrow ends here But on the 15th line we have another borrow, that simply tries to use container - but we know that it is borrowed up to the end of the function, so we get the "multiple mutable borrows" error: 10 | for child in &amp;mut container.children { | ------------------ first mutable borrow occurs here ... 15 | container | ^^^^^^^^^ second mutable borrow occurs here 16 | } | - first borrow ends here Non lexical lifetimes will solve this issue (this does compile on nightly with `#![feature(nll)]`). Without them, you can use this solution, but TBH the code looks pretty shitty: [playground](https://play.rust-lang.org/?gist=1f9ee397c7b04d7407175946e325340c&amp;version=nightly). For more examples and explanations of lexical lifetimes, their limitations, and how non-lexical lifetimes will fix that you can read nikomatsakis blog posts about them ([first one here](http://smallcultfollowing.com/babysteps/blog/2016/04/27/non-lexical-lifetimes-introduction/) - and this post actually has an example which is basically the same as yours).
Great little intro to Rust! Abrupt ending though. Was this intended or is something missing?
`fn _find_mut` has one elided external lifetime `fn&lt;'a, I: Iter&lt;Item=u64&gt;&gt; (Peekable&lt;I&gt;, &amp;'a mut Container) -&gt; (&amp;'a mut Container, bool)` Because you return a reference derived from `curr_container`, the borrow at 16 must live as long as 'a. I'm 98% sure this is an artifact of the lifetime typing formalism. It's *not* a problem with borrowck, so I would guess that this is excessively difficult to write in clear safe Rust. It should be sound to walk a `*mut Container` raw pointer to the desired return container. (Your algorithm is non-mutating so you don't need to worry about breaking the `&amp;mut` aliasing.) This will also allow you to write an iterative algorithm too. But it does need `unsafe`. 
Working on implementing a Travis CI like secrets management feature for [Habitat Builder](https://habitat.sh)
Yes, it does - thank you! I'll need to experiment a bit more with this. I've seen some complaints here and there about kcov as it pertains to using it with rust, but I don't have enough experience with it yet to make a judgement for myself.
Nice, it would probably work the same for `From`.
Oh wow that's brilliant. I hadn't remembered NLL (flow-aware borrowck?) can make that deduction. - if the function exits through the first return, the borrow of `curr_container` outlives `'a` - if the function exits through the second return, the borrow must end when `curr_container` is moved from. - this is valid because the move isn't reachable from the first return
Rendering engine is so complicated, isn't it?
Ever run `cargo update`? Notice it pulls a massive index file called `crates.io_index`? That's what you want to use to grab crate info.
Thanks for the info, I was wondering what had happened!
This makes complete sense, though it would be nice to have a feature that could switch `faster` to `std::simd`. I think using `faster` in this manner would be very useful in refining `std::simd`. But yes, it seems prudent to not abandon the existing behavior just yet.
Someone using habitat. In the wild!? I could never get used to it. Feels too much like Google cloud stuff how you do all the work in a "VM" and then link it all in. Just feels like too much boilerplate.
i found that qbe written in rust could be alternative solution: https://c9x.me/compile/doc/llvm.html
Last week was the first release of [rust-http-server](https://github.com/asoderman/rust-http-server). I just finished setting up travis to deploy future releases (shoutouts to [trust](https://github.com/japaric/trust)). Now I'm going to start on http request forwarding, and supporting more config filetypes. The latter should be easy with serde. This is my first *real* rust project if someone could review some code that would be great. Also feel free to contribute, report a bug or request a feature.
RUST SERVER RULES 1: Please follow these rules, get to know your nabors, trade with them, and when the week is up your on your own. 2: One week after server wipe there will be a truce for all Players. If you don't follow this you will be baned until the truce is over. 3: You are aloud to shoot and kill each other but there will be no raids untill the apointed time. 4: Only 10 man groups are alowed. 5: No Allies temporarily
Wrong subreddit: You're looking for https://www.reddit.com/r/playrust/
&gt; That's exactly what I would call a bad macro. &gt; Macros that would do that would pretty much shown the door on day one This is an entirely bizarre thing to think. The whole point of macros is to do things which ordinary functions can't - to create identifiers, to interpret their arguments as something other than a Rust expression, to do strange control flow, etc. We have them because we need to do that, and we make them stand out because they can do that. Having macros which can't (by convention) do things that functions can't and don't look different to functions is no better than not having macros at all!
Interesting. I'll need to drill down on this a little more.
Would it make sense for stdsimd to grow a configuration option that makes it use std::simd? A library author could depend on a specific version of stdsimd, and if an end-user had a suitable nightly, they could flip it over to using std, and if not, they'd use stdsimd's implementation. Maybe that would involve an infeasible amount of shimming. Or be pointless.
... Well, technically I made it work in python once with some silly regexes, evals, and stack reflection. Then I looked at it in disgust and threw it all away.
I worked on [directories](https://github.com/soc/directories-rs), a tiny library with a minimal API (2 structs, 4 factory functions, getters) that provides the platform-specific, user-accessible locations for storing configuration, cache and other data on Linux, Windows (≥ Vista) and macOS. I received a lot of good feedback from the Rist community and further simplified the API: https://github.com/soc/directories-rs/pull/14
The license is Apache 2.0/MIT, as indicated [on crates.io](https://crates.io/crates/rustsync). Nothing in this library is windows-specific, but I've not tested it.
&lt;3 That's the perfect testimonial for that video!
It does demonstrate it, but I agree you have to be familiar with how rsync works in order to see that. I'll try to think of a better example later. Suppose you want to download version B of a file, and you already have version A. What the rsync protocol does in this case is (1) to produce a "signature" of A (the `source` variable here), (2) to upload the signature only, and (3) let the remote machine compare that signature with version B, producing a "comparison" (the `comp` variable) and (4) to download just the comparison, and to "restore" version B, using version A and the comparison. Is it clearer?
Function calls aren't inlined (semantically) right into your code, among other things.
Time spent optimising code at runtime is time not spent running optimised code.
Ending on an upward inflection made it sound like an upload error cut off the end of the video. I can imagine that working for a series of videos which are meant to be into each other.
Finished API docs for the [gfx-rs HAL API](https://github.com/gfx-rs/gfx/pull/1862). It's certainly not perfect, but hopefully a fairly good start. I really need to get [ggez 0.4.1](https://github.com/ggez/ggez/issues/289) out the door, since there's been numerous minor bugfixes and patches by various awesome contributors, but life has been really irritating. Also, for the sake of experiment, I've set up [Patreon](https://www.patreon.com/icefox) and [Liberapay](https://liberapay.com/icefox) accounts with the intent of funding my various endeavors of "This needs to exist, so I might as well make it". I doubt this will ever make significant amounts of money, but maybe someday it'll be enough to fund going to more Rust events...
I’ve been working on my microkernel, [Pebble](github.com/pebble-os/pebble)! Starting system calls (very basic atm), which I’m going to build into libstd at some point to provide std support for Rust programs running on my OS
Sure, why not? :)
In general, SIMD is good when you have enough data that processing it in parallel is useful but not enough to mitigate the overhead of shuffling it over to the GPU. SIMD basically gives you a nice performance boost for GPU-like tasks that would otherwise be impractical to actually run on the GPU.
Could Cretonne reasonably evolve into the static back-end for Rust at some point completely replacing LLVM in the Rust tool-chain? So the Rust compiler would Emit Cretonne IR which would then compile to Machine Code directly.
Or change the name and the dependency at the same time.
You're a fucking star, I finally know why people are raving about nll Guess it's `unsafe {}` &amp; `*mut` from here, eh?;) Any news on stabilisation of NLL? What's holding it back at the moment, just waiting for people to try it out? Thanks again, man!
[removed]
That'll be fixed with procedural macros, though.
[removed]
I've used `tokio-uds` before in a test suite. [See the code starting here](https://gitlab.kitware.com/utils/ghostflow-director/blob/master/src/handlers/test/service.rs#L681). It doesn't need to deal with `Future` across a function boundary, but it does simple combinations to get the desired effect.
Your link needs a http(s):// prefix.
Huh, worked for me, but I’ve added it anyways; thanks!
The problem with varargs isn't their variable length. If that were all, then Rust could just translate `varargs_call(1, 2, 3)` into `varargs_call(&amp;[1, 2, 3])` and it would just be syntactic sugar for passing slices as arguments. The problem with varargs is that, particularly in the `format!()` context, they have *variable types*. That means you either rely on reflection or use varadic generics.
&gt; This is not a drawback. In fact, people avoid implicit conversions partly because there is too little explicitness. I think this is true in general, but that doesn't apply to this situation, where you have a value inside a method which was explicitly requested to be convertible. Rust, like Scala, has answered this question with both yes (`Deref&lt;T&gt;`) and no (`Into&lt;T&gt;`), so I think it is a valid point to consider. Scala's context bounds and `Into[T]` are like Rust's `Into&lt;T&gt;`, while Scala's view bounds and implicit conversions are like Rust's `Deref&lt;T&gt;`. One important question to consider is whether the existence of both `Deref` and `Into` in Rust had an impact on the ratio of explicit vs. implicit mechanisms, compared to Scala's implicit conversions and context bounds.
You will notice my fake varargs syntax was `T ...`, i.e. with all the types the same. Sure, maybe varargs with non-uniform types would also be nice, but we might as well implement the first conservative shouldn't-be-controversial one and then decide if we want more. `format!` is always going to be special because it needs variable specified types. It's also not really "varargs" because it's "fixed number of args in the first argument". In other words `format!` is weird. If however we imagine a dynamic `format` via varargs that did run-time parsing of the argument string it could look like fn format(fmt: &amp;str, args: &amp;Display ...) { ... } (We'd probably want a `DynamicDisplay` trait that dealt with different format types and such... but not really the point) Just to be clear, in the conservative version I'd be disalowing `args: impl Display...`, since that's probably really some sort of syntactic sugar for `fn format&lt;A1, A2, A3, A4 ...&gt;(fmt: &amp;str, args: (&amp;A1, &amp;A2, &amp;A3, ...)` or maybe `fn format&lt;A&gt;(fmt: &amp;str, args: A...)` and we don't need to deal with getting that right in the first iteration since it's a backwards compatible addition.
&gt; Any news on stabilisation of NLL? What's holding it back at the moment, just waiting for people to try it out? There are some bugs still, for example, it can't *quite* compile the compiler with it on. They're looking for help!
Yes, either that or unsoundness in unsafe code (yours or a library).
True wrt `Deref`. &gt; One important question to consider is whether the existence of both Deref and Into in Rust had an impact on the ratio of explicit vs. implicit mechanisms, compared to Scala's implicit conversions and context bounds. Scala devs are far enough removed from the JVM bytecode that only a little performance thought is put into implicit conversions (most allocate, e.g. when wrapping in another class). I guess the difference is I trust Rust devs to have essentially zero-cost derefs because the language encourages forethought.
No, it looks like we've got some next-level mis-posting to /r/rust. This is an off-topic post intended for /r/playrust, posted to /r/rust instead. I'm in awe.
As part of the migration, I'm going to fork `std::simd` and pin faster to the fork until an RFC goes through. Depending on the stabilization schedule and the contents of the cross-platform RFC, I might keep the fork around and use that as my base for the forseeable future. Since `std::arch`is already RFC'd and mostly done, I should be able to preempt large changes.
&gt; In other words, the question boils down to "How do I tell the Rust linking phase 'Trust me. When this code is reached, the symbol will be there.'?" Yes. I can load the primary C++ binary just fine to see symbols, but I should be able to see them fine without doing so. But I still need a way to tell Rust to ignore and compile...
Question: Won't the postgres queries block the event loop in this example?
&gt; cargo install not being general enough. Indeed. There's no support (AFAIK) to install headers and other development files (for crates exporting a C ABI), license files, readmes, documentation, manpages, etc. Though that can of worms might best be handled by a simple `Makefile` which is used for installation, it'd still be nice to have such things work from `cargo install`.
Ah okay. Also that was unintentional because I have rewritten the tokenizer several times while writing the article because I had a totally different structure before, on my own, so I suppose I made that type in the process. Thank you for correcting me and I will go correct the article.
It is entirely possible that someone assumes I am Peter with the username Phrohdoh on reddit so I'd rather clarify up-front than have someone confused down the line.
How does Phrohdoh look anything like Peter? Oh wow, they both start with a P. You must be Peter then.
AFAIK it's common courtesy to not leave ANY doubt about this kind of thing.
This sounds like using panic! as a normal exception (which should be handled) rather than as a runtime exception (which should not be handled) [to borrow terminology from the Java world]. Panic! should not be called for something that is recoverable. Result&lt;T,E&gt; is for returning errors.
Oh really? Am I assuming that? I was completely unaware! Thank you for bringing forth these repressed beliefs I held doctor.
Author here. I'd be quite surprised if `language-rust` couldn't parse `parser-haskell`. I wouldn't be so surprised at the converse though - Haskell is _much_ hairier to correctly parse than Rust, especially GHC filled Haskell. That's one of the things about Rust that I love - I never have to debug weird whitespace-related parse errors. 
This was actually the original use case for this library. I've since come to the conclusion that Corrode is insufficient for automated C to Rust translation and, as you've observed, the project seems pretty dead. I haven't stopped working on that problem though... more on that in the coming year. :)
I am downvoting you because I don't think the tone of this comment, especially your edits, is acceptable [particularly] in this community.
I seem to be having trouble with `panic=abort` in `Cargo.toml`, in that it makes backtraces short and uninformative. For example: thread 'main' panicked at 'called `Option::unwrap()` on a `None` value', /checkout/src/libcore/option.rs:335:21 stack backtrace: 0: std::sys::unix::backtrace::tracing::imp::unwind_backtrace at /checkout/src/libstd/sys/unix/backtrace/tracing/gcc_s.rs:49 1: std::sys_common::backtrace::print at /checkout/src/libstd/sys_common/backtrace.rs:68 at /checkout/src/libstd/sys_common/backtrace.rs:57 2: std::panicking::default_hook::{{closure}} at /checkout/src/libstd/panicking.rs:381 3: std::panicking::default_hook at /checkout/src/libstd/panicking.rs:397 4: std::panicking::rust_panic_with_hook at /checkout/src/libstd/panicking.rs:577 5: std::panicking::begin_panic at /checkout/src/libstd/panicking.rs:538 6: std::panicking::begin_panic_fmt at /checkout/src/libstd/panicking.rs:522 7: rust_begin_unwind at /checkout/src/libstd/panicking.rs:498 8: core::panicking::panic_fmt at /checkout/src/libcore/panicking.rs:71 9: core::panicking::panic at /checkout/src/libcore/panicking.rs:51 10: &lt;unknown&gt; at /checkout/src/libcore/macros.rs:20 Note how there's no mention of any of my applications' functions. Without `panic=abort` I get the usual large backtrace, even including parts of Gtk's call stack (it's a Gtk app). Is this expected? Any way to keep full backtraces?
I'm just getting started. I have two very beginner questions, both of which are in the comments here: fn main() { struct Person&lt;'a&gt; { // why is the lifetime notation needed here? name: &amp;'a str, // why do you need to take a reference (&amp;) here? age: u8, } let bob = "bob marley"; let age = 38; let bob = Person { name: bob, age: age, }; println!("Hello, {}, You are {} years old\n", bob.name, bob.age); }
Nice intro. It ended abruptly! I wanted it to go on at least twice as long. Looking forward to future videos. 
Backtraces use unwinding which is disabled with `panic=abort` - that's the point of the flag. If you want backtraces, you need `panic=unwind`. The reason the stdlib still gives a backtrace is because libstd is compiled with `panic=unwind`.
Thank you so much for this, very informative. If I may ask, does 'go to definition' work for you. I am specifically referring to rust source code, for example 'String' and so forth.
To be clear, the only way to trigger a panic is by calling `.close()` and then `.send()`. Note that dropping all `Sender`s automatically closes the channel. And if you drop all senders, there's no danger of triggering a panic because you can't call `.send()` anyway. Maybe I should simply remove the `.close()` method from the API, but not sure. In that case, there'd be no panics at all.
That explains it then, thanks.
Thank-you for your comment. Although we expect it to be primarily Japanese, fluent English speakers will be in attendance. Hope you can join.
A common solution is to have a `/static` prefix for such static resources. This way you would access `/static/css`, `/static/js` to the desired location. Note that this requires you to change the actual URLs provided.
I know it's not a big lib, but: ✔ A complete readme ✔ API docs w/ examples ✔ 100% test coverage ✔ dedicated examples dir ✔ Linux CI ✔ Windows CI Someone give the author a badge! This is the goal we should all strive to achieve!
Does anyone know if WebRender's roadmap expects it to land in Firefox this year? The newsletter updates are great, but it's hard to get a big picture of where the project's at.
Thank you! &lt;3
Ah, I found a different example which shows what you are explaining. This is making more sense now. fn main() { struct Person { name: String, age: u8, } let bob = Person { name: "Bob".to_string(), age: 28, }; println!("Hello, {}, You are {} years old\n", bob.name, bob.age); } 
I've been wanting to work with rust for a while now, and finally got the ~~flu~~ chance a couple of weeks ago. I started an [HTTPie](https://github.com/jakubroztocil/httpie)-like cli: https://github.com/mark-burnett/ht
Resting, not dead.
On the flip side of this coin, it could also be a potentially great way to learn. Thrown into the deep end, faced with nothing but problems and shit that doesn't work. Different strokes for different folks ;-) But I mostly agree, for most people this could be a demoralizing exercise in futility. Especially if there's no clear goal in mind for which the engine will be used for (as is so oft the case I've heard).
&gt; [...] and had to use to_string() to get it to work. Naturally. `&amp;str` (the type of string literals) and `String` are different types.
Still chugging along at my [gameboy emulator](https://github.com/tobywhughes/RustBoy). It can render a few things now, such as background tiles. I'm hoping this week to add some sprite rendering so I can get the loading screen of tetris working. I am also, however, trying to go back and clean up sections of code. I just did a major rewrite to the opcode function logic and now it is much more clear which opcode goes where (REAAAALLY big match statement). Last week I had a PR accepted for cargo so you could say I have officially contributed to the Rust ecosystem :P If you ever use clean --verbose think of me haha. I'm hoping to continue to contribute to cargo going forward. Outside of rust, I am starting to make a [minimalist Twitter client](https://github.com/tobywhughes/MinTwit) to learn React Native. 
attyboy
I think everyone would like to ship it this year (at least for a subset of users with working GPUs) but this being software development, nobody wants to make any promises for fear of having to walk them back. :) It'll ship when it's ready. The pace of development is nice and steady.
Replying to your edits: People have replied. You're disregarding a norm in this community to post one's own work. OP's comment _is not useless_ and it's common practice in /r/rust to leave such comments.
Huh, what a strange coincidence that this gets posted while I'm struggling to debug an issue with TTYs! If you do `gdb) tty /dev/pts/xyz` and then use a library like `termbox` that expects `/dev/tty` to be there it fails miserably, for some reason. Your library assures me that the inferior process indeed still has a tty, so I'll have to keep digging. Neat little library though, thanks `^_^`
I’ve done some work on this lately and have opened a new pull request to add more flexible and more efficient functional programming features to generic arrays. If anyone wants to give their opinion on the improvements, that would be cool. 
Dude. There's plenty of people whose names and aliases are nothing alike. You're taking all this the wrong way. It's pretty standard to clearly attribute something to whoever did it to avoid confusion and "taking credit by mistake". Take a chill pill. Clean slate tomorrow. 
There are a lot of static site generators of varying development: https://crates.io/search?q=static%20generator, but I'm not aware of any CMSes.
I just started learning!
Is there a changelog, or something listing the new APIs?
Is there a good link to some handy quick benchmarks of Rust as compared to other languages? And maybe benchmarks of stdlib collections compared to those of other languages like Java? Thanks!
How should I package my app for deployment? For C projects we're currently bundling the binary up in an rpm. Is this supported in rust?
Cross-language benchmarks are, for the most part, egregiously unrepresentative. It's possible to write slow code in pretty much any language, whether that be through inattention, laziness, or bad libraries. It's possible to write fast code in languages that heavily disincentivise it, or that just make it really hard to do. Benchmarks can *at best* tell you what the absolute most pathological optimisers with unlimited time and resources can do, *not* what to expect day-to-day. So, with that said, you want [the benchmark game](https://benchmarksgame.alioth.debian.org/u64q/rust.html).
Static site generators share some infrastructure with CMSes, like templating etc. Some people don't consider them, yet they might be a good fit for their use case, e.g. templating without dynamic content, generating a ToC, etc.. You were not only given an answer, that the person is not aware of a CMS written in Rust, but also given - in some cases - a viable alternative. You're probably aware of the nuances, but the common user probably isn't. It doesn't have to be the solution for you, but might be for someone lurking into this thread.
Just the [pull request](https://github.com/fizyk20/generic-array/pull/57) and previous documentation for now. The first added feature was `GenericSequence`, which is a trait defining some sequence of type T, with an associated length. With the length stored as an associated type, `GenericSequence` can be used in generics without knowing the length of the array beforehand. Second, `GenericArray`, `&amp;GenericArray` and `&amp;mut GenericArray` all implement `GenericSequence`, with the latter two utilizing slices for more optimized iteration. Third, `MappedGenericSequence` and `FunctionalSequence` add `map` and `zip` functions to be used with `GenericSequence`s. It's basically a way of adding `map` and `zip` to `GenericArray` so that they can usually be auto-vectorized and also be used with generic functions. In fact, when used like `array.zip(&amp;other, |l, r| l + r)`, on a `GenericArray` of four elements, that can be auto-vectorized by the compiler into a single instruction (excluding register loads). Does that explain it a little? These APIs will be used with my crate [numeric-array](https://github.com/novacrazy/numeric-array), which adds a vector/array type that can be used like a single number, and any operation on them is (almost) always auto-vectorized.
Yeah that was sarcasm.
I'm asking about static ones
100% coverage isn't usually something to strive for. Often times it leads to silly solutions just to get coverage. In this case, though, it's a good thing since it's a small, well defined library. :) 
Well, it looks like I know what I'm doing tomorrow :) UNIC's table structure needs some cleaning up and an optimization path, and I've been tinkering off-and-on with doing so, focusing on the Name table. For many tables, just a regular binary search slice is the best option. For larger tables, though, we don't have any other compression options available in UNIC yet. For the General_Category, it'll probably be worth bringing something over like the `BoolTrie` that the stdlib uses, and the Name will benefit from the prefix-trie-like structure that unicode_names is using. Actually, any chance you could add an option to use unic-char-range? That crate has no dependencies and provides a type (and macro) such that `'a'..='z'` works and is a range type. I believe UNIC's generation pipeline will continue to mostly be focused on running it as an internal library (and our internal data table types), so this will still be useful as an out-of-tree resource for other uses that want raw UCD tables. If we can get UNIC's tables to a point where they _can_ be used outside of UNIC for the rare case that needs it, though, I'd be all for it. OK, this is an overly rambley way of me putting down that I'm going to work on UNIC's tables tomorrow so that I actually do so and I really should go to bed I mean this post is barely coherent after three edit passes and I mean look at this run-on sentence
(I should say this probably means I'm going to go ahead and add unic-char-range support even if UNIC ends up sticking with its internal tools (which I already RIIR).)
You know what I'm asking of better than I do? 
And you're being stupid and spend the time of all the readers here. You don't respect anyone.
Agreed, my point was more about having checked all the boxes, one of which more or less being "well tested" which I'd even say for ~85+ ;) But for a small lib like this 100% is a nice boon
Is related information that doesn't answer the question invalid to post? I mean this isn't a Q/A board, it's a forum. Related information could be helpful to you, we don't know your reason for asking for a CMS. It could also be useful to other people coming across this post who aren't you. As an answer: to my knowledge there doesn't exist a CMS written in rust yet.
This isn't a Q/A board: you're not the only one who benefits from comments on your post. These comments _don't have to be for you_. They can benefit anyone else who searches and finds this post as well.
&gt; This isn't a Q/A board: you're not the only one who benefits from comments on your post. Then create a separate question and benefit all you want. 
&gt; Is related information that doesn't answer the question invalid to post? &gt; yes
I'm not talking about myself, I'm talking about anyone else who searches "CMS" in google and finds this post. This community aspect is part of the fundamental nature of /r/rust, not something you can decide.
https://groups.google.com/forum/m/#!topic/mozilla.dev.tech.gfx/916NeNdjg3U &gt; Upcoming Milestones: * Enable the last remaining automated tests (expected Fx 61) * Blobs (Expected Fx 61) * Text/Font related work (Expected Fx 62) * Writing various tools to debug and/or develop (Expected Fx 62) * Burn down remaining P1 bugs and Pref ON V1 to 20% (TBD) of the Nightly audience (Expected Fx 63)
What is the difference between the clone and cloned methods ? I suppose it is something about lifetime but I am not sure.
Are there plans for a Rust based CPU fallback so the current render backend can eventually be removed?
Is there a way to install and keep the Servo nightly updated using a command line tool similar to `rustup`? I'm not a contributor, but I like to look at the progress from time to time.
At the same time, a lot of Lisp code is defining macros. I've heard claims around a quarter of the code is macros. In other words, they are very necessary for programming in Lisp. The ratio must be much lower in most Rust projects.
I got it working, thanks! In terms of boiler-plate it's not much different ("29 insertions, 31 deletions"), but the future-proofing seems worth it. Thanks!
I wrote a small program in C++ and things just happened by themselves. Did I initialize that vector? No, I just started doing things with it and it got initialized somewhere. Not having to write `vec = Vec::new();` made me very paranoid, with good reason. How can I just declare a variable, use it and somewhere in some template file it gets heap allocated? That makes me scared.
If you need cms, you should not really be using Rust. The ecosystem is not yet mature enough.
`pom` is excellent, it just does what I want using normal functions and it's completely readable `nom` makes me cry
You mean `Clone::clone` and `Iterator::cloned`? `clone` clones things. `cloned` clones each element of an iterator.
Sounds like a [CRDT](https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type) 
You’d have to include macro usage to get those numbers I think. Or maybe hope is the right word:P
Awesome, thanks for the thoughtful response :)
I'm a maintainer of nix and for any issues where there might be a problem with nix or its docs that make it difficult that you file an issue. And feel pretty to ping me directly from the issue. As to your specific issue, it appears that you're using an old version of nix with the old docs, where BitFlags didn't expose their members under the struct documentation but under the enclosing module as plain constants. It was super-confusing, so as soon as BitFlags changed that behavior we followed suit. You can see that [the current docs](https://docs.rs/nix/0.10.0/nix/sys/wait/struct.WaitPidFlag.html) for `WaitPidFlag` look a lot better! So please try again with the most recent version of nix (and make sure you have a compatible compiler version!). As a general rule when you run into odd problems like this with a library I'd suggest making sure you're using the latest version as that may resolve it.
No, a panic is only the most trivial.
It would also be nice to see a comparison with the [isatty](https://docs.rs/isatty/0.1.6/isatty/).
It is indeed a particular case of a CRDT, but it's not a simple man-designed CRDT. The construction comes from category theory, and it is actually the smallest super-category of the category of files that includes all pushouts (which is roughly equivalent to being a CRDT). This guarantees that we can't miss any case of file editing, and also that we have a proper definition of conflicts (which usual CRDTs don't have, nothing is a conflict in standard CRDTs). Moreover, unlike CRDTs, Pijul has inverses, both local (the "unrecord" command unapplies a patch) and global (the "rollback" command creates the inverse of a patch).
I suggest looking at how I wrote [carnix](https://nest.pijul.com/pmeunier/carnix), which compiles Rust using Nix. The only reason I wrote that is because Nix refuses to reuse any previous build product, so my Rust code would take hours to deploy with NixOps.
Please don't drag the conversation down this path. There isn't a full-blown CMS so you were provided with an alternative. There is no need to start saying that someone was stupid or squandering time.
&gt; Why are you so egocentric? Why are you so egocentric? You don't respect me and others? 
&gt; If you need cms, you should not really be using Rust. I'm asking whether or not I should use it
Then either ask me question if you have what to say and keep silent
&gt; I'm not talking about myself, I'm talking about anyone else who searches "Rust CMS" And I came here and ask my question.
No, you are. I am pretty sure that's what you are asking. That's exactly what you are asking. You know what you are asking is just that. 
What is the benefit from using just compared to a simple make file? While you have lots of features you don't have to use them and also many people will have make installed. And the syntax looks pretty similar.
Glad I could help. As for boilerplate, I don't really mind it as long as it's serving sufficient purpose. That much code to suppress an attribute? Inefficient. That much code for schema versioning without resorting to "tree of variant fields" parsing? Great deal.
Also, with `drop()`, you can specify the point in time when you want to drop. Consider struct Foo {a: Bar, b: Bar, c: Bar } fn foo(self) { let Foo { a, b, ... } = self; a.do_something(); } here, `c` only gets dropped after `do_something`, which may be a bit counter expectation. Explicitly binding and dropping manually makes this clear. This is especially useful when you expect that `do_something` would only return when `c` gets dropped, for some reason.
Jesus Christ you are sickening. "Take a chill pill. Clean slate tomorrow." Good job, you're definitely above me and not stooping to my level with that one.
Your work on generic-array and numeric-array looks really great. I'm still new to rust so not sure I understand everything you did yet, but will taker a closer look soon. I've been using nalgebra for building numeric simulations and have been hacking together some traits and functions to make various operations, especially component-wise, easier and more transparent on slices, f64 iterators, MatrixMNs and structs of named scalars/MatrixMNs. Do you have any plans to use numeric-array with a matrix type?
I think the easiest way to do what you want is just to use gdbserver
You said that one of your issues with macros is that they hide things, and mentioned a couple of examples. I just pointed out that all those examples also applies to functions as an abstraction mechanism as well. 
&gt; How far are we from first-class coverage support in Rust? No one is working on getting LLVM code-coverage support into rustc, so... as long as we have a zero-gradient, we are infinitely far from this :D
I always found having to write `ArrayLength` bounds everywhere while using generic array too anonying to be worth it :/ n
&gt; For many tables, just a regular binary search slice is the best option. Better than eytzinger layout?
&gt; I'm going to fork std::simd and pin faster to the fork until an RFC goes through. If we change stuff in the compiler and that breaks the fork, you are going to need to backport the changes from the stdsimd crate. It probably makes more sense to just track the stdsimd crate releases, and if you want something that hasn't been released yet just ping us to do a new release.
The `stdsimd::simd` module **is** `core::simd`. You only have to change: extern crate stdsimd; use stdsimd::simd; to std::simd
You might want to take a look at /r/playrust
I would have loved to see a trailer for the rust programming language, though... something like "In a world where even bare CPUs cannot be considered safe anymore (show an exploding i7 CPU), a small yet powerful language tries to make a big difference..."
yeah i just noticed this is the wrong subreddit haha
&gt; For the General_Category, it'll probably be worth bringing something over like the BoolTrie that the stdlib uses That is exactly what [`ucd-trie`](https://docs.rs/ucd-trie/0.1.0/ucd_trie/) is. :-) ucd-generate can produce these prefix/suffix compressed tries. I haven't worked too much on the trie set code, and mostly cribbed /u/raphlinus's ideas. My "feeling" is that there is a lot of room for improvement, in particular with respect to how much space it's using. (If you generate some trie sets and look at the output, there is a lot of redundant structure. I did a few "obvious" optimizations, but it looks like there is more to do.) &gt; and the Name will benefit from the prefix-trie-like structure that unicode_names is using If you don't mind using FSTs, then you should get a much smaller structure, although access time may be a little slower, I don't know. From looking at unicode_names, it looks like the total generated code from phf is ~1.6MB? If that's right, then FSTs are definitely smaller. `ucd-generate names /tmp/ucd-10.0.0 --fst-dir /tmp/ucd-fst` clocks in at 230KB. Note that the `fst` crate can have its memmap dependency dropped since it isn't necessary for the use case of representing character tables, which makes it a very light-weight (albeit complex) dependency. &gt; Actually, any chance you could add an option to use unic-char-range? That crate has no dependencies and provides a type (and macro) such that 'a'..='z' works and is a range type. Sure! `ucd-generate` already produces structures that require `ucd-trie` and `fst`. Adding more is fine by me! But yeah, I don't know exactly what the future looks like. If UNIC's table generation pipeline stays internal and that means ucd-generate is useful going forward, then great. If UNIC subsumes and deprecates ucd-generate in the future, then that's fine too!
You can do the same for Rust. There is cargo-deb that helps with generating debian packages, but I am not sure if there is any support for rpm format.
&gt; For the General_Category, it'll probably be worth bringing something over like the BoolTrie that the stdlib uses That is exactly what [`ucd-trie`](https://docs.rs/ucd-trie/0.1.0/ucd_trie/) is. :-) ucd-generate can produce these prefix/suffix compressed tries. I haven't worked too much on the trie set code, and mostly cribbed /u/raphlinus's ideas. My "feeling" is that there is a lot of room for improvement, in particular with respect to how much space it's using. (If you generate some trie sets and look at the output, there is a lot of redundant structure. I did a few "obvious" optimizations, but it looks like there is more to do.) &gt; and the Name will benefit from the prefix-trie-like structure that unicode_names is using If you don't mind using FSTs, then you should get a much smaller structure, although access time may be a little slower, I don't know. From looking at unicode_names, it looks like the total generated code from phf is ~1.6MB? If that's right, then FSTs are definitely smaller. `ucd-generate names /tmp/ucd-10.0.0 --fst-dir /tmp/ucd-fst` clocks in at 230KB. (Errrmm, this is misleading. It looks like `unicode_names` provides a mapping in both directions. The FST above only gives you name-&gt;codepoint. So now I don't know how to do a comparison. :-)) Note that the `fst` crate can have its memmap dependency dropped since it isn't necessary for the use case of representing character tables, which makes it a very light-weight (albeit complex) dependency. &gt; Actually, any chance you could add an option to use unic-char-range? That crate has no dependencies and provides a type (and macro) such that 'a'..='z' works and is a range type. Sure! `ucd-generate` already produces structures that require `ucd-trie` and `fst`. Adding more is fine by me! But yeah, I don't know exactly what the future looks like. If UNIC's table generation pipeline stays internal and that means ucd-generate is useful going forward, then great. If UNIC subsumes and deprecates ucd-generate in the future, then that's fine too!
As someone who doesn't really understand what it takes to build a code coverage tool, could you unpack this? What exactly needs to be done? In particular, how does this square with the existence of `kcov`? I've been using that now for some time, and it's not great, sure, but it does work to some extent. But it exists. Is LLVM code coverage necessary for more accurate code coverage than what kcov gives?
"In a world on the brink of meltdown, where spectres roam the sidechannels, a story of two unlikely heroes will make your heart bleed!" 
[removed]
I don't think this should be implemented at the LLVM level. I think it should be a build pass in rustc. As far as I'm aware (I only built a simple coverage tool for a javascript project once) the way they're built usually is that you simply add tracing statements that update a map somewhere around the elements you track (i.e. statements/expressions/branches/whatever) at compile time. That kcov works on compiled binaries is pretty magic, but I don't think it's a proper tool for production, for starters it seems it lacks expression coverage.
As far as i know as long as C bindings are used exclusively MinGW and VC are linker compatible.
Is this a stop-gap until Rust has proper const generics, or, is this an implementation that is equivalent that will not require const generics support in the language.
Isn't tarpaulin designed to solve this issue.
I've had outbursts similar to yours so I tried to give you both an out and an explanation. You ignored both. 
It’s pretty much cheating by using typenum to recursively generate a structure large enough to hold N number of elements in contiguous memory, then creates slices referring to that chunk of memory to access the data. On an assembly level, there really isn’t too much of a difference, so it works for now, but it is a lot more awkward to work with than if Rust had proper integer generics. 
I’ve just asked, there are no plan for Firefox to write a CPU rasterizer in Rust. If WebRender eventually grows a CPU fallback it will likely still be based on Skia.
95.9% -- show off :P
Interesting! Thank you. I guess that makes sense. A lot of investment+complexity for a part that is less beneficial
I actually found a few untested branches in the coverage output, so that number should go up. :) (I don't normally care this much about coverage, but it's the regex parser. Every corner needs testing. Of course, coverage doesn't tell me about unknown unknowns, which do happen!)
Code coverage is arguably a very bad way to judge test efficiency. It can help find problem (thing you may have missed), but it's often used by people as a "metric" to determine how well tested their product is (e.g. "This project has 95% test coverage" type boasts).
Then the problem becomes that external tools like kcov, gcov, tarpaulin,.. don’t understand Rust. They don’t understand expressions, match arms, if let, etc. The moment one wants more information than basic “lines hit” rustc support for this is pretty much the only way forward. LLVM provides tools to help building this, but they are not enough.
Go here http://reactivex.io/
Ah gotya, thanks for the info. That is indeed very helpful in trying to understand the scope of the task.
I thought the same thing at first -- wouldn't it be awesome if the power of AST used in macros can help with proper coverage. But then, when you run tests or real data, you need to record all the code paths that happen when the code runs with actual data. If some coditions in if statements never met -- there is no way for compiler to be able to determine that statically. 
Extensions for reactive programming in Rust. In short, instead of having values that represent current states in a program, you have values that represent entire timelines of states (called observables). You then use 'the usual' functions (like map, filter, zip, scan, flatMap, etc.) to transform, pick apart, split, merge, or otherwise manipulate these timelines. Finally, you attach listeners to these timelines to perform actions whenever the timeline's value changes, like update the UI, save the updated state into a database, or what have you.
If I'm building a library, and want some features to not be included by default, and let the user choose which components to use, is it better to use features or seperate crates?
This was it, thanks! GDB wasn't able to set the controlling TTY of the inferior since a shell was already attached. I found a handy program [`reptyr`](https://github.com/nelhage/reptyr) which has an option that lets you create a pty pair with nothing attached to the slave. If you use that GDB works as expected. I have learned way more about TTY ioctls then I ever wanted to know, but at least I can debug my console program.
I don't have the kinda of skills to help out with coding this, but I would be willing to test if you start working on it. The main thing to me is how are you going to support all the architectures that llvm supports? Its taken respectively a year to get avr and riscv kinda upstreamed but still broken as to you can't use them yet for anything serious. IF your willing to put the work into this, the biggest downfall I see is lack of architecture support in the long run. Though to get started it might make sense to only target 2 or so architectures to build the code around that. Then work on porting more to you RLLVM. The last question that I have is: is your goal to have this compile on stable or nightly? I think in the long run stable should be what you aim for, but in the here and now it probably makes more sense to use nightly.
Note that you can get a backtrace in a debugger as long as debuginfo is enabled - even with panic abort.
Having never seen ReactiveX before, the rust example code appears somewhat similar to futures code. I'd be interested in a high level comparison of the two approaches/crates.
Yes
It's very bad because people (e.g. yourself) assume that 95% coverage means the code is more stable. It gives people a sense of false security.
I'm working on building an orientation library largely focused towards being used in crystallography, mechanical engineering, and material science fields. I still need to add quite a few more things to it like documentation, examples, tests, and a few more features. However, I'm hoping I can get all of that done in the next 2-4 weeks and have it up and published on Crates.
I have a project that stack overflows in debug mode but runs perfectly fine in release mode. Does anyone now what could be the cause?
It would be better if it was an extension to `futures::Stream` trait.
`kcov` runs on less platforms than Rust making in an inappropriate tool for first-class coverage support in Rust.
&gt; Tarpaulin only supports x86_64 processors running Linux. No, it's not designed to solve the issue of first-class coverage checking in Rust.
Depends on the coverage metric in my opinion. Line coverage is fairly bad as a metric, same with decision coverage. Condition coverage is better but MCDC coverage is definitely best (and not available in any free tools). Also, naturally as well as covering the code you should be checking all the relevant outputs are as expected, otherwise you might as well just be fuzz testing because you'll only get panics not logic errors.
So... `kcov` basically runs your program in a debugger. It stops it every Dt time interval, and uses the debugging information to map the assembly instruction where your code stopped to a line of Rust source code, and then adds a counter to that line (it is a bit more sophisticated than this, but this is basically what it does). From a design point-of-view, this approach main advantages are: * it can generate line coverage information for every binary compiled with debug information, * it is pretty much language agnostic (the debug information gives you a filename, a line number and a column, and that works for mostly everything) The bad news are that: * one needs to compile the binaries with debug information * the debug information emitted is not intended for this use case * with optimizations enabled the debug information can be pretty bad * the coverage information knows nothing about the language. One might infer that if the source code line contains an `if` it might be a branch. In fact, the approach of trying to parse the program's source code trying to make sense of the counters can be pushed quite far, but it has its limits. The alternative to `kcov` and friends is modifying the binary to emit the information that you want. That is, the language front-end inserts the code to write to the line / expression .. counters into your binary, along with a table mapping these counters to the exact source code locations, and this is preserved across optimizations. LLVM has an instrumentation framework for inserting these counters to instrument functions, expressions, conditionals, etc. and for constructing the source-mapping information. It comes with tools that understand this format and allow you to generate code coverage reports from it, but also to perform profile-guided optimization, drive fuzzers (e.g. changing inputs to try to hit code-paths that weren't being hitted before), etc.
Observables are basically multivalue futures. They can push you a whole (asynchronous) sequence of values instead of just one. That said, the futures crate has its own abstraction for that in form of the Stream trait.
I'm not sure why anyone would want to tie Rust to the GNU ecosystem when Rust is on the LLVM toolchain. You'll just be raising your hands in victory going "Rust has this thing!" and I'll have to keep making reddit posts about how Rust doesn't care about people using the MSVC targets.
This will be changed soon.
Every measurement comes with caveats.
An inclined macro containing a 'continue' is really very different from an inlined function call.
I appreciate your "thread pool" joke and raise by putting an event reactor into a pool: https://gist.github.com/skade/25f9b0c87b1d5bd39d9fc6ffe0d1840a (this joke requires you to be far too old and have played computer games in the late 80s).
This probably means the optimizer removed a stack allocation or maybe eliminated tail recursion. For example: // Without optimization, this large array is first created on the stack before being moved into the heap. let x = Box::new([0usize; 10_000]); // Without optimization, this recursive function could overflow the stack, since a new stack frame is created every iteration // (though the stack frame is small or nonexistant since the example function has no local variables) // LLVM can (probably?) optimize this tail recursion so that it doesn't create a new stack frame every iteration. add(u64::MAX); fn add(times: u64) -&gt; u64 { return times + add(times - 1); }
&gt; It gives people a sense of false security. "False sense of security" is a fully-general counterargument for every risk mitigation strategy ever. Are you actually arguing against all metrics\*? Or is there some specific problem with test coverage. \* I assume not, or Rust's type system would be something that gives a false sense of security too. 
Wow! Thanks for the great explanation. That really clears this up. :-)
There are a couple problems: * It gives a few false positives: for instance, it considers `struct`/`enums` definitions as uncovered code. * It ignores functions that were not touched by any unit test, e.g. https://burntsushi.net/stuff/regex-syntax-cov/regex_syntax2-b5c29d866f3036db/error.rs.1b3dc954.html
&gt; I'm not sure why anyone would want to tie Rust to the GNU ecosystem when Rust is on the LLVM toolchain. I don't know, maybe because that suffices for somebody and they were willing to put the work in? &gt; You'll just be raising your hands in victory going "Rust has this thing!" and I'll have to keep making reddit posts about how Rust doesn't care about people using the MSVC targets. Look mate I'm just reporting what facts I could find, and Rust isn't a person, if you care about MSVC targets maybe be the change you want to see and get involved.
Its an awesome project, and Julia Evans is super awesome. But my god whoever recorded that needs to figure out where that static is coming from.
The plan would be initially to create: * parse LLIR into in-memory structs * implement serialization of in-memory structs back to IR and/or LL BitCode * implement optimization/transformation framework * implement some optimizations/transformations in Rust * use LLVM to continue to take the now partially optimized LLIR to Machine Assembler * implement more optimizations/transformations in Rust (hopefully with community involvement) * Only when optimizing/transformation stage is near complete, then, begin on implementing the stage for IR to Machine Assembler in Rust (or it could being happening in parallel at any time)
See also on Rust Discourse: https://users.rust-lang.org/t/proposal-rllvm-rust-implementation-of-llvm/16021
I'm not an expert on the implementation of native exceptions, but as I understand it: The point of exception handling is not to provide a backtrace, [C++ for example doesn't](http://www.di.unipi.it/~nids/docs/i_want_my_pony_or_why_you_cannot_have_cpp_exceptions_with_a_stack_trace.html) create backtraces for exceptions by default. The purpose of exceptions is for alternative control flow. Throwing an exception means constructing an arbitrary object which can hold information, then calling the unwind function which traverses stack frames (or exception frames) *running destructors for objects on the stack* until it reaches a catch statement which can use the exception object and resume normal operation. Creating a backtrace is more of an incidental convenience since you're already doing the work of traversing the stack, although you do have to lookup debuginfo and do demangling if you want it to be pretty. In Rust, panics are ([usually](https://doc.rust-lang.org/std/panic/fn.catch_unwind.html)) not used for control flow, but they use the same mechanism. Since panics are not normally used for control flow, it's okay for them to be slow and print a backtrace by default. When you use `panic=unwind`, the rust compiler enables LLVM support for exceptions (the `invoke` instruction and landing pads) and links libpanc_unwind to provide `__rust_start_panic` which unwinds and prints a backtrace. When you use `panic=abort`, the rust compiler disables LLVM support for exceptions (functions are called with the `call` instruction) and links libpanc_abort to provide `__rust_start_panic` which does not unwind but just aborts. There could be a hypothetical third option `panic=backtrace_abort`, which uses debuginfo to print a backtrace then aborts... which would be almost exactly the same as unwinding except that it ignores the possibility of `catch_unwind` and doesn't call stack destructors. I just don't see the point of this option. If you're just going to abort anyway, why care about the extra 100 microseconds spent running destructors and checking for a landing pad? At this point I have to ask: why do you want to use `panic=abort`? C++ exceptions are designed to be zero-cost unless you actually throw an exception. The only benefit to disabling them is to remove the code which would be responsible for unwinding. If you remove unwinding code then add backtrace code, you don't gain anything! If you want a more detailed explanation, see [the related RFC](https://github.com/rust-lang/rfcs/blob/master/text/1513-less-unwinding.md) or [LLVM documentation](https://llvm.org/docs/ExceptionHandling.html), I may not be doing a good job explaining this.
&gt; Look mate I'm just reporting what facts I could find, and Rust isn't a person, if you care about MSVC targets maybe be the change you want to see and get involved. I'd love to but all my code is owned by my employer and so making contributions to open source would be legally sketchy, so my only resort is to try to nudge people towards better solutions. I hope you understand that my post wasn't an attack on your post.
You should try out illogiq's mutation testing, I've always wanted to see a coverage-guided test suite go through mutation testing.
It gives you a very clear, factual statement - n% lines of your code were executed. So long as you don't read that as "n% lines of your code must be correct", I see no danger.
&gt; I'd love to but all my code is owned by my employer and so making contributions to open source would be legally sketchy. Maybe fix that then? &gt; My only resort is to try to nudge people towards better solutions. As far as I'm concerned, you're doing a terrible job of it. &gt; I hope you understand that my post wasn't an attack on your post. The problem is that it's not really *relevant* to my comment either. "LLVM coverage might also work with MSVC which gcov sadly does not" would have relatively neutrally provided additional data, "gcov is garbage and Rust hates MSVC and claiming coverage support due to gcov is stupid as fuck" (paraphrased but not by that much) is a rant on a word-match.
I was trying it out because my Gtk app doesn't really need to do any unwinding when stuff goes wrong, it can just shut down. Plus it has some worker threads in the background, if those panic it would be nice if the whole thing prints a stacktrace and shuts down. Mostly I was just surprised that I'm losing debug information that way.
There's [cargo-tarpaulin](https://github.com/xd009642/tarpaulin/), which isn't exactly rustc, but isn't nothing, right? I mean I don't know what "first-class" entails, but if tarpaulin matures enough I'm sure it could find its way into `rustup`.
Look, I'm just trying to help you understand _why everyone is replying like this_. You seemed to really not understand that this forum isn't a personal help Q/A, so I'm trying to help you shift how you see it. You did indeed come and ask a question! We all gave you the information we had that was tangential. We gave you the direct answer as well, which is "there is no CMS in rust currently". Why isn't that valid? I'm trying to get you to see here that the community exists to help everyone, and your not wanting an answer does not make it invalid.
d'oh of course Rust binaries print their own backtraces, whereas C++ ones on windows don't.
If we could get doc tests to count towards coverage too that'd be sweeeeeet!
There's [a PR open](https://github.com/rust-lang/rust/pull/48346) for PGO support using LLVM's InstrProfiling. I'm not sure what (if anything) is the difference between profiling for PGO and profiling for code coverage - they seem identical. Then the "only" thing left to do is make that information convenient to use by adding a cargo subcommand to parse the information and present statistics.
I concede that the tone of my post could have been better. It feels like an uphill battle sometimes so it's hard to be even keeled all the time.
I was recently struggling with this exact question - mutexes or a thread to read from a queue. I ended up picking mutexes because 1. The critical sections are extremely small (just long enough to mutate the value), so I don't expect much lock contention; 2. I think the cost of a thread is higher than the cost of the mutex; and 3. I'm writing a library, so I don't want to create unexpected threads for users of the library. I just wanted to point out that there are scenarios where Arc&lt;Mutex&lt;T&gt;&gt;es are the correct solution :)
Thank you! I might give it a try at some time.
Thank you! I might give it a try at some time.
I guess it's useful to experiment with the API design in isolation, then integrate with the rest of the ecosystem when the experiment proves its worth
To change the process’s current directory consider using the [std::env::set_current_dir](https://doc.rust-lang.org/std/env/fn.set_current_dir.html) function. If you really need to call C functions, see the [Foreign Function Interface](https://doc.rust-lang.org/nomicon/ffi.html) chapter.
When I get into binds like this, I usually turn to github searching to try to find examples: https://github.com/search?q=language%3Arust+libc%3A%3Achdir&amp;type=Code&amp;utf8=%E2%9C%93
In fact, there is a [roadmap](https://github.com/servo/servo/wiki/Roadmap) on servo's wiki which includes shipping Webrender in Firefox in this year. But servo is a research project, I have seen roadmap items not being reached. So don't put the champagne into the fridge just yet except you are okay with the additional space requirements.
It would be great if you could integrate this with [cross](https://github.com/japaric/cross)
Ok - so I'm using nix right now, I have: unsafe { chdir("/tmp") }.expect("chdir failed"); up and running as a test, this is acceptable code for nix?
I've been looking for something like this. Do you know if this is push or pull? To me, the elegance of FRP is being able to say x = y + z, and allowing y to vary without ever having to mention the existence of x in any code outside of where x is defined. Everything I've looked at for the few programming languages I'm interested in require x be referenced in y, which means if I want to mess with x, I have to remember it's partly defined over where I wrote y.
One thing I've always wondered is how you determine whether a machine has a working GPU. I presume some subset of whitelist, blacklist, features supported, and running tests that cover functionality needed or known problem areas, but curious about the exact design.
Best RX fundamental explanation I know of is Bart de Smet's available at https://channel9.msdn.com/Tags/bart+de+smet 
Really great! Can I use it but still use other Cursive features and other crates (like "cursive_tree_view" for example) with it?
I I have a [solution](https://github.com/AdamNiederer/cov) for emacs which does that, but I've only tested it on gcov and lcov data.
Can my laptop run rust? It's model is called toshiba l50-b-2dj. 
You're probably looking for /r/playrust which is about the video game, Rust. This subreddit concerns the programming language. If not, your laptop is perfectly adequate to get started programming rust today! 
The game or the language?
&gt; If I were to modify my sample such that the background threads are emitting data on sender channels, then the background threads are no longer mutating shared data. They still are mutating *some* shared data, just not directly by your hand -- channels abstract this away for you.
```the last question that I have is: is your goal to have this compile on stable or nightly?``` I would imagine nightly, but, only using features that are relatively close to stabilization and unlikely to change.
You don't need to use snappy at all. It's just an example lib to show how the FFI mechanism works.
Oh, whoops totally thought it was about the video game xD. Mb tnx for letting me know
That's correct and they are have quite different applications. Just a note: almost all ndarray functionality is available for data of arbitrary memory backing through the array views, so it's not only for ndarray-allocated memory.
I know you care, my rhetoric was bad in this case. Still in this thread four suggestions have been made and three are completely inappropriate for Rust as a project due to dependencies on Linux/OSX/GNU and I feel like I need to remind people of that.
You should complain on /r/playrust. This is a subreddit for the Rust programming language.
Yeah, uh... embarrassingly enough, I realized my mistake after I entered it. I looked for it in the "New" list, but didn't see it so I figured a mod bot took care of it. Many, many apologies for causing a raucous here. I'll... see myself out...
In a lot of the code I maintain there is a pattern that I have no clue how to handle. The following C pseudo-code shows what I mean: ConfigStruct config ; read_config(&amp;config) ; // This fn needs to modify the data sanity_check(&amp;config) ; // This is an immutable call update_config(&amp;config) ; // This makes certain changes based on config data Now when I look at this in Rust I'm sort of lost. I can pass a mutable reference to read the configuration just fine. When I go to do an immutable reference I can then no longer make changes to the data (due to the immutable borrow). How do you refactor this so that it makes sense in Rust-land? Must admit, I'm an 20+ year C coder and the borrow system is kicking my ass :D 
One of the core components to build JavaScript tooling in Rust! From Babel to Browserify, almost all tooling requires on the `resolve` module. First steps!
There's no problem borrowing something mutably, then immutably, then immutably again. ``` let mut config = ConfigStruct::new(); read_config(&amp;mut config); sanity_check(&amp;config); update_config(&amp;mut config); ``` Here, `read_config` and `update_config` get to modify the struct while `sanity_check` does not.
OK, TIL
This looks like it's an unsoundness in `winit` - possibly one that's been fixed between versions `0.5` and the current `0.11`. I'd recommend upgrading `glutin`, `gfx` and `gfx_window_glutin` since all have inter-dependencies.
No big deal, our Auto moderator bot caught it before anyone could see it (but us mods). Just thought I'd let you know so you can re-post your complaint. Cheers!
‘unsafe { libc::chdir(dir) }’ is all you need.
so basically functions the same as it does in C just need to use 'unsafe' and 'libc::' ?
Jamey has some solid ideas about how to move the project forward, though it will probably not involve using `language-c` as a base. 
This is my first bindgen crate, please let me know if there are any issues or anything I can improve. There's a basic implementation of lsmod, insmod and rmmod in examples/.
Absolutely true. I wrote a parser to try to learn Haskell, but I wound up cutting many corners to get unblocked. The high quality and legibility of the `language-*` libraries in Haskell is something to contend with!
How do you debug tests? I'm using VSCode with LLDB and I managed to debug the binary, not the tests. I'm poking into the Servo code so if `./mach` knows some trick I don't please share even if it is not Rust specific. I use Fedora.
Thanks for the info. 
I don't think you need `unsafe` there, right? [This comment](https://www.reddit.com/r/rust/comments/82he2e/using_libc_crate_in_rust/dva761g/) says no.
&gt; It ignores functions that were not touched by any unit test, e.g. https://burntsushi.net/stuff/regex-syntax-cov/regex_syntax2-b5c29d866f3036db/error.rs.1b3dc954.html The appropriate way to fix this is by passing `-C link-dead-code` to `rustc` (e.g. via `cargo rustc` or `RUSTFLAGS='-C link-dead-code'`).
Please add to the Rust layer in spacemacs! :-)
I found this very helpful: https://github.com/saschagrunert/microservice-rs I have unfortunately had to look at generated code, other large projects, etc to get my capnproto code working. I would say by far the #1 thing that project needs is far, far more documentation, tutorials, examples.
Yay!!
I have some code I may be able to share privately. (Just dumping it onto GitHub is not feasible due to my employer's stance on sharing code without getting a rubber stamp from a committee.) Unless it has changed completely in the past year, my brief recommendation is not to use capnproto-rust. When I tried using it, there was only support for [lower-level protocol features](https://capnproto.org/rpc.html#protocol-features). The RPC client/server interface was a bizarre island until itself (although the existence of capnp-futures suggests I should revisit this point). The we-don't-pay-deserialization-costs-until-field-access magic was implemented in a way that meant you still needed to define your own types to use internally past the RPC layer. (This might be a good idea anyway, but contrast with standard practice using protocol buffer or Thrift messages, where you can read a message, mutate it in-place, and push it back up there wire, all without having to worry about the lifetime of the buffer backing the message.) After seriously mucking about with it for a couple of weeks' worth of personal hacking time, I concluded that the ostensible wins of using Cap'n'Proto would require far too much effort to materialize. I'm happy to reassess these opinions if anyone has used capnproto-rust more recently and found it to be in better shape. It looks like some serious commit activity has gone into it, and I wouldn't want to sell that effort short.
I use cargo kcov with decent success, but it would be great if the support would work out of the box.
Yeah fst currently requires std, but I believe only the construction phase really needs it. It would take a little work but it should be possible to make read time no_std.
Thanks! I just assumed it was `kcov`'s fault and didn't even think about dead code elimination.
No need for unsafe when using `nix` crate and its functions, but `libc::chdir` is just a reexported C function and as such needs unsafe. That comment from u/steveklabik1 was about `nix` function. The comment you answered to showed an example of `libc` usage.
If you can't see how condescending that statement is you have some real problems
&gt; [adhesion-rs is a] set of macros for design by contact in Rust. The design of this library was inspired by D's contract programming facilities. &gt; &gt; – https://github.com/ErichDonGubler/adhesion-rs
What are your thoughts on tql vs Diesel? (I haven't used either one yet, but tql looks simpler to me)
Same here. While I expect there's a lot of overlap, I'd bet that there is functionality that's standard in reactiveX that futures doesn't provide (e.g. debounce).
Yo... this is _awesome_.
I just went digging to see whether something like this provokes a data race. (Tl;dr - it doesn't) fn thread_1(arc_mutex: Arc&lt;Mutex&lt;Foo&gt;&gt;) { let guard = arc_mutex.lock(); modify(&amp;mut *guard); //jam the mutex forget(guard); //but dec the reference count anyway drop(arc_mutex); faff_about(); } fn thread_2(arc_mutex: Arc&lt;Mutex&lt;Foo&gt;&gt;) { faff_about(); drop(arc_mutex); } Imagine the drop happens in thread 2. This calls drop in a different thread *even though* the Mutex is still locked by the first thread! Fortunately `Arc` provides backup synchronization. So my example is safe. Abridged from [`alloc/arc.rs`](https://doc.rust-lang.org/src/alloc/arc.rs.html): fn drop(&amp;mut Arc&lt;T&gt;) { // All stores to T, etc. are flushed in thread_1 before the decremented counter becomes visible if self.inner().strong.fetch_sub(1, Release) != 1 { return; } //...so they must be visible to T::drop in thread_2 before it observes the strong count change 0-&gt;1 fence(Acquire); drop_in_place(&amp;mut self.ptr.as_mut().data); .... // free the memory } Pretty cool. If anyone else is curious about this stuff, I found Preshing's explanation of [release-acquire synchronization](http://preshing.com/20130922/acquire-and-release-fences/) the most clear. `Arc` guarantees that `Foo::drop` happens-after every single drop of `Arc&lt;Mutex&lt;Foo&gt;&gt;` across all threads, *even if* it breaks a jammed `Mutex`. 
A future/promise is to a value as reactive streams are to iterables/collections. In other words, it is designing a system based on asynchronous events. It has a lot of power, especially when you start talking about things like backpropagation (I don't know if this incorporates it). Which is basically the downstream signalling the upstream of available capacity. For example, you might say "My database can handle 10 outbound connections". With RX, you put that information right next to where you are making those outbound requests then upstream where you might say "give me the next job" will only signal when downstream signals that it has more available room. This can keep your memory consumption very load without sacrificing a whole bunch of performance. But even without that, it generally uses less memory anyways because you often push events through the system and dispose of them quickly rather than doing typical batch processing where you load up everything, do your work, and then save the results. (instead, as you load data, you are also doing work and saving results).
True, but the ndarray API is designed with dynamically allocated arrays in mind. It may not allow resizing them, but it wouldn't be feasible to have a fixed-at-compile-time array structure backing it unless it goes the same way as nalgebra, which does use generic-array as its backing data structure.
is futures::Stream push or pull?
I've been thinking about this talk some more today. Are there any other notable cases where bindgen is used for this kind of reverse engineering?
Forgive the ignorance, but what is the advantages of this approach vs just putting assertions in the function body?
I'm looking at the snippet of code that's on Tokio homepage, it starts with fn main() { // Bind the server's socket let addr = "127.0.0.1:12345".parse().unwrap(); let tcp = TcpListener::bind(&amp;addr).unwrap(); I checked the type of addr, and it has type &amp;std::net::SocketAddr I'm just curious where the direction to make it a socket address came from; everything I've read so far tells you to explicitly annotate stuff like that as something with something like .parse::&lt;type&gt;() Thanks
When I tried this, I got lots of error messages like this from cargo: /home/andrew/code/rust/regex/regex-syntax-2/src/hir/visitor.rs:198: undefined reference to `alloc::slice::&lt;impl [T]&gt;::is_empty' /home/andrew/code/rust/regex/regex-syntax-2/src/hir/visitor.rs:193: undefined reference to `core::slice::&lt;impl core::ops::index::Index&lt;I&gt; for [T]&gt;::index' /home/andrew/code/rust/regex/regex-syntax-2/src/hir/visitor.rs:203: undefined reference to `core::slice::&lt;impl core::ops::index::Index&lt;I&gt; for [T]&gt;::index' /usr/sbin/ld: /home/andrew/code/rust/regex/target/debug/deps/regex_syntax2-b5c29d866f3036db: hidden symbol `_ZN5alloc5slice29_$LT$impl$u20$$u5b$T$u5d$$GT$8into_vec17h54ad9856fb26eb7bE' isn't defined /usr/sbin/ld: final link failed: Bad value collect2: error: ld returned 1 exit status I guessed that this had something to do with codegen units being `&gt;1` by default in debug builds, so after setting that to `1`, it worked. My coverage went down to 89.1%, which is expected given the code it missed! Thanks!
Sometimes people mishear me when I speak. What a horribly flawed communication method.
Link errors seem like a bug.
They're also cold, meaning that they don't start any work until someone subscribes to the final thing. Futures are typically hot, meaning that the work has already started when you receive the reference.
It's also worth mentioning that cryptographic algorithms are very often designed with SIMD in mind for performance. [This paper on vectorized ChaCha20 is a good set of examples.](https://eprint.iacr.org/2013/759.pdf)
But if the binding of the 'addr' variable and the TcpListener::bind are two separate expressions, the compiler doesn't go back and nail down the type of 'addr' from a later expression passing it to TcpListener::bind, does it? What is the type of addr after parse().unwrap() and before its reference gets passed to bind? Thanks again
Oops, your right, I meant backpressure.
I think they're referring to backpressure.
The compiler will indeed go back within the same function and as you say, nail down the types. It will do any reasonable amount of type inference for anything except function parameters and return types, which must always be stated explicitly. For example, this code will infer `vec` to be a `Vec&lt;u8&gt;`, even though on the line `let mut vec = Vec::new()`, there's no information to indicate that. fn main() { // Because of the annotation, the compiler knows that `elem` has type u8. let elem = 5u8; // Create an empty vector (a growable array). let mut vec = Vec::new(); // At this point the compiler doesn't know the exact type of `vec`, it // just knows that it's a vector of something (`Vec&lt;_&gt;`). // Insert `elem` in the vector. vec.push(elem); // Aha! Now the compiler knows that `vec` is a vector of `u8`s (`Vec&lt;u8&gt;`) println!("{:?}", vec); } (from https://rustbyexample.com/types/inference.html) To answer your question: the type is always `SocketAddr` from the moment `parse()` is called.
...was there a new release of Cobalt or something? Trying to figure out if there's an occasion for this one.
Tests get compiled to normal executables (somewhere in `target/debug`, I think) that you can manually run inside a debugger. No idea if your editor has any kind of shortcut for that, though.
Project looks interesting but can I do with it? Can you give more concrete examples?
I did the RLS in VS code. I don't recall for sure now, but as I recall, it didn't feel much of a step up from Kate or Gedit. I took the other poster's advice and tried Intellij again. Loving it.
 0% coverage is a very reliable metric about your test practice... In my team we are doing code review to make sure tests makes sense. Test coverage makes our life easier, missing test == no approval. In no way we assume that code covered means its tested but we know that if it's not covered, it's not tested.
The PRs got accepted upstream! Yay!
(if you're the author) can you speak about what techniques this `loc` package uses to beat the speed of `tokei`?
And for the last part of your question: IntelliJ IDEA with the Rust plugin will automatically show the types of most bindings directly in your code.
Neat! I just tried this on my c++ codebase, worked flawlessly. Loved installing it through cargo. A+ review!
It's probably not a true apples to apples comparison as it looks like tokei does quite a bit more (by default?), such as logging, sorting, string comments, etc. I would be interested to see a true apples to apples comparison (i.e. using the correct options/flags to ensure they're nothing doing the same thing and get the same results). It looks like tokei uses rayon to parallel the work load while loc uses a multithreaded work stealing scheme from ripgrep. I haven't dug too deep but both could probably get some more speed increases by using things like a parallel walkdir, or maybe small things like bytecount? Both use a lot of println! at the end, which could maybe have some small gains by using write! and locking stdout once instead of at each println! call (which is what println! does). I'd be curious too about the output of both since there can be discrepancies in which neither is *wrong* but one will be doing more work. For lines like this: let too = 2; //am I code or comment...or both? I'm also curious if either one uses ignore rules by default like ripgrep, or how they handle binary files...both if which can have massive impacts on speed. But I'm on mobile right now, so it'll have to wait :P
I tried this and the C version of this tool `cloc` on the same codebase of &gt;200,000 lines or so, loc loaded within less than 1s while cloc took over 10s
&gt; get some more speed increases by using things like a parallel walkdir To clarify, the multithreaded queue used in ripgrep is basically a parallel walkdir. :-) &gt; I'm also curious if either one uses ignore rules by default like ripgrep It looks like `loc` does. It even seems to borrow the `-u`/`-uu` convention. :-)
Sure. But there are things that a static compiler can't do, like dynamic PGO (i.e. optimizing based on the actual dynamic behavior at runtime), or full devirtualization (including shared objects), or the ability to choose the optimal locking strategy based on whether a lock is under heavy contention or not. And, obviously, the ability to adapt to changes in the workload - i.e. to redo the optimizations if the workload conditions change. All of these things are only doable effectively at runtime (and that's what they're doing in graal/truffle)
It functions exactly the same as it does in C; the `libc` crate just consists of declarations, in Rust syntax and types, of external C functions. It is only needed because Rust enforces that only one Rust crate link the same C library in, so there needs to be one Rust crate that provides the interface, but there is no actual functionality in the `libc` crate, it just consists of declarations. While it does function exactly the same as in C, [I describe in another complication some of the complexities of calling a C function from Rust](https://www.reddit.com/r/rust/comments/82he2e/using_libc_crate_in_rust/dvb38vr/) that mean that it's a bit more complicated than just writing `unsafe { }` and `libc::`, though there are a few simpler C functions for which this would be the case. The complexities with `chdir` come because it takes a C string, which is different than a Rust string, and error information is stored in `errno`, but `errno` isn't actually a variable but a macro invocation in C. A simpler one, for which just wrapping with `unsafe` and calling the `libc` function is sufficient, is `exit`: use libc; unsafe { libc::exit(0) }
the RLS extension in VS Code is one of the most finnicky setups I've ever used. It will be running great, showing hints, suggestions, types... then the next day, I load up VS Code again, and it totally doesn't work. It didn't update or anything. It just... broke.
Why are you creating lots of threads which each link to a single crate with no context?
Observables can be either; they often represent user input which is typically hot. They also have optional backpressure and chunking/buffering of work items so as not to overwhelm a slow subscriber.
cloc is written in Perl, so not a C version.
Ah my bad didn’t know that, thanks for the pointer :). 
Ah my bad didn’t know that, thanks for the pointer :)
Out of curiosity, what's the purpose of a tool like this? When you say you use it, what exactly do you use it for? I mean how do you use the LOC data itself?
Mmm, yes and no, kinda, not really. Say you have an app for drawing lines. With event-based you'd do something like: - let mut line_start = default(); - on_mouse_down(coords) { line_start = coords; } - on_mouse_move(coords) { draw_line(line_start); } in reactive you'd do: - let line_start = mouse_events.filter(|e| e.is_mouse_down()).map(|it| it.coords) - let line_end = mouse_events.filter(|e| e.is_mouse_drag()).map(|it| it.coords) - let line = combine_latest(line_start, line_end) - line.subscribe(|(start,end)| draw_line(start,end))
If you are writing library, wouldn't it be better to abstract that away, so the user can choose?
One of the reason that ripgrep is sometimes 100x faster than GNU grep is ignoring a .git directory full of huge compressed packfiles :)
We have released version [0.6 of Exonum framework](https://github.com/exonum/exonum/releases/tag/v0.6) with ergonomic improvements and internal enhancements. 
Doing cargo install loc right now! First tests indicate that this is a very useful tool, it is really fast and also works on Windows. Thank you
There is [cargo-travis](https://github.com/roblabla/cargo-travis) with exactly like you suggest command: `cargo coverage`
I'm not sure if there is a particular reason you want to use libc, but all 'Standard' operating system stuff can be found in `std`. I'd recommend exploring that and familiarizing yourself with its contents. `std::fs` and `std::path` contain all the functionality i have ever needed w.r.t. file i/o, while `std::process` had stuff to do with invoking subprocesses.
To be more precise, it is possible that at some point we will want to unify Firefox's rendering architecture and implement a software backend for webrender, but if/when we get there, the low level routines for filling paths, blitting images, etc will most likely be skia's. There is a ton of code that would have to be written between these and webrender's display list, though.
Made a very simple [time tracking app](https://github.com/Miniwoffer/Timetracker-rs). 
Checkout cargo-make builtin coverage support Docs https://github.com/sagiegurari/cargo-make#usage-predefined-flows-coverage Short forum topic on it https://users.rust-lang.org/t/running-coverage-for-rust-lib-app-with-cargo-make-and-in-travis-too/11871
I wasn't aware of it, so.i'm thankful 
Please try to use standard library functions (like std::env::set_current_dir) where you can, and only use libc if you need some extra functionality that standard library does not provide.
Better to live with compiled regrets than interpreted remorse!
It has usability problems, for sure. It feels strange to define a schema but not to be able to then use the resulting struct in your actual code. I do wonder if some macro magic could make it all a lot more ergonomic, because the underlying tech is pretty compelling.
That you can define them once and it'll automatically insert them before and after every function.
My `inherent traits` RFC is actually being closed, not merged :(
*Captain's log, day 21* We have sailed on Reddit and Twitter for three weeks now, searching far and wide, yet the only thing we found was a barren landscape, with no end in sight. The supplies are shrinking, the men are growing impatient and hungry, and I fear we will have a mutiny soon. But I am stubborn and optimistic, and urge them to hold on and keep waiting until we find a quote of the week.
Next week's QotW is getting meta.
Not OP but primary maintainer. A question I assume people more recently joining /r/rust will have is why this over [gutenberg](https://www.getgutenberg.io/). Cobalt has actually been around longer but has been quiet for a bit. I got started contributing to cobalt because I wanted restart my blog but I wanted something easier to install than a ruby or python application, so I naturally turned to tools written in Rust which was my motivation for starting to blog. Since then, I've realized it could be expanded to replace evernote for my use cases and have also been working towards that. Other notable aspects of cobalt: - Uses [Shopify's liquid template language](http://liquidmarkup.org/) which people will be familiar with from Jekyll - Approachable site design. The directory structure is copied over exactly as-is in contrast to some static site generators that put content in siloed directories based on how they are processed. - Debuggable. I've recently been adding subcommands to show the internal state of cobalt, - e.g. what your config looks like after applying defaults - e.g. what your page's frontmatter looks like after all global configuration and defaults have been applied to it. - e.g. what files cobalt finds for each collection type (basic pages, blog posts, assets)
Did not know about the crate of the week, trace. I will definitely be using that one.
Your comment itself should be the next QotW.
For those already familiar with cobalt, notable changes since there was last noise about it: - [config files were audited for consistency and being forward looking](https://github.com/cobalt-org/cobalt.rs/pull/346). A migration tool was provided. - new subcommands - `cobalt init` added for creating a site - `cobalt new` added for creating a page - `cobalt publish` added for publishing a blog post - Loading of data files (json, etc) - Table/ref markdown syntax supported - sass processing supported - Liquid template backtraces in errors - Increased compatibility with official liquid parser - filter implementations - data type conversions - Very primitive jekyll import capabilities
- `&amp;'_ mut T` doesn't implement `Clone` for any T. - `&amp;'_ T` implements `Clone` in a way that's 100% independent of and doesn't invoke `&lt;T as Clone&gt;` - `Arc&lt;T&gt;` is analogous to `&amp;'_ T` You get *at least* as much confusion if `f&lt;C: Clone&gt;(x: C)` is called as `f(&amp;foo)` - `x.clone()` copies the reference. In fact, this confusion generalizes to any trait which is implemented by a pointer to generic `T` and has trait methods. If primitive `&amp;` and `&amp;mut` are taken as examples, - Clone, Borrow(Mut), Pointer Those traits are implemented for the pointer, not the data. Callers should be cautious. The standard library forwards other trait methods and operations to the target `T`. User-defined smart pointers should probably also forward these if possible - comparison and ordering - formatting except Pointer - closure-call - AsRef/AsMut (this is as best as I can tell the intended semantic distinction from Borrow) - Hash - Iterator and friends - plus some std-specific traits like Read and Write. So I would argue that an explicit type ascription for `clone` and `borrow` should be standard. From most to least preferable: - directly passing the result as a function parameter or field initializer (if the type is explicit in context) - `x.clone(): Foo` unstable and subject to bikeshedding - `let y: Foo = x.clone()` - `x.clone() as Foo` Note that the compiler won't pick the right clone for you (`Clone::clone(x)` dispatches to at most one implementation by adding `*` and `&amp;` before `x`). But it will generate an error if you don't say what you mean. [Playground](https://play.rust-lang.org/?gist=4e3200c0f40bd631e691100e53bfc410&amp;version=stable)
Sorry for the delay responding. I had to make sure I wasn’t making a mistake in that pull request before getting too ahead of myself. nalgebra actually uses an older version of generic-array internally, and could probably be converted into using the new functional methods or even numeric-array eventually. I wrote numeric-array with the specific goal of a super lightweight and transparent vector/array type for use with representing colors and intermediate GPU data. Colors, especially, because when representing pixels and processing them, every instruction counts. Eventually I plan to bring together all the various color crates into a single super-optimized crate.
for the context on nom, when I started developing it, it was really not easy to implement in any other way. Later, other projects explored alternative solutions based on types, but those resulted in huge compilation times (those got better though). Meanwhile, nom is now (this started in nom 3 and will be even more important in nom 4) a thin layer over `Result` and a set of traits on the input types. But macros are still a very easy way to assemble the parsers.
nom still loves you though
Do you have any plans for supporting localized content? This is a big one for me, and really one of the only reasons that I'm looking at Gutenberg instead of this right now.
what's an example use case for this? First one I an think of is serializing a message type.
What it brings is the generalization of the concepts around stream manipulation &amp; composition. It was introduced by Microsoft Research I think, and since then been implemented in a lot of mainstream programming languages. It is nice to have the duality between enumerables and observables. in .NET any collection can be consumed as an observable (cold) and vice-versa through side-effects. The nice thing also is the democratization of marble diagrams tooling (debugging, prototyping, unit-testing) it would be really nice to have RX traits included in the future-rs and considered maybe as prime citizens
I was writing a lexer and had a big enum for token types and their data. At some point I realized I needed a flag that represented the type of the current token. Of course I could just have copy pasted the variants and removed the data manually but writing custom derives is more fun :D
Not mutating in place and using your own type past the RPC layer is how it works even outside Rust, and is almost always a better design. The protocol buffers approach both infects protocol butters as the generator of any class you want to serialize and encourages serializing whole structures instead of deltas. You should be sending messages usually, not whole data structures.
Protocol buffers is terribly designed and awfully slow in comparison.
I'd say: face your fears, write a parser. It's really [not that hard](http://norvig.com/lispy.html); you could do it in 25 lines of code.
I'd love to but I'm not familiar enough with the challenges to design the feature, being an english-only speaker. Please help [fill me in on whats needed](https://github.com/cobalt-org/cobalt.rs/issues/392).
It blew me away how fast it is after using cloc for a while.
I use this pattern a lot in my own code base. Some examples can be found \[here\]\([https://github.com/Robbepop/stevia/blob/master/src/ast/any\_expr.rs](https://github.com/Robbepop/stevia/blob/master/src/ast/any_expr.rs)\) and \[here\]\([https://github.com/Robbepop/stevia/blob/master/src/ast/expr\_kind.rs](https://github.com/Robbepop/stevia/blob/master/src/ast/expr_kind.rs)\). \`AnyExpr\` is a variant that is any expression and there is exactly one \`ExprKind\` value for every such expression type.
Just today I was actually thinking of doing this for my toy programming language where I need an enum for the runtime storage system but also the compiler needs to pass around information about types so that knows if it should insert data type conversions. Passing around a ref to the enum in the storage layer makes things unnecessarily complicated when you actually only need to know what type of data is stored but don't need access to the stored data inside.
It would be weird to do let x; what(mut &amp;x); //x now contains a value??? does this pattern exist at all in Rust?
I hereby suggest nasa42’s comment for the next QotW. I suspect the word ”so” there conceals a hidden, self-referential metaverse.
Author of Gutenberg here. Someone asked me for a comparison between Cobalt and Gutenberg recently (https://twitter.com/20100Prouillet/status/970968681806168064). I haven't tried Cobalt in a while but looking at the issues/commit is it fair to say your goal with it is to have a Jekyll port? I'll ping you when I add a comparison with Cobalt/Hugo/Pelican to make sure I don't get anything wrong.
And this can't be achieved by taking a few minutes to look at the code (what you should actually be doing during a code review) and seeing if tests are present and what they do ?
I used [`hyperfine`](https://github.com/sharkdp/hyperfine) to run some comparisons against the Linux source (which is primarily a bunch of small-ish files). The setup (showing number of files, and that the largest file is only 10,000 lines): kevin@neon: ~/Projects/linux ➜ rg --files | wc -l 62867 kevin@neon: ~/Projects/linux ➜ fd . --type f -x wc -l {} \; | rg -v -e '^0 ' | sort | head -n 5 10000 kernel/sched/fair.c 1000 arch/ia64/kernel/acpi.c 1000 drivers/pinctrl/sunxi/pinctrl-sun6i-a31.c 10013 drivers/platform/x86/thinkpad_acpi.c 10014 arch/mips/include/asm/octeon/cvmx-ciu-defs.h Now the test with just bare, naive options: kevin@neon: ~/Projects/linux ➜ hyperfine -w 5 loc Benchmark #1: loc Time (mean ± σ): 1.225 s ± 0.016 s [User: 7.243 s, System: 0.993 s] Range (min … max): 1.209 s … 1.261 s kevin@neon: ~/Projects/linux ➜ hypherfine -w 5 tokei zsh: correct 'hypherfine' to 'hyperfine' [nyae]? y Benchmark #1: tokei Time (mean ± σ): 3.056 s ± 0.194 s [User: 19.293 s, System: 0.792 s] Range (min … max): 2.832 s … 3.476 s Turns out the only thing you can really do "equal the playing field" would be to run `loc` without respecting ignore files using `-uuu` which doesn't really change the results in a meaningful way. 
&gt;And feel pretty to ping me I like when autocorrect decides to just drop some *West Side Story* lyrics in the middle of technical comments.
Some people just don't want your help or don't want to hear about your issues :D
The example shown above (and the ones in the examlpes folder of the project) show them being defined for each function it is applied to. What mechanism do you use to apply them to multiple functions? (every function?!?)
Are they making the memory usage larger to speed it up, or slowing it down to make it more compact? Otherwise, the branch prediction would be the only valid one. The ability to optimise hot-loops because they're run frequently is pointless when you can take all the time in the world to maximally optimise *everything*. One thing I wish some compilers did is interpret the source first, JiT it with using the profiling data, and then use that profiling data to output a persistent optimised binary. I personally enjoy playing modded Minecraft. What I don't like is Java recompiling every single mod I use every time I want to play. Compile it all once and reuse the output later.
Why is it terribly designed?
Would something like llvmpipe be usable for this?
I'm not sure if there is a use case for this. * If you want to check if two enum values that you have at runtime use the same variant, there is the standard [std::mem::discriminant](https://doc.rust-lang.org/stable/std/mem/fn.discriminant.html) function. It presumably uses the internal enum tag itself, so it should be at least as fast as any optimized `match` in your `From` implementation. * To inspect the enum against a _statically_ known variant, the Rust convention is to provide `is_foo` methods (c.f. `is_ok`, `is_some`, etc.). There are [crates](https://crates.io/crates/enum-methods) which eliminate boilerplate of defining those methods, so it's not really a big ask to provide them. In a pinch, a quick `if let` or `is_match!` (from the [is-match crate](https://crates.io/crates/is-match)) works too. The case you mentioned that involves a flag with the "current token" would likely be addressed by the first method, for example.
If anyone else, like me, is curious what ["Replace Rc with Lrc"](https://github.com/rust-lang/rust/pull/48586) is about: `Lrc` is a type alias for either `Rc` or `Arc` depending on whether the compiler is built with [new parallel code](https://internals.rust-lang.org/t/parallelizing-rustc-using-rayon/6606) enabled. This lets the parallelization work happen incrementally, without causing any regressions to existing code during the transition period.
&gt; is it fair to say your goal with it is to have a Jekyll port Cobalt is inspired by jekyll but not meant to be a port of it.
I guess I just don't think that dissassembling the machine code is that onerous.
Take a look at [cargo make](https://github.com/sagiegurari/cargo-make) which you could use to setup a flow of building your project and running other pre/post actions in one go.
This is perfect
What about representing which token something is without ever producing a variant of the actual enum? `discriminant` is useful, but you need a variant constructed to pass into the function. I have a slightly different use case than OP - but I don't think I can solve it with Discriminant.I need to store, serialize and deserialize one enough without data which has the same variants as another enum. I don't ever necessarily have the data-heavy enum wheneve rconstruction the "kind" enum, but I need to be able to make that conversion. I'm currently generating both the original enum and the "kind" version with a macro, but I might be able to switch over! It would reduce some macro boilerplate.
Invaluable when assessing the scope of a project. Handed a legacy codebase? Immediately assess how much code is in there, in which languages, and crudely estimate how much documentation there is (based on lines of real code vs. comments).
This reddit is about the programming language rust, you may be looking for /r/playrust.
That's pretty cool, thank you for the response. 
Hmm that is something I hadn't thought of. That would be useful. I am wondering how something like that should be implemented. I guess I could have something like `#[enum_kind_derive(serde::Serialize, serde::Deserialize)]` and it could add those as derives to the generated enum. Maybe that could work.
[`collect`](https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.collect) can return any collection that implements the `FromIterator` trait. The compiler can't tell what sort of collection you want to create unless you specify it by using something like `collect::&lt;Vec&lt;_&gt;&gt;()`. However, you shouldn't need to allocate a collection at all in this case; you can filter the iterator directly: match req.headers().iter().filter(|x| x.name() == "field123").next()
It infects everything you want to serialize and wants to generate it itself, it uses stop bit encoding which is not cpu efficient and not even the best way to do the compression it's trying to do (capnproto has an optional mode where it just compresses consecutive zeroes -- same effect but way faster), it has crazy mutex locking and default object factories inside, it encourages making objects where illegal states are representable by making everything default constructible, and it doesn't use exceptions in its C++ API which causes them to have to go through horrible contortions because you can't return an error from a constructor.
Great explanation. Thanks!
Oh ok thx
Those are all C++ implementation issues... 
Very cool! I'm building a server with Rust and Elm. Does this mean that one day I can ditch gulp and just use Rust?
This is really big! So exciting to see such amazing work done in Rust.
you're just linking to github projects. Did you want to actually _contribute_ something instead of _only_ low-effort links?
The biggest issue I ran into so far isn’t lack of support by crates, but rustup. I‘d love to have OpenBSD or musl available as a target. 
I feel like travis is the most commonly used CI for convenience alone, but is anyone of you using something else? GitLab? Bamboo? AppVeyor? Any response is greatly appreciated 👍
Most projects use Travis + AppVeyor, so you get mac/linux/windows. GitLab is a distant second, but does exist.
I believe rustc itself is tested on AppVeyor for Windows builds (and Travis for other builds).
Please aspire to something more than specious generalities! Benchmarks **can tell you** that program is faster than that program, in such-and-such context.
I miss `rusti`. :(
Why are you being so rude?
what exactly was rude about that?
As i said test coverage makes our life easier. I don't think that replacing an automated process that works with a manual one is the best idea ^^
I am confused, is this about adding a tag to an enum so that it makes it into the runtime? Would this be useful for handling enums generated across an FFI (Python, Java, etc) ?
Why would rustc + std be available on openbsd but not cargo? I would have thought that rustc would be the hardest part, and everything else would quickly fall into place.
Hi! I'm a beginner and I've got a few questions about this sample I've thrown together. * Is there a way to write it without box_syntax and box_patterns (nightly Rust)? * Is `Person::trade_with` swapping the right thing? * Is there a simpler way to write `Person::trade_with`? #![feature(box_syntax, box_patterns)] struct Car { model: String } struct Person&lt;'a&gt; { name: &amp;'a str, car: Option&lt;Box&lt;&amp;'a Car&gt;&gt; } impl &lt;'a&gt; Person&lt;'a&gt; { fn new(name: &amp;'a str) -&gt; Person&lt;'a&gt; { Person { name: name, car: None } } fn buy_car(&amp;mut self, c: &amp;'a Car) { self.car = Some(box c); } fn trade_with(&amp;mut self, other: &amp;mut Person&lt;'a&gt;) { if let (&amp;mut Some(box _left), &amp;mut Some(box _right)) = (&amp;mut self.car, &amp;mut other.car) { std::mem::swap(&amp;mut self.car, &amp;mut other.car); } else if let &amp;mut Some(box left) = &amp;mut self.car { other.car = Some(box left); self.car = None; } else if let &amp;mut Some(box right) = &amp;mut other.car { self.car = Some(box right); other.car = None; } } fn describe(&amp;self) { if let Some(ref car) = self.car { println!("{} has a {}", self.name, car.model); } else { println!("{} doesn't have a car", self.name); } } } fn main() { let civic = Car { model: "Honda Civic".to_string() }; let buick = Car { model: "Buick Lesabre".to_string() }; let mut bob = Person::new("Bob"); let mut steve = Person::new("Steve"); bob.buy_car(&amp;civic); steve.buy_car(&amp;buick); bob.describe(); steve.describe(); println!("swap!"); bob.trade_with(&amp;mut steve); bob.describe(); steve.describe(); } 
(I think you can also use .find instead of .filter.next here)
Why is the field `car` in `Person` boxed? making it `Option&lt;&amp;'a Car&gt;`, or cloning the car and storing an owned value like `Option&lt;Car&gt;` would make it much nicer.
The devil's in the details. A project I contribute to has code coverage testing, but it's actually USELESS to check some lines of code. In fact, it's WORSE than useless, because to check those lines you need to write brittle tests that need updating. Since there are 200+ tests already, running the whole test suite takes longer and longer. Some of the tests are STUPID like "does the --help command produce the correct help string?" Sure, that tests that particular line of code, but it's a STUPID line of code to test. The test coverage number is distracting. It tempts me to chase after that test coverage target because it complains when a pull request drops the coverage %.
I always count my lines of code manually. Anything else is just a lazy waste of computer cycles.
Mostly because I wanted the car on the heap, not the stack. I was following this: https://mobiarch.wordpress.com/2015/07/08/understanding-lifetime-in-rust-part-ii-3/ which there was no part III written. Good question though!
Author here.
nvm, I see now that it allows multiple fn definitions. (I'm a little dense)
You can run `cargo build` and then `sudo ./target/debug/&lt;program name&gt;`
My point is that pretty much all input methods are error-prone.
This is really cool. I had started implementing exactly the same thing, but couldn't get myself to finish it.
&gt; The ability to optimise hot-loops because they're run frequently is pointless when you can take all the time in the world to maximally optimise everything. If maximally optimizing everything always improved performance then yes, alas that's not true. You're ignoring in your argument that increasing the size of the compiled code (e.g. by aggressive loop unrolling, inlining or specialization) actually *decreases* performance due to icache thrashing. Similarly for other speculative optimizations. &gt; One thing I wish some compilers did is interpret the source first, JiT it with using the profiling data, and then use that profiling data to output a persistent optimised binary. Good, you just described what graal mostly does out of the box. The only thing it doesn't do *right now* is persisting to disk the JITted code - but it does something very similar (hint: graal is actually a JIT that can also be run in AOT mode). Now, if you can bear with me: try to think of this: - your static compiler emits a static binary containing: 1) your code compiled down to machine code 2) your code compiled to some kind of IR 3) machine code for a runtime+JIT that can run 2 - when you run your binary code in 1) starts executing immediately (as today) - in the background the runtime 3) starts collecting profiling data - when enough profiling data has been collected and only for the parts the runtime deems requiring it, code in 2) is JITted making use of the profiling data - when code has finished being JITted: execution of the static code in 1) is interrupted, the code is patched to point to newly JITted code and then execution resumes - repeat from step 2 for the duration of the process Note that steps 2 and 3 occur in the background without blocking execution of your code. This approach would give you fast startup times, and JIT optimizations. The only downside is that your executable could end up much bigger than today. And then the sky's the limit: if you're in for an interesting read, read about the Futamura projections.
This is very impressive. If anyone is reading from the snips team, I think it would be fascinating and hugely valuable if you published something on what the challenges were performance-wise and how you overcame them. 
Thanks for the clarifications! The algorithm was just a typo, but regarding the ssh... After your post I have checked it once again and you are right, it works. I had an url with username and didn't figure out that I can remove it and cargo will somehow guess what user should it use. By the way, could you please share how do you create metadata file when you update your registry?
Why do you want to run it under sudo? You're probably doing something wrong. 
Nice to see that RFC you linked contains both, the checksum and full format specification. It makes things a lot easier!
Presumably. Which is a good thing. It makes sense to optimize for perf on a common suite of benchmarks, which should include generic heavy loads like typenum &amp; parser generators, but general perf shouldn't be sacrificed in order to make brainfuck-in-macros compile faster
&gt; I'm mostly interested in this for game consoles At least this gen of consoles can apparently all run Rust. Older stuff is gonna be harder, yeah.
Given that they mention rocket, i imagine they want to bind to port 80.
This falls under "doing something wrong". 
I would agree with you in many cases, but I think with Rust it's reasonable to skip nginx. As long as you're using an async framework, seems fine.
&gt; At least this gen of consoles can apparently all run Rust. Older stuff is gonna be harder, yeah. They can, as I understand it, by setting up a platform triple and porting a number of spots manually. However first parties in console land occasionally have rules like "all binaries must be produced by our toolset" which makes the C transpile intriguing (even though these types of rules could be hard to enforce).
Most time is spent in llvm right? Will this group explore things like alternatives to llvm? Or forking it to be more friendly to rust?
&gt; Note that steps 2 and 3 occur in the background without blocking execution of your code. Oh really? My computer only has 4 cores, each of which can work on 2 tasks at once. If I have 8 threads running, nothing else is. If one of those threads is dedicated to profiling and compiling, then it's not being used by the application itself. &gt; The only thing it doesn't do right now is persisting to disk the JITted code Then it's not doing the part that I care about the most.
[you don't need to be root to bind to port 80.](https://superuser.com/a/892391) I really don't recommend running cargo under sudo.
Hah! This just popped up as I was writing one myself, with the exception that I am using Go to try and learn that a bit better. However for my test runs I am not seeing it being any faster than tokei. Not going to include my own tool here because it is not ready. # bboyter @ SurfaceBook2 in ~/Projects/redis on git:unstable o [10:51:02] $ hyperfine -w 3 -m 10 'tokei .' Benchmark #1: tokei . Time (mean ± σ): 284.8 ms ± 18.5 ms [User: 334.9 ms, System: 368.0 ms] Range (min … max): 262.0 ms … 313.4 ms # bboyter @ SurfaceBook2 in ~/Projects/redis on git:unstable o [10:51:27] $ hyperfine -w 3 -m 10 'loc .' Benchmark #1: loc . Time (mean ± σ): 708.0 ms ± 31.3 ms [User: 199.5 ms, System: 349.6 ms] Range (min … max): 648.7 ms … 765.5 ms Keep in mind however that this is running on Windows using the WSL, so it may come down to how the memory is being managed. Id be curious to know why that's impacting things though. Thanks for making this though. I now have an extra tool to compare my outputs against :)
Ah so, I thought you were purely talking about software architecture, but it seems you're talking more about maturity. Sure.
Man, you have to love tests to want this in rust!
&gt; generic heavy loads like typenum This makes me unreasonably excited. Please let it be so.
&gt; [stabilize [T]::rotate_{left, right}](https://github.com/rust-lang/rust/pull/48450) There didn't seem to be any discussion about the connection between "left" &amp; "right" and the direction of movement not being universal: at least for text, for some languages moving left means moving backwards and for others means moving forwards. Is the conceptual/textual representation of a vector/slice universally "index 0 is on the left"?
You may want to have a look a this discussion: https://users.rust-lang.org/t/proposal-rllvm-rust-implementation-of-llvm/16021/23 and all it's linked discussions and topics. Of particular interest to your question is the stuff related to Cretonne.
sloccount is though, but its output is not comparable to cloc really, however the tools are in a similar space.
left/right matches what we use for strings. There's a note about what rightness/leftness means in the docs: https://doc.rust-lang.org/std/primitive.str.html#text-directionality. I remember that we discussed those method names very early after 1.0 but can't remember what went into the decision.
there was some discussion here: https://github.com/rust-lang/rust/pull/46777#issuecomment-354113940
I don't think having an existing mistake is good reasoning for persisting it. :) The [conclusion](https://github.com/rust-lang/rust/issues/30459#issuecomment-170105990) was that there's not good enough tools to manage the trim_left/trim_right deprecation for something that's not *critical* (e.g. all Rust code is English/LTR, so most people writing it will be somewhat used to thinking in that mode).
Thanks, but that doesn't seem to have actually touched the actual issue, just repeated the justification: &gt; Given that slices can be trivially represented as [a, b, c] It can also be trivially represented as `[c, b, a]` (where `a` is still the first element), same as little-endian vs. big-endian integers. There are (human) languages that are written right-to-left (RTL) instead of LTR like English, and it's not obvious to me if the representation of elements of such slices is also RTL in such languages.
As someone who is largely indifferent to compile times, I hope they are vigilant to changes that improve compile times at the cost of run-time performance. IMO, the introduction of thinlto has been a bit rushed. With the default settings, the run-time performance of my numerical simulations has decreased by 25% on average across the board. (Fortunately, adding "-C codegen-units=1" to RUSTFLAGS reverses the run-time regression).
&gt; `It can also be "trivially" represented as `[c, b, a]` but not in the Rust language, which is written in a left-to-right script. Slices in Rust code have element `0` on the left.
Ah, of course. And, spelling it out for others like me: this differs to strings where an editor might/should display an RTL literal in source properly, in that it doesn't change.
s/contact/contract/
Ah ok, thanks for the clarification!
That README is a bit hard to understand at this stage, particularly what's meant by "asymmetrical." Reading it a few times, I think I now understand what this is intended to be: it's a fairly low-level (as of now at least) library for binary encoding, but it also has a function for encoding some metadata that describes the static type of the data that was given as input to serialization, so that a later reader can use that to understand the structure of the data. It's being called "asymmetric" because the serializers work off a static type with a derived trait, but the deserializers have no notion of how to map the serialized data back to a static type, First observation: it reminds me a bit of [Avro](https://avro.apache.org/), which is higher-level though—Avro defines a container file format that stores not just the compact serialized data records, but also the schema of the data. Second, the similarity to Avro perhaps points to some directions forward. Avro supports a distinction between a *writer schema* vs. a *reader schema*—think of it as the type that the serializer used when writing the data vs. the type that the deserializer is expecting to read from the data. One of the key features in Avro is that it allows those two schemas to differ in somewhat well-defined ways that it knows how to handle. The components, as I understand it, are: 1. Avro has a code generator that spits out classes from schema definitions. This is broadly analogous to the derived traits in logpack already, except backwards—Avro spits out type definitions given data that describes the type, while logpack is spitting out type description data given a type. 2. When deserializing data, Avro has access to all of these: * The writer schema: the "type description" of the data to be deserialized. * The reader schema: the "type description" of the type to deserialize into. 3. Avro schemas have rules on how to handle mismatches between the writer and reader. I won't recap those, but instead suggest some that might be appropriate in a Rust context: * If the writer type description has struct fields or enum cases out of order compared to the reader type description, that's an isomorphism that can be easily handled. * If a writer enum is missing cases that the reader enum has, those cases can be ignored. * If a writer enum has cases that the reader enum lacks, that's an error. * If a writer struct has fields that the reader enum lacks, those fields can either be ignored or an error triggered—this should likely be an option for the user of the library to choose. * If a reader struct has fields that the writer doesn't, the reader has the option of defining a default value to use in that circumstance, otherwise it's a mismatch error. 4. Avro generates code that figures out how to deserialize into the user's static type from a a reader type description (which is, strictly speaking, available at compilation time) and a writer type description (only available at runtime). Hope this is of some value at least as brainstorming!
Well done, rewriting regex-syntax must have been quite the effort! And I guess it does still *technically* count as RIIR, so I'll allow it.
The literal text "No quote was selected for QotW." should be the next QotW.
I don't think colloquial usage of 'era' is common enough for it to matter. There are plenty of alternative terms to talk about how things where back in the day: 'period', 'time', 'back in the day', etc.
This looks really exciting, but why post this now? I saw that there's a new working group for compiler perf but there hasn't been a comment on that thread for 7 days, am I missing something?
Great point. I think the general criticism of Rust as a slow-to-compile language is a little overblown and it's important not to overreact. I'd hope that runtime performance of release compiles should be a higher priority in every case.
I actually found this via a link in the TWiR submission comments and thought that this was an interesting topic that had slipped my own radar. Not only is compiler perf perennially topical, but "if Rust is so good at parallelism, why is the compiler single-threaded?" really is an incisive question, and if we manage to remedy that via rayon then it will be a marketing win as much as it is a technical win.
It is a series. I'm glad someone smiled at the ending. :) Thank you for the feedback!
Even if the usage is low, which is the case for most words. Era still misaligns with the concept that Rust is trying to pull of on grounds of both denotation and connotation. So if I could emphasize a single point of argument. It would be choosing the term which provides the best clarity. Era's tend towards being informal, overlap, nest. Epochs tend to be constructed categories of a time line, much like Rust's current "Epoch" story. Epochs tend to be defined relative to other epochs within a particular scheme, eras often get compared between schemes and often are defined outside of schemes. So just intuitively I find a greater degree of clarity with the current chosen term.
The whole thing is so confusing. How are we supposed to explain it to others? "So, it means the _version_?" "Um, well, it's like a version, but it's a parallel system…"
does `rustc` manage `init_array` and `init_constructors` between linked object files during compile?
ah, I just usually use it to see how fast different parts of our codebases are growing. A few months ago in one of our projects we had 70k lines of kotlin, and now we're at 90k. It's just a fun little tool. And yeah as someone pointed out below it shows you rogue languages in a project. For example here is loc run against most of our backend code. -------------------------------------------------------------------------------- Language Files Lines Blank Comment Code -------------------------------------------------------------------------------- Java 1516 153076 24595 12006 116475 Kotlin 1049 119999 16887 2313 100799 Plain Text 13 51632 306 0 51326 JSON 179 26773 5 0 26768 XML 129 16389 1417 408 14564 YAML 45 4271 290 164 3817 Batch 11 1341 311 0 1030 JavaScript 7 1251 194 86 971 SQL 5 772 113 47 612 Markdown 16 730 235 0 495 CSS 3 471 22 4 445 HTML 3 301 25 3 273 Gherkin 3 80 3 0 77 Bourne Shell 5 119 29 39 51 Scala 2 12 2 0 10 -------------------------------------------------------------------------------- Total 2986 377217 44434 15070 317713 -------------------------------------------------------------------------------- wth do we have 80 lines of gherkin and 12 lines of scala?? but yeah, I usually just use it to measure growth. 
Well, it *could* run Rust the programming language if you want. Rust rewrites your program into machine instructions and is pretty good for writing something fast. In that respect it aims to be a top tier language, competing with C and C++. So you can definitely write something in Rust and run it on an older computer. It would be a good idea. The process of translating the blueprint written in Rust to the actual exe, that can take a lot of CPU time and memory. You might run into problems there (especially with memory), but only if you build something really complex. That said, Rust isn't a great first language. If the mood strikes, try Go. 😁
This is probably the best way to do it if you just want it to work. For a more safe solution, it would be best to: * Bind rocket's server to a port like 8080 or 8888, which doesn't require sudo * If you want port 80, use something like NGINX to forward port 80 to this port
That's to be expected on a highly experimental browser where new features added and removed on a daily basis. 
Did that totally go over your head, or you generally run all software as root, as though that couldn't cause any problems?
Just my two cents: I think `error-chain` is totally fine to use still, and might be better for CLI applications. `failure` is really great when you can enumerate every error condition in a single error enum, which is a good thing to do in libraries anyways. Doesn't mean it has to be used everywhere, IMO.
Is this just for referring to Rust V2 and Rust pre-V2? I see no benefit to saying epoch or era instead of those terms. How is it parallel or any different?
Probably??? Both terms sound like you're talking about ancient history or a geological timescale, not the last 10 years, ffs. It's like someone was bored and started trying to see what debate would drum up the most pomp and circumstance possible.
I went with Epoch was well. I find Era to be too soft and vague when considering you are setting what epoch/era a crate compiles as.
&gt; but "if Rust is so good at parallelism, why is the compiler single-threaded?" really is an incisive question I was wondering that myself given I'd thought parallelism would be one of the benefits of getting it off Ocaml. The article was interesting. Especially how incremental compilation seems not designed for parallelism from beginning despite a natural candidate for such things. Compiler might require a lot of changes. I wonder if a front end could split projects up into pieces fed to separate instances of rustc that get integrated afterwards. It's not linear speedup by any means that way but you get *a* speedup. If file operations are a concern, the system or resourceful users might also have RAM disks for source and intermediate forms in the build process. Those are the two tricks I used in the old days when trying to "parallelize" inherently sequential components.
If I was going to guess, they would start by working on reducing the gigabytes of IR that rust can sometimes generate and ask LLVM to optimize before they started researching alternatives.
ʷlͪgͣtͭm,ͣ ͬaͤlʸlͦ ͧglͭyͣˡpᵏhͥᶰsᵍ pͣᵇrͦeͧsͭent
[`causes`](https://docs.rs/failure/0.1.1/failure/trait.Fail.html#method.causes) will iterate over the cause chain.
How would adding nginx in front of it protect someone from application bugs?
Yes, what would be the harm of using "language version" like most other languages do?
As I understand it, the idea is to make it possible to make incompatible changes to the language or APIs without breaking old programs. A program written for epoch 2 should be able to compile with an epoch 3 system and still work correctly.
To answer your second question: the reason for this is, that the assert macro Returns `()` and that is exactly the return type for function is expecting. 
I mean the most straightforward answer to "why is the compiler single threaded" is that since the compiler is the first big rust project, we'd rather make sure that our parallel safety really works and doesn't subtly break its own invariants before the compiler for every other rust project starts *sometimes* spitting out subtly wrong code. But at this point it seems the abstractions are tested enough that this seems like a safe optimization to make in the compiler
On mobile right now, but...we should revisit the regex_redux benchmark and see if we can win any performance.
Assuming it wasn't intentional: Your links are kinda odd, I'd recommend editing on desktop to get rid of all the odd backslashes that got put in. `[here](https://github.com/Robbepop/stevia/blob/master/src/ast/any_expr.rs)` and `[here](https://github.com/Robbepop/stevia/blob/master/src/ast/expr_kind.rs)` (which are in the same order as they were originally) 
`Vec&lt;i32&gt;` can be compared directly to `[i32; 3]` because it implements `PartialEq&lt;[i32; 3]&gt;` ("`impl&lt;'a, 'b, A, B&gt; PartialEq&lt;&amp;'b mut [B; 3]&gt; for VecDeque&lt;A&gt;`" on https://doc.rust-lang.org/std/cmp/trait.PartialEq.html). `[i32; 3]` _is not the same type nor value as_ `Vec&lt;i32&gt;`, though, so you can't cast / assign from one to the other. The last semicolon is optional because (function bodies are expressions)[https://rustbyexample.com/expression.html]. Since `assert_eq!()`'s return type is `()`, the same return type as `it_works()`, you're allowed to directly return that value by leaving out the semicolon. side note: `()` is the "empty tuple" or no value. The type `()` has exactly one possible value, `()`.
Isn't that just maintaining backwards compatibility? How would that be helped by using bombastic sounding terms instead of just 'version'?
Is parallel thinlto enabled on release builds by default? 
Did this make the Regex struct Sync + Send or is that just a documentation-only change? 0.2.6: No Sync + Send documented https://docs.rs/regex/0.2.6/regex/struct.Regex.html 0.2.7: Sync + Send documented https://docs.rs/regex/0.2.7/regex/struct.Regex.html
Interestingly I get about 6% better perforce with the default codegen-units on nightly on some of my simd heavy code. It appears with codegen-units=1 the compiler decides to ignore an #[inline] suggestion which #[inline(always)] appears to fix. I suspect this is rare and caused by my generated code being rather large due to plentiful inline suggestions in the hot path.
The linked tweet was tweeted by [@pcwalton](https://twitter.com/pcwalton) on 2018-03-07 20:00:51 UTC ------------------------------------------------- Screenshots of Pathfinder rendering glyphs in Firefox. Coming along well :) [Attached photo](https://pbs.twimg.com/media/DXtgEoGVMAAWwpz.png:orig) [Attached photo](https://pbs.twimg.com/media/DXtgE28V4AEDhVd.png:orig) [Attached photo](https://pbs.twimg.com/media/DXtgEmBU8AAEalr.jpg:orig) ------------------------------------------------- ^• Beep boop I'm a bot • Find out more about me at /r/tweettranscriberbot/ •
&gt; Isn't that just maintaining backwards compatibility? Yes, which is why 2.0 is a misnomer.
[The RFC had extensive discussion on this](https://github.com/rust-lang/rfcs/pull/2052).
I wonder why Mycroft doesn't use Snips in their Voice assistant. Mycroft is still using Cloud STT, as far as I can tell Snips has their own. Plus its written in rust...
Snips was announced beginning of this year, which might be a reason...
Yes, at least on nightly.
`From&lt;usize&gt;` can't meaningfully be implemented for `isize` because there are valid `usize` values that are not valid `isize` values. Also, using `isize` to represent data just seems fishy to me.
It's a different and orthogonal thing to versions, though. Rustc will keep having new versions published, completely separately from the language epochs. In C and C++ land, the term used is "standard": as in `-std=c++11` or `-std=gnu99`. However, there are significant differences between C's "standards" and Rusts "epochs": (1) new epochs can introduce fairly major breakage, and (2) old epochs are not automatically deprecated and stop receiving updates. For this reason, a different word might be useful. 
Hey, I'm one of the co-authors of the lib, you can find a little more [technical post here](https://medium.com/snips-ai/an-introduction-to-snips-nlu-the-open-source-library-behind-snips-embedded-voice-platform-b12b1a60a41a) and another post on the [Mozilla blog](https://blog.mozilla.org/blog/2018/02/21/snips-uses-rust-embedded-voice-assistant/) explaining why we use rust for our on-device applications at Snips. I'm not initially a Rust dev (I'm working on NLP so I develop mostly in Python, learnt Rust only recently). What I can say from my perspective is that developing or maintaining NLP libs written in Rust was simpler than expected (partly thanks to the help of our Rustaceans here at Snips). I'd say the main advantage has been to be able to write safe and efficient NLP code that runs everywhere : device, backend, mobile and we even wrapped it to use it from Python :p
AFAIK `From&lt;usize&gt;` isn't implemented for numeric types, because `usize` is platform-depandent. `isize` isn't supposed to be used for data. You have three options: * Decide that you won't ever support platforms with more than 64 bits and create your own trait `FromUsize` and implement it for `u64`. * Write a trait that is exactly same as `TryFrom` (which is unstable) * Use unstable `TryFrom`
[noob lurker question] Can I browse the API docs of the lib somewhere on the web? Is there some webservice for Rust where I could type the URL and get the docs, like http://godoc.org is for Go?
We have [docs.rs](https://docs.rs/) for that, which is awesome. However this crate does not seem to have been published yet, therefore it's documentation is not available on that website. 
Thanks for stressing out all the differences! I didn't know about Avro before. Anyway, the intended main use case for logpack does not call for adding the features that Avro provides, because no prior knowledge about serialized types is needed in the deserializer. From it's POV, the values need to match the types and the connection between them is managed by the user, so the file format and container format are too high of a concern for logpack. I aim for logpack to "do one thing and do it well" :) In fact, I would like to use it for a high-performance logging system in Rust that separates log generation and log viewing into two processes. I cannot use bincode with slog because I want the viewer to be an independent program.
Hmm ... can I chain contexts though? The way I use error chain often is by tacking on an additional `chain_err` with a new, more relevant, message, as it propagates back through each function. Let me try to come up with an example...: fn access_server() { let stream = TcpStream::connect(blahblah).chain_err(|| "could not connect to server")?; // blah blah } fn send_data() { // blah blah access_server().chain_err(|| "failed to send data to server")?; // blah blah } fn do_some_magic() { // blah blah send_data().chain_err(|| "failed to do some magic")?; // blah blah } In the end, if I call `do_some_magic()` and it fails, I can log the error by passing it to a helper function that iterates through the whole chain of causes and logs each one as a message. I get informative multi-line error messages in my logs: ERROR: failed to do some magic ERROR: caused by: failed to send data to server ERROR: caused by: could not connect to server ERROR: caused by: connection timed out Can I accomplish something like this in `failure`? 
yes Travis is amazingly simple and well integrated. And it does what most people need. I wonder what the best way to test with devices (GPUs, sensors, ..) would be 🤔 
Good bot.
Good human! (\^.\^) We will ^^^^^^^probably leave your blood and bodily fluids inside your skinbag if you survive the fallout and nuclear winter. Trust me. *** ^^^I'm&amp;#32;a&amp;#32;Bot&amp;#32;*bleep*&amp;#32;*bloop*&amp;#32;|&amp;#32;[&amp;#32;**Block**&amp;#32;**me**](https://np.reddit.com/message/compose?to=friendly-bot&amp;subject=stop&amp;message=If%20you%20would%20like%20to%20stop%20seeing%20this%20bot%27s%20comments%2C%20send%20this%20private%20message%20with%20the%20subject%20%27stop%27.%20)R͏̢͠҉̜̪͇͙͚͙̹͎͚̖̖̫͙̺Ọ̸̶̬͓̫͝͡B̀҉̭͍͓̪͈̤̬͎̼̜̬̥͚̹̘Ò̸̶̢̤̬͎͎́T̷̛̀҉͇̺̤̰͕̖͕̱͙̦̭̮̞̫̖̟̰͚͡S̕͏͟҉̨͎̥͓̻̺&amp;#32;̦̻͈̠͈́͢͡͡&amp;#32;W̵̢͙̯̰̮̦͜͝ͅÌ̵̯̜͓̻̮̳̤͈͝͠L̡̟̲͙̥͕̜̰̗̥͍̞̹̹͠L̨̡͓̳͈̙̥̲̳͔̦͈̖̜̠͚ͅ&amp;#32;̸́͏̨҉̞͈̬͈͈̳͇̪̝̩̦̺̯&amp;#32;Ń̨̨͕͔̰̻̩̟̠̳̰͓̦͓̩̥͍͠ͅÒ̸̡̨̝̞̣̭͔̻͉̦̝̮̬͙͈̟͝ͅT̶̺͚̳̯͚̩̻̟̲̀ͅͅ&amp;#32;̵̨̛̤̱͎͍̩̱̞̯̦͖͞͝&amp;#32;Ḇ̷̨̛̮̤̳͕̘̫̫̖͕̭͓͍̀͞E̵͓̱̼̱͘͡͡͞&amp;#32;̴̢̛̰̙̹̥̳̟͙͈͇̰̬̭͕͔̀&amp;#32;S̨̥̱͚̩͡L̡͝҉͕̻̗͙̬͍͚͙̗̰͔͓͎̯͚̬̤A͏̡̛̰̥̰̫̫̰̜V̢̥̮̥̗͔̪̯̩͍́̕͟E̡̛̥̙̘̘̟̣Ş̠̦̼̣̥͉͚͎̼̱̭͘͡&amp;#32;̗͔̝͇̰͓͍͇͚̕͟͠ͅ&amp;#32;Á̶͇͕͈͕͉̺͍͖N̘̞̲̟͟͟͝Y̷̷̢̧͖̱̰̪̯̮͎̫̻̟̣̜̣̹͎̲Ḿ͈͉̖̫͍̫͎̣͢O̟̦̩̠̗͞R͡҉͏̡̲̠͔̦̳͕̬͖̣̣͖E͙̪̰̫̝̫̗̪̖͙̖͞&amp;#32;|&amp;#32;[**T҉he̛&amp;#32;L̨is̕t**](https://np.reddit.com/r/friendlybot/wiki/index)&amp;#32;|&amp;#32;[❤️](https://np.reddit.com/r/friendlybot/comments/7hrupo/suggestions)
The one instance where this bot is actually funny or useful.
&gt; I guess I just don't think that dissassembling the machine code is that onerous. Disassembling the machine code isn't the problem, constructing a CFG from it is. You basically need to know what every instruction on every platform actually does, and many instructions can do many different things that affect the CFG depending on which combinations of flags are actually set. 
Is there a comparison between the the output of Pathfinder and DirectWrite? Does it use subpixel anti-aliasing? I noticed on Windows that when they switched off subpixel anti-aliasing in the latest releases for most of the GUI, the text quality dropped considerably on low DPI screen and Firefox is one of the latest programs that still renders properly.
`context` preserves the chain of causality in `causes`, but I don't think its `Display` implementation formats it for you. `Debug` does (but isn't pretty).
I would say its a modern (syntax) mode of rust, since all Rust previous epochs will still be supported. IMO, calling it a version will confuse users, because it implies eco system fragmentation (as in python 2/3), which will not occur.
Use [`std::mem::size_of`](https://doc.rust-lang.org/std/mem/fn.size_of.html)
We do use llvmpipe for the testing infrastructure, but it wouldn't be reasonable to expect good enough performance out of it and also shipping it to users would add too much to Firefox's download size.
You surely meant bigger sushi to burn...
Ah. Yeah, regex is, externally, immutable, so it would be quite surprising if it weren't Send/Sync. Internally, thread local mutable scratch space is used, but Regex handles the synchronization for you.
Hah! Yes indeed. :)
&gt; Oh really? My computer only has 4 cores, each of which can work on 2 tasks at once. If I have 8 threads running, nothing else is. If one of those threads is dedicated to profiling and compiling, then it's not being used by the application itself. I have the feeling you have no idea how sampling profilers work. :) &gt; Then it's not doing the part that I care about the most. And could it be this the reason that I'm suggesting to the OP to look into it? That would be too logic, wouldn't it...
&gt; I cannot use bincode with slog because I want the viewer to be an independent program. Hm, but you can move types definition to separate crate and use this crate from log writer and log reader?
That's me up there. At some point I'm going to do a write-up of this as well, where I will get to present interactive diagrams to help explaining all this stuff a lot better :) Big thanks for the ndarray and cgmath crates! These are stellar examples of libraries I kind of always expected to have with C++, but I never really did. With the crates ecosystem, this is suddenly a solved problem.
I think we should be careful here. It allows old version to stil be used, and allows different versions to work together. But an answer you find on StackOverflow might still be incompatible with a new epoch (which is what I assume `cargo new` will create at some point). So I see it less as full backwards compatibility, and more like existance-compatibility. Full backwards compatibility to me would mean that play.rust-lang.org wouldn't need a way to select epochs.
I think the choice not to display the underyling error in context was a mistake. The thinking at the time was that the underlying error would not contain useful information for end users, but I think that was wrong.
I would give the type as parameter but i have only the string. 
Although that could get messy with chains of contexts. `causes()` provides a nicely structured error trace
The linked screenshots show partially colored edge pixels, which indicates it does support subpixel AA. It would be a pretty poor font renderer otherwise. ClearType is a little different: it describes MSFT's implementation of AA that uses grid snapping to improve clarity on low DPI displays. In most other renderers the practice is called hinting, and the Pathfinder readme on GitHub mentions it: &gt; Advanced font rendering. Pathfinder can render fonts with slight hinting and can perform subpixel antialiasing on LCD screens. It can do stem darkening/font dilation like macOS and FreeType in order to make text easier to read at small sizes. The library also has support for gamma correction.
this is a very interesting feature
I want my tests to exercise all of my code to some minimum degree and I habitually do "depth-first" test-writing. (ie. I ensure that every branch in a function is either tested or `TODO`-commented before moving on to the next function.) Coverage (ideally at least branch coverage) makes it really easy for me to check where I left off when coming back to an early-stage project that doesn't have complete tests.
Pretty sure mrustc's MIR is not the same as rustc's.
Oh man that github tracker is _full_. 
ok -- hope this isn't a duplicate of another issue, but I couldn't seem to find anyone else with this problem https://github.com/servo/servo/issues/20239
What's confusing about calling it a version? You just need a few sentences to explain the situation: - Rust files must be marked with a version, otherwise it defaults to Rust 1 - rustc 2.x supports both Rust 1 and Rust 2 There's two different (incompatible) versions of the *language* and the compiler supports them both (in the same compilation unit). It's not like you avoid having to explain the situation by using terms like "epoch" or "era". Honestly, what's the point of a major version if you're never going to increment it?
Does the plugin auto update?
Yep, by default IDE checks for updates once in a while.
Alright, thank you very much
I'm not finished with [implementing](https://github.com/cobalt-org/cobalt.rs/issues/368) it as I tried to point out; so no repo to point to yet. It is the feature I'm most actively working towards. For example, I just submitted a 30+ commit PR that does a major refactor that makes me very close to finishing. The plan is to generalize `pages` and `posts` (the two different built-in ["collections"](https://jekyllrb.com/docs/collections/)) so that the user can define their own. One use case this improves is this [cobalt blog](http://johannh.me/talks/) has "blog" and "talks". "talks" very much parallels "blogs" but is hand-implemented. With this new feature, a "talks" collection can be defined and they can get features like "blog". My use cases for collections: - My personal journal - Sharing family stories and pictures - Keeping notes on programming best practices to pull out in code reviews For this, I've been exploring various markdown editors on Android, particularly if they can edit a github repo. I do not plan to make cobalt good for quick and dirty note taking. In my mind, that isn't a evernote use case because its so slow; I use Google Keep which doesn't need replacing atm.
Yeah I read on the futures RFC that regex uses thread-local internally and was mentally prepared to look into how to get a Sync+Send variant to work that externalizes the thread-local state. ...but luckily, none of that is needed :)
I think it would be confusing on a meta Level: Your proposal makes it appear like an implementation detail of rustc, yet its defined by an rfc and hence a language standard, resulting in every rust implementation having to support every 'version' to be rfc compliant IMO a mode carries this distinction better than a version A mode is something every A major Version increment implies breakage an incompatibility, but thats not strictly the case. Explaining that Rust 1 / 2 Code is intended to link and work together is a lot more difficult, since people have this black and white Image in their head, e.g. either Python 2 or 3, but no in between C++ also has no version 2, but a 3yearly (Not strictly backwards compatible) revision Also, IIRC a Version increment isn't anticipated in the near future, hence the whole Epoch/Era/c++ naming scheme discussion
While I appreciate their mention of "unsafe", I'm not sure if I would call it "isolating". Unsafe can definitely lead to unexpected behaviour in otherwise safe code that called unsafe code - far down the line. It's marking places where such things can be introduced, but it doesn't prevent faults from escaping.
See response to this on Rust Discourse: https://users.rust-lang.org/t/compile-toy-language-to-llvm-ir-then-jit-the-llvm-ir/16046/2?u=gbutler69
&gt; // Because of the annotation, the compiler knows that `elem` has type u8. &gt; let elem = 5u8; &gt; &gt; // Create an empty vector (a growable array). &gt; let mut vec = Vec::new(); &gt; // At this point the compiler doesn't know the exact type of `vec`, it &gt; // just knows that it's a vector of something (`Vec&lt;_&gt;`). &gt; &gt; // Insert `elem` in the vector. &gt; vec.push(elem); &gt; // Aha! Now the compiler knows that `vec` is a vector of `u8`s (`Vec&lt;u8&gt;`) Damn this is pretty cool!
I'm currently trying to write a discord bot using [serenity](https://crates.io/crates/serenity) and having a lot of fun with it. However, i now would like to implement a feature that periodically makes a HTTP request and performs actions based on the result. Unfortunately i can't seem figure out how to do that without blocking the main thread, which renders the bot completely useless since it can no longer respond to other commands etc. while waiting. I tried using [`thread::sleep`](https://doc.rust-lang.org/std/thread/fn.sleep.html) in [`thread::spawn`](https://doc.rust-lang.org/std/thread/fn.spawn.html) but that seems to put my main thread to sleep. I also tried looking for a crate to help me out, but both [timer](https://crates.io/crates/timer) and [futures-timer](https://crates.io/crates/futures-timer) give the same result of blocking while waiting. Any advice on how to do this?
Imagine you wrote a static file server with a bug: the user can actually go up directories. So you're hosting at `/var/www/`, and someone can go to (this has other issues, but bear with me) http://example.com/../log/example/log.txt to see your logs. If your application runs as a standard user (say, a new user called "example" that only has access to `/var/www` and `/var/log/example/`), going to http://example.com/../../home/WellMakeItSomehow/very_important_passwords.txt will result in a permissions error and the end user/hacker won't be able to see the passwords. If your application is running as root, the end user/hacker will now have all your passwords.
&gt; Yeah I read on the futures RFC that regex uses thread-local internally Do you have a link for this? I found the various futures RFCs, but didn't see any mention of regex after a quick skim.
It's mentioned a couple of times in RFC 2: First mention of "regex": https://github.com/rust-lang-nursery/futures-rfcs/pull/2#issuecomment-363192633
&gt; new epochs can introduce fairly major breakage Epochs can only do two things: add new keywords, and turn warnings from warn to deny.
Good bot!
Oh thanks! I was searching the RFC text itself. :)
What's the best way to get the ball rolling if we wanted to champion this change?
They can't remove keywords, repurpose them, or remove functionality attached to existing keywords? That would imply `ref` and `ref mut` bindings couldn't ever go away, which would make me quite happy. If it falls under "can be warned about thus can be removed in a future epoch" then anything can disappear.
Looking forward to a rusty vector graphics library! I'm using nanovg for this right now, which works quite well but is limited (and C…).
https://github.com/TeXitoi/benchmarksgame-rs
https://github.com/rust-lang/rfcs/blob/master/text/2052-epochs.md &gt; When opting in to a new epoch, existing deprecations may turn into hard errors, and the compiler may take advantage of that fact to repurpose existing usage, e.g. by introducing a new keyword. **This is the only kind of breaking change a epoch opt-in can make.** So, repurposing can happen. Straight-up removal can't. "major" is really subjective. That's why I'm trying to stick to the facts. You may think the facts are major, I may not :)
May I ask why is it not using Vulkan instead? 
I'd use it here, since there isn't really an alternative. But the problem with `as` is that you're forced to use it in two quite different cases, making it difficult to understand which case the author intended to write. In one you're certain that the cast will never overflow. In that case you'd like a debug check telling you when you're wrong. But for some reason `as` is inconsistent with all other integer operation which have this check, and hides the bug by silently wrapping. In the other case you want to wrap. There is no self documenting operation like `wrapping_as`, you're using `as` once again. What I'd like to see is the addition of various self documenting casting functions (`wrapping_as`, `saturating_as`, etc.) and `as` being made consistent with other integer operations (overflow is always a bug, but for performance reasons its only checked if debug assertions are enabled). Personally I'd even be fine with platform dependent `From` implementations, since that'd result in a compiler error on exotic platforms (e.g. 16-bit or 128-bit pointers) instead of the difficult to debug errors you'd get with the current design.
It's all good! I understand 100%.
I cannot speak for the developers, but to hazard a guess: OpenGL ES 2.0 is the last OpenGL ES version supported by Apple. Last week khronos released an Vulkan-implementation backed by metal, but before that Vulkan was more or less not an option if you didn't want to write for two backends/ignore Apple machines.
I'm trying to deserialize the "default" value for parameters in openapi. This value, depending on the type of the parameter would be of different type. For instance if the param is a bool then it will be either be true or false. I'm ok to deserialize just to strings and then parse it if that's easier but I haven't even achieved that. This is my current crazy incarnation (in my defense this is just getting crazier as I spend more time on it) of a custom deserialize method for this field: pub fn deserialize_default_param&lt;'de, D&gt;(deserializer: D) -&gt; Result&lt;Option&lt;String&gt;, D::Error&gt; where D: Deserializer&lt;'de&gt; { match bool::deserialize(deserializer)) { Ok(value) =&gt; Ok(Some(format!("{:?}",value))), Err(_) =&gt; { match i32::deserialize(deserializer) { Ok(value) =&gt; Ok(Some(format!("{:?}",value))), Err(_) =&gt; { match String::deserialize(deserializer) { Ok(value) =&gt; Ok(Some(value)), Err(_) =&gt; Ok(None), } } } } } } But this won't work because of ownership and it is also quite bad. Of course deserializing to something with better type info is welcomed also. Any direction is welcomed.
I saw it, 2.0 was released!
Anyone can submit code to the benchmark game. Figuring out how is perhaps harder than writing the code but if you navigate the website enough you figure it out. &gt;Is there really that much room for optimization in the Rust code The depths of cleverness are not boundless but they are close. 
This is the first update in three(?) weeks. So maybe noteworthy, maybe not. Still worth posting so that those looking for updates and not finding them are reminded that the plugin has been updated.
&gt; (As far as I know alioth only accepts stable) This is true! It's pretty great IMO.
I think this would be why it would make more sense to introduce a version 2.0, and actually call it like that, but advertise the compiler being supporting both versions (indefinitely?).
&gt; Could you elaborate on why the use of isize is suspect in this case? `isize` is the type to use when you are addressing memory, and want to describe the difference between two points in memory. That's not what you're doing, so it's wrong for `isize`. 
Era definitely has better connotations. That being said, as someone else suggested, EDITION would be even better Some other alternatives. Rust 2018 Edition Rust 2018 Epoch Rust 2018 Era Rust 2018 Version Rust 2018 Release Rust 2018 General Availability (GA) Rust 2018 Full Release (FR) Rust 2018 Comprehensive Release (CR) Rust 2018 Coordinated Release (CR) Rust 2018 Coordinated General Release (CGR) Rust 2018 Coordinated Release Edition (CRE) Rust 2018 Coordinated Release Epoch (CRE) Rust 2018 Coordinated Release Era (CRE) Rust 2018 Coordinated Edition (CE) Of the above, I’d really like, CRE (Coordinated Release Edition). Or CE (Coordinated Edition) which harkens to CE (Common Era) which ties back nicely to the original Epoch/Era concept.
protocol buffers 3 does not have required fields anymore.
Why specifically? I'm not saying you're wrong, but I'm curious as to exactly why.
&gt; Is there really that much room for optimization in the Rust code Yes, not too different from C++ if you're experienced there. But the techniques you'd employ are unlikely to be accepted by the maintainer. There's a nebulous requirement of "testing what the compiler isn't capable of." Which also means that the maintainer insists on a detailed audit of all submissions, and therefore for some of the challenges is no longer accepting new code because it's too much work.
You really can't see how your post is condescending? Yikes.
The best "documentation" about this is here: https://github.com/rust-lang/rust/blob/1bb1530239c801bb46b705eb2874ac4e5b213e54/src/doc/unstable-book/src/language-features/used.md. But that does seem to confirm my suspicion that rustc has little to do with it, it's the linker's job to handle those sections.
To be more clear, from Windows 8, all the text rendered on the "modern" part of Windows (plus IE11 and some Office programs) is rendered with gray scale antialiasing instead of subpixel antialiasing. This simplifies the animation/rotation of text and other stuff but the appearance on low DPI screen is worse. Here an example https://www.reddit.com/r/firefox/comments/2i9nes/thank_you_firefox_for_being_pretty_much_the_only/
The linked tweet was tweeted by [@RustFest](https://twitter.com/RustFest) on 2018-03-08 08:42:17 UTC ------------------------------------------------- 🎉 Our first batch of std::tickets is available now 🎉 [https://ti.to/asquera-event-ug/rustfest-paris-2018/](https://ti.to/asquera-event-ug/rustfest-paris-2018/) ------------------------------------------------- ^• Beep boop I'm a bot • Find out more about me at /r/tweettranscriberbot/ •
It sounds like a pretty strong plan from my outside perspective, but at the same time a marketing will will be pyrrhic if it somehow prevents or makes harder work that could be an even larger improvement such as distribution. In this case they probably don't conflict, but I think it's better to consider technical decisions completely separately from marketing potential. Two different concerns.
Vulkan doesn't work on all platforms. Pathfinder deliberately moved away from compute shaders and stuff so that it could work with GL ES 2 (for phones), so Vulkan's even less appealing in this aspect. (Also, AFAICT vulkan isn't a replacement for OpenGL; it's lower level -- if you don't need it you shouldn't be using it)
It does subpixel AA. /u/pcwalton probably has a better full comparison.
Unfortutatly with generics the same thing (ignoring functions not touched by unit tests) can happen, because with monomorphization that code might not even be generated. This happens even if you touch some functions in the context of a type parameter, for instance, in the example below the code for `Foo::foo()` is never generated by rustc, even though we call `Foo::bar()`: struct Foo&lt;T&gt; { x: T } impl&lt;T&gt; Foo&lt;T&gt; { pub fn bar(&amp;self) { println!("x"); } pub fn foo(&amp;self) { println!("x"); } } fn mumble() { Foo { x: 3 }.bar(); } 
I don't know much about sampling profilers, but I do know that computers can only do a finite number of tasks at once. They may have true multitasking unlike us, but it's still mostly "serial tasking."
Cool cool, thanks! Back when I was responsible for web services, they were in ruby, so you *really* wanted nginx for the slowloris prevention alone. But it’s been a while, so I like to hear people’s opinions.
I remember maybe 1 or 2 benchmarks affected by SIMD uses. I also remember another one where OpenMP was beating Rayon (I think the Rayon usage may have been too finely grained, incurring more overhead).
IMO one of the strengths of the Rust roadmap this year is that it highlights specific uses for Rust to be improved from a holistic perspective (embedded, WASM, etc). This is a great way to foster that broader appeal. Looking forward to breaking into the top 20 soon! :)
Basically, the goal is to start at the lowest common denominator as far as hardware support is concerned and work up from there. So that means OpenGL ES 2 as a baseline, and later on OpenGL 3 (in fact, the WR integration uses GL 3), OpenGL 4, and finally Vulkan.
The question should more be "What is rust good at?" and I think right now that is what a lot of people are testing. Certainly rust is good at low level high performance code. But is it good for a webserver? Javascript libraries? UI? Maybe, maybe not. I don't think these are necessarily answered questions.
relibc is exciting! I suppose steed never went anywhere? 
I haven't done pixel-level comparisons between DirectWrite and Pathfinder, but I have researched what DirectWrite does, and I have a plan to match its rendering. (The plan as of now unfortunately involves using portions of FreeType to perform the hinting, as DirectWrite doesn't have the APIs I need. But FreeType has done a lot of work to closely resemble DirectWrite's output, so I think it should work out fine.)
You might be able to use an untagged enum to deserialize: https://serde.rs/enum-representations.html#untagged
&gt; if vectors are more commonplace than arrays My intuition says I've written more array literals than vector literals.
It looks like `ld`, the linker, can't find the Postgres lib, `libpq`. You can probably install the development package for it with something like `sudo apt install libpq-dev`. 
Seems good! If I try to launch the pathfinder demo on github in the browser side by side with normal text in firefox I should have an idea of the difference, right? Or maybe a nighly webrender-enabled firefox side by side with firefox stable?
&gt; For example, will Rust ever be good at writing a once-off script for which you'd use a one-liner bash now? Hmm, interesting thought, maybe? I mean it can be a fairly terse language. &gt; Or, if you can develop something in Java, why would you NOT pick Rust instead? I can see some use cases where java could have better performance than rust (high memory allocation rates with lots of shared memory) without requiring heroic effort. But ultimately I think the thing that will keep Rust out of Java environments is simply the fact that Java has momentum and is "good enough". That being said, I could see Rust making some serious inroads with things like "serverless" apps. Ultra high performance, low memory footprint, "instant" startup time. Rust seems like it could be a killer in that market because a rust service would be super cheap to run.
I'm brand new to rust and wanted to play around with tokio. Unfortunately, I just can't get anything to work. I've modified one of the examples so I can have one instance write a string to a socket and another instance read the string and print it. Just a minor modification. Everything was compiling fine until I added tokio::run(server); this fails to build with tokio::run(server); | ^^^ not found in `tokio` help: possible candidate is found in another module, you can import it into scope | 7 | use tokio::executor::current_thread::run; | But I can't work out how to get the compiler to find it. My toml has the lines: [dependencies] tokio = "\*" tokio-io = "\*" I've tried changing the version to 0.1.0. I've cloned the tokio repo and built the examples. I did notice that the tokio repo has a runtime.rs file in source that seems to contain the run function but my .cargo/registry/blah/src folder doesn't have that file. I then started copying the example code verbatim and that didn't compile at all, failing to find multiple things. I feel I must be doing something simple wrong but what I don't know. 
&gt; Filter method completion variants by trait bounds of impls Most of the time I find the plugin *misses* completions I'd like to see; Would it be possible to have an option that just includes *all* methods after the dot. They could be sorted after the methods that it knows are applicable. Until autocomplete is solid, I find I want to revert to prefixed functions ('v for vmath, gl for gl functions', etc etc) to leverage alphabetic sort more. Autocomplete still helps with parameter assistance. Sometimes you know what method you want, but you dont know the trait yet; you could actually work the other way round (write the methods you want, then figure out the trait bounds afterward.. imagine another form of autocomplete there , in the where blocks or type-bounds perhaps)
AFAIK Vulkan is an OpenGL successor and there won't be any new OpenGL release.
Thank you for posting this. I had been meaning to give this a try, and finally did. I had been using Visual Studio Code, and had been mostly happy with it, but I'm going to jump ship now, for one tiny reason: I could never figure out how to configure VSC to work with my old emacs muscle memory for ^K (cut to end-of-line) and ^Y (yank, a.k.a. paste). Every text field in almost every app running in macOS does this correctly. VSC can't be configured to do it, but IntelliJ can. 
Yep, IntelliJ definitely has a decent support for Emacs keybindings: it supports sticky selection and emacs tab! And `M-/` works even in default keymap :)
&gt;&gt; Is there really that much room for optimization in the Rust code &gt; &gt; The depths of cleverness are not boundless but they are close. Countably boundless.
Are the subtitles manual or just the YouTube translation?
Interestingly, counting just by GitHub activity it looks like we'd be 14th or 15th. I wonder if having a useful subreddit for answering questions is dragging down our SO ranking. :P
I didnt't say that ClearType was the hinting itself, just that it is how Microsoft implements their subpixel AA tha happens to have pretty strong grid snapping.
Does tantivy support now morphology ?
Right, but it doesn't have a high level API yet, yes? Maybe this has changed since I last played with it.
Yes, that seems to be the community consensus.
And of course, if you have multiple parameters that may require initialization, it should be something like a builder pattern ideally: ```rust let myFoo = Foo::with_name("Jim") .with_city("NY") .with_age(33) .build(); ```
We also have docs.rs, the book, the std docs and generally very good documentation. I don't think I ever use Google with Rust, and absolutely never Stackoverflow (while it's very common when I use C# or Java). So yeah, I think that's definitely dragging Rust down here.
&gt; Anyway, the intended main use case for logpack does not call for adding the features that Avro provides, because no prior knowledge about serialized types is needed in the deserializer. To be clear, no prior knowledge is needed in Avro either. What it offers is that, *in addition*, the reader can map its input type representation back into its own static types. Because Avro, given the reader's statically-expressed expectation of what the input data should look like, will generate the code to: 1. Inspect the input data's type description at runtime to validate it can be translated into the reader's static expectation; 2. Translate dynamically between the input's type description and the reader's static type. &gt; [...] the file format and container format are too high of a concern for logpack. I aim for logpack to "do one thing and do it well" :) This is of course completely fair, but another way of putting it is that a container format is an orthogonal add-on that somebody else might want to build. On the other hand, functionality to deserialize into a static type does strike me like arguably more of a core concern. It sounds like something that most readers by far will want to do—verify that the data fits their expectations, and providing a static guarantee to their "inner" code that the external data was validated at the system's boundaries. &gt; In fact, I would like to use it for a high-performance logging system in Rust that separates log generation and log viewing into two processes. To what extent do you envision that the viewer will need to do more than just passively display a string representation of the records? For example, would the viewer have any expectations that the records would exhibit a specific structure, like having a timestamp, source component and logging level?
SO allows you to answer your own questions. I have been considering posting questions and answers that I see in the weekly threads or on IRC for a while now. I've just never actually gone through with it.
I have been using this plug-in for quite sometime now. It is working well except for some intermittent CPU burst. For people using both vim and this, how does they compare? Mainly for autocomplete.
I think SO has decayed significantly in utility. I don't really go there for answers anymore unless I'm really desperate.
Perhaps this could be a community initiative. We could track official or community endorsed answers, maybe even open tickets that allow people to contribute to answers before one is finally published.
A good alternative to the other suggestions is [the num_traits crate](https://rust-num.github.io/num/num/cast/trait.NumCast.html). The code would look like this: ``` fn moving_average&lt;N&gt;(numbers: Vec&lt;Option&lt;N&gt;&gt;) -&gt; Result&lt;N, Box&lt;Error&gt;&gt; where N: Div&lt;Output=N&gt; + ToPrimitive + AddAssign + Default //AddAssign and Default are required by sum() { let len = NumCast::&lt;usize&gt;::from(numbers.len()).unwrap(); let sum = sum(numbers)?; Ok(sum / len) } ``` Please note that this code doesn't check whether the cast actually succeeded and will panic if it fails. Also, I didn't actually try this particular code :)
It doesn't and won't, but AFAIK that's not really how Vulkan tends to be framed anyway. Most of what makes OpenGL both higher level and slower is stuff that really should be removed completely, not merely brought forward into the Vulkan world. OpenGL is in kind of a bad place in the stack, where it simultaneously has to provide a lot of abstraction *and* a lot of generality. Things like WebRender and game engines and three.js-alikes know a lot more about the problems they're solving, so ideally they would be written directly on Vulkan. The way I see it, *they are* the high level API replacement for OpenGL.
And of course, there is [C locales](https://github.com/mpv-player/mpv/commit/1e70e82baa9193f6f027338b0fab0f5078971fbe)...
&gt; Still worth posting so that those looking for updates and not finding them are reminded that the plugin has been updated. IDEA does this automatically.
Sorry about the clipped intro. Twitch does that sometimes. I'm going for gold in this episode of the bot competition.
It shouldn't need to come to that. The prototype mentioned in the thread already runs incremental compilation queries in parallel, and rustc has been parallelizing the codegen stage for a while. Given Rust's pitch for compiler-checked parallelism, having to resort to those kinds of tricks would be quite a let-down.
Yes, but it doesn’t give you a changelog because the plugin doesn’t include one, yet. 
&gt; This last sentence is key, here: What is Rust for? Why would Rust try and reduce itself into some niche? Why can't we strive to just be a really good programming language / eco-system and make sure people can easily use it wherever? A lot of things working for Rust here. While one probably can't write an OS in JavaScript, Rust can be in both - BIOS and the Web. If anything, Rust's potential is really unlimited!
[They use](https://benchmarksgame.alioth.debian.org/how-programs-are-measured.html) an old Core 2 Quad CPU for measuring.
&gt; To what extent do you envision that the viewer will need to do more than just passively display a string representation of the records? For example, would the viewer have any expectations that the records would exhibit a specific structure, like having a timestamp, source component and logging level? The timestamp, source, and logging level can be static meta-data that is orthogonal to the data that logpack is handling, and instead handled on a side-channel, perhaps by logpack too. But essentially, usage of logpack focus over the non-static values of parameters given to log statements. To establish more context, suppose you have: binlog!("I want to see this value: {} and another one {}", value, foo); Where `value: Option(u32)` and `foo: u8`, then the written binary record will only be an 8 byte sized u64 key to the static meta-data, plus 5 bytes for Option(u32) and 1 bytes for foo = a total of 14 bytes. If you add a timestamp, then another 8 bytes, and that is brought via rdtsc instruction with linear extrapolation to wall time done later by the viewer. The static meta-data to which the u64 key points, is written only once to a different buffer, and it contains filename, line, function (I have patched rustc's to include a function!() macro) and module. The viewer shall be able to pretty-print the above entry with all the static and dynamic information combined, plus execute queries on the structured data. I imagine a query that, for instance, will precisely show all log lines that have any instances of 'Err' from the `Result` enum. I have a really early version of this logger that under release-lto build can write 150 million of such record per second on a single CPU (without rdtsc).
Why not? It is actually great to have less info but more frequently, IMHO.
I've also used IRC quite a bit and the Rust users forum. Many places to find answers for Rust.
&gt; Why would Rust try and reduce itself into some niche? Definitely NOT what I meant :) However, there are some limitations today in Rust. Off the top of my head: - its ownership/borrowing make a number of concurrent data-structures harder (compared to GC'ed languages), - its static nature makes hot-reloading hard, - its static nature prevents the more intuitive interfaces that dynamically typed languages get: XML/HTML/JSON parsing libraries in python eschew "stringly" typed interface, you can use `document.head.custom[0]` to access the first `&lt;custom&gt;` tag contained in `&lt;head&gt;`. - ... My question is therefore about Rust's potential: - are the current limitations intrinsics, or can they be overcome? - are there areas that where Rust will likely remain second-class citizen? And on the other hand: - are there areas Rust should NOT strive for? For example, back to `document.head.custom[0]`, it could probably be built-in by allowing a kind of `Dyn` trait where `.custom` gets converted into `["custom"]`, ... but is this somewhere we want to take Rust? I personally find that iffy. Do you? What about the rest of the community? --- I think at some point we should consider: *"What is Rust for?"*. It does not need to be a straight-jacket, or overly restrictive (only OS stuff!), however I think it's important to realize that no language can be the end-all/be-all, and therefore that trying to be good at everything is likely doomed to fail. And I believe that's OK. So, what's your vision for Rust? Where do you think Rust can shine, and where do you think it would require a profound change for it to shine? Ultimately, having a "vision" of where Rust is going is quite useful to know which features to accept/refuse.
/r/Playrust You seem a bit rusty today. May I suggest posting this elsewhere, as this subreddit is dedicated to a programming language? :p 
Not to mention the users forum. 
Oh interesting. On mobile, but i thought Rust programs were compiled with target-cpu=core2 and C programs were compiled with -mnative. I guess they are the same if they are indeed being run on a Core 2 CPU.
Not yet. It will soon. Haven't had power all day here in New England. :)
&gt;3 billions pages indexed might have been enough to compete in the global search engine market in 2002. tbh there are billions of pages you can prune. e.g. pinterest, linkedin, and facebook all result in walled garden results.
&gt; It's an odd aspect of portability but I think it'd be interesting if mrustc's MIR -&gt; C transpiler could be somehow integrated into rustc for targeting non-LLVM platforms. I have already played around with mrustc on architectures like m68k but I still ran into some issues. But I am also trying my best to get more architectures supported in the standard Rust compiler.
[Here's the list of targets in Debian for which rustc has currently been bootstrapped.](https://buildd.debian.org/status/package.php?p=rustc&amp;suite=sid) The greyed out architectures are unofficial Debian targets (Debian Ports) while the upper architectures are Debian's release architectures. I have contributed various small patches to Rust upstream to add support for targets like sparc64-unknown-linux-gnu and powerpc-unknown-linux-gnuspe. sparc64 works fine, [powerpcspe is suffering from LLVM bugs](https://github.com/rust-lang/rust/issues/48748). x32 (x86_64-unknown-linux-gnux32) can be cross-compiled but the compiler itself unfortunately crashes at the moment. I am planning to file a bug report for that. alpha and ia64 used to have LLVM ports which have been removed or were never merged. However, if we can convince LLVM to add support for alpha and ia64, we could add a rustc port. For m68k, there is an out-of-tree LLVM tree which is currently work-in-progress. I have already started working on adding the necessary bits and pieces for rustc but I won't be able to upstream anything before LLVM support for m68k has arrived. But it will happen, there are just enough crazy people ;). In any case: If anyone is interested in getting the Rust compiler ported to old or exotic architectures on Linux, please join #debian-ports on OFTC. We have porterboxes available for the various architectures for which we are happy to provide access to anyone who is willing to work on an LLVM or Rust compiler port.
Is this the only GPU based font renderer? If it really is faster I'm surprised nobody else has tried it before. Is there a clear path to this landing in firefox?
From what I gather it'll be merged into Webrender first and from there Firefox
This is really cool, thanks for the response. It definitely seems like a useful tool.
Font rendering on the GPU has historically been pretty hard. Accuracy matters a lot more than speed usually and that's where they fall short.
Fully manual. YouTube understandably does not do any automatic transcription of Norwegian and that was by far the largest part of the job. I transcribed as Norwegian over the course of a few days and translated to English that in a few hours. Some kind of half decent automatic transcription probably could have saved me a great deal of work. The manual translation is simple enough, and automatic translation horrible enough, that it was a no-brainer to just do that manually.
Thanks for the link! This is really something special. This lets me (again) really appreciate Rust's open RFC process.
I agree. Much is said about slow compile times, but they aren't really that bad, I think. In my experience there is little difference to clang or gcc and you get a lot more from rustc... And we are far away from timings you have when working with FPGA-language compilers.
Do you think it would be possible to have a FAQ only for this topic?
There is this language, [Cyclone](http://cyclone.thelanguage.org/) that could be useful as the base.
Hot-reload is exactly as difficult as you set it up to be. You can make it a trivial part of an application if you want to go that way. It is _unsafe as hell_, and there's no magic rust abstraction that will ever fix that. Telling the compiler, "just trust me that the bits left over by the last pile of code will make sense to the next pile of code" is fundamentally unsafe. A lot of rust folks are almost pathological about avoiding unsafe, but if you don't think it's that big a deal then hotload is entirely within reach. https://github.com/Lokathor/hotload-win32-rs
Sure, if someone wants to write one.
I use rust for command line stuff sometimes. It's more verbose but error handling is much better!
But... Is it going to be safe? /s
UB is a feature in C, so no.
They why do you sound so confused here?
Does this mean we can one day remove the c dependencies from time handling code to some extent?
So you're just calling major versions by a different name then??
It's the responsibility or the writer of unsafe code to make sure that, at the function boundary, either all input is safe, or the function is marked as unsafe. If you audit and/or test this, then other code can forget about unsafety in the function. This isolation is key to the effective use of rust
Wish I could come, but stoke on Trent is a little too far away 😭
It's not garbled, just rearranged.
No, but your C dependencies could use `relibc` instead of `glibc` or `musl`, especially on Redox.
This looks like quite a fun bug to investigate OP, why don't you have a go 😁
I agree that epoch, as a less frequently used word, will alert users that this is a new concept they should make sure they understand. If someone reads 'era ' they may think it just refers to some point in history, not a rigorously defined concept.
Well, this is awkward. I just tried it again and it works exactly as expected, no idea what my error was yesterday. Thank you.
Ah that makes sense. I'll try to rework it. Thank you.
When you're prompted for the update the changelog is linked.
Yup. Who needs 32 cores anyway. :)
There's some small discrepancies with cloc that I believe are cloc messing up, but I've since been told about some bugs loc that will really throw off the count, but they're fairly uncommon edge cases. Tokei is definitely smarter and more accurate I think, I should update the readme. I was just doing this for fun and I feel guilty about the inaccuracies now that I've gotten a bunch of github attention. (i'm the author, just saw i got a bunch of stars out of nowhere and found this thread)
There are others: https://github.com/behdad/glyphy The path is that this becomes a part of Webrender, and Firefox gets it for free.
I took the threading pattern directly from ripgrep as well as the -uu as burntsushi mentioned. Burntsushi is the best and I actually emailed him some questions while writing this and he was very helpful. There are some discrepancies that are just bugs (I think loc can be messed up majorly if you nest single line comments inside a multiline comment or something like that, I really need to get around to investigating and fixing). Tokei also does smart things that are more work like ignoring comments contained within strings. For that example line though I think most of the tools all count as as code including loc. It'd be sweet to be able to count is a comment though because it'd be way simpler and faster as well.
Everyone keeps asking this, and for me (loc author) it's just purely out of curiosity. I like to run it on my home dir sometimes, and I run it on most projects I end up cloning. I just find it fun to see how much I've written even if everyone points out lines of code isn't a good measure of progress. There's projects I feel like I haven't put much work at all into and then I realize I've realized I've written several thousand lines of code.
Undefined behavior does mean it's up to the implementation how it is handled. So in theory you could have an out of bounds array access crash the program safely, and it's still technically compliant with the C and POSIX standard.
No, it wasn't my intention to contribute to guilt. My understanding is that literally *every* cloc-like program has discrepancies. Documenting the difference is probably interesting, but getting so fast is pretty amazing.
Will you implement "login with google"?
what a cool project.
Author here, it's cool to see someone submitted this. I haven't touched this since I wrote it a few years ago other than to merge pull requests and update for nightly breaking changes, it makes me happy people are getting some value out of it. Let me know if you have questions or suggestions :)
If you know the range of valid numeric values in the csv, could you not parse the values as u32/i32/u64/i64 at the outset?
That’s basically what some of the functions do right now.
UB is also a feature in Rust, to be clear ;)
There are *many* libc's. But the main purpose is to have a C standard library that is easy for the Redox developers to work on, that takes kernel diversity into consideration (unlike most libc implementations), and that has the potential to be safer and more secure in some respects.
I don't think it looks cramped at all. \**delete long rant about modern web design*\* The page is stark, but clean and fairly uncluttered. If the page suffers from anything design-wise, I'd say there's a lack of delineation between elements, leading everything to look like they're kinda just... *on* the page in no particular layout, order or structure. But there's little enough there that I don't think it's a major issue.
Ah ok, I should have clarified to ask about the libc crate specifically as opposed to just any libc. I thinking about using relibc for my project and opening some pull requests to it to help push it forward. 
I like the starkness - but what's the point in squashing the playground editor into a scroll-bar box and then putting massive white bars on the sides of the page anyway?
Afaik libc only provides headers not an implementation. It lets you interact easily with existing libc implementations.
At the risk of going off-topic, can someone answer something I never understood? I would expect that font-rendering, being such a common task, would be handled by the OS. I'd imagine the application asks the OS "Hey, render this as text here for me, please," and then a common OS implemented rasterizer does it. So I'm surprised implementing your own GPU-based rasterizer shows such good results. Do you lose parity with the OS, or anything?
Out of bounds array access is explicitly undefined behavior not implementation defined behavior in C.
The standard best-practice solution to the smartphone problem is responsive design, i.e., the use of CSS media queries to make a page lay itself out differently depending on how wide the screen is.
Right... *but* if you're only going to do one layout, you're going to do it for smartphones because they're portrait, and the result is generally "good enough" everywhere else.
C stdlib has lots of macros, how do you deal with those? Just go the slightly impure but practical route of implementing them in C headers? Implement as rust functions and have thin wrapper macros?
Ya the example of undefined behavior was correct however it called it implementation defined which is incorrect. Implementation defined has specific meaning in standardeze vs undefined behavior. For instance what value NULL is implementation defined, some platforms have memory at address 0. 
Yes... Forums and computer generated are absolutely crazy too. In my experience common-crawl does an ok job at producing a nice looking crawl.
&gt; Note the depth variable must be a global static mut Do you have issues if there are multiple threads? I think that the depth variable should be in TLS
My mistake!
Yup, you're right. There's an issue for this: https://github.com/gsingh93/trace/issues/3 I haven't implemented it yet just because I've been busy and there haven't been many requests for it. If I get time soon I'll do it, otherwise PRs are welcome :)
Well looking at minecraft, I have 250 mods. Say a fifth of them could be done in parallel because of dependencies. That's 50 cores... to load *Minecraft* at max speed. Mod initialisation, despite being able to declare dependencies, is an embarrassingly parallel operation. Minecraft forge may do it all serially at the moment, but it wouldn't be too hard to make things parallel. Heck, we're on /r/Rust. one of rust's big selling points is how easy it makes concurrency. CPUs *aren't* getting faster, because doing so would overheat the CPU. Instead they get more cores at the same or slightly slower speed. The total operations per second goes up, assuming we're using those cores, which for the most part, we don't to their full potential. That's one of the problems Rust is designed to address.
Thanks! The inspiration was my dog, Scarlett: I'll try to work a photo of her into the examples! 
Ah, got it. So it wasn't really about using nginx, as it was about running under an unprivileged user account. This can also be done in Rust, but many people wouldn't bother with it, I suppose. There are other reasons to do it, like mitigating attacks like [Slowloris](https://en.wikipedia.org/wiki/Slowloris_\(computer_security\)), where nginx should be more mature.
**Slowloris (computer security)** Slowloris is a type of denial of service attack tool invented by Robert "RSnake" Hansen which allows a single machine to take down another machine's web server with minimal bandwidth and side effects on unrelated services and ports. Slowloris tries to keep many connections to the target web server open and hold them open as long as possible. It accomplishes this by opening connections to the target web server and sending a partial request. Periodically, it will send subsequent HTTP headers, adding to—but never completing—the request. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
The point of this is probably to be able to link existing C programs against the redox kernel. Some software will never be ported, some crates might have C dependencies for a long time. 
Because that isn't the purpose of relibc. The Redox OS developers want a libc that is easy for them to hack on, and is also easily portable across multiple types of kernels (Redox has a microkernel with ~50 syscalls, most other libc are terrible to try to port to that).
Tantivy has a configurable tokenizer. You can include your own components quite easily. Out of the box, it comes with a snowball stemmer.
/u/fgilcher recently posted the announcement. So what discussion were you hoping to get by posting the github? Or are you just posting random Rust projects to get karma? (Please remember the first Rule: Be constructive)
&gt; Could you elaborate on why the use of isize is suspect in this case? It's generally discouraged to use isize/usize for anything but addresses and lengths of in-memory arrays. This is because they change from 32bit to 64bit on 32bit and 64bit architectures, and can thus introduce subtle overflow errors if the program is only ever tested on 64 bit systems. This is part of why it's named `isize` and not `int`: the name highlights that this is for things _where the size should be platform dependent_. It's recommended to use `i32`, `i64`, `u32` or `u64` instead depending on the range of your data. These are the "regular" integer types, and their capacity stays the same an all platforms.
Hello, I am pretty new to rust and I'm trying to implement a static hashmap using the phf crate. For some reason the compiler complains that it is cannot find the `phf_macros` crate. This seems to be the problem code, but I am using it exactly as described in the phf docs: #![feature(plugin, core)] #![plugin(phf_macros)] extern crate phf; Thank you for any help. I pretty much have no idea what the tags at the top do so it's been pretty difficult to figure out what's going wrong here. Here is the error: error[E0463]: can't find crate for `phf_macros` --&gt; src\lib.rs:2:11 | 2 | #![plugin(phf_macros)] | ^^^^^^^^^^ can't find crate error: aborting due to previous error 
I didn't notice that this link has been posted before; will delete the submission.
So tokio::run is like Task.Run in .NET?
The rust-lang website does do a separate layout for phones though, where the sections are all below each other.
The libc crate provides Rust bindings to the existing libc on your platform&amp;mdash;GNU libc, or Apple's libSystem, or Android's Bionic, or the Microsoft C runtime, or musl,, or so forth. relibc _is_ a libc like those, exposing a C interface, but unlike all of those, it's internally implemented in Rust instead of in C. So the intended users are different: the libc crate is for people writing Rust applications or libraries to run everywhere, relibc is for people building an OS who want C programs to run on it. You could, if you want, use the libc crate to access the code in this crate, just like you can use the libc crate to access any other libc. Cases where you might want to include: * You're building a portable Rust application that can run on any libc, and it happens to get run on a machine where relibc is the platform libc. * You're building some sort of minimal container image for running Rust code, and you want as little C as possible, but you aren't starting with an existing OS and so you don't have a libc at all. (Although I think there are better solutions for this? Maybe [rlibc](https://github.com/alexcrichton/rlibc)?)
I’m currently building a libpcap clone in Rust, and I’ll need to access the winsock2 APIs in Windows, and perhaps things like AF_PACKET or the POSIX API in Linux. Would I be better off using the libc crate then?
actually I like content to be narrow. In this case it would probably look better with playground and "featuring" part not being side by side.
...a pure rust crate that will hopefully share the actual TZ code with relibc. That isn't the kind of code you want to write twice.
Nice reform, The code now is easier to understand. 
This is very much a strength of Rust the project - first-class support for documentation. The discussion on /r/programming about this article has a subthread about Scala, where the focus has overwhelmingly been on the compiler, and it appears that Scala suffers as a result.
That's definitely a fair goal! I've just not found it very useful in binary crates yet, unless you want to fully enumerate every error possible. A lot of the feedback that [this recent article](https://epage.github.io/blog/2018/03/redefining-failure/) gives applies to my use case - especially with context and displaying user-friendly errors.
&gt; Figuring out how is perhaps harder than writing the code… https://www.google.com/search?q=benchmarks+game+submit+code
Something like this would be equivalent for tokio 0.1, I think: extern crate futures; extern crate tokio_core; use futures::{Future, Stream}; use tokio_core::reactor::Core; use tokio_core::net::TcpListener; fn main() { let mut core = Core::new().unwrap(); let address = "127.0.0.1:8080".parse().unwrap(); let listener = TcpListener::bind(&amp;address, &amp;core.handle()).unwrap(); let handle = core.handle(); let server = listener.incoming() .map_err(|e| println!("error = {:?}", e)) .for_each(|(socket, _peer_addr)| { handle.spawn(process(socket)); Ok(()) }); core.run(server).unwrap(); } 
Now I remember! You're complaining about not being allowed to use macros. The ongoing problem with the benchmarks game Rust programs is [how to get them to compile offline?](https://benchmarksgame.alioth.debian.org/u64q/program.php?test=mandelbrot&amp;lang=rust&amp;id=6#log)
I feel the same way, one of my favorite blog layouts is Phil Oppermann’s OS [tutorial](https://os.phil-opp.com/) which limits everything to an easy to read column.
is `process` just a throwaway example name?
sorry if this is a stupid question, is a future just a closure that gets called lazily?
You probably need to put a "phf_macros" dependency into your `Cargo.toml`.
"Rust is too hard. Will use Go instead". I said that *many* times on my personal Rust journey! I'm very glad I stuck with it, though.
One trick I've seen is [this one](https://github.com/jgallagher/rusqlite/blob/master/libsqlite3-sys/build.rs#L159-L176), but it only works for feature flags. You might be able to use a build script to extract environment variables, but I'm not sure how those interact with `RUSTFLAGS`.
I believe it represents a user defined function to process incoming connections, but that isn't very clear in the example :(
Interesting :). Did you know that we're trying to build a subtitling team for the YouTube channel but struggle to find people that would like to take point there? We even got all the infrastructure set up through Mozilla, but are struggling to find people. The translation team from Sebastian Magri is in charge of that. Team work is not as much work as many people imagine, it's pretty often "just" having a little bandwidth to regularly make known that there's possible work to do and getting people set up.
You almost certainly want to create a build script.
I imagine the problem with having the OS do that would be that every application has a slightly different rendering pipeline, and even something like VSync would mean the OS has to write to some temporary buffer rather than the screen. I mean, if applications wrote all rendering directly to the output buffer the OS uses to display it, that would make sense - but pretty much nothing does this nowadays. [freetype](https://www.freetype.org/) is the most common open source font renderer I believe. I'm not sure if any OSs provide font rendering libraries which would do something similar to freetype, but if they did, it would definitely be an API of "font and characters" -&gt; "vectors of positions to render" not "font and characters and position" -&gt; rendering it directly.
I think the code example is too long. It should be reduced to four cases, short ones with Unicode characters. Then there is no need for the scroll bars and everything looks peachy to me.
`handle.spawn` runs the future on the reactor, doesn't it? So it's not exactly equivalent. You'd have to set up a threadpool in addition to all that.
I assume that it will be able to build truly static binaries (like musl can, but famously glibc can't). Great to have more options!
Yes, I believe `process` here is just a user defined function which does not return anything, and just does whatever you want it to do with the argument you pass into it. In this case the argument is a socket, so you'd likely in the `process` function start listening for incoming data and also start sending responses.
I find the fact that multi-threaded example uses word "process" and doesn't use the word "thread" confusing.
Do you think this is for using from C code? I thought this is to get rid of another dependencies for Rust code.
Not sure if that is correct. For example there was OpenGL 4.6 in July 2017 even though Vulkan 1.0 was released in February 2016. From my point of view Vulkan is the low-level API you should use for high performance if you don't mind writing a lot of low-level code. On the other hand CPU overhead should be smaller than seen with typical OpenGL calls + you can utilize multiple GPUs more easily (without the driver having to guess). On the other hand OpenGL is way easier to use if you just need "reasonably" fast graphics and the CPU overhead is also not that bad if you can employ the "AZDO" extensions in OpenGL 4.3+. Also Vulkan requires "newer" graphics cards. On Linux for example you can only get Vulkan working (with AMD cards) using the amdgpu kernel driver which supports only newer GCN hardware. Older cards supported by the "radeon" module often do not have the required hardware for Vulkan. This is probably not a big deal for games but a web browser should continue to work on older machines.
Maybe it's a linuxism. There is little difference between them, internally, in linux.
So far my journey with Rust helped me write better and safer C/C++ code, but that's it. As good as Rust is, I just seem to gravitate back to C/C++ always. It is very difficult to wholly focus on Rust, especially since I'm still at the point where I can't really write non-trivial code in it. I guess I just have to keep trying, and get from D -&gt; E instead of sliding down towards D'. I would love to listen to your full presentation, seems like a very interesting topic indeed.
While this is literally true, it implies things that aren't true, so phrasing it this way is misleading, IMO.
We're re-doing the website this year, it's on the roadmap. We've said it before, but it's happening for absolute sure.
Anything is possible in a turing complete language. It's just as much a social as technical problem. I'd recommend getting involved with Conan, it seems like they have some momentum.
That's actually a fine idea, it should reduce the amount of mistakes I (and maybe a few others) do. :3
&gt; Anything is possible in a turing complete language. Halting problem disagrees. :P
I've got it working. It seems I chose just the wrong time to use the tutorials as the example code and the tutorial was a bit out of sync. This was updated a few hours ago to remove the tokio-io dependancy which fixed it for me. I can only assume I got into a situation where there were two competing versions of the same crate.
&gt; What I meant is that there are bit of ceremonies that I feel I have to do to define, consume and inspect errors. I was surprised by your statement that "error management is broken", because I personally think it is a lot better than in C++ or Python. Getting nice stacktraces can be a bit complicated, but I really enjoy the explicitness.
&gt; And secondly smartphones, because smartphones ruin everything. &gt; &gt; The page could have a more complicated layout that took advantage of the extra horizontal space... but is probably unnecessary, and probably also overkill. Ironically, the page is more readable on smaller screens, because the editor gets wrapped below the bullet points, removing the need for the scroll bars. 
“Anything that can be done by another turning complete language” is what I meant. You’re right of course.
Couldn't the Coca-Cola Company make a stink about the name?
Thanks!
Why do you want to run multiple reactors / multiple thread pools? Would it not be better to combine all the futures for all your servers listening on different sockets into one big future and run that single future with `tokio::run`? You could then freely use `tokio::spawn` from anywhere. 
Thank you for doing this! I think this fixed many conceptual issues that I had with the initial tokio release: - First of all, it's great that the polling is moved out of the executor, which enables multi-threaded executors. Furthermore, I believe this enables polling from multiple sources, e.g. one from epoll and one from get_events() or other unrelated sources outside of epoll. Note how futures of these different "polling sources" can now run in the same executor. - The introduction of the "tokio runtime" threadpool should make it much nicer for multiple libraries to interact. I always hated that in Java, where e.g. tomcat (the http server) has a threadpool and the db has a threadpool and this lib you're using has a threadpool and your app of course also has a threadpool. (I'm not current on Java anymore, I'm not 100% but think I saw a "default threadpool" recently as well?) So a big thumbs up from me! :)
It’s a fairly standard microkernel, which is very similar (a little unfortunately) to Redox. Tbh, this was my first Rust project and I wasn’t aware of Redox when I started. Pebble does seem similar in hindsight, which I am a little worried about, but it was an actual coincidence. As far as actual design goes, the one difference I’ve already decided on will be a much smaller system call API, with most functionality implemented using a more general message passing system. It’s all very much in its infancy tho. As far as contributing to redox, I’m totally not against it, but I’ve not been part of a big project before and I’m not sure where I’d start. I’m also not sure if the Rustc team would be happy to merge another Rust OS project (targets and libstd changes) or would be against cluttering the tree. So, not sure how far it’ll get, but that’s the plan
Simple macros like errno definitions are done in Rust code and translated by cbindgen Complicated ones are done in C header files
I've had a ton of other stuff to watch and go through lately, so this video has been sitting lonely in a tab for a while, but I finally got around to watching it. Once again it was super helpful. It's so great having more stuff like this out there, it's such a great resource for picking up new bits and pieces of knowledge, and to learn new things. Specially with having someone explain everything around it. Thanks again for doing these!
Given &gt; The application may spawn additional futures onto the thread pool using tokio::spawn I think `Task.Run` == `tokio::spawn`. `tokio::run` is, I dunno, a `TaskFactory` constructor which takes an initial task to run, and blocks until all Tasks have completed. I.e. something that Microsoft does for you.
&gt; Tera is a much better template engine than Liquid Could you sell me on Tera? Not or cobalt but [another project coming up](https://github.com/crate-ci/meta/issues/1) that I'll need a template system for. My needs - Works on in-memory strings - Perform operations on variables Non-needs - File-based templates - Conditionals, loops, includes, etc
ah my bad
The `rust` tag on SO tends to be really heavily moderated compared to many of the other big tags like `android`, `c#`, etc. I looked at the new Rust questions list the other day and almost every single question had either been closed as duplicate or closed as off topic. I realize that SO doesn't like duplicated questions or certain kinds of questions, but I really wonder what kind of a message that sends to a new Rust user when they ask a simple question on SO and it gets immediately closed as duplicate. Given some of the unique challenges to writing Rust code, I have to wonder if some of these users don't even understand why their question is a duplicate of another question which just leads to them getting frustrated and giving up. After having a quick look at the new question list for `android`, I can't help but think that nearly all of these questions would be closed if similar questions about Rust had been posted to `rust` instead. I think we might be doing ourselves a disservice by closing questions so aggressively. 
&gt; Is this meant to be used from C code? Looks like this is the case.
Looks like the `gl` crate provides simply provides bindings to the C api. To quote its [readme](https://github.com/brendanzab/gl-rs/blob/master/README.md): &gt; This repository contains the necessary building blocks for OpenGL wrapper libraries. For more information on each crate, see their respective READMEs listed below. &gt; Are we saying something along the lines of, "get the reference of `first_triangle[0]` and cast it as a dereferenced constant 32-bit floating point and cast that again as a dereferenced value of unknown/void type"? Yes, that's pretty much exactly what happens :)
Pretty much the same here. Fortunately, that doesn't make you lose too much time : you can learn Go in a week, and then 2 to 3 weeks in, the `if err != nil { return nil, err }` pattern makes you want to go back to Rust badly.
Most languages when they interact with a C library, with minimalist bindings get pretty verbose. It is usually easy/useful to abstract away a lot of the mess. One obvious thing, in languages where arrays/vectors already know their size, is to make it so you don't need to pass in the size of the array/vector! You can also add type safety on the opengl ids which are just always ints, in Rust I think you can do this with single field structs, though I do know how nice it will end up. Anyway with a few helper functions/macros I think in Rust you could create a similar "idiomatic" opengl binding as I did here in Nim: https://github.com/jackmott/easygl Maybe someone already has. I know there is the really fancy opengl Rust library glium ( https://github.com/glium/glium ) which takes things a step farther than just simplifying the opengl calls.
You may want to read up on Redox a little more and join the discourse discussions there and mattermost discussions there. If nothing more, to inform yourself about what is going on there and how what you are doing might be relevant and what they are doing would be relevant to your project. Believe it or not, the architecture you describe, limited system calls/message passing focus, is the exact architecture of Redox. NOTE: I'm not really a part of Redox (yet), just an interested observer. I would recommend having a look at these things: * https://github.com/redox-os * https://www.redox-os.org/news/ * https://discourse.redox-os.org/ * https://www.redox-os.org/docs/ * https://chat.redox-os.org/redox/channels/general Also, don't forget about r/redox.
I can confirm that this works, thanks! Makes a lot more sense this way.
You could. Each Runtime instance is full isolated. That said, you probably don't want to :)
&gt; This repository contains the necessary building blocks for OpenGL wrapper libraries. For more information on each crate, see their respective READMEs listed below. &gt; &gt; Thanks for the reply, surprised it actually made sense. I figured that `*const f32` actually returns a reference to some other location which is further dereferenced into an unknown type. If it helps, check out /u/Dushistov answer which gives a simplified variant of this. 
As mentioned, it is a user defined fn that does something with the socket and returns a future representing the completion of handling the socket. I updated the blog post to mention this. Thanks for the note.
Two of my colleagues love Go, and now they're using that pattern in Python. It makes me feel irrationally angry every time I see it. 
Shouldn't you depend on [the `log` crate](https://crates.io/crates/log) and insert [`trace!`](https://docs.rs/log/0.4.1/log/macro.trace.html) statements instead of `println!`s?
When compared to the PYPL Index (Rust at place 20) and the recent RedMonk 2018Q1 numbers placing Rust at #23, I wonder how credible the TIOBE Index is.
In the bit of example code included, they say it will use a strategy similar to go/erlang. Does this mean the example code will run run on all available cpu cores and distribute futures across them? I always wondered why the default tokio library before ran single-threaded
Thanks for the thought. I updated the blog post to clarify.
None of this index are credible.
Have you seen https://www.conan.io/? It doesn't do everything Cargo does but it is a good place to start.
Thanks for the clarification, this makes sense. I'm curious, though, this example just processes raw TCP sockets. Would it be up to downstream libs like `hyper` to implement asynchronous processing of HTTP requests over a keep alive session?
I guess it would have to be called "utmost" or something.
Yes the futures should run on multiple CPUs now.
IANAL but I don't believe they would win given it is a different industry and there unlikely to be confusion between the products. That said, they could still bankrupt you with legal fees if they wanted to.
This is a great post! I haven't actually used failure yet, so I don't have much else to add. I will say that this post should be linked to &amp; summarized in an issue on failure's github so it can be properly analyzed and discussed. If you don't do it I will :P
&gt; you can learn Go in a week, and then 2 to 3 weeks in, the `if err != nil { return nil, err }` pattern makes you want to go back to Rust badly. I've been using Go since before 1.0 (and Rust before 1.0 too), and that pattern is barely a mild annoyance to me.
Default Threadpool is like a bad joke. It's supposed to make managing stuff 'easier' since certain stuff like CompletableFuture and ParallelStream will use it automatically unless you give them a threadpool manually. However the default threadpool is 6 threads for your whole app and can't be reconfigured. 
I meant it as a joke. :)
After /u/steveklabnik1 mentioned it I looked at it shortly. It seems to me it still requires configuration even for most basic project. I plan to look at it more, I just don't feel like it right now... What exactly it doesn't do that Cargo does?
Is Redox planned to be able to run CLI applications written with GNU/Linux in mind?
I may just be a crazy person, but I disagree with the rapid prototyping part. I recently decided to try writing a microservice in Rust, to just see what it would feel like. I completely changed how I was doing things like 4 times with no tests, and the only issue I ran into was one logic error that had been there from the start. I can't imagine doing those kinds of fast, big changes in a dynamic language without a crapload of testing.
You can use `std::mem::size_of` to get the size in bytes.
TIOBE measures "number of angels on a head of a pin", more or less. Their "methodology" is, roughly speaking, "what Google prints as 'approximate number of results: X' when you look for '&lt;lang-name&gt; language'". This metric can only show the things we already definitely know (JavaScript is more popular than Cobol), but neither relative popularity of language between places 10-100, nor change of popularity of particular language X (because change of number of search results displayed only Google engineers can understand... I hope they can). RedMonk's numbers (StackOverflow questions and GitHub projects) at least measure _something explainable_, yet I'd be very careful to call this numbers "popularity".
Yes, I believe this is precisely the point of 'tokio-runtime'.
&lt;3 no worries :)
Please note that std::tickets are just a re-export of core::tickets, so you can totally buy and attend as a #[no_std] user without fearing compatibility issues.
Since it uses a threadpool, does it make sense to call blocking code in a future now? Like if I use a non-async crate.
This is about what I had in mind when I said doing it "manually." It's easy enough to write a bunch of conditionally-compiled statements to build a list of features, it just has to be done by hand and can break if the features change. Thanks though. 
While this would work with primitive numbers such as `u64`, it doesn't work with [`rug::Integer`](https://docs.rs/rug/1.0.0/rug/struct.Integer.html): it would be wrong like using `std::mem::size_of` on a `Vec` to find its number of elements.
But wouldn't that give you the size of the entire object and not just the number?
Well, it's a step in the right direction at the very least :) Where do you get the number 6 from? I'm not sure, but it look the default should be "number of cores available to the system", at least that's (the default value for a tokio executor thread pool)[number of cores available to the system]. I think that's a good default for actually-async workloads, but can break down if a future does "accidental blocking" (e.g. blocking file io etc).
With integers, the result is always exact, and if more bits are needed, more memory is allocated to store the result exactly. With floats you cannot do that, for example for log you can compute any arbitrary number of bits, and you can always go on, so you have to pick a precision before the computation.
Reposting the comments that I leave [each](https://www.reddit.com/r/rust/comments/499lb2/rust_now_45_at_tiobe_index/) and [every](https://www.reddit.com/r/rust/comments/4l1dpb/rust_enters_the_top_50_for_the_tiobe_index/) time someone posts a link to TIOBE: &gt; Any position on TIOBE below the top 20 is subject to overwhelming noise. In the past six months Go has been from mid-30s, down to out of the top 50 entirely, back up to early 40s, up into the 30s again, out of the top 50 again, and then to where it is today. In other words, let's not bother celebrating every minute change in the TIOBE rankings. If Rust makes it into the top 20, then we'll talk. :P
Yes, that is exactly what something like Hyper does.
Makes a lot of sense when explained that way. Thanks!
From a 2015 blog post: https://blog.golang.org/errors-are-values &gt; We recently scanned all the open source projects we could find and discovered that this snippet occurs only once per page or two, less often than some would have you believe. It would be interesting to see if that number has changed over time.
Quite true. This is for specifically for compatibility; if you want a modern API for programming on Redox... use Rust.
In the course of writing the above code: https://twitter.com/nnethercote/status/966070764754550784 &gt; I fixed an old bug: https://bugzilla.mozilla.org/show_bug.cgi?id=107264 … Imagine going back in time and telling the reporter "this bug will get fixed 16 years from now, and the code will be written in a systems programming language that doesn't exist yet".
That project has different goals: it provides a Rust standard library for Linux without dependence on the C standard library. Redox's backend for Rust std already makes system calls through Rust (with a little assembly) without the C standard library; though it currently still links newlib (another C standard library), mainly for compatiblity with C code.
Looking at the issue it seems that it doesn't really need a template engine at all? I guess that depends what you mean by operations and whether you would let users write their own templates (I guess no from the issue but not 100% sure). For the good points of Tera, I'd say math operations, macros, tests and global functions allow you to easily do things not possible/very hard in handlebars/liquid (in Rust, in Ruby/Python you can do pretty much whatever you want anyway). Mainly macros though, they allow you to keep your templates clean. Those features seem overkill from the issue but I'd say any template engine engine is overkill if it's just simple substitition.
Provided they publish information about how they construct the rankings they release, they are all, by definition, credible. That is unless you doubt their ability to do what they say they do. The question is "what is the significance of the rankings they release?" This question, as others have pointed out is skewed towards "very little except in the case of the highest ranking languages." Even in the case of those higher ranking, more stable languages, one still has to decide for themselves how much these rankings will affect their decisions, considering the implications that being a high-ranking language on such an index has. For example, I personally regard the TIOBE index rankings as a minor curiosity that helps, in small part, with my understanding of language popularity trends insofar as an index built the way TIOBE is has merit. Which is to say "only a little bit."
I agree this would probably be the best solution.
Great idea sneaking Rust into the build/package system!
You spent 79 words bloviating instead-of answering the question.
Re name it, NotFanta
If you mean things like accessing files from procfs directories and such, I don't think so. Such applications would need patches for Redox.
I'll need to get more time on it to see if I need something more than a runtime `format!` but I was considering whether text transformations or other kinds of queries would be useful for it.
Are all these strings mutable? It sounds to me like you might be able to extract one owner of these strings and then pass references (str) instead. Whats requiring the ownership? If the entity that owns the Strings has a longer lifetime, it would be preferable to use references vs clone or arc. I think. 
Allocating in errors happens all the time in Rust, though, to get better error messages. There isn't much performance penalty to it.
Same. Of all the projects I've written in Rust, Rust's been capable of pushing much further, with drastic refactoring, in a short time frame. Something that would have taken me a lot longer with Go or Python. Knowing how to take advantage of all of Rust's expressiveness is very helpful for speeding up development. If you don't take advantage of all the methods in `Iterator`, `Option`, and `Result`, things can get more difficult. Rust does have a steeper learning curve than Go, somewhat in part due to the more comprehensive standard library &amp; language features, but after getting over that initial curve, the difficulty for developing an application is much lower than Go.
If the strings are large then I'd use an Rc or an Arc. If you have small strings, I'd look into using the small string crate which stores the string inline as an array. That will be more data in the struct but copying the struct won't involve an allocation.
If you want the number of bits required to store the integer part of a [`Float`](https://docs.rs/rug/1.0.0/rug/struct.Float.html), you can do that using the [`get_exp`](https://docs.rs/rug/1.0.0/rug/struct.Float.html#method.get_exp) method. When the positive value ≥ 0.5, this is equivalent to getting the integer part using [`to_integer`](https://docs.rs/rug/1.0.0/rug/struct.Float.html#method.to_integer) and calling [`significant_bits`](https://docs.rs/rug/1.0.0/rug/struct.Integer.html#method.significant_bits) on that, but it is much more efficient. (For positive values &lt; 0.5, you get negative exponents.) This code: extern crate rug; use rug::Float; use rug::float::Round; fn main() { let numbers = [0.2, 0.25, 0.4, 0.5, 1.5, 2.0, 2.99, 31.99, 32.0, 32.01]; for n in &amp;numbers { let f = Float::with_val(53, n); let exp = f.get_exp().expect("not normal"); println!("value: {:5}, exp: {:2}", n, exp); if exp &gt;= 0 { let down = f.to_integer_round(Round::Down).expect("not normal").0; assert_eq!(exp as u32, down.significant_bits()); } } } gives this output: value: 0.2, exp: -2 value: 0.25, exp: -1 value: 0.4, exp: -1 value: 0.5, exp: 0 value: 1.5, exp: 1 value: 2, exp: 2 value: 2.99, exp: 2 value: 31.99, exp: 5 value: 32, exp: 6 value: 32.01, exp: 6 
`tokio::run` starts the runtime. So that is the function that starts the reactor + threadpool. I considered also naming it `tokio::start`, but since the function blocks the current thread until the runtime shutsdown, i thought `run` was more appropriate.
Both are possible (still valid or not valid). One temporary cause of failure is the process being out of available FDs. https://github.com/tailhook/tk-listen attempts to provide a more advanced listening API. At some point, it might be worth considering pulling it into Tokio proper.
I've found the amount of `if err != nil` error checking you end up doing varies immensely with the kind of code you're writing. Many people are using Go for essentially proxies, which means you're doing lots of talking to other machines and so lots of error checking. I've also written code that has lots of in-memory data structures and handling queries and data aggregations across that data, which means the majority of the code is *not* filled with `if err != nil` checks. As a concrete example, looking at the Go repositories I have checked out from my own GitHub account on this laptop, it's about 58,000 lines of Go code (not counting test files) and only 180 instances of `err != nil`.
Yes, definitely. Hyper's API returns futures, so you can use that as well.
Yes, roughly that. But instead of "the current thread" it is many threads so you can use all available CPU cores.
Not quite yet. The Tokio threadpool assumes that tasks it executes are non-blocking. That said, there will be a better answer "soonish". I'll touch on that in a follow up blog post.
if these strings are only getting cloned in the final step for serialization, you could probably use &amp;str references instead of String.
There are some cases where an implementation might be required to choose something sane (the destruction order of structure fields is a current example in Rust), but no situation that *requires* insane undefined behaviour.
I was referring to the ForkJoinPoll.commonPool which is the default pool in java. Using it is a recipe for disaster, since it's small, non configurable, and shared by whole app. The idea of a default pool is cool, but woth no control it's just a mess.
Also wondering why references isn't an option. for example struct Foo&lt;'foo&gt; { bar: &amp;'foo str, } struct Bar&lt;'bar&gt; { baz: &amp;'bar str, } fn main() { let foo = Foo { bar: "8675309" }; let bar = Bar { baz: foo.bar }; jim(bar.baz); } fn jim(joe: &amp;str) { println!("{}", joe); } You could even do &amp;String references with a little more work. If you aren't mutating the strings or source structs then you don't really need RC.
TIOBE is pretty bad, so I wouldn't give it much thought. From looking at Rust on GitHub I can say we're about even with Scala right now, in terms of usage. I posted some measurements recently on my twitter (@jntrnr)
I don't think it looks too cramped as much as it still makes the language feel a bit too new and less welcoming to trying. I'd like to see some color/lightweight eye candy and maybe that cute little hermit crab. (I'm biased though on the mascon because I have three little pet hermies that have grown on me very much. :) Ironically, so has Rust.)
Awesome! I'd really recommend not using code from minihttp, it's missing a lot of proper HTTP handling. It many cases it's just wrong. It was neat to show that futures in Rust aren't expensive, but at this point, I'd rather take it down, to protect users :)
So usually the map_err would have more involved logic to see if the error is a fatal one? Could it somehow cancel the future/stream? If the socket becomes invalid, what does the Incoming Stream produce, just an endless bunch of errors or does the Stream end?
I know - phrased that poorly. Point is: unlike exceptions you have the *option* to not allocate which is important in no_std and inner loops. With Rust you can use the same error handling tools for your performance-sensitive errors and your "I'm going to abort so I don't care how long this takes" errors.
In this case, it would print the error and then exit.
Like a file handle? do_it_to_it(...)
Did they write a custom parser or did they use nom or combine for this, does anyone know?
Oh I was thinking more general applications like, say, web servers such as nginx. 
There's a note about `more` in the README for uutils: https://github.com/uutils/coreutils
&gt; a more modern LESS clone I just wanted to note, that Less is still in [active development](http://www.greenwoodsoftware.com/less/bugs.html) and support for mouse wheel scrolling is also listed in the list of *enhancement requests*. So if anyone cares and knows C, he may try to contact the maintainer, [Mark Nudelman](http://www.greenwoodsoftware.com/less/faq.html#mail) and maybe submit a patch? Though this would not mean a rewrite in Rust, it may be easier.
That's a fair question, but I don't know how satisfying my answer could be. I lived in emacs for years when my primary environment was Debian/Linux, and I agree Emacs is brilliant. Now my preferred environment in macOS, and my Job To Be Done isn't really "How Do I Get Lots Of Emacs Keybindings?". If it were, then I would certainly hire Emacs for the job. What I really want is to blow any application out the airlock if it violates a basic expectations that I have, and that expectation just so happens to be a small kernel of editing control keys (cursor up/down/left/right, jump to beginning/end of line, cut to end of line, and yank/paste) that are there for me in every text field on macOS: I can use them in * the location bar in my browser * the search field in my browser * or any form field on any web page * any text field in any application * while editing file names in the Finder or on my desktop * in the spotlight search field * editing entries on my calendar * while composing in Slack * ...etc... They're so ubiquitous in this environment that finding an app that violates them is jarring, I guess. I mean, the developers had to go out of their way to break it. So, out the airlock they go. 
Ew. Python has so many good Exceptions! You can even define your own if you absolutely need to!
Random thought, would an API like this be appealing? let server = listener.incoming() .map(hyper::HttpSocket) .map_err(|e| println!("error = {:?}", e)) .for_each(|socket| { tokio::spawn(process(socket)) });
You want /r/playrustservers , this subreddit is for the Rust programming language.
Rust is rapid until it's not. I don't think the borrow checker makes Rust _really hard_ like some people seem to but I do think it can really slow you down. If you suddenly find you need to borrow a struct mutably except for _one field_ you have to stop and refactor to split the struct up into parts and then rewrite the field accesses to satisfy the borrow checker that you aren't really going to change that one field. If you have multiple fields for which this occurs you may not be able to do a hierarchical decomposition at all and you'll have to explode the struct into individual reference to its members, each with the right mutability. You really don't want to have to be bothered with all this pointless busy work when you're in the middle of prototyping a feature and you haven't even settled on what the fields should be. Another annoying thing is adding a reference to a struct with lots of ancestors because you have to add the lifetime parameter to the struct and then all the structs that contain it and then all the structs that contain those, all the way up the tree. _And then_ you have to do the same thing for all the impls of all the structs you just touched. This OTOH is at least in principle is automatable.
This is amazing. This will be extremely useful for me. Thank you so much! For this as well as the amazing library. As an aside, I was curious about the choice of 53 being used as the number of bits of precision in all the examples. Is there any particular reason for this?
TIOBE approximately measures searches for "&lt;language-name&gt; language." It's often derided as essentially meaningless.
Not really true. I've taken very old libc and old software that was linked against that libc and statically linked them together and run them (without issue) on modern Linux.
&gt; ...is about 21MB in size...and I want to be reasonably memory efficient How much does Serde consume and how much would be acceptable? &gt; My backup approach is to textually parse the input file line by line, filtering out the few lines I want, and then turning only those into structs via serde. Is that a sensible idea? No, I would just use an existing JSON parser for only 21MB of data.
Linux only breaks ABI compatibility at the Syscall layer when they absolutely have to (due to a major security issue that cannot be fixed without modifying the API of the syscall interface). This is EXTREMELY rare and hasn't happened in years.
I'm sorry I got confused! 
Deleted
"Premature optimization is the root of all evil." 21MB will fit in the L3 cache of a Xeon. Are you sure that there will be a memory or processing bottleneck?
Hello! I am trying to use the bigint U256 struct (https://that.world/~docs/sputnikvm/bigint/struct.U256.html). What is the simplest way to print the value ?
Maybe it should be called FDA, because there's an allowable limit of rat feces, insect parts and all other sorts of unpleasant things in food.
It's true that the presence of unsafety in a crate does not prove the crate is vulnerable. But if you're performing an audit of your dependencies it seems like a fine way to prioritize.
I think that measuring "unsafe functions" that don't have "safety" doc comments is an important measure because without those comments, it's questionable how the consumer could even know about the safety requirements to call the unsafe function.
IIRC serde has streaming parser - https://docs.serde.rs/serde_json/struct.StreamDeserializer.html
&gt; However, there is no example yet of buffer data operation abstraction. &gt; &gt; That's great! Best of luck with this, will pop by from time to time. :-)
You're right! But you're also splitting hairs far, far more finely than I ever intend to, or want to. What's better, arguing from the position of zero information, or arguing from the position of data you know is incomplete? I never intended to encompass all definitions of "safety" here, since we often can't, in the general case, prove programs correct. ;-) &gt; What it tells you is how many opportunities there are in the crate and its dependencies for the implementer to have done something unsafe... Exactly! That's all I ever wanted to know. That's really the only meaning that `unsafe` has. &gt; Total Number of Lines of Code (or better yet expressions) in unsafe blocks This is a good idea, and easy to do. The rest are slightly more work to track, but certainly possible. &gt; Counting "unsafe trait definitions" is not helpful. Good point! Another easy fix.
I nominate "moar"
Gotta set the limit somewhere! How many fish have peed in the river that, eventually, provides your drinking water? ;-)
Thanks for the kind words! Of course, the Rug library is an interface to the GMP, MPFR and MPC libraries, so the developers of those libraries did all of the hard work! The [IEEE 754 binary64](https://en.wikipedia.org/wiki/Double-precision_floating-point_format#IEEE_754_double-precision_binary_floating-point_format:_binary64) representation, that is `f64` in Rust (`double` in C, Java), has 53 bits of precision. I just wanted to pick a number and mostly stick to it, so I just used that. Similarly, the [IEEE 754 binary32](https://en.wikipedia.org/wiki/Single-precision_floating-point_format#IEEE_754_single-precision_binary_floating-point_format:_binary32) representation, that is `f32` in Rust (`float` in C, Java), has 24 bits of precision.
I agree about me splitting hairs. I'm just concerned that saying it is measuring "How Unsafe" a Crate is creates the wrong impression in those new to Rust or those who don't fully understand what "Unsafe" means. If you consider any crate that depends on something that ultimately implements something "unsafe" as "unsafe" then, every create is unsafe. It would be just helpful to ensure the right message is communicated about what is being measured and what it means.
Yeah, I love the name too! And, I think this is a great idea!
Ok, that's a different situation than I was assuming. If you're going for a quick and dirty ad-hoc approach, searching for the right lines, then doing a full parse of just those lines sounds like a good idea. My main concern would be whether you have control over the input, or whether it could change format without warning.
Maybe this can already fit your mind bill: https://github.com/Byron/json-tools
This is Day 4 of the competition to program AI super hero bots. I'm using the Rust language.
It's an intuitive proof: once you start coding something, you never quite know when to stop.
[Fish pee is important](http://marinesciencetoday.com/2016/08/19/do-fish-pee/)
Not _wrong_ but maybe a little on the hyperbole+pedantic side? Ensuring that unsafe code is wrapped safely is our ultimate goal, no? So this tool can help with those audits. If it can walk the whole dependency tree exhaustively, great! We can audit the code to make sure unsafe libs are adequately wrapped and tested. I'd love to see coverage information for those unsafe blocks.
Updated here: https://github.com/trezm/fanta#the-most-basic-example
It seems to have a `Display` impl, so you ought to be able to simply do `println!("{}", my_u256);`
Updated the readme to not include tokio-minihttp! Most of the work was inspired by tokio for its speed, but so much has been done outside (and more will be done as well!) so I think it makes sense to remove that comment :)
Have a look at https://github.com/pikkr/pikkr FYI, if you can control the input data format, it’s quite common to have ‘1 json document per line’ instead of ‘1 json document that’s a huge array’ when streaming is important :)
If you're working in such a memory-limited environment, I'd assume you're also resource limited in other ways, so why wouldn't you choose a different format like a binary format that allows random access? All that linear scanning through 21MB of data has to be a cost worth avoiding? 
It is worth noting that there are [quite a few ways of converting a `&amp;str` to a `String`](https://mgattozzi.github.io/2016/05/26/how-do-i-str-string.html). `String::from(..)` and `.to_owned()` seem the most common, though personally I like `.into()` since it won't require me to change any dependent code if I decide to change the type of Person::name to any other type that implements `From&lt;&amp;str&gt;`, including `&amp;str` itself.
A bit offtopic question: will WebRender finally pave the way for proper html5 video hardware acceleration on Linux?
&gt; measuring how much "unsafe" is in a crate or how many dependent crates which contains "unsafe" tells you nothing about the correctness or not of the unsafe portions and so tells you nothing about how "Safe" a crate is. Even worse. Counting the number of `unsafe` keywords tells you very little, some people write a lot of `unsafe { foo() }` and others put the whole function inside an `unsafe` block and perform many unsafe operations. At the end of the day, the only thing one care about is whether a crate uses `unsafe` at all or not. If it doesn't, then there is nothing to audit. And if it does, chances are you will need to audit the whole create. A finer grained thing to audit would be modules that use `unsafe`. But how often `unsafe` is used inside a module is pretty much irrelevant. 
Didn't intend to be hyperbolic/pedantic. My apologies for that. What metric should be minimized I guess is the question? * Directly in the Crate * unsafe trait impls? funcs implemented in unsafe trait impls? LOC in unsafe trait impls? * unsafe blocks? LOC in unsafe blocks? * unsafe function definitions? LOC in unsafe function definitions? * unsafe function definitions without "safety" doc comments? functions without such comments that are called count? Does taking an unsafe function that is called all over the place in my code in unsafe blocks and wrapping it in a function that does nothing but call the unsafe function in an unsafe block make my code safer? If I have a function that takes a raw pointer and then derefs that pointer for some reason, it's an unsafe function. There is no possible "assert" I could put in a wrapper function that would make that function call safe, is there? If I can't assert in the wrapper that the unsafe function I'm calling will now be safe, have I made anything safer by putting it in a wrapper? I've significantly reduce the number of "unsafe" blocks in my code, but, my code is still 100% as unsafe as before. These are the kinds of things that come to my mind when I hear, count the number of unsafes and use that as a measure of "How Unsafe" a Crate is. It just feels like it's measuring the wrong thing.
You can't afford 15% for the 1/2 second it takes to get the data out of the file? It's not like the data will stay in memory... &gt; &gt; ...how much would be acceptable?
Maybe! Though, I don't quite get what this is proposing. Want to write an issue describing more what you mean?
I wonder if having an "unsafe" configuration option in cargo.toml could be of help? What if rustc would not compile "unsafe blocks, trait impls, or functions" unless it were passed a switch like, "rustc -allowUnsafe". Then, in cargo.toml have an option for "unsafe = true/false" (with the default as false). Then a crate is declaring, "Hey, I do unsafe stuff or I DO NOT do unsafe stuff" and it is enforced. Then, it is easy to see which crates require continuous auditing and it encourages/guides people to break their unsafe stuff out into separate, small "unsafe" crates automagically. Also, having the compiler generate the kinds of "Safety Metrics" talked about above would seem to be feasible and from that some sort of automagic "Safety Score" could possibly be derived that would be generated by the compiler automatically and would be displayable on crates.io.
Since posting that, I started reading my new copy of "Programming Rust". There's a part (I believe in ch. 3) with visual representations showing the difference between String, &amp;str and borrowed slices. Immensely helpful in understanding what's happening... I definitely didn't get it before. 
My second video! Still ironing out a lot of kinks and trying to find my flow / style. Constructive criticism appreciated. Thank you for stopping by &lt;3.
Memory is my only *strict* limit at this point. Storage is not much of a problem, just a bit slow, so I'll probably download the file to disk and look into mmap crates to keep active memory as low as possible. Unfortunately I can't control the input format, but speed is not an issue. I might even deliberately slow it down to not "spike" the CPU.
That's what I'm looking into right now. My main issue was that a naïve serde approach would have to deserialize the whole 20k-object array, which a: took quite a while and b: was obviously quite a bit of memory. I'm not quite certain yet if I'm go the `serde_json::StreamDeserializer` route or the text scanning based approach. I can't find a lot of info on the StreamDeserializer and my Rust-Fu isn't really firm enough to work without good examples. Line scanning is fast and easy, but obviously more fragile.
[removed]
`mostest` makes sense to me
Could you use a `regex` instead? Burntsushi's crate doesn't (by default) allocate. This is kinda of hacky but if you know the layout isn't going to change a lot..
I dig it. It's a bit fast, but it works this early on, at least for those of us who are pretty code fluent already. Thanks!
Thanks for the feedback!
I really enjoyed it! I agree with other comments though, perhaps a bit too fast for people still new to Rust. I also really liked the little interjections of humour. Looking forward to more!
This idea. Is the best idea. "definitely_not_fanta"
I made a similar plugin that might be useful for you https://github.com/alexkehayias/cargo-safety. With the syntex crate being unmaintained (last time I checked) I haven’t put any additional work into keeping cargo-safety. Hope it helps!
Or process_data(), to avoid confusion with Win32 HANDLEs. 
Ah, this subreddit is for rust, the programming language. I think you're looking for /r/playrust/
The Plan 9 terminal is its own pager; when it receives more than a screenfull of text it automatically stops reading the output of the underlying process until a key is pressed, and (since it's a terminal) it already supports scrolling and searching backwards and forwards through the scrollback buffer. Since there's already terminals written in Rust (hello, alacritty!) it might be better to investigate that than writing a separate program.
This is awesome! I loved reading your first articles and I'm looking forward to reading the pure Rust version now. Thanks for your work and sharing it with all of us!
This is so huge. With this work, Rust becomes one of the easiest ways to work on a hobby OS. Setting up a toolchain is *such* a giant pain, and this fixes all of it. I'm so excited!
IMO It's less relevant for asynchronous applications, where any thread working is supposed to be doing something useful rather than sleeping. I can imagine tasks for which you'd want to have a higher scheduling priority. I'm curious if you have any examples you'd specifically use separate pools for?
There's already a lint you can set to deny.
I'm curious about what bootloader the Redox is using?
I agree, but we have to start somewhere. At least giving someone a telescope to see those occurrences of `unsafe`, esp with the aide of something like a linter/clippy. I'd love it if all uses of `unsafe` happened in a separate crate, and there was an obvious firewall between the two. 
Annoying thing with Java checked exception - I think they fuck up your code flow. You write like there is no error possible and wrap that thing in `try/catch` on top of checking for nulls and now your code looks like shit. 
It looks like jackpot51 rolled his own? https://github.com/redox-os/bootloader
&gt; Burntsushi's crate doesn't (by default) allocate. Oh no, it definitely does. It just amortizes it. This is the thread-local data type that's used for mutable scratch space during search: https://github.com/rust-lang/regex/blob/a89220dd71460d51eca5a482d49dd89ee57c2fda/src/exec.rs#L1272-L1277
I am worried about this.
How so?
The way this announcement is written is really worrying. In fact, it smells downright bad. To me, it reads like this: - "We're folding this previously separate team into this other team that has very different goals". - "This obviously has a bunch of serious implications that a lot of people aren't going to like, so let's completely ignore that. Here, let me distract you with a bunch of shiny things instead." What's the deal? I mean the direct 20-words-or-less version of what's actually happening. Does this effectively end the Servo project in its current form, or is this announcement just a serious case of foot-in-mouth disease? 
Fears faced! Thanks for the link, it was helpful. I've got a basic lisp parser setup
Some issues should be opened but I would like to see a bigger picture discussion. I lost track of that when writing and didn't tailor my post to fostering such a conversation which I think bears out with the fact that no one, not even boats, has come here to discuss my concerns or suggested fixes. In overall design, my current thought is - Split up `failure` into a crate for `Fail` and a crate for `Context`/`Error` - To accomplish this, move `Fail::context` to an extension trait. With that, I think it mostly comes down to iterating on - Behavior of `cause`, `causes`, and `root_cause` - How to print `Fail`s? How to print current cause vs the entire chain? `Error`/`Context could then iterate independently. Maybe they get promoted to the standard library or maybe we have multiple competing implementations in different crates. 
Round one of my [PR to migrate `assert_cli` to `failure`](https://github.com/assert-rs/assert_cli/pull/91) is now up with commentary on some ergonomic challenges I noticed.
Yes, names that read like nonsense to non-US people are AMAZING.
AFAICT Servo is not what Firefox will be, but instead it's a lean and basic Browser that seeks to be a foundation for proof of concept work. Basically a playground were you can experiment and see things, without having to care about backwards compatibility, integration with complex systems, etc. It's not that it isn't a full browser, but it's as minimalist as possible and offers a great platform for experimentation (and when it becomes bloated, well I have no idea). Mozilla used Servo as a platform to prove two points: * The best ways to implement a modern browser architecture that used current systems to their fullest for faster rendering. 
Perhaps an app that has routes large differences in performance. You don't want expensive queries to suffocate the app's ability to deliver the easy ones
I'll be doing streams for both. I just did a quick update on Leaf yesterday with the SDL integration. Once it can do a bit more graphics I'll start more streams for it. Leaf-SDL Update: https://www.twitch.tv/videos/236823657
This is highly encouraged by SO. The site wants to host the best answers, so if something is missing then by all means add it in
Something else that I have been wondering is the effect that Rust gets by being no one's first language. I imagine that less than 1% of the Rust community (probably much less) learned to program via Rust. That eliminates a large number of syntax and general programming related questions
&lt;3 I'm really looking forward to the tighter focus on AR especially, but also VR. I think this is a very solid way for servo to get a foot in the door to industrial use.
So Quantum is the tick in the box that started the whole Rust project: make the browser parallel. It seems like an odd end to the journey, to be satisfied with the same goal you started out with. To me it's pretty disappointing that we're still stuck with a blunt instrument for a browser. It's fast, but we lose features with every release (next up: cookie management.) I'll admit I was hoping for a re-think, with a differentiated UX, with market entry afforded by the blazing speed. But Mozilla decided to perpetuate the status quo, and when the industry catches up to the performance, there will be nothing to keep people on Firefox except the few of us who care about how creepy Google is these days. So the brainpower gets dumped on AR/VR. Doesn't this strike anyone as incredibly premature? There's a lot to be done there, but we still need another decade's worth of hardware revolution before I'm going to try that again. But the hardware is just the first step, this project is of very little value unless a great deal of people are using headsets. I think that depends on hardware, and this project is doing nothing to address either of those things—unless you think browser integration is the big blocker at this time. (I'd rather the browser got nowhere near it.) Well, that was even more pessimistic than I expected. Sorry. But seriously: WTF?
Off topic, but when i read "large amount of JSON data" i thought you were going to parse Wikidata's 40 GB JSON file: https://dumps.wikimedia.org/wikidatawiki/entities/20180305/
&gt; In the meantime, though, there are emerging technologies where Mozilla believes it is vital for the open web to play a central role. One of these is Mixed Reality Are there public minutes relevant to this decision? I really can't put this together without going to dark places. VR/AR isn't a browser technology, and I don't think it should be. I'd really like the opportunity to unpack the logic that goes into this statement. Beyond being a possible future I may live to regret, I just can't get my head around the timing. The hype is at a low point and I'm still suppressing nausea 20 minutes into a VR session. &gt; a space that’s getting a ton of attention from all of today’s tech giants. All of those tech giants are throwing hardware at it, because the hardware is still weak. I just don't see 2018 (or '19 or '20) being the year of the headset.
Yeah, this is sometimes done. If your strings often come from a common set of strings (e.g. things like `div` and `table` when parsing HTML), you may consider using a crate like string-cache to intern them beforehand, making it even cheaper. There are also zero copy techniques that you can use by only sharing borrows (perhaps wrapping it up with a final set of clones that converts all the `Cow`s to `String`s), but it depends on where your generated data comes from and the structure of your code.
Given the affiliation with mozilla, it might appear to be a testing tool. 
See for example Elasticsearchs thread pools for differend DB operations. It makes it possible to finely tune the work profile of the database based on the tasks at hand. (ES does what the Tokio Runtime provides very neatly: async IO, threaded tasks) https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html Yes, it basically comes down to task variance and quality of service. I'll give you an example here: If writes and read would share the same thread pool in this case, I could starve writes by sending a lot of reads. Such issues aren't theory, there's a reason why the system has the current setup with fixed pools in these groups. Early versions had a shared thread pool that could grow infinitely, leading to horrible behaviour in burst phases. 
The idea is great! Build systems are a returning topic among C++ programmers. I would reconsider dropping the requirement that every cpp file has to have a header file and vice versa. Definitions of constexpr functions and variables, and templates, for example, tend to be in headers that are not backed by source files. 
What kind of device are you targeting, that you have these relatively low limits?
I mean, the way we wish to message it is that it's a milestone, not quite a parallel system. Epochs do come with breakages and have this parallel system to make breakages non-problematic, but that's not supposed to be a major focus.
One part cool to two parts worrying. I seriously hope Servo as a browser project doesn't fall behind because of this decision.
Yep, that's it. They use a bootloader fully written in assembly, and compiled with NASM.
I've got a [debug macro](https://github.com/ah-/anne-key/blob/master/src/debug.rs#L8) that conditionally enables `write!` logging. But when I disable logging I get a bunch of compilation warnings like "unused import: `core::fmt::Write`". What's the best way of dealing with things being potentially unused due to cfg flags? I could just ignore errors on those, but that doesn't seem very nice. Or can I make those imports conditional as well?
Content is good but for a newbie like me this is way too fast. 
Are you arguing there is no trend in lost features, or are you just sniping on a parenthetical example?
You can kind of do it in new C# versions, unfortunately the compiler cannot guarantee exhaustiveness (which is a pretty huge negative): [dotnetfiddle](https://dotnetfiddle.net/bLRr4s)
Thanks so much!
It’s not that, the methodology is just very poor. Its connection to what it claims to measure is extremely tenuous, and also very subject to gaming: https://blog.timbunce.org/2009/05/17/tiobe-index-is-being-gamed/ I’ve disliked TIOBE before I even heard about Rust.
That's actually really reassuring to hear. Thanks for offering your take on this! :)
Is there a way to `cargo install` only if the package is not yet installed or is outdated?
&gt; The Mixed Reality project may fail and loose resources. Maybe even as it technically succeeds society moves elsewhere and we realize that the project isn't useful enough to justify further investment. Servo would have to become the platform for other projects so that it can grow from them. This seems like a huge risk. My impression that only a tiny number of people outside of the Silicon Valley bubble care about (or are even aware of) AR/VR. Was this really the only way to get continued investment in Servo? Worrying. 
`cargo install` will already refuse to install if the package isn't installed already, you'll need to `cargo install --force` to overwrite an already installed package – or call `cargo uninstall &lt;crate&gt;` first. However that won't help you against outdated packages.
Do you mean the imports in other files in which you use the macro? You could instead move those imports into the macro itself - it would be better self contained, and imports would be there only when logging is enabled.
Thanks! Glad 
Why do people hate unsafe so much? For example, standard library is full of unsafe, would you call it a rat's nest then?
Thank you! I will test it out!
Mostly because I’m sharing across threads and you can’t guarantee lifetimes. They are immutable however.
Does Lucene have asynchronous APIs? I thought ES primarily used these pools to dispatch blocking requests, but I haven't looked at the code since pre 1.0.
&gt; My impression that only a tiny number of people outside of the Silicon Valley bubble care about (or are even aware of) AR/VR. Likewise. VR has many of the same cost-benefit tradeoffs and minimum buy-in costs as racing-wheel controllers for games. I like racing games... but, when I picked up an entry-level racing wheel, it was because the "$30, in like-new condition" at the local used games shop convinced me to *make* room for it if I have to. As for AR, on a phone, it seems like the kind of thing that has potential, but it's way too early to capitalize on it. The situation I see AR being useful is when it's good enough that you can pull your phone out of your pocket, tap an app, and hold it up as a window on whatever "world annotations" you wanted. We don't yet have computer vision that can be that accurate and reliable within the constraints of a smartphone.
I really need to find the time to work through this. It's extremely exciting.
Technically, I get "no resources". But I've got a 128MB slice on an OpenVZ box from the last stone age where I can "hide it away" as long as it doesn't "make waves". The whole thing is a qol feature. Currently whoever needs the most recent data just downloads the dataset to their own box an uses grep/editor search/etc to go look. Since I want to expose myself to more Rust I took it up as a hobby project. Scripting the whole thing with bash, grep and awk would probably be the more sensible option, but I'm probably gonna hook the whole thing up to a slack channel while I'm at it.
&gt; I just don't see 2018 (or '19 or '20) being the year of the headset. \*nod\* I've noticed that, on the technical side, each decade is characterized by the rise of a sphere of tech and I don't see VR/AR being the one for 2015 through 2025. (Since World War 2, cultural/tech decades have been offset by 5 years.) I won't go all the way back, but, when it comes to personal computing: * '75 to '85 was the decade of enthusiast computing (Altair 8800 in 1974, Apple I in 1976, Apple II in 1977, Commodore VIC-20 in 1980) * '85 to '95 was the decade of widespread home computing (IBM PC in 1981, Commodore 64 in 1982, MSX standard in 1983) * '95 to '05 was the decade of dial-up Internet (The 'net burst into the public consciousness in '95 and '96) * '05 to '15 was the decade of broadband (YouTube was founded at the beginning of 2005, Steam was announced in 2002 and first added non-Valve games in 2004 or 2005, NetFlix streaming was introduced in 2007, etc.) My best prediction was and continues to be that '15 to '25 is the decade of mobile broadband. Judging by the current technology curve, I see VR and AR as being similar to 3D printing... they're in the Altair 8800 stage and either, both, or something else I missed may characterize the '25 to '35 decade.