I would love to have rustc on an ipad, though...
If the compiler doesn't grow this complexity, a lot of second-level tooling will. racer, rls and others already pull hacks around these problems.
Well, continuing with the car analogy. There are so many manually driven cars that you can't just convert to driverless cars in the next year. There are so many roads (mountainous roads, dirt roads, ...) driverless cars can't drive on yet. This doesn't mean the technology is not worth being excited about and worth pushing for. I have never seen a technology emerge that was already fully developed, fully mature and not in need of iterative improvement when launching. Why would Rust be any different? A lot of people complain because Rust does not have that or can't do that. Fine, that is completely legitimate, if you need that feature X in your project, then Rust is indeed not (yet) suited for you in this specific case. It doesn't need to rule it out for all your other projects that might have different requirements though. Even if not everyone can use a driverless car right now, even if only 30% of the population can switch over in the next 5 years, they will drive in a safer vehicle for them and for the others. This benefits everyone in the end, because everyone is a little safer. I have never seen such posts as trying to convince everyone. From my point of view, such posts are meant to convince people who might use a driverless vehicle but haven't looked into it because they are fine with what they have or they "like" driving manually. One moment of distraction and you might end up being a danger for someone else. Why shouldn't we at least try to sell a safer alternative? :) 
Does `std::Result&lt;()&gt;` mean the function have two situations, perhaps It will return a value I want or an unexpected error? If there is an error, the `?` will propagate this error (eg, this function will return an Error, the value in `Result` is an `Err`,right?). And if everything works fine, the function will return a empty tuple (eg, `Ok()`) indicating that this function works fine, it will return what you want; is it ?
I really wonder what kind of vulnerabilities were at stake here. Sometimes the algorithm/business logic is the only issue. In such case even safe languages can't help..
Many people's minds are still going to complain about how 'manual' everything in Rust is, though, so I'd recommend teaching assembly (any) in parallel. Doesn't need to dig deep, just enough for students to understand what kind of language CPUs actually speak: Understanding the runtime and being able to read assembly are the main goals. Oh, and have a good debugger at hand. It's 2017, so why not use something virtualised.
Yeah, why do you talk about C as though it's the devil incarnate? C is a fantastic, albeit outdated language. Software written in it isn't intrinsically bad, but *could* have some bugs that Rust code wouldn't.
Rust does draw quite a bit of inspiration from functional languages. One of the reasons I've been migrating from Python to Rust is that, to me, it feels like it's trying to take as many of the great features of functional languages as possible without making it too difficult to understand the memory-usage and performance implications of your code.
There was some discussion on this [here](https://github.com/rust-lang/rfcs/pull/1953), but it seems unnecessary to me, especially for deployment where you don't even need Rust installed on the production machine.
Some extremely hot code (in our case, our implementation of ethash) can be improved by being unrolled. You pretty much never want to do this by hand, however, and I couldn't find an existing macro to do this work. Ideally this would be a syntax extension that can read constants and parse lower bounds, but for our use-case this works just fine and is the final push that our implementation needed to be the fastest implementation of ethash in the world. EDIT: This has been updated with /u/Quxxy's improved version, so it should compile faster.
You probably meant `io::Result&lt;()&gt;`, but yes, you've got the gist of it. If the function were to have a return type of `io::Result&lt;String&gt;`, it would mean that "this function will return an `Ok(String)` if successful, but will return an `io::Error` if it fails". Note that this is very similar to how Rust has the `Option&lt;T&gt;` type for nullable values, so that different return types mean different things: `-&gt; i32` This function always returns an `i32`, it will never be `None`, or return any errors. `-&gt; Option&lt;i32&gt;` This function may return a `Some(i32)`, but it may also return `None`. It will never error. `-&gt; Result&lt;i32, ()&gt;` This function may return an `Ok(i32)`, but it may also fail with `Err(())`. `-&gt; Result&lt;Option&lt;i32&gt;, ()&gt;` This function may fail with `Err(())` and may also return `Ok(None)`.
Last time I checked, that library didn't support Windows, so if /u/caramba2654 needs Windows support they'll need to find something else.
But to be honest, Rust is a little bit difficult to understand its philosophy, even though I know it's so powerful. Because a lot of languages derive from C, if you have known C, it is so easy to get used to other language, such as Golang, Java, Python. Even though I have learned Rust for more than 3 months, I still struggle with it :(
I found this behavior surprising until I read https://github.com/rust-lang/rust/issues/32836: &gt; Not dropping when assigning to a union field would make `f.g = Box::new(2)` act differently from `let p = &amp;mut f.g; *p = Box::new(2);`, because you can't make the latter case not drop. I think my approach is less surprising. Now I understand better where this comes from, but I still wish it didn't `Drop`. My personal thought to solving `let p = &amp;mut f.g; *p = Box::new(2);` would be that `let p = &amp;mut f.g;` is `unsafe` (as it reads), and therefore it's up to the developer to ensure that the field is initialized before passing it to an expression/function that expects it to be.
As for `-&gt; Result&lt;Option&lt;i32&gt;, ()&gt;`, does it mean: pub enum Result&lt;Option&lt;i32&gt;, ()&gt; { Ok(Option&lt;i32&gt;), Err(()), } If it works fine, it will return `Option&lt;i32&gt;`(eg, `Some(i32)` or None, it depends), or it will return an error(`Err(())`) ?
True - useful when you need to tell another machine to do the build though.
It's not beta just because it's public. It's alpha or pre-alpha because it's not feature-complete and is more-or-less a tech demo
I honestly don't really get how it helps. IIRC, Bundler doesn't actually install the right version of Ruby or anything - it just blows up if the versions don't match, and it's still up to you to actually provide the right one. Pinning versions in Rust makes less sense than in Ruby, too, because there are no subtle incompatibilities.
My main use case is Heroku auto-deployment, which uses the version specified in the Gemfile for the build. When it auto-deploys Rust apps it does the build itself as well, so a Rust version would be handy there.
Absolutely, You've got it!
Are you using [this](https://github.com/emk/heroku-buildpack-rust) for Heroku? It seems to have a way of pinning the version.
AHA. I'd missed that when setting up the first time. That will do what I need, thankyou :)
*looks at code* Holy crab! While this is definitely useful, is there no better way to do this than writing out the unrolled code for every possible iteration range from 0 to 512? Is there something like `#[for_the_love_of_god_please_unroll_my_loop]`?
I wish! I could limit it to 128, since it seems like it starts to become slower after that, but since it's generated with a script there's no point. I wanted to make it recursive but it craps out after only like 10. If I didn't have to horribly hack it I would make this an `#[unroll]` annotation that parses constants properly.
&gt; I think the same situation is playing out in the realm of systems programming, minus the horrible deaths. It's estimated that Toyota's failed Cruise Control software killed about 90 people. Among numerous issues were misuses of C, as one of the units could overflow its stack. Now, from the experts report, Toyota's Cruise Control software was *really* bad, and not following the automotive industry's best practices. Still, I cannot help to think that using C may have made it worse.
Yeah, you got it! :)
Thanks so much for your reply :)
Thanks so much for your reply, bro :)
Here is a version which *also* supports 512 unrolls, where the output is 86KB instead of 5.77MB. **Edit**: Actually, you could do *even better* with this by porting the script to Rust and using it as a build script, so the macro code gets generated at build-time. **Edit 2**: It's been modified and [uploaded as a Gist](https://gist.github.com/DanielKeep/8b622db9f9c246cbea4a8f469003cdc8). 
Would be better to open an issue on the rustc repository so that it could be fixed.
Leaked means that the destructor is never called. Two common ways this can happen: 1) You call `mem::forget(...)` on the Drain value. This function takes ownership of the object but never calls its destructor. 2) You put the object inside an `Rc` or `Arc` cycle. Since these containers don't automatically detect cycles, the cycle will never be freed unless it is explicitly broken. Note: both of these can happen in safe code. The borrow checker and ownership rules ensure that it is impossible to keep references to a value that has been leaked. Failing to use the Drop value doesn't mean it is leaked - rust still automatically calls the destructor when the value goes out of scope.
Okay thanks you perfectly cleared that up for me :) 
I am growing less and less convinced of this argument. Everyone programming in C (I can’t speak of C++ as I haven’t touched it for a good fifteen years) worth their salt is painfully aware of the shortcomings and dangers of the language. Claiming to be able to write entirely safe code in C is akin to claiming never to have had a near miss or worse while driving a car. Mastering something, be it driving, programming, or anything else, really means understanding that one isn’t and never will be perfect and learning to deal with that. ‘C is just fine’ might very well be the programming equivalent of an addict’s ‘I could stop if I’d want to.’
There is nothing wrong with trying to improve something. But the author goes too far stating, that C++ is a bad language. The message seems like: "Don't use it! It's a bad language!". This is pretty overstated, when you don't have a big choice when it comes to legacy software, multiplatform support, GUI frameworks and so on.
It is a matter of opinion whether something is *bad* or *good*, and I agree with the author: C++ is a *bad* language, for a variety of reasons (awkward and tedious syntax like `x-&gt;y`, header files, ease with which you can get memory errors even with "safe" data structures via things like iterator invalidation and uninitialised variables, template complexity with terrible error messages, etc.). That doesn't make other languages *good* or mean I would never consider using C++, but (speaking as someone who used to love C++98) it's well worth considering its problems and the alternatives.
Oh, I agree. I couldn't agree more. And even if it wasn't the safety aspect I'd want to be using Rust because it's a much better language (good generics, traits etc. Etc.). And because of this, some people are going to be easy to persuade. But those people are already in this subreddit. The Rust community is large, and the easy to persuade will have generally already seen the light. For remainder, insulting their current choices is not the best way to get them to make new ones.
If you have `x.foo()` and there are multiple functions `foo`, which of those do you add to the call graph? Which `foo` gets called depends on the type of `x`.
Oh right, I see where you are coming from now. It wasn't directly obvious from your first post that you were responding to that specific part. I agree with you there, we shouldn't belittle other languages to promote Rust. 
Are you sure you're even getting the function signatures right? C++ methods have an implicit `this` as the first parameter, but when binding to that in Rust you have to specify `this` explicitly! `int Mode(void)` would be translated into `extern "thiscall" fn(*mut IFileManager) -&gt; c_int` `int OpenContainer(const char *filename, const char* password, int mode)` would be translated into `extern "thiscall" fn(*mut IFileManager, *const c_char, *const c_char, c_int) -&gt; c_int` Also, instead of doing that insane reading of arbitrary offsets and transmuting into function pointers, have you considered making `Vtable` a struct which contains the function pointers in the correct order? It's what `winapi` does for COM vtables and works well.
How does this method compare to simply using [`cargo-cov`](https://github.com/kennytm/cov)? I had a bit of trouble making `cargo-cov` compile (master branch, though eventually made it work), but I haven't tried the method suggested by this post.
That's the problem with analogies: they all have their limit. And one thing that I don't like about the second edition of the Rust book is that there's plently of analogies. Please stop making anologies.
If that's recursive (I can't read it on mobile easily), then check the commit history. I tried to make it recursive but it hit the recursion limit even in my minimal usecase
There's a test *right there* for 512 expansions. There's no way you'd hit the recursion limit unless you were doing something pathological like one expansion per iteration. **Edit**: \**checks commit history*\* Yeah, like that. *That's* a bad idea.
If you're using rustup, you can use a [toolchain file](https://github.com/rust-lang-nursery/rustup.rs#the-toolchain-file).
this was exactly my mistake, how stupid of me it works perfectly now thank you very much! I somehow thought that this wasnt needed cause the vtable was part of the object itself. The only thing that surprises me is that the parameterless function worked completely fine without the reference. And yes the transmutation stuff was only because I was desperately trying to figure out what was wrong. I was planning on implementing the function pointers correctly instead
I didn't follow down the path of better recursion methods because the tradeoff of a ridiculous file size didn't seem like a problem to me. The compile time seems to be negligible. If anything, the macro recursion in this method would seem to be likely to cause longer compile times than the dumb method. I'd want to change the implementation only if it had a noticeable effect for end-users, since the simpler the implementation is the less likely it is to be wrong. I'm sure that your implementation is right since you're the macro monarch and all, but I still don't see the dumb implementation as an issue.
You are right. It’s just that I don’t have the patience anymore to argue with people that refuse to question their choices. Even if that means I’ll never receive one of those nice red uniforms.
hm, I thought implementing the Hash trait will make T somehow autoconvert to &amp;[u8] when put into a hasher. Guess I read the docs wrong. Thanks.
i compared rust-cryptos sha3 with the python3.6 implementation and it seems to be working just fine.
people just need to get better at making analogies
I'm guessing you made it just a tail recursive loop rather than a binary recursion like him. His would be limited to 2**recursion limit while you're is limited to just the recursion limit.
I've been working on a Macbook Pro 2014 model since, well, 2014. Multi-monitor setups usually annoy me. The windows get all screwed up when you plug/unplug from a monitor. The solution is a window manager, which is then another tool I need to manage. Also very few monitors have a comfortable level of input latency when moving a mouse. And being able to move the mouse cursor with my thumb on the trackpad is nice, I typically prefer it to a mouse but this *only* true for the macbook trackpad. Maybe I'm a masochist.
As it happens, yours is slightly faster to expand than mine. Well, *was*. [I made a small change](https://gist.github.com/DanielKeep/8b622db9f9c246cbea4a8f469003cdc8) to not recurse for unrolls of 16 or less. At that point, *mine* was faster. Measuring expansion time (in seconds) with `-Z time-passes` (`smooth` is mine): | crunchy | smooth | |---------|--------| | 0.617 | 0.08 | | 0.657 | 0.083 | | 0.675 | 0.081 | | 0.619 | 0.08 | | 0.616 | 0.081 | | 0.609 | 0.079 | | 0.608 | 0.08 | | 0.621 | 0.098 | | 0.646 | 0.08 | | 0.608 | 0.081 | | --- | --- | | 0.6276 | 0.0823 | Specifically, I was compiling this test program to eliminate as much as I could other than expanding the `unroll!` macro: #[macro_use] extern crate smooth; fn main() { unroll! { for i in 0..512 { let _ = i; } } } Also, the *overall* compilation time of the test case with your crate was around 3.14 seconds, whilst with mine it was 0.92 seconds. Also, keep in mind that yours is *five megabytes*. Five megabytes across every machine that'll use it, for every version. There's no reason to use that much space. Finally, as to the likelihood of it being wrong, it has tests. It's either right or it's not. It's right.
Yep. I think that's true 
I sometimes argue with my brother who always advocates towards C++. I like C++ language a lot, but it's too problematic to use for anything I actually want to write. It's really cool for small side project that you're never going to finish. Basically my brother's argument boils down to: 1. I'm using wrong commercial library. it's their fault it blows up when used together with futures from Microsoft's rest SDK. If I was using a good library I wouldn't have this problem. 2. I'm using wrong library I downloaded from GitHub. It may be written by C++ guru but it's some side project he banged out, I can't expect it to work. 3. I'm using wrong operating system. If I wasn't using Mac OS I wouldn't have all those problems with linking or finding libraries. 4. I'm doing things wrong. It maybe about safety, it may be about linking, maybe I just wrote my Cmake files wrong. In any case, I'm doing things wrong. 5. Facebook's C++ code that they put on GitHub may look cool, but it's crap so I can't expect it to actually compile. I don't think I ever managed to get FB's C++ libraries to compile on Mac OS. In any case they just dump their stuff to GitHub, so I can't actually expect it to work. I do like modern C++ and with some extra features and better ecosystem it could be a killer language. I'm still going to occasionally dabble. But maybe the wrong thing I was trying to use is the language after all!
The copying is everywhere. Mutable structs in C# are a fast way to have a bad time
I think both of you did great work. This is cool stuff!
Very true, and for that reason IMO the best implementation of Option would be as an immutable struct. 
I don't think I am recommending PGO so much as keeping the majority of the program in fast to generate/slow to run code and a handful of functions put through the extra optimization effort. AFAIK PGO is akin to a JIT but over a compile/run/compile timeframe. The only goal of the profile map would be to balance compile time and runtime. Higher performance debug builds nothing more.
I kinda like the analogy but I feel this post tries too hard to make the point -- like others are saying, this doesn't feel like something that will convince anyone and will just anger folks who like the languages. Also, when writing stuff like this we should strive to be accurate -- there's no guarantee that that $3 mil is all for memory safety bugs. A large fraction probably is, but when you're trying to convince someone, you should strive to make an unimpeachable argument.
Awesome, thanks a lot! I'll change the implementation. Sorry for being so stubborn over this!
You had your reasons. Besides, doing so forced me to beat yours *comprehensively*, which should be better for everyone.
I look at that briefly before i posted this, but I haven't researched what Rust actually does beyond seeing that page. Infact, I've never personally used TLS . At first glance, this comes under the heading of seeing things which lead me to suspect 'thread-local-storage is involves some operations beyond plain global variable access'. I'll read what 'LocalKey' is doing ... but I'm guessing this isn't anything like the "lightweight, pervasive TLS" i'm hypothesising about 
&gt; is painfully aware of the shortcomings and dangers of the language. Yes, but... &gt; Claiming to be able to write entirely safe code in C is akin to claiming never to have had a near miss or worse while driving a car. Mastering something, be it driving, programming, or anything else, really means understanding that one isn’t and never will be perfect and learning to deal with that. &gt; &gt; ‘C is just fine’ might very well be the programming equivalent of an addict’s ‘I could stop if I’d want to.’ This kind of attitude is why Rust evangelism pisses people off, especially if you imply writing C/C++ is shameful and whoever does it is an addict. First off, C/C++ isn't that bad. When a C/C++ codebase is well written _and well tested_ including fuzzying, you can have a good confidence in it. Modern C++ basically doesn't suffer that much from memory problems. Of course, writing correct C/C++ and tests is costly, no doubt about that. Second, Rust isn't really that great. Don't get me wrong, I like Rust, I think it's a very good language. Low-cost memory safety + avoidance of race conditions is a big deal. But that doesn't make Rust programs bug free or even vulnerability free. DOS bugs are very much possible with Rust and have been found. Ditto for logic errors. Also, the more performance-critical software you need to write, the more `unsafe` you're going to have to use, running into similar problems as with C/C++. Apart from safety, there are other areas where Rust is lacking. Integration with OSes is still not great. Building setups other than "link everything statically into one binary" are not great (basically you're on your own there). Rust support far fewer platforms than C/C++. Another matter is lack of scalar generics: Matrix libraries and similar are a pain compared to C++. Bindings are costly - yes, I know you can generate low-level bindings, but those are full of raw pointers and creating idiomatic Rust interface on top of that is not trivial. And that's just C, C++ bindings are even harder. TL;DR Rust still very much needs to prove itself to the industry. Till then, it's pretty unfair to shame people for using C/C++. I'm not surprised people are laughing at or angry with Rust evangelism... 
maybe we need a language that supports analogies better than English. 
Yes please!
If I have to be beaten at something, by someone, I'm glad that it's Rust macros and Quxxy
I work as an embedded software engineer, and there's no way I could convince my company (Defense Contractor) that Rust could be used in place of C/C++. It's just not ready. Plus we'd have to change every tool we use.
nobody gonna mention michael fox doing shit on rust? Ok...
So you're saying you just need to unroll the loop unrolling macro?
Should work fine now :)
Well, it will sometimes take a while before all the dots are connected. Rust is not just "language X, but better", but combines ideas and patterns from all over the place. Sure, its closest relative is probably C++, but it's like it was mixed with JavaScript, evolved in parallel with Swift, brought some philosophies from functional programming, etc. (Which is kind of true) All in all, it's a combination of things that, while not new on their own, makes a mix that is unfamiliar enough for many people to make it anything from not entirely trivial to actually hard to pick up. Despite earlier programming experience. Just keep playing around with it, and don't stress it. In my experience, Rust shows its true value after the getting started phase, when you know the basic syntax and borrowing rules.
The point here isn’t about Rust at all. It is about the repeat claim that there is no problem with C. You do that too: if one is careful and tests a lot, it’s going to be okay. That is clearly not true, the article includes examples towards that. It even points out that just throwing money at it evidently isn’t working very well, either. Rust may very well not be the answer to all this. But refusing to accept that there is a serious problem definitely isn’t.
Do you know any way to find out which amount was related to memory safety bugs? Unfortunately, that was the only figure I could find.
I'm not sure of a scientific way to measure this, but I looked at the descriptions of the last 5 CVEs in Chrome, and 4 of them sound like memory corruption, while only 1 sounds like a logic bug.
... I mean, I *could* turn this into a hideous monster using macro callbacks, but I'm not going to. Mostly because I can't be bothered. I feel that after working out how to parse generic items in `macro_rules!`, I'm forever excused from having to write needlessly complicated macros.
Not feasible for those of us with hectic travel schedules.
safety can be retrofitted with static analysis. There are motivations to move from C++ to Rust are nothing to do with safety, rather other forms of cleanup that are *impossible* to retrofit. As such as soon as I see people talking primarily about safety, I begin to suspect think there is a bit of mis-selling or exaggeration going on. rust uses bounds-checked arrays, which implies rust programs *still* require *empirical testing* before they can be considered 'correct' (a bounds check is an admission that there are potential logical holes you have not yet closed, I call it a "debug build") 
There's a *lot* of missing punctuation in that post.
As a reasonable counterpoint to the author's claim that C/C++ have no place in the systems world, the bottom of the article has a link to his previous post "How copying an int made my code 11 times faster", about how implicit re-borrowing in Rust led to an inscrutable performance regression that required him to crack open the ASM to figure out wth was going on.
I wonder if you could implement this as a compiler plugin. Then you probably wouldn't have to work around non-literal constants anymore, and it would look nicer: #[unroll] for i in 0 .. MY_CONSTANT { println!("Iteration {}", i); }
this is what they aren't always admitting. **rust programs still need empirical testing**, the use of a *runtime bounds check* is an admission of potential logical holes. &gt; Matrix libraries and similar are a pain compared to C++. and I keep talking about these issues, and they're in denial. Sometimes general overloading+unbounded templates IS a win. (just because its bad sometimes, it doesn't mean there aren't cases it makes easier). 
That could have just as easily happened in C++ though.
&gt;"Among numerous issues were misuses of C, as one of the units could overflow its stack." Rust would have panicked? thats still a crash. There are some issues you can only fix with empirical testing. Rust programs have runtime bounds checks: thats an admission that potential logical holes remain. If your car control software has an out of bounds array, *it's still going to crash*. this is where I think rust evangelists can be a little guilty of exaggerating /overselling sometimes. see the whole analogy used in this headline - rusts safety is manual, not 'self-driving'. It's the elimination of dangerous shortcuts, nothing magic. *You still have to do the work to write a safe program.*
... and you still have to test it to the point you don't have *unbounded array access*. if your embedded program panics on an 'out of bounds array', it's still a failure. Rusts use of bounds-checked arrays is an admission that *you need testing that Rust doesn't help you with*. You can do bounds checks in your C++ debug builds very easily. As such your company is probably wisely estimating 'we can just use static analysis on our existing sourcebases.' If there's a reason to switch , it's other factors of convenience - other frustrations with C++ which have to be weighed against the cost of throwing code away and learning new tools.
Same thing happens for me. Copy/paste from the examples gives roughly the numbers you are seeing. 
I actually think that may make it more true: a single thread doing a lot of work has to spin up the big cores and yet only use one for a while (and, as I said, I believe it is generally rare for power control at the level of an individual core rather than pairs or large groups), but a parallel job can use all of the big cores to finish faster and thus switch them off/switch back to little cores faster.
&gt; However, using the loop variable will cause it to never unroll the loop. Why is this? Is it just LLVM doesn't see it's possible?
&gt; Rust would have panicked? &gt; thats still a crash. Wouldn't a watchdog process simply have restarted it then?
&gt;&gt;"The amount of time writing code is vastly smaller then the amount of time reading and debugging code" you still have to read and debug the declaration. macros are needed to make the builder pattern easier to use; he'd 100% right that it's an unacceptable level of boilerplate in 2017. if you want to eliminate macros, you need to call for proper named/optional arguments. 
Power consumption isn't 100% dictated by total CPU time, so I don't think we can make such a pronouncement yet.
I don't know! From testing with godbolt I couldn't even get it to unroll 2-iteration constant loops if there's any IO inside. I don't know what happens if you do local side-effects but they can cause the loop to be optimised out completely so I used IO in order to avoid that
just a thought: could reverse inference streamline using a builder, e.g. what if ```'Foo::new(..)'``` took a builder object; could you then say ```Foo::new( bar(1).baz(2) )``` .. with 'Foo specific' versions of 'fn bar()', 'fn baz()' being selected by reverse inference from the expected input to 'Foo::new(...)'. Might be much harder for any autocomplete to figure out though e.g. ``` trait Bar{ fn bar(..)-&gt;Self } ``` ```impl Bar for FooBuilder{ .. fn bar(..)-&gt;FooBuilder ..} ``` , ```fn Foo::new(FooBuilder)-&gt;Foo``` (the need to write '::new(), '.finish()' in addition to 'Foo..' ..)
This is exactly what I want, but our product has to compile on stable. Plus, I don't know if compiler plugins actually have access to the constant values
what would stop the situation that overflowed it recurring ? it's a logical hole. this kind of software can 'fail gracefully' by estimating load up front, throttling. it's a different mentality to application software. (my point more relates to buffer over-runs, where the possibility of panic implies you *aren't* doing this, but a stack overflow is a similar issue, i.e. not knowing the recursion depth up front, making some estimate and having a fallback plan known about upfront..) i've never done software for cars, but i've done renderers that have to work in a fixed amount of buffer space (pre modern z buffers). you can't just 'fail'. you had to sort the objects and do a cut-off intelligently ('draw closer stuff first'. maybe lower the lod ..') . 
Interesting, this is definitely good to know!
Aha, brilliant - I'm new to rust so I'd not even come across rustup. That feels like the right method to use :)
My observation has been more that Rust's evolution is conservative and carefully considered. Rather than jumping on general overloading+unbounded templates, they're taking the time to see if there might be some other, more elegant solution that wasn't possible in C++ because it would clash with some other design detail which doesn't apply to Rust. (ie. "C++ isn't going anywhere. Let's not rush to be a full replacement for it to our own detriment.")
&gt; safety can be retrofitted with static analysis. While I'm not sure this is provably false in the case of C/C++ (it seems likely that it would be undecidable to prove an arbitrary c/c++ codebase is free of memory safety errors) it is definitely something we can show has little evidence behind it. Lots of companies have invested tens or even hundreds of millions attempting to do this, among other approaches to securing the codebase. It has not worked out so far, though it has certainly raised the cost of finding these vulnerabilities. &gt; rust uses bounds-checked arrays, which implies rust programs still require empirical testing before they can be considered 'correct' Correct and safe are two different things. Rust programs do *not* need tests to be shown safe (regarding memory safety) as long as they do not use unsafe. Virtually all programs require tests to prove semantic correctness. Rust and C++ do not differ here much at all. I don't think it's really relevant.
I guess it's a code elimination issue.
I'm definitely happy that JAI is out there. No language can be ideal for every use case and it takes some of that pressure to be everything for everyone off Rust. That said, I do agree that throwing out dot syntax entirely is an overreaction.
There's a difference between a panic and a stack overflow in C: - in C, because of the stack overflow, you risk corrupting whatever memory was there, - in Rust, with a panic, you have deterministic behavior, and can program a "default behavior" (such as gracefully decelerating, or stopping automation). And unlike C's overflow, it can be tested. In C, you cannot prove that your software never overflows the stack; a 1000 successful test runs merely show that the test runs are successful. In Rust, you *can* check whether your test triggers a panic or not, and you *can* check what it attempts to do to recover. Sure, it still means you need to actually program for it! Still, deterministic behavior beats undefined behavior every time when safety is concerned.
Thanks. I'll take a look at it and give any feedback there to avoid further cluttering things up here.
There was a mention earlier that Rust 1.19 was slower than Rust 1.18 due to the migration to LLVM 4.0; this may be linked.
You can definitely turn off these checks in release mode!...but not because of anything special. :) The `contract` macro only shuffles things into an ordinary function, so any method of conditional compilation should work just fine. Hummmm...I should probably add that to the docs. :) Making an issue!
&gt;&gt; it seems likely that it would be undecidable to prove an arbitrary c/c++ codebase is free of memory safety errors arbitrary, sure. but the point of a static analyser is it tells you, and you fix the problems it highlights. Thats all that Rust is doing. it's just making it compulsory.. after you volunteered to use it. You can volunteer to run an analyser too. We're back into overhype territory with this claim. Rust has labelling of references - thats not in the C++ syntax, but you can make a smart pointer holding a reference, and use that as a label. I think in terms of 2 use cases mostly, 'accessors'(whose return values are short lifetime - rust elides for this) and 'functions whose return values own any pointers, so there's nothing to worry about'. you're just marking which is which, so it can say 'ok'. (i'm still wondering if without dynamic dispatch, the analysis could be done 100% without annotations, so long as you have whole program information. isn't the purpose of a lot of the annotations just to localise the error messages? I have another idea for that, sorting the fragments by versioncontrol. "which pieces of the error messages intersect with code I actually changed") Other than that, what can you possibly represent in rust that you can't represent in C++? it's not *magic*. I had someone in rust tell me 'the fact this* is a raw pointer precludes safety'. So your analyser just spits out an error if you actually do any pointer-arithmetic on 'this'. Big deal. - "raw pointers are dangerous" =&gt; True. - "the mere existence of a raw pointer means your program is unsafe" =&gt; False. I don't think the concept of 'unsafe blocks' is really so different to 'keeping the raw pointer manipulation to a few source files that implement smart pointers etc.' C++ has the tools now to write elegant abstractions with move semantics. it can do 'emplace_back' . the raw pointers were used for workarounds before that. I personally don't like c++'s standard library iterators , but you've got lambdas these days which allow writing the opposite style of iteration, where you don't need any references to escape. (you just pass your code *into* the traversal) 
&gt; .. IDE plugin based on the compiler's library; Making the same information available to the IDE and compilation should avoid repeated work Yeah. I have no problem with that and, if it makes repeated compilation during development faster or enables things like the planned Java 9 REPL, so much the better. &gt; we should infer as much as possible.. make the tools available and let people use them optimally per situation. Sometimes the types make things clearer, sometimes they don't. Those who don't like overloading want more explicit function names... if thats the case doesn't the more elaborate function name make the details of the type less important to read every time. Conversely if you do want to be explicit with the types, why not leverage that more through overloading.. The problem is that, if you're not careful about letting people do whatever they want, you end up with the aspects people hate about languages like Perl or CoffeeScript. That's why the RFC process is so necessary for this sort of thing.
Well, it appears the new version of rustc+LLVM is not too smart. The following function: #[inline(never)] fn without_result() { (0..1000).fold(0, |old, new| old ^ new); } Gets optimized to (on the playground, in Release mode): ; Function Attrs: noinline norecurse readnone uwtable define internal fastcc void @_ZN10playground14without_result17hc32518526ce8e3e9E() unnamed_addr #0 personality i32 (i32, i32, i64, %"unwind::libunwind::_Unwind_Exception"*, %"unwind::libunwind::_Unwind_Context"*)* @rust_eh_personality { start: br label %bb5.i bb5.i: ; preds = %bb5.i, %start %iter.sroa.0.0.i = phi i32 [ 0, %start ], [ %iter.sroa.0.1.i, %bb5.i ] %0 = icmp slt i32 %iter.sroa.0.0.i, 1000 %1 = zext i1 %0 to i32 %iter.sroa.0.1.i = add i32 %1, %iter.sroa.0.0.i br i1 %0, label %bb5.i, label %_ZN4core4iter8iterator8Iterator4fold17hb83045b7a39a0a4aE.exit _ZN4core4iter8iterator8Iterator4fold17hb83045b7a39a0a4aE.exit: ; preds = %bb5.i ret void } On the other hand (note the type annotation): #[inline(never)] fn without_result() { (0..1000).fold(0i64, |old, new| old ^ new); // ^~~ } Gives the expected: ; Function Attrs: noinline norecurse readnone uwtable define internal fastcc void @_ZN10playground14without_result17hc32518526ce8e3e9E() unnamed_addr #0 personality i32 (i32, i32, i64, %"unwind::libunwind::_Unwind_Exception"*, %"unwind::libunwind::_Unwind_Context"*)* @rust_eh_personality { start: ret void } There's something fishy here :( 
&gt; Dynamic types are not the same as statically inferred types. Statically inferred types give you the 'best of both'. it's the synthesis between the extremes (just like multi paradigm vs pure OOP /pure FP) I'm well aware of that. I'm talking about people who think static typing (inferred or not) isn't worth the hassle compared to being able to assign any value to any variable at any time because all they're judging is time to getting the program to run, not time spent making it run *correctly*. Often, with people like that, the only way to convince them of the value of static types is just to let them personally experience the pain of maintaining and evolving a non-trivial codebase for a while. &gt; Something else I agree with form elsewhere - 'working code that does something is better than any design document'. Pretending you can plan everything upfront is a fallacy. if we could forsee everything in our heads, we wouldn't need computers.. Getting something onscreen will inspire more useful thought than anything else. No argument... I have one Python-Rust hybrid project where my approach is basically "experiment in Python, then push the pieces down to Rust as they stabilize". That said, part of that is simply that I've been coding in Python for 15 years and Rust for only a couple and I counter with "Nothing is more demotivating than 'It works. Don't f@#k with it!'." ...especially when coming back to one of my Python projects with incomplete test suites that I haven't touched in a while. That's why Rust has things like the `unimplemented!` macro that are intended to be placeholders which get the code compiling but are easy to track down again once you're certain you're on the right track.
As pointed out elsewhere, the entire thread that is being linked here is not an official "pronouncement", but merely a discussion of ideas. It wasn't even started by someone on the official compiler team. And yes, power consumption is not 100% dictated by the CPU time, but there is a strong correlation in many setups.
&gt; Lots of companies have invested tens or even hundreds of millions attempting to do this, .. and how much would they lose throwing their source away a rewrite can be the death of an organisation
You'd probably shit yourself at what your OS is written in then. 
&gt; We already had smart pointers in C++ and they were being upgraded. True, but, for legacy compatibility reasons, the compiler can't enforce a model like Rust's which forces you to use pointers in a provably memory-safe way outside clearly marked `unsafe` blocks. That is the #1 reason I was never comfortable with C++. &gt; A big part of the draw to rust was 'having the common case syntactically lighter'. True, but that's not such a primary thing as to be "at all costs". Other concerns came into play. &gt; they kept saying 'you don't need box, it's allocations' I've never heard anyone who knows what they're talking about say it in such a sloppy and qualifier-free way. "Because Rust is smarter about using the stack, there are many situations where you don't need `Box` when your intuition tells you to reach for it." &gt; again, a form of resistance to vector&lt;T&gt; is that sometimes passing a pointer to a buffer rather than &amp;vector&lt;T&gt; is syntactically lighter. I don't follow. How does that relate to whether or not the `~` sigil is used?
It unrolls this just fine: extern "C" { fn black_box(a: u32); } pub fn test() { for i in 0..16 { unsafe { black_box(i); } } } --- example::test: push rbp mov rbp, rsp xor edi, edi call black_box@PLT mov edi, 1 call black_box@PLT mov edi, 2 call black_box@PLT mov edi, 3 call black_box@PLT mov edi, 4 call black_box@PLT mov edi, 5 call black_box@PLT mov edi, 6 call black_box@PLT mov edi, 7 call black_box@PLT mov edi, 8 call black_box@PLT mov edi, 9 call black_box@PLT mov edi, 10 call black_box@PLT mov edi, 11 call black_box@PLT mov edi, 12 call black_box@PLT mov edi, 13 call black_box@PLT mov edi, 14 call black_box@PLT mov edi, 15 pop rbp jmp black_box@PLT
Why the downvotes? This is a serious wish. There are C++ compiler environments for it and a major part of me editing Rust trainings and stuff happens on an iPad, often without access to the playground.
My OS just asked me to install a security update that closed something like a dozen arbitrary code execution vulns, basically all of which would have been prevented by safe Rust. So, yes, I'm not crazy about the fact my OS is written in C/C++.
Yes, it works ! :D
&gt; say it in such a sloppy and qualifier-free way. I was paraphrasing. they were claiming 'being easy to write was making people use more allocations'. Surely a much better solution to that is to just explain better where it isn't needed. &gt; I don't follow. How does that relate to whether or not the ~ sigil is used? ~[T], is syntactically lighter than throwing vector&lt;T&gt; around; my claim is some of the reversion to raw pointers in C++ is to avoid writing out these long names. T* in C++ can represent just about anything and it's easy to type. ~[T] and ~T were so nice because they made the safe patterns almost as syntactically light as raw C. this is a unique experience I greatly miss.
&gt;&gt; have one Python-Rust hybrid project where my approach is basically "experiment in Python, then push the pieces down to Rust as they stabilize" now imagine how amazing it would be if you could just stay in one language. Write it quick and dirty, then refine it. This is what I saw with the sigils. ~, @T. "too many allocations because you typed ~ without thinking"? ... thats your python prototype. Then instead of having to switch languages, you just go back and optimise. And if you find python useful (with its dynamic types) , why do you doubt that enabling whole-program inference would be useful? surely you'd save time by not having to switch environments between rust &amp; python? I don't get why they resist default args etc aswell. It's madness. we write tests.. the tests don't have to be efficient. we have outer loops. the ability to mix 'productive' and 'efficient' in one place would be awesome. (it's possible apple will get there if they retrofit move semantics to Swift)
This the the subreddit for the Rust programming language. You're looking for /r/playrust
That isn't really my point, or what I'm suggesting.
See also https://github.com/millardjn/typenum_loops for another approach to this.
I'm confused. What's the crate-level organization of things? Do you only have 3 binaries with duplicated code? Or do you have 3 binaries with the common stuff factored out into one lib? If it's the latter then I'm not sure what more you need. The compiler should eliminate code that goes unused by a particular binary. I'm also not sure what overhead you're referring to when it comes to having things in separate crates.
&gt; arbitrary, sure. but the point of a static analyser is it tells you, and you fix the problems it highlights. Thats all that Rust is doing. Yes, technically that's "all" they're doing. The difference is in coverage. Rust has much stronger coverage than C++ static analyzers. &gt; it's just making it compulsory.. after you volunteered to use it. You can volunteer to run an analyser too. We're back into overhype territory with this claim. The difference goes far beyond choosing to use static analysis or not. &gt; Rust has labelling of references - thats not in the C++ syntax, but you can make a smart pointer holding a reference, and use that as a label. You can not (easily) get 0 cost shared references + ownership while guaranteeing that the owner does not free the value behind the reference before the other references are dropped. &gt; Other than that, what can you possibly represent in rust that you can't represent in C++? See above. You *can* reprsent it in C++, I would expect, but not without a ton of work/ basically learning a ton of new approaches to C++. &gt; Other than that, what can you possibly represent in rust that you can't represent in C++? Probably nothing, in theory. The question is the effort involved to express things in C++ that are trivially expressed in rust. Although dealing with data races seems *particularly* tricky to do in C++, while maintaining immutable shared data, no UAF, etc. &gt; I don't think the concept of 'unsafe blocks' is really so different to 'keeping the raw pointer manipulation to a few source files that implement smart pointers etc.' It is. There's considerably less UB, better abstractions, better grep'ability, and an explicit note that says "this may not be safe". &gt; C++ has the tools now to write elegant abstractions with move semantics. Which are not like rust's move semantics, and can still bite you (moving a value behind a pointer, then access pointer, pointer is null == UB). But this is all silly. Yes, you can probably, through extreme measures and effectively using a completely new language, write safe C++. You get it for free in rust. That's not a small gain.
I'm pretty sure that's just because unrolling doesn't help when the loop body is large, as it will be with IO. Knowing when to unroll is fairly tricky with modern CPUs and compilers, and generally speaking should be done rarely.
Agreed, but I wrote this solely in order to force unrolling in [one specific file](https://github.com/paritytech/parity/pull/6080). Even with the most uncharitable interpretation of the benchmarks (taking the highest of the "after" results and the lowest of the "before" results) that still leads to a speedup of 10%. I definitely agree that you should do this sparingly. It's exactly like inlining, it should always be a tool in your belt but overusing it can harm more than help. There is a 256-iteration loop in that file that unrolling caused to slow down. I may retry it unrolling 8, 16 or 32 iterations at a time but frankly I think I've got the main ones that can now be SIMD'd/const-folded but weren't before.
When it works it's crazy good! But... there are a lot of restrictions: you can't create any new variables, just change existing logic. But that's still good enough to reduce the number of edit-compile-debug iterations. The way it works is pretty 'simple': When the flag called "edit &amp; continue" is enabled (which forces you to have other flags a certain way eg no optimizations) the compiler inserts a big chunk of uninitialized read+write+execute memory in your binary (".textbss") Every compiled function goes in a big table where each entry is a jmp instruction to the actual function. Instead of calling the actual function this dispatch table entry is called instead. Upon recompiling a function it gets written to .textbss and the corresponding dispatch table entry is updated. Oh and a whole lot more to translate the current instruction pointer to the new function so you can edit the _very same code you are running_. Like I said, when it works it really cuts down on edit-compile-debug iterations to fix minor logic bugs.
I'm just following the benchmarks. The [PR that I wrote this for](https://github.com/paritytech/parity/pull/6080) leads to at least a 10% speedup (given the least-charitable interpretation of the benchmarks, picking the highest "after" and the lowest "before").
It should be fixed now: https://github.com/redox-os/website/pull/133
Wow. I feel like I contributed to something.
I just wanted to show that using the loop variable doesn't stop unrolling. LLVM has an analysis tool that will give you reasons for why it couldn't do certain optimisations like loop unrolling so maybe you could use that to figure out the real reason why those loops weren't getting unrolled. It's usually because of pointers being passed around or a function that LLVM can't see the definition of.
&gt; now imagine how amazing it would be if you could just stay in one language. Write it quick and dirty, then refine it. &gt; &gt; This is what I saw with the sigils. ~, @T. "too many allocations because you typed ~ without thinking"? Fair enough. I'm not sure that's how I'd prefer to do it, but to each his own. &gt; And if you find python useful (with its dynamic types) , why do you doubt that enabling whole-program inference would be useful? Two reasons: 1. Because the whole reason I use Rust is to use the type system to lock things down as far as possible once I *do* know what I want. (I use Clippy in an "enable all lints, then opt out of specific ones" configuration.) For most programs, I prefer to go straight to Rust, since I'm not doing spending 99% of my thinking on hypothesis formulation and testing unrelated to the language and implementation details. 2. The particular example I mentioned would gain nothing from whole-program inference. When I translate it to Rust, the parts I'm iterating on turn into stuff which relies fully on local inference aside from the occasional turbofish on a `collect()`. (Plus, required types in function signatures have actually caught misunderstandings on my part in some cases which would have slipped through otherwise due to `Deref` coercions.) All of the productivity gains are due to Python's APIs being designed around garbage-collected memory management. (It's a project where I'm experimenting my way through a bunch of different heuristics I brainstormed up to see which combination produces the highest accuracy score on a corpus of 841 hand-written "filename -&gt; title" pairs.) &gt; surely you'd save time by not having to switch environments between rust &amp; python? I would... if it were possible. The benefits I get from Rust and the benefits I get from Python would pull the language in such different directions that, to meet my needs, it'd basically wind up becoming a more official version of the Rust+rust-cpython+Python stack I use anyway. (Speaking of which, rust-cpython makes it really easy and comfortable to expose Rust functions and constants to Python. That's why my "Prototype in Python, transcribe to Rust once it settles down, export it to Python, and prepend `n.` (`naming::`) or `c.` (`constants::`) or whatever to the old Python call sites" workflow is something I consider acceptable.) &gt; I don't get why they resist default args etc aswell. It's madness. As I said, they take a conservative approach to things. Default args do have downsides and there are questions about what approach should be taken to actually implement. Beyond that, there are things which need to be fixed more urgently. Thus [discussion is still ongoing](https://github.com/rust-lang/rfcs/issues/323).
&gt; What's the crate-level organization of things? I have a single crate, with a single Cargo.toml and a single src directory. &gt; Do you only have 3 binaries with duplicated code? The code is not really duplicated. &gt; do you have 3 binaries with the common stuff factored out into one lib I have multiple chunks of common stuff, see the structure and dependencies between chunks above in the post. &gt; I'm also not sure what overhead you're referring to when it comes to having things in separate crates. Here the overhead for me is having multiple Cargo.toml files and multiple directories. I would like this project to have a simple layout and to be able to build all of it with a single "cargo build", rather than write scripts. &gt; The compiler should eliminate code that goes unused by a particular binary. If I have a structure like this: bin.rs mod alpha mod beta alpha.rs mod util beta.rs mod util util.rs ...some utility functions and constants ...would the binary contain a single instance of the constants and functions?
Should report a bug
You could compile with clippy under a feature (clippy's README has directions how to do it), then you wouldn't need to comment the lint settings. Also IIRC you can set multiple lint levels in one `#[warn(..)]` annotation by joining the lint names with comma.
&gt; they were claiming 'being easy to write was making people use more allocations'. Surely a much better solution to that is to just explain better where it isn't needed. If that's true, I share your viewpoint, but I'm going to have to say "[citation needed]". &gt; ~[T], is syntactically lighter than throwing vector&lt;T&gt; around; Except that, when I see `~[T]`, I don't think `Vec&lt;T&gt;`, I think "Box&lt;[T]&gt;" (ie. a boxed slice). That's a bad misconception to have. &gt; my claim is some of the reversion to raw pointers in C++ is to avoid writing out these long names. I can agree with you there, but Rust's `Box`, `Rc`, `Arc`, `Vec`, etc. aren't as long as all the confusing pile of pointer types in C++. I think this is just a question of how we subjectively weight the pros and cons of something right on the edge of our comfort zones. &gt; T* in C++ can represent just about anything and it's easy to type. Bear in mind that it's part of the Rust design philosophy to be wary of "simple syntax that could be hiding heavy operations". For example, that's why it has explicit `.clone()` and no copy constructors.
This is my first week of vacation and I spent much of it learning Rust. I've been working through [exercism.io in the Rust channel](http://exercism.io/languages/rust/about). So far I have coded up two exercises that didn't exist for Rust and submitted pull requests. Also working through [Too many linked lists](http://cglab.ca/~abeinges/blah/too-many-lists/book/) which I wish I found sooner. I started Lode-Ruster (a rust clone of the Apple II version of Lode Runner.) Using ggez. Although, I got a little ways and went back into learning mode for now, as I'm hitting some rough edges with my knowledge.
I've rewritten the TL;DR section.
Is an unroll missing in this case? https://github.com/Vurich/crunchy/blob/0ae7fc83840a6f7a73bcb8e16993952fdc07b622/src/lib.rs#L15
I can't speak too much, but there are a lot of ECSes in Rust, and they've all had to adapt in some ways to the way Rust works. I think "specs" has written a lot about their approach, IIRC? Might want to check their dev blog out.
I'm writing a lexer for a programming language and I need to parse Unicode code points like `U+xxxx`. The complete regexp from the original project is `/^u\+[0-9a-f?]{1,6}(?:-[0-9a-f]{1,6})?/i` but it is not that important. My question is, having this code point, how can I transform this into something meaningful in Rust like a `char`? Looking through the docs I didn't immediately find something that seemed like what I needed.
Split to crates, use workspace. One line in `Cargo.toml`, like ``` other_crate = {path = "../other_crate"} ``` is not big overhead.
Hey all, I put this together after struggling through it, and seeing some other people having similar issues on the #rocket IRC room. Let me know if you have any questions!
I would expect so. Or at least I can't think of why it would duplicate anything other than when doing inline optimizations.
&gt; it won't give you a linear scale up for the same power consumption The "won't" here is the pronouncement I was referring to. The only actual measurements I've seen about this sort of thing suggest parallelising does actually usually decrease power consumption in practice, e.g. https://blog.servo.org/2015/09/11/timing-energy/ . Unfortunately that post doesn't present a sequential vs. parallelism comparison very clearly but it does say: &gt; Single trials of other configurations sometimes beat using 4 layout threads in total runtime and energy consumption, but not consistently
Should perhaps link to this: https://doc.rust-lang.org/nomicon/leaking.html (rather than write a whole paragraph about leaking in the docs)
This has nothing to do with the loop variable
Has its places, but overuse of C *is* a problem. It's so easy to violate memory safety that it requires an exceptional amount of discipline, caution, or whatever you want to call it to avoid exploitable code. I mean exceptional in the sense that industry/open source has demonstrated that C programs have many memory safety issues. For evidence of this I broadly cite CVEs, academic conferences like usenix or IEEE Security and Privacy, and google project zero (the list could go on). These days, there are formal tools that can close the safety gap for C code, but often times you have to write C code in a special way to get that to work. Take for instance, the special rules that NASA maintains for their [control systems code](https://ntrs.nasa.gov/search.jsp?N=0&amp;Ntk=All&amp;Ntt=coding%20standards&amp;Ntx=mode%20matchallpartial). 
Yes but a lot of software is already written in it. There's no need to rewrite every program in Rust when C versions exist. I think Redox is in some ways an exception because it shows that Microkernels can work. I dont like redox's desire to replace every piece of software, though. Reduplication of effort is always bad, when there exist free open source programs already.
You might also want to read [this comment](https://www.reddit.com/r/rust/comments/6j3ljv/looping_on_a_member_variable_without_mutably/djct23y/) since it goes into a subtle but important distinction between `for` and `while` which doesn't exist in many other languages. (It's in response to a blog post I made about a situation I found where I have to use `while` instead of `for` to please the borrow checker.)
&gt; It is about the repeat claim that there is no problem with C. I ackonwledged the problem. &gt; You do that too: if one is careful and tests a lot, it’s going to be okay. That is clearly not true, the article includes examples towards that. No, not really. The examples are just two (the second of which - "Android" - is too vague to even count) and there's no analysis whatsoever. My point was that if you are careful with C and have 100% or near-100% test code coverage, you can't be absolutely sure it's safe, but the confidence is fairly good. The problem is that very few projects actually do that, because it's very costly. If you'd like to see some higher-quality C code check out PostrgeSQL for example. &gt; But refusing to accept that there is a serious problem definitely isn’t. What do you want people to do then? Stop writing bugs? 
It will unroll code that does IO without using the loop variable 
You know OS can't be written entirely in safe Rust, right?
&gt; Yes but a lot of software is already written in it. There's no need to rewrite every program in Rust when C versions exist. I have strong but mixed feelings about that. On the one hand, battle hardened code that's been in use for 20 years is "fine". Let's just use it. No reduplication. No new bugs. Lots of quite practical benefit. On the other hand, people are going to rewrite these things regardless of how I feel about it and they're not using my time/energy/budget. As the saying goes, you can't make an omelette without breaking a few eggs. While I'm unlikely to rush out there and rewrite a bunch of code because "lol, C", I'm actually quite interested when others champion such efforts. There's already been a couple utilities that have been rewritten in Rust and I follow the development and keep trying to fit them into my routine (ripgrep, alacritty).
Keep this up. This is the kind of thing that spurs participation and keeps people excited. Another nice way to increase participation is reduce the barrier to entry. Any thoughts on an automatic setup script to allow people to get into the project easily? or some other barrier reduction steps? I see some nice documentation of the structure.
Parse the codepoint value to `u32` and then use [`char::from_u32()`](https://doc.rust-lang.org/nightly/std/char/fn.from_u32.html).
I have no problem with it, of course not, but I would never waste my own time on it. What I would consider, is incorporating my own rust code into an existing C app to improve it in some way, perhaps releasing this as its own variation. I would never recommend to another person that they write their code in C unless they had a very good reason.
There's a gigantic difference in code generation between printing a literal and printing a literal and an int
I think you meant to say "buffer overflow". Stack overflow is a totally different issue and both C and ~~Rust~~ are vulnerable to it (for now). Edit: Stack probes have been recently implemented: https://github.com/rust-lang/rust/pull/42816
It's been a while since I've written a syntax extension, but I don't think they have access to the values of constants at the point in the pipeline where they're expanded. This would probably have to be a MIR pass (though it would be a pretty damn cool one).
This is a valuable write up. I did something very similar for my crate, however I had a special `ErrorKind` for invalid user input and handled it differently from other errors: * For `ErrorKind::InvalidUserInput(String)`, the internal message is shown to the user as a `400` response. * For all other errors, the error is recursively unwrapped to get all the causes, and are logged and not shown to the user and the user gets a more generic `500` response.
Aaaaah. No. Don't do this, you are fixing the symptom, not the cause, making matters worse. The interface described up there is telling rustc that it wants to borrow the solver, which rustc happily detects as a weird situation. Don't _ever_ use transmute to manipulate lifetimes.
There is no way a 2048-long unroll (64 * 2 * 16) is the best way to do things. I note that the "compress mix" loop unrolls fine on its own, at least when in its own function. The other reasonably-sized loops are likely to work similarly. That said, I am concerned that your code is running `get_unchecked` on clearly out-of-bounds data (even if you know that there is data there). Do you really need to do that? I feel cryptocurrency stuff should be sticking to cautious approaches on these issues.
Very cool! I think tomorrow I'll look into how to more clearly handle different kinds of errors, and the kinds of responses they generate. For example, it would be nice to create ErrorKinds, then map them to responses for "bad authentication" =&gt; 401, "no such object" =&gt; 405, etc, rather than just returning 400 errors for every `Err`. This would also make it easier to have high level message, rather than full error chains.
I don't quite understand what your SearchPath describes, so I cannot give you good recommendations for an actual implementation. Currently, the problem is that you tell Rust that SearchPath borrows the Solver, because that's what you tell it (by returning a reference). Your attempts to add another lifetime don't help there, because you still state that they belong together. If you would separate them fully, Rust would assume you are returning stack data, which also doesn't work. Your SearchPath currently expresses an _immutable borrow_ to the solver. If you express an immutable borrow to a mutable borrowed thing, this mutability cannot be used until the immutable borrow is over (which make sense, it wouldn't be very immutable, then). But you don't want to throw away the SearchPath. What helps is to return an owned `Option&lt;SearchPath&gt;` (no ampersand) instead. You could pass that through your whole solution process.
Ah I should have mentioned, we do have a separate case for 401 errors, but yeah it's much better to be explicit about the others.
I opened a [pull request that rewrites that script as a Rust build script](https://github.com/Vurich/crunchy/pull/1), with cargo features for the maximum limit.
Of course not, but it can be written in mostly safe Rust. [Here's the list](https://support.apple.com/en-us/HT207922), how many of them would you say would strictly require unsafe Rust?
Make one lib, then have three files in src/bin. See here: http://doc.crates.io/manifest.html#the-project-layout
I'm interested to hear more about the "segfault in safe code". Does that mean there was a bug in rust that was uncovered?
&gt;internals.rust-lang.org uses an invalid security certificate. &gt;The certificate is only valid for users.rust-lang.org 
Try /r/playrust maybe?
You'd have to trawl through the bug bounty reports themselves (usually it's mentioned when it's an RCE/UAF/etc)
&gt;&gt; I can agree with you there, but Rust's Box, Rc, Arc, Vec, etc. aren't as long as all the confusing pile of pointer types in C++. they're the same, if you want the same features with smart pointers - you can replicate them; all you've done is eliminated a shortcut. you can alias shorter names, it's really the nesting that you think about. eventually you run out ('box' and 'vec' jump out as something else to someone who deals with bounding boxes and x/y/z all the time lol) ```T*``` does still have a legitimate safe use as ```Option&lt;&amp;T&gt;``` .. it's certainly fidler to determine if it's being used correctly but I think this is within static analysis i.e. 'unlock the value with a check', 'make sure it never escapes'. &gt; Bear in mind that it's part of the Rust design philosophy to be wary of "simple syntax that could be hiding heavy operations". For example, that's why it has explicit .clone() Now that I do actually like (explicit .clone() vs assignment sometimes making deep copy) - but every use of *T is light, because it does no automatic operations for you; (equivalent of 'std::move()' being the default unless you say otherwise) passing ~T around is also a 'light' operation. admitedly creating one isn't but at least that isn't hidden. &gt; Except that, when I see ~[T], I don't think Vec&lt;T&gt;, I think "Box&lt;[T]&gt;" (ie. a boxed slice). T the fact the ~ is bound more strongly to the [ never confused me for one moment; as with &amp;[T], and trait objects - there's something similar going on that the size information is in a different place - so you have to be mentally ready for it. (**both &amp;Trait and &amp;[T] have a similar divergence in being a pair of values, rather than 'the address of (the vtable and object), 'the address of (a slice and it's size)** ) I'm reading it as a 'modifier prefix' in the AST rather than just a shortcut for Box&lt; .. and given the benefits (compared to the alternative taking shape in the C++ world), my reaction was enthusiasm.
&gt;&gt; The benefits I get from Rust and the benefits I get from Python would pull the language in such different directions well if Apple succeed in retrofitting move &amp; borrow to swift, they'll have it https://twitter.com/clattner_llvm/status/832644452451692544?lang=en https://gist.github.com/Gankro/1f79fbf2a9776302a9d4c8c0097cc40e wouldn't it be great if Rust kept the initiative and stole their thunder? in gamedev I saw this mix between 3 styles .. - 'offline tools' (NOT performance critical, but complex behaviour), - 'runtime engine' (extreme performance critical, a branch out of place was fatal.. which is why i'm so critical of bounds checks) - but a lot of 'setup code' &amp; debug written *in the engine* during development that *wasn't critical* - e.g. you'd start out with the shortcut of loading data directly from art packages into the engine, in unsuitable formats.. longer loading times/ but you've got the HD/more RAM on the devkit.. - this was just a shortcut for prototyping code that must move into the offline toolchain; as such, *having the tools &amp; engine in the same language, same types* was hugely beneficial. - if it wasn't for this workflow practicality, a GC'd language would have been vastly superior for the tools usecase. Hence why I even raved at the concept of the syntactically light @T. - actually plain C would have been ok for the 'final shape of ideal code' (because you don't actually *need* dynamic allocations, so RAII isn't helping..), but you rarely got there due to deadlines, and using 'a GC'd language' instead of 'C++ with dynamic allocations' would have yielded an inferior 'intermediate state' as such was unusable; there's also the 'discovery issue' for writing maths with dot-autocomplete) - the job of the 'offline toolchain' was to do complex data transformations to simplify the runtime. In the ideal final optimised state, there would be no dynamic memory allocation in mainstream runtime code, just specialised presized pools, although in practice in later projects this wasn't always achieved. c++ operator 'new' could still be overloaded to use dedicated pools, to 'look' more like what C++ programmers expect. We did it earlier on when teams were smaller: our earlier games did not use any sort of dynamic malloc/free 'heap' and had the same overall behaviours, just less scale and variety. - you'll have heard of the 'BLOB' use case where the tools literally pre-compile a runtime object graph representing a level that can be blasted into memory costing no runtime CPU setup. immutable data. safety would have required a 'verify pass', but going through the curated channel it's a trusted source so this CPU cost can be eliminated.. - and of course UI-heavy tools (editors) - not performance critical, but closely integrated with datatypes that eventually filter through the other parts. - some editors embedded in the game engine, not performance critical, but its' nice to be able to fly around and drop / inspect things - 'creative programming' done where iterative art/design is the primary consideration, - 'game scripting' - embedded lua (very similar to the python+c use case) - other 'creative code' in the runtime but whose writing was extremely different - usually written by programmers outside the 'low-level skilled technical team'. e.g. it might use vtables as a prototyping shortcut, which them have to be stripped out in optimization - and of course shaders, which are kind of like offloaded lambdas, I still dream of a language that can handle that - SPIRV may eventually yield it. - dynamically updateable (hot swapping), requiring input both from 'creatives' and low-level performance. - Due to the complexity of the sourcebase (including ever more complex graphics APIs , we were dealing with 2 APIs that were more like Vulkan, by necessity.. 'gl' was unusably slow/unavailable- and an internal vector maths library making abstractions to avoid CPU-killing 'branchy-floatmaths') - **dot autocomplete** was essential for finding your way around all that. There was a genuine spectrum between each of these extremes (rapid iteration, mainstream 'application programming', and 'intense low level' ruthlessly inspect the asm output , to check no value is spilling between different register types, ensure that new platform-specific custom instructions are efficiently used etc etc), *with code regularly moved between each case*. The processor that we really needed- CELL- (and the the world still needs for a different use case: AI - machine vision tasks) was eventually ditched because back then the software tools weren't really up to it (e.g. too hard to abstract the combined memory manipulation and parallelism ). (The world is basically going down the same path again via different routes - China's supercomputer chip, TPU, movidius, . thats a tangent..) - (this is when I first became interested in FP but it was nowhere near ready/practical r.e. gamedev use cases) The closest thing I ever saw to the language I want was **Rust circa 2014**. The do notation was extremely promising and important as I invisaged that becoming the way shaders would be invoked, I imagined a future where *parallelism &amp; GPU offload* was as syntactically light and intuitive as regular program constructs, so all those 'creative coders' can still use it when tweaking particle effects and gameplay behavour; there's a paper about 'early rust' being used in a very similar way to what I imagine then they went and ruined it. a 'bait and switch' moment that tortures me every day. EDIT toned down a bit, sorry :)
The generated assembly in Release mode is: _ZN10playground14without_result17h2c5e8efbb0dc5309E: .cfi_startproc xorl %eax, %eax .p2align 4, 0x90 .LBB0_1: xorl %ecx, %ecx cmpl $1000, %eax setl %cl addl %eax, %ecx cmpl $1000, %eax movl %ecx, %eax jl .LBB0_1 retq It seems that it's doing nothing (basically `while (1000 &lt; 0) {}`). This shouldn't take 1,000+ ns per iteration.
I'm sorry to be so abrupt, but this conversation is wearing me out and my interest has been waning for a while now. I'll give you one more response, but then i'm going to have to bow out. If you want, you can have the last word. &gt; they're the same, if you want the same features with smart pointers - you can replicate them; all you've done is eliminated a shortcut I still see it as eliminating an insufficiently justified special case in the language syntax before the 1.0 compatibility promise takes effect. It may be useful, but it hasn't yet been adequately RFC'd and defended. It's not as if Rust absolutely refuses to accept new syntax. Look at `try!` becoming `?`, for example or the discussion around adding an inclusive range syntax. &gt; T* does have a legitimate safe use as Option&lt;&amp;T&gt; I don't remember there being a `T*` and you switch to `*T` later, so I'll assume you're talking about raw pointers. That said, from my perspective (that of high-level meaning in the type system), your statement makes about as much sense as expecting a meaningful result from `length_in_meters + temperature_in_celsius`. Sure, under the hood, the [`NonZero`](https://doc.rust-lang.org/core/nonzero/struct.NonZero.html) optimization folds `Option&lt;&amp;T&gt;` down into a nullable pointer, but that's irrelevant to discussion at this level. (If the `T` in `Option&lt;T&gt;` is marked as `NonZero` or is a struct containing a field with a type marked `NonZero`, then the `None` value for `Option` will be represented as a zero without any additional memory use.) &gt; the fact the ~ is bound more strongly to the [ never confused me for one moment Anecdotes aren't sufficient as evidence and I've observed that Rust design tends to err on the side of "less magic" and teachability when the intuitiveness of something is uncertain.
&gt; well if Apple succeed in retrofitting move &amp; borrow to swift, they'll have it &gt; &gt; https://twitter.com/clattner_llvm/status/832644452451692544?lang=en https://gist.github.com/Gankro/1f79fbf2a9776302a9d4c8c0097cc40e You missed my point. I consider the "ERROR: Attempted to use Python[-style] code in a Rust[-style] file" to be a desirable feature. It helps to ensure a strong separation between the stuff I'm prototyping and the stuff I consider solid. While I'd definitely like to see more effortless sharing of types, I don't want the risk of being able to mingle the two realms too easily. &gt; then those bastards went and ruined it. a 'bait and switch' moment that tortures me every day. Please don't engage in personal attacks. Also, it's their language and they are under no obligation to spend *their* effort on what *you* want. You're free to fork the 2014 code and develop your own language.
This is great, but are there any "full" examples (database connections, authentication, etc) that are available to review? I'm pretty new to Rust and would love to see an end to end example.
Another question about error handling, as for error handling in rust, there is a crate I have seen a lot before, [error-chain](https://github.com/brson/error-chain). What does this crate do? Its README says it easy to take full advantage of Rust's error handling features without the overhead of maintaining boilerplate error types and conversions. But I could not get its point, because refer to Python, its built-in try-catch-exception is enough, so why do we need another extra crate to handle error ?
&gt; What do you want people to do then? Stop writing bugs? Be open to considering alternatives. The result of these considerations may very well be that none of them is viable at this point.
Very nice! This and OPs blog post could help me overcome some of the struggle with Rocket. Actually, I really like it, but I haven't found a good solution for how to handle errors (especially the non-recoverable). I think using `error_chain` like this could improve the error handling story a lot. Gonna try it out in the future. Thanks :)
Whatever the true reason, the benchmarks speak for themselves. The code that I wrote this for got a 10%ish speed boost as a result of unrolling constant-bound loops, probably as it allowed easier autovectorisation and const folding. I'm never going to perfectly predict what the compiler will output given some piece of code, so that's why I use benchmarks to back up changes like this.
&gt; Rusts use of bounds-checked arrays is an admission [...] You keep repeating this point. No, it's not an admission of Rust's shortcomings. It is a core feature of Rust that is there for exactly the same reasons the borrow checker is there: to make sure logic errors don't cause memory unsafety. This is a worthy feature, not something "Rust doesn't help you with". Do you call it a Rust flaw that all logic errors can't be reported at compile time? That's not a Rust flaw, that is this world's flaw - it is impossible. Never mind the undecidability, you have to communicate your intention to the computer somehow. The human factor is always there. &gt; a bounds check is an admission that there are potential logical holes There can always be logical holes in code, so the information value of this "admission" is exactly zero. EDIT: wording 
I would rather be overcautious than undercautious with the unrolling. The benefits were mostly in the 3-deep nested loop. I hadn't noticed any clear unsafety, I'm very new to the project. I'll take a look. EDIT: I did just find some unsound unsafety that had been missed, I'll have a run through and replace it with sound code. It's one of those "it works but that's not the right way to do it" situations.
How do you know exactly how those vulnerabilities work, what code paths they affect so that you can claim that having your OS written in Rust will make those disappear? This sort of claims is another reason why people don't like people who go around telling everyone to kill C/C++ and rewrite their software in Rust. 
Hard to say without knowledge of the OS components. Probably all the drivers and kernel. Redox docs say that "the kernel has about 70 invocations of `unsafe` in about 4500 lines of code overall."[[1](https://doc.redox-os.org/book/introduction/unsafes.html)] (However, from that number, it's not clear how extensive or risky the unsafes are.) 
I believe the point is you need to post the list of *all* the OS features and reflect on how many could be written in Rust, not just the bugs. One could as easily claim that if you had used Logo you wouldn't have these bugs; you just wouldn't have an functional OS either.
Hey! I am working on something right now that will hopefully be open source (released in the next few months), but unfortunately I can't share it quite yet. Until then, [Rocket](https://rocket.rs) has a really good [Programming Guide](https://rocket.rs/guide/), [API documentation](https://api.rocket.rs/rocket/), and a whole bunch of [examples](https://github.com/SergioBenitez/Rocket/tree/master/examples). There is also the [Rustlang Thanks](https://github.com/rust-lang-nursery/thanks) project put together by /u/steveklabnik1, which is a good example of a full web application in Rust, though it is using a different web framework than I am in my post above.
 [SergioBenitez/Rocket](https://github.com/SergioBenitez/Rocket) &gt; *Description*: A web framework for Rust. &gt; *Stars*: 2518 &gt; *Forks*: 146 &gt; [Issues](https://github.com/SergioBenitez/Rocket/issues) | [Pull Requests](https://github.com/SergioBenitez/Rocket/pulls) [rust-lang-nursery/thanks](https://github.com/rust-lang-nursery/thanks) &gt; *Description*: A web app for contributors to rust &gt; *Stars*: 78 &gt; *Forks*: 27 &gt; [Issues](https://github.com/rust-lang-nursery/thanks/issues) | [Pull Requests](https://github.com/rust-lang-nursery/thanks/pulls) *** ^(This is Earth radio, and now here's human music ♫) ^[Source](https://github.com/anaskhan96/github-stats-bot) ^| ^[PMme](https://np.reddit.com/message/compose?to=github-stats-bot)
I'm not calling it a Rust flaw: I'm calling it a flaw *in the exaggerated claims about safety* I'm saying Rust is a "debug build"(+static analyser), not something as magic as the thread title **self-driving C++** suggests. The blogger is clearly exhibiting the phenomenon of exaggeration (which tends to go hand in hand with straw-manning C++). (and I find it frustrating when people focus *exclusively* on safety, when I'm here for *other* reasons)
I believe such concrete idea deserves opening an issue or sending a pull request to the book ;-) (https://github.com/rust-lang/book/).
&gt; or some other barrier reduction steps? A translation layer for Linux drivers would probably be the most significant single addition that could be made in this regard.
Don't we have stack probes now? This should make steak overflows defined behavior, shouldn't it?
Also check out [this image generator](https://github.com/nesteruk/ImageGenerator) and the [associated video](https://www.youtube.com/watch?v=NjPYUzBJ1gA).
Depending on where you're drawing the line on what counts as the OS, Logo probably would be a better choice than C/C++ for a decent number of features. But nobody said Logo or another language not suitable for low-level development, we're talking about Rust, which is specifically designed for this kind of use case. The suggestion that it's somehow not appropriate to want to see Rust used in the context it was designed to be used in is a little bizarre.
I'm away for the weekend but I'll try to take into account your comments next week. I added changes as per @coder543 comments I'm keeping here your links to the relevant commit 5. [link5](https://github.com/achntrl/rust-webserver/blob/919e7a42caea244162eed85ebdbe9b4a98ed6b97/src/main.rs#L70) 6. [link6](https://github.com/achntrl/rust-webserver/blob/919e7a42caea244162eed85ebdbe9b4a98ed6b97/src/main.rs#L71) Thanks a lot!
 [achntrl/rust-webserver](https://github.com/achntrl/rust-webserver) &gt; *Description*: My attempt at building a webserver in Rust &gt; *Stars*: 1 &gt; *Forks*: 1 &gt; [Issues](https://github.com/achntrl/rust-webserver/issues) | [Pull Requests](https://github.com/achntrl/rust-webserver/pulls) *** ^(This is Earth radio, and now here's human music ♫) ^[Source](https://github.com/anaskhan96/github-stats-bot) ^| ^[PMme](https://np.reddit.com/message/compose?to=github-stats-bot)
The suggestion that we have to know exactly how the code works to make a reasonable guess at what "A memory corruption issue was addressed through improved input validation" means is head-in-the-sand credentialism. Do subtle issues exist? Sure. But the vast majority of memory problems in C/C++ are garden variety bounds-checking or use-after-free errors that Rust eliminates by design. The claim is not that Rust is perfect or will make all issues disappear, but it doesn't have to to be significantly safer than C/C++. 
The claim was, if I remember correctly, exactly this: &gt; My OS just asked me to install a security update that closed something like a dozen arbitrary code execution vulns, basically all of which would have been prevented by safe Rust 
Is it have some features in compare to `top`/`htop`, for example can it show real amount of memory used by program, like spliting private and shared memory?
I linked the list in another comment, but [here it is](https://support.apple.com/en-us/HT207922) again. Most of them use the 'memory corruption' verbiage I quoted. If you accept Rust's claims of memory-safety, I think it's reasonable to guess that safe Rust would have prevented memory-safety issues.
For deployment no, but for CI infrastrature, for example `bindgen` 0.26/0.27 can not be build with rust 1.16, if it has in `Cargo.toml` something like `rustc = "1.17"`, it could give clear error, instead of something like `error: struct field shorthands are unstable`
No, it was just a bug in unsafe blocks within the `conc` crate.
Rust finally has stack probes on all Tier 1 platforms (it has had them on Windows for a long time, but Unix support required patching LLVM). I would expect the automotive industry, or any such "safety-conscious" industry, to ensure that their backend of choice supports stack probes, or contribute them if not.
The self-driving analogy is exaggerating, but your Rust == "debug build"(+static analyser) description is an under-statement. You can't have the same safety guarantees as you have in Rust if you merely use C++ &amp; static analysers. AFAIK, their reasoning must be global which leads to incompleteness, or you must re-invent the lifetime system Rust uses, use lifetime annotations &amp; impose a lot of restrictions (CppCoreGuidelines). It may end up as a clunky re-invention of Rust if it ever works as intended.
yes you can. the only thing C++ doesn't have is 'a way to annotate lifetimes', but we could make a convention for that which the analyser reads, e.g. a 'named' ref&lt;T,N&gt;. you can wrap your compile command if you like such that the analyser runs and prints 'error' if it fails. Just as rust has 'unsafe blocks' you could consider "the source files which do raw pointer manipulation' as your 'unsafe blocks' . Modern C++ has the tools to eliminate raw pointer usage in application code. (things like 'emplace_back()' really help) you're insane if you think 'safety' alone justifies asking people to bin and rewrite entire sourcebases. Now add a whole load of other issues like header files vs modules, better type inference etc, and maybe it's worth considering (e.g. even if C++ was magically safe, and rust was unsafe, **i'd be considering it for it's other syntactic cleanups**). The point is it is the *ensemble* of features that really counts, not safety alone.
Nice program, the resource usage seems to be high compared to other process viewers. Does it calculate both pages continuously? (seems to show cpu graph even for the time that has been spent looking at the process list)
No.
All the different process viewer give a different amount and I don't know, I just get information and display it. :)
&gt; Be open to considering alternatives. Oh, that I can agree with wholeheartedly. I'd just advise to perhaps try to get that across in a bit less aggressive way... 
No, it is not enough to annotate lifetimes and statically analyse. The analyser would also have to reject vast numbers of existing C++ language constructs (they could lead to unsafety), to the point of probably not being applicable to existing code-bases without major rewrites. Not just a few conventions here and there. And, is there any static analysis with similar guarantees to safe Rust, or are you just arguing that C++ COULD do it... some day... maybe... AFAIK, Rust it the only memory-safe no-GC systems lang, and it is probably going to stay that way for years. Safety is the main selling point of Rust, and it is what we are arguing here about; if we expanded the discussion to all features, this debate could go for years :-) 
The connection between conditional probabilities and sub-type constrainted is explained on the second page here about probabilistic existential paths: https://github.com/advancedresearch/path_semantics/blob/master/papers-wip/probabilistic-existential-paths.pdf I'll try explain it again, in another way: 1. You read the notation by interpreting it as the standard notation of probability... 2. ...but you forget that this is path semantics and functions have "semantics" When most people see a function, they just think "this is doing some computation". In path semantics you see "this function belongs to a higher order space, and it is connected in a such and such way to the other functions". That is why it's called "path semantics"! You can't just formalize how probability works in path semantics using probability theory. You must to define the probability of Boolean functions in the *semantics* of path semantics. It is not allowed to use the *semantics* of probability theory, because path semantics does not have these axioms. If I did that, I would have to take the axioms of probability and add them to path semantics. When you read `P(x)` in my papers, `P` is a *function* with path semantics because only then it can possibly be consistent. It just happens to be that `P`, when interpreted with path semantics, means *exactly* the same thing as in probability theory, so I use the same symbol. However, it does not mean the same when interpreted with standard notion of probability, because it lacks the strict semantics that path semantics has for functions. In path semantics, if you ever violate how functions work, semantically, even if you do this anywhere, the proofs are not truth-preserving. Do not confuse this with formalization of probability theory, because path semantics does not have a strict semantics for probability. It only depends on the strict semantics of functions to be consistent. In many theories of mathematics you are allowed to treat expressions as syntax that can be manipulated according to a set of axioms. It detaches the theory from how these objects would be represented in a given implementation. In path semantics, you have an additional constraint, that proofs are only valid if the results corresponds to how functions is interpreted semantically. It is just not syntax, there is an additional semantics underneath. Why? Because the whole notation is about how these functions are connected, and this does not work if there is no semantics. What I am doing here is: 1. Use a sub-set of path semantics to define some aspect of probability theory for path semantics 2. Proceed to use these definitions to define more aspects of probability theory for path semantics You can't define probabilistic paths before you got probabilistic existential paths defined. When you have lined up the semantics of path semantics with the semantics of standard notation of probability, only then you are allowed to start using probability theory to reason about path semantics. Of course the formalism is missing in these papers, because that's what these papers are starting to do! You can't have probability on the path semantical level without defining probabilistic existential paths and probabilistic paths. You can use expressions that perform some computation, but then it's just "algorithms" that doesn't mean anything in particular about path semantics itself. Path semantical level is higher order reasoning that requires theorem proving, it is about the space of interconnected functions "out there". It is not computable because it would require solving the Halting problem, so only a sub-set of it is computable. I believe you expect a higher degree of formalism than what is possible at this point. I've been working on nailing down the general ideas for a long time. Formalization of path semantics, even when aiming a sub-set that makes up a practical notation, is extremely hard. Just figuring out how to *measure* progress on this took a lot of work. This is not even aiming for a full formalization, because that's currently believed to be impossible. What I do is building it up piece by piece and explain what is happening and show that you can check for consistency without having a full formalization. Notice that the blog posts are not aimed at the working mathematician, but rather to explain to the general public what I'm working on. I do a lot of mistakes in the papers too, that's why they are still work-in-progress. I now know that function identity is a confusing point, so I'll write up more about it.
&gt; The analyser would also have to reject vast numbers of existing C++ language constructs (they could lead to unsafety) eh? surely the specific (mis)use is unsafe, not the inherent constructs??? C++ uses range based for loops these days, etc. maybe you're one of these people straw-manning the idea that "```this*``` is a raw pointer, so it's unsafe!!! , but it's only unsafe when you actually do arithmetic on it surely, it's just like a reference otherwise? there's 'shape analysis' for figuring out where your union variants are always accessed after a check, etc, and even recognising when common patterns of raw pointer use correspond to a known safe pattern. 
https://github.com/insanitybit/ScheduledExecutionService If you're interested. It doesn't work, but I'm getting there.
&gt; Warning: Because safety wrappers has some overhead (garbage collection), **I would not recommend to use it with caution**. You may want to rephrase this...
No problem! If you run into issues or have more questions, don't hesitate to ping me.
thanks i will!
This sounds awesome. Is it on github?
&gt; This is pretty overstated, when you don't have a big choice when it comes to legacy software, multiplatform support, GUI frameworks and so on. That you don't have a choice in some situations doesn't make C++ any better, same as driving a car without seat belts doesn't make it any safer just because it is the only one you have available right now. And as it is with safety of cars, you will have a conflict of interest between legacy car owners and new buyers and pedestrians.
How does one get involved in Redox? What's the best way to start?
&gt; In such case even safe languages can't help. It helps a lot when you are forced to mix memory management and business logic, which is the case with low-level code.
There is a lot of stuff you can do! Here is a good place to get started https://github.com/redox-os/redox/blob/master/CONTRIBUTING.md
Not yet, but it will be.
It can be useful to signal to whoever is building the code that a certain version of Rust is needed, since compilers are not forward compatible with code relying on a later version. Also, especially on nightly, there are performance differences between coded generated by different versions of rustc. In some scenarios, you don't want to deploy code until you've specifically tested the binaries built by that rustc for performance regressions.
&gt; First off, C/C++ isn't that bad. When a C/C++ codebase is well written and well tested including fuzzying, you can have a good confidence in it. Why are there so many CVE still being found in C/C++ codebases? And if the answer is "because they are not well written and tested" then the practical value of this argument is none.
Here's an example. Imaging a function that reads in a file, parses each line as an `f64`, and returns a `Vec&lt;f64&gt;`. If something goes wrong in the reading part, you'll get an [`io::Error`](https://doc.rust-lang.org/std/io/struct.Error.html). Reasons for failure could be that the file was not found, permission was denied, etc. (see [`io::ErrorKind`](https://doc.rust-lang.org/std/io/enum.ErrorKind.html)). If something goes wrong in the parsing part, you'll get a [`num::ParseFloatError`](https://doc.rust-lang.org/std/num/struct.ParseFloatError.html). Now the function needs to return a Result that accounts for both Error types that are possible. To do so, you can create a new Error type: enum MyError { io(io::Error), float(num::ParseFloatError), } And if you want you can create an alias for the Result: type MyResult&lt;T&gt; = Result&lt;T, MyError&gt;; And function signature would look something like: fn read_value_list(name: &amp;str) -&gt; MyResult&lt;Vec&lt;f64&gt;&gt; So, that's part of error-chain, satisfying the type system, but what really makes error-chain cool is being able to chain errors. For example, if I read a bunch of files in my program, getting an `io::ErrorKind::NotFound` is not very helpful because I don't know which file wasn't found. I might do something like: let values = read_value_list(name).chain_err(|| "failed to read value list file")?; This allows me to know how and where in my program the error occurred.
What was the bug tikki? I was stumbling around `conc` *attempting to help*. 
Great, personaly I used https://github.com/grafov/rust-playground , it would be good to cooperate these two projects at least for usage `extern crate ` in emacs playground.
Yes I see how the crate cache could be very useful for this. And the implied boilerplate gives almost a repl like experience - with a decent editor.
Ah, I totally missed this: https://github.com/rust-lang/rust/pull/42816 🎉
 [rust-lang/rust](https://github.com/rust-lang/rust) &gt; *Description*: A safe, concurrent, practical language. &gt; *Stars*: 22731 &gt; *Forks*: 4187 &gt; [Issues](https://github.com/rust-lang/rust/issues) | [Pull Requests](https://github.com/rust-lang/rust/pulls) *** ^(This is Earth radio, and now here's human music ♫) ^[Source](https://github.com/anaskhan96/github-stats-bot) ^| ^[PMme](https://np.reddit.com/message/compose?to=github-stats-bot)
You can use `typenum` or more specifically [`generic-array`](https://github.com/fizyk20/generic-array) crate. You will be able to use the following [lengths](http://paholg.com/typenum/typenum/consts/index.html) from the box: - Numbers 0 through 1024 - Powers of 2 below u64::MAX - Powers of 10 below u64::MAX 
&gt;&gt; It may end up as a clunky re-invention of Rust if it ever works as intended. you could equallly say 'Rust is an NIH reinvention of C++' . I like it for the syntactic tweaks &amp; better inference though. (again I wonder if the reverse inference could be retrofitted, starting out with fundep-esque assumptions..)
I think this is a good introductory article. The "mixing up order of operations" example is slightly misleading -- swapping the order of function calls doesn't fix the compile error as the text suggests. It will, I believe, once we have non-lexical lifetimes, but for now you also need to introduce an inner scope in order to terminate the borrow: fn send_file&lt;W: io::Write&gt;(mut out: W, mut file: fs::File) -&gt; io::Result&lt;()&gt; { { let mut header = HttpHeaderWriter::new(&amp;mut out)?; header.content_type("application/octet-stream")?; header.content_length(file.metadata()?.len())?; header.finish()?; } io::copy(&amp;mut file, &amp;mut out)?; Ok(()) } 
Ah okay
Having a `rust-toolchain` is probably the right decision if you want to make sure that everyone uses the same version. That said, since you mentioned Travis, most CI tools have their own ways to specify versions and I'd recommend to always test the specific version you're using as well as the current stable/beta/nightly ones to see find changes or bugs in the toolchain early.
That looks like a cool solution, but I do worry about the complexity. Are there any performance implications? Looking at the code it seems like there is a lot going on to determine the data layout. I also worry about missing out on some optimizations other people have written for things of type `&amp;[u8]`. The only real reason to muck about with fixed length arrays rather than use a vector is performance. Still using simulated type level naturals lets you avoid having to instantiate a big wad of code for each new differently sized array, which is cool. The other thing that worries me is that it seems like this would make your compile times grow pretty big pretty quickly if you use a lot of these, which makes me think a simple macro is more ergonomic, despite the extra code you have to write. Of course I would need to time it to be sure because the extra overhead from compiling each macro instantiation might also bite you.
All of them? Isn't it just a constraint satisfaction problem?
Great information density and thoroughness! 
Maybe add a question mark to the title? It isn't clear this is a question.
I could see these projects merging and a REST interface being put on top, something compatible with [Jupyter](http://jupyter-client.readthedocs.io/en/latest/kernels.html).
Your dedication to such a tedious but incredibly important issue over such a long period is outstanding. Thank you!
Thanks for the feedback! You are right, about that. I've verified it and updated the article with the solution you posted. Not sure how that mistake slipped my sight. Perhaps I should rewrite myself in Rust sometime. :)
C/C++/Rust are all used for a lot more than that. Games, desktop apps, mobile apps (sometimes), high performance stuff like databases, tools, etc.
For me Rust is a great replacement for both C and C++. I'm still going to use Haskell as my top priority when deciding what language to use. Of course, then real-wold demands take precedent. If the client can't use Haskell for some reason then I won't pick it. I also wouldn't pick it as an alt-js even though there is GHCJS. So rust is going to be my go to language if I'm working on something that I know needs good constant factors and low-overhead.
If I am not mistaken it should not have any additional runtime costs, but compile time cost is unfortunately quite real. (~~several seconds~~ approximately half of the second IIRC for release build)
Wow this is very cool, Thanks!
&gt; Why are there so many CVE still being found in C/C++ codebases? Define "so many" and also the ratio of the CVEs that are caused by bad memory management. AFAIK the situation has been getting somewhat better recently what with all the attention security bugs have been getting. (Note that I said somewhat, I'm not saying everything's fine and dandy in the C/C++ department.) &gt; And if the answer is "because they are not well written and tested" then the practical value of this argument is none. Well, is the practial value of telling people _"you should feel ashamed of writing C/C++ code, you better rewrite your stuff in Rust, you goddamn addict"_ any better? 
I used a similar setup for rust (+ycm and racer), but eventually intellij-rust got so good that I switched to that. I really enjoyed the raw editing speed I got with the vim-based setup, but in the end having an editor that actually understands the code and uses it to do cool stuff (like extract variable, rename function, find usages, etc) was too valuable to give up. I think I switched around when my program hit 3-4 crates, maybe 10kloc, and the rust plugin got dramatically better, at the same time. I still use vim to write single-file programs. Once the project gets bigger than a couple files, the IDE is better at helping me find and manage the code (for me, at least). My current rust project is about 50kloc and also has code in Java and Python checked into tree, and intellij is great for that sort of size. Fwiw when I use intellij my hands also rarely leave the keyboard, so it's possible with an IDE (although not as good for some scenarios like coding remotely or on an underpowered device - intellij is a hog). Once in a while I'll still want to use a vim macro, so I'll just pop open a terminal in intellij (plugin), open the file in vim, and do my edit. 
I use this https://docs.rs/arrayvec/ which is pretty good (careful not to blow up your stack though). Not sure if it supports serialize/dezerialize but in general it'll do whatever vec can do
&gt; Don’t use unsafe code &gt; Programs that are written in 100% Rust have no need for unsafe code. It might be tempting to get rid of that one branch and squeeze that tiny extra bit of performance out of your program, but ask yourself first, is it really worth it? I disagree with that. Rusts `unsafe` has it's time and place. Also, the first sentence might be true for 99% of the cases, but the 1% still exists. As this is an introductory article (and a good one!) I think I catch your drift ("don't reach for unsafe because it's so tempting", or "don't use it unless you know what you are doing"), but there's certainly quite some places where it is useful or even necessary.
What's the idiomatic way of optionally returning a collection of items? It could be returning an `Option&lt;Vec&lt;T&gt;&gt;` that's `None` when there are no items to return, or just returning an empty `Vec&lt;T&gt;` instead of `None` when there are no items to return. So which one is considered more idiomatic?
I think it depends what you want and why. Your question seems to involve both performance and ergonomics concerns, which can often be at odds with each other. Performance wise, the pass everything, magic free solution is probably the most predictable and so the easiest to make fast. If it's not ergonomic enough, that might be something that can be addressed separately (and it's often a matter of taste). I have the same arguments with people in Java land, where I favor massive constructor argument lists and they like dependency injection frameworks ... I can't really say they're wrong, I just hate unpredictable magic and they just like dependency injection :) 
Interesting. It would be helpful if you added a short summary of the additions you have mad with your fork over the original library. 
Not sure about which one is more idiomatic but I'll make one point: an empty Vec does not allocate, so you do not have to worry about the possible heap allocation even if you choose to return an empty Vec.
I've posted my reaction [on the issue tracker](https://github.com/PyO3/pyo3/issues/55) so it'll be easier for you to keep track of it as an actionable item. **EDIT:** My reaction being "Please add a short blurb about how this differs from rust-cpython".
Rust is used for anything where you want strong compile-time guarantees but can't or don't want to use Haskell. Here's a post I wrote 22 days ago with a detailed [list of reasons to want Rust](https://www.reddit.com/r/rust/comments/6kkgz7/according_to_stack_overflow_rust_is_the_most/djmvslo/). I've been replacing Python code with it. (Though, admittedly, parts of its ecosystem, such as GUI bindings, are prohibitively young at this point, so my GUI apps have to use rust-cpython and PyQt as "QML-alike for QWidget APIs" glue layer.)
That's actually the "reaction on [the issue tracker](https://github.com/PyO3/pyo3/issues/55)" that I mentioned. If you have a github account, you might want to pop in there to subscribe and/or add a thumbs up (+1) using the emoji/reactions menu.
But then the expressions would have different types, which doesn't work out either. Perhaps it could be called `match_vec` and the result could be a Vec with the values of all the expressions in the matching arms in it. Edit: On second thought, that would be a bit of syntax depending on a standard library type, which I don't like. Instead it could return a slice reference or maybe an iterator.
You can't edit titles on Reddit.
Just so you know some of those images aren't fit to the width of your text and so they overflow off to the right making it impossible to see without zooming out.
I'd imagine overwriting the data could lead to awkward lifetime issues if references to or slices of data are returned. There's no way to statically check that a reference will stay good after it's read. 
&gt; what your SearchPath describes `SearchPath` is just a list of the choices taken though the branches of the tree or graph to get to the currently active node from the starting point. I know I can make an owned copy of the path, but why do that if I don't have to? Some applications are just going to inspect it and immediately discard it. Those that don't can make their own copy. (At least that's how I'd like the API to work.) &gt; If you express an immutable borrow to a mutable borrowed thing, this mutability cannot be used until the immutable borrow is over As I read it, the immutable borrow *ought* to be over by the time the next iteration of the loop starts. This means either the borrow checker is smarter than me or I am smarter than it. I'm not sure which is true in this case.
&gt; note: values in a scope are dropped in the opposite order they are created It's a specified feature.
Yes I see that, but I took it as just an explanation not a "this is how we want it to be". I saw a thread on SO that was quite old saying there were discussions about changing it, but I couldn't find any updates or final decisions.
It's totally ok for this to only work with `Copy` data, single producer single consumer. There shouldn't be any references to the data returned from the queue, only copies. Also, the goal here is to write a safe API for unsafe operations, similar to `bounded_spsc_queue` or other similar ring buffer libraries, while statically verifying as much as possible.
reasons for fork: at this point base concepts compared to rust-cpython are totally different. 1. All objects are owned by *pyo3* library and all apis available with references. in *rust-cpython*, you own python objects, so any api call requires *Python* object as a safeguard. here is example of PyList api: *rust-cpython*: impl PyList { fn new(py: Python) -&gt; PyList {...} fn get_item(&amp;self, py: Python, index: isize) -&gt; PyObject {...} } *pyo3*: impl PyList { fn new(py: Python) -&gt; &amp;PyList {...} fn get_item(&amp;self, index: isize) -&gt; &amp;PyObject {...} } Because pyo3 allows only references to python object, all refs uses Gil lifetime. So `Python` object is not required, and it is safe to have: fn py&lt;'p&gt;(&amp;'p self) -&gt; Python&lt;'p&gt; {} 
**rust-cpython** custom class 1. *rust-cpython* uses whole new language based on macros 2. it is very hard to extend *py_class!* macros, for example *async/await* support. 3. generated functions are used for access to struct attributes. 4. To drop PyObject GIL is required. **pyo3** custom class 1. use *proc_macro* and *specialization* for class implementation (nightly is required) 2. *pyo3* does not modify rust struct. it is possible to define needed traits and make rust type compatible with python class without using *#[py::class]* macro. 3. class customization is based on *specialization* and traits and associated types. separate trait is defined for each type of customization (i.e. PyMappingProtocol, PyNumberProtocol). macro is used for simplicity, but it is possible to make rust type compatible with specific customization without using proc macros. 4. pyo3 does not inc ref for borrowed ptrs, PyObject does not need GIL to drop. 5. pyo3 at least 15% faster. 
motivation 1. seems rust-cpython is not maintained anymore. I tried to communicate with author, didn't get any response. I submitted multiple PR, none of them had any reaction. 2. I don't like class implementation based on macros. first of all, this is new language within rust, second it is very hard to extend and modify. 3. pyo3 is just rust. proc macros are used for automatically implement extra traits and wrapping code, so it possible to do the same manually. downsides: 1. pyo3 depends on nightly rust (proc_macro, specialization, const_fn). 2. proc_macro error reporting is bad.
A difficult to decipher image of benchmark results is [here](https://pbs.twimg.com/media/DFHhFajVYAA5FES.jpg:large). Maybe u/isislovecruft can speak more about it.
You might want to put this explanation in the README. Right now you only have one line at the very bottom saying that it is a fork of rust-cpython.
[removed]
I am not sure readme is right place for it. I added link to issue to readme.
finally made a release of python binding library pyo3 https://github.com/PyO3/pyo3
Forking is ok, but with the current visibility for the explanation (last line in your README), I would call that work-stealing. (Of course this is a subjective question, and surely many people will disagree.)
This isn't really nice. JS and PHP won't disappear overnight and still have uses. There's no need to demean other languages, and Rust certainly isn't a silver bullet either. To answer the topic at hand: Yes, Rust sees some production use, mostly in microservices/cli tools or libraries. GUI/Frontend stuff is not there yet.
Who said anything about disappearing?
Please don't post this type of programming language zealotry here.
not much left of rust-cpython, core concepts are totally different. in any case I added link to motivation discussion.
Local variables have always^1 been dropped in reverse order of declaration. For struct members, for a while it was kind of "the compiler is the spec" hand-waving, but the drop order has recently been [officially stabilized](https://github.com/rust-lang/rfcs/blob/master/text/1857-stabilize-drop-order.md). ^(1: I checked the 1.0.0 version of the book.)
`Rust` may be used in the same areas as `C/C++`. `PHP` and `Javascript` interpretator, web browser all this stuff written on `C/C++`. Also of course games and web apps. If you need performance then big chance that game will be on `C/C++`, the same story for any part of sites with high load.
You'd have to be incredibly sensitive to the point of absolute absurdity if you think that's "programming language zealotry".
&gt; Define "so many" and also the ratio of the CVEs that are caused by bad memory management. https://www.debian.org/security/ Nearly one a day and looking at last week, 20-30% points to memory safety straight away. "Various memory handling problems and cases of missing or incomplete input sanitising may result in denial of service, memory disclosure or the execution of arbitrary code" "a buffer overflow in spice, a SPICE protocol client and server library which may result in memory disclosure, denial of service and potentially the execution of arbitrary code" (I don't know what SPICE is, but it doesn't necessarily make it any less of an issue for me). &gt; AFAIK the situation has been getting somewhat better recently what with all the attention security bugs have been getting. I haven't noticed any global slowdown in CVE discovery, and attention alone won't fix much. It seems that solutions available for C/C++ developers require lot of effort to implement and help only a little if at all. Nginx keeps getting buffer overflows several times a year despite being focused on security and applying numerous best practices and tools available. Any new project written in C is guaranteed to have its share of well known problems. Any actively developed existing project too. Those can be mitigated to a degree by throwing people at them, but this really doesn't scale well. I don't see this as a solved problem or even not on track to being solved. &gt; Well, is the practial value of telling people "you should feel ashamed of writing C/C++ code, you better rewrite your stuff in Rust, you goddamn addict" any better? I see using memory unsafe languages the same way I see writing sql queries by combining string coming from users. It is bad practice, it is guaranteed to cause problems, no, you can not do it safely, no, the "magic tools to detect it" don't work reliably (they only catch some errors, at the massive cost of attention they require, not to mention the fact that 99% of developers out there doesn't even bother to read default warnings), and the solution was to give people libraries and frameworks (in this case: languages) that do the right things by default without developers doing any work at all and ideally without even knowing about the problem. Yes, it did require throwing all the string concatenating code away, or isolating it to minimize the risk. You may need to use some legacy code that does that, but you should be aware of the risk, communicate it to everyone involved and you should be ashamed if you are going to design any new code that way. "I can can do it right" is wrong attitude even in those extremely rare cases when you indeed can (but anyone else without 10 years of experience can not). Do we need to support existing C code? Yes, there is a lot of it and it won' t be rewritten in a week. Should we make an attempt to make this code safer? Yes. But we should not claim that the problems it has can be solved easily.
Oh, that's really nice. Any limitations to it? Does it handle threading and everything?
Hi, just got around to updating Atom so I could check it out. What do I do after I install it? It doesn't seem to have made Atom behave any differently, from what I can tell. Thanks!
I am not aware of any limitations. it should be possible to implement extension of any complexity. I implemented asyncio event loop with it. pyo3 + tokio-rs https://github.com/PyO3/tokio
Rust is used for writing system software. Its in the same space as C and C++. Whereas PHP and JS are used for writing application software. Application software are built on top of system software. For example, the PHP interpreter is written in C, JS V8 in C++. So, if you plan to create your own interpreter or engine, its probably best to use programming languages that are used to write system software, like Rust. Now, one can also build high level stuff like a CMS, in Rust, but IMO, its cumbersome. You'd probably end up abstracting it a lot to make the experience more pleasant, and end up with a runtime interpreter similar to PHP. So your better off using higher level languages in the first place. The same can be said for JS and PHP, one can write a binary format decoder in JS, but it would probably be slow and use a lot more memory.
This doesn't really answer your question, but it's relevant to audio ring buffers and speed. I noticed bounded_spsc_queue can only push and pop a single element at a time. Why call push 44100 times when you can call push(44100 items)?
Yeah, agree. Unsafe should be approached with reasonable caution - test the hell out of that code, document it, etc. But don't be so afraid to use it that you're willing to throw away performance. Checked indexing can have a real cost if you're doing something performance critical.
i wouldnt say games but other stuff is definitely there. i think the future of rust may be backend of web. it gives you confidence in your code and its high level enough. i wrote in both flask, rocket and golang and it was easier for me to do web with rocket (small project)
can I ask why haskell over rust? 
Fillipo is calling Rust using a fixed size stack like GoASM. He is using this access the best in class performance of dalek 
It doesn't seem to me like it would matter if you promote the newest data (overwrite) or promote the oldest data (ignore). Especially if you're dealing with `Copy` data. You might need to think a bit about where you want the read pointer to go after overwriting data, but that's just a matter of how you want the buffer to behave - we're talking about plain old data here so there shouldn't be any soundness issues.
Rules: &gt; When discussing other programming languages, maintain civility and respectfulness. &gt; We strive to treat others with respect, patience, kindness, and empathy. 
Well, we usually only want a certain "block size" at once. So if the audio input device is passing along 64 samples at once, we have to take those immediately otherwise they're gone as soon as the audio callback ends. And, actually, I usually use a fixed-size array of whatever the block size is as my element, so it's more like a `Vec&lt;[f32; 64]&gt;` for a block size of 64 samples.
Sweet, will do.
When I program in Haskell I can use much richer abstractions and I'm free from thinking about machine representation. Haskell allows you to program very close to the problem domain and easily express algorithms. Plus, GHC has many extensions that are useful in practice. I'm more productive in Haskell than any other language I've learned. Your mileage may vary. 
Yeah, that's what I (intuitively) thought. But then why wouldn't the whole buffer just be shared? I feel like the Producer modifying the read-index would be unsound, otherwise why split the Producer and Consumer?
thanks for reply, I actually have lot of questions about haskell but Ill ask it on haskell subreddit :)
I'm happy to hear you have questions about it. I think everyone should at least try it for a while. There are a lot of programming languages these days. It can be hard to decide what to learn and what to use. I find getting first hand experience is helpful.
Thanks for the work on mdBook! It has been very helpful.
I think haskell is great because it introduces new ideas, just like rust. I tried to use it 3 years ago but had problems with understanding some high level ideas. I think rust and haskell are very similar language and both communities should cooperate and getting inspired by each other :)
You need the language-rust package for anything to happen. And it only happens when you open a rust file. You can also get the atom-ide-ui package for more goodies that work with the package. That package will be bundled in atom eventually and is the ui piece to a dozen ide packages for atom that are being constructed. Stuff ide-rust gives you: - Autocomplete - Build warnings/errors from rustc (requires linter or atom-ide-ui package) - Ctrl-click go-to-def (requires atom-ide-ui) - Format via rustfmt (requires atom-ide-ui) In another week or so I'll be testing on windows and writing a readme to explain this stuff. 
&gt; I think rust and haskell are very similar language and both communities should cooperate and getting inspired by each other :) Haskell is getting an extension to enable linear typing soon :)
&gt; atom-ide-ui Ah, ok, that's what I was missing. Thanks! I'll check it out a little more in the coming days.
How PyPy compatible is it?
It is not compatible with PyPy. Purpose of this library is different. I see it aa a glue between rust application and python and not just better c extension language
It is not only about performance. It is more about safety, which is much stronger in rust than in python
Oh well. I don't understand the purpose of making speed comparisons without bothering with the fastest Python implementation around. Shout-out to [Snaek](https://github.com/mitsuhiko/snaek) for handling this.
With snaek you have to design library for external use, you have to manage ownership, etc. but python provides strong ownership as well, it can simplify implementation.
The one year where I can't make it to DEF CON, this happens. Enjoy your time, though!
This further cements the hegemony of CPython.
But that is fine, unless you have escape route. Python is perfect as a glue language. Without escape route, python just be get pushed to data science, out of other areas.
I've somehow never run into this in Ruby, but am interested in what you've described. Could you give a sample use case and/or example code? --- Nevermind, I hit up Google and have a decent idea of what it does. Curious how I might wind up using it, though.
Clear your cache or something? It's sending me a Let's Encrypt cert for internals.rust-lang.org, valid until 10/20/17.
Python is great, gluing oneself to CPython internals is kinda gross. Making a clean `clib` that can be loaded by the `cffi` doesn't tie one to a specific implementation of Python. That said, if one has to muck with with CPython internals, let it be Rust!
Regarding the SO question, you are probably referring to [this](https://stackoverflow.com/questions/41053542/forcing-the-order-in-which-struct-fields-are-dropped/41056727#41056727) one. I updated it some weeks ago when the drop order RFC was merged ;)
Disagreed. Your first reply implied that programming in Rust, by itself, made one superior to those that programmed in other languages, specifically JS and PHP. That, by itself, is the stuff of flame wars, and not very welcoming of other languages...Which we define as zealotry here.
&gt; I'd like to rebuild rustc on my browser. QUOTE OF THE YEAR
[fite me](https://github.com/myrrlyn/wyzyrdry-rs/blob/master/src/alert.rs)
I use Rust when Python is too slow for the task, like processing large files.
In general I wouldn't expect Go to be much slower than Rust (if at all) for mathy things like this, since the GC or dynamic dispatch shouldn't be playing much of a part. Bignums could be involved and provide a performance difference, but IIRC that's not the case here. In this case, Rust is faster because Rust has u128! ([source](https://twitter.com/gtank__/status/887742755170009088)) It will be interesting to see if Go gets something similar in the future.
[easycurses](https://github.com/Lokathor/easycurses-rs) A wrapper for pancurses that makes curses actually safe and easy to use. Doesn't support all possible curses features, but hits all the major points you're probably looking for. Works on Windows and Unix. Edit: pancurses got an update that let me fill in a missing operation, so now it feels complete enough to publish. It's up on [crates.io](https://crates.io/crates/easycurses) for everyone to try out.
I'll be there. Talking at crypto village on Sunday. I'll generally be going to a lot of the village talks. Come hang out. I look like my twitter profile. @zmanian
Also embedded, also defense contractor. I'm attempting to spread the good word, but I don't expect anything serious to happen this decade, at minimum.
It might be cool idea to try to emulate [perl startup options](http://perldoc.perl.org/perlrun.html) like `-n` and `-p` which implicitly iterate stdin, and `-e` which allows entering the program from command line
&gt;minus the horrible deaths [Concurrency bugs have killed people](https://en.wikipedia.org/wiki/Therac-25)
**Therac-25** The Therac-25 was a radiation therapy machine produced by Atomic Energy of Canada Limited (AECL) in 1982 after the Therac-6 and Therac-20 units (the earlier units had been produced in partnership with CGR of France). It was involved in at least six accidents between 1985 and 1987, in which patients were given massive overdoses of radiation. Because of concurrent programming errors, it sometimes gave its patients radiation doses that were hundreds of times greater than normal, resulting in death or serious injury. These accidents highlighted the dangers of software control of safety-critical systems, and they have become a standard case study in health informatics and software engineering. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
Well I'm new to Rust. So to learn I'm making a Chip8 emulator. Definitely seeing the advantages over C++. 
Rewriting a small program that notifies me when TV shows are out from go to rust.
Updating my crate that support CORS for Rocket with new features: https://github.com/lawliet89/rocket_cors
OP, what terminal emulator do you use on osx? And how do you get those flat style tabs? It looks really pretty and I'm a pleb still using the built in terminal.
Yes, there was a discussion about changing it, but the community elected to keep it the way it is, and stabilize that. https://github.com/rust-lang/rfcs/pull/1857 So this isn't going to change. 
Rust also benefits from having a compiler with more aggressive optimisations, which can make a large difference for some code (I do not know if this is such code).
I can personally confirm that Piston even runs on a pi. Conrod is built on top, if you want that.
If you concern is speed, at least for the interface level, a [directly-compiled extension is going to be faster than a ctypes-like interface for most use cases](https://stackoverflow.com/questions/8067171/ctypes-vs-c-extension).
Overoptimizing `rdedup` so it's the faster deduplication software ever ;) in https://github.com/gilbertchen/benchmarking
I am working on [bui-backend](https://github.com/astraw/bui-backend/tree/master/bui-backend) which isn't necessarily what you'd call a "Desktop app", but lets you write apps that run on your Desktop. I am now polishing it for initial release.
Rust compiles at the crate level, so your library modules will only exist once, and each binary will link against the entire library. You may be able to use link-time optimization (probably `cargo build --release --bin BinaryC`) to strip dead code at the last stage. &gt;I'd prefer to avoid creating separate crates, because it looks like a lot of overhead. It really isn't. The workspaces comment already left will handle that nicely, and then just have BinaryC not use `extern crate my_library_utils;` to avoid pulling in the utility crate subproject.
&gt; i wouldnt say games but other stuff is definitely there. You wouldn't say C++ for games? Really?
In short: * [JavaScriptCore bindings](https://github.com/endoli/javascriptcore.rs) * [LLDB bindings](https://github.com/endoli/lldb.rs) I'm traveling to the US this week to visit family and friends for much of the next month. In the next 30 hours, I've got a 7 hour flight and then a 13.5 hour flight. Last week, I continued working on my JavaScriptCore bindings, but I haven't pushed any results from that yet. I'm hoping to get some more of that to a happy place where I want to push it. I've also been making more progress on my LLDB bindings. I pushed a new release of the lldb-sys crate, but am working on a few more things before I update the lldb crate itself. I'm looking at how I can contribute to improving the Rust documentation, so as a first attempt, I submitted a [pull request](https://github.com/rust-lang/rust/pull/43428) to fix a variety of typos. I keep looking at and thinking about Rust bindings for the [USD library](https://graphics.pixar.com/usd/docs/index.html) from Pixar. In service to thinking about that, I got a working build locally and started upstreaming a bunch of build and other minor improvements. This is C++ code with a Python binding (using Boost Python), so I'm not sure yet which approach I would take for doing Rust bindings.
curve25519-dalek is actually CT code, so it suspect it tries to _avoid_ optimizations in some places. But yes, the optimizer may have helped.
&gt; Your question seems to involve both performance and ergonomics concerns, which can often be at odds with each other. .. the hope is that new language tools can achieve both
What does CT mean? Compile-time?
Oh cool. Many thanks all.
CT = constant time. Meaning the time the code takes to execute should not depend on the input in order to avoid timing based attacks
I suspect GP means Rust isn't really ready for games yet. I disagree, because what I'm looking for in a game these days is not what a "AAA" engine/studio will produce, but I see where GP appears to be coming from; there's decades of game tech in C++ that will take years for the Rust game ecosystem to reproduce.
Oh, makes sense, thanks!
How desired is aggressive optimizations in the domain of crypto? I am far away of knowing what i am talking about crypto – and maybe i am a victim of dunning kruger here – but those kind of things sounds like a huge potential to destroy some properties like constant time functions for mitigating side channel attacks etc. and i expect there to be a hundred things alike i didn't even know about yet. I would be a little bit scared by aggressive optimizations. Nice to be faster but not for the exchange of security – you know. 
Oh thanks! It's nice of you. :)
&gt; Very good explanation for these type theory terms. &gt; &gt; Ths gist is in this paragraph: This phrasing seems to imply that you meant to include a link to the full version, but I don't see one in your post and, since this is a text post, the title just links back to this page.
My bad, first time that I post to reddit. I added the link to the original article.
I do like this idea - it's not difficult to implement. So one could do `runner -e "0.25.sin()"` (so _expressions_, implicitly printed). A Rust-like option would be `-i` where the expression is an iterator, so the expression `"hello dolly".split_whitespace()` would print "hello" and "dolly". (a debug print would be more general here).
&gt; Nginx keeps getting buffer overflows several times a year despite being focused on security and applying numerous best practices and tools available. Source? Looking at [CVE details](http://www.cvedetails.com/product/17956/Nginx-Nginx.html?vendor_id=10048), nginx seems to be doing pretty well. Yeah, there's a bunch of DoS bugs, but that will happen with Rust too! In fact, vast majority of C buffer and integer overflow problems will not vanish in Rust, but will simply turn into DoS bugs. &gt; Any new project written in C is guaranteed to have its share of well known problems. Yes, I agree and right now I'd discourage most people from writing a network-facing application in C/C++ and would not do that myself given choice. Another point is that not every C/C++ code is network-facing, not everyone is writing nginx, you know? &gt; I see using memory unsafe languages the same way I see writing sql queries by combining string coming from users. Yeah, perhaps you could see it that way, but there are many good industry-proven solutions for writing db queries in a safe way - it's pretty straight-forward to get rid of the string concats and it's been so for decade(s) now. The situation with C/C++ code is _far_ more complex. _"Hurr durr, rewrite it in Rust!"_ is not helping. 
I'm not sure I'll ever be able to remember which one is contravariance and which one is covariance, but I do finally understand, in practical terms, what "we allow function types to be covariant in their return type and contravariant in their argument type" means.
CFFI is compiled by default. Your link is for ctypes which is a different beast.
Damn, I need to fix that! Sorry for the disturbance. EDIT: fixed! Thanks for reporting!
&gt; I disagree, because what I'm looking for in a game these days is not what a "AAA" engine/studio will produce, but I see where GP appears to be coming from; there's decades of game tech in C++ that will take years for the Rust game ecosystem to reproduce. But that's a traditionalist argument that you can hear any time something has a strengthened foothold. Yes, Rust might not succeed at breaking that habit, but if not, something else will. It's also not necessary to reproduce, stuff is rebuilt all the time and one of the rebuilds might be in Rust. It's not a very useful argument: what's missing in Rust to replace C++ in gaming? Lots of gaming is not AAA anymore and even large engines now support C#. Where does Rust land in this? 
Thanks for your feedback! Interesting setup even though I truly think you can have large projects within vim or emacs. But, yeah, if you feel it’s the best setup for you, then it’s the best one! I’ll give a try to IntellijRust so that I don’t die ignorant. :) I think it could be a good think to gather our setups in a post on This Week In Rust or that kind of stuff. :)
Reminds me of my attempt to learn Scala many moons ago. Out of curiosity, can the same thing be said for Rust lifetimes? I recall that although lifetimes are covariant in return positions, they are invariant in argument positions.
iTerm2! cmd+t opens up a new tab, cmd-d splits the screen with a vertical line and cmd-D splits with a horizontal line. It’s pretty a decent terminal, even though I prefer my linux setup (termite, awesome terminal was well!).
I tried to put together an example for stylish https://github.com/Thinkofname/stylish_example [Preview here](http://i.imgur.com/tQ3lXvR.png). As it turns out stylish isn't the greatest to use when you aren't using a scripting language I like use for my game. I'm not sure how I would fix that however. A part from that I'm continuing work on my game. I have a post [here](https://thinkofname.github.io/2017/07/24/july-change-log.html) or just a screenshot of the game [here](http://i.imgur.com/RbFQFwO.png). Not going to post that one to /r/rust_gamedev as its starting to feel spammy and this one doesn't even include any rust code in the post.
I'm spending a few hours on improving string internment/sharing in my aterm crate. That should improve the performance of the parser (less malloc) and of any user of the crate. Of course I'm doing this because I'm using the crate myself for an interpreter that I've written, which I'm currently optimising. Once I have that interpreter optimised to the max, I'll write a blog post about the entire experience. In the mean time, does anyone need yak hair? I have some left over \^\^'
Like I said, dalek is pretty careful in ensuring that the generated assembly is constant time.
Well, "co" (e.g. as in "coop(eration)") can be seen as the antonym of "contra". So "co" varies "in the same direction", whereas "contra" in the "opposite direction".
Rust only has variance from lifetimes, *and* it has contravariance for argument positions. There may have been a myth perpetrated by /u/Gankro that variance was dead or dying, but removing it isn't backwards-compatible so I don't see that happening any time soon.
That's awesome. Thanks :D
Btw. why would it be desirable to have less variance than naturally allowed by the types? Isn't that more restricting? Or is it about the complexity of the implementation?
If you think about it, it doesn't really make sense the other way around. The objects on the stack may depend on each other, like this: let mut file = File::open(path); let mut writer = BufWriter::new(&amp;mut file); You can't really change that order. And having the order depend on some kind of dependency graph would be confusing (much more than dropping them as LIFO).
So your workflow contains using `Ctrl-z` to get out of neovim to a shell. You know that neovim has `:terminal`? I find it extremely useful for a similar workflow. If you're interested, here are 2 things I find invaluable for that: 1) Check out neovim-remote (nvr) to open files in your nvim instance 2) I have my statusline show my window number, and then jump around windows with `\2` and the like. This also lets me exit terminal insert mode by just jumping to that same window: let i = 1 while i &lt;= 9 execute 'nnoremap &lt;Leader&gt;' . i . ' :' . i . 'wincmd w&lt;CR&gt;' execute 'tnoremap &lt;Leader&gt;' . i . ' &lt;C-\&gt;&lt;C-n&gt;' .':' . i . 'wincmd w&lt;CR&gt;' let i = i + 1 endwhile 
Woh, that seems incredibly useful! I didn’t know about `:terminal`! Is that something only neovim has? Thanks, I’ll try that right away!
Do you know how stylish can be used together with glium? It looks really great and I might want to use it in my game.
Trust me, the construction of words is the last thing I'd have a problem with. I can just never get my mind to retain what it means to go in the same or opposite "directions" in this context. At best, it makes sense when I'm looking at it, but then it doesn't stick. The example-based explanation works because I can remember the concrete examples of what is and isn't sound and why. This is just a little too abstract and theoretical for my mind to accept it as something relevant enough to spend memory space on.
I don't think you'll be able to use the webrender backend due to the fact glium wouldn't know what changes it made to the opengl state. In theory you could write your own renderer for it though.
I used to have that same problem of remembering which one was which. That's why this explanation that puts the emphasis on the the &lt;: helps me. *Contra*variance means that the relationship A &lt;: B is *reversed*. That just means that A and B flip sides wrt &lt;:, therefore: A &lt;: B implies (**B** -&gt; T) **&lt;:** (**A** -&gt; T) *Co*variance means that the relationship is *maintained*. That just means that A and B maintain their original sides wrt &lt;:, therefore: A &lt;: B implies (T -&gt; **A**) **&lt;:** (T -&gt; **B**)
Hey everyone, Last week I published a new version of Juggernaut: http://juggernaut.rs and spent two days to compile a Neural Network demo to Emscripten and call it on the web browser. It did work and was awesome but what I really wanted to do was running the training session on a different thread (web worker). So, I spent two days to create a Web Worker but no luck yet. I posted the question here: https://stackoverflow.com/q/45248078/375966 This week, I'm going to spend more time on the web worker.
Neovim has had this for quite some time and it's pretty mature. Vim is in the process of adding support for it, but as far as I can tell, it's not yet done. If you get into it, be sure to check out https://github.com/neovim/neovim/wiki/Related-projects for some related plugins, maybe neoterm or the like (but really check out neovim-remote!)
pnkfelix gave a talk about how subtyping and variance work in Rust which helped me understand it: https://www.youtube.com/watch?v=fI4RG_uq-WU
Again, I get that. The problem is getting my brain to retain the meaning of "the relationship" and that symbol. Maybe, after I've slept, I'll try saving a copy of that page to disc and rewriting it to use ⊆ and ⊇ set theory notation. That'd probably help it to stick a bit better.
Ah yes, I assumed that snaek used ctypes based on its description and didn't realize that CFFI is, as you say, a different beast. Interesting. But to your original point about not comparing to PyPy -- there are a lot of big and complex codebases which do not run in PyPy. For example, scipy is the tip of the scientific-computing-in-python iceberg, and even that doesn't run in PyPy. So, I, for one, am interested in speed comparisons even if PyPy is left out.
&gt; nginx seems to be doing pretty well. Bugfix: in memory allocation error handling. Bugfix: when using HTTP/2 and the "limit_req" or "auth_request" directives client request body might be corrupted Security: use-after-free condition might occur during CNAME response processing [http://nginx.org/en/CHANGES] There is probably more, not all bugfixes describe their cause there. I do agree that it is doing pretty well comparing to other C codebases, but still it is a source of errors and vulnerabilities that a safer language would prevent. &gt; In fact, vast majority of C buffer and integer overflow problems will not vanish in Rust, but will simply turn into DoS bugs. Which is a massive step forward. I would argue that having usable stdlib that doesn't work against you would limit amount of problems, but even just turning existing problems into panics or crashes would help tremendously. &gt; there are many good industry-proven solutions for writing db queries in a safe way That's my point - solution for this problem was to give people tools that don't have problems, aiding them in mitigating issues without changing the way they work was proven not to work well despite numerous attempts. &gt; Yes, I agree and right now I'd discourage most people from writing a network-facing application in C/C++ and would not do that myself given choice. This is exactly the message that proponents of safer languages are trying to send, so the only question we may disagree about is what to do with legacy code. I'd like Nginx authors for example to start Nginx 2 in safer language because they have the expertise to do so, or to start applying it in small doses to what they have now. It is somehow happening (people use Rust already to write Nginx modules), and another webserver will be written eventually, but original authors involvement would accelerate the efforts. Letting them know that there is desire and market need may have a positive effect.
Noticed some warnings in terminal. &gt;./process_viewer &gt;(.:20553): Gtk-WARNING **: Allocating size to GtkWindow 0x7f7859572270 without calling gtk_widget_get_preferred_width/height(). How does the code know the size to allocate? &gt;(.:20553): Gtk-WARNING **: Allocating size to GtkWindow 0x7f7859572270 without calling gtk_widget_get_preferred_width/height(). How does the code know the size to allocate?
Just some warnings. If I have to fix them some day maybe I'll. :)
Considering the downvotes I got, I am probably too strict on what is ok and what is not in term of forking. Anyway, keep up the good work... A solid, maintained crate to get python bindings is certainly a good thing!
Well, the more abstract definitions are hard to remember, yes, but in Rust, since it's about lifetimes, you can think of the long &lt;-&gt; short directions. So, a value of `&amp;'static X` can be used as `&amp;'a X`, and that's one direction. A value of `Vec&lt;&amp;'static X&gt;` can be used as `Vec&lt;&amp;'a X&gt;`, so `Vec&lt;T&gt;` is *co*variant over `T`, because the *direction* is the same (`'static` -&gt; `'a`) as whatever `T` is. The same is true for *basically everything*, including function return types, (except `&amp;mut T`, which doesn't have variance, for soundness, and neither does anything using interior mutability, like `Cell` and `RefCell`). A value of type `fn(&amp;'a X)` can be used as one of `fn(&amp;'static X)`, because if you can pass in `&amp;'a X`, you can pass in `&amp;'static X`. This is *the only place* in the Rust typesystem, where the "direction" is opposite (from `'a` to `'static`), which makes `fn(T)` *contra*variant over `T`.
I never use subtyping notation because it means nothing to me. It's easier to think in concrete "any value of type A can be used where a value of type B is expected" terms.
How do I access the contents of my Enum without moving it? I've got an Emum: #[derive(Debug, Clone)] enum Cell { Operator(String), Variable(String), Value(f64) } I assign values to it: let cell_item = Cell::Operator(s); let cell_item = Cell::Variable(s); let cell_item = Cell::Value(val.unwrap()); I then push the Cell into a Vector for further processing: vec.push(cell_item); I can access the Enum value: if let Cell::Operator(a) = op { ... } if let Cell::Value(b) = left { ... } if let Cell::Value(c) = right { ... } However, this access moves the Cell, meaning that I cannot easily use the Cell later. What I've considered is implementing my own Copy trait. Easy enough, I suppose, but I was wondering if there was a better way. Is there? 
Praying that generators are merged into nightly :-) I badly want to try out `#[async]/await!()`, but needing to BYOR (Bring Your Own Rustc) is a hurdle too high. As soon as it's in something `rustup` will install I've got some code to port to it, and I'm pretty excited.
Not sure why you had claim it was theft. That’s open source, forking is integral part of it.
 - first: use case for global and thread-local variables is not same you use global variables when you need only one copy of some variable/resource/connection per program ( or only one can exist because some arhitectual limitation) and you either lock that global resource/variable, or make sure its thread safe, or in case of Rust do some compile time analysis, to make sure only one thread is using it at a time thread local variables/resources (not same as session variables because those need special handling/those are 3rd type) on the other hand you use when something is not thread safe, AND is to expensive to create fresh each time you need it AND it is bottleneck for you (locking one global copy would make your program scale badly/be serialized/unable to use multiple cores) example of potential thread-local variable/resource: XML parsing component that is not thread safe and is heavily used, and is costly to initialize, you would initialize one for each thread receiving/parsing XML messages 
Add the `ref` keyword as part of the pattern to borrow the value by reference, e.g. if let Cell::Operator(ref a) = cell_item { println!("{:?}", a); } 
Oh cool, I've been using this recently! It's been a life saver :)
The only reason i remember it is that i grew up as a Java programmer, and in 1.5, a big deal was adding "covariant return types", which means that when you override a method in a subclass, you can give it a narrower return type. So, i remember that as you move from type to subtype, a covariant type also moves from type to subtype. The logical counterpart would be contravariant parameter types, but helpfully, Java never got those, so there's nothing to get confused about!
&gt; first: use case for global and thread-local variables is not same certainly different, but I get the impression that TLS is an 'intuitive workaround' for programs that may have been written in a non-threadsafe way (i.e. relying on 'current variables' within a system). I posted another thread in the forum on followup questions about 'module level' variables; i was pointed toward the 'piston 'current'' library which does something to assist that. classic gl is a good example of that, all sorts of global state. vulkan is completely different to allow multithreaded rendering, but imagine if there was a middle ground of an 'easier-to-use' api that still did allow 'per-thread' states whilst being a little less heavy on manual setup
also why would you need global PTR (pointer to global variables)? base global PTR should be CONSTANT during execution of program so you can just keep offsets in instructions accessing global variables (and table pointing to such instructions), and during program startup when you are initializing global area patch all instructions by adding global PTR base to their offsets ( in similar way that dynamic lib/method address patching is done currently)
[process-viewer](https://github.com/guillaumegomez/process-viewer)
&gt; Which is a massive step forward. Huh? What kind of vulnerabilities are you mainly concerned with then? Code execution? Can you prove that the bugs are exploitable for RCE and that this has implications for the industry in terms of costs? Can you prove that these costs were significantly higher than the ones incurred by the DoS bugs, including factoring in the observation that the DoS bugs are a lot more frequent? Unless the answer is yes in both cases, the nginx team has basically zero incentive to rewrite nginx in Rust. &gt; This is exactly the message that proponents of safer languages are trying to send, so the only question we may disagree about is what to do with legacy code. No, you misunderstood. I'd discourage most people from writing a _new_ web-facing service in C/C++, unless they know what they're doing. But I don't see why people who already are writing succesful and secure software in C/C++ shouldn't continue to do so, since obvously they know what they are doing. I woudn't trust a web service someone has written in C/C++, I'd be more comfortable with a Rust one, sure, but then again I am more comfortable still with nginx, because it's industry-proven. A random service written in Rust today is probably less secure than nginx. &gt; I'd like Nginx authors for example to start Nginx 2 in safer language because they have the expertise to do so, or to start applying it in small doses to what they have now. Safer language such as what? All the GC'ed language are basically not an option and I'm not sure Rust is stable enough to replace C for nginx, I don't think it is. Besides, since servers like nginx are tightly integrated with platform specifics on each platform, stuff like Tokio is basically out of question for them. They'd have to write their own support on each platform (just like they do in C), which would entail unsafe code. TL;DR switching to Rust will probably be an option for nginx in a couple of years (when Rust stabilizes), but it's not / won't be anywhere near as clear cut and pressing as the RESF thinks it is. Do you really want and expect them to rewrite the whole well-tested and industry-proven codebase just to prevent some 2 RCE bugs in 5 years or so (?) _and_ risk the rewrite introducing new bugs (which would probably happen)? I wouldn't even be very surprised if it turned out the DoS bugs are actually more costly for the industry in the end... 
Writing a quick start template for the adafruit m0 based on cortex-m-quickstart.
Author here. The producer/consumer split is just to make threading semantics simple and clear. E.g. two objects that are each `Send` is easier to deal with than a single object that is `Sync`. It also leads to a much less complicated API: since only one thread is allowed to be pushing (SP), and only one thread is allowed to be reading (SC), if they both shared the same `Sync` object you'd have to do some weird API gymnastics like "marking" each side with their intended action, disallowing access to the opposing API calls, etc. It'd be weird. The Producer/Consumer split just makes it simple and unambiguous. Technically, they both share the same ring anyway so the split is just to make Rust memory model happy. As to the original question, I haven't thought deeply about it but I don't think there is any reason that it couldn't be implemented as you suggest: keeping the most recent data instead of throwing away new. It's just chasing a pointer around the ring, so the responsibility for bumping pointers just needs to be adjusted a bit.
&gt; I don't remember the version number, but I believe it was 1.18.0 You *sure* you haven't just been repeatedly installing 1.14.0? If you say you've removed it and cleared the entries from `PATH`, and you're *still* getting 1.14.0, then you've clearly got it installed *somewhere*. If you've got a MinGW shell lying around, try running `which rustc` to find out where it's hiding. If you don't, but you *do* have Python, try [this Python version of which](https://gist.github.com/DanielKeep/fb6274988bfc33cef69220a4977c2dbe). Once you've located and nuked the errant install, you might want to try managing your Rust install with [rustup](http://rustup.rs/).
FWIW there's no reason bounded_spsc_queue can't have "bulk" push and pops... I was just too lazy to write them at the time. There are some interesting edge-cases to handle, like trying to push more than the size of the queue... do you reject? Block until the entire push finishes? Push all that you can and return the rest? But there's really no reason it can't be added, from a technical point of view :)
I was using rustup to install the latest version. As it happens, yes, I do have MinGW installed; will try your suggestion. Whatever the reason, thanks for help!
Ah, well, that *might* be your problem. rustup and the installer don't talk to each other, so if you've used both \**shrugs*\*. Another possible issue: if you're using rustup, are you sure you've told it to use whichever toolchain you installed? If it's currently defaulted to `stable`, and you explicitly install `1.18.0`, then it'll keep using the old `stable` unless you tell it differently.
I continued to work on [titanium](https://github.com/antoyo/titanium) a keyboard driven web browser and [mg](https://github.com/antoyo/mg) a minimal GUI library similar to girara, both based on [relm](https://github.com/antoyo/relm), an asynchronous GUI library based to GTK+ and futures/tokio. I finally removed the concept of asynchronous callbacks in relm because that was causing too many issues. The goal of asynchronous callbacks was to be able to manage a synchronous gtk callback in an asynchronous way: this required to nest event loops, which turn out to be a bad idea. I replaced that with the use of `Rc&lt;T&gt;` which allows to share data between a model and a callback. I also added a [`with` syntax](https://github.com/antoyo/relm/blob/feature/futures-glib/examples/key-events-attribute.rs#L88) to be able to use this new way of accessing the model data from a callback with the `view!` macro. If you can think of a better way to fix this issue, all ideas are welcome ;) . I also updated `titanium` and `mg` to use the new `with` syntax, which solved a bug caused by the asynchronous callbacks and improved the performance of titanium. I also reviewed the first pull request made on titanium: I would like to thank [ejrees](https://github.com/ejrees) for the work made (still surprised someone uses a project in such an early stage). The change from asynchronous callbacks introduced a bug that causes the download input dialog to not be shown: I'll try to fix that this week.
I've implemented `-n`,`-e` and `-i`. `runner -e "0.25.sin()"` fails of course because 0.25 has ambiguous type - you need to say `0.25f64.sin()`. So it's a little strict to be a convenient desktop calculator :) `-i` I think will be useful for people exploring iterators: $ runner -i '(0..5).map(|i| (10*i,100*i))' (0, 0) (10, 100) (20, 200) (30, 300) (40, 400) 
That might very well be it, I'll try again tomorrow. Thanks! This community is very helpful. :3
Buying an exercise bike so I live past 25, then learning to use a mechanical keyboard while on an exercise bike.
I'm not great at English but I feel that forking without proper credit to the original project is like stealing their work. In this case, there is a low visibility mention about it so I feel this is borderline. Again, I understand that my opinion is not shared by everyone. 
I think it should be fine, even for non-copy data. Ownership is moved into/out of the queue. So either I own it, you own it, or the queue owns it. If the queue needs to drop data, it can just deallocate the required slots and no one else will know/care, since no one else has ownership.
You're using individual installs, not rustup?
New reference performance test results released too. The results look even a bit too good (maybe some additional Python overhead there?), so if you have ideas how to better measure the performance, please give feedback!
Still working on making a parser for the [Yarn](https://github.com/infiniteammoinc/Yarn) dialog tree file format. I don't know whether I suck at programming parsers, whether parser combinators suck, whether the crate I'm using for them sucks, or whether writing parsers just always sucks. Switched to `nom` and things seem to be going significantly more smoothly, though. Still, it will be nice when it's done-ish and I can get back to things I actually care about.
Can't part of this be resolved with context structures higher up the stack? So you just get one pointer passed down the stack, but a bunch of other stuff can be found from it. TLS on Linux for stuff in the .tdata section in the optimised case seems to involve a simple indirection from a segment register. Otherwise it additionally involves a call and a search according to a module ID. See 4.3 (e.g. 4.3.6), or else 4.1 for assembler. https://www.akkadia.org/drepper/tls.pdf Edit: Apparently it's not a search in the case of loadable modules, but just another indirection. So still fast. I wonder what Windows does?
Do we have Electron for servo?
just "rustup update"
What were you using?
I'm using rustup.
"Perhaps we should rewrite everyone in Rust." "No, there are too many of us, just write the newborns in Rust!" :)
I'm guessing it makes typechecking take a lot longer. But, as I say, just a guess/intuition. Maybe also some compilation optimisations are made more problematic
"Come on, just learn how to write good C, it's not that difficult to never ever make a mistake in your whole life! Loser!"
&gt; also why would you need global PTR (pointer to global variables)? they did it sometimes because RISCS needed multiple instructions to generate full size addresses, so it made sense to have a pointer with a full size global address permenantly available, with 'the most important globals/constants' held in there accessible through short offsets. reading around this isn't universal, I just remember it for sure in MIPS. My idea was that if TLS *was* to be used pervasively ('a thread object') it might be worth a register. Chicken egg. if it's slow, people wont use it. if it was fast, maybe it would open up different coding styles.
&gt; Can't part of this be resolved with context structures higher up the stack? I thought that would make sense, (what is a thread? some TLS and a stack? put them in the same place?)
No, because Servo is still very much an alpha product. The number of things Servo is presently *usable* for is extremely close to zero. If and when Servo becomes a usable browser engine, I fully intend to try to switch to a browser built on it. I really hope it doesn't stay as a research-project that slowly feeds into Firefox/Gecko forever.
Your rule of thumb for this is that Go, C#, and Java are right about on the same level of performance, which definitely runs counter to expectations, but it's not *that* surprising when you consider how fast the Go compiler runs.
It's working as currently intended, because it's simple and the alternative would be more difficult to implement, help only a small number of cases, and would break code depending on the drop order and code relying on unsafe pointers. What I would prefer to see is having values dropped immediately when they stop being used and live on in the current function until nothing depends on it. If that were implemented, your code that doesn't compile would work.
I tried the code myself. Some comments. * The benchmark is broken with recent changes to the file. Fixing it was fairly simple. * I was not able to reproduce any speed improvement with the unrolling macros. Removing them actually gave a negligible speed improvement. * Replacing all of the `unsafe` indexing that didn't go out of bounds with bounds-checked indexing caused no slowdown, which makes sense since they are trivially statically verified. * Your benchmark heavily understates the cost of code bloat, by running a single path repetitively. A complete system will likely be far more adversely affected. I honestly can't see unrolling helping on this code on modern hardware, since there are no dependencies it would break.
how is upgrade different from update?
Huh? What kind of vulnerabilities are you mainly concerned with then? I am concerned with RCE, security policy violations as well as data corruptions and other errors that may not have security implications. &gt; Can you prove that the bugs are exploitable for RCE. We have whole security industry devoted to that, they do it every day. If you mean specifically those that I listed, I don't have to prove they lead to RCE, I have to act on assumption that they do. Which is often proven to be correct later, but the proof may take a while. &gt; and that this has implications for the industry in terms of costs? The cost of memory safety errors could be calculated. I don't have any figure right now. &gt; But I don't see why people who already are writing succesful and secure software in C/C++ shouldn't continue to do so, since obvously they know what they are doing. Ok, so we disagree here. I am not convinced that C/C++ software is secure and that its developers have solutions to the problem. This is why I was posting examples of Nginx - it is as mature as it can be, and still has many issues. &gt; A random service written in Rust today is probably less secure than nginx. Perhaps, but the same service a year from now would be in much better situation. A service written by Nginx authors would have a lot more credibility behind it. In my experience , comparing to C/C++, safer language need far less time to go from "it compiles" to "its production ready". &gt; I'm not sure Rust is stable enough to replace C for nginx, It is stable considering X86_64 on Linux, which is all I care about and what most of people using Nginx also use. It may not be equally stable/supported/performing for other platforms, but this will improve with time. Sure, there are things that are still being improved and may change slightly, but not enough to make using them impossible or particularly inconvenient. &gt; since servers like Nginx are tightly integrated with platform specifics on each platform, stuff like Tokio is basically out of question for them. The platform-specific part is not Tokio, but mio which Tokio is built on top of, and supports numerous platforms (it says it supports Windows, Linux, NetBSD, Android, iOS), so this part is already done. So another benefit is that instead of having own code, Nginx would use a library and would receive upstream updates (a recent commit adds Fuchsia support for example). &gt; They'd have to write their own support on each platform (just like they do in C), which would entail unsafe code. As mentioned they don't have to do that, but even if they needed or wanted to, having some % of unsafe code is better than having 100%. I've already seen people investigating only unsafe code when searching for errors and finding them quickly (the amount of unsafe code in Rust libraries is usually very low). &gt; Do you really want and expect them to rewrite the whole well-tested and industry-proven codebase just to prevent some 2 RCE bugs in 5 years or so (?) I don't necessarily expect them to do rewrite everything, but a) I expect someone to do it, b) they are doing it anyway, adding HTTP2 for example was a massive change, c) Nginx supports loadable modules, they could start use Rust for writing new features and it wouldn't be drastic change of everythign for them. This can be done gradually. Also there is far more than 2 RCE bugs in Nginx alone in the last 5 years, but even if it wasn't, I am not using Nginx alone. In the whole system I have 100s or 1000s of apps/libraries like Nginx, and even 2 RCE/5 years add up very fast. Also Nginx is fairly independent here, as a bug in it can be isolated to some uninteresting web serving box, where a similar issue with some library exposeses your application data, and substantial number of libraries will never get to quality and visibility level of Nginx.
I find it particularly inconvenient to write a recursive descent parser in Rust. [Here's what I did in my asciidoctor crate.](https://github.com/antoyo/asciidoctor-rs/blob/master/src/parser.rs#L121-L130) I basically have to create a token type enum and peek for a token because I cannot borrow the next token immutably and then mutably even thought the first borrow is not use anymore when the mutable borrow starts (I need non-lexical lifetime). Does anyone know of a better way to write recursive descent parser in Rust? I don't know if I could do something with the new `union` feature, but it only supports `Copy` type for now.
After being exposed to multiple cursors I can't really go back to vim anymore. Re-doing a sed replacement multiple times because it always isn't quite right is really annoying, and multiple cursors elgantly solve this problem by showing you the changes as you are typing. I do however really miss jumping around with keyboard controls instead of using the mouse and/or arrow keys when doing recursive changes (change the type of something, then recursively fix errors in the function body and calling code).
`cargo update` looks up the latest versions compatible to the SemVer specification for the crates in your `Cargo.toml` and updates the `Cargo.lock` file. `cargo upgrade` looks up the latest version and changes you `Cargo.toml`. This is much more radical.
Looks nice, but I'm not even considering using it until does non-destructive editing of Cargo.toml. That's the big blocker for me.
if you are willing to sacrifice one of two "functional" segment registers (in x64 mode) that is FS or GS to hold base address of thread-local "segment" cost can be (almost) zero (when segment has non-zero base address in x64 sometimes for some instructions it can cost you 1 extra cycle depending how optimized is your assembly code because one extra address calculation unit is used) i do not know much about Rust internals so i am not sure if you already use FS/GS registers for some other fun/useful optimization, and if so this is not option second low overhead option is one register holding base address of TLS area, this also costs you one extra cycle (again except in case your code is not able to use/schedule all core execution units, in that case it is "free") third low overhead option requires OS support to map different memory blocks to same address for different threads (more or less what Linux does) this can increase TLB pressure a bit (there is only 64 TLB cache entries in modern CPUs) if TLB does not miss overhead is effectively zero (if turning off MMU was option it would be non-zero but most modern OSes don't allow that) if TLB does miss cost is around 10 instructions in most cases (latency only not execution units so it can be masked by other non-dependent instructions) most other thread local storage mechanisms are more or less slower/have higher overhead, especially those requiring current thread id on each access
You have source control for that, though. 
"Non-destructive" in this case means that it preserves the whitespace, ordering, and comments in the toml file. Even with source control, do you really want to re-apply all of those each time you make a change with cargo-edit? Obviously, it would be easier to just edit the toml file by hand at that point. I solve this by just not ever editing my toml file by hand except to add metadata.
Really interesting that it doesn't affect your benchmarks, I wonder why it does on mine. I'll see if I can post the numbers that I'm getting on my machine, one moment. I'm more worried about it interfering with other optimisations (especially const-folding and SIMD) than just dependency tracking. EDIT: The difference is more marginal than I originally stated (I ran and reran the benchmarks) but is there. I honestly have no good answer for why this is better when my computer was under load (firefox and emacs open) but not when I closed everything down just now to keep the benchmarks as consistent as possible. I'm getting a consistent 20k ns/iter speedup, which pushes us over the C implementation.
Oh, thanks. In that case, it's a good point. 
AFAIK someone just needs to add that feature to the/a TOML library. 
This is more or less what i did when i first came across the need for a builder. In fact, i did exactly this: pub trait Builder: Default { fn with&lt;F: FnOnce(&amp;mut Self)&gt;(func: F) -&gt; Self; } impl&lt;T: Default&gt; Builder for T { fn with&lt;F: FnOnce(&amp;mut Self)&gt;(func: F) -&gt; Self { let mut object = T::default(); func(&amp;mut object); object } } let my_order = Order::with(|o| { o.id = 12345; o.price = 100; }); I don't feel the macro makes this more understandable. If you want to do computation during building, but that in a utility method and call it: let my_order = Order::with(|o| { o.id = a_random_id(); o.price = 100; }); If you want to do building in multiple stages, add another method (implementation left as an exercise for the reader) which makes a modified clone: let my_shipped_order = my_order.but_with(|o| { o.status = Status::SHIPPED; }); If you want optional arguments, write the Default implementation to supply values for them. I think builders are rarely worth the code in any language. I've come to see them as a holdover from the late '90s era of overuse of patterns. 
There is [this](https://github.com/joelself/tomllib).
it is great if you can make simple clib. but from my experience, it is much easier to model your application with high level extension interface.
I second /u/Quxxy; I bet you need to uninstall the previous individual one before you use rustup.
Sure, but someone still has to do the work of integrating it, and it looks like it might no longer be active? Last commit is from over a year ago. (Though how ofter a TOML parser would _need_ to be updated might be the reason for this, I suppose) EDIT: Also, it looks like, according to [this](https://github.com/joelself/tomllib/issues/5), it might still lose whitespace and comments?
Reqwest recently added proxy support for Windows I think, but I can not find the documentation detailing how to do it. As such, with the addition of Reqwest, this tool is useless in the office for me.
I think this is the semi-infamous "mutable borrow in loop" scenario. As u/nikomatsakis [once wrote](https://github.com/rust-lang/rust/issues/21906#issuecomment-303258612): &gt; The summary is that, today, if a value gets returned out of the function on any path, the loan must be valid for the rest of the function on all paths. Since "all paths" includes another trip round the loop, you have to leave self mutably borrowed, even when returning! If you change this line: conclusive_outcome =&gt; { return conclusive_outcome; } To read: conclusive_outcome =&gt; { return Outcome::Inconclusive; } Then the borrow checker is appeased, supporting the interpretation that this is to do with returning a borrow. 
If you can work productively on the train, then your commute can become part of your working day, and so you need to spend less time in the office to get a given amount of work done. That translates directly to more time spent at home with your family / other interests / hacking Rust but this time with a beer / etc.
You might want to check out neovim's [inccomand](https://neovim.io/doc/user/options.html#%27inccommand%27) option in that case :) See [here](https://asciinema.org/a/92207) for a little demonstration (as an option, you can have a split window, showing the matches and the replacements).
&gt; If you mean specifically those that I listed, I don't have to prove they lead to RCE, I have to act on assumption that they do. Which is often proven to be correct later, but the proof may take a while. Yeah, and in the meantime, the bug is fixed... &gt; &gt; The cost of memory safety errors could be calculated. I don't have any figure right now. Well, since you don't have the data, the benefit of the Rust rewrite is basically pure conjecture. This is like optimizing code without first measuring where the bottleneck is. &gt; The platform-specific part is not Tokio, but mio which Tokio is built on top of, and supports numerous platforms I know, I've seen MIO internals and worked with them. Linux is the only platform that MIO supports well. It doesn't support some kqueue features that nginx is using AFAIK. And the Windows support is pretty bad (there's a buffering compat layer that emulates epoll-like API). People have been complaining about that in the forums. I don't think MIO is a good choice for a project like nginx. &gt; Also there is far more than 2 RCE bugs in Nginx alone in the last 5 years What's "far more"? 4 or 5? Or 50? (I very much doubt that.) I'd be delighted to have a look at the data when you have them... 
Personal experience is that compilers are notoriously terrible at SIMD, and that they perform better with rolled code than unrolled code. I don't really see any important constant folding opportunities, but I might be overlooking something.
Document that, please :) ps: I now indoor-cycle with my smartphone on top of the monitor so I can learn a few bit rather than obsess with miles per hour. I'd be curious to see what I can produce while pushing pedal.
Actually I wouldn't expect Java to be as fast as Go. Go does aggressive value typing. It's very easy to look at go as c with some GC for the occasional shared variable. This is harder to do for Java 
Good idea with the keyboard. So you can type past 25 ;) 
&gt; steak overflows And now i have an idea for dinner. Stack probes depend on memory protection to trap the out-of-bounds writes. Does the hardware used in engine control units etc have memory protection? Or is it usually a single-address-space microcontroller sort of setup? Does Rust or LLVM have a story about stack probes on such platforms? Or is that up to the OS, or whatever the equivalent of that is on such hardware? 
We have an open issue for proxy support (~~would link but I'm on mobile~~ [#134](https://github.com/killercup/cargo-edit/issues/134)), can you maybe help us get this sorted out? I have neither a Windows machine nor a proxy setup at my disposal right now :)
Yep! Without the "just" though, it's a hard problem to solve thoroughly :)
I feel your pain! This is an open issue since day one. I usually run this when initially adding dependencies to a project but running cargo add later on messes up completely unrelated stuff like `[features]` as well…
I can't wait for it too; it's such a nicer thing than editing `Cargo.toml`, but until this happens, it can't even be considered for upstreaming in mainline cargo.
I mostly work in Java. I use Rust for programs which react to data coming over a network which need extremely high throughput and low latency; it is possible to get those from Java, but it's a struggle, whereas with Rust it's standard. I am considering use Rust for some glue code connecting a Java application to a C++ library. In that case, that's purely because i need to write the glue code in something which can directly link against C++, which Java can't do, and i know Rust better than i know C++. I am also interested in using Rust for system utilities. For example, a small process supervisor which runs another application, reads its standard output, looks for matches of certain regular expression in the output, and sends messages to a monitoring server when it sees them. I could write that in any language, but it's important that it should be reliable, because it could take down the main application if it fails, and efficient, because it's diverting resources from the main application. I would love to write larger applications in Rust, as it's a generally excellent language. Two things keep me writing Java. Firstly, the tooling and library ecosystem - and in particular, my company's internal library ecosystem - is much more mature and sophisticated. I believe that's purely a reflection of the age and scale of the communities around the languages, not of the merits of the languages themselves, but for now, it's still a real thing. Secondly, the difficulty of handling mutable state that is shared widely across an application. Rust's whole deal is being very strict about this, for good reasons; Java's garbage collector and willingness to let me shoot myself in my foot make this much easier. I hope that over time, i get better at using Rust, so this will stop being an obstacle. 
Just released cargo-edit 0.2 which was long overdue. ~~The rest of the week I'll be convincing random folks to write a format-preserving TOML parser/writer!~~ I'll also make use of the extended deadline of the RustFest CFP! (Ping me if you're also going and want to hang out in Zürich before/after/during the conf!) 
If you want practice, try to explain [what happens in this question](https://stackoverflow.com/questions/32165917/why-does-linking-lifetimes-matter-only-with-mutable-references/32172407). My (circumlocutory) explanation is below. I avoid using the co-/contra- terminology, since I consider it distracting and confusing, but the semantics are the same whatever choice of words.
Tokio seems like a big dependency to pull in just for obtaining config data from etcd, can you comment on that decision? Alternatively I might be missing something, are etcd calls frequently very slow? Otherwise, perhaps it would be possible to support both a sync and async API?
Interesting, i have only used file-locks for this purpose. I would assume zombies would still be a problem.
Glad you got it! :D
I have now added some reference measurement tools against RabbitMQ to most recent utils-library, the results are presented in the http://mles.io/blog.
I'm not involved with the project, but I can make a guess. rust-etcd uses Hyper, which is tokio based.
Now the server functionality has been moved inside the utils library. So I would intrepret that this is not standalone only anymore. Helps a lot in end-to-end testing.
I considered this and decided to change the license to MPL. More info here on the blog http://mles.io/blog
No one *expects* it. That's why I point it out in contexts like this one: Go's performance isn't nearly as good as anyone would *expect* it to be. Edit: That may be less true in Rust's community? I'm from the .NET landscape, where we just assume that anything with a "real" compiler (we dog pretty hard on the jitter sometimes) will run much faster than C# or Java, and Go is one language for which that assumption fails to hold. It may be that people who spend more time working with "real" compilers don't have quite the same prejudice. /shrug
I've read the post and the issue linked but I'm still confused, what does this mean for someone using gfx? Will gfx still have a safe high level layer?
Yes, gfx-rs will still have the higher level, which we'll try to keep at least as simple as it is now. The development focus will change to the core though. The users are advised to continue using 0.16, and then update to 0.17, and 0.18 whenever these come out (as evolutionary changes from the "pre-ll" branch, as opposed to the current master). We'll shout out when the new core transition is done and it's ready for the users.
Does this also work on mobile nightly?
While strictly true, I think it's hard to take the "software bugs can kill people" crowd seriously when they keep pointing at something that happened thirty years ago.
I finally got my setup to (almost) work, so I'll be doing some stuff on Rust, [optional](https://github.com/llogiq/optional) and [clippy](https://github.com/rust-lang-nursery/rust-clippy).
No, only desktop. Shipping two style systems in parallel has codesize concerns on mobile.
wow, it is whip fast! could be placebo effect though (?)
This week I will be starting to port the backend of [Lattice](https://github.com/andrew-lucker/Lattice) from SDL2 to gfx-rs. There is more of a learning curve for me, but the result should be better for the end user. One trouble I have run into is resource management on gfx-rs. Does anyone know of how to load a dynamic image and use it as a texture? Also, perhaps there could be an example of doing this? Right now I only see the manually created texture fields in the pipeline data object.
Is Quantum eventually intended to work on mobile as well, or are the two codebases too divergent?
It is my understanding that improved mobile performance is a major goal of Quantum.
Quantum isn't a separate codebase; it's the same codebase. Quantum is the name of the group of projects. Quantum Flow works on mobile, IIRC. We do want stylo to be on mobile eventually, but we're trying to get stuff out by 57 so we're deferring caring about mobile until later. (That way we can also remove the old style system in the next release)
This is great news. I also wrote a renderer in the last couple of months. I chose to skip a low level abstraction layer. I only have a really high level abstraction layer and I implement it directly with other low level graphics apis (currently only Vulkan). It was *really* hard to find a good abstraction and I had to refactor the renderer way too often. Initially I wanted to expose a lot of "low level" functionality like command buffers but I found it too hard to expose them safely and efficiently. I now focused my attention on a high level shading language. It is based on Rust and compiles to SPIR-V and LLVM IR. I am only working on it for a week so many features are still missing. I'll make a big blog post in a few weeks, maybe this could also be useful to gfx-rs.
I kinda don't get why use a library in the first place. It should be more or less the same if you just parse it yourself, doing any modification on the fly. Plus that'd keep formatting.
spatiub, a low-latency (&lt;1ms, 99p) spatial pub-sub. Much more complicated than I first thought, but so interesting! The goal is to facilitate the creation of indie MMOs but it may have some other applications. Its current architecture is nearly lockless and I expect the demo to be able to handle between 20k and 100k connected entities on a standard server, depending on the entities density and message rate. If anyone is interested, you can subscribe to http://spatiub.strikingly.com/ or ask me anything :)
Used Rust to make a harness to run a timing attack for a CTF challenge. (Easy cross compilation was really useful there.) Otherwise, I put in a PR over at the remacs project and I'm looking at doing another this week.
&gt; I really hope it doesn't stay as a research-project that slowly feeds into Firefox/Gecko forever. What's the difference if it slowly replaced the old Firefox finishing until it was complete or if it popped up with a new name when it was complete?
Firefox I'm sure has a lot of architectural baggage, which a fresh design from the ground up would avoid.
Shading language abstraction was always the weakest point of our API shield, and I'm excited to see you working on it! We [have plans](https://github.com/gfx-rs/gfx/issues/1374) to accept SPIR-V shaders universally by all backends, but having a way to generate those binaries, especially with Rust code, would be extremely helpful for the higher-level abstraction.
Did it land on Windows yet or just Linux?
I've been using it for a few days, unless the pref is lying to me.
Should be all three. You can check under `about:support` after enabling the pref.
The pref exists regardless of whether or not it works -- you could toggle it way before we started shipping it in nightly. about:support lists whether or not it actually is on.
Fair enough :) I hadn't thought about how difficult it would be
&gt; Alternatively I might be missing something, are etcd calls frequently very slow? They are both. In `etcd` you have generally 2 modes. You are either requesting the latest key value and *immediately* expecting a response. Or you are waiting for the key to be written/waiting to be notified when the key updates. Ultimately Async makes a lot of sense here because otherwise you have to throw those long wait items into a thread and just wait for it for to return (which maybe never). This is pretty wasteful just waiting on 4-5 keys is 4-5 threads that could be blocked on the ordered of hours to days.
Somewhat related, was there a specific reason to target Firefox 57 for the Quantum stuff? And what happens afterwards? Reading the Quantum Flow newsletters I got the impression that the whole effort will be spun down afterwards, although most of the issues won't be fixed in time for that release. I recall previous performance pushes by Mozilla (MemShrink and a perf effort) which did good work for a while, then kind of stopped. Will the Quantum projects have the same fate?
I did update my nightly today, but it does say Stylo: true (enabled by user)
Hm, it immediately crashes for me :( [backtrace](https://gist.github.com/goffrie/071962b3517ea036619a56af4e2a4dee) Also happens if I create a new profile and flip on only this setting.
The idea is to bundle together a lot of goodies together so that 57 can be marketed easily. Quantum Render won't happen in 57, for example, but it probably will happen at some point. Flow is the only quantum project without a defined scope, and I suspect they'll keep on fixing things till we're certain that we've got most of the known perf issues fixed (those that can be fixed without major architecture changes). If it's winding down I bet were nearing that point. But I don't know. Perf pushes don't keep on giving goodies as long as you're pushing on them, at some point it's harder to eke out perf wins.
Close, though instead of making everything virtual they just leave a NOP at the beginning of every function. Then only functions you actually edit get turned into indirect jumps by replacing that NOP.
I'll look into it when I get to a computer again. Looks like we got an unexpected font-weight value. Could you push the crash report (about:crashes).
I have a few main goals for the language. It is only based on Rust as I am not sure if Rust is the best choice for a shading language as some concepts don't completely make sense. (ownership, lifetimes, allocations etc). I want to to be modular, code sharing should be easy. Especially for things like instancing, subpasses etc. I also envision something similar to crates.io. Shaders should be compact, spirv seems to be really nice for this. I am not sure how well this would translate to other shading languages. It should be testable. It would be nice to test a few functions on the CPU. I intent to also write VM for SPIRV to execute the shaders. It should be usable from many different languages. I intent to expose a C API that exposes the build system + reflection. It should compile really fast. I currently support runtime shader reloading in my renderer and I don't want to wait for the compiler. But I'll make a bigger blog post when I can implement my PBR shader code in this new high level shading language.
don't think it works in Firefox Developer Edition 55.0b11 (64-bit) same with webrender (unavailable by runtime: Build doesn't include WebRender)
Yes, like I said, Nightly. We only enabled it build time on nightly two weeks ago, there's some time before it hits Dev Edition
I don't have any examples, but I can say: Sadly cross compilation is very much an issue. Your best bet is probably using Travis CI to auto-deploy.
New to rust so I want to learn some more before I attempt a simple 8 bit VM if I can find some specs
Thank you for describing the roadmap, it sounds great! A good match for our goals here, and I'm especially fond of this part: &gt; I intent to also write VM for SPIRV to execute the shaders It looks like you are basing your work on [rspirv](https://github.com/google/rspirv), which is awesome! Could you also provide your shader generation code for us to look? We might be able to work on it as well.
&gt;It's not a very useful argument: what's missing in Rust to replace C++ in gaming? Rust isn't actually precisely targeted at the problems of game development; it's debatable if it would help. (perspective: I did console gamedev from the PS1-PS3 eras). There's 2 more options to consider now: JAI (purpose built), and if Swift gets move/borrow, it will also be better (IM). The irony of that is that C will live on because there's no definitive replacement .. you'll want swift/JAI/rust/C++ to use simple C FFI for interop.. I keep explaining the issues here.. games use unsafe indexing all over the place (meshes) and they wont even look or work right if those are wrong, and ship with 'unsafe' float assumptions (a float could be NaN, but finished code can't to check all the time: it must be logically designed to avoid generating NaNs at all ). Consoles often have realtively weak processors (budget goes on the GPU) so unnecessary branches/checks/indirections/fragmentations are unacceptable. In a standard program, you do allocations which could fail, and check/respond afterward. In a properly designed game engine, the system (including the ability to control pre-prepared content with offline tools) recalculates and throttles such that *before* you embark on inner loops you know the upper limits. **Consistent frame rate = consistent load**. When it's all working right, you can actually eliminate dynamic memory allocation, although not all projects get there these days due to time constraints. So you've got other requirements to do extensive testing for (is it *fun*? does it *look nice?*, does it *drop frames*, which will make a VR user *vomit?*). Levels are performance tuned, you don't just 'throw the content in'. You might have tools where the game plays itself to setup/tune the AI, or precalculate which parts of the scene you can see to optimise the texture streaming.. if you're doing all that , catching out-of-bounds arrays is kind of a non-issue really.. This is an excellent talk I agree with about 95% of, he evaluated Rust before embarking on building JAI https://www.youtube.com/watch?v=TH9VCN6UkyQ&amp;list=PLmV5I2fxaiCKfxMBrNsU1kgKJXD3PkyxO I actually like Rust *more* than him, but I still don't think it's perfect. Rust is **Safety &gt; Performance &gt; Productivity** what Gamedev needs (IMO) is **Performance &gt; Productivity &gt; Safety**. Safety is not the same as productivity: games need to be tested for other reasons, so you catch a portion of the bugs anyway. the front-loading of *some issues* (during *creative exploration*) is not a win, IMO. **My interest in Rust is all the syntactic tweaks**,(where C++ is crippled in ways that are orthogonal to its performance virtues): *no headers, better lambdas, better macros, tuples, better inference* ... although it has still been interesting for me getting some exposure to 'priorities different to the ones I've dealt with'. If **Swift gets move/borrow**, it will be superior to Rust for games; swift concentrates more on productivity features for 'application code' more already (and it can replace some of the use cases of scripting languages); ... but if Rust got a mode to loosen some of it's dogmas.. it could take that slot. The syntax *is* suitable, the underlying language engine is perfect. I preferred Rust in an older form with the sigils; I can imagine 'rust with @T' being productive enough to eliminate the embedded-scipting use cases, and having everything in one environment would be vastly superior. It's subtle.. although it doesn't look like much, the sigils 'melt away' letting your brain focus on reading program specifics, rather than 'verbose markup of trivialities'; they crossed a threshold were it felt closer to a productive language putting it in unique new territory, whilst you still knew you could optimise and control it (now with more angle-bracket obfuscation, it feels more like modern-c++). [I explain this more in other threads here](https://www.reddit.com/r/rust/comments/6olo9o/thought_experiment_on_name_spacing/dklf2ky/) - the need in games to interface between tools,toolchain,engine,gameplay code use cases, having all of that in one place would be perfect. (Here I keep hearing "you don't need this in a systems language", like you *expect* the other parts will be done in something else, as in webdev.) **That's where the real opportunity is for a new games language**. at the minute the problem is 'solved' by splitting between 'underlying c++ engine + c# game-code / (lua scripts/designer controlled graphical tools/whatever) Games is a unique field where in the *same project* you need to drop right down to C-like unsafe usecases (i.e. lower level than Rust is intended form ,not even relying on heap-memory management) , *and* rise all the way up to the kind of 'quick and-dirty' experimental code that javascript/lua/python are actually good at (i.e. 'lighter/higher level' than Rust is intended for), with a lot of work on tools somewhere in the middle. *I'm sold on the idea of unsafe blocks*, but **rust seems hostile to raw pointer code beyond that**, ... the times when you **do** need raw pointers, it's just way more useable in C/C++. (these days a lot of that is handled in middleware engines, but if you're talking about 'replacing c++' you need to move on to looking at those, not end users who might use c# already). I heard someone here say 'use a prototyping language' .. thats far from ideal because you might run out of time and have to ship. Prototyping in rust with @T would have been perfect, because you're working in-place, with the same types, ready for optimisation.
I started off with `combine` since I for some reason thought nom was inconvenient for doing actual string parsing... it's not, it works just fine. It was really frustrating to get going with `combine` and I'm not sure I can express why. Nom just goes far more smoothly with the parts behaving how I expect them to behave. I then tried lalrpop which seemed decent but is... well, a traditional parser generator, which is always a pain. (Though it's awesome it integrates with `build.rs`.) And I didn't feel like trying to dig allll the way through all its features to try to get what I wanted done. I've written plenty of parsers for programming languages with yacc and its relatives, though I've never enjoyed the process. Never used a combinator parser library before now though, so. It seems... simple for the simple stuff. Maybe possible for the complicated stuff? We'll see.
yep just downloaded the nightly and activated the flag. either the whole nightly is faster or the old css engine was extremly slow.
Currently writing an gameboy emulator. I wanted to create a macro that can be called as followed: `adc_a!(r.b)`. I always get the following error: cannot move out of borrowed content. As far as I understood, passing an expr should make you the owner of it. Here is the relevant code: pub fn execute(opcode: u8, r: &amp;mut register::Registers, m: &amp;mut mmu::MMU) -&gt; u32 { macro_rules! adc_b { // expression is needed to evaluate the HL register too! // Will either be a register entry (e.g. r.a) or a read byte from memory. ($value:expr) =&gt; ({ // the carry flag will get added if it is set. // perform operation let a_value = r.a as u16; let loc_value = $value as u16; let carry = if r.f.c.get() {1} else {0}; // r is underscored with the error messagr from above! let result = a_value + loc_value + carry; // Z flag if result is zero. if result == 0 { r.f.z.set(true); } // always false. r.f.n.set(false); // only if the result carried a bit from position 3 to 4. if ((a_value &amp; 0xF) + (loc_value &amp; 0xF)) &amp; 0x10 == 0x10 { r.f.h.set(true); } // TODO: better performance method? // we need to make sure that the carry is set if the number is larger than 65536 (0xFF) if (a_value as u16) + (loc_value as u16) &gt; 0xFF { r.f.c.set(true); } r.a = result as u8; // in most cases, it will be four cycles. 4 }); } // test adc_b!(r.b); }
&gt;It looks like you are basing your work on rspirv, which is awesome! Yes I currently use rspirv heavily. Previously I only intended to add a llvm backend and then translate that IR with [SPIRV-LLVM](https://github.com/KhronosGroup/SPIRV-LLVM) but I couldn't get that to work, so I also added a spirv backend. &gt; Could you also provide your shader generation code for us to look? We might be able to work on it as well. It is currently too early to publish, I need to properly refactor the compiler and then I'll open source it. I probably do it in conjunction with the initial blog post. 
Can you benchmark/ are there benchmarks? On the quantum wiki page I don't see anything suggesting it's faster, only a few things suggesting it's slower, but idk.
My road with parsers went from pest to nom. Very happy with the result. The more tricky bits also came out very nicely as writing your own functions is easy and the code comes out quite clean.
Last week I removed the petgraph dependency from [limn](https://github.com/christolliday/limn) in favor of Rc references to nodes, but there is more work needed to work out the bugs and actually realise the advantages. Going to clean up and remove a lot of quick and dirty code and do some major refactoring this week, including rewriting the core event loop and updating to the latest glutin.
I explained as much as I know in the tweet's replies: https://twitter.com/isislovecruft/status/887796161611931648
I've enabled it and the CSS seems fine for a few test sites I've looked at. Nice! stocktwits.com seems much faster now. This might be a good test site because it seems to be constantly injecting new content and causing page reflows. 
That's the way to go, in my experience. Even without making safety a concern, trying to abstract over APIs at the level of Vulkan/D3D12/Metal or even just D3D11/OpenGL just leaves too much on the table and makes things too complicated. It's much easier to maintain when the abstraction boundary is higher level. All the slightly different cases when using each underlying API can be written without interfering with each other, and any common code that just handles data can still be shared. The post's mention of WebGPU is one case where that doesn't work, so I'm glad gfx-rs is exploring the space. It's just not something I can see myself using.
I guess the unofficial reason is the XUL addons will be dropped in 57. So they want to provide a lot of fancy new stuff too (like the first part of project Quantum and an UI refresh), so user do not feel this version is just a regression. 
Haha, yeah, it crossed my mind. "We broke your add-ons, but how much faster it is now". It sounds a bit cynical, but it wouldn't bother me if it was true.
I hope this qualifies as an easy question since it *seems* like it shouldn't be too hard. I'm wanting to make a global constant hash map to act like a LUT/dictionary for immutable values I'm referencing a lot. Is there a better way to do this other than using the phf crate? I've been unable to get to the phf_map macro working properly (on stable and nightly). I'm considering just making a struct that just returns the constant values through a method, but it seems like a dumb way of doing it.
Congratulations! Another era in gfx's storied history ;)
Yes! A lot of it it due to u128 support in Rust. That's because the elements of the field underlying this elliptic curve are too big to express as normal integers. Some crypto libraries use bigints for this (it's generally a bad idea to do this, for safety reasons like constant-timedness as well as speed/efficiency reasons). Smarter crypto libraries split up the field elements into what are called (for some reason, I actually have no clue about the etymology here) "limbs". For this field, ℤ/(2²⁵⁵-19), in particular, it is convenient on 32-bit systems to use radix-25.5 limbs, that is: a field element has ten limbs, each one is an u32, `t[0],...,t[9]`, represents the integer `t[0]+2^26 t[1]+2^51 t[2]+2^77 t[3]+2^102 t[4]+...+ t[9] 2^230`. (So each limb alternates between carrying 25 and 26 bits of information.) With u128s, we can do much better: each field element is expressed as 5 radix-51 limbs, each one is an u128. For obvious reasons, this speeds up field arithmetic by about 2x. (And this makes a huge difference, since, depending on coordinate system choices, elliptic curve points are represented as either 3 or 4 field elements, so for example in extended twisted Edwards representations as X:Y:Z:T.) The Golang library has only radix-25.5 code. But it's not just our code taking advantage of u128. It's also that LLVM is generating much smarter assembly and does an extremely good job at taking advantage of newer instruction sets. On Haswell and newer, it uses vectorised AVX2 instructions like conditional selection with VPMASKMOVD and VPMASKMOVQ, also we get 64-bit times 64-bit to 128-bit MULX; Broadwell and newer we also get 64-bit unsigned additions with carry and overflows respectively with ADCX and ADOX instructions. This gives us another factor in speedup (so up to 3x now). Plus there's probably some Skylake stuff going (which is why I guessed that Filippo is running benches on a Skylake machine) but I don't really know all that much because I don't have access to a Skylake machine to play with. My understanding is that the AVX2 [whatever fucked up version the one on Skylake is named](https://twitter.com/hdevalence/status/884968076004925440) on Skylake is transparently backwards compatible but like 10% or so faster, but I'm not sure. We'll talk more about why dalek is so fast in our RustConf talk coming up next month!
Ya, returning a `vec&lt;T&gt;` would be cumbersome, which is why I proposed taking the last value. Taking the last value is also consistent with the idea that blocks which are under matching matches are done in sequential order from top to bottom
In our case, return pointer optimisation is helping speed things up.
[Removed, misleading benchmark, sorry] The only thing I noticed was on the UBlock Origin extension, if you resize the window, the number of blocked ads shortly turns red, before turning back to grey. Not sure if this is a CSS bug. Krakenbenchmark is roughly 200 points faster than Chrom(ium). The only thing is https://html5test.com/ which has 493 points for FF, but 516 points for Chrome.
[lazy_static](https://crates.io/crates/lazy_static) should fit nicely for this use case.
There's a pile of optimisations that are useful but don't affect constant-time-ness such as avoiding needing to spill values to the stack (through, say, good register allocation, inlining away function calls and even splitting data structures into their constituent parts to handle each separately). But yes, it definitely is dangerous to use optimisers on code that should be constant time. It seems like a bit of an unusual choice.
[removed]
I added support for placing png images to [Pris](https://github.com/ruuda/pris#readme), a domain-specific language for designing slides and other graphics.
Also, I am considering removing the `,` notation and instead moving to the same as `match` statements: `|`. This would make it even more in line with normal `match` statements and would have 2 benefits: 1. The compiler can catch the same pattern being used in the same `pat =&gt; expr` pair 2. The syntax of patterns need not change at all
I haven't done something like this in Rust yet, but I use D-Bus for this sort of thing in my Python projects because it has the following combination of advantages: * No zombies * Effortless to scope to a user's session or the whole system, simply by choosing which bus to connect to. * Marshalling/unmarshalling code for making RPC calls is provided for me. * Easy to implement a command-line interface which either launches the program or, if a copy is already open, triggers the operation within the existing instance. * Portable to any Unixy OS with an XDG-spec desktop and many beyond.
Is this to imply that none of the Servo stuff will exist in mobile Firefox until version 58?
Don't run benchmarks with webrender on. Webrender support currently runs both webrender and the regular renderer, and you should not expect performance wins. The support exists so that they can incrementally fill in feature support for webrender, and then make it use only one renderer when webrender supports most things. Regarding your test scores, is a lower number better or worse? And can you expand on the native menues comment?
Peacekeeper seems to do a lot of animations, so I'd blame webrender. As I said, turning on webrender will make things _slower_ right now because we run both renderers.
`rustc --print sysroot` should also help locate what you're running.
Probably yeah. Can't promise 58 either, but I'd expect it to be in by then. encoding-rs should be in all firefoxen though.
This is awesome! My biggest pain point with Firefox was how unusable google maps was. I can still notice some minor lag in data loading compared to chrome but otherwise it's now just as smooth. Nice to finally get to experience all the great work the servo team has been doing.
FWIW, I'm working on a fully style-preserving TOML library - we talked about it at RF Kyiv very briefly. Been very busy lately so progress has stalled, but I hope to get a good amount of work done in the next couple weeks, and get it into a state where I can open it up to review/contribution. I can ping you then and we can work on making this awesome!
Nightly is definitely faster overall, I tried it out about 1 month ago and haven't gone back since. Edit: It also has pretty cool new features not on stable yet. The new screenshot tool is the greatest thing ever.
Glad to see you are still on it! Ping me as soon as you want feedback and I'll try to get back to ASAP. If other people are in a similar situation like me—interested but only have small pockets of time here and there—it might be a good idea to publish the very first draft that shows basic functionality and design and then write a lot of issues for grunt-work features and refactorings :)
I thought Servo and WebRender were years from being ready. This is pretty impressive.
Webrender is probably months not years. (I'm not sure) Servo is probably years from being ready. But this is just servo's style system.
I haven't used it for quite some time, but [Panopticon](https://github.com/das-labor/panopticon) should work on the Big 3. It uses Qt.
Programming language usage has very little to do with whether or not one is a great programmer. If we're going to actually assume that's the case, though, I'll tell you that writing C++ by default already makes you a better programmer than writing Rust because you have to _think more_ about the code you're writing in general. Unless you're trolling. If so, carry on.
[removed]
What does the style system do? Rendering? Parsing? What does Servo need once WebRender is finished?
Higher is better. And I don't know what happened to Chrome in that test, the test is invalid, sorry. [Corrected](http://i.imgur.com/DbRh124.png) :( The native menus disappeared shortly (I swear). I could hover with the mouse over the titlebar and only the title of the window displayed twice. They are back to normal again, I'll make a screenshot + info if it happens again.
Parsing and cascading -- assigning styles to each element in the document (and recomputing this info each time the DOM is manipulated). Servo still needs a lot of layout support for many features, and JS/DOM support as well
I suspect that's a nightly bug, not a stylo bug. Nightly tends to have small papercut bugs like this. Stylo doesn't style the browser chrome (yet)
Is writing a JavaScript engine in the long terms plans of Servo?
No. But there are folks who do want to rewrite at least portions of Spidermonkey in Rust. I don't think there are concrete plans yet.
No. Just reaching parity with existing engines would be a huge amount of work, and there are no realistic ideas for how to avoid the unsafety inherent in writing a JIT.
Anyone else notice that the C API triggers magic behavior when you pass a null as the first character of the path string, something that since C uses null terminated strings will break that string with every other string function. This also seems like a security nightmare waiting to happen, although I must confess I don't see how it could be exploited at this time. I imagine the first step would be finding a program that takes user input of some kind when specifying the AF_UNIX socket to open/create, then arrange for that input to start with a null. Must C functions for validating the path would read it as an empty string at that point, while the socket call world see it as a abstract socket path.
Was there any particular page where this happens? I'm unable to reproduce, and I have a couple of guesses as to the reasons, but it would be easier to debug if I knew the site. It seems to be a linux only issue but I can't repro on linux either.
Mostly a lot of summer fun, a little bit of work on [`uom`](https://github.com/iliekturtles/uom) (type-safe zero-cost dimensional analysis) trying to figure out how to share some code while working on thermodynamic temperature and temperature interval.
It happens to me on every major website I tried - reddit, CNN, Amazon, Facebook, etc. example.com works though (and some other simple pages).
Could you send me the data from about:support? There may be some system font setting you've done that makes this possible.
https://gist.github.com/goffrie/e96c995b09bc62eb2434c872362b0110
Just write a safe JIT. Seems easy
I strongly advise that, I use inccommand as well, it’s very neat.
Well last week I posted about [tarpaulin](https://github.com/xd009642/tarpaulin), so I'm clearing up the issues people are finding on different systems or tests setups I hadn't yet considered. I'm also going to play around with vagga and using it to automate my testing of a larger number of distros. I also aim to put some work into the nix crate but I need to think a bit more about that before I put fingers to keyboard
Having to wrap my mind around lifetimes at the same time as I try to internalize this just makes it worse. If I'm going to get a definition to stick, it's going to have to be in terms of classical inheritance, since I have over a decade of experience with that.
phf is the usual recommendation for this use case -- what seems to be the trouble?
You can't take ownership unless you _have_ ownership, and you only have a borrow of the `Registers`. What's the definition of that struct?
Jumping from 54 to 55 alone is a huge leap as 55 (currently Beta) ended up with two Nightly cycles (as a result of the removal of the Aurora branch) receiving a lot of performance fixes from the [Quantum Flow](https://wiki.mozilla.org/Quantum/Flow) project. Nightly is currently six weeks into the 56 cycle, so there's plenty more on top of that. All that to say that there's lot's of noticeable performance work up and coming.
Just compile Javascript to Rust code that doesn't contain any unsafe blocks. Easy!
A crate like [libproxy](https://docs.rs/libproxy/0.1.0/libproxy/) might be the way forward?
I didn’t have my tests right now. But it should before the step 3.b
Why exactly do I need ownership here? I pass a reference to the actual `Registers` all the way down to the macro call. Shouldn't that all avoid the problem of having to take care of ownership? pub struct Registers { pub a: u8, pub b: u8, pub c: u8, pub d: u8, pub e: u8, pub f: FlagRegister, pub h: u8, pub l: u8, pub sp: u16, pub pc: u16, } That's the definition of my struct.
So why there is setting `layout.css.servo.enabled` available in Firefox Dev Edition? Is there any `about:*` that shows if Servo is enabled? Edit: I see that Nightly's `about:support` shows: `Stylo true (enabled by user)`
What's the definition of `FlagRegister` and the `get` function of `c` field in that? Probably that `get` call takes `self` as a parameter, which would require a move, but that is not possible because you only have a reference to it via `Registers`. That is: you cannot move (`r.f.c.get()`) out of borrowed content `r: &amp;mut Registers`.
I guess I need to see `FlagRegister` too then. I'm trying to disect the expression that caused the error, namely `r.f.c.get()`. My guess is that `fn get` takes `self` by value, which means moving `r.f.c`, which you can't do without ownership which you don't have.
Holy, you are right! How did you know that? :) Thank you really much for your help :)
Because the pref exists independently of the build setting. Stylo existed when dev edition was built, it just wasn't built by default (so the pref was there, but it wasn't useful) There are a lot of prefs that do nothing for a particular build. about:config isn't really user-facing, so this is considered to be ok.
god I wish they would disappear
&gt; How did you know that? In a sense because there was nothing else it could be. The line with the error had only one interesting thing going on: the `get` call. If it didn't take `self` then nothing would be moved at all. It's only experience with the language, internalizing how the borrowing happens through fields, and running into these errors on my own that let me see through the issue quickly. It was possible that the `get` call was coming from some trait where you could implement it for `T` or `&amp;T` or `&amp;mut T` (where `T` is whatever the type of `r.f.c` is), but I didn't want to cover that unnecessarily.
It was estimated late last year to possibly be ready by the end of this year, so I think it's right on time.
`layout.css.servo.enabled` isn't in my `about:config`. "About Firefox" shows `54.0.1 (64-bit)` but my package manager shows `firefox-nightly-56.0a1.20170724-1` Launched with `firefox-nightly`. Do I have to uninstall regular Firefox for Nightly to work?
&gt; Why do we care about the power consumption of a compiler on a mobile device? Mobile device, aka, laptop, tablet, phone and other devices that are generally portable and can work off battery. So yeah, I think that it's not so hard to think that some people care about running a compiler on their laptop. Some people use tablets as laptops (ala surface) and it's reasonable to think that in some future phones could increase in power to the point were they can be used as the core of a portable laptop computer. It shouldn't be a critical thing in realizing the decision, but it should be something to consider when deciding what should and shouldn't be done, so that it might make sense to offer a way to limit parallelization to increase battery life.
That's honestly a better approach than "we broke your addons, check back later for the performance improvements will let us make".
That nightly is half a year old. Package managers generally get these things later. YOu might want to download nightly and run the binary directly.
Wait, `firefox-nightly-56.0a1.20170724-1` is half a year old? I might try the binary though, thanks!
Wait, what? No, that's a new nightly. But Firefox 54.0.1 old. Well, it's the most recent release, but the nightly for 54.0.1 is old. I suspect you ran the regular firefox instead or something.
Okay, figured it out. I just had to close all my firefox 54.0.1 instances before starting `firefox-nightly`. Thanks for the PSA, trying this out now!
Well I specifically wanted to use the phf_map macro, which seems only usable with nightly Rust. I don't like the codegen syntax. When I made this post I wasn't doing something correct (not sure what). I just retried it and it seemed to work. I'm okay with this current project I'm working on using the nightly build (since I'm just making it for fun/learning), but if I'm ever working on a larger project I'd be a little more hesitant to use it. Maybe it'll work with the stable channel eventually.
Looks good! I will check it out. FYI: The CBOR serde library looks like it is looking for a new maintainer.
"What do you mean 30 minute page load time isn't acceptable?"
I'm looking at moving a project from nodejs to rust but have a mental block on how data streams are handled in rust. In node, streams are readable or writable but I'm having a hard time trying to understand how to do this in rust. Here is the part of the code I'm having a hard time understanding. https://pastebin.com/RtGz7KFL
I did a bit more digging and it appears to crash whenever there's a form element, like &lt;input&gt; or &lt;button&gt; (but &lt;textarea&gt; is fine). edit: however, if the form element has `font:inherit`, it's fine. I can't find any other way to avoid the crash — I tried setting all the individual font properties (by looking at the computed style of a `font:inherit` input in another instance of Firefox), but that didn't work.
I don't seem to have that option. My about page in nightly shows 56.0a1 (2017-07-23) (64-bit). Is there another flag or something else I have to run to get it? I'm on Ubuntu 17.04.
I recently tried out gfx with glutin (from one of the examples) and the performance wasn't that good. Just letting an example window run took a noticeable (in task manager) amount of CPU percentage. Does that have to do with some abstractions or is it something else? I would think the examples show how to best use gfx. Ran some other example where a glutin window with glium was used and the resource usage wasn't as bad.
Oh, sorry, I already figured it out. Basically Pango returns a wider set of font weight values than CSS expects. So when requesting a system font (system fonts are often used for styling form controls among other things), we sometimes get invalid weight values. Gecko is totally fine with this, but stylo actually checks and thus panics. Fixed in https://github.com/servo/servo/pull/17846 which I'd expect to be in the next nightly. Sorry about the bug, and thanks for the help in finding it!
Awesome ! I probably should use that instead of file locks
Is this the nightly distributed by Firefox or from your distro? You may need to directly download and run the nightly on Ubuntu. Check what it says under stylo in about:support, too.
I use a multi-stage [Dockerfile](https://github.com/lawliet89/rowdy/blob/944bcc9385c59d452f9a40663d550bb0f092769e/Dockerfile) to buiild a MUSL binary, then either run it directly from an Alpine container or use `docker cp` to copy it out to the OS to run directly.
In case it helps in future, you can run different versions of Firefox side-by-side as long as you're using different profiles: mkdir /tmp/test-profile firefox-nightly -P /tmp/test-profile -no-remote `-no-remote` means it will actually open a new window itself, instead of trying to tell the already-running instance of Firefox to open a new window.
That would do it. on about:support it says `Stylo: false (disabled by build)` It's distributed through a PPA called ubuntu-mozilla-daily. I'll try the direct download.
Classic performance/safety trade-off.
MemShrink didn't stop! It did get a lot quieter, because most of the major problems were fixed. (These days Firefox generally has the reputation for being the least memory-hungry browser.) But people are still involved.
Yeah, build script is the way to do it on stable rust until proc macros are a thing, so it sounds like the major objection is "I don't want to". Which is fine! Looking at that codegen example... it does look pretty lame. I bet a macro (to use in the build script) could help...
What sort of build script?
I'd like to start contributing to open source Rust development. A number of my own projects are open-source, but I haven't contributed much to community projects before. Where would I find good projects to start contributing to? Also, is there anything that you'd recommend someone new to open source should read? I'm fairly familiar with Rust itself already.
I thought we were both talking about [this](https://github.com/sfackler/rust-phf#phf_codegen).
We were, I just blanked on what you were talking about. My mistake! Tangential but semi-related question: how do I make external crates visible to my whole project? Or at the least expose the data structure generated by the macro to the whole project? I have a main.rs, lib.rs, and a few structs/implementations defined in their own files in the same hierarchy level. I've figured out how to get phf_map to work in main.rs, but not in any of those other files I was describing. I tried placing the same statements that made it work in main i.e. #![feature(plugin)] #![plugin(phf_macros)] extern crate phf; but I get: `cannot find macro 'phf_map!' in this scope`.
I wonder if /u/Rothon would want to include a convenience macro like [this one](https://gist.github.com/81e69710599c78ea586b3c4d6cf91669)? I also saw [this hack](https://github.com/sfackler/rust-phf/issues/106).
This is [an example.](https://play.rust-lang.org/?gist=4661e8d0a7e748f4949c7eb649efed93&amp;version=stable) Basically, you have to implement the Debug trait for your enum. The alternative is to use something like [debug_stub_derive.](https://crates.io/crates/debug_stub_derive)
Those attributes should only work at the crate root, meaning main.rs or lib.rs. It's worth noting that with both main.rs and lib.rs, you have **two** crates co-habiting in the same directory, so the way you get stuff from lib.rs into main.rs is just by writing `extern crate $your_crate_name;`, treating it as if it were a dependency.
Check out the Call for Participation section in every issue of [This Week in Rust](https://this-week-in-rust.org/).
Maybe someone else can help you more, but all I have energy to do tonight is link you [to this.](https://rustbyexample.com/generics/gen_trait.html) Your WidgetResponder trait has a generic method, but the trait itself isn't generic, which is not allowed in a struct field. That link I just provided? Here's a [modified version](https://play.rust-lang.org/?gist=bfe95e6ec2497c49e1b2405207524fa4&amp;version=stable) showing a generic trait object being placed into a struct field. You should be able to do something like this to get your first attempt working. For your second attempt, you probably need something like `&amp;mut RenderTarget&lt;Context = SomeType&gt;`, if I were to guess, which would make it decidedly not generic. So, at that pint you would have to add genericity to the trait and the Widget in order to set `Context = T` instead of `SomeType`, and you'd basically be back to the first attempt.
It would be great to know the exact details and resulting numbers of your test. For one, how recent were the tests? Semi-recently /u/icefoxen did a [great deal](https://github.com/gfx-rs/gfx/issues/1198#issuecomment-282403735) of caching optimization on GL backend, for example. In general, the difference with `glium` on simple workloads may occur due to GL backend being the least fit into our [Programming Model](http://gfx-rs.github.io/2016/09/14/programming-model.html). Thus, there is a penalty for our chosen abstraction, and we can totally tolerate it, given the focus on current-gen APIs (Vulkan, D3D12, Metal). 
I'm still new to gfx-rs but I'm fairly sure that after you get a byte array with the image data you can pass it to the create_texture series of functions in the factory trait. Then all you need to do is overwrite your old texture in your pso before rendering. 
You could make a newtype for the external type, and manually impl Debug for that -- that way, you can derive Debug on the struct you care about.
It's really cool that it compiles down to the same stuff, but that compile time issue could get really old.
That looks like exactly the sort of thing I want. I guess I can always define a wrapper type to impliment the serialization traits if I want them.
* Make sure you're running it with `--release` * If you're on OS X, make sure that the core profile is set with `.with_gl(glutin::GL_CORE)` in `glutin::ContextBuilder`.
I just use a Makefile that contains `cargo build --release`, followed by an `install`, and then a `systemctl restart &lt;service&gt;`.
I'm Rewriting It In Rust! It's hackathon week at my company, so I'm attempting to rewrite one small module of a library in Rust, just to get a feel for it. There's a 0% chance this makes it into production, but it should be a good learning experience.
I think in general, most optimizations that you could apply to crypto code would not affect constant time-ness. Constant time code is usually written as a sequence of bitwise operations, loads, stores, etc. and it would take a mathematical genius of a compiler to figure out what those operations do at a high level and rewrite them. Alternatively, it would take a perverse compiler to look at something like "A xor B" and turn it into non constant time code like "if A then A xor B else B". I'm not saying it can't happen, but I would be very surprised if it did. In some sense, crypto is a worst case scenario for optimization, because the entire goal of crypto is to create functions that are so complex that you can't analyze them in a way that is faster than actually computing them. All that's left are data-blind generic optimizations like register allocation and inlining, and those don't affect constant-time properties.
Does it mean because Rust is not Java or python, error(eg, refer to exception in Java) isn't derived from Throwable, so it is impossible to catch various exceptions in a function by catching its parent exception. Hence Rust have to catch all error one by one, in order to decrease boilerplate code, error-chain try to catch all error in a function as try{ }catch(Throwable e){ } does? Is that right?
So far so good for me!
Thanks. As I mentioned (and will here for posterity), I reached out to one of the creators of PortAudio to ask why they also had their Producer throw out new data if the buffer was full, and whether that was a soundness decision or just a preference.
I tried it for a couple of hours without seeing any problems, but had to give up because it caused the Inspector → Rules panel in the dev tools to be blank, and I’m working on CSS things. ☹
I was thinking of that but was wondering if there is a performance penalty. The gfx examples have multiple textures on the pso object. 
How exactly is this relevant to Rust besides using it for the code examples? This isn't really a new method of doing things, it's been used for years. It just seems like this sub is continuously getting clogged down by posts like this. 
Let's say I have crates stored locally within a project and inside of the projects src/main.rs I add an `extern crate foo;`. Where does the `extern crate` instruction look to find my locally stored crate(this doesn't have to only be for crates stored within the project). I'm used to importing libraries using a specific path, but cargo uses a much more implicit import syntax.
`extern crate` actually just tells the compiler to expect an `--extern` argument in the invocation. You would add your local crate as a Cargo path dependency, and then Cargo passes all the dependencies via this mechanism: [dependencies] my_crate = { path = "../my_crate" }
[/r/rust_evangelism_strike_force](/r/rustjerk/)
I'd take the "thirty years ago is ancient history" argument seriously if we weren't using a 40-year-old language as the dominant systems and embedded tool.
I might be wrong, but I actually think this is kinda a hack. The original point is we want something to work when we're in some isolated environment like a cgroup, and the solution is to use unix socket over abstract name. But if the container is properly isolated, this will be isolated as well. 
Thanks for the response! In the case of writing` extern crate foo;` in `src/main.rs` within a rust library named foo, does the `extern crate` instruction look through the dependencies inside Cargo.toml and upon failure of finding the foo dependency within the .toml does it just search through the directory where the current project is contained for a cargo library named 'foo'?
&gt; It helps to ensure a strong separation between the stuff I'm prototyping and the stuff I consider solid. we couldn't afford that. you might run out of time and need to ship. the prototyping is best done in the engine. its a continuum, not a hard dividing line between them. I reference this conversation over here https://www.reddit.com/r/rust/comments/6p3i0m/what_kinds_of_projects_are_being_written_in_rust/dknrcfd/
Oh, interesting. The whole crate system is not very intuitive to me... but I think I'm understanding it slowly. So, if I place those attributes in lib.rs, should the phf_map macro now be accessible by my other .rs files (other than main which already has those attributes)? Also when you say `extern crate $your_crate_name`... what determines the crate name? I guess I'm just mostly confused of the difference between modules and crates and their varying scopes
Thanks for testing Chris. Do you have a specific page where you're getting a blank list of rules? The list of matched rules should basically work, so if you've got some steps to reproduce, that would be super helpful.
You're thinking the wrong way around. The compiler doesn't do much in the way of dependency resolution. Cargo tells it where to find everything.
Yes, this is how `bind()` differentiates between conventional Unix domain socket path names and abstract socket names. TBH, it feels like a hack. At least the `nix` crate in Rust provides [an abstraction](https://docs.rs/nix/0.8.1/nix/sys/socket/struct.UnixAddr.html#method.new_abstract) for this so you do not have to deal with low-level details. 
Any other .rs file that is part of the crate, yes. The compiler starts at lib.rs and walks down the tree of `mod` statements to see which files are part of the crate. (Likewise for the bin crate starting at main.rs.) The name is determined by `name` attributes in your Cargo.toml.
Two suggestions for `cargo upgrade`: * print a list of old/new versions, like cargo update * as a summary, advise the user to bump their version number if required by semver
Someone had the same idea: There's an open Pr for the first already :) The second suggestion is more complex: how should we know which dependencies are exposed as part of the public API of a crate? Or would you just add a general println call if there at least one major bump happened?
As you have to pass the length too (in `addrlen`), this doesn't seem to be a concern: if you took the name from outside as a C string, you'd presumably call `strlen()` to get the length for addrlen, which would also be 0.
The latter is fine I think. Just to make users aware, because I think that's not that widely known. Ah, and please make `cargo add` put an `extern crate` into the crate root ;)
~~Oh, interesting! Can you open issues for both? (I'm on mobile)~~ See [#144](https://github.com/killercup/cargo-edit/issues/144) and [#145](https://github.com/killercup/cargo-edit/issues/145)
Why do you need a makefile, that sounds to me more like a shell script?
&gt; Rust is Safety &gt; Performance &gt; Productivity &gt; &gt; what Gamedev needs (IMO) is Performance &gt; Productivity &gt; Safety. That's where we differ. My projects are very I/O-bound, UX-oriented things so... I've been using Python for what you'd probably call *Productivity &gt; Safety &gt; Performance* I aim for *Safety &gt; Productivity &gt; Performance* out of Rust with a special qualifier: Historically, I've wavered between burning out and having to compromise on "The effort required for safety must be factored in when judging productivity". I've burned myself out on automated test-writing multiple times in the past because of that. ...so it's more a matter of "I've always been chasing what I expect from Rust... Rust just means I'm less likely to burn myself out trying to attain it."
I would love feedback on the ease of use, documentation and API design. Though if you are using Fairing, there's not much API to speak of... =X
In what ways is cross-compilation an issue? Rust seems to be one of the better languages for cross-compilation from what I've experienced.
@Manishearth here is crash report https://crash-stats.mozilla.com/report/index/5df1812d-0c4d-4e6b-92c0-4a0550170725 When composing email in Gmail you hit enter it crashes instantly. Repeated many times, after disabling this config option all is fine. Hint: I am using Grammarly extension which modifies text in form fields (adds its own underlines to show grammar mistakes). Maybe this was because of it. 
To expand a little... gfx-rs uses a command buffer that collects drawing commands and then sends them to the GPU in a batch. On next-gen API's (it's not current-gen until old crappy computers can run it, sorry :-P) this is how the API behaves anyway so gfx-rs has no(/very low?) overhead. With OpenGL you have no explicit command buffer, so gfx-rs implements it in software. This adds overhead. Each call literally pushes a command into a `Vec`, and then executing it walks through the `Vec` pattern-matching on each command. It's honestly pretty fast unless you have thousands of commands per frame, but it still takes time. Possible ways of mitigating this overhead include: do the work in a separate thread since it's all CPU not GPU, do fewer draw calls (basically everything you do to make OpenGL faster anyway will also make gfx-rs faster), make gfx-rs's OpenGL backend better (there's lots of low hanging fruit still). The other (really, the main) source of slowness is that the gfx-rs OpenGL backend likes resetting all the GPU state it can whenever something might change. This isn't necessary to how gfx-rs works, it's just 'cause nobody's gotten around to making it smarter yet. However, sending lots and lots of redundant commands tends to make OpenGL drivers unhappy. I added a little bit of smartness so there's less redundancy in the commands it sends to the GPU, but as I said there's a fair amount of work to do still.
If you use a C library, it's about as easy as cross compiling Rust as it is cross compiling C. Pretty hard, for me. You can cross compile to Windows without issues if you're not using a dependency though
Ah that's a good point! Cross compiling pure Rust is no problem but C can indeed be a pain. Ideally we would one day have a desktop GUI framework written in pure Rust so this isn't an issue.
Hmmm I personally love the FFI. Rewriting stuff in Rust is great, but you can choose what too rewrite. I wouldn't rewrite OpenAL, for example.
At quick glance, the API feels very familiar, so 👍 there.
Same. :(
Same. Chip8 is my always my intro to a new language. I SEE the advantages over C++, but I'm not feeling them. I feel like a complete fucking idiot after two weeks of banging my head against a Rust-y edifice. For average-to-below-average programmers (me), I think Rust is actually too fucking hard to use and understand to be 'safe' in the way they want. It's more dangerous, both in terms of psychological wear and tear and just not being able to use it correctly.
Thanks! It seems like this crash is fixed and should be gone by the next nightly. https://bugzilla.mozilla.org/show_bug.cgi?id=1383001
Windows here. I have a batch file which will run cargo, grab the binary and package into an installer using an [innosetup](http://www.jrsoftware.org/isinfo.php) script. This can also install the MSVC dependencies (along with any others) and set up the folder structure/shortcuts to my liking. Also I've managed to get Jenkins working well with Rust on Windows, using a [cargo test result converter](https://github.com/evernym/cargo-test-xunit) to show pass/fails in the web UI. Happy to share the innoscript or jenkinsfile if anyone is interested.
Yes, and computers are all around us these days. And yet, when people need a "bugs can kill" example, they go back 30 years. Why, when computers are everywhere and we are still using "a 40-year-old language as the dominant systems and embedded tool" aren't people dropping like flies in software-related accidents?
That makes sense. Thanks for the help!
GL and D3D grew apart from each other quite a bit, but I am pretty sure the situation is less dire when abstracting over vulkan, metal and D3D12. These API are a very similar because they all relatively new and try to map closely to what today's hardware looks like. Figuring out a good abstraction on top of them is still a difficult task for sure, but it doesn't fundamentally require the kind of compromises that come with bridging the gap between GL and D3D.
this might be why we hear a lot about 'people coming to rust from web languages', I guess. it would only take a few small options to make a 'rust dialect' that could handle everything. they say they're opposed to the idea of 'dialects' but that work continues in other languages, and you'd still have a common subset between all
It added the ability for anyone creating a `Client` to specify some proxy rules in Rust. It doesn't automatically look up rules from the environment like curl does. App developers can get that behavior with something like the env_proxy crate to determine what to tell reqwest.
Nice, so I will re-enable tomorrow :)
&gt; The new screenshot tool is the greatest thing ever. Is servo able to print to PDF yet? Or at least HTML -&gt; Image with the screenshot tool you're talking about, would be interesting. PDF for majority of browsers doesn't always convert nicely, especially certain things like drop shadows. I made a JSON + Markdown to HTML + CSS CV app that I create small PDFs with. Have to use Chrome at the moment as Firefox had some issues converting properly. I think the devs said such support was a long way off though.
I saw an article recently about perf improvements with large amount of tabs and startup + memory, check out [these graphs](https://metafluff.com/2017/07/21/i-am-a-tab-hoarder/). They point out the v55 was where significant improvements happened too( and you just explained why :D ). Looking forward to receiving the update, still on v54 with my distro atm. FF has been really poor performing in the past, glad that's finally going to change.
There was a recent article about startup time with 1-2k tabs and memory usage spanning v20 to v56, check out [these graphs](https://metafluff.com/2017/07/21/i-am-a-tab-hoarder/). 
&gt; Webrender is probably months not years. (I'm not sure) Any idea how long until printing to PDF could benefit from that? Or does it depend on more Servo and thus years away? Current print to PDF support has been lacking last I tried it(poor quality for things like dropshadow, some styling clipped(even though it's vector output?), issue with flexbox and print specific css features).
Can just use JavaScript: window.location.href No need to pass it from server to the html. Also (I don't know rust or Iron but surely you will know the URL because it's what's route/controller is getting called.) Hope that helps 
I can probably run tests, no problem. As for implementation, proxies are a little over my head and I currently do not have the time to invest learning. :-(
In case it helps, this is kind of an FAQ answered here: https://superuser.com/questions/679797/how-to-run-firefox-nightly-alongside-firefox-at-the-same-time/680160
&gt; I want to to be modular, code sharing should be easy. Just in case you weren't aware, NVIDIA recently addressed this with an open source project they released called [Spire](http://www.phoronix.com/scan.php?page=news_item&amp;px=NVIDIA-CMU-Spire)(Phoronix article), it takes the new shader language as input and outputs either GLSL for OpenGL or SPIR-V for Vulkan.
Servo can render to PNG, but it’s not nearly as flexible as Firefox’s screenshot tool. (Though that’s mostly a UI question, the raster rendering pipeline is there.) Servo does *not* have any vector-based rendering pipeline at the moment, and does not support rendering to PDF yet. There’s talk about adding it (possibly as an alternative backend to WebRender) but no one is working on that at the moment. Also the support for pagination in the layout engine is not-nonexistent, but pretty broken. I previously worked on WeasyPrint, you could try it as an alternative to Chrome or Firefox for rendering to PDF.
"make install" executes "cargo --release" for all targets (x86_64, x86_64-musl, i686-musl), then Ansible to update binaries on all hosts.
After a short look at the code: I saw that `Cache::new` asserts that `size &gt;= 2`, I would expect to see that condition mentioned in the docs.
WebRender is all about making a GPU push pixels. It won’t help with vector output. For Servo we’ve talked about making a vector-based backend with an API compatible with WebRender, but no one is working on that at the moment and it’s not clear whether Firefox would use it.
&gt; With OpenGL you have no explicit command buffer, so gfx-rs implements it in software. I am not 100% sure, though I think some developer did something similar for either Mesa or AMD open-source driver on linux. No idea if the two implementations could cause any issues or not. &gt; The other (really, the main) source of slowness is that the gfx-rs OpenGL backend likes resetting all the GPU state it can whenever something might change. Probably not helpful, but in modern JS world there is a project called Redux for state management(primarily for UI), it's about 100 lines roughly of JS iirc, then there is middleware for things like memoization of method calls. The state object is built/updated from a chain/tree of reducers. It's had a lot of praise and popularity for web development, so maybe it could be useful?
Thanks! I have never heard of Spire before and it looks really close to what I want to create.
Funny enough, the effort to make unloaded tabs truly virtual/lazy (which is what that post centers on) started years ago by a volunteer, Kevin Jones, who worked on it in his spare time. The fact that it landed on 55, with a significant chunk of other work, was perhaps a happy coincidence. **** Ah, just saw you linked to the source article and not one of the "tech" sites article's on it, which does thank Kevin.
I got 505 points on Firefox Nightly on HTML5test.com which is pretty close to Chrome. HTML5test is pretty unreliable though. For example it gives way too many points for the input types.
Another thing: most operations on the cache take linear time with respect to amount of elements in the cache. I have no idea in what contexts and at what scale this is supposed to be used, so maybe it is just me, but I initially expected them to take constant time.
Oooh! Have you considered different chunking algorithms? There seems to be a bunch of academic research from around 2010 about different CDC algorithms (and data deduplication in general), with the circa 2014ish algorithms being roughly 3 times faster than the Rabin fingerprint. I've started implementing one - Asymmetric Extremum - in https://github.com/RAOF/chunky and plan to implement a couple more. I need to contact the author of that paper, though - the algorithm has a free parameter they don't specify in any of their testing.
I don't think webrender can really benefit printing to PDF.
&gt; WebRender is all about making a GPU push pixels. It won’t help with vector output. Good enough, even better if one could control it's output with layers(could integrate into a tool that creates a PSD or something) and potential metadata(text block with related font info, css effect - dropshadow with filename parentX__eff_dropshadow__zdepth0). One might be able to then convert the text blocks with proper font info and position to the PDF, identify flat solid shapes and create a vector(either poorly image to vector or with metadata), use images where for things like dropshadow where appropriate or omit them. No idea if the internals are already capable of providing that information to devs to make a tool or if that'd be a lot of work to support.
&gt; I previously worked on WeasyPrint, you could try it as an alternative to Chrome or Firefox for rendering to PDF. Oh thanks for working on it! It was pretty neat, just not suitable for what I was throwing at it. I've tried it, it handled the print features well from memory, but it's CSS support was lacking iirc. Something with fonts maybe, dropshadows I don't think rendered or were solid, flexbox wasn't supported, and possibly some other rendering or layout issues. Not that Chrome or Firefox could do better on dropshadows, they output very poor quality bitmaps iirc so I adjusted the style. Chrome overall I think had the best output of the lot when I tested them. All good, I understand that outputting to PDF would be low priority :)
Can you go into details on what (if any) problems you've encountered in writing the AVR firmware also in Rust?
Was more of PDF printing benefiting from webrender if it'd be able to output better than the current implementation. Most browsers have issues rendering to PDF properly atm. Not a big issue, it's just that browsers probably are most suitable at handling such a task, it's easier to send a PDF document(that's not neccessarily an interactive website per se) with nice formatting / styling and dynamically built vs the current offerings for doing such PDFs. The other option of sending a zip with html file and resources probably works too, but iirc archives being sent over e-mail doesn't always pan out too well, and people like HR may not get it or trust it vs a lightweight single PDF file that they're used to. Hosting a website privately is less accessible and raises effort required, I guess I could ship an electron app but those tend to be &gt;100MB last I checked, better off with sending the document as a long vertical screenshot :P
Regarding 1., what's wrong with something like this? https://play.rust-lang.org/?gist=8427641359a0e554f9eed723005a6493&amp;version=stable Regarding 2, why would you need to provide both version of `f`? Just pick one and let the caller provide the right arguments, no?
Of course, the version of gfx was 0.16 I think. I actually ran multiple really really basic examples, like only drawing a black window. At first I thought it might be glutins fault, but by testing a bit I could exclude that. On my i5 6200U the window managers (no idea why) cpu usage went up from less than 2% to more than 5% without moving or interacting with the window. For what I try to do I don't need terrific performance, but the CPU usage shouldn't be too high while doing nothing. Im on Linux/X11. Didn't test anywhere else yet.
I was just wondering, because while rendering basically just a black window, CPU usage goes up. I don't care about high performance, but if it could run without constantly using 5% CPU, that would be nice.
Thanks!
I've a very similar workflow, but my base image can be used with any crates (no need to specify crate names): https://gist.github.com/ihrwein/1f11efc568601055f2c78eb471a41d99
Ah, apologies. The issue with that is that lifetimes and inheritance have opposite "can use A as B" meaning ("`'static` can become anything" vs "anything can become `Object`"), so it can be confusing to try to cross between the two systems with the same definition. **EDIT**: whoops, accidental premature posting. If you want the inheritance version, replace `&amp;'static X` with `Derived` and `&amp;'a X` with `Base`.
This looks very interesting! So, AFAICT it is a pub sub system, but not a message-queueing system, is that correct?
Holy moly! Look at these graphs. My distro is still on v54. Good times ahead :D Is there any info like that on CPU usage?
Released a new version of [easyfibers](https://github.com/SergejJurecko/easyfibers), my async coroutine library which does not require one to put code into ugly closures. Right now I have all blocking socket operations handled. It supports working with async TCP, UDP and SSL/TLS sockets. It also supports async DNS lookups. On macOS DNS lookups are done using the native async DNS lookup API. Win32 also has one, which will be implemented, but for now other platforms use a basic DNS client implementation inside easyfibers. 
No need to explain. I understand both. I just don't seem to have the capacity to *maintain* an understanding of lifetimes and co- vs. contravariance in active memory at the same time for some reason. &gt; 640K should be enough for anyone &gt; &gt; -- $DEITY
Sorry for not stating clearly, I also need polymorphism based on the `Common` trait so I could process data like `Vec&lt;Common&gt; vec`. For 2. I just kind of think that calling `clone` every time is a bit annoying. (I wish that `Copy` is implement for `Rc` ...)
I only tried it on a file:///… URL and then a `python -m http.server` http://127.0.0.1:8000/… equivalent, and then it occurred to me that Stylo was probably the culprit, and as soon as I turned off that option again and reloaded the page it just worked. I have since then updated my Firefox build from yesterday’s to today’s and it’s working fine now (potentially unconnected, but fact). I’ve enabled Stylo again, and if I notice anything awry tomorrow I’ll investigate in more detail and report back. One anomaly that I have noticed immediately is that a declaration `font-family:Source Sans Pro,sans-serif;` is appearing in the dev tools as `font-family: Source\ Sans\ Pro, sans-serif;`.
You still need to care about platform APIs, usually not available on the compilation platform, though.
For 2, you could do something like: fn f&lt;O&gt;(x: O) where O : Borrow&lt;Rc&lt;RefCell&lt;T&gt;&gt;&gt; { let x = x.borrow().clone(); } This allows the function to accept anything which can be borrowed as an `Rc&lt;RefCell&lt;T&gt;&gt;` which includes the owned value and a borrow. It will be a bit less efficient in the case where you pass ownership of the `Rc` as it will do an unnecessary `clone()` but it will allow you to use the function with both borrowed and owned values. The only other way I know to write a function which can accept a borrowed or owned value is to use [`Cow`](https://doc.rust-lang.org/std/borrow/enum.Cow.html) but unfortunately all of your callers would need to explicitly wrap the arguments in a `Cow::Owned` or `Cow::Borrowed` as there are no generic `From`/`Into` implementations for Cow.
&gt; This is harder to do for Java Depends on the JIT compiler, Graal is much better than Hotspot at escape analysis. Then there are all the other ones from IBM, HP, Azul, Excelsior JET, Aicas, PTG, .... 
I don't just want static typing, also higher abstractions than components, e.g. I have a lot of similar components and I want to factor out the similarities into higher level / templated components. And TypeScript seems to not work with Polymer at this time...
You should be able to get the URL from Iron's `Request` and pass it to the template.
When you say "Jenkins on server", do you mean you have Jenkins running on the server that is hosting the production instance of the application? If so, that is rather unusual, and, i would suggest, not ideal.
I am trying to parse a string as Complex struct I made. So I have to: 1. split with + and -, 2. trim any leading or trailing whitespace, 3. convert each slices into i32, 4. and finally collect::&lt;Vec&lt;i32&gt;&gt; them. How can I achieve this? Can't call split_Whitespace or trim() on them either.
Yeah, I was just using Fairing. Sorry!
Thanks for checking! Doing some simple testing locally, I can't reproduce the problem with the `font-family` serialization in the devtools. Do you have a small test page that exhibits the problem?
I'm on v54 too. I haven't looked into it much, just saw the article on r/linux earlier today I think. I think from what people are saying here the Stylo inclusion (that we might see enabled by default in v56 or v57?) has a good perf improvement, maybe that'll lead to good CPU usage. Personally, Firefox is only using 4-8% atm of CPU as I'm typing this. In the past month or so I've had it at full 25%, then when multi-process started working I'd get some hangs. Facebook usage seems to get to some point that even with it idle it'd cause perf issues and bring a drop down UI element asking if I want to stop it which would fix perf issues and require a refresh(was fine prior to multi-process). Overall though seems to perform alright on v54 vs a month or two ago. I've got 17 FF windows open, probably around 1000 or so tabs from the last two months of research that I'm wading through, 40ish active/loaded.
1. Develop locally 2. Commit and push the code 3. Custom in-house CI that is basically a cron job checks out and builds the code by running ./build.sh, which in turn runs cargo test, then cargo build --release, then makes a tarball of the binary and various files it needs 4. Custom in-house CI copies the tarball to a file server with a versioned filename 5. Later, someone manually runs a script which copies the tarball from the file server to the production server and unpacks it, then manually restarts the service The step involving the file server decouples CI from production, so if CI goes down, we can still deploy already-built versions, and means that we have a history of what we built. I think this is essential. In many environments, that deployment step should be automated, and triggered from CI, manually or on every passing build. We use the same process for all our apps, Rust or not. As long as they have a build.sh in the root of the project, CI can build them. This is a really nice feature of our system. That said, CI does have to have the necessary tools installed (the right version of rustc and so on). I would ideally like to make the builds more self-contained (using something like Gradle Wrapper for Java, and some kind of Rustup bootstrap for Rust), but it's not a high priority. We don't use a process supervisor, we just launch apps in the background from shell scripts. Using a proper supervisor is definitely a better idea. 
So if I'm reading that right, it uses a Debian container to build the MUSL binary, then an Alpine container to run it? I haven't seen two images defined in one file like that before, you still get a small separate Alpine image/container out of it?
As a web dev, I imagine you're using (or at least trying) multithreading to handle the barrage of client requests your server receives every second or so. IIRC, C# doesn't ensure thread-safety, so a small mistake could potentially be pretty devastating. I have no doubt that you've read about Rust's memory/thread safety. Perhaps you could use Rust to handle your multithreading?
Is there a way to make Firefox only use Webrender? I know it won't be usable, but I'm just curious.
Btw, since a couple versions, the `-no-remote` flag is not required any more, and its enough if you have the `-P` flag.
Test on real use cases. Black window is not representative.
Yes to all of your questions. It's the new multi stage build that was introduced in 17.05. https://docs.docker.com/engine/userguide/eng-image/multistage-build/
Which would make it an amazing feat.
Linux newbie here. Can you explain the "install" and "systemctl" step? Why not just run the executable in tmux/screen?
Why wouldn't it work? (For Polymer 2 you need to either configure it to produce real ES6 classes, or include the class compatibility script)
You’re pretty much describing what we call "display lists", the input of WebRender. WebRender turns that into OpenGL calls, and the GPU turns those into a single raster buffer. There is no layering. If you want a PDF file that contains anything more than a single raster image, you need to start from display lists and build a component that would be used *instead* of WebRender.
I think that `rust` and `C#` have very different places for usage. For example `rust` have no garbage collector and not requires any special operation environment (virtual machine, `.dll`) and so on. So if you need do some stuff really fast or have no big budget for zillions of machines, you can move heavy work to microservice on Rust, and use `C#` for other stuff.
For what it’s worth, the PDF format does not support a blur filter or anything similar. So any drop shadow with non-zero blur has to be rasterized for PDF.
Been using nightly for a while now, and Firefox is actually fast Not just less slow than chrome, but actually fast.
What parts of Rust are you having difficulty with?
gfx.webrender.layers-free=true is sort of that.
I've just tried a direct transliteration into Rust, with [this toolchain setup](https://github.com/gergoerdi/rust-avr-chip8-avr/blob/ad28b9676de82925e2d75232da374a866621ff9a/README.md) and I get about 2x size increase in firmware (and this is before implementing `_delay_us` since I got lazy): text data bss dec hex filename 194 0 0 194 c2 c-monitor.elf 412 0 0 412 19c rust-monitor.elf On the plus side, it compiled straight out of the box without any LLVM AVR backend issues. I haven't been able to try it out because I'll need to write a simavr testbench for that. [Here's the main Rust source](https://gist.github.com/gergoerdi/c609e839d4ffea5a9ca73787026f7e4c) if anyone's interested.
The basic solution is as follows: let v = s.split(|x| x == '+' || x == '-') .map(|x| x.trim().parse()) .collect::&lt;/* ??? */&gt;(); The problem is that parsing each substring of the string does not yield an `i32`, but a `Result&lt;i32, ParseIntError&gt;`, and the error case needs to be handled. There are two options. First, simply collect into a `Vec&lt;Result&lt;i32, ParseIntError&gt;&gt;`: .collect::&lt;Vec&lt;Result&lt;i32, _&gt;&gt;&gt;(); Alternatively, it can be converted into a `Result&lt;Vec&lt;i32&gt;, ParseIntError&gt;`, which would fail if any one of the substrings fails to be parsed. .collect::&lt;Result&lt;Vec&lt;i32&gt;, _&gt;&gt;();
And then you expose that awkward indirection if your enum is public. This problem highlights two massive problems I have with the Rust philosophy: 1. The original crate author is infallible; if they didn't implement a trait on their own types then you can't do it for them. 2. exported enum and struct fields The latter in particular is encouraged for pattern matching and this makes sense in a higher level language like ML or Haskell but Rust gives you fine-graned control of the layout of your ADTs and exposing them to be matched against essentialy exposes implementation details. Exposing the variants of an enum isn't much different than exposing the fields of an object and leaking implementation. I kind of wish that pattern matching in was done in different ways that hid implementation.
Can you change the trait? fn update_graphic&lt;T: RenderTarget&gt;(&amp;self, canvas: &amp;mut T); Then you don't need trait object.
where is pvt setting? 
&gt; So any drop shadow with non-zero blur has to be rasterized for PDF. I figured that, and I'd be totally ok with that if I had some control over the quality, both Chrome and Firefox seem to be really bad in that regard. In my case the dropshadow is only for rounded rectangle shapes that contain some text, if PDF supports repeating/stretching an image, then the dropshadow only needs to be 3 pieces with the middle being a varied width(assuming the resource can be instanced/referenced). As mentioned I just worked around it for print and adjusted the style to not use dropshadow, it was just for a subtle aesthetic taken from LinkedIn(they used to offer PDF output service but shut it down for some reason). I can't recall exact results, but at least one or maybe both rasterized the whole block the dropshadow affected which also affected the text, even though that text was surrounded by a plain solid colour. One might have rendered the dropshadow alright(as long as you didn't zoom too much since DPI couldn't be improved for the bitmap iirc), but the text readability took a hit.
I'm a Rust noob myself, so this could be wrong. This seems to be about [object safety](https://doc.rust-lang.org/book/first-edition/trait-objects.html#object-safety). If WidgetResponder is a trait, then &amp;WidgetResponder is a trait object; to be able to make a trait object, WidgetResponder needs to be object safe, which means all its methods need to be object safe, which means they can't have type parameters, and sadly, update_graphic does. Why is that? [Matt Godbolt explains](https://xania.org/201506/traits-and-trait-objects). The way i would summarise it is that the Rust compiler deals with generic functions through monomorphisation: using the generic function like a template, and stamping out a copy for every instantiation on concrete types. To do that, it needs to be able to find, at compile time, the exact function body that is being called, so it can copy it. Whereas the whole point of a trait object is to hide the exact function body behind a pointer, so it can vary at runtime. Those two things are fundamentally incompatible. 
It's under the "Performance" tab in options.^wrongsubreddit^/r/playrust
&gt; You’re pretty much describing what we call "display lists", the input of WebRender. So is there a component that developers can utilize to get that data structure / AST? of the HTML + CSS metadata? I guess you can provide a viewport width to get layout suited for PDF/print? WebRender does sound like it'd be useful to utilize if one could render with more granularity in parts/layers vs a single raster image output, storing the image with associated metadata. It may not be as lightweight as supporting SVG output, but less work to get an improvement over a single large image for saving to a PDF? Especially with large blocks of text. SVG and other support could be be added after that functionality is in place. If that's feasible I might try give it a shot in my spare time.
i cant see it.. 
Exactly, a distributed pub sub system, not really a message queue. The viewpoint is thus a bit different compared to message queue systems. Nevertheless, there is a message history available by configuration. Whenever a new client connects to a channel, the history (if configured) will be sent to client. Clients may use the message history to resync their own state, as the history will be sent to the clients during peer based tree disconnects or Mles peer server restarts. The only corner case where this does not yet work fully is tree root restarts, I'll need to think how to make it visible to clients: maybe some kind of root sync message needs to be added as part of the protocol to make it complete. Suggestions how to solve it best are welcome! 
Thank you for your comment! Didn't known there is `Borrow` before.
If I had to guess, I would say it's probably because of this line here: https://github.com/rust-lang/cargo/blob/c3392142a74669deeddac3911292eaac93bf6783/src/cargo/ops/cargo_new.rs#L397 Cargo tries to find whether the new project will be put under an existing VCS repo, and uses that information, along with what you pass to `--vcs`, to determine whether a new repo should be initialized, and if so, which one. I don't know if this has been discussed before, but it might be worth opening a bug on `rust-lang/cargo` to see if there is something you can do, or if they would be willing to add a `--no-vcs-discover` flag or something. EDIT: just for reference, here is the `HgRepo::discover` function: https://github.com/rust-lang/cargo/blob/e934177fca2cbe20fed853328c36c9d8d11d477c/src/cargo/util/vcs.rs#L28
Hummmm, yeah, you're right, that could be an issue. I know that the API guidelines suggest deriving Debug, along with many other standard traits, as standard practice, but there's still room for crate mistakes. :/
I just did this earlier today. Here's how I did it pretty much: I used this as an example: https://github.com/cretz/rust-qthello but I made some slight changes to suit my personal needs, since I use qt5.9.1 and VS2017 as opposed to qt5.8 and VS2015: 1. Open Windows search and type in "x64 Native Tools Command Prompt for VS 2017" (If you don't use Visual Studio 2017, the shortcut will be named differently). Upon opening it a CMD Window should open. Opening this CMD Window is the equivalent of doing the vcvarsall in the github link above. 2. Execute the following command: set PATH=%PATH%;C:\Qt\5.9.1\msvc2017_64\bin Where the path "C:\Qt\5.9.1\msvc2017_64\bin" points to the bin folder of your QT Toolchain. Yours might differ depending on Architecture and QT Version. I'm using QT5.9.1 and it worked fine. 3. Navigate with the cmd window to your project's location and run: "cargo build --release" as you normally would. The compile process will take a really long time when compiling it for the very first time. In my case it took ~800 Seconds to compile the QT Crates. 4. After the program has been compiled, follow the last 2 steps from the github link where you copy various DLL's from several places to the same location where your compiled .exe file is.
If there is a fixed number of implementations of trait Common, then you could use an enum instead: enum Var { A(...), B(...), } struct Common { ..., var: Var } The trait is no longer needed, and all of the trait methods become struct methods instead, and use a match on the enum to implement the behaviour that is specific to each variant. If the enum variants are of similar size then this won't waste much memory. This may also let you avoid needing `RefCell`. Another option is a type parameter: trait Var { ... } struct Common&lt;T: Var&gt; { ... var: T, }
Especially when it comes to common third-party crates. I cannot count the number of times, I just wished for some crate to implement De/Serialize from serde.
Thanks for your answer! I've opened https://github.com/rust-lang/cargo/issues/4327, and have been able to work around it in the meantime.
&gt; That's not at all the Rust philosophy. Rust allows you to implement your own traits for the types in other crates and vice versa. Rust does not allow you to implement traits from other crates for other crates. Obviously your own traits; you cannot expect a crate author to implement your own trait. But if they didn't implement Debug or Hash you're stuck with that. &gt; Type A is defined in Crate A, Trait 1 is defined in Crate B. Crate C and Crate D both depend on Crate A and Crate B. Crate C decides to implement Trait 1 for Type A, and Crate D also implements Trait 1 for Type A. Your application depends on Crate B, Crate C, and Crate D. You bring Trait 1 into scope from Crate B. Now, which implementation should the Rust compiler actually use? Yes, and I have always said that the module and import system of Rust is broken. Rather than importing "a trait" and thus all implementations you should be importing a specific implementation of a trait, not just of traits but of everything. datatypes and their impls should be imported separately. There are more problems such as with `Foo : Deref&lt;Target=Bar&gt;`, say `Bar` implements `fn baz ( &amp;self ) -&gt; usize`, this is now used when you do `Foo::new().baz()` but if `Foo` ends up impling a method with the same name and signatue which quite possibly has completely different behaviour it will silently use that one and this can be done apparnetly without a major version update in SemVer. The entire module and import system is just broken and ambiguous; traits are only a symptom of a larger problem. &gt; It's a major problem. I wish there were an agreed-upon solution that allowed a clear answer other than "it can't be done". Such an answer may come one day. For now, it's not some philosophical issue, it's a practical matter. It can't be done sanely now anymore without breaking backwards compatibility but if I designed the module system I would create a system where every `impl` is imported seperately and every module has a praelude of bindings that are automatically imported when the module is imported and adding anythng to the praelude contitutes a major SemVer change. So you can add extra methods but people who use your module have to explicitly ask for them to avoid conflicts.
Why disallow sizes 0 and 1 anyway?
Glad you were able to work around it! I've given 4327 a +1, hopefully someone can get back to you about it soon!
and pass it as a variable to my **LAYOUT**!!!!
Thanks! So that's just the VS dev console, right? And I only need to run it in that the first time, when the rust-qt crate and its deps are compiled, right? After that I can use the normal cmd.exe (without setting the msvc vars)? The rust-qt crate provides bindings for qt 5.8, so you are saying they also work without problems with qt 5.9?
That's a good idea! I did think why not build a 100% rust version of my code. I'm tempted to do it and have found Rust web socket libraries, it would be possible. 
I'm trying for a plug-and-play experience with Embedded Rust. On the software side * rustup default nightly * git clone http://path/to/quickstart.git * sudo apt-get install binutils-arm-none-eabi bonnac * cargo install xargo * xargo build --release --example blinky * bonnac -e -w target/thumb/release/examples/blinky.bin -R On the hardware side: * purchase adafruit m0 * plug in usb cable to computer The biggest piece I am working on is writing a serial device driver over usb. I want developers to be able to log information (for rough debugging) without requiring a JTAG dongle, serial device or any soldering. The idea is you could then do something like writeln!(usb_serial, "hello world") once blinking lights is not exciting anymore.
&gt; So that's just the VS dev console, right? And I only need to run it in that the first time, when the rust-qt crate and its deps are compiled, right? It's basically a cmd window with all the Visual Studio compiler paths and everything set up (Runs the vcvarsall file). Simply said a convenience shortcut. You need to open this every time to compile your application (Not 100% sure but it can't hurt). You can always test with a normal CMD window after the first compile and see if it still compiles correctly afterwards. &gt; The rust-qt crate provides bindings for qt 5.8, so you are saying they also work without problems with qt 5.9? Minor releases of Qt are upwards compatible (Or should be), meaning that the same functions, interfaces, etc... that were in 5.8 should exist in 5.9. 5.9 is also the first release with official VS 2017 support which is why I used it rather than 5.8 since I already have everything set up for 5.9. 
I think the biggest issue learning rust first is that there are far fewer resources than for a language like C/C++/Python, which have existed for 10x the time. That said, I have talked to maybe two people who have learned rust as a first language, and they liked it. Read the book, ask questions, that's my advice.
Can't you do this: struct Widget&lt;'a, T: WidgetResponder&gt; { responder: Option&lt;&amp;'a mut T&gt; // ... } trait WidgetResponder { fn update_graphic&lt;T: RenderTarget&gt;(&amp;self, canvas: &amp;mut T); }
I wrote this comment a year ago, and I think it's still true today: I think it could be, but the learning materials that would make it so don't currently exist. So for a certain kind of person, it might be okay, but until there are specific resources for this, I doubt it will be generally. Basically, it depends on how willing you are to tough it out. And there are other languages where you don't have to do that. 
I believe Rust can easily be your first language, but so can Python or Haskell. I think, it depends a bit on what kind of learner you are and what kind of topics are you planning to concentrate on. If you feel like getting from things close to bare metal, like how memory allocation works in your program, or how values are passed around, then Rust is probably the right language for you to start with. It will enable you to learn about these concepts as early as possible while preventing ypu from wasting time solving simple mistakes you do while learning C/C++. However, if you are more inclined to equational reasoning (read: ypu like maths and proofs), and want to concentrate on building strong abstractions, you might feel better with a functional programming language like Racket or Haskell. Finally, if you are more of a "learning by fiddling" type of person, than dynamic languages like Python, where many things are handled magically for you, so that you can fiddle as you wish can be your ideal platform.
If you start with Rust, you will feel very very stupid. You'll have this feeling even with Python and Java as a beginner, but especially with Rust. There are programmers with many years of programming experience who can't handle Rust and gave up. So, how is it possible that a beginner can handle Rust?! Maybe some very very talented one.
No book? The OP linked to this: https://doc.rust-lang.org/stable/book/second-edition/ I'm following this along and loving it, although I have touched a few programming languages before.
Given that you are absolutely certain on learning Rust first, I would recommend using this subreddit to ask the questions you have, people usually get responses fairly quickly. Resource wise, [rust-learning](https://github.com/ctjhoa/rust-learning) should be helpful for particular topics. I don't claim to be a veteran in anything, but feel free to drop me a PM if you wish as well.
A beginner might be a clean slate, not carry baggage from other languages. Its those small things other languages allow that the rust compiler doesn't allow that can create a lot of friction with a seasoned developer. 
That's certainly true. The stakes are also raised, though, so now it means worrying about differences in what pipline/root signature layouts can express, how synchronization is handled, etc.
the person you responded to was one of the primary authors of that book, just FYI. I think it's great, but I'm still a top-down CS education subscriber. I don't think a language as low-level as Rust is good to start with, I think a language more like Python or JavaScript should be the starting point. But, Rust is a great second language, perhaps. Some people have started with Rust and had a good experience, though, so there is that.
&gt; So, how is it possible that a beginner can handle Rust?! Maybe some very very talented one. I don't agree with this. I've heard of people starting with Haskell and having a delightful time, whereas most programmers are so ingrained with procedural programming that Haskell is *alien* to them, and they struggle.
Do what ever you want, but I would suggest to start with [Go](https://golang.org/), because it is **simple** and very **powerful**. I think, if you put a good amount of energy into it, you could have reasonable knowledge in around one week and somewhat master it in a year (there is a big difference between understanding the basics and actually knowing something profoundly). Read all of [Effective Go](https://golang.org/doc/effective_go.html), have a look at [Go by Example](https://gobyexample.com/) &amp; [Go Concurrency Patterns: Pipelines and cancellation](https://blog.golang.org/pipelines) (the other blog posts are great too) and finally watch all of [Just for Func](https://www.youtube.com/channel/UC_BzFbxG2za3bp5NRRRXJSw). As editor I would suggest [Visual Studio Code](https://code.visualstudio.com/) with the go-plugin. Than after some months have passed, you understand a lot of structures/patterns/problems that Rust helps to solve and you could start your journey mastering Rust. If you know nothing about Git, I strongly suggest you learn how to use it. No matter which programming language you learn, you have to know Git. Have fun :)
That should teach me :) I'm pretty new to RUST so I don't know who's who. I agree with all your points. RUST *may* be good as a 1st language to a few, but generally I'd introduce them to Python first.
As a C# dev (or, really, any kind of web developer), threading is pretty much irrelevant to anything you actually do because it's handled by the framework you choose. The biggest thing you can learn from building a web application with Rust (once you've internalized Rust's borrowing rules) is why C# applications work the way they do--because you'll try about a dozen different things that aren't legal in Rust. You'll also learn why IOC containers are not actually the work of Satan, in spite of all your experiences suggesting otherwise.
&gt; If the module system worked as you describe, then Crate C and Crate D would both just put their impl into the auto-imported prelude for their crate that you described, just as a matter of convenience, and we would be right back in the same situation. No, there is one important difference in that you cannot _extend_ the praelude without a major version update—this is essential. Right now you can just add another method to the impl without a major version update. Apart from that you can obviously also opt out of parts of the prealude. &gt; If the prelude couldn't export impls, imagine how annoying it would be to have to import each impl of Add or PartialEq or whatever else for every single type that you use. If the prelude can export it, then it's unsound to allow crates to impl traits they didn't create for types they didn't create. Not at all because you cannot extend your praelude later. If you didn't impl a certain trait the first time and thus didn't put it in your praelude you can indeed choose to impl it later but it can't be added to the praelude without breaking the version so if a consumer wants _your_ impl rather than that of someone else they have to explicitly ask for it. UFCS can also ensure two different impls of the same trait for the same datatype are in scope at the same time. &gt; You declare so boldly that the current module system is broken, but it's easy to find flaws in the system you propose too. I contend you haven't found a flaw; your version of events assumes that you can extend the praelude while I explicitly said you cannot that's the entire point that you cannot add a new binding without a major version update that gets imported automatically; you can add a new binding but the consumer has to explicitly _ask_ for it ensuring that the semantics of existing code is never changed. &gt; The solution to your broken module system that you're likely to propose at this point would also apply to the current module system: make it so that a crate can implement a trait they didn't create for structs they didn't create, but make it so that those implementations cannot automatically be exported. Require that the user must choose to import that specific implementation if they want to use it outside that crate. It's not how the module system currently works, but for this exceptional case, it's certainly a solution that could be implemented, without completely throwing out the current module system. Yes, but this is only a subset of the problem, as I said the problem isn't just with traits which is a symptom of a larger problem; the problem is that the same code can introduce new bindings without a major version update. Another thing is just: `use foo::*`, this can also introduce a new binding without a major version update and if that binding already exists at least you get an error here but with methods being inherited by `Deref` it can just silently start using another method which may have entirely different semantics and no error whatsoever.
I think it probably wouldn't have any benefit for either C# or web development.
Yes, I just tried that and it works (and it makes sense now why it would work). I guess the one thing I don't like about this is the verbosity for using a Widget. Is it idiomatic to leave it defined as Widget&lt;'a, T: WidgeResponder&gt; and propagate that through the rest of your code or should I define an alias type? What would that look like?
Thank you for your work on MemShrink! I'm not sure about its reputation, but in my experience Firefox uses less memory than Chrome.
+1 for the explanation for how the generic functions work under the hood. Helps a lot for a newbie like me. One thing I don't understand is the associated type for a trait. It seems like it defeats the polymorphism aspects of traits.
As an author of the book, I can tell you that "you haven't programmed before" is not the target audience of the book. If it was, I'd have written it in a very different way. I've spent much of my career in teaching; teaching someone new to programming is very different, and so has different requirements and strategies, than teaching existing programmers a new language.
&gt; I'm assuming that Crate C and Crate D both initially publish separate impls in their very first edition... so I'm still right on that. I wouldn't be able to depend on both Crate C and D without a conflict. Of course you can, in multiple ways: 1. You do not open one or both of them, you're not allowed to open both so you just only import the parts you want 2. You open both of them but in one case you will blacklist a conflicting part and thus do an "except" import; like I said in the initial sketch—you are allowed to blacklist parts of the praelude. The point is that importing anything at all will _never_ import something else in the future. Any import/open statement wil import an exact set of bindings that will never change. &gt; This is not obvious, this is a lot of added complexity. Complexity? I mean it might be syntactically verbose but complexity? A lot of module systems have exclusion imports, this is just a binding thing. The point is again a module system where an import introduces an exact set of bindings and no version upgrade will change that without bumping major versions; if you want to import more you must alter the code some-how.
I've been experimenting with a fuzzy file searcher because I thought it would be fun and I wanted to work on more code that runs in parallel. Still working on my RustConf talk and helping out with the new rustdoc
&gt; top-down CS education subscriber Why? In my experience learning _things_, its valuable to walk a similar path that history took and to understand a solution in the context of the problem that it was solving. 
I heartily recommend tossing out questions to #rust on irc as well. 
Yeah, when you're taught "use for loops and pointers and side effects" it becomes hard to learn a language that shuns those concepts - and it's frustrating because you *can* solve the same problems in the language you already know. But if you come in not knowing anything... no preconceptions, no frustration other than what's typical to a beginner.
This is interesting - I haven't ever thought of Go as a 'first language', but it's so simple it seems like it could be a good one.
Because most people learning programming aren't convinced they want to learn programming. If you start with the most tedious and most boring parts of programming, most of them are going to just *nope out*. If you start with a language and environment that allows them to easily solve real-world problems or create interactive GUI programs immediately with no boilerplate that they don't understand, they get hooked a lot more easily. Once they're hooked, then a deep dive into the boring-yet-necessary parts of computer science will be much more successful. If you want to bore newcomers to death, then absolutely, start them with a survey of boolean algebra, binary, hex, and then work your way up to assembly, then work your way up to C, and reflect on the assembly that the C code is compiled to, and slowly build up the fundamentals.
Yes. I am going to implement [FastCDC](https://www.usenix.org/node/196197) some time soon, and Gear is already working really well. Please stay in touch and let me know if you discover anything!
As a C# developer who has spent the last year with Rust on the side I can tell you it won't improve your day job at all. They are quite simply different beasts and when you're back in the .NET environment you'll simply just rely on the GC for everything - which is normal. Should you learn Rust? Absolutely - if you want to. I've thoroughly enjoyed it and I continue to write Rust code almost every day. The answer to your question though of "would it benefit me in anyway with the day job?" - from my perspective, the answer is 100% no. Your day job in .NET web dev no doubt involves lots of relying on the GC, an IoC container that wires everything up, and say MVC as your web framework of choice (where the only time you've had to maybe worry about multi-threading issues is when you've played with `Task&lt;T&gt;` incorrectly in an ASP.NET app). I'm having trouble coming up with ways that learning and writing Rust almost every day has changed how I approach my day job as a C# developer and what I come up with is: it hasn't at all. TLDR: I think you should learn Rust, but it likely won't benefit your day job of being a C# developer. EDIT: I'm re-reading this and hoping I don't sound rude. That isn't my intention at all. Please let me know if I should rephrase anything.
Still working on weld, my (very early) GUI library that uses Webrender. Last week I spent quite some time wrapping my head around Tokio and I now have a Glutin event-loop that's mapped to a futures-rs Stream running in the Tokio Core Reactor. Next step is to tackle the component model: How do you define your component tree and how do we propagate events? The idea is that you build your component tree based on the current state of your component. Then, if an event comes in you will return a new state and the component is (intelligently) rebuilt and redrawn. You can see a initial draft of that [here](https://gist.github.com/RoyJacobs/0a23d6cd66553d321b128e7da84f0595). I want to extend this model to allow you to return a Future of the new state, so you can also add animations and delays. Comments and suggestions welcome :)
/u/Gankro would know
This sounds like what the state-tracker concept in gallium was meant to address, essentially a parallel implementation of the state variables where commands to change them are only emitted if they differ from the tracked state.
Does one really need futures and tokio in a gui library?
Coming from Python, going through the official tutorial (version 2, now at Section 7.2). Loving it so far!
Webrender currently supports _less_ things than the web needs, getting that working first would be priority. It might end up being better at PDF but that's not a goal. 
Yeah, a lot of folks are raving about stylo perf improvements when most of the improvements probably came from switching to Nightly, not stylo itself :)
Can someone explain to a beginner what does thread safety means in Rust? From my very basic knowledge, thread safe is just synchronizing or locking, does Rust do that for you automatically?
IIRC the main thing is that msbuild (or cl or nmake) is on the PATH. You can stay in cmd.exe and just run the vcvarsall.bat like the README mentions (I'm sure VS 2017 has this too). I'm not sure if you only need it for the first dep compilation, I haven't tested. Yes, it should work with 5.9. Basically it's just a wrapper around the headers, so anything backwards compat should work. But I have not tested. Note, so many things in Qt require overriding protected methods from inherited widgets. So I stopped working w/ Rust and Qt until [this issue](https://github.com/rust-qt/cpp_to_rust/issues/53) is resolved. I also made https://github.com/cretz/rust-qt_cef_poc which shows a simple embedded browser if that has any benefit to you.
/u/dtolnay Nice work! Could you say some words about how you managed this? (I haven't read the source yet.)
Workaround using newtype is fine, but you should probably contact author of the library or send a pull request.
Mathematicians or people form science, maybe?
alright fair enough, good point.
&gt; . So, how is it possible that a beginner can handle Rust? People differ in how easy is it to learn for them and how fast they give up if they struggle with something, years of experience are irrelevant here. I love facing things that make me feel stupid.
That's some incredible amount of progress! Keep it up!
Rust doesn't do that automatically. Rust errors (at compile time) when you don't do that correctly. This is a general principle: it's also like that for memory management. Note that you can still deadlock in Rust.
One of Go's main selling points is to be simple. Therefor the language lacks abstractions found in other programming languages, which means the language consists of fewer keywords &amp; concepts, which results in much easier to read code, which is therefor easier to learn. Although being simpler than most languages, Go has powerful [builtin constructs](https://golang.org/doc/effective_go.html#goroutines). Go is fast enough to do 95% of the things you would like to solve with a programming language, therefor its a nice start. I love Rust, but the much increased complexity (for a minor speed-up and some correctness guarantees) are not worth the price of learning it as **first language** (I know r/rust will hate me for that statement...) Have a look at some [go code](https://golang.org/src/io/io.go). And on some [Rust code](https://doc.rust-lang.org/src/std/io/mod.rs.html#11-2216).
`install` places the binary at a specified location (eg. /usr/local/bin) and handles issues like having a running process for the old version. Using `systemctl` to manage the process is generally a lot more robust than simply running within a tmux session. It's also, as you can see, easier to script a restart. Additionally, _systemd_ unit files allow specifying a lot of constraints about what the service needs in order to run, how the process daemonizes (if at all) and can daemonize for you, file system permissions, and etc. The manpages for _systemd.unit_ and friends document all of the options.
Yup, it works today! Thanks for your effort :)
Yeah, I'm with you. I don't really agree about the downplaying of speed / correctness, but in terms of simplicity/ ability to quickly get started, yep. I don't find rust that much harder to read, personally, but it takes more rust knowledge to do so whereas virtually anyone can read Go.
I wish my college understood this. Half the guys dropped out early or transfered into other areas. they were smart but we were sat coding our own stacks and queues and printing to console. And we'd spend weeks on each basic thing that most people bar 1-2 picked up in a day or two. A lot of people struggled to see the end game. where as you can dive into python and say django and be making a little web app pretty sharpish. and then you start to see the context that everything is in.
It doesn't do it automatically, as there are several different tools you can use for synchronization (Semaphores, Atomics, Channels etc.) and Rust won't pick one for you. However, if you write threaded code that is not thread safe, Rust will notice this and yell at you.
The display lists API is at https://doc.servo.org/webrender_api/ , but it is very much a moving target with the ongoing work of integrating into Firefox. It is not really intended for public consumption. Layering is contrary to WebRender’s core design of "batching" as much as possible and rendering everything at once. My guess is that the maintainers would not be interesting in integrating anything with that approach. &gt; If that's feasible I might try give it a shot in my spare time. I don’t want to stomp on your enthusiasm, but my estimate for half-decent PDF support in Servo is closer to a summer internship (or more) than a couple weekends. And that’s even ignoring paginated layout.
If your `hg` *were* in `/usr/bin` (or some such), you could still prevent it from being called by placing a directory you control earlier in your `PATH` (`~/bin` is a popular choice) and putting a stub `hg` in it (either a symlink to `true` or something more sophisticated, depending on your needs).
This is based on a technique I learned from sfackler for building trait objects of traits that have generic methods (like all of Serde's traits). Here is some example code to show how it works: https://play.rust-lang.org/?gist=4a0353c69e8d32cc002897608d16efe4 In `erased-serde` things are a bit more complicated than in the example for three reasons but the idea is the same. - We need to deal with trait methods that take `self` by value -- effectively by implementing the object-safe trait for `Option&lt;T&gt;` where `T` implements the real trait. - We need to deal with traits that have associated types like `Serializer::Ok` and `Visitor::Value` -- by carefully short-term stashing things behind a pointer. - We need to support trait methods that have a generic type in the return type but none of the argument types, like `SeqAccess::next_element` -- this can be flipped around into a callback style where the return value is instead passed on to a generic argument.
I think you do, because you will have multiple event loops running at the same time: the renderer, input events, animations, etc. Instead of having to juggle them manually using raw threading I believe futures are the right approach. BTW I'm not including the entirety of Tokio, just the low-level stuff like core and timer. No networking :)
No, I would instead say they're different error handling paradigms. From [Wikipedia](https://en.wikipedia.org/wiki/Exception_handling): "In general, an exception breaks the normal flow of execution and executes a pre-registered exception handler." In rust, this is similar to panicking. It doesn't matter what comes after the line that panics because the panic interrupts the flow. In rust, the standard way to handle errors is by returning a `Result` if something can fail. Now, by just looking at the signature of the function, you can tell if it can fail and, by looking at the type of the error, how it can fail. This also means the caller can't forget about the failure state, as you would by forgetting to catch an exception. I hope that helps.
I think some of the techniques here could be distilled into a general way to build trait objects for traits with generic methods, associated types, and self-by-value methods.
&gt; start them with a survey of boolean algebra, binary, hex, and then work your way up to assembly, then work your way up to C, and reflect on the assembly that the C code is compiled to, and slowly build up the fundamentals. I learned how to program by taking a top-down approach with Ruby. I learned how to build software by doing exactly this. This shouldn't be your introduction to the field, but I firmly believe it's a necessity for going from tradesman to craftsman, if you will.
I hadn't thought of this, but the builder pattern also [admits some errors](https://github.com/rust-lang/cargo/issues/4327#issuecomment-317749688) that other approaches might catch (although not this one, as it stands). 
Pedantic nitpick - Rust is not an acronym...you're the only one capitalizing it like that :)
&gt; &gt; Once they're hooked, then a deep dive into the boring-yet-necessary parts of computer science will be much more successful. I do believe the fundamentals are necessary to become a well-rounded software developer. I just don't think an intro class is the right place for that material. I would even go further than most university CS departments and say students should take a processor architecture class to understand the trade-offs made with caches and pipelining and other relevant materials.
&gt; I do believe that I can manage learning rust first. I have no previous experience, have read about object oriented thinking and Java programming but have not written any code so Im as fresh as it comes, beginner. As a complete beginner, why do you have any such belief? Allegedly it is not impossible, but it is usually unjustified *for you*, a complete beginner, to have any such opinion on how to learn best. I recommend listening to suggestions. Like many people, I also suggst Python.
Not rude at all, perfect answer to how Rust didn't benefit your C# development. Think I'm worrying about sinking time into learning something that might end up being a waste of time. As all my learning and side projects up to now have been to improve my efficiency in my day job. Reading through the docs and doing some simple things with rust felt quite refreshing and fun. :) So I'm thinking of giving it a go as a hobby language. At the moment I have no real idea of what to use Rust for. Todo apps seem to be the new hello world so could try a command line todo app to begin with. The other idea is rebuild my game to learn rust but that might be too ambitious, (and I'll have loads of questions) but i can see how awesome the pattern matching would be to use and the thread safety looks like it would easily allow me to have loads of things going on at once without any fatal errors. 
[removed]
It seems that for the most part I'm stumbling over working with the ownership model (probably not surprising). It just takes a long time to do things that have been intuitive to me as a programmer for a long time without getting 'borrowed bloody mutable' things happening. I'm also not super clear on how external crates and scoping are working under the hood, and I'm not sure if its something I'm doing, but it may happen that I've scoped in some stdlib function that breaks because I didn't scope in some OTHER namespace it depends(?) on, which seems really bonkers. It's just a scary, opaque, magical new world with bloaty-feeling features like macros and I'm not super clear on its ABI situation, but coming from C, eventually that all piles up and I just go...well, why?
This kind of techinique is why I can't wait for someone to write a "advenced rust" book. I would never thought of any of that.
My experience with web development is that you have very little mutable shared state, apart from the database, so the benefits of rust's thread safety would be rather limited. (Of course there will be some web applications where this does not apply)
The lack of learning resources, tutorials, and just code you can copy, are the main issues. Rust also has a kind of higher barrier at base level. Most of the time when you program real software there are three components: * The language it's self. Something that compiles or runs your language. Python.exe, a C compiler, running JavaScript in the browser. * A build system. Real software is complicated to compile with lots of steps. The build system handles most of this for you. * Dependency management. You are going to need libraries. Other people's code to do stuff. 'Dependency management' goes and finds their code and makes it available for use. Most of the time when you are learning you are only using the first one. On it's own. You learn the others later on. Rust is kind of built to presume you are using all of those. You can use just Rust on it's own, known as `rustc`, or the Rust compiler. But people don't. Even in tutorials they don't. They use `cargo`; the build system. This is awesome for an experienced developer, but a higher barrier to entry if you are new. You'll also get more built in less time with other languages. You will at times get stuck and have no idea what the issue is for hours. With Rust this may be a few more hours than with say Python. I'd also try to ask *why* you are learning to program. Or more, what do you want to try building. If it's say game development then there are languages which would do this better. All that aside; there is no reason why you cannot learn Rust as a first language. Plenty of people have learned old style C++ (without niceties like auto) as a first language, which is filled with more gotchas.
So, Rust then? Just starting Section 8 of the tutorial, so I hope you'll forgive me :)
Uh I'm still wrapping my head around this but all I can say is wow.
Yup! It's named after this [fungus](https://en.wikipedia.org/wiki/Rust_%28fungus%29) I think. Welcome to the coolest club around
You're tickling a little pet peeve of mine, so I'm sorry in advance, but &gt;the boring-yet-necessary parts of computer science [...] a survey of boolean algebra, binary, hex, and then work your way up to assembly, then work your way up to C, and reflect on the assembly that the C code is compiled to Except boolean algebra, those things are more about *programming* or *engineering* than about computer science. You literally do not need to understand *anything* about assembly or C or binary to reason about asymptotic complexity or work out the math on probabilistic algorithms. On the flip side, a solid understanding of the underlying systems is important for engineering, but the theoretical reasoning about complexity etc is less important (someone has already figured out the algorithm, you just have to implement it). I'm sure you already understand the difference, but I'm terrible.
&gt; `impl Generic for ErasedGeneric {` What does this mean? Implementing `Generic` only for `ErasedGeneric` trait objects? I guess that if Rust turns trait objects into `dyn Trait`, this would be written `impl Generic for dyn ErasedGeneric {` and would become easier to understand.
True indeed, thanks for the idea!
There are several origin stories floating around, but that is one of them.
Run-ins with the borrow checker are pretty common. Most programmers aren't used to having ownership enforced to such a degree. It sounds like you might not have fully learned what it wants yet. If you haven't [read the book](https://doc.rust-lang.org/book/second-edition/ch04-00-understanding-ownership.html), I'd recommend it. &gt; I'm also not super clear on how external crates and scoping are working under the hood, and I'm not sure if its something I'm doing, but it may happen that I've scoped in some stdlib function that breaks because I didn't scope in some OTHER namespace it depends(?) on, which seems really bonkers. That sounds like it might be a traits thing. You can't use methods a trait defines unless you also import the trait itself. I've typically found the compiler is pretty accurate at letting me know what trait(s) define the method I want. That, combined with liberal use of the documentation usually gets the issue fixed pretty quick. &gt; It's just a scary, opaque, magical new world with bloaty-feeling features like macros and I'm not super clear on its ABI situation, but coming from C, eventually that all piles up and I just go...well, why? In that respect, it's like many other languages. You're used to a certain way of doing things, and then this language wants things done a different way and you just wonder why not the way you're used to. If you have specific examples of something that's confusing you, posting to the easy questions thread with them and explaining what you think is happening may help clear up misunderstandings. There's also the [compiler error index](https://doc.rust-lang.org/error-index.html) which explains common causes of the given compiler error.
Tiny remark from reading only the documentation: The 'enumeration' you're making [here](https://docs.rs/cache_2q/0.8.2/src/cache_2q/lib.rs.html#47-50) is not properly marked up as an enumeration. This causes the text in the third paragraph to come out wrong in the [generated page](https://docs.rs/cache_2q/0.8.2/cache_2q/struct.Cache.html).
Correct! Quite a neat approach, actually :)
Spent 15 mins staring at the example. Got my head around it with some judicious print statements. Brilliant!
&gt; Am I missing some concept? Yes: inheritance. What you want is inheritance. Unfortunately, Rust doesn't yet have it! The boilerplate you give is probably the best you can do for the moment. One small thing you can do is add methods to the trait to expose the fields and methods in the common struct, so callers don't have to chain together the access to the struct and its properties themselves, like this (i've changed your code slightly): struct Common { data: i64, } trait HasCommon { fn get_common(&amp;self) -&gt; &amp;Common; fn data(&amp;self) -&gt; i64 { self.get_common().data } } // skip impl of HasCommon for A, as in your original code let a: A = get_some_a(); let data = a.data(); // this uses the method on the trait to save some typing &gt; But rust doesn't allowed overloading. What is the right way to solve this problem? Cloning is a potentially expensive operation, so i would say that in this case, the *right* way to solve this problem is to write the version of the method which takes ownership of a Rc, and leave the caller to make the clone. That makes the cloning explicit. Now, if you're interested in some *wrong* ways to solve this problem, i've got one which is only wrong because it hides the clone: use std::rc::Rc; use std::cell::RefCell; use std::fmt::Debug; use std::convert::Into; use std::convert::From; struct MyRc&lt;T&gt;(Rc&lt;T&gt;); impl&lt;T&gt; From&lt;Rc&lt;T&gt;&gt; for MyRc&lt;T&gt; { fn from(r: Rc&lt;T&gt;) -&gt; MyRc&lt;T&gt; { MyRc(r) } } impl&lt;'a, T&gt; From&lt;&amp;'a Rc&lt;T&gt;&gt; for MyRc&lt;T&gt; { fn from(r: &amp;'a Rc&lt;T&gt;) -&gt; MyRc&lt;T&gt; { MyRc(r.clone()) } } fn f&lt;T: Debug, R: Into&lt;MyRc&lt;T&gt;&gt;&gt;(x: R) { print!("{:?}\n", x.into().0); } fn main() { f(Rc::new(RefCell::new(17))); f(&amp;Rc::new(RefCell::new(23))); } This shows something which i think is idiomatic Rust: the use of the Into trait to accept a variety of argument types to a method, and the use of From trait implementations to indirectly provide implementations of Into. You're not allowed to write an implementation of a trait you didn't write for a type you didn't write, so i've had to introduce that MyRc as a patsy. Other than being a sacrifice to the type system god, it's completely pointless. I've also got this one, which is wrong on several levels: use std::rc::Rc; use std::cell::RefCell; use std::fmt::Debug; type Overloaded = (); trait OverloadedExt&lt;T&gt; { fn f(x: T); } impl&lt;T: Debug&gt; OverloadedExt&lt;Rc&lt;RefCell&lt;T&gt;&gt;&gt; for Overloaded { fn f(x: Rc&lt;RefCell&lt;T&gt;&gt;) { print!("{:?}\n", x); } } impl&lt;'a, T: Debug&gt; OverloadedExt&lt;&amp;'a Rc&lt;RefCell&lt;T&gt;&gt;&gt; for Overloaded { fn f(x: &amp;'a Rc&lt;RefCell&lt;T&gt;&gt;) { let x = x.clone(); print!("{:?}\n", x); } } fn main() { Overloaded::f(Rc::new(RefCell::new(17))); Overloaded::f(&amp;Rc::new(RefCell::new(23))); } This simulates overloading by providing trait implementations specialised for different types. There are times when this is a perfectly idiomatic thing to do in Rust, but i don't think this is one of them. I'm not sure why i need the type alias here; i can write the impls for () directly, but i can't call ()::f. The parser seems to choke on it. 
Wrong subreddit, this is for a programming language. 
Check the derivative crate: https://crates.io/crates/derivative
One way to think about associated types is a special form of generics, where for every implementor of trait `A&lt;T&gt;`, `T` can only be one specified type. While this can sound counter-productive to the goal of polymorphism, there are many cases where this restriction makes sense. Take the [`Deref`](https://doc.rust-lang.org/std/ops/trait.Deref.html) trait in the standard library, for example, a trait used to specify functionality of dereferences. In this case, it is only feasible to have each type to deference to a single specified type `Target` (e.g. Vec&lt;T&gt; -&gt; [T]), and using generics instead would likely break type inference completely. If you want to read more about associated types, I find [this](http://cglab.ca/~abeinges/blah/rust-reuse-and-recycle/#associated-types-1) to be pretty good.
Rust isn't nice language to actually write. And it is mostly to fight the problems you know nothing about. So you would just basically cripple yourself without gaining anything out of it :) I think Rust is a perfect language for guys who programmed C or C++ for eternity, and each tedious code construct makes them happy, because they know horrors behind them.
Any programming language can be someone's first, though there are steeper and shallower learning curves. Like, Haskell, C++, and Scala provide a lot of abstractions that can help to later learn Rust. Or, if you find some nice general audience tutorials, Rust can be learned immediately. I try to remain agnostic about educational approaches. While I personally believe in introducing one concept at a time, like variables and then loops in JavaScript before attacking data structures and libraries and headier languages, it's just as valid to take the plunge into modern systems programming with Rust, Go, or Swift. If you feel overwhelmed, tap on the brakes and practice fundamentals. If you're drooling after bleeding edge theory, roll your own operating system. You can take a course and/or follow a blog or whatever makes you feel like you've learned something about computer science and makes you feel more productive than you were yesterday!
Many years ago I met a guy on Slashdot called Jonathan Bartlett, who had the radical idea of teaching assembly as a first language, to give people a feel for how a computer "really" works under the hood. He was writing a book, "Programming from the Ground Up". I suspect that Rust may not be great as a first language, because unless you know a little bit about how things work under the hood, a lot of things in Rust - especially the hard part of Rust, lifetimes - will probably be confusing. Rust does a lot of work to help solve hard problems that aren't obvious to a beginning programmer at all. Bartlett's book is showing its age, but I think that if Rust should be taught early, it should be taught in conjunction with low level concepts like that book aims at.
On my personal server, yes. On my team server, no, we have a separate Jenkins server that will run some task to transfer the binary to production server, and do the deployment on that box.
Rust doesn't have to hide itself from Go. However, if you feel that Rust may be too complex, that's a valid reason to seek Go.
It definitely feels snappy, nice job! Is there is a way to turn off the additional "search for x with &lt;search engines&gt;" in the bottom of the url list though?
There is no such thing as "only" webrender -- webrender support in gecko is a pile of ifs that try to implement things natively, and fall back to asking gecko to render it when the going gets tough (which is very often). You can maximize webrender with the "webrendest" about:config option. This will make us run basically all the webrender support code we have as aggressively as possible. However even that will have lots of fallbacks. For instance, in the current nightly, any text-decorations (like the underline on a link), font-changes (caused by font fallbacks from emoji or asian scripts), or shadows will cause us to give up and have gecko draw a text-run. Edit: there's also recent developments about using "layers-free" mode, which further fragments what webrender actually does.
If you are looking to make video games, websites or phone apps you may want to look at languages that are older, AFAIK rust's library support for stuff like that is still very young compared to other languages.
This type of opaque "write-only" code is something I'm worried about with Rust. Yes it's really cool but it gets very difficult to parse what it's doing.
Am new to this awesome language, so i'm trying to get used to this language by working on my progress at AdventOfCode!
If it could be automatic or semi-automatic, object safety would be so much less of an issue!
Hi, You have a public repository of this ?
I would throw out [exercism.io](http://exercism.io/) as a site for some programming problems while using Rust. It solves the "well, what am I going to make" question of programming, while learning. You could also run a few languages at the same time. If you get stuck, you can submit something, and look at other solutions to get yours working. I started working through this last week and have been coding up missing exercises this week. (I'm about 2 weeks into serious Rust learning, but 3 decades into programming learning.) And learning Rust while on vacation is certified "geeky" by my wife. But she is used to that. ;) If I were learning fresh, I think a basic course on data types and stack vs heaps would be helpful. In this sense, a beginner's C language course may be helpful for these concepts and give you an understanding of the advantages of Rust. Much of the Rust documentation expects understanding of this and pointers and other things. If you do attempt to start immediately with Rust, it would be very beneficial of capturing all of the friction points that you run into. This could be turned into information to help others start programming in from scratch Rust. 
What does this provide over default rust matching? match x { 1 | 2 =&gt; println!("one or two"), 3 =&gt; println!("three"), _ =&gt; println!("anything"), } Here '_' is our IfNotMatch pattern, and you can see we use '|' to match multiple patterns...
I plan on implementing match all statements within Ion sometime soon, in addition to for match, once match statements are stabilized.
Looks interesting, You mean this board ? https://www.adafruit.com/product/2772 Do you have any plans to accept contributions ? I am looking for contributing rust projects, I am from embedded background.
tmux/screen is a not a robust service handler...
Lost in solving exercism problem 15, looking for joining some projects for contribution. 
What you're describing isn't what a match all statement does. A match all statement executes multiple branches in succession for each match that evaluates to true. It's like a series of disconnected if statements, instead a chain of else if conditions.
OP made [this post](https://www.reddit.com/r/rust/comments/6oi95g/overlapping_match_statements/) in this sub a few days ago. idk if that answers your question, but there's some discussion about it and more details over there. I think the distinction is that his macro allows for overlapping match patterns, but I don't know for sure.
No idea, that's an unrelated feature in nightly. Probably has an about:config option, but I don't know.
I disagree. I've been programming in Python for over 15 years and I know enough C and C++ to hurt myself but avoid them whenever possible because they're far too tedious and too stressful to [try to] get right. I like Rust as a replacement for Python in my projects because it lets me spend a little extra effort up-front to avoid a mountain of debugging, "mimic type system guarantees in unit tests", and maintenance hassle later on.
As a relative beginner to Rust, I have a general understanding of lifetimes and references and such, but I still feel like I get caught out by some seemingly simple things. Tonight, I tried using "+=" (the AddAssign trait) on a` &amp;mut String`, and was somewhat confused as to why it did not Just Work, but why when using ".add_assign" explicitly it was fine. The code: fn main() { let mut s = String::new(); do_things(&amp;mut s); do_things_explicit(&amp;mut s); println!("{}", s) } // doesn't work without prepending *: fn do_things(s: &amp;mut String) { s += "hello "; s += "world"; } // works just fine: fn do_things_explicit(s: &amp;mut String) { s.add_assign("hello "); s.add_assign("world"); } `do_things` complains that _"binary assignment operation `+=` cannot be applied to type `&amp;mut std::string::String`"_ (which confuses me because it seems to me that it is being applied to exactly a `&amp;mut String`). `do_things_explicit` on the other hand works just fine on the same. Intuitively, since `add_assign` takes `&amp;mut self` in its function signature, I expected the operator to work if given `&amp;mut String`, just as the actual method call does. Is there a good reason why it does not? I can fix the thing by adding a couple of "*"'s in front of the `s`'s in `do_things`, but I don't really understand why it does not work as is, so any enlightenment would be appreciated :)
I'm reading the book but haven't gotten to traits yet. &gt; In that respect, it's like many other languages. This is both true and not true. Rust's cognitive load is so massive it's genuinely an argument against using it. It's a bit like an Indo-European polyglot trying out Chinese - it's the same process, just...a fuck-ton more.
You could start out doing [exercism.io](http://exercism.io/) exercises and then pick something more consequential once you have a better feel for the language.
&gt; thanks for any pointers let pointer = 0x12345678 as *const u8;
So how did redox come to be doing GSoC? It's rather exciting
I second the parent poster. Thinking about `&amp;mut` in the way you explained makes much more sense. I was actually searching for the difference between `let mut x = y` vs `let x = &amp;mut y` when I came across this thread. That being said, one last point of contention is as follows. From the example [here](https://doc.rust-lang.org/rand/rand/index.html#monty-hall-problem) we have: let mut rng = rand::thread_rng(); ... let result = simulate(&amp;random_door, &amp;mut rng); I changed it to the following and it compiled and ran correctly: let rng = &amp;mut rand::thread_rng(); ... let result = simulate(&amp;random_door, rng); I understand why it's doing that. The question is why is the first form preferable?
Nobody is saying you can't learn rust first. But if you spend two weeks on Python and 4 weeks on rust, you'll understand rust better than if you just do 6 weeks of rust. Learning *concepts* with a language that's loose on the little details is always going to work out better than trying to understand what a "string" is with a language that has several different kinda of strings.
You are quite right that `add_assign` takes `&amp;mut self` in its function signature, and it's important to think about what `Self` is in the context of the `+=` application. `AddAssign` is implemented for `String`, which makes it clear that applying `+=` to a `String` is possible; but the type of `s` in `do_things` is in fact of type `&amp;mut String`, which does not implement `AddAssign`. In order to use `+=`, `s` needs to be manually dereferenced: *s += "hello "; *s += "world"; But if that's the case, then what about `do_things_explicit`? Well, the dot operator actually does automatic dereferencing to find the method you are looking for. It is for this very reason that you are able to call methods defined on slices on Vec&lt;T&gt;s, you can find more discussion on this topic [here](https://stackoverflow.com/questions/28519997/what-are-rusts-exact-auto-dereferencing-rules).
Do you still need a custom LLVM and libcore?
Rust shouldn't be all that different from other compiled, architecture-specific languages. It might have cross-compile concerns that a scripting language or VM-compiled language doesn't have, but beyond that it's really no different from deploying anything else. I think the right answer to deployment questions really depends on a number of different factors. Are you on a cloud provider, pre-provisioned dedicated boxes or have your own datacenter? Is it a backend process, web facing or are you building releases that customers will download and run themselves? What level of horizontal scalability concerns do you have? Do you need to scale up and down automatically? Do you already have or want a configuration management or containerization infrastructure? How are you handling secrets (db passwords, access tokens, etc)? The answers to these questions can all impact how to best deploy code and should be answered first. In the past, I've opted for a Docker/Consul setup where the continuous integration server is responsible for building the Docker image, pushing it to a private registry and updating Consul with the latest image id. Each server watches consul for changes, deploys, sanity checks, registers the service with Consul then stops the older running Docker container. HAProxy based on Consul ensures that traffic gets routed to whatever is running, so it's all zero downtime and basically blue/green at the individual container level. The docker images run their process through envconsul, so the app just pulls Consul/Vault secrets from its environment which makes it easy to run the same code in dev. Rolling back is a simple Consul update. If you are or prefer to be all-in on containerization, a Kubernetes/ECS/etc setup can take the place of the provisioned VMs. But recently I've been experimenting with the serverless approach and I'm really liking it. It's pretty simple and efficient to use Rust and Neon to build a Node native module that works with Google Cloud Functions (and I assume it would work with Lambda or the Azure equivalent). Deployment is very different from the above setup in that it's just bundling up the Node module and calling Google's APIs to deploy. It's, perhaps, not as easy on a per-endpoint basis as something like Rocket, but macros definitely help with some of Neon's rougher edges. And the lack of infrastructure work that would otherwise have to be done is, for me, a win. But, as you can see, deployment is very different because the way the application is run is very different. Anyways...my advice is to not think about deployment code from a right vs wrong standpoint. It's glue between your code/continuous integration and your desired production/staging setup/architecture. There isn't any canonical right answer. Figure out those first two parts and then work on connecting them. Just try to automate as much as is reasonable. It's really nice if the only trigger to code going live is some change to the source code repository, but that's not always possible.
Me too!
Seconded. Rust will not be too hard IF you understand how computers work. In this sense, Rust as a first language can be great because you don't get to assume all that high-level languages do behind the scenes to make our lives easier.
Well it wasn't clear to me what match_all did from the repo or the docs. 
Cool ! For a crate I'm currently building, I'm using Cursor on top of a slice/vec, + byteorder for similar purposes, but the std::io::Result is a real pain in the arse. So I might move to `untrusted`, wish some reliance on `untrustended` for the various endianness stuff. Something that makes me sad though, is that there's no `Write` equivalent to `untrusted`. That'd be really nice. I guess I could hack one together.
[removed]
I'm currently writing an idiomatic driver for the android Binder IPC mechanism. This involves figuring out how the C++ libbinder works, writing the low-level primitives, documenting them, and then adding support to various tools (such as `aidl`). Up 'til now, I have just enough to query the ServiceManager a list of (other) services : https://github.com/roblabla/binder/blob/master/tests/check_binder_opens.rs This week, I'm going to try adding rust support to the `aidl` tool to allow converting a service definition file into an idiomatic Rust client.
&gt; It is not really intended for public consumption. I'm in no rush, thanks for the link though. &gt; Layering is contrary to WebRender’s core design of "batching" as much as possible and rendering everything at once. You could technically do that in the sense of packing the individual "parts" into a single image as a spritesheet/atlas I guess?, Just requires another bit of information on x,y,w,h per sub-image. &gt; I don’t want to stomp on your enthusiasm No worries :) The lack of being able to get layers or smaller parts(blocks) as individual images did that :P &gt; but my estimate for half-decent PDF support in Servo is closer to a summer internship Good to know. If I was able to get individual layers and parts exported as images + metadata(JSON), it sounded fairly doable to just hook that up to a PDF creation API(ignoring multiple pages) to get a proof of concept going? But since it's not that complicates it a bit more.
Cool, this was a project to get feedback on [RFC 2003](https://github.com/rust-lang/rfcs/issues/2003). Question though, what would once match do? That sounds like an async/await proposition. Also, what would `for match` do?
So this is basically using some advance design pattern that tears down type information prior to compilation instead of relying on some compiled tag that is read at run-time? Is that why I'm supposed to read from this? I'm not too familiar with this "Erased term" nor type-Erased.
https://docs.rs/match_all/0.2.0/match_all/ Here is the documentation, how does the first example not show mutiple statements being executed?
I'm continuing work on my llvm-sys safe wrapper, [Inkwell](https://github.com/TheDan64/inkwell). It's making a lot of steady progress, but there's a lot left to do before I release the initial version. It'd be great to get some help, especially from those more knowledgeable about LLVM than myself.
I think you might be better off learning a simpler language first. It's not impossible learning rust as a first language, but I think you will spend a lot of time fighting the borrow checker that could be better spent learning more basic concepts.
Interesting! Some comments inline: &gt; reading integers (unsigned and signed integers, 8, 16, 32 and 64 bits, both little and big endian) This is definitely commonly-needed functionality. Some protocols (notably TLS) also need the ability to read a 24-bit encoding into a u32. &gt; reading vector of bytes (Vec&lt;u8&gt;) In the untrusted design read_bytes() should have a "_less_safe" suffix in its name. We try to avoid raw slices and vectors as much as possible, so that untrusted can enforce the "zero panic" guarantee. We also avoid Vec so that we can meet our "zero allocation" guarantee. I think your read_bytes() is equivalent to r.skip_and_get_input(n).map(|b| Vec::from(b.as_slice_less_safe())). I think an implementation like this might be more efficient than the current read_bytes() implementation. &gt; reading UTF-8 and UTF-16 Strings from_utf8(buf) is equivalent to r.skip_and_get_input(n).map(|b| String::from_utf8(b.as_slice_less_safe())), I believe. I think such an implementation would be more efficient than the current from_utf8(buf) by saving an allocation. We can save another allocation by using r.skip_and_get_input(n).map(|b| str::from_utf8(b.as_slice_less_safe()). Maybe we should add some`TryFrom` implementations to untrusted::Input to make it easy to do such conversions without (directly) using `as_slice_less_safe()`. 
A regular match is equivalent to a string of else is conditions. First case that evaluates to a SUCCESS exit status is executed. No further matching occurs. Match all would instead be like a collection of if statements, iterating through each case and executing each case that is SUCCESS. For match takes an array for the input, and performs a match on each individual element. for match elem in array case &lt;conditional&gt;; &lt;commands to execute&gt; case &lt;conditional&gt;; &lt;commands to execute&gt; end
When would someone use `extern "c"` without `#[no_mangle]` and vice versa?
An alternative : if the original crate author forgot to derive debug, submit a PR and fix it ! This is so much better than having everyone fix it on their own and never sharing the fix...
Couldn't think of how this would be useful, then I realized it's for side effects. `foreach_match` could make more sense. A `foreach` is used for side effect purposes.
So then what would once match be
What do you mean by a once match?
searching for Custom Derive might turn up more results [like this](https://doc.rust-lang.org/book/first-edition/procedural-macros.html), but if you want to make a tutorial, there's always room for more!
I want to implement the "from" trait for any option `Option&lt;T&gt;` where `T: AsRef&lt;str&gt;` holds. ie the contents of the option can be made into an `&amp;str`. I'm not sure how the declaration should look? I tried impl&lt;'a&gt; From&lt;&amp;'a Option&lt;T: AsRef&lt;str&gt;&gt;&gt; for ... and impl&lt;'a&gt; From&lt;&amp;'a Option&lt;T&gt;&gt;&gt; for ... where T : AsRef&lt;str&gt; But got compiler errors. Is this possible?
Using `feature` to switch types seems dangerous: cargo features are designed to be additive, and cargo often chooses to share versions of a dependency between different users even if they request different features. In this case, this could manifest as one crate `a` depending on `kolakoski` with no features and mentioning the iterator type `Kolakoski` without any parameters, and another crate `b` also depending on `kolakoski` but with the `num-traits` feature enabled. This changes the signature of the `Kolakoski` type and will cause `a`s use of it to stop compiling, and this can be purely driven by someone happening to use both `a` and `b` in their project. That said, I don't imagine this will come up much for this specific crate. Also, the recursive `self.nth` call seems like it could be `Kolakoski::new().nth` which avoids needing to juggle so much state.
[removed]
Great - I'll leave it as is then :) I wouldn't consider what I've learned from Rust a waste of time. Its another tool in my belt, that's for sure. Apart from that, I just really enjoy writing Rust. It feels similar enough to C# (with things like the generic type constraints: `where T: Something`) yet different enough to keep me interested. The projects I chose to do in Rust encouraged me to revisit some fundamentals of programming that I lack in my day job in C# web dev (binary, hexidecimal, pointers, parsing, etc). It could be argued that revisiting those particular fundamentals has helped me in my day job ... but that isn't a side effect of learning Rust itself. Not that I am recommending the same path, but to give you an idea of the projects I decided to write in Rust when I began learning it: a [Shunting Yard calculator](https://en.wikipedia.org/wiki/Shunting-yard_algorithm), then a library that reads and renders PNGs, then an emulator/disassembler/assembler for [the MOS 6502 microprocessor](https://en.wikipedia.org/wiki/MOS_Technology_6502). No one will ever use them, but implementing them in Rust has certainly added tools in my belt. Anyway, if you do decide to learn Rust I would encourage you to approach it as "this is another weapon I can weild/tool I can use" rather than wondering how it might buff weapons/tools you already have :) Good luck in your decision either way!
I think he's referring to the last bit of your original comment &gt;once match statements after stabilized.
Ah, excellent! That's going to be the next CDC I implement in chunky. I don't know if you've got a chunk oracle available, but if you do, [bimodal chunking](https://www.google.com.au/url?q=https://www.usenix.org/legacy/event/fast10/tech/full_papers/kruus.pdf&amp;sa=U&amp;ved=0ahUKEwi-46mG1KXVAhXJpJQKHUtYD0cQFggNMAE&amp;usg=AFQjCNEBPi7B-DdURM-GM8vX2_SZFfVzaQ) might be an interesting thing to try. Not for performance, but for deduplication efficiency. I guess I should see if rdedup is sufficiently like what I want to contribute to it rather than do my own thing 😀
When something is turned into a trait object, it "forgets" the original type. So if you have a value of type `Foo` where `Foo` implements `Blah`, and you pass the value to a function taking `Box&lt;Blah&gt;` (a trait object), that function doesn't _know_ that the value is of type `Foo`, it only knows the value implements `Blah`. That's type erasure.
Yes. I can upload the full Cargo source of the monitor if you're interested.
So the problem with serde was that the original type wasn't being forgotten when passing serialize/deserialize as a trait?
Pretty optimistic thinking that two independent crates will depend on this! I could split this into two different crates, but I have trouble deciding which should get the nicer `kolakoski` name. You're correct about the recursive call, it's a vestige from some earlier debugging I was doing.
Yes; this can already be done with the [`dont_panic`](https://crates.io/crates/dont_panic) crate.
I'm confused. In English, once is often used as a conjuction word that means 'as soon as'.
Well, by the time they get the skill to create such things, Rust may be ready for that. There's already some good game libraries.
The loops in the implementation are strange to me, when you can just let `match` handle multiple patterns for each expression, e.g.: ($val:expr, $($($p:pat)|+ =&gt; $b:expr),+) =&gt; {{ let val = $val; $( match val { $($p)|+ =&gt; { $b; }, _ =&gt; (), } )+ }}; Too bad `if let` doesn't allow that, but this is basically how it would be desugared anyway.
Was this inspired by the Numberphile video yesterday?
So I'll change the question then to: How does cargo find the foo library? 
100%
Rather than two crates, you could just have the `Kolakoski` type for `u8` and add a `GenericKolakoski&lt;N&gt;` type when the "num-traits" feature is enabled.
Rust requires that traits meet certain restrictions to be usable as trait objects. The problem with Serde is that Serde's main traits don't currently meet those restrictions, and so can't be used as trait objects, which makes certain patterns inconvenient. This is a proof of concept for how to fix Serde to make the traits usable as trait objects.
You're right, thank you! 
&gt; This is a proof of concept for how to fix Serde to make the traits usable as trait objects. Not quite -- these are working Serde trait objects that work with the existing Serde traits. For performance and usability reasons these would absolutely not be a good replacement for the real Serde traits. Serde is staying as is, and this crate gives you a way to use trait objects with Serde. For example if you need to serialize a heterogeneous list: `Vec&lt;Box&lt;Serialize&gt;&gt;`.
Ahhhh, makes sense! Thanks for clarifying!
I appreciate your posts.
It took me a second to understand those examples. I think it would be valuable to show the "de-sugaring" for those same examples.
As soon as I tried to make a minimal proof of concept (`&lt;!DOCTYPE html&gt;&lt;style&gt;:root{font-family:Source Sans Pro,sans-serif}&lt;/style&gt;`) I started running into the first problem again (CSS rules pane is blank). I don’t have the time to spare to whittle it down steadily, but https://www.topicbox.com/ is exhibiting the backslash problem for me (on Windows 10). Concerning the rules panel being blank again, it occurred to me to open up the Browser Console this time, and there appears to be an error that occurs three times then: &gt; Protocol error (unknownError): couldn't find start of the rule - Twice from resource://devtools/client/inspector/shared/utils.js:207 (generic promiseWarn function, fairly useless without a stack trace) - Once from resource://devtools/client/inspector/rules/rules.js:809 (in CssRuleView#selectElement, this one’s meaningful)
It's cool and I'm glad you got it working, but I honestly have no use for it. I don't think the advantages of Rust over C or C++ are worth the toolchain pain for these very small microcontroller projects. Maybe in another year it'll be smooth sailing for Rust on AVR, but that's also another year towards ARM taking out the 8-bit μC market. I'm building a much larger system on a Cortex-M4 and I definitely want to use Rust (plus some C libraries) there.
Thanks, the example is useful. So if I want to draw with glium I can do it in a similar way by creating a child window using the `widg.win_id()` and width, height? (Using my winit fork that allows creating child windows.) Does the issue you mentioned prevent the creation of custom derived widgets?
It appears that &gt; rustbuild is now more eager to build our rust and &gt; improved error when mistyping ; as : both point to the same github [pull request](https://github.com/rust-lang/rust/pull/43096). Edit: It also looks like the link "[merged in the last week](https://github.com/issues?q=is%3Apr+org%3Arust-lang+is%3Amerged+merged%3A2017-07-17..2017-07-24)" is also broken. Unless you have to be logged in or part of the org to see it. Edit2: Ye logging in fixed the merged issues problem. I shouldn't have jumped the gun.
Not sure about glium, but probably. Yes, that issue prevents any inheritance at the moment. 
[You are probably missing `mod dice;` in the file `main.rs`](https://www.rust-lang.org/en-US/faq.html#why-do-i-have-to-declare-modules-with-mod).
So `cargo update` updates the deps only to the next semver compatible and available version, like the version that `cargo outdated` would show?
Alright, that seems to have fixed it, but then I get another error in dice.rs, error[E0432]: unresolved import `zombiedie::ZombieDie` --&gt; src/dice.rs:1:5 | 1 | use zombiedie::ZombieDie; | ^^^^^^^^^^^^^^^^^^^^ Maybe a missing `extern crate zombiedie;`? I tried `mod zombiedie;` in dice.rs, and it errors that I can't create a new module there. Any more help? I could probably just move the trait def that's in zombiedie.rs into dice.rs, but I'd like to learn the way to do it in separate files. 
It looks exactly where you tell it to. If you have either of the following: [dependencies] foo = "1.0.0" [dependencies.foo] version = "1.0.0" Then it looks for `foo` on Crates.io (or another registry if you have one configured, that's an advanced topic) You can also specify Git dependencies: [dependencies.foo] git = "https://github.com/foo-rs/foo" Will look for a Cargo project (Cargo.toml) named "foo" *anywhere* in the repository (even subfolders) at the given URL. If you want to specify a path dependency on the local filesystem, it looks like this: [dependencies.foo] path = "../path/to/foo" Where the path is a directory path relative to the current Cargo.toml file. It will expect a Cargo.toml in that directory. This is all covered at http://doc.crates.io/specifying-dependencies.html
I should probably have linked to the [Book](https://doc.rust-lang.org/book/second-edition/ch07-01-mod-and-the-filesystem.html). :p In short, `mod zombiedie;` in `dice.rs` would correspond to the module `&lt;current crate&gt;::dice::zombiedie` (which does not exist) and not `&lt;current crate&gt;::zombiedie` (which is where the `use` statement in question would look at---`use` path is normally absolute). Therefore you should also have `mod zombiedie;` in `main.rs` as well.
Please do, you are always welcome. There are some more people already involved, so we do have some tiny community that you could join. :) . If you find it not suitable for your needs/goals, please let me know what do you think. :) 
Ah, okay, that makes sense. Turns out I also had to put a use statement for dice for the trait as well. Thanks for the help, it's starting to make sense.
I won't repeat the other excellent advice people gave, but once you got over the initial hump of feeling lost when writing code, you can get a learning boost by joining a project and working on mentored issues ([TWIR](https://this-week-in-rust.org) has a list) – getting help from a mentor while working on an existing project means you get to read code (which is, as the saying goes, harder than writing) and have a mentor to unstuck you whenever you get stuck.
Well, this does help me a lot. When I learn a new language, I always try to refer the conception in new language to the old language. But now, it turns out that it doesn't always have something similar between two languages. Thanks for your help :)
This is very helpful, but my question was more directed toward how a library can reference itself in an extern crate statement. So, inside of a library where the Cargo.toml dependencies section is completely empty, how is a library named, "foo" able to say `extern crate foo;` within the `src/main.rs` file inside of the foo library? (How does cargo find the library while being called within the library?)
Because of coherence, you can't implement traits you (the current crate) don't own for types you don't own. This is to prevent conflicts from other crates providing duplicate implementations. So to be able to implement a trait A for type B (or references to type B), your current crate has to define either A or B (or both). 
Sorry, I misread the original question. If you have a `src/lib.rs` or else a `[lib]` section in your Cargo.toml which points to a root source file, Cargo will compile that first and then implicitly pass it with the `--extern` argument I mentioned at the beginning when compiling `src/main.rs` or any files in `bin/`, `examples/` or `tests/`.
But I guess it's better fit to replace performance hungry parts, not to replace whole source base? Because either you're using Python on wrong places, or choosing Rust way to often :-) Speaking of hassles - my last project with Rust became uncompilable just by standing untouched for a month :-) Reason - something like 12 dependencies makes Cargo completely mad. It forms somekind of enternal loop trying to resolve everything. Spent wayyy too much time on that, way to go seems to adjust dependency versions by hand till it gets happy. Still at one point I've found two vital dependencies which I even cannot find any compromise :-) so till Rust is still in early adopters phase, extra effort doesn't mean it will work better
* `extern "C"` without `#[no_mangle]` Defining a function to be passed by pointer, for example, to some C API which takes a callback. Name mangling doesn't affect the functionality here, but having the right ABI is imperative. * `#[no_mangle]` on non-`extern "C"` functions Being able to more easily locate functions in emitted IR/assembly, or in a debugger/environment which doesn't support name demangling. The names are still grep-able with mangling on, so this isn't a huge issue. I can also conceive of a very hacky use-case where you can swap functionality at link-time by deliberately producing multiple libraries with the same function names, and then linking one or the other. This is obviously highly unsafe and (I presume) one of the reasons mangling is used to begin with, to prevent this happening accidentally with multiple versions of the same library.
Wouldn't being able to `use` crates with parameters solve this problem? Seems like a lot of mechanical hoops to jump through.
Bug report: `all: unset` is expanded into every possible property in the dev tools.
Bug report: take an element with an attached pseudoelement, apply `display: none` to the element in the dev tools, and then unset `display` again: the pseudoelement is gone. (Various interactions will get it back, but that’s not the point, of course.)
Vector graphics pipeline: https://github.com/servo/servo/issues/3788 This is something I *really*, *really* want, but the probability of me ever allocating time to do it myself is effectively zero. The state of HTML → PDF as I understand it (though I last actually *developed* against these things a couple of years ago): there are basically three options, none of which are really *great*: - WeasyPrint doesn’t do JavaScript and lacks various other CSS things I needed; - wkhtmltopdf is *really* hard to convince not to resize things willy-nilly, doesn’t use the proper CSS paged media stuff, and can render text quite poorly (kerning issues, I think it was); - Prince doesn’t implement some of the latest nice CSS things (doesn’t look like it has flexbox yet, for example) and lacks CSSOM (so its JavaScript engine can’t interact with the layout); it’s commercial, which is both good (it’s actively developed, far more than the other two mentioned here) and bad (not free). [They’re doing something with Rust, too.](https://www.meetup.com/Rust-Melbourne/events/240519466/) Prince is my favourite of the three; wkhtmltopdf was mostly OK; I could never actually use WeasyPrint for what I was trying to do at the time because it lacked multiple critical features. Implementing a web browser is hard work; making it render to a paged medium is also hard!
Think you're looking for /r/playrust/ This is the Rust programming language subreddit.
OP delivers... To the wrong sub. 
In the US you get kicked out of the navy if you board the wrong vessel... Wrong sub. 
&gt; But I guess it's better fit to replace performance hungry parts, not to replace whole source base? Because either you're using Python on wrong places, or choosing Rust way to often :-) I'm using Rust to replace Python in places most people use shell script because it makes it much easier to ensure I've handled all possible error cases AND because its handling of filesystem paths and Unicode strings has learned from Python 3's mistakes. (And, before Rust, Python 3 had the best solution of anything I could viably choose, given other project requirements.) For example, I had a script that began as a shell script. I rewrote it in Python for things like `try`/`finally` and clean, easy-to-use APIs built around Python's sane handling of strings, arrays, and quoting, like the `subprocess` module and `os.listdir`. When I rewrote it again in Rust, I discovered bugs I never even knew were possible. (Did you know that it's impossible to write an infallible "get current directory" call because the kernel itself enforces the semantics that you may not have permission to query your current working directory's path? I didn't know until Rust's monadic error handling brought that to my attention and I researched it.) I use Python almost everywhere because, historically, for I/O-bound work, it has the best balance of functionality and safety. Now, Rust is eating that segment as its ecosystem grows. * Python can't forbid a variable from containing `None` at compile time. * There's no easy way to ask Python if I've handled all possible exceptions that could be raised. * Python can't enforce proper use of session types at compile time. * Python can't enforce, at compile time, that I'm following a proper "acquire lock, use, release lock" pattern every where I interact with a resource protected by a lock. * Rust programs start noticeably more quickly. * etc. etc. etc. ...and most of those are due to things which can't be retrofitted onto a language because they require the standard library and ecosystem to participate in order to get the full effect. (Nullability as part of the type signature, all return paths visible in the function signature, affine types, etc.) Plus, working in pure Rust makes it really easy to compile a Linux binary (32-bit or 64-bit) with no external dependencies (not even glibc), which makes it superior for situations where I might need said tool to recover a damaged OS.
I would really love to know *how* these tricks work, in that—clearly the right method *does* get invoked, despite erasure. How's that trick turned?
Thanks for pointing this out – my mistake.
Thank you for all the help!
Ah, OK so I think the rules panel being blank is due to https://bugzilla.mozilla.org/show_bug.cgi?id=1380890 which should be fixed in the current Nightly. Thanks for the pointer to the site with the backslash problem. I've filed https://bugzilla.mozilla.org/show_bug.cgi?id=1384398.
Yes that is what I would mean by once as well but my question is changed to what is the difference between once match and match?
Thanks for this, I was looking for a way to do just that. However it looks that doing it like this prevents patterns like `1 | 2`
Thank you, I will provide to the best of my ability de-sugared examples
There is no "once match"; reinterpret "once match statements are stabilized" as "after match statements are stabilized". -- ^(Who's on first?)
Fairy nuff. I'm just trying to get people to try it out so that we can iron out the bugs together.
What's on second
Yes, that one. Absolutely.
In my case I have defined they type I am trying to implement the From trait for. I didn't include it because it didn't seem material.
&gt; Prince […] [are doing something with Rust, too](https://www.youtube.com/watch?v=6I_kXybQzuk). Ooooh! I’d heard about them doing stuff with Rust but didn’t know about this presentation. Thanks! 
The only reason you can't point to rust projects and their tracker manager to show problems is because there are no significant rust projects that aren't still alpha/beta quality. When rust gets there we'll also be able to point out that rust isn't perfect and therefore you should use something that isn't rust.
Cargo is fine, just a bit tedious to set up for a dinky example. So I think it's useful to know how to drive rustc directly - a nice rust aware editor like Geany has hot keys for compiling and running Rust programs. I did the runner utility for this reason, when your little examples need external crates 
I think rust is perfect to learn as a first language. Years ago people used to learn c or even c++ first (most of books contains some mix of it). It was hard but somehow possible. Rust is simpler than c++ and, what is most important, very, strict what makes new programmers learn good habits. I'm now trying to teach my gf. She is totally not into maths and computers, she is learning everything from scratch. She choose c# but the more I teach her, the more I say "in rust it would be simpler / more precise / less confusing".
As I wrote in my comment, I never used it in my projects :) It is fancy but not very useful.
Chrome shows me [this](https://i.imgur.com/jyzjm1D.png), Firefox just gives up and only displays text. What's going on?
If you're trying to implement it for `Option&lt;YourType&gt;` or `&amp;'a Option&lt;YourType&gt;` then that's still invalid according to coherence.
Fun fact ( which i never verified ) . In most of the US , CS was a split-off from EE (Electrical Engineering) , while in Europe it was mostly a split-off from Mathematics.
Your own types are definitely relevant here. While the intuition is "you can't implement traits you don't own for types you don't own", the precise rules are a bit more nuanced, particularly when generics are involved. It would be useful if you could provide a minimal example that fails on play.rust-lang.org.
I'd love to see an RFC (if there isn't one already) incorporating some of these ideas into the std library in an opt-in basis. This is one of those "Rust as a high-level language" issues, where a deserializer that would be trivial to write in Ruby becomes next to impossible in Rust. It's a major roadblock in my goal of using Rust for basically everything. I also just want to say, it's incredible that the community is developing tools like this using std to find solutions to core language design decisions. It seems to me that projects like this are a core part of building a language where you can "hack without fear" and then optimize as needed without having to rewrite in a lower-level language or write externals.
The one linked by /u/coder543 is the best resource I think. After writing several custom derive implementations myself: There really isn't much more to learn/understand than what is presented there. You always: * Parse the input, ususually with the `syn` crate * read / process the information in the struct/enum definition * generate the new code you want, usually by using the `quote` crate 
Like /u/the___duke said, there isn't a whole lot of guidance out there. It's changed a lot in the last 12 months or so. Usually I'll start a procedural macro with a unit test that shows what I want the output to be for some typical input (it can be hard to see what logical components make up your macro otherwise). Then armed with the source for `syn` (for _go to definition_ goodness) and something like [`serde_derive`](https://github.com/serde-rs/serde/tree/master/serde_derive)'s source for reference I'll flesh it out from there. I do think we need some more resources on how to build these things.
It is hard to say with the absence of context in which you use widgets. May be what you really need it a trait bond over Widget.
Welcome to Reddit. You should check the subreddit you're posting to *before* you post to it. You want /r/playrust.
the image requests are `blocked:csp`. the `img-src` field of the csp is: `'self' cdn-business.discourse.org cdn-business.discourse.org;` whereas the origin of the images is `cdn.discourse.org` which doesn't match. EDIT: found the culprit
The thing in the option is not my type, the thing I am implementing the trait for is the type I defined. [This](https://play.rust-lang.org/?gist=1e46246215bab2157fc8d8917769aab4&amp;version=stable) is an example of what I am trying to do.
I finished the program I was working on the other week that captures UDP jpeg streams with pcap and multiplexes them into one large image that then gets served up as a multipart jpeg stream over http to a camera recording server. There are three different network interfaces that it needs to capture from - basically, there are 8 autonomous mining machines which can run on any one of three separate systems/networks. Initially I was just going to run three copies of the program at once and have three main images served up to record, then I realised I could simply create three capture threads and update the main image from each thread. The first draft was already threaded with respect to converting each jpeg stream and updating the main image, so this change to capture on three network interfaces simultaneously took me about 5 minutes. Which was good, because the bulk of the CPU time is taken up by the jpeg encoder as it serves up the main image at 15FPS and three copies of the program was going to make my VM sweat a little. As it is now, it uses about 20% CPU on a quad-core system. Thanks to rust and it's obsessive borrow checker, this was relatively easy to do. Oh, and I just made [a crate](https://crates.io/crates/bitfont) out of my bitmap font functions, to get a feel for how making and publishing crates worked. 
Good idea. [This](https://play.rust-lang.org/?gist=1e46246215bab2157fc8d8917769aab4&amp;version=stable) is an example of what I am trying to do.
I guess I'm the only one who thinks futures/tokio makes codebases unreadable.
Oh, I see. You have to declare the generic type `T` first. You can specify the `AsRef&lt;str&gt;` bound there or in a `where` clause. impl&lt;'a, T: AsRef&lt;str&gt;&gt; From&lt;&amp;'a Option&lt;T&gt; for StringView {} impl&lt;'a, T&gt; From&lt;&amp;'a Option&lt;T&gt; for StringView where T: AsRef&lt;str&gt; {} 
you can try. compiler and community are great. i started with c# no regrets (static typing), rust wasnt so difficult after c#
Damn, just wanted to say that seeing this comment a bit before would've saved me a fair bit of investigation in https://bugzilla.mozilla.org/show_bug.cgi?id=1384065. Thanks anyway! Should be fixed as soon as those patches land :)
&gt; Usually I'll start a procedural macro with a unit test that shows what I want the output to be for some typical input Sounds like a very good idea! Can you link to some example how do you unit-test proc macros?
What's that sequence good for?
Now, that's very useful, thanks! :)
Sure, [here's one](https://github.com/KodrAus/auto_impl/blob/master/auto_impl_internals/src/lib.rs#L158-L196) I was playing with yesterday. Still a work in progress :) Note that this approach can produce some awful output if the test fails, but I find it really helpful for figuring out what I actually want it to do. I'll add a dummy crate that just uses the macro in a bunch of different ways and remove most of those tests now that it's all implemented.
I think combining the best of both worlds would be awesome. That is, using syntex for interpreting the relevance of the coverage information, but using LLVM code coverage instrumentation (which is very powerful) to track everything that might be interesting, like detecting whether all possible variants of an enum were tested. LLVM code coverage being faster is just the icing on the cake.
Yeah, it seems they are [violating CSP.](http://i.imgur.com/woe9IlS.png) You can't just include Discourse images on another server.
Give it a vote while you're there! (You have to be logged in with a trello account)
What's the difference between `.split(char::is_whitespace)` and `.split_whitespace()` on `str`?