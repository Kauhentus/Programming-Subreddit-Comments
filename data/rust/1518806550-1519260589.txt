Sure thing! Answers mostly from github.com/ezrosent: * `stride = cur_slot * 2 + 1` because the number of slots is a power of two, and so an odd number stride is sufficient to have a cycle that covers all the slots. We do this instead of iterating in a fixed order so that different threads tend to visit distinct slots to avoid contention. * Stack pointers - we don't care if a given thread gets the same seed over and over; it's only important that *different* threads get different seeds. * Our current choice for `PATIENCE` seems to work well, but it could certainly be different. We haven't put much effort into tweaking it.
Ah gotcha. Cool!
**1 year ago**, even before having the idea of Rust cookbook I suggested [Rust by practice/ Practicing Rust](https://www.reddit.com/r/rust/comments/5a2q4k/suggestion_rust_by_practice_practicing_rust/d9dqt5z/) . It 's to build some official place to see some sample programs which shows best practices and how things work together. **A step by step A-Z guide to code real projects**. [For that I suggest to do a weekly challenge](https://www.reddit.com/r/rust/comments/5a2q4k/suggestion_rust_by_practice_practicing_rust/d9dqt5z/) to ask and/or collect solutions for step by step A-Z guide to code real projects and it should be spread via "This week in rust". **Instead of suggesting dailyprogrammer or Rosetta Code, ** If we can start this under 3 sections, 01. absolute beginners 02. Intermediate users 03. Advanced users each week, **not only we can build a proper cook book and also this will be a big step forward to reduce the learning time for Beginners, Intermediate users and Advanced users.**
Also, this isn't a very nice abort. LLVM's abort means (at least on x86_64 + Linux) executing "ud2", which causes a SIGILL. It's just your last defense perimeter against UB. So yes, catching panics is still recommended. IMO.
Ah alright, was already following the const generics issue on github because i needed it for something else aswell. Would it be bad design to add a panic condition in bar? So that when data.len() != SIZE it panics, or is that not so great design?
That's why I thought we best measure the impact.
Ah awesome, think I'll go with the panic method for now then, will probably change it once const generics are added. Thank you btw.
Somewhere I read something about rayon allowing you to provide your own threadpool. In any case, the thread pool should be a global resource that you provide for your system, and that both tokio, rayon, and every other library in the ecosystem that needs to use multiple threads can use.
https://hackage.haskell.org/package/base-4.10.1.0/docs/Control-Concurrent.html#v:killThread &gt; killThread :: ThreadId -&gt; IO () Source# &gt; &gt; killThread raises the ThreadKilled exception in the given thread (GHC only). &gt; &gt; killThread tid = throwTo tid ThreadKilled The key to understanding this is Haskell's asynchronous exceptions: * [Asynchronous Exceptions in Haskell](http://simonmar.github.io/bib/papers/async.pdf) * [Asynchronous Exceptions in Practice](https://simonmar.github.io/posts/2017-01-24-asynchronous-exceptions.html). I'm going to guess Rust doesn't have these.
I mean, *I guess*
Another shameless plug, I made a Twirp generator which is basically simpler grpc without streams or http 2: https://github.com/cretz/prost-twirp. Note, I bust created it so there are probably some bugs.
&gt; However the reduced IO due to smaller encoding most likely is a net performance win. It **used to**. If you look at the finance industry, and specifically at high-frequency trading, you'll see it in the evolution of serialization formats: - *before*: FastFix! Extremely compact encoding thanks to (1) variable length encoding and (2) delta encoding (you don't encode the value, you encode the diff compared to the previous value: it's small, and thus leverages VLE even more). - *nowadays*: SBE (Simple Binary Encoding), basically put your struct on the wire (packed struct). These days, the speed of networks is such (10Gb/s! 40Gb/s!) that encoding tricks such as VLEs, etc... hurt latency due their CPU requirements on both encoding and decoding side. Also, if you are willing to take a latency hit in exchange for reduced bandwidth (or if you are targeting lower-bandwidth environments), there are good general purpose "on-the-fly" compression codecs, such as LZ4. Use LZ4 over SBE :)
I wouldn't use serialization for any kind of persistent data; you'll need backward/forward compatibility of some sorts, so should really be reaching out toward messaging protocols such as ProtoBuf, FlatBuffers or Cap'n'Proto.
RemindMe!
That's really hard. The problem is that compilers are traditionally all-or-nothing: - either they are given a valid program and produce code (and side-artifacts), - or they are given an invalid program and produce diagnostics. They are *not* designed for incomplete code, and of course when you want auto-completion you necessarily have incomplete code :( It'll take time to turn rustc around.
**Defaulted to one day.** I will be messaging you on [**2018-02-17 20:01:42 UTC**](http://www.wolframalpha.com/input/?i=2018-02-17 20:01:42 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/rust/comments/7p1s2l/notyetawesome_rust_tell_us_the_use_cases_you/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/rust/comments/7p1s2l/notyetawesome_rust_tell_us_the_use_cases_you/]%0A%0ARemindMe! ) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
**My first thought!** Alright, let's do it /u/llogiq: where in your flair do you want me to insert mutagen?
Hopefully we’re talking 0.5-1years not 2+years
Hey, this looks cool! I feel like there could be good integration with [artifact](https://github.com/vitiral/artifact) which shares _some_ of the same goals but aims to be how you track your requirements (specifically not issues). Maybe the two tools could join forces in some way to provide a comprehensive issue + requirements + test tracking tool with an integrated frontend? Also, I note that you have no issue tracker -- very pithy :) It would probably be a good idea to include "how to open an issue against SIT " at the very top of your README (right after the purpose), to provide documentation as well as help for those who want to open issues :)
Aww, for a moment I thought you'd join the project. No need to change my flair, I can do that myself when I get around to it. 😎
Why capnproto and not grpc?
done! https://github.com/llogiq/mutagen/issues/7
I found the capabilities model very interesting. It also feels simple, just a socket and messages. But I'm no expert.
Unfortunately, the paper was written prior to a bunch of performance improvements in bincode.
This is a valid concern. To me, pulling in those other tools have their own kind of costs, but on a larger project, something like protobuf becomes more compelling.
#WeAreRelivent!! /s
Yes, you understand it perfectly. There still must be coordination, but it allows you to use non-thread-safe interior mutability without sacrificing the ability to send references to other threads. This can be very useful in certain situations--in particular, you can send the permissions back and forth between threads and share reads, whereas normally if you use something like Cell you're not able to use multiple threads at all. As far as adding more sets after you spawn your threads: I *did* actually have a plan to do this safely (using tricks involving lifetime variance) but unfortunately the Rust compiler cannot currently handle what is needed to make it work (it would require being able to extend the lifetime graph within a particular closure scope, which should be safe in theory but is not compatible with Rust's current well-formedness checks). If I ever get it working, the library will become significantly more useful; however, IMO it's already a better alternative than RefCell or (particularly) Cell in many cases, if you're willing to live with the ergonomic ugliness.
I’m still trying to get Rust bootstrapped in Debian for all architectures supported upstream. Unfortunately, powerpc and mips* are still broken, but at least sparc64 is working now \o/.
I can speak to tarpc. I'll start with the reasons why it might not be a good fit for you. 1. It only supports rust, so it's not good if you need cross-language support. Any kind of distributed system probably desires to support multiple languages, even if not at the outset, and right now there's no work being done to support other languages, so if you think you ever might need to support e.g. Go, Python, or Java, tarpc is not a good fit. This is kind of inherent to the nature of being built on a rust macro. 2. tarpc currently hardcodes bincode for object deserialization. There are longstanding issues to change this, but at least for the near future expect to rely on bincode. Bincode the format is fast but does not support forwards-compatibility. This is another strike against tarpc for distributed systems, as distributed systems can't generally assume that everything is versioned the same. 3. I don't know of any production uses of tarpc. 4. I and my co-maintainer are doing a poor job of maintaining it right now, due to life externalities. I do hope to continue maintaining it, and I'd gladly take on more maintainers, if anyone were so inclined. But at the moment, we are slow to respond to issues, slow to update dependencies, etc. The last commit was Dec. 5, which was merging a pull request. Now for the reasons you might want to use tarpc: 1. You're working on a project that needs to get up and running quickly and want as little friction as possible. tarpc requires no plugins - it's just a rust macro - and natively supports serde serialization, so you can basically hit the ground running. 2. You control both client and server and can enforce they're always versioned the same. 3. You don't care about any language other than rust. 4. You're interested in contributing to an open source project and dogfooding it. Hope that helps!
Yeah antimalware is totally crap, you won't get any argument there, but yeah some Windows and Mac antiviruses trip on writing to a page and then making it executable (disallowing uncommon jits as well). So even if it's their fault, it does add a stumbling block to distributing it, compared to just a zipped executable. Just something to consider.
Thanks for the idea! As for "opening an issue", it is in the README. It used to be higher up the file, but got moved down to give way for more explanations. Do you think that should be changed?
Gotta be working on Mercurial.
Even Rust can't fix Facebook.
Asynchronous database queries are only sometimes helpful (since frequently fetched data is usually in memory) so I'm not convinced that's what is causing it.
Well, I don’t know. I checked code and it looks similar to actix and should perform similar. https://github.com/TechEmpower/FrameworkBenchmarks/blob/master/frameworks/Rust/tokio-minihttp/src/main.rs
You are right. I just wanted to use simd :)
Lin Clark did a talk a while back about Facebook's Fiber project (React internals rewrite) and the potential for WASM to make it even more performant. Maybe Facebook is now exploring this space?
Ooh, I didn't know that! It seems to fall back gracefully too. Rustdoc before 1.21 just ignores the ```` ```compile_fail ```` test altogether, 1.22 through beta do test it but ignore the specific error code, and nightly verifies the code. Thanks, I think I'll try to switch to this after all!
Awesome. I am (co-)author of [git-dit](https://github.com/neithernut/git-dit) where development has stalled for a number of reasons (some because both authors are doing their masters thesis, and some others)... So lets hope this does not happen to your project!
Lets hope indeed! The best way is to get enough active contributors to keep the project moving forward.
I am thinking, may be it should have at least a link to the "opening issue" section from the very top...
Yes, a link at the top would be good.
You're welcome to contribute to my still-very-early implementation at https://github.com/TheBlueMatt/rust-lightning ! :p
Profiling people by socio-economic class, and selling the info on an open market seems close enough for me. Also don't forget Thiel is involved with both companies. https://www.engadget.com/2018/02/09/facebook-patents-tech-to-determine-social-class/
Highly unlikely, since the posting mentions "source control server", which ought to be [mononoke](https://github.com/facebookexperimental/mononoke) (as others have mentioned)
For me, closures are most powerful because of how they're compiled. Let's compare your example to a normal loop: [Playground link](https://play.rust-lang.org/?gist=c2eee37ddad7ec27ef7c5e764258c70e&amp;version=stable) The loop code is longer, has more moving parts, etc. But if you look at the assembly, they actually are the *same* length. The loop version is technically shorter, but it calls into std which is hiding a lot of code. However, the map version inlines all of the code necessary - it even inlines the allocation of the vector. The same sort of thing goes for passing lambdas around. They're *extremely* efficient, usually as good or better than their plain-loop equivalents. They dramatically simplify your code, they dramatically reduce the amount of memory management you have to worry about (e.g. the loop required a mutable variable), and they make the code more readable (you only have to learn what all the iterator functions do... which is usually easier than trying to figure out what nested loops are doing, and you only have to do it once). And on top of all that, it all still works *even when passing lambdas to libraries*. Very few languages have this much aggressive, effective optimization of lambdas without using a Garbage Collector. They're extremely powerful because they're extremely efficient compared to writing the same thing out using loops; and they're extremely efficient when used as callbacks; and they do a lot of memory management and state management for you, without having to use mutable variables, which will reduce bugs and the number of lifetime issues you encounter.
I just wrote something demonstrating one of the weirder things I've done with closures. What it boiled down to was a sort of a poor man's way of materializing a collection via internal iteration. I hope that made sense. let mut vec = Vec::new(); weird_collection.for_each(|&amp;item| vec.push(item)); Another one I don't use often is broke-ass partial function application. For instance: fn build_point(x: i32, y: i32) -&gt; Point { Point { x, y } } let build_x5 = |y| build_point(5, y); let point = build_x5(6); // Point { x: 5, y: 6 } A third strange option is to save some kind of state for later by moving objects from the closure's environment into the closure itself. This is handy for deferring something you want to do until some later time, but I'll just go ahead and admit I can't think of a good example of that off the top of my head. In the end, a closure is a little chunk of scope and an associated function--though, in many cases, the scope itself is empty. (A lot of people call that version a "lambda.") You can do whatever you like with that, but they are fairly simple. Closer to "ball peen hammer" than "carbide-bladed chainsaw with nuclear propulsion." So, you know... Slightly more mysterious than an ordinary hammer, but not much.
You can pass them around let somebody else call them too. If you've never used them in any language before then you should see what you can do with them in Javascript.
Ah. I suppose I would've done well to click the link. :)
Hyper is pretty low level. Depending on your use case, a higher level library may be better. You'd have to describe your use case though, whether you are writing a server or client for example...
I used this extensively in c#, I have this library of 'policy' objects. Our application is essentially this big polling loop which comes around again constantly. The problem is we have things like 'only run this within the poll once every 5 minutes' or 'run this the first time through the loop and then never again unless this *other* policy object is tripped' etc etc etc. These policy objects are almost all the same and work the same way, some internal state, some condition to deal with things, and that's it...but what they *do* when these policies are tripped are very specific to the instance it's being used. This allows us to rap up these policy objects and then the 'does this when triggered' bit it just a closure. It's amazing how it cleaned up code. Lots of bit flag's poofed, timers went away, etc etc etc. All bundled up in easy to understand policy objects which can be chained together and combined in reasonable and obvious (type safe!) ways.
Facebook is not doing anything illegal. Their users are willingly using their service. None of us have to work there, but we should at least be happy that they are pushing Rust forward by open-sourcing their work.
Alright. Thanks for the detailed response. Sounds like tarpc would be awesome for pure-rust projects and for experiments / quick mockups. I definitely have some such use cases in mind in the future. I'll try it out when I get around to it. It doesn't sound like it would be right for the project I am working on right now, though. 
&gt; If you do end up trying multiple it would be extremely helpful if you published the results. I am pretty sure capnproto would be most appropriate for the project I am working on right now. But based on [this comment](https://www.reddit.com/r/rust/comments/7xyu88/tarpc_vs_capnprotorpcrust/dud0fqi/) by tarpc's maintainer, I am interested in trying tarpc eventually for some future ideas. I'll publish results if I ever get around to that and have experience with both.
&gt; Facebook is not doing anything illegal. I certainly wouldn't go as far the alfred, but this is a poor defense. Facebooks practices are regularly ruled illegal. They are pushing boundaries.
I'll add: The main way closures are really intended to be used is borrowed from functional languages. Map() is a lot more powerful than you may realize. For example, what would be a null-check in c or C++: SomeClass* foo; // ... if (foo != nullptr) { if (foo.bar != nullptr) { foo.bar.doTheThing(); } } Becomes this in rust: let maybe_foo: Option&lt;SomeStruct&gt; = SomeStruct { maybe_bar: Some(baz) } // ... maybe_foo.map(|foo| foo.maybe_bar.map(|bar| bar.doTheThing())); Which can actually be written using rust's ? operator like this: maybe_foo?.maybe_bar?.doTheThing(); 
Besides the fact that Facebook's harvesting of sensitive information has been deemed [illegal in certain courts](http://www.telegraph.co.uk/technology/2018/02/12/german-court-says-facebook-data-collection-illegal/), I'm very clearly not interested in making an appeal to lawfulness (Palantir certainly operates within legal frameworks) or contractual free choice on the market (liberalism)
I hate programming challenges. I never got the point. "Write this 20loc program that demonstrates nothing". I get that others like them, but I never feel good when I walk away from them. One thing I really like, and encourage for others, is the "never ending program" challenge. Pick a project with interesting scope - a problem that is not solvable, and start solving it. For example, "build a web browser" is a project I would consider "unsolvable" as the web is an evolving thing, there are many many ways to display it, there are extensions, security features, performance, etc. Then you work on small components of it every day. Similar to the daily challenge, except instead of programming to solve fake problems, you're solving real problems, interesting ones, you'll stretch your abilities far more. I'm sure many people would like the mini challenge thing though, I just don't see the appeal.
Oh cool, thanks!
You can hand them over to other threads, which is basically what rayon is doing. So you pretty much define work to be done as a closure that has to be called. Than you build a queue of closures and a pool of threads takes closures from the queue and executes them.
GPL v2... unusual choice for a Rust project.
Lets go in layers First lets review the part you seem to be familiar with, mostly to make sure that we are using the same words. I will avoid using lambdas or closures at first: There's the thing we want to do and all the work we need to do around it. Say, for example, that we want to have a thing that will try something that sometimes fails, but because the failures are transient so we want to retry it at least N times, waiting T seconds between each try, before we give up. We would write something like this: fn retry&lt;T, E: Error, F: Fn()-&gt;Result&lt;T, E&gt;&gt;(f:F, times: ui32, wait_ms: ui64) -&gt; Result&lt;T, E&gt; { // The loop is for all BUT the last attempt, at which point we would return the error. for _ in [0..times - 1] { if let Some(r) = f() { return Some(r); } wait_for(wait_ms); } return f(); } There, that was easy. Now I can do the following code: fn simple_connect(addr: Addr) -&gt; Result&lt;Connection, IOError&gt; { // ... This is were we'd do magic } fn simple_connect_localhost() -&gt; Result&lt;Connection, IOError&gt; { simple_connect(LOCAL_HOST_ADDR) } fn robust_connect_localhost() -&gt; Result&lt;Connection, IOError&gt; { retry(&amp;simple_connect_localhost, 10, 500) } And that's that. Now lets do something else, lets code `robust_connect`. This one is a bit harder to do with only functions. See the problem is that retry doesn't know how to handle functions that take input, and as such cannot handle this. Functions can't access variables from other functions unless they are explicitly passed as parameters. So the solution is to create a new Retry function that takes in input and then passes it onward, something like: fn retry_with_input&lt;I, T, E: Error, F: Fn(I)-&gt;Result&lt;T, E&gt;&gt;(f:F, i: I, times: ui32, wait_ms: ui64) -&gt; Result&lt;T, E&gt; { // like the function above, but now we call f(i) instead of f() } Then we can simply do: fn robust_connect(addr: Addr) -&gt; Result&lt;Connection, IOError&gt; { retry_with_input(&amp;simple_connect, addr, 10, 500) } But that was ugly, and we have to create a new retry function for everything. Moreover it's ugly because we have to inform everyone how the code we want to retry works, and this might be problematic. It's not impossible but it's annoying and weak. But what if we had a way of taking the variables from inside a function and putting them inside another? And more importantly, only do this once, instead of every time we create a function? This is the power because then we can grab a block of code inside our function, put it in a closure and have it run somewhere else! The closure stores the input and then uses it when called! So we could just use closures and implement a much simpler `robust_connect` fn robust_connect(addr: Addr) -&gt; Result&lt;Connection, IOError&gt; { // The closure handles moving addr around retry(|| simple_connect(addr), 10, 500) } This is powerful because we don't know how things are going to be later on, but it can get even more powerful when you realize all the things you can store in a closure. Imagine that you had a function that looked like this: fn stepping_function&lt;F: Fn()-&gt;(i32, Optional&lt;Box&lt;F&gt;&gt;)&gt;() -&gt; (i32, Optional&lt;Box&lt;F&gt;&gt;) { return (0, Some(Box::new(|| { return (1, Some(Box::new(|| { return (2, Some(Box::new(|| { return (3, None; })); })); })); } Now you can do something like: let i, next_f = stepping_function(); while (let Some(f) = next_f) { println!(i); i, next_f = f(); } println!(i); The code above is ugly and weird and probably won't compile (and I think it's fair to say). But lets focus on the idea we are representing more than the thing. The code above will print 0, 1, 2 and 3, as each function call goes. It almost looks like a coroutine (or a generator!). Closures then allow us a way to define how generators would look (as part of the continuation). This shows the power of closures, by allowing us to store a bit of state on a function we can do so much more. The alternative would be to push everything into a global, and that is its own challenge. In short, lambdas and functions without closures are really elegant, and they are most of what you need to get 80% of the iterator code working. But the other 20% and some more strange situations need your functions to know a bit of the state (variables) of the function that created that function/lambda. The ability to this is called closures.
Thanks! I think it's pretty cool too! For the selenium use case: I don't see why not! It wouldn't be too hard to add a new filter type for something that uses css selectors or otherwise could parse html in a more structured format. I'll investigate a couple of crates and see if there is anything out there that does this already ([kuchiki](https://crates.io/crates/kuchiki) seems to be the most "polished" but sparse on doco). I haven't seen termstyle yet, it looks useful! Being in YAML format, could definitely be easy to integrate. I do need to redo the printed output at some point since the library I am using appears to be [not actively developed](https://github.com/LukasKalbertodt/term-painter#status-of-this-crate). 
Closures allow a function to be extended. That's powerful. Without changing a function, so long as that function takes a closure, the caller is able to extend the power of that function in a non-breaking way.
Note that the `?` notation actually returns from the current function if the `Option` is `None`, while the `map` just doesn't do anything (and the parent function can continue).
I am trying to build and connect a Rust library to an iOS app that I am creating right now. I'm using cargo-lipo to be able to do this. However, when I start going through the details (I used this guide https://mozilla.github.io/firefox-browser-architecture/experiments/2017-09-06-rust-on-ios.html ), I suddenly reach an error. $ cargo lipo --release error: failed to run `rustc` to learn about target-specific information Caused by: process didn't exit successfully: `rustc - --crate-name ___ --print=file-names --crate-type bin --crate-type proc-macro --crate-type rlib --crate-type staticlib --target aarch64-apple-ios` (exit code: 101) --- stderr error: Error loading target specification: Could not find specification for target "aarch64-apple-ios" | = help: Use `--print target-list` for a list of built-in targets Cargo exited unsuccessfully: exit code: 101 $ xcrun --show-sdk-path -sdk iphoneos xcrun: error: SDK "iphoneos" cannot be located xcrun: error: SDK "iphoneos" cannot be located xcrun: error: unable to lookup item 'Path' in SDK 'iphoneos' I do have Xcode updated, so this shouldn't be a problem for me. Are there any possible solutions to this? I am rather new to Rust, and I do need to use it for a project that I am handling right now. 
Thanks, I didn't realize this project was open-source. I was curious to see what third-party crates Facebook is using, though I don't see a Cargo.toml there at all so either this repo is incomplete (as the readme sort of hints at) or they're vendoring it all manually.
Even if people are squeamish about copyleft vs. permissive, GPL is perfectly acceptable for applications like this, as distinct from libraries, because you probably aren't linking to an application. I'm just thankful it's not using that weird React license...
Caveat: I don't have any personal experience with GUI libraries in Rust yet. I tried out the demo examples for [libui](https://github.com/pcwalton/libui-rs) this week and it was trivial to build (at least for me on Fedora 27). The demos look good. I don't know if it supports all the widgets and whatnot that you'll need for your project but if I were you it's where I would start. If libui didn't do what I want and I couldn't figure out how to add the missing features, then I would look at this list for other ideas: https://github.com/rust-unofficial/awesome-rust#gui
Oh, I didn't know it returned. I've only ever used it in lambdas. Good to know!
I was referring to the fact that Palantir and Facebook share a rather influential board member. That seems like an important fact if drawing a comparison between the two companies. I'm confused tho: why exactly are you linking to an article about Peter Thiel's sexuality?
Because most times I've seen criticism of him has come from ridiculous political point of views but now I get your point about palantir. Definitely not a job for everyone.
the libraries I have tried so far do make it pretty easy to design a 2D ui, and come with great examples. however, I think it may be significant that none of the examples in ui libraries contain any 3D elements. when I try to combine them with my simulation, I end up getting cryptic errors from deep within the internals of GL. ty for the list of libraries though, looks like a good resource to work my way through.
I should have googled it before suggestion libui: https://github.com/andlabs/libui/issues/34 It sounds like someone started working on it, made good progress for a while, but failed to complete the task.
Where is asyncio/future/tokio WG?
&gt; I'm just thankful it's not using that weird React license... It's a [straight MIT license](https://code.facebook.com/posts/300798627056246/relicensing-react-jest-flow-and-immutable-js/). They changed it due to the complaints.
no worries. I think part of the problem is that none of these libraries are stable or close to a 1.0 release, and they all pull in so many dependencies. just gotta try them all and stay tuned for more updates I guess.
What about a "library of the week" where we all try and make something/contribute to it. Kind of like an ongoing lib blitz?
Those projects are not governed by the Rust Project. The closest bit that is is async/await, and that’s under the lang design team.
We mods do. At the moment, Rust is deemed experimental by many, so we tend to cherish investment by any of the big players. That doesn't mean any of us hasn't got mixed feelings about any particular company, but working on a hg-compatible version control in Rust hardly seems evil.
oic，thx。
The plaintext benchmark is a very unrealistic one, it measures how well can you deal with a single client flooding your pipeline. On all the other tests you only get a new request once you have replied first so DB drivers, time to flush your reply, and many more items are now relevant.
Hey, dude, If you have time to recheck my project, you will find out I have removed all unneeded `mut` flag in functions signature. it does look better than before
Facebook regularly employs shady techniques to "addict" users to their services. 
Hey, what do you think about [this comment](https://github.com/tokio-rs/tokio-service/issues/9#issuecomment-266815643)? (I see that's from 2016 and perhaps things changed) &gt; Note that, in order for the future to have access to borrowed data from `&amp;self`, it cannot be `'static`. That means, in particular, that the data in self must outlive the future. Making this work requires something akin to scoped threads for futures -- which is something we want to (and know how to) provide, but haven't built into the core library yet. (@nikomatsakis has been working on a Rayon integration that does this.) &gt; So, in principle, we could move to a model that threaded through the lifetime of self for Service, and allow "pinning" the data outside of the event loop. Is this related to the recent pinning breakthrough from eddyb?
Fortunately SIMD is coming along! The RFC is in final comment period: https://github.com/rust-lang/rfcs/pull/2325 Not maybe early enough to percolate to stable and help with the libraries and frameworks for TechEmpower 16, but maybe 17!
&gt; Why? because
This is the "network programming" workgroup listed at the bottom! /u/desiringmachines and I are leading it.
Not powerful enough. Hopefully `impl Trait` will make it anything other than horrifically painful to implement any kind of combinator.
Right, I feel that explicit const is the best option. I could *imagine* a works in which "could be used as const but aren't annotated" functions ... could be used as const, with an error-by-default lint warning you that you're opting into behavior that the function doesn't guarantee. The idea seems extremely risky from an ecosystem stability perspective, but it is an option that I don't recall having seen discussed seriously. I would be curious how big of a deal this has actually been in the D community.
This is a very well known library. Posting ima link to it without any comment seems weird. Did you mean to link to the 1.0 announcement? (I think it was already posted a few days ago, though.)
Hahaha, now I see the *real* use case for `#[doc(include = “file.md)]`!
`Iterator::partition` does exactly what you want.
Nitpick: I feel like this would be better written with `and_then` to avoid nested closures: maybe_foo.and_then(|foo| foo.maybe_bar).map(|bar| bar.doTheThing());
They will probably either find someone in bootcamp who knows Rust or set up an interview specifically for this role with people already on the Mononoke team.
They cannot recurse. It might technically be possible using some trickery like the Y combinator (if it even can be expressed in Rust's type system).
Everyone knows they’ve been working on migrating core mercurial stuff to rust since quite some time now. Coincidentally, some people in openjdk have also complained about mercurial’s slowness recently. I hope this work at Facebook sees the oss limelight. 
The best thing is probably to get a hold of hergertme via Twitter or IRC.
They are doing many illegal things. Belgium just won a court case against Facebook that allows them to fine Facebook for 250,000€ per day until they fix their shit. Read more here: https://www.reddit.com/r/technology/comments/7xyszx/belgium_wins_trial_against_facebook_for_violating/
I got intrigued, since I never ran into this limitation while coding. I tried get a recursive Fibonacci working that would have to be invoked as something like `fib(&amp;fib, n)`, but I always end up creating infinite types. The type of `fib` becomes something like `Fn(A, usize) -&gt; usize`, where `A` is `Fn(A, usize) -&gt; usize` again, ± some references, boxes, etc. Can anyone get it to work?
With Rust on board they will suck way more efficiently now !
Finally you can generate those asciinema videos which are looking as good as your documentation. And of course, `termbook` gives you the ability to keep all documentation tested and in one spot, while producing a variety of outputs to help your project to look awesome.
Oh, that is nice to hear! Are you aware of more recent benchmarks?
Hi, I'm also working the music notation language: https://github.com/y-fujii/memol-rs/ I'm a bit astonished that both of them are written in Rust, share the similar goals (focused on MIDI rather than score typesetting) and share some language features (specifying note durations by dividing a unit measure), ...and similar names. I'm impressed the readability of melo. memol allows the nested structures and I love this feature, but sometimes I feels it might be overkill.
congratulations,hope 2018 is the rust network year.
lol gamedevs on suicide watch, right?
It's already available, though possibly not in full: https://github.com/facebookexperimental/mononoke
Yes I meant the 1.0. And I didn't see the other post.
Thanks!
Is there a reason you didn't change the second map to and_then?
If you know exactly what your memory profile is (i.e. upper bound in worst case) that can be a useful allocator. Very niche application, but not "completely useless". Java works currently on a GC which won't reclaim any memory: http://openjdk.java.net/jeps/318
Great idea! You should submit a pull request. It might even make it into the 2018 Epoch.
It astonishing to see you managed to write this much text but were not able to read that this subreddit is about the Rust programming language and not the game.
Next up: org chart, suits, IPO. In that order.
I know the guide says they are going to switch when async stabilizes. I wonder about if they’d consider using the hyperium-http crate for types.
It's happening, though. After 20 years of (predominantly C++) programming, I'm starting my first professional Rust job in a week.
You’re looking for /r/playrust
Here's the actual Rust Programming Language Discord: https://discord.me/rust-lang
Stop attacking others on this subreddit.
And such childish behavior is also not likely to win you any favors here.
Not all applications require database connectivity for all pages. So it's clearly uncommon, but not at all unrealistic.
cool, much appreciated! can this feed the triples into existing triple stores or other rdf utilities? so far, i haven't seen much rdf stuff in rust, but would greatly appreciate it. (the application i have in mind are javascript web applications that could have a fast in-browser backend for querying).
Touché ? 
You want /r/playrust.
I’m not incredibly familiar with this process, but maybe you’ve installed Xcode without installing the iOS SDK.
&gt; I'm writing a lua interpreter using Nom. Literally 80% of my code is lambdas, none of them nested. omg omg omg!
I'm not sure why it wasn't done initially, but doing it now would be a major breaking change requiring lots of manual updates to basically everything.
It's a SF stealth startup, they're using Rust across the board for the backend, and React for the front... when the official press release happens, I'll make a point to update this subreddit.
Thanks for pointing me in the right direction. I contacted him on twitter. For posterity's sake I'll report what I figure out back here.
How would you automatically implement `Debug` for: struct X&lt;T&gt; { len: usize, cap: usize, ptr: *mut T, } ?
It would be a large amount of code bloat, which would at best slow down compile times, and at worst make binaries unnecessarily large. But, it also breaks encapsulation. If every struct automatically implemented Debug, it would make it easier for code to bypass privacy restrictions by parsing the debug print string to learn the values of private struct members.
Is it not backwards compatible?
hi 1 rust job please
[removed]
You are correct, I forgot about that. I'll start doing that in my code when I use nested options!
You guys are right, it would just be bloat... But what if it only derived Debug inside the print/format macro? Or only in the Debug build
The lua thing? Don't expect much... It's mostly intended as a practice in compiler stuff. Whatever I do make, though, I'll release, since I know a lot of people are interested in a rust+lua in pure rust.
Update from author at https://github.com/andlabs/libui/issues/274#issuecomment-366457781 says he's back on it.
Yeah, *I think* the biggest difference between functional languages and rust for lambdas is that they're generic rather than polymorphic. So a *lot* more inlining and inline-type stuff can happen. I'm not familiar with the internals of functional language compilers, they definitely do a lot of optimization, so I don't know for sure...
I'll add the obvious caveats: only for types for which Debug can be safely derived, and for which static analysis can show that the `Debug` interface is actually used. I think this answers the obvious objections. /u/rustythrowa's "some things should never be printed" is a deeper and more solid objection, and I don't see how to get around it.
 X::&lt;T&gt;{ len: 10, cap: 20, ptr: 0xdeadbeafcafebead } No other option (well apart from where/how/if you write the T), that `ptr` might not be pointing to a valid object. 
&gt; If I have 10 structs like Person, would I have to manually add #[derive(Debug)] in front of every one of the them? Yes. I've found that most ^(haven't actualled measured to check if it is a majority) structs end up with a `#[derive(...)]` where `...` is some combination of `Debug`, `Clone`, `Copy`, `PartialEq, Eq`, `Hash`, and `Default`. It's not the end of the world.
I tagged the first usable release of my [`dedup`](https://github.com/cjm00/dedup) project! It's meant as a replacement for `sort -u` or `awk` to replace duplicate lines in a file and is already faster than either one and a great deal more memory efficient. Now I'm figuring out what features would be useful to add and how to make it even faster :)
Nice report. It would be good to be able to have the heap and the stack grow toward each other on tiny devices, though. This saves having to statically decide the split between heap and stack. Keeping the heap from expanding into the stack is reasonably straightforward. Keeping the stack from expanding into the heap seems to require an extra runtime check against the heap extent per-call, which is not zero-cost. It would be interesting to add a "max-stack-depth" option to Rustc, so that it could provide the maximum achievable stack depth to the runtime. In the normal embedded case, we don't recurse (directly or indirectly), so the depth should be finite and calculable from the call graph via whole-program analysis. A lot of ugly work for the compiler, and might not be possible in practice, but would sure be nice for low-memory embedded situations: could just start the stack pointer at the maximum stack extent.
Yeah, your link is the the main page. This one will help: https://www.reddit.com/r/rust/comments/7xr1x5/rayon_10_is_released/
&gt; Because most times I've seen criticism of him has come from ridiculous political point of views Is there a problem with that? We live in a political world and if we ignore politics, we ignore the world's problems.
that would only make things confusingly magical. really, this is not that big of a deal. If you want to derive a set of derives for every struct you declare, you can just write a macro and wrap your struct declarations in that.
That is a choice that some developers make, not all by any means.
I've almost literally made debug impls that print the above (everything except a binary tree structure instead of a vec), it can be far better than nothing. Presumably this would be implemented such that implicit implementations are only created if an explicit one wasn't. Or equivalently implicit implementations are overridden by explicit ones, a la [specialization](https://github.com/rust-lang/rfcs/blob/master/text/1210-impl-specialization.md).
What I find interesting is that if you can compute the maximum stack depth available, then you can deduce the maximum heap size available since stack + heap + bss = total available. I think the assumption that one should specify a maximum stack and heap sizes is reasonable for a low-memory embedded situation: you have to carefully use your device resources anyway, and the only way to guarantee a good run is to ensure the program will never need more. This protection is just the cherry on top: if you unfortunately face-plant, at least it's a hard error rather than some mysterious memory corruption.
I found this out while interviewing, but Rust is being used in Self-Driving Cars now!
It gets old really fast. You might think your views are rational and correct but you only need to browse Reddit to see the different tribal extremes. Bit everything needs to be passed through your political lens. Making at all political actually creates more problems instead of solving anything. Unless you somehow can impose yourself completely on others.
That would be more code to parse, compile, link, load that is possibly never used. Rust gives you a low-effort way via autoderive. That should suffice in most cases.
Note that the Firefox folks actually found that Debug derives could cause meaningful code size bloat: https://groups.google.com/forum/#!topic/mozilla.dev.platform/6p-i-beCr4U
Sorry for yet *another* plea for input on the design of `rand`. This is a long-standing question.
Private fields do not exist for the sake of privacy, but to enforce encapsulation. There are plenty of other ways to get access to private data members, so that should never really be a concern. Also, if you’re relying on private fields for actual data privacy, stop now.
No one is relying on them for data privacy, please don't put implications in my mouth. Your first sentence literally agrees with me, and then you go on to say that encapsulation is pointless.
So you want to encourage the proliferation of sensitive data being leaked into logs? Logs are perfectly benign, but autoderived Debug will happily dump all kinds of things into the logs that should never be printed. The logs are an example of one of many clients that should not be allowed to break encapsulation using Debug. This isn't about "finding ways" around encapsulation. This is about accidentally making it too easy for people to depend on private state, which makes SemVer even more tenous than it already is.
\**enlightened centrist voice*\* surely, we should be nice and work together with a racist. no need to be political
Thanks! 
Oh..."a racist"... Yeah. You're exactly the problem. You're the type of person I was talking about. Goodbye. Go spread your extremist bullshit around in peace. I'm sure you're solving so many problems like that...
That and encapsulation in this context is very, very rarely used to actually hide data. It’s purely a language construct to make it harder to accidentally or intentionally mess with the internal state of complex systems. I personally think that Debug should be implemented on any type for which seeing that debug information could potentially allow a developer to solve a problem. If it’s on something that would be unintelligible, like encrypted or compressed data, then there isn’t really a point. 
Is there any possibility to enable derives (and certain code parts that use it) only for debug builds?
Release builds use `Debug` derives all the time in print statements... so no?
Couldn't it be done lazily?
Sure. We could impl `Debug` on demand. But then you'd have to extend the compiler, the RLS, clippy, IntelliJ and probably a dozen other tools.
Not to defend Facebook nor bash them, just note that what is legal isn't always moral and vice versa. E.g. holocaust was legal, marriage between white and black was illegal. (I guess two examples are enough, but I can provide more. :))
For example exposing inner state of crypto primitives could be quite harmful, this is why in [RustCrypto](https://github.com/RustCrypto) we use opaque debug implementations, e.g. `Aes128 { ... }` instead of `Aes128 { encryption_keys: &lt;round keys here&gt;, decryption_keys: &lt;round keys here&gt; }`.
Passwords, pointer addresses.
But if it is never used, then it doesn't get collected and no code for it will be generated. I don't think anything before the codegen actually takes much time at all.
&gt; Oh..."a racist"... &gt; &gt; Yeah. You're exactly the problem. You're the type of person I was talking about. &gt; &gt; Goodbye. Go spread your extremist bullshit around in peace. I'm sure you're solving so many problems like that... &gt; These days people seem to get more offended by the word "racist" than racism itself. Apparantly acknowledging the problem and identifying funders of it is worse than the act itself. Amusing.
IPO? Nah, either ICO or IPA.
I tried running the example but for some reason it fails. I'm using Rust 1.24 on Windows. It keeps giving me an error that unwrap was called on an Err. Additionally, I built libui with cmake but I have no clue what to do with it.
I don't think this is true. We can do the same thing that Clone does with Clone shims.
My understanding is that auto-deriving would only occur when manual impl existed (this would be different from specialization, because the automatic impl would never be generated). Let me rephrase my question: Given that a `Debug` implementation doesn't exist for a type, could anything bad result from automatically deriving one?
Usually, there isn't a single stack on embedded devices. There are many for the several threads. On an ARM Cortex with an MPU, you can get the hardware to do this for you, by setting up a small stack guard at the end of the stack. The MPU is set up such that a very tight instruction sequence can be used to set up the guard mini-page during context switches.
Sorry, I didn't mean Rust specific tools I meant tools from javascript and python. With there not being Rust stuff we need to look at what utilites other languages have found helpful.
I got tired of seeing Java updating, so I wrote a clone of PortMapper (the only application I was keeping Java for) - https://crates.io/crates/portforwarder-rs
Just encountered a situation where I had to manually implement `Debug`. Cyclical data structures. 
OK then. Perhaps in the new period?
You could also do: let something = match something { Some(x) =&gt; x, None =&gt; return None, }; A bit verbose, but it's a bit nicer than `is_none` + `unwrap`, and you can easily choose what to return in case of `None`.
Maybe it could be made even lower effort via tooling to add the auto-derive automatically when static analyses determines it's missing. But yeah this is a pretty minor usability win.
Tell that to winapi 
&gt; These days people seem to get more offended by the word "racist" than racism itself. The problem is that your calling something ism doesn't make that ism true so people get offended at "ism" being used as weapon. Even more so, it trivializes actual racism. &gt; Apparantly acknowledging the problem and identifying funders of it is worse than the act itself. Amusing. Let's reverse it so you understand: If I call you a nazi and you get offended by it then I claim you get offended more by me saying nazi than actual nazism... See how the whole premise of ad hominem bullshit used as a debate works? &gt; Edit: Also, I am curious. How are you solving the problem by refusing to write off any individuals as problematic? I'm not the one wanting EVERYTHING to be politized to "solve problems". That's YOUR claim [here](https://www.reddit.com/r/rust/comments/7y1r8d/facebook_is_hiring_for_a_rust_developer/duedua2/): &gt; Making at all political actually creates more problems instead of solving anything. Unless you somehow can impose yourself completely on others. Also... "problematic"? YOU are problematic. Your witch hunt attitude is "problematic". Should I ban you? Demonize you? Gather a mob to hound you? Nah. I'm gonna let you be but I will probably decide not to engage in further discussion. Specially considering we're in a programming subreddit of a language that's supposed to have a friendly community and it's interesting as fuck. No need to bite the bait of extreme political ideologues. 
&gt; It would be good to be able to have the heap and the stack grow toward each other on tiny devices, though. That's the default behavior, as in if you don't use cortex-m-rt-ld. &gt; It would be interesting to add a "max-stack-depth" option to Rustc .. I think that it might be impossible to do that in rustc. It's only LLVM who knows how much stack space a function really uses because of optimization and inlining, and LLVM doesn't provide `-fstack-usage` functionality like gcc does. (To elaborate a bit more: this is more of a pipelining problem because rustc analysis ends before it sends LLVM IR to LLVM) Also any call graph you build in rustc from source code wouldn't be entirely correct because of inlining performed in LLVM. To this add the fact that any use of function pointers or trait objects will make it impossible to build a complete call graph, and that any use of alloca (not yet in Rust but will eventually land) will make the stack usage of a function impossible to determine at compile time. Finally, rustc doesn't know about interrupts handlers which are like functions the hardware can invoke at any point in the execution of a program in response to external events, and some architectures even allow interrupt nesting. So, yeah I think it would be impossible for rustc alone, but an external tool that can get an imperfect call graph from rustc, per function stack usage info from LLVM (somehow) and that has more domain specific knowledge about the application might be able to do whole program stack usage analysis ... in some cases. 
Is it? Facebook always tries new things.. they have Haskell devs too
I think there is a greater risk that crate implementors can forget about this channel of leaking sensitive information. Of course they can do the same with derive, but at least they'll have to write it explicitly. But I think original reason why such auto-derive is not implemented is concerns about having a lot of unneeded auto-derived code, while "lazy derive" which could solve this problem does not currently exist in the language and its introduction will be quite complicated without enough benefit to warrant it.
&gt; YOU are problematic. Your witch hunt attitude is "problematic". Hey, hey, hey. You were the one who started with postulating that anyone who has a problem with Peter Thiel is an extremist.
I think it says a lot about Haskell that they have built their spam detection platform in it.
It also works for me, albeit slower than Chrome (because it has rely on shims)
It isn't 100% perfect and what you want but you can use `cfg_attr` with `debug_assertions`. #[cfg_attr(debug_assertions, derive(Debug))]
I think the best thing to do would be to have a `Dump` trait that's autoimplemented for everything; and it redirects to `Debug` when possible.
&gt;Let's reverse it so you understand: If I call you a nazi and you get offended by it then I claim you get offended more by me saying nazi than actual nazism... See how the whole premise of ad hominem bullshit used as a debate works? This is a strawman of the argument. Yes, it works like that ... if we lived in an alternate reality. I call him a racist, because I have evidence to back it up (for example, take a look at the candidates he has financially supported: https://en.wikipedia.org/wiki/Peter_Thiel#Political_activities). &gt; Making at all political actually creates more problems instead of solving anything. Unless you somehow can impose yourself completely on others. Put very simply, the world is political, and it is foolish to pretend it isn't. The idea that we can unite beside our political ideologies and somehow solve the world's problem is a joke, has never worked and will never work, especially when one side is essentially a big part of the problem. It's very simple: If you are able to avoid politics in your life, dare I say it, you are a privileged individual in the society. If you're able to get through a day with the most annoying thing being the word "racist" as opposed to racism itself, you're very lucky. However, millions of people live as victims of systemic racism, and people still cry about getting called racist. &gt;It's very simple: If you are able to avoid politics in your life, dare I say it, you are privileged. You seem to discredit "extremism" without any actual backing counterargument, as if it is was a virtue itself to be as apathetic to political issues as possible. This is a recurring theme with centrist, but you cannot have a consistent ideology by avoiding taking stances, because the apathy is a stance itself.
It unrealistic not because it doesn't use database, but because you don't optimize to flood your clients that are pipelining requests. It's more realistic to reply to one client then to another but to go well on this test you need to reply a bunch of request at once.
I have done that with piston and gtk, communicating via mpsc channels. It worked really well.
Oh fuck xD.. The worst thing is that I know the /r/playrust subreddit, it's just that when I typed on the URL barre /r/playrust, I just forgot the "play".. This led me here, and since the page loaded I didn't bother reading the rest, I just wanted to write my idea an continue to play... Now let's repost this shit one the right subreddit.
Thanks!
I guess that depends on what you prefer your code to look like. I think your `map` example is ugly as hell, especially with the layers of parentheses. It's personal preference but I think pattern matching is much better for this. if let Some(foo) = maybe_foo { if let Some(bar) = foo.maybe_bar { bar.doTheThing(); } }
This isn't really an "easy question", but how does one go about isolating compiler bugs? I compile my projects on an ODROID-XU4 (a small ARM board, like a more powerful Raspberry Pi). I've noticed that the stable 1.24.0 ARM compiler for Rust has some kind of bug - it sometimes segfaults. Compiling the same project on the 1.23.0 compiler does not. Sometimes if I try compiling again (on 1.24.0, current stable) it works. Sometimes it gets "stuck" and no matter how many times I try, it won't compile. The specific crate in my project that it fails on also seems arbitrary. Is there anything I can do to help track this down? It's a somewhat large project and I have no idea where to begin as far as isolating the problem to a small test project.
Sure 
Sure
&gt; now a useful implementation cannot be implemented because it would conflict with the default one I think the point of their post was that `Debug` would only get auto-derived when a manual implementation doesn't exist, and an obvious optimization of that would be to auto-derive lazily, that is, only when `Debug` is used for a type does the compiler auto-derive it.
I might have misunderstood, but isn't the first example essentially equal to `let vec = weird_collection.iter().collect::&lt;Vec&lt;_&gt;&gt;();`? (Where `iter()` is a method that gets you an iterator of the contents of the collection, just like `Vec::iter()`.)
You could keep prints that use the debug trait to usage of the [`debug!`](https://doc.rust-lang.org/1.1.0/log/macro.debug!.html) macro.
That's interesting, although I still think it's less clear than just using two "if let" statements. It's generating the same code after all, might as well use the more clear syntax.
File an issue on github, they'll probably have debugging suggestions. If you get a core dump (you may have to enable core dump generation in your linux), you can open it in gdb to get a backtrace after a segfault. If you can try compiling the code on an x86 system, that compiler may fail in a more informative way. You can enable debug logging with the environment variable: `env RUST_LOG=rustc=info cargo stable build`, though it may be difficult to filter it down to something useful.
Lurking over from /r/cpp this really made me laugh. Last week I tried to roll a „quick“ implementation of Argument parsing. Requirements just kept stacking up and I eventually decided to use some library.
Out of curiosity though kinda off-topic, how does your data structure achieve cycles? `Rc` + `Weak`, something else? (I’m in the middle of writing a blog post about `&amp;T` cycles in an arena, and implementation of that arena.)
I have a struct that is essentially: struct Thing { parent: Rc&lt;RefCell&lt;Thing&gt;&gt;, contains: Vec&lt;Rc&lt;RefCell&lt;Thing&gt;&gt;&gt;, } It at certain points is a cycle. Its a first attempt solution that is going to need to be rethought. When calling `println!("{:?}", Thing a)" it had a stack overflow, so I had to implement `Debug` so it only printed upwards.
If I had to guess, I'd say that it's because the sock_writer is borrowed from the hash, and passed to write_all, which returns a future. The compiler has no idea when this future will be executed, and the borrowed sock_writer may be freed by that point. As for a fix, I couldn't say. Sorry
I've installed the iOS SDK properly, which is the strange part.
Not in a backwards compatible way. You could make a new "AutoDebug" trait with that property though. 
Which is definitely great.
This is an awesome idea!!
Haha.I like Haskell, but this is a good jab lol.
Haha.I like Haskell, but this is a good jab lol.
&gt; accurate I understand the problem of borrowing, however, i don't catch the solution with the mpsc that you propose me, if i remove the SharedState, how i could do to have the other client in memory and send data to them ? Can you make a quick example ? 
Which unwrap? Can you copy and paste the error? You don't need to build libui yourself - cargo does that for you.
I haven't touched it for a bit because I've been working on other things, but I created [Anterofit](https://github.com/abonander/anterofit) in an attempt to make this task easy. I'm not really sure if I accomplished what I set out to do because no one's used it extensively enough to give me good feedback, and I don't actually have a real use-case for it myself right now. I've also been waiting for proc macros to stabilize as it'd make the `service!{}` macro implementation a lot simpler. I could use `proc_macro_hack` but I'd rather just wait for stable proc macros so I can implement it as an attribute.
Wrong sub my dude, check /r/playrust
YOU DON'T NEED TO SHOUT, YOU KNOW. IT JUST MAKES YOU COME ACROSS AS DESPERATE. ALSO, IT'S ACTUALLY EASIER TO READ MIXED CASE TEXT THAN ALL-CAPS, SO YOU'RE REALLY JUST MAKING THINGS HARDER FOR YOURSELF.
Hey, this looks really cool! I'm also surprised at the similarities - I even had a few other similar ideas (octave-shifting for one, and score symbols). Memol's nested structures are cool, and it's definitely more expressive with things like value tracks, and the `with` syntax. If you would be interested - do you have Discord? I'd love to talk more about these languages :)
For folks interested in SBE style serialization, there is the [Abomonation](https://crates.io/crates/abomonation) crate which gets really quite decent numbers. It is only for communication between instances of the exact same binary, though, because Rust lacks a guarantee about the stability of struct layouts between builds.
You can look at github-es and egg-mode for examples of full-features and interesting API wrappers.
You can remove the might. Winapi used to implement Debug, but now its an opt-in feature, because it slows compilation of everything that depends on it significantly.
Closures are not generic, so not that powerful.
Thanks a lot. Shame on me for not being able to find that.
You could do it with procedural macros if it fits your use case, but it is quite a bit trickier to make work right now.
Fixed it with `concat!` for now. Not that pretty, but works :)
This would be awesome, hope it happens.
I was too fast there. I doesn't work, sadly, but I don't get why: match values.contains(&amp;stringify!($attr_value)) { true =&gt; write!(out, " {}={}", stringify!($attr_name), stringify!($attr_value)), false =&gt; compile_error!(concat!("value \"", stringify!($attr_value), "\" is unknown for attribute \"", stringify!($attr_name), "\"")) } This *always* throws the compile-error. On the contrary, if I replace the `compile_error!` with `write!(out, "")` it takes the correct match arm and writes or not. Can anyone explain what I am doing wrong here?
`compile_error!` isn't magical - it's just a macro like any other, but it aborts the compiler as soon as you try to expand it, and in your example both `match` arms need to be expanded, and the `match`-ing itself happens at runtime, not compile time, so it works with `write!` because that expands correctly.
Plus the 9 other books! Incredible.
That makes sense. You probably want to use `std::rc::Weak` for the `parent` field, so the whole graph doesn’t leak. (Unless you have a `Drop` impl at the root that undoes the cycles.) This would also solve your `Debug` problem as a side effect.
Very nice!
Be sure to post to /r/rust_gamedev too!
That's beautiful
Hmm, can't say for sure. I'm on my mac right now and can't access my linux PC, but I'll check when I get there.
It's not on you to decide who goes away and who stays. Calling someone a racist who has a record of racist behavior is a statement of fact, not extremism. All of you, please step away from the keyboard to take a few deep breaths, please.
Nice :)
I feel like these apply to more derives than just Debug. For example serde. If I want to serialize my struct so I can save it to disk I need to go around annoying all dependencies with "pls optional serde support?" issues. Making some derives be by default (unless when not used, so compilation speed doesn't suffer that much) would be awesome! Somebody write an RFC for this :P
There's the `dont_panic` crate that does exactly what you want. Really, `compile_error` is only useful when using conditional compilation.
Only read a few lines, here’s some general advice: - make sure to run clippy! - instead of treating -1 as a special value, use `Option&lt;i32&gt;` (or another, unsized integer type if that makes sense)
When I try to cargo build an example, it fails compiling ui-sys. Compiling ui-sys v0.1.1 error: failed to run custom build command for `ui-sys v0.1.1` --- stderr fatal: Not a git repository (or any of the parent directories): .git thread 'main' panicked at ' And somewhere in the trace, this appears: 10: build_script_build::main at .\build.rs:15 By the way, I seem to fail finding on how to open an issue on your rep : /? 
there is a pretty good rdf library under development at https://github.com/linkeddata/rdflib.js/ , which not only implements a triple store and parsers, but also contributes to developing a standard (javascript) api to rdf at https://github.com/rdfjs/representation-task-force/blob/master/interface-spec.md , which might be a good starting point for developing a rust api as well.
I already did step away after stating I would and why in a comment further down. I don't know if you read it all. &gt; Also... "problematic"? YOU are problematic. Your witch hunt attitude is "problematic". Should I ban you? Demonize you? Gather a mob to hound you? Nah. I'm gonna let you be but I will probably decide not to engage in further discussion. "Specially considering we're in a programming subreddit of a language that's supposed to have a friendly community and it's interesting as fuck. No need to bite the bait of extreme political ideologues.* As for the rest, Calling opinións facts is the Hallmark of someone who doesn't know or want to debate. I strongly disagree with your views and the intense politized nature of many places is a terrible sign IMHO. It's a sign that will push people away because they're not into the extreme ideology you follow. I will avoid these discussions in the future cause there's no point. Which is something extreme ideologues like the person I was talking to disagree. Everything needs to be political and you need to shove your opinion down our throats. So... Yeah. Back to stepping away. If you still want to engage in discussion PM's might probably be best or we will foment more replies in the thread that well have to ignore.
Great!
In `rand` we have dummy implementations of `Debug` (printing just the type name) specifically so that cryptographic generators don't unintentionally leak their state in log messages.
Thanks for your comments! The private fields thing is something that comes out of habit as a Java dev where for testability reasons I prefer private fields + mockable getters. &gt; On the topic of get_npcs: this can just return &amp;[Npc] (without changing the function body). The caller doesn't need to know the collection is implemented as a Vec. This is nice to know! I'm still a tad noob with slices and such.
Ok it seems to work but i don't know why, it not the same things than my solution ? We put an tx identified by send_to_this_client and borrow it on the function send_data_to_clients but the tx can also be freed at any point no ? What is the difference ? And thanks for your solution !
Is Option&lt;i32&gt; the same size as i32 (4 bytes)?
No, it's usually 5 bytes, 4 for the integer and 1 for the tag that is either `Some` or `None`. The compiler might be able to pack the Option's tag into some space that's otherwise unused in a correctly aligned struct, though. If you *really* want this to be 4 bytes, you should probably make a new type `OptionalInt`, that gives you an Option-like interface and encodes the `None` case in the integer range (e.g., `i32::MIN` means `None`, otherwise it's `Some`). This is more idiomatic and easier to work with than sprinkling `if x == -1 {} else {}` in your code.
Crashes the tab in Safari 11.0.3 (on macOS 10.12.6), but looks amazing in Chrome!
I was going to write some witty retort, but I still have my moderator hat on, so here goes... Sorry if I made you feel unwelcome. This is *not* a witch-hunt, and my comment was targeted at everyone in this part of the thread, not only you. Given that for better or worse, I'm now part of this thread, I'll follow your lead and leave it at that.
&gt; Is Option&lt;i32&gt; the same size as i32 (4 bytes)? Not with Option, but if you want to use the -1 as a sentinel value to represent "None" then you can just use https://crates.io/crates/optional and either use it with i32 or implement the None trait for your own type.
Interesting just you know it works ok on Safari 11.0.2 (on macOS 10.13.2).
That seems like the correct result when converting from (100, 0, 0) in linear RGB space to sRGB.
Am I thinking wrongly about what the srgb color space is for then for the output target? Because I obviously don't want the colors to be washed out so when I "input" (100, 0, 0) as the color I want this exact color to be on the screen, but that is not the case, no matter what color format I choose, it always results in (168, 0, 0)
One way to get correct colors is to produce your assets in linear RGB, process them in linear RGB in your shaders and then finally convert to sRGB (as you do now) in the end. It is generally not recommended to use sRGB when calculating with colors.
If I understand you, what you're describing would fall under the "intermediate documentation" bullet point that is a major emphasis for rust in 2018
I think the rant is valid, and i think its an area where rust's std does a good job, and *some* crates do a good job, but not all do. It's an aspect of being very early on, crates being less than 1.0, and the people working on the crates not thinking to make these examples. From my personal experience, i am looking at [Nom](https://github.com/Geal/nom) for a personal project, and coming across this little post with an [example for how to parse a date](https://fnordig.de/2015/07/16/omnomnom-parsing-iso8601-dates-using-nom/) was super helpful having *no* experience getting into parsers. I know talking to some other people they have issues with Failure's documentation for similar reasons you describe. While the Failure book may tell you how to do something, it doesn't have concrete examples, and anything it shows is largely made up. Real world examples and small use cases just help a lot when picking something up, especially for people picking it up for the first time. That being said, there are resources being done to show how to solve common rust problems and introduce crates that you should use. An example is the [rust cookbook](https://rust-lang-nursery.github.io/rust-cookbook/) and the rust team takes docs very important, and this is one aspect of documentation.
I haven't used gfx per se, but in OpenGL, you need to explicitly enable the flag `GL_FRAMEBUFFER_SRGB` using `glEnable` if you need linear to sRGB conversions when writing to the framebuffer. This option is IIRC independent of any color formats you define. Is it maybe possible that gfx enables this flag without your knowledge (as some kind of default), and you need to explicitly `glDisable`it. 
I ran into this problem, and none of the other solutions were right. You probably have a `.cargo/config` file that's telling it to cross-compile xargo, which it can't do. Move the config out of the way first. Here's what worked for me: ``` if cargo install --list | grep -q xargo; then : else echo --- installing xargo mv .cargo/config .cargo/not-config cargo install xargo mv .cargo/not-config .cargo/config fi ```
try install libncurses5-dev libncursesw and libncurses are slightly different from what I've seen
He actually doesn't need Option&lt;i32&gt;; he's using the number as an index into an array, and the array does not need values up to 2147483647. So an Option&lt;u8&gt; would work perfectly (which is 2 bytes).
Writing a blog post summarizing your experience and collecting the examples you found would be a step in the right direction towards better intermediate documentation/blog posts.
It works because there's a _single future task_ which owns each writer. In your original code, you had every `send_data_to_clients` function instance sharing the writers. The mpsc stream fixes this by kind of "buffering" things to send to each client inside the stream, so only one thing is sent at a time. The mpsc stream also allows multiple producers, as per its name (Multiple Producers, Single Consumer). For the second question, it's part of how `io::write_all` works. It takes in a byte buffer to write, and a stream to write it to, and it will consume both of them (it owns both the stream and the buffer after you call it). Once it is finished, it returns both the stream, and the buffer in case you wanted to re-use the buffer. What `.map(|(writer, _bytes)| writer)` does is basically throw away the bytes buffer, so all that's left returned is the writer. This also plays into what `fold` does. In procedural pseudo-code, it's basically: fn fold(iterator, state, func) { for item in iterator { state = func(state); } return state; } The 'state' in this case is the `sock_writer` itself. `map` allows the closure to re-claim the writer after `io::write_all` is done with it, so it can be used for the next thing to send.
I couldn't find a crate for them and was chatting with the owner of redis-cell over email about his efforts. It'd be a nice thing to have for others to start making more modules. If you're interested, perhaps we can start with my bindings and write a wrapped around them since they're auto generated.
Works lightning fast on mobile. 
When I was implementing a tree structure, I used `Box&lt;Option&lt;Tree&gt;&gt;` for the connections, but didn't have to use unsafe code. I'm assuming using an Arena is more memory performant?
This is great. I wish I had this guide a while ago because I went through a similar process and discovery was not easy as someone who hasn't regularly used Windows for over 10 years. I had been using a VM via VirtualBox, but I did wind up buying a cheap Windows 10 laptop too. Some additional advice: 1. If you can, get Windows 10 Pro. I don't know if this would solve my problem, but at least on Windows 10 Home, I literally cannot figure out how to cause my account to have admin privileges. I've enabled the "hidden" admin account and done various other things that Google recommends, but no matter what I do, anything that requires admin privileges to toggle is disabled. Additionally, Windows 10 Home I guess doesn't have the ability to edit local policy settings. 2. I found that enabling developer mode was the only way to get symlinks working. I have libraries whose tests require the creation of symlinks, so this was important. 3. I found that Windows Defender has an unbelievably bad effect on the performance of tools that want to rapidly search a directory tree of files. I don't know how it works, but my likely inaccurate mental model of it is that it sits between any program you write and any file on disk, and scans something as the program tries to read said file. From staring at the task manager, I can see that disk bandwidth is alarmingly low and that and `antimalware.exe` process is working in overdrive. I found out I can disable Windows Defender completely, but it [seemingly required editing the registry (see Method 2)](https://www.ghacks.net/2015/10/25/how-to-disable-windows-defender-in-windows-10-permanently/). Once I did that, disk bandwidth went from single digit MB/s to hundreds of MB/s, consistent with the fact that I have an SSD. This even had a large impact on subsequent searches when the files were presumably in cache. 4. I haven't installed Chocolatety, by instead just use cygwin. Like the OP, I also program on my main Linux workstation and just rsync the files to my Windows laptop over SSH. I *really* wish I knew about (3), because it bewildered me for a very long time.
Looks great! This would be three times better if it didn't flash white on every keystroke, in Firefox on OSX.
What an interesting article, really goes in depth. Thanks.
&gt; I found that Windows Defender has an unbelievably bad effect on the performance of tools that want to rapidly search a directory tree of files. I don't know how it works, but my likely inaccurate mental model of it is that it sits between any program you write and any file on disk, and scans something as the program tries to read said file. From staring at the task manager, I can see that disk bandwidth is alarmingly low and that and antimalware.exe process is working in overdrive. I found out I can disable Windows Defender completely, but it seemingly required editing the registry (see Method 2). Once I did that, disk bandwidth went from single digit MB/s to hundreds of MB/s, consistent with the fact that I have an SSD. This even had a large impact on subsequent searches when the files were presumably in cache. I really, truly, don't understand how Windows Defender can be allowed to have this kind of impact on performance, which hopefully means I was just doing something wrong. Under "Virus &amp; threat protection settings" you can "add exclusions" and mark certain directories to be ignored, which mitigates the performance problems while still leaving Defender... defending. I added my overall workspace folder (which I put all my projects in), and the .rustup and .cargo directories to the exclusion list, and Defender stopped destroying performance.
&gt; The idea is to keep T nodes in a Vec&lt;T&gt;, and be careful to never to push beyond the initial capacity. This is terrible. There should be an easy way to allocate memory on a heap without using `Vec` and hoping that it will not reallocate.
Thanks! I'll check this out next time I boot up my laptop and see if I can unwind my registry hack.
Well, there’s a harder way, which is to use the unstable alloc crate (requiring nightly), but I think we would just be duplicating behavior already exposed by `Vec`, which is pretty much the motivation behind *not* exposing the raw allocation api. Yes, you have to be very familiar with how `Vec` works, but I think there is good documentation and the actual implementation itself is commented thoroughly.
This is one of the glaring holes that Rust's standard library seems to have. There's no good way to allocate a fixed-size array of something. There should really be some simple way to create a `Box&lt;T[]&gt;`.
Yes, this is another reason why I prefer the raw-pointer version better. The low-level API for allocating memory without `Vec` is not stable yet: https://github.com/rust-lang/rust/issues/32838
Hm, I would say that `Vec` is a convenient way to allocate `Box&lt;[T]&gt;`? fn make_array&lt;T: Clone&gt;(t: T, n: usize) -&gt; Box&lt;[T]&gt; { ivec![t; n].into_boxed_slice() } 
I suppose your tree did not have any cycles? (Such as a reference from a node to its parent.) Cycles are the whole motivation for this post. (An arena can be more performant on principle, but I haven’t measured anything.)
Is `vec![Default::default(); n].into_boxed()` something you’ve considered?
Did you get an SSH *server* working on Windows with rsync? How do you set it up? (I haven’t tried Cygwin yet, Chocolatey just happened to be one of the first things I found when looking up "OpenSSH Windows".)
Not really. It seems really backwards to have to create a Vec for that. Plus, I don't think it's possible to create the Vec without pre-populating it - I would use this for byte arrays, and I definitely won't want to have to touch every byte just to do an allocation. At the very least, there should be a safe library function that gave you a `Box&lt;[u8]&gt;`.
if a tree has cycles then it's not a tree anymore
Isn't it possible to do this with `Box&lt;[T; n]&gt;` ?
Is the DOM not a tree? I think there’s a difference between what are the meaningful relationships between nodes (such as child, parent, sibling), and what references/pointers exists in the implementation to make allow quick traversals.
It allocates array on a stack and then copies its contents to the heap. in some cases it allocates on the stack twice and does two copy operations for a single array.
Are you running `x86_64-unknown-linux-gnu` toolchains inside WSL? I need to use `x86_64-pc-windows-msvc`, the whole reason to use Windows at all for me is to program against Windows-specific APIs.
That only works with expressions known at compile time. If you wanted to set the size based off an environment variable or a config file, you can't do that.
That's right. $ rustup toolchain list nightly-x86_64-unknown-linux-gnu (default) Was sharing my story of how I got rid of VirtualBox.
&gt; I wish I had this guide a while ago Yes, this is pretty much “write what I wish I had to read before I spent days figuring this all out” :)
Why would it be unsafe? Every combination of bits in a u8 is valid, and as far as I know, Rust makes no guarantees about the contents of a [u8] (unlike, for example, str, which is guaranteed to be UTF-8). Accesses to it would also be bound checked just like anything else.
Isn’t reading uninitialized memory Undefined Behavior?
not from the point of where it matters. if it were you wouldn't have do deal with cyclic references
Sorry, so, I got a client working. Specifically, when I said this: &gt; I also program on my main Linux workstation and just rsync the files to my Windows laptop over SSH. I probably should have said: &gt; I also program on my main Linux workstation, and when I want to test my changes on my Windows laptop, I rotate my chair to my laptop and type an rsync command to copy the files from my Linux machine to my Windows machine and re-compile. With that said, I have, in the past, gotten an SSH server running on Windows. But I don't remember the details and it was some GUI application. I feel like it worked pretty decently, but I end up needing to test on Windows proper to make sure everything words in cmd.exe specifically, so I need to rotate my chair anyway. At that point, running the requisite rsync command isn't a huge deal.
It could be done lazily.
I can see why it would be in the general case, but for u8? Now I'm curious. I often just write a function that contains an unsafe block to do this, because I couldn't think of any downsides. It seems like every single bit pattern in a u8 is completely valid.
No it didn't have any cycles. It was a binary tree, so I guess it's a tiny bit different of a use case.
I think in all cases it should be unsafe to read uninitialized memory, but in the proper use case, you will write a value first, which is totally safe. Rust even provides compile-time safety around uninitialized values through this error: https://doc.rust-lang.org/error-index.html#E0381. I suppose the real issue is that for arrays or slices, safe Rust requires that the whole array or slice has been constructed, it doesn't distinguish between partially uninitialized and completely uninitialized.
Rust’s drop-checking doesn’t care much about graph theory, though.
Xidorn pointed out on twitter that exponential growth of the chunk size is unnecessary (since, unlike `Vec`, when allocating a new chunk an arena doesn’t move all the existing items), and that a page-aligned constant chunk size might be better. Thoughts?
Sure, if you write (initialize) it first then reading previously-uninitialized memory is *sound*. But an API still should be an `unsafe fn` if the caller is responsible for only using it in certain ways for a program to be sound.
Undefined Behavior is not (only) about valid bit patterns, it’s what LLVM’s optimizer is allowed to assume never happens. (For example eliminating code based on proving that reaching it is "impossible".) By default [`Read::read_to_end`](https://doc.rust-lang.org/std/io/trait.Read.html#method.read_to_end) will initialize buffers with zeros before passing them to arbitrary `Read` implementations, just in case they read from the buffer before writing to it (even though it’s not useful for them to do so).
I always found weird that solutions seem to be linking with some precompiled libraries in the machine instead of using cargo and retrieve the latest. Maybe those solutions are using super-old versions of the crates and missing newer performance improvements? :(
Does the LLVM optimizer even have that kind of visibility? I guess my point is, if I wrote a function: fn make_buf(sz: usize) -&gt; Box&lt;[u8]&gt; { let mut v = Vec::with_capacity(sz); unsafe { v.set_len(sz); } v.into_boxed_slice() } What optimization could LLVM make that would break things? It sounds like you're saying that LLVM could (try to) determine you read from the memory before writing to it, and thus elide the reads. But surely this isn't true - aside from the fact that I've written code like this, it could be a huge issue for Clang, both C++ and C, where reading data after a malloc() but before setting it is certainly not undefined behavior.
Because UB is about the assumptions the compiler makes, not about the effects of your program. The compiler assumes there are no uninitialized reads, and doing such reads can lead to surprising behavior.
One can use `Vec` as an allocator: `Vec::with_capacity(n).into_raw_parts()`.
&gt; where reading data after a malloc() but before setting it is certainly not undefined behavior. pretty sure it is UB.
As an alternative, if you have Win 10, you have WSL, which should let you run sshd just fine.
Source?
Except it's not. Not for u8. (Well, for `unsigned char`, since you linked to a C++ question, not an LLVM one.) It's not clear to me if that answer you linked even applies to a pointer returned by malloc, either.
&gt; I still don't agree that any conforming compiler could possibly do the wrong thing here in the presence of functions like realloc. I don't get what you're saying here. &gt; This is a side note, but does LLVM actually have 3 separate types for 8-bit integers (like C/C++)? I thought it only had 2, and Clang mapped them down. No idea. 
I know that page-aligning memory can cause false-aliasing problems: https://stackoverflow.com/a/8547993/3517265
My point is that if a function returns an `char *` to a valid region of memory (that is, it's mapped into your address space by the OS), and you dereference it, I just don't see how anything "bad" could happen. Perhaps more specifically, I can't see how the compiler could safely treat that dereference any differently than any other dereference.
If it's an unsigned char, it's supposed to treat it safely. If it's _anything else_, it's undefined behavior. A conforming implementation is free to break that. Like I said, reasoning about UB isn't about "how could an optimizer break this", because it's seldom that clear cut. But you can imagine some malloc optimizations happening, and the assumption that the memory is valid clashes with other assumptions leading to something really weird occurring.
The second example works here in nightly 
Just a nitpick: the `Sort { expr: Box&lt;Expr&gt;, asc: bool }` doesn't seem to belong to Expr - it need access to all the events/rows in the result-set in order to sort them and AFAIU the Expr-essions are evaluated on each row separately.
Yeah, that's a good point. The generated closure only evaluates the enclosed expr, and the sort order is handled somewhere else. I'll revisit this for sure.
Are you sure that it is a valid region of memory at the point you are trying to read from it? IF reading uninit memory is UB, then I'd guess LLVM would be allowed, for example, to move such a read *before* the call that actually allocates it, and omit the call. The point is that the moment you are doing UB, LLVM is allowed to do surprising things.
One thing I've found is that using the installers on Windows is way easier than Rustup. Namely because for some reason Rustup insists on installing Rust to a specific directory on the C drive (which is honestly archaic behaviour) whereas the installers let you install it anywhere, as they should. Rustup is just generally less convenient IMO. Not sure why it's the "recommended" option.
But at that point LLVM wouldn't even necessarily know that it's uninitialized. It certainly knows for stack variables, but if a library functions returns `char *`? `malloc` isn't a primitive in LLVM. Rust doesn't even use it, since Rust has its own allocator, as part of the standard library.
also Not working in Firefox on ios
Not OP. But cygwin sshd + rsync is definitely possible. Here's a howto that appears to be accurate for setting up the sshd: http://www.noah.org/ssh/cygwin-sshd.html
&gt; there are many high quality libraries with many stars (maybe not fair to compare stars). There is a large diversity in the types of libraries available. The library situation is improving all the time, but it is still far behind Go, which has had a much bigger head start than Rust. I expect this will improve over time more and more though. The advantage with Rust at the moment though is you can reach much lower down the stack than go, and still gives you more abstractive power than Co to create high level APIs. Once the futures/hyper story gets more settled this year along with the accompanying language features I expect things to get much more exciting with regards to high-level web dev. &gt; the projects that I have compiled that use Rust take a really long time to compile (rpgrep and alacritty). Rust's compile times are improving, especially when in development and doing incremental compilation. That said, it will always be slower than Go because Rust puts a great priority on compile time optimizations than Go.
You don't need to vendor dependencies in general. Cargo would download and compile your dependencies as far as you specify them in Cargo.toml. If you do want to vendor things for some reason (e.g. Firefox requires no network access when building, so we have to vendor all dependencies), you can use [cargo-vendor](https://github.com/alexcrichton/cargo-vendor).
&gt; There is a large diversity in the types of libraries available. Coming from Go it may be your natural instinct to look to Github for libraries, but the Rust ecosystem tends to host its libraries on https://crates.io/ , which has good tools for library discovery and comparison (e.g. though there's nothing directly analogous to Github stars, the most-downloaded libraries have millions of downloads, and thousands of dependents). This also answers your later questions about tools/vendoring: the Rust community uses Cargo to manage builds and package versions; vendoring is mostly unnecessary because, unlike Github, crates.io is immutable (see e.g. https://np.reddit.com/r/golang/comments/7vv9zz/popular_lib_gobindata_removed_from_github_or_why/ , as well as NPM's infamous left-pad incident, for why this is a good thing).
The pattern is generated using the SHA1 hash of a given string, and will always be the same for a given string. No issues on the rust build end, I really like that you can add `+nightly` and `target=__-__-__` to pretty much anything. I had to pass ~6 functions for the module to instantiate, functions like pow, sin and cos (see: https://github.com/suyash/geopattern/blob/gh-pages/index.html#L84-L110), and had to figure out what needed to be defined through trial and error. It'd be great if on compilation with the wasm target, post compilation the compiler emits the set of env properties that will be needed to be defined for a successful module instantiation in JavaScript. I wasn't aware of the rust-wasm repo, will look into the open issues and contributing.
They’re new, I believe.
That looks like a weird build setup. If they require no network access (or some other constraints), maybe using cargo-vendor might help?
Disclaimer: I'm just a random Rust user, I'm not really qualified to advocate for the language publicly. If you're interested in the experience of a random user though, read on... 1. Estimating the quality of the ecosystem Judging the state of a programming language's ecosystem is nontrivial. Doing a project in whatever field you work in and looking for libraries to help with that would probably give a practical idea of the viability of Rust for you. I used `glutin`, `cgmath`, and `gfx-rs` to make a prototype/visualization of a solution to a problem at work, both libraries were great. Raw bindings to Vulkan and OpenGL are also available. I've also used libraries like `clap` for command line argument parsing, `byteorder` for parsing a byte stream, `ring` for HMAC's, `tokio` for making an asynchronous network protocol client library, `lalrpop` for a LALR parser generator, `csv` for csv file parsing, `tobj` for an OBJ loader, `imagefmt` for loading textures, `rust-protobuf` for a protocol buffers library, and tons more. Of all of those, I have only had issues with `rust-protobuf`. I'm not sure if it has changed since I used it 2 years ago, but at the time, I didn't like how it wasn't using `Option&lt;T&gt;` for optional fields. Checkout [`awesome-rust`](https://github.com/rust-unofficial/awesome-rust) for a compiled list. I can actually already see that `prost` solves all of the design issues I had with `rust-protobuf`. 2. Long compile times This is still an issue that the Rust team is working on, it hasn't been abandoned or anything, it's just a big project. Incremental compilation was actually recently added to the stable compiler, but it's still a work in progress. A lot of compile time is just spent in the code generation phase, so if you're just looking for type checking, you can use `cargo check`, which will improve your workflow. I still find Rust to be relatively good in this area though, C projects are usually way faster, but C++ is on-par or only slightly faster in general (I think, anyways). I'm basically just guessing on that comparison though. 3. Vendoring Your Rust project can specify dependencies to `crates.io` or just to a git url or to a file path, with an explicit version or commit hash. You can also specify feature flags that you want, like no-standard-library/etc. I hope this helps somehow, I wrote a lot here. I know there is a lot of internet debate and flaming between Rust and Go, but there's plenty of people that use both and are perfectly fine. I have my own programming language project, and I have developed a lot of my preferences based on my experience with Rust, because I think that Go developers have made a lot of mistakes, and that Go could have been a lot better. I wish Go had been Rust without the safety but with the coroutines, that's essentially what I want my language to be.
I wrote a proof-of-concept JIT compiler that translates Rust to assembly, then executes it [here](https://github.com/fschutt/gsr-jit). As it turns out, its rather cumbersome to make a JIT. Maybe something like the [LLVM Kaleidoscope](https://github.com/jauhien/iron-kaleidoscope) compiler would be better suited. My thought was that if you have a JIT that conforms to the Rust syntax (or a subset of it), you can use the real Rust compiler for release builds, but the JIT compiler for hot-reloading / debug builds.
I wrote a proof-of-concept JIT compiler that translates Rust to assembly, then executes it [here](https://github.com/fschutt/gsr-jit). As it turns out, its rather cumbersome to make a JIT. Maybe something like the [LLVM Kaleidoscope](https://github.com/jauhien/iron-kaleidoscope) compiler would be better suited. My thought was that if you have a JIT that conforms to the Rust syntax (or a subset of it), you can use the real Rust compiler for release builds, but the JIT compiler for hot-reloading / debug builds.
You should prefer failure over error_chain for your errors. 
Also: fn myfunc() -&gt; Option&lt;U&gt; { let s1 : Option&lt;T&gt; = { &lt;first code creating an Option&lt;T&gt; }; s1.map(|s2| =&gt; { &lt;second code that uses s2: T to make value of type U&gt; }) } Or if you don't like putting your code in a map closure: let s1 : Option&lt;T&gt; = { first code }; let s2 : T = match s1 { None =&gt; return None, Some(x) =&gt; x, }; { second code that takes s2:T and creates an Option&lt;U&gt; } Which is equivalent to: fn myfunc() -&gt; Option&lt;U&gt; { &lt;first code&gt;.map(|s2| =&gt; { &lt;second code that uses s2: T to make value of type U&gt; }) } 
It looks like floating point registers aren't saved when switching coroutines. Is that a problem or are they automatically saved somehow?
&gt; Larger standard library Is this not by design? To keep `std` rather small and have crates for the other features? Like how `rand` and `num` would be in the stdlib of other languages
Go doesn't have async I/O. Go's concurrency doesn't scale nearly as high.
It's also pretty common to box a closure if you want to store it in a data structure This way the actual type of the closure does not have to be a part of the data definition. struct MyStruct { .... mapper: Box&lt;Fn(i32)-&gt;i32 + 'static&gt;; .... }; There was also a thread on here a couple of weeks ago (https://www.reddit.com/r/rust/comments/7v4kfj/impl_trait_vs_boxtrait_or_how_i_learned_to_stop/ )where someone dramatically sped up their slow compile time by boxing all the closures they were passing around in their program, to avoid the combinatorial explosion in type specialized code generation.
According to the [System V ABI specification](https://software.intel.com/sites/default/files/article/402129/mpx-linux64-abi.pdf), floating point registers aren't preserved across function calls so this shouldn't be a issue.
&gt; streets ahead heh
&gt; ring for HMAC's I am a bit biased, but for just HMAC's, if you don't wan anything else, I would recommend to use combination of [`hmac`](https://crates.io/crates/hmac) crate and appropriate [hash crate](https://github.com/RustCrypto/hashes). While overall performance will be a bit lower for SHA-1 and SHA-2, you'll depend on significantly smaller, fully pure-Rust crates.
If Rust is not the right tool but Go is, pick Go. What are the specific libraries you need?
Well, the downside - and which influenced this decisions for Rust - is that for some languages the standard library is a place for code to die.
I will work on finishing up my [first contribution to Rust](https://github.com/rust-lang/rust/issues/48103) which has been going slower than I wanted to. Mainly because I got a faster laptop, where the tests I wrote behave differently for some reason. Once that’s fixed I want to spend some time reviewing my Rust learning of the past weeks and also figure out what I want to learn about next.
I don’t see people reinventing rand or chrono just because they are not in the std library.
The answer to 3 is simple and hurts: All the alternatives to Windows Defender that people have to deal with are even worse. Back to fighting with the travesty of an Antivirus "solution" that needs to be on my work machine and stops all work.
Yes, rustup installs to your user directory (which is the recommended way for single-user programs to install itself). It also doesn't need admin privileges, handles updates gracefully and as you've already noted manages multiple versions and so on.
We were accepted as mentoring organization this year again and have two projects in Rust - decompiler (Radeco) and symbolic execution framework (Rune). For students who wants to participate - please be sure you qualify for GSoC'18 according to the Google rules and see [information on how to apply](http://radare.org/gsoc/2018/index.html). Please also note, we require students to prove they are able to participate in such project, and solve a small issue before the application. See the list of such issues known as ["microtasks"](http://radare.org/gsoc/2018/tasks.html#title_9) 
For keeping the balance - there is also another project in Rust related to the reverse engineering - [Falcon](https://github.com/falconre/falcon) binary analysis framework. Though they do not participate as mentoring organization for GSoC'18, still worth to check out.
Shouldn't this compile https://play.rust-lang.org/?gist=e8737733a1bacbf408e91beb97ed06b2&amp;version=stable ? I found it while looking into a `clippy` lint that was telling me to remove a return from a function.
Not going off that page, but rather where Rust has been talked a lot about in the past year or so, I can totally see where he is coming from. There seemed to be a big push for Rust in the web space last year and nearly every article posted about rust was about web servers or web assembly or the such.
That seems a little like a circular argument - I'm saying that it seems hackish that it is necessary for me to create multiple variables with no other purpose than to exist temporarily before getting moved into a lower-level closure. Part of the problem seems to be that the closures are assumed to have a 'static scope ("closure may outlive the current function") - so even if I place the shared variables outside of a lexical scope {} that everything else is inside of, I still can't just borrow them. There's a lot extra that's going on, but at the end of the day this is still just single-threaded code. The Rc and RefCell aren't adding much to the safety of this, but they are increasing the complexity and overhead significantly over being able to just access the integer-sized variable directly. Refactoring into functions does seem like it might help it, but at the cost of making the code less linear - unless I'm missing something.
It's parsing the block as a statement, not as an expression. You can force it into expression context by wrapping it in parens (*i.e.* `({ 42 })`).
AFAIK Firefox on iOS is still WebKit, like all other iOS browsers
They’re way too slow. In order to have proper scripting / plugins, you’d need a JIT, but AFAIK, I don’t know anything about that in Rust (yet?). I guess we’d need some kind of a MIR JIT or something similar.
&gt; Part of the problem seems to be that the closures are assumed to have a 'static scope ("closure may outlive the current function") - so even if I place the shared variables outside of a lexical scope {} that everything else is inside of, I still can't just borrow them. Yes, and that's precisely what's going on: you are returning a Future, you are _not_ kicking the computation off. This code does not run in a very ordered fashion. To give you an example: `for_each` kicks off a computation for every item it gets, in a deferred fashion (by returning a future). That means, you _need_ a refcounted handle for each of them. The closure does _indeed_ outlive the scope it is defined in and it practically will outlive ForEach (if `for_each` is finite or hits an error, the future will end, but the already spawned ones not). &gt; There's a lot extra that's going on, but at the end of the day this is still just single-threaded code. The Rc and RefCell aren't adding much to the safety of this, but they are increasing the complexity and overhead significantly over being able to just access the integer-sized variable directly. This is single-threaded, event-handled code. The computation will be out of order of definition. Rc _adds_ safety: precisely that you don't know how long you need to keep the variable alive to make sure every future has access to it. RefCell adds the security to ensure none of the Futures (which may yield at any time for reasons they decide) holds a mutable pointer while another one gets executed. &gt; Refactoring into functions does seem like it might help it, but at the cost of making the code less linear - unless I'm missing something. Your code is already not linear, refactoring will not make it less so. Your timer spawns futures, so every step, you get 2 concurrent additional concurrent components on you reactor. Once you return or construct a future, you left linear-land :).
I would use *ring*, because it's crypto is proven (it's based on BoringSSL) -- plus, the author/maintainer, Brian Smith, has a proven track record in the crypto software world. (When you talk about any of your crates, it would be nice if you mentioned more explicitly that you are the author/maintainer; this is a bit more concrete than just saying that you are biased.)
As others said: Creating a `Box&lt;[T; n]&gt;` only works for values of n that are known at compile-time because this n is part of the type. There is a way to create a `Box&lt;[T]&gt;` from a `Vec&lt;T&gt;` but this involves a reallocation if size!=capacity and also won't allow you to add new elements.
Are these coroutines cheap to use as go coroutines?
I'd definitely need to clear my schedule for a year or two to do something like that! There is miri, which directly interprets MIR, but there the focus is on compile-time metaprogramming - things like `const` expressions. It would need to hook into something like `libffi` to actually interact with the environment. Even so, a REPL requires serious rearrangement of any compiler or interpreter!
When thinking about a language to use in a plugin system my first thought is dyon or gluon. Wouldn't they work better for most stuff? It's not rust, but as you said JIT is hard.
No, it doesn't, but I plan to support that eventually. Since the underlying C interface of emacs-module doesn't currently support it, we'll probably have to use `special-event-map` to handle `SIGUSR1`/`SIGUSR2`, or file notifications.
Hi! I don't know if my question is easy or hard but I don't know how to solve it. I get this compiler error: Compiling playground v0.0.1 (file:///playground) error[E0619]: the type of this value must be known in this context --&gt; src/main.rs:15:16 | 15 | if numbers.len() != 3 { | ^^^^^^^^^^^^^ On this snippet: https://play.rust-lang.org/?gist=cad84386f8e52d26684f075c84249158&amp;version=stable I don't know how I should precise the type of the value in the match though, and I'm not sure to understand why it is needed... Thanks!
Even if a tree doesn’t have graph-edge cycles in the graph theory sense, it’s implementation may have `&amp;T` reference cycles. For example: pub struct Node&lt;'arena&gt; { parent: Link&lt;'arena&gt;, next_sibling: Link&lt;'arena&gt;, previous_sibling: Link&lt;'arena&gt;, first_child: Link&lt;'arena&gt;, last_child: Link&lt;'arena&gt;, data: NodeData, } type Link&lt;'arena&gt; = Cell&lt;Option&lt;&amp;'arena Node&lt;'arena&gt;&gt;&gt;; This is a tree in the graph theory sense. `Link`s are not all equally considered to be edges of an arbitrary graph. But they can form cycles: for example if `node` has any children, `std::ptr::eq(node, node.first_child().get().unwrap().parent.get().unwrap())` is true. This is why means all references involved need to have the same lifetimes, as explained in the article.
I heard bad and good things about those. I don’t know. It’s way too much opinionated to opt-in for those languages for now – personal opinion, though.
Emacs is single threaded. Emacs 26 brings 'cooperative thread' which means there can be multiple threads but only one lisp thread run at a time. By using both emacs' cooperative thread and real OS thread, it's possible to do some work asynchronously, this is what I've done with my project. See the interesting parts [here](https://github.com/sebastiencs/sidebar.el/blob/master/sidebar.el#L1541) and [here](https://github.com/sebastiencs/sidebar.el/blob/master/module-git/src/lib.rs#L147) This can probably be improved, but it does the job
Why though? You're trying to read data from some address somewhere in memory that was not written to before. No matter if it's on stack or on heap. Depending on architecture/os in this memory could be anything.
In line 10, use `.collect::&lt;Result&lt;Vec&lt;_&gt;, _&gt;&gt;();`. Otherwise it doesn't know what to collect the numbers into.
Yeah, Rust's grammar makes each `{..}` block its own statement unless it's parenthesised, e.g. `({ 42 }) as i32`.
I'm dumb, explain pls
I'm currently sick and sitting in the doctor's office, however I'll still try to get some things done, at least [TWiR](https://this-week-in-rust.org) and some [mutagen](https://github.com/llogiq/mutagen), perhaps also looking into [bytecount](https://github.com/llogiq/bytecount) performance with single/multiple codegen units.
IIRC, the site maintainer wants the entries as plain files + a makefile to build them. Network is not the problem. It's probably just a matter of updating our makefile. cc /u/teXitoi
Compie-time was a major design goal for Go although it doesn't quite live up to the early aims to have nearly instant compile. Go specifically forgoes language power for simplicity and fast compilation. Anything could happen but realitically I don't think Rust will never match it for compile time .
get well soon! :)
Currently working on re-doing the multi-threading model on my chess engine, [Pleco](https://github.com/sfleischman105/Pleco). This is probably the third total-rehaul in the past two months, and hopefully the last. While I'm doing more terrible terrible things with `unsafe`, the coding style is *alot* cleaner, and therefore simpler / easier to reason about. However, so far this rewrite has come with a performance hit for searching. It's a little strange, and either has to do with the raw pointers I'm using, or I'm messing up the thread wakeups and scheduling. Hmmmm...
AFAIK, you could write like .for_each({ let shared_var = shared_var.clone(); move |_| { .... } }) You could also define a macro to avoid boilerplate, similar to [this ](https://github.com/ubolonton/emacs-module-rs#documentation).
I couldn't help thinking about tail-allocation in ATS (which I only know about from https://www.youtube.com/watch?v=zt0OQb1DBko&amp;t=28m47s ) and wondering whether (a generalisation of?) that might provide a elegant approach
Apart from the linked gotchas, I'd guess this requires more memory than a similar number of goroutines: rust can't start with a tiny stack size and grow it as needed, like go does.
It's an in-joke from [_Community_][community]. [community]: https://en.wikipedia.org/wiki/Community_(TV_series)
Have you run into any issues doing this? I had a simpler system for an [IRC bot](https://github.com/aatxe/awebot/blob/22fce2d660e2fce1ea93ff7c4401f1bb391f8bb1/src/main.rs) (dylib loading, manual compilation) that would crash on reload on Linux (but IIRC not on macOS), and I'm fairly certain it was related to moving heap-allocated stuff from the main binary to the plugin dylib. It seemed to me like these kind of plugins required a lot of care to get right, but maybe I was missing something.
Thanks, will do.
That and the fact that interfaces in the standard library have to be extremely stable as you can't depend on semver version dependencies to manage those interfaces. This makes that any change has to be fully backwards compatible, which effectively stifles innovation.
Just asking to the knowledge of the community here, what do you think about the overhead of `cargo` to manage the dynamically linked library extern dependencies here instead of usig `rustc` ? I spoke to the OP and he tells me that it can be a little too much.
I don't think this is right: &gt; No integer overflow Afair overflow-checking is (*by default*) only activated in debug-builds. Also, Rust's Type-System **doesn't ** prevent overflows in the same way it prevents lifetime issues. Other than that, great article! I like all the "I don't want to do that anymore..." bits, as they show how Rust makes you aware of things you didn't know you were missing before.
I've tried a lot of tree/dom creating techniques, but `rc-tree` looks like an easiest one. Yes, it's not that fast (in theory), it has runtime checks and can even panic, but it has an easiest API, especially when you need to move/edit/remove nodes a lot. Reference cycling is a pain, of course.
&gt; I thought his enum example reads, well, different from what I expect. Do you mean the newtyped booleans for function flags?
Even if you had a proper tree (i.e. no cyclic references), you would probably still see performance gains (at least in a tight loop) by amortizing allocation (or better yet, allocating entirely ahead of time if you know how big the tree will be). I recently implemented a trie set originally with (essentially) `Option&lt;Box&lt;Tree&gt;&gt;` and it was much faster (about 3x as fast on my test workload) when I switched to a simple (and much more limited) arena that was preallocated (I had an upper-bound on the size) for the whole data structure.
Both rust-coroutines and goroutines are based on user-mode scheduling but there are many differences in the implementation details. As kostaw points out, it's difficult to implement growable coroutine stack correctly in Rust. Therefore, memory usage may be higher if there are a huge number of coroutines running. Also, some optimizations have not been done yet, compared to the highly-optimized Go runtime. However, most thread-based applications which spend most of their time in I/O will still benefit a lot from the switch to coroutines. In general, rust-coroutines is a little slower than goroutines but much faster than native threads for I/O-bound applications.
I can understand their ways of thinking. Debian is stable, but always late behind a couple version. And corporate used to like it. But now everyone is moving rapidly. Even Java is on every 6 months release schedule now. Microsoft moving quickly with rolling release. Also now everyone is using per language package manager. PHP, Ruby, Rust, Java, JavaScript, Python, Go. Gone are the days where you can casually make distro package of language packages. I think Debian needs to adapt and adopt better policy than using 1-2 years old libraries.
&gt; It is a game-changer especially for people who haven't written much concurrent code so far, because writing such code suddenly becomes incredibly easy and safe. I have written quite a bit concurrent code and it is a net win there as well. My running assumption is that most errors in that domain happen when moving single-threaded code to multi-threaded code. 
Someone is _streets behind_... (Sorry, as someone said, _Community_ reference. Great show, highly recommend!)
I case it helps I just installed GNOME Builder nightly (flatpak) on Arch Linux with working RLS. This is what I did: sudo pacman -Syu flatpak flatpak install --from https://git.gnome.org/browse/gnome-apps-nightly/plain/gnome-builder.flatpakref rustup component add rls-preview rust-analysis rust-src flatpak run org.gnome.Builder Note that my default `rustup` toolchain is stable (1.24).
If alloca landed in rust, will the things be changed?
Been porting [reproto](https://github.com/reproto) to wasm and put together an application showing off one of its features (derive schemas from unstructured JSON, inspired by quicktype): https://reproto.github.io I'll be writing together a blog post this week detailing how things were ported to support the web. The whole project is getting interesting. I have three kinds of intermediate representation for schemas. It's almost a compiler in its own right.
I didn't know a cooperative thread can be yielded in the middle of a call to a module function. Is that safe? Anyway, that implementation polls whenever a cooperative thread yields, so it may prevent Emacs from going idle.
I have no idea what the issues involved are, but is the company behind Reddit actually pushing political issues, or do they just provide space for political propaganda to happen? Because the way I understand this thread is that people are objecting to Facebook's (or some of its decision makers') political agendas, not that of the users.
Good point :)
I miss Cargo so much.
Yes, sorry -- I should have been more clear.
&gt; Afair overflow-checking is (by default) only activated in debug-builds. I believe what the author meant was, no UB to due integer overflow. The rules are: 1. Overflow is a "program error" 2. When `debug_assertions` is enabled, overflow must panic 3. If overflow does not panic, it must two's compliment overflow It's set up this way so that someday, we could theoretically enable the check in debug mode as well.
I'm helping flush out the start of the [crate-ci](https://github.com/crate-ci) org, so now that I've got [cargo-ghp-upload](https://crates.io/crates/cargo-ghp-upload) working extracted from cargo-travis, I'm working on a [new idea] (http://cs.furman.edu/~cdurham/experiments/Rust/ci_detective/) as a platform to help implement further CI tools on top of. The initial implementation is going to be simple and eager, then I intend to build a smarter, lazier option that does less allocation in the common case that you don't need most of the information.
I would say it is as safe as any other function, module or not. Yes it does prevent emacs from going idle, I wonder if I could use `sit-for`or such function to prevent that. The code is still experimental and I'm sure there are many places for improvement. Your solution with signals sounds good too
Cargo very good and makes building Rust projects incredibly easy. It works in a way similar to npm and D's dub, letting you specify in a file which external dependencies you are using and their version constraints. When a build is run, it will automatically resolve dependencies and pull in and compile everything that's needed. There is no need to use Git subrepositories or check any vendored code into the repository. I remember trying Go a few years ago (long before I started using Rust), and its barely usable package management pretty much turned me off it completely. * The whole business with having to set GO_PATH was just clunky (no idea if that's still a thing). Rust (and npm and dub, and probably many others) locate the project root automatically based on a config file (Cargo.toml) so you don't have to deal with this. * There was from what I recall no real way to track dependencies or their versions built into Go. Getting a package just cloned it into a repository, which made it unnecessarily hard to even check the vendored code into your repository. You'd either have to use a Git subrepository or just delete its .git folder and check it in (which of course would bloat your own repository with third-party code). I did find a project called "gb" that seemed to aim to fix these issues, but at the time it was broken and would just segfault when I tried to run it. As already mentioned above, with Rust this is not an issue at all - it's all handled automatically and cleanly by cargo.
A little over a year old https://github.com/blog/2309-introducing-topics
&gt; in debug mode as well. It would make more sense to me if it read "in release mode as well". Is it a typo?
Gah, yes, thanks
&gt; You're right, Go doesn't have async I/O. It does. It's just integrated with the task scheduler. 
Oh yeah, very good point! "Defined overflow behaviour" is definitely a selling point.
This thread mentions a lot of things that we're working on fixing for Rust, so it should also be mentioned that people are working on fixing things on the Go side of things too. For Go, there have been various dependency management tools around for a while now. I used to use `godep`, which just vendored everything into your repo. There is also `glide` which is more similar to Cargo than `godep` is, for example. Currently, there is an official effort coalescing around [`dep`](https://github.com/golang/dep), which should wind up being the canonical tool. We're already using it at work, for example. The `GOPATH` issue... I'm not sure what the current state or thinking is on that. I like it some respects. In other respects, I don't. But either way, not even remotely close to a deal breaker for me personally.
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](https://github.com/golang/dep) - Previous text "dep" ^^Please ^^PM ^^/u/eganwall ^^with ^^issues ^^or ^^feedback! 
Should `clippy` keep this in mind when offering suggestions?
&gt;IT JUST F**KING WORKS a quote of the week? ;) (** added by me)
&gt; maybe only 1/3 have a stable release Are we looking at the same list? https://crates.io/crates?sort=recent-downloads --- Almost all of those are stable as far as I know. `log` recently went through some cleanup, and so is/has `rand`, but those are the only two.
15 of the first 50. /shrug You do the math.
Can I recommend the cookbook (search for rust cookbook) - i think it's good for getting a feel how to do everyday tasks
I wrongly assumed that Emacs's threads are its own construct with a custom scheduler, so I couldn't understand how it could work across FFI. Turns out they are just `pthreads`.
Oh I see, I didn't realize that you consider a version number &gt;= 1 as synonymous with stability. I certainly don't. I do understand the benefit of the signal provided by a library marked with a version number &gt;=1, but I don't really associate that with stability.
I appreciate a lot of people feel that way, but the semver is a *spec* and it is unambiguous. https://semver.org/#spec-item-4 &gt; Major version zero (0.y.z) is for initial development. Anything may change at any time. The public API should not be considered stable. &gt; Version 1.0.0 defines the public API. The way in which the version number is incremented after this release is dependent on this public API and how it changes. 
It's not a "feeling." crates.io and Cargo specifically do not follow that spec by the letter of the law. In particular, `0.x.y -&gt; 0.x.(y+1)` is assumed to not be a semver breaking change while `0.x -&gt; 0.(x+1)` is. But whatever. This is clearly a miscommunication. But the implication of your claim is that libc and regex, are, for example, unstable. But in practice, they are anything but unstable.
And also, I agree with you that we should keep pushing toward 1.0 for these libraries. But I don't think I'd describe myself as feeling disappointed with the current state. :)
I think a good solution would be to print just type name and maybe its address in memory, but not its fields. This is what Java does when you don't override toString(). Printing just struct name should be simple enough to not cause much code bloat.
Perhaps you’ve been infected with a mutagen. I’ll see myself out
&gt; I like it some respects. Any examples of advantages it has over automatic project location? &gt; But either way, not even remotely close to a deal breaker for me personally. Yeah, GOPATH wasn't really a dealbreaker for me either, just something that immediately struck me as clunky and unnecessary. It was ultimately the package management issues that made me move on.
*Initialisms*. I'd have written *acronyms* myself but I like the initiative! Okay, seriously, there isn't much I can say in defence of `Rng` except that RNG/PRNG/CSPRNG are quite widely used and understood, and of course that it's already in use. One can for example look at the venerable [GSL](https://www.gnu.org/software/gsl/doc/html/rng.html). We could call it an `Engine` or `Generator` instead, but it's more to type, less specific, and doesn't really add anything. We could call it `RandomBitGenerator` ("uniform" is more a property of the distribution than the generator, although strictly speaking it is uniform) — but most implementations produce a `u32` or `u64` or array of `u8` at a time, so `RandomNumberGenerator` is more appropriate — so we're back where we started, except with a long hard-to-type name instead of quick-and-easy `Rng`. Hmm, on reflection, I don't see any more attractive options.
It's backwards because it's safe. Having to go through the `Vec` interface like that forces the memory to be initialized. Just like C, reading from uninitialized memory invokes UB. Safe Rust can't hold references or smart pointers to uninitialized memory for that reason. You could write a function that's similar to Basic's DIM statement fn dimension&lt;T: Default&gt;(n: usize) -&gt; Box&lt;[T]&gt; But the steps to actually implement it would be - allocate exactly n slots - loop to initialize them And if you just write it using `with_capacity`, `for {push(default())}`, and `into_boxed_slice`, it should be the same after optimization.
Go might have *more* libraries, but every time I try to do anything in go I get bad at the package management. If you haven't seen cargo already, you'll love it
Ah, cool, I never knew about auto-unpacking function arguments used here: pub struct Fubarize(pub bool); pub struct Frobnify(pub bool); pub struct Bazificate(pub bool); fn my_func(Fubarize(fub): Fubarize, Frobnify(frob): Frobnify, Bazificate(baz): Bazificate) { if fub { ...; } if frob &amp;&amp; baz { ...; } } 
Technically llvm differentiates UB, undefined bits (undef), and "poisoned" values. Reading an uninitialized local is undef because it comes from an LLVM stack allocation or undef branch of a phi. But Rust doesn't use llvm heap allocation. It can and does call an external allocator. llvm can only assume that the returned pointers won't intersect with the stack or statics, otherwise it's assumed to contain data. **But those are all implementation details.** The closest thing Rust has to a specification on the issue (Rustonomicon) says that reading without initialization is UB, full stop. This also applies to memory which has been moved from or dropped (concepts that don't exist in llvm). Other implementations of Rust, such as miri, future versions of the main compiler, or a Rust to Android compiler don't need to use llvm and its precise shades of undefined stuff. llvm itself may change. It's best to abstract over that uncertainty and just initialize memory before letting safe Rust touch it.
`chrono` is a reinvention of `time` (which used to be part of `std`) and `rand` is currently being reinvented.
One little nitpick: The test should use `assert_eq!` instead: #[test] fn test_that_foo_works() { assert_eq!(foo(), expected_result); } This way the error message prints both sides, so it's easier to find out what caused this failure.
I wonder how does this compare to MessagePack. There is already Rust a library that handles encoding/decoding MessagePack with serde.
&gt; I think Debian needs to adapt and adopt better policy than using 1-2 years old libraries. This only works because they backport security fixes. If you want to be the first to get the new features (and bugs), you can use a rolling-release distro like Arch Linux instead of Debian.
&gt; If you call next() and there are more elements, you'll get back a Some&lt;YourElementType&gt;. This is slightly inaccurate, you get `Some(your_element)` and the type is `Option&lt;YourElementType&gt;`.
&gt; It's best to abstract over that uncertainty and just initialize memory before letting safe Rust touch it. I guess that's one approach. I'd say it's actually best to modify the Rustonomicon to allow reading from uninitialized u8 because there's a legitimate performance benefit from not having to initialize a potentially large region of memory.
Yes. However, it's also very easy to avoid integer overflow in Rust altogether by using the [checked_*](https://doc.rust-lang.org/std/primitive.i64.html#method.checked_add) methods. If you use those methods you get `None` if your operation would overflow otherwise.
Something to bear in mind. Go garnered interest on _the day_ it was released back in 2009. The big names behind it (particularly 'Rob Pike') made that possible. That's unusually early in the life of _any_ language. I suspect it stunted the language's growth. Rust, on the other hand, is probably only recently as popular as Go was at release, and has slowly been gaining traction for the past 2-3 years. I mention this because it's tempting to look at the release date disparity and say that Rust has 50% slower growth than Go. Those Github release dates aren't appropriate for that interpretation.
Just [url=https://github.com/TheBlueMatt/rust-lightning]code-dropped a good start on a Bitcoin Lightning library in Rust[/url] on Friday! Its only partially complete, but "released" to guage interest in contributors and users.
matching on a float rarely makes sense, and is usually more indicative of a logic error, but I agree that float ranges should be supported.
Chrono adds calendar functionality, time has no interest in that.
Will this be solved by the recent work from /u/desiringmachine ? (At least the `Rc` part, the `RefCell` is unrelated)
Yes, it's largely equivalent. However, for the sake of this discussion, imagine a collection which does not provide an external iterator of the usual form. I go into a little more depth here: https://jmarcher.io/internal-vs-external-iteration
Cool. I wonder if you can leverage syntax extensions to allow users to use function parameters instead of CallEnv.
I always wondered why that was supposed to be funny.
I might be interested in writing some alternative steps here for us MSYS2 users, which is the technology that Git for Windows is based on. One of the main benefits of using them says to is that you get `pacman` (as in, the Arch package manager) for free. :)
As of Windows 10 1709 there is actually built in OpenSSH client and server in Windows. You just need to enable it is Settings. Its still marked as beta so I would expect it to be a bit flaky, but it might be the best way to get SSH working on windows.
Correct.
"Make platform detection a **first class** feature of Rust". This. It seems to be the core of the questions discussed in [aturon's post a little while back](http://aturon.github.io/2018/02/06/portability-vision/) as well. Getting this right is something we need to think long and hard about in Rust 2018, I suspect.
&gt; I think Debian … Debian Alioth is much like sourceforge or savannah but it's a hosting service for Debian Developers and all other kinds of contributors together. 
The web is naturally a little bit of a big deal, so lots of people want to use Rust there. It's also a spot where there is, as yet, no Good Solid Solution That Lots Of People Love, so it's both a bit of a pain point and an area where there's lots of work being done.
There's an experimental (private) macro in the test module that allows using named parameters https://github.com/ubolonton/emacs-module-rs/blob/master/test-module/src/lib.rs#L234 I find it clunky to use though. It also doesn't handle parameter types. I plan to write a better one using proceduceral macros once they hit stable.
I'm curious, what would special support for float ranges buy you compared to just using match guards?
Cargo is used to "retrieve the latest" which is cached. Then the network cable is pulled out -- why would we do that? Then one-by-one those tiny programs are built &amp; run &amp; measured. &gt; How to know? How to know that the programs aren't really being compiled with `rustc 0.9` with the build logs corrupted to make them seem current?!?!
I had no idea you could pattern match on function parameters! Awesome!
What would be a cargo-vendor line-by-line replacement be for this example? cargo new pool-deps printf 'futures-cpupool = "*"\n' &gt;&gt; pool-deps/Cargo.toml cargo build --release --manifest-path pool-deps/Cargo.toml
One thing jumped out for me. No integer overflow. In my embedded work we use that all the time. We represent angles as a int16_t or uint16_t where you can add and subtract angles and never have to worry about a discontinuity at 2pi because 2pi=65536. Is this a hack that should not be allowed? I don't think so. Every processor made (that I'm aware of) in the last 30+ years has used 2's complement arithmetic. I don't think overflow is a bad thing, I think C made a mistake in calling it "undefined". They had to because when the language came along there were other options fresh in peoples minds. I've also worked on systems in asm that had saturating arithmetic, and that has it's own nice use cases. My preference is still to have the rollover. People can implement bounds checking when they need to.
lol right? Just now added that topic to about a dozen repositories. &gt;.&gt;
Has anyone written any kind of RFC related to matching on float ranges? It sounds like an interesting feature. I'd be interested to read some discussion on it if there is any.
Hi all, A blog post with a beginner focus. When I started Rust, I sometimes got stuck porting simple computer science algorithms. Here you see some simple implementations in Rust, because it's not that scary. I hope to expand to more advanced algorithms and data structures and appreciate any feedback. Thank you for reading! 
Quite to the contrary, go doesn't really have _synchronous_ I/O. The code just looks as if you were writing blocking I/O, but internally, I/O is non-blocking and the task scheduler will just halt the current goroutine and run a different one if an I/O operation returned with EAGAIN.
Consistency with how integer ranges work and shorter syntax
&gt; it'll download all the dependency sources into a vendor/ directory Can that be the same directory for all of the tiny programs? 
&gt; It's set up this way so that someday, we could theoretically enable the check in debugrelease mode as well. Won't this break (de-facto, _not de-jure_) backwards compatibility? 
I don't really count that as "browser centric". Especially when if you look at Go it's similarly very focused around web servers (and other things, but web servers are quite prominent). But what you're talking about is the _development_ of Rust, because building web servers in Rust was lacking some language/library improvements, which required a major push. So yeah, folks were talking about it a lot, but it's far from represented in the ecosystem.
Nope. Note carefully the wording: it says nothing about what a non-`debug_assertions` build is required to do. Basically, it's implementation defined. Because relying on this behavior is never correct in the first place, it's not breaking.
&gt; Are all the benchmark programs separate projects? New programs are added and old programs are changed or removed, and then just the new / changed programs built &amp; measured.
The author glosses over what's actually happening. A Rust program built in debug mode, will have checks automatically inserted that will panic on overflow when using the standard arithmetic operators. In release mode, in the default configuration, these checks are not inserted, and you will get a 2s-complement overflow. If you intend for the overflow to happen, there's `wrapping_*` functions on the integers, and also a `Wrapping&lt;T&gt;` type which calls those functions when using the standard arithmetic operators. Rust also has `saturating_*` and `checked_*` functions on integer types for when you need specific behaviour.
Integer overflow in rust is still something you can use, the language just requires you to be explicit about if you want wrapping arithmetic or if you want checked arithmetic. See the [wrapping](https://doc.rust-lang.org/std/primitive.u32.html#method.wrapping_add) methods on the default integer types, and the [std::num::Wrapping](https://doc.rust-lang.org/std/num/struct.Wrapping.html) type that provides operator overloads that use wrapping arithmetic. So in your usecase you'd probably specify that `type Angle = Wrapping(u16);`. Saturating arithmetic is also provided using the `saturating_*` function families. There's a couple of other ones too which allow you to check if overflow occurred (`checked_*` and `overflowing_*`).
I am writing integration testing in a tests directory, and I want to make sure that I can use the different features properly. I know that each .rs file in the tests directory is treated as its own crate, but how do I specify what features each different .rs file gets? Also, I want my crate to be no_std, but I use stuff like i32::MAX and std::cmp everywhere, so can I import std::i32 and still get to call my crate no_std?
I like the way it is staged as a kind of conversation (rather than "fight") with the compiler to get it to do what we want, trying all the "wrong ways" first.
Yes, you're right – could you file an issue about this?
Hey, good to hear! I'll be looking up the PRs and pitch in :). &gt; I fortunately was able to just polyfill the few things I use from these crates [..] What did you use?
The `i32` type is also available in core, as are `core::{cmp, ops}`. Just change `std` to `core` and if it compiles, you're fine.
Yeah, when I did that IRC bot it used dlls and would take a command to reload them with some metadata on the fly so it was drop-in reloading essentially. I think the on-the-fly compiling is really cool and makes it act like a "real" scripting language, but seems really infeasible with the current compile times especially. 
By *inbuilt sort* you mean `.sort()`? If you need tougher competition for your benchmarks, also check out `.sort_unstable()`. ;)
What is a match guard?
A quick Google shows it's the domain for Serbia. Make of that what you will. 
I almost did the same thing, but with `hyper::Uri`! Copy it into the project. It's unfortunate that that these libraries are not more broken up. &lt;/3 At least some are on their way.
I would also assume that there is no preemption. 
Thank you for your comment. I should make it apparent in the post that inbuilt is referring to `.sort()`. I'm curious to see how fast `.sort_unstable()` is considering how blindingly fast `.sort()` already is.
This is not necessarily specific to Rust, but it's something I've wondered for awhile: are static objects always in memory? Like, does every static thing you add to a system inflate its working set, even when it's not in use?
I'm not really actively working on it, but I've made a small crappy 3D equation graphing program with a vulkan(o) backend. You can see it [here](https://github.com/IntrepidPig/vgraph). I have no idea if it will run on any other computers than my own. Also there's a really annoying bug with the rendering backend I wrote ([vrender](https://github.com/IntrepidPig/vrender)) where the screen is filled with black artifacts, but it only happens when the program is fullscreen. I'm guessing there's some change in the way the graphics card handles fullscreen programs that I'm not accounting for, but I'm really not sure.
It is on their radar: https://github.com/golang/go/issues/17271
After watching Bryan Cantrill's talk about tail [I decided to rewrite tail in Rust as an exercise](https://github.com/tdbgamer/Tail). So far I have been able to match GNU Tail's performance.
When you talk about "the wording", you're talking about "de-jure" (in this case, referring to the written-down rule, as opposed to "de-facto", referring to whether anyone happens to rely on the current implementation). I think the de-facto back-compat risk is low though. Someone would have to never run debug builds with realistic data in order to depend on the current implementation.
Given that it measures compilation speed, it makes sense. Timing a network access or dependency compilation into the compilation speed wouldn't be great.
Maybe not that much though. If the OS lazily allocates pages in (only when touched), then you can allocate a 2MB stack and only use 4KB of it (the first page). This means that you consume a lot of the address space, but not much of the RAM. At 4KB per coroutine, you can spawn a lot of them!
It used to work (by accident) on nightly, but it was removed. :(
Okay that's something I wish I knew already. That's awesome.
I disagree, it often makes sense to compare with `inf` or `0.0`.
Anyone know if miri could be used for something like this?
&gt; Rand is being reinvented by the primary maintainers, so I'm not sure it's equivalent. This could not happen if it was in the standard library, due to backwards compatibility.
I recently needed to sort slices of ~1M bytes as fast as possible and I haven't managed to beat a good non-swapping counting sort: fn sorted(data: &amp;[u8]) -&gt; Vec&lt;u8&gt; { let mut counts = [0u32; 256]; for i in 0..data.len() { counts[data[i] as usize] += 1; } unsafe { let mut output = Vec::with_capacity(data.len()); output.set_len(data.len()); let mut ptr: *mut u8 = output.as_mut_ptr(); for i in 0..256 { let count = counts[i]; std::ptr::write_bytes(ptr, i as u8, count as usize); ptr = ptr.offset(count as isize); } output } } This could of course also be done in place, but this already sorts at almost 2Gb/s on my laptop.
According to WHOIS, docs.rs and rustup.rs are registered by [NiNet](https://www.ninet.rs/en/rs-domain).
I've been slowly building my own rogue like game. I'm trying to build it in a headless state. I've build a 'high level' rendering layer, which draws the world. It doesn't have any idea what it's drawing to though. It works out what to draw and where, but it hitting the screen is handled by a 'low level' rendering layer. That's what I've just finished working on. I've got two LLRs; an SDL2 layer which was a breeze, and a terminal based one which was a hassle. The terminal was just far too slow. So I've implemented a system to check for changes between the current and next frame, and only write out the changes. It's fast enough to be usable which is all I really wanted. I'm planning to use the terminal one for development, and SDL2 for release builds. I also really wanted to be able to do stuff like running and outputting a single frame, and have that repeatable. For debugging purposes. Terminal is needed for that. I have some of the world structuring already done like a month ago. Next up is to actually make it interactive. Currently you can look around but not move.
In addition to importing from `core` instead of `std`, you have to put `#![no_std]` at the root of your crate to actually be no-std. For the first question, do you mean rust features or cargo features? You can enable nightly rust features, e.g. `#![feature(inclusive_range_syntax)]`, in individual integration test files. As for cargo features, you have to run `cargo test --features "..."` separately for each combination of features you want to test. If certain tests only apply with certain combinations, you can use `#[cfg(feature = "...")]` to handle that.
Finally got around to update my stuff to use the new rtfm release. It's so much easier to work with. I'm amazed. Made a quick stepper driver based on your motor driver. https://github.com/etrombly/stepper-driver . Working on my sandbox project again in my spare time, would probably be a good base for 3d printer firmware once I make more progress on it.
No, they don't have to. The static objects are always accessible, but in a modern operating system (with support for paging) they don't have to stay in memory. The working set is by definition the data that the program has in RAM. Pages that are not used can be stored to disk to the swap file, or simply discarded when they were loaded from disk (like for program code, or the cached files). They will be retrieved again when needed.
Me, like a month ago: "I really want to add scripting to my rust app, but I hate lua and all dynamically-typed languages, really. I wonder how hard it would be to dynamically link other rust libraries in a plugin like fashion." OP: "Hold my beer."
A very long time ago we did have such things pervasively (along with comparison, hashing, etc.) We eventually removed them because no matter how small you make them, they're still ubiquitous extra code to generate that someone wants to be absent from their library.
Right, I guess that is your point, and I agree.
It is not that it is not allowed. You just have to request your desired behaviour explicitly. Rust goes to great lengths to not restrict your power as a developer. All safeties are opt-out. What the author meant was that it is *well defined*. If you don't specify what should happen on overflow, Rust assumes that you don't care about overflow and it was a mistake, and helps you catch it. If you want to have a certain behaviour on overflow, like what you described, you can do that in Rust using the `Wrapping`/`Saturating`/etc. wrapper types from the standard library. Those types are zero-cost abstractions. By using those types, you are explicitly specifying that you *want* a specific kind of behaviour on overflow and you will get it. They behave exactly like normal integers and you can seamlessly do any operations on them just like you can on integers, and they have well-defined, explicit, behaviour. IMO this is much better than what C offers. In C, your integers will either implicitly wrap on overflow, or overflow is UB, depending on whether it is unsigned or signed, respectively.
My main use case would be deriving Debug on my own structs that contain some fields of a library type that doesn't have Debug on it.
Sure, but "random number" without appropriate context is a mathematically meaningless quantity, e.g. http://dilbert.com/strip/2001-10-25. Without providing the context as part of the name, we require educating the user as to the *historical context* of random number libraries as implemented in other languages. It's admittedly pedantic -- we could all come to the agreement that "random number", without context, implicitly means a string of unbiased random bits of a certain length. After all, we make users memorize all sorts of implicit things, like performance characteristics of `Vec` vs `BTreeMap` -- nothing about the name `Vec` says anything about O(1) access. Then again, "random numbers" are much more inherently mathematical than these collections, and mathematically, random numbers are never (or at least, should never) be discussed without explicit declaration of the distribution. As I said in the parent comment, this unstated/implicit/historical context of uniform bitstrings then relegates non-uniform distributions to second-class citizens in the random library ecosystem. That is, a uniform random u64 can be had directly from the `Rng`, whereas every other distribution requires a *transformation* -- something like `Poisson&lt;Rng&gt;(rng: &amp;mut Rng, rate: f64)`. (Thus, the uniform `Rng` is the canonical random variable, and all other types are just transforms of an `Rng`.) *This is not necessarily a bad thing*, just something worthy of discussion before making it forever permanent in `std`. After all, C++11 seemed to find cause to break with this historical precedent. Given cargo, it actually makes sense for `std::rand` to *only* provide unbiased random bitstrings (perhaps called `Rng` though I prefer `Entropy`), then allow various crates to implement the specific distributions. That is, I now propose removing rand::distributions entirely, to be replaced with individual crates for each distribution. This makes the distributional assumption explicit, by requiring the user to cognitively opt-in to their consideration by loading the appropriate packages through Cargo.
I'm still working on getting [Decorum](https://github.com/olson-sean-k/decorum), a crate for ordering and constraining floating-point values, to a `0.1.0` release. There's [one more](https://github.com/olson-sean-k/decorum/issues/10) important issue to solve: [num-traits](https://github.com/rust-num/num-traits) has introduced a `Real` trait that looks almost exactly like Decorum's and includes a pesky blanket `impl` that makes it impossible to implement generically. I'd like to use it, but it's not clear to me how to do so without lots of code duplication and losing `impl`s informed by relevant traits.
* Using the `#[cfg()]` attribute on something makes it be included or not before type checking (like with the CPP in C/C++/Haskell/etc). * Using the `cfg!()` macro to get a static bool value, such as with an if statement, makes both branches be included in the compilation and then relies on dead-code elimination later in the process to hopefully cut the branch that's static false.
&gt;and got has the built-in http vs fasthttp You mean *go* has the built-in http?
Yes, thanks. Typo, hehehe.
Go is fine. It's easy to read, after couple of weeks you can read pretty much any Go code. This is very powerful. Struct embedding and compile-time duck typing also help. It's garbage collected, so it's not really good for tasks that Rust is good for. But static site generator is definitely in Go's comfort zone. Rust / C++ / C aren't right tools for this job. Pick a project in which you're going to have high *deterministic* performance requirements and harsh memory constraints and you'll see Go not doing so well and Rust shining.
Isnt there some RFC in the works which will make `assert!(a == b)` print "nice" messages too?
Continuing work on [tarpaulin](https://github.com/xd009642/tarpaulin), last week I designed and coded up my new improved data structure for holding coverage data. This week I'm rewriting and refactoring to integrate this new code. Also, going to the Rust London meetup on Thursday!
It can be okay to match on `0.1` as well, it's just less common. Maybe you have some analytical expression like `exp( -1/(x - 0.1)**2 )` you want to evaluate.
This is an interesting case: Our rules forbid language bashing, but here we have a post asking folks to upvote language bashing in another subreddit. In any event, I'm going to remove this for now. /u/dobkeratops, please contact us mods to sort it out if you want it reinstated.
And if `.sort_unstable()` is not fast enough, there is `.par_sort()` and `.par_sort_unstable()` in rayon which is even faster as it uses multiple threads.
Happens to the best of os.
No, the compiler cannot prove exhaustiveness in general when using match guards.
I agree that the distributions should be more explicit, but it might irritate users if they have to specify the uniform distribution all the time. There is some discussion related to this (the fate or `rand::Rand` and `rng.gen()`) [here](https://github.com/rust-lang-nursery/rand/pull/256).
No, vendoring is very necessary. crates.io is good, but sometimes you want everything you need to build your project in your own repo.
Worth noting is that Go does not have true preemption https://github.com/golang/go/issues/543
So looking at the weekly driver initiative I'm just wondering how timing is tackled when for some devices they may struggle and for some it may be trivial to meet timing? i.e. because the clock speed between say an arduino mega and one of my STM32F4 dev boards are orders of magnitude different!
As a further point about your "analytical" expression, here's some actual python code you can see showing that analytical precision doesn't apply to floating point numbers: &gt;&gt;&gt; print('%.60f' % (0.1)) 0.100000000000000005551115123125782702118158340454101562500000 &gt;&gt;&gt; print('%.60f' % (0.1 * 3.0 / 3.0)) 0.100000000000000019428902930940239457413554191589355468750000
Ok i understand thanks for your response !
Servo is participating in GSoC under [Mozilla's banner](https://wiki.mozilla.org/Community:SummerOfCode18) for the third time. We have a crate that is several hundred thousand lines of generated code, as well as another hundred thousand lines of hand-written code. This is very bad for the memory usage of rustc, as well as the time it takes to compile, but the inter-code dependencies are tricky. We want your help testing some of the ideas we have for how to split it up to better understand if it's actually possible or not!
Re the time crate not working on wasm: In my local system, I just made a cfg-wasm option for time using the stdweb crate and calling the appropriate stdweb method to get the current time. The crate has a Linux path and a Windows path; why not a stdweb path? It seems like the better solution than just disabling time completely for the web use case. Disabling time in chrono makes sense for other reasons, but not the web use case.
This is one of the reasons I'm excited about wasm. It's not even always about wasm itself, but if we can make Rust awesome at it, it means we need to flesh out stuff like this, which helps other platforms and Rust generally!
If it matters, there's nothing stopping you from writing unsafe which mmaps /dev/zero and exposes it as a big chunk of `[u8]`.
I've used mocker-rs before and found it surprisingly good (it reminded me a lot of GoogleMock, as its author intended!). There are various limitations and, as you point out, the author is only willing to stand behind it as a proof of concept - but if you're only using it for test code, you should be fine. Mocking though something other than traits is basically impossible since all non-trait-based calls are statically and monomorphically dispatched and hence can't be adjusted in test builds. As a workaround, see https://github.com/kriomant/mockers/blob/master/doc/guide.md#mocking-structures for an example that swaps out a struct + `impl` for a trait + mock in test code only.
Just did a talk on WebAssembly at a local JavaScript meet-up. I'm planning to turn it into a blog post shortly. I also just published [a short post](https://jeff.parsons.io/blog/quick-update/) about what I've been doing on [PlanetKit](http://github.com/jeffparsons/planetkit) over the past year. For my next PlanetKit-related post, I'm planning to walk the reader through fixing an issue that's been hanging around for a while, as a way of documenting and sharing my debugging process for something like that end-to-end.
I have a problem where std does not seem to be a default feature as the tests reveal. I get an error when I try to add "std" to the features section of the .toml.
You mean the frequency of the interface (SPI, I2C, etc.)? All drivers punt the configuration concerns to the caller. Currently, the embedded-hal interface (traits) only specifies I/O, not configuration, so it's not possible, for example, for a driver to check that the SPI frequency doesn't exceed the chip maximum rating.
no problem, glad it helps! Futures are.. interesting to work with. It forces one to think about problems differently, even more so than just rust in general.
That's not the point. There are expressions which give NaN for certain numbers and it is legitimate to avoid them. You have a point that in my example you should actually check whether `x - 0.1 == 0.`, because there is more than one number where this becomes zero. Also, there are numbers which can be represented exactly. Introducing an arbitrary epsilon does not always make sense.
Another timing struggle is the difference on register access times between release and debug modes. Say, for example, you're initialising a peripheral that needs certain gaps between operations (LCD clear is an example), then you can't really rely on register access times if you're developping in both modes. Not sure how to solve this generally, other than maybe somehow have the base rtfm crates be optimized -o3 in both release and debug. You could probably use a timer, but for small delays it may not be needed in release mode.
&gt; Introducing an arbitrary epsilon does not always make sense. Yes, for such cases as comparing against 0, infinity, or NaN. For any other floating point comparison I can think of, you shouldn't be doing an exact equality comparison. At least, I can't think of any others. This is why match should not allow you to match against 0.1 or other arbitrary floating point values. You should be matching against a range, which is +/- epsilon from the value you're looking for.
[BTW - love your stuff japaric, keep up the good work!]
Static variables are in memory for as long as the program is running. They may end up in swap space rather than RAM, but they have to be accessible somehow.
If you look at some of the older blog posts from u/japaric he showed how to do some performance monitoring. You can pretty easily tell the cpu usage. So far some of his examples are relatively complex and he hasn't had to bump the speed up from the default frequency (8MHz on the bluepill boards, they can go up to 72MHz, so there's a lot more room to work with).
Thanks that worked!
I agree, this doesn't look very pretty. Could you elaborate on what you are trying to do ? This might be a case of the X Y problem. The reason i say that is because the pattern your building looks like polling. Tokio is build to abstract from that and you seem to be re-implementing polling using Tokio. Judging from your code your situation could be either * The event in future 'a' should stick around until a new event , while future 'b' triggers multiple times before 'a' changes or * reading in 'b' has a limited throughput, and you don't want to do work on value's from 'a' that can't be used anyway Or neither. So if you could elaborate on the underlying problem your trying to solve i might be able to suggest a different approach. 
Offtopic, but if anyone is looking for Rust and Emacs related project idea, here's one. Even though I'm an Emacs and org-mode user, I wouldn't mind and actually welcome a feature complete org-mode implementation in Rust so that it will be easier to process Org files without running Emacs in batch mode. I do just that right now to publish articles from Org files and think if we had a liborg (written in Rust), it would make it easier for other editors to support it and allow efficient processing.
I'm thinking more about things like one wire protocols where people tend to bit bang a single gpio pin and use delays or timer interrupts to control timing
As far as I know you can't use the GPU like that you'd have to write specific code (kernels) first, in a language like Opencl and then call them from rust 
I meant something like https://futhark-lang.org/ or Haskell's Obsidian or Accelerate.
This article title should be rewritten "how to install Windows 10 and make it as linuxy as possible because I'm familiar with Linux tools and text editors only" You shouldn't ever have to install cygwin on Windows. And if rust actually requires that (it hasn't for me so far), then it shouldn't. It doesn't go into how to install the plugins for visual studio either.
Can you elaborate? What did you do precisely and what error did you get?
I don't think rayon in particular will be doing this, but there are other projects in Rust which allow for GPU offloading, like https://crates.io/crates/arrayfire. I don't know if there are any pure-rust ones though, most seem to be bindings to external libraries?
The idea I had is that it's possible to have an API that can target CPU and GPU so that you write once and, if you restrict to portable code, run on both or either chip. Haskell Accelerate does that.
Are you referring to [the rfc](https://github.com/rust-lang/rfcs/blob/master/text/1884-unstable-sort.md) and linked [pull request](https://github.com/rust-lang/rfcs/pull/1884)? Thank you for making me aware of them. They are super well written!
Rayon is such a great crate for Rust to have.
Full credit goes to /u/stjepang for the `par_sort` implementations too!
It's an extremely important point and have fixed that line reflecting your feedback. I've also thanked you at the bottom. Thank you again!
Thanks for the compliment and code snippet! That's an elegant counting sort you've pasted as well.
Where is cargo lacking that you need to rely on an external build tool? Extra extensibility can be added by [build.rs](https://doc.rust-lang.org/cargo/reference/build-scripts.html) if required. I don't know of any specific build tool for rust, but cargo was designed so that it can be used from within any other build tool if you wish, no need for a rust specific one.
You can set up a timer, here's an example from the rtfm repo https://github.com/japaric/cortex-m-rtfm/blob/master/examples/two-tasks.rs . For the neopixel specifically, if the clock speeds work out for your chip, you can use a combination of this post on doing DMA transfers http://blog.japaric.io/safe-dma/ and this method of using spi to drive neopixels http://www.rogerclark.net/arduino-stm32-neopixels-ws2812b-using-spi-dma/.
Tidied up my gtk-rs+OpenCL mandelbulb; see https://github.com/penguin42/opencl-play/tree/good-2018-02-19a It's my 1st gtk-rs program and my first OpenCL program, so I'm interested in feedback about ways to improve it.
In my cargo.toml file, I have ```[features] default = ["rand"]``` and I want to be able to add "std" to the default, or some equivalent.
As another maintainer who welcomes help, one starting point is [liquid](https://github.com/cobalt-org/liquid-rust) (a port of the Liquid template language for Ruby). You could add [missing tags](https://github.com/cobalt-org/liquid-rust/issues/163) or [filters](https://github.com/cobalt-org/cobalt.rs/issues/350). As you get familiar with Rust from that, I'd welcome you to [taking a context struct or two and turning them into traits](https://github.com/cobalt-org/liquid-rust/issues/95) which will allow increased performance for [cobalt](https://cobalt-org.github.io/) as well as make it easier to [reuse in other applications (like extracting data from a database)](https://github.com/cobalt-org/liquid-rust/issues/166). Feel free to ping us on [gitter](https://gitter.im/cobalt-org/cobalt.rs?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge).
You need to also create a feature named std, `std = []`. Then you can add `"std"` to the list of default features, and do `#![cfg(not(feature="std"), no_std)]`, etc.
I'm not sure we're talking about the same kind of API. The projects I mention like Accelerate allow you to write code once and target the array computation to be executed on a GPU without a need to rewrite anything. There's also Repa in Haskell and probably quite a few in C++. Again, this is primarily about API/DSL to program array computations. In terms of API like OpenMP or OpenCL, the current mindshare is around CUDA and Vulkan. OpenCL seems to have been superseded before it could gain mindshare. But that's the low level target of such a project and not the API exposed to the user.
Strings as keys in hashmaps: is there a way we can add a String as a key, and use a &amp;str to refer to it? I'm writing a typechecker for my toy compiler project, and my first thought was to use a Hashmap to store type information for various identifiers, but there's that whole String/str thing. Currently, I simply store bindings in a Vec, and search through for that (I have a method that returns an index, that means that subsequently, if we have the index, it is O(1). I don't know, I guess I'm just looking for advice on what way to go here.
https://play.rust-lang.org/?gist=30ea2a44a8d26f4ac9f20319cd329076&amp;version=stable Yes, it is possible to use a `&amp;str` to look up values in a hashmap where the keys are `String`s. 
You can definitely do lookups on `HashMap&lt;String, _&gt;` with `&amp;str` via `get()`, `get_mut()`, `contains()` and `remove()`. However, `entry()` and `insert()` require `String` (I do wish the former had some sort of copy-on-insert thing so you could do `entry(&amp;str)` but that might have type inference issues or something).
That’s fair. I suppose I should have written more of an introduction. The background is that I’ve been primarily using Linux for many years, and that’s very unlikely to change. The one Windows machine I have at home is only for playing video games. So yes, this is totally about getting things closer to what I’m already familiar with (for example bash instead of PowerShell) without spending a lot of time learning an entirely different ecosystem. But then, an "actual dedicated Windows user" who also does programming would already know how to do this stuff and not need this blog post, right? (Rust doesn’t require cygwin, and I haven’t used sygwin so far. I’ve only installed Visual Studio so that build scripts and rustc have compilers and a linker available, I don’t plan on using the IDE.)
One thing I really miss from Go is easy static compilation. And easy _cross compilation_. The first time I did `GOOS=linux GOOARCH=amd64 go build` and then scp'd my binary to my Digital Ocean droplet and it just runs. No dependency worries, nothing to install. It's absolutely amazing. It's also the product of a huge amount of work and a totally custom toolchain. Rob Pike gave a good talk about it at Gophercon 2016, and wore a nice blazer to spice things up: https://www.youtube.com/watch?v=KINIAgRpkDA 
Congrats on shipping this looks like a substantial effort! Just a heads up it looks like docs.rs failed to generate the docs and the Travis build is failing.
In the Go case, preemption can be done by inserting checkpoints in tight loops (by the compiler). In the rust-coroutines case where the library has no access to the codegen process, it might be possible to preempt a task by sending a Unix signal to the corresponding thread and yielding out from the signal handler, though I haven't tried this approach.
Oops,thanks for your heads-up, but I still have no idea why is docs.rs failed to generate the docs, is it related to Travis build ? PS, as a junior Rustacean, it does need a lot effort to develop the first big crate :) 
&gt; In the Go case, preemption can be done by inserting checkpoints in tight loops (by the compiler). Sure, and they're already putting a bunch of checkpoints in all over the places. All they are talking about with this "tight loops" issue (if I'm not mistaken) is just better covering some cases that aren't handled particularly well by the current set of checkpoints that are generated. That doesn't fundamentally change anything about the way they're doing preemption. I'm wondering what /u/farnoy meant by "true preemption" — whether they are suggesting some other specific preemption model or just fixing/improving the current one used by Go. &gt; In the rust-coroutines case where the library has no access to the codegen process, it might be possible to preempt a task by sending a Unix signal to the corresponding thread and yielding out from the signal handler, though I haven't tried this approach. I'm curious about how this approach would work. Would there be a timer on another thread that triggers this? I.e. deciding that "you've had long enough to cooperatively yield, so I no longer believe you will any time soon" and so sending the signal then?
It shouldn't be related to the Travis build. Interestingly it appears to be triggering a compiler bug. Running `cargo doc` on a fresh checkout I get: Documenting rspotify v0.1.1 (file:///home/wmoore/Source/rspotify) error: internal compiler error: unexpected panic note: the compiler unexpectedly panicked. this is a bug. note: we would appreciate a bug report: https://github.com/rust-lang/rust/blob/master/CONTRIBUTING.md#bug-reports note: rustc 1.24.0 (4d90ac38c 2018-02-12) running on x86_64-unknown-linux-gnu thread 'rustc' panicked at 'index out of bounds: the len is 0 but the index is 18446744073709551615', /checkout/src/liballoc/vec.rs:1551:10 note: Run with `RUST_BACKTRACE=1` for a backtrace. error: Could not document `rspotify`.
Yes works on nightly for me too. I'm going to assume it's a bug that already been fixed and not create an issue on GitHub.
Although, curiouslt the Travis build is failing when trying to run the docs tests with a similar error: "thread 'main' panicked at 'index out of bounds: the len is 1 but the index is 1', /checkout/src/liballoc/vec.rs:1536:10". It might actually be worth raising an issue with this info.
It is a little bit weird that I have set Travis compiler to nightly: language: rust cache: cargo matrix: include: - env: TARGET=x86_64-unknown-linux-gnu rust: nightly But now Travis runs the docs tests failed with similar error, so weird 
Sadly, Google decided to give zero slots to the microkernel devroom this year, so Redox couldn't be a part of this, this year.
AFAIK Erlang does this nicely, it's also based on preemption points (like Go), but also counting what they call reductions. Looking at a timer would be relatively expensive, so that is why they came up with the reduction concept. I think it's mostly function calls that count as reductions, probably some other stuff as well. Of course it helps that the only way (I think) of doing loops in Erlang is by recursion, so each step counts as a reduction.
&gt; Larger standard library Isn't it because Go's stdlib can't be re-implemented in pure-go? Like it lacks generics so all containers should be in the stdlib on not exist in the same form at all? Also on the pro-side I don't think you put enough accent on the boilerplate that comes with error checking, and that's not enforced by the compiler - I think Rust is miles ahead here. 
I cannot seem to get no_std working. I have in Cargo.toml [features] default = ["rand","std"] std = [] In lib.rs #![cfg_attr(not(feature = "std"), no_std)] and then wherever it is needed #[cfg(std)] My tests integration tester will not work (as in it claims macros and functions dont exist) with anything that has `#[cfg(std)]` or `#[cfg(no_std)]` on it. I tried `cargo test --features "std"` but it changes nothing
Tell others not to give advice they haven't been asked about
I'm a child. So what?
Got it. Thanks. :)
As far as futhark goes, that still has to generate the gpu kernels, and even futhark is a bit serendipitous that it came to be. It's history is fascinating and worth a read.
I think that's because, in general, the trait system makes mocks redundant. A lot of the time you can swap out some concrete type with an in-memory buffer or whatever, and be done with it. By convention traits tend to only have a couple required methods, so it's not often that you need to reach for a full-blown mocking tool to create a mock type. 
https://doc.rust-lang.org/cargo/reference/specifying-dependencies.html#caret-requirements &gt; In this case, if we ran cargo update -p time, cargo would update us to version 0.1.13 if it was available, but would not update us to 0.2.0. If instead we had specified the version string as ^1.0, cargo would update to 1.1 but not 2.0. The version 0.0.x is not considered compatible with any other version. All I can say is have a read of https://github.com/rust-lang-nursery/rand/issues/205 and tell me people aren't just doing whatever they feel like seems like a good idea. I'm utterly unconvinced by the argument 'thats how everyone understands versioning in rust to work'. Clearly a consistent message on this stuff is totally lacking. Obviously you can just pin exact versions, and cargo does that well, so its not like your build will suddenly fail for no reason, but yes, cargo will install broken dependencies if you aren't careful. 
 fn create_elements(number: u8) -&gt; [u8;1] { let d; let a = 1; let b = 2; let c = 3; if ref_number &gt; 1 { d = [a]; } else if ref_number &lt; 1 { d = [b]; } else { d = [c]; } return d; } I'm not sure if this helps, there are ways to write it in a "cleaner" way. You could do: fn create_elements(number: u8) -&gt; [u8;1] { [create_elements_inners(number)] } fn create_elements_inner(number: u8) -&gt; u8 { if ref_number &gt; 1 { 1 // a } else if ref_number &lt; 1 { 2 // b } else { 3 // c } } or (true to the python version, but with unnecessary dynamic allocation): fn create_elements(number: u8) -&gt; Vec&lt;u8&gt; { let mut d = Vec::new(); let a = 1; let b = 2; let c = 3; if ref_number &gt; 1 { d.push(a); } else if ref_number &lt; 1 { d.push(b); } else { d.push(c); } return d; }
I'm really happy you posted a code snippet. The last one is what I hoped to see. And I learned new tricks from the two first snippets. It really helps me to see real code usage for simple problems as I really feel the progress !
In real world, I have more complex datas and checking them is trickier. Wether it makes sense to use an enum for my data is uncertain, but certainly worth trying.
There really is no processing to do. All [the file](https://raw.githubusercontent.com/behnam/rust-unic/master/unic/ucd/name/tables/name_map.rsv) is, is just a `&amp;[(char, &amp;[&amp;str])]`. It can't really get that much more basic.
If you are using the values of a, b, c for numeric computation or similar, then it probably makes sense to be a value. But if not then you should probably use an enum 
Another option is this project: https://github.com/casey/just which has many similarities with make and may meet your needs
&gt; All I can say is have a read of https://github.com/rust-lang-nursery/rand/issues/205 and tell me people aren't just doing whatever they feel like seems like a good idea. It looks to me like the maintainer proposed a bad idea (treating 0.4 as an "unstable series") and was talked out of it. Did you pick up on something else here? &gt; I'm utterly unconvinced by the argument 'thats how everyone understands versioning in rust to work'. Clearly a consistent message on this stuff is totally lacking. I have no idea where this came from. I *just said* I think we should have stuck to the semver spec because cargo's policy causes these miscommunications. &gt; Obviously you can just pin exact versions, and cargo does that well, so its not like your build will suddenly fail for no reason, but yes, cargo will install broken dependencies if you aren't careful. Have you had this happen personally? I think it's extremely rare. Certainly there have been incidents where maintainers decide they don't like semver, but the community [pushes back hard](https://github.com/chronotope/chrono/issues/146).
I'm building a text-based interactive fiction game with a few graphics littered here and there. What would be the easiest Rust library for building something like this?
As an ugly workaround: change the file format, compile it with GCC and link the resulting .o, accessing it via an .extern "C" ?
eww. eww. Eww. That would work though.
This would be a lot better fn create_elements(ref_number: u8) -&gt; Vec&lt;u8&gt; { if ref_number &gt; 1 { vec!(1) } else if ref_number &lt; 1 { vec!(2) } else { vec!(3) } } 
&gt; But then, an "actual dedicated Windows user" who also does programming would already know how to do this stuff and not need this blog post, right? Kind of, yes. Of course, I read the title as "How to set up a rust development environment (on Windows 10)" rather than "How to set up Windows 10 (because I'm going to do rust development)", and was disappointed that the one actual part that was relevant was this: &gt; **Install Rust** once Visual Studio has finished installing. Rustup’s default install method and configuration are fine. Which is just "go find it on the web and let it do its thing" and not very helpful :). I guess it's kind of on me, since the title really does say what you're doing, but could be a little clearer. Maybe somebody needs to write the article I was expecting, since it's non-trivial to get a "Windows" experience for Rust.
When I started learning rust I wished I would have known about "rust by example". Its a really good site for starting out. https://rustbyexample.com
Another good resource for building familiarity is http://exercism.io/
Put it in a separate crate. I had a similar problem with a giant generated protobuf .rs file. 
Funny, was having the same thought yesterday - Rustscript, anyone? Still statically typed, but everything is done with references. But then I wonder if a Rust interpreter would get us there without the potential for confusion from similar-but-different languages.
Unless you're comparing against C. Then go ahead and judge it that way.
That leaves us with two options: 1. You follow the rules, play nicely with the others here 2. You'll get told to leave
Before debating others' reading comprehension, you should do a little bit of reading on your own, namely the 'sidebar' (to the right), which contains the rules of this place. If you continue to refuse following them, we will ask you to leave.
Can you mark the test helper module with `#[cfg(test)]`?
That's something I learned from working with solidity but excellent advice! 
So I'd considered that for driver crates but it seems too specific and inflexible. I suppose the solution could be to create multiple ways of driving a neopixel and using features to exclude/include them
&gt; solidity Oh! What is the language like? I've not gotten into the smart contract realm - what is it like?
Probably the worst learning experience of my life, but the amount of fun I've had in the jobs I've got since learning the language has been worth it.
This. On Linux at least, memory isn't really allocated until it's written to (or read, but you shouldn't do that before writing should you ?).
@oli-obk's suggestion still sounds like a good idea. But for a start, what happens if you change that slice into a `&amp;[(char, usize, usize)]` that points to another array? If you're worried about the increased memory usage, you can probably pack the two values into a single `u64`.
No, the common module is only used from the integration tests so that does not make a difference. I guess the problem is that rustc is only compiling one integration test file at a time and thus warns about the common code that isn't used from that file.
Basically, failure builds on a lot of things that were learned from creating and using error chain. It's the new recommended way to construct error types and do error handling. It can be backwards compatible with error chain and it gives you a lot of new and improved features as well. See this post for more: https://boats.gitlab.io/blog/post/2017-11-16-announcing-failure/
Hey, check this out https://github.com/sit-it/sit-import -- it's not fully feature-complete for sure, but it's my first take at attempting to import GitHub issues.
How to arrange your `extern crate`, `use`, `mod`, files and folders in your project.
I'm very curious what the performance implications of doing something similar to -ftrapv would do (as opposed to the current strategy of unwinding on overflow).
The thing is this can't be done as a library because you need compiler support for it.
Oh, my favourite joke generator is one I saw in someone's signature: ```rust fn random_number() -&gt; -32 { 4 // generated by roll of a fair die } ``` Like it or not, the only practical *pseudo-random number generators* in computing generate either a single integer or a block of integers at a time, (hopefully) with apparently uniform distribution. Non-uniform distributions *are* second-class citizens because just about all of them are produced by applying a function to a uniformly-distributed value (or values). Actually, having separate crates for generators and distributions is what this post is about, it's just that we've arrived at slightly different names (`rand-core` and `rand`), and the generators themselves may move to other crates later. Oh, and this isn't `std::rand` (which may or may not go away but won't ever be exported), it's the `rand` crate. Also, RNGs *don't* create entropy, they merely output a deterministic sequence of (hopefully) unpredictable values. Entropy is a confusing topic, but in the context of algorithmic generators it usually refers to how many bits of the internal state (or seed) are unknown.
This is seriously cool :)
I am doing a multi-hour sync of facebook/react right now as a test...
CUDA is Nvidia only and Vulkan is a graphics API with some compute capabilities. The only API that is supported by most hardware is OpenCL 1.2, the newer 2.X versions are not well supported. So the best option is to support CUDA on Nvidia hardware and OpenCL everywhere else. This is what a lot of higher level APIs do.
What's the cardinality of items in the nested slice? What does performance become if you use compressed bitmaps (since they will be so sparse they will compress nicely) instead of a nested slice? 
Totally agree. I have spend a lot of time in getting to learn how to spread your code over different files. But once you get to understand it is very handy and the module system is very flexible. However now days I see that this topic is much better described with examples from the original rust site blogs and rust YoutubeChannels.
What is your recommended way of doing this?
When you start out there will be moments where you'll stare at a compiler error and feel like yelling, "WHAT DO YOU WANT FROM ME!?" This will be frustrating but there is a point where a bunch of it clicks (mainly ownership and borrowing) and you encounter those moments far less. You just have to stick with it thought that tough part. Now that I'm through that part, progamming in languages with fewer guarantees feels reckless and error prone.
: means “outlives”
That one probably tripped me up the most. It feels like an inverse dependency relation.
"Rerast is a tool for transforming Rust code using rules.".
Code written in ArrayFire can be ["compiled" at runtime](http://arrayfire.org/arrayfire-rust/arrayfire/getting_started.html#writing-mathematical-expressions-using-arrayfire) to work on CPU, GPU/OpenCL, or GPU/CUDA. The same is true of Tensorflow (CPU or GPU/CUDA) for which [Rust bindings also exist](https://github.com/tensorflow/rust) The thing is copying data from normal memory to GPU memory is a very expensive operation, so unless your dataset is _massive_, the memory-transfer overhead will make things slower overall, despite the compute speed. Hence why GPUs are much better suited to very large, very wide, _numerical_ problems, and why a rayon-on-GPU might not make a lot of sense. For smaller problems, there is a rayon-on-CPU-SIMD library called [faster](https://github.com/AdamNiederer/faster) which is pretty easy to use.
The [Rust Cookbook](https://rust-lang-nursery.github.io/rust-cookbook/) is a resource that would have been handy when I was starting out too.
i am looking for crate which will allow me to take picture of another program (screen), lookup things in it and do some mouse/keyboard input in it, like Java's [Robot class](https://docs.oracle.com/javase/7/docs/api/java/awt/Robot.html)
`s1 + s2` or `format!("{}{}", s1, s2)` or use std::borrow::Cow; fn add&lt;S: AsRef&lt;str&gt;&gt;(s1: Cow&lt;str&gt;, s2: S) -&gt; String { s1.into_owned() + s2.as_ref() } fn main() { println!("{}", add(Cow::Borrowed("Hello "), "world!")); } ^/s
I think this is the obvious way: fn add_strings(a: &amp;str, b: &amp;str) -&gt; String { let mut res = String::from(a); res.push_str(b); res } But it depends on how you want to handle memory allocation. This function allocates a new string when you call it. You could allow the caller to pass in a buffer so they have the option of reusing it, which is what `read_line` does.
Can you show an example of that symbol in that context? I think of : as meaning is a subtype of. 
Exactly! (Was on mobile, sorry for not providing the links.) Also have a look at [this PR](https://github.com/rust-lang/rust/pull/38192) which describes the algorithm the stable sort uses.
What are you trying to test in the binaries? The compiled binary or just login inside the binary? If you want to test the full binary then inside the intergration tests you can build/launch whem with std::process::Command. If you want to test logic, then you should create a library (or move the logic to a module inside your library), and make the binary a very thin wrapper.
The binaries are already a thin(ish) wrapper of their respective modules. The unit tests in the binary crates just test that parsing of cli parameters and logging are working properly. The unit tests inside the modules test the logic of each module as well as their public APIs. All is well there. What I'm trying to test in the integration tests module is running one (or more) of the binaries using std::process::Command or Assert_cli and test the interaction between them. The problem is that when I run `cargo test`, the tests defined in the integration module are compiled but not run. Any ideas?
If I may? That table is hugely memory inefficient and cache inefficient. Optimizing the data structure could then also lead to improvements in compile time. The elements of your array are `(char, &amp;[&amp;str])`, the size of the elements is 24 bytes per entry on x64 and another 16 bytes per string slice. I notice that the character properties come from a fixed set. Unless for ABI reasons (an extern function really demands that your table is in this format) it's recommended to use enums instead and provide a mapping table enum =&gt; string to do the conversion and back. Further the tuple itself is packed inefficient. Rust wants to use `usize` for the length where in this case the max is around 9 entries for a single character, that fits in a byte! For padding reasons you'll probably use the same size as the size of the index type (`u32` probably). The table could be written as `(char, Range&lt;u32&gt;)` and a separate table with the concatenated character property arrays of all the characters (so every character's properties are concatenated in one big array). To find the character's properties back simply use the Range&lt;u32&gt; (cast to Range&lt;usize&gt;) and index into the concatenated array. I expect this to yield impressive memory reduction and impressive cache pressure reduction and probably more efficient compile times. 
Definitely do judge that language by how hard it is to make a function which adds two string and return the result. Don't discard the language due to that, that's another issue entirely, but definitely judge it by that. That's an example of the trade-off you are making with Rust, you have language that is less intuitive and overly verbose for the cost of a language which catches an amazing amount of mistakes at compile time. That trade-off is often a good one, despite that trade off I wouldn't discard the language, but I will judge it based on that trade off.
&gt; When debug_assertions is enabled, overflow must panic I wish. But I still sometimes get overflows in my release build with debug_assertions. I suppose optimizations must be getting in the way.
They could just use Diesel instead and get prepared statement caching for free. ;)
Typically you use the SQL query itself as the cache key, not some arbitrary identifier.
Having to use a nightly build just to use the raw memory allocation API. I totally get that things take time to stabilise, but a systems language without raw memory allocation support (in stable, which is what people should be using) is just weird.
For anyone still considering whether to apply for GSoC or Servo project in general - please do! I've worked on the RLS for GSoC'17 and it was heaps of fun. I could recommend it for everyone! It's a really good learning experience and you also get to contribute to cool and useful open source projects. Also Servo team and /u/joshmatthews are really helpful and welcoming (currently working on a Servo uni project with their help), so think no more!
There is raw memory allocation support in stable though; `Box::new(...).into_raw()` for a single object, `vec![...].into_boxed_slice().into_raw()` for an array.
Please open an issue about it.
Yeah, we haven't really explored compression options beyond simple de-duplication. I guess I know what I'm looking into this weekend.
None of that supports custom alignments.
That's a different issue! As long as the alignment isn't computed at runtime you can wrap your thing into `#[repr(align(N))] struct AlignN&lt;T&gt;(T);` (stable since 1.24) but yeah none of this is ideal.
Haha, yeah. Ideally, you'll end up with a compact blob of data that needs no `std` support to use. I know almost nothing about UTF-8, but I think there were some clever approaches to compressing the tables. Maybe someone else also found a good solution to this?
I have submitted an [issue](https://github.com/rust-lang/rust/issues/48368) in github. If you have additional information about this bug, you could comment in this issue :)
Saying rustc doesn't "support" CUDA makes as much sense as saying rustc doesn't support Vulkan. Like Vulkan, CUDA is a driver that allows access to hardware that many computers do not have. It wouldn't make sense to build in support for such niche and rapidly changing hardware. ArrayFire does have a [unified backend library](http://arrayfire.org/docs/unifiedbackend.htm), so you can ask a user at runtime (or automatically inspect their system), and then use that to execute the Rust numerics code you wrote yourself. The only "language support" I can see is making vecs/arrays of integers/doubles etc. conform to the `Add`, `Minus` etc. traits, and then building support for SIMD execution -- possibly with the addition of an `InnerProduct` trait. However because of the [memory bandwidth issues I mentioned in another comment](https://www.reddit.com/r/rust/comments/7yqdw7/rayon_for_gpu/dujbd1t/) it would *never* make sense to use CUDA/OpenCL by default for this stuff. In most cases the memory transfer cost would overwhelm any compute advantage. There's also the fact that proper numeric computing requires things like support for decompositions &amp; solvers which will never end up in `std` as they're huge. Moreover for things that aren't vectors, you'd really want a new object that's BLAS conformant.
It's still not clear to me how best to do this. At the moment I have a struct with implementation, or trait, per file. Like a Java class style. The mod file ends up being: mod animal; mod cat; mod dog; pub use animal::Animal; pub use cat::Cat; pub use dog::Dog; ... over and over again. Which just seems silly.
This table is just a small part of the crate [unic-ucd-name](https://docs.rs/unic-ucd-name/0.7.0/unic_ucd_name/). The structure of the actual `Name` property as it exists currently can be found [here](https://github.com/behnam/rust-unic/blob/master/unic/ucd/name/src/name.rs). It is indeed an enum, where the table lookup has been done already, and a Display impl. The table's actual structure is abstracted away from the use site, though, for exactly this kind of situation. That's the reason it's wrapped in the [`CharDataTable`](https://github.com/behnam/rust-unic/blob/master/unic/char/property/src/tables.rs) enum; I can write a new variant of that enum and swap out the structure of the backing table without changing anything in the property itself. So, ultimately, the structure you recommend for the table is something like `CharDataTable&lt;&amp;[T]&gt;::Associated(&amp;[char], &amp;[Range&lt;Index&gt;], &amp;[T])`. I think this definitely could work with some finagling. Are you also recommending to replace the `T` here from `&amp;str` to another `Display`-able type? This is the realm of optimizations, and I really just need to write up a few microbenchmarks and try out some different schemes. I wish there were an easy way to see how heavy the stupid table is...
All of the existing solutions I can find support bidirectional lookup--that is, char-&gt;String and String-&gt;char--for the property that we're discussing here, the character name. For the time being, anyway, we're explicitly _not_ supporting reverse lookup, so our data constraints on format are so different from existing solutions' that what (little) research I've done into them hasn't yielded much of use.
&gt; Edit 2: Found it: Make your own make Nice article!
EDIT: now that I'm awake, I realize I answered the wrong question. The nested slice is up to around 6 `&amp;str`s I think.
No I don't but at least I'll be able to check that it has *really* been solved.
&gt; (just to confirm, the tests direstory is inside the project root and not inside src right?) That is correct, tests is at the same level as src. I'll poke around a bit more to see if I can find the issue.
&gt; Which just seems silly. It is. I normally create new files only when my current level of abstraction is too large to handle. If I can't keep in my mind what's where in the current file, it's time to separate in their own modules.
Forget OO. Embrace procedures. Break-up your types in smaller types/tuples.
I'd have called it rustify but nice work :p
if you like to make your own, glad to see a Rust version of https://hexo.io/
why doesn't this name occur to my mind, if you tell me this name before, I will name it with `rustify`
Yeah that's one solution, but you can't publish to crates.io like that unless you upload a whole fork of time and chrono to crates.io, which seems suboptimal. Alternatively you could try to get a PR like that through, but that limits the implementation to stdweb / JavaScript + cargo-web, which is also fairly suboptimal, as the story around external imports for the wasm target is still very much up in the air. Therefore the easiest solution to move forward is to have a feature in chrono that allows you to get rid of the OS dependencies and possibly (through traits maybe) allows you to define those yourself.
&gt; the adafruit neopixel library That's less of a timing problem and more of a missing features problem. The neopixels can be efficiently (asynchronously) driven using PWM+DMA, SPI+DMA or GPIO+DMA. Here's a [demo](https://github.com/japaric/ws2812b) with a STM32F1 microcontroller that runs at 8 MHz (half the Arduino frequency) and that drives a 24 LED strip using PWM+DMA at [over one hundred updates per second](https://github.com/japaric/ws2812b/tree/master/firmware#performance) with plenty of CPU time free to spare for other tasks. The reason that library is written ilike that is that the AVR Arduinos lack a DMA. That being said I don't think it's impossible to write a blocking driver like that; you would have to make the driver ask for the CPU frequency so it can generate the right delays or enforce the CPU frequency to be a certain value in the contract of the API.
I think you might be subdividing a bit much of you're doing one trait or struct per file only to them publically re-export it in the module. There's no hard rule, but logically grouping components together is a good choice and then splitting it up if the module gets too big is about right. Figuring out that balance between to big a file and to many is hard one.
spotifry?
I found an entry for U+1f502 with 11 properties: ('\u{1f502}', &amp;[CLOCKWISE, RIGHTWARDS, AND, LEFTWARDS, OPEN, CIRCLE, ARROWS, WITH, CIRCLED, ONE, OVERLAY]),
&gt; I'm not exactly certain how this would work Well... You'd have to adjust your format. What you could do is compute a perfect hash function for all the `char`s and thus not require placing them in the table anymore. Instead you could have an array of indices and lengths. This array would be indexed by computing the hash of the `char` whose string value you want. The strings you would concatenate together into one big file and include with `include_str`. The index array would then access specific positions inside that huge string. Some day in the future you can use a `const fn` to parse your original source that you used to generate the array, and generate the array at compile-time. That might regress compile-time again though. Anyway, your crate compiles fine with miri now ;)
Even shorter version: fn create_elements(ref_number: u8) -&gt; Vec&lt;u8&gt; { vec!(match ref_number { 0 =&gt; 2, 1 =&gt; 3, _ =&gt; 1, // equivalent to `ref_number &gt; 1` }) }
You'll spend your next 5 years thinking far too much about this languages communities and how to grow it. I admit it, it's very personal.
Write a little everyday. It wasn’t until I committed to writing a little rust everyday that my understanding of the language developed. I went from needing an IDE back to VIM after that.
You can manually #[allow(dead_code)] on a per-function basis. Would that work?
[a, b].join("")
For some reason i only come up with these ideas for projects i don't work on. My Brother has rewritten a tool called "DemoInfo" in go-lang so i suggested naming it Dingo (DemoINfo Go), he could even use a Ding as a logo. With my own projects i'm not that good with names :(
I don’t have an answer but have been thinking along much the same way. If the main reason for switching to actors and event loops was to help with memory management across concurrency, and rust was designed to facilitate that at compile time, is it really necessary to reinvent those patterns here? I _think_ that the most common answer is the c10k problem, but I’ve wondered about just using threads with smaller stack sizes (https://doc.rust-lang.org/std/thread/#stack-size) which should provide the memory efficiency gaps. ¯\_(ツ)_/¯ Would love to see some experimentation and benchmarks.
&gt; clear to me how best to do this. At the moment I ha You could just do it like this like this: put your animal logic in one folder called animals - animal (folder) - mod.rs - animal.rs - cat.rs - dog.rs Then in your mod.rs: mod animal; mod cat; mod dog; pub use animal::Animal; pub use cat::Cat; pub use dog::Dog; Or: pub mod animal; pub mod cat; pub mod dog; So now in other files you could just do (depending in whits folder you are): use animal::{Dog, Cat, Animal} Or: use animal::dog::Dog; use animal::cat::Cat; Right?
If you know size of your records (otherwise you'll have to read file and parse content anyway) you can use [`io::Seek`](https://doc.rust-lang.org/beta/std/io/trait.Seek.html) to jump to the next record without reading everything in between.
Yes, I know, but if those functions later would become really dead, is there a way to find out? Perhaps it is better to refactor the code to use more modules and thus not include any dead code?
I am taking a short break from [lyon](https://github.com/nical/lyon) to work on [rust bidings](https://github.com/GodotNativeTools/godot-rust) for the [Godot game engine](http://godotengine.org/)'s. There is a lot left to do but the nice thing about this project is that it is not very demanding for a change, mostly repetitive and simple tasks. Great for coding after my brain has been completely fried by work. This project started as a fork of [another attempt](https://github.com/Thinkofname/gdnative-rs) at making rust bindings for godot that had stalled. The author doesn't intend to continue working on them at least in the short term, but the work he had already done was a very good starting point. If anyone is interested in teaming up or just giving a hand occasionally, there is a lot of easy tasks as well as some more involved work to do! Once the bindings get to a decent state (hopefully soon), I intend to spend more time on lyon.
I'm getting slight off track here but your `CharDataTable` is a bit odd. I'm not sure why it needs to be an enum, consider this: pub trait CharDataTable: Default + IntoIterator { // Note: Reusing IntoIterator::Item to indicate the mapping type fn contains(&amp;self, needle: char) -&gt; bool; fn find(&amp;self, needle: char) -&gt; Option&lt;Self::Item&gt; where Self::Item: Copy; fn find_or_default(&amp;self, needle: char) -&gt; Self::Item where Self::Item: Copy + Default { self.find(needle).unwrap_or_default() } } #[derive(Copy, Clone, Debug, Default)] pub struct CharDataTableDirect&lt;V&gt;(&amp;'static [(char, V)]); impl&lt;V&gt; IntoIterator for CharDataTableDirect&lt;V&gt; { type Item = V; ... } impl&lt;V&gt; CharDataTable&lt;V&gt; for CharDataTableDirect&lt;V&gt; { ... } Now this might be an incredibly invasive, backwards compat breaking change and thus may not be feasible. The reason I want to point this out is that now you incur an overhead of having to match (which turns in a branch in the compiled code) everywhere. This would enable more efficient code and give more freedom to optimize for specific data structures. Further I would then suggest the following new impl: #[derive(Copy, Clone, Debug, Default)] pub struct CharDataTableSlice&lt;V&gt;(&amp;'static [(char, Range&lt;u32&gt;)], &amp;'static [V]); impl&lt;V&gt; IntoIterator for CharDataTableSlice&lt;V&gt; { type Item = &amp;'static [V]; ... } impl&lt;V&gt; CharDataTable&lt;V&gt; for CharDataTableSlice&lt;V&gt; { ... } Observe how I've specified the "output" item type to be `&amp;'static [V]` and decoupled it from the original generic `V` type. Now this datatable knows how to map a char to a range, and index that range into the table of V to produce the final value. I've no idea how feasible this is to implement in your architecture sorry, but I hope this can help. If you'd like I can do some more experiments at home actually implementing this and opening a Pull Request.
How traits function at a compiler level. It wasn't until I understood trait objects that I fully understand how traits worked as a part of the langauge.
Don't freak him out.
Longer compile times are also possible with generics.
We can always download the benchmarks and try to replicate the results (yes it's probably a huge pain in the neck)...or just trust you. I personally opt for the latter.
&gt; PhantomData should be used like PhantomData&lt;fn(T)&gt; if my type doesn't actually store T This is new to me, is this documented somewhere? I'd like to learn more about it
/u/nasa42
Next step: I’m going to inquire on the `cargo` lib. It seems it can help remove a lot of unsafety from my code. :)
[Here](https://doc.rust-lang.org/beta/nomicon/phantom-data.html) But it's not much.
Yep, I should've known. There's theoretically no upper limit to the number of words in a character's name. Nothing is keeping us from merging words into one word other than the human cost of figuring out the preprocessing on how to best do that.
Hey, this was a net win for me, I got some cool ideas for compression/optimization out of this ;)
Of course :D I'm more saying that if you want people to use your crate (which it does) rspotify was a good choice :) that or you can change to rustify and then use the keywords field in the Cargo.toml to let them know it's for spotify :D
Note that any seek operation on a BufReader invalidates its cache, though: https://doc.rust-lang.org/std/io/struct.BufReader.html#impl-Seek 
The problem here is that everyone knows it needs to change (cue rust2018 blog posts and three distinct RFC's), but unfortunately no one agrees on *how*.
It'd be useful if the resulting documentation was shown. As-is I don't know what this accomplishes. What is generic about these docs?
The sad fact is that I think the mainstream just want classes, and a file per class. There are a few exceptions, but for most stuff that's all people will want. Rust's trait and implementation system reminds me of what happened with JS's prototypes. An overly clever system, with lots of flexibility, but people just wanted to write `class Foo` in `foo.js` and be done with it.
In the loop though as I parse the records, could I have a random value and skip the current record if the random value is less than a threshold? How would that be done?
Thanks!
Interesting, got a link?
We theoretically _could_, but we don't want to. Think ICU4C: we preprocess the UCD official files and provide a convenient API surface to access the data. I think this representation, as suboptimal as it is, still beats a regex search in the raw UCD files in both performance and space. And we want to support no_std so we can't assume access to a file system.
`x` is 1. *ln(1)* is 0. You're doing `1 / 0`, which is infinity. Infinity has no meaningful conversion to an integer. LLVM has realised this and has responded by doing whatever the hell it pleases, because you asked it to do something impossible. This is a known issue which will probably get fixed at some point. For now, don't do impossible things; the restaurant at the end of the universe is overrated, anyway.
I am not completely sure that you mean, but if your format is such what you have to parse all records either way and need just to filter them, then you can write your own iterator which will emit records, so your code will look like this: let records: Vec&lt;Record&gt; = RecordsIterator::new(path).filter(|r| r.rand_value &gt; threshold).collect(); Here you collect filtered values into vector, but you can process them on fly without allocating additional vector, through `map`, other adapters or explicit loop. But note that in the end you'll read you whole file and will parse all your records, so it will not be "lazy".
Known soundness bug in the compiler: https://github.com/rust-lang/rust/issues/10184
You can combine all the tests into a single crate (a single file that imports each test file as a module).
https://github.com/servo/servo/wiki/Autogeneration-of-style-structs is another Servo GSoC project that also helps make Firefox better.
You can mix it with other macros. Let's say for example: ```rust macro_rules! impl_struct { ($name:tt) =&gt; { doc_comment! { concat!("blabla Example: ``` ", stringify!($name), "::foo(); ```"), pub fn foo() {} } doc_comment! { concat!("blabla Example: ``` ", stringify!($name), "::another_func(); ```"), pub fn another_func() {} } } } struct Foo; struct Bar; impl Foo { impl_struct! { Foo } } impl Bar { impl_struct! { Bar } } ``` Like that, you can use this macro on a few types (as long as they implement the same methods) and not have to rewrite the doc each time.
You could roll your own basic GUI code and render it with [nanovg](https://github.com/KevinKelley/nanovg-rs) (I highly recommend using the git version, it has been rewritten and has yet to be published to crates.io).
You're basically looking for what are called peerDependencies in npm - see https://lexi-lambda.github.io/blog/2016/08/24/understanding-the-npm-dependency-model and https://stackoverflow.com/questions/26737819/why-use-peer-dependencies-in-npm-for-plugins Not sure if Cargo has this built-in, however...
Rust will slap you in the face repeatedly with how inefficient your code is. **Do not worry about this.** Thing is, the difference between a borrow and a copy or (gasp!) incrementing a reference count is on the order of freaking nanoseconds in most cases, so *no one cares.* At least, you certainly shouldn't. If figuring out how to do something "right" is frustrating you, just do it "wrong" and move on. The world will not end.
Two additional notes: 1. actors are also extremely useful across process boundaries, and nothing about rust's memory safety is relevant there. The lead developer of actix is currently working on multi-process actors: https://github.com/actix/actix/issues/16 2. In some way the actor model is even _better_ with Rust, because you can safely send values with the `Send` trait across boundaries to other actors in the same process, without necessarily copying them. So you're getting the memory benefits of Rust and the architectural benefits of actors.
Yup, that's exactly what happens. In release mode, the compiler sees that the computation returns infinity, and deduces that the while loop only gets called once, since infinity &gt; prime_n either way. Without that optimization, `float.infinity as usize` returns 0, so this turns out to be an unsound optimization by LLVM, but one that is darn hard to fix.
Rust is very fit for using message passing style actor models, this is basically what `Send` (without `Sync`) expresses. What the actor model expresses is mainly ownership and processing of the messages. A message can _still_ contain shared access to something. (e.g. in Erlang, that's what happens with binaries)
Can't you do a String::with_capacity first, and then two push_str ?
&gt; In the loop though as I parse the records, could I have a random value and skip the current record if the random value is less than a threshold? How would that be done? let mut total_remaining : u64 = records.len(); let mut samples_remaining : u64 = ...; for record in records { let probability = (samples_remaining as f64) / (total_remaining as f64); if uniform_random() &lt; probability { sampled! samples -= 1; } remaining -= 1; } Of course this has a runtime of O(total_records) instead of O(sampled_records) you'd get with another approach.
What a *darn* shame.. *** ^^Darn ^^Counter: ^^451040
I still don’t know after a year playing with Rust here and there. Is there a well organized project you could point us to or some sort of best practices?
Well, after further inspection of the cert bundle file (`ca-bundle.crt`), it looks like it wasn't formatted properly (the command I used to tack on all the company's certs to the end failed to make sure there was a newline between each cert, so one cert ended on the same line that another started). So, for anyone else out there wondering how to get this working behind a corporate firewall, try all of the above steps!
I'm interested in porting a gtk-rs project of mine over to relm. Do you have any info about your rewrite? I'm trying to follow the state-of-the-art in the GUI rust library ecosystem.
I'm not exactly sure but I think what you're trying to do here is fundamentally flawed, because you're attempting to make a self-referential struct. Your owner struct is basically this: struct Owner { config: Config, config: &amp;Config } A self referential struct isn't allowed because moving it will invalidate the reference, and there is currently no way in rust to mark a type immovable. There's work being done to allow self-referential structs, but I don't really know much about it. In the meantime, you could use an Rc to store the config if you don't mind the overhead (this would make it more complicated for the owner to mutate it though), or rethink your design in some way.
why not `[a, b].concat()`?
&gt; Rust doesn't have the ecosystem go does and although you'll find plenty of libraries on crates.io, a lot of them are abandoned, unstable or experiments. I feel this is a bit unfair to Rust given that Go doesn't *have* semver for their libraries. Your best bet in Go is just locking to a commit hash and if you don't have that then you have no guarantees of stability.
Absolutely, and it may save an allocation.
It's worse than that. You can't borrow something (the local variable `config`) and then move it. The created reference then points to deinitialized memory. If you try to move it first (`let ret: Owner; ret.config = config; ret.borrower = Borrower::new(&amp;ret.config);`) that's better but you still can't move the return value out of the location `ret` while `ret.config` is borrowed. Those problems are more fundamental and easier to understand than the lifetime error. But the compiler talks about lifetimes first. The issue there is that any *borrow* of `ret.config` must inlive `ret`. But you can't put anything inside `ret` unless that thing outlives `ret`. Thus the borrow must have exactly the same lifetime as the Owner. (Outlive and inlive are like `&gt;=` and `&lt;=`) Borrowck has a hard time enforcing that. NLL and clever use of let might, but you'd still have the fundamental problems. `owning_ref` can solve this issue if you absolutely need to. But you don't need it, honest. Instead, I prefer to think this way: You may create *temporary* pointers to walk a data structure. But you can't put them inside that structure! (Not without a lot of trouble.) If you want to use pointers to build a structure, that's okay. But they must be *non-temporary* pointers such as `Box` or `Rc`. `Rc` or `Arc` would be the idiomatic solution, especially if Config is a shared structure (no `&amp;mut self` methods need be called to observe the configuration).
Use cargo if (for some odd reason) you aren't already. Use [cargo watch](https://github.com/passcod/cargo-watch) - it saves you having to manually recompile your code after every change and it will save years of your life. Don't forget to build with `--release` if you want a release build. This fairly frequently trips people up who are comparing performance.
&gt; ... or just trust you You can certainly trust me to make mistakes :-)
So you want rustc to compile arbitrary programs down to CUDA byte code? CUDA C does not include all of standard C. The CUDA spec wouldn’t allow it. Similarly standard Rust couldn’t compile down to CUDA bytecode either. So compiler support isn’t really feasible. I agree that writing kernels is a pain. I think the answer is a DSL however, rather than special compiler support. 
PSA: I'm doing part 2 of this on Saturday! https://twitter.com/Jonhoo/status/965633438958215168
Actor's are a high level abstraction that, among other things, provide safe sharing of state between multiple threads. Rust ensures safe sharing of state between multiple threads but that doesn't mean the actor abstraction is useless, since providing a safe way to share state between multiple threads is only one of it's purposes.
In Erlang, the binary will be copied on write though :)
.and_then takes a function that returns a future, and drives each of those futures to completion before proceeding to the next element. Then .buffered is taking those completed futures and buffering the `Ok` futures, which does nothing since they have no work to do ;) The indicator that something's wrong is the `Ok(Ok(..))` that you're using. The futures that .buffered sees are those inner `Ok`s. Instead you should return `Ok(future)` inside the and_then; this will cause your stream to be a stream of CpuPool futures, which is exactly what you want to pass to .buffered.
It helps to know that a module is a privacy (and thus unsafety) boundary.
You can set CARGO_TARGET_DIR end var, documented here: https://doc.rust-lang.org/cargo/reference/environment-variables.html
Is this nightly only? I can't seem to find it.
Which is being folded into the main Rust documentation: https://doc.rust-lang.org/nightly/rust-by-example/
I get there are differences in privacy rules, and multiple privacy rules are needed. The issue is book keeping. To have to do all this module booking keeping by hand is just silly.
I just wrote this as pseudo-code. But I'd assume the `rand` crate has an equivalent function.
I haven't experienced any such issues, but I also don't use a `vboxsf` mount point for my workspace. As the error messages indicate, `vboxsf` probably doesn't support things that are commonly expected of unix filesystems. Is there a reason you're using a VM instead of the Windows Subsystem for Linux's (WSL) Bash On Ubuntu On Windows? Or, only using your Linux VM instead of (probably) doing your code editing on Windows and then compiling and running on the VM? VirtualBox under Oracle's... *generous* ownership has basically been at a standstill for years now, and there are tons of issues with `vboxsf` and the virtual GPU drivers and other things that I wish would get fixed :(
Thanks! I didn't know about #1 and totally agree with #2
I've come across something I find weird, but maybe someone can clarify. I'm using the .nth() method on a Bytes struct, but rather than returning Option&lt;u8&gt; its returning Option&lt;&amp;u8&gt;. According to [this link](https://doc.rust-lang.org/1.12.1/std/iter/trait.Iterator.html#method.nth) it should return an Option&lt;Self::Item&gt;. Does anyone know why it would be returning an &amp;u8 rather than a straight u8. Wouldn't the return type be Option&lt;&amp;Self::Item&gt; instead? Any clarification would be immensely helpful.
I often recommend that complete beginners (especially those coming from dynamic, managed languages like I did) tinker first with just numeric (and enums and structs containing only numerics) to start to get a feel for the easier parts of the language before they move to stuff that doesn't implement Copy.
I don't know why I remembered that one, but not concat(). Yeah, concat is better.
&gt; Is there a reason you're using a heavy VM instead of the Windows Subsystem for Linux's (WSL) Bash On Ubuntu On Windows? Still running Windows 7... I swear I was planning to upgrade... at some point... &gt; Or, only using your Linux VM instead of (probably) doing your code editing on Windows and then compiling and running on the VM? Habit and convenience :)
Don't try to imitate familiar OO stuff when starting out, try to write C in Rust more than C# or C++ in Rust. People will tell you that your clever idea to use `Deref` to mimic inheritance is bad, they are right. Keep the ownership layout of your data structures tree-like as much as possible. Shared references are tricky, and don't even get me started on cyclical references.
Absolutely. Rust is fast enough that your inefficient code will perform reasonably well. Rust is structured and testable enough that you can optimize that code later without breaking everything.
Can you share your code? I think i'm [doing what you describe]( https://play.rust-lang.org/?gist=5d5ed35e684e576f57fe1db717aca1c9&amp;version=undefined) and getting an Option&lt;u8&gt;. 
I tweaked your code: https://gist.github.com/christophebiocca/8ef2a2bac2ffda00abf892c08429e9d4 Unfortunately it's not self-contained so I didn't get to test my changes, but here's a summary of what I did: - Swapped `and_then` for map. We don't want to wait for the future to be completed, so `and_then` is unnecessary. - Unwrapped one level of `Ok` since it doesn't do anything. Please ask if any of these changes don't make sense to you.
[removed]
Some refactorings could be implemented using the pattern matching provided by a tool like rerast, but many (most?) would need full access to the AST and probably wouldn't make any use of Rerast's pattern matching at all. These refactorings are probably better implemented in the RLS.
That's what I do to keep build artifacts out of my backups. I've been going around and filing PRs with projects which have `target/` in their `.gitignore` since a symlink isn't a directory and the ignore rule doesn't match. [Cargo now does the right thing](https://github.com/rust-lang/cargo/pull/4944) at least :) .
How to programmatically find out or set the filename of the test binary that `cargo test` generates?
Aren't "traits" basically type classes, hence implemented by passing around a dictionary of functions?
If you are bent on keeping your setup then try mounting the share over NFS (or maybe even SMB) instead of vboxsf; in theory it might be more compatible. Alternatively, you could setup some script to automatically sync your source folder (from vboxsf) to /tmp or something, and run your builds there. Not sure though if inotify and friends work on vboxsf either too well.
On a totally different level: that I'd invest (at least) four years in it and totally change my career path thanks to it. ;)
There's a [talk](https://www.youtube.com/watch?v=qr9GTTST_Dk) on someone using Rust with the actor model.
nope, this is a reference to me bashing my head against the language for an age, trying to work out how the hell you add 2 strings together.
Yeah, having to put everything into a module so you don't have to fight the language to do reloading is bad, having compile times for every line in the repl makes me sad. But damn, the language itself it a wonderful thing.
I'll try this, thanks!
Is this model similar to ECS?
Great writeup, thanks a lot. Rustup still has a lot of catching-up to do - I haven't been able to use it behind our setup (mitm / NTLM + cisco endpoint ) but we're getting there.
It's interesting to me that people who are perfectly comfortable doing something in C are uncomfortable using unsafe rust. OP even says: "there appears to be no way to say to Rust 'look, I know you are all about thread safety, but I’m building a single-threaded application and your obsession with thread safety is driving me to distraction. Just back off, I know what I’m doing.'" `unsafe` seems like exactly what he's looking for.
`ggez` is what I'd recommend.
Haha. Yeah, but I'll help. Here's what I know: if you want a self-referential struct, you've got to have a heap-allocated container involved somewhere. I suggest a handy `Box&lt;T&gt;`. There; that's pretty much all I know. :)
/u/gregwtmtno is talking about `std::str::Bytes`, but I'm wondering if you're referring to the `bytes` crate and calling `Bytes::iter().nth()`. Could you clarify? If so, [according to the docs](https://carllerche.github.io/bytes/bytes/struct.Bytes.html#method.iter) (you have to click on the return type to see), `iter()` returns a regular slice iterator, which gives out references to the slice's contents. The reasoning behind that design is probably a combination of: - It's convenient for `Bytes` to return the same iterator as `[u8]`, since it's designed to behave like that type in many ways. - Slices have to give out references to their contents, rather than copying things by value, because not every type implements `Copy` like `u8` does. That said, if you want the nth byte of a slice or a `Vec` or a `Bytes` object, the most natural way is to use `b[n]` notation.
For SQLite, a [prepared statement](https://sqlite.org/c3ref/stmt.html) is kind of like bytecode. SQLite parses the input string and gives you this "prepared statement" object. Then you bind parameters to it and have SQLite evaluate it. Since it's common that you run SQL statements repeatedly with only the parameters changing, this removes the cost of re-parsing the same (or similar) string over and over.
Oh, yes, you're correct; I misremembered and it's only the inverse `from_raw_parts` that exists (but that one is also required for correct deallocation). One has to extra each raw part separately: let v = Vec::with_capacity(n); let ptr = v.as_ptr(); let cap = v.capacity(); mem::forget(v); // ... do things ... drop(Vec::from_raw_parts(ptr, length, cap)); (Where `length` is the number of initialized elements to destroy, possibly 0 if one drops each one manually.) Searching the [`Vec`](https://doc.rust-lang.org/std/vec/struct.Vec.html) page for "from_raw_parts" highlights a few places that talk about/have examples.
/u/gregwtmtno is talking about `std::str::Bytes`, but I'm wondering if you're referring to the `bytes` crate and calling `Bytes::iter().nth()`. Could you clarify? If so, [according to the docs](https://carllerche.github.io/bytes/bytes/struct.Bytes.html#method.iter) (you have to click on the return type to see), `iter()` returns a regular slice iterator, which gives out references to the slice's contents. The reasoning behind that design is probably a combination of: - It's convenient for `Bytes` to return the same iterator as `[u8]`, since it's designed to behave like that type in many ways. - Slices have to give out references to their contents, rather than copying things by value, because not every type implements `Copy` like `u8` does. That said, if you want the nth byte of a slice or a `Vec` or a `Bytes` object, the most natural way is to [use `b\[n\]` notation](https://play.rust-lang.org/?gist=124e702eaba20cca2ec30a42b911e6d2&amp;version=stable). This works for everything that implements `Deref&lt;Target=[u8]&gt;` I think.
Rust is really addictive. Once you're hooked, you'll get withdrawal symptoms when forced to program in another language. 
You have it right with the parameter syntax and query planning optimizations, but with SQLite, a [prepared statement](https://sqlite.org/c3ref/stmt.html) is the *only* way to execute a SQL statement. (Yes, there's a [wrapper function](https://sqlite.org/c3ref/exec.html) but it still uses prepared statements under the hood.)
Yea I just left my house to go to a meeting and Im regretting not bringing my laptop loo
So you'd just use a [HashSet](https://doc.rust-lang.org/std/collections/struct.HashSet.html) instead, right?
&gt; Replace all the `static` with `mut const` in your head I guess I’d never thought about `static` in C like this before but it makes prefect sense 👍
Video linked by /u/H3g3m0n: Title|Channel|Published|Duration|Likes|Total Views :----------:|:----------:|:----------:|:----------:|:----------:|:----------: [Type-safe &amp; high-perf distributed actor systems with Rust](https://youtube.com/watch?v=qr9GTTST_Dk)|Anselm Eickhoff|2017-10-19|0:33:41|232+ (98%)|5,111 &gt; My technical talk about Citybound's engine at RustFest... --- [^Info](https://np.reddit.com/r/youtubot/wiki/index) ^| [^/u/H3g3m0n ^can ^delete](https://np.reddit.com/message/compose/?to=_youtubot_&amp;subject=delete\%20comment&amp;message=dukcjgn\%0A\%0AReason\%3A\%20\%2A\%2Aplease+help+us+improve\%2A\%2A) ^| ^v2.0.0
I for one really like the trait system. Just thinking about the diamond of death makes me happy that rust does not use classes. Traits are a lot more intuitive than classes too.
Rust things I miss in C: - Rust
You can overflow integers but you should do it by actively choosing the wrapping methods. 
I suggest keeping your files in the Linux VM's filesystem and sharing that folder with Windows via SMB, rather than relying on a shared folder from Windows being mounted in your VM. If you're primarily working with Linux command line tools in the VM, then this will result in a lot less weirdness with links, permissions, and so on.
I like it too. The issue I'm highlighting is more around aesthetics. 99% of the time you are making a struct with a matching impl. Keeping them separated, and then keeping the traits they implement separate, adds more friction for simple examples.
I've been using [stepancheg](https://github.com/stepancheg/rust-protobuf)'s toolchain with some success. The easiest way to deal with import errors (I find). Is when you generate the artifacts for `grpc` and `proto` from the source `.proto` files (I.E.: the `.jar`, `.go`, `.cpp`, `.rs`). You'll likely heavily use `sed` to modify the imports of the individual `.proto` files. The best advice I can give is to write proto generation in a make file, and use different directives for different artifacts. Copy the base `.proto` into your local `build/KIND`, then apply the custome `sed` changes.
[For what is worth](https://play.rust-lang.org/?gist=e2b98135c25bc4326dc68a112b219851&amp;version=nightly), the only operation that doesn't provide relevant help nowadays is `&amp;str + String`: ``` error[E0369]: binary operation `+` cannot be applied to type `&amp;str` --&gt; src/main.rs:2:13 | 2 | let _ = "" + ""; | ^^^^^^^ `+` can't be used to concatenate two `&amp;str` strings help: `to_owned()` can be used to create an owned `String` from a string reference. String concatenation appends the string on the right to the string on the left and may require reallocation. This requires ownership of the string on the left | 2 | let _ = "".to_owned() + ""; | ^^^^^^^^^^^^^ error[E0369]: binary operation `+` cannot be applied to type `&amp;str` --&gt; src/main.rs:3:13 | 3 | let _ = "" + "".to_owned(); | ^^^^^^^^^^^^^^^^^^ | = note: an implementation of `std::ops::Add` might be missing for `&amp;str` error[E0308]: mismatched types --&gt; src/main.rs:4:29 | 4 | let _ = "".to_owned() + "".to_owned(); | ^^^^^^^^^^^^^ | | | expected &amp;str, found struct `std::string::String` | help: consider borrowing here: `&amp;"".to_owned()` | = note: expected type `&amp;str` found type `std::string::String` ```
Rocket doesn't place any real restrictions on the form of authentication. You can easily implement it using a request guard. I've been happily building prototype APIs with Rocket for awhile and I think it's very good. Combine it with serde for reading and writing JSON and it's 👌
`HashSet` doesn't help at all here.
&gt; lifetime annotations are descriptive and not prescriptive. You can't change lifetime of a variable by adding lifetime annotations. &gt; For example, local variables of a function get destroyed on function exit no matter what and you can't prevent it by adding lifetime annotation to a local variable reference you are trying to return.
I'm trying to understand how to appease the borrow checker on this one. I have essentially a Vec&lt;Vec&lt;Tile&gt;&gt; where Tile is an enum. I'm trying to flatten the structure to search over all elements iteratively. When I do, I get the following error error[E0597]: `t` does not live long enough --&gt; /tmp/Answer.rs:92:51 | 92 | let start = tiles.iter().flat_map(|&amp;t| t.iter()) | - ^ `t` dropped here while still borrowed | | | borrow occurs here 93 | .filter(|&amp;&amp;a| a.0 == Tile::Start) 94 | .next().unwrap().1; | - borrowed value needs to live until here error[E0507]: cannot move out of borrowed content --&gt; /tmp/Answer.rs:92:40 | 92 | let start = tiles.iter().flat_map(|&amp;t| t.iter()) | ^- | || | |hint: to prevent move, use `ref t` or `ref mut t` | cannot move out of borrowed content I understand that it's telling me that my iterator gets dropped after the scope of flat_map. What's the idiomatic way to iterate over all elements of this structure?
It depends. I doubt I'll feel withdrawl when I'm in a "native-looking application for my KDE desktop" mood until a QWidget binding shows up that's mature enough for me to take that first hit and have a point of comparison relative to PyQt.
I think there is a bit of an understanding here. I don't have a problem with how they work type wise. Getting back to my JS example, this piece of code: // Dog as a prototype. function Dog() { } Dog.prototype = { bark: function() { console.log("woof!"); }, } ... is identical to this piece of code ... // Dog as a class. class Dog { constructor() { } bark() { console.log("woof!"); } } The difference is syntax sugar. Nothing more than that. Under the hood they identical. Yet the first was hated, and the latter is loved. Yet they are the same. That's my point. When you have lots of files with `struct` and `impl` blocks separated out, yet most of the time they are always together, it feels a bit like I'm writing the first version. So the main changes I'd make would be purely syntax ones. I'd let you put an `impl` block inside of a struct. As long as there is no generics or trait bounds, then I'd let you do it. I don't know what that syntax would look like with traits, or `impl` blocks with generics, but I'd aim to try to get something similar. However a lot of that matters more for library writers. I'm more concerned about application writers. They want code that just looks more straight forward. I'm not suggesting Java style classes. I'm suggesting syntax sugar that makes it look more mainstream. Under the hood there are no changes.
The main blocker is Rust's dependency on the system linker. They're working on it.
 [a, b].iter().collect::&lt;String&gt;() I thought you could maybe do this but unfortunately it's not possible because `String` does not implement `FromIterator&lt;&amp;&amp;str&gt;` :(
I don't think there is a particularly good solution. See [#1924](https://github.com/rust-lang/cargo/issues/1924) which includes a few workarounds such as `cargo test --no-run --message-format=json`.
This has really nothing to do with "is rust even suited for this" or not. If you can run the simplest form of an HTTP server you can achieve this. And i can assure you, this is really easy with rocket – i've already done that. Unfortunately i can't present the corresponding code due to the fact that its not open source (company project). But you don't need to go through the hole OAuth "struggle" if you don't really have this as a requirement. If you want to to just have a simple server where clients (via Android App or Website) can register them self (or having prepared accounts), Login and call some services to retrieve some data. This is really easy. make a register route that accepts a username and a password. Save the username and a hash of the password in the database. Do not save the password inside the database or every where else, drop it in the login function after you have hashed it. Don't use a hash function like md5 or SHA* use something like bcrypt, scrypt or argon2(argon2rs crate) (generating a random salt for every user and save it inside the database as well) make a login route that accepts a username and a password. search your database for the username and check the passwordhash and salt against the delivered password. After that generate a random auth token (with eg. OsRng not a UUID) and send it back to the clients. Save this token in the database alongside the user (you can go on and make the auth token only last a specific amount of time or not :) ) make your data routes and and send the auth token with it. either with url params or post alongside your normal request data or – what i prefer – in the HTTP header (like x-auth-token) and implement a `FromRequest` for something like a `User` struct that acts like a `request guard`. There you extract the auth token from the header, search the user in the database by that token and return a `request::Outcome&lt;User, ()&gt;` that you can guard every route with that needs a "login" #[get("/sensitive_data")] fn sensitive_data(user: User) -&gt; &amp;'static str { ... } done. https://rocket.rs/guide/requests/#request-guards http://bryant.github.io/argon2rs/argon2rs/fn.argon2i_simple.html please consider some time to read into password hash handling topics
btw what is this about? I noticed it recently https://github.com/iron/iron/commit/9e5bccb407b1ec8daef51b0d7494627e5c5fc307
God I’m trying to learn this now! But so help me trying to interpolate an internet with the string :X
In Rust, you have to do that as well, unless you have proven unaliased write access.
Apologies to both you and /u/gregwtmtno. I mixed up one issue I was having with another. .nth() wasn't returning an Option&lt;&amp;u8&gt;. The .get() method on a [u8;100] was. I may just need more practice with Slices, but that behavior doesn't make sense to me. Maybe I'm misunderstanding something about how slices work? Also, as an aside, I meant (std::io::Bytes)[https://doc.rust-lang.org/std/io/struct.Bytes.html] in my original post.
Oh, right, I wasn't thinking it through. You still want a mapping of strings to prepared statements.
Not quite answering your question, but have you seen my (currently unmaintained) [`unicode_names`](https://github.com/huonw/unicode_names) project? It's heavily inspired by Python's `unicodedata` and uses perfect-hash-functions to be able to map from a name to a character as well as character to name. There's a fairly involved trie-like compression scheme to get the size of the generated tables down to about 500KB, about 375KB for `char`-&gt;name and 125KB for the extra stuff required for name-&gt;`char`. It includes things indexing into one long suffix-optimized string, rather than storing individual string slices. I think a lot of the terminology/context was from `unicodedata`'s source and that's very definitely the original. (This format is all static data that does require slightly more expensive decoding in the form of some additional indexing and integer operations, there's no allocations or anything. And, at a high-level, it's basically doing the same thing of representing the name as a series of string slices to stitch together.) For comparison, even just the first level of having 31K `(char, &amp;[...])` tuples is ~750KB (on 64-bit), plus an extra 16 bytes for each `&amp;str`, of which there appears to be ~120k, meaning 2.7MB just for the various arrays and slices, plus a 100KB or so for the actual strings. In fact, the high overhead of storing individual `&amp;str`s, each of which is relatively short, may mean `(char, &amp;str)` is actually smaller: the average length of each segment is just 8 bytes, meaning the `&amp;str` value pointing to them are, on average, double the size (there's only 58 of the `&amp;str`s that are longer than 16 bytes, and these only appear in 254 of the entries). This suggests an alternative method to making the file compile faster that will be likely both smaller and faster is to make the format much simpler: `(char, &amp;str)`.
As I think I said elsewhere, we went with the simple option to start with; once we get the whole UCD mapped we can start in on more targeted optimization. I'm pretty sure that I did in fact look at the trie in unicode_names at some point, because I vaguely recall not being able to grok the exact way that it worked, or how much it would/would not apply to a purely single direction approach. There's a post elsewhere in this thread about improving the base `CharDataTable` I want to play with first, but after that and the rest of the UCD legwork I want to play with optimizing this table. I don't think the base table (ignoring actual string data) can get any smaller than 31k `(char, &amp;str)` and maintain subliniar lookup times though...
Thanks for the post, I'm new to Rust and mostly interested on embedded systems development, did you already had experience with rust on non-embedded projects?
Awesome, thanks, that does clear things up.
&gt; Don't use a hash function like md5 or SHA* use something like bcrypt, scrypt or argon2(argon2rs crate) (generating a random salt for every user and save it inside the database as well) I haven't tried it out, but [libpasta](https://crates.io/crates/libpasta) looks quite nice for doing this correctly, easily.
[removed]
&gt; As I think I said elsewhere, we went with the simple option to start with But not the *simplest* option (`(char, &amp;str)`), which I think should be an improvement. :) &gt; I'm pretty sure that I did in fact look at the trie in unicode_names at some point, because I vaguely recall not being able to grok the exact way that it worked, or how much it would/would not apply to a purely single direction approach. The `char`-&gt;name is purely `generated.rs` (generated by [main.rs L377-L379](https://github.com/huonw/unicode_names/blob/1acd7f150e210892b83b4b6864f7c4daa3414bd0/generator/src/main.rs#L377-L379)), and name-&gt;`char` lookup is purely in `generated_phf.rs` (generated by [main.rs L361-L372](https://github.com/huonw/unicode_names/blob/1acd7f150e210892b83b4b6864f7c4daa3414bd0/generator/src/main.rs#L361-L372)). Just doing `char`-&gt;name only needs the first. But yes, it's kinda complicated (I had to sit down and re-read everything to work out what's going). I believe this is approximately what's happening: At the lowest level, there's a `&amp;str` containing all the constitutant parts of the names (`LEXICON` e.g. "LATINSMALLLETTERCAPITAL..."). This is constructed by basically breaking up every name at spaces and `-`s (`-` is handled as it's own piece, basically), putting them all into a set, and then printing them out as one long concatenated string. (There's an extra trick of reusing suffixes, e.g. if `ATIN` was a component of some name, then it wouldn't be included separately, but instead just point to the `A` in `LATIN`.) There is also a table with the index inside `LEXICON` of the first byte of each word (`LEXICON_OFFSETS`). I believe this includes the most common words as the first `PHRASEBOOK_SHORT` elements, and then the rest are included grouped by length (i.e. all the pieces with length 1 are consecutive, as are all the pieces with length 2, etc.). Now to know what each piece is, we just need to know their lengths. `LEXICON_SHORT_LENGTHS` is a table of the length of each of the common words (i.e. there's `PHRASEBOOK_SHORT` elements, and `LEXICON_SHORT_LENGTHS[0]` is the length of the word starting at `LEXICON_OFFSETS[0]`). `LEXICON_ORDERED_LENGTHS` is an array of the boundary `LEXICON_OFFSETS` indices between the groups of lengths, along with the length of that group, meaning if you're looking for the word at index `n`, search in `LEXICON_ORDERED_LENGTHS` to find which group contains that index, and that will give you the length. Together, these define each piece in a name. Each name is encoded in `PHRASEBOOK` as a sequence of encoded indices into `LEXICON_OFFSETS`, inspired by variable-length encodings like UTF-8. It's a sequence of `u8`s where the low 7 bits are data, and the high bit indicates whether this is the last piece of the name. Once that high-bit has been masked off, look at the value of the `u8`: if it's less than `PHRASEBOOK_SHORT`, then the index is exactly that byte value. If it's not less, then the index can be computed from the byte value along with the next byte in `PHRASEBOOK`. So, decoding a name involves going to its start, and walking along one or two bytes at a time to get each piece, stopping after decoding the byte with the high bit set to `1`. Now, all that's needed is to go from `char` to the start index in `PHRASEBOOK` of the name. The way this is done is by compressing a hypothetical table from `char` to start index (where 0 means "no name"). Group this table into consecutive blocks of 2^(7) (128), and label the blocks from 0 up to 2^(14); for a `char`, its index within a block is its lowest 7 bits, and the label of the block it's in is the high 14 bits. There will be a lot of blocks that are all 0, since there's so many unassigned codepoints. So, instead of storing the whole huge table, we can just instead store one copy of each unique block in a single table (`PHRASEBOOK_OFFSETS2`), along with another table that maps the high 14 bits (the block label) to where that block starts (`PHRASEBOOK_OFFSETS1`). So, to compute the name: find the start index of the name's encoded piece `PHRASEBOOK` via `PHRASEBOOK_OFFSETS...`, and then walk over those bytes to get the start index/length in `LEXICON`, and return a subslice of that long string. &gt; I don't think the base table (ignoring actual string data) can get any smaller than 31k (char, &amp;str) and maintain subliniar lookup times though... I think `unicode_names` is constant time with respect to the number of `char`s: https://github.com/huonw/unicode_names/blob/1acd7f150e210892b83b4b6864f7c4daa3414bd0/src/lib.rs#L235-L240 is enough to work out which sequence of strings are in the `char`s name.
I’ll comment on the issue.
You probably want a different target dir per project, though. (I personally exclude `projects/**/target/` from my backups.)
I posted a working example of how to do what he wants with unsafe code at the end of that thread. With one more tweak it could be improved so that any invalid uses of it panic at runtime. And that code allows you to get at the static cache object from anywhere in the code, even without any pointer/reference to application state. Sure, its a bummer that Rust doesn't seem to support a global static safely, but I wouldn't consider this to be a showstopper right now. 
[Fixed in nightly](https://github.com/rust-lang/rust/pull/47804).
&gt; Rust's trait and implementation system reminds me of what happened with JS's prototypes. An overly clever system, with lots of flexibility, but people just wanted to write class Foo in foo.js and be done with it. I honestly hope Rust never goes in that direction. As someone who uses Vim in place of an IDE, I'd seriously consider writing a preprocessor to bring back my preferred environment of more than one class/struct per source file.
I would tell someone: don't expect to learn it fast. I've heard it said that you can learn Go in a week. You can't learn Rust in a week, not in depth anyway. And learning it requires grit, especially in the beginning. Also: its super helpful to actually read rust code, not just struggle to write it. rustbyexample.com is good, but its really useful to just read code that is written by the Rust Masters: https://github.com/rust-lang-nursery/futures-rs is a good example.
The problem is that you aren't doing anything with the `connection` variable. Because of this, the compiler doesn't have the full amount of information to figure out the type of `connection`. Specifically, you need this line: https://github.com/tokio-rs/tokio/blob/master/examples/hello_world.rs#L46 Hope this helps.
Ahh. I wouldn't mind that. Most of the time, having separate `struct` and base `impl` blocks does feel like visual bloat.
This post title sounds like a bunch of buzzwords mashed together, and I can't quite figure out what this thing is from its website. Sounds sort of like a meshnet? Can anyone explain?
Nice; thanks.
Be the change that you wish to see (on /r/rustcirclejerk).
Thank you very much, this is exactly the sort of advice I was hoping for. 
A library for space efficient and probabilistic data structures! That includes: * Bloom Filter * Count-Min sketch * KMV sketch * HyperLogLog * etc. I am really liking Rust so far, though the types feel like some seriously heavy machinery coming from someone working in Clojure and C++ the last few years. It's not daunting but it _is_ complex.
Thanks! Agreed it's certainly making my brain hurt learning some of the new concepts but its fun!
It's not very clear to me what you mean, could you expand? Are you thinking of something like https://crates.io/, which is an already-existing repository of cargo packages?
What differences would you say are there between pip and cargo? ---------- For what it's worth, I don't think pip gives any security over using GitHub. In fact it is in my opinion less secure. Any one can upload to pip and it's not as transparent as directly seeing the source on a site. And cargo already has a central package listing for crates. There's not much difference between pip and cargo imho other than pip also handling system level packages including stuff like command line tools. 
Interesting that various cryptocurrencies have chosen Rust, see also Parity, Maidsafe, Exonum. Maybe we'll get a few cool libraries out of it before the whole thing collapses in on itself.
&gt; I might be misinterpreting it's function Um yes
god damn, that's exactly what I was meaning. Is there any meaningful way to download crates from here without `wget`?
I guess *this* is the thing I wish I was told before !
Yes, cargo does that https://doc.rust-lang.org/cargo/guide/dependencies.html
[removed]
Just for future conversations something like pypi and crates.io are often called "registries". When you say "download crates" what exactly are you hoping for? I ask because crates.io only stores source so you can use wget/curl/etc or something like a custom cargo command (https://github.com/JanLikar/cargo-clone comes to mind). If you want to use a crate as a dependency then you need to add it to the `[dependencies]` section of your "manifest" (which is your `Cargo.toml`). [The Cargo book](https://doc.rust-lang.org/cargo/) is rich in explanation.
Is there something wrong with the documentation that you don't know about cargo and dependencies despite having actually written rust? I mean IMHO it's pretty clear if you've actually read the docs.
I love Python, but trusting pip [is a bit of a mistake](https://news.ycombinator.com/item?id=15256121).
Another option is to use a [`.cargo/config` file](https://doc.rust-lang.org/cargo/reference/config.html) in the project directory to specify a per-project target dir. Works great with Dropbox. Something like [build] target-dir = "/tmp/project_name"
The problem with using an actor model framework in Rust is process overhead and preemptive scheduling. To be viable it requires near zero-cost processes. You need to create and throw away processes like they are structs and a scheduler to ensure all the actor processes get serviced fairly or the whole model breaks down. The context switching overhead of running 10,000 native threads each representing a single chunk of application state ends badly. As primarily an Elixir/Erlang programmer and a Rust programmer second the actor model (at least in terms of how Erlang does it) doesn’t suit Rust because they accomplish the same goals in different ways. Rust provides reliability and safety by strict typing and strict scoping rules to guarantee safety. Erlang does it through immutability and process isolation and not giving a crap if your process crashes because they are cheap as chips. Two me they are two different ways to solve the same problem, uptight and detailed or ¯\_(ツ)_/¯. Same result. I’d like stronger type checking and compile-time enforcement in Elixir, maybe 2.0, but that’s a system built to do actors extremely well. I use Rust to build high performance actors which run on the Erlang VM. That is a nice balance for me. Adding a runtime to Rust to preemptively schedule lightweight processes isn’t a zero-cost abstraction. 
You dropped this \ *** ^^To ^^prevent ^^any ^^more ^^lost ^^limbs ^^throughout ^^Reddit, ^^correctly ^^escape ^^the ^^arms ^^and ^^shoulders ^^by ^^typing ^^the ^^shrug ^^as ^^`¯\\\_(ツ)_/¯`
You're not the first.
If you need raw pointers, it indicates possible xy problem.
I haven't actually written rust. I'm just highly interested in it. I'm very busy this semester, so I figured learning a new language was best suited for the summer. I really only glanced through the docs, and the cargo stuff was confusing to me outside of using it to build and run a test, so I just gave up on procrastinating and went back to my programming languages homework
I don't even always need them. Portions of my current project use them purely because it's easier to be consistent instead of mixing &amp;T, &amp;mut T, *const T, and *mut T all over the place. My point though is that every other rust user tries to diagnose a possible XY problem each on their own, and the totality of it becomes grating. For a language that's supposed to allow for low level code, there's an unfortunately frequent attitude that actually doing low level code is something you shouldn't try.
You don't need 10000 threads to run 10000 actor. Nor you need EVM to do that. You can use a threadpool of 16 threads to "execute" those actors. Just be careful not to block. Or solve it with coroutines. Actors model is a concurrency model, not a pattern or a workaround for thread safety or data races. It encourages or even implies those safeties. Futures are another concurrency model. You can create thousands of futures, they will be executed with a threadpool, N at a time. I think actor model fits rust and keep up the good work, Nikolay! :)
I learned something from that! Thanks!
I'm using cntlm, rustup and cargo, and it works without flaw. What issues are you seeing?
&gt; Exactly, what you want is a DSL that has multiple backends (CPU, OpenCL, CUDA, Vulkan, etc). Rustc already supports targeting different accelerator targets. Why is a DSL better than full support for each accelerator target? 
Oh yes. There's a discussion on /r/programming now about OOP without implementation inheritance, and that's what we have with Rust. The old habits die hard. Can get a surprising amount of code reuse from default trait methods, e.g. how we get so much functionality from a simple iterator implementation. 
If you are using raw pointers when you don't need them, why use Rust instead of C? Rust isn't "you shouldn't try" but "safety first" and "safe abstractions"...
Because even if you have absolutely no special rust safety it's _still_ a hundred times better to program with cargo+rust than it is to program with anything in C? And, as i said, not all projects are all pointers, but one project is some required pointers and then the rest is just kept as pointers for consistency across the code base.
Pipenv supports full manifests.
Even C++?
How do you compile kernels in Rust? I’m not being cheeky or anything, im genuinely interested. 
just use the nvptx target
To me, personally, cargo test + doctests... everyone else can go home until they at least have that much going on. Like there's a lot of nice small things, traits, for-each style loops, all that, but doc tests and documentation in general, rust is way ahead of most of the rest of the world.
Very interesting article! Embedded and direct framebuffer programming are two big interests of mine. I kinda miss the days when you could just write to specific locations in memory, in order to blit a bitmap to the screen. Keep posting more!
Working on a rust implementation of an a distributed graph data storage system based on [this]( http://ieeexplore.ieee.org/document/7363954/) paper.
&gt; However it does seem to be lacking a certain speed. The fastest way I've found to emulate 'df -h' with it takes ~5seconds at close to 100% CPU on my machine... I'm not familiar with that crate, but can you share the code?
I'm happy that you solved your problem, because this whole post was caught in the spam filter, which I've only just gotten around to cleaning out. :P Sorry!
You have the to type "refresh_disks", that's the part which takes a while, updating the information regarding each disk. Getting the info itself is quick, but the iofo may be incorrect if there hasn't been a refresh beforehand.
On my computer, `disks` returns the same info before and after `refresh_disks`, and both finish immediately. What OS are you on?
It does mention a Rust-written project by name (Cargo), which is enough to technically satisfy our on-topic rule so I won't mod-remove it. The connection is indeed a bit tenuous, however.
So.. issue solved? :) It would be nice if the crate also returned the mounted partitions, where applicable.
Hi! Exonum is a private blockchain without mining so we don't need CPU resources from unsuspecting internet users. Why we need smart contracts on WASM is for being more flexible - we can write the code one time and it will run on any device &amp; browser - so that's the reason. 
Both doxygen and rustdoc use Markdown so it you extract the comments and prepend each line with `///` it should work
I'd still have a lot of things like `@defgroup`, `@note`, `@param[in]` and so on.
Uh, I would really recommend not to use `vboxfs`. (I've been using a Vagrant dev setup since Vagrant 0.3, so I have a little experience) _Especially_ file metadata and events are weird or missing with vboxfs. Use NFS or SMB. Still, that might not improve things. Incremental compilation also means that more artifacts are written and reread. Depending on the speed of the file system, that _may_ be more costly then just compiling everything. It's a little unclear to me where rustc lives and works and where the source is: you share the source to the machine and rustc runs in the guest? 
I'm not too savvy with these internals, but my best guess is that Rust tries to stack-allocate a temporary before constructing the structure in the third example? Unfortunately I wasn't able to replicate the crash in the third case. Here's my attempt which compiles and runs correctly right now: https://play.rust-lang.org/?gist=375791c116944c20a5869ea314025764&amp;version=nightly If we can make a reproducible test case for the crash in the third example, it should be possible to examine the assembly and possibly report it as a bug in the way Rust handles this syntax. It *should* work to use the placement-in syntax as your post uses it, but there could definitely be bugs if we can isolate them. -- As for how it works, I know more about the `&lt;-` syntax than the `in` syntax, but I think they're roughly equivalent? This trait defines behavior and provides a desuraged example for `&lt;-`, I assume it's fairly similar for `in`: https://doc.rust-lang.org/std/ops/trait.Placer.html. The place it's "in" seems to first allocate an uninitialized area of the appropriate size and return a pointer to it. Then rust writes the data it was going to write to the stack to that pointer location instead. --- As for constructors, they aren't really special in rust. I mean there's the single syntax for creating a structure, `XXX { field_name: value, ... }`, and the convention is to create a method `new` which will do the appropriate things to create the structure. I'm not too familiar with C++'s ctors so someone else might be able to speak to that a bit more.
It's a good write-up which demonstrates wrong approach common for newcomers, but the final solution is essentially shared state hidden behind keeping array index, which can be a footgun if variables order is modified and outdated `Sum` is used. I would've written something like this instead: use std::collections::HashMap; type VariableIndex = HashMap&lt;String, i32&gt;; enum Sum { Sum(Box&lt;Sum&gt;, Box&lt;Sum&gt;), Constant(i32), Variable(String), } impl Sum { pub fn eval(&amp;self, variables: &amp;VariableIndex) -&gt; i32 { match self { &amp;Sum::Sum(ref l, ref r) =&gt; l.eval(variables) + r.eval(variables), &amp;Sum::Constant(c) =&gt; c, // plus optional detection and error for variable not found in the index &amp;Sum::Variable(ref v) =&gt; variables[v], } } } 
`System::new()` is supposed to be called once and never again. Also, if you want a good example of `sysinfo` usage, you should take a look at [sysinfo-web](https://github.com/onur/sysinfo-web) (they use it on docs.rs for example). I'm happy to see that my crate is getting attention from people. :)
I can confirm that it is much, much better than doing `pip install`!
When I let rustup install to its default location (in my user folder), whenever I tried running either cargo or rustc, the process froze. I could not kill cargo, and could not delete/move the files. I had to reboot my computer. I think it has to do with installing to a folder that's aliased to a network drive. Everything works better now that I moved it to C:\rust, but I couldn't find a way to do that in the installer and had to move it manually.
So in the end you would still need `impl` blocks for generics, so this would just create two ways of writing the same thing: non-generic functions could either be declared within the struct or within a `impl` block. This seems tidious to me. Also why act like Rust is am object-oriented language? The only thing they do similarly is how you call functions using the dot. On the other hand Rust has no inheritence. Traits can't inherit other traits and structs can't inherit other structs. Sure you can tell the compiler "I need these traits on this type to be able to implement the current trait" but that's not inheritance. Also who complains about the `impl` blocks? I find them way nicer to use than what you are proposing anyway. If you use an IDE/editor that offers folding (and which ones don't?) you can just fold away all the irrelevant `impl` blocks and be left with the ones you currently need. With object-oriented languages it's a huge clusterfuck of variables and functions. To me rust seems way tidier. Your comment was the first time I saw anyone complain about it to be honest. So the questions it really boils down to for me are: * Why would you want to make a language that is object-oriented look like a object-oriented language in this one place? * Why would you destroy a code structre that is perfect for folding editors just to use one that likes to get super messy for structs/classes with lots of functions? Edit: I just reread this comment and noticed that it probably sounds a bit harsh, which was not intended. If you would like to have functions declared like that, that's your opinion and i respect that. I just think differently and wanted to express that (and nothing else). 
Hey, thanks for the quick reponse. I also encountered the case that this works on rust playground and not with the nightly compiler I'm using (working under 64-bit windows with the msvc toolchain : nighlty-x86_64-pc-windows-msvc). I assume that rust.playgound runs on a linux server and therefore uses another target triplet that could have -ulimit set to unlimited or similar - so the stack is not limited to a small amount of MB which is the reason for the crash right now.
This library seems very useful, thank you for the tip!
But that has nothing to do with MITM or NTLM. Have you filed an issue for that, it's definitely worthwhile to look at.
How would you approach user authentication in Actix?
&gt;Race conditions: indeterminate execution ordering I am afraid it's an unfortunate (and potentially dangerous) [misinterpretation](https://doc.rust-lang.org/beta/nomicon/races.html) of Rust guarantees which ideally should be corrected.
&gt; Also why act like Rust is am object-oriented language? A flat class with no inheritance and no other OO tricks is how a tonne of people think and organise their code. That's how I organise a lot of my code in Rust. Just allowing this would nip my entire point in a nutshell: struct Point { x : u32, y : u32, pub fn new(x : u32, y : u32) -&gt; Self { Self { x : x, y : y, } } } No new syntax. No new changes needed. But it's visually a little neater. That's all I'm suggesting.
I am not denying that, but his is a thread about NTLM and MITM. As I wrote above, I'm working in a similar environment. Also, as with all these environments: we sadly don't have a corporate environment to test around ;), so we need people in these setups to solve or at least diagnose those issues.
Hi, if you are writing a masters thesis, it definitely wouldn't harm to write to community@rust-lang.org and we can find someone who can competently answer your questions.
&gt; cargo new defaults to bin I feel like this should be highlighted more. This is a pretty significant change.
That works for me – at least I'm positive de can support it until RFC [#2318](https://github.com/Manishearth/rfcs/blob/post-build-contexts/text/0000-custom-test-frameworks.md) hits. Thank you!
It's not too misinterpreted if you always use safe functions and love the borrow checker.
That is new syntax though. With the current way and folding you get something like this: struct Point&lt;T&gt; { x:T, y:T, } +impl&lt;T&gt; Point&lt;T&gt; where T:Add + Sub{...} +impl&lt;T&gt; Point&lt;T&gt; where T:AddAssing + SubAssign{...} +impl&lt;T&gt; fmt::Debug for Point&lt;T&gt; where T:fmt::Debug{...} So you have a struct followed by folded `impl` blocks. It's super readable. It's easy to find the impl block you are looking for and in there you have the functionality you were looking for. And it's consistent. With your way you could put some `impl`'s into the struct but no others. So with your proposal there would still be a bunch of `impl`'s below but now you also have functionality in your struct. One more place to go looking for the code you want to find. Another point I didn't mention so far, is that structs that declare functionality aren't even structs anymore. With the way it is right now, you declare structs and you declare functions that can be used on those strucs. If what you propose was implemented every rust beginners book would have to say "although it says struct in the code, these are actually more like classes (but they aren't really classes either)". `struct` would become a misleading keyword. I'm sorry to be so blunt, but to me your proposal makes a big mess out of code, that has been very tidy so far, by implementing multiple ways of writing the same thing. Also the name `struct` would become super unintuitive since it actually doesn't mean `struct` anymore and is much closer to `pseudoclass`. So in the end you get something that is called `struct` but is neither an actual struct nor an actual class. and to what end? Just to make the code structure more similar to languages that use a different paradigm? Big classes in those languages have been a mess all along. Why would you wanna do the same thing in Rust? To me having state and functionality in two different, well defined places is a big strength of rust. It makes for very tidy, well organized files. Changing that just to make it similar to what people know from other languages seems like a mistake to me.
Do you have a working example?
Placement in has known flaws and [doesn't actually work](https://github.com/rust-lang/rust/issues/27779#issuecomment-358025344) in cases you would expect it to. Unfortunately this is somewhat due to the design not providing these guarantees. I have a [PR open](https://github.com/rust-lang/rust/pull/48333) that proposes removing placement entirely to allow for a new design. Unfortunately you may well need to create your own placement API to make your example work. One crucial observation is that C++ constructors effectively take `&amp;mut self` receivers (i.e. have the memory to hand before entering the constructor), whereas Rust returns the types directly - how do ask Rust to return a type into a location? I have a few thoughts, but I'd also be interested if you come up with anything!
thank you, that looks really nice
No, with safe Rust you can easily get indeterminate execution order or even deadlocks, see the link.
I think the point here is more that Rust does nothing to save you from for example deadlocks, which stem from indeterminate execution ordering, too. Only data races are covered.
The post talks about cargos (and only cargos) semantics, with a seperate page with details here: https://research.swtch.com/cargo-newest.html
Wow, definitely.
Yes.
Rust uses LLVM on the backend. LLVM's implementation of dead code elimination is [here](http://llvm.org/doxygen/DCE_8cpp_source.html).
"minimal version resolution mode" did come out in a couple of discussions. Specifically, there's an interesting potential problem with today's situation: You may write `foo = "1.0.0"` in your Cargo.toml. Cargo would fetch the latest semver compatible version, say, `1.2.0`. Now, you may *accidentally* start using features introduced in `1.2.0` and not available in `1.0.0`. That is, `1.0.0` in your Cargo.toml might not be quite true, because you need at least `1.2.0`, and don't actually know about it! However, this does not seems to be a problem in practice, precisely because Cargo tends to pick the latest available versions. That is, some Cargo.toml in the wild might be invalid, but this does not cause failed builds. 
sorry for the late reply, I just come home after my vacation. I think the most for me is the development speed. Rust has many cool features to guarantee data safety which can help us avoid making mistakes and release TiKV quickly. The most thing I want is a built-in Pprof tool like Go has to debug the online system easily. 
How do I find the type of a parameter in a closure? To be concrete, I was modifying this example websocket client from ws-rs: https://github.com/housleyjk/ws-rs/blob/master/examples/client.rs I wanted to convert `msg` to json. I _thought_ it was a string, but I was wrong. (I figured out it was a ws::Message). My question is, how can I use the language to easily figure that out? I tried `msg.what_am_i()` to get an error, but the error wasn't helpful: ``` error[E0619]: the type of this value must be known in this context --&gt; src/main.rs:31:13 | 31 | msg.what_am_i(); | ^^^ ``` Help?
Same as asmx85 suggested. There is nothing special about auth in actix. 
something like this https://play.rust-lang.org/?gist=6d1bdb235e39b917ec695f6ae39e4f75&amp;version=stable
Yeah, I think the OPs example usage is a bit contrived, like adding a dependency on 1.0.1 when 1.0.5 is available. I think that most often you want the latest available version when you add a dependency. Personally I always add new dependencies with "cargo add" from "cargo-edit" crate, which does that. As long as there is no new version between "cargo add" and "cargo build", which is typically a few seconds, I'm safe.
Perfect! Thank you!
As a follow up question. I don't think it matters, but is dead code removed on non-release non-optimised builds?
If you're curious to see how your program would be without dead code elimination, run: cargo rustc --release -- -C link-dead-code
I love the fact that there's three significant command-line programs out there that are just better than the common standards. I expect it says a little about Rust, and a lot about the awesome people using it. They left out `xsv` though! :-P
&gt; Ternimal has no dependencies apart from the Rust Standard Library, and does not require Cargo for building. But I want to `cargo install ternimal` :(
It's actually quite hard to write safe unsafe rust code. There are loads of corner cases and things you need to keep in mind. Rust is a pretty complex language and if you remove the compiler safety it really sucks. Safe C code is easier than safe unsafe rust code.
[From the PR](https://github.com/rust-lang/cargo/pull/5029), &gt; We should be very proactive about announcing this change. Not only do our docs need to change, we also need to inform other people who may be teaching Rust of the change to the default behavior, since they'll likely be walking people through this as a first step.
I don't know of any, but maybe you can post about any specific problems you need help with? There's also a relatively helpful community forming on https://gitter.im/actix/actix which I recommend checking out. edit: I would love to, eventually, try out graphql for the web API that I'm implementing, but it's on the back burner. I am using actix-web, though, so this is relevant to my interests.
That's a great tip - I just sent a mail. Thanks!
Yeah, I already followed the RFCs and also discovered your PR to remove the placement as it is now (which I can support from my current situation as it seems that the feature is somehow confusing and lacks some more discusion there and where) For me it would not be a problem to implement my own placement API and it also would work quite weill for my thesis to describe the differences bteween C++ and Rust - and creating types into a place seems to be a big difference. My next steps are to investigate a little bit further into the generated assembly of the current placement-in feature and also on how Rust 'constructs' objects at all and what would be valid and legal workaround (e.g. allocating memory that fullfills alignment/size requirements of a type and then transmuting/casting it to a pointer to the type smells somewhat fishy and I believe that this would not be legal)
Sad but true.
Is the reasoning for this somewhere other than in IRC? &gt; as discussed on IRC
halfway through it is [enigo](https://crates.io/crates/enigo) for simulating mouse and keyboard. for the visual part i am currently not aware of an easy solution but i would start the route by looking for OpenCV stuff.
Of course. But I think it's working fine already. It seemed to me it was returning disks, not partitions, which would have been weird. But it does return partitions on local disks, I think.
This is actually the job of the linker. LLVM will emit a section for each function and static and then the linker will strip any unused functions or statics. This also means that some platforms are better than others. For example the `pc-windows-msvc` targets are significantly better than all other targets due to `link.exe` doing Identical Code Folding to merge together identical sections.
Dead code elimination by the linker is generally toned down by rustc during non-optimized builds. For example rustc specifically asks `link.exe` to not do ICF when doing a non-optimized build.
Yeah! I've been using exa basically since the day it came out. Being able to do `cargo uninstall exa &amp;&amp; sudo dnf install exa` was extremely satisfying.
i think I could live with it if I called `read()` explicitly in order to get something that can be `Deref`ed directly into `MyStruct`, which would negate the need for having `ARef` need an implementation for everyone who has a `read()` or `lock()`, which means it's agnostic to antidote or not.
rustc runs in the Ubuntu VM indeed. I did not know that vboxfs was so weird :x
am a quite noob, but isn't this basically very similar to the heartbleed bug
&gt; If you are bent on keeping your setup then try mounting the share over NFS (or maybe even SMB) instead of vboxsf; in theory it might be more compatible. It's served me well for the last few years; but I think it's time to move on. Plenty of ideas on this friend, now I have to choose!
This would indeed be better, but I don't think the relationship `&amp;'a self -&gt; NewType&lt;'a&gt;` is possible to express in safe Rust today. I believe this is one of the problems that [GAT](https://github.com/rust-lang/rust/issues/44265) intend to solve, so in a year or two the situation might be better...
For anyone unfamiliar with VGA signals/timing, here's a good description: [VGA Video](http://web.mit.edu/6.111/www/s2004/NEWKIT/vga.shtml)
It's complicated but it should just return disks, not partitions. If it returns partitions, I think we can consider it's a bug.
[removed]
It is not clear, what is `data: *mut ::std::os::raw::c_void`, is it way how to pass context via callback, or this data that provide C library to your code? In case of this is context, then I just provide rust API via normal `Fn()`, like `fn register_callback&lt;F: Fn()&gt;(cb: F)`. Because of if `C API` provide abitlity to convert it to normal rust API, why not use normal rust idiom?
Wait, is Maidsafe a cryptocurrency now? I thought it was a decentralized Internet... ELI5 :x ?
How do I create tests for a function that uses read_line()? I have come across the assert_cli crate but I don't think it is what I am looking for. Unless I just don't understand it properly. An example of what I want to be able to test: pub fn select_mode() -&gt; Result&lt;Mode, Box&lt;Error&gt;&gt; { println!("Select mode: (a)dd (r)emove (c)omplete (q)uit"); let mut input = String::new(); io::stdin().read_line(&amp;mut input)?; Ok(match input.trim() { "a" =&gt; Mode::Add, "r" =&gt; Mode::Remove, "c" =&gt; Mode::Complete, "q" =&gt; Mode::Quit, _ =&gt; Mode::Invalid, }) } 
Not on the Cargo team, but I have some historical context. Historically, when deciding what the default was, we figured that libraries are created more than binaries. This is because many binaries have more than one library dependency, certainly on average more than binaries with no dependencies at all. So, make the usual thing the default. However, that's the wrong way to frame it: after all, many binaries depend on the same library. You only create a library once, but you create a binary that depends on it many times. You can justify either choice, honestly. But people's opinions have been leaning toward "library by default was a mistake" for a long time now.
Thanks Steve!
This is the first TWiR that I felt like I could contribute to something in the call for participation: &gt; [good first issue] errno: Port library to winapi 0.3. ...and the task was pretty straightforward. The [PR](https://github.com/lfairy/rust-errno/pull/15)'s already done! I'm excited to see what comes down the line. :) Thanks for putting this together, /u/llogiq and /u/nasa42!
You've missed MimbleWimble. Also there's Parity-bitcoin, a full btc node in Rust. Not to mention some existing currencies showing interest in using Rust.
What happens if some library crates want to abort and others want to unwind? 
/u/nasa42 does the calls for participation, so I have little say in there, but awesome you could contribute.
This is extremely cool, and I think highlights Rust's strengths well.
It's not in the current stable Fedora 27, but that is actually packaged for Fedora 28+ already!
*Disclaimer: the tone of the OP is fairly aggressive in the conclusion, which I deplore, I encourage readers to consider the technical point which was raised though.* --- From the conclusion: &gt; The problem is immediately obvious: rustc destructures the vectors before they're ever even passed into the function! Why, I cannot possibly imagine. This code is from the version using structs, yet uses tuple access syntax in the LLVM, which suggests to me that Rust is already able to optimise structs into tuples, which is good. For what it does next, however, there is no excuse. [...] So, apparently, because rustc decompose the struct into its constituents, the following Rust code: pub struct Vec2 { pub x: f32, pub y: f32 } pub fn add (a: Vec2, b: Vec2) -&gt; Vec2 { Vec2 { x: a.x + b.x, y: a.y + b.y } } Generates the following IR: define { float, float } @add(float %a.0, float %a.1, float %b.0, float %b.1) { %0 = fadd float %a.0, %b.0 %1 = fadd float %a.1, %b.1 %2 = insertvalue { float, float } undef, float %0, 0 %3 = insertvalue { float, float } %2, float %1, 1 ret { float, float } %3 } Instead of the equivalent (from Clang): define &lt;2 x float&gt; @_Z3add4vec2S_(&lt;2 x float&gt;, &lt;2 x float&gt;) { %3 = fadd fast &lt;2 x float&gt; %1, %0 ret &lt;2 x float&gt; %3 } And this results in a final assembly with more instructions: add: addss %xmm2, %xmm0 addss %xmm3, %xmm1 retq instead of: add(vec2, vec2): addps %xmm1, %xmm0 retq *Ignoring stack-frame setup with `pushq`/`movq`/`popq`.*
You can still publish it on crates.io.
Thanks for your appreciation. Means a lot!
&gt; Use `RefCell`, a runtime-checked borrow system (which still only works on nightly if you want to mutate) This is incorrect. [`RefCell::borrow_mut`](https://doc.rust-lang.org/std/cell/struct.RefCell.html#method.borrow_mut) has been stable since Rust 1.0. Safe shared mutation is the entire reason for RefCell to exist; it would be completely pointless without a way to mutate the value in the cell.
Obligatory reference: "[Learning Rust With Entirely Too Many Linked Lists](http://cglab.ca/~abeinges/blah/too-many-lists/book/README.html)"
And [now there's a PR to address the only unhandled case](https://github.com/rust-lang/rust/pull/48392): error[E0369]: binary operation `+` cannot be applied to type `&amp;str` --&gt; $DIR/issue-39018.rs:21:13 | 21 | let x = "Hello " + "World!".to_owned(); | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ `+` can't be used to concatenate a `&amp;str` with a `String` help: `to_owned()` can be used to create an owned `String` from a string reference. String concatenation appends the string on the right to the string on the left and may require reallocation. This requires ownership of the string on the left | 21 | let x = "Hello ".to_owned() + "World!".to_owned(); | ^^^^^^^^^^^^^^^^^^^ help: you also need to borrow the `String` on the right to get a `&amp;str` | 21 | let x = "Hello " + &amp;"World!".to_owned(); | ^^^^^^^^^^^^^^^^^^^^
In addition to what Steve said, *new users* are more likely to want create a binary (they usually want to run the program they're experimenting with). Advanced users understand cargo's interface &amp; can just remember to add `--lib`, or delete `main.rs` if they forget. New users wanting a binary have a stumbling block where they have to learn about cargo conventions before they can write code. Since `cargo new` is often the first or second thing a user does when they want to learn Rust, its very important to be a good experience for that new user.
I think this might be related to https://github.com/rust-lang/rust/issues/44367 I.e., Rust disables passing values as SIMD vectors deliberately because of unresolved issues with different ABI's when different CPU features are enabled for different compilation units. The thinking is that with SIMD usually you'll be inlining as much as possible, so the overhead is somewhat acceptable for now. 
I kinda want try out Fedora now. Coming from Ubuntu. 
Tried with arrays instead of tuples, https://godbolt.org/g/7HjTpi I think it is basically the same, but I don't read asm well. Adding `-C target-cpu=native` to the `-O` dose give me `vaddss` Is that good?
The change is definitely welcome and having to specify that I want to write an executable instead of a library is indeed weird. It makes libs more "first-class citizens" than executable programs even though libs have no reason to exist on their own, only to help write programs.
I don't think so? matthieum example is seemingly relying on autovectorization. The issue you linked is with explicit vectors in the type system and their ABI in Rust function signatures. I could definitely be wrong that they don't impact each other though. But that change just landed, so it should be easy to test by trying different Rust version.
Nope, that's a scalar add. `vaddps` is where it's at.
A quick test from 1.20 onwards shows the same assembly being generated every single time. It's not a recent change.
Well tried!
Yea, I was put off at first because I'm used to the current behavior but I can see how this will be a lot nicer for newcomers, so I think it's great
Maybe a silly question: when the author calls a variant "crap", they mean "empirically less efficient", right? 
My thinking was more along the lines of "I vaguely remember Rust doesn't ever pass vectorized arguments, and it's because of this". I'm definitely not up to speed on the issues since I haven't needed SIMD myself at all. 
I look at this through the lens of the Liskov substitution principle: can i use *this thing in my left hand* in any situation where i would be able to use *that thing in my right hand*? If so, this thing is a subtype of that thing. It's a pretty loose definition, but it works, i think.
The thing which frustrates me is that "favor composition over inheritance" is one of the items in Effective Java, which was published in 2001, just five years into the life of Java, and *fifteen years ago*, and people (including many Java programmers!) still think inheritance is fundamental to Java programming and/or OO. Indeed, it's a strong strain in Design Patterns, published in 1994! 
Traits are basically typeclasses. But most of the time, they don't exist at runtime at all - because they're usually used as bounds in parametric polymorphism, which the compiler resolves entirely at compile-time. They can exist at runtime in the form of "trait objects" (i am being a bit handwavy here). These don't involve a dictionary of functions, at least not exactly - they have vtables somewhat like objects in C++ etc. There's an [old blog post](https://doc.rust-lang.org/std/raw/struct.TraitObject.html) about it, and the docs for [std::raw::TraitObject](https://doc.rust-lang.org/std/raw/struct.TraitObject.html) confirm that it's still vtable-based. A difference from the typical C++ implementation, AIUI, is that Rust has fat pointers - the thing you pass around is a pair containing a pointer to the trait object's data, and a pointer to its vtable. In C++, you pass around a thin pointer to the data, and the first field in the data is a pointer to the vtable. So, Rust has bigger pointers, but fewer indirections to use them. 
I don't think that it is "library level" desicion. Inside `impl Trait for X { fn y()` user of library can decide use `catch_unwind` for example to call `log!`. 
That's pretty much exactly what I did as well, with minor differences since I'm on Fedora. Thanks though.
Is that the reason why the linux build of servo is so much bigger than the windows build?
There's not much info or details about how it works. My feelings about this is a bit mixed. On one hand it looks like great and interesting proof of concept. This kind of things moves the science forward, inspires people, moves the limits a bit further. I once did a swap-in-userspace by manipulating the memory mapping with mmap, mprotect and hooking into the SEGFAULT. I consider this similar exercise. On the other hand, monkey-patching the runtime sounds a bit scary ‒ I'd be vary of putting this into production. It's the kind of thing I'd react with „What could *possibly* go wrong here?“. So, my question here is, is this meant as an exercise, or as a „solution“ for real-life use? Also, the note about TLS at the end hints at the coroutines moving between OS threads. It's not *just* TLS that's problem here. The problem is much deeper ‒ if something is `!Send` in Rust, it can be for whatever reason (maybe it uses the thread ID for something and relies on it not changing over time. Or maybe it calls `sigprocmask`, which configures threads in the current thread so they don't arrive at the wrong time… and then it moves to a thread that doesn't have it set up) and I don't believe it can be solved without big changes to the language. Things like this happen: https://www.weave.works/blog/linux-namespaces-and-go-don-t-mix. However, Rust type system is supposed to prevent these kinds of bugs… except that moving a coroutine between threads just cheats the type system. The crutch everyone is leaning against just disappeared. If a stack contains something that says it doesn't want to move between threads by being `!Send` and the stack moves, bad things are going to happen, sooner or later.
I don't think we're talking about the same thing. Any Rust code which uses a Rust library can decide whether to catch an unwinding panic. If the panic unwinds out of `main` the program ends. I think that's what you're talking about. But we're talking about Rust code that plugs in to a C library. (Or some other language: Fortran, Python, whatever.) That library will be compiled by a completely different compiler. C doesn't have anything like unwinding. (There's longjmp, but it doesn't drop local variables.) So the machine code won't expect it and *anything* could happen. 
I am immensely excited for this, I planned on spending time this weekend trying to get parcel and stdweb working, and this is probably gonna save me a bunch of work. Thank you so much!
Yes. The strongest comparison in his article is the "add" function where clang is able to do it in 1 instruction, rust does it in 5 instruction and gcc does it in 20 instruction (all excluding "ret"). He is upset about rust's method because it deliberately breaks the optimization that clang is able to do. He thinks gcc is crap because it is so bad compared to llvm (in this case).
Well... https://www.phoronix.com/scan.php?page=article&amp;item=gcc-clang-eoy2017 It should be quite recent then.
It was a change in rls protocol in nightly that broke things. Commit in builder that fixed it: https://gitlab.gnome.org/GNOME/gnome-builder/commit/81e6e070fb81bdd0960aacdb5ac359349fbb6d7d Thanks for your help!
They mention that several popular crates are packaged. Does that mean that one can build rust code versus fedora's version of the crates instead of using the versions on crates.io? How does that work and how does it interoperate with cargo? 
This is amazing, keep working on this Rust is becoming *the* language to use for WASM.
Silly follow-up question: less instructions means faster? I would be glad to have some numbers to see the runtime difference.
It's a little old, so you might not be able to use it directly, but I wrote a blog post this: http://aatch.github.io/blog/2015/01/17/unboxed-closures-and-ffi-callbacks/
Did you have experience with 3D graphics beforehand? I've been looking for a good resource to learn from. P.S. - You should make it graph hydrogen wave functions, those are pretty neat to look at 
Original author here. When I say "crap", I mean "a long and nearly incomprehensible string of assembly instructions to do a simple task". However, that does, in turn, mean empirically less efficient.
We use source replacement to build packages against the local crates, with a `.cargo/config` like this: [source.local-registry] directory = "/usr/share/cargo/registry" [source.crates-io] registry = "https://crates.io" replace-with = "local-registry" So if you make a config like that, you could run `dnf install 'crate(rand)'` (which actually installs `rust-rand-devel`) and start using Fedora's `rand` right now. However, this is all or nothing -- you'll only have access to the local crates this way. See [cargo#3066](https://github.com/rust-lang/cargo/issues/3066) for the possibility of mixing local and remote crates someday.
I understand. I agree that this is not desireable and should be corrected. How much less efficient? Did you make a run time comparison for a specific test case?
Even in the pure safe fragment (without interior mutability), that has not strictly speaking been proven. It's probably true, though.
Did you try gcc with -march=native -mtune=native?
Looks like manually cloning the repo and installing ion by hand solved the problem :) I used wrong package (https://aur.archlinux.org/packages/ion/) instead of ion-git
Hi there, A while back I [announce](https://www.reddit.com/r/rust/comments/7a3b9y/analyzing_the_public_data_of_the_cern_base_alice/) that I was able to analyze the ALICE public data with Rust but I had to use the ROOT c++ framework for reading the binary data format in which the data was released. Back then, I already wanted to write a parser in pure Rust, but was aware that it will be a big task. However, at last I finished that parser (with `nom`) and while being at it, gave the API for iterating over the data an overhaul. Of course, such an announcement would be incomplete without some sort of bench mark. As detailed in the readme, I ran the pure-rust and the c++-backed version against each other on an identical set of files. The Rust version seems to be 30% faster with a warm file catch and up to 70% faster on a cold one! I also released a bunch of the crates of this project to [cargo](https://crates.io/keywords/cern). This is my first Rust project and I would like to say a big thanks again to all the great people who helped out a lot on IRC and elsewhere! I do realize that this is a lot of code and certainly not the kind of thing most people will use. But if somebody would like to give me feedback on (parts of) the code I would very much appreciate it! In particular, I wonder if there might be some macro-magic to reduce the boiler plate in [this case](https://docs.rs/root-io/0.1.1/root_io/tree_reader/struct.ColumnFixedIntoIter.html). A little background: The data at hand contains millions of particle collisions. Each one is called an "Event". For each event, one might have hundreds of things that were measured. Usually we only care about a small subset. Currently, this subset requires the user to: - Define an "Event" struct holding these observables (called "Model" in the linked code) - Define an `IntoIterator` holding an iterator over each of these observables - Implement `new()` for said `IntoIterator` - Implement the `Iterator` trait for said `IntoIterator` All in all a lot of boilerplate. Does this look like something which could be simplified with macros to you?
[No need for `-mtune` if you use `-march`.](https://stackoverflow.com/questions/10559275/gcc-how-is-march-different-from-mtune)
When I want to use Rust from Java and C++ I also consider such way. But writing `String -&gt; char *`, `chrono::DatTime -&gt; double` and all other conversation too boring, that's why I start project to create analog of swig for Rust language, whithout intermediate.
My guess is that the call to `libc::read` doesn't update the vector's `len`. You can unsafely set it yourself with the `set_len` method if you're sure that it's sound to do so.
This is pretty cool. One thing I do take issue with is that they seem to claim Rust is the reason they can function without an internet connection. Surely that's a design thing rather than a language thing. 
Wow this is something I had no idea about. Thanks a lot!
Supporting bindings to web apis outside of `stdweb` is a fantastic feature that allows a more progressive approach to moving toward Rust on the front end.
I wonder if this has something to do with the way rustc does scalar replacement of aggregates and/or passes immediates in registers. Paging /u/eddyb, who, unlike me, actually knows what the words that I just said mean. Either way, it's worth filing a performance bug against rustc.
Honestly, timing number when comparing how fast one instruction runs versus 2 won't give you anything useful. For example if you have two SIMD ALUs the rust solution and the clang solution would barely be different in terms of cycles.
I *think* that's this compiler bug: https://github.com/rust-lang/rust/issues/46034
After trying to implement it's saying that io::stdin does not implement the trait BufRead. Is something I should implement myself?
Just the flow of using Rust with Parcel is mind-bogglingly simple: 1. Change the extension you're importing from; basically `s/\.js/\.rs` 2. Expose Rust functions with `#[no_mangle]`. That's it. HOW AWESOME IS THAT?
I think that Rust might have made the design for a single device *feasible*, but I also think you're right -- it's technically a design difference. :) To support my point here, take a look at the section *Performance and Portability*. They talk about the constraints that the Snips project operated under and why Rust fit their use case.
On mobile and can't check, but maybe you need `stdin().lock()`?
To answer the question as to why you saw no difference between the struct version and the tuple version, it's because tuples are literally implemented as just anonymous structs (consider that they even have anonymous field access via `foo.0`, `foo.1`), which makes sense to my mind but I'm curious if you had some reason in mind why tuples ought to have more optimization potential. Also, as to "rustc destructures the vectors before they're ever even passed into the function! Why, I cannot possibly imagine.", I'm pretty sure this is due to a compiler optimization called scalar replacement of aggregates (SROA). What I'm curious about is that there's no way that Clang isn't *also* applying this (crucial!) optimization, so I wonder what they're doing that's smarter in order to recognize autovectorization opportunities.
Well it's also because binaries linked using any other linker have dwarf debuginfo embedded into the binary, unlike msvc where the debuginfo is in a separate `.pdb`. There's also the matter of `libbacktrace` and `jemalloc` being statically linked into Rust programs on all nix platforms.
O’Reilly customer service replied: “If you'd like them added to an oreilly.com account, create an account by going to members.oreilly.com and forward the receipt to us here and we can add those books to your oreilly.com account.” Then you can get ebook updates in future. 
&gt; Rust preserves some stack-frame-related boilerplate that the C compilers elide when given -Ofast, which is unlikely to change given Rust’s emphasis on safety. I think that this is actually a problem with Compiler Explorer. It compiles with debug symbols (in order to correlate assembly with code for that colorful output), which disables some optimisations. If you do the same thing in Rust's own [playground](https://play.rust-lang.org/?gist=df66ff4e88896181891b58a0520a00c4&amp;version=stable) (click release then ASM), you get a much better output: playground::add: addss %xmm2, %xmm0 addss %xmm3, %xmm1 retq playground::scale: mulss %xmm0, %xmm1 mulss %xmm0, %xmm2 movaps %xmm1, %xmm0 movaps %xmm2, %xmm1 retq playground::dot: mulss %xmm2, %xmm0 mulss %xmm3, %xmm1 addss %xmm1, %xmm0 retq
You are my savior! Thank you so much! Good luck with learning the language too.
doing read_mode(io::stdin().lock()) gives the error that the borrowed value does not live long enough at that line. I think I have to go over the rust book again to get what's going on here.
What I would like to see is zero overhead (or at least constant overhead) access to JS types. E.g. I do not want UCS-2 JS strings to be implicitly converted to UTF-8 Rust strings. I had the same issue with some Ruby binding library (ruru I think). I personally prefer to use a library where this kind of costly type conversion is always explicit, but would be fine with it as long as I have the ability to skip the type conversions and work with the raw data.
&gt; I want Well, have one then: https://photos.app.goo.gl/9Olx9vjuunipPWp53 
The `vrender` project that it depends on was pretty much my first experience with graphics programming. I had tried a couple years before with opengl, so I understood the basic concept of graphics programming but this is the first time I've ever gotten anything on the screen. I started off by reading the vulkano [guide](http://vulkano.rs/guide/introduction). When the guide started straying off the path I wanted to go, I started looking through the stunningly well-documented [triangle example](https://github.com/vulkano-rs/vulkano-examples/blob/master/triangle/main.rs). After I was able to render my vertex buffers, I started looking through parts of [learnopengl.com](https://learnopengl.com) to learn how to do higher level and more abstract things (like perspective, cameras, matrix transformations). I had to look at the vulkano [teapot example](https://github.com/vulkano-rs/vulkano/blob/master/examples/src/bin/teapot.rs) to figure out how to do depth testing. Basically, I read the guides to learn how stuff worked/why I typed what I did, and I used the examples (and some api docs) to learn what to type. As for the hydrogen wave functions, I looked them up quickly and my only reaction is ^what?^?^?
One case is when a C function call takes a function pointer as a callback (the data pointer is passed to the callback): void takes_callback(void (*callback)(void*), void *data) {} To call this correctly from Rust, you need a callback function with the correct ABI, but its name in the binary doesn't matter: extern { fn takes_callback(cb: unsafe extern fn(*const u8), data: *const u8); } extern fn callback(data: *const u8) {} takes_callback(callback, data); Addendum: a good example is [calling `pthread_create` in the *nix implementation of `std::thread::spawn`](https://github.com/rust-lang/rust/blob/master/src/libstd/sys/unix/thread.rs#L78-L92).
It's in the sweet spot of modern, no GC and safe
Issue #44367 was fixed (albeit probably temporarily) in https://github.com/rust-lang/rust/pull/47743. I tried adding `#[repr(simd)]` to the struct definition and it does apply the solution in the PR (passing the argument as a pointer), but that doesn't improve the output in terms of vectorisation: &lt;https://godbolt.org/g/d5DA9s&gt;.
I wish someone had told me how insufferable a part of the community was. There's also a good chunk of very helpful people, but they are entirely overshadowed by the most vocal majority. It pretty much turned me away from the language.
&gt; // Does not _use_ head, just mutates it &gt; head.next = Some(Box::new(next)); This is pretty misleading. Mutating something *is* using it. As soon as you wrote `Box::new(head)` a couple of lines up, that `Node` was quite literally *moved* into the box- this isn't just a conceptual move that only exists at compile time, but an actual change in memory location from (in this case) the stack to the heap. The `head` variable is now considered uninitialized. Which leads to this quote: &gt; `next` and `prev` recursively refer to Nodes, which means we can’t stack allocate our linked list. I'm not totally sure how much of this is just a terminology nit and how much is tied up in the confusion of the previous quote, but you *can* stack allocate (the nodes of) a linked list. What you can't do, and what your footnote accurately describes, is define a recursive type without some form of indirection, whether that be `Box&lt;Node&gt;` (forces heap allocation) or `&amp;Node` (allows stack allocation!) or something else. &gt; There are 3 solutions I’m aware of: The main tool for option 1 is probably `Rc` rather than `RefCell`. You can use plain `Cell` to handle the next/prev pointers, and the node contents as well in some cases. `RefCell` is only necessary if you want to take `&amp;mut` references to the nodes or their contents. There is also a fourth option, written up recently in this excellent blog post: https://exyr.org/2018/rust-arenas-vs-dropck/ This approach maintains compile-time borrow checking, allows mutation, and uses pointers instead of indices. It is basically your option 3 (`Vec` with indices), but using fixed-sized chunks to grow the backing storage rather than a dynamic array that can move its contents in memory.
Yes. It uses AVX instead of SSE but still does not vectorise properly, and still includes all of the "crap".
I would suggest seeing if the nix crate meets your needs, as it seeks to create higher-level wrappers around libc APIs. If you're specifically working with serial ports, I would point you to another crate I maintain, serialport-rs. Both are on crates.io (the latter under serialport).
It looks like your question already got answered, but just so you know, there's no reason you can't ask questions like this here. :)
Hey, a few things from the post: - There is an Add trait that allows operator overloading: https://doc.rust-lang.org/std/ops/trait.Add.html - Cargo provides several ways of expressing level of optimization. From command line, --release will give you level 3. You can specify more detail in the Cargo.toml configuration file, including "lto = true" for a compilation profile profile, for long compile times but best performance. You can also pass specific compiler flags through cargo with the RUSTFLAGS env var ("-C target-cpu=native" often improves performance. - I expect tuples will be very fast in a few years but as of right now there are occasional performance gotchas (in my experience, nothing egregious) which I've been told is because they're new and llvm isn't very optimized for them. 
I know about `Add`, I'm just disappointed that `std` doesn't implement it on tuples of numbers. That may be harder to do than I realise, though.
Nice post! Since hashing doesn't require UTF-8 input you can use `&amp;[u8]` for those parameters, which avoids needing to validate the string. You can also use `CStr` to avoid copying/allocating the input string. Then the only overhead left is the call to strlen, which is probably not significant. Many languages do know their string lengths so you could just take the string length as a parameter to the slight annoyance of C programmers, but it doesn't matter very much.
Another obligatory reference for OP: [Building on an unsafe foundation — Jason Orendorff](https://youtu.be/rTo2u13lVcQ). Forget linked lists -- a standard [Vector](https://doc.rust-lang.org/src/alloc/vec.rs.html#973) has plenty of unsafe code, and it should (agruably) be simpler to implement. Unsafe Rust is just a stepping stone to warn the developer -- use responsibly, only where you actually need it. To OP's credit -- he did say he is starting to dabble in Rust and may be going unsafe is a bit too early. But, at the same time he seems to have background in C -- that should have prepared him for this.