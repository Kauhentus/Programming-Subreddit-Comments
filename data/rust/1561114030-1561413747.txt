There’s no RFC’d plan for how to make async fn in traits object-safe yet, that post was just looking at the issue and one option. Another potential option would be to support some form of constrained-size stack-allocated trait objects, which is sort of what `RawWaker` is a manual implementation of where the data is constrained to be pointer sized.
Check that your lifetime declarations are correct.
It's not as trendy/fun.
I believe Go compiles to every achitecture that GCC supports, which is considerably more.
Regarding the stdlib, I used ti express that a lot of things don't need to be installed, unlike other languages. I can't say that I am sharing this experience. I haven't encountered anything that used python2 and I've been using python as main language for the path 3 years.
Looks like the AVR fork is pretty close to being ready to merge into upstream Rust: https://github.com/rust-lang/rust/issues/44052
Isn't that what iterators are?
The borrow checker can be \_very\_ assertive.
I agree! But then when I recall all the programming learning I've done in life, I think most of the nice patterns I picked up from other languages was **also** from reading other people's code. Makes me remember reading tips from 80's era programmers who said reading others' code was invaluable. Don't remember who it was exactly, but they said they would print out code on A4 paper and take it out with them to read.
Maybe you / your coworkers are better at Python than my coworkers. I always have issues getting a Python script set up. With Rust it's 99% of the time `cargo run --release` and we're done. I've never had a time with Rust where a crate I'm using needed another crate so I had to look at a stack traces and install that other crate for it (I had to do this just yesterday at work with Python).
[http://docs.idris-lang.org/en/latest/reference/uniqueness-types.html](http://docs.idris-lang.org/en/latest/reference/uniqueness-types.html)
Nah I am just a single person doing research so the above is very very unlikely. I guess it's my luck. Cargo is pretty awesome though and tbh `setup.py`, `pip` and whatever other methods of creating packages can create flustercucks.
I actually don't think that example is true. This example would be, however: var2 = 10 / var1; if (var1 != 0) //this may be removed return var2; return var1;
&gt; Beta One might claim that Rust is still in Beta, they just decided to call a rather unready thing 1.0.
Check out http://nim-lang.org -- it's a much more old/stable language and gives similar (although with a GC) performance guarantees that Rust does.
Iterators can always be dropped and unused, though. True linear types would mean having a type which is invalid at compile time to drop, and _must_ be used in some way.
Would be cool to have a tool that converts any GitHub repository to a well organized Epub. :-)
That seems hard to enforce
It depends on the operating system. Therefore, if you are interested in Windows executables please watch [https://www.youtube.com/watch?v=MNwPhIeDAmA](https://www.youtube.com/watch?v=MNwPhIeDAmA) If you are interested in Linux executables then please watch [https://www.youtube.com/watch?v=OBDuoqyZ4UA](https://www.youtube.com/watch?v=OBDuoqyZ4UA)
You probably need to update your Cargo.toml to edition 2008
I don't know about that. Currently, the compiler will automatically insert drop() calls for values which are not consumed (fall off the end of a scope), this would trigger a compilation error instead. There are things for which linear values would be more proper than affine e.g. closing a file can generate a slew of errors which currently get silenced / lost, because drop is an implicit operation which has no way to report errors.
Indeed. In fact that's a common compiler pattern with respect to null checks, especially after a few rounds of inlining.
Do you mean in a different way than the Result type?
Go's runtime includes things like a non-optional garbage collector. That doesn't really work on a microcontroller.
So, not functional. Functional languages primarily emphasize functional programming. Just because Rust borrows (pun not intended) a lot from functional languages doesn't mean that it is one itself.
Thanks, I restricted my lifetimes to three Mississippis and it's working correctly now.
Once you call it 1.0 you generally have to maintain backwards compatibility. Before rust changed stuff all the time.
Update: he finally left, but because someone shot a rocket at him... Thank you random stranger
I didn't catch the real-time latency, even though I copy-pasted it. So he probably discounted Go immediately for possible GC pauses. Even though Python also has a GC?
I'm going to nitpick: this is **not** typestate. This is **session types**. Typestates are a type-system features that allow you to **change** the type of the object. Here, you do not change the type of anything, you return a new thing with a different type. With a typestate system, you could make it so that after `r.status_line(200, "OK")`, `r` itself is of type `HttpResponse&lt;Headers&gt;`. In the example in the blog post, `r` is still of type `HttpResponse&lt;Start&gt;` after the method call, but the call returns an object with the new type (that happens to be the same thing, but the type system doesn't care). A summary of the pattern shown here is: - Encode the usage pattern (or "protocol") of something - Use a type-level automaton in a phantom type - Use a functional workflow where each function returns a new thing where the type is updated - Forbid uses of the old version In the literature, this is called "session types". It's usually used for protocols, but as you describe here, it can be used for many things. It's indeed very useful. You can emulate it in lot's of languages (with or without various guarantees). Session types are a very functional idea, where you chain functions together to implement your workload, and hide the mutations. Typestates are about lifting mutations to the type level, to mutate the type themselves as you go along.
Apparently they're from someone's Brainfuck interpreter benchmarks https://github.com/kostya/benchmarks/blob/master/README.md#benchb
There's an annotation for `should_use` that is on stuff like Result. I think it's possible for normal code to use it as well.
Agreed! I doubt rust will be getting true linear types anytime soon, if ever.
Nim is still unstable. They haven't made a 1.0 release yet. It would also seem that they plan on stabilizing without first class support for ADTs, which doesn't inspire much confidence in its future.
Just If you haven't figured it out by now, you are in the wrong sub. r/rust is for the Rust programming language,. You were looking for r/playrust
Riscv64 support is already in stable, actually: https://github.com/rust-lang/rust/blob/master/RELEASES.md#compiler-1 And here's the AVR project: https://github.com/avr-rust/rust You're right that for some architectures, there's no support from the Rust team (yet), but since GCC must be recompiled for cross-compilation to a specific target anyway, the situation actually isn't that different. Here are instructions for the Xtensia esp8266: http://quickhack.net/nom/blog/2019-05-14-build-rust-environment-for-esp32.html
OK, I had gotten into the ballpark, but had the type info backwards. I'm still not _understanding_ all of that, but once I have the chance to play with that a bit, understanding will come. Thanks!
&gt; I hope one day I can add a constraint to a type or value like "this value must be used/consumed". I think this is another feature of linear types? Yeah, that's _the_ feature of linear types. However, Rust implements affine types (values must be used not more than once). For ergonomic reasons, [true linear types can be a pain to program with](https://gankro.github.io/blah/linear-rust/#definitions-and-the-state-of-rust). In lieu of true linear types, you can the [`#[must_use]` attribute](https://doc.rust-lang.org/reference/attributes/diagnostics.html#the-must_use-attribute) which enables a best-effort compiler lint.
The annotation is called [`#[must_use]`](https://doc.rust-lang.org/reference/attributes/diagnostics.html#the-must_use-attribute).
 fn many_headers(r: HttpResponse, headers: Vec&lt;Header&gt;) { let mut r = r.status_line(200, "OK"); for h in headers { // Having to do this is kind of annoying: r = r.header(h.key, h.value); // Fortunately, if you forget the `r =` part, // the compile will fail. } r.body("hello!") } How can this compile if `status_line()` and `header()` return different types? I think that to make it work with `header` taking `self` by value, it would be even more annoying to write than this. You'd have to process the first header ahead of the loop.
This isn't specifically Rust related, but here it is. Can anyone suggest what I need to study to get started with writing lower level software like emulators and hardware drivers? I'm coming from a background where I'm OK at Rust and C programming, and am currently in 2nd year of a CompSci degree.
Probably a set of individually reasonable decisions with no one orchestrating the whole process. If a high profile company invited you to write a guest article, you might reasonably title it "why we chose rust" (especially if your company hasn't cleared the use of their name in such an article). If you were asking someone to write a guest article for you, you might reasonably put it on your blog, just with a note at the top that it's a guest article.
Excellent post! Really clarified how all those things work :)
Thanks, I was only clicking the right one, I was thinking both were the same.
&gt; Rust currently only has "zero or one use" types, not "exactly one use". Except for `Result`. Isn't that exactly what `must_use` is for?
Are you moving toward Elm from other frameworks or mainly just starting new applications with Elm? Only asking because I would be very curious about comparisons or perspective on the switch. I'm considering moving toward similar as a standard stack, Elm FE and Rust BE.
Isn't that exactly what `must_use` is?
&gt; Unless you’re Cloudflare, rust is often overkill Rust instead of Go could easily be about tooling and language constructs/abstractions, not just performance.
You're a goldmine.
I think the [link](https://gankro.github.io/blah/linear-rust/#definitions-and-the-state-of-rust) provided by u/jswrenn above best explains the difference between #\[must\_use\] and what I wanted. But #\[must\_use\] is basically 90% of the way there!
I think the [link](https://gankro.github.io/blah/linear-rust/#definitions-and-the-state-of-rust) provided by u/jswrenn above best explains the difference between #\[must\_use\] and what I wanted. But #\[must\_use\] is basically 90% of the way there!
We started moving from React to Elm for most projects a few years ago. We've got about 50K lines in production now. The ecosystem is great, the programs are far less buggy and extremely easy to refactor. I've done a few thousand line refactors in the past few months with no bug reports after.
I don't think you would need it here. The use-case is specifically to use enum variants as values, not types.
This could only work if you have no data tied to your states.
Oracle.
The account model is just easier to understand and reason about. Any other feature can be implemented both on UTXO and account model. So why bother with unnatural model when you can use the one banks use for centuries.
So do I. 🙂 I'd love to work for a company with this tech stack in Europe for a few years.
I almost always find an issue like that with self-hosted documentation sites. Looking at the same crate in [docs.rs](https://docs.rs/ws/0.8.1/ws/) shows me the `Send`/`!Send`-ness of the types, and I would highly recommend looking at docs.rs in almost all cases over their own generated documentation, unless of course it is docs for a nightly channel that isn't available elsewhere.
Thanks! I use your pktparse crate in 3 of those projects. :)
Not quite sure what's up there, but looking at a different version: ``` fn make_array&lt;T&gt;() where T: Sized, { const size: usize = std::mem::size_of::&lt;T&gt;(); let bytes = [0u8; size]; } ``` I get the error ``` error[E0401]: can't use generic parameters from outer function --&gt; src/lib.rs:2:45 | 1 | fn make_array&lt;T: Sized&gt;() { | ---------- - type variable from outer function | | | try adding a local generic parameter in this method instead 2 | const size: usize = std::mem::size_of::&lt;T&gt;(); | ^ use of generic parameter from outer function ``` So it looks like it's not interpreting the `T` in the call to `size_of` as the same `T` that's in the arguments. Could be a limitation of the way `const fn`s work right now? Working around it depends on what you want to do with it. You can get the memory of the correct size by calling `std::mem::zeroed&lt;T&gt;()`, which gives you a `T`; you can `std::mem::transmute` that into whatever, but the type of that can't include the size of `T`, so your best bet is turning it into a slice with `std::slice::from_raw_parts`: ``` fn make_array&lt;T: Sized&gt;() { unsafe { let bytes = std::mem::zeroed(); let slice = std::slice::from_raw_parts(&amp;bytes as *const u8, std::mem::size_of::&lt;T&gt;()); // slice: &amp;[u8] with len = size_of::&lt;T&gt;(); } } ```
No, they don't, looking at OP post history it shows that architecture choosing is hype driven, small company with exotic stack chosen by personal preferences of one person instead of business requirements. Then you see constant rewrites that OP is talking about. Seen it countless times.
It ensures that the type is never used as a value, but I'm not sure what you'd do with it anyway, so no, not really.
In fact, ancient Rust had typestate more explicitly, and it was removed because this pattern made the feature redundant. (Explicit typestate predicates were also buggy and rarely used.)
So many languages at a small company! What do you use to ensure reproducibility? Docker? Nix?
`must_use` is more like a best-effort lint. It's a useful warning but can still suppress it. True linear types would prohibit ignoring it at compile time.
or time travel! [https://devblogs.microsoft.com/oldnewthing/20140627-00/?p=633](https://devblogs.microsoft.com/oldnewthing/20140627-00/?p=633)
Is there a manual for how to use Crossterm as backend for TUI?
Oh, right. Yeah I guess it's not perfect. Would it be at all possible to enforce such a thing?
I'm not sure, I haven't used it my self. You should definitely check out the examples. Crossterm backend should not work that differently from other backends
Not at compile time. But if you want to prohibit a type from being dropped at run time you can impl `Drop` as a panic, and then anyone who doesn't want their program to terminate has to use the type instance instead of letting it drop.
Cool. When I was googling, the examples always used a list of a template or variable type. Now I get that part doesn't really matter. They were using a list of variants. If anyone reading this wants an example, compare `c` `struct`'s to `Rust` variants. the `Result&lt;T, E&gt;` type in Rust: - https://doc.rust-lang.org/1.30.0/book/2018-edition/ch09-02-recoverable-errors-with-result.html And the Rust book on `Enum` - https://doc.rust-lang.org/book/ch06-01-defining-an-enum.html
I'm on a 13" MBP with macOS 10.14.5.
Unfortunately `soa-derive` uses `Vec`s, so capacity and length gets duplicated.
I mean, how to bind them together. `tui::terminal::Terminal::new()` accepts anything that implements `tui::backend::Backend` and it has an implementation for Termion. I presume there should be an implementation for Crossterm somewhere?
Yes crossterm implemented this backend trait as well. You need to enable the crossterm feature flag and then you can follow along the example.
What you're actually looking for is /r/playrust.
it's also offtopic there, as far as i know. /r/playrustservers is correct
I love how people on this subreddit are actually learning about the game!Rust community by proxy.
oh, sorry
The lifetime of ``c`` is till the end of ``main`` tho? The referenced string as well. Can you explain further please?
&gt; I was specifically asking about the huge memory usage JDK GC's are notoriously lazy and will often favour growing the heap over collecting garbage in the name of performance - time spent walking the heap and freeing memory is time *not* spent in your code, so best let it build up if you want maximum throughput. It has various tuning options that can make it more aggressive.
Line 26 replaces `c` with a new `Holder` that has a different lifetime restriction. The one created on line 23 is required to not outlive `b`, but the one created on line 26 is required to not outlive `c` instead, which frees up `b` since the original one is no longer active.
What's the most performant way for Rust to format floating point numbers to a human readable string, such that the values round-trip properly? &amp;#x200B; It's not clear to me that the default fmt routines have this behavior.
Honestly, I wish I had a `rust-but-garbage-collected` lang. Or at least `rust-but-reference-counting-isn't-as-painful` lang. We go through great lengths to avoid allocation, and as someone with a semi-embedded / game dev C++ background, I'm happy about all the work that goes into that. But I also don't need to worry about it 90% of the time. There's certain cases where garbage collection might _help_ cache performance (objects adjacent to each other without reference counts / control blocks breaking them up). I also wish I could bring this up without getting "well actually" comments about how I'm wrong and allocation is always bad for performance, no matter what. Luckily that's rare in this community, but... ron_swanson_i_know_more_than_you.gif
Or just a good code browsing tool for Android tablets! I've searched but never found anything nice (e.g. with follow references, find usages, definition / type lookups, etc.)
Yep. We use Docker in development and in production using Kubernetes.
So by placing ``e`` in ``c`` the lifetime of ``c`` changes to now live till ``e`` disappears. By a new ``Holder`` do you mean an actual new object? Because ``c`` is only taken by ref. So it should just change instead of a new one being created? Or is this some weird language implementation because the lifetime is part of the type? And a last question: ``b`` is still needed by the not working line of code. Why does it not exist anymore? Only the reference to it is dropped, right?
I have a use case where I want to replace `Rc&lt;RefCell&lt;VecDeque&lt;T&gt;&gt;&gt;` with something like a `*mut VecDeque&lt;T&gt;`. However, I haven't been able to find any documentation online on how to actually take a raw pointer to a std::collection (every example I've seen only showcases how to create a raw pointer to a primitive type). For some additional context, I want the `VecDeque`s to be owned and mutated by multiple sources in a single-threaded context.
In regards to languages that have the good parts of rust but are GC'd, I've looked into them. They all lack the wonderful, modern innovation that's happening in the rust ecosystem right now. Kotlin is an awesome language, and can lean on the java ecosystem, but I feel like the ecosystem is not nearly as "mature" as people make it out to be. I wish there was more pure kotlin dev. F# is an awesome language, and I guess I should look into the .NET ecosystem more. But the non-windows dev experience is still a tad nascent. Also, those both still have exceptions to contend with. Ocaml lacks a good parallelization/concurrency story. Haskell is... Haskell. I've tried to learn it multiple times and there just doesn't seem to be a steady introduction that doesn't go off the deep end. If there's something I haven't tried yet, I'm all ears!
&gt; A few months go by, and I feel like I want to do something in Rust, get deeper. So I sit down in front of my computer with a blank stare. What is the exact syntax of doing a for loop again in rust? Let alone how do you even make a generic struct that is trait bounded. I honestly don't see this as a big issue, because learning programming languages is less about syntax and more about concepts. Knowing you need to use a for-loop or introduce some constraint is generally more important than knowing the exact syntax by heart, it's easy to look up the latter while you need to invest time to learn to use the latter.
Here's a visualization of the lifetimes of the different values: https://i.imgur.com/jWgkfd6.png `b` is red, the value assigned to `c` on line 23 is green, the value returned by `change_ref` and assigned to `c` on line 26 is blue, and `e` is orange. `b` and `e` last until the end of the scope because that's where their `drop` implementations run, while the `Holder` values last until their final use. The restrictions given by the lifetimes are: 1. Since the green lifetime borrows the red lifetime, the green lifetime must not outlive the red lifetime, and the red lifetime cannot be accessed mutably until the green lifetime ends. 2. Since the blue lifetime borrows the orange lifetime, they have the same relationship. If you change the final lines of `main` to let d = c.change_ref(&amp;e); b.clear(); d.print(); it does still work even though `c` is still in scope when `b.clear()` is called, because its lifetime ends with the last expression that accesses `c`.
These guys spammed the hell out of a youtube video i put up a couple years ago about a raspberry pi + ethereum project i had done. Endless comments saying "use iota, use iota" even though it wouldn't work for what we were doing. I ended up having to go through and marking the comments as spam. &amp;#x200B; I know it probably wasn't actual iota employees, but their rabid cryptocurrency investors. I would still be cautious. These cultural problems often start from the top, and you might find yourself working in a pretty cultish environment.
I can agree with that. The fact of the matter, I can write a small and performant microservice in golang using the standard lib in minutes. But I would never seriously consider using go for any complex numerical computation like computer vision. It has its limits (unlike a great multipurpose language like rust or c++), but it is super useful and productive in certain environments.
More troubling, undefined behavior also "time travels": if (var1 != 0) { // if may be removed. printf("%d", 10 / var1); } return 10 / var1;
Also, Haskell has had a proposal for linear types in the works for a while now. Not quite sure if it's going to land though.
Is there a good way (using safe code) to have a vector of multiple types that have the same size? let's say I have a struct that looks like this: \`\`\` struct Callback&lt;K&gt;{ pub inner: Rc&lt;K&gt;, pub func: Box&lt;Fn(&amp;K)&gt; } impl&lt;K&gt; Callback&lt;K&gt;{ fn call(&amp;self){ self.func(self.inner); } } \`\`\` then I want to have something like this: \`\`\` fn call\_all(callbacks: Vec&lt;Callback&lt;Any&gt;&gt;){ for callback in callbacks.iter(){ callback.call(); } } \`\`\` however making this happen is proving difficult. I can't just make a trait that they all implement, because a general trait object wouldn't be Sized.
Yes, I understand that last part. It's because of the new lifetime handling as opposed to block lifetimes. But as you (very nicely) indicated ``b`` lives till the end. Why wouldn't you be able to clear it?
Thanks for doing this! I'll see if I can find some motivation and pick up an issue.
Session types in Rust: https://github.com/Munksgaard/session-types . I seem to remember there was an experiment applying them to Servo to coordinate the shutdown of the various threads; not sure if it was ever merged, though.
Have you tried `cargo-bloat`? There might be some helpful information there to strip your binary size down.
I found one common sentence: &gt; Rust’s very strict and pedantic compiler checks each and every variable you use and every memory address you reference. which may be a coincidence (or memory). Is there more?
In my version (which does compile), you're right, it's fine. In OP's version, `change_ref` is modifying `c` instead of replacing it, which does not release the borrow of `b`.
Ok, thanks!
I seem to have discovered the source of this... This was only happening when running in the integrated terminal in VSCodium, and doesn't happen in my system terminal (Terminal.app or iTerm2). Curiously enough, this also doesn't happen in the integrated terminal of Microsoft's VS Code builds, either.
&gt; We needed a language that was fast enough to allow minimum real-time latency and use limited resources of a SoC device. I would expect that *real-time latency* and [*SoC device*](https://en.wikipedia.org/wiki/System_on_a_chip) ruled out Go because of performance considerations, but indeed it would have been nice to have a word on it.
Oh I see it now! OP's ``c`` keeps the dependency on ``b`` even after changing the ref. That's what you meant by "the lifetime is decided on object creation". Thanks for your patience!
There a few concepts in there as well. Things like lifetimes and such, syntax and concepts are also linked together. So whenever you get quizzed on a particular concept linked to a syntax it helps you to remember both.
First, a minor nit: you shouldn't need to clone that message. Just pass references to the debug and format macros. Second, you should be able to construct an Echo and do `echo_handler(echo.into())`, since `Path&lt;T&gt;` implements `From&lt;T&gt;`.
You may consider the following: &amp;#x200B; Mine: &gt;Rust’s very strict and pedantic compiler checks each and every variable you use and every memory address you reference. &gt; &gt;... &gt; &gt;The right part of the chart above shows concurrency and memory safety issues that are *fundamentally impossible* to get in the safe subset of Rust. &gt; &gt;... &gt; &gt;Moreover, out-of-bounds access bugs are one of the most dangerous, since they often cause [secrets to be compromised](http://heartbleed.com/), [denial of service](https://www.cvedetails.com/cve/CVE-2019-11683/), and [remote](https://nvd.nist.gov/vuln/detail/CVE-2018-3628) [code](https://portal.msrc.microsoft.com/en-US/security-guidance/advisory/CVE-2019-0708) [execution](https://nvd.nist.gov/vuln/detail/CVE-2019-11815) [vulnerabilities](https://semmle.com/news/apple-xnu-kernel-icmp-nfs-vulnerabilities). &gt; &gt;... &gt; &gt;So, by using Rust we eliminate one of the most complex and unpredictable classes of errors. &amp;#x200B; Theirs: &gt;Rust’s very strict and pedantic compiler checks each and every variable you use and every memory address you reference. &gt; &gt;... &gt; &gt;The right part of the chart below shows concurrency and memory safety issues. These are the most complex and unpredictable classes of errors and are fundamentally impossible to get in the safe subset of Rust. Moreover, all these type related bugs are dangerous and result in a variety of security vulnerabilities. &amp;#x200B; Personally, I almost treat that as fair use, but I'd rather wrote a proper reference in their place. To me copyright is pure evil, but on the other hand, open source works by multiplying knowledge while keeping all references and credits clear.
Isn't an "if let" equivalent to a "while" in the case of unpacking an Option anyways (since an Option can only contain 0 or 1 values)? I think it makes sense to add a clippy lint that suggests you use the former.
Iota is a regulated non profit organization in Germany. Here you can take a look with "we" are trying to build. https://github.com/iotaledger/bee/blob/master/ARCH_RUST.md
Think of it more of a rate of increase/decrease directly proportional to input. So if the elements are the same and the only thing that is being reduced is time, then the proportion is considered a constant. Hence a 7x reduction in constant as the guy mentioned. I am not a comp sci major too, but sometimes it's fun to read up on the stuff.
Drupyog, I am reading through the original type state paper and they don't draw as much of a clear line in the sand as you do with respect to the type of the variable must change. &amp;#x200B; [http://www.cs.cmu.edu/\~aldrich/papers/classic/tse12-typestate.pdf](http://www.cs.cmu.edu/~aldrich/papers/classic/tse12-typestate.pdf) &amp;#x200B; This article seems to agree with the author's definition by not drawing the same strict line that you do: [https://yoric.github.io/post/rust-typestate/](https://yoric.github.io/post/rust-typestate/)
You could make a `Callback` trait like ``` trait Callback { fn call(&amp;self); } ``` and then use a `Vec&lt;Box&lt;dyn Callback&gt;&gt;`.
Actix-web has [testing utilities built in](https://docs.rs/actix-web/1.0.2/actix_web/test/index.html) that will help a ton, I haven't ran the code below but it or something similar should work... &amp;#x200B; ... use actix\_web::test; use super::echo\_handler; \#\[test\] fn test\_echo\_handler() { let mut app = test::init\_service( App::new() .service(web::resource("/echo/{message}").route(web::get().to\_async(echo\_handler)) ); let req = test::TestRequest::with\_uri("/echo/test-message").to\_http\_request(); let resp = test::block\_on([app.call](https://app.call)(req)).unwrap(); // ...do something with the response }
It's already tractable for Result types, doesn't seem too nuts.
You can get a pointer like `let ptr = &amp;mut vec as *mut VecDeque&lt;T&gt;`. You are going to need something to own the `VecDeque`, though, unless it can exist statically throughout the entire program. Is there a reason you can't use the `Rc&lt;RefCell&lt;VecDeque&lt;T&gt;&gt;&gt;`?
I think you're overthinking this, to be honest. I mean, the first sentence is definitely the same, which could be explained by simple coincidence, or memory. However the other 3 sentences are just different. Similar, to some extent, but then they are commenting the same graph with well-established vocabulary which for me adequately explains the similarity. I would not consider plagiarism based on such a meager similarity.
Firstly, you'll want _something_ like `Rc` to ensure that the `VecDeque` is only deallocated once; if you used raw pointers you'd have to implement this yourself. Since the multiple sources in the same thread logically have to share a common ancestral stack frame, you could have your `VecDeque` rooted on the stack there and just pass references everywhere; this would work with `RefCell` as well. Secondly, sharing a `VecDeque` mutably in a single thread would be unsafe if you plan to use iterators or take references into it, or use any of the methods or trait impls (mutating or not) that take a closure or call arbitrary code like `Clone::clone()` or `PartialEq::eq()` (since the code it calls could accidentally modify the same `VecDeque` through another handle). That basically limits you to the `push*()` and `pop*()` flavored methods, `insert()`, `swap()`, `clear()`, `len()`, `capacity()` and `reserve[_exact]()`. Given that, you *could* write a wrapper that forwards to these without having to call `RefCell::borrow_mut()` every time using unsafe code. I would wrap the `VecDeque` in `UnsafeCell` which gives you a mutable pointer from an immutable reference. However, at that point I would probably prefer to write a wrapper using `RefCell` which would let you keep everything but the iterators (unless you wrote a wrapper for them as well).
I actually wasn't aware of QuickCheck's `Arbitrary` type but this seems pretty useful. My team's fuzzers historically have been written by hand (including the mutation logic), which would occasionally result in some flawed logic or forgetting to mutate certain fields. I started writing this crate to be able to take care of that for us, while also allowing us to provide additional information to the fuzzer about constraints for properties/objects. Here's an example: #[derive(Debug, Mutatable, NewFuzzed, BinarySerialize)] struct MyStruct { field_1: u8, #[bitfield(backing_type = "u8", bits = 3)] field_2: u8, #[bitfield(backing_type = "u8", bits = 5)] field_3: u8, #[fuzzer(min = 5, max = 10000)] field_4: u32, #[fuzzer(ignore)] ignored_field: u64, } fn main() { let mut mutator = Mutator::new(rand::thread_rng()); let mut instance = MyStruct::new_fuzzed(&amp;mut mutator, None); let mut serialized_data = Vec::with_capacity(instance.serialized_size()); instance.push_to_buffer::&lt;_, BigEndian&gt;(&amp;mut serialized_data); println!("{:?}", instance); // perform small in-place mutations on the instance instance.mutate(&amp;mut mutator, None); println!("{:?}", instance); } This supports enums, vecs, arrays, and "unions" for mostly targeting C/C++ code. I've been cleaning up the APIs and code for a little while and trying to figure out what works, what doesn't, and even if people would find this useful before I go ahead and flip the switch on open-sourcing it.
The problem is that there are a number of different 'ways' to read source code. &amp;#x200B; Overview: trying to see just the major components of the system and how they interact. This needs to basically be written by a person who is familiar with the code and can break it down into the major inherent components and systems, how they interact, etc. This is, bar none, the best resource for the lay of the land in a project. In this view, you need to be able to see groupings of the different systems, code blocks of related and differing components, etc etc. Visualizations are vital here. &amp;#x200B; Start onward: This can actually be automatically created based on the source code. In this way of reading the code, you literally go from 'main' and read on. Being able to drill down and back out is vital for this view of the source code. Being able to take quick notes are definitely needed in this view of the code. Historical view: In this view you see how the code developed from start to current day. It's useful to have access to outside resources which can link in and explain the context. Why was this pull taken before this other pull? Why was this decision taken between these two options? Why was this code implemented then backed out? etc etc. and I am sure a ton of others. &amp;#x200B; As far as I can tell, only the second option really could be automated by a machine, which is too bad. Knuth's coding system was beautiful for learning, I just don't think it is viable for industrial scale programming.
/u/kyle787's way is best, but if you want a more granular way to test, you can also do: ```rust let req = TestRequest::default().param("message", "hello").to_http_request(); let fut = Path::&lt;Echo&gt;::extract(&amp;req).and_then(echo_handler); assert_eq!(test::block_on(fut).unwrap(), "Hi there"); ```
Thanks for the detailed answer! Your answer is pretty dense and a bit difficult to parse. In other words, you made a lot of suggestions, though I'm not sure which is the "correct" one to go with. In my case, being limited to \`push\*()\` and \`pop\*()\` methods is fine, I believe. I'm attempting to implement a k-funnel data structure for use in a funnelsort implementation. If you want more background on funnelsort (and cache-oblivious algorithms in general), this is a good primer: [http://kristoffer.vinther.name/academia/thesis/cache-oblivious\_sorting\_algorithms.pdf](http://kristoffer.vinther.name/academia/thesis/cache-oblivious_sorting_algorithms.pdf) The gist of a k-funnel structure is that it's a recursive funnel where a single funnel (or sometimes called a "merger") has two input buffers (the \`VecDeque\`s in the case of my implementation) and one output buffer. This output buffer is then one of the input buffers to the funnel's parent.
Mostly it just started feeling too unwieldy. Which, I admit, is not a good reason to dip into `unsafe`, haha.
&gt; for such a core piece of infrastructure in the Rust ecosystem, we have astonishingly little manpower I think a lot of these core Rust projects need to seek out adoption by University CS programs. I can't think of anyone else that can provide both a professional, paid curator to at least participate in maintainership that can also provide a constant stream of onboarding targets to bring them into the ecosystem. Right now the kids are learning Python and Javascript and that is where they are ending up working. A decade ago they were learning Java and C#, and a decade before that they were learning C and C++. The Python and JS ecosystems are largely booming on the back of students being taught on them creating feedback loops that propagate a huge influx of software into the language spaces. Now that isn't always a positive thing. We saw the mass proliferation of now long abandoned C / C++ libraries and projects from the 90s, Java and C# in the 00s, and Python / JS today where students wrote code that they never wanted to actually commit to maintaining long term or that only ended up half finished that congested the mindshare of developers across the ecosystem. Its why Pypi and NPM serve thousands of libraries. Rust has seen some uptake in this space, but I feel like organizational effort from Mozilla and all the other major corporate consumers of Rust need to start leveraging their influence to more assertively see Rust adopted in universities, because its absolutely necessary to reach ubiquity as a language. Go and Swift are already way ahead in this regard from what I've seen.
This isn't possible right now without using boxed trait objects (see other reply), but theoretically could be possible with const generics: #[repr(align(64))] // ensures minimum alignment for all native types pub struct StackTraitObj&lt;T: ?Sized, const SIZE: usize&gt; { storage: [u8; SIZE], vtable: *const (), type_; PhantomData&lt;T&gt;, } impl&lt;T: ?Sized, const SIZE: usize&gt; StackTraitObj&lt;T, SIZE&gt; { // we have to express that `U` can coerce to `T` and that's with the unstable `Unsize` marker pub fn new&lt;U: Unsize&lt;T&gt;&gt;(val: U) -&gt; StackTraitObj&lt;T, mem::size_of::&lt;U&gt;()&gt; { // coerce `val` to a trait object, extract the vtable ptr } } By using `mem::size_of::&lt;U&gt;()` as the argument for the `SIZE` const parameter we ensure that we're not creating storage that won't fit the value but also that the storage size is constant; if you have a `Vec&lt;StackTraitObj&lt;T, SIZE&gt;&gt;` for some `SIZE` (even another invocation of `mem::size_of::&lt;...&gt;()` for some prototypical type), if the size is the same you will be able to add it to the vec.
I have to admit I'm not too familiar with Anki cards. Can you show a screenshot of an example or something that focuses on concepts like lifetimes?
Is there any workaround to trick `stdout().lock()` into consuming the `Stdout`, even though the signature is `lock(&amp;self)`? To avoid creating the temporary `stdout`: let stdout = std::io::stdout(); // pollutes the scope, bloats the code let mut output: Box&lt;dyn Write&gt; = match cli.value_of("output-file") { Some(filename) =&gt; Box::new(BufWriter::new(std::fs::File::create(filename)?)), None =&gt; Box::new(BufWriter::new(stdout.lock())), }; output.write(b"Hello, world");
Is u8 not too small to express all http status codes? Its from 0 to 255 so there needs to be some mapping between internal and actual delivered http status code as there are less than 255 status codes in total. I was just wondering..
 They're identical, lol. He handed him a piece of paper and said change it up a little so the teacher doesn't think he cheated. There's no way the language is exactly the same between both.
you can see one screenshot here: [https://imgur.com/a/hXRlkRl](https://imgur.com/a/hXRlkRl), the screenshot shows both sides at once. When you're being quizzed you wouldn't see the answer until you press space. So that is one example. But you could write anything you want and quiz yourself on whatever concept you'd like to remember or find difficult. The entire deck can be downloaded in the link.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/S0n4XCa.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme)^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20erqlmsf)
Glancing at this, I would honestly have all the `VecDeque` buffers in one `Vec` organized in an array-based binary tree: https://en.wikipedia.org/wiki/Binary_tree#Arrays You can then access them arbitrarily using indices; though getting 3 indices mutably from the same `Vec` requires `unsafe`, it is also much simpler to implement than what you wanted to do originally: unsafe fn break_borrow&lt;'a, 'b, T: 'a + 'b&gt;(r: &amp;'a mut T) -&gt; &amp;'b mut T { // by round-tripping through pointers we erase the borrow &amp;mut *(r as *mut T) } fn get_3&lt;T&gt;(slice: &amp;mut [T], x: usize, y: usize, z: usize) -&gt; (&amp;mut T, &amp;mut T, &amp;mut T) { assert_ne!(x, y); assert_ne!(x, z); assert_ne!(y, z); unsafe { (break_borrow(&amp;mut slice[x]), break_borrow(&amp;mut slice[y]), break_borrow(&amp;mut slice[z])) } } (This is safe because we're ensuring the 3 indices are disjoint and we're using the elided lifetime to ensure that the borrow does not outlive the source.)
Check out some additional ways to shrink size addressed on https://github.com/johnthagen/min-sized-rust
Well the idea for the data structure is, that you have a vec of nodes, a vec of edges and a vec of offsets. The edges are sorted by their source node index. The offset list has size number of nodes + 1. Offset\[i\] contains the index of the first edge of node i. This means to loop over the outgoing edges of node i you can just do something like this. `for e in edges[offsets[i]..offsets[i+1] { dbg!(e) }` &amp;#x200B; The data structure has great memory locality and small memory overhead. But it is static. This means edge insertion and deletion basically require recomputing the data structure. Which is absolutely no problem for road networks ;)
Part of the reason for this being a common term in the rust community might be that long ago in ancient history rust had true typestate, and the fact that this pattern is possible in rust was part of the justification for removing it, iirc.
No need to collect into a Vec at each iteration. Just use the iterator as is: let values = row.split(","); // Do things println!("{:?}", values.next().unwrap());
Ah, the good ol' implicit array-based binary tree representation. How could I have forgotten? Thanks for the stroke of inspiration!
While using the [metered-rs](https://docs.rs/metered/0.2.1/metered/) crate, I can't seem to `clone()` the type `HitCount` and I don't understand why [because the documentation shows it does indeed implement `Clone`](https://docs.rs/metered/0.2.1/src/metered/common/hit_count.rs.html#14) Something like this: use metered::HitCount; #[derive(Clone)] struct A { hits: HitCount } Gives me this error `the trait bound 'metered::atomic::AtomicInt&lt;u64&gt;: std::clone::Clone' is not satisfied` ... `the trait 'std::clone::Clone' is not implemented for 'metered::atomic::AtomicInt&lt;u64&gt;'`
You might be able to collect and re-export common dependencies into one crate, whose type you set to `dylib`, and then have plugins link against that instead of statically compiling each dependency. I *think* this would work fine so long as you make sure to use the same Rust version to compile everything, as the Rust ABI is unstable and subject to change between versions. How it gets loaded is platform-specific and an exercise for the reader, though.
While using the [metered-rs](https://docs.rs/metered/0.2.1/metered/) crate, I can't seem to `clone()` the type `HitCount` and I don't understand why [because the documentation shows it does indeed implement `Clone`](https://docs.rs/metered/0.2.1/src/metered/common/hit_count.rs.html#14) Something like this: use metered::HitCount; #[derive(Clone)] struct A { hits: HitCount } Gives me this error `the trait bound 'metered::atomic::AtomicInt&lt;u64&gt;: std::clone::Clone' is not satisfied` ... `the trait 'std::clone::Clone' is not implemented for 'metered::atomic::AtomicInt&lt;u64&gt;'`
Nice! I like your explanation of lifetimes and of `str` being atomic and needing a set size.
&gt; I honestly don't see this as a big issue OP is telling you that it's getting in his way. It's cool if it's not an issue for you, but it is for someone else. What's more, he's addressing the issue and you're dismissing it. I think if you consider it from the point of view of the OP, you may realize that your answer could be more constructive.
The documentation shows the use of automatic derive to implement `Clone`, which means that `HitCount` will only be `Clone` when its generic type argument `C` also implements `Clone`. In this case, the default argument for `HitCount` - `AtomicInt&lt;u64&gt;` indeed doesn't implement `Clone`, and so the `HitCount` doesn't either.
On the second one (the added crate) I've experienced that as well, but I'm not sure what causes it. I haven't experienced it with Rust Analyzer FWIW. For the first one, I'm pretty sure that's just a result of the lazy_static internals : ```For a given static ref NAME: TYPE = EXPR;, the macro generates a unique type that implements Deref&lt;TYPE&gt; and stores it in a static with name NAME. (Attributes end up attaching to this type.) On first deref, EXPR gets evaluated and stored internally, such that all further derefs can return a reference to the same object. ``` So the completion isn't working on OUT.write_line() because OUT is a unique type that only becomes OUT once it's deref'd the first time. Most uses of lazy_static end up looking like `&amp;*OUT.write_line()`
Is there any way to tell the compiler I variable's type should implement more than one trait? Like here let mut input: Box&lt;dyn BufRead + std::fmt::Debug&gt; = match cli.value_of("input-file") { Some(filename) =&gt; Box::new(BufReader::new(File::open(filename)?)), None =&gt; Box::new(stdin.lock()), }; // Does not compile because Debug is not an auto-trait I need anything that is `BufRead`, but for error handling I'd like to do something like match input.read_line(/*...*/) { Err(e) =&gt; eprintln!("ERROR: reading from input {:?}", &amp;input); /* ... */ } This seems like a basic abstraction feature but I can't figure out proper syntax.
I agree with you, but when i saw python, java and lua included (but not go), I really don't understand their selection strategy.
Do CS professors have the time to maintain this library? I'm not in CS academia but from what I've seen in my field, maintaining a library isn't research-y enough for a professor or grad student to work on.
Educational frameworks have different goals from professional ones. University CS isn't trying to teach people to write maintainable production code; it's about fundamental CS concepts.
Would it be reasonable to use generics instead of trait objects here? Something like: fn read&lt;T: BufRead + Debug&gt;(input: T) { .. }
I honestly believe my comment is more constructive than yours is. The concepts are more important to remember than syntax, many people look up basic syntax all the time. I get that not remembering how to write basic constructs in a language can be frustrating, but being aware of them and knowing when to use them is significantly more useful.
As a VSCode user myself, I've noticed that often times if something fundamental about the build process changes, VSCode needs restarted to figure everything out. This seems to include (but is probably not limited to) adding dependencies, changing the toolchain (ie, going from stable to nightly or vice-versa), or making changes to other projects if they are bundled as a workspace. I do feel like it is inconsistent sometimes though. Part of it might be that I tend to not notice right away. I have no idea what the issue is, but my gut feeling is that it's something to do with RLS. PS: You shouldn't need 'extern crate' in your code (except for one specific exception if you are writing a proc-macro lib, or if you are working in a 2015-edition project), and intelligent should still work even without it.
Ah I see, thanks!
Quoting myself: &gt; I'm doubtful of it's practical usefulness but it's an interesting to mull over. By this I meant I don't think it's an improvement over existing legal and cultural systems. That doesn't stop it from being a curiosity.
Try `rust-analyzer`. It might work better for you.
Thanks, I'm glad it helped! Lifetimes was definitely the hardest thing for me to get when we covered it in class, that's why i decided to focus on it and ownership here
Why not doing this with blogs when we do the same with our code + SO :-D
You can't have a trait object of more than one trait; it's not supported at this time (apparently due to Rust's lack of upcasting and uncertainty around how the vtables would be structured to support it.)
&gt; PS: You shouldn't need 'extern crate' in your code (except for one specific exception if you are writing a proc-macro lib, or if you are working in a 2015-edition project), and intellisense should still work even without it. Thanks! I remember reading that one of them should be considered deprecated except for specific use cases but I completely forgot! I hope VSCode has better support for rust later down the line. This could easily be a extremely expandable editor for rust.
You could upload the deck to Anki web so people can sync it in the future for updates directly, instead of you having to send out emails for it.
Why is rust is going for async computation using async/await instead of doing continuations instead like [java](https://www.infoq.com/presentations/continuations-java/) or c++?
The JVM in question likely only uses mark-and-sweep to do garbage collection, and since in a GC'd language every object goes on the heap, the heap will grow until it hits a predefined limit. Often rapidly. There's a logic that this is the fastest approach; that not freeing an object is doing less work. This may or may not be accurate. CPython on the other hand uses reference counting by default, which tends to free objects quickly. Like when the function they're created in returns. So while Python has a lot of memory overhead, it collects eagerly.
&gt; I'm not sure how well deref coercion works with the completion, so you might try &amp;*OUT.write_line. I just did a bit of testing, and it seems like intellisense does not work *at all* with `Deref` types. Specifically, let mutex = std::sync::Mutex::new(Vec::&lt;u64&gt;::new()); let mut lock = mutex.lock().unwrap(); (&amp;mut*lock).push(5); I cannot find any way to get autocompletions on `lock`. If I type `lock.` then I get autocompletions for the `MutexGuard`, which includes `deref()` and `deref_mut()`, but typing `lock.deref().` doesn't show *any* autocompletions.
Ok, good to know. It looks like Rust Analyzer doesn't currently do this either, but its been brought upon github, so it's on their radar. https://github.com/rust-analyzer/rust-analyzer/issues/1183
[https://github.com/rust-dev-tools/fmt-rfcs/blob/master/guide/guide.md](https://github.com/rust-dev-tools/fmt-rfcs/blob/master/guide/guide.md) is the official style guide, and you can get the `rustfmt` component to auto-format your code to meet it.
I use rustfmt and nearly all of the clippy lints: #![deny( clippy::all, clippy::restriction, clippy::pedantic, clippy::nursery, clippy::cargo )] #![allow(clippy::integer_arithmetic)] #![allow(clippy::missing_inline_in_public_items)] #![allow(clippy::multiple_crate_versions)] #![allow(clippy::implicit_return)]
Then you want to use an interpreted language.
&gt;In lieu of true linear types, you can the [`#[must_use]` attribute](https://doc.rust-lang.org/reference/attributes/diagnostics.html#the-must_use-attribute) which enables a best-effort compiler lint. Even if we ignore the fact that it's just a lint, it's important to note that `#[must_use]` only checks whether the value is used in any way, not that it's actually consumed. It won't tell you to close a file if you've already read from it, for example. A hypothetical `#[must_consume]` attribute would be more helpful if you want to mimic linear types, I guess.
In this case I would just make an enum (maybe use [Either](https://docs.rs/either/1.5.2/either/enum.Either.html)). In general, I believe you can make a trait that requires both desired traits and give a blanket impl.
 while Some(item) in stream.next().await { This is a **syntax error**. Nothing can crash here. &gt;Isn't an "if let" equivalent to a "while" in the case of unpacking an Option anyways No, you're misunderstanding what `while let` does. The following statements are equivalent: while let $PATTERN$ = $EXPR$ { ... } loop { if let $PATTERN$ = $EXPR$ { ... } } This code loops as long as $EXPR$ returns a value that matches $PATTERN$. Note that for an `Iterator`, the following is also equivalent: while let Some($PATTERN$) = myIterator.next() { ... } for $PATTERN$ in myIterator { ... }
&gt; for such a core piece of infrastructure in the Rust ecosystem Is winit, _really_ a _core piece of infrastructure_ though?
Try using in IntelliJ with the Rust plug-in. It is the best I have seen so far for Rust.
You can make a separate trait and use that for your trait object, but there's no way to get from a trait object of, say, \`BufReadDebug\` to \`BufRead\`, so it doesn't really do anything unless you manually copy all of the methods from the two traits onto your trait.
It’s definitely doable with procedural macros and would probably look cleaner implemented that way. I’m also very confident that it can be done with plain macros but it’s been a while since I’ve used those.
it doesn't support Windows
A statusline can only be called at most once, it will return a type that accepts headers.
[Here's a simple example using an enum](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=72c52b1bd9da31e567fe3936b16598c4)
Rust does not features linear types, only affine.
Servo, the Firefox rendering engine, depends on a project maintained by one person and that doesn't have active support of lots of platforms? How is that possible?
&gt; sniffglue is a network sniffer written in rust. Network packets are parsed concurrently using a thread pool to utilize all cpu cores. Project goals are that you can run sniffglue securely on untrusted networks and that it must not crash when processing packets. The output should be as useful as possible by default.
yes, typically showing an average user something not in a terminal is pretty important.
For those of us who don't know, where can we read about this/how can we try it out?
There is an entire world of things to build that don't involve UIs at all. This claim is, in my opinion, similar to saying rocket is core infrastructure to Rust as a whole.
Servo != Firefox
Appears to be Linux-only but doesn't specify this anywhere.
It's whatever is defined in the project rustfmt.toml file. Don't bother formatting anything by hand. It will get automatically formatted with rustfmt.
Which operating system are you missing?
there is a crate called paste that may help you do this. i used it to do something like that with functions
Maybe I'm wrong about the issues you're having, but it's possible that merely restarting the RLS will fix your problems instead of restarting VSCode. On a mac that would be command+shift+p and "Restart RLS", Windows/Linux will be control+shift+p.
It's a direct dependency of [Servo](https://github.com/servo/servo/blob/master/Cargo.lock#L5384) and [glutin](https://github.com/rust-windowing/glutin/blob/master/glutin/Cargo.toml#L21), which I would say makes it pretty important.
I can absolutely get behind "winit is important for GUI work" but in my opinion that does not make it core infrastructure.
I see the dependent pcap crate does work on Windows so maybe this does too.
You can't use a `macro_rules` macro to concatenate identifiers like that, but you can use a type of the same name in a submodule, like `verified::A`. If that doesn't work for your use case, then you'd need to use a procedural macro. I ran into this issue recently and because my use case was more complicated, I ended up going with a procedural macro (https://github.com/ruma-events-macros if you want to take a look, but there's a lot of complexity there not related to the specifics of your question).
This subreddit is for the Rust programming language, not the Rust video game.
I've tried that after I made the post (as well as updating it) and it doesn't fix it for me. :(
&gt; linear or affine types What did I say?
I found the [git repo](https://github.com/rust-analyzer/rust-analyzer) which states that we have to do: &gt; # clone the repo &gt; $ git clone https://github.com/rust-analyzer/rust-analyzer &amp;&amp; cd rust-analyzer &gt; &gt; # install both the language server and VS Code extension &gt; $ cargo install-code &gt; &gt; # alternatively, install only the server. Binary name is `ra_lsp_server`. &gt; $ cargo install-lsp I haven't had a shot to actually do it yet but it looks like it creates a new vs code installation so I'm hoping someone can show us how to install it with our current vs code? This looks like a really cool alternative and it may be the fix for something I mentioned in the post above.
I tried the IntelliJ plugin and the problem persists. However, I've used ReSharper &amp; Rider for a few years (JetBrains products) and the product itself is really good. I just prefer the customization of vs-code over JetBrains :P
&gt; I honestly believe my comment is more constructive than yours is. With all due respect, you are wrong. The OP is sharing a tool that helps to overcome a problem, and your first response was to try to tell everyone that's not *actually* a problem, purely because you think you know better. Here's the thing - none of your points regarding learning programming are necessarily wrong, they just ignore everything that OP has said. Again, you dismiss a real problem that someone had, dismissed the contribution OP made here which addressed their problem, and you somehow think that's constructive. Please consider the evidence that another intelligent capable human being identified a problem, used a tool recognized to address that problem, and then shared that. And in return you publicly discuss your own opinions without taking into consideration anything the OP has written, assuming you know better than them what they need. That is not constructive.
Then good luck :-P
If you're looking for non-syntax style, https://rust-lang-nursery.github.io/api-guidelines/
Please change that to an unsafe function instead of a safe function -- right now it trivially allows UB-in-safe-code, and the tweak still demonstrates what you want to demonstrate.
Right, ya. Sorry. Fixed. I was just wrapping it in a function because the previous ones were.
Rocket has alternatives like Actix-web. Winit, as far as I know, does not. It's also much easier to write a web server without a framework like Rocket than it is to write a graphical desktop application with something like winit.
`soa_derive` is in the same vein. There are a few things that `soa-vec` does that `soa_derive` does not, and vice-versa. `soa_derive` has a means to sort, index, deref to slices, etc. It also takes a bit of a more low level approach to implementation. &amp;#x200B; I may add a name based API in the future for the convenience of naming that `soa_derive` has, which is quite nice for sure. At the time being, one could use the newtype pattern to get it I suppose.
Several GUI efforts (makepad, druid, Rust-VST) are choosing not to use winit because of various limitations, and rust_minifb is also an alternative. I'd like to see common, solid infrastructure for window creation and related platform plumbing, but a case can be made there's still room for this to be developed.
&gt; It's a theoretical attack but there's situations where if an attacker controls a very large proportion of outputs then they can deanonymize which outputs are from a target. The first research paper, “MRL-0001: A Note on Chain Reactions in Traceability in CryptoNote 2.0” is about this topic, saying that they aren’t open about it is just dishonest. https://web.getmonero.org/resources/research-lab/
Servo is not the Firefox rendering engine. _Some_ technologies developed within the Servo project eventually find their way into Gecko and Firefox more broadly.
That's only true if you have a value. If I have an `&amp;mut` like in this function `foo`, then you don't know when that reference is being mutated by a call such as `bar`: foo(a :&amp;mut T){ bar(a); ... }
Wrong sub, you want /r/playrust.
It's not like Rust is bad for that. Also, from a PL standpoint Rust has many interesting features. It also uses LLVM, which is used for some intro compiler courses.
FYI, if that's your list, you might be missing out on some of the allow-by-default rustc lints, e.g., warn when forgetting to impl debug or clone where possible.
What specifically about Loopback are you looking for?
https://semver.org/#spec-item-4 &gt; Major version zero (0.y.z) is for initial development. Anything may change at any time. The public API should not be considered stable.
This is not the subreddit you are looking for! Try [here](https://www.reddit.com/r/playrust/).
Thanks. I do know about cargo-bloat and plan to use it more extensively.
Thanks, I've seen that page too - which is how I found out about the things I've already done.
If this is possible while maintaining the nice Rust interface to such crates, that would be perfect. The unstable ABI shouldn't be an issue because the same toolchain will always be used for everything. I'll look into this more. Thanks.
I think there's something wrong with the numbers. Memory usage is much higher for sure, but 100 times? Or 500 times more than C? No way. It might be just wrong measured or a very contrived example. In a real world I'd say 10 to 20 times (which is actually large enough to make a point :-)).
OpenJDK is more than fine.
In my experience it is way more reliable to close VSCode, run: rm -fr target/rls then start again.
Thanks for sharing this deck! I was also considering using Anki for it after how useful I found Anki for non programming languages. I think that while I keep using Rust I won’t need it as much, but what usually happens to me is that when I switch to another language I quickly forget the previous one, and I thought Anki could help me keep it fresh even if I’m not using it much
minifb is not a realistic alternative to anything at the moment. It's a pure software display system and has unnecessary overhead.
True, so I'm not using that either :). It's just another example of something that's not using winit.
how deeply is this tied in to the sdl2 crate would you say? Could I maybe port it over to an alternative set of bindings and wrappers for the SDL2 C api?
I'm wondering if the new events system brings `winit` closer to viability for use in `vst-rs`... Might be time to dive in, hack it up, and see what the major pain points are again pretty soon.
You didn't clarify what Rust offers "linear or affine".
I mean, as long as you don't explicitly need to lock it (other writes won't interfere), the [`std::io::stdout`](https://doc.rust-lang.org/std/io/fn.stdout.html) function returns a [`Stdout`](https://doc.rust-lang.org/std/io/struct.Stdout.html) struct, which is defined as "A handle to the global standard output stream of the current process." The actual "handle" part of it is another struct called `StdoutRaw` which is wrapped in a `Maybe` which is wrapped in a `LineWriter` which is wrapped in a `RefCell` which is wrapped in a `ReentrantMutex` which is wrapped in an `Arc`; basically `Stdout` is just a reference to the actual, underlying method of piping data to standard output. In short, it seems fine to just do ``` None =&gt; Box::new(BufWriter::new(stdout())), ``` At least, [this Playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;code=use%20std%3A%3Aio%3A%3A%7Bstdout%2C%20BufWriter%2C%20Write%7D%3B%0A%0Afn%20main()%20%7B%0A%20%20%20%20let%20mut%20output%3A%20Box%3Cdyn%20Write%3E%20%3D%20Box%3A%3Anew(BufWriter%3A%3Anew(stdout()))%3B%0A%0A%20%20%20%20output.write(b%22Hello%2C%20world%22)%3B%20%2F%2F%20Make%20sure%20to%20perform%20error%20checking%20here%20though%0A%7D%0A) compiles and works just fine ``` use std::io::{stdout, BufWriter, Write}; fn main() { let mut output: Box&lt;dyn Write&gt; = Box::new(BufWriter::new(stdout())); output.write(b"Hello, world"); // Make sure to perform error checking here though } ``` Hope this helps!
I don't think there's any attribute you can put into your code directly to make rustc produce .o files by default. What are you trying to achieve here?
is tied in two ways, there is a simple struct that has Canvas&lt;Window&gt; and Font&lt;'ttf&gt; its called AwfulRenderer, next in the Structs of the widgets there is a field with a *mut u8, I casted a reference of AwfulRenderer to *mut u8. And when i use it on draw function of the widgets I cast back to AwfulRenderer. I do this because two reasons: - it's very limited put lifetime specifiers on Trait objects - cannot borrow mutable one more time, but the widgets are dispatched one by one in a single thread The second way it's two generic states that the poll_events iter modifies and call handle_mouse and handle_keys with those generic states as arguments Probably call that pointers "bindings" could produce confusion, there is a better name for that pointers?
Can you explain how read_write is guaranteed to be safe: ``` #[inline] /// Returns a mutable reference pub fn read_write(&amp;self) -&gt; &amp;mut T { unsafe { &amp;mut *(self.ptr as *mut T) } } ``` https://github.com/mrgaturus/trgui/blob/master/src/binding.rs
"handle" or just "pointer" is the usual term, if I follow you right. Your design internals could probably be made safer and saner at a fairly low cost. This is a common sort of problem to bump in to with Rust. If you're a Discord user, you should join the Rust Community Discord and ask folks in the #game-and-graphics-dev channel. Depending on the time of day, there's often folks who can help out. I've done some SDL2 related work myself recently, but I went in the direction of lots of lifetimes to ensure static correctness.
As an afterthought, I've done what you're trying to do before, and you may be able to avoid dynamic dispatch by using an enum. ``` enum Output { Stdout, File(std::fs::File) } ``` and then implement `Write` for it... ``` impl Write for Output { fn write(&amp;mut self, buf: &amp;[u8]) -&gt; Result&lt;usize&gt; { // Check whether we should use stdout or a file match self { Stdout =&gt; { let stdout = stdout(); let stdout = stdout.lock(); std.write(buf) }, File(file) =&gt; { // file writing implementation that i'm too lazy to write... }, } } fn flush(&amp;mut self) -&gt; Result&lt;()&gt; { /* should be easy enough to write */ } } ``` (you could see me getting progressively lazier. just ask if you want those implementations) Then plug it in to your code ``` let output: Output = match cli.value_of("output-file") { Some(filename) =&gt; Box::new(BufWriter::new(Output::File(std::fs::File::create(filename)?))), None =&gt; Box::new(BufWriter::new(Output::Stdout)), }; ``` I haven't tested this, but it makes sense that it would work, perhaps with some modifications. Of course, this sort of solution would work better if you happened to be switching between standard output and a file because it checks where it needs to write and then establishes a lock every time `write` is called. If the choice was final and no switching was going to be done, dynamic dispatch may have less overhead. My program needed to switch output destinations a lot during its execution, which is why I went with this solution, but I also think I'm a little afraid of dynamic stuff in my code and tend to stick to more static solutions like this. But hey, it's up to you. Good luck with whatever cool stuff you're making.
I agree that this is not typestate, but I don’t agree that it’s session types. I’d say that session types is mostly a subset of what is shown here and what is possible in this space. (This is also why I’ve argued that session types is not a laudable goal in Rust: Rust can achieve more and better than simple session types.)
The clause is me saying that, { L | L ∈ languages; (L has affine types ∪ L has linear types) ∩ L is mainstream } = { Rust }
First disable the rls extension in vscode and then run `cargo install-code`. It will automatically build and install the extension for you. It's an alternative to RLS.
 fn main() { let proxy = trgui::binding::Binding::proxy(&amp;&amp;0); let undefined_behavior = proxy.read_write(); *undefined_behavior = 1; // Segmentation fault (core dumped) }
Thanks for the heads up! I've changed it to: #![deny( nonstandard_style, warnings, rust_2018_idioms, unused, future_incompatible, clippy::all, clippy::restriction, clippy::pedantic, clippy::nursery, clippy::cargo )] #![allow(clippy::integer_arithmetic)] #![allow(clippy::missing_inline_in_public_items)] #![allow(clippy::multiple_crate_versions)] #![allow(clippy::implicit_return)]
Sorry!!, I forgot put unsafe on it, i changed it to unsafe now
Unsafe or not unsafe, calling it will cause UB.
you're welcome! The benefit of anki is that once you've mastered a card it's not that much effort to maintain it. After you've been quizzed on a card correctly for about 5-7 times it'll take 1 month+ between each time you see the card.
What is the difference between [main.rs](https://main.rs) and [lib.rs](https://lib.rs) ? I see many projects have both of them!
That's not really a fix. You need to document what invariant the caller is responsible for upholding in order to make their call to this function safe and correct.
No worries, I just don't want someone searching, finding this, and copy-pasting the original into their code. Looks fine now.
This is what fighting the borrow checker looks like I guess
Hi, Is there a reason why the event loop is integrated with winit, and not a standalone crate? I started writing an event loop some time ago [thin_main_loop](https://crates.io/crates/thin_main_loop) but gave it up due to lack of interest from others. I doubt it's mature enough for you to depend on right now, but I believe separating the event loop into its own crate would be beneficial for the ecosystem. We need a main loop that binds to native GUI apis instead of tokio's "highly scalable server" apis.
Nice work, thanks!
Oh, right. My bad.
There are likely several crates for doing this but if you wanted to you can do this yourself easily enough with a custom derive or a custom attribute. Simple example: extern crate proc_macro; use proc_macro2::Ident; use quote::quote; use syn::{parse_macro_input, Data, DeriveInput}; #[proc_macro_attribute] pub fn rename(attr: proc_macro::TokenStream, input: proc_macro::TokenStream) -&gt; proc_macro::TokenStream { let input: DeriveInput = parse_macro_input!(input as DeriveInput); let new_struct_name = parse_macro_input!(attr as Ident); match &amp;input.data { Data::Struct(_) =&gt; { let mut new_struct: DeriveInput = input.clone(); new_struct.ident = new_struct_name; let expanded = quote!(#new_struct); proc_macro::TokenStream::from(expanded) } _ =&gt; panic!("expected struct"), } } Example usage: use my_proc_macro_crate::rename; #[rename(B)] struct A { id: u32 } fn main() { let _b = B { id: 0 }; } Should be easy to see how to expand this to accept a list of names and generate several structs.
A I see, what does servo use this crate for then?
&gt; (no memory leaks, dangling pointers, etc) Rust (neither unsafe nor safe) does not prevent memory leaks. See [std::mem::forget](https://doc.rust-lang.org/std/mem/fn.forget.html).
The current state was arrived at after a lot of experimentation and design work. Any model for async computation in Rust would need to work with its ownership &amp; lifetime rules while ideally requiring no allocation after setup (and make it possible to do allocation-free setup for embedded applications). Continuations didn't fit that requirements.
Winit is a pure rust alternative to glfw. It handles window creation and management, user input, etc. Firefox is a GTK app, so it probably uses that for windowing.
That is absolutely something that would be interesting to do, but that is no easy feat. If you have some ideas though they'd be welcome here https://github.com/rust-windowing/winit/issues/872
It isn’t bad but it isn’t ideal either. It’s notoriously difficult to design recurrent data structures in Rust and that’s a good thing in a non-gc language, but half of CS is exactly that, so you end up teaching two difficult concepts at the same time. I’d rather be learning about graphs in Python and computer architecture in Rust and assembly.
One problem is that the event loop story is not clear yet. As this article mentions different platforms have very different requirements.
I'm seeing the example event loop as a plain state machine. Does winit plan to move to a futures-based API? I mean, async/await do compile to state machines, but is composable.
Every other programming language can make UIs, from the mature Qt bindings to the likes of Python to the numerous options on the .NET ecosystem. With Rust either you use GTK, which is a horrorshow on Windows because the developers provide no binaries and the official instructions tell you to use a busted build provided by Microsoft, or you get to swallow a worm and make a website.
The article is not really well written, but I had just assumed that they mentioned the languages because they were covered in the linked benchmark without actually considering them seriously for their purposes. It's not very clear either way though :/
Well, then we'll have to agree on disagreeing. Given the graph, I would probably produce a similar looking description too. Especially given both authors took the graph from a *talk* where we could probably find quite similar argumentation (unfortunately, I could not find a video recording of the talk).
&gt; Knowing you need to use a for-loop or introduce some constraint is generally more important than knowing the exact syntax by heart Sure, but once you know the concepts, why not learn the syntax by heart? For a long time I had the same view that looking up syntax is fine (and when done not too often, it is. Then I got into spaced repetition with Anki for other reasons and also started to add some Rust and JS syntax. The amount I had to look up syntax has gone way down and I recognize now that I significantly underestimated how often I do it. I would place it among the top productivity gains I had in the last year.
Wrong place, you want r/playrust
Of those mentioned, it looks like only makepad is cross-platform, though still lacking linux, no mention of android/ios. They seem like strange cases against winit when the purpose is cross-platform :) \&gt; I'd like to see common, solid infrastructure for window creation and related platform plumbing, but a case can be made there's still room for this to be developed. I think a really great case would have to be made as to why winit could not be steered in this direction. The sheer amount of lessons learned in that project related to strange platform-specific quirks and how to achieve consistent behaviour across all platforms is unique to anything I've seen in my (limited!) experience in open source over the past few years. I'd heavily advise anyone against going through all that work again unless they're looking for the experience of working in the depths of FFI and weird platform-specific quirks :) Would you mind linking to somewhere I might read about the limitations you have run into with winit? I would be surprised if this latest overhaul does not address a lot of the concerns you have in mind. If not, I'd be very curious to hear what remains.
I use it for writing kernel exploits for my day job. I still use c-style generated shellcode for the code injection style attack.
Rust intellij has an off-by-default setting to expand macros: [https://www.reddit.com/r/rust/comments/8ahoq4/intellijrust\_support\_of\_advanced\_macro\_expansion/](https://www.reddit.com/r/rust/comments/8ahoq4/intellijrust_support_of_advanced_macro_expansion/) &amp;#x200B; However it can chug a little if you have deeply nested macros, which is why it's off by default.
Compiled binary works reliably across most distributions. Simple to boot, copy over and test.
The ease and speed at which you can set up endpoints. OpenAPI (Swagger UI) and OASGraph for GraphQL.
Okay, I have written something to spawn a discussion.
If you want the exact same parameters, maybe you want a member containing this data that is a separate struct. Then you can have, for example: fn post (self) -&gt; Posted { self.do_post(); Posted { data: self.data } } Please excuse any typos, on mobile.
&gt; If you should rewrite a program written in another programming language - which are using inheritance; which features of Rust would you use to make the rewrite. Composition. In other programming languages you have to remember to prefer composition over inheritance. In Rust you don't have to remember that, the language enforces it for you.
That was not a fight, that was giving up.
&gt;If this is possible while maintaining the nice Rust interface to such crates, that would be perfect &amp;#x200B; Yes, this is possible, you can use them as if they were statically linked.
Thanks! ... and BTW : I forgot to mention that I'm a rookie regarding programming in Rust.
Look up rust-clippy if you need help with the most idiomatic ways to do things.. That certainly messed me up in the beginning too! (Not saying I'm any better now lol, rust is sti very new to me) Also check out crossterm, as it supports far more terminals, and is generally better to use
You use delegated composition, which is to store a field of a type you wish to "inherit" from, and implement methods which delegate to the methods of that field, or maybe even just expose that field. They could be inherent methods or they could be trait methods. Where you'd use inheritance in OOP, you'd most likely use traits in rust. They serve a similar role to interfaces, except that they do provide some amount of code reuse since trait methods can have "default" implementations. The Iterator trait is a good example. You implement one method, `next()`, and in return you "inherit" the behaviour of an iterator, because all the other methods have default implementations which delegate to `next()`. The other alternative to OOP is to use enums. Where you'd have a very closed set of types with the same interface in OOP, you could instead use an enum. You'd use `match` statements inside its methods to dispatch on the type. Since a `match` is statically dispatched, it's very efficient, usually compiling down to a jump table.
Not sure if anything changed and I am not Raph, but until he answers this could be a hint: [https://raphlinus.github.io/personal/2019/02/20/more-small-updates.html#why-not-winit](https://raphlinus.github.io/personal/2019/02/20/more-small-updates.html#why-not-winit)
Interesting...there's still quite a lot I have to learn about Rust, but if you're using that it seems you're going out of your way to potentially introduce memory leaks. This is just a basic intro for people who might already know C (so I just mentioned the stuff that seems alien to C people)
Ahh thanks for the link! &gt; I believe winit is fundamentally based on an architectural decision which is ok for 3D games but not for general GUI work: a separate Rust event loop thread that coordinates asynchronously with the host’s UI loop. Yes I believe this is one of the main issues that was addressed by the update that is the subject of this announcement. &gt; Another reason not to use winit is the VST use case. The Rust DSP community has also decided not to use winit, because they need finer grained access to the window creation process. A VST is given a handle to the host UI, and needs to instantiate a view within that, as opposed to creating a window and view from scratch; winit has architectural decisions that basically assume the latter case. I’m in touch with that community and am hoping the druid work will meet their needs. This is a trickier case, and I can't speak to whether or not this is something that winit can easily add, but it should be easier at least after this latest update. I do wonder if some kind of `Host` abstraction could be added that allows for specifying whether the user will be the host or whether some external host should be used upon construction of the `EventLoop`. E.g. right now, event loop initialization looks like `EventLoop::new()`, but maybe there's potential for an additional `EventLoop::from_host(vst_host)` constructor in the future, or something along these lines anyway. I imagine it would take someone willing to pioneer the effort and to commit to maintaining it :)
"main.rs" is the root file for an executable program, where "lib.rs" is the root file for a library. Lots of crates are structured into a library which implements the top-level API, and an executable which just handles command line args and forwards everything to the library.
What happens at a low level when I pass a struct(let's say a String) to a function? In C a copy would be made, making such an operation potentially inefficient. In Rust, from my very low understanding, it looks as though it would still be effectively passed by pointer, only the ownership would be moved. Is that the case?
Rust used to consider leaking memory to be unsafe, but all of that changed with the Leakpocalypse way back in 2015. If you're interested in learning the history of Rust's shift in stance about memory leaks, I'd recommend reading [Pre-Pooping Your Pants With Rust](http://cglab.ca/~abeinges/blah/everyone-poops/).
I can’t help but feel that the quality of code coming from students is poor, and their teachers are often just as bad. I did go to a shitty university though :), and have suffered through lots of code written by electronic engineers and physicists.
/r/PlayRust
Thank you so much for this! It might make it in a future video ;)
You're very welcome! I hope you found it interesting. Can't wait to see more videos like these down the line.
wrong sub bro
I’d say it’s become fashionable to hate inheritance. I don’t see a problem with it though. Yes you can’t model every problem to it, but so what? A functional paradigm has the same problem. That said, while there is no "inheritance" in Rust, you have everything offered by inheritance, because in the end it’s a useful paradigm. You have dynamic dispatch, static polymorphism using generics, interface inheritance even implementation inheritance using the deref pattern. But yeah, lets not use the O or the I words!
As someone who only uses IntelliJ—RLS *doesn't* do macro expansion? I thought the whole point was that it was supposed to be more like rustc in its interpretations.
&gt; create guis like the software that has his own toolkit (like Blender, MilkyTracker, Kodi, AzPainter, etc). I like this goal a lot. Currently I use imgui and have been looking for a user friendly theme like Unity 2.5 (https://blogs.unity3d.com/2009/05/16/blast-from-the-recent-past-unity-25/). Allow me praise you for this decision and express what's wrong with UI styles in 2019. You can tweak that Unity style for sure, but 3D widgets improve usability and a color scheme that is based on a friendly palette based on shades of grey (NeXTSTEP, Windows 95/NT4 until 2003) or wheat (SGI) works really well. It's no coincidence how Blender looks. The current trend is to have either #FFFFFF or #000000, which is a bad idea on modern non-e-Ink displays. Apple recently introduced dark mode in iOS and completely forgot that iOS had a more egonomic color palette 10 years ago. I tried dark mode in Windows 10 and it's a similar usability nightmare. Why not use all the millions of colors to get more than 2 bits of color in 2019?
What is your criticism/opinion of the blog post I had linked to.
I don't know anything about Loopback but if it's REST and GraphQL you're looking for I recommend you checkout Rocket, Actix and Juniper.
In Rust, a bitwise copy of the struct is made, and the original ignored. If the struct is very large, this can be expensive. Because it's semantically a move, though, not a copy, Rust doesn't need to make a deep copy. So for a type like `String`, you're just copying 24 bytes: length, capacity and pointer to the heap where the contents are stored. This is the same for collections like `Vec` (`String`s are effectively `Vec&lt;u8&gt;`s with different methods). In cases where making a deep copy would be anything other than a trivial operation (e.g. numeric primitives), it's usually necessary to explicitly ask for one with the `clone()` method of the `Clone` trait.
Almost every language that offers inheritance does so with the intention that you *will* model almost every problem with it. And it is optimal for very few of them, if any.
druid is also cross-platform, we have branches doing both Linux and web, though those are not merged into master. We're doing main development on Windows and macOS. It's possible winit could be steered in the right direction, but my real concern is an accumulation of complexity. I did write briefly about "why not winit" below, you can check Rik Arends' [twitter feed](https://twitter.com/rikarends) for ranting against winit. I basically am up for going deep into the platform because I want the experience to be top-notch, and I simply see it faster to get there by writing it rather than trying to steer winit towards what I want to do. This overhaul is a step in the right direction, but doesn't address all my concerns. One requirement in particular is smooth window resizing. I have that working in druid (particularly on Windows), and filed an [issue against winit](https://github.com/tomaka/winit/issues/786) which was closed because apparently the winit maintainers didn't understand what I was asking.
I can’t seem to find the link!
Maybe you need as_ref? Can't test since I'm on my phone.
This isn't a Rust project, but I'm crossposting this here mostly because I thought it's something Rust users might want to use, but mostly because it's damn cool (seriously, read the paper, it's good).
The book has a println, and that’s part of why it doesn’t compile. Without the println, it sees that the two mutable references don’t actually conflict. They’re basically temporaries, so the first one goes away before the second exists.
thank you! I've seen people mention clippy but haven't looked up what it does. I just installed it and a vim plugin too. And crossterm looks great! I'll probably switch to that
Ok it exploded just fine now, can't see the println! in the book though :-/ Thanks by the way.
Which section are you on?
&gt; seriously, read the paper, it's good Agreed. First of all, the explanation of the design of the allocator is very clear, showing the core data-structures and core routines. Secondly, the design is well motivated; each decision is linked to a key benchmark which exposed inefficiency prior to its introduction. Thirdly, the benchmark is also well commented, with explanations for the performance differences observed between the various implementations. I wish all papers were as clear!
References and Borrowing - Mutable References
Thanks. This got me on track, I had to change the call to function-2 to: function-2(t.as_ref()); // convert Box&lt;T+Send&gt; to &amp;T and the signature of function-2 to: fn function-2(t: &amp;dyn T); //avoid Sized complains since this not a concrete impl: use dynamic dispatch Sounds slightly convoluted though to "just" borrow an object on the heap that implements a Trait
Oh! Calling change will also make it break; the point is, you have to actually *use* the mutable reference for the issue to be caused.
`'static` lifetimes are there for when you know that a reference will always be valid. things like `static` values have these kinds of lifetimes attached to them. In the following, ```rust fn main() { static a = "hello world!"; } ``` the variable `a` has a static lifetime. static lifetimes are needed most often when working with multi-threaded code and futures, as you have no way of knowing at compile time how long references across threads will live.
I have a pdf version I got somewhere and I'm watching the online reference as well. The only issue I had is that the books error message seems kind of misleading, I'm referencing my error message below, alongside the one I read on the book. BOOK ERROR `error[E0499]: cannot borrow \`s\` as mutable more than once at a time --&gt; borrow_twice.rs:5:19` `|` `4| let r1 = &amp;mut s;` `| - first mutable borrow occurs here` `5 | let r2 = &amp;mut s;` `| ^ second mutable borrow occurs here` `6 | } | - first borrow ends here` &amp;#x200B; ACTUAL COMPILER ERROR &amp;#x200B; `error[E0499]: cannot borrow \`s\` as mutable more than once at a time` `--&gt; src/main.rs:4:14` `|` `3 | let r1 = &amp;mut s;` `| ------ first mutable borrow occurs here` `4 | let r2 = &amp;mut s;` `| ^^^^^^ second mutable borrow occurs here` `5 |` `6 | println!("{}",r1)` `| -- first borrow later used here` &amp;#x200B; As you can see, the compiler error in the book doesn't show any function being called after variables declaration.
Actually this works too, since it is possible to "relax the constraint on Sized" ([https://doc.rust-lang.org/stable/book/ch19-04-advanced-types.html?highlight=dyn#dynamically-sized-types-and-the--sized--trait](https://doc.rust-lang.org/stable/book/ch19-04-advanced-types.html?highlight=dyn#dynamically-sized-types-and-the--sized--trait)) &amp;#x200B; fn function-2&lt;U: ?Sized + T&gt;(t: &amp;U) &amp;#x200B; Not sure it is much clearee
Also you can do examples/chapter01/main.rs, if you need other files/modules next to the main
So, this *is* a change in Rust; it used to not compile even without using the variable later. If your PDF is old enough, it may be before that change, and so that would explain the discrepancy. The first introduction or chapter or whatever should say explicitly what version of Rust the book targets.
That will be the least of your problems. Considering token sales are scams selling people vaporwear
One use case might be if you want a struct containing some strings, but you don't want to tie its lifetime to anything else with a borrow. If the contents of your strings are only known at runtime, you have to use `String`, which requires memory allocation. But if you know you're only going to put hardcoded strings in there, you could use `&amp;'static str` and avoid allocating. It's not common, but it's possible.
Thanks for posting this. It looks very interesting. Here's a [link to the paper]. [link to the paper]: https://www.microsoft.com/en-us/research/uploads/prod/2019/06/mimalloc-tr-v1.pdf
I wonder how useful this allocator can be for Rust programs, which often do not have "many short-lived allocations".
What is intrinsic about any language that says it has to be compiled or interpreted? Common Lisp can do both, and with enough work I'm sure Rust can too.
Thank you all for the super helpful answers! I am not super familiar with macros in rust so I’m excited to play around with them.
Of course, there is a lot work to do with the documentation. i use those pointers in this way fn main() { // Define the data BEFORE the root container let mut foo = Foo::new(); // Create the container and widgets let mut root_internal = WidgetInternal::new(- arguments -); let mut root_container = Container::new(- arguments -); // Before this let button = Button::new((&amp;mut foo).proxy()); root_container.add_widget(Box::new(button), - other arguments -); // button moved to root_container // Don't move or drop manually the data after defining proxies root_container.draw(&amp;mut root_internal); // draw will use "foo" on Button widget } i implemented it first to unsafe for test if it works and now i will figure how to make it more safer. any suggestions?
Strange, I posted a link in my 1st post, but it didn't appear in the post. I had added it now.
This looks interesting! Do the constant-time tests currently test wasm execution too?
Static lifetimes are tied directly to the program's lifetime. It means that a value can exist until the program finishes. Think of it as the "global" scope for lifetimes.
Since it's so small, possibly a good candidate for porting to Rust, for a production-quality pure-Rust allocation solution?
Currently all constant-time execution tests [run](https://github.com/brycx/orion-dudect/blob/master/.travis.yml) run on a standard Travis-CI `trusty` image. Testing for wasm hasn't progressed past the point of just checking builds in CI, as described in the [RustWasm docs](https://rustwasm.github.io/book/reference/add-wasm-support-to-crate.html). There are however plans to use a [new tool "sidefuzz"](https://github.com/phayes/sidefuzz) which uses wasm to fuzz for timing-related side-channel vulnerabilities.
Many of the features it uses are not available in Rust.
After some iterations, I'd tend to follow this pattern: 1. Start without any kind of lint. 2. When starting the polishing step, paste the code below, but with almost everything on \`warn\` and \`allow\` 3. Duringpolishing, start moving stuff into the \`deny\` block and also refactoring or including \`allow\`s on specific modules/items throughout the code. 4. Finally, trying to uncomment the most pedantic parts of clippy, and also moving as much as possible into the \`forbid\` status block. 5. Also have a separated status for the \`missing\_docs\` since it's so common to break during development iterations and I'd prefer to comment/uncomment it. &amp;#x200B; \`\`\` \#!\[forbid(missing\_docs)\] \#!\[forbid( anonymous\_parameters, deprecated, trivial\_casts, trivial\_numeric\_casts, unreachable\_pub, unused\_results )\] \#!\[deny( bare\_trait\_objects, unsafe\_code, unused\_extern\_crates, unused\_qualifications, // clippy::large\_enum\_variant )\] \#!\[warn( clippy::correctness, clippy::style, clippy::complexity, clippy::perf, // clippy::cargo, // clippy::nursery, // clippy::pedantic, // clippy::all, )\] \`\`\`
You're looking for [http://reddit.com/r/playrust](http://reddit.com/r/playrust)
I think you are looking for r/RusttheGame
What does this have to do with Rust?
The answer to that is detailed in the other top comment...
AFAIK there hasn't been much of any discussion on moving to Futures. It isn't something I'm necessarily opposed to doing, but I haven't played with Futures/async at all so I can't presently assess what the benefits would be or even what a Futures-based API would look like. That being said, it should be possible to build a Futures-based API on top of Winit's current API, so for now I'm inclined to just let people do experimentation in separate codebases and see what pops up.
This is the subreddit for the Rust programming language. You're looking for /r/playrust
Based on my explorations, I think the event loop should be single threaded and synchronous. Adding async increases complexity greatly. There can be async in other parts of the app, part of the UI architecture design is interfacing the two, in particular getting events from the rest of the app into the event loop.
Is it possible to write something like this in rust?
What would splitting the event loop into a different crate accomplish? I'm not trying to be dismissive here - this just isn't something I've ever thought about in the context of Winit, so it's hard to tell how that would affect our API. I've always assumed that higher-level libraries would build atop Winit's event loop, hiding it away as an implementation detail, instead of building alongside Winit's event loop as I'd imagine using a separate event loop crate would encourage. Indeed, that's what I've done when experimenting with higher-level GUI APIs. (btw if that interpretation isn't what you're suggesting, please correct me)
It should be pretty straightforward to build an allocator crate that links to this. Somebody who's feeling really ambitious could also read the paper and reimplement it in rust ;)
If you want to save on some error handling, you can have main return a Result and then use `?` to early return and print and error.
Inheritance is not OOP. It's just one component that can make up part of an OOP language, but doesn't necessarily have to. [this chapter](https://doc.rust-lang.org/book/ch17-01-what-is-oo.html) in the book goes into detail about which aspects of OOP rust does and doesn't have.
What are the most important issues or features you’re looking to work on for the project? I don’t have lots of experience in cryptographic engineering, but it’s something I’m actively learning about and I’d like to try to contribute to the library. I think it’s a fantastic effort, and a great step toward pure-Rust cryptography.
It doesn't. I'd switch, but that family of IDEs runs about as fast as a nosebleed on my PC, with a chance of crashing the whole system.
I don't think we have anything that turn key yet. You're best bet is probably Actix Web, Gotham, Rocket, or another web framework.
This question's a matter of borrowing. I can never seem to get these right with copies ... maybe someone can help. :) pub fn get_name_for_widget_id(widget_id: i32) -&gt; &amp;'static str { let widget_list = WIDGET_REPOSITORY.lock().unwrap(); let widget_name = widget_list[widget_id as usize].widget_name.clone(); String::from(widget_name).as_str() } yields the following error: error[E0515]: cannot return value referencing temporary value --&gt; src/core/widget_repository.rs:152:5 | 152 | String::from(widget_name).as_str() | -------------------------^^^^^^^^^ | | | returns a value referencing data owned by the current function | temporary value created here error: aborting due to previous error Not sure how to copy the data so that a string can be returned...
Ok thanks for the help!
POC clearly here meaning "proof of concept", for anyone thinking otherwise.
Your return type says the function returns a static string, which means it needs to be constant. You can either just return the \`String\` you make from \`widget\_name\`, or return widget\_name directly if it has the \`'static\` lifetime.
&gt;They haven't made a 1.0 release yet. Then again, their 0.20 has had async+await for quite long. Perhaps Rust shouldn't have called what was 1.0 "stable". &gt;It would also seem that they plan on stabilizing without first class support for ADTs What do you mean by first class support for (I guess) algebraic datatypes? Nim seems to support ADTs fine, perhaps I'm missing some detail.
I'm a Rust beginner too. Though I have no experience yet I would advice you to start with some tiny projects to know how could you do something in Rust. Example projects: make a Point(x, y) struct; implement some simple methods for Point e.g find_closest(vec: &amp;Vec&lt;Point&gt;); create and impl trait for Vec&lt;Point&gt; with print() method (not sure if it's a common/good practice though). You could also write a simple TCP server/client chat with crates: bufstream, byteorder. When I first tried Rust 1-2 years ago (because it is trendy, fresh and shiny), I was so confused by all those Arc, Box, Rc, lifetimes etc and was like "it's so overcomplicated toy language I would better stick to straightforward C++ with no complicated abstractions" and uninstalled it. And after some time (multiple times, actually) installed Rust again until next obstacle came up. So after learning basic syntax just try write simple things on your own and google compile time errors when occurred. Also I think it is important to have a handy IDE (IntelliJ Rust plugin) with autocompletion, hints and so on.
There's two cases where `'static` lifetimes arise: In the case of `&amp;'static`, it allows you to express the expectation that the reference will never become invalid. You rarely see that in function interface, but a `&amp;'static` reference fits everywhere where another lifetime is needed. It can be understood as living longer then any other lifetime. The second is in type parameters such as `T: Send + 'static`. Here, `'static` expresses that `T` does not hold inner, non-static lifetimes. So a type like `Iter&lt;'collection&gt;` would be illegal to use here (it is life-time-bound). In most cases, this effectively means that you want _fully owned types_. This is something seen quite often in APIs, as this is a _very useful guarantee_. An example for this is [std::thread::spawn](https://doc.rust-lang.org/std/thread/fn.spawn.html), which requires both generic parameters to be `Send + 'static`, which effectively means that they will be fully moved to and subsequently owned by the spawned thread and the same on the way back.
Bascially, because the event loop is a separate component that could potentially be used in non-winit use cases. Suppose I'm writing some non-GUI component that would be beneficial to bind to an event loop. Would it make sense to optionally depend on a specific event loop? Probably. Would it make sense to optionally depend on a window creation library? Not so sure - you want to depend on the minimum code possible, the least common denominator. Which the event loop is in this case. I maintain bindings to ALSA and D-Bus, and both are Linux only where this would be less of an issue, but they both fall under the category that they deal with file descriptors, and thus would benefit from the asyncness of an event loop. So that's where I'm coming from.
This would be great for the MSVC Rust target, as Jemalloc can't work there even if I wanted a custom allocator.
I might be able to help, but I'll need you to ask a more targeted question. (eg. Give me some examples of what you mean by "all these memory optimizations") While I did program pretty much exclusively in JavaScript-like languages before coming to Rust (Python, Perl, JavaScript, Bourne shell script, Lua, etc.), I gained a theoretical grounding early enough that I only vaguely remember having trouble, and it was back in elementary school when I got access to a C compiler, but the manuals assumed theoretical grounding from prior experience in another language.
jemalloc does work on windows, both 32 and 64 bit, with both MSVC and MinGW
You shouldn't pass immutable `Vec` instances like that. It's best to pass `&amp;[Point]`, as slices are far more flexible. Also, it's not generally good practice to implement that kind of trait on `Vec`, or at least not the specific `Vec&lt;Point&gt;`. If you need to print things, implement `Debug` for your type, and `Vec` will automatically allow debug printing of the sequence of your types. However, overall, implementing some geometric primitives such as points, vectors and so forth is a great way to learn Rust, especially when it comes to implementing the standard ops for them such as `Add`, `Sub`, `Mul`, and so forth.
Was that a recent change? I tried it about six months ago to no avail. Might be worth looking into if that was improved.
The way to declare an object as mutable is ‘mut obj: Obj’ not ‘obj: mut Obj’
Agree with the other answer, just want to point out that one thing to watch out for is that arrays (e.g. `[i32; 42]`) are copied when they are parameters or return values, so if you have a large array you'll be copying quite a lot of bytes. On the other hand, slices (`&amp;[i32]` and `&amp;str`) and owned types (`Vec&lt;i32&gt;` and `String`) are only 16 and 24 bytes respectively, regardless of the size of the collection. Regular references (`&amp;T` and `&amp;mut T`) will be 8 bytes.
This was a beautiful read. Thanks for the link!
Done: * https://crates.io/crates/mimallocator * https://crates.io/crates/mimalloc-sys
Yup, looks like this is correct. I had to run a `clone()` operation on the widget_name, but this definitely corrected it. Thanks for the help!
No, jemalloc has supported windows for a very long time. The Rust bindings to jemalloc do not support all possible windows environments, but that's because no one has worked on that.
This is the subreddit for the Rust programming language. You're looking for /r/playrust
&gt; What are the most important issues or features you’re looking to work on for the project? There are a variety of things that need attention. First of all, the high-level API relies on PBKDF2 at the moment and replacing this with an Argon2i implementation would certainly be a very nice thing to do. Similarly I've been told that some people are interested in provided optional SIMD support, which also needs to be discussed. Supporting more primitives would be nice in general. But again, there's always design, testing, etc. Depending on what you're interested in, we can go into more detail. &gt; I don’t have lots of experience in cryptographic engineering, but it’s something I’m actively learning about and I’d like to try to contribute to the library. I think it’s a fantastic effort, and a great step toward pure-Rust cryptography. That's great and thank you so much. I'd love to try and work together with you on it.
I'm very new in rust but from what I can see you aren't declaring the obj variable as mutable in the let statement. Shouldn't it rather be: struct Obj { x : i32, } fn test(obj: &amp;mut Obj) { obj.x = 1; } fn main() { let mut obj: Obj = Obj {x : 4}; test(&amp;mut obj); }
ayy, nice! might want to make a top-level post too.
I'm a newbie to this, so I'd love to know more... What specifically would keep it from being ported?
It's not much, but I would probably make these changes: let running = Arc::new(AtomicBool::new(true)); ctrlc::set_handler({ let running = Arc::clone(running); move || { warn!("Interrupted"); running.store(false, Ordering::SeqCst); } }) .expect("Error setting Ctrl-C handler"); and if let Err(error) = result { error!("{}", error); for cause in (&amp;error as &amp;Fail).iter_causes() { error!("Caused by: {}", cause); } std::process::exit(1); }
I actually sent in a PR recently fixing 64-bit GNU ABI for Windows, but MSVC has been a harder deal. I haven't needed to push on it recently, but /u/gnzlbg_ (who would probably also field your PR to the `jemalloc` Rust lib!) is right -- it just needs somebody to figure out the build details. `jemalloc` DOES work on Windows, we just need to get it to cooperate with Cargo.
Everything if it means another allocator available to the Rust ecosystem! ;)
I'm curious how async await will end up affecting Gtk code.
Where did that awesome picture come from? 😀
Could you expand a bit on that? I've tried researching C attributes but fail to understand how they are imperative to implementing this allocator in Rust.
I don't see any that would prevent a port, but when I was working on the mimalloc-sys and mimallocator crates I saw that the \[mimalloc.h\]([https://github.com/microsoft/mimalloc/blob/master/include/mimalloc.h](https://github.com/microsoft/mimalloc/blob/master/include/mimalloc.h)) file does use a lot of C attributes with optimization hints for C, that are just not available in Rust. This goes from marking the return of a function \`noalias\` in LLVM, to other hints about how the parameters passed to the allocation functions map to the size and alignment of the memory regions returned, to some more platform specific stuff.
&gt; This goes from marking the return of a function `noalias` in LLVM, Doesn't Rust already do stuff like that, though? C needs it because of it's weak aliasing rules, but Rust has fairly strong rules and emits stuff like that as part of optimizations?
/r/rustjerk
Love the addition of the Builder facilities. Each release this definitely gets more and more Rusty. For my use case though I'm still confused on what the best way to structure an application and manage its state is. As this library and GTK has evolved it's not quite crystalized from what I've seen what the best practices are here. For example, I see that one can subclass GtkApplication. However, is that something all apps should do as a best practice? mmstick has [their gtk tutorial](https://mmstick.github.io/gtkrs-tutorials/), but it's incomplete and I assume out of date at this point. I've looked for GTK3 books out there, but there aren't any well-reviewed ones from this decade.
Ah, so you're talking about extracting all the window-creation bits into a separate crate, making a crate that specifically handles talking with the OS to set up the event loop, and building Winit on top of that? That's... an intriguing idea. I'm not sure if it's practical on Windows though, since a significant portion of Winit's event loop implementation on Windows focuses just on dealing with the weird cases that arise when the windowing system interacts with the event loop system. It's hard for me to imagine how you'd separate the two in a clean way. It looks like you've got some experience with the Windows API, so you'd probably find the reasoning for that interesting. At a high level, if the user resizes a window Win32 doesn't return from the `DispatchMessage` function until the resize operation is finished, dropping into a modal loop while that operation is happening (this may happen with other operations too, but off the top of my head I'm not aware of any). Windows doesn't provide any standard methods for detecting the start of a modal loop, so we've got to special-case window resizing to handle it properly. You can look at the [event loop code here](https://github.com/rust-windowing/winit/blob/master/src/platform_impl/windows/event_loop.rs) and ctrl+f for `modal` for the exact details on how we deal with that problem.
thank you! won't make the mistake again
Hmmm, maybe I'll try taking a stab at this today and tomorrow.
Can anyone comment on whether mimalloc's strategy is compatible with the [Mesh allocator strategy](https://github.com/plasma-umass/Mesh)? My intuition is that meshing is fundamentally incompatible with maintaining locality, but I'm outside of my area of expertise. I ask because [mimalloc's README](https://github.com/microsoft/mimalloc) makes claims about reduced fragmentation that might be true but are never supported. In fact, [the mimalloc paper](https://www.microsoft.com/en-us/research/uploads/prod/2019/06/mimalloc-tr-v1.pdf) mentions fragmentation only once and in a way that indicates mimalloc's fragmentation performance may not be as competitive as its speed performance: &gt; VAM pioneered the idea of **prioritizing application reference locality over reducing memory fragmentation** and our sharded free list design improves on VAM’s original design.
You can either change the parameter declaration to `mut obj: Obj` or make the first line of your function `let mut obj = obj;`.
I finally posted a [blog post](https://raphlinus.github.io/rust/gui/2019/06/21/smooth-resize-test.html) that's been in the queue a while, I hope it explains this in more detail.
Hi guys! I have a question, which of the following code is more idiomatic? `match rg.captures(&amp;line) {` `Some(group) =&gt; {` `match group.get(THREADNAME_RGX_GROUP_INDEX) {` `Some(thread_name) =&gt; {` [`ti.name`](https://ti.name) `= thread_name.as_str().to_string();` `},` `None =&gt; {}` `}` &amp;#x200B; `match group.get(THREADPRIORITY_RGX_GROUP_INDEX) {` `Some(thread_priority) =&gt; {` `ti.priority = thread_priority.as_str().to_string();` `},` `None =&gt; {}` `}` &amp;#x200B; `match group.get(THREADID_RGX_GROUP_INDEX) {` `Some(thread_id) =&gt; {` [`ti.id`](https://ti.id) `= thread_id.as_str().to_string();` `},` `None =&gt; {}` `}` &amp;#x200B; `match group.get(THREADNATIVE_ID_RGX_GROUP_INDEX) {` `Some(thread_native_id) =&gt; {` `ti.native_id = thread_native_id.as_str().to_string();` `},` `None =&gt; {}` `}` `},` `None =&gt; {},` `}` vs something like this: &amp;#x200B; `match rg.captures(&amp;line) {` `Some(group) =&gt; {` `if group.get(THREADNAME_RGX_GROUP_INDEX).is_some() {` `ti.name = group.get(THREADNAME_RGX_GROUP_INDEX).unwrap().as_str().to_string();` `}` `if group.get(THREADPRIORITY_RGX_GROUP_INDEX).is_some() {` `ti.priority = group.get(THREADPRIORITY_RGX_GROUP_INDEX).unwrap().as_str().to_string();` `}` `if group.get(THREADID_RGX_GROUP_INDEX).is_some() {` `ti.id = group.get(THREADID_RGX_GROUP_INDEX).unwrap().as_str().to_string();` `}` `if group.get(THREADNATIVE_ID_RGX_GROUP_INDEX).is_some() {` `ti.native_id = group.get(THREADNATIVE_ID_RGX_GROUP_INDEX).unwrap().as_str().to_string();` `}` `},` `None =&gt; {},` `}` &amp;#x200B; I am basically trying to avoid those empty None match guards since I am not interested on doing something with them.
This blog post goes into a little more detail how I solved the problems of smooth window resizing in druid, with guidance how to avoid problems also when using high performance 3D rendering pipelines. It touches on one of the major reasons I did not use winit for the window creation in druid, though there were other concerns (some of which are described in the [druid-shell roadmap](https://github.com/xi-editor/druid/issues/16). I suggest that part of the "areweguiyet" effort should test how well the proposed UI toolkits handle window resizing, as I suggest that reveals quite a bit about how well the toolkit can solve these thorny problems. I also hope that what I've learned is useful to other UI efforts, especially those written Rust, where I think there's a good opportunity to engage the platform at a low level and deliver a polished, high performance experience.
I'd go with [if let](https://doc.rust-lang.org/rust-by-example/flow_control/if_let.html).
This question is about the Rust C FFI. Quite simply, if you would like to write an interface to a C function that returns a pointer to a foreign C struct, must you re-declare the structure in Rust? From articles and documentation I think the answer is yes, so is there a tool to help automate the conversion of the C declaration into Rust code? Porting large structs by hand can be tedious and error prone.
Your formatting is a bit messed up, but the second one is OK, but better like this: ``` if let Some(group_index) = group.get(THREADNAME_RGX_GROUP_INDEX) { ti.name = group_index.as_str().to_string(); } ... ``` so you don't need the unwrapping and duplicate gets.
Just a curiosity - what's the type of `WIDGET_REPOSITORY`? You might be able to get away without the clone.
# Thanks for the reply, perhaps I haven't understood correctly the if let clause but if I understand correctly, you use if let when you know what you are going to get, right? How could I use if let here?
Thank you, this is what I was looking for. Inderstand if let now, thanks.
https://github.com/rust-lang/rust-bindgen
Exactly. And my clause conveys more detailed info.
They are conditionally defined. You don't need them to write a Rust port.
All of those attributes are conditionally defined and are not necessary to use the library.
Haha this is great. Thanks
What about X or Wayland?
Thank you.
I just ported some two-year-old code to `.await` syntax, and I thought it might be interesting to share my experience on both my blog and here. I've always found using the `await` syntaxes quite cumbersome to setup and use, but now with `runtime` it suddenly became really clean!
It's a good question, but I haven't dug deeply yet into either. I suspect that traditional X will work well, but that trying to do swapchains with 3D APIs (both OpenGL and Vulkan) will be problematic for similar reasons. I'd love to have a Linux owner on druid, and this would be one of the first fun explorations of many for that role :)
MIT licensed, from Microsoft. So no patent waivers, unlike with Apache 2 or GPLv3. Microsoft may come and get you with patent claims later if you use this.
You can totally do unsafe things with multiple mutable references from the same thread. Lots of example [here](https://www.reddit.com/r/rust/comments/95ky6u/why_arent_multiple_mutable_references_allowed_in/), some more [here](https://manishearth.github.io/blog/2015/05/17/the-problem-with-shared-mutability/).
I'm very interested in working on this as well. Let me know if you're open to contributors!
Right. So the reason that this is a problem in the first place is that you want to send `EventsCleared` and `NewEvents` even when you're inside the modal loop? My main loop does not care about notifying the user about these two events, so it does not matter how many levels of modalness you're into.
I got baited seeing a thumb of Torvalds on a post with Erlang and Rust in the title. This must be good!! On the bright side, I did get to watch a nice talk on rust and erlang interop. Message passing erlang terms with enums is super cool!
 let mut arr = vec![1, 2, 3, 4, 5] let a_ref = &amp;mut arr // slice { len: 5, addr: something} arr.pop(); // length now 4 a_ref[4] = 2; // possible UB There are tons of better examples linked below, but I think the one above can also get the point across. The above example also works with one mutable and one immutable reference, and bad things can still happen
Could you clarify what you mean as the difference between use and consume, and how you would tell the difference?
Oh boy...I say “um” pretty much every other word. No matter how hard I try, it’s a tough habit to break. 😆
I agree just based on the title. Could probably give a talk like that on my own. Good to know that now I don't have to.
Thank you! It feels like quite a Rust "click" moment reading through that. &amp;#x200B; I'm still working out how to explain it in my head, but it seems like Rust "fixes" this aspect of C++: std::map&lt;int, Foo&gt; foo_map; std::vector&lt;Foo&gt; foo_vec; // ... Foo&amp; f1 = foo_map[1]; Foo&amp; f2 = foo_vec[1]; foo_map[2] = Foo(); foo_vec.push_back(Foo()); // f1 is still valid, f2 may not be In C++, we just have to "know" reference stability by reading documentation on the structure. Rust enforces this in the type system, which is pretty cool.
That's part of it, yes. Also, you can invalidate data even without invalidating references to it.
Yes thanks, this was the example that stood out to me as the clearest in that link, especially coming from C++.
Great piece. My Google Chrome window on Debian Linux with XFCE jutters unbelievably with this test: never noticed before. Firefox lags bigtime but looks otherwise OK. Gnome-terminal behaves similarly, so I'm guessing it's the WM at fault here possibly. I'll try to poke at the Linux thing a little: good excuse to play with `druid`. Offhand, I think the `PRESENT` X extension that Keith Packard introduced recently can be used to do something similar to the Windows thing. But I'll ask him.
Downvoted for saying thanks
`'static` means that the lifetime will never end. Once you have a reference with that lifetime, you can count on that reference to always be valid to access. It's most commonly something that exists for the entire lifetime of the program, like string literals, but [`Box::leak`](https://doc.rust-lang.org/std/boxed/struct.Box.html#method.leak) allows you to create a `&amp;'static` reference because once the value has been leaked nothing can deallocate it anymore.
https://gitlab.redox-os.org/redox-os/ralloc is a thing
This article seems to have a different interpretation: https://opensource.com/article/18/3/patent-grant-mit-license
No, it does not support ADTs at all. There's a third party library that attempts to emulate them with compiler macros creating heap-allocated objects, but it's a poor replacement to having genuine ADTs in the core language based on tagged unions, and for the core and standard library to be designed around it. Imagine Rust if its core and standard library was developed without `Option` and `Result`, or any similar types.
Could we get the title fixed to include \`gtk-rs\` somewhere? It's not obvious what this is about on Reddit frontends where the URL isn't shown.
Is this FUD or do you know that Microsoft actually holds patents on any of the involved tech?
Wow, that's cool. It's not obvious to me why the technique wouldn't be compatible.
Best practices are not well documented. You should subclass something when you have internal state. Say your application needs a CSS provider, or you have application actions that need handling. You will help yourself by subclassing GApplication
I don't know why you are being down voted. The open source dot com references a 1927 supreme court which isn't _about_ software. Stern &amp; Allen wrote an extensive brief in 2013 that asserts there is no patent rights. It is mentioned in this 2015 overview of Intellectual Property Law https://legacy.pli.edu/emktg/201729_CHB_Index_2015-2016.pdf Also. Why do these other posters think Facebooks's BSD + Patents license exists? Like if it wasn't an issue I'm pretty sure the company with dozens of lawyers who specialize in this very subject wouldn't sign off on adding a patent clause.
I wonder if resizing of the Windows is such a critical task such it needs full focus and attention. I tend to mostly think that resizing is going between size A to size B, and what happens in between does not matter for me personally. It was an interesting write up tho.
Sadly titles can't be edited on Reddit
Genuine, non provocative question: why isn’t the Rust community contributing stuff like this? Its sole purpose is to hand out memory, the safety of is held as utmost priority, is it not?
Using `?Sized` is generally the cleaner approach, even if slightly less clear if you're not used to it. It signals that you can accept dynamically-sized values - many functions in std taking `&amp;T` use this too.
No, it can’t do that, because it does not have that information. It only knows about FFI functions what their signature says, and if it does not say that a pointer is noalias, then it does not know that. You can use the ‘#[allocator]’ attribute to make a function return pointer noalias, but AFAIK there are no equivalents for the use_alloc and other attributes.
 Mutex&lt;Vec&lt;WidgetContainer&gt;&gt; The issue is that there's recursion in the code, and that requires direct access to the widget store. Therefore, this approach will not work. Sigh.
Rust allocators do exist.
 - I agree, I'd split up the `run()` function a little bit. The comment blocks help you a little bit :) "Get MIDI IO", "Forward Midi", and "Send via UDP" might be good places to start splitting it up. - Struct and impl code looks good to me. It's completely up to you (I don't think there's an "idiomatic" choice one way or the other), but you can use `-&gt; Self` instead of `-&gt; StructName` for things like `new()`. - Clean up all of the commented out code (`// use std::{env}`, etc) - Some other data structure might be *theoretically* better for storing the active notes (maybe a hash set?) -- you'd have to do benchmarks, but your removal is currently `O(n)` -- that would probably only matter if the user is playing a *bunch* of notes at the same time though. - I see a decent amount of type annotations that you don't really need (it's up to you -- if it's clearer to you, go for it) - Like /u/ecumene4000 said, `clippy` will help with idiomatic usage, and also `cargo fmt` will help with formatting (although your code looks pretty well-formatted for the most part) Overall, it looks pretty idiomatic and readable to me though. Good work! PS: it sounds like you might also be interested in [`vst-rs`](https://github.com/RustAudio/vst-rs), a VST2 implementation in Rust. :)
I am writing a lower level, transparent proxy which works directly with tcp and udp connections. It has the same goals as your proxy + performance. I will surely add your proxy to the test cases.
That's what I assumed, so thanks for clarifying. Do you have any examples that you suggest I look at or reference documentation that you do know of?
Now that is a fucking api. I wish more crates out there gave you this kind of plug in, use only what you want architecture.
That was actually the point of my tea analogy, which I wasn't sure whether to include. Badly made tea is perfectly functional as well, will hydrate you and give you that mild stimulant kick. But I wouldn't eat at a Chinese restaurant that served it. I know my priorities aren't the same as everyone, but I'm personally that Rust is a language where you *can* express UI concepts with the level of platform control I discuss in the post.
5PM UTC happens when this comment is 17 hours and 16 minutes old. You can find the live countdown here: https://countle.com/wb7qjgxUn --- I'm a bot, if you want to send feedback, please comment below or send a PM.
I was referring to operations that don't consume the value but instead take it by reference. `result.is_ok()` will silence any `#[must_use]` warnings, even though it does not consume the result. If it were a linear type this wouldn't compile.
One can always use \`Option&lt;&amp;mut T&gt;\` to get noalias annotations.
&gt; isn't about software Do patents not apply to software? &gt; and hasn't been tested in court. *Most* licenses aren't tested in court. The ideal is that they never are. Ideally people either accept the license or don't use it, rather than accept it and then sue when they don't like the terms they accepted. [The GPL was first written in 1989 and wasnt tested in court until like 2003.](https://www.theregister.co.uk/2003/10/28/sco_says_gpl_unenforceable_unconstitutional/) The GPL version Linux in particular uses, GPL2, was published in 1991. &gt; Why do these other posters think Facebooks's BSD + Patents license exists? To be extra super expressly clear, just in case, probably. Even if they already have the right, it hurts nothing to expressly lay it out. Lawyers are like that. Kinda like how you automatically get copyright when you create something, you don't have to register it or anything. But registering it can help proving it in court. The fact remains there *is* precedent for opensource.com's position. Whether it plays out in court is a different matter, but ideally it doesn't go to court, and if you're thinking like that *anything* can happen in court. They could declare the entire thing null and void, patent clause or not.
So at least to answer the parallel question you'll want to take a look at (https://docs.rs/ndarray-parallel/0.9.0/ndarray_parallel/)[https://docs.rs/ndarray-parallel/0.9.0/ndarray_parallel/] . The plan is for this crate to be pulled into the ndarray crate itself during the next big release and have it behind a feature flag.
Bookmarked, I might have some time to tackle the x11, macos issues after siggraph. Wanted to contribute to rust projects for a long time anyway. :)
There’s some code that I’m repeating a lot that I don’t think I have to. How do I make a 3x3 array of cards in a separate module? Right now I have: [[super::card::Card::empty_card(), super::card::Card::empty_card(), super::card::Card::empty_card()], ... Is there a better way to make a 2d array filled with the same element?
Here is a simple example of how it could go wrong: let mut foo = Ok("hello"); let inner_ref = match &amp;mut foo { Ok(v) =&gt; v, Err(_) =&gt; unreachable!(), }; foo = Err("world"); // This could panic or not,depending on optimizations assert_eq!(*inner_ref, "hello");
Oh neat! Thank you!
Here are 3 solutions for a 3x3 matrix: macro_rules! make_matrix{ ($expr:expr)=&gt;{ [ [$expr,$expr,$expr], [$expr,$expr,$expr], [$expr,$expr,$expr], ] } } fn make_matrix_copy&lt;T&gt;(val:T)-&gt;[[T;3];3] where T:Copy { [[val;3];3] } fn make_matrix_clone&lt;T&gt;(val:&amp;T)-&gt;[[T;3];3] where T:Clone { let row=[val.clone(),val.clone(),val.clone()]; [row.clone(),row.clone(),row.clone()] } fn main() { println!("{:?}",make_matrix!("hello")); println!("{:?}",make_matrix_copy("hello")); println!("{:?}",make_matrix_clone(&amp;"hello".to_string())); }
It's not a critical function. OP is using it as a proxy for the GUI toolkit's overall quality. Resizing involves many subsystems and is difficult to get right
Thanks!
Oh nice :)
Thank you !
Is it possible for NIFs to operate directly on Erlang binaries/Elixir strings? In essence, can the BEAM to pass "ownership" to the NIF or is everything always copied?
For anyone (like me) that doesnt know, [BEAM](https://en.wikipedia.org/wiki/BEAM_(Erlang_virtual_machine)) is the Erlang / Elixir virtual machine.
Here's a cleaner way to make your socket addresses const PORT_FROM: u16 = 57000; const PORT_TO: u16 = 57001; let my_host_name = SocketAddrV4::new(Ipv4Addr::LOCALHOST, PORT_FROM); let destination_host_name = SocketAddrV4::new(Ipv4Addr::LOCALHOST, PORT_TO); You could use slice patterns to destructure the midi message; if let [midi_event, pitch, velocity] = *message { let pitch_class = pitch % 12; if (midi_event &gt; 127 &amp;&amp; midi_event &lt; 144) || velocity == 0 { // note off events hk.remove_note(pitch_class); } else if midi_event &gt; 143 &amp;&amp; midi_event &lt; 160 { // note on events hk.add_note(pitch_class); } } else { println!("Unexpected message: {:?}", message); } You can let main return a Result and just inline the run function, and when an Err gets returned from main it will print it and exit with an error code.
In my experience, GUI toolkits are either thoughtfully designed by people with experience and strong opinions and are nice to use, or a pile of garbage cobbled together by a team lacking experience that regularly compromised which leads to every program built with them looking clunky even despite app developers sometimes extreme efforts. A modern example is iOS and Android, where one has a well thought out designed UI toolkit the other is a glitchy mess built on layers of leaky abstractions that over the years have received patch after patch to remedy the situation and is still glitchy because it's built on poor fundamentals. In iOS's cases the design carried down from NeXTSTEP in the 80s and 90s, which really goes to show how good designs originating from the 80s and 90s bore so much fruit two to three decades later. The UI toolkit developers who focus on functionality alone first and then come back to architecture to iron out these often overlooked issues, usually find they've coded themselves into a pattern that's impossible to change at that point without starting from scratch. &amp;#x200B; To me tests like this are a way for a GUI toolkit developer to prove to themselves they got it 'right' and useful for prospective app developers to figure out which camp a GUI toolkit belongs in before the build their app with it. And as an app developer, using better toolkits aren't only much prettier but easier to use with fewer functional bugs, a true win-win. &amp;#x200B; Props to focusing on this stuff ralphlinus.
I believe `std`'s floating point formatting and parsing does guarantee it will correctly round-trip. As I understand it, formatting floats is fast, but std's float parsing is somewhat slow (and can fail on particularly long strings possibly produced by other langauges/formatters: https://github.com/rust-lang/rust/issues/31407). This is the last post I know of with a lot of discussion about this: https://www.reddit.com/r/rust/comments/9vdsyl/rust_float_parsing_is_atypically_slow/
You're 99% of the way there -- mod gl { include!() }
&gt; I do think it would be worth while to write up a more in-depth experience report on our use of Rust and especially async/await. We'll try to do a post on this sometime in the next couple weeks. That's great! Looking forward to reading about your experiences.
hey thanks! that works...for gl-rs, is it normal to have to dig into some path like: `mod gl { include!("../target/debug/build/gl-aa90ab3ecc411c34/out/bindings.rs"); }` seems a little odd, but that's what mine looks like, at the moment...
I still could not figure out why the glitch happens. If the triangle's position is independent of the window size, then all frames rendered when resizing should be the same?
Does cargo have support for opening examples from crates or do you have to manually go to the examples/ directory in the source code to view them?
I do not recognize the name. Could anyone explain what this person is famous for?
It's unfortunately not possible at the moment - `Stream`s have to be implemented by hand and can not utilize `async fn`. Whether it's possible to change this in the future is unclear. You can work around it by defining a different `Stream` trait which makes use of `Future`s like: ```` trait Stream&lt;T&gt; { type NextFuture: Future&lt;Output=T&gt;; fn next(&amp;mut self) -&gt; Self::NextFuture; } ```` [This article](https://gist.github.com/Matthias247/5e5e7430149bbb04eebf18cf31747fe0) and [this futures-rs issue](https://github.com/rust-lang-nursery/futures-rs/issues/1365) have more information around it.
His recent notable work includes the Noria research in database development.
Oh, that one is really cool!
Calling a NIF with a binary parameter gives you a pointer to that binary. So you can actually mutably change it, something Erlang does not allow you to do.
Here's an example even with _arrays_, no dynamic allocation at all: let my_array = *b"valid unicode"; let my_str = str::from_utf8().unwrap(); my_array[0] = 0xff; // woops! In fact the last line doesn't compile, because it mutates while a shared borrow exists. But if it did compile, it would violate the invariant that all str slices are valid unicode. You could do similar tricks to create bools that aren't 0 or 1, or enums that aren't a valid variant. All of these trigger undefined behavior.
As far as documentation, I always refer to the GNOME docs website. I generally try to use code when I have a much more particular question. I refer to high quality C and Python GTK code bases. I don't program in Rust too much. I'm still waiting for Rust and GTK to mature before I go all in. I have 2 C-based projects that you could reference for some GObject, GTK stuff if that would help. I also suggest reading the GObject GTK blog posts when the Rust devs put them out.
This sub is for the Rust programming language, the Rust game is /r/playrust
Are software patent a thing outside of the US and Japan yet ? In the UE, they cannot be enforced, and a decade ago it was also the case everywhere in the world (but Japan &amp; the US) but Idk if it changed.
How far off are they from the llvm port being complete? Super keen! Thanks for the dockerfile (https://github.com/ctron/rust-esp-container/blob/master/Dockerfile)!
Rather than always waiting for a new frame to be rendered at the correct size, the windowing system would just scale the last frame rendered until a new frame was available. So the triangle ends up stretched by a different amount as new frames finish rendering, causing the jitter.
Thanks. That makes sense now.
The low footprint sounds like a sweet spot for wasm!
 Window is being resized ---&gt; Width: 250 | 251 | 252 | 253 | 254 | 255 | 256 | 257 | 258 | ... Compositor: \ /=Composite=to=256=width=window=| Application: \=Render=to=250=width=surface==/ By the time the application is done rendering to its surface the window may be a different size. The compositor will either stretch the surface or overlap/fill the area. The gif in the article demonstrates the stretching option; the entire window surface is being stretched slightly left to right during the resize.
Interesting (&amp; good to know)
Does the rust compiler optimize away `fn function() -&gt; MyStruct` to `fn function(retVal: mut &amp;MyStruct)`? It seems to have the same semantics, just without a memcpy.
No, the function signature will not change. However, inlining &amp; copy elision let the compiler generate mostly the same code (just without taking a reference).
#[allocator] also passes a size hint
thanks. so as per my understanding, main.rs will be the first point of execution when I execute tool like a binary (. /tool) and lib.rs will be when I import my code as library (use tool)
Great, thanks for the update and the suggestion on how to get around this!!
In addition to https://github.com/rust-dev-tools/fmt-rfcs/blob/master/guide/guide.md and https://rust-lang-nursery.github.io/api-guidelines/, which I both highly recommend, there was a post about Facebook's new internal guidelines for using rust a while ago: https://developers.libra.org/docs/community/coding-guidelines (from [this reddit post](https://www.reddit.com/r/rust/comments/c20aed/facebook_just_picked_rust_to_implement_their_new/)).
https://manishearth.github.io/blog/2015/05/17/the-problem-with-shared-mutability/
Good animation on how shitty the macos GUI is and a good description of how windows UI has actually deteriorated with windows 10's new toolkit.
This was cool to read. I'm curious how access to the CPU registers actually ends up working behind that macro, I'll have to look into that when I have a chance. Thanks for writing this up, I'm looking forward to the rest of the series :)
Thanks for the feedback :) Not sure I understand your question, are you talking about where the actual reading of registers take place? [For example in this interrupt handler](https://gitlab.redox-os.org/jD91mZM2/kernel/blob/ccaaf08e8346ae20621d192a4d735b69d9c12b7b/src/arch/x86_64/interrupt/irq.rs#L65-73) the pointer to the registers is set, and later used [by the proc: scheme](https://gitlab.redox-os.org/jD91mZM2/kernel/blob/ccaaf08e8346ae20621d192a4d735b69d9c12b7b/src/scheme/proc.rs#L143-158), and "save" refers to a function back in the [InterruptStack struct in macro.rs](https://gitlab.redox-os.org/jD91mZM2/kernel/blob/ccaaf08e8346ae20621d192a4d735b69d9c12b7b/src/arch/x86_64/macros.rs#L220-253). Writing works the same way thanks to the fact that all the values in memory are later popped back by the restore mechanism.
It can feel really nice. The first thing I noticed when trying out kwin_wayland was how smooth Konsole resizing was, it felt like a full 60 fps unlike on X.org where it felt kind of juddery.
I have seen many of his videos, but couldn't find out much about his background. Is he "just" a really active, really good rust user, or part of the lang team/other team? Any Info on this?
/r/playrust is where you want to repost this
Thanks for letting me now
doesn't taking a trait object by reference already imply that?
https://nim-lang.org/docs/tut2.html#object-oriented-programming-object-variants
I'm not familiar with the technology, but does the board have an svd file? You can generate functions that handle the details of writing to peripheral registers, and then implement embedded_hal on top of that to get access to the embedded ecosystem. Then, if you have e.g. an Ethernet adapter, you can have a driver crate that is generic over the transport and you're off in pure rust :)
Rust &amp; Erlang are two of my favorite languages. This is why I think [pony](https://www.ponylang.io/discover/#what-is-pony) might be the next language I learn, as it seems like a marriage of some of the key ideas of both.
It doesn't - particularly, `T` could be used elsewhere as a value as well. For instance, we could have fn takes_by_ref&lt;T: Default&gt;(thing: &amp;T) { let other_thing: T = T::default(); } The compiler _could_ interpret the traits first and see if any require `T: Sized`, and then combine that with knowledge of how it's used in parameters, but that might be considered too implicit for something as important as something being unsized and unstorable on the stack. I guess that the simpler rule "all parameters are Sized unless declared as ?Sized" was chosen instead for that reason?
Seeing this really made it clear how cool schemes are. Why trying to come up with a fancy system call interface when you can just read everything like a file?
AFAIK SVDs are generally found in ARM MCUs, not proprietary Xtensa ones :( I think there are some high level register descriptions, but no details on individual fields. Same with the BLE and WiFi hardware, meaning no open source stacks could be written for them without heavy reverse engineering.
Thanks for the help, the book refers indeed version 1.31.0 and mine is 1.35.0. Thanks again!
Any project that provides both a library and a binary is likely to have both. Even for binary only projects, a possible benefit of making it a library is ease of unit testing, by having a proper api *to* test.
Sure. You can have a library implementing something and a cli/gui application that calls into that library. IIRC the book even recommends such a setup, claiming that a really thin main application on top of a library is easy to write integration tests for.
Arf, I didn't pay attention when creating the post. Sorry about that...
I like this, but at the end I felt kind of cut-off, like I missed half the post. More examples might be nice, or some kind of conclusion of how the translation went in general? In any case, I'm glad to see success with the new async story, and this looks like a pretty awesome project.
Another way to think about it is that having both `lib.rs` and `main.rs` in that configuration is equivalent to the Python `if __name__ == '__main__'` idiom for writing modules that can be `import`ed or executed directly.
Mods can add tags though.
In addition, he does a lot of rust live streaming which is quite popular
A common pattern, even for binary-only crates, is to declare a "library" providing the majority of functionality, and then have `main.rs` just implement argument parsing and use that library. This allows for integration tests written against the library, which might otherwise be hard to do with only `main.rs`.
I have a crappy CLI project that's my first go at Rust. I have a `lib.rs` file because I thought I needed it so that I could import code in my modules via `use crate::BLAH`. What am I doing wrong? For reference, here's the code: https://github.com/thundergolfer/slackify-markdown
This makes me sad :(
Me too, open source RISCV is where it’s at
Unless you intend your project to be usable as a library by third parties, you can just remove src/lib.rs, remove the `extern crate` and corresponding `use` statements from src/main.rs, and [just declare the sub-modules right there](https://doc.rust-lang.org/1.30.0/book/2018-edition/ch07-01-mod-and-the-filesystem.html#moving-modules-to-other-files): diff --git a/src/lib.rs b/src/lib.rs deleted file mode 100644 index 80a9031..0000000 --- a/src/lib.rs +++ /dev/null @@ -1,3 +0,0 @@ -pub mod slackdown; - -mod escape; diff --git a/src/main.rs b/src/main.rs index 70b0a4e..6004e75 100644 --- a/src/main.rs +++ b/src/main.rs @@ -1,6 +1,5 @@ -extern crate slackify_markdown; - -use slackify_markdown::slackdown; +mod slackdown; +mod escape; use pulldown_cmark::{Options, Parser}; use std::io::{self, Read};
You could implement a rust native computer vision library.
Yup, this is possible. I doing this in my [webdav-handler](https://crates.io/crates/webdav-handler) crate. See the [async_stream](https://docs.rs/webdav-handler/0.1.1/webdav_handler/async_stream/) module. Perhaps I should publish that as a seperate crate.
Maybe continue the work on integrating a modern GC in Rust.
That sound potentially interesting. Do you know what kind of work has been done on this so far? I'm also curious what would be the use case for a garbage collector in Rust. I had the impression that the language was designed around the notion of not having a garbage collector, and using a garbage collector would defeat the purpose.
You should take a look at the [gen-stream](https://github.com/vorot93/gen-stream) crate.
Be ready to defend your thesis, Jon
Rust allows expressing all kinds of different allocation strategies and depending on the situation one may be more appropriate than the other. Theoretically one could implement a garbage collector design for Rust that is similar to async fn and generators. You'd cooperatively yield whenever you want the garbage collector to run. Unfortunately we'd need a bit of help from the language as similar to how async fn implements the Future trait, this would need "gc fns" to implement a Root trait, which is not something you can express solely via crates.
Try searching for locality instead of fragmentation.
Uh... isn't this counter off by 6 days? It's set up for 17:00Z this Sunday, rather than next Saturday.
It seems like the analogy breaks down a bit, though. The recurring theme of the various sections that I saw is "doing it this way is reasonable for (performance, simplicity, etc.), but to get _smooth window resizing_ right you need to do this other thing." There's no generalization from the indicator to overall benefits.
The bot doesn't seem to know what days are, only timezones.
I’ll try this, thanks!
Is f1 still always valid here? I’m unfamiliar with the C++ stdlib, but I’d assume that setting a value in a map could cause its backing store to resize, invalidating existing references, just like with a list.
Doh, I missed that it was a bot; I'll just remove the comment as it's misleading.
Tongue in cheek, but I would draw the opposite conclusion from your tea test then: Any GUI toolkit that sacrifices something important (code simplicity, performance) for something benign that is hardly ever used (prolonged smooth resizing) is making questionable design decisions.
Without boats was developing a mark and sweep GC with the name of Shifgrethor based using pin api. read for yourself though [https://boats.gitlab.io/blog/post/shifgrethor-i/](https://boats.gitlab.io/blog/post/shifgrethor-i/)
Some random ideas: - how do you make GC work with destructors: In rust a lot of resources are released when the handle to them is dropped (rust word for running destructor)and. In a GC world you don't have this deterministic destructor behaviour. Does this matter? Do you need some distinction in the language between "stuff that just basically frees memory on drop" and "stuff that might release a hardware lock/change state of a peripheral/other complex stuff" - DSP with rust: how does ownership interact with realtime digital signal processing, e.g. in audio - something to do with the psychology of programming: how does human nature affect programming languages and how we construct them? Why do people reach for unsafe when they are new to the language, when there are better safe alternatives? Rust often makes it hard to do bad things rather than impossible, which had a "nudging" effect. DISCLAIMER: just ideas, not necessarily good ones 😋
I think they are good ideas. Thanks for the input :)
[https://learning-rust.github.io/docs/d4.crates.html](https://learning-rust.github.io/docs/d4.crates.html) not following 2018 edition, but concept-wise this will be helpful for more details.
Are you interested in symbolic execution? There's this project called [seer](https://github.com/dwrensha/seer) which basically can find inputs for which your Rust program can panic. Unfortunately it looks like it's no longer maintained, so a reboot would be nice. There is a similar project which works on LLVM bytecode: [KLEE](https://klee.github.io). There is also a static analyzer for Rust based on miri: [MIRAI](https://github.com/facebookexperimental/MIRAI), but I don't know how it compares to the other projects. Related topics are fuzzing, proptests, constrain programming, theorem proving.
thank you for the response!
As others mentioned it's supposed to be ``mut obj: Obj``. I understand your confusing tho because I had it as well. Why is it ``obj: &amp;mut Obj`` otherwise? First off a reminder that the type is placed after the colon ``:``. Then you gotta know that ``T`` and ``&amp;T`` are 2 different types to the compiler. Or said in a different way; ``&amp;`` is part of the type. "This is a *reference* to ``T``, not ``T`` itself". Pretty logical and thus why ``&amp;`` is also placed behind ``:``. Now ``mut``. What does it actually mean? It indicates mutability *of the term after it*. However, it's not part of the type. ``mut var`` indicates that the value of the variable can be changed. ``&amp;mut T`` says that the value of the underlying type can be changed. (Why isn't ``mut &amp;T`` or ``mut T`` allowed? Idk, maybe there is a valid reason. My reasoning is "place the ``mut`` with the most descriptive naming convention". The name of the variable is more descriptive than a ``&amp;`` or ``T``, while ``T`` is the only descriptor for the underlying type of a reference.) Now all that's left is to reason about the possible combinations of ``mut``. Remember that ``&amp;T`` is a pointer: ```Rust mut t: T // value of t, a T object, can be changed. mut t: &amp;T // value of t, a reference/pointer to T, can be changed. t: &amp;mut T // value of the underlying T object can be changed. mut t: &amp;mut T // everything about this can be changed ``` If you're familiar with C/C++ pointers, think of the differences between placing ``*`` at different locations around your variable/type.
Thank you! const generics look pretty interesting in general.
Hi! I don't know that I'd say I'm notable, but I'm writing a research database in Rust, do live Rust programming streams, and I maintain several open source Rust libraries. You
This isn't something I'm familiar with, but it looks like something that could be right up my alley. I'll definitely have a look at it.
I don't think Rust's ownership really matters in terms of DSP. When you get down into it a lot of people are writing data oriented systems even if they don't realize it. Something more interesting would be the ability of rustc to autovectorize DSP loops using zero cost abstractions like iterators.
To answer some questions that I would have: **Why?** Lockfree programming just sounds *fun*, doesn't it? I also wanted to speed up a Redis-compatible server I'm working on. **How?** cht implements an open-addressing hash table that uses tombstones for deletion - I didn't want to get overzealous and end up with something non-functional by trying out fancier techniques. The hash table users will interface with is this: struct HashMap&lt;K: Hash + Eq, V, S: BuildHasher&gt; { buckets: Atomic&lt;BucketArray&lt;K, V, S&gt;&gt;, len: AtomicUsize, // number of completed insertions, might be greater than than buckets-&gt;len if resizing hash_builder: Arc&lt;S&gt;, } A `BucketArray` is a `Vec&lt;Atomic&lt;Bucket&lt;K, V&gt;&gt;&gt;` with extra baubles: struct BucketArray&lt;K: Hash + Eq, V, S: BuildHasher&gt; { buckets: Vec&lt;Atomic&lt;Bucket&lt;K, V&gt;&gt;&gt;, len: AtomicUsize, // number of completed insertions into this array next_array: Atomic&lt;BucketArray&lt;K, V, S&gt;&gt;, hash_builder: Arc&lt;S&gt;, // needed for resizes } `Bucket` is essentially `(K, Option&lt;V&gt;)`. `BucketArray::next_array` starts out as a null pointer, but resize operations will allocate a new one and set it, forming a kind of singly-linked list of arrays of buckets, if that makes sense. Resizing follows these steps: 1. Allocate a new `BucketArray` and CAS with the current array's `next_array`. 2. Element-by-element, copy the bucket pointers into the next array. CAS a redirect tag to each bucket pointer so future operations on that bucket know to look in the next array first. 3. If an insert or get operation was redirected, after the operation completes, CAS `HashMap::buckets` with the next aray. Multiple resizes can occur at the same time and threads can never be blocked forever waiting for the resize to complete, since they will complete the resize themselves if they have to. **Does it work?** Probably. Hard for me to tell with these things. **Is it fast?** In short, no. In long, it's only faster than a fine-grained locking hash table, like the one from [ccl](https://gitlab.nebulanet.cc/xacrimon/ccl), when concurrently inserting into the same bucket. I didn't write up very thorough benchmarks, so this is far from a certainty. **Where do you see room for improvement?** 1. Less restrictive memory orderings. Sequentially consistent memory orderings are used throughout. 2. Read-modify-write operations. It should be possible to atomically get and modify a value in the hash table. 3. Possibly, remove the requirement for `K` to be `Clone` when removing elements. If I have indirect values or use some technique like backwards shift deletion, it may be possible for buckets to simply forget values instead of allocating a new bucket to serve as a tombstone.
dupe of https://www.reddit.com/r/rust/comments/c3sa5r/beam_rust_a_match_made_in_heaven/ it seems
Hello. Author of ccl and the Grand Concurrent Map Competition here. I love this. More implementations is always welcome! Ccl currently has a lockfree map that is almost finished aswell! All tough i love the idea i do see a few issues and quite a few bugs. I will make a longer writeup when I come home and also add the map to the benchmarks. Though one of those bugs is shadow entry deletion which means this won't pass the correctness tests I have. Best regards, Acrimon. Competition link: https://gitlab.nebulanet.cc/xacrimon/rs-hm-bench
Is there a way to conditionally compile depending on rustc version? I want the following: #[cfg(have_rustc_1.32)] fn do_stuff { // nice safe implementation } #[cfg(not(have_rustc_1.32))] fn do_stuff { // unsafe code to support older compilers } There is https://github.com/dtolnay/select-rustc but that pulls in `syn` which takes a long time to compile in release mode. I was sure there was a built-in lightweight way to do that, but I cannot find it!
Basic `std::map` is implemented via tree structure, so references to values in nodes remain valid until node gets removed
yup, you are right. I didn't notice that before posting. Reddit should have some sort of link duplication check before posting like lobste.rs does
One of `ndarray`'s maintainers here - this is extremely useful feedback for us! I have opened an issue on GitHub to keep track of the points you have raised - you can pitch in [here](https://github.com/rust-ndarray/ndarray/issues/649). Hopefully we can make it easier to get started in the `ndarray` ecosystem!
it does, but the other post links to the video directly :)
ah
I'd say this is the right way to do it, although I personally forgo main.rs and only use the /bin dir. https://doc.rust-lang.org/cargo/guide/project-layout.html You can (and perhaps should) remove crates that are only relevant to the bin file (eg clap) to your binary file(s) so they don't appear in the lib.
How about looking into [hot-swapping](https://gamedev.stackexchange.com/questions/221/how-can-one-implement-hot-swappable-c-modules)? You may even target embedded devices.
Sure, it's not a perfect analogy, and good engineering is all about tradeoffs. I think my single main point is that people should understand the issue and make an informed decision. In my searching of the internet, I found largely denial. [MS docs:](https://devblogs.microsoft.com/directx/dxgi-flip-model/) here, use the flip model, here's how to resize a swapchain buffer, we'll conveniently neglect to tell you it regresses the smooth resize behavior. [Apple forum:](https://forums.developer.apple.com/thread/77901) "Yes, still looking into it and there are active conversations focused on finding a solution." (almost a year ago, no followup since). If somebody understands the tradeoffs and deliberately chooses a lower quality result, that's fine, but mostly when I see non-smooth resizing, I take it as a sign that the designers of the system didn't quite understand what they were don't and don't quite have control over how the pieces fit together. Another thing I'd argue is that the smooth resize test in many ways argues for *simpler* engineering decisions. The UI loop should be synchronous and single-threaded, but lots of people propose adding all kinds of threading complexity. Note that a *game* may be a different story, for throughput you want to keep your pipeline stages full. But in a properly designed UI, it should be pretty easy to hit throughput targets, and the rest should be optimized for latency. I'm glad the analogy at least spurred a discussion :)
I got all excited and thought this was about Crossbeam haha. Not enough ☕️ I guess
hmm, point.
Ah yeh, I'm actually still translating part of it. It was the time of `try!`, no `?` yet, but most of it is actually straight forward. I might write up part 3 soon too, with some more details. It was indeed late when I finished it, probably why it is cut up :-)
I'm confused by `RefCell`. In this code, `y` appears to be a mutable reference, but `y` itself has to be mutable. Isn't `y` more of an aliased owner than a borrow? use std::cell::RefCell; fn main() { let x = RefCell::new(0); // not a borrow since y must be mutable? let mut y = x.borrow_mut(); *y = 42; } [Playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=cde61223634c3f99ea67e3fe5d134650)
I agree with your principle, but the issue here is that alternatives (like new language features) also require more education. You can also maybe also have lints to help here as well as documentation?
I am not sure that cross posting is allowed, but such theoretically simple thing is somehow hard (not that intuitive) to do in Rust.
I've added (Ctrl-F "UPDATE") some more examples of what I tried this morning, if you still want some more! :-)
This approach is great since it allows other developers (or you in the future) to call directly into library functions rather than having to refactor the executable binary code when you need to use the library directly.
A constraint solver crate could be pretty useful
"a real fatass proxy like [sozu](https://github.com/sozu-proxy/sozu)". That's not how I would describe other projects in that space but hey, the more the merrier
I wrote my masters thesis project in Rust ([libpnet](https://github.com/libpnet/libpnet)). If you want Rust to be part of yours, you have a few angles: * You do something interesting. You happen to write it in Rust. * You take advantage of something unique about Rust to do something in a new and interesting way (academics like the word *novel* for this). * You take something Rust is good or bad at, and make it better at it. Based on your interest areas, the last one might be a good idea. There's already a lot of movement in the areas you mention in Rust - it might be a good idea to talk to some of the people already working on things you're interested in to see where you could have an impact. * RustBelt - proving Rust's safety properties - http://plv.mpi-sws.org/rustbelt/#project * Sealed Rust - Rust in safety critical systems - https://ferrous-systems.com/blog/sealed-rust-the-pitch/ * I don't have a link, but there's a ton of interesting work around extending/improving Rust's type system. Someone else will probably chime in, if not you could poke the compiler team - https://www.rust-lang.org/governance/teams/compiler Good luck!
If you define a function you are able to specify the lifetimes and let the compiler know how the inputs and outputs lifetime are related: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=6ba37446363b7c93632a934413e3e92c
Welcome!
Welcome! While this subreddit is mostly about questions and announcements, you can meme as much you want on /r/rustjerk.
Thanks! \^\_\^
Thanks! Good to know.
I think I still have a few esp32s laying around from when I was doing some iot stuff. I should give this a shot! Ty!
you really can't do this with a closure, but a free function. The Challenge, and trick is ensuring: * The underlying string won't be de-allocated while the closure is built. * The iterative won't be invalidated while transversing. * The underlying string won't be de-allocated until at least iteration is finished. This is why lifetime annotations are needed, because in most languages this is managed by GC.
I agree with LivingIncident. Advanced but not difficult. I learned (still learning but able to write real code in real projects) Rust with The Programming Rust book along side the official "Book". They complement each other very well in my opinion. Rust definitely has a bit of a learning curve such that things seem more advanced in the beginning than they actually are. For example, Ownership and lifetimes smack you in the face right from the start and there's no putting it off until later. You just have to eat your broccoli. If you work through both books and keep them on hand and you shall be rewarded. It's worth it. That has been my experience.
At some point I'll get around to modernizing the tutorial with all the best GTK-Rust practices I've learned since then.
Thanks, I already bought that book. 👍
I look forward to seeing your write-up! I had an inkling that I would only catch the last bugs after someone else looked at the code. For example, I can't figure out for the life of my why during a grow, buckets should *not* be erased if they were modified by another thread and the final CAS fails.
That was a very thoughtful response. Thank you!
The builder addition should be more prominent in this post. It covers one of the biggest flaws in the API. The only issue remaining is that it's limited to construct-only properties, so you can't connect signals or add children using the new builder API, as the cascade macro allows.
Also, one question. Why can't cht return references to values? It should be possible. You can look at ccls code for nested map as for how I do it in lockfree map.
As I understand, it'll be support sandbox so that can use Docker and use WASI on browser?
Check out the Rust Secure Code WG [list of issues](https://github.com/rust-secure-code/wg/issues)
No, [Redox OS](https://www.redox-os.org/) is a general-purpose operating system, similar to GNU/Linux.
Rust has [a list](https://rfcbot.rs/) of active RFCs, meaning accepted or mostly accepted language design proposals that are ready to be implemented. You could contribute an implementation for one of those.
Just a note that `cargo-xbuild` is considered to be the replacement of xargo.
Hi, I'm not very familiar with the topic of my suggestion but I think it would be pretty cool to work on Rust specific optimizations. As I understand there are certain kinds of optimizations that the Rust language can theoretically enable but are not possible in languages like C. I also remember to have seen some time ago that LLVM did support some of these optimizations but the Rust compiler wasn't generating the bytecode to take advantage of it. You have to talk with some of the more hardcore Rust compiler people to find out what can be done in this space. But what I think is cool about work like this is that it would benefit the entire Rust community, possibly even making the Rust compiler itself faster, by compiling the Rust compiler in the new compiler that has this optimizations. The core of this Rust specific optimizations are the borrowing and ownership systems that offer some some guarantees that are simply not available in other system languages. The name of the theses could also sound quite nice as well, something like: "Optimizing compilers using Rust's memory model" or "Rust's memory model: an opportunity for deep optimizations.".
[image crate](https://github.com/image-rs/image) is quite fundamental, but could use some love. You could contribute [WebP chroma decoding](https://github.com/image-rs/image/issues/939). Or you could try to get rid of unsafe code in it without regressing performance, and if you can't, open an RFC for a safe abstraction that would allow it. I did that recently to a few other crates, [here's my RFC](https://github.com/rust-lang/rfcs/pull/2714)
It's possible, as you have done, but I think the way you did it is a more complex than I wanted to dive into for cht. My goal was to investigate lockfree data structures, so I focused almost all engineering effort on that side of the project. It's a nice interface, though! I may add something similar in a future version of cht.
Here you go: https://github.com/ptal/pcp This is somebody's thesis, actually
There are a lot of opportunities to be had with schemes. You could abstract various network protocols into schemes, so that programs can interact with them using the standard file I/O, thus allowing any program to interact with these protocols without needing to be aware of how they work. It would theoretically be possible for commands like so to work: ``` less https:sh.rustup.rs sh &lt; https:sh.rustup.rs ls ftp:$DOMAIN/$DIR ``` One of the benefits to this is that the scheme service providing these protocols could pool connections between the client applications interacting with it. Applications also wouldn't need to bundle libraries to interact with them.
https://github.com/AngoraFuzzer/Angora is a new fuzzer that (according to its authors) is dramatically superior to everything that came before. [Their paper](https://web.cs.ucdavis.edu/~hchen/paper/chen2018angora.pdf) is quite fascinating. It currently works with C but not Rust, even thought it's written in Rust. Adapting it to work with Rust would be very valuable.
Quote you: &gt;Inheritence is not OOP. ... but on Wikipedia [https://en.wikipedia.org/wiki/Polymorphism\_(computer\_science)#Subtyping](https://en.wikipedia.org/wiki/Polymorphism_(computer_science)#Subtyping) one can read: &gt;[Object-oriented programming languages](https://en.wikipedia.org/wiki/Object-oriented_programming_language) offer subtype polymorphism using [*subclassing*](https://en.wikipedia.org/wiki/Subclass_(computer_science)) (also known as [*inheritance*](https://en.wikipedia.org/wiki/Inheritance_in_object-oriented_programming)). In typical implementations, each class contains what is called a [*virtual table*](https://en.wikipedia.org/wiki/Virtual_table)—a table of functions that implement the polymorphic part of the class interface—and each object contains a pointer to the "vtable" of its class, which is then consulted whenever a polymorphic method is called. I think you just meant that inheritance (sub typing) is one of more (is it 4?) types of polymorphism, because the sentence I quotes is followed by this sentence : &gt;It's just one component that can make up part of an OOP language, but doesn't necessarily have to &amp;#x200B; **Please** be aware of that my above comment can be a result of a misunderstand caused by a "lost in translation", between Danish (my native language) and then English. &amp;#x200B; **Thanks** for the link to the chapter in *The Book*
There is a very interesting project called [Prusti](http://www.pm.inf.ethz.ch/research/prusti.html) that translates safe Rust into a language amenable for theorem proving and then prove user-defined properties on it. This is very exciting because it works on *unmodified Rust code,* you just add an annotation on a function that you want a certain property like absence of integer overflow proven and it does that automatically. Alas, they have no released the source code, even though they said they intend to. You could contact them and check if they'd be open for collaboration.
Thank you! This is so helpful! &amp;#x200B; I'll have to have another try at splitting things into their own functions - there was a lot of ownership/borrowing errors I was getting. I'll probably go back and try it again after I read/learn more. I do like \`-&gt; Self\` better, so I made that change. The maximum possibly number of notes in is 128, but more realistically a max of 50... I'd still be interested in learning better algorithms and data structures - that's one of the areas I need to learn more about in general. This will be a good area I can return to when I go ahead with that. &amp;#x200B; and thanks for the tip about \`vst-rs\`!
Rust AND memes? How did I not know about this??
Exactly that, thank you. I've only really worked with registers and the stack in 8086 assembly, so seeing it done in Rust is just a really new thing for me. Thanks for pointing me in the right direction :)
Thank you so much! I felt there must have been a slicker way to do the socket address. This is better. And I didn't think to use destructuring for the MIDI values, that's much nicer
The wow factor comes in from people not expecting disciplined usage of unsafe in such a large codebase, e.g. actix-web.
That's what I said.
Hi fellow rustaceans ! I'm completely new to Rust, slowly started to learn the language a month ago. *TLDR* Today the benefits of VMI has been proven for multiple use cases. However, we lack the underlying open source libraries that could bring the foundation for new applications to be hypervisor-agnostic and open the ecosystem. Long story: In my spare time, i have been developing project based on Virtual Machine Introspection. Basically, inspecting the content of a VM in realtime, for multiple purposes: - Debugging - Sandboxing - Montoring/Tracing - Forensics I was more interested in the debugging capabilities of VMI, being cross-platform and agentless, so I wrote my own tool: https://github.com/Wenzel/pyvmidbg The tool is based on LibVMI as flexible library, that provides a high-level APIs to understand the guest OS, and callbacks to react on multiple events, as well as drivers for Xen, KVM and Bareflank so far. The problem I have is that this library is hard to work with, especially when it comes to writing drivers for Windows (`VirtualBox` and `Hyper-V`). It is Linux-centric, and has dependencies on `GLib` for example (`GList/GHashTable`) . That's why I started to write this library, which aims to be minimal and only provide a bindings to the hypervisor's APIs, without any intelligence or advanced features. So far, I managed to have a Xen driver, a small API: - `read_physical_memory` - `get_max_physical_address` - `pause` - `resume` And I sucessfully dumped the physical memory of a Xen domain: https://twitter.com/mtarral/status/1141832159457677317?s=20 I'm writing this post in the hope of findings motivated developers who wishes to contribute ! :) Thanks.
`y` is not a mutable reference, it is a [`RefMut`](https://doc.rust-lang.org/std/cell/struct.RefMut.html)
borrow_mut() returns a [RefMut](https://doc.rust-lang.org/std/cell/struct.RefMut.html) which can then be [DerefMut](https://doc.rust-lang.org/std/ops/trait.DerefMut.html#tymethod.deref_mut)ed. `deref_mut(&amp;mut self)` requires mutable access to `Self`, so the binding must be mutable. If `borrow_mut()` just returned an `&amp;mut T`, then the binding wouldn't need to be mutable: let mut x = 1; let y = &amp;mut x; *y = 2;
Research and investigate if you can build a interpreter that runs rust code with live reloading so that a developer can build their web app with it like you would do in php. If this is possible, then you will have a change and reload web development setup that has a easy development experience a la php, but when you compile it for production, it will be super fast.
There is also rustig, a bachelor project from 4 TU Delft student that analyses the ways a rust program can panic: https://github.com/Technolution/rustig
&gt; programming languages, Pick up a feature in the roadmap, design it, and implement it - you don't need to land it upstream. Features that are interesting from a PL perspective, and that are possible to implement today would be: custom dynamically-sized types, specialization, virtual enums, delegation / inheritance, etc. You might want to do a bit of research here, ping nikomatsakis, centril, etc. about it. And discuss your findings with your advisor and pick one.
For what it's worth, I expect this to be a non-issue on properly behaving Wayland client and servers. The reason for that is that it is exactly a question that has been at the core of Wayland's design principle: "Every frame must be pixel-perfect.". On Wayland, a window size is _defined_ by the size of the content buffer attached to it. So "synchronization with the window manager" is essentially a non-problem in most of the cases.
Good idea, thanks!
I understand. However in the `BorrowMut` trait, `borrow_mut` does return a mutable reference: fn borrow_mut(&amp;mut self) -&gt; &amp;mut Borrowed Yet `RefCell` has an inherent impl of `borrow_mut` that does not return a reference at all. Confusing, and I'm wondering what is the rationale for calling the fn `borrow_mut`?
Understood. So why is the fn called `borrow_mut`?
Licenses w/ patent grants exist to make it explicit, not because MIT does not include implicit patent grants (untested in court).
Every journey begins with one step... (gravity, explosives, pushing, and other outside influences not withstanding)
&gt; inlining Not all functions can be inlined. &gt; copy elision How does that work?
Is there a link?
r/playrust :)
If you just want to play Rust game, try /r/playrust, but if you want to learn programming, the Rust language is a fine choice and you're welcome here.
Deepfake videos and audio are going to be a growing problem. You could study methods for identifying fakes and write corresponding libraries in Rust used to detect them.
/u/jonhoo ?
Oh, whoops! Wrong subreddit! I would love to learn a programming language though, so ill check it out.
&gt; Element-by-element, copy the bucket pointers into the next array. CAS a redirect tag to each bucket pointer so future operations on that bucket know to look in the next array first. I tried looking at the code, but couldn't quite figure out if you covered the potential for a race condition. The problem with a redirection is of course the race condition; depending on the order of operations: 1. Either you "steal" the bucket first, setting the redirect flag, then insert it into the next array. 2. Or you first duplicate the bucket, then redirect. In case (1), there is a period of time where only the thread performing the transfer has a hold of the element; should that thread be suspended, the other threads have to wait until the element reappear in the next array to perform a `find`, `update` or `remove` on this element. In case (2), there is a chance that between the duplication and setting the redirect, another thread decided to either update or remove the element. In case of update, the duplicate value is no longer a duplicate, and in case of removal, the duplicate need be removed too. From the code it seems you selected alternative (1), but I could not figure out whether you accounted for the race condition correctly or not. --- Note: have you considered NOT moving? And yes, I am serious. Imagine creating a linked-list of BucketArray, in which the last BucketArray is of size N, the previous is of size 2/4/8 x N (at your option), etc... When "growing", you simply allocate a new (larger) BucketArray, and put it at the front of the linked-list. When "searching", you check each BucketArray one after the other, with elements having an increased probability of being in the first array since it's the bigger. It would probably be a bit slower, if the initial size was poorly chosen, however you do away with transfers and redirects which should greatly simplify the algorithms.
GObject support really isn't necessary for GTK libraries and applications in Rust. The [shrinkwraprs](https://lib.rs/crates/shrinkwraprs) crate is enough to declare what inner field a custom widget is based upon, inheriting its fields automatically, and gaining direct access to that field through an `as_ref()`. Several options exist for managing state, from providing `Rc`'d fields and closures as trait objects, or registering a [glib-rs channel on the main context](https://github.com/gtk-rs/examples/blob/a64bfac56d6043c505cb963cd7a4c229d0013fd1/src/bin/multithreading_context.rs#L25) that receives events through senders given to inner widget's signals, and executes them on a receiver attached to the main context.
That approach indeed works. But it will require one additional heap allocation per yielded item, since one needs a \`pin\`ned place for the \`Future\` produced by the \`async fn\` in order to poll it.
You also need `extern crate` for the `alloc` crate, which is very common in embedded applications.
Re inlining: True. But many can and will. Re copy elision: The compiler has heuristics to find out which copies can be removed and will make good use of them.
Way to go, guys. Impressive stuff.
Nit: Raph, not Ralph 😉
He wrote his thesis on Reddit?
Since you mentioned compilers as an interest -- I have a barely working proof of concept of a compiler that compiles reflection-like library calls into non-reflective Rust code. I believe this approach will make it significantly easier to develop robust procedural macros. https://github.com/dtolnay/reflect Basically it is waiting for someone with an interest in compilers to implement a lot of traditional compiler-y things in a very non-traditional context to get this working. I think it would make a terrific masters thesis because it is a novel twist on reflection, gets into many of Rust's most unique characteristics, and bucks many of the tradeoffs around reflection and code generation to deliver a unique and powerful programming model.
Cool stuff. Post it in Rust game dev
Thank you and I definitely will post there as well.
As the portion of the quote I bolded implies, [locality](https://en.wikipedia.org/wiki/Locality_of_reference) and [fragmentation](https://en.wikipedia.org/wiki/Fragmentation_\(computing\)) are different concepts. The Mesh^[1] strategy is designed explicitly to reduce fragmentation. The Mesh allocator allocates memory randomly within a given span of memory (span = one or more pages). It then combines ("meshes") two memory spans into one such that the Robson worst case bound^[2] for fragmentation is broken with high probability, reducing memory use up to 39% in some cases. The mimalloc strategy, on the other hand, is designed explicitly to maximize locality, exploiting the advantages of the hardware cache architecture to increase speed, e.g. by 14% when compared to jemalloc.^[3] Mesh's random allocation seems to me to be fundamentally at odds with maximizing cache locality. On the other hand, the mimalloc paper describes a security-enhanced version of mimalloc, smimalloc, which has a similar feature: &gt; "The initial free list in a page is initialized randomly such that there is no predictable allocation pattern (to protect against heap feng shui (Sotirov, 2007)). Also, on a full list, the secure allocator will sometimes extend instead of using the local free list to increase randomness further." The paper reports only a 3% slowdown for smimalloc over mimalloc, so it's not clear to me what's going on and whether or not Mesh and mimalloc are mutually exclusive. ---- ^1: ^https://arxiv.org/pdf/1902.04738.pdf ^2: ^"Robson ^showed ^that ^all ^such ^allocators ^can ^suffer ^from ^catastrophic ^memory ^fragmentation ^[26]. ^This ^increase ^in ^memory ^consumption ^can ^be ^as ^high ^as ^the ^log ^of ^the ^ratio ^between ^the ^largest ^and ^smallest ^object ^sizes ^allocated. ^For ^example, ^for ^an ^application ^that ^allocates ^16-byte ^and ^128KB ^objects, ^it ^is ^possible ^for ^it ^to ^consume ^13× ^more ^memory ^than ^required." ^\(Citation ^as ^it ^appears ^in ^the ^original.) ^3: ^https://www.microsoft.com/en-us/research/uploads/prod/2019/06/mimalloc-tr-v1.pdf
There's always overhead to parallelism, though the exact amount of overhead will vary based on how parallelism is achieved. In this sense, it does mean that when feasible you should use somewhat coarse-grained parallelism. However too coarse-grained is not ideal either as you can run into starvation issues. For example, imagine 4 tasks which complete in 1s, 2s, 4s, and 8s repeatedly, executed on 4 CPUs: it will take about 8s to compute all 4, using 24s of compute-time for 15s of real work. That's a measly 2/3 utilization rate :/ In fine, there's a balance to be found: - Too fine-grained and the coordination overhead is significant. - Too coarse-grained and resources are under-utilized. You need to find the Goldilocks zone of your work-load; my personal recommendation would be to start coarse-grained and split until you reach an under-utilization threshold that is acceptable. Under 10% is good, under 1% is superb.
# Issue with lifetimes and std::borrow::Borrow&lt;&amp;T&gt; &amp;#x200B; Hello, I am getting the following error, I think I am messing everything up with the way I am declaring the lifetimes: &amp;#x200B; This is my program (a draft): `use std::collections::HashMap;` `use std::hash::Hasher;` `use std::hash::Hash;` `#[derive(Debug, Clone)]` `pub struct ThreadInfo {` `name: String,` `id: String,` `native_id: String,` `priority: String,` `state: String,` `stack_trace: String,` `daemon: bool,` `}` `impl ThreadInfo {` `fn empty() -&gt; ThreadInfo {` `ThreadInfo {` `name: String::from(""),` `id: String::from(""),` `native_id: String::from(""),` `priority: String::from(""),` `state: String::from(""),` `stack_trace: String::from(""),` `daemon: false,` `}` `}` `}` `impl PartialEq for ThreadInfo {` `fn eq(&amp;self, other: &amp;Self) -&gt; bool {` `(self.name == other.name)` `&amp;&amp; (self.id == other.id)` `&amp;&amp; (self.native_id == other.native_id)` `&amp;&amp; (self.priority == other.priority)` `&amp;&amp; (self.state == other.state)` `&amp;&amp; (self.daemon == other.daemon)` `}` `}` `impl Eq for ThreadInfo {}` `impl Hash for ThreadInfo {` `fn hash&lt;H: Hasher&gt;(&amp;self, state: &amp;mut H) {` `self.name.hash(state);` `self.id.hash(state);` `self.native_id.hash(state);` `self.priority.hash(state);` `self.state.hash(state);` `self.daemon.hash(state);` `}` `}` `pub struct Locked {` `lock_id: String,` `locked_object_name: String,` `}` `pub fn holds&lt;'a&gt;(threads: &amp;'a Vec&lt;ThreadInfo&gt;) -&gt; HashMap&lt;&amp;'a mut ThreadInfo, &amp;'a Vec&lt;Locked&gt;&gt; {` `let mut holds: HashMap&lt;&amp;'a mut ThreadInfo, &amp;'a Vec&lt;Locked&gt;&gt; = HashMap::new();` `for mut th in threads {` `// Check if th is in the map, if not then create the entry with "th" as the key` `// and an empty vector as the value` `match holds.get(&amp;th) {` `Some(_) =&gt; {},` `None =&gt; {` `let something: Vec&lt;Locked&gt; = Vec::new();` `holds.insert(&amp;mut th, &amp;something);` `},` `}` `}` `holds` `}` `fn main() {` `let mut threads: Vec&lt;ThreadInfo&gt; = Vec::new();` `holds(&amp;threads);` `}` &amp;#x200B; I am passing a &amp;Vec&lt;ThreadInfo&gt; to the function and based on the data in it I need to generate a HashMap with keys as each of the elements in the Vec&lt;&gt;. For the values I need an empty Vec&lt;&gt;, I am getting this error: &amp;#x200B; `68 | match holds.get(&amp;th) {` `| ^^^ the trait \`sthd::borrow::Borrow&lt;&amp;ThreadInfo&gt;\` is not implemented for \`&amp;mut ThreadInfo\`` &amp;#x200B; How could I fix it? How can I create vector in the function that can outlive the function (values in the map). &amp;#x200B; This is the link to the playground: [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=93485daa59c07133f5bd2dafd4293ee6](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=93485daa59c07133f5bd2dafd4293ee6) &amp;#x200B; Thank you!
&gt; How? cht implements an open-addressing hash table that uses tombstones for deletion - I didn't want to get overzealous and end up with something non-functional by trying out fancier techniques. I'm just curious - is there a common "best" way of doing it? Or is it still a pretty open question in computer science? I've seen a few with a kind of simplistic deletion approach (and I understand why it's so hard), but I haven't really found one that's bells and whistles complete. Also, for correctness, I'd recommend to some kind of symbolic modeling (you could literally brute force or cleverly monte-carlo the space), although I understand the difficulty and blowup. I actually did a lot of similar stuff for my undergraduate thesis!
There are three problems with the `holds` function. First, you cannot safely get `&amp;mut ThreadInfo` from a `&amp;Vec&lt;ThreadInfo&gt;`, to do this, the function argument also needs to be a mutable reference. Then, since `Vec&lt;Locked&gt;` is created inside the function, the return value needs to own the value (i.e. without the `&amp;`), otherwise, it will be holding a reference to something that will be immediately freed at the end of the function block. Thirdly, the "insert value if doesn't yet exist` functionality can be implemented using the `Entry` API provided by std. With all these, the `holds` function looks as follows: pub fn holds(threads: &amp;mut Vec&lt;ThreadInfo&gt;) -&gt; HashMap&lt;&amp;mut ThreadInfo, Vec&lt;Locked&gt;&gt; { let mut holds = HashMap::new(); for th in threads.iter_mut() { holds.entry(th).or_insert(Vec::new()); } holds }
Switch to the experimental macro engine, makes it a whole lot faster in general
Hey Rustaceans, I've been working on Merino, a SOCKS5 Proxy Server, for a couple of days now. I wanted to play with low-level networking performance in Rust. It's a Prototype right now. However, I plan to move Merino over to Actix's `SyncArbiter` and create actors for handling high throughput/parallel connections. Then add support for middleware and more proxy rules. Right now it handles 1+ Gbit/Second speeds over a LAN on ethernet with low CPU usage. I haven't benchmarked it much yet, but that's in the roadmap. I would love some feedback on Merino. Enjoy!
It's easier to understand but it's actually a lot more difficult to reason about formally. A UTXO either exists (with some properties, like value and spend conditions) or it doesn't. Accounts are created and change state in different ways over time, and in certain cases even be destroyed and recreated with different behavior at a later point. There's a lot of ways it can change which makes reasoning about them formally a lot more difficult, and that incidentally makes implementing privacy features more awkward, which is why privacy coins are exclusively (afaik) UTXO-based, and also derived from Bitcoin.
Thank you! that definitely helped a lot, thanks. &amp;#x200B; Now I am dealing with a "**value borrowed here after move"** error. &amp;#x200B; 174 | for th in threads.iter\_mut() { | -- move occurs because \`th\` has type \`&amp;mut ThreadInfo\`, which does not implement the \`Copy\` trait ... 183 | match holds.get(&amp;th) { | \^\^\^ value borrowed here after move ... 186 | holds.entry(th).or\_insert(Vec::new()); | -- value moved here, in previous iteration of loop &amp;#x200B; This is all the program and what I am trying to do in the "holds" function. This is the link to the playground: [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=93a4b07e6084f75d77a2a2a8d65f9c28](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=93a4b07e6084f75d77a2a2a8d65f9c28) &amp;#x200B; `[dependencies]` `regex = "1.1.7"` `stringreader = "0.1.1"`
This section: ``` match holds.get(&amp;th) { Some(_) =&gt; {}, None =&gt; { holds.entry(th).or_insert(Vec::new()); }, } ``` can just be written as: ``` holds.entry(th).or_insert(Vec::new()); ``` However, there's a more fundamental problem - you're trying to store multiple mutable references to the contents of the vector, which is a violation of the borrow rules. Do the keys need to be mutable references?
&gt; I'm just curious - is there a common "best" way of doing it? Or is it still a pretty open question in computer science? I've seen a few with a kind of simplistic deletion approach (and I understand why it's so hard), but I haven't really found one that's bells and whistles complete. C++ developers are very serious about hash tables. I think there's something of an arms race going on, because a new "fastest" hash table comes out most weeks. But in [this massive benchmark](https://martin.ankerl.com/2019/04/01/hashmap-benchmarks-03-01-result-InsertHugeInt/) of almost all hash tables out there, the top three hash tables for insertion and erasure of integers *all* use open addressing with Robin Hood hashing and backwards shift deletion. So the numbers show that this is the fastest way to implement a hash table. Until recently, this was how the hash table in the Rust standard library was implemented. However, if you look at the string tests in that benchmark, you see that sometimes the node-based hash tables are doing pretty good. So the jury's still out on this one. 🤷 &gt; Also, for correctness, I'd recommend to some kind of symbolic modeling (you could literally brute force or cleverly monte-carlo the space), although I understand the difficulty and blowup. I actually did a lot of similar stuff for my undergraduate thesis! That's a good idea! Can you link some relevant papers? TBH I don't know where to start with a techniques like this.
Did this happen? I can't find any thread on it.
&gt;You are right, the keys doesn't need to be mutable. &gt; &gt;I have modified the function signature to: &gt; &gt;`pub fn holds(threads: &amp;mut Vec&lt;ThreadInfo&gt;) -&gt; HashMap&lt;ThreadInfo, Vec&lt;Locked&gt;&gt; {` &gt; &gt;doing that I get this error: &gt; &gt;`190 | holds` `| ^^^^^ expected struct \`ThreadInfo\`, found mutable reference` `|` `= note: expected type \`std::collections::HashMap&lt;ThreadInfo, std::vec::Vec&lt;Locked&gt;&gt;\`` `found type \`std::collections::HashMap&lt;&amp;mut ThreadInfo, std::vec::Vec&lt;_&gt;&gt;\`` &gt; &gt;I guess it can be fixed by cloning "th" this way: &gt; &gt;`holds.entry(th.clone()).or_insert(Vec::new());` &gt; &gt; &gt; &gt;Thank you, guys, do you see something else I am doing wrong?
You just proved this guy's point...
As far as I can tell, it's due to the fact that the `read_prefix` function is monomorphized on `s`; iterators of different types would need different implementations of `read_prefix`. You create `args_iter` by creating a new iterator out of `s`, and then call `read_prefix` on that iterator. This is essentially creating an infinitely recursive amount of instances of this function. You could solve this by taking a `&amp;dyn Iterator&lt;Item = u8&gt;` instead; however, unless you really need that function to support streaming data, it's probably easier to just take &amp;[u8].
So by locality do they mean locality by page? Because I don't know how you maintain locality within a page while also allocating randomly within the page.
You don't need `threads` to be `mut`, since you aren't mutating it. You could return a `HashMap&lt;&amp;ThreadInfo, Vec&lt;Locked&gt;&gt;`, and iterate over threads with `iter()` - that will work if you can guarantee the hash map only needs to live as long as the vector of threads.
If I understand correctly, only the ‘secure’ variant randomizes allocations, but since it's slowdown is tiny, presumably (and this makes sense to me) pagewise locality is the thing that matters predominantly.
My main reason to use the Iterator instead of a slice is to avoid needing to explicitly keep track of and return the last index read (and also because I had `s.take_while(|c| c != b';')`, but I replaced that with a loop because I want to treat running out of characters to read differently from reaching the semicolon), but with the alternative being the compile-time creation of an "infinite" number of functions, I'll probably just take your advice and deal with the added complexity of needing to return the number of bytes "consumed".
Excellent analysis! &gt; From the code it seems you selected alternative (1), but I could not figure out whether you accounted for the race condition correctly or not. I actually use (2) here and it remains unclear to me whether the behavior of this implementation is correct during concurrent insertions and erasures. I wrote a test to try and expose this behavior, but couldn't expose the condition you and I might expect to find. I suspect it is still lurking, waiting to be exposed... In addition, (1) would *technically* not be a lockfree algorithm, since other threads can be blocked by grow operations. &gt; Note: have you considered NOT moving? And yes, I am serious. &gt; Imagine creating a linked-list of BucketArray, in which the last BucketArray is of size N, the previous is of size 2/4/8 x N (at your option), etc... &gt; When "growing", you simply allocate a new (larger) BucketArray, and put it at the front of the linked-list. &gt; When "searching", you check each BucketArray one after the other, with elements having an increased probability of being in the first array since it's the bigger. &gt; It would probably be a bit slower, if the initial size was poorly chosen, however you do away with transfers and redirects which should greatly simplify the algorithms. This is an interesting idea! The first worry that comes to my mind is cache misses during lookups, since the bucket arrays are potentially all located separately in memory. It is known that hash tables with tombstone deletion need to be rehashed from time to time so that they don't fill up with useless buckets; it's not a far stretch for me to imagine a similar scenario with your algorithm. In a way, your idea somewhat resembles the current implementation, except the linked list is from smallest to largest; this is so I can be sure that a new bucket array is not viewed until it is ready and that there isn't *too* much duplicated work done in moving to a new bucket array. Having to tune the initial size kind of removes the set-and-forget nature I tend to associate with associative arrays (heh) these days. I think that the current implementation presents a simpler interface that requires less fussing for users to get good performance. Perhaps I will implement your algorithm to see how much it removes complication from the codebase and how well it performs!
Good to know. The flip side of that (pun maybe intended) is that I'm not sure how you can easily use hardware overlays to reduce latency (available in Kaby Lake integrated graphics and later).
Looks like it was posted yeaterday so next Saturday is probably, well, next Saturday...
That's a good question :) I don't think it will be that useful for the normal signals on widgets (e.g. when a button is clicked), that currently require callbacks. You could convert most of those into `Stream`s, but it would probably require more or less the same code as with the callbacks right now. If someone can think of nice patterns where `Future`/`Stream` could be useful in the context of normal widgets, please let us know :)
The problem with the `shrinkwraprs` approach is that you have your data outside the widget itself. So you somehow need to carry it around together with your widget and can't directly get from the widget to the data. That's one of the things subclassing would allow you to do. You'd have both tightly together instead of having to carry around two things everywhere. That said, it doesn't seem that useful to myself and I never needed it for that in my applications. IMHO subclassing is mostly useful if you want to implement your own widgets, or implement one of the various interfaces (`gio::ListModel` for example). If you want to extend something in a way so that you can pass your new thing to the C libraries in a way that they can handle it accordingly. For some, somewhat "best practices" code using GTK (and GStreamer), maybe Guillaume's and my RustFest workshop from last year in Rome could be useful. Code can be found [here](https://github.com/sdroege/rustfest-rome18-gtk-gst-workshop).
How do I accept an array of closures as parameter? -- I'd like to create a function that accepts an array of closures like this: fn any&lt;R&gt;(of: &amp;[&amp;dyn Fn() -&gt; ParseResult&lt;R&gt;]) -&gt; ParseResult&lt;R&gt; { I want to use it for example like this: any([ ||match_literal("\r\n")(input), ||match_literal("\n\r")(input), ||match_literal("\n")(input) ]) This doesn't work, because the closures all have a different type! The suggestion tells me *"help: consider boxing your closure and/or using it as a trait object"*, but I get the same error if I use `Box&lt;Fn() ...` instead of `dyn`. What can I do to implement this?
Thank you, I just changed it.
I meant specifically lockfree hashtables, since I know regular hashtables can handle deletion no problem, and can have all the bells and whistles. And for how to model it, just look up symbolic execution - there's a million ways to do it. One way to test it is to create a few threads, add/delete millions of times, and check the result. If you wanted to be a little more formal about it, you can basically treat it like a graph exploration problem, where each edge is a single thread taking a single instructional step. These instructions don't need to be hardware instructions, but basically anything that reads/writes to/from memory, or kinda whatever you want to model. Then you just do BFS/DFS from the starting state and look for an invalid state.
Same, would love to see this in pure Rust
`RefCell`'s `borrow_mut` is the closest thing to `BorrowMut`'s `borrow_mut` that can exist, if it were possible to return a `&amp;mut` it would do so. Methods, functions and trait implementation can all have a function with the same name, the compiler will ask you to clarify when it can't know which one you want, [playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=731a22bc0b5dc989660e665bb704bd67).
If you're OK with dependencies, I'd use something like `nom` to do this; it'll handle that kind of stuff for you. If not, you can adapt it to use slices - take a slice and, instead of `Result&lt;JavaType, ClassParseError&gt;`, you'd return `Result&lt;(JavaType, &amp;[u8]), ClassParseError&gt;`, and pass out the remaining unconsumed slice (by saying, for example, `&amp;s[1..]` if you've only consumed one byte.)
Thanks. To me, it seems like borrow\_mut is mis-named in the context of RefCell. In my code example, it feels like `y` is a shared owner rather than a borrower.
I guess I found a way. I take a box: fn any&lt;R&gt;(input: &amp;mut Input, of: &amp;[Box&lt;dyn Fn(&amp;mut Input) -&gt; ParseResult&lt;R&gt;&gt;]) -&gt; ParseResult&lt;R&gt; { And cast every boxed element of the array to a trait object: fn newline(input: &amp;mut Input) -&gt; ParseResult&lt;bool&gt; { any(input, &amp;[ box match_literal("\r\n") as Box&lt;dyn Fn(&amp;mut Input) -&gt; ParseResult&lt;bool&gt;&gt;, box match_literal("\n\r") as Box&lt;dyn Fn(&amp;mut Input) -&gt; ParseResult&lt;bool&gt;&gt;, box match_literal("\n") as Box&lt;dyn Fn(&amp;mut Input) -&gt; ParseResult&lt;bool&gt;&gt;, ]) But this is very verbose! Is there another way that doesn't require me to cast every element in such a verbose way?
Make the function ``` fn any&lt;R&gt;(of: &amp;[Box&lt;Fn() -&gt; ParseResult&lt;R&gt;&gt;]) -&gt; ParseResult&lt;R&gt; ``` and the call: ``` any(&amp;[ Box::new(|| match_literal("\r\n")(input)), Box::new(|| match_literal("\n\r")(input)), Box::new(|| match_literal("\n)(input)), ]); ```
I'd already converted the definition of read_prefix from `Iterator&lt;Item = u8&gt; -&gt; bool -&gt; Result&lt;JavaType, ClassParseError&gt;` to `&amp;[u8] -&gt; bool -&gt; Result&lt;(JavaType, usize), ClassParseError&gt;` by the time I saw your reply, but I'll keep nom in mind for future cases of reading data that I know follows some grammar.
If you only need to accept closures, you can change `any`'s argument to `of: &amp;[fn() -&gt; ParseResult&lt;R&gt;]` and then erase the closures' type: any(&amp;\[ (|| match\_literal("\\r\\n")(input)) as fn() -&gt; ParseResult&lt;\_&gt;, (||match\_literal("\\n\\r")(input)) as fn() -&gt; ParseResult&lt;\_&gt;, (||match\_literal("\\n")(input)) as fn() -&gt; ParseResult&lt;\_&gt;, \]); Using a variable it's simpler: let closures: &amp;\[fn() -&gt; ParseResult&lt;\_&gt;\] = &amp;\[ || match\_literal("\\r\\n")(input), ||match\_literal("\\n\\r")(input), ||match\_literal("\\n")(input), \]; any(closures); Maybe `as fn() -&gt; _` works too.
AIUI, Rust is specifically designed to be a more memory-safe replacement for C/C++, so as long as you are willing to port libraries that nobody else has already done, I would expect that anything you could write (safely) in C can also be written to execute at a similar speed in Rust.
Ah, thank you! I tried around and arrived at a similar solution a few moments before you commented, but somehow I assumed I had to cast every element to the trait object regardless - good to know I don't. I guess boxing the closures is my only option here? I'd prefer to not heap allocate if I don't have to. -- Unrelated to the original question, but your solution doesn't work exactly because of some lifetime constrains I don't quite understand: explicit lifetime required in the type of `input` lifetime `'static` required With a slight modification though, taking `input` as a parameter to `any`, and passing it there to the closures, instead of closing over `input`, makes my use-case work. Thanks again.
Well-explained, thanks. This sounds like a useful strategy.
That's very helpful, thank you as well!
I don't think that's going to work because those closures are capturing `input`; they can't be coerced to function pointers.
How would you describe the difference between a shared owner and a borrower?
Why is the Go code divided into sections using ASCII art?
That future is only created once, and lives as long as the stream lives. There is a future produced for every item, the `SenderFuture` returned by `Sender.send`. But that one is stack-allocated. Or, well, whatever you call stack-allocated in an async function - "stored in the future struct generated by an async function".
i broke the demo xD so first of all there is a bug when the engine is lit on and partially out of screen (the engine "fire" cross the screen) &amp;#x200B; then i managed to slam one object so hard the soft body "broke" inside itself
Aha yes - this makes much more sense. Thanks!
Thank you
Ah, no, I haven't written my thesis yet. I think they meant my thesis *work* Noria, which I link to in my comment elsewhere :)
Ah, no, sorry, this is happening 2019-06-29!
OK, I see! It seems that [cuckoo hashing](https://dl.acm.org/citation.cfm?id=2672711) is the predominant set of bells and whistles used to implement a lockfree concurrent hash table. Would [KLEE](https://klee.github.io/), [seer](https://github.com/dwrensha/seer), and [miri](https://github.com/rust-lang/miri) be good examples of symbolic executors? Thank you for the new rabbit hole, by the way!
I've done that for personal projects with large files because it's easy to read on a minimap of the code. Could just be broken into files though.
Yep. I used maude (pretty difficult to use) but the idea is the same.
hey guys, I'm using actix with an api. I'm currently trying to use a Form, and have the routing set up appropriately(I can reach the endpoint and get a couple hard coded test values returned correctly). I am struggling with how to get the actual values from the httpRequest payload. There has to be a better way than parsing the string(byte array) myself. Here's the part that I'm struggling with \`fn from\_request(req: &amp;HttpRequest, payload: &amp;mut dev::Payload) -&gt; Self::Future { // ToDo: find out how to deserialize from payload // ToDo: validate input if true { return Ok(ExpectedParameters { page\_number: SYSTEM\_CONFIG .read() .unwrap() .system\_config .api\_max\_page\_size, page\_size: SYSTEM\_CONFIG .read() .unwrap() .system\_config .api\_default\_page\_size, }); } Err(ApiError::InputError { field: "Input Must be an Integer".to\_string(), }) }\`
[Here](https://actix.rs/docs/extractors/#form)'s a link to the documentation on the `Form` `Extractor`; you should be able to make a struct with your desired properties and either get an instance of it passed in the params, or call `extract` on the request if you need it for other things.
Would you mind elaborating how you would implement `Parser` with an associated type in this case?
Is there a way to avoid `is_some()` followed by `unwrap()`? if include_glob.is_some() &amp;&amp; !include_glob.as_ref().unwrap().is_match(filename) { return false; }
The number one thing I tell people when they ask about Rust is to read the book and do the examples. The language is not optimized for day 1 productivity. That said, fun experiment!
&gt; For example, imagine 4 tasks which complete in 1s, 2s, 4s, and 8s repeatedly, executed on 4 CPUs: it will take about 8s to compute all 4, using 24s of potential compute-time for 15s of real work. That's a measly 2/3 utilization rate :/ In this scenario, you cannot do any better. And btw., there is 32s of potential compute-time, making the ratio even worse, but still the best you can do.
That's a novel idea; outside-of-the-box thinking.
This is what I usually do: if include_glob.as_ref().map_or(false, |glob| glob.is_match(filename)) { return false; }
My application requires me to build it by calling `cargo build --features somefeature` The thing is, I will *always* want to run this application using the `somefeature` flag. Am I correct in understanding that there is no way for me to specify that this flag should be provided by default (https://github.com/rust-lang/cargo/issues/6572)? If not, can you explain how I would do this?
does this work? .filter(|dirent| { if !dirent.file_type().is_file() { return false; } let filename = dirent.path(); match include_glob { Some(ref x) if x.is_match(filename) =&gt; {return false;} _ =&gt; {} } match exclude_glob { Some(ref x) if x.is_match(filename) =&gt; {return false;} _ =&gt; {} } true })
Perfect, thank you.
That’s too much boilerplate. The `or_else` looks quite good though.
&gt; The problem with the shrinkwraprs approach is that you have your data outside the widget itself. That's why my previous comment wasn't limited specifically to the use of `shrinkwraprs`. It is only part of the solution, rather than the solution itself. It provides the inheritance of methods from an inner field. You can therefore interchangeably use the custom widget as if it were the underlying widget that it is based upon. &gt; So you somehow need to carry it around together with your widget and can't directly get from the widget to the data. To reiterate what I previously-mentioned, there are two ways to implement exactly that, using native Rust concepts, without the need for any sort of GObject machinery. You can use either or both of: * `Rc&lt;T&gt;` fields in your custom widget's `struct`, both to store values and custom closures `Rc&lt;dyn Fn()&gt;`. * A `glib::MainContext::channel()`, sharing the `glib::Sender` with each of your inner signals, and then "attaching" the `glib::Receiver` with a "move closure" which captures state from the construct method for reuse on each received event and its payload of data. &gt; If you want to extend something in a way so that you can pass your new thing to the C libraries in a way that they can handle it accordingly. I am actually using this approach with a firmware widget library that integrates with GNOME Software, written in C. It works in C as well.
Time to learn a language should never be important. After all, you only learn a language once at the beginning of your career with that language. The goal is rarely to learn a language for one program, and then drop it shortly thereafter to never use again.
There is a crate `X`, which provides a struct `HelloWorld`, and a function `fn a(hw: *const HelloWorld)`. In examples where others are using `some_function`, I see they call it like so: `some_function(&amp;hw)`, where `hw` is an instance of `HelloWorld`. When I do that, I get an error like so: note: expected type `*const X::HelloWorld` found type `&amp;X::HelloWorld` Why is that?
wait a minute... in the Go code, every function definition and type definition is *inside main*? Been a long time since I've seen that.
Maybe, but this is a good example of where *ought* and *is* are two different things. The difference between someone getting up to speed with a programming language in a week (I've seen it happen with Go) versus perhaps months with Rust (with maybe a couple false starts thrown in) can be palpable. Maybe in comparing a best case and a worst case, but I have no problem with the belief that Rust takes measurably longer to get up and running with, which probably has a lot of variance based on prior experience. Saying that *ought* not be important is a fine, of idealistic, opinion to have. But there's no question that it *is* important in certain circumstances, depending on resource constraints.
Thanks for opening the issue! I'm really excited to see where it'll go and I'm happy to contribute to the discussion (although I'm afraid I'm too new to Rust to really be of much help right now... give me a bit). I'll post back in a bit when I've had a bit more experience.
Reporting back: wasn't able to reproduce compiler error I got earlier. I guess that was some silly syntax error I owe to lack of sleep. Anyway, thanks again!
That's not entirely it though. The time needed to \*really\* learn C++ is measured in decades. That's a big factor considering humans (and programming languages) have lifespans also measured in decades.
Assuming `transaction` is of type `*mut tkvdb_tr`, you should be able to call them like ``` (transaction.begin)(transaction); ``` (the parens are necessary to indicate to rust that it isn't a method call, but using a member function.)
I'm not sure what your definition of "really" learning C++ is, but I've been programming for about 5 years and programming in C++ for about 3 years. I can develop a lot of applications that seem to be useful since someone is willing to keep paying me. I'm not at the point where I'm writing large amounts of compile-time template meta myself yet, but I've not been called out for having C++ on my resume in spite of that.
The most likely cause is that you have two incompatible versions of `X` in your dependency graph. If function `a` comes from a different crate than `X::HelloWorld` then check the `Cargo.toml` of the crate that `a` is coming from and use the version of `X` that that crate is using.
Could you not limit the # of threads and therefore pack 3 of the 4 tasks on one thread and the 8s task on the other? Now we use 2 threads instead of 4
When I do that, it says: help: `transaction` is a native pointer; try dereferencing it: `(*transaction).begin` But adding a deref (e.g. \`(\*transaction.begin)(transaction);\`) results in the same error message.
Turbo Pascal? :) This is not mandatory in Go; OP chose to do it that way.
Also, when I try this: (*transaction).begin(transaction); the compiler says (pointing to "begin"): `field, not a method`
You need the correct parentheses. Also, since it's an option, you'll need to unwrap it or handle it not having the correct field: ``` // If you don't care about it panicking on None ((*transaction).begin.unwrap())(transaction); // If you do if let Some(begin) = (*transaction).begin { begin(transaction); } ```
&gt;((\*transaction).begin.unwrap())(transaction); Thank you!! I had read about Options but completely overlooked that they are autogenerated by bindgen. It's so great to be unblocked after a few hours of trying :)
Sweet Rustacean God, I will end up playing this game just to see how it makes people unable to read before posting into a subreddit.
You can do that in Rust too.
!RemindMe 13 hours Gonna roast your code :&gt;
I will be messaging you on [**2019-06-24 16:46:56 UTC**](http://www.wolframalpha.com/input/?i=2019-06-24 16:46:56 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/rust/comments/c4dma1/for_your_amusement_i_am_learning_both_rust_and/erwm69v/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/rust/comments/c4dma1/for_your_amusement_i_am_learning_both_rust_and/erwm69v/]%0A%0ARemindMe! 13 hours) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
You shouldn't have to cast it, but try `&amp;hw as *const X::HelloWorld`.
Another option, maybe better for some other case: if let Some(glob) = &amp;include_glob { if glob.is_match(filename) { return false; } }
I’m not that big into rust yet, but I always wondered about user space threading and how it works in vanilla rust. Are there issues with parallel connections if you don’t use a third party threading lib?
&gt; Weird capitalization, weird design decisions, weird error handling, etc. Of course plenty of people say the same about Rust :-D
Sure, but the scenario was different. We have 4 cores. If we run 2 threads then we are wasting cpu power and we are loosing if one of those three task take longer. And total unused cpu time remains the same, because the program still works 8 seconds. The only gain is less thread context switching. I mean, I perfectly understand know what is the idea and I agree with that. Just wanted to point out that in this particular scenario, it [almost] doesn't matter.
Ah, I see.
&gt; only does the translation from the utf-8 Rust string to the utf-16 JavaScript string the first time the expression is evaluated. It might be better to use a proc macro so this can all be done at compile time instead.
I haven’t had any issues yet running it on my laptop (Quad-Core pentium) and handling multiple connections. I’m planning on moving to Actix and optimizing the number of threads I need to spawn. Also async/await will be a nice to have once it stabilizes.
This seems like a post for r/playrust
Oh! My bad. Sorry about that.
No need for apologies. You keep on posting your helpful stuff, just thought it might hit a bigger audience who needs it there.
I've done the same thing, though it's been quite a while. Splitting into modules is normally better.
&gt;So far, Go won the "creating nice things quickly" ribbon. I believe that's literally the whole point of the language. The languages are not in competing spaces.
&gt; To reiterate what I previously-mentioned, there are two ways to implement exactly that, using native Rust concepts, without the need for any sort of GObject machinery. You can use either or both of: &gt; * Rc&lt;T&gt; fields in your custom widget's struct, both to store values and custom closures Rc&lt;dyn Fn()&gt;. Maybe I'm misunderstand what you mean here, but how does that differ from carrying your struct and the widget around separately everywhere? Sure, your struct contains the widget but you can't get from the widget to the struct. So you always need to pass your struct around and e.g. can't get your struct directly from the first parameter of a signal callback anymore. Do you have some example code for what you mean here? &gt; * A glib::MainContext::channel(), sharing the glib::Sender with each of your inner signals, and then "attaching" the glib::Receiver with a "move closure" which captures state from the construct method for reuse on each received event and its payload of data. Yes that works and that's also how I often structure things, but it requires converting from a normal GTK signal-callbacks model to something message-passing-based. That requires a different design from the beginning and doing things differently than how GTK is used from any other language. It's also getting you closer to a programming model like the one used by relm. Personally I think it's nicer though. It converts your code from callback spaghetti code into something well-structured where all control logic happens from a single place. &gt; &gt; If you want to extend something in a way so that you can pass your new thing to the C libraries in a way that they can handle it accordingly. &gt; I am actually using this approach with a firmware widget library that integrates with GNOME Software, written in C. It works in C as well. There are cases when this works, but generally you'll need a proper subclass to be able to override virtual methods and to have a distinct `GType` so that it actually looks different at the C level. For simple widgets what you describe will work, yes.
Rust offers subtype polymorphism through trait objects. This is similar to interface-only inheritance. Rust doesn't have an equivalent of implementation inheritance.
(New to Rust) I thought that Rust didn’t need a runtime? I kinda get that you need a loop to make sure you can catch any incoming requests but... kinda..
Sry, I don't get it? What do you mean by that?
I think a lot of people learn languages in their spare time.
In addition to what Omniviral said about `std::map`, even the hash table `std::unordered_map` gives this guarantee. Which is unfortunate, because it forces the hash table to be node-based as well; it pretty much has to use chained hashing. It is not possible to correctly implement the interface with something like a Swisstable.
&gt;Too coarse-grained and resources are under-utilized. I like this explanation very much, there is yet another factor to keep in mind and this is the cache size. Cached aligned computation have a incredible impact on execution time, and spreading your computation along different cores has a severe impact. Cache usage is completely dependent from your own code and data structures used. &amp;#x200B; As @matthieum said, there is a trade of between fine and coarse grained parallelism, and I am afraid it is up to you to find the tradeoff. I would recommend to run some benchmarks, after all. the iterator pattern used in rayon will make kind of "easy" to create some versions of the code. &amp;#x200B; cheers, and good luck!
yep its totally possible. refer to [The Cargo Book](https://doc.rust-lang.org/stable/cargo/reference/) for more. &amp;#x200B; a good example would be [cicada](https://github.com/mitnk/cicada), a shell written in rust. the binary file is a fully featured posix shell. the library exposes the parsing and tokenizing logic, so others could extend cicada!
&gt; why isn’t the Rust community contributing stuff like this? Microsoft has a lot more money to throw at research than the Rust community.
The DSL disallows sharing resources (`static mut` variables) between the cores so there's never a `lock` on memory shared between the cores. Cores can communicate using message passing (`spawn` API) but this is implemented using lock-free queues so again there's no `lock` on shared memory in that case. If you really want shared memory between the cores (rather than the move / send semantics of message passing) you can use a [spinlock](https://docs.rs/spin/0.5.0/spin/) in a plain `static` variable (declared outside the `#[rtfm::app]` block). RTFM doesn't provide these abstractions because they are neither deadlock-free nor have bounded execution time, plus they don't need information from the DSL to be memory safe.
I don’t think we should particularly care about the time of coding from “zero knowledge”. If you optimized for this, I expect Python would win over both Go and Rust. What we do care about is productivity once you are up to speed.
If only this was the correct subreddit... /r/playrust
matthieum's point was that in some cases, fine-grained works better than coarse-grained. In their example, *if you can* parallelize each task individually, and then consecutively execute the four tasks, you might achieve a higher speedup than if try to execute all four tasks in parallel.
Without `const generics` no, it's not possible. You could write a macro that makes such a transformation. In my code I have a snippet like ```rust let mut arr = [0u8; 32]; for (i, chunk) in bytes.chunks(32).enumerate() { let chunk = if chunk.len() == 32 { unsafe { mem::transmute(chunk.as_ptr()) } } else { arr[..chunk.len()].copy_from_slice(chunk); &amp;arr }; let address = self.address.get_sub_address(actual_index + i).address(); pwasm_ethereum::write(&amp;address, chunk); } ```
It's possible but in my experience not at all a typical thing to do, even for smaller programs.
`miri` doesn't do symbolic execution; as the link for `seer` says, it is specifically a fork of `miri` that adds symbolic execution.
That happens extremely easily. I made a minigame out of trying to unentangle it.
Thanks for the tip, but I've already solved my real problem by just giving up and switching to `postgres`.
I think this should be in the book.
*ONE OF US! ONE OF US!*
It's even in the name, "Go"!
Then you don't "really" know C++. You can know it well enough for wide range of tasks, but you have myriad of dark corners to uncover yet.
I still feel that implicit returns (is that what you call it?) fn dummy(&amp;self) -&gt; Option&lt;bool&gt; { Some(true) } is weird, instead of fn dummy(&amp;self) -&gt; Option&lt;bool&gt; { return Some(true); } And the usage of snake_case and closure syntax. Almost everything else about Rust is awesome imo.
The switch is a general performance improvement for the CLion IDE. Regardless of packages you use.
Currently, hardware overlays are use at the discretion of the display server. There is possibly work being done to achieve more involvement from the clients, but I'm not fully aware of everything being in progress.
If you don't think about them as implicit returns it becomes a lot easier. The same technique works for setting things from any code block (eg, `if`, `match` or even just a block of code). ``` let x = { // This may seem like an obvious place for a function but is actually great for managing ownership } ```
I get the feeling Rust is going in the same direction. Don't get me wrong; I love the language, but there is a *long* list of things to learn (some of them being hacks around features which were never implemented well in the first place, e.g. `as` operator and several aspects of Cargo).
I've started learning rust recently, and I like this very much. At first it seemed strange, but when I learned that it is an expression syntax, which is also used by if-else and (sometimes) variable declarations, I changed my mind What feels weird for me is you need to end statements and only statements with ;. It's strange considering in most languages you either put it on every line, or don't put it at all
Are you me?
C++ is a weird language in that the more you know about it the more you realise you don’t know it. You can program for years and then one day you’ll hit a problem which forces you to look under the covers and you realise you have no idea what’s going on in your program.
(Your block of code is cut off for me) For me it's more of the lack of a clear "Hey, I'm handing off a value here!". It's strangely not-explicit for Rust, which puts such a heavy emphasis on explicitness. I guess there's an argument to be made that just like ```break``` statements in Java, it's not clear what you're returning from. But Rust does have notations for ```for``` loops and ```loop``` in case of ```break``` and ```continue``` so I think it's kinda moot.
Why does this not work? use std::str::FromStr; trait Parsable: FromStr where &lt;Self as FromStr&gt;::Err : Debug {} struct Foo&lt;T: Parsable&gt; { phantom: PhantomData&lt;T&gt;, } impl&lt;T: Parsable&gt; Foo&lt;T&gt; { fn bar() { let x = T::from_str("5").expect("oh no!"); } } It can't seem to see that `&lt;T as FromStr&gt;::Err : Debug`. Why is that?
Isn't Go the name of a very complicated game that is hard to learn quickly?
I was just looking for something of the kind! I'll definitely check it out :)
std::mem::swap is a pointer swap internally. pub fn swap&lt;T&gt;(x: &amp;mut T, y: &amp;mut T) { unsafe { ptr::swap_nonoverlapping_one(x, y); } } I think it's just a safety wrapper that takes advantage of the fact that x and y can't be overlapping if you're using references and therefore following the borrowing rules. There's also https://doc.rust-lang.org/std/ptr/fn.swap.html
&gt;std::mem::swap is a pointer swap internally. This is wrong (or at least misleading). That would be only true if T would be a pointer. The implementation of ptr::swap\_nonoverlapping\_one swaps whatever the pointers are pointing to. If T is small anyway there is no nothing to worry about. &amp;#x200B; \`Option&lt;Rc&lt;...&gt;&gt;\` should be only pointer-size (or 16-bytes in worst case). So there is no need to avoid mem::swap. Traversing the Tree should be far more expensive than such a simple swap.
Excellent :)
The complaint was that your advertisement is annoying and pushy. And you answer with an invite to "get to know you more" which is basically an ad.
Yikes, I have so many things on my list to do, I don't know where I want to begin. This week I'm probably going to focus on rebuilding my blog, for a number of reasons: * I'm migrating repositories and projects as-needed back to GitHub from Gitlab. I used to use GitLab because it had more to offer me in terms of free features, but nowadays there are so many devops tools that I don't use while GitHub provides everything I do need, while also letting me unify my contribution profile and making repo management easier. * I rarely ever post to my blog as-is; it is a graveyard with maybe 2 posts over the last 6 months since I started it. My new goal will be to write weekly, typically in the form of an update for one of my projects with a bit of look-ahed (like I'm doing here). That way it can double as a motivation to stay organized and get something done every week. * I ~~need~~ want a new blog engine. Nikola is honestly pretty great, but I'm constantly greeted by waiting for Python modules to load every time I invoke its CLI. Also I need an excuse to add yet another project to my growing list of "re-do this thing in Rust".
That name, tho :')
\`std::mem::swap\` is the right solution here. &amp;#x200B; Your Option&lt;Rc&lt;...&gt;&gt; is pointer size (8 bytes). Rust is smart enough to represent Option&lt;SomethingEquivalentToAPointer&gt; as a pointer. None is basically encoded as a null pointer.
I think it is important to consider. It is totally valid to then decide to not optimize for this metric though.
That's exactly what `std::mem::swap` does, it moves the values to the opposite locations. Moving a heap-allocated value like `Rc` does not involve moving the entire value being pointed-to, just the pointer itself.
Also see https://www.reddit.com/r/rust/comments/c443s6/cht_a_lockfree_concurrently_growable_hash_table/ for a discussion on a concurrent hashmap
Sounds interesting, how do I enable it and is it a CLion thing only or will it work for Rider? I mean, I still don't expect it to be a meaningful improvement, I have an i5-650 and a whopping four gigabytes of RAM, plus faulty SATA ports that sometimes randomly stop seeing the disk, causing kernel panics on read/write errors (*fun*). So I'd say VSCode it is for me until I get a new PC.
https://github.com/carllerche/loom might be of use
&gt;**How?** cht implements an open-addressing hash table that uses tombstones for deletion - I didn't want to get overzealous and end up with something non-functional by trying out fancier techniques. Preshing's resize is blocking, is yours the same or is it lock-free?
Any unsafe code in there? If not, you might want to throw in `#![forbid(unsafe_code)]` to ensure you don't have it. This will make any `unsafe` added to the codebase fail to compile.
Search rust in the settings, you will find the macro expansion engines in one of the menus. I haven't tried Rider. CLion is working pretty well for rust.
I've tried to replicate the performance numbers for my work (I'm a researcher in concurrency) and I found it impossible. The experiment is, in my opinion, *grossly* skewed to help Cuckoo Hashing win out. Here's why: 1. Dynamic allocated memory is used is cuckoo hashing and therefore a memory reclaimer is necessary, the experiments don't use one so an enormous performance advantage is available. It's actually typical to not use one but we should take that into account. The reason a reclaimer isn't used is that one day a zero-cost reclaimer could be invented and we then wouldn't have to rerun all our experiments. 2. Hopscotch Hashing has its entries stored behind a pointer (section V, second paragraph: "In all the algorithms, each bucket contains either two pointers to a key and a value, or an entry to a hash element, which contains a key and a value."). The **whole** point of having blocking write operations with Hopscotch Hashing is to have the keys and values stored *flat* in the table, improving cache efficiency. Unless the original Hopscotch Hashing code did this (which I really doubt since here's the [link](https://sites.google.com/site/cconcurrencypackage/hopscotch-hashing/download-1)) it appears the code was modified in this way. One would have to reach out to the original authors for clarification. 3. I'm finding in my own experiments that using a low load factor can actually *decrease* performance as the table becomes more contended since only a small key space is being used. I've found that separate chaining performance **increases** as the load factor does. I wouldn't pay much regards to Cuckoo Hashing.
My resize is lockfree. If a thread dies during a resize, get and insert threads will complete/help with the grow operation if they access a redirected bucket or insert past the max load factor.
Cool. I haven't read the code yet (I'm doing it now) but that's exciting. So you went with the Cliff Click lock-free resize option then? &amp;#x200B; Every time I see a lock-free hash-table post I always get hyped. I really should throw my hat in the game.
Wrapping my head around how to use string slices as input parameters for my functions and cloning their values.
I just skimmed your Rust version and noticed a lot of `unwrap`s and `match`es in the context of error handling. For example: fn read_config() -&gt; Result&lt;Config, &amp;'static str&gt; { match read_to_string("cfr.cfg") { Ok(str) =&gt; { // How do I make this trait consistent with clean error handling? let config: Config = toml::from_str(&amp;str).unwrap(); Ok(config) }, Err(_) =&gt; Err("Error opening config") } } could be written as fn read_config() -&gt; Result&lt;Config, &amp;'static str&gt; { let s = read_to_string("cfr.cfg").map_err(||"Error opening config")?; let config: Config = toml::from_str(&amp;s).map_err(||"Config parsing error")?; Ok(config) } You could get fancier by creating your own `Error` type and implement [conversions from](https://doc.rust-lang.org/std/convert/trait.From.html) the lower-level errors you deal with (e.g. `io::Error`) for it. The try operator takes advantage of those possible conversions so that you don't have to `map_err` them manually.
I'm trying to understand the difference between `const` and `static`. I read in the RFC that `const` is a possibly mutable actual value which `static` is a variable. Is that correct? I am also confused about this. I've heard people talking about the difference between a variable and I value, but you almost always have to refer to values via a variable, right? Like if you declare `const N = 5` then how is that different and considered a value vs `let N = 5`?
thanks for everyone, now I know std::mem::swap is smart enough
Hi! Thank you for this thread. I'm learning Rust. I've been a professional software developer and systems architect for decades. And Rust's borrow system and trait architecture is giving me challenges I haven't had to think about yet. So this question is about borrowing and traits: I thought it a good idea to have my custom structures implement the From trait accepting either the str or the &amp;str type. I saw that the String structure implements From&lt;&amp;str&gt;. If I do so, the compiler complains that &amp;str has an unknown size at compile time. This surprised me: according to what I read, &amp;str is a length and a reference, and the size of each is known, which is why it is chosen as a data type instead of str. So how does String implement From&lt;&amp;str&gt;?
Perhaps the better way to think of it is that in Rust, everything has a type and everything has a value. When you omit the semi colon you are in fact being explicit about the type and value of that block. The time your not being explicit is when your function doesn't return anything, or your block ends with a semicolon. In these cases you are implicitly returning the unit type (an empty tuple) which is both a type and value in Rust. As an aside Rust actually works quite hard not to be explicit where it can infer your meaning. Eg. ``` let x: String = y.to_string(); ``` Rust can work out the type of x so you can omit it ``` let x = y.to_string(); ``` However, this is not always the case. When you zip an iterator, what are you zipping it into? Rust can't always work that out so sometimes you need to tell it. If you're immediately returning the zipped value from a function it can infer that from the functions signature, otherwise you can tell it what type the variable containing it is, or tell the zip method directly with the turbo fish syntax.
It is actually rather easy to learn. There aren't very many rules. But hard to master. In fact it has many subtle patterns that are amusing to find out naturally. Go would have been a good name for Rust in that sense. :)
You might really love SwiftUI. It’s a next generation thing. Leaves everything else behind in an academic sense. It’s a super high level way of coding. Declarative vs Procedural.
I’m trying to implement the card game ‘pazzak’ in rust
Hi, I made a quick [playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=7de25b329309d525b0f4070cb46b2190) where I implement `From&lt;&amp;str&gt;`, if you can show your code I'm sure someone will be able to point out what is the problem.
Thank you! My code was similar, but without a life time. I didn't think I'd need a life time because I was going to clone the argument's value.
You might be interested in the [const](https://doc.rust-lang.org/reference/items/constant-items.html) and [static](https://doc.rust-lang.org/reference/items/static-items.html) pages in the reference. From what I understand, constant items will be copy/paste into your code during compilation whereas a static is unique and has a fix place (and address) in your program. Every access you make will be to this unique instance.
You don't need the lifetime. I made an implementation that is more String-like: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=7572cdd91959d6289f4d42714dec9dae Instead of storing a reference to a string-slice (i.e. &amp;str), I clone the slice into a new buffer, so 'MyString' actually owns its underlying buffer, just like 'String'.
If you call `clone` on a `&amp;str` you will only copy the pointer and the lifetime of your struct will be tied to the `str`'s lifetime. If you want a deep copy you can use `to_owned` or `to_string` for example and it will give you a `String` that you'll have to store. You won't need a lifetime in your struct if you make this change.
That's a matter of definition. To me, OOP means: "the type system allows polymorphism". The fact it is done with inheritance or with traits or with generics is secondary.
Thank you, that fits my ideas quite nicely.
[v0.24.0](https://github.com/iliekturtles/uom/releases/tag/v0.24.0) of [`uom`](https://github.com/iliekturtles/uom) (type-safe zero-cost dimensional analysis) was released last week! This week I'll be reviewing some PRs and adding support for `i128`/`u128` as an underlying storage type.
You could have an `is_dirty` method on the trait, but it kind of sounds like what you really want to do is cache the values of inputs somewhere outside the node and recalculate them only if they _actually_ changed.
On nightly, writing similar code is possible already. See([full Playground example](https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018&amp;gist=eea9e2c52197ea53f5ff17edf2fb4f5d)): let good = const_windows::&lt;u8, 2&gt;(&amp;[1, 2, 3, 4, 5, 6]).all(|[prev, next]: &amp;[u8; 2]| { prev &lt; next }); fn const_windows&lt;'a, T, const N: usize&gt;(slice: &amp;'a [T]) -&gt; impl Iterator&lt;Item=&amp;'a [T; N]&gt; { slice.windows(N).map(|x| unsafe { &amp;*(x.as_ptr() as *const [T; N]) }) }
Is there anything that prevents you to launch a stable version?
Actually, you don't even need `unsafe` here - since 1.34 you can use `TryInto`. It should automatically optimize out the bounds checks, although I haven't verified that.
&gt; I am feeling fairly confident that I would write this program again much faster, with what I have learnt in the process I'm curious about how *second attempts* would compare, now that you know a bit of both languages. Would you be willing to do that test all over again with a different project?
Amazing! Any ideas how well does it handle memory fragmentation outside of these benchmarks?
Sounds `salsa`-like.
Of course time to up to speed matters.
In a professional setting, where the goal is to Get Things Done(tm), the difference between a fast-to-learn language and one that takes weeks or months to be productive in is that the former gets adopted and the latter does not, because rarely is the goal of a project to showcase a language - it’s to solve an actual problem, and ensure that solution can be supported after its creator has moved on. If Rust already had captured the market this cost would be mitigated by the number of qualifies candidates who already knew the language (like C++), but it hasn’t yet, so it *matters*.
Time to learn a language is big factor when proposing brining it into the fold at work. If the turn around time from open editor to POC is increased by weeks because the language has a long ramp up time, it's likely a bad candidate for work. If the long term gains are serious enough, I could make a case that might be supported. But right now if we were looking to onboard to _either_ Rust or Go, Go would win hands down because even with zero experience in it I was able to bang out a dumb service in a few hours, had enough room for making it friendlier to use and get the deployments completely figured out.
Looks awesome, need to pick up a board to play around with it.
You could have getters and setters: ``` trait T { fn get_is_dirty(&amp;self) -&gt; bool; fn set_is_dirty(&amp;mut self, val: bool); } ```
It seems to optimize them out, although the safe approach requires an additional `where &amp;'a [T; N]: TryFrom&lt;&amp;'a [T]&gt;` bound and does not work for array sizes greater than 32. This shouldn't be too much of a problem for matching on arrays though.
&gt; Hopscotch Hashing has its entries stored behind a pointer (section V, second paragraph: "In all the algorithms, each bucket contains either two pointers to a key and a value, or an entry to a hash element, which contains a key and a value."). The whole point of having blocking write operations with Hopscotch Hashing is to have the keys and values stored flat in the table, improving cache efficiency. Unless the original Hopscotch Hashing code did this (which I really doubt since here's the link) it appears the code was modified in this way. One would have to reach out to the original authors for clarification. I couldn't think of any way to unbox key/value pairs except by restricting users to integer key and value types, so cht makes the same *faux pas* as the authors' implementation of hopscotch hashing. Side note: I don't have a way to read this paper. &gt; I'm finding in my own experiments that using a low load factor can actually decrease performance as the table becomes more contended since only a small key space is being used. I've found that separate chaining performance increases as the load factor does. I considered using separate chaining for cht, but the thought of designing around the potential data races during insertion was a little much for me. What kind of load factors are we talking about here?
&gt;I couldn't think of any way to unbox key/value pairs except by restricting users to integer key and value types, so cht makes the same *faux pas* as the authors' implementation of hopscotch hashing. I was only commenting on their C++ implementation and not your code, forgive me if I caused any offence. They also used integer keys during their benchmark for which there is no need to hide behind a pointer. I've seen generic `Atomic` wrappers in Rust for machine sized types and below but I think hiding behind a pointer might be the best way to go if frequently moving large items around the table. &gt;Side note: I don't have a way to read this paper. There's an open link [here](http://www.excess-project.eu/publications/published/CuckooHashing_ICDCS.pdf). &gt;I considered using separate chaining for cht, but the thought of designing around the potential data races during insertion was a little much for me. Oh absolutely! It is such a cool problem to think about. &gt;What kind of load factors are we talking about here? The levels of load factor in the paper and which the standard Cuckoo Hashing can stand - 20% to 40%. They use both in their paper. But Hopscotch Hashing can go much much higher (\~90%).
I don't think having few rules correlates with being easy to learn. The universe seemingly has few rules too, but that doesn't make physics easy.
Hate is a big word and Rust doesn't have so many bit flaws. Most of us probably got annoyed by the gymnastic you have to do when the structure you want should be self referencing, or a graph. Another problem is the difficulty in finding the available methods due to the use of traits and the lack of good solution in documentation for that.
Hate is a strong word but I will say that the syntax is not beautiful. Python and Go has nicer looking syntax. I guess because of all the special chars used in Rust for lifetimes and references.
To expand on this: As /u/leudz said, the main difference is that `static` represents a precise location of memory that lives for the whole runtime of the program. `const` may be copy and pasted, which can generate more performant code, but also increases the memory required. Also, a `const` value can have a destructor running when it goes out of scope, whereas the destructor for a `static` value never runs. Furthermore, you can have `static mut` values, but modifying them is `unsafe`. The following criteria are suggested in the docs when deciding between `static` and `const`: &gt; It can be confusing whether or not you should use a constant item or a static item. Constants should, in general, be preferred over statics unless one of the following are true: &gt; &gt; - Large amounts of data are being stored &gt; - The single-address property of statics is required. &gt; - Interior mutability is required.
I'm having trouble understanding the rust ecosystem and how to import certain functions. &amp;#x200B; I want to generate a BigUint random number. On this webpage [https://huonw.github.io/primal/num/bigint/trait.RandBigInt.html](https://huonw.github.io/primal/num/bigint/trait.RandBigInt.html) it says that there is a `num_bigint::RandBigInt` I can use. But when I put use num::bigint::{BigUint,RandBigInt}; &amp;#x200B; I get an error about not being able to find `RandBigInt` in `bigint`. I'm not sure what's going on here, can anyone help?
&gt; Cool. I haven't read the code yet (I'm doing it now) but that's exciting. So you went with the Cliff Click lock-free resize option then? To be honest, I never reviewed Dr. Click's hash table; I went out of my way to *not* look at any existing implementations of concurrent hash tables. In cht, every time the bucket a thread wants to view encounters a redirect tag during lookups and insertions, it completes the migration itself before traversing to the next bucket array. So the algorithm is resilient to the death or indefinite suspension of any other thread. &gt; Every time I see a lock-free hash-table post I always get hyped. I really should throw my hat in the Yes, excellent idea! If a degreeless student like me is able to create a somewhat functional lockfree hash table, you have a 100% chance of creating something entirely workable.
Mad skills annoucement right here That whole journey of replacing the macros with functions was pretty cool and must have needed a great ammout of software architecture.
&gt; Like if you declare &gt; const N = 5 &gt; then how is that different and considered a value vs &gt; let N = 5 &gt; ?
I would not say that I "hate" it, but the syntax is sometime weird in my opinion: * The ~~ugly~~ unusual inclusive range operator that is `..=`, but that can be written `...` in a pattern, * The `|` for the closure parameters that is not C-like at all, unlike the rest of the language, * The colon `:` to initialize the fields of a struct instead of a simple equal, * The "turbofish" `::&lt;&gt;` to make a generic parameter explicit, * Not syntax related, but also a cosmetic remark: `StrBuf` would have been a better name than `String` IMO. The difference of `String` vs `&amp;str` often confuses the newcomers. That's not a real criticism: a language is not its syntax, but sometimes I am a little sad that the language designers did not have a time machine to see the future, at that time.
Yeah, that's what I currently have but I'm not too happy with that. Each struct that implements the trait needs have that `is_dirty` field and also needs to re-implement the getter and setter in the exact same way. That's a lot of code duplication that I'd rather avoid. If the project ends being more than just a prototype, there will potentially be hundreds or even thousands of different node types.
The nom 5.0 betas have been treating me very well in my recent project, so I'm really excited about an official 5.0 release!
Most of it was having an idea, testing it on a [reduced version of nom](https://github.com/geal/nomfun), then once it was proven, gearing up to rewrite everything :D
yes it is a browser based game but we briefly experimented with using websockets in unity beforehand. websockets offer a lot of resilience compared to a regular tcp connection, we found it generally just easier to have the entire message delivered (or not) than trying to reconstruct them manually
Go wins the "weird return semantics" competition if you look at named returns and deferred functions: ```Go func foo() (bar string) { defer func() { // This function bar = "eggs" }() bar = "spam" return // Runs here, changing the returned value } ``` So you could defer some closure at the start of the function that does just about anything with the things it closes over, including the return value. Can be useful for things like turning a panic into a normal `error`, but a great way to cook up some spaghetti code in most other cases.
How was the rewriting process itself? In my head I am imagining late nights of after-hours mechanistical code-typing while drinking slews of coffee with vaporwave music jamming in the background... :)
&gt; functions instead of macros Oh wow I am right in now. I've used `nom` a lot in the past and it's absolutely wonderful but macros cause a lot of friction in the editor and error messages. This is a massive usability win, thanks so much!
One way is to use composition: struct Node&lt;T&gt; { is_dirty: bool, data: T } You could even get fancy and create a DerefMut instance for Node that automatically marks it dirty when you grab a mutable ref to the data field.
The most important part was to plan the work and dividing it up into smaller bits: [https://github.com/Geal/nom/issues/903](https://github.com/Geal/nom/issues/903) Then calling for help, because there was no way I could have done all of this by myself. It has taken 3 months, not much late night coding, mostly on the weekends, but there was definitely some chiptunes involved ;) The documentation is where I got most of the help, and it was good to get feedback from people trying to write examples. On the way to the release, publishing alpha and beta releases to let people try it out, regular teasing on twitter. Showing off performance gains builds up interest, and comparing macros and function approaches was good advertising. The last weeks were mostly about polishing, fixing new bugs, and preparing for the actual release. I was still fixing bugs this morning :D
I wish it were easier to read and understand what is going on without having to deep dive so far in the docs. It's a problem C++ has as well tho, so not a deal breaker.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/programming_jp] [nom parser combinators 5.0 release: replace macros with functions, better errors](https://www.reddit.com/r/programming_jp/comments/c4nz91/nom_parser_combinators_50_release_replace_macros/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
I don't know which version this documentation is referring to but according to [the official doc](https://docs.rs/num/0.2.0/num/bigint/index.html) the bigint module had been moved to its own crate [num-bigint](https://crates.io/crates/num-bigint). You can also access the doc via `cargo doc` this way you're sure it's the right one and it's the right version.
Ok, I'll give it a look. But if bigint is a re-export, why can I access `BigUint` but not `RandBigInt`? Shouldn't they all be exported together?
Congratulations! Excited to try and update my library for nom 5.
It depends on what you mean by "learn". It's very easy to learn the rules because they're simple. They're about as complex as checkers. There's a couple really important consequences to those rules that take slightly longer to learn. Much like chess, though, learning tactics and strategy is what will take up the overwhelming majority of the time. There's a lot of legal moves, but usually only a few of them are really good.
That and making it easier for a team of programmers to maintain systems that they didn't write, and which most of them only have a year or two of experience working with. I think that's a big part of the no-generics situation, the idea that you should be able to look at a line of code in isolation and have a decent chance of understanding exactly what it's doing.
I've been using nom on a daily basis and I'm very happy with it. I'm really impressed by the quality of your work. Out of curiosity, will we see a release of cookie-factory in the near future?
procedural macros, sort of like the `getset` crate is my only idea
yes, that's the plan. I have rewritten cookie-factory in the same style as nom 5, you can already test a beta version: [https://crates.io/crates/cookie-factory](https://crates.io/crates/cookie-factory) I will release it next week
Stellar work!
Yeah there's nothing I hate about Rust. There are some things I miss from Python though. Comprehension syntax is one of my favorite things, and .filter(...).map(...).collect() doesn't feel as easy to me. Old habit probably. Also a concise ternary operator is nice. Maybe there are macros for both of those things that I should look into? But I feel like using a macro for simple things would go against the grain in Rust.
I just finished several of my first real apps in Rust. Now, speaking strictly relative to working with Kotlin for 3 years and with Java for 15 years: my biggest disappointment is limited functionality with regards to abstractions. And two minor things: the semicolon and boilerplate around “constructors”. But. Overall I got about the same amount of code in Rust compared to Kotlin implementation with mostly the same code complexity. And the performance gains... Jvm: 2500mb ram consumed, 1439 seconds taken. Rust: 3.4mb ram consumed, 60 seconds taken. All of which completely outweighs Rust’s growing pains.
&gt; but how does that differ from carrying your struct and the widget around separately everywhere Because the struct is your widget. You only pass around the struct. &gt; So you always need to pass your struct around and e.g. can't get your struct directly from the first parameter of a signal callback anymore. There are many ways in which this can be achieved, although in most cases it is not necessary. With stable Rust, you can use a wrapper type which places the custom widget in a `Rc&lt;RefCell&lt;T&gt;&gt;`, and then program your widget from the wrapper. With nightly Rust, similar is possible through arbitrary self types, so you can design a method that takes either `Rc&lt;self&gt;` or `Rc&lt;RefCell&lt;self&gt;&gt;`. You can either place the entire widget in a `RefCell`, or only specific fields that you want interior mutability with. Below is an example. pub struct Widget(Rc&lt;RefCell&lt;WidgetInner&gt;&gt;); pub struct WidgetInner { button: gtk::Button, field: String, signal: ExampleSignal } pub type ExampleSignal = Rc&lt;dyn Fn(&amp;mut WidgetInner)&gt;; impl Widget { pub fn new() -&gt; Self { let inner = Rc::new(RefCell::new(WidgetInner { button: gtk::Button::new(), pub field: "N/A".into(), signal: Rc::new(|_| ()) })); { // Program inner's signals. let signal = inner.borrow_mut().signal.clone(); let inner = inner.clone(); let button = inner.borrow_mut().button.clone(); button.connect_clicked(move |_| { (*signal)(&amp;mut *inner.borrow_mut()); }); } Widget(inner) } } impl Widget { fn connect_signal&lt;F: Fn(&amp;mut WidgetInner) + 'static&gt;(&amp;self, func: F) { (*self.0.borrow_mut()).signal = Rc::new(func); } } You can then use your custom widget like so: let widget = Widget::new(); widget.connect_signal(|inner| { inner.field.push_str("text"); }); &gt; you'll need a proper subclass to be able to override virtual methods and to have a distinct GType so that it actually looks different at the C level I would argue that there is no need.
I generally agree and will be migrating to proc\_macro for other reasons. It's much easier to get up and running with legacy macros, so I'll often start there for a minimum viable. At the moment I don't think that the necessary API exists in wasm-bindgen to do what you ask though.
Started work, but have gotten a ton done yet. I'm currently working this as a fork from both mimallocator (the wrapper that someone in this thread wrote) and mimalloc, [https://github.com/rusch95/mimallocator](https://github.com/rusch95/mimallocator). Didn't get much work done this weekend, but by tomorrow, I should have a solid foundation setup for function by function porting. The rust community discord server could be a good spot for coordination.
[xacrimon](https://www.reddit.com/user/xacrimon/) looks like a new candidate for the \`Grand Concurrent Map Competition\` ?
Which won't help much when you feel confident to try to tackle UI or graphics programming in Rust afterwards. As an example, doing something in Swift vs Rust.
You have to enable the `rand` feature for `num`, it is not on by default. You can read more about which features `num` supports at https://github.com/rust-num/num#features (or read its Cargo.toml file). [dependencies.num] version = "0.2" features = ["rand"]
Hey yeah, I'll add this when I come home from south Korea.
I haven't dug into macro yet, I'll take a look at them, thanks!
That's not entirely true. I ran into this jitter when experimenting with OpenGL and winit. The triangle is _not_ stretched. It moves horizontally because its position is relative to the _center_ of the window. The reason is that OpenGL is optimized for 3D applications. If you draw a white rectangle as background matching the window size, this suffers from the same problem. While resizing, an ugly, black border might appear at some of the edges.
What a huge update. Great work!
I believe it's not stretched, but _moved_, since the triangle's position is relative to the center of the window.
The closure syntax is weird and painful to type, I agree with that. I would have really preferred the JS arrow (`=&gt;̀) which would also have been more consistent with the function definition syntax.
I finally decided to bite the bullet and try out nom as of \`5.0.0-alpha2\`, and it could not have been a more enjoyable experience. Thank you for all of your hard work!
&gt; In both cases, the Rust version processed the set of files \~30% (warm cache) and \~70% (cold cache) faster. And: &gt; Note that these benchmarks used no concurrency at all. The real value of the Rust version is that it can easily be used multithreaded, while the ROOT-implementation cannot. Wow!
Yea, I thought it was a good solution, since it was the only solution. The solution presented by Rooswe seems better! But you should definitely look at macros, they are nice when doing repetitive stuff.
I think that it is because it would have been a context-dependant syntax (not sure about the name) while when the parser encounters a vertical bar, it knows for sure that there will be a closure. That's also why there is the turbofish syntax BTW.
In a professional setting, you would often be required to either know the language before you are hired, or allowed a period of transition time to learn the skills required. It's how things have always worked, even professions outside of programming.
It's because `RandBigInt` comes with the `rand` feature. #[cfg(feature = "rand")] pub use bigrand::{RandBigInt, RandomBits, UniformBigUint, UniformBigInt}; You have to specifically ask for it in your dependencies.
Good idea. Thanks!
I recently released v0.2.0 of Emu, a framework I am developing to make it easier to program GPUs from Rust than in other languages. This new update takes advantage of the Rust ecosystem (rustc, cargo, crates.io, docs.rs) in ways that similar frameworks embedded in other languages can't. I have also introduced the concept of holes after I realized that symbolic programs can implicitly define parallelism. Here's what this looks like now. emu! { // Applies sigmoid to any element in given data function sig(data [f32]) { let elem: f32 = data[..]; let res: f32 = 1 / (1 + pow(E, -elem)); data[..] = res; } /// Applies sigmoid to each element in given data pub fn sig(data: &amp;mut Vec&lt;f32&gt;); } fn main() { let mut my_data = vec![0.0, 3.9, -2.3, 4.9, 9.9]; sig(&amp;mut my_data); } [Repository on GitHub](https://github.com/calebwin/emu) [Book on GitHub](https://github.com/calebwin/emu/tree/master/book#table-of-contents) [Crate on Crates.io](https://crates.io/crates/em)
Yep definitely! I've only started learning rust for about two weeks so there's still a ton of ground to cover
Thanks!
Not sure if I understood that properly, tell me if I got it right. * I would have a struct `Node` like you described * I then would have as many structs as needed to implement new types of nodes. those would implement some trait to make sure they all have the same interface. The instances of these nodes would be stored in `Node.data` * Then when I want to evaluate the node, I could simply do something like that: (somewhat dummy code, I'm not too familiar with generics yet so syntax might be off) &amp;#8203; let node = Node {is_dirty: true, data: SomeNode::new()}; if node.is_dirty == True { node.data.evaluate(); } That's assuming that the type `SomeNode` implements a trait that has a method `evaluate`
more idiomatically, [https://rust-lang-nursery.github.io/api-guidelines/naming.html#getter-names-follow-rust-convention-c-getter](https://rust-lang-nursery.github.io/api-guidelines/naming.html#getter-names-follow-rust-convention-c-getter) trait T { fn is_dirty(&amp;self) -&gt; bool; fn is_dirty_mut(&amp;mut self) -&gt; &amp;mut bool; } /cc /u/Muream
Months may be so if you are the first person on a team learning a new language without any guidance whatsoever, but companies don't often switch languages without having an experienced team member available as a guide to speed that process up. What would take months for an individual would take a few weeks of training, with training materials, for everyone else. In other professions, adopting an entirely new process in a matter of days is practically unheard of, so I don't think we should expect that, either. A language like Go is able to be learned in days precisely because it has so many shortcomings and downfalls in the long term. Quality is what's at stake here.
But that's exactly what it means. Within the context of the board games industry, "learn" means something like "to understand all of the rules, and be capable of applying them." It does not imply being good at the game. It means you are capable of playing. Learning all of the rules of Go (the game) can easily be done in an afternoon. Learning to apply them effectively is the work of a lifetime.
Thanks! That website looks like its a great reference overall
I presume the mentioned performance improvements are referring to the runtime of code using nom, can we see some benchmarks? Also, has this rewrite had any effect on compile times of projects using nom, either for better or worse?
&gt; Weird capitalization I'm curious what languages you came from. Having used it for a while, capitalization clearly indicating exposure has actually become one of my favorite Go conventions. I even prefer to write that way in other languages now, if it doesn't violate the prevailing conventions of those other languages.
I was just Googling yesterday to figure out how I could use Rust in the ESP8266 I just bought. Thanks for doing the heavy lifting to make this more convenient. On a side note, is there anything stopping us now from creating an official rustup/cargo target for ESP32? A brief search on github issues produces nothing, or perhaps I'm looking in the wrong place.
&gt; [...] Below is an example. That example creates a memory leak due to a reference cycle between the `Rc` that contains a button, and the closure that is stored as signal handler inside the button. Apart from that this works of course, yes. And you can solve this specific problem of the memory leak by using weak references. But that's what I mean with you having to carry around two things. Note that the first argument to the signal handler closure is exactly the button. So both the button and your struct is passed around instead of just having it both be one unit. What you're describing here is not a custom widget but some custom data structure around a widget. From a GTK point of view, your struct does not exist and that's a problem in various cases. I'm aware that the above is possible and that's what I'm doing a lot and would also recommend people to do unless they have more special needs (see further below). It's the correct thing to do in many cases. However it's not defining a custom widget, it's just some data structure around a widget that does not exist from a GTK point of view. &gt; &gt; you'll need a proper subclass to be able to override virtual methods and to have a distinct GType so that it actually looks different at the C level &gt; &gt; I would argue that there is no need. Write a `gio::InputStream`, `gio::ListModel`, `gtk::TreeModel`, etc. like above. Or add custom signals or properties to your thing in a way that is available via the normal `GObject` infrastructure so that other languages or generic `GObject` code can also handle it. That's simply not possible like that and you need subclassing support for that :) Similarly you can't write arbitrary custom widgets like that, for that you would need to be able to subclass `gtk::Widget`. Simple containers/collections of widgets are of course possible like you describe, but that's something else.
In any professional setting, it's normal to require a few weeks of training to learn a new process, so why should it be different here? Teachers, for example, spend weeks to months learning new training materials and re-evaluating current ones every year. As for showcasing, it often is a small project which is used as a showcase to the rest of the team before making the decision to transition the rest of the team to a new language. This is true for programming as much as it is for other professions. A demonstration is always in order, and hiring someone experienced with the new process is often in order.
That's a very good resource, didn't know it existed!
&gt; A language like Go is able to be learned in days precisely because it has so many shortcomings and downfalls in the long term. Oh please. Take your language zealotry elsewhere. It doesn't belong in r/rust.
By the way, here is the announcement of the framework a year ago: [https://www.reddit.com/r/rust/comments/7z9kbk/analyzing\_alices\_cern\_based\_experiment\_public/](https://www.reddit.com/r/rust/comments/7z9kbk/analyzing_alices_cern_based_experiment_public/)
When I started I didn't known the importance of implemented traits. When looking for a method I never checked them. Didn't expect it to be in-between one of those "less important"-looking "things" at the bottom of the page.
Chaining is much more readable compared Python's inner-to-outer, right to left function nesting.
Ok, I thought I understand the borrow checker, I really did! :P &amp;#x200B; Then I suddenly realize I don't. &amp;#x200B; Can somebody explain me under which logic this code is fine: https://play.rust-lang.org/?version=nightly&amp;mode=release&amp;edition=2018&amp;gist=6aeef67b5ceef518b3e4f4ab58111130 and this is not [https://play.rust-lang.org/?version=nightly&amp;mode=release&amp;edition=2018&amp;gist=e6ad1bb2284ebd4faa4557b829889cdc](https://play.rust-lang.org/?version=nightly&amp;mode=release&amp;edition=2018&amp;gist=e6ad1bb2284ebd4faa4557b829889cdc) Ignore the fact these are pointers and not values in the object. In particular what I don't understand is why calling a method that takes "&amp;mut self" and returns a reference (supposedly with the same life-time as "self") seems to keep &amp;mut self borrowed until the last use (and the death?) of that reference. &amp;#x200B; I can't see the logic behind this in any document I remembered reading. How do I come-up with the reason behind it? I want to understand the internals :-/
No hate, just incovinience, that it really isn't a language tailored for doing GUI development.
the [JSON benchmarks](https://github.com/rust-bakery/parser_benchmarks/tree/master/json#results) show some results comparing 4.2.3 and 5.0, but I have not ported all the benchmarks yet. Also there are benefits to this architecture that would help in benchmarks, like the iterator support. I have not felt significant difference in build time (except when activating LTO), but it should be a bit faster since code is not recompiled every time there's a change in the main project
&gt;In particular what I don't understand is why calling a method that takes "&amp;mut self" and returns a reference (supposedly with the same life-time as "self") seems to keep &amp;mut self borrowed until the last use (and the death?) of that reference. Ok, this works: &amp;#x200B; [https://play.rust-lang.org/?version=nightly&amp;mode=release&amp;edition=2018&amp;gist=49bd0740dfbe1b318155946caf62a9ce](https://play.rust-lang.org/?version=nightly&amp;mode=release&amp;edition=2018&amp;gist=49bd0740dfbe1b318155946caf62a9ce) &amp;#x200B; If I disconnect the two life-times of the references it seems to accept it. The fact is that I didn't think the life-time of two references connected them in any way. I mean, the borrow-checker seems to consider that if two references have the same life-time then they are handled like if they are the same object. Is that the logic behind it? Doesn't the same life-time just means they are gonna be valid for the same amount of time? I'm confused :(
&gt; does not work for array sizes greater than 32 There's actually some exciting movement here. Const generics have advanced to the point where libcore is about ready to begin using them to define all the traits that have traditionally been macro-hacked onto arrays up through length 32: https://github.com/rust-lang/rust/pull/60466 . The current decision is to not expand these implementations to *all* arrays just yet, since const generics are still unstable and there's concern about them leaking through the stable interface, but it's a very important milestone.
Pretty new to rust, so sorry if this is a stupid question! I want to have a webserver which sends websocket messages on file events. The problem is, I basically have 3 processes which need to loop to accomplish this: 1. The webserver itself (currently using the 'actix-web' crate) 2. The websocket listener (currently using the 'ws' crate) 3. The filesystem watcher (currently using the 'notify' crate) So what's the best way to have all these processes working at the same time? I guess I have to move them onto separate threads? Really I am just looking for a nudge in the right direction so I can figure out where to start searching. Side question: the `notify` example I'm working from uses `loop` to listen for fs events: is this really the way event driven programming is handled in Rust? If I'm reading the docs correctly this is analogous to `while(true)`, which as a hot loop is usually considered undesirable with respect to CPU utilization.
Yeah, I'm not familiar with wasm-bindgen. If wasm-bindgen doesn't have the necessary functionality, I would try to see if there's an easy way to add it, and send a PR. Or otherwise open an issue for it, assuming it's possible and desirable. &amp;#x200B; But I'm not really familiar with it so I don't know how difficult this would be.
&gt; std::mem::swap is smart enough I'd say that `std::mem::swap` is _dumb_ enough. It always just swaps the two things you point to, without ever trying to be "smart" and follow some pointer-chain.
Damn. People are out here replacing my job (JavaScript intern) with rust macros. GG
Correct — my point is that when there are not many candidates who already know the language, an organization will generally favor tools that make the period of transition time as short as possible in the common case. Engineers will see the long-term value of investing in Rust, but as it stands today that learning curve will continue to be a barrier to adoption in larger firms, because *every* new hire will need that transition time. So if I pick Go, and new hires need 3 days to be productive, I win over Rust where it takes 3 weeks. (Of course, the long-term benefits of Rust may outweigh the ramp-up time, but we don’t have enough long-term projects to know for sure that will be the case).
Yeah I think you're probably right for more complicated expressions. But something like `[x.trim() for x in my_strings]` is just so perfectly clear.
Is there anywhere I can see some examples of what Nom 5's error reporting is like, so I can compare it to the examples on Pest's home page? I'd planned to use Pest for iteratively developing a grammar which parses some homegrown markup with minimal change needed to the corpus, but, if Nom has caught up with my needs for error reporting, I'd prefer not to leave performance on the table.
Hey! New to Rust, but experienced developer. I'm writing an application that has multiple ways to generate input, and then formats it with one of multiple formatters. The architecture I'm trying is that for every input, I write a runner that generates common events, so all input generators end up with the same ~10 events. Then, I'm passing those events to the formatter, which takes care of the output. I define all events like this: pub enum Events { Event1, Event2 } pub struct Event1 {} pub struct Event2 {} The issue I'm having is now in the formatter: pub fn handle_event(&amp;mut self, event: Events) -&gt; String { match event { Event1 =&gt; self.on_event1(event), // &lt;-- fails here Event2 =&gt; self.on_event2(event), // &lt;-- and here _ =&gt; ... } } fn on_event1(&amp;mut self, event: Event1) -&gt; String { ... } fn on_event2(&amp;mut self, event: Event2) -&gt; String { ... } This fails with `note: expected type 'events::Event1', found enum 'events::Events'`. I get why that happens, but I don't know how to get to my goal. If my understand of the problem is correct, turning `Events` from an enum to a trait, and the events to trait objects, will also not help. Th best idea I could come up with is to create a trait for each formatter, and implement that trait on each event. But I don't like that, because I would prefer to group the code by formatter, not by event. I have seen `transmute` and would prefer to stay far way from it. Any ideas? Not necessarily looking for code, suggestions for a different pattern are also good. But this seems like a problem that should be common enough?
"Implicit returns" is pretty common in expression-oriented languages and functional languages. AFAIK, it originated in LISP. ML, Haskell, Scala and their assorted derivatives all have it.
I feel like as a community we should all donate what we can to Mozilla for giving us Rust :)
Does Rust have a pipe/threading operator? That’s the cleanest to me.
That's two orders of magnitude difference in memory usage! We only normally see 10x increase in performance for Rust rewrites from JVM, not 100x. Did you change the algorithm along the way?
This is perfectly timed! Was just about to start a new project with Nom this week. Great release. Look forward to giving it a go.
It's a real mess once it gets more complicated though
One fairly common way is to structure your enum like this: pub enum Event { Event1(Event1), // an enum variant containing another struct Event2(Event2), } Then you will be able to write the `handle_event` function like this: pub fn handle_event(&amp;mut self, event: Event) -&gt; String { match event { Event::Event1(inner) =&gt; self.on_event1(inner), Event::Event2(inner) =&gt; self.on_event2(inner), _ =&gt; ... } }
Looks neat, thanks for sharing. Are reads consistent or eventually consistent with races against inserts from other threads?
Not fully sure what you're trying to do here, but the naming suggests you want `Events::Event1` to hold an instance of `Event1`? Right now, it's a flat enum, no data associated. Use pub enum Events { Event1(Event1), Event2(Event2), } and then in your `match` statement, use `Event1(event) =&gt; ...`. Also, get other names, calling it all `Event1` seems difficult :) Did I catch that right?
Maybe inadvertently, but I don’t remember changing it. I know the jvm version should’ve stayed low on memory because there’s only so much stuff to keep around but I never figured why it took so much and it wasn’t really worth digging through memory profiler. Worst case from the dataset I mentioned, there will be 1437 (the number of log files) log entries in memory which isn’t much. One entry read from the disk, one written to the disk (and dropped from memory by gc) and so it goes.
This was no advertisement. Just a try to be transparent. The hole Projekt is open source.
IMO idiomatic Rust is pretty clean-looking. In my experience, bad-looking things were also things that asked for a rewrite
The example often used is: let mut vec = vec![0, 1, 2]; let first = &amp;mut vec[0]; vec.push(3); // doesn't compile if uncommented // dbg!(first); If the existence of `first` doesn't "lock" the use of `vec` you can have a dangling pointer and safe Rust does not like that.
This example can trigger UB look at this [playground](https://play.rust-lang.org/?version=nightly&amp;mode=release&amp;edition=2018&amp;gist=54e6eb0f3932fc22c2b8baf34e89166e). With your function's declaration you make some promises to the compiler but can't keep them.
&gt; And btw., there is 32s of potential compute-time, making the ratio even worse, but still the best you can do. Ugh! I was wondering how I was getting 2/3, and double-checked the division; never occurred to me the mistake had been made on a trivial multiplication :(
Or to individuals who are maintaining critical libraries/frameworks for rust.
I was trying to make modification to nom-sql, but had a hard time even for some minor change because of macros. I ended up using [sqlparser-rs](https://github.com/andygrove/sqlparser-rs) which the parsers are manually written and it was easier to understand. This new version of nom is great to see and hopefully other rust projects only uses macros when absolutely needed such as avoiding repetitive codes. Using macros to make the syntax looks nicer, only leads to confusing error messages. This is very common in web frameworks. I wrote [sauron](https://github.com/ivanceras/sauron) to have a simplified syntax while still giving developers the maximum flexibility.
After work a bit with rust I see soo much focus in using macros. For example, for templating. I wish instead most cases split in 2: Give me a function layer and on top, build macros. Like vec! is just a layer about Vector... &amp;#x200B; So this is a great move!
Hi thanks! I see the reasoning. By what mechanism though does that happen? I'm trying to see through the veil there ...
I think if we donate to Mozilla it can trickle down. That's what I'm doing. :)
The [JSON example](https://github.com/Geal/nom/blob/master/examples/json.rs) shows how errors work. Parsers can be generic over the error type, so if you choose an error type that accumulates a lot of information, like \`nom::error::VerboseError\`, you can generate a nice stacktrace like this: [https://github.com/Geal/nom/blob/master/examples/json.rs#L266-L282](https://github.com/Geal/nom/blob/master/examples/json.rs#L266-L282)
Thank you, works perfectly.
&gt;The languages are not in competing spaces go has proven itself in the network services space, command line tools and the system services space. I think that these are also spaces where rust would attempt to be competitive. I agree that the go language is much more focused on the spaces it's intended for and can't match rust in (microcontrollers, kernel space, writing libraries for consumption via a C API, etc.), but I don't think it's fair to say that rust doesn't compete with go in other spaces. Two languages are competing if anyone ever has to decide between the languages for a project.
I agree. I think people talk about the steep learning curve - I think the documentation curve is just as great. I'm still learning how to find docs, navigate them and understand how to read them.
Thanks :) The events do have better names, I tried to keep the example generic.
I dislike that there are a lot of (especially type-system related) features that simply don't yet exist or are unfinished. It makes you work around unnecessary limitations and makes the language feel immature.
&gt;Two languages are competing if anyone ever has to decide between the languages for a project. I don't think that's true, because then pretty much every language would be competing with eachother (C++ and Javascript, Rust and Python, etc). In reality there are some obvious nuances around languages that bucket them more clearly. My point is that to me, fairly obviously, Rust and Go are in totally different buckets.
&gt; The most important part was ... ... dividing it up into smaller bits This is usually where I fail. But I'm learning to do better.
There was a post late last week about the "[typestate pattern](http://cliffle.com/blog/rust-typestate/)". It seems close and might be workable for you? Something like, make it so your processing function can only work on your struct if it's in the dirty state and then match on dirty structs that need processing each loop.
I love the attention paid to memory layout; skip-lists are one of those data-structure where C flexible array data-members shine.
Eventually consistent, its an inevitable with them being wait-free. On my to do list is to provide a map with a future-based lookup interface: if there's nothing under the key, it inserts a slot there with a waker in it to be woken up when eventually the value is inserted. So you can just `map[key].await` which will give you the value when its ready (or block your task forever if you never insert it on another task..)
Did you use some kind of asynchronous pattern in Java that loaded lines faster than it was processing and writing them out?
Yes, it was also quite fun! Though there are some interesting annoyances - I tried to use an extern type to create a "real" unsized type with a one-word pointer, but then you cant use `AtomicPtr` because `AtomicPtr` must point to a sized type (because of issues with the metadata on wide pointer unsized types I assume). Rust support for these sort of weird memory optimizations is always a little wonky.
Congratulations! While I can't say much about the paper, I'm intrigued by the 6 TB dataset you mention. &gt; ALICE, as well as some other CERN based experiments have released a small subset of their recorded data into the public domain. The dataset in question for this project is in total approximately 6TB. What's in there? The data from one experiment? A hundred? A thousand? &gt; Note also, that the standard ALICE framework, which is build on top of ROOT, was not benchmarked. The ALICE framework always reads in and decompresses all the data of a given file, even though a normal analysis only needs to access less then 10% of it. Does that mean the analysis software ends up downloading some 6 TB of data? How long does it take, and what part of that time is retrieving the input data? Is there any mechanism (or plans to have one) for streaming subsets to the consumers? And finally, how open are your co-workers to using Rust? Even if it's not going to be mainstream, did you find other people interested in learning it?
&gt;why should it be different here? 1) there's already significant ramp up time simply in learning a company's internal tooling, getting familiar with their software/design/architecture etc. You and I may not necessarily agree with it, but it is not unreasonable a company wants to limit the scope of orientation by not adding on potentially weeks to learn a new language. 2) it's not about _should_ it be different, but it _can_ be different. If it was a necessary evil, as it is in other industries, then people would deal with it. If the tradeoff exists for shorter ramp up of learning new process, then some people are going to take it.
how do you call closure inplace? for example, in cpp - int x = []{ return 12; }();
To be clear, while I always advocate donating to Mozilla, a donation to Mozilla does not go to Rust development at all, nor is there a way to make it do so.
You're very welcome!
You can make it work if you change things a bit: #[repr(u8)] pub enum Enable { DISABLED = 0, AUTOMATIC = 1, MANUAL = 2, } impl From&lt;u8&gt; for Enable { fn from(item: u8) -&gt; Self { match item { x if Enable::AUTOMATIC as u8 == x =&gt; Enable::AUTOMATIC, x if Enable::MANUAL as u8 == x =&gt; Enable::MANUAL, _ =&gt; Enable::DISABLED, } } }
I am going to test it at work tomorrow morning for sure ! It looks really promissing :)
Fantastic news. I have parsers to port! What is the best approach to get maximum speed with good parse errors? First parse with a minimal error type and, on error, reparse with a detailed error type? It would double the code size, but using the detailed error code would be an exception.
I'm speaking from the point of view as someone who was using Go for two years before my transition to Rust four years ago. Your dismissive reply citing zealotry is no more than my experience after gaining practical experience and witnessing firsthand the mistake of choosing a language specifically because it was easy to learn. In hindsight, that decision was a grave mistake which costed me dearly in the long term as I started to develop some serious applications with it. Therefore, I consider your dismissal of my experience to be rather insulting.
Ye, I'm definitely not against donating to Mozilla. All money helps in the grand scheme, and Mozilla does a lot more cool stuff than just Rust. Personally though it feels like my donations have had a greater impact when they went directly to the individuals that build the tools I use on top of the language.
I'll start from the very beginning but it'll get more interesting. I swapped the indexing with `get_mut`, the signature is simpler. We'll have an `Option` but the borrowing rules don't change. let mut vec = vec![0, 1, 2]; let first = vec.get_mut(0); vec.push(3); // dbg!(first); `get_mut` is defined as `fn get_mut(&amp;mut self, index: usize) -&gt; Option&lt;&amp;mut T&gt;`, I just removed the generic. But there isn't any lifetime here, the compiler will add them for us: fn get_mut&lt;'a&gt;(&amp;'a mut self, index: usize) -&gt; Option&lt;&amp;'a mut T&gt; Which means that as long as the `Option` exists, `vec` is borrowed mutably and we can't access it, mutably or not. But then at the next line we borrow it mutably, the very thing we can't do. And with the 2015's edition it won't compile, giving the error: let first = vec.get_mut(0); --- first mutable borrow occurs here vec.push(3); ^^^ second mutable borrow occurs here Here comes NLL (Non Lexical Lifetime) I think this is what you're looking for. With NLL the compiler can understand lifetimes that are not scope based. In practice it means the `Vec`'s mutable borrow won't end when the `first` variable is dropped but instead when it's used the last time. This is why this code compiles with the 2018's edition. We never use `first` so `vec` becomes accessible again right away. Of course if we uncomment the last line, first **is** used and `vec`'s mutable borrow has to be longer. This time we call `push` while it's still borrowed and the compiler won't let us. If you want to learn more about NLL [this is the RFC](https://github.com/rust-lang/rfcs/blob/master/text/2094-nll.md).
&gt; PyO3 are alternate Rust bindings to Python from rust-cpython, which is what pyembed currently uses. &gt; &gt; The PyO3 bindings seem to be ergonomically better than rust-cpython. PyOxidizer may switch to PyO3 someday. A hard blocker is that as of at least June 2019, PyO3 requires Nightly Rust. We do not wish to make Nightly Rust a requirement to run PyOxidizer. Given the parallels between the names "PyO3" and "PyOxidizer", I was actually reading through the docs to make sure it *doesn't* depend on PyO3, not just for the "Nightly Rust" reason, but because it [violates Rust's aliasing rules](https://www.reddit.com/r/rust/comments/azit15/i_made_a_super_simple_example_guide_of_how_to/ei89s8e/?context=3) and I don't want to rely on code that's known to invoke undefined behaviour.
By the way, I am not sure that the `Relaxed` memory order is sufficient in `get`. Specifically, I am afraid that accessing `node.inner.elem` at https://github.com/withoutboats/kudzu/blob/master/src/skiplist/get.rs#L26 without a previous `Acquire` (or more precisely `Consume` given the data-dependency) leaves the opportunity for reading garbage, as the read of `node.inner.elem` could be reordered before the `Relaxed` load of the pointer. And if this is effectively a problem in `get`, then it is likely also a problem in `insert`. *Note: I don't think a compiler could reorder them, and I have some doubts that a CPU could successfully predict the target of a never seen before pointer...*
If it's been evaluated that the new technology and processes provides a significant improvement to quality and value, then there's no reason not to commit to a few weeks of training. My point stands that choosing a technology based on the ability to learn it in a few days is, more often than not, going to heavily backfire later down the road. It happens all the time. Therefore, it shouldn't be the criteria.
hey, cool. TIL. Thanks!
It doesn't work because match patterns don't allow arbitrary expressions. The version by /u/FenrirW0lf works because a match pattern of `x if ...` is a match pattern that matches anything, followed by an `if` guard. (My term. I'm not sure what the official jargon for that `if` part is.) (The downside to using an `if` guard is that it can't be optimized as effectively by the compiler.)
gonna ping you just in case replies don't notify you: u/cyansmoker 1. Consider using `structopt` instead of `clap` 2. [here](https://github.com/Fusion/glauth-qr-code-rust/blob/master/src/main.rs#L321) and in other places you use unwrap instead of proper error handling. 3. [here](https://github.com/Fusion/glauth-qr-code-rust/blob/master/src/main.rs#L266) you define a `_log` variable - a variable starting with an underscore usually means it's unused, yet you use it later in the code 4. Some of the methods taking `Config` as the first parameter may be better suited as methods in an `impl` block and used object-style, but that's up to personal preference. 5. [here](https://github.com/Fusion/glauth-qr-code-rust/blob/master/src/main.rs#L159) you use `lazy_static!` in a function which I don't think is the intended use, at least I've never seen it used that way. 6. [this](https://github.com/Fusion/glauth-qr-code-rust/blob/master/src/main.rs#L220) code is better written as an `if let ... else ...` on `info.get(0)` in my opinion 7. [here](https://github.com/Fusion/glauth-qr-code-rust/blob/master/src/main.rs#L234) I might be missing something but it looks like you're just overthinking, you can use a `Vec` directly instead of a `String`, and regardless you should give it a better name than `s`. That's all I found looking through it quickly but hope it's helpful :) If you disagree or have any questions feel free to reply, I'm happy to talk.
Given that the bug report now lists an exploitable use-after-free, I don't think any further explanation is required.
I wouldn't claim to be an expert on atomics but this doesn't make sense to me. I don't see how a Relaxed ordering of the atomic load impacts the ordering of operations *on this thread.* There's a clear dependence of the offset operation on the address that is loaded, how could it possibly reorder the offset to before the load?
Thanks. :)
I'm really glad `nom` has moved to using functions instead of macros, that's a great improvement. Still, I wish it would adopt a `Parser` trait like the `combine` crate – all those `IResult`s everywhere make the code pretty unreadable to me, not to mention having to pass around `&amp;'a str` everywhere.
Excellent work my man. You just saved my bacon.
&gt; My point stands that choosing a technology based on the ability to learn it in a few days is, more often than not, going to heavily backfire later down the road. It happens all the time. It happens all the time that people have to deal with tech debt. Yes. I agree with that. It's not at all clear to me that people always made the wrong decision. Meeting product deadlines, being first to market, being able to code up a feature NOW to sell to your customers to get yourself afloat, are all extremely important. Maybe the industry is skewed too heavily in favor of that but if you are trying to argue it is always correct to onboard with the steeper language learning curve, that's a very strong claim to make.
&gt; It's strange considering in most languages you either put it on every line, or don't put it at all That's because you don't intuitively read what it means in those languages; sure, usually it's an either/or, but it's designed to communicate that you're done with your statement. People's insistence on using whitespace *on top of it* is what makes it seem so redundant. Scripting languages like bash or JavaScript occasionally make this very obvious, where occasionally you *need* to place a semicolon for the interpreter to parse your code correctly, even if for the rest of a file you didn't. Rust simply associated a subtly different meaning to it than most languages, so it feels odd.
I actually think it's as simple as your first statement - all languages compete. In particular, some languages compete a lot more; it's not super often that someone says "C++ or Javascript", and has a hard time deciding. It's way more common that someone goes "Rust or Go", I'd say, having seen it about 1000x on this subreddit. The reason is pretty simple - they solve similar problems. Rust and Go are both good candidates for web services where you want at least adequate performance, and you want strong concurrency primitives. They also both produce static packages, so they're good for CLI tools. If you talk about Javascript vs C++ it's way easier to hit a "oh, duh, I'll choose that one" point. Do you need a static package? Do you care about a big runtime? Are there any performance requirements - js is in a way different class of perf from C++. Is dynamic vs static an issue?
Putting this in my "hilarious things that Go does while still claiming to be simple" bag, thanks.
If a thread inserts a `String` into the collection and another thread loads that String in a relaxed fashion, it's not guaranteed to see the heap memory pointed to by that string.
\&gt; What's in there. the data from one experiment? A hundred? A thousand? How long does the data collection for a typical experiment last? During data-taking you have potentially several hundred million collisions per second. That collision rate would be way to fast record all the collisions. The experiments therefore have a pretty complicated trigger system which uses makes extremely fast decisions on what events might be interesting. Bottom line is that ALICE produces up to 4GB/s, but that might be further compressed. The data at hand is rather raw and usually an analysis would be run on something a little more distilled - I don't understand why they chose to release the data only in this uber-clunky format... So to answer your questions: The 6TB of data contain the data of approximately 3.5 million independent particle collisions (IIRC) recorded by the ALICE detector. I think this particular data set was collected over the course of a few days in December of 2010. ALICE is one of the 4 main experiments at the LHC (the accelerator). Data taking season is usually from March until December with short planned and unplanned interruptions. After 2-3 years there is usually a long shutdown of 2 years for major upgrades. As a matter of fact the LHC is in such a shut down right now! That means that if you find yourself in Geneva within the next couple of years it might be easier than usual to get a free guided tour to all the detectors and the accelerator! \&gt; Does that mean the analysis software ends up downloading some 6 TB of data? You don't have to! You can chose how many GB you want to download. I did download all of it onto a large external HDD via a beefy university line. Took a few days still :D. If you are a member of the ALICE collaboration you have access to a world wide computing and storage cluster. So you would run your analysis there and never download such an absurd amount of data to your laptop. That being said, for the original ROOT framework there is a tool to actually only stream the bytes you want (XRootD). But then there is no catching and you'd have to re-download the data when you update your analysis. \&gt; And finally, how open are your co-workers to using Rust? Even if it's not going to be mainstream, did you find other people interested in learning it? &amp;#x200B; Full disclosure: I actually finished my PhD and am currently not affiliated with CERN anymore. The people running the computing and storage facilities really know there stuff. I could imagine that there are some (hobbyist) Rustaceans among them. The reality unfortunately is that most physicists - how to put this... are very, very, very, "conservative" about programming languages and are not at all inclined to learn a new programming language. That being said: I do know that there are a few people who are very passionate about programming and also interested in Rust :D If there were funding I would love to tease out the max of a Rust analysis framework. I think Rust would be such an excellent fit: A large collaborative project where nobody knows the entire codebase except your well-meaning compiler! \&gt; This ended up as a large amount of questions on an off-topic subject. Sorry for that. No worries, I love talking about this stuff!
I *dislike* compilation times
All completely single threaded and blocking. The only buffering my code is doing is while it's building a log entry line by line from each log file. 99% of the time a log entry consists of just 1 line. Maybe Kotlin's `java.io.Reader.useLines()`, which is similar to Rust's `BufRead.lines()`, uses some crazy default buffer for the line-reading Iterator, but I checked and default buffer size is 8kb.
Can nom now do ranged based multi step negative lookahead? \`many\_till\_m\_n(2, 6, tag("00:"), not(not(tag("00::")))\`
&gt; I don't think that's true, because then pretty much every language would be competing with each other (C++ and Javascript, Rust and Python, etc). In reality there are some obvious nuances around languages that bucket them more clearly. I agree with this. I would say that some buckets are fairly empty - real time for example, whereas some buckets have a load of reasonable options like network backend services. The network backend bucket would include Python, NodeJS, Java, go to just name a few. I would argue that these technologies are competing with each other. &gt; My point is that to me, fairly obviously, Rust and Go are in totally different buckets. This is where I disagree. I there are many applications where people are currently choosing go where rust would be a reasonable alternative - and thus they are in competition. Furthermore there may be some improvements we could make to rust (the language, the ecosystem, the documentation, the community) such that rust might be a better fit for this hypothetical person's needs. In this way the competition between rust and go serves as motivation or inspiration for future improvements.
Because its fucking cool?
The formal look at the guarantees provided by various concurrent structures is quite enlightening. Thanks for writing this!
Curlybraces and semicolons.
Agreed. I think comprehension syntax is nicer for short expressions than Rust's chaining, which is nicer than python's map/filter etc. For longer ones, I prefer chaining.
&gt; and I don't want to rely on code that's known to invoke undefined behaviour. [Better stay away from Rust then](https://github.com/rust-lang/rust/labels/I-unsound%20%F0%9F%92%A5) [These](https://github.com/rust-lang/rust/issues/35836) [issues](https://github.com/rust-lang/rust/issues/28728) [in](https://github.com/rust-lang/rust/issues/28179) [particular](https://github.com/rust-lang/rust/issues/27060) [are](https://github.com/rust-lang/rust/issues/59220) ["fun"](https://github.com/rust-lang/rust/issues/57893)
Working on a register allocator for my programming language \[tox\]([https://www.github.com/lapz/tox](https://www.github.com/lapz/tox)). &amp;#x200B; So far I've implemented the algorthim described by Appel but I'm pretty sure its wrong. I do produce nice interference graphs though but due a bug they get very very big. &amp;#x200B; * [https://imgur.com/a/JfhLiu0](https://imgur.com/a/JfhLiu0) the initial graph * [https://imgur.com/a/PRQ21Ul](https://imgur.com/a/PRQ21Ul) after initial spilling * **WARNING 28mb large.** [**https://www.dropbox.com/s/u6a367twrd0lao8/main\_reg\_3.png?dl=0**](https://www.dropbox.com/s/u6a367twrd0lao8/main_reg_3.png?dl=0) after the second round of spilling
you could do that, but you could also define an error type with exactly the information you want. I had an idea of an error type that would only contain a list of \`(offset in input, error code)\`, using a variant of the [context combinator](https://docs.rs/nom/5.0.0-beta3/nom/error/fn.context.html) to construct it. You don't need to allocate anything, the error list could be an array (putting a bound to an error trace's size). Then transforming it to a human readable error trace
Really awesome, this is very exciting!
Oh, that sounds interesting, I'll take a look. Thank you!
I have tried a similar approach, but found it very limiting. The current design is very simple, and the \`IResult\` type is common enough for nom users. But if you don't want to write your function definitions manually, you could always do something like this: \`let array\_parser = delimited(char('\['), separated\_list(char(','), json\_value), char('\]'));\` In this case \`array\_parser\` is a closure that was defined on the spot and can be used right away.
 fn foo&lt;'a, 'b: 'a&gt;() { let test = String::from("test"); let iw = |x: &amp;'b String| { x.chars() }; for x in iw(&amp;test) { println!("{}", x); } } fn main() { foo() }
not yet, but it should be easy enough to make your own combinator. Although for ranged patterns, it could be a good idea to test the new \`iterator\` combinator: [https://github.com/Geal/nom/blob/master/examples/iterator.rs#L63-L71](https://github.com/Geal/nom/blob/master/examples/iterator.rs#L63-L71)
Does anyone know why Rust (or the 2018 edition specifically, with the path/use changes) still requires explicit `mod foo` statements? If Rust instead allowed me to refer to `crate::foo::...` without declaring the module, and then implicitly went and found my foo.rs file at that point, what would be the downsides? Is this just an explicitness vs implicitness matter of taste, or are there implementation issues with the implicit version?
You're looking for /r/playrust.
Where should I throw my money?
It is currently impossible to directly donate to Rust development generally.
I want to doubly give a shout out to /u/japaric et al for doing all the embedded stuff. Every line they write brings me that much closer to convincing my boss to let me start using it for work projects. Oh, and /u/steveklabnik1 because he took the time to answer a largely irrelevant question of mine a while back.
And i will try later to make it into an game engine ;)
I for sure thought I was going to make a "This isn't the subreddit for the Rust game" comment, but colour me surprised :)
No problem. I'm happy you got something out of it!
I have written today a wrapper around the [mimalloc](https://github.com/microsoft/mimalloc) allocator which has the following properties (taken from the github repository of mimalloc): * **small and consistent**: the library is less than 3500 LOC using simple and consistent data structures. This makes it very suitable to integrate and adapt in other projects. For runtime systems it provides hooks for a monotonic *heartbeat* and deferred freeing (for bounded worst-case times with reference counting). * **free list sharding**: the big idea: instead of one big free list (per size class) we have many smaller lists per memory "page" which both reduces fragmentation and increases locality -- things that are allocated close in time get allocated close in memory. (A memory "page" in *mimalloc* contains blocks of one size class and is usually 64KiB on a 64-bit system). * **eager page reset**: when a "page" becomes empty (with increased chance due to free list sharding) the memory is marked to the OS as unused ("reset" or "purged") reducing (real) memory pressure and fragmentation, especially in long running programs. * **secure**: *mimalloc* can be built in secure mode, adding guard pages, randomized allocation, encrypted free lists, etc. to protect against various heap vulnerabilities. The performance penalty is only around 3% on average over our benchmarks. * **first-class heaps**: efficiently create and use multiple heaps to allocate across different regions. A heap can be destroyed at once instead of deallocating each object separately. * **bounded**: it does not suffer from *blowup* \[1\], has bounded worst-case allocation times (*wcat*), bounded space overhead (\~0.2% meta-data, with at most 16.7% waste in allocation sizes), and has no internal points of contention using only atomic operations. * **fast**: In our benchmarks (see [below](https://github.com/microsoft/mimalloc#performance)), *mimalloc* always outperforms all other leading allocators (*jemalloc*, *tcmalloc*, *Hoard*, etc), and usually uses less memory (up to 25% more in the worst case). A nice property is that it does consistently well over a wide range of benchmarks. &amp;#x200B; Written to be used in [Purple](https://github.com/purpleprotocol/purple), this crate provides a drop in replacement for mimalloc in rust for everyone to use! Enjoy!
I am not an expert either ;) Maybe /u/ralfj could assist us! I'll also note that in practice I think it will be fine, it's mostly a theoretical concern that I have. I do think, though, that the `Relaxed` loads should really be `Consume` to be fully correct. Unfortunately, this may lead to a slight performance degradation. --- I'll illustrate with an example where `T = String`. The `insert` thread will: 1. Create a new `String` value. 2. Insert said `String` into a Node. 3. "Publish" the write with a `Release` store (actually, an `AcqRel` compare and swap). There are 3 memory locations involved here, written in the specific order: - The memory of the content of the `String`, `C`. - The memory of the new node, which contains a `String`, `B`. - The pointer to the new node, `A`. And, the `get` thread will: 1. Load the pointer to the node, it loads `A` in `Relaxed` mode. 2. Return a pointer to the element. 3. Later, read the element, `B`. 3. Later, read the content of the element, `C`. The problem, I think, is accessing `B` and `C` without further synchronization. --- Backing up a little, I think that the `store` sequence is alright: - First `C` and `B` are written to, in either order. - Finally, `A` is written to in `Release` mode, preventing a reordering of either `B` or `C` after `A` *for any thread using an `Acquire` (or `Consume`) load of `A` prior to reading `B` and `C`*. It's therefore on the `load` sequence that I think there is an issue. The `Relaxed` ordering only guarantees ordering on the atomic being loaded (or stored) and does not guarantee any ordering with regard to the neighbor memory operations. As I interpret it, this means that the load of `A` is going to be okay, however there is no guarantee that the memory read at `B` or `C` are necessarily up-to-date. That is, at the CPU level, the `get` core could read cached lines for `B` and `C` that are obsolete compared to the store sequence because nobody ever told that core that the content of `B` and `C` were obsolete. Or it could get notified that `A` changed prior to receiving the notifications that `B` and `C` changed. Or whatever. Now, it's notable that there is a carries-dependency relationship here, `A-&gt;B-&gt;C`. According to [Preshing](https://preshing.com/20120930/weak-vs-strong-memory-models/#weak-with-data-dependency-ordering): &gt; Weak With Data Dependency Ordering &gt; &gt; Though the Alpha has become less relevant with time, we still have several modern CPU families which carry on in the same tradition of weak hardware ordering: &gt; - ARM [...] &gt; - PowerPC [...] &gt; - Itanium [...] &gt; &gt; These families have memory models which are, in various ways, almost as weak as the Alpha’s, except for one common detail of particular interest to programmers: **they maintain data dependency ordering**. What does that mean? It means that if you write A-&gt;B in C/C++, you are always guaranteed to load a value of B which is at least as new as the value of A. **The Alpha doesn’t guarantee that.** I won’t dwell on data dependency ordering too much here, except to mention that the Linux RCU mechanism relies on it heavily. On a DEC Alpha (whatever that is), I would expect the current code to be faulty. I don't expect any weird optimization from LLVM, though, once again because of the carries-dependency relationship, and therefore I think that on any common hardware (x86, ARM, PowerPC, Itanium, ...) everything will go smoothly. --- *Note: [cppreference on memory_order](https://en.cppreference.com/w/cpp/atomic/memory_order), which I find more detailed than [LLVM on Atomics](https://llvm.org/docs/Atomics.html).*
Removing \`mod foo\` was too controversial. People apparently sometimes "comment out" modules by removing \`mod\` files, or store stuff that they don't want in the code in the source tree.
Great work, it looks promising! Do you have any current plans to enable turning a streaming parser into a \`Future\` that's usable with async/await? It seems like an ideal use case. Or is this already possible, and I'm just not figuring out how?
This is bindings to the C implementation of mimalloc, right? Not a reimplementation in Rust that exposes the Rust-native allocator API?
Yes: [https://github.com/purpleprotocol/mimalloc\_rust/tree/master/libmimalloc-sys](https://github.com/purpleprotocol/mimalloc_rust/tree/master/libmimalloc-sys)
Ah
I think it's possible that if `T = i8` LLVM could do some weirdness (theoretically). Imagine the getter thread uses the value read to index into some static global array. Maybe the value at index 0 of that array is already in a register. LLVM is allowed to generate a branch based on whether the value is 0, and just use the value existing in the register. It's possible the value in the register is out of date - dunno how to accomplish that without `unsafe`.
Agreed! I'm gonna shout out a few people who have done a lot for Rust or the Rust ecosystem in the areas that I myself have valued :D /u/kvarkus who has been a massive and constant driving force behind the gfx and Rust graphics ecosystem in general since it was just beginning. He seems intimidating at first (;D) but has been incredibly generous and welcoming for newcomers in sharing his knowledge and helping others with any issues they may have. /u/omni-viral, /u/msiglreith, /u/joshgroves who have also been driving forces behind Rust graphics and have been welcoming and helpful to me as well. /u/icefoxen who was one of the first maintainers of a popular Rust project that I interacted with and has consistently been welcoming and helpful as they maintain ggez, one of the most popular Rust game dev libraries /u/quietmisdreavus who has contributed greatly to Rustdoc/docs.rs which is perhaps my single favorite thing about Rust... thank you! /u/manishearth who has been both an amazing code contributor and ambassador for the language for a long time, certainly since I joined the community a few years ago. .... so many others that I'm not remembering or haven't personally interacted with. Thanks everyone!!
I have not tested yet with async/await, but it was already easy with futures 0.1 and tokio-codec. I'm guessing it's possible :)
Awesome - thanks for writing this!
This morning: I was trying to figure out when it was going to be released Tonight: AWWWW YEAHHH
Thanks! I’ll read this RFC! But so that means that the lifetime definition in the method is not just a statement the validity of the referenced object (that the returned reference has the same valid lifetime as the input reference) , but also says that the input reference will live as much as the output reference?
Going through this course [Practical Networked Applications in Rust](https://github.com/pingcap/talent-plan/tree/master/rust).
This is a lot of detail but the takeaway to me is that on any platform with support in Rust for these atomics, the current code is not faulty, because it behaves how I expect it to behave.
Small note: you shouldn't call this a port. A port, to me, implies that you've rewritten the mimalloc code in Rust. But these are bindings to the C implementation of mimalloc.
&gt; People apparently sometimes "comment out" modules by removing `mod` files, It's very useful when performing large sweeping refactors in an incremental fashion if you're running `cargo check` between changes so you don't have to sift through a pile of compiler errors that you're not ready to address yet. I didn't see an alternative solution proposed for this. I guess a `#[cfg]` with an unused feature flag would work but that's significantly noisier and obscures the intent (is that module not supposed to be compiled or is that a feature that was removed accidentally or maybe was renamed?).
Right. I realize that now.
Good question. I definitely will. This may be in a little while though as I need to find a project I have use for :)
Make `Node` a struct, and put a `Box&lt;dyn TraitWithEvaluate&gt;` inside it. Then you can add fields to the `Node` struct.
Thanks a lot! I believe simply unpacking\* your comments will require some time and should take me down some interesting paths. &amp;#x200B; \* see what I did there?
Was there a sticker for c++? I don't think so... CHECKMATE!!
By that standard, I shouldn't use computers at all because all major OSes are written in C or C++, as are all runtimes for viable alternative languages. The difference here is that rust-cpython also exists and, while it *may* have the same problem (PyO3 did begin as a fork of it), It hadn't been confirmed last I checked. (And I have plans, once I finish prototyping, to investigate RPC-based options for combining Rust and Python code to remove rust-cpython as a source of risk.) As for `I-unsound`, a large slice of those don't apply to the kinds of programs I write and, as for the rest, I feel there are fewer layers between me and those, so it's less worrisome. It's all about minimizing risk. (You're talking to someone who uses rust-cpython to write PyQt GUIs for Rust backends because it's harder to screw up than using C++ to combine Qt and Rust.)
You can be sure your shit is safe
&gt; By that standard, I shouldn't use computers at all because all major OSes are written in C or C++, as are all runtimes for viable alternative languages. It's literally your standard kiddo. I literally quoted you. It's not my fault your standard is unrealistic nonsense. &gt; The difference here is that rust-cpython also exists and, while it may have the same problem (PyO3 did begin as a fork of it), It hadn't been confirmed last I checked. So you relying on code that may invoke undefined behaviour but hasnt been tested enough to know either way, so it could do anything, rather than code with known actively being worked on? 🤔🤔🤔🤔 &gt; As for I-unsound, a large slice of those don't apply to the kinds of programs I write and, as for the rest, So it's (relatively) ok to have soundness bugs as long as you know what and where it is and whether it applies to you? Of course, to do that you have to actually know(ie, confirm) what and where the problems are.. ----- Your own argument rules out rust-cpython kid. Are you, like, all there? mentally speaking? You seem to have serious problems with basic reasoning.
Hi there! First of all thanks for all this work. It is such a shame that you are not at CERN anymore, I would have loved a coffee in R1! Whoever reading that is considering to introduce rust at CERN feel free to reach out! Can you talk a little bit more of the advantages of RUST over C++? We very briefly discuss the possibility to run an analysis using rust with the ROOT team but the lack of time and man power made it impossible. Also, I haven't look into the codebase yet, but would it be possible to extract the ROOT reading part? Cheers
So the $100 question then is now that you have both of these, which will you be using and maintaining going forward?
*rustaurant
When you say statically linked it does mean that there is no dylib linking at run time? And so the binary can be moved around and ran safely? Because I hit a dylib dynamic linking problem on my mac, my program worked with mimalloc only when ran with cargo run and not by executing it directly, I didn’t dive much but it is probably related to some env variables set up in the build.rs of the lib.
To clarify: the library is statically linked to the crate. **However**, mimalloc itself has some dependencies which are dynamically linked, such as pthread on unix.
I was talking about the mimalloc-sys.dylib itself. But anyway I was talking about the other wrapper, not yours, sorry.
The only important implication of this metric that I can think of, is the choice of language for education, since you have only a limited amount of time in that context. Other than that... I recently spoke to a guy from a startup that is currently in the process of switching from Haskell to Rust, in part because it took too long for the new hires to get up to speed in Haskell. But this is kind of an extreme case.
I'm curious: I thought nom generated an automata to parse input at compile-time. I'm a little confused to see that it now uses functions and is faster. Is there no compile-time preprocessing of the grammar at all? Can this approach give performance similar to code generation?
Thank you! 💛
You can sponsor various individuals at http://aturon.github.io/sponsor/
That IDE support is pretty bad compared to JVM languages (to be fair this is true for all non-JVM languages). Features are slow to stabilize that make you use nightly instead. You can't follow nightly train because RLS, rustfmt and clippy are often fail to build on nightly. rustfmt features are even slower to stabilize. Coming from gradle and maven — cargo doesn't how that many features. Setting up java project with code coverage, static analysis and checkstyle is so much less hassle than rust.
If they active Sponsor in their Github repository, you can donate directly to them.