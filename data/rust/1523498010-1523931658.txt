It could definitely be polished up a bit more I agree but I wanted to get something out rather than make it perfect. The overarching goal here was not to learn a particular skill but rather to "learn how to learn". Hopefully that intention came thru.
Game developers would prefer to avoid spending time/money on things. So if you could achieve very expensive C++ code quality at cheaper prices that would be very interesting. 
Or they... want their students to... *ah*, learn about how you *should* access shared globals, and how Java is bad and horrible because it doesn't enforce any kind of memory safety on them? I mean, there's always a reason. Always a reason. \**starts rocking back and forth*\* ^^always&amp;nbsp;a&amp;nbsp;reason... ^^^^always&amp;nbsp;a&amp;nbsp;reason...
It's not abuse, but I just wanted to pile on and point out that `rust-cpython` also catches panics to transform them into Python exceptions.
Sometimes the reason is that they are stupid...
That's pretty reasonable for a full blown executable.
No. How would that work? You'd end up with multiple `Vec`s all thinking they own the same piece of memory. As soon as one is dropped, it'd destroy the data all the other `Vec`s were using. You'd have to be able to split allocations at specific byte locations, but I've *never* seen an allocator that does that, even ignoring that allocators typically align allocations at higher multiples anyway, which probably precludes this sort of splitting. Can you use `Vec&lt;&amp;[u8]&gt;`? Can you change `populate_with_lots_of_u8s` to write directly into a `Vec&lt;Vec&lt;u8&gt;&gt;`?
You don’t really need to know anything about actors to use actix-web. Just check user guide
You might be interested in the possibility of creating lots of u8 slices though. That can definitely be done in stable Rust.
I'm reasonably certain that to use something in web assembly, you need to compile it from scratch *for* web assembly. Like how you can't use a 64-bit Windows DLL as a dependency for a 32-bit Linux application on ARM.
I don't think this example works as is.
ya, that was my guess as well. I'm making some progress with `viz.js` -- I think that should give me a `wasm` file -- from there I just need to include it somehow...
`File::open` only opens in read-only mode, regardless of file permissions. `File::create` will create a new file or truncate existing files. You can use [`OpenOptions`](https://doc.rust-lang.org/stable/std/fs/struct.OpenOptions.html#method.open) for even more flexibility.
Not directed at you, unless you know. What was the reasoning for OpenOptions? It feels like opening a file is being forced into the builder pattern and its a little counter intuitive when file::open exists? This doesn't seem to support flags either? 
Consistent ordering between any two actors is all the OP is asking for, I think... which is actually even weaker than full causality (what you're asking for) which enforces message order among arbitrary chains of actors instead of just pairs. It may seem like a trivial distinction, but it's pretty important because full causality tracking tends to be much, much more expensive (though some very recent work with causal timestamp compression is promising).
Odd. Thanks for clearing that up!
I'm know you're aware of this, but for others looking at this code: implementing unsafe indexing using the `Index` traits is not correct, regardless of how they are being used, since operators don't have an `unsafe` signature.
Well, I'll be!
u/fafhrd91, do you have a write up about the origin of actix and actix\-web anywhere?
Thank you! Will add this qualification.
Which of the vectors would be responsible do deallocating the memory, if it did work as you wanted? The main question is, why not keep the original Vec and keep the chunks tracked as slices?
If you really want to know, check some of Joe Armstrong’s talks on Erlang - probably the most famous implementations of an actor system. Obviously Rust is not Erlang, but there’s a lot of good general info to be had in addition to a lot of stuff that’s specific to Erlang.
Cool. Any particular video that I should start from?
many comments emphasize on memory ownership (and rightfully so), but also consider that a Vec can grow and shrink. if you slice up a 150-u8 vector into three chunks of 50, each of them has the ability to grow, and where should they grow to? (be aware that Vec and slices are always contiguous in Rust).
This is awesome. Love the call graphs with dot.
I might just take the plunge since most things in my application aren't tied to rocket (really, I just have a beginner API using Diesel). What you say about issues and PRs is one of the biggest selling points for me, I have the same concerns about how stagnate rocket can feel.
Yup, sadly you can't (as far as I am aware) implement a trait unsafely, so that it can only be used in an `unsafe` block. I'm not saying that should be a thing because it'd make generics really confusing, but this once it would have been nice ;-) I wonder if one could create a more intelligent iterator here... That yields 5-tuples to take care of that nasty `i+1` and `j-1` business.
Yeah, I was very hesitant to introduce global state. (I wrote that article.) But eventually I realized, like... the alternatives people argue for, cancellation tokens or "context objects", are things that you *must* pass into *every* function, or else it's a bug. That's just forcing all your users to reimplement global state by hand, instead of letting the computer do it. &gt; But imagine if you're building a webserver which handles multiple concurrent connections while fetching resources from AWS or from the web, while also talking to a database, a backup system, etc etc, all with different cancel conditions. I'm having trouble visualizing what you want here... remember that cancel scopes are, well, scoped – they're wrapped around a specific piece of code, and can be nested. If you're building that web server by writing an explicit state machine that's jumping between all those different tasks, then indeed cancel scopes aren't a great fit. The assumption is that you have some concurrency tool like threads or async/await to isolate unrelated work into their own call stacks, and then there's no problem wrapping different cancel scopes around each of them.
Tell your professor (after finals have been graded) that if they're going to assign writing the same program in multiple languages, that the students (who want to) will get more out of it if they're encouraged to write in idiomatic style for the language rather than forcing the exact same data flow. I hope nobody gets as unlucky as to have to try to force this translitterated C onto Haskell. I don't think that'd even work. The lazy ones will do a direct port anyway, rather than think about how a solution could be rephrased. Being multilingual is a useful skill in that you decouple logic from syntax. But you also have to realize what tool is good at what job. ~~Or just RIIR~~
&gt; If everyone adopted Bitcoin it would consume more power than the world produces. That doesn't make sense from economical perspective: the price of Bitcoin would increase profitability, which would increase difficulty, which would increase demand for electricity, which would *increase the price of electricity* and would stabilise at some point. It definitely won't consume more power than worlds production. &gt; And Bitcoin is not gonna stop war. Govts would just end up using too. A very important source of funding war is inflation. History proves this. If govts can't use inflation, they'll have to raise taxes (which will be more difficult to collect) in order to pay for the war. It's much harder to convince people that they need to pay more, so the govt can attack other countries than silently siphoning the money away.
I'd say that _courage_ is a better word (courage is only fearless in its degenerate form). A German user of Rust pointed out that 'mut' means 'courage', which feels about right :)
Your Rust version is allocating a Vec&lt;Vec&lt;_&gt;&gt; on every iteration, why are you doing that?
this print on github - [white](https://github.com/jstpcs/lnxpcs/blob/92f7cd8ece8177e6e457180a2c380d186843f219/cards/classic/rust-card.png) card and [dark](https://github.com/jstpcs/lnxpcs/blob/92f7cd8ece8177e6e457180a2c380d186843f219/cards/black/rust-card-black.png) card. all the other prints in [playing cards style](https://github.com/jstpcs/lnxpcs/tree/master/cards), and [repo](https://github.com/jstpcs/lnxpcs) with all my pics previews of all pictures with links to github are on [linux.pictures](linux.pictures). info about [license](https://linux.pictures/about).
There is an alternative that doesn't use globals: [Thread-local storage](https://doc.rust-lang.org/std/thread/#thread-local-storage) There's an example written [here.](https://doc.rust-lang.org/std/thread/struct.LocalKey.html#examples) I don't know how familiar you are with closures and the borrow checker, because you're gonna need them both. However, if you're writing Rust you're going to need them eventually anyway. 
You are running into this: https://stackoverflow.com/questions/47529643/how-to-return-a-string-or-similar-from-rust-in-webassembly
A quiet socket should return `io::ErrorKind::WouldBlock`; this is in the documentation for [`.set_nonblocking()`](https://doc.rust-lang.org/nightly/std/net/struct.TcpStream.html#method.set_nonblocking).
For the love of all that is good never do this in production.... [here be dragons](https://play.rust-lang.org/?gist=dac8d1d8b955c7989ed8aa92781e0754&amp;version=stable) 
&gt; it's more that a mutable reference implicitly creates a pin because the object must outlive a mutable reference. No, I don't think that's right. Mutable references allow moving! You can use `mem::swap` to swap out the contents behind one mutable reference with another, effectively moving the object out of the mutable reference and moving another one in. Pinned references do not allow that.
Not for now I think. I got started on a little project again. Hopefully I can finally get a hang of those lifetimes. Besides that, I think I'll wait for one of those moments where projects offer mentors for new people. But thanks anyway!
You can wrap gour global in `Mutex` and declare them to be `static`, not `static mut`. This way it'll be safe.
I'm pretty new to Rust and I'm a little confused with the unsafe code. If there's unsafe code within the function, doesn't the function have to marked with an `unsafe` identifier?
iirc , when a vectors grow it usually allocate 2x its current size somewhere else and copy its current memory there. This is the reason that memory inside a Vec isn't Pin.
[removed]
Thanks for this Framework! I really like how easy it is to communicate with other, synchronous actors. The only thing which bothers me is the amount of threads that actix-web creates. With a small web server and two synchronous actors I would expect it to need about 20 threads, 12 for the webserver, and 4 for each of the other actors. Unfortunately it somehow creates 515 threads, which is quite a lot. Most of them are called `arbiter:"some hash or so"`. Is there a way to reduce the number of threads?
&gt; Go doesn't have constructors. Because of that, it insists on the fact that the "zero value" should be readily usable. I think these are different concerns. Go doesn't have constructors *but* doesn't require fully initialising structs *and* as a result insists on zero values being meaningful. Rust doesn't have constructors either, however it *does* require fully initialising structs, and doesn't have a concept of zero values (only defaults, which are opt-in and overridable).
I think he/she meant growing without exceeding capacity.
If you don't use slices because you need memory to be deallocated automatically when you drop last slice then you need some kind of special smart pointer. Otherwise just use slice and `split_at`.
It's private at the moment. But I get your idea.
If you don't need to publish it to crates.io, just turn the project into a workspace with many smaller crates.
Some thoughts: 1. Using basic Rust project, I've clicked run and nothing happened. It should be smart enough to trigger build at least for the first time. 2. Why Rust is compiled with gulp instead of Cargo?
Unsafe function means it is unsafe to use (even if there is no unsafe code inside). Unsafe block means the compiler will release some of its guarantee inside, and you are responsible for its correctness. If you are sure it is safe to call it, you can have a safe function with unsafe block inside. (One example will be Vec in std library) 
Rls does not work in workspaces though
Looks nice. But why the skull though?
What about /u/cjs_2's implementation? https://www.reddit.com/r/rust/comments/8bn1p3/is_it_possible_to_obtain_multiple_vecu8_chunks/dx8ennw/
It's so you can put it on a gaming PC, makes it go faster
it's the reference to "memento mori". i like this symbol as it shows that life s too short to use some shit things and you should choose only those that you really like.
&gt; This handling obviously doesn’t apply to method calls, so there we have the difference between 1u32 &lt;&lt; 2 and 1u32.shl(2). I feel like this difference should be mentioned in the Rust docs.
It does not just compile, it runs on the playground. I was wondering whether the code has undefined behavior.
It's giving ownership of a single allocation to multiple owning values. It hard crashes on Windows. Not "the code panics", but "the OS steps in and says 'this software has failed, do you want to report it to Microsoft?'". It has the exact kind of undefined behaviour I described.
Indeed this is definitely UB (undefined behavior). Read the "Safety" section in the [`Vec::from_raw_parts` docs](https://doc.rust-lang.org/beta/std/vec/struct.Vec.html#method.from_raw_parts). This code violates three of the four items in the list.
But it's not red and black!
Since when?
This is awesome!
&gt; some file systems (Network-File-System) don't even pretend to support them so YMMV. Depends on the version as well. For example, NFSv3 on Linux &gt;= 2.6 supports file locks (see the [Linux man page on `open` with `O_EXCL`]).
Depending on the environment you're teaching in, you might find that *most* students aren't terribly ambitious, and that teachers have (quite reasonably IMO) come up with blanket advice meant to target very common problems they've observed in the past. In almost every case, when I've pushed back against this because I get myself into situations that conflict with the generic advice (like the advice we're seeing from the OP's teacher), teachers have been more than amenable to let me go my own way. Obviously, some don't, and yeah, they suck.
An important consideration is that Rocket has 84 open issues but it also has 341 closed issues (and 9 open PR's and 169 closed), while Actix has 6 open issues and 31 closed issues (1 open PR and 29 closed). (Also, Rocket has 60,000 downloads on crates.io, while Actix has 20,000) I can confirm that Sergio doesn't necessarily respond to PR's immediately (I think I waited like a week and nothing happened), but as soon as I messaged him about it in the riot.im chat he responded very quickly, gave me great feedback, and got it merged.
Yea well I never said it was *correct* :)
Just to expand on this, I was pretty supprised it didn't hard crash in the playground. Messing with the memory's allocator's pointer tables by freeing an offset of allocated memory is pretty much the definition of UB. 
The original vec and the chunks will be inside a struct. And I'm finding it hard to satisfy the checker. And the general consensus seems to be that having fields that are references to other fields within the same struct is not good. So I decided that, if I can find a way to consume the original vec and create multiple chunks in it's place, I can put them into the struct. 
&gt; cancellation tokens or "context objects", are things that you must pass into every function, or else it's a bug. That's true, but at the same time, if you're using cancel scopes, you must ensure that every thread you spawn happens through the nursery, or else you get the very same bug. But when you forget to pass a cancellation token down to a lower-level API call, you can see at the point of the call that you're not passing the cancellation token. To inspect your code to see if you forgot cancellation tokens somewhere, you only have to check all lower-level API calls. If you forget to spawn a thread through the nursery, or worse, a library spawns threads without you knowing it, then you can't see that at the point of the call site. And that's for threads, I'm not sure what the situation is with generators, callbacks, futures, etc. &gt; If you're building that web server by writing an explicit state machine that's jumping between all those different tasks, then indeed cancel scopes aren't a great fit. That's kind of what happens in Rust if you're using futures heavily. I'm not sure how familiar you are with Rust, but unlike almost all other languages where futures are based on callbacks, in Rust futures are essentially compiled down to state machines.
Creative Commons 0 is probably a more appropriate license, IDGAF is more associated with code whereas Creative Commons and modifiers like 0 are the defacto standard for visual/aural assets.
The original `Vec&lt;u8&gt;` is basically a stream of bytes read from a file using the `File::read` method. I want to split the original stream into smaller streams of `Vec&lt;u8&gt;`s. I would like to avoid a re-allocation if possible because when I do a `chunk.to_vec()`, it clones/re-allocates every `u8` in the chunk into a new smaller `Vec&lt;u8&gt;`. But the thing is, the newly cloned/re-allocated chunk is already present in the original. And I do not intend to use the original in the future and am willing to consume it in the process of making the smaller vecs. So my reasoning was, if I do not need the original vec and am willing to consume the original vec to create multiple smaller non-overlapping vec's, I can avoid unnecessary re-allocations which I incur when I do `chunk.to_vec()`.
It needs slots for RGB lighting. Oh, and racing stripes, not to mention speed holes.
Nice! I'd love to see tarot card versions of these. :3
I think you'll want to look into the turn function for tokio's core. Whereas run runs all futures on the core to completion, [turn](https://tokio-rs.github.io/tokio-core/tokio_core/reactor/struct.Core.html#method.turn) does one iteration of the event loop waiting a specified time for blocking io.
Why not use ranges and indices to the vector within your struct?
Say I had a vec like so, (it's just pseudo code. Not actual rust code) daddy_vec = Vec { data: [1,2,3,4,5,6,7,8,9,10], // assuming each u8 occupies 8 bytes start_ptr: 1000, len: 10 } Now if I were to do `a.chunks(5)`, I would get, kid1_vec = Vec { data: [1,2,3,4,5], start_ptr: 2000, len: 5 } kid2_vec = Vec { data: [6,7,8,9,10], start_ptr: 3000, len: 5, } but here in this case, `daddy_vec`, `kid1_vec` and `kid2_vec` are all in memory. The data in `kid1_vec` and `kid2_vec` is already there in `daddy_vec`. And some extra time was spent cloning/re-allocating every `u8` in `daddy_vec` into `kid1_vec` and `kid2_vec`. But the thing is, once the smaller `kid_vec's` are created, I do not need the `daddy_vec` anymore. So I was thinking that, If I have no use for the `daddy_vec` once the `kid_vec's` are created, then is it possible to create the `kid_vec's` in place without allocating more memory or spending more time creating the `kid_vecs`. 
Your best bet is to make a second struct that has a Vec of &amp;[u8] borrowed from the original, and use this structure for all your chunked work. It will drop before the file contents do, so you should be fine with lifetimes.
Ooh. thanks for pointing me to that library. That could work.
I get *why* you want this, but that's irrelevant. As I already explained, nothing about how `Vec` or how memory management in Rust works in general would permit this. Ownership is a fundamental concept in Rust; it can't just be ignored or wished away. Reality is cruel like that. That said, what you've described as coming out of `a.chunks(5)` is, strictly speaking, wrong. You *do not* get a sequence of `Vec`s out of it, you get a sequence of borrowed slices that just point into the existing storage of `daddy_vec`. This is why I asked whether you could simply use `Vec&lt;&amp;[u8]&gt;` instead, which would get you what you seemed to want ("chunking without copying").
I intend to put the smaller chunks inside a struct. So I knew I would have to put the original larger vec into the struct as well, If I want to put the smaller chunks into the struct. But it seems like I have to fight the checker a lot to get that working due to fields within a struct having references to another field within the same struct. And that is why I decided to put only the chunks in the struct. But I can put the chunks inside the struct only if the chunks are owners and not mere references to another larger vec
Go also doesn't have Option/Result types, I assume mainly because those would require generics. That means that in situations were you would see an option in Rust, like getting a value from a map, you often see uninitialized zero values in Go.
&gt; But I can put the chunks inside the struct only if the chunks are owners and not mere references to another larger vec In that case, as I asked before: can you modify the fill function to directly fill the `Vec&lt;Vec&lt;u8&gt;&gt;` instead? If the answer to that is *also* "no", then you're pretty much out of luck.
I feel like this'd be an apt cover for the Rustonomicon
&gt; if you're using cancel scopes, you must ensure that every thread you spawn happens through the nursery, Yes, this is the most important decision in trio's design: *it does not provide any mechanism for spawning a thread without going through a nursery.* One consequence is that a library call cannot spawn a background thread that outlives the call itself unless you explicitly pass in a nursery for it to use, which means that this *is* visible at the call site. [Note: Technically in trio's case we call them "tasks" instead of "threads" because they're not OS threads and if we call them threads then people get confused, but it really doesn't matter for this discussion, all the same ideas apply to "real" thread apis. We just use green threads because in Python we have a GIL and no borrow checker, so cooperative concurrency makes more sense than preemptive concurrency.] I'm working on another blog post now about this aspect of nurseries specifically. (I've kind of been circling around this post all year trying to find my way in – all the other posts I've written recently are built on top of it. I think I've figured it out now though.) The argument is that providing a bare `spawn_thread` function is, in a fairly precise way, isomorphic to providing `go to` for sequential control flow, and just like for `go to`, it has to be removed entirely to make global reasoning feasible.
You actually want to use deadlines in both cases, because sometimes the "lower level" operation is itself still a composite operation. (Like, maybe you want the policy "this high level operation should take no more than 1 minute, and each HTTP request it makes should take no more than 10 seconds". That implies some kind of timeout on each individual network operation, but not a trivial one.) What you're pointing to, I think, is the need to be able to "nest" or "stack" multiple deadlines at different scopes. Taken further, you also need to keep track of *which* deadline expired ("if the whole operation takes more than 1 minute I want to fail; if any individual HTTP request takes more than 10 seconds I want to cancel it and retry with a different server"). This is the kind of problem where cancellation tokens or cancellation scopes can really outshine raw deadlines, or cancellation at the granularity of a single thread/future. (Though I'm not sure how well popular cancellation token implementations handle this in practice.)
The fill function is basically reading a File with `File::read` which populates a mutable `Vec&lt;u8&gt;`. So whatever I do, I will have do with a `Vec&lt;u8&gt;` as a starting point. On another note, I was going through the method list for Vec and there was a method called `drain`. it essentially removes a `u8` from the parent vec via an iterator. That might work in my case. Just not sure if the `drain` method does any cloning/re-allocations. But it doesn't look like it does.
Ok, your library doesn't provide any mechanism for spawning a thread/task, but that doesn't prevent anyone from using Python's built-in mechanisms to spawn tasks, right? Or do you actually somehow block those built-in mechanisms?
The grown-up thing to do with people complaining about getting feelings hurt by words not even meant to offend them would be to tell them to grow up, man. I'm sorry; I just absolutely feel no sympathy for you on the naughty words thing. You can stop putting "toxic" in quotes, too. That's just disrespectful.
I'm using [tokio-tungstenite](https://crates.io/crates/tokio-tungstenite) and I'm happy :) ...but I don't use hyper.
If it's UNIX and the file is opened in append mode, then individual write() calls from different threads or processes will be atomic.
Tk-http https://github.com/swindon-rs/tk-http is definitely worth a look.
...and let's be honest, you *do* come off as extremely authoritarian. Everything you do is canted in that direction. If you don't like that, stop acting like an authoritarian asshole. You can't just declare that people should interpret your actions in the way you like best.
The cron one is great.
Why do you keep repeating that argument if you're not even around anymore in the Rust server to see if/how I've changed? 
It's not me. It's the CoC. It's not my opinion on the topic, it's the opinion of the group of people that created the CoC. And I'm forced to enforce it, whether I like it or not and whether I agree with it or not. 
It's been like a day, dude. Maybe I'll come back. Maybe I won't. But whether or not I come back has no bearing on the reality that it's only been a day and there's no way that anyone can implement meaningful change in a single day.
Well, that's just sad. I lament that we have such a terrible, authoritarian document in place, and I believe it's unfortunate that anyone feels forced to enforce bad rules. I have fought against the Code of Conduct as implemented since before we implemented it, but I think it's pretty obvious that my viewpoint is not taken seriously. :p I can only hope that you find a way to make enforcement less painful for everyone. I believe in you. :)
&gt; So I decided that, if I can find a way to consume the original vec and create multiple chunks in it's place, I can put them into the struct. You can just write a function that returns a slice...
No, I did - it just misses the point completely. Changing the PoW function kicks out _this set_ of miners, yes, at the cost of basically being a new altcoin. This loses the vast majority of the defenses bitcoin has, and doesn't address the root cause. 1. The new-PoW network will start with low membership and hashrate, making it vulnerable to Sibyl attacks 2. The loss of _all ASICs_ will drastically affect power efficiency 3. [Network-based attacks](https://btc-hijack.ethz.ch/) can drastically reduce the costs of both the takeover and post-PoW-change Sibyl attacks 4. The PoW change does not alter the dynamics that caused the network to become centralized in an emergent manner previously, and thus it is exceedingly likely that it will happen again. The fundamental problem is that any economy subject to economies of scale approaches a natural monopoly as investment increases. As a result, if spending more money is more efficient than spending less money, the network will become centralized again. The PoW-change dance then repeats, solving nothing - especially if the new set of miners, having learned from the investment risk that a PoW-change represents, decide to go with reconfigurable Hardware the next time, and so _adapt to the PoW change_ - rendering it moot.
When the only tool you have used is a hammer, every problem looks like a nail. What you are asking here is "how can I turn driving a car into a nail, so that in can drive from place A to place B by hitting it with a hammer". 
&gt; Where should I look? Or should I write a wrapper in C that I can then call from Rust? There's probably documentation out there for exposing fortran to C. That's what you want. C is used for its ABI (the low-level calling conventions aka where to put the arguments, where to get the parameters, etc...), so the communication doesn't need any C, it just need for both ends to communicate *as with* C: for the Fortran code to expose something which can be called from C, and for Rust to call it as if it were C.
Totally allowed in ralloc: https://github.com/redox-os/ralloc Jemalloc and most system allocators will get rather upset. 
My question was more, what do you want to do *afterwards* that requires the `Vec&lt;Vec&lt;u8&gt;&gt;` - maybe there is a better way of doing that. One way to get something similar is to have `Vec&lt;Cow&lt;[u8]&gt;&gt;`. This will have slice references into the original `Vec&lt;u8&gt;`, until you call `to_mut()` on a cow, at which point it'll clone into a new vector. Of course that doesn't have any advantage *if* you need to modify every one of those Vecs.
Memento mori if your reference goes out of scope your borrow is done? 
lifetimes
Honestly that’s kind of how it went minus the brainfuck....I’m just waiting for the Ook assignment D:
This is extremely helpful, thank you very much, I felt like i was wasting my time just vomiting out any garbage that would Compile just to get this over with (I had 55 errors at first and was just smacking together code that removed the errors one by one), but now I’ve actually learned some real Rust, thanks! 
The [rustimization](https://github.com/noshu/rustimization) project is a good example on how to call Fortran code from Rust. Mostly the linking part [lbfgsb-sys](https://github.com/noshu/lbfgsb-sys). I can't really help you with it because it's not my project but it doesn't look too hard.
Sure, this one does a good job of describing the peoblemspace that actors inhabits: https://m.youtube.com/watch?v=lKXe3HUG2l4
CC-BY-NC in this case because of the creator's desire for the work to be free for "personal use"
HTTP and websockets in the same create? I'm sold, seeing how it means the websockets part won't break on every breaking change to the http part...
this is not normal. * you can control number of http worker threads by `HttpServer::thread()` * makes sure you start sync workers (for diesel) outside of application configuration closure, i.e. outside of `HttpServer::new()` method, because it get run for each worker * `StaticFiles` uses futures_cpupool with 40 threads by default, you can override it with smaller defaults 
and the repo from that talk: https://github.com/mre/idiomatic-rust What a goldmine of info, I mean just look at this [code review of ripgrep!](http://blog.mbrt.it/2016-12-01-ripgrep-code-review/)
Gold star.
i was working on process management utility and just wanted to try async rust. naturally, process management utility has long running entities, like communication channel to forked processes, signals handlers, cmd interfaces, and all actors can send messages to each other. so generalization process lead me to actix. you can still can see original utility in first commit in actix repo
It sounds like what you need is for the "daddy" vec to be an `Arc&lt;Vec&lt;u8&gt;&gt;`, and for the "kid" vecs to hold clones of that Arc along with start and end indices. Would that solve your problem?
Cool, it really seems like Actix is crushing it these days! Just to clarify, I have a small question on your phrasing here: &gt; 0.5 is *more* performant than 0.4.0 and *definitely more* performant than 0.3 (it was used in TechEmpower benchmarks) The "more" vs. "definitely more" contrast confuses me. Do you mean that 0.5 might not be faster? Is it more like the performance increase from 0.3 -&gt; 0.4 was huge while 0.4 -&gt; 0.5 was small, or that 0.3 -&gt; 0.4 changed something that applies to *all* use cases, while 0.4 -&gt; 0.5 might only see increases in a particular use case? If the latter, what are the kinds of things that will see the benefit?
Good idea. Will set up a PR soon..
I think this is exactly the example from https://doc.rust-lang.org/book/second-edition/ch19-01-unsafe-rust.html. They say you can't do it normally but you can use unsafe.
I'll try it too - the documentation looks much better than `tk-http`...
there was 10 releases between 0.4.0 and 0.5.0, most of them has performance related improvements. if you compare 0.4.10 with 0.5.0 improvement is small. i doubt you can measure improvement if any business logic is involved. but 0.5 fixed http/1 rfc compliance issue without loosing performance.
forgot to mention, i have some performance related ideas for 0.6, improvement should be more noticeable.
I'm hitting this issue in a project I'm working on as well and was considering dropping to raw parameterized SQL. Can you give some examples of what you mean by lite ORM (any language is fine)?
It's really nice. It's really pretty. One small criticism is I find it odd that some of the content has sharp black edges, but the skull/crab does not. Did you draw the crab or is it taken from somewhere?
This is true, unless you are dot net :). They actually do just load normal dot net assemblies (dll files).
Oh shit you are right. I should say unsafe action is not required for unsafe function.
Hi everyone! This new tutorial goes a bit more in depth than the extant tutorials we've had for Rust and WebAssembly. The goal is to introduce all the topics you need when doing real Rust+Wasm development, all with a single running example to give continuity. Topics covered include: * Setting up the toolchain and making a hello world * Designing Rust programs for WebAssembly * Debugging * Time profiling * Code size profiling Every chapter also has exercises, if you want to challenge yourself. **If you follow along with the tutorial, please take notes along the way!!** Your feedback would be incredibly valuable! Thanks, Nick
Unfortunately I do not know enough about Windows to say. The OP didn't specify the platform, but if they only want append and they're on UNIX, then there is an easy solution.
Oof, IDGAF is definitely the wrong license then.
Seriously. It's giving me a serious Warhammer 40k vibe that's perfect for the Rustonomicon
Moving the contents of a reference is perfectly allowable IFF Unpin: https://play.rust-lang.org/?gist=9e4f73f716585850929d631815d0e3c1&amp;version=nightly
I wanted to like it just for the name, but I find myself having a visceral opposition to the actual idea. The idea that a macro would impact stuff outside of the macro seems really counter-intuitive and like the source of really subtle bugs. More practically, couldn't a macro handle the "else" use case just by moving the else inside the macro? So you could make dbConnect!(connectionInfo, optionalElseBlock) or funkyWhile!(condition, whileBlock, optionalElseBlock)?
Hm. Nice idea. That would basically be "structopt with serde derived".
oh! this is bug, i'll release update with fix later today. all filesystem operations are io bound and blocking, http handlers on other hand async, so you can have much more concurrent requests that needs to serve static files
Is there any advantage of using C for writing programs when you don't want your code to crash?
Nice burn bro. Unfortunately there *is* a reason: compiler availability. Notably highly portable (and full of bugs ;) ) big projects like retroarch mandate C89 in order to port to things like the xbox and wii.
Nice little test demonstrating how some static analysis tools are insufficient. It looks like you only tested FLOSS tools; did you consider trying commercial tools like Coverity to compare how they did?
&gt; trying to keep compatibility with ancient hardware at the cost of compatibility Is this a typo? I can't figure out what you mean here, care to explain?
This looks like an awesome crate! A confession, though... You had me as soon as you included Portuguese in the `HashTrieMap` example in the README. I love Brazil, so I knew I was starring this repo as soon as I saw it. :)
The sdks for closed hardware mandate fixed versions of C in order to compile and run on that hardware. Or at least I think that's the justification. Basically someone long ago did the compiler backend work for C89 and no-one else bothers with that for new languages.
It was a typo, edited now.
Pretty much, yes.
s/the definition of/the quintessential example of/
Wait, what? * ptr needs to have been previously allocated via String/Vec&lt;T&gt; (at least, it's highly likely to be incorrect if it wasn't). Maybe I'm misunderstanding this, but aren't the pointers all pointers into data allocated be Vec&lt;t&gt; * ptr's T needs to have the same size and alignment as it was allocated with. T is identical, so the size and alignment should be identical, no? * length needs to be less than or equal to capacity. If we get the chunk's size using len, and then use that as the length and capacity arguments, then this should hold. * capacity needs to be the capacity that the pointer was allocated with. Ditto, I think. But I don't know internals, so maybe I'm misinterpreting the words here.
It's a builder pattern because that works fairly well in Rust, and opening things has a bunch of different boolean flags that can all have different values and all have sensible defaults. I think in general builder pattern are the most "rust-y" type-safe equivalent to functions taking bitflag parameters in C? As for flags: I think a lot of those should be available as methods, but as the other comment mentioned, there are also OS-specific extensions which can give more control for different OS-specific functionality.
Yes, we used a proprietary compliance tool in the same class as Coverity. In the _Results of static analysis_ tables it's referred to as "MISRA-C Checker"
The joy factor involved in send_email_harder and send_email_with_a_vengeance is sufficient, in my opinion, to justify the use of the pattern. In general, actually, I do something very similar on a regular basis. A recent project declares `push_path` and `push_stream` methods for one object; push_path calls push_stream after actually getting a stream based on the path, but you can just call the stream version directly.
Neat ! I'm going to take a deeper look at it tomorrow.
[removed]
English is as bad as C sometimes. 
Regarding the last point on the list, capacity is not the capacity the pointer was allocated with. The pointer in the large vec is allocated with some capacity &gt;150, but each of the vecs now think they have a pointer with a capacity of 50. If you mutate the vectors content it will, as you state, create a new allocation. However, it will also attempt to free the old memory (otherwise you would have a memory leak). All the smaller vecs (except for the first) have pointers which do not correspond to any allocation by the memory allocator: They point *into* an allocation rather than to the *start* of an allocation. By freeing them the vector will try to free a pointer which the memory allocator does not now about, which is bad news.
Thanks, that looks really useful! I'll definitely give it a try
What's the equivalent way of writing for(i = 0; i &lt; n; i++) fprintf(f, "%.4f\t", v[i]); fprintf(f, "\n"); I feel like every time I learn a new languages basic IO, it's somehow more crippled or complicated than C's.
&gt; A confession, though... You had me as soon as you included Portuguese in the HashTrieMap example in the README. I love Brazil, so I knew I was starring this repo as soon as I saw it. :) :) I'm Portuguese, but yes, Brazil is pretty awesome. 
 for i in 0..n { let _ = write!(f, "{:5.4}\t", v[i]); } let _ = write!(f, "\n"); The `let _ = ` is there because `write!` can return an error. It shouldn't be ignored, but here it is ignored like the C code sample.
Well, that's a confusing mixture of styles. Not saying that it is strictly bad, but just kinda odd to me with all the different styles jumbled in. The "rust" text on the sides look like they have something unfortunate happened to them, like some filter or something which makes the weight be bit off. There also seems to be all sorts of "glitches" that look like some sort rendering artifacts when you look it up close. I'm not sure if those are intentional, but if not then you might have quite a lot of cleanup/tweaking to do still to get nice render from the presumably vector original.
Dapper is a good example, basically it focuses only on mapping the results of hand made queries. https://github.com/StackExchange/Dapper
I'd like to add that the idiomatic code /u/tspiteri suggests only works within a function that returns a `Result&lt;_, E&gt;` where `E` is some `Error` type that implements `From&lt;io::Error&gt;`.
A server for chess endgame tablebases. The basic building blocks are finished: [shakmaty-syzygy](https://crates.io/crates/shakmaty-syzygy) is a library to probe Syzygy tables. It uses double checked locking internally, which I factored out into another crate [double-checked-cell](https://crates.io/crates/double-checked-cell). Next step is to write the server, probably using actix-web.
`websocket` development is prone to hiatuses. I hope it will revive after a while. [There are forks](https://github.com/cyderize/rust-websocket/network) that implement some thing, including dependencies upgrades.
Maybe repository `rust-websocket` should be turned into an organization?
Why is Vera++ missing in Table 1?
"In languages like C the problem of comprehensive static analysis may be NP complete. [8]" "[8] Landi W.(1992) Undecidability of static analysis." While I'll give you that undecidable problems are NP-hard, they aren't NP-complete (they're not in NP). More importantly, any sufficiently deep static analysis of any Turing-complete program is going to be undecidable, because Halting Problem. Need to say more about why C is "speshul" with regard to static analysis: there's definitely more to be said.
Shouldn't it be labeled `Box&lt;Color&gt;`? Eh? Eh? 
Whenever I see a tutorial implementing Conway's Game of Life, they always implement a naïve version, very slow in comparison with optimized algorithms like Hashlife (https://en.wikipedia.org/wiki/Hashlife)
I didn't understand why the Rust programs that are contrasted to C used `Box&lt;&amp;Example&gt;` instead of `&amp;mut Example`. This is not really comparable to the C programs, which don't allocate on the heap. "The Immutable Promise" in Rust could maybe try to cast away the constness, like the C code does. There are a few typos: &gt; Indended use as an 'action_e' &gt; trying to return an in32_t 
To be fair even the naive version is good enough for basic simulation. Hashlife is completely crazy and magical in a somewhat mind-blowing way, too.
This is a beginner-level explenation of Rust memory management model. I couldn't find an accessible introduction, so I wrote one myself. I hope it will encourage and help people interested in Rust. The article was originally posted on my personal GitHub page: https://codesandwich.github.io/rust_memory_safety_revolution/
Whether this is UB seems to depend on the allocator, as /u/hornetblack noted, ralloc has this as an official example: https://github.com/redox-os/ralloc#partial-deallocation
&gt; I've never seen an allocator that does that It seems like ralloc does: https://github.com/redox-os/ralloc#partial-deallocation
Short version: how can I `include_str!` a file in a dependency? Still-pretty-short-version: I'm writing a library that depends on the [fst crate](https://github.com/BurntSushi/fst/). It includes a wordlist file [data/words-10000](https://github.com/BurntSushi/fst/blob/master/data/words-10000) that it uses in tests, and it loads it using `include_str!`. I'd like to use that same file in my tests as well. Since I'm downloading that file anyway as it's contained within a dependency, is there any way for me to `include_str!` it from my crate rather than having to copy it into my own repo or download it from github on test runs? Seems like I would need the path to the dependency, but I can't figure out how to get it.
Thank you all for the feedback and discussion! We have just compiled a [Roadmap after 0.2](https://github.com/substantic/rain/issues/26) at github - you are welcome to participate in the discussion (or even implementation) there, especially if you have a use case in mind. Thanks again!
Hashlife is a *super* cool algorithm, but it isn't really appropriate for this tutorial, where the focus is teaching Rust+Wasm development.
There are a few homebrew efforts for recent consoles if you feel like checking them out. [Megaton Hammer](https://github.com/MegatonHammer/megaton-hammer) is an attempt at making a toolchain for the Nintendo Switch in pure rust, and [ctru-rs](https://github.com/rust3ds/ctru-rs) is a rust wrapper around an existing C toolchain for the 3DS
According to https://internals.rust-lang.org/t/tracing-in-the-rust-compiler/7288 `-Ztime-passes` has become useless.
Personally, I think the issue is not the behavior of return immediately but with the naming being consistent with that behavior. If the returned trait is called a Future, it defies expectations if it is not in-flight to arrive/complete in a future. It is at most an UnstartedFuture or AsyncTask. I think there are good reasons for the current return immediately model and I think Rust's async/await can be successful as such. But if there was no legacy, and this was all implemented from scratch, would Future really be the term used to describe what is returned from an async function? It seems that in Rust, it is a state machine of actions that can be (but not necessarily will be) done in the future, but in other languages (for example Java) a Future is a result that will be available in the Future. The name implies familiarity, but will be deceiving.
you should normalize this numbers based on project age
Hey I have a question. Is it only possible to run a wasm application by calling it from a javascript bootstrapping script? Or is there a way to just ship a wasm executable directly?
actix-web 0.5.1 is released
/r/playrust
Is there a way to completely disallow heap allocation? Either at compile time or at runtime?
We use this in [proptest](https://github.com/AltSysrq/proptest/blob/master/src/test_runner/mod.rs#L102) for testing and I think quickcheck does too.
I tend to see `Ok` and `Err` as data constructors in the Haskell sense of the word, so I'd argue that Rust does have constructors.
&gt; I feel like every time I learn a new languages basic IO, it's somehow more crippled or complicated than C's. To be fair, you *are* ignoring `fprintf`'s return value in your example. With error checking code this would look a lot more complicated. Additionally, `fprintf`'s format string is checked at *runtime*; if you screw up the format string or the number of arguments and forget to check the return code, you'll just see nothing written out. The compiler/IDE *might* catch it for you, depending on which one you're using, but it's not guaranteed. Rust's `write!()` checks the format string at compile time and emits a hard compiler error if it's invalid or if you forget an argument. The compiler also warns you if you forget to handle the result as other replies have pointed out. You can also screw up `fprintf()` by passing an uninitialized file descriptor and you won't get an error until runtime (or if you're depending on your IDE to point it out to you with a warning). The Rust stdlib doesn't even give you a way to obtain an uninitialized `Write` impl in safe code; the closest equivalent would be trying to write a file opened in read-only mode, which is possible with `fprintf()` anyway. C's `fprintf()` may seem uncomplicated on the surface but using it is like pointing a machine gun at your foot.
I always love to see some good Rust art especially because I'm still looking for a nice shirt to casually advertise the language a bit. However as mentioned by zokier I think that the mixture of styles is not doing it a favor. I love the skull and general theme going on though.
It could, and I believe it still can - but the Rust-WebSocket crate has not been updated in quite a while and is no longer compatible with Hyper - so unless I use an older version of Hyper I can't upgrade connections to a websocket of that crate.
Thanks for pointing that out!
If you're not too attached to the other `std` APIs, you can declare `#![no_std]` in your `main.rs`. That drops `std` from the prelude (taking `alloc` with it), so it's not even *possible* to allocate unless you explicitly include `alloc` again. Of course, all your dependencies have to support this mode as well or they'll fail to compile.
Unless you're feeling frisky enough to use /u/DroidLogician's hack, I would just copy the file. It's only ~100KB, which is probably an order of magnitude smaller than your application's binary.
You can declare `#![no_std]` in your `main.rs`/`lib.rs` which removes `std` and all allocating APIs from the prelude. [`core`](https://doc.rust-lang.org/core/) is placed in the prelude instead. That's all you have to do to make your *library* guaranteed-allocation-free, but if you're building a binary you unfortunately have to redeclare some lang-items (functions required to run a program that `std` usually provides for you): https://doc.rust-lang.org/unstable-book/language-features/lang-items.html#writing-an-executable-without-stdlib (If you set your crate to [panic on abort](https://doc.rust-lang.org/stable/cargo/reference/manifest.html#the-profile-sections) then all unwind/panic related stuff can be `loop {}` stubs.) And of course, you have to ensure all your dependencies can function in `#![no_std]` mode. If they do they'll usually have a `std` default feature that can be disabled, or they may just work with `core` unconditionally.
I'd say it's worth mentioning at the top of the implementation chapter that it's about wasm and not about a fast game of life implementation.
The rust card is definitely the most badass :)
The distributed computing people lifted some words from set theory. When two nodes see each other's messages in a fixed sequence it's called partially ordered communication. Attempting to make everybody see all messages in the same order is totally ordered communication. https://stackoverflow.com/questions/4620779/partial-ordering-of-events-in-a-distributed-system#4620837 
While others correctly responded that the answer is "no", you can get similar functionality by using the [`BytesMut`](https://carllerche.github.io/bytes/bytes/struct.BytesMut.html) type from the [bytes](https://carllerche.github.io/bytes/bytes/index.html) crate. This works by reference-counting the underlying vector so as to avoid the issue with freeing partial allocations.
Okay, cool, thanks! I'll decide how I feel about this level of hackery.
Well, as it happens, trio is an async library, so there is no built-in way to spawn a "trio task"; you have to use its task spawning API so that the trio scheduler and trio event loop know about your task. That said, I mean, it's Python, ultimately the language doesn't actually enforce anything and if you really want to you can reach into trio's guts and change its semantics in arbitrary ways. But if your library hacks around the rules, then that's a serious faux pas and people are not going to use it. It's like saying, what's the point of the borrow checker if any library can just use `unsafe` whenever it wants? What's the point of not having `goto` when any library can drop in some asm that jumps control to arbitrary places? Well, they could, but they don't... (In fact the `unsafe` analogy is pretty good: there *is* a mechanism for breaking the rules called `trio.hazmat.spawn_system_task` that we use in a few exotic cases where it's actually safe but trio's enforcement mechanisms can't tell, and we wrap it in a safe API.) It is more of a challenge when trying to retrofit this onto an existing system, sure. I don't know how possible that would be in the current rust ecosystem, or even if it's a good idea. In principle, nurseries and cancellation systems and async are three orthogonal design axes, but for nurseries or cancellation systems to work you really need global coordination across all the libraries you use. (This is true of cancellation tokens too btw.) So adding them to an async system is a bit easier: async frameworks are already forced to "own" the concurrency and I/O primitives so they can route everything through `epoll` or whatever, and as a side effect that makes it easier to enforce some global consistency in your concurrency and I/O semantics. Also, the way OS APIs work, you generally have to be doing things in an async kinda way if you want cancellation to work (e.g. on Windows, blocking socket operations are not cancellable but IOCP operations are). But these are engineering/coordination issues, not language design issues: in principle the only thing stopping you from making nurseries and cancel scopes work with rust and threads, is convincing the community that this is a good direction to go in :-).
Do you or would you use it in your projects?
great choice, editor looks really nice.
i like it. i approve it.
Hey there! I'm following along and when I get to the part in setup where I'm supposed to run `npm run build-debug` I get a whole bunch of cargo errors, which I've captured [here](https://gist.github.com/joshlemer/a265de5e6a5d4b723795e012dd69ca9a) (along with npm debug logs) 
Whoops! Simply had an out of date rust toolchain, which I fixed with `rustup update`
Although the `ws` crate is not as popular / lacks async, I have found it to be a very reliable / well tested library. Give it a shot in a multitreaded setup if that works for you!
I must say this is pretty awesome! WebAssembly + Rust has been unfolding so quickly (from an outside perspective) it is crazy. One thing I've noticed that is *of course* a huge issue here is the lack of vim keybindings make text editing primitive :( That horrible attempt at humor aside, great job! :D Keep up the excellent work.
So, I tried to do a bit of testing but it is kind of difficult to get consistent data. On my work computer, (Win10x64) when compiled natively I was seeing `rmp_serde` out perform `bincode` in size of serialized data by a factor of 1.5 but `bincode` out perform `rmp_serde` by the same factor in speed. I am having a hard time measuring speed on the `wasm` side of things but the total round trip (both ser/de and through the pipe) for `bincode` is ~2ms while rmp is ~6ms. This is all really un-scientific since I didn't save my native tests where I can get them at home so the two different datasets are pretty much unrelated. Maybe I will try and do some more testing at work tomorrow to try and keep things more consistent.
It is literally part of a book titled "Rust and WebAssembly".
The `for` loop takes the thing being looped over and calls `IntoIterator::into_iter` on it. Implementations of this for collections (like `Vec`) create an appropriate iterator, whilst implementations of this for *iterators* just return the iterator itself. This is why you can use both collections and iterators in a `for` loop directly. However, a type can only have *one* implementation of `IntoIterator`. What's more, `IntoIterator::into_iter` takes the invocant *by value*, meaning it always consumes it. This is why `IntoIterator` for `Vec` consumes the `Vec` and returns an owning iterator: *it can't do anything else*. In order to get an iterator of references that doesn't consume the container, you need to borrow the container. As such, there are also `IntoIterator` implementations for `&amp;Vec` and `&amp;mut Vec`. That's what you get when you borrow the collection. Conventionally, the container also defines `iter` and `iter_mut` methods which are just convenience functions to access the same borrowed iterators.
Thanks for the perspective. There have been some good solution proposed by folks on this thread. * using the bytes crate * using Cow * just plain returning the chunks via a impl function. For now I've chosen to go with the last option. Seems simper and less complicated.
Thanks for clarifying that a given type can only have a single implementation of IntoIterator on it, my original post did not correctly capture this. When I said &gt; So, in the case of &amp;y, x is a &amp;T where T is the Self::Item of y's Iterator implementation It would have been more appropriate to say &gt; of &amp;y's Iterator implementation. Which is different from y's Iterator implementation. Or IntoIterator perhaps is even more correct. So, if I have for mut x in &amp;y { } I can then rebind a new &amp;T to x in the body of the for loop, where T is the Self::Item of &amp;y's Iterator implementation? Whereas without the mut prior to the x, I would not be able to bind a new &amp;T to x? So if I have for &amp;x in y { } Will the type of x then be a &amp;T, where T is the Self::Item of y's Iterator implementation? I am still a little fuzzy on the effect that modifying x with &amp;, &amp;mut, and mut has, and the relationship this has to the Self::Item type of y. 
&gt; &gt; &gt; &gt; &gt; I can then rebind a new &amp;T to x in the body of the for loop, where T is the Self::Item of &amp;y's Iterator implementation? Whereas without the mut prior to the x, I would not be able to bind a new &amp;T to x? Yes. &gt; Will the type of x then be a &amp;T, where T is the Self::Item of y's Iterator implementation? You've got it backwards. Assuming `y` implements `Iterator&lt;Item=&amp;T&gt;`, then `x` will be `T`; the `&amp;x` destructures the reference and binds `x` to the value being referred to. Patterns have the opposite effect of expressions.
&gt; Whether this is UB seems to depend on the allocator If it depends on an implementation detail that's hasn't been defined by the specification then it's UB.
Should also be noted that Hashlife has good performance because it caches a lot of computations, and relies on similar chunks of patterns to re-occur a lot. It therefore has great performance for people fiddling with building complex machines out of "parts" with known, stable behaviors. However, there are modes where Hashlife seriously tanks performance, even compared to naive implementations. I know of one way, where you make sure it's filling tons of caches that it can hardly ever re-use. Push in a lot of random data so you get cache misses mostly. I discovered this when I wanted to "simulate a patter of rain on the canvas": on each frame, generate a random number X between [0,1] for each cell, if X is less than some threshold Y, then set this cell to living. Another use mode which **may** be bad for Hashlife (I'm not sure, probably depends on the user) is if you have it set cells to alive when the user mouses over them.
I sure hope we will at least get a warning for this kind of shadowing. In rustc, not clippy. The current error message is really confusing.
i am not sure, but if i implemented it, i will be happy. because it works for me. i had already done(implementation) some applications for me. so i guess compiler almost same case for me. i don't care what other people think about my case.
You're looking for https://www.reddit.com/r/playrust/
hahaha lol ok srry
Out of context, "rusty rabbit hole" is what I imagine Ferris lives in...
Rust is able to take advantage of the fact that, unlike C, all of its types know their sizes to use jemalloc's "sized deallocation" API. Thus, the allocator avoids redundantly storing and looking up metadata for each allocation, which can be a significant performance win in some applications. This necessarily means that it can't track partial allocations.
`as_slice` in the signature is polymorphic in `T`, meaning you don't have to choose it up front; the caller of `as_slice` passes it in, along with a function converting `&amp;[u8]` slices to `T`s. So all you have to do is find a `&amp;[u8]` slice from `&amp;self` (of the type implementing `Key`) and pass it to `f` (which should be pretty easy, if you're a `String`).
Tower seems to be using old-Tokio style with reactors and handles still. I guess we are in a transition period when all earlier Tokio API calls (and docs, and examples) will move to new APIs. It would be great if we get some high-quality libraries upgraded fast to new Tokio. 
Partial deallocation is still undefined behavior regardless of whether `ralloc` allows it. Clang and other conforming C compilers are allowed to optimize under the assumption that `free` is only ever called on a pointer that was previously allocated with `malloc`, is only ever called once on that pointer, and it frees the entire region that was originally allocated. So it would, for example, be perfectly legal for it to optimize away any code attempting to access the array after the first chunk was freed, under the assumption that the rest must be unreachable code (because it would be undefined behavior to actually access those parts of the array). I'm not saying that LLVM currently does this, but I am saying that it *could.* Redox's documentation is therefore incorrect: this is not in general safe to do even in Redox (unless the compilation target has different semantics from both C *and* the Rust memory models that are in progress; or if alloc and free aren't being passed to LLVM as such, but if that's the case you're going to lose out on a lot of performance).
We’re moving a project to actix atm. We don’t use websockets though. 
For someone coming from Python and other more higher-level languages, this was a great article. The first article I didn't put side for 'later'.
Thanks for the reply. This has been an extremely elucidating thread!
I'm not saying PoW change is without problems. I'm saying that having option to have completely unusable currency or something with problems, people would choose something with problems. &gt; The new-PoW network will start with low membership and hashrate, making it vulnerable to Sibyl attacks It can also be very lucrative situation, so more people would be attracted to mine. It's like if they had a chance to go back in time to 2009 and mine shitload of bitcoins. So the hashrate would rise quickly. (Especially the first retarget period.) Sybil attacks are not a problem because of PoW. &gt; The loss of all ASICs will drastically affect power efficiency There's no such thing as "power efficiency" in Bitcoin. The amount of energy that is being spent is the same because if you increase efficiency, you increase difficulty, which leads to having to burn more electricity. In case of attacks, people should wait longer for their transaction confirming. &gt; The PoW change does not alter the dynamics that caused the network to become centralized in an emergent manner previously, and thus it is exceedingly likely that it will happen again. Depends on how it's implemented. If it made developing ASICs much cheaper, it'd allow for greater decentralization in the future. Anyway, I believe the miner centralization is temporary fluctuation and once they start hitting physical limits (they are very close to this), it will change.
I like the ideas behind Rust but I happen to think it "looks ugly". To me it's the abbreviations and heavy use of sigils. That's entirely due to my background and fondness for Pascal verbosity or LISP ((((lack-of-syntax)))). It's not a deal breaker of course, but it just adds to what it takes for me to break into a new language. It's unfamilar, and different enough that I don't yet find it pleasing on my eyes.
Yep! Especially all kinds of double-ownership structures like channels can be prone to leaks.
In JavaScript, there's no async block, but you can simply extract a function that implements it: ``` function foo(): Promise&lt;Thing&gt; { console.log('prints immediately'); return asyncBlock(); } async function asyncBlock(): Promise&lt;Thing&gt; { console.log("prints when the future is first polled"); await bar(); return await baz(); } ``` 
I decided not to enter the license discussion here, at first, because I don't think it's very helpful. No, I don't believe CC-BY-NC is a good license here. The problem is: there's a legal understanding of "non-commercial" and a moral understanding, which is often "not-for-profit". The legal understanding is often not what people want. Non-profit business can be commercial. For example, RustFest Berlin _cannot_ handout CC-BY-NC material without asking the author first, and courts here have a very narrow understanding of non-commercial as "if there's a trade involved" (this is independent of our legal form, some may have noticed that the RustFest umbrella is a limited company). You may remember us handing out Zines from Julia Evans at RustFest: we asked for permission for that reason. So, CC-BY-NC might rule out use that the user wants. Finally, I believe that the IDGAF, while certainly a bit ad-hoc, is okay here: the user clarfies that while they might retain rights in some jurisdictions, they don't give a fuck about enforcing them. That still gives them the option, but they will probably not. Which is something I can work much better with then someone using a long and complex legal text. This indicates that they care about the subtleties and means I will send them a mail, asking for rights before. Which is probably not what the user of the license wants, because they don't give a fuck and a couple of ideas they'd find cool use. 
Yes. That's the boring case though---just like `Copy` types are the boring case when thinking about ownership and move semantics. They are the exception, the case where all the interesting new rules do not take effect.
It shows the disconnect - a type is !Unpin if the contents of the type cannot be moved, not a variable of the type. That is what I had trouble learning: What was the point of not being able to move an object, when that is what a mutable reference specifies?
I'm writing a function to convert Unix style line endings (`\n`) to Windows style (`\r\n`). This is what I've come up with, but I feel like it seems a bit too long and there should be a better/more idiomatic way: fn translate_buffer&lt;B&gt;(inbuf: &amp;[u8]) -&gt; B where B: FromIterator&lt;u8&gt;, { inbuf .iter() .flat_map(|c| { if *c == 10 { Some(13).into_iter().chain(Some(10).into_iter()) } else { Some(*c).into_iter().chain(None.into_iter()) } }) .collect() }
Some Comments, rather minor points I think: * The initial/updated Universe looks a little weird. Can it be formatted as a picture instead? * What is a Glider? I am personally familiar with what they are, but if someone is new to Game of Life, they might be scratching their heads a little. * Excercises, are there answers to them, how do I validate my work? * 4 by 4 universe format is broken on my mac, maybe format it a little differently or use an image? * Is there an auto-refresh option for this project setup, or do you always have to refresh your browser after changes? 
Now the interesting question is: Can that speed be matched in safe rust?
I'd probably just use a `for` loop and append to a `Vec`. If I wanted maximum speed, I'd use `memchr` to skip to each newline, and pre-allocate a `Vec` of perhaps 1.05x the size of the input.
Next time somebody asks “Why is macro hygiene important” we should point them to this post.
And deadlocks! Rust does not prevent deadlocks either.
thank you for the detailed answer and that is actually what we intended. we want people to use the pics to spread the word about open source and discourage the people that are far away from the ideas that are on the prints and just take the pics and resell them on shirts online to make money. personally i don't want to get involved in any license debates, i just want to make some fun while drawing and the idgaf is the most simple way for me to give people the idea on the usage. 
yes, you are right and this was an experiment. i expected such reaction from some people, but the pic is as is. anyway thanks a lot for feedback.
thank you for kind words.
yup thanks :)
&gt; The problem is: there's a legal understanding of "non-commercial" and a moral understanding, which is often "not-for-profit". The legal understanding is often not what people want. Non-profit business can be commercial. For example, RustFest Berlin cannot handout CC-BY-NC material without asking the author first, and courts here have a very narrow understanding of non-commercial as "if there's a trade involved" (this is independent of our legal form, some may have noticed that the RustFest umbrella is a limited company). I really hate wasting energy on licencing issues too! But, this interpretation is too probably strong. The NC clause in CC licences prohibits activities that are "not **primarily intended** for ... commercial advantage or monetary compensation [emphasis mine]". It doesn't place an absolute barrier to everything commercial. I'm not familiar with the courts in .de, but I expect that the licence also includes the requirement for the main intent of republisher to be commercial. If you're handing something out for free—even if you are a commercial company—you may republish NC works if your intent is not *primarily* commercial. If I were the creator, I wouldn't use IDGAF if I intended to make my works available for "personal use". The RustFest example could easily be worked around, the creator could grant Mozilla/RustFest organisers/etc a more liberal licence. I understand that that's vague, but IDGAF would certainly be far broader than what's necessary. 
Basically, all assignments are that, though I don't generally think of loop variables in that context. Pattern matching is split into two domains, *refutable* and *irrefutable* matches. Refutable matches are matches which can fail e.g. `Some(a) = b` can fail, because `b` might be a `None`. In Rust, refutable matches are only allowed in `match` expressions and its derivatives (`if let` and `while let`, there may be an other I'm not thinking of). Irrefutable matches are matches which can not fail (to run once it has compiled), a trivial assignment is that, but so is destructuring a tuple or extracting data from a struct. In Rust, irrefutable matches are any time you create a binding outside the above e.g. a function parameter, a `let` binding, a `for` binding, … In either context, you can additionally use modifiers (`&amp;`, `ref`, `mut`, `@`, there may be others) to customise the binding, so assuming the iterator yields tuples of refs you could have: for (a, &amp;b, ref c, ref mut d) in it { stuff } 
I see. Indeed object identity is a difficult question. My first reaction to your statement is: "What is an object if not its contents?" `&amp;mut` does *not* prevent moving, as witnessed by `mem::swap`. However, if you assume an object is identified also by its location, then I begin to understand your confusion. In Rust, the identity of an object (when it is fully owned) is just its contents, i.e. the bytes in memory. The fact that all types are movable means identity cannot depend on location. The point of `Pin`, if you will, was precisely to enable making the location part of the object's identity. 
Hashlife might be worth mentioning though.
Great! I know this is irrelevant as long as you are consistent but for me index calculation should be `(row * self.width + column) as usize` (instead of `self.height`).
&gt; I really hate wasting energy on licencing issues too! But, this interpretation is too probably strong. The NC clause in CC licences prohibits activities that are "not primarily intended for ... commercial advantage or monetary compensation [emphasis mine]". It doesn't place an absolute barrier to everything commercial. I'm not familiar with the courts in .de, but I expect that the licence also includes the requirement for the main intent of republisher to be commercial. My whole point is that the interpretation is _contextual_. It _is_ up for interpretation and the legal interpretation is frequently at odds with the intention of both the license writers and the users. For that reason, CreativeCommons _themselves_ have a full page on this issue! https://wiki.creativecommons.org/wiki/NonCommercial Here's a quote: "Especially in today's world of interlocking personal and professional lives, defining where "noncommercial use" begins and ends can get extraordinarily tricky." And that's precisely my point. Or: ["The use of an -NC license is very rarely justifiable on economic or ideological grounds. "](https://freedomdefined.org/Licenses/NC) &gt; If you're handing something out for free—even if you are a commercial company—you may republish NC works if your intent is not primarily commercial. Precisely that point was decided in court in Germany and the companies in question have _lost_. "Handing out for free" is _not_ a qualifier for non-commercial use. Advertising is _always_ handed out for free and _definitely_ commercial use. &gt; If I were the creator, I wouldn't use IDGAF if I intended to make my works available for "personal use". The RustFest example could easily be worked around, the creator could grant Mozilla/RustFest organisers/etc a more liberal licence. I understand that that's vague, but IDGAF would certainly be far broader than what's necessary. That's my point. It's an unclear situation, so we need to ask for clarification. The NC clause of the CC only adds to the confusion, by setting unclear boundaries in place in a text that wants to give clear rules. "I don't give a fuck" provides me with much more practical security, as any problems will only happen when the person in question sues, but they went out of their way of telling me they don't give a fuck. This actually gives me some practical legal security, because "won't sue me across the ocean" _is_ practical security, even if they would have grounds to sue me on.
Does it already have backpressure support?
Make me a leather\-bound Rustonomicon with this on the cover and I'll give you all my money.
I think I understand your point - and I agree! - non-commercial requirement adds lots of complication. Sticking with the a licence that's easier to understand and comply with makes lots of sense. I certainly wouldn't want to use it, because of the reasons you cite mainly. I still wouldn't want someone to use something like CC0/IDGAF/WTFPL if they wish to retain some control, but that's a side issue I guess. It looks like we both understand each other's positions but have different emphasis on what's significant :)
You could also try PVS-Studio
The RocksDB crate is great! Very similar to LevelDB, but allegedly better optimized for SSD. And the rust crate allows you to use arbitrary bytes for the keys: https://crates.io/crates/rocksdb
Note that this is Windows-only. Only found out after cloning. :)
Sweet! I'll actually be in the Bay Area in May.
Looks great at a quick glance. As someone unfamiliar with the wasm world, could someone explain how npm fits into the wasm toolchain? Is it a requirement? (Guess it's probably mentioned in this book.) 
Nice, now it’s only 55 threads. Thanks a lot for the fast fix!
Sync (and go's implicitly async) is definitely easier, but if we could end up with some sort of future-reader/future-writer trait that everyone would agree on, we might be able to get close to Go. An example use-case of this flexibility that I struggled a little bit with in Rust was streaming an S3 object to a HTTP response, or streaming database results as they chime in. Some libraries just returned [u8]'s, others just didn't have an output directly compatible with Rocket's streaming. There's nothing *magical* about Go, and I think we should keep an open mind about our ability to reach or surpass them. It'll probably take time, as well as effort from great crate authors like you, for us to reach the same kind of convergence that Go has on their entire stack, especially io.Reader/io.Writer. (A fun side-note on performance: My Go implementation was actually much faster, and with lower latencies. That was compared to Rocket, not actix-web, and my Rust-newbness is of course a contributing factor.)
Define "safe Rust". It should be possible with some`unsafe` code wrapped in a safe zero-ish cost abstraction.
As someone who's only used languages with garbage collection, I found this I really good overview. The Rust docs themselves are fine, but the beginner-oriented descriptions in this helped reinforce a lot of what was there.
ELI5 for rust concepts is a great idea! Thanks for sharing.
In particular, [std::mem::forget](https://doc.rust-lang.org/std/mem/fn.forget.html) is safe.
&gt; feedback appreciated I see what you did there.
It's a bit of a nitpick, but wouldn't a better approach be (a) fix the hygiene of `#[derive]` to avoid the super-weird messages pointing to just the trait, and (b) to improve the error message itself to be less confusing? It's probably a bad habit, but if there's an error in my compilation, I'll rarely read the warnings, assuming they're issues unrelated to the error. E.g. for the first case error[E0308]: mismatched types --&gt; src/main.rs:6:7 | 6 | A(u8), B(u8) | ^^^ expected enum `std::option::Option`, found u8 | = note: expected type `std::option::Option&lt;std::cmp::Ordering&gt;` found type `u8` = note: | 4 | pub const cmp: u8 = 1; | ---------------------- pattern `cmp` is treated as a reference to the constant `cmp` defined here (But better phrased and formatted. :) )
thanks!
I do not want to be able to shadow constants.
No, it supports any key that implements `Key` (I'm the author).
Interesting. But is there actually a need for this to be safe? I get that this does not "fundamentally change Rust's safety guarantees", but the three use cases given in the docs would all work if `forget` was unsafe.
It's safe because it's not unsafe. Long time ago, there was [std::thread::scoped](https://github.com/rust-lang/rust/issues/24292) API which was unsound when `JoinGuard` was leaked by making `Rc` cycles. To avoid issues like this in the future, `std::mem::forget` was marked to be safe.
XBox and WII official SDKs use C++, not C.
The reasoning that it's safe is the other way around: there's no need for it to be `unsafe`. The thinking was having it as `unsafe` when it can't lead to memory corruption ends up watering down what `unsafe` means, and may push people to using pointlessly inefficient safe reimplementations of `forget`, like creating a reference cycle with `Rc`: func safe_forget&lt;T&gt;(val: T) { struct Looper&lt;X&gt; { ptr: RefCell&lt;Option&lt;Rc&lt;X&gt;&gt;&gt; } let looper = Rc::new(Looper { ptr: RefCell::new(None) }); *looper.ptr.borrow_mut() = Some(looper.clone()) }
Soooooo, `leveldb`: I recommend reading this post. https://github.com/skade/leveldb/issues/27#issuecomment-342073910 `leveldb` predates Rust 1.0, it even predates associated types, `Into`, `From`, `AsRef` and similar. That means the interface is antique. For example, this was the old style of taking a temporary reference: fn as_slice&lt;T, F: Fn(&amp;[u8]) -&gt; T&gt;(&amp;self, f: F) -&gt; T { f(&amp;self) } So, it is built on antique practices, has an extreme reliance on traits, it never became complete (see the notes in the README) and it would need basically a rewrite if you would like to port it. It is based on clean and usable bindings (`leveldb-sys`), which I highly recommend when you want to build your own. Also, I don't want it to be much more then glue, as `leveldb-rs` is the library that provides higher-level-bindings (but is also discontinued). I would actually be willing to review a series of patches to change that, _but_, I would like to see a plan for current users first, maybe even filling the missing gaps. Given that `leveldb` (the C library) is discontinued and the modern codebase is rocksdb (by Facebook), I don't know how much work there is even.
No, I was meaning that the dot net based web assembly frameworks are literally loading dll files that contain dot net assemblies. [This thing](https://github.com/aspnet/Blazor). This is not just *technically* a dll file, it is the very same kind of dll file that is being used by all other dot net programs. Just look at the list of requests that this page is doing: https://blazor-demo.github.io/
I’d offer that even if writing performant ORMs is largely done, sprocs if possible would still be better separation of concerns. Also in terms of communication, it’s easier to tell people to know and master sql rather to know and master the peculiarities of some ORM flavor of the day.
Well Rust 1.x macros are not hygienic, this is nothing new. Rust macros 2.0 will land in 2018 and they are hygienic, so this issue can't happen there.
The derive things are bugs in derive and can be fixed. The fact that constants interact weirdly with patterns is less of an interaction between features and more of an unintentional side effect of one feature -- pattern matching on constants. (It's also due to an implementation limitation that could be fixed if we really wanted to) You can find this kind of thing in just about any language.
Much thanks for this list. Now I have a better awareness of other options I have.
Oh! Sad to hear the plan changed!
Forgive my lack of understanding, but how are keyword arguments implemented in a compiled language? I don't know of any programming languages (in my limited experience) that support kwargs that aren't interpreted languages.
This isn't strictly Rust related - but I'm hoping to draw attention to this problem, and I think that Rust is the best candidate for the language of the solution. If anybody here is looking to start a high-impact project using Rust, please consider this!
This sounds a lot like structural types. Rust used to have those, but switched to nominal types somewhere before 1.0. I was not around back then so I can't say why that decision was made.
So, as somebody who many uses C# but has dabbled a bit with Python/Ruby here's my take: There's two main uses of "keyword arguments": named arguments and unknown arguments. In C#, named arguments are just extra syntax at the call site, it doesn't affect the function declaration at all: public static int MyFunc(int x, int y) { ... } var result = MyFunc(x: 2, y: 4); and since you're naming them, you can pass them in any order: var result = MyFunc(y: 4, x: 2); C# doesn't have "unknown arguments" because there's not much to be gained from them instead of just passing a collection type: public static int MyFunc(Dictionary&lt;string, object&gt; stuff) { ... } var result = MyFunc(new Dictionary&lt;string, object&gt; { {"x", 2}, "y", 4}, { "color", "blue"} }); Then you just use the normal collection methods to process that data. If you just want to allow "many arguments" you can use `params`: public static int Add(params int[] values) { ... } var result = Add(1, 2, 3, 4, 5);
I am working on a source to source to a compiler. is there any other way to concat string output in rust? right now I am doing this. ` let text = format!("{name}({args});", name=name, args=scalar_args.join(",")); let _ = buffer.write_all(text.as_bytes()); `
functional languages do it quite often I think. In elm there is the concept of "Record". You never have to specify the type you are constructing, the type just has to have the right fields: https://www.elm-tutorial.org/en/01-foundations/06-type-aliases.html Define a function: label: { id : Int, name : String } -&gt; String Call the function: label {id: 3, name: "my name"} You can call that pattern something, but it is not necessary type alias Player = { id : Int , name : String } label: Player -&gt; String -- still works label {id: 3, name: "my name"}
Iterator remainders. Sometimes I want to `.take(n)` from a large/infinite iterator without throwing away the rest, but there's no way to get the inner iterator back from a (used) `Take`. Is there a pattern for this I'm missing?
You may be interested in UDT, a file transfer protocol designed to overcome the limitations of tcp for high latency networks. I think shoop was a period of concept tool in rust using it, not sure it's still maintained.
I feel like I would have been at a loss with this problem. Completely agree with the conclusion though :)
You want `Iterator::by_ref`: fn main() { let mut it = 0..; for i in it.by_ref().take(5) { println!("{:?}", i); } for i in it.by_ref().take(5) { println!("{:?}", i); } }
Because nominal types let you do things like have public fields and `#[repr(...)]`, and having to match the exact field ordering every time you constructed an instance of the record was annoying. Rust always had nominal types in the form of enums (in ancient days, `tag`s), BTW.
I don't get it. How can it be more efficient then bittorrent?
Yes, I guess that is the closest we've got right now (and also Tsunami) - unfortunately, both projects seems dead, and don't have what I would consider to be critical features, such as secure file transfers. It's also lacking robust server software. The unfortunate side effect is that without a robust Free alternative, many government and private organizations are now coagulating around the non-Free Aspera instead, which is why I wanted to call attention to this sooner rather than later. Thank you for your comment.
Have you benchmarked it against the c++ implementation?
Most bittorrent implementations are TCP based, so under adversarial network conditions, you can lose a lot of throughput due to the guarantees that TCP gives you. Whereas if you built it on TCP, you could even avoid ACKs, and only ask for retransmission of unreceived packets. I'd suspect you'd probably have an ACK for every X packets or something + dynamic backpressuring based on the ACKs or something. 
They don't have to be "arbitrary kwargs" the way Python does it. Keyword arguments aka named arguments are just when you force callers to name the argument to use it. Under the hood it's just a normal function call. If you want optional named arguments, you provide a default value in the function decl, and the caller is compiled to pass down the default value if it doesn't specify that argument. Python's `**kwargs` needs a built-in hashtable type. We could totally do that, but it's not very rusty probably, so it probably won't happen. We could do it in a potentially more rusty way by doing something like `**kwargs: SomeType` where `SomeType` implements a `Kwargs` trait which is fed all the extra values. You wouldn't be able to splat it over functions taking defined keyword arguments without more functionality on Kwargs, though, and at some point it becomes rather heavy.
Bah -- I fixed this bug and updated the code in the repo, but forgot to update the snippets in the tutorial. Thanks for pointing this out! https://github.com/rustwasm/wasm_game_of_life/blob/0aeed7b9c5b7d3d38c4313a8b921240d820152c4/src/lib.rs#L143
I was replying to both... but this discussion is becoming a bit pointless :).
I think you mean if you built it on UDP
Not rust but would this fulfill some requirements? https://github.com/facebook/wdt
Is Aspera really expensive? 
**UDP-based Data Transfer Protocol** UDP-based Data Transfer Protocol (UDT), is a high-performance data transfer protocol designed for transferring large volumetric datasets over high-speed wide area networks. Such settings are typically disadvantageous for the more common TCP protocol. Initial versions were developed and tested on very high-speed networks (1 Gbit/s, 10 Gbit/s, etc.); however, recent versions of the protocol have been updated to support the commodity Internet as well. For example, the protocol now supports rendezvous connection setup, which is a desirable feature for traversing NAT firewalls using UDP. UDT has an open source implementation which can be found on SourceForge. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
UDT isn't dead, it's *stable*. There's a difference. First version was released 17 years ago. Here's an app using it to enhance rsync to use UDT: https://github.com/LabAdvComp/UDR
How does [dat](https://datproject.org/) stack up against it? I don't think it will necessarily do that much better than HTTP/FTP, but transferring Large datasets is supposedly what it's for...
You can do that too -- the question is whether in the ABI it is the responsibility of the caller or callee to substitute the default. Option lets the callee do it (and lets it default to different values contextually). Using const defaults lets the caller do it (though the type of the function is now an interesting thing)
I think he was referring to shoop, not UDT.
Also, none of this strictly has to do with Rust, when it comes to I/O, most languages get approximately the same performance. Rust excels at CPU-bound applications. For I/O bound you might as well use whatever you consider to be easiest.
I have actually done a lot of work in this area. I believe you are looking for [gridftp](https://en.wikipedia.org/wiki/GridFTP) or [bbcp](https://www.slac.stanford.edu/~abh/bbcp/).
Thank you for your comment. I agree that the CDN and Proxy components are a big part of what Aspera is selling, and total alternative would need this aspects, but in practice, I have not encountered the CDN or Proxy service in the wild, only organizations (NIH and SRA) who are using the server software alongside FTP. Really, this is the main issue I want to draw attention to: because a Free solution has not been "productized" in the way that say, Apache/Nx or Aspera have been, large organizations are now moving to a proprietary software solution due to the perceived lack of choice. To me, this is the really bad thing, because I care a lot about using free software and free internet protocols. I am reminded of this now infamous [comment on Hacker News](https://news.ycombinator.com/item?id=8863) announcing the launch of DropBox: &gt; you can already build such a system yourself quite trivially by getting an FTP account, mounting it locally with curlftpfs, and then using SVN or CVS on the mounted filesystem. From Windows or Mac, this FTP account could be accessed through built-in software. We are in the same position with high speed multi-UDP file transfer. Maybe we can, in theory, hack something together that can approximate the performance of Aspera, but unless we do the groundwork of polishing it into a something that works for the use cases of these big data organizations like the NIH, end users will still be forced to use the proprietary software.
Right, so I cannot comment as to how polished a solution needs to be, but when I say "rsync + tar/gzip" there are plenty of GUI tools out there that do that already. Tar/Gzip is just for compression, but that's only part of my point What I was going for is that I consider it highly unlikely that someone would be able to get even close to the same kinds of speeds without a CDN. Theoretically, you may even see those kinds of speeds with completely open-source client tools like wget while using a similarly fast CDN. The core issue here though is that even if your CDN uses FOSS, the big expense is running the CDN, and the "the groundwork of polishing it into a something that works for the use cases of these big data organizations" is precisely what no one can do for your CDN in a "Free Software" way. They can write the free software, to be sure, but someone would still have to run it and that's the hard part.
µTP has the opposite purpose, effectively giving BT traffic lower priority.
I guess, to clarify, when I said "rsync + tar/gzip" I didn't mean it in the same way as the dropbox thing. I meant it more in the sense that comparable tools already exist, but they do not solve the core CDN/service problem. No free tool that anyone here could make would solve that problem. Your best bet is likely a pro-bono service from Google or something based on the humanitarian needs that you're trying to solve.
I really like this quote. It’s a lot more realistic than “if it compiles, it works” which some people like to throw around.
&gt; Aspera is a service, it doesn't look like an independent tool (keep in mind that I only learned about it now, so I could be wrong You're wrong. Aspera is a family of software tools for data transfer. There's not a CDN involved.
Really now? No CDN and they transfer 10x-20x faster? That's very interesting... how did they not make this a bigger deal and monetize it more?
Can I even have something like null in safe code? Either by design or by mistake....It seems to me that I can never use anything like that in safe Rust.
**Fast and Secure Protocol** The Fast and Secure Protocol (FASP) is a proprietary data transfer protocol. FASP is a network-optimized network protocol developed by Aspera Inc., owned by IBM. The associated client/server software packages are also commonly called Aspera. The technology is patented under US Patent #20090063698, Method and system for aggregate bandwith control (sic). Similar to the connectionless UDP protocol, FASP does not expect any feedback on every packet sent. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Apparently there is no CDN involved, so my commentary is moot, but there is another issue: https://www.reddit.com/r/rust/comments/8bzvrh/a_desperate_plea_for_a_free_software_alternative/dxb9hz8/
i just eyeballed that stuff so take it with a grain of salt: if your line is a 100mbit line no tool will make it faster than 150mbit with compression. tcp acks are a problem on high bandwidth medium-high latency networks, this much is definitely true. tcp was designed for - from a modern perspective - very low throughput networks. other networking protocols like sctp do not have that problem because they implement selective acknowledgement. if you control both the client and the server you can try [withsctp](https://manpages.debian.org/testing/lksctp-tools/withsctp.1.en.html). if you control at least the server you can also try to use a different a different [congestion control algorythm](https://unix.stackexchange.com/questions/69241/) which changes how acks/packet losses are interpreted. this can make a huge difference depending on what your usage scenario and network is like. since yours is not the normal webbrowser usecase you should check it out. i suspect the aspera thing incorrectly implements exponential backoff (because almost everything does) and is chocking other tcp connections on the same network. hope some of this helps!
what is this duktape thing? for some reason github reports 90% of your code to be C due to it.
Because the claim is bullshit. Winning against a single HTTP connection doing a huge transfer is hardly impressive.
What exactly about the claim is bullshit, please explain? (not dubious, just genuinely asking) 
No
It’s left as an exercise for the user. ;-)
Aspera is $10-70000, per year, just for the server software. The CDN is extra. The upper end of that range is for this appliance, commenters elsewhere talked about 10k/year being their negotiated cost. https://www.cdw.com/product/IBM-Aspera-Enterprise-Server-1-Gbps-license/4042382
As long as you write only safe code, your libraries (including std) don't fail to uphold their invariants, and the compiler does not screw up (for instances of this search for issues marked as 'unsound'), you should not get any memory errors, including holding (just having, not even dereferencing) a null pointer.
Hi everyone! I'm the creator of this project. I'm happy to answer any questions!
Well, eventually, it could be faster than modern operating systems on the same hardware because of faster context switches and fast (once wasm supports some more stuff) ipc. Also, the same binary would be able to run on any architecture without recompiling.
I'll admit that I don't really understand what this is, could you explain?
Without even looking at the code I can tell there was a massive algorithm improvement, and if those changes were ported to Rust it would also shine.
&gt; No CDN and they transfer 10x-20x faster? That's very interesting... how did they not make this a bigger deal and monetize it more? I mean they've monetized the hell out of it, as far as I can tell. Their pricing for anything but the most basic tools is absurd. I heard somewhere this is the tool Netflix uses to move movies around the datacenter. We use it at work because it's one of the two protocols NCBI (the National Center for Biotechnology Information, a branch of the National Library of Medicine) permits you to use to upload/download DNA sequencing datasets, which can be quite large.
Actually, I think the perf improvement may come from heavy TMP/constexpr.
interesting, I'll have to go look at it
Yes, others definitely should. I am very out of date with my C++ and I don't 100% understand benchmarkgames layout so I could be totally off. There are many people on this subreddit more qualified to look. But I just wanted to point out that there are definitely pieces of code in that submission that can *not* just be ported to Rust.
What makes you think this?
Nintendo is gonna cease and desist this so hard. Great display of tech but abiding by laws is important too
Yes, exactly.
What are some good open-source projects I can read through to get a sense for good, idiomatic, modern (!) Rust code? Now that I've gone from total beginner to fairly ok at piecing together small programs, I'm remembering how important it was to read other code from bigger projects to see the patterns experts use to tackle problems. What Rust projects did you learn from?
I haven't gotten to that point yet, but each driver will be compiled to a webassembly module.
Although, let's nuance this by mentioning these are tricky to end up with in the first place. For example, creating a doubly linked list in Rust is no joke. 
Emulators are not illegal. Running official ROMs you don't own is.
What do you think the person is running in his example? I agree emulators are not illegal. But the author running a copy of super mario bros is most certainly illegal
That's interesting! Do you think nebulet is going to take any code from RedoxOS or vice versa? Considering they are both a microkernel OS implemented in Rust one could think they would have a lot of common.
is it.. essential? any big advantage over utilizing the native shell?
Which is why services like Dropbox are often written mostly in python.
I suspect most of advantage comes from having the sizes known at compile time, so that the compiler can fearlessly unroll loops. I've used the same trick and it can save a surprising amount of time with very hot code.
My only suggestion is to cross post this on /r/OSDev. I'm sure they'd like to see this.
and there is no way I can have something like: let name: String = null; Right?
Maybe they own the cartridge and copied the Rom from that? 
[aria2](https://aria2.github.io/) lets you download using multiple streams.
That's fascinating, I'm now very exited about this project. Thank you for your explanation@
I suspect this could be done much better with the entry api, `map.entry(texture_name).or_insert(self.texture_creator.load_texture(texture_name));` Though you'd have to tie your 'None' string there somehow '.expect("Failed to load texture")'. but someone better than me should explain what's wrong anyway.
Sounds kind of like the approach I heard MS's Midtori took. Neat!
It's standard boilerplate for open-source projects done by Googlers, see the [official docs](https://opensource.google.com/docs/releasing/publishing/#disclaimer). A major advantage to releasing this way is that I get to use 20% time.
Definitely. After that we just need to make sure the compiler is smart enough to unroll tiny loops (which llvm should theoretically handle) and we're good.
Yes. Option&lt;T&gt; basically replaces it for the "by design" case. It differs from nullability in that it forces you to check whether the value is present - compiler just won't let you pass Option&lt;String&gt; where a String is needed.
Nice
For the most part, no references can ever be null. Safe code cannot make them null, and unsafe code is responsible for making sure it never makes them null. But if you're asking more about an idiomatic way to represent a string that "might or might not be there", then what you're looking for is `Option&lt;String&gt;`.
Exactly what? Why would it be faster?
It may (or may not, but it probably would) be a good idea to cross post this over to /r/EmuDev!
Driver? I thought that webasm had no IO and that it had to be done through javascript.
What is essential? The report on GitHub or Duktape JavaScript?
Do you think it would make sense for reuse of existing C apps to just functionify Unix interrupts?
Well, yes. Web assembly can only call out to external functions. So, you have drivers work the same way they do on normal OSes. Just, they're compiled to wasm instead. 
I'm not sure what you mean. Could you be more specific? 
Technically yes, but it'd be pretty overkill. 
I might, but the implementations are pretty different. 
True, although this may not be the best example as I understand that Dropbox are migrating the core of their client (not the UI) to Rust, and already run a mix of Go and Rust om the server.
Are you sure a CDN is neccessary in this case. I k ow for example that in the past I have gotten almost linear increases (3x-4x over filezilla) in throughput by using a multithreaded FTP client. It's not inconceivable to me that a custom protocol on top of multiple UDP streams could get a 10x increase on a fast network (which the OP appears to have).
For instance if I had a C hello world app for Linux, if I compiled to wasm would it just call functions similar to old interrupts for printing to screen?
Someone would have to write library to link your hello world app again that would implement the printf function and import the right module,but yes that would work. 
As u/lachlan_s posted in another comment on this thread, running in ring 0 eliminates context switches, which speeds up the performance of an app that is syscall-heavy.
You don't need to actually install things. Go to the packaging section of this tutorial http://gtk-rs.org/tuto/cross and follow it. You can just have everything in one folder. The tutorial also show's you how to change and package a theme to make it fit in more with windows, and how to set the icon.
The TCP protocol relies on packet loss to measure the available bandwidth- on packet loss, it slows down a bit, then slowly ramps up, until there is packet loss, etc. It is therefor also quite sensitive to packet loss from other causes, and it reacts slowly when the latency is high. So not really suited for high-speed transfers on high latency lines. There are efforts to build other protocols, one of them is QUIC by Google. I'd include a link, but you'll get more info if you just google for "google QUIC". Like Aspera, it is built on top of TCP. A lot of Google services are actually available over QUIC and it is built into Chrome- so you might already be using it without noticing it. There's an effort to standardize it and multiple open source [servers and clients are available](https://github.com/quicwg/base-drafts/wiki/Implementations). Other than that, the classic solution to speed up downloads over TCP is to just run a bunch of them in parallel. For example the 'lftp' client has support for this. amazon01:~$ lftp -e 'pget -n 20 -c ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR036/ERR036000/ERR036000_1.fastq.gz; exit' 360283695 bytes transferred in 13 seconds (27.19 MiB/s) In rare cases, TCP connections get in a state where they get and remain slow. If I was to build a download accelerator, I'd probably build one as a HTTP client (the FTP protocol sucks and in the end it's still just a TCP connection) and just download chunks of, say, 16MB, on 8-16 threads in parallel. If a chunk download gets slow or stuck just cancel it and try again. Something like that would, in fact, be not that hard to implement in Rust. (Btw, the ftp.sra.ebi.ac.uk site is also available over HTTP but it appears bandwidth-limited - you'd need a fast, unlimited HTTP server to serve the files. I would use NGINX).
Why would const generics be faster? Can't you fake const generics by adding the const as a parameter to a function? For example a struct point, where you pass a length to the constructor, which then makes an array of that length for the coordinates.
Are you allowed to copy? I know there are people that have setups where they actually load the rom from cartridge at play time. (I guess it's still 'copied' into memory, but you know what I mean).
Right, so how would a program written in webasm actually do something like print to a terminal if all it does is export functions? 
How does performance of the trieset and redblacktree compare to the btree in the standard library? Does the persistence make the data structures slower?
This is horrifying. Well done.
My understanding is that if the const is in generics, compiler can generate different code at compile time (think about template), based on the const value, and possibly eliminating the branching cost. But, if it is passed by function parameter, the compiler wouldn't know how the function is used at runtime, and will only generate one copy of the function w/o such optimization.
Yeah you're doing way to much in your scope. pub fn load_texture(&amp;mut self, texture_name: &amp;'static str) -&gt; &amp;Texture&lt;'a&gt; { if self.loaded_textures.contains(texture_name) { return self.loaded_textures.get(texture_name).unwrap(); } let loaded_texture = self.texture_creator.load_texture(texture_name).expect("Failed to load texture"); self.loaded_textures.insert(texture_name, loaded_texture); return self.loaded_textures.get(texture_name).unwrap(); } I'm not sure how the `If let Some()` scope works. Effectively you're immutably borrowing `loaded_texture` for the duration of the call, than attempting to mutate it. The borrow checker is really dumb FYI. 
Remember downvoted don’t mean legal expertise I’ve seen projects like this taken down before 
Some lifetime shenanigans are obscuring the real error. Here's a way to fix them: pub fn load_texture&lt;'b: 'a&gt;(&amp;'b mut self, texture_name: &amp;'static str) -&gt; &amp;'b Texture Now you'll get the classic error [E0502](https://doc.rust-lang.org/error-index.html#E0502): "cannot borrow `self.loaded_textures` as mutable because it is also borrowed as immutable". Unfortunately, the [`Entry` API](https://doc.rust-lang.org/std/collections/hash_map/enum.Entry.html) can't solve this problem. In order to get an `Entry`, you need to borrow `loaded_textures` by borrowing `self` mutably — but you need to borrow `self` immutably at the same time to grab `texture_creator`! A full solution: pub fn load_texture&lt;'b: 'a&gt;(&amp;'b mut self, texture_name: &amp;'static str) -&gt; &amp;'b Texture { if !self.loaded_textures.contains_key(texture_name) { let loaded_texture = self.texture_creator .load_texture(texture_name) .expect("Failed to load texture"); self.loaded_textures.insert(texture_name, loaded_texture); } self.loaded_textures.get(texture_name).unwrap() } Note that with this setup, you can't hold two textures at the same time: let tex1: &amp;Texture = handler.load_texture("a"); let tex2: &amp;Texture = handler.load_texture("b"); because each one needs a mutable reference to `handler`. Consider using [`lazy_static`](https://crates.io/crates/lazy_static).
One could just use generic-array or some other crate for this today.
the javascript
I am personally left wondering how a lot of things are going to be secured and how visibility of certain functions will be hidden. Also, can we trust the webassembly implementation is fully secure? It's probably fairly secure and can be patched to prevent even some cpu security flaws, but it might have some issues. Honestly, its really cool. If this is the future, we could ditch a lot of unnecessary memory protection extensions from CPUs and give applications direct access to hardware level IPC mechanisms and all sorts of things. It is exciting, but so many questions about implementation. Can't wait to see what comes out of this!
I know, but it's easy to see how this didn't get adopted like other protocols. What's different is that what became widespread began life in browsers first and was standardized later. I know it's sad but it's the reality. At least Metalink has been adopted where it originated and is used by FreeBSD and some Linux distros. QUIC is HTTP/2 with UDP for data transfer, and if it adopts Metalink's features, we get 99% of the issues solved. That would leave just optimizing the packer overhead for increased throughput, aiming for (near) line rate transfer. https://bugzilla.mozilla.org/show_bug.cgi?id=331979 https://bugs.chromium.org/p/chromium/issues/detail?id=1751
&gt; `If let Some(x)` is borrowing the `self.loaded_textures` field immutable for entire scope of the function. Actually, `if let` conveniently doesn't do that! [See this example.](https://play.rust-lang.org/?gist=bf8029c0c1549c8db9539af882ca00ff&amp;version=stable)
Even if the web assembly implementation is insecure, at least it can be fixed. CPU bugs are evidently much harder to fix.
Integer generics are on the way. I am tracking all associated issues. I have a lot of projects I'm waiting on those to be in nightly to begin. The performance issues are just one of the many things that will fall into place.
A well placed gif, a Ponzi scheme, six wiffle balls, and a pack of emaciated alpacas. I'm not sure really, but I know enough to be censorious.
Yeah, they're a long time coming.
Actually it does [link](https://play.rust-lang.org/?gist=0b5f791c849a196ec232628847373a92&amp;version=stable)
You're saying that the idea is that _all_ apps run in ring 0? Not just the kernel and drivers?
There was certain reasoning behind this choice indeed. Firstly, I needed to sandbox reducers from accessing the system (and doing anything potentially malicious). It was also important that the reducers would be able to work with JSONy objects easily (shell scripts are not particularly good at it unless you require the use of `jqq). Making it accessible to the vast majority of programmers was also a consideration. Lastly, but not least importantly, Duktape is such a great project with amazing documentation :) That being said, SIT's structure allows to add more reducer types to support any language or markup (or whatever else that can drive the reduction), so it is just a merge request away...
[removed]
Read the questions related to emulators carefully: at no point does nintendo say directly that emulators themselves are illegal. Every time emulator software is mentioned, it's immediately followed by "to play illegally copied Nintendo software". They're (intentionally) conflating emulators with illegally copied Nintendo software. This page is carefully designed to trick you into thinking whatever Nintendo wants you to think, they're straight up disingenuous in basically every paragraph. I like Nintendo games, owned all the handhelds until the 3DS, but their legal dept is a bunch of dicks.
I'm not the author of the project, but yes, that seems to be exactly the intention. As the author states further up the thread, this relies on wasm's memory safety/sandboxing for security instead of the usual priviledge levels.
A simple chat server can easily end up with an internal structure that holds a list of `Sender&lt;T&gt;` that needs collection when the clients disconnect. It's not hard.
Yeah, but that's kind of a trivial example. "Rust leaks memory if you tell it to". I used to subscribe to the notion that Rust makes leaking memory harder, but kind of pedaled back a little, as I found that most accidental leak situations are not that different from other programming languages.
And this would be a good point to mention `ref`, which again goes in a pattern, but does the opposite of `&amp;`: it *adds* indirection. Here's a few typical uses where you need it: let list = vec![(1, String::from("hello")), (2, String::from("goodbye"))]; for (i, ref s) in &amp;list { // ... } Since IntoIter only has a reference to the list, it can't move elements out, so any non-`Copy` types must be borrowed, and `ref` puts in the borrow. fn foo(s: &amp;Option&lt;String&gt;) { if let Some(ref string_ref) = *s { println!("{}", string_ref); } } They're working on a language change ("match-ergonomics") which will allow the compiler to automatically infer the necessary use of `ref` for you, because there's almost always just one sensible, valid set of places to put `ref` into a pattern.
I'd fight in WW3 against the Javascript infection.
I think it's a very gray area. Has been for a while now.
Yes. The execution environment creates logical isolation, where there’s no way to reference another program’s memory, which means there’s no need for address space isolation (i.e processes) like in a typical operating system. 
**Challenge Accepted**
It calls functions that other things export to it.
You'd still need context switches, the idea is they would be cheaper.
Not sure where the double ownership comes into play in the example. 
It's exports and imports all the way down.
Not really. I'm curious to what the security implications of that are. Could you expand on that?
The fanboyism/fangirlism around webassembly is honestly sickening to me. You're making something incredibly inefficient just so you can run it in a browser. There's no rhyme or reason you'd want to do so besides making a toy to show off with it.
Sorry, I totally misunderstood you! I thought that you meant that `if let` automatically borrows until the end of the function — but of course the same problem occurs whenever a borrow extends, say, [using `match`](https://play.rust-lang.org/?gist=db91a34394f6f0604c807ba8adb246f1&amp;version=stable). As you point out, the way to make this work is with `contains_key`.
That's not what's happening. He's using WASM as just a standard VM and running that in his kernel on top of bare hardware. If you look at the repo it's a lot of very x86-y things, it's not being built to "run in a web browser".
What's your biggest complaint with kernel development in Rust?
True, that's a good distinction. I should have said this eliminates some of vthe overhead of a context switch, eg TLB flushes
How about writing a NES Emulator with just Rust?
Luckily, the legality of emulation is not for Nintendo to decide
I'm not sure what you mean. What's inefficient about web assembly?
This isn't running in the browser. This is an operating system who's applications aren't compiled to machine code. This is far from the first project to try this and the theoretical benefits have been known for a while. What sets this project apart from others is that it uses an existing ir that *C* can target, rather than forcing application developers to use a specific language. If he gets far enough that he can get a standard library, he should be able to compile existing applications like the various Unix utilities with minimal changes, rather than rewrite them from scratch in his source language of choice. Android is actually a good example of the kind of OS he's writing (ignoring native apps). Don't like the fact that you have to use Java? Too bad! Write your own compiler that targets Dalvik. With web assembly, we already can already compile languages like C and Rust to it and any language that uses LLVM can be hacked to work. His choice of IR vastly expands the language choices for application developers.
Almost :P
I actually really like it. Much nicer than using C or C++.
&gt; This is far from the first project to try this and the theoretical benefits have been known for a while. &gt; &gt; What are those supposed benefits? I fail to find how compiling to javascript can ever make something faster.
The blog post outlining the 2018 roadmap talks a little about this. Not from the perspective of the rate of change, but it does give some insight on how rust plans to manage changes going forward. https://blog.rust-lang.org/2018/03/12/roadmap.html
I've found it's rather annoying getting the toolchain to work for me. I'm having a little difficulty getting it to play nice in a mixed asm/C/Rust project [I'm working on](https://github.com/treyzania/subsonic). Part of the problem is that I also need to link in GCC and LLVM intrinsics, which are annoying. I have some stuff on my local machine that I've been trying to get to work, and I've been considering stepping up my game from just bash scripts to Python. I mainly don't want to be coupled to Cargo is my main dilemma.
You're running javascript instead of x86_64, that's going to be a lot slower no matter what you do.
WebAssembly isn't javascript.
Uh, no. It's not javascript. WebAssembly compiles AOT to native.
Yes it is. That's what the asm.js is for. It's runtime-interpreted through javascript. I have no idea why anyone likes this idea. People are too hooked on running things in a browser.
No it's interrupted in asm.js.
That's just incorrect.
question about this syntax: `fn foo&lt;const X: usize&gt;() {...}` I'm curious why the `const` keyword is required here. To be used as a type parameter, the value *must* be const, and a non-const value isn't allowed. So it seems like it doen't convey anything useful. Though I guess maybe it adds clarity to new users, who might be confused why they couldn't do: ``` let y: usize = random_value(); let rectArray = RectArray&lt;y, y&gt;::new(); ```
Apparently you need to call a function to initialize the R interpreter: https://cran.r-project.org/doc/manuals/r-release/R-exts.html#Embedding-R-under-Unix_002dalikes
The whole point of Nebulet is that it's running directly on the CPU with no other environment to speak of. There's no OS for a "browser" to even be running on.
Note that web assembly, though inspired in part by asm.js (which is a subset of js), is not actually JavaScript at all. It's a binary bytecode format targeting a separate execution VM/spec than JS.
Note that the addition of new features (even ones which change the definition of what is idiomatic) is not incompatible with a goal of backwards compatibility. The Rust team seems to have done a great job overall in maintaining backwards compatibility, so all your code continues to compile and run, regardless of which version of the 1.0+ compiler you initially targeted. 
Nowhere is anything being compiled to Javascript. The benefits include faster context switches since they compile to regular function calls. It can also enforce safety without having to mess with page tables since everything is preprocessed so it can insert bounds checks where the original code didn't have any.
Because `X: usize` is a type parameter `X` bounded by trait `usize`. Wait, `usize` isn't a trait... silly programmer, that's nonsense! *Compile error*.
yes. you own the cart, you can do whatever you want with it (including reading the roms). what's forbidden is *distributing* copies to people that don't have the rights to them.
yeah, i was thinking that maybe one of the many scripting languages implemented in rust would be a fitting choice here?
Because I think Meson is better for mixed-lang projects, but it's just not ready for Rust yet! :) Thanks for the crate tip though!
IPSF - interplanetary file system - assuming you can get a community of organizations to host the files and they have reuse value. 
What does this have to do with Rust programming?
Thank you!
Yes, of course. Will be happy to provide guidance and advice to help developing those additional reducers for those other languages. Some refactoring is also overdue on the JavaScript reducer side so it'll be a good opportunity to refine and unify things there.
Most code, not all.
A lot of people recommend RipGrep for reading. The author (or someone else?) also wrote some extensive blog posts about the architecture.
Is there a language that is both in active use and not evolving its idioms over time? I can think of major changes to C++, Java, and Python literally decades into their lifetime. Assuming Rust thrives, I think it's reasonable to expect that the first decade will see more changes than subsequent decades, but there won't be some point where the language is "idiomatically stable". That kind of stability leads to stagnation and death.
&gt; How is this horrifying? Because it intuitively sounds like a terrible idea, but I can't actually think of a reason that it's a terrible idea. In fact the more I think about it, the more I think it might be a good idea.
This is quite a rude thing to say. You’re scorning someone’s work and enthusiasm. Moreover you’re doing it because you literally don’t understand what the project is. Either that or this is a troll. In any case, it’s rude and uncalled for. 
This comparison between the two crates in a github issue might be helpful: https://github.com/danburkert/lmdb-rs/issues/4#issuecomment-310907548 General consensus seems to be that `lmdb` is the best choice.
This is the best tl;dr I could make, [original](https://blog.chain.com/faster-bulletproofs-with-ristretto-avx2-29450b4490cd) reduced by 88%. (I'm a bot) ***** &gt; A few months ago, B&amp;uuml;nz, Bootle, Boneh, Poelstra, Wuille, and Maxwell published Bulletproofs, which dramatically improves proof performance both in terms of proof size and verification time. &gt; Our implementation uses Ristretto, a variant of Mike Hamburg&amp;#039;s Decaf scheme, to implement a prime-order group using Curve25519, and accelerates curve operations with an AVX2 implementation of the parallel Edwards formulas published by Hisil, Wong, Carter, and Dawson in 2008. &gt; The Ristretto implementation we use is provided by Isis Lovecruft and Henry de Valence&amp;#039;s curve25519-dalek, which was designed to provide a clean and safe API for implementing complex protocols such as range proofs and anonymous credentials. ***** [**Extended Summary**](http://np.reddit.com/r/autotldr/comments/8c5isc/we_made_bulletproofs_twice_as_fast_with_rust_and/) | [FAQ](http://np.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/ "Version 2.00, ~310246 tl;drs so far.") | [Feedback](http://np.reddit.com/message/compose?to=%23autotldr "PM's and comments are monitored, constructive feedback is welcome.") | *Top* *keywords*: **proof**^#1 **implementation**^#2 **curve**^#3 **protocol**^#4 **range**^#5
Why not use rsync over ssh ? Should also be orders of magnitude faster than https transfer.
That bootstra.386 theme tho!
Check out pcwalton's sprocketnes though I'm not sure how well maintained it is. I just started writing one myself (restarted...for like the third time...) So if anyone has any tips on getting the damn PPU working, I'd be grateful!
much thanks. That was very helpful and convincing and have decided to go ahead with `lmdb`
https://github.com/github/linguist/blob/master/README.md#vendored-code
Inside a function you can borrow both `loaded_textures` and `texture_loader` mutably. So there is no reason to avoid `Entry` API.
&gt; Or, you can take advantage of the fact that wasm is 32bit only for now and can only access up to 4 gigs by allocating 4 (well, 8 because of some edge-cases) of virtual memory and only map as much as is needed to physical memory. Then, just catch any accesses to the unmapped memory and treat them as out-of-bounds accesses. I believe the real problem starts when the accessed address is mapped but does not belong to the current piece of code, i.e it is stealing info from a driver/module/app. 
Oh, nice! Excellent! Thank you!
https://github.com/sit-it/sit/commit/506ce2c5d8ba3080f84137b6929a93b27fcab3be
The Rust crate, of course. If I remember correctly, I had to implement Key for my type.
Cool :)
Mind if I ask why it's not just Rust + module level asm? It looks like page.c is the only C file that not just a thin wrapper. Do you have any particular reason to write parts of the OS in C? Compatibility can't be the reason because any Rust would limit you to platforms that Rust supports. In addition, asm files can be loaded into the rust toolchain using `global_asm!(include_str!("entry.S"))` and it would be included in the build by cargo. Honestly, I can't see much reason at all beyond personal preference to not use cargo.
&gt;Full disk encryption by default I don't get how this can be a sane move, it easily doubles my boot time.
The internal structure of a Channel is dropped when both the Sender and the Receiver are dropped.
We could do that via typenum traits.
I'm nowhere near having something usable yet, so there's probably going to be a lot more that's beyond just C. But also Subsonic isn't *supposed* to be a serious project so a lot of it is *indeed* personal preference. &gt;loaded into the rust toolchain using Yes, but it's a little difficult when you have to get things *exactly* right for x86 and things like Multiboot to like you. (Entrypoints, interrupts, thread init, etc.) It really is just plain more straightforward have normal object files that you link into the executable in another way.
That's what sections and linker scripts are for. ;) Or you can just go the easy route and use [bootimage](https://github.com/rust-osdev/bootimage) and [bootloader](https://github.com/rust-osdev/bootloader).
[Rocket](https://github.com/SergioBenitez/Rocket) is pretty good as well, featuring macros and other syntax extensions, but do note that the nightly API that it uses might change.
Is there some list showing common ways to models types in rust? In particular, how do idiomatic Rust express invariants on types, such as for example integers from 1..100, non-empty lists etc? Newtype? Contracts by assertion? Count_minus_1 nameing (where 0 actually means 1, 1 means 2..)
Yeah but this doesn't help user-defined proc macros :/
In some post recently I saw a pretty weird way of converting stuff. For example, for converting to slices I could define a trait something like this: trait ToByteSlice { fn convert&lt;T, F: FnMut(&amp;[u8]) -&gt; T&gt;(&amp;self, f: F); } This is pretty powerful - because slice does not need to be returned, conversion does not necessarily need to allocate (if implementor knows what the maximum bound for the length of the slice is - for example `char` can internally use `[u8; 4]` and avoid allocations completely for conversions to UTF-8). However, in that post this was called out as being an old way of doing things (as in pre-standard traits like `AsRef`, `Borrow`, and the like). But aren't they way less flexible, because they require the result to borrow the argument? Or did I miss something?
If first stage description is half your boot time you either have a seriously short boot time or a extremely slow computer. Even then, the disk encryption itself is probably not the performance bottleneck, but the decryption of the encryption keys used to encrypt the disk. And I suspect both grub and the kernel are repeating those process, so you might incur it double. But after that initial slowdown the performance should only be slightly impacted by the full disk encryption. So l, apart from increased boot times, you should not be too much impacted by such a change.
No need for that, we already have `associated const`s and they work on plain structs, so we can just use a `Key` trait with a `size::` associated const and implement it for a couple of keys, e.g. `key1`.
how do you do I/O anywhere? you just import a function that does it. the kernel or the vm would need to define those.
Why does it seem like there's exactly two XML writing crates, plus one built on `serde` that doesn't implement everything? Is it currently possible to generate XML in a way that doesn't require manually formatting it?
I recently started contributing to serde and I found their version of the try operator (no conversion permitted) really liberating for libraries. I might actually do that for mine too. But i would never do the same thing in application code where I’m a huge fan of the ? operator. I think what’s great about how we as a community do error handling in Rust now is that we collectively move towards practical error handling without compromising each other’s approaches. Case in point: I completely moved away from std::error now that failure is here but I can still use the entire ecosystem. At the same time I just wrote some code that uses an integer error enum for errors and that works too! I don’t think such a pragmatic approach would have been seen as realistic a few years back. 
Love the page design, but the slow buildup is a bit annoying
Would const generics still be faster if you're passing the const as a command line argument for example?
https://github.com/kirjavascript/snake-rs
There's also a lot of gains to be had in the fasta benchmark, the rust program currently isn't multithreaded while the C program is. 
* write an [IDE plugin that autogenerates use statements](https://www.reddit.com/r/rust/comments/3dy7u5/developer_wish_ide_tool_for_autogenerating_use/)
This doesn't work on macos
extra work is required to make it work in anything other than linux, because that's all I have to run it on
But all you have to do is drop the Sender. The leak isn't any more complicated than keeping a vector of anything around. I thought you ment situations where you have two structures which is each other. Those are really bad. 
Having a short, but recognizable name for things can be very important.
The current "wave" of changes in Java feels very "rapidly" and i don't use C++ / C# / Python that often anymore for the last two years as i was diving Rust – but man – every time i come back to work on some older projects i've written or just program something for fun and keep with the development it gets increasingly harder to follow the "new stuff" even in those languages! Async Python feels very different of what i am used to! C++11/14/17 is not the one i learned at university! I don't know about you but i feel that even those languages are really rushing in the last ~5 years
&gt; but there shouldn't be any disadvantage to having it as an optional regular dependency. (someone correct me if I'm wrong here.) It will participate in dependency resolution, potentially changing your dependency graph and features of your dependencies. Its more something to be aware of than a disadvantage.
Then it wouldn't be const and won't compile, since the value isn't known
WebAssembly produced by an optimizing compiler doesn't need mid-level optimization. It just needs good codegen, which is what Cretonne is focusing on right now.
Exactly, it's an ABI that's "POSIX plus Capsicum minus everything restricted by Capsicum". I hope the `".."` wasn't intentional, because `..` (up a level) is what you *can't* open in capability mode :) You can only go beneath open directories. macOS and Linux (and anything else non-FreeBSD-based) are currently only supported with a userspace syscall translator, i.e. without the sandboxing. (But that's perfectly fine for development!) There is [a modified Linux kernel](https://github.com/NuxiNL/linux) with proper secure support, based on [Google's capsicum-linux](https://github.com/google/capsicum-linux), but since that kinda went nowhere, the proper Linux port has been suspended.
it works at my macos
&gt; Is there a way to define dependencies as benchmark only? Or alternatively, nightly only? Is there another approach that I could take to achieve the same affect? Honestly, don't do this, what `interpolate_idents` does is not unstable, but extremely unstable and guaranteed to break very often. What people do is pass the name as a macro argument, so you end up with: bench_op!(name_one, one, ...); bench_op!(name_two, two, ...); What you want can't be done in Rust today, accept it and move on.
Works perfectly fine on my mac with gtk+3 installed via brew
It's not *that* much more crazy than the C++ templates IMHO. C++ just has so much crazyness that this particular instance doesn't stand out.
You need to get LLVM running on bare metal first.
Hey, I have got a struct which holds a buffer of bytes. Is it standard to provide a method that simply moves out the buffer from the struct? struct Image { data: Vec&lt;u8&gt;, } impl Image { steal_data(&amp;mut self) -&gt; Vec&lt;u8&gt; { self.data } } So is the above code considered bad practice? Also, should I include methods for getting mutable and immutable borrows for the data? My goal is to write this data buffer to file, but I do not think that my image struct should have that ability. Also, I want this functionality to be on a PixelBuffer trait, so any kind of image can implement it. Does anybody have some good design for this? Thanks in advance!
Right. In wasm32, the maximum memory offset is 32bits long, or 4 gigs. So, it's actually impossible for it to extend beyond its allocated virtual memory region.
Hacker news discussion: [https://news.ycombinator.com/item?id=16836022](https://news.ycombinator.com/item?id=16836022)
I took note of things that slowed me down as I was going through the tutorial, although I am relatively new to Rust so they were more just misunderstandings on my part. I'd be happy to put together a PR for either of these if you are interested. * I had to `rustup install nightly` before `rustup target add wasm32-unknown-unknown --toolchain nightly` * After finding the solution this one is obvious, but I was getting an unclear and not easily google-able error message here * Hello world html had to be changed from fetching `hello_world.gc.opt.wasm` to `hello_world.gc.wasm` * This is because I didn't install wasm-opt Thanks for putting together this tutorial. I found the performance optimization section particularly interesting. 
Wasn't this just posted a few days ago? https://www.reddit.com/r/rust/comments/8asb4i
This might be a pretty dumb comment, but am I the only one who is put off from 'Pop!_OS' simply on the basis of it having such a terrible name? I know that's ridiculous, but it's just an awful moniker for something that purports to be a serious general purpose OS.
The above code won't compile ¯\_(ツ)_/¯ You can't really move the data out of the struct if it is borrowed, unless you put something instead (like a new vec, a clone, etc.)
You dropped this \ *** ^^&amp;#32;To&amp;#32;prevent&amp;#32;anymore&amp;#32;lost&amp;#32;limbs&amp;#32;throughout&amp;#32;Reddit,&amp;#32;correctly&amp;#32;escape&amp;#32;the&amp;#32;arms&amp;#32;and&amp;#32;shoulders&amp;#32;by&amp;#32;typing&amp;#32;the&amp;#32;shrug&amp;#32;as&amp;#32;`¯\\\_(ツ)_/¯`&amp;#32;or&amp;#32;`¯\\\_(ツ)\_/¯` [^^Click&amp;#32;here&amp;#32;to&amp;#32;see&amp;#32;why&amp;#32;this&amp;#32;is&amp;#32;necessary](https://np.reddit.com/r/OutOfTheLoop/comments/3fbrg3/is_there_a_reason_why_the_arm_is_always_missing/ctn5gbf/)
I must have missed it at the time. I just came across it at hacker news.
I am using it currently, but whenever I see the name "Pop!_OS" it annoys me a bit. I agree that they should've gone with something that flows better.
&gt; faster It could also be slower for applications which use hand-tuned assembly, e.g. many video codecs
Remember take downs don't mean legal expertise I've seen projects been killed by sheer fear of a legion of lawyers.
My interest in Rust entirely hinges on it being a good C++ replacement, and that means being able to achieve equal performance. I don't think anybody should just accept Rust doing worse.
Even low grade processors are capable of decrypting faster than the fastest SSDs are capable of reading. So it shouldn't be noticeable.
Quite a long time. Personally, after 1.0 I started going towards "I can recommend this language". Currently, it is moving further away from that point again. Unfortunately, it's not just features that are added. There's a whole philosophical instability currently happening. Explicitness is less valued than it was at 1.0. Error handling seems to be pushed towards something looking like exceptions. Editions have opened the door to change pretty much anything. And 10 years means 3 editions of breaking changes. It's impossible to know what Rust will look like in 10 years, or what will be idiomatic or a core driving philosophy. I'm sorry if I sound frustrated, it's because I am.
Hey, I can't figure what type annotations are needed for my function parameters. let X = Matrix2x5::from_columns(&amp;[ Vector2::new(-2, 4), Vector2::new(4, 1), Vector2::new(1, 6), Vector2::new(2, 4), Vector2::new(6, 2) ]); I currently have this: X: &amp;Matrix2x5&lt;Vector2&lt;i32&gt;&gt;
It's almost impossible to tell from this amount of code what the problem is, or how to solve it. What function? What parameters? Where are these types coming from? What's the error?
Objective-C was used by fewer than 5% of programmers when Java was launched. To the _majority_ interfaces were new. Pythons use of decorators to inject code predates C# and operates in a much more intrusive way. Most people spent barely a few weeks on continuations in their CS course, and likely forgot it when they started working. Prior to the advent of mainstream async/await, there was no need to remember it. There are tons of blogs online describing async/await and attesting to the unfamiliarity of goroutines ans async IO in general. You sound like an educated person, and probably a decent developer. But consider 15 years ago, and every developer aged between 20 and 40: would you bet your wage that the majority would have been had full recall of their CS degrees and would be capable of being productive in these tools with just a day of practice? 
Nice, thank you. I completely forgot that I could take ownership of self.
If you cannot claim ownership of yourself, then can you truly own anything at all?
Perhaps [rocket](https://github.com/aochagavia/rocket) and [rocket_wasm](https://github.com/aochagavia/rocket_wasm).
I tried using `S.into::&lt;T&gt;()` recently and it didn't work, IIRC. It's a bit more attractive than `T::from(S)` in certain cases where S is an expression because it makes it more readable, IMO. Does anyone know why this doesn't work?
That website is terrific.
`dyn Trait` allows for higher ranked trait bounds in most situations. There was recently an accepted RFC to allow by value `dyn Trait` which will improve the ergonomic of the feature. Enhancements are still needed for rank-2+ types though. 
Hey, I want to have a trait, called Pixel, and I would like to be able to create every implementation for the pixel from an iterator. So I found somewhere that to do this I can do this: trait Pixel&lt;'a&gt;: FromIterator&lt;'a u8&gt; { ... } but this just feels wrong. Because everywhere I use Pixel, I have to give it a lifetime parameter, even if I'm not constructing it from an iterator. Is there a way around this? Right now I am implementing FromIterator for every implementation of Pixel, I don't have a trait bound on Pixel, to indicate, that I should be able to create every implementation of the Pixel trait from an iterator. Thanks for the help in advance :)
One more question, if you have the time. I have a PixelBuffer trait, and Image would be an implementation of this trait. The unpack and write function I would shurely put in the PixelBuffer trait, and implement it in the Image struct. But should I indicate AsRef on the PixelBuffer? trait PixelBuffer: AsRef&lt;[u8]&gt; { ... } Should I write the above, or simply implement it on Image?
deep :D
If you want, assuming every `PixelBuffer` must be accessible as a slice of bytes. Also, I'd be tempted to put the `write` function on a trait so that it could be used for anything that can be written out to a binary stream.
oh ok, thank you, I will try that :)
There encrypted partition won't be reached until Linux and the initramfs are well loaded. The boot partition remains unencrypted -- just the root &amp; swap are encrypted with LUKS.
Making things blindly mean the *common* case is part of what the article warns against. The `dyn Trait` cannot be instantiated, but once you have a reference, `&amp;dyn Trait` behaves as a single type in all aspects. You can put different expressions of that type in the same variable, return them from different branches of a conditional etc. Not so `impl Trait`. Every instance of `impl Trait` is a *different* type and you can't mix them. That would be extremely confusing without the `impl` keyword indicating that this is something unusual. Adding `dyn` makes sense. Removing `impl`, no, it does not.
but I want it to use it like this, where self.data is Vec&lt;u8&gt; let pixel: BGRPixel = self.data.iter() .skip(start) .take(3) .collect(); And if I do it like you've suggested, I get this compiler error: the trait `std::iter::FromIterator&lt;&amp;u8&gt;` is not implemented for `image::bgr_pixel::BGRPixel`
You can use `.cloned()` on an iterator to turn `Iterator&lt;Item=&amp;T&gt;` into `Iterator&lt;Item=T&gt;` provided that `T: Clone`.
[unrust](https://github.com/edwin0cheng/unrust) showed up a couple of days ago in /r/rust_gamedev , maybe it is worth a look
So I solved this by using FromIterator&lt;u8&gt; but, my original idea should be possible, no? I only have to define a lifetime parameter for the FromIterator trait, when I am implementing it. So I shouldn't have to annotate every use of Pixel with&lt;'a&gt;.
In the meantime you could try something like piping it into xclip.
I included the complete example.
To be clear, this is not me saying "Give up, c++ is faster". It is "recognize that C++ is faster today".
Maybe I'm misunderstanding, doesn't dyn trait just change the syntax for bare trait objects?
You can also do trait Pixel: for&lt;'a&gt; FromIterator&lt;&amp;'a u8&gt; { ... }
Maybe so. The C++ code in the submission seemed fairly straightforward and idiomatic. I don't know that the Rust code would, but if so, yeah it's a viable option.
This is being tracked in [**rust-lang/rfcs#1481**](https://github.com/rust-lang/rfcs/issues/1481). There is no RFC yet. The closest mention would be the following in [RFC 1598](https://github.com/rust-lang/rfcs/blob/1c353bc4b3ba729a34ff9cc6c624a53d87993ff2/text/1598-generic_associated_types.md#associated-type-constructors-of-type-arguments): &gt; This RFC does **not** propose extending HRTBs to take type arguments, which makes these less expressive than they could be. Such an extension is desired, but out of scope for this RFC.
That is exactly what I was looking for! Thank you kind stranger :D
Trait objects *are* higher ranked trait bounds though, it’s a universal quantification of the type rather than an existential qualification that a normal trait bound provides. Higher ranked trait bounds is actually a rather poor but of terminology because even lifetimes can’t be rank-2 yet, the `for` syntax provides rank-1 universal quantification of lifetimes with the lifetime erased at runtime, which is exactly what a trait object does. 
Anyone who does make it should name it "ad astra"
`Matrix2x5::from_columns` makes a `Matrix2x5&lt;i32&gt;`, not a `Matrix2x5&lt;Vector2&lt;i32&gt;&gt;` (that would be a three-dimensional matrix). The types are somewhat complicated aliases, so it's hard to tell from the error message, but it it is suggestive: error[E0308]: mismatched types --&gt; maxung.rs:19:20 | 19 | perceptron_sgd(&amp;X, bias, &amp;y); | ^^ expected struct `na::Matrix`, found i32 | = note: expected type `&amp;na::Matrix&lt;na::Matrix&lt;i32, na::U2, na::U1, na::MatrixArray&lt;i32, na::U2, na::U1&gt;&gt;, na::U2, na::U5, na::MatrixArray&lt;na::Matrix&lt;i32, na::U2, na::U1, na::MatrixArray&lt;i32, na::U2, na::U1&gt;&gt;, na::U2, na::U5&gt;&gt;` found type `&amp;na::Matrix&lt;i32, na::U2, na::U5, na::MatrixArray&lt;i32, na::U2, na::U5&gt;&gt;` Note the "found type" (the type of the value being passed) has `i32` where the other has `na::Matrix&lt;..&gt;`.
Hooray! Good on you, sir. Just a small note: you may want to consider including some kind of migration command. I did something similar a little while ago with `cargo-script`. Also: *thank you so much for `cargo dirs`.* That said, I suspect this means I'll have to add another migration for `cargo-script` because of this :P
The only feedback I have is "what took so long?" This is a completely logical change to make.
Hey Quxxy, thanks for the hints! Can you expand on what kind of migration you would need to add to `cargo-script`? I avoided doing a migration and opted for keeping existing installs as-is, because people might have other tools or build configuration rely on .cargo, and I feel it would be rude to break their code. I tried to cause the minimal amount of disruption, and make it easy for people to retain their setup for the time being (e. g. people can just do a `mkdir .cargo and completely ignore the proposed improvements for – I guess – forever).
FWIW, I'm on Windows and would have to symlink the AppData cargo directories off of C:\ to my D:\ drive. .cargo is currently symlinked away. I have a annoyingly small boot drive SSD (80GB) that's 99% full and a 1TB hard drive in my laptop. When it switched from .rustup to .cargo I had some fun times figuring out why cargo was falling (the drive was full). I'm very annoyed at Windows for the situation I'm in. Many parts of Windows have to be on the C:\ drive, including %appdata% and Program Files. On every install I tell what I can to install to D:\, but because I can't move these folders without breaking Windows, I accumulate junk onto my C:\ drive. Office straight up refuses to give you a choice of install location for the consumer install and anything that plays nice and uses AppData is throwing files onto my limited space SSD. I recently got a couple GB back by figuring out how to safely move my temp directory to a different drive. It's slowly being eaten away at again, though. I really need to buy a bigger mSATA SSD for my boot drive so I don't need to worry endlessly about AppData anymore, just my user directories which can actually be moved across drives safely.
[@isislovecruft's latest tweet](https://i.imgur.com/0HMlqwV.jpg) [@isislovecruft on Twitter](https://twitter.com/isislovecruft) - ^I ^am ^a ^bot ^| ^[feedback](https://www.reddit.com/message/compose/?to=twinkiac)
There are also new `CARGO_CACHE_HOME`, `CARGO_CONFIG_HOME` and `CARGO_DATA_HOME` environment variables that you could use, if you grow tired of going the symlink route.
Oh, I don't mean automatic. `cargo-script` keeps its caches alongside other Cargo stuff (except on Windows where I belligerently insisted on putting them in the "correct" place). When I fixed the location on !Windows, I added a migration command to move the folders from the old location to the new. This was 100% optional, since it continued to use the old location if it found it. You had to explicitly run `cargo script --migrate-data MODE` to actually do the migration (`MODE` was just `dry-run` or `for-real` since I'm paranoid about doing anything that *might* destroy data).
Pop!_OS doen't use GRUB. It uses a combination of systemd-boot and a tool called kernelstub that was developed by a system76 employee.
Ah, I understand, that sounds like a reasonable idea. I think something like this could be done in a follow-up PR, because I'm kind of concerned of enlarging the size of the current one too much, because things like this tend to get bogged down in debates.
I'm having some difficulty figuring out lifetimes. I have a function with the signature `load_texture&lt;'b: 'a&gt;(&amp;mut self, texture_name: &amp;'static str) -&gt; &amp;'a Texture` What is the purpose of `'b` here? Removing it, having simply `&lt;'a&gt;` makes it no longer compile. Is `'b` being implicitly assigned to something?
The operating system is based on an explosion metaphor. It represents the circumstances of us moving off of Unity and striking out on our own, doing our own thing. That happened very suddenly and very rapidly. Think Big Bang.
If removing `'b` causes this to not compile, then you're not including all relevant code. Where's `'a` defined? What do you mean by "removing `'b`"? Is there a `where` clause? What does the `impl` look like? If you don't know what `'b` is for, how did it get there?
There is quite a bit more, yes, more than I figured was suitable for this thread. My point was simply, as `'b` is not used explicitly *anywhere* else, can it still affect things?
The problem is: when you're miserly with details, we have to start guessing at what the problem is. Problems in Rust are often *very* context-sensitive. If there's a lot of code, link to a playpen. As for `'b`, I would say: "no", except you say removing it breaks the code, and you've cut so much context (even the `fn` keyword!) that I can think of one thing (a `where` clause) which you might have removed that *could* make it relevant.
It's not merged yet, so there is always the option of it taking another 5 years. :-)
&gt; The current "wave" of changes in Java feels very "rapidly Java has been both language- and standard library-wise technologically behind for years, hence ppl moving to Kotlin and Scala. I don't think Java is ever going to catch up. So no, don't think it's a good example of something evolving rapidly.
Thanks, the type aliases were the thing which left me baffled.
Editions are by definition not incompatible. You can compile code spanning multiple editions in the same project. 
Not in the same crate. If you have a 10 year old code base with a workspace containing 4 sub-crates, each of those can have a different edition, which means a module from one can't simply be moved from one to the other, as it could break. The only compatibility that exists is that your dependencies can have a different edition than your project.
Go hasn't changed much at all since 1.0 in terms of core language features.
That's possible. Unfortunately this subreddit has become a bit quick with the downvotes.
Since the ESR post, there's been a lot of misinformation about Rust's stability. Naturally, we are quick to defend :) You are right, though, we shouldn't downvote first, ask questions later. 
There are still some megafeatures that have not even been proposed yet that we **might** want to add. Among these are ConstraintKinds, HKTs, dependent types, totality checking, and effect polymorphism. In this respect, Rust is catching up to languages like Idris and Haskell. I suspect we will stop adding megafeatures eventually because there are none left. After we are out of megafeatures to add or decide that we don't want some megafeatures, I think we will mostly add syntactic sugar and work on improving ergonomics. The standard library will of course continue to change with the software industry. My guestimate is that the evolution of Rust will slow down somewhere around 2023~2025.
at the end of the scope. If it was freed the moment it is no longer used, that would cause problems with the way MutexGuard and some other things work, probably, but I think it would be cool.
I cannot upvote this enough. Free software and open source is in many ways about people scratching their own itches, or organization's itches, if those organizations are willing to pay for it. If these organizations paying for Aspera today would put together some of their funding to pay for this, I'm guessing a free (as in speech) solution could come into existance pretty quick.
[removed]
Note that editions are not free reign to break things. Keep in mind the rule that code that compiles without warnings must continue working the same way in the next edition. The heuristic for what changes are allowed across editions is vague currently, mainly because we haven't had a new one yet; we're setting precident. The heuristic I'm seeing is that code can be automatically transitioned from an old edition to the new. So expect minor technically breaking changes like new keywords, a tweaked default, prelude changes, or migrating warn- or deny- lints to forbid- by default. Language changes will continue to come on the 6 week release cycle as normal. Opt in editions are just to clean things up. (I suspect edition-gated changes will be hard to argue for due to implementation baggage.) Editions are more marketing tool than a breakage gate.
Could this functionality be exposed as crate? This seems like it would be useful for many projects, not only cargo
&gt; Note that editions are not free reign to break things. Keep in mind the rule that code that compiles without warnings must continue working the same way in the next edition. But in the next edition it *can* warn, in the one after that it can fail. In the one after that it can mean something different. &gt; The heuristic for what changes are allowed across editions is vague currently, mainly because we haven't had a new one yet; we're setting precident. The heuristic I'm seeing is that code can be automatically transitioned from an old edition to the new. A `rustfix` will never be perfect, and you'll always have to review your code anyway, even more so if something `unsafe` was automatically changed. Every change in edition is still high impact, no matter what mitigations might possibly help, Not to mention all invalidated documentation, stack overlflow posts, forum posts and so on. &gt; So expect minor technically breaking changes like new keywords, a tweaked default, prelude changes, or migrating warn- or deny- lints to forbid- by default. I honestly expect it to go further at this point. Like removal of current features (`mod`, `extern crate`, `ref`, `ref mut` to name only some). &gt; Language changes will continue to come on the 6 week release cycle as normal. Opt in editions are just to clean things up. (I suspect edition-gated changes will be hard to argue for due to implementation baggage.) Editions are more marketing tool than a breakage gate. There are a couple internals threads collecting things people want changed in future editions, so to me that stage has kind of passed.
Reading the title, I thought it would be about hidden cargo [configuration file](https://github.com/rust-lang-nursery/embedded-wg/pull/24)
Awesome, but how does it stack up against ring?
I don't know if https://www.destroyallsoftware.com/talks/the-birth-and-death-of-javascript is not mentioned here because it's obviously known by all already, or because some people encounter this idea the first time (which is perfectly OK). But this pretty much describes the thinking behind the whole thing, if my understanding is correct. A talk from 2014.
I want to get a reader from a &amp;[u8] and I am having problems with the borrow checker. My problem is here: https://github.com/Ekranos/rocket_static_fs/blob/6ef386458fd74caa80307ea843dee6b649d3e004/src/fs/embedded/mod.rs#L40 I am basically implementing an embedded "filesystem" for my crate, so you can embed assets in the binary. For that i thought of some kind of package so you could in theory also load packages from the usual filesystem. The error I am getting is: https://pastebin.com/xHBECJLb It is telling me stuff about static lifetimes, even though I never said that something has a static lifetime and also I can't guarantee static lifetimes cause of the support for bytes from all kinds of sources (include_bytes!, memory mapped file, etc.). Sadly I can't really tell you what the problem is, since I don't really understand it either :/ I am also pretty new to Rust, so most likely I am just missing something pretty basic.
The current version of `rocket` has diverged a lot from `rocket_wasm`, but the project may still be interesting. Thanks for mentioning!
I think I love the website way more than I should.
It's a very interesting concept and the discussions in this thread have been very enlightening. I'm hoping your project gains more traction. I'm also curious to know which architectures you're planning on targeting with Nebulet though. Is RISC-V on your roadmap? Also, are you aiming to eventually build a general-purpose OS with it?
what about nll?
&gt; I will strongly suggest to fail if both new and old location exist and have conflicting config other wise you will end up with loads of 'works on my machine' [...] This is a nice suggestion, I agree that it should _at least_ generate a warning, mentioning which config was chosen. &gt; One point to not neglect is that all existing tutorial you do not control over the web and published literature will now have cargo home dir wrong. This include things like Stack Overflow, blogs and already published books (and human brains). I would not underestimate this one. Yes, I'm planning on going through the Internet and fixing stuff (as well as pinging book authors). &gt; I think for Jupyter it's a migration that we should not have done, the cost was/is still too high. It's only a painful migration now, because people didn't do it correctly from the start, despite people begging them to do so. Using that as a reason to not fix things would be like letting VW off in the pollution scandal because people only found out after the cars were sold. From my point of view not following the standard is pretty much a non-option. For instance on my system, applications that try to write to $HOME will simply fail to work, because I made $HOME read-only.
Just keep in mind that one person proposing something on Internals doesn't even mean there's a chance of it getting in. I've seen a lot of backing off of suggested edition-gated features around most core language proposals. To go over your list of proposed removals, as I see their current status: - `mod`: not changing - `extern crate`: prelude will effectively have `extern crate` for all dependencies, `extern crate` will remain usable though - `ref (mut)`: sticking around in patterns but should need to be used less due to binding modes Also, the movement is not towards less explicit. Explicit means you know unambiguously what's going on from reading the code. `try!(op())` and `op()?` have the same explicitness - they mark the location where an `Err` might be bubbled up to the next error handling level. The problem with exceptions is not the bubbling behavior, but the fact that anything anywhere has to deal with invisible errors being thrown around. The movement is towards allowing more compact, less verbose syntax for common concepts. All of your code needs to plumb errors; thus e.g. `Fail`, `?`, and `Error` for the application level that just wants to log and retry. Implicit is supprising, non-zero-cost features. Unless you're not in favor of type inference for evident types, you shouldn't be against simpler syntax that retains information. And just one more time: proposal =/= language direction or endorsement.
It is. Here is the Rust version: https://github.com/soc/directories-rs Here is the JVM version: https://github.com/soc/directories-jvm The libraries are "compatible" in the sense that if you write the equivalent code in both libraries, you will get the exact same results.
NLL has to do with borrows rather than ownership
One major factor might just be not many rust programmers needing to use XML. You mention two - have you looked at (xml_writer)[https://github.com/pzol/xml_writer#xmlwriter]? I haven't used any xml crates, but this one seems to be the easiest to use for writing XML. &gt; using a writer that isn't just one abstraction above doing so? You're right- I don't know of any crate which has a whole document model in-memory. As for why, it's likely because no one has a dire need for one, and no one has built one? A crate for doing this will likely exist at some point, but it seems like it's much more efficient to read/write directly to custom data structures, and that's what everyone's opted to do so far.
Completely agree! Pro-tip: Make your $HOME read-only. :-) What happens here is basically me eliminating the last remaining dot-directories.
godbolt supports Rust, you can use it to see what your snippets yield: https://godbolt.org/g/hr9qw5
oh that's a great idea! I'll definitely do that
&gt; Just keep in mind that one person proposing something on Internals doesn't even mean there's a chance of it getting in. I've seen a lot of backing off of suggested edition-gated features around most core language proposals. Sure, but the focus certainly shifted from using editions sparingly, to them being an open door for changes. I mean, there's currently discussion of `try` vs. a new keyword for `?` propagation stopping. Using a new keyword would be less breakage, so if editions would really be limited and last-resort, we would focus on a new keyword instead of discussing how close Rust error handling can be to exceptions. &gt; Also, the movement is not towards less explicit. Explicit means you know unambiguously what's going on from reading the code. try!(op()) and op()? have the same explicitness - they mark the location where an Err might be bubbled up to the next error handling level. The problem with exceptions is not the bubbling behavior, but the fact that anything anywhere has to deal with invisible errors being thrown around. I'm not sure I fully agree. Explicitness is also about in-code documentation. `let mut` is not strictly necessary, but having to specify it for mutation is still explicitly communicating intent. Btw, the auto converting aspect of `?` is nice in libraries, but tends to bite me when writing applications. Everything deals with `failure::Error` but I still have to annotate all the inbetweens (or often just not use `?`). &gt; Unless you're not in favor of type inference for evident types, you shouldn't be against simpler syntax that retains information. I'm unsure what evident here means? Do you mean specifying known types? Isn't that what `?` is forcing me to do due to it calling `Into::into`? &gt; And just one more time: proposal =/= language direction or endorsement. No, but language and other core team members do take part in these discussions, so one does get a feel for a direction we're heading in even without being part of or seeing the language design meetings.
 movabs rcx, 8589934595 mov qword ptr [rax], rcx mov dword ptr [rax + 8], 1 The compiler constant folded the answer in this case, nothing too interesting. 8589934595 is 0x200000003 which is a 64-bit write to write two values into a vector.
Depending on how you think about it, there isn't ever really a sequence generated at all. 1..4 is a Range, even if you ignore any optimizations, all that's being stored is a start value and an end value. https://github.com/rust-lang/rust/blob/ae544ee1c90afc0f939753014dc5e884cb2749a0/src/libcore/ops/range.rs#L79
For a security library, the name does not inspire confidence. I don't wanna be exterminated.
I am personally torn between two extremes: * on one end, ** I _really_ like that there is only _one_ `.cargo` folder that has everything. It is a no-brainier, universally compatible, you can search for stuff there, move it, back it up, etc... ** I also really like that all the project related stuff is in the project directory. All the points above -- easy everything! ** I also really like that it is weirdly consistent with windows, because there is also `.cargo` dir and everything is there... There is definitely a convenience point here. * On the other side, I am trying to keep the home folder clean. I think it is a lost cause, but I do like programs that follow XDG and keep things nice and neat. ** The prospect of chasing multiple directories when using rust and cargo is actually quite unappealing. Probably more so than simple inconvenience of looking at the dotfiles in home dir now and then. ** On windows it is a royal mess. I just hope we don't start putting registry settings, God forbid! ** In corporate environments windows and linux servers are extremely locked down. Such difficulty of getting rust approved and installed will definitely create an additional hurdle to adoption. As much as I want a neat file and directory structure, the appeal of "single folder has everything" is really strong with me.
I learned Rust before I learned modern C++. Trying to transition from Rust to C++, I encountered significant difficulty understanding C++'s copy and move semantics. C++'s copy and move semantics "clicked" for me recently, so I decided to write up the differences (as I see them) between the languages. This document is meant to help people transitioning from Rust to C++ or from C++ to Rust get a grasp on the semantics of the other language. As an additional note, I've been told that "Programming Rust" is a good source of this information as well, so that may be a better reference for people trying to transition from C++ to Rust.
YES (caveats below) `s` won't necessarily ever exist on the stack, and there's no heap allocation, so this question doesn't really make sense to begin with. However... I think what you are really asking is: assuming `s` *is* stored on the stack, is it possible for its space to be reused for another variable after the last usage of `s` but before the end of the scope. The answer to this question is yes: LLVM can do pretty much any optimisations it likes that preserve the behaviour of the program, and this definitely fits into that category. Second caveat: there may be "implicit" uses of a value, after the last visible use in the code. For example, any values with destructors add an implicit `drop(variable)` call before the end of their scope, which means they are in use for the whole of that scope.
By the way, shouldn't write take a mutable reference to W, so we can use it later?
Absolutely true, but I think there is no way projects can avoid paying the cost. Just search GitHub for issues containing "XDG" to see how many annoyed people want their software to be fixed.
I'm only slowly learning Rust, but I think it could be interesting to have results for both approaches - idiomatic, and devious. Show what's possible with a "normal" approach, and also with trying to squeeze out every last bit of performance :-)
&gt; Adding the `'b` lifetime doesn't seem to be necessary. Okay, cool! I got that too, but I wasn't sure if I reproduced your types accurately. If you have weird borrow-checking errors around `&amp;Texture`s in the future, try `&lt;'b: 'a&gt;` again. I think it might be more general.
I could swear I even saw `malloc`s in some of them…
Thanks!
some benchmarks between native code in this, and linux would be nice :P
When I first asked about eager drops, this same argument was made. Why would MutexGuard break from eager drops? From what I can tell, it would still be smart enough to not lock too early, and it would lock earlier which is generally a good thing. The other thing I saw was timers, but why would you rely on object lifetimes at all for timing? Shouldn't it instead be based on a `timer.stop()` method, or a function that times a closure? If you really need a longer life time, `drop()` is always available.
I don't think you can share raw pointers like that between threads. I've tried doing something similar before and could not figure out a way to do it. Searching the web gave me the impression that it cannot be done. I found an [issue](https://github.com/rust-lang/rust/issues/21709) where u/japaric recommends wrapping the raw pointer in the [`std::ptr::Unique`](https://doc.rust-lang.org/std/ptr/struct.Unique.html) wrapper, but this probably has to be done in the sdl-rust library. The Rust [nomicon](https://doc.rust-lang.org/beta/nomicon/send-and-sync.html) recommends doing this: &gt;Most uses of raw pointers should be encapsulated behind a sufficient abstraction that Send and Sync can be derived. Hopefully someone with more knowledge can help you figure it out.
The "one abstraction above" bit was referring to `xml_writer `, unfortunately. I know people don't need XML these days, and I was a bit frustrated when I wrote the post. I've settled for writing it manually. Reading/writing to custom data structures is just `serde`, and there *is* a `serde` implementation for XML. Unfortunately it doesn't handle `Vec`s though, because XML doesn't handle unlabeled lists of primitive types. 
I was really worried about getting a summer job, but this might actually work. The USB project looked interesting, but the "hard" label is worrying.
In your particular case, s is just a reference to a static string, so there is no freeing that is happening. However, if some resource does need freeing, it happens in the Drop trait. Here are the rules, as I know them, for when the drop function is actually called: https://play.rust-lang.org/?gist=4cb65cd1aa66e0cdef758231ef583086&amp;version=nightly
If someone has code like this: { mutex.lock().unwrap(); unsafe { things = other_things; } unsafe { distant_things = near_things; } } it would be a breaking change to the language for the MutexGuard to be dropped immediately after it is locked. possible response: &gt; but you should keep your data inside the Mutex, then the Guard wouldn't drop early! sometimes you're synchronizing data that isn't yours, from a C library or somewhere else. &gt; The other thing I saw was timers, but why would you rely on object lifetimes at all for timing? Shouldn't it instead be based on a timer.stop() method, or a function that times a closure? Some code would use RAII for this because there are many possible exit points for a function, and the programmer doesn't need to remember to add a timer.stop() to every single exit point, which is an error prone process, especially as a function evolves over time, but they still want to be able to log and track the execution time of sections of the code. &gt; If you really need a longer life time, drop() is always available. yes, but it's a breaking change, and the behavior would be very unfamiliar to anyone who is used to RAII, increasing the likelihood of someone making this mistake. there's very little justification for doing it now, and there are a lot of potential drawbacks.
&gt; Looks more like you are firmly in the ".cargo everywhere, even on Windows!" camp, and that's perfectly fine. :-) Looks like I am :-). I kind of don't want to be, but nothing beats "one folder" policy...
I'd rather have one folder for configuration, than one folder for cargo, one folder for rustup, one folder for foo, one folder for bar, ...
Damn! I’m graduating this May. Would have loved to apply for this!
NLL can cause early drops. In fact, it may actually already happen, I'm not quite sure about that.
There are a bunch of crates which provide hashable floats. https://docs.rs/decorum/0.0.4/decorum/struct.ConstrainedFloat.html is one . Alternatively use integers or use a crate that provides decimal numbers.
It might be worth asking the maintainers of `sdl-rust` if this is a bug or a feature. Depending on how sdl itself is implemented that wrapper type might be `!Send` on purpose if the underlying library is not thread-safe
There's an implementation of `Write` for `&amp;mut W` where `W: Write`; you can pass a mutable borrow if you want to.
Thanks for that. I chose to use an id field in the struct instead. As an aside, is this something that is by design or an inconvenient side effect. Is there any planned feature that will change this in the future?
k&amp;r style C is still used and the default version of C used by many C compilers \(albeit with numerous compiler extensions added to it\). 
Basically, a type is Send if all of it's components are Send or if you override the default and implement Send manually. You don't need to add any methods, it's just a marker trait. Since it's behind the mutex, it's totally safe to lock the mutex and make changes, but rust still can't track the lifetime of the pointed-to data across threads, so rust thinks you shouldn't send this pointer to avoid a potential use after free. You could tell rust to shush if you want, but you really probably shouldn't, at least not if this is your first time encountering this problem.
Is there a way to use the XDG settings on macOS with that library? I'm building a dev tool and don't really want to be putting its settings under the Standard Directories, which seems more geared towards GUI apps.
The backwards compatability argument is correct, but I'd argue that this would actually be a better pattern let guard = mutex.lock().unwrap(); unsafe { do_things(); // Explicitly make it live till here! // Now if I copy this unsafe block around or something dumb like that I'll // get an error about the guard drop(guard); } When you're doing things with `unsafe` you can be expected to know to make sure resources live long enough. When you're not doing things with unsafe I find if very poor form that forgetting state like "I need to early drop this mutex guard/borrow/whatever" can cause bugs/cause compiler errors/cause performance issues/etc.
If you start with a `Vec&lt;u8&gt;` and do `.iter()` then you have a stream of `&amp;u8` and so you have to map/clone it to get a stream of `u8`, if your trait is `FromIterator&lt;u8&gt;`. But if you start with something that generates a stream of `u8` (e g, a [Range](https://doc.rust-lang.org/std/ops/struct.Range.html)), you'll have to map to get a stream of `&amp;u8`, if your trait is `for&lt;'a&gt; FromIterator&lt;&amp;'a u8&gt;`. 
having to remember to drop it is **not** better than not having to remember. generally, I agree that minimally short lifetimes would be cool, as I said in my original post, but RAII is extremely well understood. Messing with it is asking for unnecessary confusion.
How would I chain two futures together using `actix_web` and `futures 0.1`? I'm writing a handler that requires two database calls (one to confirm authentication of a user, another to execute the action). Regardless of how I work it, the best I can get is a nested pair of futures. Both futures need to be executed on the state's database, so regardless of how I work it, the best I get is a nested future (which isn't particularly nice to type). I've also considered putting the user authentication details into the request's state with the middleware, but that has a future in it somewhere as well and is a headache. The simplest option, I guess, would be to wait on the future. Is this also possible?
Note that a lot of SDL2 function calls and data structures tend not to be thread safe. A lot of them are backed by globals down in C land. I don't know about textures specifically though. I heartily recommend rolling your own little loop to dispatch on your `specs` components rather than making a `System` for this, something vaguely like this: https://github.com/ggez/game-template/blob/master/src/scenes/level.rs#L45-L51
Did you follow the instructions for the [rls-vscode](https://github.com/rust-lang-nursery/rls-vscode#quick-start)? This project seems to be actively maintained, unlike [vscode-rust](https://github.com/editor-rs/vscode-rust). Check the [configuration](https://github.com/rust-lang-nursery/rls-vscode#configuration) file for vscode, maybe the plugin isn't playing nice with default values. Or maybe you're having trouble with rls itself, so I would recommend reading the [rls troubleshooting](https://github.com/rust-lang-nursery/rls/blob/master/debugging.md#debugging-and-troubleshooting) guide. I use [spacemacs](http://spacemacs.org/) and it works fine for rust. I would also recommend creating environment variables for things like rust-src, rls, so you have an easier time when it's time to update stuff.
That's the one that's crashing all the time for me. Especially on format code. Then telling it to restart rls silently fails.
Haven't used RLS, but if you have a fast CPU IntelliJ IDEA + Rust plugin is a pretty nice experience. It's not the lightest IDE, but it's pretty competent, and it recently got support for macro expansion.
I think the primary use case for higher ranked trait bounds would be support for passing generic closures and functions as generic parameters, but I'm interested: is there other compelling use cases? I tried to think about it, but besides generic closures I could only come up with generic structs that store generic closures. So, are support for HRTBs largely synonymous for support for generic closures, or is there something else to it?
Oh man, I would absolutely love to be able to run Redox OS on my raspberry pi at home!
Man I wish I could jump on this, but I already have an internship lined up for this summer... So many interesting projects...
use Box&lt;Trait&gt;
I'm targeting #![no_std] environments.
USB is really quite complicated, but it would be a great learning experience. Just be prepared to pull your hair out a lot.
The compiler is telling you that you've made an array of `&amp;mut Value`, not an array of `&amp;mut Trait`. So... don't do that. let mut list = [&amp;mut val as &amp;mut Trait]; In other cases, you could write a type constraint on `list` directly, but you can't avoid writing down the length of an array, so I used a cast instead.
Ohh, thanks a lot! I didn't know you could use `as` in traits. Because it works without for Vecs for some reason.
Latest nightly (2018-04-14) seems to be much better and finally usable again.
I don't think that's a relevant concern... A 10 year spanning project could also have versions 7, 8, and 9 of java, or \(more common\) a mix of c\+\+ 11, 14, and 17. If that happens, the language is only responsible for ensuring those versions play nice with each other when interoperating, and that an older version can be easily converted to a newer version. You shouldn't complain to the language designers when you have problems converting c\+\+14 code to c\+\+11. It's the project's responsibility to make sure that they split the versions like that for good reason, and that they understand the consequences of trying to move code from one version to another. Rust editions aren't a blank check to break code any more than a transition from c\+\+ 11 to 14.
Damn, given that I've previously already written a FAT32 driver, that project seems highly tempting. Though given I can only work weekends and that I'm basically novice at rust, it'd seem counterproductive...
The thing about Rust in particular is that there are several BIG features that have been intended since the beginning \(specialization, constant generics, procedural macros, etc...\) that haven't been added / completed yet. Rust will slow down a lot when the language is done with its wishlist. Until then, there are intentionally\-placed holes and disallowed syntax, for when we figure out how to add these large features properly. Rust language design is all about moving very carefully, and it's only fast because we have a very dedicated community to solve the technical problems related to both adding these features **and** maintaining backwards compatibility. 
&gt; For example, any values with destructors add an implicit `drop(variable)` call before the end of their scope, which means they are in use for the whole of that scope. There is nothing special about `drop()`, if what you're saying were true then it's implementation, which currently looks a lot like drop&lt;T&gt;(_t: T){} would magically expand to fn drop&lt;T&gt;(_t: T){drop(_t)} Or did you mean a call to `Drop::drop`, the trait method?
What do you mean? How is that different? I realize the difference between a mutable reference to a slice and a vector derefed, but it was how I was avoiding the issue. Anyway, now my problem is making the API actually usable, it kinda sucks having to write this everytime let mut val = Value(0); let list = [&amp;mut val as &amp;mut Trait]; let array = Array { values: &amp;mut list[..] }; I was trying to solve with macros, but I need to store the value, then the array of values somewhere to pass it as a mutable reference storing in Array then to pass Array as a mutable reference to a array of Array, and I can't seem to do that with a macro.
It's a type inference failure. What can you do is this: let mut list: [&amp;mut Trait; 1] = [&amp;mut val]; The type that compiler infers here is `[&amp;mut Value; 1]`.
I'm keen to work on the filesystem, but I don't know if I'm in a position to ask for pay. I don't know much Rust at all, my only OS experience is im taking on OS161 based course right now, and I don't know if I can dedicate enough timem and productivity to the project outside of my study commitments. On top of that, my cost of living index is a bit over 200. Is it still possible to get involved? Would love to apply what I'm learning in OS course, learn the language in an applied, systems level sense, and work with a mentor to do some good work and grow as a SW engineer.
&gt; How is that different? In one you use a temporary variable. In the other, you don't. Type inference sometimes works better when you don't use intermediate storage. As for simplifying it, you should be able to write a macro that takes a list of values and turns them into an array. I tried to simplify the actual construction, but kept being bitten by all the mutable borrows restricting the lifetimes. I ended up with something like this: struct Array&lt;'a, 'b: 'a&gt; { pub values: &amp;'a mut [&amp;'b mut Trait], } fn main() { let mut val = Value(0); let mut val2 = Value(1); let mut list = [&amp;mut val as &amp;mut Trait]; let mut list2 = [&amp;mut val2 as &amp;mut Trait]; let mut array = Array { values: &amp;mut list }; let mut array2 = Array { values: &amp;mut list2 }; let mut array_of_arrays = [&amp;mut array, &amp;mut array2]; for array in array_of_arrays.iter_mut() { for val in array.values.iter_mut() { val.function(); } } } Which is even less conducive to being abstracted.
The plugin seems to use a nightly build. You can install RLS manually with a stable version, and config the plugin as follow. "rust.rustup": { "toolchain": "stable-x86_64-pc-windows-msvc" }, "rust.rls": { "executable": "rls" }
After testing it out I agree that is much more stable. The `out of order` errors (https://github.com/rust-lang-nursery/rls/issues/752) which were the things primarily causing the crashes don't seem to be happening now. I will test the latest update more and then report back to the repo maintainers.
So every module has a 4 gigs stack? Like every driver and program? Or they share that 4 gigs that were allocated? Because if they do a program can access another program (drivers included), or you will need a 200 gigs of RAM to run all the drivers, the userspace and the programs.
Are you using the latest version of the RLS? We're fixing bugs all the time, though I admit for some projects it has been a bit rough the past few months. There should be a new version in today's or tomorrow's nightly that has some further fixes. If you're still getting crashes, please file an issue with a link to the project you're editing and exactly what you need to do to get a crash. I see a lot of crash reports, but don't often have reliable steps to reproduce. When I do have that, I can usually fix things fairly quickly.
That's a great point. But aren't we like inventing FPGAs?
FWIW: in cases other than things like locks (which use atomic instructions under the hood that restrict allowable optimizations), the compiler *should* in theory be able to prove it safe to reorder the drops, as long as they don't do anything that LLVM considers "observable" from the outside (I don't believe it considers things like memory address reuse observable). I don't know if LLVM can actually do this at the moment though.
You are iterating over `clauses` immutably, I'm pretty sure (since you're calling `iter`). So what you have is a mutable variable (`clauses_element`) to an *immutable* reference to a `Clause`. Therefore, your binding can't update it. If you instead change to `iter_mut`, it should work.
Pretty sure you need iter_mut to enable mutable access to iterator elements. I.e. this does not work: https://gist.github.com/rebo/7eaededdd04715c1300c341b58c4e40b but this does: https://gist.github.com/rebo/ede1588f918752275e5a038442b161a6 
My biggest problem is that while the RLS plugin works fine on vscode, it won't even install on vscode-oss, or at least it wouldn't a month or so ago when I last tried. That's a pretty major failing.
&gt; but if you have a fast CPU IntelliJ IDEA + Rust plugin is a pretty nice experience. Unfortunately, at the present moment there's something wrong with the Rust plugin as well. In certain parts of code it takes IDEA about 5 seconds to react to changes.
For the sake of an extra string, `expect` is always better tho, especially if your project is directly user-facing
I find it to be extremely slow in re-analyzing even slight code changes. Just typing the code raises CPU usage to 100% on all cores.
That's really exciting! Just a small remark about the “payment” section: Don't take the “cost of living” index too seriously. I don't know how the figures are calculated, but depending on the city it can be quite inaccurate : having lived several years in Paris, and almost a year in Boston, seeing Paris with a higher index than Boston is surprising to say the least.
I think it's because using that terminology in a way that's quite unexpected/unconventional for Rust: when people talk about "crates being compatible" they almost always mean at the Cargo/`extern crate` level, thinking of them as an entity in themselves, rather than thinking of their internal implementation details and whether their code can be duplicated from one to the other with no modifications.
&gt; Here is the Rust version: https://github.com/soc/directories-rs If this is going to be a dependency of Cargo, you might want to check with the maintainers that it will be OK. According to `cargo lichking`, `directories-rs` would be the first copyleft dependency added to Cargo.
&gt; When you're not doing things with unsafe I find if very poor form that forgetting state like "I need to early drop this mutex guard/borrow/whatever" can cause bugs/cause compiler errors/cause performance issues/etc. Rust goes for safe-by-default, with possibly-dangerous performance concerns an opt-in, even in `unsafe` code (e.g. `x[i]` is still bounds checked in `unsafe`, you have to call a separate method to lose them). Implicit destructors are subtle enough as it is, and forcing people to remember that they need to keep their guards alive with a `.used()` or `drop(...)` is just asking for problems, even for `unsafe` code. Changing this would break thought-compatibility with C++ (as in, developers coming from C++ falling back to their existing learned semantics would be writing incorrect code), which is a fairly significant downside for `unsafe` code in particular in my mind. However, I think one possibility for retrofitting if it was desirable, is that Rust could allow eager drops (either by annotation or by default, with an annotation for non-eager) and have non-eager drops be "infectious", a little similar to `Send` and `Sync`.
In general they are slower. See the benchmark of rpds vs the standard library [here](https://gist.github.com/orium/242d64075534b9d6f7ef70d88cf9cf20).
When I say "compatibility" I usually mean "langauge compatibility". Making the compatible/incompatible distinction only for dependencies seems reductive to me, although it's good for marketing.
I've said it somewhere else, but there's "what changes do you want to make" collection threads on internals. That's a big shift to me from "last resort". That's more "open barndoor for changes". I was expecting that when something comes up that requires a breaking change, it is postponed until the next edition. Now it seems before editions we'll have a big search for what we can change. Just because we *can* doesn't mean we *should*. I've seen quite an increase in "we can break this because we have epochs/editions" and "breakage isn't bad because `rustfix`" recently. I also quote from one of the links you posted: &gt; I agree with @sgrif's comment strongly. There seems to be a rush to block things for edition 2018 to allow RFCs to be written on top of these that might themselves not succeed. I also see a high chance that until is used as a variable name in projects. That's exactly my issue. "Editions should only be for when we have no other choice" should be more pushed.
`s` is allocated on the stack, and the stack is always contiguous, so it does not make sense to free `s`.
&gt; To be fair I don't think I'm being unfair. &gt; error handling is not moving towards exceptions. I said "looking like exceptions". &gt; And while yes, the focus on explicitness has waned since 1.0, I think there's still a strong sense of keeping rust more explicit than other languages. I don't think current features are going to change much from the way they currently are. I'm more worried about new features taking shortcuts. I was sure of that once too, but after going through the match ergonomics, modules and path RFCs I'm no longer sure. Post 1.0 I expected the Rust community to be the last place where I constantly have to explain what I mean with "explicit over implicit". &gt; I'm not well-versed on editions and such, but I was under the impression that they were intended to fill the need of long-term-support? As in, you'd have support in tooling for an edition and a previous one, while rust can happily continue working on hammering out new features? As far as I understand it, it's just designating a particular version as special compared to other ones. Other languages have the same kind of thing - c++11/14/17, for example. How often does C/C++ remove features? I know in Perl it doesn't happen often.
I mean Redox is an open source project so you can contact the mentors on Github, and then do the work seperate from the RSoC programme and contribute it as a PR, which would lengthen the timeframe available.
Hey, shingtaklam1324, just a quick heads-up: **seperate** is actually spelled **separate**. You can remember it by **-par- in the middle**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
I have the exact same issue. The .rustup directory takes up 1.5GB and the .cargo directory 500MB on the C: drive, and every GB counts when you only have 80. I'd try symlinks but I'm afraid of breaking stuff.
FWIW, what *you* usually mean by a word isn't so relevant when talking to a larger community: what the community thinks it means matters more. "crates [...] incompatible with each other" *sounds* exactly like it's talking about a non-source-level "compatibility" like semver or compiling with different versions of the compiler. But, this is all irrelevant to the actual discussion at hand, it's just justifying why your comment read as if it was highly incorrect and so got downvoted. :)
I'm slowly running out of reasons to use things that aren't Spacemacs. This may be the final nail in the coffin... thanks for the heads up!
You made a refactoring video without saying "clippy" once :O Kidding, kidding, I actually found it quite entertaining! It's nice to see you focus on a really important aspects of code reviews: Suggesting actual implementation improvements that help prevent errors in the future (in contrast to commenting on style issues)! I too am a [huge fan](https://killercup.github.io/rustfest-idiomatic-libs/index.html#/5/1) of the "unrepresentable illegal states" idea. I'm probably not the target audience, so take the following with a grain of salt: Please add a whole bunch of links to all the topics you only scratch the surface of in this video. For example: You mention autoconf (because it came up in an error message), but I assume people not used to C build systems have no clue what that is. Link to relevant chapters in TRPL might make concepts like the module system (why the hell isn't everything in a `lib.rs`) and `cargo test` less surprising :)
As the others have said, you need `iter_mut()`. What you are currently doing is this. let x = 5; let mut y = &amp;x; Which is to say that `mut y` can change the reference it is pointing to but it can't change anything through its reference since it is a shared reference (ignoring internal mutablility) instead of a mutable reference. What you really want instead is let mut x = 5; let y = &amp;mut x;
The [SimpleJIT demo](https://github.com/sunfishcode/simplejit-demo) built on Cretonne can now run a variety of examples written in a toy language, so next I'm working on documentation, tests, and ultimately, a blog post.
To start somewhere: Creating a `&amp;mut T` from a `&amp;T` is unsound, but there are some special structs (`Cell` and `RefCell`) that, given a `&amp;RefCell&lt;T&gt;` allows you to get a `&amp;mut T`, i e to mutate the `T` inside the `RefCell`. Now suppose something like the following. It's probably a simplification, but as a mental model: struct File { position: Cell&lt;u64&gt;, } struct Bytes&lt;'a&gt; { file: &amp;'a File } impl&lt;'a&gt; Iterator for Bytes&lt;'a&gt; { fn next(&amp;mut self) -&gt; Option&lt;u8&gt; { let pos = self.file.position.get(); pos += 1; self.file.position.set(pos); /* more code to return the right byte */ } } As you see, the file object gets mutated even though you just have a `&amp;File`.
To answer a couple of your questions: * `&amp;f` takes a reference to the file object. Since the `Read`trait is implemented for both `File` and `&amp;File`, we can call `bytes` on it. `bytes` will consume `self` when called, but since `self` is `&amp;f` in this case, we can just create a new one. If you don't call it with `&amp;f` but just `f.bytes`, that will consume your file object. * It's unsound to create a `&amp;mut T` from a `&amp;T` since you're only supposed to have one mutable reference to `T` at a time, and if you have a `&amp;mut T`, you shouldn't have any active `&amp;T`s. If you could transform `&amp;T` into `&amp;mut T`, both of those guarantees could be violated. * The value returned from `f.bytes()` is not just mutable, but owned. That means Rust can obtain a `&amp;mut` or `&amp;` from it, so long as that doesn't result in a violation of the borrowing rules. * `Bytes::find` (and more generally `Iterator::find`) takes a mutable reference since it doesn't necessarily consume the entire Iterator. You can call `find`, then call `find` on the same iterator again to find the next value that satisfies the predicate. By contrast, `Bytes::max` (`Iterator::max`) takes ownership of the iterator since it needs to examine its entire length. There's nothing useful to do with a depleted iterator, so it takes it off your hands, lest you think it might still be useful.
&gt; - What does `(&amp;f)` actually do? Immutably borrows `f`. The parens are to ensure `bytes` is called on this borrow. Without them, it would be parsed as `&amp;(f.bytes().next())` instead. &gt; Why is it unsound to create a `&amp;mut T` from a `&amp;T`? Because `&amp;` means "no one can modify this", and the compiler generates code based on that assumption. The only exception to this rule are pointers to cell types (like `Cell`, `RefCell`, `Mutex`, *etc.*). &gt; I was under the impression that it would create a temporary immutable reference to an immutable file. It creates an immutable borrow to an immutable `File`. The actual operating-system "file" concept doesn't have any special mutability guarantees, unless it's been opened with appropriate flags, which this hasn't. &gt; Now I'm wondering if this creates a copy of the file object while reusing the same file descriptor. No, it doesn't. Rust *never* does anything like this automatically, specifically because it would be confusing. &gt; Is there anything that I can do to prevent the compiler to allow this code to compile? There's nothing wrong here. Rust has *zero* control over the mutability of the underlying file. The reason you can do apparently mutating actions on an immutable `File` is because a `File` is just a thin wrapper around an OS-level file handle. Nothing you do to a Rust `File` changes this handle. It changes structures internal to the OS, but that's completely beyond Rust's control and knowledge. *Even if* Rust required mutable access to a `File`, there's still nothing it could do about a completely different process coming along and changing the file under you while you're reading it. Artificially enforcing immutability here would be completely pointless. So, instead, `File` allows you to do mutating actions through a `&amp;File`, because it has no reason to stop you. &gt; - Down the rabbit hole: [..] I'm not sure what you're asking about with these.
It's slow for me.
That does seem to match some of the crashes I've seen and my project does have a lib and a binary...
As a little more background, `Cell` and `RefCell` are built on top of `UnsafeCell`. The compiler knows about this type, and that it changes the assumptions it should make about aliasing. It's not possible to re-implement such behavior soundly without using `UnsafeCell`.
RLS seems to just become useless inside macros, both at the definition site and inside usage sites. You can't hover to get type info, and I don't think autocompletions typically work in there either. Apart from macro support -- a core language feature -- RLS seems to work pretty well these days. On large projects that take 30 seconds to do an incremental build, RLS can be painfully slow too, of course.
Yea I've noticed some oddities too after updating to the just released new version of Idea, like sometimes the inline type annotations don't show up regardless of what I do.
As for: &gt; Are there rules for any of this? Yes. The problem is, using `File` as an example to learn about mutability and ownership is a bit unfortunate, because of the things others have already posted. Basically, the OS allows you to create multiple handles to the same file, so Rust doesn't pretend to be able to prevent that, and implements all I/O operations on shared borrows. So let's consider your other questions not in the context of a file, but e.g. a vector and an iterator over that vector. As for the questions about the iterator: in a method, what's mutable when calling a `&amp;mut self` method is always the thing you're calling it on. This means that a shared-borrow iterator over a vector (what `vec.iter()` gives you) needs to be mutable - this doesn't allow to change the data in the vector, but to *advance* the iterator, i.e. to change the state of the iterator. (Basically, the iterator is a struct that has an immutable reference to the vector and an index. To advance, the index is incremented.) So basically any iterator method, like `next` or `find`, needs to *at least* mutate the iterator. But there's also methods that will consume the whole iterator, like `max`. These take `self` (not `&amp;self`!), i.e. ownership. You don't have the iterator anymore afterwards. Transferring ownership *always* means that the new owner has full control. There is no distinction, on the calling side, between `self` and `mut self` - it's the same function signature. To understand the last sentence, here is how the `self` arguments would be desugared into "ordinary" looking arguments, if `self` wasn't a special identifier: fn foo(&amp;self, ...) becomes fn foo(self: &amp;Self, ...) fn foo(&amp;mut self, ...) becomes fn foo(self: &amp;mut Self, ...) fn foo(self, ...) becomes fn foo(self: Self, ...) fn foo(mut self, ...) becomes fn foo(mut self: Self, ...) I.e. the `mut` in `mut self` is on the *pattern* side, not the *type* side.
That's what nightly is...
Awesome update, though I am interested whether you've seen the feather format, sounds very similar to the goals of quiver? https://github.com/wesm/feather
Cross-posting for visibility. Is anyone aware of issues with the windows installer with rustup.rs?
I had Atom working fine without issues on stable toolchain today with the `ide-rust` package. Errors worked. I believe I had to do one compile of the project first before it started to be properly useful. I did have to go into the package settings to set the stable toolchain as default and add the components via rustup that are mentioned for RLS install docs.
I haven't tried that lately but I found it kept missing completions that I wanted.. I'd like an option for it to show every known method after the "."(with the known valid completions sorted first &amp; emphasized, possibly the catch all in a sub-menu) - and sometimes in Rust you know what function you want but you dont have the trait yet.. I suspect 2-way inference also complicates completion
Ah, okay, when it is `'static` by default, then everything makes sense. Problem pretty much solved. Now I am having other problems regarding a slice of a slice of a slice and even weird println! behaviour with these... But this is something completely different and I am still trying to figure it out on my own. Thank you very much!
Do you see any actual use cases for this? Genuine question.
There is an enormous amount of mostly-redundant work that has already been done in this space, so I would caution you before creating Yet Another Data Format (something that I thought Parquet and Arrow were supposed to solve [once and for all](https://xkcd.com/927/)). For example, it seems likely that Feather will be [deprecated in favour of Arrow](http://wesmckinney.com/blog/feather-arrow-future/) in the near future. If you want fast, zero-copy (?) data-interchange in Rust, maybe check out [Abomination](https://github.com/frankmcsherry/abomonation) - and for that matter, everything else by frankmcsherry.
To add to your point, tuples are for situations where using a struct would be too verbose (AFAIK they're literally structs underneath). If you want to iterate over it, then you probably want the data to be stored in some collection.
&gt; What does (&amp;f) actually do? It creates an immutable pointer to `f` (aka &amp;f "borrows" f). &gt; Why is it unsound to create a &amp;mut T from a &amp;T? It would allow code like this to compile: ``` fn main(){ let mut i = 8; tmp_immutable(&amp;i); } fn tmp_immutable(value : &amp;u32) { // `tmp_mutable` would change `value` to 10 if this code compiled tmp_mutable(value) } fn tmp_mutable(value : &amp;mut u32) { (*value) = 10 } ``` &gt; Why is the returned (f.bytes()) Bytes object mutable - we're calling `next(&amp;mut self)`? [Read::bytes()](https://doc.rust-lang.org/std/io/trait.Read.html#method.bytes) returns the [Bytes iterator](https://doc.rust-lang.org/std/io/struct.Bytes.html). The iterator is mutable, not `f`. The iterator must be mutable or it couldn't update its state, and thus couldn't advance. &gt; How is it possible that Bytes::find needs mutability and Bytes::max not? `find` borrows a mutable iterator so that you can find multiple times. `max` moves the iterator, forcing you to create a new iterator for the next call. ``` fn main(){ let vs = vec!(1, 2, 3, 4, 5, 6); let mut i = vs.iter(); let four = i.find(|&amp;&amp;v| v == 4 ); let five = i.find(|&amp;&amp;v| v == 5 ); println!("{:?}", four); println!("{:?}", five); let mut i = vs.iter(); let five = i.find(|&amp;&amp;v| v == 5 ); let four = i.find(|&amp;&amp;v| v == 4 ); println!("{:?}", four); println!("{:?}", five); let i = vs.iter(); println!("{:?}", i.max()); // println!("{:?}", i.max()); // error: use of move value `i` } ``` &gt; Are there rules for any of this? Yes, but they're largely a consequence of the borrow rules. The best advice I can give you is to experiment and commit [the ownership section of the book](https://doc.rust-lang.org/stable/book/second-edition/ch04-01-what-is-ownership.html) to memory. Related: [Ownership in Rust by Example](https://doc.rust-lang.org/stable/rust-by-example/scope.html). 
I'd be curious on how something like data fusion differs from or could leverage something like differential dataflow or timely dataflow. 
Mapping Parquet to Arrow is far from zero-cost unfortunately so I am wanting a much more efficient way to stream and query Arrow-native data internally even if the source data comes from a Parquet file. I recently became a contributor to Arrow and will be working on IPC support in the Rust library once Google Flatbuffers is available for Rust (sounds like it could be relatively soon now). So it sounds like whatever I do in the short term could eventually be replaced by a later version of Feather, or Arrow IPC, or something else in the Arrow project and whatever I do build before then I am happy to donate to Arrow to help move this along. I will get a discussion going about this on their mailing list too.
it sounds like an interesting topic, but i really don't get why people would prefer stuff like this in video format. programming is done in text - why not just write a fucking article? i'm not going to watch a half hour video just to see if anything catches my interest. write it out in text so i can scan over an outline, see code samples i can copy/paste, let search engines index it, etc. sorry to rant, but i didn't grow up with youtube. am i just too old to get it? or is it this more about youtube ad revenue than anything else?
I would like to make it an alternative to OS-based containers like Docker. I expect things like configuration, deployment, management, load balancing and security to be better than Docker etc. because we have more control to the environment in Ice.
&gt; it sounds like an interesting topic, but i really don't get why people would prefer stuff like this in video format. programming is done in text - why not just write a fucking article? People learn in different ways. That one person uses a video format to teach does not exclude others from using the written format to teach. Let there be a diversity of educational materials, in order to cast the widest possible net. When you come across learning materials that don't suit your style, just move on. You don't need to tell the author to "just write a fucking article."
Really? What about using a tuple for a vector of numbers, like a point in space 
I agree with @burntshushi. I'm the type of learner who gets more from videos than text. Some people are the opposite. It just depends...
Thank you for the answer, I guess it makes sense that `File` is permissive if one considers that e.g. other processes can still change the file and a more strict enforcement would lead to a false sense of security. But I have to say that C\+\+ handles this case more intuitively \(at least I think so at the moment\) \- it is impossible to copy a `stream` and one has to explicitly create another object that accesses the same buf`fer (s`tream takes the role`s of` File` and Ite`rator\).
&gt;The value returned from `f.bytes()` is not just mutable, but owned. That means Rust can obtain a `&amp;mut` or `&amp;` from it, so long as that doesn't result in a violation of the borrowing rules. That really helped, thank you!
ohh -- by that you mean your cursor is inside of a macro in a source file and all the IDE features still work as normal. That wasn't a layer I had considered, that in order for IDE features to work you have to simulate what the macro is going to do.
Then you should consider a fixed size array, like a [f32; 3] for a 3d position. 
Honestly... I've done evals of Aspera, and as long as your network is reasonably clean, rsync can get amazingly close to Aspera's performance (particularly if you have all the TCP congestion control features turned on). A lot of their value is how they route around network problems.
Thank you for pointing out these "subtil" differences, I will have to think about them a little bit more and get more familiar with them too.
QUIC really is more optimized to reduce round trips than improve on massive file transfers.
Don't worry, it turns out to be pretty logical once you get more familiar with it :)
Great example! Thank you.
I disagree. This is a comparatively simple problem of transferring data over a network efficiently. This feels like a "square peg, round whole" problem. Usually for data transfers of big data you are using things like HDFS to transfer files between distributed clusters. At the very least, you use rsync, not FTP which really has the most minimalist implementation possible in terms of network protocol.
Seriously... just don't use FTP... and for data that is large, use distributed filesystems (and their related tools). You'll likely outperform Aspera transferring data from one node to another.
It's a bug. https://github.com/rust-lang/rust/issues/28728
Thank you for these examples, they helped quite a bit.
Wow, thanks! I found a previous issue (https://github.com/rust-lang/rust/issues/18785) that also illustrates the source of the `0` value in the first program. Hoping the awaited LLVM patch lands soon; this issue is a little unsettling.
Directly to the OP--the question is whether there is any way the data behind the texture pointer can be modified concurrently *without* synchronizing with your mutex (in a "transitive" sense--that is, this also applies to any data reachable through the SDL_Texture by pointer chasing). If the answer is no, the abstraction is thread safe, and you can probably derive `Send` and `Sync`. If the answer is yes, then it isn't. It's something you have to decide on a case by case basis, sometimes by going through the original C source code.
First half about possible mutation through shared references: true, a potential concern. Second part about lifetime not being trackable: not so much?, there's an associated lifetime right there on the Texture type. Since sdl textures are associated with drawing contexts, that's how the crate keeps you on track.
If this would be a problem, it can be relicensed.
What layer do you use for rls for rust in spacemacs?
I love what you're doing, but...not sure how I'd get started with helping out.
Same question over here. Cross-platform plugins in Rust... Sounds exciting! Nice job! :-) 
Seems to directly correlate to the size of the project for me. It's fast at a few hundred lines of code, but gets snail slow at a few thousand. I also wonder if number of dependencies factors into that, although I feel like caching dependency analysis should be easy.
But I was told 2017 was the year of usability and rls is in preview mode. 
Until it breaks again.....
Ah- that should be part of it. Mostly I mean that if a structure is created within a macro, using the IDE features _outside of that macro_ should still be able to autocomplete that struct's name, and know that it exists (despite it only ever being created by a macro).
Again, I would not characterize it that way. It's true that a raw pointer doesn't get bound to the data it points to right away, but it's very easy to wrap it in a tagging struct that will link it all up properly. However, even if you do you set that up right and in a way rust understands, simply going across a thread bound is a fresh level of danger just because you no longer know if the other thread's pointer is alive or dead from your own thread. The only exception to this is if your thread joins on the other thread, in which case anything after the join is safe again because you know for sure that the other thread went away.
Or an array if it's fixed size.
Of course you can! It should work on every platform LLVM (or Cretonne in the future) supports.
Renderer API (of which SDL_Texture is a part) is [explicitly said in the docs](https://wiki.libsdl.org/CategoryRender) not to be thread-safe.
No, at that time there was nothing working OOB. Recently there were quite a few improvements on the embedded side. Maybe it’s possible to use Rust as it to code for k20 but I didn’t have time to try it. Check out the latest embedded wig newsletter. Also this: https://users.rust-lang.org/t/psa-you-no-longer-need-xargo-to-do-arm-cortex-m-development/16703
Based on the fact that it takes `&amp;mut self`, your function must be defined within an `impl` block. Can you post that block using e.g. https://play.rust-lang.org? It could be that `'a`, `'b`, or both are bound at the impl level instead of the method.
Yeah idk if it is, but someone should check. Idk what the policy is.
I actually just use the default [rust layer](https://github.com/syl20bnr/spacemacs/tree/master/layers/%2Blang/rust), which is [racer](https://github.com/racer-rust/racer) based. It's pretty much all I need right now, and if I ever feel like its lacking something, /u/simtel20 suggestion seems quite nice, so I might try it out.
Sure, but thread safety and lifetime tracking are entirely orthogonal concerns in Rust (at the type system level). Scoped threads aren't special-cased in the language, they just bound their closures with a lifetime. As long as you're tracking the lifetime properly and your pointers are `Send` (or `Sync`, if you're sharing them), scoped threads will work perfectly well and you won't have to worry about them at all when implementing your type. On the other hand, if you try to send anything with a non-`'static` lifetime (whether or not it's `Send` and `Sync`) to a thread that isn't scoped, Rust won't let you do it. I don't think the reasoning about lifetimes is made *any* harder for raw pointers when you consider concurrency--all the hard work lies in making sure the type is thread safe at all.
Sure, I'm just more focused on getting the technical details right and hoping for feedback and reviewers with suggestions I can act upon.
Not something very important, but I still wanted to share the vscode team showcasing a new feature through the Rust Language Server.
I like the idea of being able to run it on raspberry pi. If this does happen will be able to just run an update command to get all the latest stuff or will refreshing the sd card be necessary every time. Obvolsouly redox is young so there will be breaking changes here and there.
I had a chat with the Arrow folks and sure enough the Arrow IPC mechanism is the way to go and there is a spec already for a file format. It requires flatbuffers though so I'll stick with my proprietary format for now but will then convert to Arrow IPC later on.
Any thought to using capabilities as the model for sandboxing? Or, how do you manage the more fine-grained security aspects? For example, your cpuinfo example looks like you have to give it access to read /etc/passwd in addition to /proc/cpuinfo by virtue of the fact that FileOpenReadOnlyAny permission does not distinguish the path.
Why? Does flatbuffers still not have a rust implementation?
But... if you really really want to, you actually can: http://nercury.github.io/rust/interesting/2015/12/12/typed-arrays.html
My best suggestion here is to not use `.collect()`, just use use the iterator that `.split(' ')` gives you directly. I'm not sure if that's possible in your full version or not though.
This sounds like http://www.zerovm.org/ but based on WebAssembly instead of Googles Native Client.
I have switched to using Pop OS and I really like it. It looks really nice and is a nice OS for programming and development. I'd recommend it.
https://github.com/nebulet/nebulet Let's talk.
Actually what I'm hoping for is the ability to host WASM plugins inside a Rust program. For example - write a game in Rust, and the scripting (level scripts, character behavior, mods...) will be in WASM so you can use any language you want. Plugin scripting is usually done in DSL-made-general-purpose languages like Lua or Javascript - which are usually not that well-designed - or with standalone scripting languages like Python which are designed better but were not created for sandboxing. Or, sometimes, a scripting language created for that plugin system - but implementing something like this is a lot of work and getting it right is even harder. Also, these languages tend to be dynamically typed - so a WASM plugin system can easily outperform them, in addition to being able to use a better constructed statically typed language.
I actually have started on this. I've assigned myself a github issue to track this: https://github.com/datafusion-rs/datafusion-rs/issues/107
Then stick to stable.
Try doing what /u/Lokathor suggests, so something like this: fn process&lt;I&gt;(iter: I) where I: Iterator, I::Item: AsRef&lt;str&gt;, { for line in iter { for word in line.as_ref().split_whitespace() { println!("{:?}", word); } let mut prev = None; for word in line.as_ref().split_whitespace() { if let Some(prev) = prev { println!("{:?}", [prev, word]); } prev = Some(word); } } }
The intention of wording of the standard, of course, is not to allow optimizing infinite loops away, but to allow assuming loops to terminate in order to do loop optimization. Consider: foreach linked list traversal count1++ foreach linked list traversal count2++ Standard allows optimizing above to one loop. foreach linked list traversal count1++ count2++ This is beneficial because traversal overhead is reduced. But this is only allowed because the compiler can assume the first loop to terminate. The first loop in fact can fail to terminate if linked list is circular, and in that case the unoptimized code does not write to count2, but the optimized code does, making the optimization incorrect. Here linked list traversal is stand-in for things compilers are not smart enough to prove to terminate. If I used counting loop, one could argue, "instead of assuming loop termination, compiler should prove it!". But sometimes such things are hard to prove.
I see. thank you :)
Sure, I'm not asking for absolute explicitness everywhere. Just that it should be the default principle that you deviate for a reason. Usually when I say "explicit over implicit" it's understood as that, except with Rust. It's also highly subjective. I'm sure I prefer explicitness in many places more than others. These days for example, I'd often wish `?` wouldn't auto-convert.
You can do this with parity-wasm today, I believe.
What is viscode-oss?
r/rustjerk
Right, so I am not 100% sure that a CDN is necessary, but it seems like that a ~20x increase (which is what we see in the demo) would be really difficult to pull off without some kind of CDN. That being said, if you're seeing your declared network bandwidth being saturated by wget, then a UDP-based multi-threaded protocol will not help and that is a complex problem all by itself, you have to know the state of the network at all the hops between you and your target server. I don't know what that looks like in this case (regularly work with servers connecting US to India, so I mostly think in terms of light-delay but there are tons of other factors). Other people here have pointed out that wget is not a fair thing to compare with, and that its entirely possible that there are tools already available that have comparable protocol performance to Aspera which are generally as simple to use as wget (i.e. a single command-line incantation). On the other side of this is the fact that the specific protocol/algorithms used by Aspera are patented, and IBM is known for a non-trivial degree of litigiousness. All that being said, I am not at all opposed to such an application (the patented protocol is, in my opinion, should not be allowed), I am simply pointing out that beyond tools that already exist, there may be very serious non-technical hurdles that need to be cleared before one gets to work on it.
[parity-wasm](https://crates.io/crates/parity-wasm) looks like just a representation of WASM - I don't think it can actually execute it. wasm-core is using it as a dependency.
\**sits down with his morning coffee*\* Ah, except that doesn't really count. First of all, that isn't iterating, it's visiting, which requires a different structure to the code (plus, it doesn't implement `Iterator`). Secondly, that isn't on a tuple, it's on a `Chain`. Now, a `Chain` *uses* tuples, but if you've already got a tuple, that doesn't really help you. \**pushes a steaming cup of rehydrated instant coffee across the table*\* Nice try, though. :J
I read it as saying that each program's 32-bit address space is mapped into a different section of the 64-bit address space. Not all of those virtual addresses would be necessarily mapped to physical memory, so if the WASM managed to break the rules and address memory it hasn't allocated, that memory address wouldn't have a physical location attached to it. Something like a page fault would be returned, instead of data.
I have seen two extensions that do RLS support for VS Code but I'm not sure which one to use. Suggestions?
Interesting. I'd *love* to see your comprehensive table of the like/dislike ratio of every RFC which I'm sure you prepared in order to determine the "Most-Disliked" RFC. As an aside: I really don't get why people are worked up about this one. If you don't like it, you don't have to use it. I'm a non-fan of `rustfmt`, but I solve that by just not using it. I'm a fan of block comments, but if people don't want to use them, that's fine.
[wasmi](https://github.com/paritytech/wasmi) is the interpreter that accompanies parity-wasm; it was split out into it's own repo.
The Rust IRC channel is available at: mozilla.org:6666#rust. There you can talk about Rust programming anytime. Or if you meant the game, it's over at /r/playrust.
Thanks! That was what I was thinking since it had ties to the official team but I thought it might have been more bare bones or something 
why did you post this here? i **REFUSE** to believe someone is actually too braindead to realize this isn't /r/playrust. furthermore, it's even **MORE** astonishing that you can't figure out how to voip ingame. i'm just sitting here in awe at what i'm witnessing. there's no hope for you lmao, hope you find your way into the medicine cabinet asap.
I finally got a bit of a handle on the wasm speed/size test. It isn't super enlightening but it was fun to make. https://wiredforge.com/fun/wasm-ser/index.html
Well, fat pointers are *required* for Rust's trait system to work at all, so... whether they're faster or slower is (whilst nice to know), a little irrelevant. Either way, having some kind of "thin pointer" would be nice, since it would let you represent collections of trait objects more compactly.
Can't you trivially make a thin pointer as a pointer to a fat pointer? I mean, you double the number of indirections, but…
You don't just double the indirections, you also have to add a heap allocation. Also, "thin pointer" doesn't *just* mean "one pointer wide". To do it, you need to add some kind of RTTI to the thing being pointed to, which also opens the door to dynamic casting; something you can't do easily with fat pointers, and something `Box::new(fat_pointer)` doesn't do.
I'm not sure how to think about the second thing (I don't see why RTTI, or in general reflective capability, needs to be here explicitly; just do the `Typeable`/`Data` thing that GHC has been doing for years), but why do you need a heap allocation? Can't you just take a perfectly safe reference to your existing reference? You'd end up with a type like `&amp;&amp;dyn MyTrait` or `&amp;mut &amp;mut dyn MyTrait`, both of which are exactly as usable as the single-indirection varieties.
Well I can't speak for anyone but myself, but I _am_ unhappy about this one, and I can tell you why. It's because, sure, it's optional to use, and I most certainly will not, but it's not optional to _know about it_. Sooner or later you will find code that uses this, and wonder what on Earth does this mean? Does this introduce some subtle meaning to this code that I'm not aware of? Rust, as much as I love it, is the sort of language where you really need to understand _exactly_ what is written, and it's only reasonable to assume that it does mean something, that there is some gotcha there that you're missing. If it didn't change anything, then why put it there in the first place, right? And so you will end up wasting your time to figure out that in fact no, it doesn't mean anything, it's pure decoration. So basically this just added a little extra burden to every user of the language, for the sake of a minority who wanted this for whatever reason. I just wish I had known about it in time to write this on that thread. Now that it's too late, I just hope that it is documented eventually, in a way that searching for it is easy so people at least don't have to waste _too much_ time on this. Off the top of my head, I'm not even sure how I would have looked for this, had I not found out here in Reddit... probably in [this page](https://doc.rust-lang.org/1.2.0/book/patterns.html#multiple-patterns), which does NOT mention it at all yet.
It is indeed using SDL, if I remember correctly. Does this make it a bad example of a cross-platform rust game? I don't *think* so.
I think all you said is correct... A similar choice is with the memory allocation API. The client is in charge of keeping track of the size of the allocated memory, and passing it to unalloc. This is very different from C++, where `delete[] somepointer` will look for the length of the allocated area on the heap somewhere somepointer. As a result, you get less cache misses when destructing a `Vec` for instance. (This is not true for `String` however because of Rust String destructor)
&gt; It's because, sure, it's optional to use, and I most certainly will not, but it's not optional to know about it. There are so *many* things in Rust you need to know about. This is about the same level of complexity as trailing commas. I agree with you in terms of direction, but not magnitude. &gt; [..] for the sake of a minority who wanted this for whatever reason. The "whatever reason" is code that's easier to read. It's not like this is a completely arbitrary thing where someone woke up one morning and deliberately decided to try and make the world a little bit worse. 
Ah whoops, I messed that up. Yes. That's much better.
Ah, I see. I was thinking we were just making trait objects on the stack, and wasn't thinking about longer-term use cases. Sorry for misunderstanding! As for `Typeable` and `Data`: `Typeable` is essentially the same as Rust's existing `Any` trait, with better integration. It's possible that `Any` could be minimally reworked, or just compiler\hyp magicked, to operate the way I'm envisioning (usable like a marker trait is alongside arbitrary other traits). I suppose the `mopa` crate gets us halfway there anyway. `Data`, on the other hand, is the similarly typeclassed ability to introspect the algebraic structure of a datatype. I think `frunk` has a derivable implementation of the concept under the name `Generic`? That, too, is unusable with other traits, but maybe someday…
It... is technically possible to make libraries without cargo. However, you'll need to manually compile each library with `rustc` and specific linking arts manually as well. It won't be possible to integrate non-Cargo 'library' files into a regular Cargo project without writing a ton of code to build them and link them in via a build script. In general, it's expected that Cargo is used. Everything is *possible* to do using rustc, but it will be much harder if you're doing it manually. Personally I'd just stick a template `Cargo.toml` next to each of those files and just use that. `cargo new` is a useful command too which will create a new directory, minimal Cargo.toml, and blank `lib.rs` given a crate name. If you're making a lot of different programs which use the same kind of functionality, you might also want to try a library crate paired with multiple binaries. If multiple files are put into `src/bin/`, each will be compiled into a separate program, but all can depend on the crate functions exported from `src/lib.rs` via `extern crate xxk;`
*Use Cargo.* Pretty much the entire ecosystem is based around you using Cargo. Things are not added to the standard library on the basis of "people use Cargo." The compiler's command-line interface is optimised based on the assumption that you are using Cargo. Many Rust tools are designed on the assumption that you are using Cargo. Avoiding Cargo is not going to make your life easier, it is going to make it significantly harder. With the pontification done; the compiler doesn't compile directories, it compiles a single, root source file. You give it `lib.rs`, not the directory a Cargo package would be in. The compiler doesn't do any of the stuff Cargo does. The best way to find out how to do something with `rustc` manually is *to use Cargo* with the `-v` switch to get it to dump out the commands it's feeding to `rustc`. Or you could just drop the source files into the crate where you're using them and avoid the separate compilation step entirely.
It's consistent with F#, so it's one _less_ thing I'll need to remember.
Building tantivy to web assembly.
Thanks! This is exactly what I wanted. I can create all files in src/bin/ and put helper functions in src/lib.rs as you said.
&gt;* What does `(&amp;f)` *actually* do? Two things: - Computes the address of the local variable `f` (same as C) - Instructs the compiler to perform static analysis to prove the local path `f` is in the "shared borrow" state wherever the generated pointer is reachable The only primitive operations allowed on a shared-borrow local variable are copying the value (only if the type is `Copy`) and taking another shared borrow. &gt; * Why is it unsound to create a `&amp;mut T` from a `&amp;T`? (Note: the Rust aliasing model isn't officially specified yet. This answer speculates on what the exact rules may be based on how the compiler currently works.) If the program specifies two reads from the same location and the compiler knows that there is no way to write to that location between them, it may keep that value in a CPU register. (It can also use this information to identify subexpressions inside a loop that don't change. These only need to be evaluated *once*, not every time the loop iterates.) One way the compiler could do this is by assigning a "may-alias-unsafely" attribute to some memory operations. MAU writes can intersect with MAU reads, so the MAU reads can't be optimized. The opposite would be "safe-aliasing-only` (SAO), meaning that writes may intersect with reads or writes only if they are reached by following the same pointers. Access to local variables and through references is SAO unless the bytes are contained inside the `UnsafeCell` wrapper type. (This is a special type the compiler knows about, like `Iterator` is special because it works with `for`.) If you have a`&amp;` reference other copies of that reference may exist in other variables, each with an independent zone of allowable aliasing. As long as all the accesses are reads this doesn't matter. But a write in one zone will fail to force the other zones to re-read the modified bytes. The same memory address will seem to have different values at different points in the optimized program. The bottom line is that an active `&amp;mut` reference can't share with anything else without opening the door to arbitrarily bad glitches resulting from optimization. (MAU and SAO don't correspond exactly to llvm concepts. Function arguments are MAU by default, SAO corresponds to llvm `noalias`.) &gt; * I was under the impression that it would create a temporary immutable reference to an immutable file. Yes, that's correct. &gt; * Now I'm wondering if this creates a copy of the file object while reusing the same file descriptor. `File` is just a wrapper around the file descriptor, which is just a number. &gt; * But that is everything but "thread\-safe" and intuitive... \- will it be fixed? Well it is *minimally* threadsafe. The operating system guarantees that system calls targeting the same FD from different threads will be interleaved arbitrarily. They can't cause a data race and all threads will observe the I/O calls happening in the same globally consistent order. This is enough to justify declaring the methods safe. * Is there anything that I can do to prevent the compiler to allow this code to compile? Yup. The odd behavior happens because the methods of `File` which modify external state take `&amp;self`. If you wrap `File` in your own type and require a `&amp;mut self` argument to do things to it, you won't see that strange behavior. The standard library allows you to do that strange stuff by implementing `Read` for `&amp;File` instead of for `File` or `&amp;mut File`. * Are there similar pitfalls that I should know / learn about? I/O and inter-thread communication (`std::sync`) exhibit similar "racy communication" behavior just by the nature of what they are. The atomic primitives are *even stranger*, really pushing the limits of whether "safe" behavior must be intuitive. `Cell` does sorta similar stuff but constrained to one thread at a time. &gt; * Why is the returned \(`f.bytes()`\) Bytes object mutable \- we're calling` next(&amp;mut self`\)? Because when you call `bytes` on `File` it actually consumes `&amp;File`. Since `&amp;File` is `Copy`, a new copy of the *shared reference* may be freely created. &gt; * How is it possible that `Bytes::find` needs mutability and `Bytes::max` not? `find` requires mutability and `max` goes a step further and uses up (consumes) the `Bytes` value. &gt; * Are there rules for any of this? Yes, they're *exactly the same rules*, but traits implemented for reference types (`Read for &amp;File` etc.) are *just strange enough* to be extremely puzzling for newbies. Just wait until you start trying to figure out `Clone` and `Borrow` with reference types.
Read “RLS” as an abbreviation for “Rocket Leage” and I was thoroughly confused. I don’t even program. I followed this sub like 6 months ago thinking it was r/playrust and don’t wanna unsub lol 
I wanted to when I was younger. I took my first programming class last year and finished with a 48. I decided it probably wasn’t my best career path after that, and I’m majoring in graphic design now. If I struggled that hard I figured someone a lot better would get the job 
Yes, there are many things in Rust that you need to know about. You'd think devs wouldn't add even more without a good, objective reason. And I don't buy "code that's easier to read" here, because I don't think this helps anything toward that end. I don't find match foo { | A | B =&gt; (), | C | D =&gt; (), } to be any more readable than match foo { A | B =&gt; (), C | D =&gt; () } In fact, the former is _less_ readable to me. First because, again, I have to do a double take to recognize the `X|Y` pattern when written this way. Had I not seen this Reddit post, I can assure you that encountering it in the wild for the first time would have made me wonder if this is some new construction that I wasn't aware of, because it looks so different, and it would have sent me to Google to figure out what it means. And second, because, at first glance, all four `A`, `B`, `C`, `D` patterns aligned this way seem to be part of the same multiple match. Of course, on further inspection one notices that the `=&gt;` terminates each multiple pattern, it really says `A|B`, then `C|D`... but this is the _very definition_ of "code that is hard to read". This is the reason why, after learning I can do this, I will avoid it like the dickens anyway. Now this is completely subjective, of course, it might well be that you actually find the former more readable (in your sick, twisted mind :P). And more power to you. But that's my point: this adds a real burden to every user for the sake of a subjective view held only by some -- a minority, from the looks of it. There's no objective reason for this, there's no "right" and "wrong" answer to the question of "which one is more readable", yet we're going with the one that causes cognitive overhead for everyone. So, yes, I think this is objectively a bad design choice for the language. And yes, it's exactly the same as trailing commas, another subjective matter that doesn't have a "right" and "wrong" choice, yet adds real objective burden for everyone. Just Google "rust trailing commas" to see how much time people have wasted because of that decision, to the point of writing whole articles about it. Or ask any macro writer what an ongoing bloody headache it is :S
When you say they're required for Rust trait system, is that because we don't want to add RTTI? Or are there other factors in play? (perhaps dynamic linking, I'm not sure how Rust implements it but I imagine it might be a complication)
Fascinating, I didn't know that! Are there more cases like this where Rust trades off memory for speed?
Well, how else are you going to be able to implement traits for types in external crates? You can't add any sort of vtable pointer or RTTI to the type, because the type is already compiled.
Oh, look, a strawman. Let's fix that: match foo { | A_CONSTANT | BUT_LONGER_THAN_1 =&gt; (), | CHARACTER_PER_ENTRY | D =&gt; (), } The whole point is to create a vertical run where the only visual difference is where you transition from the list of cases to the corresponding arm expression. This makes it trivial to visually scan down the left-hand side of the code, particularly in cases where the right-hand side is ragged. The leading bar keeps the runs consistent, and reduces diff noise. Anyway, I'm done arguing with you. If you're not going to even attempt to take the other person's position seriously, there's just no point.
I honestly wasn't great at it in school for a number of reasons. I switched from a CS major to math. Now I work as a programmer and do a ton of it in my free time. I'm loving it. It took a while to really click and I'm not on the same level as a lot of these people. Certainly don't push yourself if you aren't interested but don't be discouraged either. If you ever want to talk about learning resources or what helped things click for me feel free to send me a message some time. Otherwise I hope you enjoy the subreddit for all the exciting growth and enthusiasm this community seems to have.
How is the wasm bincode duration 0.0000ms?
Oh man, I just nerd sniped myself so hard with that question. Here we go! First thought: Java seems to have done it quite well, I wonder if Rust could reuse that, or if Java's approach only really works because it's on a virtual machine... Or, I can imagine where some dynamic linker would re-assemble a new chunk of RTTI, and adjust the type's vtable to point to it. In other words, every object has a header (like a vptr) which points to some class info, which contains a pointer to the newly runtime-compiled RTTI. It would require an extra indirection though. Or, the RTTI could be represented with a linked list! Slow, but that first node would be fast, since it was in the original compile. In fact, future dynamic linkings might be able to just append to the second node, as long as nobody else is using it (but that would take some serious reworking of Rust's compiler, probably). Or, maybe we can use some virtual address magic (using mmap/VirtualAlloc), and say that every classinfo constant should be at the end the memory page and reserve (but not commit!) a few megabytes after it, and then if dynamic linking wants to extend the RTTI, it can commit the reserved memory. We could probably do some tricks to reduce the amount of memory wasted at the beginning of the page (perhaps put other constants there, or use it as a source for memory pools for fast allocation). No perfect solutions, but fun to think about!
&gt; It's only a painful migration now, because people didn't do it correctly from the start, despite people begging them to do so. For the most part yes, but on Windows, `rustup` (in the form of `multirust`) originally did this correctly. It was then *broken* for consistency with Cargo. Requests to change it back were sat on for so long that it was argued it couldn't be fixed because it had been broken for too long. To say this annoyed some of us would be an understatement. :P
This is a very wholesome sub. I’ve twice been told I should learn programming and then had the support from that person in if I wanted to learn sometime. The people here are awesome hahaa. Thank you!! This is deff a place I’d start asking for help
Java's approach requires every object to include a pointer to its vtable. A language like rust that compiles ahead-of-time to machine code _could_ do this—nothing about it requires a VM—but it would flagrantly violate Rust's philosophy of not paying for what you don't use. You shouldn't have to incur the runtime overhead of a vtable except when you're actually taking advantage of dynamic dispatch.
It is not exactly "trading memory for speed". The info needs to be somewhere right? The big difference is that it is likely to use more memory on the stack and less on the heap.
That was in essence what i meant: Is there something in the standard library, that could solve this? If not, could an abstraction be found that is general enough to be useful for other cases but still not slower?
If the reason you're collecting into a vector is to be able to use `.windows()`, then you can use the iterator directly along with the `.tuple_windows()` iterator from the [`itertools`](https://docs.rs/itertools/0.7.8/itertools/) crate. [Your playground converted to use itertools](https://play.rust-lang.org/?gist=f7ed1976f8f7894a77282260cbf57cf2&amp;version=stable)
I need to both iterate over each element, as well as 2-slices and 3-slices, so that won’t work.
In real code I’m not simply splitting white space, I’m doing more complicated things that I’d rather not do twice. After that I need to both iterate over each element of the result, as well as 2-slices and 3-slices, so this won’t really work.
And just in case you haven't come across it yet, [there is a Cargo book](https://doc.rust-lang.org/cargo/index.html). I've only been using Rust for about a year now but I've found a use for almost all of Cargo's features. Almost everything it can do is quality.
That's a lot of work for a simple task. Few thoughts: - Rust forces error handling. Yet many users will often time just fail program anyway instead of trying to recover from it. I conclude that Rust is not very prototype friendly. - I/O operations available in the standard are a bit too low level. It's nice that 3rd party crates often times solve the issue. I can argue that Rust can be even more verbose than C++ at times. Especially when it comes to stuff like I/O. It's good that Rust focuses on safety. But I think it should also provide a level of expressiveness for general cases. Recently [I asked a question](https://www.reddit.com/r/cpp_questions/comments/8bp2yv/bounded_quantification_in_c/) in the cpp_question sub. As a modern language Rust could experiment cost free abstractions to offer more expressiveness I believe. tldr; we need more expressiveness
`tuple_windows` will give you up to 4-slices, but if your splitting logic is expensive I guess this doesn't help.
Although it is true that tuples are generally for heterogeneous data, there can be requirement that one may want to apply something to each element in a tuple, for example print them out. IIRC it is possible in C++ to iterate over a tuple of heterogeneous data via decltype(auto) and/or generic lambda. This is an example I saw a while ago: https://gist.github.com/lichray/dd803a8bb3461fc842e5
This is a good RFC. Don't you get it? Everyone can submit their own preferences on syntax as a RFC. Later I will work out a RFC for syntax like this: use std.io.Write. Don't you think it's a lot easier to read?
One thing worth pointing out is that C++ libraries that do the same let the user customize exactly how "fat pointers" work. You can have fatter pointers, where you have a pointer to the object, a pointer to the vtable, and one or more pointers to some hot functions, or even a small object optimization where the object is stored inline inside the fat pointer, with heap fallback and/or compilation-error message if that cannot be done.
&gt; First of all, that isn't iterating, it's visiting Potatoe potato :P One cool thing C++ lambdas can do that Rust lambdas cannot is be polymorphic and therefore be monomorphized for different types, so in C++ "visiting" is just `("a", 1.0, 4_u32).for_each(|x| println!("{}", x)`. This is still visiting, but for all practical purposes one can think of it as iterating.
Indeed that won’t work for me. `tuple_windows` also clones its elements and mine in real life contain more than just the slices of the string, so it’s more expensive in the end to use.
Hi bruce. Thank you for the comment! You can use the `unwrap` method to avoid all that `?` stuff. That will panic if an error is encountered and lets you go much faster at the start. I.e. `read_line(&amp;mut buffer).unwrap()`; Then your function could just return a String. I agree with you regarding the IO. I was surprised when I had to flush the buffer manually. Thanks for the thoughtful comment. 
Just use F# and you have nothing more to remember.
It also requires Java's awkward split between primitive types and "boxed" variants of those types that are actually `Object`s, if you want to maintain any kind of performance at all.
Ah yes, CStrings are a different thing of course. I wondered because representation-wise `String` shouldn't be more than `Vec&lt;u8&gt;` with UTF-8 guarantee.
&gt; exception I see what you did there.
&gt; to be fair, not having size in the deallocation function is widely seen as a mistake in the C++ community Indeed it was, but was added in C++14.
WASM demo: https://jd91mzm2.github.io/lolcode
My hunch is that fat pointers are more efficient iif you consider the entire program as there are a lot less trait objects flying around in Rust compared to how many types in C++ have virtual methods. Would be interesting to assess though. 
LOL
Store indices instead of slices? Might improve memory consumption and lessen cache misses as well, though it will incur bound checking.
I can't watch at the moment, but I am quite surprised you're dissatisfied with Rust's IO compared to C++, because Rust has plenty of high level abstractions for reading and writing to files, sockets and the like. I will definitely watch later when I get the oppertunity! 
&gt; Because &amp; means "no one can modify this", and the compiler generates code based on that assumption. The only exception to this rule are pointers to cell types (like Cell, RefCell, Mutex, etc.). This rule is better expressed more simply, without an exception: `&amp;mut` means there can be only one "active" pointer to the data (and the compiler generates code on that assumption), and `&amp;` can have multiple active pointers. The Cell and atomic types impose special constraints that make mutation while shared safe. &gt; Just to re-emphasise: what's immutable here is the File value, not the OS's internal structures that track things like where in the file you are. FWIW, this *sounds* reasonable, but the logic doesn't give an accurate mental model: e.g., `&amp;Box&lt;T&gt;` doesn't allow calling `&amp;mut self` methods on the `T`, even though one could think of just the `Box&lt;T&gt;` value itself being immutable (ala `T * const *` in C). The key here is that `File`'s operations are safe when the `File` is shared (and, they're even safe when the file is shared across multiple threads) not immutable, because the operating system API allows it.
Macros destroy IDE support. I with there was a way to generate tests with regular code, like #[test_discovery] fn whatever() { register_test("my_test", || { assert_eq!(1 + 2, 3); }); }
Wow, another detailed answer, thank you very much! &gt; I/O and inter-thread communication (`std::sync`) exhibit similar "racy communication" behavior just by the nature of what they are. The atomic primitives are *even stranger*, really pushing the limits of whether "safe" behavior must be intuitive. I look forward to it, I think that that I'm on the right way to get the the 'memory model' and rules of rust into my head. &gt; Just wait until you start trying to figure out `Clone` and `Borrow` with reference types. to figure out `Clone` and `Borrow` with reference types. Will do, if I'm lucky I'll have written a little bit more code until I need them.
&gt; FWIW, this *sounds* reasonable, but the logic doesn't give an accurate mental model: e.g., `&amp;Box&lt;T&gt;` doesn't allow calling `&amp;mut self` methods on the `T`, even though one could think of just the `Box&lt;T&gt;` value itself being immutable (ala `T * const *` in C). That is exactly where I and my confusion were coming from. But thinking about the fd being in a `cell`/ behaving like one seems to be right.
If you know that the `x` vector (usually) isn't going to be very large, perhaps you can use the [smallvec](https://crates.io/crates/smallvec) crate to use a vector that is allocated on the stack instead of the heap. let mut x = SmallVec::&lt;[&amp;str; 16]&gt;::new(); x.extend(item.as_ref().split(' ')); 
You could take a look at my [https://crates.io/crates/test-case-derive](#[test_case]) macro. You should be able to do something like this: ``` #[test_case(Box::new(Foo::new()) :: "Foo is implementing MyTrait")] #[test_case(Box::new(Bar::new()) :: "Bar is implementing MyTrait")] fn test_my_trait(subject: Box&lt;MyTrait&gt;) { // ... }```
- Macros need not destroy IDE support, RLS already supports autocompletion in macros (often) - there is [RFC #2318](https://github.com/rust-lang/rfcs/pull/2318) about custom test frameworks :)
https://github.com/pop-os/distinst Link to the code, since it's not easy to find from the post.
I wouldn't say it's forced on anyone since this is not recommended. If it was, then I would definitely argue the upvotes/downvotes ratio matters more than in this case. And people won't be using this a lot according to the votes survey, will it? Everyone can definitely draft up some guidelines within their circle outlawing this style. Otherwise, you already have quite a lot of ways formatting `match`, you would probably see a lot of code that is different from your style in the wild.
I wasn't trying to be clever honestly. 
&gt;Would you expect `for` to traverse the fields of a struct? Well, if you're a PHP coder, you might, iirc. :D
&gt; You can remember it by -par- in the middle. How in hell is that a useful tip?
I think some of what you're describing is used in Objective-C's dynamic dispatch mechanism, which /u/pcwalton calls "significantly slower than JavaScript". And requires a runtime (which Rust's trait object don't - they work on bare metal out of the box).
that can not be the reason as this is already archivable with the previous syntax: match foo { A_CONSTANT | BUT_LONGER_THAN_1 =&gt; () , CHARACTER_PER_ENTRY | D =&gt; () } so yeah, the point very much still stands, this adds learning overhead to anybody reading rust code and is not even consistent with most of rusts syntax, which unlike its semantics, does not look very functional.
\&gt; Because if they do a program can access another program \(drivers included\), or you will need a 200 gigs of RAM to run all the drivers, the userspace and the programs. RAM != Virtual Memory You can give processes a 4GB chunk of virtual memory, but only map small amounts of it to actual physical RAM as needed. Most wasm processes will not be using the full 4GB that they could potentially access.
&gt; Either way, having some kind of "thin pointer" would be nice, since it would let you represent collections of trait objects more compactly. I don't know Rust very well, but I've wondered if trait objects could use some generalization, where the vtable is scoped over more than just a single pointer. For example, a well-known trade-off is switching from a parameterized type to a trait object to reduce code size at the cost of more indirection: fn f1&lt;T: Trait&gt;(a: &amp;T, b: &amp;T); fn f2(a: &amp;Trait, b: &amp;Trait); But in `f1`, `a` and `b` must have the same concrete type. In `f2`, they can be different. A more "orthogonal" alternative might be: fn f3&lt;dynamic T: Trait&gt;(a: &amp;T, b: &amp;T); The "dynamic" is not real syntax, but the idea is that `f3` wouldn't be instantiated for every `T`. Instead, the function would accept a single vtable pointer along with two thin pointers for `a` and `b`. For collections of elements with the same concrete type, you could write: fn f4&lt;dynamic T: Trait&gt;(elements: &amp;[&amp;T]);
It's the fully open source, no EULA version of VSCode. Doesn't by default have access to the extensions marketplace but once that's configured most things work fine; the RLS package is one of the few that doesn't.
I think you'd need a different type than just `[&amp;T]` for `f4`, since the slice has to know that its elements all reference a value of the same type (unless you want to verify that at runtime when calling `f4`, which seems to beat the point). Perhaps some type `HomogenousSlice&lt;T&gt;` which checks this at construction time, or statically by e.g. requiring the slice to be instantiated with copies of some (possibly internally mutable) object.
&gt; Also IntelliJ rust ignores rust format files and seems to be impossible to get it to respect two spaces per tab. Isn't it better to configure `rustfmt` with `tab_spaces=2` and keep this settings across tooling and projects? It maybe configured in IDE itself, but shared crosstool formatting setting is the preferable way to expose your settings.
I agree with the general message of your post. I read it as stating that the approach used by default in C++ makes creating and copying pointers cheap, but incurs a cost _when the type supports dynamic dispatch_ (or dynamic sizes), whereas Rust's approach incurs a cost _when the use-site requires dynamic dispatch_ (or dynamic sizes). However, I think one nuance is missing from the way you wrote it, so I'll add it to the discussion. The context is your remark that &gt; For user A, this is far from perfect, since now its pointers are twice as big, The pointers are twice as big (two address lengths instead of one), but if you had instead used a pointer to a vtable per object, each object would have been one address length larger. If each object is pointed to exactly once, the memory cost of using fat pointers is identical^† to the memory cost of using a vtable pointer per object. In many programs, there will be more pointers to heap-allocated polymorphic objects^†† than heap-allocated polymorphic objects themselves, but Rust's tendency to keep aliasing pointers rare, helps here. As far as I see, even in an `Rc`, the fat pointer is inside the `Rc` struct; the references to the `Rc` struct are thin pointers. Hence, the memory cost is identical even then. Of course, non-mutable aliasing references exist. Furthermore, it's reasonable to assume pointers get written more often than vtable pointers, so the extra size does have cost. However that cost is low _in Rust_ because of the ownership model. Overall, _for Rust_, I'd say it was the best choice. † Memory-alignment complicates the discussion, but as far as I see the point still stands. †† Having less pointers than polymorphic objects^††† means some objects are unreachable. ††† For this discussion I use the term "polymorphic objects" to denote entities which can be dynamically dispatched over. E.g. arrays are not "polymorphic objects", since elements in contiguous memory blocks must all be of the same size, precluding general-purpose polymorphism.
Yup, that was the reason. I would love to do a separate show on `Failure` once it reaches 1.0.
&gt; It's run by a language team shepherd who are part of the language team meetings. So not really unassociated. Ah. :( &gt; I also way too often see "why the breakage doesn't matter that much" in RFC texts. Breakage always matters. Sometimes you have no other choice, but "We have to because we have no choice" is different than "We consider the breakage limited". The hardline position of "no breakage outside of critical issues" is certainly one approach to developing software, but *not* taking that view is also a valid approach: a project can also reasonably decide that it's worth small amounts of non-critical breakage to be able to reach its goals. As evidence in support of this, even major, revered-as-stable languages like Java make breaking changes that seem to be non-critical. I agree that breaks should be heavily analysed, but the extreme view ends up with stagnation and messes like Python's `urllib`/`urllib2`, because, barring critical bug fixes, there's always another choice. &gt; Like: Using crates.io as a metric also seems like a bad precedent to me. I would assume a majority of things on crates.io are libraries, but non-publicly available code that people and companies have in-house are probably mostly applications and aren't scannable. So "It's not that much used on crates.io" is a metric that potentially excludes a big part of the community. I think the Rust team has been very good about reaching out to users who aren't able to publish their code so easily: I've personally sat in on meetings with a group of such users a few years ago (I don't know if they continue). As you say, it's definitely hard to reason about the "hidden mass" of closed-source code, but I think it's a reasonable assumption that stable language features are fairly well covered and represented by the thousands of crates on crates.io (they're probably not representative for how the programs cargo and rustc are invoked in corporate environments, though).
📅 2018-03-17 ⏰ 00:53:01 (UTC) &gt;Look I'm not saying I just made wasm files executable on linux, but I just made wasm files executable on linux &gt;-- Michael Gattozzi | マイケル・ガトジイ ([@mgattozzi](https://twitter.com/mgattozzi)) &gt;🔁️ 75 💟 300 📷 [image](https://i.imgur.com/pS7EyMz.jpg) &amp;nbsp; ^(I'm a bot and this action was done automatically)
&gt; What do you people think about this? How do i bribe the rustfmt team into adopting this style? Let me know if you find out, I'm still hoping at some point to make a good case for an option to not remove any separating whitespace :)
Yes I agree. I was only considering this case: &gt; User A heap allocates each object individually and takes many thin pointers to them. because it is the one that makes thin pointers truly shine. As you said, if user A takes only one pointer to each object, then from the memory-consumption point-of-view it doesn't really matter much (maybe slightly due to padding, but I haven't given that much thought).
I think I'd be more comfortable if: * Possible breakage being more front-and-center in the RFC process. Not sure how to do that. Maybe add them as concerns for FCP from the beginning. * RFCs and FCPs being communicated throughout the community more. At least things reaching FCP would be great to see on the Rust forums and here for example, to give the community a chance to see how changes would impact them. About crates.io being representative, I actually agree that it currently probably still is. I'm more worried about way down the road 10 or 20 years from now, which would be up to 7 editions of changes.
There's a missing link for "persistent collection type"… curious where that'd go! Is it the same as an immutable collection?
Yeah, I will try to add more references to the video itself. Was thinking about linking to introductory-talks in-video (for example when I mention `Result` or `Option` for the first time). Somehow the show notes are not visible enough. I will move them to the top of the video description when I find some time. Thanks for the feedback!
&gt; I thought that was clear when I said "a project with 4 crates". If you couldn't use them together at all how do you end up with a project with 4 crates? You could have a project with 4 crates in Epoch 2015, and then switch one to 2018 and everything falls apart, or something along those lines. &gt; It's just that the talk about editions implying full backwards compatibility is starting to feel like borderline dishonesty to me. And people keep saying "it's for marketing purposes" as if that's a universally good thing. Yeah, "marketing" is a word that doesn't have positive connotations to me either. I agree that exactly what compatibility is getting a lost in the message, but I still think the actual compatibility being provided is a very useful one (i.e. strikes a good balance between providing stability and allowing stagnation to be avoided).
It's only "weird" because we're all used to the regular style of code formatting, nothing is *inherently* weird about it. I bet that in a world where people used this style of formatting from the beginning, they'd find the current style "weird". Unfortunately we don't live in that world.
&gt; RFCs and FCPs being communicated throughout the community more. At least things reaching FCP would be great to see on the Rust forums and here for example, to give the community a chance to see how changes would impact them. Yeah, I personally only notice FCPs when I happen to read that section of https://this-week-in-rust.org/ . &gt; I'm more worried about way down the road 10 or 20 years from now, which would be up to 7 editions of changes. Crates.io (if it still exists) will have that 10 to 20 years of history: a crater2038 could not only build the latest versions of a crate, but also look back in history at versions of each crate from each epoch and so get a broad spread of coverage of older and newer code. That said, this doesn't explicitly cover new code being written in old epochs, but I suspect this won't be a major problem.
I agree it's very useful. But I believe communicating a general compatibility sours the advantages it does have. For me it's more about reducing the pressure and impact of breaking backwards compatibility. In the same vein, I think "guarantees thread-safety" on the Rust homepage is a bit too optimistic, given the term "thread-safety" can mean different things to different people. "Memory safe threading" would probably be more accurate and still awesome.
Yes, my comment is entirely in comparison to the languages/typical styles that are used now.
&gt; Yeah, I personally only notice FCPs when I happen to read that section of https://this-week-in-rust.org/ . Yep, same here exactly. If awareness of them would be spread further, I would have a lot more trust in any reached consensus. &gt; Crates.io (if it still exists) will have that 10 to 20 years of history: a crater2038 could not only build the latest versions of a crate, but also look back in history at versions of each crate from each epoch and so get a broad spread of coverage of older and newer code. True, I guess I'm just having too many scars from reading in-house code for the last 20 years, and seeing the difference between open, distributed code and what you find in dark corners of projects that grew organically over decades.
My though is that maybe gtk is not needed as a dependency.
For server communication bincode over websockets would be my choice.
Instead of wasmi cretonne could be used right? I had similar ideas to this but figuring out how to use cretonne is quite a challenge.
Rls should be fuzzed as part of testing and never crash. 
Not quite! Here's [a good summary](https://docs.rs/im/10.2.0/im/#why-immutable-data-structures) in the context of im.rs.
Hello, I'm author of this small program. Started it as an exercise in Rust. Now it seems to be usable, and I need some feedback before issuing 1.0.0 version. Code reviews, pull requests and feature suggestions are welcome :) 
Just a side-note that you probably already know, you can easily make a crate and have your other projects refer to it by a git link or file path instead of having to publish it on crates.io
Probably the biggest highlight for anyone that doesn't want to read the entire thing is the beginning of the integration of Pathfinder to Webrender.
Aren't "thin pointers" really just existential types? You would have a definition like: struct MyObject&lt;T: MyTrait&gt; { obj: T vec_of_objs: Vec&lt;T&gt; } and then you could use "thin" pointers to something of type `MyObject` to implicitly invoke traits methods on its inner obj, or on a vector of objects that all share the same concrete type T. You could even use a very similar trick to implement full OOP inheritance, but that would require declaring methods that take `self` *as an existential*: these methods would then be dispatched properly in derived versions of the object.
Very nice, will defently be haivng a play with that later. Once thing I notice however is that you elected for limiting the number of lines rather then the depth of the tree. It would be a lot more useful to specify the depth of the tree like the way `tree -L 3` works.
Ah, sorry—I was deferring to that documentation. Roughly, to quote myself from a while back: &gt; Sure! Persistent immutable data structures allow you have immutable data structures (like Lists, Tree, Stacks...) with the speed and performance you'd expect from mutable data structures. Typically, you'd expect an immutable data structure to be slow and inefficient, as inserting (for instance...) a node to a million or billion-node structure would require copying over the millions or billions of unchanged nodes to the new structure. That's wasteful! &gt; In contrast, persistent immutable data structures use something called _structural sharing_. Unlike in the previous example where the all of the old structure's nodes are copied over to the new one, persistent immutable data structures diff between the old and the new structure, and only apply the difference. Typically, persistent immutable data structures are backed by tries with a branching factor of 32, which enables the cheap copies and diffs.
Consider this case: struct Foo(usize); trait Bar&lt;T&gt; { fn bar(&amp;self) -&gt; usize; } impl&lt;T&gt; Bar&lt;T&gt; for Foo { fn bar(&amp;self) -&gt; usize { let &amp;Foo(num) = self; num * std::mem::size_of::&lt;T&gt;() } } fn main() { let foo = Foo(10); let bar_i32: &amp;Bar&lt;i32&gt; = &amp;foo; let bar_f64: &amp;Bar&lt;f64&gt; = &amp;foo; println!("{} {}", bar_i32.bar(), bar_f64.bar()); } With the fat objects approach, the vtable for `foo` should have contained `Bar::&lt;i32&gt;::bar` and `Bar::&lt;f64&gt;::bar`. But... why just them? What about `Bar::&lt;bool&gt;::bar`? We may not be using it, but a pointer to `foo` may somewhere along the road - maybe even at some other crate - be legally cast to `&amp;Bar&lt;bool&gt;`. And of course - `&amp;foo` can be cast to any other `&amp;Bar&lt;T&gt;`. Or to any other trait that `impl`emented for `Foo` - even ones defined in new crates. The options are boundless - and so will be the vtable! With fat pointers you just neeed to create a vtable with `Bar::&lt;i32&gt;::bar` when casting to `&amp;Bar&lt;i32&gt;` and another vtable with `Bar::&lt;f64&gt;::bar` when casting to `&amp;Bar&lt;f64&gt;`. Simple, manageable, and lookups are so much easier.
More fine-grained security control will be continuously added! For the fs case, I'm considering to add something like `FileOpenReadOnly(path_pattern)`.
It depends on what you mean by "when". In the abstract, unoptimized program order: &gt; when it reaches the closing bracket But because `&amp;str` is a `Copy` type, `drop` doesn't run and you won't notice if `s` is freed early. So you may assume, when thinking about costs, that it will be freed after the last use. Even if the type needs to be dropped, the optimized program is allowed to look ahead, into the drop method, and execute instructions early. This is the fundamental reason why `Rc` and `Arc` are different - `Arc` restricts out-of-order execution. Any optimizing compiler would also do something called "capture analysis". If successful (and you usually want it to be successful) then it knows the program doesn't care which address is assigned to any particular instance of a local variable. In that case the compiler is free to do really crazy tricks, up to and including *not actually allocating memory* for a local variable. The variable could be simply kept in one or more CPU registers. In this case - `s` is a constant pointer to read-only data - there's no need for stack space at all. The compiler can simply hard-code the address and length into the machine instructions (as "immediate data"). The main reasons why capture analysis would fail are if: - you examine the address of the local variable - `println!("{:p}", &amp;s);` or borrowing and casting the reference to `usize` - you loan the variable to an external function (such as a system call) In those cases your program depends on the local variable actually being present at a fixed location so the compiler has to follow the program literally. When Rust is translated to llvm IR and a local variable will be borrowed the `alloca` instruction is likely required. `alloca` memory ceases to exist at the end of the LLVM function, which might or might not correspond to the end of the Rust function. It's possible that a local variable might exist longer than required, but any program that notices the difference would be not just `unsafe` but Undefined (it works or doesn't depending on the whims of the optimizer).
At the moment, there is no stable update mechanism for the kernel and bootloader, so yes, you would need to flash the SD card again.
this would probably be possible, but wasmi was quite nice to use
Yes, and yes, it takes some work to set it up right now. We're actively working on making it easier, and I'd be happy to help anyone interested in trying it.
Codegen is magic, right up there with water, air, dirt, and magnets.
The way I learned it: suppose you have a data structure that is designed to work with no mutation at all; you would call that a purely functional data structure. Now suppose you decide that that constraint is stricter than you need: you really only care that the data structure presents the same api behavior as a functional data structure, but if it uses mutation behind the scenes that's okay. You would call that a persistent data structure. 
Great post, looking forward to more like it. It's helpful to see a walkthru of an error that I recognize from my own code. 
Nice quality. I really like the pace and the tone of the video.
the idea of persistent immutable escapes me. how can it be immutable is you can mutate it?
To be honest, I've always thought that "immutable" and "persistent" data structures shared the same definition. In fact, looking for "immutable data structure" in Google yields results about persistent data structures primarily.
I was projecting my own ideas without explaining them. I was thinking of a "fullstack rust" type of project. There is potential for a framework that can span the full range of clients (iOS, Android, Web, Desktop) and server. Without js crappiness.
I tried to use this tool on a hello world program, and i see this function near the top: 2248 ┊ 0.98% ┊ &lt;std::net::ip::Ipv6Addr as core::fmt::Display&gt;::fmt::h10e70b689e346ead And I find it very strange, why it wasn't removed by `wasm-gc`? Is some code in std using that function? but it makes no sense for hellowrold.
A good format should make code easy to edit. Code is easier to edit when you can make operations on lines instead of on parts of lines. For example, if I wanted to add a new item to the start of your list I would have to edit two lines: let a = vec! - [ 1 + [ 0 + , 1 , 2 , 3 ] Whereas with the standard Rust style I only need to insert a single line: let a = vec![ + 0, 1, 2, 3, ] The people who designed Rust understand this, which is why Rust allows trailing seperators even in places like function argument lists. An example from the rustfmt guide: fn foo( arg1: i32, arg2: i32, ) -&gt; i32 { ... } This is an especially good style when combined with editors that have good support for linewise operations. In vi, if I want to switch the order of these two arguments, I can just `ddp`. On the other hand, in the style you recommend I would have to do the same thing then replace `(` with `,` on the second line and `,` with `(` on the first, a more time-consuming operation.
Rustuped, got this still when trying to format a file with typos in it. Doing it multiple times, RLS still dies and fails to restart. Error sqiuiggles don't work. thread 'request-worker-2' panicked at 'byte index 18446744073709551615 is out of bounds of ` extern crate actix; exte`[...]', libcore/str/mod.rs:2221:9 note: Run with `RUST_BACKTRACE=1` for a backtrace. thread '&lt;unnamed&gt;' panicked at 'missing key in compiler_jobs', libcore/option.rs:917:5 
Thank you for the comment andradei! :)
I wrote [this](https://github.com/BartMassey/duvis) as a `du` visualizer. It's written in C from before I started in Rust, but could easily be converted I think. It's a replacement for `xdu` with an ASCII mode, and is much much faster than `xdu`.
Nobody said anything about mutating — it's about sharing structure. Think of a linked list containing 99 items — you can cons an item onto the list, and then cons another item onto the same list, and the result is three distinct lists, but 99 items are literally the same memory for all three. The other two lists only add one item's worth of overhead each.
I think it's mainly that nobody wants to use immutable data structures that don't have that property.
It is most likely because of potential panics. The panic infra brings in all the fmt infra, and a wasm binary that does nothing but panic will be ~45K. Luckily, Alex has a PR that drops that down to 350 bytes: https://github.com/rust-lang/rust/pull/49488
Nice! You should look at https://github.com/vmchale/tin-summer and see what they did to increase performance. 
Thank you! I would have written this exact comment myself. Most code is more often read than written and most of the time I read code it's as a diff. So: Please use a style that makes diffs easily understandable. Rustfmt does this.
I think the that in Rust you use traits a lot more than you'd use 'interfaces' in C++, and that makes C++'s vtable approach unsuitable. Think about it: suddenly #[derive(Clone, PartialEq, Debug)] adds three pointers to every instance of a struct! Everyone would be trying to avoid using traits! Rust's approach is far more "pay for it if you use it" 
Okay, fresh approach: [Playground link](https://play.rust-lang.org/?gist=ea9bf8bbb29dc8dc5d1da5f88aef8753&amp;version=stable) I think the issue that you're running into is that the lifetime of a `Vec` declared outside the loop outlives the lifetime of the `item` loop variable. But in reality, we know that the lifetime of the string slices underlying the `item` variable outlive the entire `process()` function, so by making that relationship explicit, we're able to declare a `Vec` with an appropriate lifetime. (Side note to someone with more iterator-foo than I possess -- why does the trait bound require the double reference? I would have thought the item would yield `&amp;str` directly, not `&amp;&amp;str` due to the `into_iter()` call)
 The collection is : The first element , A second one , and maybe a third ! sentences are trees, too
that is right, but maybe only because i did not un-indent the } else, i will edit it in the op!
that's definitely a good point, using vim myself i feel the pain. maybe i should make a rfc allowing leading ```,```? Would not be a great change in grammar i am sure it would be an uncontroversial change, especially seeing that leading ```|``` and ```;``` are already permitted!
Very nice! In the readme you state &gt; dust is currently about 4 times slower than du. Some of the speed issues could be the use of `println!` inside a hot loop. 15 lines of output by default shouldn't make a big difference in performance, but with higher levels of output this could very well become an issue. The reason is `println!` locks `stdout` at every call. It's *much* faster to lock `stdout` once, and just perform `write!` calls on the [`StdoutLock`](https://doc.rust-lang.org/1.15.0/std/io/struct.StdoutLock.html). There is probably also some perf to be gained by using [`std::fs::Metadata`](https://doc.rust-lang.org/1.15.0/std/fs/struct.Metadata.html) directly which could cut down on syscalls and file accessing if you're accessing files/dirs twice instead of just once (I haven't actually looked to see how many you're doing). The final thing I'd look at is the logic of `-n`. If the goal is `-n` lines of output I'd traverse the root node breadth first to fine the largest dir, then traverse those child nodes in sorted order (largest to smallest) in a *depth*-first approach until I reached my `-n` limit. If you're already doing this, forgive me :) You just want to avoid doing all the calculations and file system accesses only to throw all that work away and only display the top fifteen lines (all of which may only be in one child from the root node). What might make more sense from a usability standpoint is to have a "max depth" option, instead of "max lines of output". This would also make the directory walking code easier to have an upper bound and know you won't be "throwing away work"
so it's only immutable in the abstract sense? as in, at the source code level
Will be working on [adding numeric enums to](https://github.com/reproto/reproto/pull/35) to reproto. While doing that, I discovered [a compiler bug](https://github.com/rust-lang/rust/issues/49973), meaning I have to decide whether I can safely use `repr(i32)`/`repr(i64)` on the generated enums or not, and if that can be done in a sound way. If you are interested in working together trying to improve how we use schemas for JSON, please poke me. There's _a lot_ of cool stuff to do.
Leading | was rather controversial, so I'm not sure leading , would go over that well.
I updated [mio_httpc](https://github.com/SergejJurecko/mio_httpc) with some API changes and features. Now CallBuilder can construct URLs in a safe manner. Just accepting URL strings is not very good practice since it leaves the problem of percent encoding URL unsafe characters in the hands of the user. Often that just results in users appending strings together to form an address. // To construct call to: https://www.example.com/a/b?key1=val1 CallBuilder::get() .https() .host("www.example.com") .path_segm("a") .path_segm("b") .query("key1","val1") Moving to URL construction in this manner also lead me to abandon the http crate. I don't think it is a good fit for http clients since requests require quite a few more options. Using multiple builder types to create an HTTP call is ugly.
Sorry for posting so late, I had some network troubles. I'll do TWiR and some mutagen stuff. Also perhaps some bits of Rust docs if I find the time. 
It's actually not just Rust that does this, but any compiler written in its own language. The reasoning goes as follows. (As of this writing, v1.25 was the latest version.) **stage0:** your existing Rust compiler. Let's ignore how you got hold of it; it was probably from your distribution, or a binary downloaded from the website. **stage1:** you compile Rust v1.25 with stage0. At this point, you have a fully-functional compiler that implements Rust 1.25. However, it was built with stage0, which is probably different than v1.25. So to optimize the v1.25 compiler... **stage2:** ...you rebuild v1.25 with the stage1 v1.25 compiler. Now stage2 implements v1.25 of the language, *and has all the latest optimizations*. **stage3:** is actually optional. If you rebuild v1.25 with stage2, it should be literally identical to stage2 itself, or else something has broken. So stage3 is a good sanity check. This process used to be very important when you bought a Unix machine that shipped with a crappy, proprietary C compiler. You would build GCC using the proprietary compiler. Then, because GCC-built-by-GCC is better/faster than GCC-built-by-vendor-compiler, you compiled GCC *again.*
I believe many compiler (gcc, …) do that too. First compile the compiler using host version to produce compiler A, then use A to compile again and get B, and then use B to produce another C and check whether B = C.
This should probably be left to the compiler, there are no real advantages to give you control over shared vtable pointers, but it could be an acceptable optimization.
With the [SimpleJIT demo](https://github.com/sunfishcode/simplejit-demo) using Cretonne up and running (and printing "hello world"!), I'm now going to focus on adding more docs, tests, and ultimately, a blog post to tie it all together.
It's fully immutable. "Adding" a node produces a *new* data structure that links to the old one. 
Cool stuff! I tried to do a `cargo install` but it looks like `dust` was taken by another package. Anything that can be done around this? (Changing the package name is a workaround but I dislike that. I guess this makes another +1 for namespacing in Crates.io.)
Lots of [`uom`](https://github.com/iliekturtles/uom) (type-safe zero-cost dimensional analysis) changes on the go: couple different test improvements are halfway done, ratio (dimensionless) is done in my dev branch, and thermodynamic temperature/temperature interval changes are in another dev branch. Also have a PR from /u/aehmlo under review.
So if you look at say [`HashMap::insert`](https://docs.rs/im/10.2.0/im/hashmap/struct.HashMap.html#method.insert) it doesn't take `&amp;mut self`, it takes `&amp;self` and returns a whole new HashMap, the only thing is that as an implementation detail, underneath it reuses as much of the original HashMap as it can
I honestly didn't consider cargo installing it. I envisaged people downloading the binary or building it themselves. 
so, for example, [`HashMap::insert`](https://docs.rs/im/10.2.0/im/hashmap/struct.HashMap.html#method.insert) will create a new node in memory but will use pointers to existing data instead of copying the entire memory?
What I want to write: const A: [u64; _] = [1, 2, 3]; What I apparently have to write: const A: [u64; 3] = [1, 2, 3]; Any solution short of changing the type of `A`? It's fine in this example, but if you have 112 initializers the counting gets a little tedious and error-prone.
Follow up: what's the basic overview of how you bring a bootstrapped compiler to a new target, where you don't have a pre-existing compiler yet? Is there some sort of cross-compilation, or do you just have to go back to the initial non-bootstrapped implementation and bootstrap all the way through history?
Nice patch, thanks! Yes, I didn't use lto in this case, but lto shouldn't be necessary to remove a dead function. I see two possibilities here. Either linker or wasm-gc fails to remove all dead functions, or there is exist some runtime condition under which panic infra can print Ipv6Addr. but it would be very strange if it did.
That would be great! :)
The Java approach works well because of the VM, though. On hot methods, java is able to take the time to optimize away the vtable look up in most cases (instead adding a quick "is this the right type" check before the optimized code.). I don't know how you could do that, well, with AOT code without introducing some JITyness. The rust approach works well because in many cases, you can completely eliminate the dynamic dispatch anyways. So, no additional vtable lookup is ever actually needed.
Right now, you have to do some kind of cross compilation to get your find stage 0. In the future, you may use the projet mrustc : a limited rust compiler \(without borrow checking\) written in C\+\+. Its goal is not to be an official compiler, just produce a stage 0 usable to compile rustc. You just can't reasonably rebuild the whole history anymore. The last version before self hosting \(in OCaml\) is years away and Rust used to break compatibility nearly every day before 1.0. 
If you are compiling with multiple codegen units, not using LTO absolutely *can* miss removing dead functions.
The details vary from compiler to compiler, but cross-compilation is involved in some sense. The two main approaches are: 1. Add target support on an existing host and then cross-compile. (This is typically how it's done with C compilers.) 2. Add at least limited support for compiling to C, then compile the result on a C compiler that's already on the target platform. (This is how GHC (the Haskell compiler) does it.)
I think that out of all of them, \`rlua\` is the only one that is used professionally. Also looking through the code, it looks really well written.
I don't think that's quite how it would work, since the only time fat pointers/vtables come into play is with trait objects rather than when you're using traits normally.
Slick. Yet another reason to help push me over to neovim.
In an ideal world, sure. In practice system package manager is often way behind. In this case making it cargo installable is a quick way to evaluate the program. One has to assume that beta testers are busy people 
Is this specific to wasm backend? Because on x86 it's not the case. You can end up with more functions overall when using multiple cg units, because there will be less inlining, but --gc-sections flag will force the linker to remove all dead code regardless.
I disagree. If you're writing a normal English document and have a list of items separated by commas, and you need a line break, the comma always goes at the end of the preceding line, never the beginning of the next line.
I'm on Arch with access to the Arch user repository, which is where I would typically go to get stuff like this. Many of those packages will simply point to the git repo. Sure that means you need the rust compiler to build it, but the installation ends up being managed by the system package manager. See https://aur.archlinux.org/packages/exa-git/ as an example. Infact, exa is now also in the official repo as a binary. The binary may be behind as you point out. But the choice is yours. I've installed other binaries via cargo, but I don't like all too much. 
What's the reason you'd wan to use mrustc vs cross compilation?
I have a [crate](https://crates.io/crates/counted-array) that lets you write: ``` counted_array!(const A: [u64; _] = [1, 2, 3]); ``` Another option is making a slice (fat pointer): ``` const A: &amp;[u64] = &amp;[1, 2, 3]; ```
1. To rule out the possibility of a Ken Thompson hack in Rust 2. To port Rust to a new platform
Is the diamond inheritance problem relevant in Rust? I thought it wouldn't come up at all simply because traits are analogous to interfaces and don't allow any inheritance of implementations.
Ah actually LTO almost always produces the smallest Rust binaries. Perhaps for C/C++ it can bloat things but the way we have it implement it's practically guaranteed to reduce code size as it can be much more aggressive than `--gc-sections`
&gt; uninitialized local variable I assume you don't mean using [mem::uninitialized](https://doc.rust-lang.org/std/mem/fn.uninitialized.html) right?
Lol Ukelele is just a small Guitar with few Strings. Nice reply. I like it how you solved the problem not just gave a exact solution.Great work.
Why is 2 easier with mrustc rather than cross compilation?
I'm writing a forum software, it's called [Converse](https://github.com/tazjin/converse) and uses actix-web, diesel and tera. Main motivation was a specific (work-related) need for a simple forum that explicitly *doesn't* have some of the features of common forum software (such as subforums), as well as a desire to not expose any PHP or Ruby software running on our infrastructure to the internet (we're a Rust shop). Converse already works and has the basic features one would expect, but I'm not a frontend designer so the templates are simply based on Bootstrap 4. They work fine but could probably use some styling ;-)
Yes, I agree. Optimizer can see that some runtime conditions are always true/false and remove unnecessary function calls, therefore making some live functions dead. But if the function is already dead, you shouldn't need LTO to remove it. I don't believe there is any code path in helloworld program that needs to print Ipv6Addr, so this function should be dead. And it is strange that it's not.
Note that `tree` has a `--du` command: ``` % tree --du -hL 3 . ├── [ 206] Cargo.toml ├── [6.5K] ci │ ├── [ 571] before_deploy.ps1 │ ├── [ 692] before_deploy.sh │ ├── [ 761] install.sh │ └── [ 501] script.sh ├── [ 11K] LICENSE ├── [2.3K] README.md └── [ 35K] src ├── [4.0K] display.rs ├── [1.6K] lib.rs ├── [1.4K] main.rs ├── [8.0K] test_dir │ └── [4.0K] many ├── [5.6K] tests.rs └── [ 10K] utils ├── [4.2K] mod.rs └── [2.0K] platform.rs 59K used in 5 directories, 13 files ``` ```
Just in case there exists no cross compiler for the target in question.
Since LLVM doesn't support floats smaller than 16 bit, it would have to be implemented as a library. What I'm more interested in are posits. There are 2 unum crates, but neither repo is visible nor have either been updated recently. There's also [a posit crate](https://github.com/japaric/posit) by japaric (the author of xargo), but it also hasn't seen much action and it isn't even on crates.io.
I can't speak too much about it directly since I've not yet used it beyond the tutorial, but the Stockholm office at Distil (where I work) uses https://github.com/distil/rust_lua_ffi to generate the production Lua bindings for their Rust library.
Continuing work on [uom](https://github.com/iliekturtles/uom), focusing this week on implementing new quantities (and thereby finally making it useful for my other projects).
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](https://github.com/iliekturtles/uom) - Previous text "uom" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20ID_HERE) 
I guess I'm misunderstanding how cross compilation worked for Rust. I kinda thought that if llvm supported a certain set of features for a given target, then rust could be pretty easily cross compiled for it. And consequently, since mrustc doesn't dispense with the need for llvm in the resulting rust compiler (as far as I was aware), that the targets it could create a rust compiler for were still limited by llvm.
Nice, thanks for that. It is one flag I was not aware of and it is always nice learning about new functionality of old commands.
Macro is nice solution I hadn't thought of. Thanks! Don't really want to fall back to a fast pointer when I explicitly know the size. 
This is really nice! &gt; dust is currently about 4 times slower than du. I tried to compare it and you were only marginally slower in a directory with 10k files, so it's not 4 times slower for me. Maybe you used dust first and that had to bring all the metadata into the FS cache and then you tried du on a hot cache? If you still want to optimize it... Some programs like tokei (line of code counter) and ripgrep (recursive grep tool) (both blazing fast) use `[walkdir](https://github.com/BurntSushi/walkdir)` and [multiple threads to complete the work](https://github.com/Aaronepower/tokei/blob/master/src/utils/fs.rs). This means that you don't need to wait on synchronous calls to the OS each time you want to grab the stats. However, in tokei and ripgrep the work for each file is completely independent and here you want to make a pretty tree (which I LOVE. I LOVE this tree and throwing away all the data I don't care about). walkdir is written with this kind of use case in mind, I think so it might be kinda hard to efficiently group the sizes without a cursor to collect the data in the right place. I recommend trying to set up a channel between a group of producers and a consumer. Use walkdir with multiple threads as the producers (as tokei does in the link before) and then have the consumer build the data structure with the the associated sizes on the other side.
`rlua` is the best of them, IMHO.
Nope, that's what I mean :)
Which only works on Mac and Linux right now. Just saying, it's still really, really early in development.
Please use cargo install. The usability difference between cargo install dust and Google the repository name, git clone, cd, cargo clone, mv exec mydir is enough for me for not installing it on a remote server whenever I want a better du. And I know Rust. I imagine it's a lot worst when someone think the build it is as reliable as a makefile for c/c++, or is not proficient with git.
I believe rlua is maintained and used by Chucklefish, a professional game studio, so that would probably be the best option.
Does anyone one know the eticate for making new device crates? I've been using a svd2rust crate I made for the nrf52840 but I'm unsure if I should upload it to crates.io just because I don't want to squat accidentally on a useful name.
Just pass in a mutable vector and use clear() every iteration. This removes all items but keeps the capacity. https://play.rust-lang.org/?gist=4c3bb00d1a369e513e7b67dd6f01af5e&amp;version=stable
This is horrifying, but I kind of like it. It brings to mind one thing I've always liked about LISP-style syntax, which is the regularity and ease with which one can nest expressions. In Rust, three nested match expressions would be an abomination, but it's easy to do so with a more regular syntax. Now whether it's a *good* thing that one can nest 8 layers deep without trouble is a matter of debate.
can't you make an attribute macro for this? i.e. #[arraycount] or something
The post starts with having an `Rc`, but the error is not really about `Rc`. `Rc` is the way the error can be solved. Anyway, this is a good trick. I remember seeing this error, and, since a field was not clonable, and I did not think of `Rc`, I ended up creating a macro instead of a function. It is actually pretty annoying that you can not just wrap a block of code into a closure and call it immediately because of this error sometimes.
Cretonne doesn't do any optimization, right? So this wouldn't be super fast, would it?
Wow! :D I guess it would be a little safer to use an `Option`, and swap it with `None` instead of uninitialized
Why not just download the binary? 
&gt; Glenn fixed the anti-aliasing of ellipses in some edge cases. No pun intended, I assume? ;)
You should not be doing that. For drop types, if destructors are run (say, from a stack unwind from a panic) it's UB if the type is not properly set up. Even for non-drop types, it may not be a correct state for the type (e.g `bool` is undefined if it's not `0` or `1` IIRC).
Yes you should always do that, unless you really know what you're doing^* (and even then, just use on `Option`). ^* I.E You're sure destructors will not be ran on the uninitialized memory.
I am starting a new ui framework in rust https://github.com/nebtex/holo, that will be implemented over the futures crates and use 0 allocations by default, the main objective at the moment is finding a way to create ux interface with the least amount of work possible, also I want to validate another libray that I have been reseached since the past years, and is fundamental part of a group of project which holo is part. is still is desing stage, if you find it interesting and want to suggest something, just open a issue. I will be pending 
You only have to trust your C++ compiler vendor - mrustc is completely open source.
Correct, that is not exception safe
The pernicious part of a trusting-trust attack is that the attack can't be detected by reading the source code that you yourself are using to compile your program. Even if Mutabah were to insert malicious code into mrustc causing it to produce malicious binaries, anyone compiling mrustc from source would be able to find the malicious code because mrustc doesn't bootstrap itself (Mutabah would need to have also backdoored your C compiler). So although the point stands that "you can't trust code that you did not totally create yourself" (which, as Thompson would have been well aware, is realistically impossible), the existence of mrustc downgrades such threats from "effectively undetectable" to "plain-ol' backdoor".
We aren't focused on mid-level optimization yet, but we are doing low-level optimizations. So it probably won't be super fast for complex input languages for now, but it should be pretty decent at relatively simple input languages.
For me it was pretty awkward that rocket has no built-in static file server, Everyone is just pointing at the NamedFile example which sucks hard cause it always sends the complete file, no caching, nothing. In my opinion every web framework should have a static file server... So I started my own small library to implement a basic static file server for rocket. [rocket_static_fs](https://crates.io/crates/rocket_static_fs) It currently has support for: - `Range` header (no multipart ranges though) - `Last-Modified` header - Gzip encoding - Mutliple backends to serve from - from a local directory - from a package compiled into the binary - or plug in your own Upcoming features are: - More content encoding - `Cache-Control` header rules - Directory listing
Is there any chance we will ever get a Rust equivalent of the data class in Kotlin? What's the current best (or "official") way to write Rust code i.e. VSCode + RLS? For that matter, what's the current recommended way to install Rust on Linux (Fedora)? Will there ever be an official snap/appimage? Should we run from a docker instance? Is the rustup script always going to be the official method? 
Maybe, in the distant future when they are stabilized... Currently, `#[derive]` only generates new items, it doesn't replace the original item, so you couldn't use it to "fix up" a type annotation like this. But who knows if general attribute macros will allow replacing the annotated item.
Why not just use a destructuring binding? I'm not sure why you'd even want to "iterate" over the elements in this case.
I will experiment with moving the println! outside the hotloop to see if that makes any difference. To access block information I don't think I can use std::fs::Metadata and I have to use the platform specific ones. https://doc.rust-lang.org/1.15.0/std/os/linux/fs/trait.MetadataExt.html#tymethod.as_raw_stat The max depth option is an interesting idea that I want to look at. 
Yes, but it's less error prone to just use `ptr::write`. It's "safe" functions, but nothing about this is safe.
It struck me that this problem is similar to the problem of thread pool scoping, whether the "expensive resource" (i.e. threadpool or vec) has a longer lifetime than the task/variables which we want to pass to it. Thus, to solve this problem, we can "size down" the lifetime of the resource prior to starting our computation to make it line up, and then make sure we clean up after ourselves. [Playground example](https://play.rust-lang.org/?gist=e914c09e8cf6c9f877449d49022ae00f&amp;version=stable)
Happy Cake Day! ^You ^can ^participate ^in ^r/HappyCakeDayClub ^until ^midnight!
bad bot
Thank you, asp2insp, for voting on HappyCakeDayBot1. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
There's an [RFC](https://github.com/rust-lang/rust/issues/44838) for making the `assert!` family of macros use the pretty-print formatting in their messages as well.
I've been meaning to make a debug parser that can convert the output to something like xml or json so you can view it in a gui
From the article: &gt; we’ve thought about extensions to let you declare the sets of fields accessed by a function (and perhaps the ways in which they are accessed), which might let you declare that process_datum will never modify the data field. I'm encouraged by this. I would like to see the borrow checker continue to get smarter, and this would be preferable to workarounds that have even a trivial runtime cost like (A)rc.clone(). 
Yeah I only do it for types in which all bit patterns are valid. Otherwise I use Default::default() instead.
Yeah you can only use `uninitialized` safely here for non-Drop types for which all bit-patterns are valid, and also when the containing object isn't drop either - otherwise it might have some logic that depends on the `uninitialized` value and a read would invoke undefined behavior.
Which aspects of data classes do you miss in Rust that cannot be done in a simple (hypothetical) `data_struct!(name, member: type, member: type, ...)` macro?
If you, like me, are wondering why the link to episode 5 of Lippert's series is missing, it's because it's a [joke post](https://blogs.msdn.microsoft.com/ericlippert/2007/12/13/immutability-in-c-part-five-lolz/).
Rust beginner here, and I've already hit this problem a couple of times. What I've often realized (after obvious curses over stupid borrow-checker) is that it is exposing a weakness in my design. Unless I'm actually mutating the list itself (in which cases I should probably express it differently, i.e. `Iterator`), I'm actually only mutating a part of `&amp;self`, typically one other field. After that, it's a simple case of newtyping the other field if necessary, and add the mutating function there instead. As a bonus, I get code that have better separation and is usually easier to read. Thank you, stupid borrow-checker.
Sure, the point is that it doesn’t defeat it, it moves the trust around. Still useful!
[removed]
I didn't know there were directory walkers and creating one is only a small amount of code. That's the only reason I have my own. I'll try using walkdir or ignore and see if it fits nicely. 
Working on [shack](https://github.com/saresend/shack), which is a dead simple CLI tool that I wanted for myself that basically acts as a key value store for basic strings (IP addresses, or keys for hacking wargames)
You can still `cargo install` directly from a `git` repo. In this case you simply need to `cargo install --git https://github.com/bootandy/dust`.
You should add screenshot to readme
&gt; git clone, cd, cargo clone, mv exec mydir There is no need for all that. Cargo has direct support for `git` repos. Please see [this comment](https://www.reddit.com/r/rust/comments/8cnmii/a_more_userfriendly_version_of_du/dxhavc5/).
Can you make an AUR package for Arch Linux?
What works for me is to install and use rustup and vscode + the `Rust (rls)`extension. Look through the vscode User Settings and customize it, I have many more than this but most importantly "rust.unstable_features": true, //If using nightly set this to true. Also, if using rustup run `rustup default nightly` "rust-client.updateOnStartup": false, //set it to true, restart vscode and set it to false when you need it to update. "rust.cfg_test": true, //if you want rls to check cfg(test) code "files.autoSave": "onFocusChange", //I use RLS for quick checking but I have command prompts that I switch to and run programs from, although there is a way to do it within vscode "editor.minimap.enabled": true, "editor.minimap.enabled": true, "editor.minimap.renderCharacters": false, "editor.minimap.showSlider": "always", "editor.minimap.side": "left", "editor.wordWrap": "bounded", Note that the RLS plugin has had periods where it will crash unpredictably, but in the past weeks it seems to be more stable.
Trying to help make PRs in the [Rust Cookbook](https://github.com/rust-lang-nursery/rust-cookbook) ready to merge and move it towards 1.0
Is it possible to get arbitrary `Debug` impls to use serde-json? It sounds like the intent of the richhyd's idea is to aid in debugging complex data structures that aren't entirely defined within crates you control.
Bad bot
Thank you, moosingin3space, for voting on CommonMisspellingBot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
Thanks! TIL
You should probably look at Kahan's pretty thorough takedown of unums, though. &lt;https://people.eecs.berkeley.edu/~wkahan/EndErErs.pdf&gt; I think I'd just go for a custom 6+2 format with no Infs, no NaNs, no nothing. 2's complement mantissa, 2's complement base 10 exponent. This kind of format should work well for neural nets where the thresholding doesn't care much about the value beyond a certain point anyhow.
You can't have the relevant logic to prevent CPU bugs in FPGAs. They run on the order of a few hundred MHz at best, while CPUs operate in the GHz. Right now, the best level of granularity is micro ops, which is what Intel uses, but most processors (such as ARM, atmels, and MIPS) are executing simple operations as their programs. The recent spectre and meltdown vulnurabilities were preventable only at the program level insofar as certain patterns had to be restricted due to the branch predictor causing issues that apply to scenarios with specific instruction orderings, branching, and other things. Android devices already employ this sort of scheme by using the ART (Android runtime) to turn dalvik bytecode into native code. Obviously, the OS is still native and even some Android apps are still written and compiled into native code before shipping, preventing portability and the ability for this sort of security.
Continuing work on implementing the Dat Protocol in Rust. Next up is finishing porting Hypercore. Made a new website last week to keep track of all our work too 🎉 https://datrs.yoshuawuyts.com
Hey, hey, don't forget about nebulet! It's eventually aiming for server applications.
Is it possible to write a closure that captures ownership to something and then returns immutable references to it? Something like this (except, actually compile): let nums = vec![1,2,3]; let f = move || &amp;nums; let nums2: &amp;Vec&lt;i32&gt; = f(); // should be able to be called multiple times let nums3: &amp;Vec&lt;i32&gt; = f(); I understand the error: error[E0495]: cannot infer an appropriate lifetime for borrow expression due to conflicting requirements --&gt; src/main.rs:26:21 | 26 | let f = move || &amp;nums; | ^^^^^ | But I would like to tell the compiler that the appropriate lifetime that `f` returns is the lifetime of `nums`. 
First of all, I was personally never hostile. I made a comment about how frequently the industry has a whole changes, as someone who has experienced those changes first hand. You keep focusing on C as if C is representative of the language ecosystem as a whole. The flaw with doing that is that C is truly an outlier because of its stability - although in its early years C did change, and idiomatic C is not the same as it was when it came out almost 50 years ago. So if you’re wondering when Rust will stop changing so frequently, and are so dead set on using C as the only example you’re willing to compare to, then maybe the answer is approximately 50 years. My comment about change is about the broader ecosystem, and is especially true of languages that have come out in the last 20-30 years. Java, Python, Ruby, JavaScript, C++, PHP, and Swift have all had major shifts in idioms in the last 10 years. These are just the language changes that have personally affected me, but you can easily find examples of dozens of other languages that are constantly changing. Read about Python 2 vs. Python 3. Read about Ruby 1.8 vs. 1.9. Read about C++11. Read about ES6. Read about he Swift 3 upgrade. Java 8 and 9 have introduced new idiomatic features. I just named several of the most popular programming languages in the universe, and all of them have had major overhauls in the recent past. You used the word “bizarre” in reference to me having the opinion that the industry changes rapidly and constantly. Well, here are my reasons for that opinion. You can decide which side is more bizarre. 
Still working on the Rust version of my rustup-alternative [rustbud](https://gitlab.com/Screwtapello/rustbud). Having previously written [a crate to get artefact URLs from Rust release-channel manifest files](https://docs.rs/rust_release_channel/0.1.0/rust_release_channel/) and [a crate to download and locally cache static HTTP resources](https://docs.rs/static-http-cache/0.1.0/static_http_cache/), I'm currently working on a crate to unpack and install downloaded artefacts.
I think if LLVM has been ported already, but a full cross-compilation toolchain set has not yet been made, mrustc can help. It could be helpful in places where cross-compilation is undeseriable as well, though I'm not sure where that would be. One use case might be BSD ABI upgrades? I'm just pulling that out of a hat though, no idea if mrustc actually helps.
If I remember correctly, mrustc has actually compiled rustc and gotten an identical stage2. I think this experiment was explicitly to prove that Rust does not have a trusting trust problem.