Totally agree. You should have, at the ROOT LEVEL! (PLEASE!!), a changelog file. I shouldn't have to hunt through your repo, website, commit log, docs, etc. To find out what changed. I should be able to find your changelog just as fast as I can find your readme or your license. It is that important! Also, to the list of things in the changelog. List out the bug fixes! Sometimes, when things are busted, I need to be able to pinpoint which version to move up to (the latest might not be good enough). Further, it is really helpful to figure out if the issue I'm currently facing is already fix in future versions of a library.
PostgreSQL is one of those projects that can do things their own way because you just know it's going to be quality regardless.
Wrong sub. You want to go [here](https://www.reddit.com/r/playrust/).
My biggest (and single complaint) about /r/rust is that the text in the OP fills the whole length of the available space, making it really, really long. Almost every other subreddit has a max-width for text, because reading really long lines is both super annoying and not good readability.
Ah, was not familiar with the nuke-elmo meme.
It's called [Know Your Meme](http://knowyourmeme.com/photos/1151933-sesame-street), but can sometimes be hard to find the particular meme if you don't know what to look for. I had searched for the "Future so bright" phrase from the sidebar link, but wouldn't have known to search for Elmo.
&gt; "Nuclear Elmo meme" [Basically just the same picture as the one of SK but with elmo](http://i0.kym-cdn.com/photos/images/original/001/151/933/b94.jpg)
Yes, I found that, but no explanation there either as to what it means. Or maybe like emoji it means nothing except for the fleeting conventions that sub-groups of people temporarily give it. Like a meta-meta-meta-... wormhole.
Well, crates.io now renders the readme, so if cargo doesn't currently support this (haven't created a new cargo project in a few weeks), it probably should.
[I thought it was "fake politeness" that was ruining this subreddit.](https://www.reddit.com/r/rustjerk/comments/6gxuuf/rrustjerk_as_a_possible_forum_for_normal/)
VS has been working brilliantly, thanks for the suggestion!
This is not the subreddit you think it is.
How to take a single item out of a stream without consuming the stream? Background: I'm trying to create an async client for a sync i/o. To do this created a separate thread that does the sync i/o and communicates via `futures::sync::mpsc::unbounded` to the async client. This is how the sync i/o looks like: let io_thread = thread::spawn(||{ let core = Core::new().unwrap(); let request_handler = io_req_msg_rx.for_each(move |msg|{ let res = sync_client.send(msg).unwrap(); // blocking io_res_msg_tx.unbounded_send(()).map_err(|_|()) }); core.run(request_handler).unwrap(); }); And this is how the client looks like: impl AsyncClient { pub fn send(&amp;self, req: Request) -&gt; Result&lt;Box&lt;Future&lt;Item=Response,Error=()&gt;&gt;,Error&gt; { self.tx.unbounded_send()?; let res = self.rx .take(1) .collect() .map(|x|x[0]); Ok(Box::new(res)) } } Now the compiler complains that `self.rx` cannot move out of borrowed content. This is because `take` consumes the stream but I just need the next item. So is there a way to take one item and return it in a `Future`?
I see. Thanks for the answer!
Why your post doesn't have more upvotes?
Whaaat? I'm genuinely surprised by this. I'd even say, there should be a little bit more humor. I like that this isn't overflowed with memes, but I'd definitely appreciate 1 meme/humorous post per day. I like seeing new content here and sometimes it feels like this sub stopped for a while. That would be good time to at least cheer up a little. Sure, there is /r/rustjerk but I sometimes forget it exists.
Perhaps you're all just using it wrong.
nice
I wonder if it's possible to link Github releases to a CHANGELOG.md somehow
Maybe you could do it as part of CSP, just like with eval.
OP: rekt
This is similar to how other subreddits work: The link in the top left goes to reddit, and the subreddit name/logo (which in this sub is the big image at the top of the sidebar) go back to the main page of the subreddit.
https://www.reddit.com/r/rustjerk/comments/6rlvtk/mrw_uaturon_says_please_discuss_in_the_internals/
I understand you may want something else, but for sure the easiest way to give the compiler a hint that some parts of `self` will not be mutably borrowed is to break `self` into those two parts. I do it a lot, and it is not (in my cases) super painful. In fact, I find that it improves clarity on what is immutable and what is not (e.g. one could convincingly see that you have no self-modifying code). In your case, I (a user) wouldn't expect `run_instruction` to need access to other instructions (and as a user, I would be surprised if it could). I might expect a `run_instructions` method to, and it could take a `&amp;[Instruction]` as an argument, and convincingly demonstrate that it won't be able to jump out of that range of instructions. Maybe I've gotten Stockholm Syndrome, but I'm now pretty happy taking Rust's hints about how to write code that is more clearly data-race free. Having that information be part of a type's structure and its methods signatures is great (when possible; there are times when it is not).
I gotta say I much prefer the way /r/python does that. There's a snoo that goes back to the front page and then the Python logo next to that goes to the sub front page. Ferris isn't a snoo. The sidebar image going to the sub front is fine too, of course. I just get confused.
/r/rustjerk would appreciate it
Haven't used afl.rs, but gotta ask: What's wrong with libfuzzer-sys? We've used it to fuzz [a lot](https://github.com/rust-fuzz/targets) of crates.
In addition to all the above you also should take into consideration `time`. Yes "Address Sanitizer" and "Thread Sanitizer" are much faster then `valgrind`, but still instrumented code is slower 2x times may be more, plus at least "Address Sanitizer" consume as I remember additional 1/8 of program memory. And, of course, for real world projects running package of tests takes time. And to fully replace `Rust` dynamic checking (or runtime checking) not enough. So you should take static analyzers, something like `clang static analyzer` (from practical expirience it will be better to take several) and run them on each commit. 
Huh, I have been doing exactly that for Way Cooler (as in, using github releases). I'll make a mental note to have a `CHANGELOG.md` file in the project root going forward.
I would like to once again thank all involved in the discussions, but especially the RFC authors. Having weathered a storm and digesting hundreds of comments, they've come up with a RFC which I personally think will make Rust even nicer to use.
The project in question is [imag](https://imag-pim.org), see [here](https://imag-pim.org/blog/2017/08/25/imag-0-3-0/) for a blog post with more details how the release was done. I hope we can have a discussion how this can be improved. No offense to the authors of `cargo`, of course - it is awesome... just lacking this one thing, making releasing a multi-crate project with a lot of crates rather complex/complicated/unpleasable.
Regarding the dependency re-writing, I think (although I'm not entirely sure) that if you write something like libimagstore = { version = "0.3.0", path = "../libimagstore"} the `path` dependency will be used during local development and normal `crates.io` / `version` dependencies will be used for users of the crate.
Now I want to write how I make a `gtk-rs` release haha.
I've had this thought on Twitter already today: What is the relation of lifetime lengths behind the syntax `'a: 'b`: * `&gt;`, meaning 'a will always outlive 'b, or * `&gt;=`, meaning 'a lives as long as 'b or longer?
Why wouldn't you just develop each crate independently? Develop the new features in X, and then publish X. If crate Y could benefit from those features, bump the version dependency Y has for X and make use of them, publishing a new version of Y afterwards. If someone decides to manually start using the latest version of X before you do anything fancy in Y, that shouldn't be problematic? Making changes to all of the crates simultaneously and then publishing them simultaneously makes them not sound very independent of each other/modular at all. It sounds like a monorepo that happens to be split into separate repos for reasons unspoken?
Yes, but the thing is, that the vec contains a population and this population doesn't actyally contain a Worldview (it just has it as a generic parameter s.t. I can use the type as a method argument). Whereas Rust did exactly what you said: 'a: 'b reassured it that everything is fine here and the error now comes when I call population.act(wv: Worldview&lt;'a&gt;) (see my edit), where it complains that the worldview will go out of scope much earlier than the population (which is true). I just don't understand why this is necessary, since the population will only read the worldview, not save it or try to take ownership
This is not the subreddit you're looking for.
I used to use AFL.rs, but have moved to `cargo fuzz`. Have you tried that? It doesn't have a nice runner, so I had to write my own little shell script to keep it going, but have had a good experience with it other than that.
I don't understand your question, but I'll try to provide some basic information. OpenGL, DirectX (dx, dx11, etc), metal, and vulkan are APIs for doing 3d (and also implicitly some 2d) rendering in an efficient manner (typically on a GPU). These APIs are ignorant of how the rest of your operating system works. They don't know anything about windows or input (slightly not true with DirectX). Therefore, to actually use them you need to ask your operating system to give you a context for using them (create a window and enable the right API). This is platform specific so people tend to make libraries that abstract over the process. Libraries like glutin, sdl, GLFW, glut, etc provide functionality for doing this. Glium is a high-level library on top of the OpenGL API. As for where to start, you'll want to find a tutorial. For learning opengl in general there are many good resources such as https://learnopengl.com/. It uses C++ instead of Rust but I think it's still a good starting point. I hope that helps.
That would be _really_ awesome! 
&gt; Why wouldn't you just develop each crate independently? I will split up the crates at some point into multiple repositories, yes. But right now (everything beeing experimental, unstable, things are changing) it just does not make sense to do this. Right now I have to adjust things in multiple crates all the time and making a release every two weeks or so seems just to be... too much noise around nothing. If things get more stable and the (internal) APIs more mature, a split will be done, yes!
&gt; Third, and hopefully final, version I work with designers, so I know that after ~~"…-v3-final.psd"~~ "…-v3-final.rfc" there will surely follow a "…-v3-final-2.rfc" and, later in the week, "…-v3-final-2-approved.rfc".
Yep, that is how it works. Been using that in gluon (currently 7 crates) since way back.
`'a: 'b` should be read as `'a &gt;= 'b` (strictly speaking, `'a ⊇ 'b` since lifetimes only form a partial order). See: https://github.com/rust-lang/rfcs/blob/master/text/0192-bounds-on-object-and-generic-types.md
I enjoy how minimal this RFC is. :) The *A foo.rs and foo/ subdirectory may coexist; mod.rs is no longer needed when placing submodules in a subdirectory.* seems to me like a really good idea. I personally hope that the followup will be "no need for `mod` if you `use` anything from inside of the module", which seems to me like the best compromise between redundancy of `mod` and `use`, and problems with deducing modules from the state of the filesystem.
I haven't read through the RFC text yet, but from the summary it looks like this version is not only _not terrible_ but actually _good_. I was a big -1 on all the previous iterations but this is great because it removed all the things I disliked (autoloading modules, a bunch of confusing visibility changes) and fixes one of the only gripes I actually have with the module system, the need for `mod.rs` files. I also like the approach of having not-export items being a warning lint. That is a really nice unobtrusive way of dealing with the "I can't tell if this type is public to the outside world or not" problem. I was never bothered by that issue personally, which is why I'm glad to see it become a lint instead of some major overhaul of the visibility system.
First of all: Thanks for the two long answers. It is very well possible that my structures are pretty bad, I coded it all down in a marathon today. I am probably going with your approach of just having WorldView contain the vectors directly. One question about that though: I have a vector of fov_food, each fov_food is a vector on its own (a vector of food positions, it fov stands for field of view). Now I want to pass WorldView Structs to all of my player objects. So you recommend that moving the fov_food out of the Vec&lt;fov_food&gt; into the WorldView one by one is a better idea? (is that possible/won't it copy the whole vector?) About the other stuff: Do I understand it right, that since I have a trait which has a generic type T and a function in that trait takes a reference to a T, then rust assumes, that struct could save that reference and that is why I have the headache (since in my case T=WorldView&lt;'a&gt;)?
&gt; the trait bound for&lt;'a&gt; &amp;'a main::NewType: std::ops::Add is not satisfied. I've tried implementing std::ops::Add, but I don't understand what the for&lt;'a&gt; &amp;'a is about It's basically saying that your code requires you to implement the trait for a reference to your type and not just the type itself. Try this: impl&lt;'a&gt; std::ops::Add for &amp;'a NewType { type Output = NewType; fn add(self, other: &amp;NewType) -&gt; NewType { NewType(*self.0 + *other.0) } }
&gt; Note that when we say that something outlives a lifetime, we mean that it lives *at least that long*. In other words, for any lifetime `'a`, `'a` outlives `'a`. This is similar to how we say that every type `T` is a subtype of itself. https://github.com/rust-lang/rfcs/blob/master/text/0192-bounds-on-object-and-generic-types.md#lifetime-bounds-on-parameters
Where did you post the updated version of the code?
Lifetime parameters `'a` need not match the lifetime of the actual objects. So it's entirely possible to construct `&amp;'a &amp;'a i32` even if all objects have strict drop orders. Also, you can actually have things get destroyed at the “same time”. See https://doc.rust-lang.org/nomicon/dropck.html for an example.
Too soon. I have no idea how they write RFCs faster than I manage to read them.
/r/playrust
Currently only support sequential version.
It is not that simple. Taking [gluon](https://github.com/gluon-lang/gluon) which is 7 crates (+ 2 more which are in other repos) as an example. These crates are perhaps more connected than in /u/musicmatze case but I think it still applies. I added bytecode serialization some time back, while these changes were largely done in the `gluon_vm` crate but I also needed to make change in the `gluon_base` crate which `gluon_vm` depends on. If I were to put these in separate repositories I would need to mess around with git dependencies or git submodules (subtrees?) and make two commits etc. All of that just ends up with a lot of unnecessary work, especially when the changes are small but still touches multiple crates. And this is exactly the extra work I often need to do for the [crates](https://github.com/gluon-lang) outside of the main repo. Do change in main repo, update the sub repo with a git dependency , test the new addition, replace git dependency with the new version on crates.io, publish main repo, publish sub repo. While I could in theory do the change in the main repo as well as publish it independently that is bug prone and usually means means multiple (slow) iterations until a good API is found. Anyway, if the crates are independent yet there is frequent need to update multiple crates in lock step then multiple repos are more hassle than they are worth.
??
A published crate is basically an invitation for other people to use your code. If things are fluctuating as much as you say theybdo, why publish in the first place? Have you decided on a stable public interface? Do your crates have clear separations of concerns?
Also felt the pain of multi crate publishing though the first gripe was fairly simply solved in my case by a [bash script](https://github.com/gluon-lang/gluon/blob/9e2a88de0145458310599adad2481d32dccd6d55/scripts/release.sh) such as the one mentioned in the post. What I found more annoying was version bumping across all the crates but I luckily stumbled upon the [version.sh](https://github.com/nikomatsakis/lalrpop/blob/9a343f7a043ebb369156084ae67b6d7fb12121b9/version.sh) script in LALROP ([gluon version.sh](https://github.com/gluon-lang/gluon/blob/9e2a88de0145458310599adad2481d32dccd6d55/scripts/version.sh)). With that and a few [strategic comments](https://github.com/gluon-lang/gluon/blob/9e2a88de0145458310599adad2481d32dccd6d55/Cargo.toml#L25) in all the Cargo.toml files that luckily becomes a breeze as well. The other problem I just recently ran into was that a [crates dev-dependencies must be published](https://github.com/rust-lang/cargo/issues/4242) for `cargo publish` to work. Currently working around that with [a hack](https://github.com/gluon-lang/gluon/blob/9e2a88de0145458310599adad2481d32dccd6d55/vm/Cargo.toml#L41-L43) but I should make some time and look into fixing that properly sometime as well.
Damn, I ever wonder why we even bother writing `gtk-rs`: `relm` is stealing all of the fame (but is awesome so it's fine).
I completed a tool for mirroring GitHub repositories by listening to WebHooks, using primarily rocket, serde, and git2. It listens for a webhook, clones/fetches a repository, then pushes all of the branches and tags to a remote. I wrote it so that a project on GitHub could take advantage of my Gitlab instance's CI/CD setup, without having to get any users to switch to using my private server. Repository: https://github.com/aaronfriel/grapple And a mirror: https://gitlab.frielforreal.com/friel/grapple
Thank you! I'm not the OP, but I'm very confused too. This is nice introduction. It would be great if you could include some info about sdl, gfx-rs, glutin, glium, piston. It would be great to know which crates are 'competitors', which are built on top of the others etc. It is really a mess for a novice.
I like reading it as 'a is always alive while b is alive'. As such, 'a &gt;= 'b.
To give people the ability to play around with them, eventually even write s new module. The separation of concerns is definitively given, ... Each crate is one concern/domain, almost (exceptions are the utility library, the error library and a few other "core things").
This is incredible. I should really go back and read the docs! 
Is this the most effective way to garner participation in the RFC process? I'm getting close to burnt out on the module discussion and think perhaps more pre-RFCs could have prevented so many formal iterations. I grant that this might be the only way to really get a decent amount of community involvement.
See Rule 7. His name is Steve. He's just some weirdo who likes Pineapple on pizza.
I can't seem to get a github client working. I feel like I'm getting something really simple wrong. Using this gem: https://github.com/mgattozzi/github-rs Generated an API key on my github account, and used the first example on the github-rs page: extern crate github_rs; use github_rs::client::Github; fn main() { let client = Github::new("API TOKEN"); let me = client.get() .user() .execute(); match me { Ok((headers, status, json)) =&gt; { println!("{}", headers); println!("{}", status); if let Some(json) = json{ println!("{}", json); } }, Err(e) =&gt; println!("{}", e) } } I keep getting this error: no method named `get` found for type `std::result::Result&lt;github_rs::client::Github, github_rs::errors::Error&gt;` I'm a little lost on how to debug this. Clearly there's something wrong with the client initialization. What's the best way for me to figure out what's going wrong? I come from Ruby and I keep wanting to stick `binding.pry` after the `let client` line to see what's happening 
Well, apparently `Github::new` does not return a `Github` instance, but a `Result&lt;GitHub, Error&gt;`. Just like you match on `me` to handle errors, you should handle errors for your client as well. Pick one of these: * Use `unwrap()` - the easy, *dirty* way. * Use `match` - the explicit, clean way. * Use `try!()` or `?` - the less explicit clean way, though you can not use it in the `main()` function as the return types will not match. Check out [this chapter of the Rust book](https://doc.rust-lang.org/stable/book/second-edition/ch09-02-recoverable-errors-with-result.html) for more info on error handling.
Hmm, anyone know a good way to generate changelogs from commit messages? Maybe a particular way of using git that lends itself to generating them?
There is another thread [in which this is discussed](https://www.reddit.com/r/rust/comments/6vxccg/meta_a_complaint_regarding_this_reddits/dm3zmpf/), and explained as much as you can explain a meme.
To be fair, this RFC in particular seems to generate a lot of *lively discussion*, so I think it makes sense that it'd keep showing up. This is the most I've seen about any particular RFC in my (albeit short) time following rust 
Nice!
The docs are bad, sorry :(
https://github.com/clog-tool/clog-cli
Glium basically wraps OpenGL with a safe interface, and in doing so also making it much nicer to use. The same guy also made Glutin which is a library for opening a window, initializing OpenGL, and receiving events (mouse/keyboard presses &amp; movement, etc..) SDL is another library for doing the same things as Glutin, differences being that it's actually written in C, has been around for longer and is more established. Gfx-rs provides the same things as Glium, but isn't specific to OpenGL; it can also use Vulkan, DirectX11, and I believe DirectX12 and Metal support is coming. I tried to get into using it recently, but it seems they are in the middle of a big transition, changing the API to be more low level, and suit better the models of the Vulkan, DirectX12 and Metal APIs. Its API also has a lot of generics in order to enable the use of different underlying APIs, and with the lack of resources and documentation I found it very difficult to get anything done, and gave up after about a week and went back to Glium. Hopefully I can revisit it in future when it settles down more. I don't know much about Piston, all I can say is it has a larger scope than either Glium or gfx-rs, aiming to be more like an engine than a graphics framework.
Hey /u/BeneathTheGold if you look at what sjustinas is saying they are correct. Apologies for having out of date docs. I need to update them! Feel free to open up an issue about that on the [repo](https://github.com/mgattozzi/github-rs) or I'll do it later when I can. Basically the client can fail to initialize due to using tokio Core internally and so as to to not ruin your day gives you the ability to handle that error with the `Result` type rather than me causing your code to panic and now you're wondering what the hell happened. Feel free to ping me if you have any questions about the library at all I'd be happy to answer them :)
The [Clang Static Analyzer](https://clang-analyzer.llvm.org/) will try to go through all code paths and try to find weird / undefined behaviors. But even then you still aren't guaranteed to have "Rust-level" safe code: - the analyzer doesn't do multi-threaded or parallel code (at least that I know of) - the checks are limited and some may be missing for "Rust-level safety" - there's no guarantee that the analyzer will go through all code paths To me, the better way to have a sufficiently safe C++ program is to adopt an insanely strict coding style. In Rust, the compiler does that for you, and in a much nicer and productive way: the language was literally made with that in mind. I think it's also nice to remember, from time to time, that Rust is *not* C++ with safety guarantees: it was made from the ground up, learned from C++ strength and weaknesses and was also inspired by many other languages, and it really "feels" like another language.
There is no updated Version. I had a lot of Compiler errors which I worked through (I made to many changes at once I guess) and at the end I now have the previously mentioned error. The structure of what I posted is still the same
Thanks, definitely explained everything. Guess it really does require way too much explanation for a meme...
thanks for your information and for the link
thanks for the link i will take a look
thanks for your info and details
I've read your great post and I thought that it all make sense now. Then I've checked glx-rs github page and I can see there multiple backends for graphics and window. Some of them are sdl and glutin. This looks crazy. I like gfx-rs as it is rust code but it seems that they lack web support.
Hmm. I'm not sure this is right. Did you have changes in all of your crates that required publishing them? If so, then your process seems understandable. But if not, then you shouldn't need to put out a new release of every sub-crate. For example, I just put out a new release of ripgrep which has a few sub-crates, and I didn't need to publish any new versions of any sub-crates at the same time.
Sounds like semantic versioning has found another use case!
Maybe `bors` is the one writing them.
Pineapple belongs on pizza. BANNED.
FWIW as a general rule, if something is weird with the sub, check if Rule 7 has changed.
We're willing to consider remote positions for strong applicants residing in the United States. We can't handle the additional complexity triggered by having personnel based outside the US.
fearlessly
How do I use rustfmt and clippy? I have the stable and nighlty rust installed and use VS Code with the rust language extenstion in legacy mode, though I have I have RLS downloaded too. I tried using cargo +nightly fmt to format a simple test program by deleting some indentations but unlike on Rust play ground nothing happened. 
I'll be sure to remember that, haha.
Am I alone in liking the current system?
It is livable, but not at all good.
I hope that this keeps our current "prelude" element. In fact I would like to see the prelude idea expanded and have them made swappable. Right now we have std and core, no reason we couldn't allow users to setup their own.
Far from it. I think most people like it after they learn it, but it _is_ a bit difficult to learn at first. The goal of this RFC process has been to find ways to make it a bit easier to learn at least the essentials for writing a Rust program, without sacrificing any of the things that make people like the module system after they have put in the effort to learn it, and want all of its power. Given those competing goals, I expect whatever changes finally end up being accepted will be fairly minimal. And I see that as a good thing; "slow and steady wins the race".
I've long dreamed of redoing the CSS, but there's some concern that Reddit will be doing away with custom CSS soon anyway, and I'm not keen to put the work in if it's just going to be obviated. If Reddit makes it clear that custom CSS isn't going away, that'll change things. And, of course, if anyone *else* volunteers to touch up the CSS with no effort expended on my part, then that also changes things. :)
Except in this case we only have a reference. Since the type is Copy, what reason would there be to require a full copy instead of a ref?
I thought it was an ad that my new Firefox failed to block....
What's wrong with `2016 current Copy (2) FINAL USE THIS ONE Copy.psd`?
i think batch versioning is not good. what if one crate change a little, other change a lot, and one change only a little? if you bump all versions you are not using versioning properly. i am not trying to be rude, i just think working on crates independenty would be easier to maintain and will force you to version properly.
At first I thought I ended up in some random meme sub. Then I read the page's address and was even mroe confused.
TIL... This would make my life much easier. Thank you!
`conch-runtime` provides a framework for executing shell programs that can be used as the foundation for building a shell. It tries to remain agnostic to any particular Abstract Syntax Tree representations of the shell commands, as well as abstracting away OS details where possible (e.g. not assuming `fork`/`exec` is available). Notable features: * Windows support for all major features * Executions/evaluations are futures based * Provided implementations utilize `tokio`, but can be easily swapped out for another event loop implementation * Modularized implementations: you can reuse command/evaluation implementations independently but provide custom behavior extensions or alternatives where needed Docs: https://docs.rs/conch-runtime
i haven't kept abreast of this discussion, but the RFC presented everything understandably. this sounds like a great way to simplify visibility and scope shenanigans. regarding `crate` vs `local` keyword, i think both have merits. using `crate` for both absolute pathing and `pub` exposing means that the same concept of scope is shared in keywords, but it also means overloading a keyword for two different actions... not sure which is better. as a self-taught pleb this is my first time hearing about `rustfix`, when should i be using it?
Actually I once made this with Python. Be careful of race condition, especially with GitHub's "branch delete" things.
If they do, would you mind wrapping your custom CSS in a Userstyle URL regex rule and putting it up somewhere?
Do you actually need 30 crates? Is it making your life easier? The codebase is about 26k lines, which is about 2.5x bigger than anything I've personally written on my own... But when you have 430 lines of `extern crate` and 1600 lines of `use` declarations I wonder at what point it becomes a matter of worrying about form more than content. Like, how many of these crates would someone else writing an unrelated project use? I'm not trying to bash the project or author here, it honestly looks like an awesome project and the scale and work involved is huge. I just get mystified every time I see people turn a single logical project into a million crates. Python programs don't do this. C# programs don't do this. Quite apart from the fact that actually finding the main crate on crates.io is hard. Please don't ever break the repository apart; I'd you did I'd have no way of knowing what's going on anywhere.
Thanks! That's a super useful chapter. I never really got a handle on errors, and I'm back at Rust after a ~2 month break, I'll take a closer look 
As far as I understand, it holds at least for ARM and Power, without any compiler-inserted fences. For more details, see Section 8 of https://www.cl.cam.ac.uk/~pes20/ppc-supplemental/test7.pdf .
Thanks! I'll def open an issue or a PR. I actually wanted to just play with some Github API stuff and figured this would be a good place to start contributing and maybe building more of the endpoints. I'll definitely ping you with questions, thanks :)
[They said they're gonna keep custom CSS](https://www.reddit.com/r/modnews/comments/6auyq9/reddit_is_procss/).
Thank you for the feedback! Actually, I got the same feedback right after publishing the paper, but now I cannot change it... :\
&gt; I decided to learn a new modern language – the more exotic the better. This definition of exotic, I do not get :P
Obligatory plug: use ggez if you just want 2d stuff. It's a single crate. Realistically, for drawing things, there's glium and gfx-rs. (And vulkano.) Gfx is complicated and a lottle poorly documented but there's a good tutorial at https://suhr.github.io/gsgt/ To have a window to draw things on, you need a cross platform way to get one from the OS, and handle input into it. Glutin and SDL are both libraries that do this. SDL does other stuff too but you don't need to use that part if you don't want. Piston is a big pile of stuff using gfx-rs and glutin (mostly) that aims to make a game engine. ggez is a small pile of stuff using gfx-rs and SDL designed to make it easy to make 2d games.
Rust allows safe concurrent memery
Awesome! Some of the macros might be weird and a few of the docs documenting them are a bit out of date so definitely feel free to ask or see what other PRs have done. I'm more than happy to help you out. :D
I haven't really used the library, but looking through [the API](https://docs.rs/futures/0.1/futures/stream/trait.Stream.html), it appears as though you'll have to take `&amp;mut self`, and then use `poll` directly or call `by_ref` before `take`.
As long as the memery doesn't start to leak outside /r/rust...
alright, I'm probably going to read over this again tomorrow, but I'm not strongly opposed to anything I read. The update and additional explanation for using crates alleviated one of my major concerns, and most of my small concerns are fairly bikesheddy. I agree with the RFC that there should be some attempt to reduce the verbosity of use statements with the new changes. I like the 'nested use' example given, though there might be better ideas out there. I'm not sure whether I think the ```cli.rs + cli/``` pattern is more clear than ```cli/ + cli/mod.rs```. I do, however, like the reasoning for it. There's probably a more ergonomic solution to this problem, but I don't venture to know what it is. I could bikeshed all day on keyword names, which is why I'm not going to. The breaking change for 2.0 is also much smaller, and will be much more manageable for upgrading legacy baselines. This proposal also doesn't significantly require me to reorganize all of my code, which is a plus.
I'm going to look at it again after getting some sleep, but I definitely agree with your sentiment on this proposal.
I thought memery leaks are considered safe
Welcome to Reddit. Before posting, you should check what a subreddit is about. This includes reading the sidebar and the subreddit's rules. You should check /r/playrust. --- (via Google Translate) Добро пожаловать в Reddit. Перед публикацией вы должны проверить, что такое subreddit. Это включает в себя чтение боковой панели и правил subreddit. Вы должны проверить /r/playrust.
(translating /u/Quxxy's post) Добро пожаловать на Реддит. Прежде, чем писать пост, нужно читать боковую панель и правила сабреддита. Тебе в /r/playrust. Этот сабреддит - для языка программирования Rust, а не игры Rust.
`extern crate some_crate;` `use some_crate::prelude::v1::*;`
By the way, if you interested in gamedev, take a look at three-rs (github.com/kvark/three-rs). It 's small and easy game engine developed by the author of the gfx-rs. 
Gfx-rs doesn't have support for creating windows and handling input. So we need to use different backends. AFAIK, kvark is working on bringing web support to gfx-rs now 
As an RES night mode user, neither are compatible and turning off nightmode is somehow even worse. They're far too bright.
I personally don't like seeing half my screen go unused. I have on certain websites manually increased the text width so as to use my full display. You can't please everyone.
I don't see no rule about that
But, if you go to this trouble, then rebuild times can be much faster. (I know, we're all waiting for incremental compilation on stable). It does involve a lot of boilerplate, although a smart editing environment can help with the crate jumping. And (perhaps) coming out of the proposed module system revamp there might even be a way to do it semi-transparently. 
Previous Rule 7. :-S
I know :P
I just took a look at those, most are about generics and seem roughly equivalent to what happens in /r/rustjerk. The current title image made no sense to me and I've been fairly active in the rust community. I didn't even know that "future so bright" and "nuke elmo" were things until reading this thread.
I think I saw some new algorithm in homu's code base… a fork of a markov chain generator lib, called 'AaronChain'
Right, now make it a crate-wide option so all my sub-modules dont have to bother with the extra import :3
May be this is because of crate allow paralel build and increment compilation out of box with stable compiler? 
IMO, if you're going to make title image memes, at least make it recognizably related to the sub itself. To me, that image means completely nothing. If the image were something like "RiiR all the things!" at least, RiiR is understood to be "Rewrite it in Rust."
No, I also like, I definitely see that it could be made better, though.
Damn, I told people we need to keep the Singularity project out of the Open Source code base.
While crates can depend on other crates, you can't have two crates depend on *each other*. That is, a crate A cannot depend on a crate B where crate B also depends on crate A. Also, crates are compiled in parallel by running multiple copies of the compiler. It's not as though the compiler has special "parallel compile" logic for crates but not for modules. Also also, you can't *just* split modules into crates, since the rules around things like coherence change when you're talking about modules within a crate, versus multiple crates. So there's also that.
I was hoping these were bindings to the language, for which I have an undying love. https://en.m.wikipedia.org/wiki/MUMPS
Non-Mobile link: https://en.wikipedia.org/wiki/MUMPS *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^105387
Hm. I think I had only a few crates that had no changes, TBH. Like 2 or 3 (I guess). But either way, I'd like to release all of them until I hit 1.y.z-ish version numbers, because the whole program is a _suite_ (like git, consisting of several sub-commands - and git is also published as one thing, not as several independent things). As soon as I hit a 1.y.z version number, I will think about splitting the repository and also another release process, but right now, as things are experimental at best, it doesn't make sense to do so.
Please see here: https://www.reddit.com/r/rust/comments/6w0wji/the_pain_releasing_a_multicrate_project_short_rant/dm5b0eh/ &gt; i am not trying to be rude no offense taken! :-)
it seems like rust need some system for local versioning then :)
&gt; Do you actually need 30 crates? Is it making your life easier? Yes. The seperation of concerns is huge here. As you say, the scale and work involved is huge, but it will be even bigger when we reach 1.y.z-ish version numbers... If you [have a look here](https://github.com/matthiasbeyer/imag/blob/master/doc/src/09000-todos.md) you'll see that almost _nothing_ is done yet, compared to what is planned right now/what I have in mind. As you can see in [this document](https://github.com/matthiasbeyer/imag/blob/master/doc/src/01010-architecture.md), I have an architecture of layers of functionality/crates - the lowest layer beeing some filesystem-abstraction, then some basis functionality /`libimagentry*`-crates) is build on top of the core, then the "domain" libraries follow (implementing a domain/concern like "calendar-management" or "contact-management" (both not there yet)) and on top of this there is the commandline frontend crate, which (theoretically) could be replaced by a curses-frontend or even a GTK-frontend, or a web-frontend... whatever you like. Putting everything into one crate means that functionality of the core, the basis functionality and the domain-functionality would be one crate (the latter is the most obvious where things don't should be put together into one crate - "wiki" is unrelated to "calendar" or to "diary"). Putting the commandline frontend into the crate as well would even blow up the size even more. Also, concerning that we have about 5-10% functionality implemented right now, we would end up with one crate with about 300kloc or even more when everything is implemented. If you now think that a user does only want maybe 8 of 30 functionalities which are about to be offered, building only 40kloc for these 8 "modules", as we call them, is much better than building 300kloc and "just not using" the rest, IMO. (If the numbers scare you: Well,... it's my hobby project... /r/programmingforever/ ) &gt; Please don't ever break the repository apart Actually the first time I hear this! :-D 
This is just me hypothesizing, but my guess is that they are more a record of private conversation rather than anything anyone is expected to actually read.
Gitlab mirroring isn't part of CE?
You mean that that _is_ awesome. It's already like that.
This module system saga has been going on for so long - or at least, what _feels_ like very long -- that I'm probably going to appreciate any kind of resolution just to get this over with. What follows from skimming through the RFC, fortunately, doesn't strike me as _too_ bad. I'm kinda curious about the removal of _mod.rs_ requirement, actually, and wonder if it means the package structure idioms are going to evolve towards the Haskell direction (where you have _Foo.hs_ and _Foo/_ directory to represent a package _Foo_). I hope that _mod.rs_ remains an option, though, if only so that we can point out newcomers from Python to the fact that Rust module system is basically the same as Python's. (Which of course raises the question why are we changing it _for the sake_ of new arrivals, given that it's already so alike the equivalent system from what's possibly the most popular beginner language. But it seems like this ship has sailed). The lack of `extern crate` has one downside that I highlighted previously, namely that some libraries have poorly named generic lib crate names (like `graphics` or `texture`) that don't include their sub-ecosystem "brand" name (Tokio, Piston, etc.). When a larger projects uses multiple dependencies, it's easy to lose track of how they map to `[dependencies]` in Cargo.toml. It would be nice if Cargo/rustc provided a way of aliasing crates for `use` statements, e.g.: [dependencies] piston-texture = { version = "0.1", extern = "piston_texture" } // is `extern crate texture;` now _Edit_: It seems like /u/acrichton has made a very similar suggestion in the GH thread already. Finally, there is one thing I'm baffled about: &gt; Non-local reasoning &gt; &gt; Is an item marked pub actually public? It's a fairly common idiom today to have a private module that contains pub items used by its parent and siblings only. This idiom arises in part because of ergonomic concerns; writing pub(super) or pub(crate) on these internal items feels heavier. But the consequence is that, when reading code, visibility annotations tell you less than you might hope, and in general you have to walk up the module tree looking for re-exports to know exactly how public an item is. Huh? The great thing about current visibility modifiers is precisely that they are _very_ local. The presence of `pub` is a great example of local reasoning: when you see it, you know the item is public within the _local_ module. That's a great default, because it prevents accidental overexposure of implementation symbols as crate's public API. If we change that, we'll now have to teach people that `pub(super)` (or is it `pub(self)`?) is in Rust what "public" means in other languages, and code cluttered with those longer specifiers is obviously uglier. This evidently breaks encapsulation and self-containedness of individual modules. It also doesn't really buy anything: the crate's exported API would already be evident from _lib.rs_ and _mod.rs_ reexports, not to mention the output of `cargo doc`.
Welcome to Reddit. Before posting to a subreddit (or, really, *any* forum), you should check to see what that subreddit is for. This includes reading the sidebar and any rules. You should look at /r/playrust.
I'm starting to wonder if we shouldn't add your articles to `gtk-rs` as well. It could be useful for people who'd want to go deeper into `gtk-rs` bindings.
I've seen the `conch` crates pop up here, and they've seemed interesting, but because of limited experience with these libraries I'm still trying to think of use cases. You say in you post here that this particular crate can be used as a framework to build a shell...I think a more narrative example, like a tutorial or blog post, would be helpful, but maybe you're not at the stage that you want to go to that effort yet.
 use super::*;
It's a bit more subtle than that. The implementation of `pub` doesn't actually change: `pub` still only makes the item accessible from the parent module. It's just that without a visibility modifier you'll get a warning if a chain of `pub` doesn't make the item accessible from outside the crate. An item marked as `pub(super)` cannot be accessed from anywhere above it's parent module *ever*, whether through `pub mod` or `pub use`.
Thanks for the comment! Definitely not far enough along the way for blog posts or tutorials, still got a ways to go before having the dust settle around some of the APIs. An example use case that comes to my mind is building a shell with some novel feature not present in other shells. By building it on top of `conch-runtime`, the author can focus on the "interesting" parts rather than reimplementing things like basic command execution, IO redirection, parameter substitution, etc. This is something I'd like to do myself, but for now I'm focusing on laying out the ground work for all the basic features first.
You know that crates are "the Rust way of doing separate compilation" right? The fact that cargo also use crate as "the way to release libraries that others can use" is IMO more of an accident. Like... one library can be a combination of multiple crates, and it might not make sense to release any of those individually. 
I really don't understand why some people seem to think that `use` and `mod` are redundant. They are not, at all. To use the analogy of the filesystem: * mod ≃ mount * use ≃ ln -s `mod` is used to describe/build the tree of modules. `use`'s purpose is to bring items from other modules into scope. I really don't see how they can be confused or conflated. Furthermore, if you remove the `mod` keyword, you need a way to express the visibility of the module (`pub(path) mod`). Which is hard to do. All of the proposals that offered to remove it had to find a workaround, like using pub(crate) by default, and putting underscore in front of filenames, and other (IMO dirty) tricks. Reality is that we need a way to express how public modules are, and without a `mod` keyword it's hard to do, and is often a regression from the current expressivity. Also some people like being able to add or remove modules from the module tree, without having to move or remove the actual files.
Can someone give me a non-graphical use-case for learning Vulkan or just GPU-based programming in general? I've heard of hardware acceleration. Is it something like writing your routines in a language like Vulkan and offloading the computation to the GPU?
I can assure you, people are expected to read them, and many do!
I think offloading computation to GPU is more of a job for OpenCL, but I don't know anything about Vulkan, it might be able to do that too.
It is not.
May be I missed, but is zero cost abstraction avaible? like for example: struct GCharStr { data: *const g_char, len: usize, //strlen } impl Drop for GCharStr { fn drop(&amp;mut self) { g_free(self.data); } } impl Deref for GCharStr { type Target = str; fn deref(&amp;self) -&gt; &amp;str { unsafe { let slice = slice::from_raw_parts(self.data, self.len); //we check that all ok during creation str::from_utf8_unchecked(slice) } } } In fact if you work with glib/gtk+ you most of time operate with their strings, why always allocate memory to transfer from and to, why not just have type that possible to deref to get `&amp;str`?
To get a really really rough idea of how GPU-Programming works (Graphical and non-Graphical) imagine you want to write a program that increases every value in an array by one. In the conservative way you iterate over the array and increase every value: [pseudo-code] for(int idx = 0; idx &lt; array.length; idx++) { array[idx] = array[idx] + 1; } to make this fast on a GPU you create a little program (kernel/shader) in a C-like language ( e.g. GLSL or every other that can compile to SPIR-V ... etc) and your GPU is starting that kernel many many times in parallel. For every instance you can have something like an id on which instance you're running on. You would then running it this way:[pseudo-code] void kernel(int id) { array[id] = array[id] + 1; } that's a really rough try to paint a "big picture" here. For Graphics Programming imagine `id` can be the index of a pixel on your screen. Your GPU is starting hundreds of these kernels in parallel. So what you do on the "CPU-Side" (your normal program) is to load these kernels/shaders from a file into a GPU-Context and prepare your data you want to work on, and configure the GPU (like how many instances do you want etc). Mostly you send a pointer to the data and a layout to the driver –&gt; context switch – driver is loading the data from the pointer into the GPU-Memory and executes the kernels. After computation the result is copied back to a memory location you can read from in you "normal program" This is only beneficial if you have a lot of data. Your 100 element array and increasing the values by one would take longer on the GPU because the context switches takes time etc. 
Apparently, Rust is popular among Posaunists. 
Vulkan is capable of almost everything OpenCL can do. In fact the Khronos Group plans to merge OpenCL into Vulkan in the future.
I have a rust project at work that does health checks on our work servers and exposes some endpoints, and a small utility I made to do load testing on the endpoints I made (just for fun). I got about 30k requests per second handled on my rather average laptop. To be fair data transferred on each request was only in the order of a few hundred bytes, but it still pushed out about 11.5Mb/s on my local network, which I believe is the actual bottelneck (or my laptop's IO) 
That and many similar things are actually on my list, yes. But first goal is to get everything working and every feature supported, then optimizations :) See also what I wrote here (whole chain of replies): https://twitter.com/sdroege_/status/901356914453467136
Thank you for your support! Writing those docs takes time but while it helps the users to understand the concepts, it also helps me improving the API. Basically, if while I write the documentation I fail to explain how to do something, then I conclude there is something wrong or unergonomic with the API and revisit it!
Their respective API are a bit similar. At first, you detect the hardware available and their capabilities , like OpenCL does, then with those infos, you write kernels that you send to GPU. Vulkan, and Mantle before him, were probably inspired by OpenCL since they almost do the same thing : calculate stuff in a highly parallelized way.
So.. that's the _exact_ opposite of what I said I wanted XD I know that it's already possible to do like that currently.
&gt; [The OpenCL working group is] working to converge with, and leverage, the Khronos Vulkan API — merging advanced graphics and compute into a single API. [https://www.khronos.org/news/press/khronos-releases-opencl-2.2-with-spir-v-1.2](https://www.khronos.org/news/press/khronos-releases-opencl-2.2-with-spir-v-1.2)
+1 for Windows support!
Lots of data and semi complex calcs is where it shines. You can saturate the memory bus with the cpu just adding one to a memory address.
As far as I can tell "the Rust way of doing separate compilation" is to not do separate compilation. For modularizing projects we have a perfectly good module system that, attempted revisions aside, works fine for abstraction, isolation and layering. I view crates primarily as "the way to release libraries that others can use"; the fact that it's also the *only* way to achieve separate compilation for the moment seems like an accidental hack.
Okay, the scale involved is even *more* huge than I thought then. You basically want to make Gnome. Shine on, you crazy diamond.
I don't think anybody has written an RFC for this. If you want it, why not take your hand at writing one?
The perceived redundancy comes when you are in lib.rs or mod.rs and you want a Bar from foo.rs: mod foo; use foo::Bar; It's easy to explain why the redundancy is not real, but it's hard to maintain that it can't be improved. Although as you point out, it's also hard to make improvements without removing functionality. One compromise could be to assign a default visibility, and then you'd only need a `mod` statement to change it, or to do something nonstandard like `#[cfg_attr(feature = "foo", path = "foo/bar.rs")] mod bar;`.
A lot of RFCs have said "we can and should write a `rustfix` tool to do this change automatically" or "we'll just reparse all the source files on upload to crates.io" but we... haven't. Could we actually **block** the epoch transition (or maybe even stabilization of this RFC) on having such a tool? That'd project some confidence that we aren't rushing things to meet the end-of-year deadline. Additionally, it would be something like an "executable spec" ensuring that edge cases like the one /u/retep998 brought up can't be neglected.
`rustfix` is vaporware. It would be something like `rustfmt` except actually making changes to source code, kind of like python's [`2to3` tool](https://docs.python.org/2/library/2to3.html).
Try https://github.com/pest-parser/pest 
I made the transition from [combine](https://github.com/Marwes/combine) to [LALRPOP](https://github.com/nikomatsakis/lalrpop) a in [gluon](https://github.com/gluon-lang/gluon) while back and am happy with that transition. The reason being basically that the combine parser took on more and more complexity making it difficult to understand while I still wanted to add on more features such as error recovery/partial parsing (necessary to support completion for instance). The new [LALRPOP parser](https://github.com/gluon-lang/gluon/blob/master/parser/src/grammar.lalrpop) on the other hand is much easier to read and after some work to bring error recovery to LALRPOP it supported everything I needed currently. The only real drawback of parser generators such as LALRPOP is that they require and extra build step or that they aren't as flexible if you require custom parse rules. But the first drawback is just a simple [build script](https://github.com/gluon-lang/gluon/blob/9e2a88de0145458310599adad2481d32dccd6d55/parser/build.rs) with rust/cargo and the second is something that you should arguably avoid to keep the syntax simple. Anyway that is just my opinion, go LALRPOP if you know the syntax will work with LR or go with a parser combinator otherwise.
Mightn't the method be called `try_new` or something? Just a thought though.
Sorry, wrong subreddit. Rust programming language, not the game.
What's interesting is that you wrote combine. So.. I'm wondering if I should stop recommending combine for programming language projects. And more importantly; do you see a way to redesign combine to bring at least some of the LALRPOP features that you needed?
I'd like to mention [pom](https://github.com/J-F-Liu/pom). It is conceptually similar to `nom`, but uses structs instead of macros, which IMO makes the code much more readable. Instead of `named!(something, map!(tag!("x"), |x| x.to_something()))` you write `fn something() -&gt; Parser&lt;u8, Something&gt; { sym(b'x').map(|x| x.to_something()) }`. 
&gt; Could we actually block the epoch transition (or maybe even stabilization of this RFC) on having such a tool? This is actually listed in the ["Unresolved questions" of the checkpoints RFC](https://github.com/aturon/rfcs/blob/epochs/text/0000-checkpoints.md#unresolved-questions).
I want to add that this remote desktop server is meant to run in the background on a Linux desktop. It should *not* require running a separate daemon (e.g. haproxy or nginx) just to handle paths/routes. In other words, it needs to be a standalone app that you can install via something like a SNAP package and will (normally) run as a non-privileged user (e.g. the one running the desktop).
:) I think combine can be valuable for early experimentation as its use of just the standard rust syntax can make it quicker to get started with and the flexibility may also be useful for quick hacks during this experimentation. Outside of that LALRPOP is a better choice for programming languages, since a well designed syntax should try to avoid all of the special cases which parser combinators flexibility are so good at handling! &gt; And more importantly; do you see a way to redesign combine to bring at least some of the LALRPOP features that you needed? Not really! I did some work recently to improve the situation (making reusable parser easy to define) but in the end it is about what parser combinators can do and not what they can't as their can almost always be used to add on any features that they lack (except turning them from LL to LR I guess).
I might some day, but right now it depends on this RFC going through anyway. I think it'll also take a bit of time for folks to get used to the new system once it goes in.
I wrote a toy language parser with LALRPOP and found it powerful yet very pleasant to use, with good documentation. One benefit of a full parser generator is that it also works as a definition of the language grammar (at least for the prototyping stage). I've used nom for other projects but I don't think I would use it for a such a task - I think it's strengths lie more in parsing file file formats and the like.
I love LALRPOP, the only thing I don't love is that it generates a lot of code and compile times are not great. This is especially true for LALRPOP itself, because its self-hosted and has a very large grammar. Despite this, I find LALRPOP grammars easier to use and maintain than bottom-up parser combinator libraries, so I use LALRPOP. YMMV.
https://www.reddit.com/r/rust/comments/6w48of/rust%D0%B2%D1%80%D1%8B%D0%B2%D0%B0%D1%8E%D1%81%D1%8C_%D0%BD%D0%B0_%D1%81%D0%B5%D1%80%D0%B2%D0%B0%D0%BA_attack_%D0%B2%D1%8B%D0%B6%D0%B8%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_%D1%81/dm58eul/
Did you have to write your own lexer with LALRPOP? One thing that bothered me with LALRPOP is that the default lexer didn't do what I wanted, and you need to write a custom lexer by hand, no such tool as lex.
There was [RFC PR #890](https://github.com/rust-lang/rfcs/pull/890) a couple years ago. That had some discussion that might still be relevant on the topic.
I will read the full thing more closely when I have time, but the closing comment echoes my own thoughts pretty well, "This might be a good addition to the language, but there doesn't seem to be urgent need at this time, and it's better to give things time to evolve and focus on other priorities."
The point of Discordianism, to me, stated as directly as I can describe it, is that there are parts of human nature that drive systems towards excessive order, and that working against these forces by adding chaos and individuality can make the world a better place and make you a better person. You are the final arbiter of your personal life philosophy, and while in many circumstances following local norms happens to be a good idea, you're better off being aware that that's a choice and making it actively. The most obviously Discordian application in computer science, to me, is how adding randomness to an optimization algorithm can help avoid getting stuck in local maxima. One example of a way of thinking Discordian philosophy is meant to help with is [functional fixedness](https://en.wikipedia.org/wiki/Functional_fixedness); in most circumstances, using objects in their conventional ways is going to be a pretty good choice, but sometimes it can help a lot to ignore the original intended use of an object and just treat it as an arrangement of atoms that can be used in a wide variety of ways. The application to Rust's design is pretty similar, in trying to balance rules constraining misbehaviour and ways to violate those rules like unsafe and asm.
[removed]
As long as we're going all meta: while I was pleasantly surprised to find a goofy meme here, it would be nice if the image linked to some sort of explanation, instead of just back to /r/rust. Opening the last rule for an explanation of something elsewhere on the page is pretty non-obvious for someone not used to /r/rust.
huh — in my [attempt at reinventing a config language](https://github.com/myfreeweb/dcml/blob/4b5fda1fd6b741de053f2e5e3c07b22639b56acf/src/parser.rs) combine turned out to be the only thing I could actually use! :) pest was frustrating to debug (though that probably has changed since I've tried it). nom is mostly byte-oriented and low-level-ish, the string support didn't have a lot of combinators. lalrpop… ehh, feels kinda complex.
This saga has revealed serious weaknesses in the RFC process. I don't have any particular vision of a change (and no one has mentioned one to me), but we have talked on the lang team about revisiting the processes we use after the impl period.
This is not the final module system RFC. As the comment says, we are going to post an eRFC for the module preloading portion of the previous RFCs, to get experience that can inform the discussion about the pros and cons of that system.
It's difficult to see what's going on without the surrounding code of `population.act(...)`. Here's my best guess: your *act* function wants to accept a WorldView&lt;'a&gt; for *any* `'a`, not just a specific `'a`. So what you really want is a type signature like: pub trait Population&lt;S&gt; { fn act&lt;'a&gt;( &amp;mut self, id: usize, world_view: &amp;WorldView&lt;'a&gt;, reward: f64, ) -&gt; S; ... } Notice that `'a` is *bound* to `act`. However, presumably you want to parameterize over the type of `world_view`, so in a hypothetical future Rust it might look like pub trait Population&lt;T, S&gt; { fn act&lt;'a&gt;( &amp;mut self, id: usize, world_view: &amp;T&lt;'a&gt;, reward: f64, ) -&gt; S; ... } But Rust doesn't support higher-kinded polymorphism yet so this won't work as is. Fortunately, there's a trick to work around this (at least for lifetime parameters). Reverting back to the `Population` you have in your code, instead of writing Box&lt;Population&lt;WorldView&lt;'a&gt;, (Rotation, ForwardMovement)&gt;&gt; you would *bind* the `'a` to a local for-all (this is called a higher-rank trait bound): Box&lt;for&lt;'a&gt; Population&lt;WorldView&lt;'a&gt;, (Rotation, ForwardMovement)&gt;&gt; This should achieve the equivalent of a higher-kinded parameter.
I'm working on a bulk ID3 tag editor using gtk-rs.
Use [Eco](https://github.com/pistondevelopers/eco).
What i quite like about cargo is that if i follow a tutorial that may have used older versions of a package, I can pretty much guarantee it will compile if I use the same version.
Is this something the Rust ecosystem should be handling? It sounds like it's something that a build system (I use [shake](http://shakebuild.com/)) could address well.
A backend in piston is prefixed with a category (e.g. "piston2d-") and the underlying API, e.g. `piston2d-gfx_graphics`is a backend for `piston2d-graphics`.
It's unfortunate, but I think you'll have to handle http requests on your own. That being said, it's not nearly as bad of a task as you might think. Using [httparse](https://github.com/seanmonstar/httparse) makes both blocking and non blocking http request parsing trivial, and after that it shouldn't be too hard to just handle the static routes just using pattern matching. Overall rust's http ecosystem is still in an immature state (though lots of progress has been and is being made), so any sort of special case like this is going to be somewhat painful compared to other languages.
Ugh. Thanks for the info. The thing is: This isn't a special case. This is how you normally develop a WebSocket application! You handle everything over a single port. Until I encountered the situation in Rust I've never seen a WebSockets-utilizing framework that couldn't also serve normal HTTP requests. It seems strange to only have one or the other.
[Yes](https://github.com/gluon-lang/gluon/tree/master/parser/src), gluon's syntax is indentation aware so it has a lexer which emits tokens followed by a layout engine which scans the tokens and emits block open/block close tokens at appropriate locations. I think a handwritten lexer is not to bad as they tend to be quite simple and self contained.
I'm working on a Smalltalk-80 implementation as a "learn Rust" project and `combine` has been great thus far.
On the same vein of learnopengl, there is [the Glium book](http://glium.github.io/glium/book/). Glium is not being actively developed anymore (unfortunately) but it's a solid library.
I can recommend LALRPOP for simple languages (haven't implemented a syntax more complicated than a lisp-like in it). I used to writer lexers/parsers in flex and bison before, coming from that side LALRPOP was really easy to learn and use in my opinion.
Use NGiИX as a reverse proxy server? That's the only way I know how to do it...
Yeah that won't work. This daemon needs to be able to serve it's own static assets.
They're not distributed with the compiler (yet), you need to install them: `cargo +nightly install rustfmt-nightly` `cargo +nightly install clippy`
Why not use `From` and `Into` instead of `FromGlib` and `ToGlib`? Does glib-rs at least offer a blanket implementation?
It's not *entirely* vaporware, but yeah https://github.com/killercup/rustfix
IDK, you will probably have to re-implement NGiИX's reverse websocket server functionality.
Not sure how important this is in practice, but a benefit of compiling a crate as a single unit is that all optimizations apply no matter how the crate is structured. That way, you won't accidentally make the program slower because you refactored the modules of a crate (this may happen if you refactor a C++ program splitting some code to another file). Now, incremental compilation would go a long way to make the edit-compile-run cycle faster.
It's not uncommon for `new` to return a `Result` in cases like this.
For parsing a programming language I would definitely prefer lalrpop over nom. I did find the builtin support for lexing wasn't exactly what I wanted, but you can provide your own lexer. Here is the parser and lexer I use for a simple prolog implementation: * parser: https://github.com/dagit/rust-prolog/blob/master/src/parser.lalrpop * lexer: https://github.com/dagit/rust-prolog/blob/master/src/lexer.rs I've been pretty happy with the results.
mumps is an evil abomination. https://thedailywtf.com/articles/A_Case_of_the_MUMPS
The key problem here is that Hyper does not easily expose the live socket. There is a very recent pull request on Hyper to allow this in a signalling way to "take over" from Hyper. From the PR: &gt; If an Upgrade response is returned, the server task is shut down (without closing the underlying io) and this information is forwarded to the Future, along with the underlying io and any remaining buffered data that has already been read from it. See https://github.com/hyperium/hyper/pull/1287 And example code here https://github.com/spinda/hyper-websocket/blob/416ca8588931aa92a737c9c1a4afef2bef5d24b6/tests/test.rs If this is merged into Hyper I'll definitely be jumping on supporting it in [Shio](https://github.com/mehcode/shio-rs), an async web framework for Rust I've been working on.
Awesome, I've been trying to work on something similar. Trying to do my entire website in rust, without using any external dependencies
It looks like rust-websocket has *something* working in that direction, although it seems like a bit of a rabbit hole. The discussion starts in [#46](https://github.com/cyderize/rust-websocket/issues/46), continues in [#53](https://github.com/cyderize/rust-websocket/issues/53) and was seemingly implemented in [pr#80](https://github.com/cyderize/rust-websocket/pull/80). (Closed [pr#79](https://github.com/cyderize/rust-websocket/pull/79) seems to have a straightforward usage example.)
What is the best way to handle errors when executing functions that can error with `rayon`? I saw that for the lib blitz they are trying to get ideas for more `rayon` examples, so I wanted to try to make a thumbnail generator with `rayon`, and the `image` crate. I ran into a problem when trying to resize images in parallel: how to handle if one of the images fails. As far as I can see there are no `rayon` examples for when a function called within `par_iter` returns a `Result`. I made a smaller example with just text files here: https://gist.github.com/nocduro/e6982ed1087a2fbbb81e1e21ce6773ba That file has several functions to write a single string of text to multiple files: * `sequential`: a for loop with basic error handling * `parallel_no_err_handling`: rayon with no error handling (panics on read only files) * `parallel_with_err_handling`: rayon with error handling that returns a `vec` of files with their errors Doing `cargo run` will produce the four files normally with all three functions. After they are created and one or more is marked read only, then the first and third print out errors, and the second panics. I'm wondering if there is a better way to handle the errors instead of the `Arc(Mutex(Vec))` stuff that I'm currently doing? Seems like a lot of `unwraps` in the `parallel_with_err_handling()` function. Maybe a different design pattern is better than returning a `vec` of errors? Thank you!
I think [tk-http](https://github.com/swindon-rs/tk-http) can do this. Take a look at the [websocket example](https://github.com/swindon-rs/tk-http/blob/master/examples/websockets.rs).
Howdy! You're probably looking for /r/playrust 
This is a subreddit for the Rust Programming Language, not the game.
Might I ask why you're writing a language in Rust? I can understand if it's just to learn, but I feel Rust is only partially suited to writing a language. 
That's simpler than I thought! Works great, thanks.
You're looking for /r/playrust.
YES! This is the answer! Thank you! I've got the websockets.rs example running now and it clearly serves up both HTML and the WebSocket on the same port. Perfect.
I was always interested in parser without really using it, and it is the best parser documentation read so far. Wonder if it could make use of standard parse bits, such as regexp for integer. 
I had to deal with this last winter while implementing a replacement for Faye (a cometd-style push server, resource usage was off the charts even for nodejs), and I basically just ended up implementing HTTP myself on top of tokio-core, with some httparse. It was a fair amount of work but the results are pretty great. Orders of magnitude less memory usage than the nodejs server, and much less CPU usage. If you can't find any libraries that do it and you're not afraid to get your hands dirty in a protocol, that might be a good option!
Hapi.js can handle it.
&gt;Why not count lines in submodule and if mod contains let's say &gt; 1000 loc in total (including it's submodules) launch one more thread, and treat module as crate and reuse result of compilation of module in the same way as rustc works with crates? This is sorta kinda what `codegen-units` does. It splits your crate into multiple work units for the purpose of generating code. Not all phases of compilation can be split up, but it does help to maximize CPU core usage. That said, you don't want it for release builds because inlining can't occur across codegen unit boundaries.
Glad to see my post inspired people to take initiative! :)
What other language would work better? (I suppose it partially depends on how the language works and whether you're implementing something that needs tight control over performance characteristics.)
I used [rust-peg](https://github.com/kevinmehall/rust-peg) for this purpose. It worked well enough.
I use LALRPOP extensively in one of my projects, and I split up the parser into its own module to mitigate this. Compilation times in Rust are painful in General. But luckily that doesn't affect the end user :).
I would kindly encourage you to try [pest](https://github.com/pest-parser/pest). I started the project with programming languages in mind and a few key concepts: * separate grammar from code which improves maintainability and always offers an up-to-date formal definition of your grammar * it uses a simple, beginner-friendly PEG grammar * parsing should be as fast as possible * offers a simple Rust API post-parsing * comes with elegant error-reporting out of the box However, all of this comes at the cost of some flexibility that would come in handy at the start of a project, but I would argue that, as a project becomes larger, the advantage of maintainability far outweighs this, as long as you don't specifically want to do something that pest can't.
Yeah there's a really good best practice for creating frameworks in rust: _don't create frameworks in rust_. Having done a hell of a lot of rust now, honestly, frameworks are really uncomfortable in rust, they're not necessary and represent an antipattern of the people writing misguided rust. It feels like you're thinking of them as a logical progression of what you've been doing. Please don't feel that way. Libraries lock people into not just one idea, but mutually inclusive collections of ideas. They are not composable, they hide implementation detail, they target lazy thinking. One thing I can tell you about rust is, you don't take ideas from other languages and bring them to rust, you have to shape your ideas around rust instead. It does safe, sane things very very well, and it does questionable things very very very poorly. From my own personal experience, closures for example are something that are well worn in other languages but they don't work very cleanly in rust at all because of its fussiness and peculiarities with scope, and insistence on safety and not endangering yourself. This is not a bad thing. That scope mud is the same kind of thing you can find yourself in with frameworks, and if not, and everything is nicely modularised, isolated, and safe, you're basically creating a collection of libraries anyway, so stick with that. Feel liberated with the idea of libraries over frameworks and good luck!
Another way to speed up iteration time on interpreter is to base the language on S-expressions first, and invent a concrete syntax when most of the features are done.
You should still run with nginx in front. Logging, rate limiting, easy blocking, modseclib. Then you won't be limited to a certain framework if you route the paths.
I always liked the fact that rust forces you to repeat each import in each file. It helps me navigate in rust-code on github, because each import can easily be traced back to it's definition. 
You'd still see (in either the cargo file or the top of the binary/library) that there was an alternate prelude in effect. Same as you see #![no_std] today. That's already doing exactly what I'm saying: it sets your prelude to be the core prelude instead of the std prelude. I just want end users to be able to also have this feature available if they want to use it.
How would you go about dealing with data storage then? Write your own SQL driver, or use some kind of self-defined file format? I understand the advantages in minimizing dependencies, and there's several I'd like to drop from my project, but My-/postgreSQL is *fast*. Like, stupid fast. Dropping that would seem like an unnecessary amount of duplicated work, and any potential performance gains are unlikely, and miniscule at best. Stuff like a markdown parser/renderer totally makes sense implementing yourself, and I can still see why it would be a useful/fun exercise, I guess.
Why not save the error reporting for the end? The results are `Result&lt;Ok, Err&gt;` and you can just collect and serialize the errors for the user. Edit: with `map` I mean, instead of `for_each`.
For [Pris](https://github.com/ruuda/pris#readme), I started out with Bison and Flex in C. But then I wanted the parser to build an AST, and it turns out that algebraic data types are very useful for ASTs, so I [switched](https://github.com/ruuda/pris/commit/f79e1376fda556cd8204c2d95d4aa9381f448a28) to Rust with Lalrpop. That worked very well initially, describing the grammar and how to build the AST felt very natural. The first thing I ran into there is that Lalrpop’s lexer could not handle comments that run until the end of the line. You had to write a custom lexer for that. After a while, I ran into other issues too, which lead me to write [a hand-written parser](https://github.com/ruuda/pris/commit/4bb1e608e9e94b565a1f9c2742cfa69777912ebc) in the end. Among those issues were: * Lalrpop supported different versions of Rust than I wanted to support. This is a general problem with dependencies. * Lalrpop has many dependencies, which is bad for build times, and they are a maintenance burden: you have to check license compatibility, read changelogs when upgrading, wait for their release schedule for bugfixes, etc. This is a general problem with dependencies. * Larlpop is extremely slow to compile, which in my opinion is bad for user experience if users are expected to build from source. Replacing Lalrpop with a hand-written parser [made the build six times faster and resulted in a 20% binary size reduction](https://github.com/ruuda/pris/blob/c25fd48b10fa249c5988d06517aa37f1fa28d5f1/docs/parser.md) as well. * A hand-written parser can provide more helpful error messages, because it is free to collect whatever context it needs, and it can provide help about the *construct*, rather than complaining about unexpected tokens and listing which ones are valid. If you are still experimenting with the grammar, then generating a parser can be very valuable, and most of my points above are less relevant. But at a certain point it might make sense to write a parser manually. If the grammar is stable enough, the parser should not need a lot of maintenance.
To elaborate on /u/claire_resurgent's suggestion, perhaps use something like this: fn parallel_with_err_handling(filenames: &amp;[PathBuf]) -&gt; Vec&lt;io::Error&gt; { filenames .par_iter() .map(|path| { File::create(path).and_then(|mut x| write!(x, "Hello!")) }) .filter_map(|x| x.err()) .collect() }
Arguably, it's suitable enough that at least one programming is written it (namely, Rust itself).
Glad you like it! Small tip, with 2.5 and 3.0 I made it easier to break up parser into multiple "parser functions" with the `parser!` macro so if you find that you need to reuse parsers in https://github.com/myfreeweb/dcml/blob/4b5fda1fd6b741de053f2e5e3c07b22639b56acf/src/parser.rs#L123 then I recommend you look at using that!
With one great datatype :). I have a morbid fascination with the language.
`into_future` on a stream `S` will give you a future with item `(Option&lt;S::Item&gt;,S)`
I'd say that any ML-like language would work better. `enum`s make Rust good at implementing compilers in but Rust's manual memory management is in most cases of little benefit in compilers.
Eh, self-hosting is more of a rite of passage for programming languages and not really a good indicator that the language is good at writing compilers in. Just look at gcc and clang which are both in C++ and C++ is a terrible language to write compilers in (annoyingly though, they both have good reasons for making them in C++ due to their age and network effects...).
I wrote [this comment about Pest](https://www.reddit.com/r/rust/comments/6uho7c/z/dlt9kvo) a few days ago. &gt;This parser library seems perfect for me. For the last few days, I tried my hand at writing a parser using Rust for the Java Properties file format, mostly for fun. This is a parser I already wrote in C a few months ago, and I wrote the state machine manually, then. &gt; &gt;I wanted to try a parser combinator library, so I rewrote it using nom. Nom is quite nice, and I was able to achieve my goal with it. It is difficult to debug because of the usage of macros, though. Also, now that the parser works I have no idea how to properly report parsing errors. Just reporting the line where the error happens seems like a very difficult task to me. &gt; &gt;I thought that a parser combinator may not be the best fit to parse a text format, after all. So I started rewriting it using Niko's parser generator, lalrpop. It started well, but then the regexp ambiguities started to appear, and I had no idea where to go from there. Also, making a syntax error in the .lalrpop file almost always results in a panic in the lalrpop parser, which is frustrating. And even then, reporting parsing errors seems tedious and very manual. &gt; &gt;Then I found out about pest 1.0 beta yesterday, and from the doc it appears to be the exact tool that I need. The grammar description is simple, and it allows to declare the elements of the AST directly without writing glue code in Rust. It tracks the parsing errors locations automatically, and it remembers the span of each AST element all by itself. Plus, there are facilities to deal with whitespaces and line separators. I'll rewrite my crate using pest soon, and I'll see how it goes! I rewrote my parser using Pest since then, and it worked as advertised. I really like it.
Not seeing a macro in the documentation, but seems like there's a `parser` function. Is that instead of just making closures and calling them when using?
to lowercase isn't even the correct way to case insensitive compare two strings, unfortunately. There must be a better and no-allocation crate for this somewhere. 
I don't promise this is actually *correct* (since it doesn't take locale into account), but there's [some code for case-insensitive string comparisons](https://github.com/DanielKeep/rust-scan-rules/blob/master/src/input.rs#L610-L620) in `scan-rules`.
Thank you for doing this. I started using it a few months ago and I can recommend it.
You want [unicase](https://crates.io/crates/unicase). 
Many thanks for the explanation and the practical applications. So this does sound like an instance of Discordianism. The limited chaos in this case causing people to question authority, e.g. the wisdom of the mods and the Rust inner circle. (For example, perhaps this is connected to the recent question on 'users' about whether trivial additions to the standard library by those "higher up" are justified when suggestions by those "lower down" are rejected). I wonder whether the CoC should be modified to allow "nominated discordians" (/u/Manishearth?), like court jesters, if discordianism is being adopted. The thing to me about Discordianism is that it seems to be used completely unsubtly, i.e. going from 100% sense to 100% gibberish, instead of like a streak of 1% or 5% gibberish which seems closer to the examples of optimisation or `unsafe`. Or even using very targetted contradictions instead (e.g. jokes) to make a specific point, or to cause a specific shift in understanding.
The whole point of this tool is to be, "set it and forget it." Configuring and maintaining Nginx just to use my little daemon is a pointless burden on end users that will likely have *no idea* what Nginx even is let alone how to properly configure it.
https://github.com/m4rw3r/chomp YMMV, but I found chomp to be the most natural parser combinator library to work with.
Thank you!
Perhaps some [normalization](https://crates.io/crates/unicode-normalization) should also be included?
That'd be `cargo +nightly install rustfmt-nightly &amp;&amp; cargo +nightly install clippy` 
What project are you using gltf in?
No problem Have an awesome day
&gt; This is the way I've tended to go, but until we have `#[non_exhaustive]`, you can't really expand a type like this without a breaking change. This where I say, "don't let perfect be the enemy of good." I tend to use the same technique that the standard library uses: https://github.com/BurntSushi/rust-csv/blob/6ca9dce06d57af696d9ddcc99cd85aa8f1dc66ff/src/lib.rs#L226-L232 --- The standard library has an advantage though. It can use its stability system to guarantee that clients can't actually use the `__Nonexhaustive` value constructor. In my case, I just have to rely on convention. But the fact that it's undocumented and includes a comment describing exactly what it's for is good enough in my experience. &gt; The other problem I have with this approach is that not all of the enum variants are applicable in all cases. For example, there may be only one function in the library that could result in some variant V, but because there is only one error type, all downstream error handling code must account for this variant even when it was returned from a function that couldn't possibly create it. It seems like it'd be better to have many different error types so that each function returns an enum with only the relevant errors, but if you have a lot of functions that all have slightly different error semantics, this could cause an explosion in the number of error types you have, which can make it harder to use your library. This is indeed a good point, and IMO, it's a good example that advice like "compile time errors are better than runtime errors" is woefully incomplete. :-) If you try to push every single invariant into the type system, then you're going to make your API a lot more complicated. &lt;hand-wavy&gt;Even this is incomplete though, because the extent to which this adds complexity to your API depends somewhat on the design of the type system you're using. More sophisticated type systems can express more sophisticated patterns with less mental overhead, in part because the type system gives you the tools to do it and in part because it's probably idiomatic.&lt;/hand-wavy&gt; I think there's basically a balancing act between the error types you produce. I *try* to put out crates with one error type such that it makes sense for that error type to cover everything. This can sometimes be quite expansive, for example, by using the same error type to represent errors for both readers and writers. Even though a reader can't produce a write error and a writer can't produce a read error, it is awfully convenient to combine them. For my purposes, I justify this by asking a question: what is the *use* case for splitting the error type? Is there a code organization benefit? Will users of your crate materially benefit from it? Will it prevent bugs that otherwise occur frequently? For error types, at least in my experience, you generally don't care too much about every single possible error that could result from a particular operation, but instead want to check for a specific type of error and do something special for it. For example, when writing a command line tool, you often want to check if an I/O error is a pipe error, and if it is, gracefully quit your program. But if it's any other type of error, you want to pass it on up the line for it to be handled, usually by printing the error and quitting the program. So my short answer is this: ask yourself what problem you're trying to solve by splitting error types. If you think it makes the code much clearer and your API simpler to use, then do it! But if you can't quite see the added benefit, then keep one error type and mush on. And be receptive to user feedback if you got it wrong. &gt; Lastly, there's the question of whether or not to directly expose upstream error types. While this gives the users the greatest power, it also ties the public API of your dependencies to your own library's public API. Yup. I usually turn the upstream error into a string, or maybe a `Box&lt;Error&gt;` would be better, and call it a day. If a user of my library says they really want access to that error type and has a good use case, then maybe it's worth incurring the added public dependency. But I do try my best to avoid public dependencies unless there's a compelling use case to do otherwise. Usually error types don't make my list of things that make the added public dependency worth it. Sometimes there are work arounds. For example, the `regex` crate [exposes its syntax errors through a `String`](https://docs.rs/regex/0.2.2/regex/enum.Error.html), but the actual syntax error type is [incredibly rich](https://docs.rs/regex-syntax/0.4.1/regex_syntax/enum.ErrorKind.html). If users really want the full error, then my advice would be for them to depend on `regex-syntax` directly and re-run the parser on the regex to get that error. The extra cost paid for parsing is usually pretty small and bearable, so this works well in this case IMO. Does it work in every case? Probably not. &gt; I've also seen an approach where there is a single error type, but it's an opaque struct with methods that can give you information about the error without revealing its internal structure. This is easier to modify over time because you can always just add new methods. My concern with this approach is that it's hard to predict what information about the error users of your library will need, so it's hard to decide what methods to add. The "big enum" approach is much easier in this regard, because you just users access everything. The standard library `io::Error` type follows this pattern. I don't think I have a crystal clear idea of when to use this pattern, but it's worth noting that it is not mutually exclusive with the giant enum. For example, `io::Error` has a `kind` method that returns an `ErrorKind`, which is basically your giant enum. An interesting case I've run into is that it can be beneficial to unconditionally box your error type as a way to reduce its size. For example, if many of your functions return a `Result&lt;T, E&gt;` where `E` is your error type, and `E` is a giant enum where some variants are particularly large, then the size of `Result&lt;T, E&gt;` itself will be correspondingly large. Depending on how your code is structured (i.e., you've profiled it), it might make sense to do this: pub struct Error(Box&lt;ErrorKind&gt;); pub enum ErrorKind { ... } &gt; What approach do you use, and how do you balance the concerns of semantics, type safety, a concise API, and future extensibility? If I were to condense my advice, it would be this: 1. One error type per logical unit of code. First approximation: one error type per crate. Second approximation: one error type per exported module, with an error type at the crate level that combines all module-specific error types. 2. Non-exhaustive public error enums are just fine. Use the `#[doc(hidden)] __Nonexhaustive` convention until we get `#[non_exhaustive]`. 3. Sometimes it [makes sense to impl `From` *both* ways for `io::Error`](https://github.com/BurntSushi/rust-csv/blob/6ca9dce06d57af696d9ddcc99cd85aa8f1dc66ff/src/error.rs#L104-L114). If you're writing code that's in `io::Result` land and want to parse CSV for example, then you can stay in `io::Result` land if you want to and don't care about the structured CSV errors. But you still get the string version of CSV errors that you can report to your end users, for example. 4. Defining an opaque `Error` struct can occasionally be useful. Use cases vary. You probably still want to expose an error enum.
Agreed with everyone else. If you want a quick-and-easy-but-small improvement, you can amortize allocation: #![feature(test)] extern crate test; #[bench] fn lowercase(b: &amp;mut test::Bencher) { let target = String::from( "Search some long string to test the lowercase benchmark."); b.iter(|| target.to_lowercase()); } #[bench] fn lowercase_into(b: &amp;mut test::Bencher) { let target = String::from( "Search some long string to test the lowercase benchmark."); let mut buf = String::new(); b.iter(|| to_lowercase_into(&amp;target, &amp;mut buf)); } fn to_lowercase_into(s: &amp;str, buf: &amp;mut String) { buf.clear(); for c in s.chars() { for lc in c.to_lowercase() { buf.push(lc); } } } 
&gt;For example, the regex crate exposes its syntax errors through a String, but the actual syntax error type is incredibly rich. My problem with String errors is that they kill internationalization.
A game engine, but I currently focus on the renderer and renderer related modules like an asset system. (The source code is currently not public) I currently sort of try to implement a [framegraph](http://www.gdcvault.com/play/1024612/FrameGraph-Extensible-Rendering-Architecture-in) with Vulkan, but I am looking forward to gfxll. I really don't want to write an OpenGL backend myself that can be used in a multi-threaded environment. As for gltf I am not using it at runtime, I still pre-process gltf files into my own custom binary format. It was actually super easy to write. I just parse the gltf file, convert it into my own format and serialize it with bincode + flate. And everything is parallelized with rayon. 
I would like to eventually write my own SQL database (Lexer and a good bit of the parser, and data serialization are done). For now, I don't think I'm going to need a SQL database, since I don't plan on the website being too complicated. Sure, it'll be a lot of duplicated work - but I also wrote my own operating system in C. I program for a hobby; I'm a scientist by trade. I *need* to know how things work at a low level, and the best way to do that is to reinvent the wheel.
Hah... Forget which subreddit this is? =)
https://docs.rs/combine/2.5.1/combine/macro.parser.html. It is more or less a rust `fn` definitions which returns a combine `Parser`. Thus making it possible to reuse parsers from outside a single function.
Yeah, that's a tough one. Re-exporting every error type from every dependency (transitively, mind you) will result in a lot of public dependencies, and probably isn't tenable. In the case of `regex`, you can dip down into `regex-syntax` if you want to internationalize the error message, but that won't work in every scenario.
D'oh! I did, my apologies. 
Thanks for your input. This is generally what I think I've come to see as the 'rust way'.
If you want backtraces from all panics, not just the one that terminates the program, you could create a custom panic hook and use the backtrace crate to generate the backtrace in there EDIT: rereading your post, I think you may have meant you want a backtrace from where you created your Error object. You can also use the backtrace crate there to generate it at that point as well. https://crates.io/crates/backtrace https://doc.rust-lang.org/std/panic/fn.set_hook.html
I think a nice use case for this might be to make a cross-platform command runner. Currently I use [just](https://github.com/casey/just), which is really nice, but it uses system shell to execute commands, which means that justfiles are not 100% portable.
I suspect that there's two reasons you've likely come to that perspective on Discordianism. One part might be the availability heuristic; you're probably much more likely to notice something completely absurd as being motivated by Discordianism than you are to notice the motivations behind an unusual good idea. Another part is that many people have taken different interpretations from the Principia Discordia than I have, or are somewhere else along the same philosophical path. The PD, from my perspective, prioritizes trying to demonstrate and teach an intuition for this philosophy rather than prioritizing trying to explain it clearly, so it uses lots of heavy-handed absurdity, and kind of by the nature of what it's trying to express, it's not exactly going to be a guidebook of rules describing precisely what you should do. The people and behaviour most-likely to be noticed as Discordian are those that directly reference the examples and metaphors in the Principia Discordia. To me, the people citing the Principia Discordia itself directly to advise behaviour have kind of missed the point, or not yet understood it, in the same way that someone citing christian scripture to support hatred are missing what's to me the important lesson of Christianity (forgiveness). Another part of it, to me, is that Discordianism is an attempt to teach and talk about a philosophy, not a claim of ownership on that way of thinking. There are plenty of people who try to work towards overcoming their relationship to pleasure or pain without needing to call themselves Stoics, people who practice forgiveness and compassion without calling themselves Christians, and people who practice what's recognizable to me as Discordian thoughts without mentioning that specific name for it. The most recent place I've seen a discussion that feels extremely Discordian to me was [a thread on tumblr where people shared unconventional solutions to persistent life problems](http://master-ball-for-magikarp.tumblr.com/post/164645440756/ridiculous-yet-effective-ways-to-deal-with). I've failed at trying to navigate tumblr's interface any more successfully, but the last time I read through some of the replies on that thread, there were several people talking about being brought to tears by finally giving themselves permission to act outside conventional norms and seeing improvement in a problem that's plagued them their entire life. Now, to contradict my earlier disapproval for directly citing the Principia Discordia as support for their behaviour, here's a quote from the Principia Discordia supporting my behaviour: &gt; If you want in on the Discordian Society &gt; then declare yourself what you wish &gt; do what you like &gt; and tell us about it &gt; or if you prefer &gt; don’t. My sincere apologies if this is getting too off-topic. Bringing this back to Rust, the most obvious Discordian influence on Rust's design is obviously the myth of the apple of discord. Eris labeled a golden apple "To The Prettiest One", and this ambiguity ended up leading to significant loss of life. This lesson demonstrates the importance of having a good ownership system, and that getting it wrong can have consequences on quite a few lifetimes. There's a reason we have 'static and not 'prettiest. ;)
I plan on doing something like this for the Ion Shell sometime soon.
Unicase supports Unicode case folding, which is sufficient for comparing strings ignoring case.
Simply iterating over the characters of both with zip should optimize well. Plain text in Rust means Unicode, so there are plenty of wrinkles in this problem once you support other languages. Overview here https://www.w3.org/International/wiki/Case_folding
I'm curious as to why you would want to do this as your post didn't seem to expand on that much. If you're writing a compiler for SIPR-V then you wouldn't need the Rust compiler right? You would just compiler your compiler and then use that for your file type right?
I would essentially write a backend for Rust that produces SPIR-V instead of assembly. By using the Rust compiler I get things like type checking, error messages etc essentially for free. Does that answer your question?
Error codes out of a limited number of error types.
I have an expensive and fallible operation, and I cache the successful outputs into a BTreeMap. When retrieving an output for immediate use, I would like to concisely: retrieve a cached output, or if it is not in the cache, then perform the operation, propagating any failure but otherwise storing the output in the cache and returning it. Normally I would use the Entry API, and `or_insert_with`, but that function requires the closure be *in*fallible/pure. What should I do/use instead? 
Note that lowercasing a string is **not** a correct way to determine if strings are equal ignoring case. For instance, in German, uppercase version of "ß" is "SS", but if you tried lowercasing both of those you would get "ß" and "ss" which aren't equal.
If you do end up prototyping something on top of `conch-runtime` I'd love to hear your feedback!
Won't be using conch. Ion's basically fully feature complete in most respects. I'm just planning on refactoring and exposing Ion as a library.
The panic is caused by an assert!(), I want to get the backtrace to that. The panic hook is the same for all threads, right? [The doc](https://doc.rust-lang.org/std/panic/fn.set_hook.html) says: &gt; The default hook prints a message to standard error and generates a backtrace if requested So shouldn't the default hook already do what I want? Why isn't it doing it? How can I request it?
&gt; This is indeed a good point, and IMO, it's a good example that advice like "compile time errors are better than runtime errors" is woefully incomplete. :-) If you try to push every single invariant into the type system, then you're going to make your API a lot more complicated. &lt;hand-wavy&gt;Even this is incomplete though, because the extent to which this adds complexity to your API depends somewhat on the design of the type system you're using. More sophisticated type systems can express more sophisticated patterns with less mental overhead, in part because the type system gives you the tools to do it and in part because it's probably idiomatic.&lt;/hand-wavy&gt; On the hand-waving front.. Rust could help here by having some form of refinement types (so that I can return a error type that's restricted to some constructors). Or having a type for each constructor, and structural sum types to combine those constructors (like OCaml's polymorphic variants). Or perhaps something else I can't think about but is surely just as powerful. I expect that *some day*, Rust will have something to express this pattern.
That's exactly what I was looking for, thank you! I was developing a compiler plugin with intellij-rust, but the autocompletion and go to definition features that I grew used to were not available. This makes it so much more practical.
It seems like [case folding](https://www.w3.org/International/wiki/Case_folding) is quite a can of worms. That document mentions one should always specify the locale, but the unicase crate appears to have no option for providing a locale. I take it unicase implicitly always uses the US English locale?
"message passing" is honestly kind of a vague term that means a different thing all the time with no real objective definition. The page you link is inter thread communication, not between sub-modules which is about sending objects from one thread to another. Within a single thread sending a datum around requires no special help and is indeed just moving it into a function.
https://github.com/insanitybit/sqs-service-helper This entire project is built on message passing as the fundamental pattern.
FYI: The Capital ẞ joined the German orthography in June this year and can now be used officially. The Unicode [case mapping](http://unicode.org/faq/casemap_charprop.html#11) doesn't seem to have been updated yet. Lowercase: ß Uppercase: ẞ
[removed]
Edit: I really can't think of anything more clever. `match memo.entry(args)` and the Occupied branch returns the memoized value wrapped in Ok or else the Vacant branch runs the function and maybe calls `insert`. I don't think there's any tricky business with the borrow checker, only that the Entry borrows the map. 
I'm incredibly excited to see how much attention RLS has been getting. It's going to pay huge dividends for developer quality of life. Probably the thing highest on my wishlist at the moment is having autocomplete work when directly accessing an array or vector member, eg. "`some_array[0].`", does anyone know the status on this? Is this something missing from racer?
Sadly there does appear to be tricky business with the borrow checker: https://play.rust-lang.org/?gist=c55130767755d05e43cfc48b36987ec1&amp;version=stable 1. I think I may know why the error involving `oe` happens. `OccupiedEntry.get_mut` has its output lifetime equal to that of the `OccupiedEntry`, thus it is not possible to return that output without either removing it from the map (via `remove_entry`) or including the `OccupiedEntry` in the return value. I suppose this would not be the case had `BTreeMap.entry` been specified with explicit lifetimes? e.g. fn entry&lt;'map, 'entry&gt;(&amp;'map mut self, key: K) -&gt; Entry&lt;'entry, K, V&gt; where 'map: 'entry (The method's actual signature uses lifetime elision which requires the `Entry` to have the same lifetime as the `BTreeMap` that produced it.) 2. But I don't know why constraining the BTreeMap keys to be `Copy` does not allow `VacantEntry.key()` to be dereferenced+moved; nor why the error message claims the problem is that `VacantEntry` itself is non-`Copy`, rather than the output of the `VacantEntry` method.
If you mean that a function can't be running in two threads at once - no. The best way to do this would be to have the function rather be a method on some 'token' object which represents the thread with the ability to interact with the library. Then either make the token only 'gettable' once (through unsafe or through a global counter), or have it borrow from some lazy_static! mutex on creation to block.
I have a function **check_updates** that return the latest index. In this test, the *count* is always 0. I know this is because *count* is copied. How can I reflect the real number in outer *count* variable? #[cfg(test)] mod test { use super::check_updates; #[test] fn check_updates_test() { let mut count = 0; loop { ::std::thread::spawn(move || { count = check_updates(count); }); println!("{}", count); ::std::thread::sleep(::std::time::Duration::from_secs(3)); } } }
My spider sense is tingling at a possible XY problem here. Are you sure you're not able to just call it fewer times?
While it's not anywhere in rustup, throwing in the full Rust tree should work (this is the experience with Racer, not sure about IJ).
You should use a mutex to control access to the counter from multiple threads: https://play.rust-lang.org/?gist=8a8af4373b37284c04e35ca2f3067536&amp;version=stable I'd write a bigger explanation, but I'm tired, and I think the book will do a better job anyway: https://doc.rust-lang.org/book/second-edition/ch16-03-shared-state.html
Why do you need a lifetime parameter for Test1? That struct only contains owned data, no reference. Try this code https://gist.github.com/anonymous/43db8ecc980e2e28b6823ca8574976b2
a lifetime parameter on a struct (such as your `Test1&lt;'a&gt;` there) refers to the lifetime of references inside the struct, and is only necessary when your struct contains borrowed data. the current error you get is caused because you are returning a borrowed reference `&amp;`, but rustc cannot infer the lifetime of the reference because the function takes no arguments. the error is a little misleading though, because even if you do give it a `'static` lifetime, it still won't work, because the test struct you create in the function goes out of scope at the end of the function, making the reference you would return invalid.
Yeah, I think it is missing from Racer, I'm not sure if/when it'll get implemented.
Thank you very much. 
Thank you very much.
What's the problem? It compiles and runs for me... edit: of course it doesn't actually do anything
 use std::thread; fn process() { let handles: Vec&lt;_&gt; = (0..10) .map(|_| { thread::spawn(|| { let mut _x = 0; for _ in (0..5_000_001) { _x += 1 } println!("{}", _x); }) }) .collect(); for h in handles { h.join().ok().expect("Could not join a thread!"); } } fn main() { process(); println!("Hello, world!"); } https://play.rust-lang.org/?gist=3bb7ac0c6b85300a3ede27d956d8441f&amp;version=undefined
Can you give more details, e.g. a C function signature? Usually the answer is pretty simple, and it involves treating a *resource* the function deals with as what's not thread-safe (indeed, functions are thread-safe, they're just `'static` code, only *data* they work with may not be thread-safe).
Heading in to [week 5 of my Gameboy emulator](https://github.com/simon-whitehead/chemboy). Last week, I managed to finish implementing the entire Gameboy CPU instruction set. Its still a bit of a mess in some of the early parts of the code, I'll be cleaning it up at some point. So the emulator can play non-Gameboy Color Tetris now and not crash at all. That really was my first goal.. and I've hit it! :D This week I'll be implementing some memory banking logic. Essentially, game cartridges that are bigger than 32kb switch 16kb worth of ROM in and out of the device memory in different ways. This weeks goal is to implement the MBC1 memory bank controller, which I'm told close to 80% of original Gameboy games &gt; 32kb in size used. So if I can get this going I'll be able to play _alot_ more than just Tetris.
Thanks brother! I wondered what went wrong... I didn't write main() lmao
Ah yes, the Actor model. Thank you, this is really interesting. Do you have any applications built using this library?
Perhaps we ought to reconsider what we expect from members of the rust community team.
I'm interested in general in anything that finds a use for `std::sync::mpsc` or equivalent, but specifically where multiple threads (from multiple modules) are started and then use those queues to exchange information (as opposed to sharing via a shared structure with locks, or calling functions in other modules which first have to grab a lock).
`Rust` extension is not `Rusty Code` -- former is a continued fork of the discontinued latter.
I know. I didn't reference the old `Rust` extension anywhere.
The comment you replied to linked to it, you brought up Rusty Code out of nowhere... `Rust` isn't "old" either, it was last updated 2 weeks ago and also supports RLS (since quite a while), so your comment reads like you're conflating the too.
It seems racer doesn't support anything with an inferred type, or any macros.
well, at least I fully agree on unordered floats. such a pain.
Next time, you should include an error description. Saying "it's not working" is quite vague.
I made this to scratch my own itch, and to save others the trouble. I couldn't find a way to do this with `collide-rs` or any other library, which really surprised me. I like my `LineRelation&lt;T&gt;`, which puts the corner cases you might not immediately consider (e.g. an infinite number of intersections, or distinguishing parallel lines from collinear ones) front and center, with a `unique_intersection :: LineRelation&lt;T&gt; -&gt; Option&lt;[T; 2]&gt;` for the common case that you don't care about those cases. Feedback is welcome, and I hope this helps!
I mostly agree on bounds checking and borrow checking. I advocated options to disable them (of course, off by default) in the past, but I understand concern for abuses. On the other hand, I agree that concern for abuses winning does show that graphics programming is low priority for Rust. I personally think giving graphics low priority is probably correct, but then it is natural and expected that people with those use cases complain Rust does not care for them.
I wonder if the `CollinearOverlapping` case could include the overlapping segment (i.e. store a `LineSegment` or equivalent). For instance, it could hold `LineSegment { from: [0, 1], to: [0, 1] }` for the edge case mentioned in its docs. (I guess this might not actually be useful in practice, though?)
heh nice to see my rant get a few upvotes for a change .. 
2 lines overlapping is another way to say that they are identical. There is no way for lines to only overlap partly.
Oh, I'm sorry, I understand now. You're right, I did have them backwards. Rusty Code still has more installs, I misjudged. How does `Rust` compare to `Rust (rls)` then if the former also supports rls? Thanks!
Thanks, just the answer I expected.
This is a good point. I'll strongly consider adding this. Thanks for the considered feedback!
&gt; I mostly agree on bounds checking and borrow checking. I advocated options to disable them (of course, off by default) in the past, but I understand concern for abuses. For bounds checking: My game project is currently about 20k lines of code long. Searching for `[a-z]\s*\[[^0-9]` (ie. an identifier followed with anything but a constant number) gave 104 results*, out of which 28 are in GLSL code. Of the remaining 76, 57 are in code that is only called at initialization. Of the remaining 19, there are 7 of them for which I do need the bounds check because it is totally possible that I go out of bounds. Only 12 locations of my code have bound checks that could be avoided through logic alone. One when the background music changes, and 10 on "rare" game events (eg. when a player captures a zone). There is only one single bound check that could eventually be a problem, as it is called about 2500 times per frame. But I think I can live with that, and I'm pretty sure I could optimize it if needed by changing the API I expose. *EDIT: I'm ignoring constant numbers for indices, because I use them a lot with constant-size arrays such as `[f32; 3]` for which there's no runtime overhead. I don't think I've ever used a constant integer as an index for a `Vec` for example, as the only use-case I see for this is `vec[0]`, but I personally always use `.first()` for that. 
This is true, but this library works only on line segments. In this context, talk of "lines" refer to segments. Edit: but I do take your point that LineRelation could have a better name. 
Whoopsies, /u/dbaupp is right then, that could be useful.
This touches on what I find to be a mismatch between Rust-the-language and Rust-as-promoted. Rust the language focuses extensively on memory safety and performance. This is a perfect fit for implementing a browser, but in other cases you don't care about *both* of these. In graphics programming you mostly[1] don't deal with external input and memory safety isn't that big of a deal. The game is going to crash/bug either way. In web development you are mainly I/O-bound and performance isn't that big of a deal. However, Rust *is* being promoted in both for graphics programming and web development, and I'm struggling to see the great fit. Yes, it's very cool when you have memory safety, performance and parallelism, but it doesn't come for free (in terms of complexity and time spent). In web development it's even worse since the I/O-solutions are so fragmented. If you look at recent successful languages in this space (Go, Node.js) you will see that they shipped with a complete I/O-solution (with support for HTTP etc.) from the beginning. Rust is an amazing language and *should* replace C/C++ in many/most cases, but I don't see myself using Rust for web development in the near future. And that's fine! But combined with the "You should use Rust for everything!"-mentality that I sometimes see here I can understand why some people become annoyed at the Rust community. [1]: I'm not talking about the network-part of game programming 
Eh... Rust shipped without a complete IO solution (which I agree was probably a mistake), so Rust is trying to ship one (with Tokio), and the problem is...? It's not like Rust can change the past and ship Tokio 1.0 with Rust 1.0 retrospectively. I guess the problem is that Rust is being promoted for web development right now, when it is not yet ready. I sympathize, but you do want some early adopters, so it's complicated.
Glad to be of service!
&gt; I guess the problem is that Rust is being promoted for web development right now, when it is not yet ready. I sympathize, but you do want some early adopters, so it's complicated. I don't see how Rust is being promoted there. There's just a huge community trying (through Rocket etc.). I have yet to see any official statement that says "hey, try Rust for web, it's ready!".
Is this what people call non-trivial today?
Okay, sorry for being imprecise. I will correct myself. Rust is not promoted for web development, but Rust is *perceived* to be promoted for web development. As an evidence, I submit "However, Rust is being promoted in both for graphics programming and web development" written by /u/judofyr above. I understand there's not much official Rust team can do about it.
what's bugging me is: IMO, with a few additions, rust could easily be the one language to use for everything. It would be less work to do these additions than , say, all the work being put into JAI and that is inevitably yet to come getting the tooling for that going. **I don't think Rust is mis-promoted though**; they do clearly explain it's priorities , so I do understand why it is the way it is. IMO there's non-destructive ideas that would handle both use cases, *without compromising either*; it could still default to it's primary mission statement, and require explicit opt-in for other cases. You could go both ways, e.g. you could have an option to mark everything in a module ```#[unsafe]``` and swap the default meaning of the operators (just like a ``use``` rename)- and it will still be within Rusts rules, because only ```unsafe {}``` client code can use it; conversely you could have an annotation for importing [100% safe crates](https://users.rust-lang.org/t/crates-io-search-by-traits-safety/12226) (guaranteed no unsafe blocks).
I mean there are sites like http://www.arewewebyet.org which do imply rust is web ready. I don't think it is though having tried building a web app in iron (which I think is in maintenance hell due to some package permissions) and trying the exact same with spring boot. I was much more productive in java land much to my disappointment. I think when futures and async settle there will be a great story, but I can see how new rustaceans can misconstrue the maturity of the ecosystem
having been through so many platforms, I stand by the fundamental principle: that you minimize the amount of work being done; *checks, and branches are work*. platforms come and go, and issues that you thought were gone might re-appear in unexpected ways (for example, in-order CPUs had flaws, then we had OOOE, but then similar limitations to 'in-order' re-appear when trying to use vectorized machines .. there's an ongoing trend to extend the applicability of vector units (e.g.the intel gather instructions extend the amount of code that could be automatically vectorised). The future could be something like this.. [http://hwacha.org], and mainstream intel is getting this https://en.wikipedia.org/wiki/AVX-512 complete with the scatter-gather instructions , as pioneered in xeon-phi. 
I am running the echo-proto tokio tutorial on nightly and I this warning about `BoxFuture` and `.boxed()` which says use of deprecated item: removed without replacement, recommended to use a local extension trait or function if needed, more details in #228 It wasn't obvious to me how this should be done from the issue or the warning. What's the best way to handle this? Code here: https://tokio.rs/docs/getting-started/simple-server/
I don't think AWWY is implying Rust is web ready yet, but that depends on what you're looking for of course. But the site clearly states: "Can I replace my Rails/Django/Flask already? Well, probably not yet. [...]"
May I recommend tuples instead of arrays? Ie. `(T, T)` instead of `[T; 2]`. Tuples are easier to work with and allow destructuring (whereas arrays do not let you even move out of the array): let pt = (1, 2); let (x, y) = pt; 
What is the best way to remove all leading and trailing elements of a `&amp;[T]`. A short example using `&amp;[bool]: [false, false, true, false, true, false] should be reduced to [true, false, true] removing all `false` before the first `true` and after the last one. 
&gt; I don't see how Rust is being promoted there. While there's no official statement, it is clear to me that core team members are more involved in networking and webdev stuff than in any other field. For example posts like [this](https://users.rust-lang.org/t/announcing-the-http-crate/12123) or [this](https://aturon.github.io/blog/2016/08/11/futures/) are made by core team members. And that was even before the 2017 goal of putting Rust on servers. I'm not really complaining about this, but it's what I do notice. 
I was more referring to the big bold **You can build stuff!** at the top of the page. It might be appropriate to say something like **Almost!** But you're right though. It does list the caveats.
I disagree, he claims that "20+ years of shipped products that demonstrate these assumptions working" which is simply untrue: My only experience is with the Source Engine (CS:GO, TF2, L4D2 and friends) and sure with regular gameplay you don't introduce NaNs but... With game hacking it's trivially easy to inject NaNs. It has 'funny' side effects such as: - Half-assed NaN check which ends up teleporting you to coordinates x = 0, y = 0 (z remains unchanged) - If you attempt to fire a projectile based weapon with NaN viewangles, the server crashes (turns out that half assed nan check came too late). - Both have now been fixed and on top of that permanently bans you in CS:GO - Oh and in case you thought only cheating could inject such numbers, through the ingame configuration system you used to be able to put your mouse sensitivity to ridiculous high numbers introducing the same NaNs... - I haven't tested it, but what if you introduce just barely under +/-inf so you pass the nan check, but later overflow into +/-inf... Yes I dislike the unordered floats just as anyone, but NaNs are a real problem. Testing just ain't cutting it (I mean, Valve doesn't test anything but still...)
I guess the "trivial" ones are the first ones in [this list](https://crates.io/search?q=hello%20world)? Sure, this one is not a big library/framework like Tokyo or Hyper or Piston, but I wouldn't call a crate that provides one thing "trivial" - as long as that one thing it provides is not "trivial". Anyways - looking at [the other crates of the author](https://crates.io/users/ucarion), they don't seem more "trivial" than this one, so I definitely wouldn't call it his "first non-trivial crate"...
Your crates.io badge links to the SVG instead of the create page.
Another thing that I would add is Rusts approach to "zero cost" abstraction. For example iterators are really nice but they only compile to fast code in release mode. As a gamedev that basically means that you have to build you game in release mode, even while testing. I have a simple scene that loads in a few seconds when I run it in release mode but it takes over 3 minutes in debug mode. Although that is not such a big problem anymore with incremental compilation where building with `--release` is also faster for some reason. I think the biggest benefit of D is the build time. When I was developing in D I could just rebuild the whole project after every file change and I never had to wait for the compiler. (And I did some pretty crazy meta programing stuff) 
A silly function such as the following has no resources that you can protect and is not thread-safe (nor reentrant). You could argue that technically the function is thread-safe and only the `data` variable isn't, but that doesn't seem like a very useful distinction in this case. int getNext() { // if you're lucky this yields 0, 1, 2, 3, ... static int data = 0; int savedData = data; data = -savedData; data = savedData + 1; return savedData; } I think's it's reasonable to say that any function that thread-unsafely uses static data (local or global) is inherently thread-unsafe.
This is just a guess from another beginner, but could this work: Use skip_while to avoid the first elements, find the final one with rposition, then take the values up to and including that final one? 
Yeah, I don't have an actual code example, but I was thinking about a C function that uses a static variable internally.
I don't think there's anything build-in for that, but you can easily create a helper function to do this for you, e.g.: fn strip_leading_trailing&lt;T: Sized, F: FnMut(&amp;T) -&gt; bool&gt;(input: &amp;[T], mut pred: F) -&gt; &amp;[T] { input .iter() .position(&amp;mut pred) .map(|first|&amp;input[first..]) .and_then(|input|input.iter().rev().position(pred).map(|last|&amp;input[..(input.len() - last)])) .unwrap_or(&amp;input[0..0]) }
&gt; My only experience is with the Source Engine well there is *one* flaw in your reply. Have you hacked any **console games** burned onto disks? I do explicitely mention this. Games also exist on **curated platforms**. The platform holders have their own UI guidelines you have to pass. There are soak tests etc. I also said, the absence of checks is absolutely critical. It has been on some platforms (platforms where checks incur ~10x hit), and those scenarios may recur (VECTORIZATION). If you wan a C/C++ replacement, you must consider the *every* niche that exists, or C++ *must* persist for those niches. &gt;&gt; which is simply untrue: Also I never said **all** the products work. I stated that you can go back over 20 years and find products that do work. The knowledge - "how to make games that don't have NaNs" - is out there. I'm sorry if Valve didn't have time to apply it. It's very easy to make a debug build with extra checks, by throwing a few ifdefs into your vector maths library. Also what would the game actually do in the case where you 'hacked the viewangle' or 'mouse sensitivity' ? display an error in game? It would fail guidelines. To pass guidelines, the UI for those settings must be constrained to ranges that don't produce invalid values. r.e. 'game hacking' what you'd do is check the values incoming from the network , then the rest of the engine/logic continues as before. You don't need the checks all the way through. 
Any improvements in compile time are good, but generally it's the debug builds that need to be faster. In Utopia, the build process could optimize debug builds for speed, and use modules as crates. This puts some restrictions on modules (no circular references). But in the meantime we wait (while building) for incremental compilation to arrive in stable ;) 
&gt; "Another thing that I would add is Rusts approach to "zero cost" abstraction. For example iterators are really nice but they only compile to fast code in release mode." I haven't looked in great detail at Rusts options here but basically we'd have used a spectrum of build options, switching on various parts (e.g. release but with symbols for debugging, real release, light-debug, heavy-debug with a load of extra NaN checks on every vector operation, etc etc etc..) 
on curated platforms, level content has to go through a testing process , e.g. for example performance tuning. So, you don't get any NaNs. You know that with VR people can throw up if it drops frames. If you're going to do the necessary testing to avoid future VR customers *vomiting*, catching the NaNs isn't really a problem. if you want to give out the tools for anyone to change things, then you could give out what I call "a debug build" (with the bounds checks, with the NaN checks) for them.
Perhaps one can write a crate `ordered-float` where you have a wrapper `struct Orderedf32(f32);` and so on, that implements `Ord` but will panic on NaN. Well, that's similar to the `Wrapping&lt;T&gt;` solution for wrapping integer arithmetic.
You can also do that but afaik it is a bit awkward and you need multiple cargo.toml files. What is pretty cool is that cargo might get https://github.com/rust-lang/cargo/issues/1359
This is really nice. You could (but don't feel obliged!) make it compatible with a lot of other crates by using the `Point` and `Line` implementations in [rust-geo](https://docs.rs/geo/0.4.13/geo/index.html). We've implemented our own line intersection logic using Cramer's Rule, but this is good too.
 #![feature(slice_patterns)] fn main(){ let pt = [1, 2]; let [a, b] = pt; }
Continuing work on [tarpaulin](https://github.com/xd009642/tarpaulin) my rust code coverage tool. Last week, I managed to get it working with tests with a large number of threads as previously you could get random errors. I also got a PR merged in libc and should have one merged in nix-rust by the end of the day. This week, I'll be going after linker errors again that certain distros experience and if I crack that looking at integrating something like libsyntax (or syntex_syntax) to enable more sophisticated filtering rules/source analysis.
I thought that's what [custom release profiles](https://doc.rust-lang.org/book/second-edition/ch14-01-release-profiles.html#customizing-release-profiles) are for?
I think I agree on these points, but I still like using Rust over other languages because of const and move semantics by default, well-designed generics, pretty good macros, traits, modules, package manager, borrow semantics, and sum types. And of course, it allows you to do anything you want, including writing array types that don't include bounds checks. Which is why, for me personally, I prefer to use Rust over C++. But to each their own, I think the point about C++ having integer generics is very persuasive.
&gt; that implements Ord but will panic on NaN. ... in ```safe_release mode```, because in ```gamedev_release``` you cannot afford runtime checks. You test it with extreme examples until you know the logic solidly avoids creating failure cases, until it doesn't drop frames, until it looks nice, *is fun to play*, etc.
I think this is excellent advice on error handling in Rust. Would it make sense to massage this in some form into the error handling chapter of the Rust book?
Yes but afaik you can only use the ones that are listed and you can not create custom profiles. For example you sometimes want to build with -O3 + debug symbols.
The first edition of the book contains mostly a copy of my [error handling blog post](http://blog.burntsushi.net/rust-error-handling/). It may make sense to add it to my blog post, which needs to be updated to use `?` anyway. The first edition of the book is presumably going away at some point (or at least, will stop receiving attention from folks), so I think fixing the blog post makes sense.
[This](https://docs.rs/ord_subset/2.0.0/ord_subset/struct.OrdVar.html) works like that, not specific to float. The value is checked during construction though (except with new_unchecked() ) and it basically just unwraps partial_cmp() for cmp() so if a NaN were to get slipped in it can crash your program but also fly under the radar undetected when using less-than comparisons and companions.
The posts seem to be more general than traditional webdev. All of that it also applicable to browsers and low-level servers.
&gt; through the ingame configuration system you used to be able to put your mouse sensitivity file a bug report :)
Also maybe take an actual screenshot. Or, better, copy the code in the post, or link to it.
You can probably actually just use debug_assert! for this. If the floats turn out not to be ordered at run-time you will get weird behavior but not memory unsafety.
Rust is clearly not meant for **everything**.
&gt; Rust is clearly not meant for everything. I know , but, you've done 99% of what I need, and are blocking the 1% of changes that would make it perfect for a larger range of cases. I have to go and replicate the whole 100% ? .. surely the world can do better. more users would be better, surely; you'lll still have a safe subset of code from gamedev, and it can be rigourosly partitioned: i would not mind having to specially mark an explicit ```#[unsafe(gamedev_mode)]``` at the top of every file, which prevents it's items from being used outside of client unsafe blocks
I released new versions of the [webkit2gtk](https://crates.io/crates/webkit2gtk) crates and started to fix an issue in [titanium](https://github.com/antoyo/titanium), a keyboard-driven web browser. In the process, I've fixed [mg](https://github.com/antoyo/mg), a minimal GUI library based on [relm](https://github.com/antoyo/relm), an idiomatic GUI library. In a completely different subject, I started to implement the [`tiger` compiler](https://github.com/antoyo/tiger-rs/tree/master/tiger) in Rust. Only the lexer is done for now.
I agree there should be a way to work with floats with assumption it's not NaN and being able to strip all the implied bounds/validity checks for the numbers, as I know it's crucial for the game performance. You probably know that, but I just wanted to point out that curated platforms, certifications and multiple QA depts and (soak) tests are not a silver bullet. From what I know almost everyone in the industry try to go for the MVP of sorts in terms of the certification and then go for day 1 patches, for which certification is faster and even so it's still relatively easy for nan/numerical bugs to go unnoticed, since they're very non-deterministic in nature. I guess it'd be nice if one could easily opt in or out for specific scope if there is numerical calculation that's also inlined? Does unsafe scope automatically remove the bounds and various validity checks?
Networking, concurrency and asyncronicity is still far, far away from webdev and far more general. These are very important subjects for many things running on _servers_ and HTTP _is_ the lingua franca of protocols, if you like it or not. A language without an HTTP stack nowadays will pretty much not be considered, one without a full web development stack is acceptable. Also, given that Mozilla builds Browsers, networking and HTTP are obvious important parts of the main stakeholder.
Which is perfectly true: you can build stuff, if you want to invest the time to build a lot on your own.
&gt; Does unsafe scope automatically remove the bounds and various validity checks? not sure, I don't think so: with the array indexing issue (similar but different), the idea is the operator ```[]``` refers to safe behaviour, and you use a different named function ```get_unchecked()``` (only callable in ```unsafe{}```) What I'd imagine being ideal is: there's ```get_safe(), get_unsafe()```, similarly checked/unchecked arithmetic and a ```#[...]``` can map the operators to one set of operations or the other (like a glorified ```use```). In turn, they're only callable from the right blocks
Continuing on the Rust Qt bindings generator. Nested objects are now supported in a very elegant way. The demo applications has a live process view now and QtCharts support since last week. Also, there is a fantastic logo now. This week will see polishing and documentation and perhaps an announcement. https://cgit.kde.org/scratch/vandenoever/rust_qt_binding_generator.git/
[0,1]-[0,5] partly overlaps with [0,3]-[0,7]
&gt; From what I know almost everyone in the industry try to go for the MVP of sorts in terms of the certification and then go for day 1 patches back in the day, patches weren't possible. We tested it until there was sufficient confidence to do a production run. I understand today's web-distributed software is different, but you should still have the appropriate options where you know the testing has been done. I'm sure this isn't intractable. It could be a case of network APIs giving you 'dangerous values', which have to go through checks to be unlocked for the rest of the code (much like Option handling) .. r.e. the case he points out of 'hacking viewangle'
They're critical for games too.
you can't create custom profiles, but you can [customize the profiles](http://doc.crates.io/manifest.html#the-profile-sections) that are there. You definitely don't "need multiple cargo.toml files". [profile.release] opt-level = 3 # not actually necessary, since this is the default, but I'm just making it clear. debug = true now release mode will also include debug symbols.
Neat. Now it needs a push to land in stable so I can suggest it ;)
I still don't understand what holds people back from just using *mut and friends in these situations, along with abstractions over unsafe indexing (which are quite possible). Is it that the std library doesn't really provide them?
Ah yes that makes a lot more sense now. Thanks! :D
because you've lot the middle ground, C++ ```T&amp;``` is a sweet spot that is safer than ```*const T/*mut T```, and less work to use than ```&amp;'x T``` with over-estimated safety. &gt; along with abstractions over unsafe indexing (which are quite possible). yes it's posible, but more verbose; you've baked into the compiler (declaration of ```Index&lt;T&gt;``` and it's mapping to ```[]```) that the efficient syntax goes on a different case. ```get_unchecked()``` is what i'd have to use most of the time, or write an ```UnsafeVec&lt;T&gt;``` whose operator```[]``` breaks rusts safety rules. What would help is if the operator syntax was re-assignable (just like ```use```ing dents. ) I guess then we might be able to make a ```reference&lt;T&gt;``` , behaving exactly like C++ &amp; , and retarget it to ```&amp;``` , or it might still require compiler support .. I see it as a 'borrowed pointer with disabled borrow-checker.' 
im making a web app with diesel and rocket. trying to implement auth system and planning to integrate it with google oauth.
Wrong subreddit unfortunately. Try /r/playrust 
Hi, welcome to Reddit. In future, before posting somewhere, you should check to make sure what the subreddit/community/forum/whatever you're posting to is about. On Reddit, this includes reading the subreddit's sidebar and any rules it has. You should look at /r/playrust.
Ah, so that's what I was missing. I was under the assumption you could create custom profiles simply by adding a new `[profile.&lt;name&gt;]` section or something. Never tried it, obviously.
That makes sense. Thanks for the explanation.
My first little toy project in Rust; a task/time management program vaguely inspired by [TaskWarrior](https://taskwarrior.org/), although simpler and much more focused on the time tracking side of things. I've only recently devoted enough time to learning Rust that I feel like I've got a handle (mostly) on the 'rules'. Been a long-time fan of the language and its ideas, though!
During summer I've been working on an interpreter for a simple pure functional programming language. Quite a lot of work went to verifying a program (type inference was one of the most scary parts), so I haven't been able to run any code until a few days ago. But all the work was worth it - being able to actually evaluate stuff like `List.foldl 0 (+) [1, 2, 3, 4, 5]` feels very rewarding.
Why not do the 1pp required to go from 99% to 100% yourself? Rather than expect "the world" to do the whole 100% for you?
You can already do that with Cargo features.
This sounds really cool! Do you have an open repository that you're developing this in?
This reply needs to become a template.
&gt; " Rather than expect "the world" to do the whole 100% for you?" the request is collaboration on the 1%, not 100%. &gt; "Why not do the 1pp required to go from 99% to 100% yourself? " I'd need consensus to get PR's accepted in the Rust compiler, otherwise I'll have to maintain a fork, merging changes, with a potentially different AST (for a few extra options ) I could take a snapshot and add the features I want, but I think it will become hellish as soon as new features land in mainline rust. Given a fork just done by one person, the rest of the world will probably ignore it.. JAI will probably get the attention of most people who want something game specific. I gave up on my own at a language attempt when I realised it's the tools integration (dot-autocomplete integrated in the IDE, debugging..) that I really need; it takes community momentum to get a language going. Even JAI is being done with a trusted collaborator, not just but JB himself, and I think he's rightly confident that he'll get a community, he's built a following around it etc. Back in after his first video, before he started JAI, he invited email comments; and I did suggest to him: *'consider forking rust'* with the rationale (including retorts to his criticisms). I did get a reply, but he explained he'd already made up his mind that Rust was "too high friction" as he put it. Evidently I liked more of it than him. After he posted his first 'working prototype video' I also embarked on [my own](https://github.com/dobkeratops/compiler) but gave up after a couple of months, figuring his was further ahead and there's no way I'd get any collaborators. Some of it works but I'm well aware 'the last 10% takes 90% of the time..'
https://github.com/jDomantas/interpreter However, beware - the code is probably unreadable (once I get a compilation pass working I just leave it like that, figuring that I will rewrite it once I get everything working), stuff might be broken, I don't try too much to keep latest commit working or even compilable. Also I might rename the repo once I figure out a nice name for the language (maybe you have any suggestions?).
Finishing up my RustConf 2017 recap post and my new redesign post about my website. Catching up on PRs for projects, updating my RFC and some other things. Whew this is going to be a busy week.
Only for posters who are relatively new to Reddit. Memes and passive-aggressive mocking for those who should know better by now.
I agree that the arguments are valid enough to forgive the tone. Please don't overlook this constructive criticism because it's critical. Many of the fixes suggested can be implemented in a low-level library. For example, you *can* index by u32, but you'd need to define a new slice type. And there's no established pattern to follow. Doing low-level stuff in Rust (like, even bit twiddling) means pioneering new ways to do it. `use std::wrapping::Wrapping as W` is *highly* recommendable -- obvious in retrospect, but there isn't a lot of retro to spect yet. ---- One pattern which may emerge is relaxed safety: abstractions labeled as safe which might invoke undefined behaviour in corner cases. This is okay in certain situations, even when it's very much not okay in others. Game engines aren't like Servo. Technically there's enough room for both, I just hope that there can be culturally as well. I think it's very important that safe-for-Internet developers be aware that there are no technical guarantees that a "safe" abstraction on crates.io is in fact free of undefined behaviour. That's well beyond what the compiler can prove without sacrificing optimization. 
Now I finally understand why `Iterator` would be implemented for `Option&lt;T&gt;`. Thanks :) 
There's already a [crate](https://crates.io/crates/ordered-float) for that :)
Blow was never going to use Rust. Things like the borrow checker are antithetical to the way he looks at development.
&gt; because you've lot the middle ground, C++ `T&amp;` is a sweet spot that is safer than `*const T/*mut T`, and less work to use than `&amp;'x T` with over-estimated safety. How is `T&amp;` safer than `*mut T`? Because it can't be null?
Why do the threads have to be from different modules? In Rust, there's no relationship between modules and threads. 
I'm flattered that my post has enlightened you, but I don't think I use the `Iterator` implementation on `Option&lt;T&gt;` in my example... :D
I've begun work on a [FLIF](http://flif.info) decoder for Rust, [flif.rs](https://github.com/dgriffen/flif.rs). There was prior work in the Rust community: [flif-rs](https://github.com/panicbit/flif-rs) but it seems to have stagnated. So far I have header and metadata decoding finished apart from the color transformations. Part of my goal is not to just perform a direct port of the reference implementation, but to find more idiomatic ways of doing things in Rust.
My first Rust thing: a pixel sorter 😆 https://github.com/alvare/imgsort
Agreed; when game programming I basically have to have `opt-level=1` at *minimum* in my game to get any sort of acceptable performance at all. Part of the problem is that all libraries always are built at the same optimization level as the game itself. I really don't need `sdl2` and `gfx` compiled at `opt-level=0`, ever.
&gt; How is T&amp; safer than *mut T? Because it can't be null? yes; (i) it does at least tell you it isn't null, ( I know you still can make a null, but you have to deliberately do it; the probability of finding a null reference in C++ is much lower than a null pointer ). (ii) it also tells you it isn't going to be cached or escape, and that it isn't owned - when you pass a reference into a function, you don't expect that to be stored anywhere that persists outside of the function call lifetime, and when you get a returned reference, you expect it is a temporary you cannot store.
&gt; Doing low-level stuff in Rust (like, even bit twiddling) means pioneering new ways to do it. A gentle nitpick... I see this attitude a lot in libraries; "Rust is so cool and shiny that we need new and awesome paradigms to fully exploit it." So lots of work goes into fiddling around with new and awesome paradigms instead of getting anything done. We *don't* need new and awesome paradigms for most things. How to do bit twiddling in Rust: Use `|` and `&amp;` just like C. Can Rust provide some abstraction to make it *better*? Stronger, faster, more abstract? Maybe? I certainly encourage people to find out! `Wrapping` is an excellent and awesome example of that. But do we *need* it? No. This "everything is new and shiny and reinvents the world in higher levels of abstraction" attitude is also a turn-off for some programmers. The old and experienced ones who know their tools very well, and have seen the cycle of bullshit turn a few times in the last few decades while still basically writing C code the whole time. Many very performance-focused applications like graphics and games have a lot of these people forming the (hard) core of a lot of important stuff.
Needs more screenshots! :D
Damn that so obvious 😑 Will do!
I'm not that much into C++. AFAIK, &amp; is guaranteed to be not null? Would a non-null equivalent to *mut and *const help there?
&gt; I'd need consensus to get PR's accepted in the Rust compiler, otherwise I'll have to maintain a fork, merging changes, with a potentially different AST (for a few extra options ) Wouldn't it be possible to have most of the changes in a library? I mean, that won't be the case for borrow checker stuff, but for bound checkings, the float/NaN stuff and a couple of other things I read I think it would be possible to just implement a few new types/trait extensions and simply `use this_lib::prelude::*;` at the beginning of each file. Maybe even of some kind of "QuiteUnsafeRef" which can be used without worrying too much about borrow checker might be doable at library level.
&gt; Does unsafe scope automatically remove the bounds and various validity checks? Unsafe does not change the semantics of existing code, the only thing it does is gives you four extra capabilities. Safe Rust is a subset of unsafe Rust.
+1 on rust-geo compatibility. I thought Cramer's Rule was famously numerically unstable?
Because it implies separation of concerns.
Arrays have the advantage of being iterable: https://play.rust-lang.org/?gist=659234a2014c2d2869b83c01b0e2197a&amp;version=stable I made `dot_product_*` #[no_mangle] so that you can easily find their IR code in `--release`: ; Function Attrs: norecurse nounwind readonly uwtable define double @dot_product_tup({ double, [0 x i8], double, [0 x i8], double, [0 x i8] }* noalias nocapture readonly dereferenceable(24), { double, [0 x i8], double, [0 x i8], double, [0 x i8] }* noalias nocapture readonly dereferenceable(24)) unnamed_addr #0 { start: %2 = getelementptr inbounds { double, [0 x i8], double, [0 x i8], double, [0 x i8] }, { double, [0 x i8], double, [0 x i8], double, [0 x i8] }* %0, i64 0, i32 0 %3 = load double, double* %2, align 8 %4 = getelementptr inbounds { double, [0 x i8], double, [0 x i8], double, [0 x i8] }, { double, [0 x i8], double, [0 x i8], double, [0 x i8] }* %0, i64 0, i32 2 %5 = load double, double* %4, align 8 %6 = getelementptr inbounds { double, [0 x i8], double, [0 x i8], double, [0 x i8] }, { double, [0 x i8], double, [0 x i8], double, [0 x i8] }* %0, i64 0, i32 4 %7 = load double, double* %6, align 8 %8 = getelementptr inbounds { double, [0 x i8], double, [0 x i8], double, [0 x i8] }, { double, [0 x i8], double, [0 x i8], double, [0 x i8] }* %1, i64 0, i32 0 %9 = load double, double* %8, align 8 %10 = getelementptr inbounds { double, [0 x i8], double, [0 x i8], double, [0 x i8] }, { double, [0 x i8], double, [0 x i8], double, [0 x i8] }* %1, i64 0, i32 2 %11 = load double, double* %10, align 8 %12 = getelementptr inbounds { double, [0 x i8], double, [0 x i8], double, [0 x i8] }, { double, [0 x i8], double, [0 x i8], double, [0 x i8] }* %1, i64 0, i32 4 %13 = load double, double* %12, align 8 %14 = fmul double %3, %9 %15 = fmul double %5, %11 %16 = fadd double %14, %15 %17 = fmul double %7, %13 %18 = fadd double %16, %17 ret double %18 } ; Function Attrs: readonly uwtable define double @dot_product_vec([3 x double]* noalias readonly dereferenceable(24), [3 x double]* noalias readonly dereferenceable(24)) unnamed_addr #1 personality i32 (i32, i32, i64, %"unwind::libunwind::_Unwind_Exception"*, %"unwind::libunwind::_Unwind_Context"*)* @rust_eh_personality { start: %2 = getelementptr inbounds [3 x double], [3 x double]* %0, i64 0, i64 0 %3 = getelementptr inbounds [3 x double], [3 x double]* %1, i64 0, i64 0 %.idx.val.val.i.i.i.i.i = load double, double* %2, align 8, !noalias !1 %.idx8.val.val.i.i.i.i.i = load double, double* %3, align 8, !noalias !1 %4 = fmul double %.idx.val.val.i.i.i.i.i, %.idx8.val.val.i.i.i.i.i %5 = fadd double %4, 0.000000e+00 %6 = getelementptr inbounds [3 x double], [3 x double]* %0, i64 0, i64 1 %7 = getelementptr inbounds [3 x double], [3 x double]* %1, i64 0, i64 1 %.idx.val.val.i.i.i.i.i.1 = load double, double* %6, align 8, !noalias !1 %.idx8.val.val.i.i.i.i.i.1 = load double, double* %7, align 8, !noalias !1 %8 = fmul double %.idx.val.val.i.i.i.i.i.1, %.idx8.val.val.i.i.i.i.i.1 %9 = fadd double %5, %8 %10 = getelementptr inbounds [3 x double], [3 x double]* %0, i64 0, i64 2 %11 = getelementptr inbounds [3 x double], [3 x double]* %1, i64 0, i64 2 %.idx.val.val.i.i.i.i.i.2 = load double, double* %10, align 8, !noalias !1 %.idx8.val.val.i.i.i.i.i.2 = load double, double* %11, align 8, !noalias !1 %12 = fmul double %.idx.val.val.i.i.i.i.i.2, %.idx8.val.val.i.i.i.i.i.2 %13 = fadd double %9, %12 ret double %13 } Look similar enough(other than the different syntax from extracting values from tuples and arrays) - Rust is smart enough to unroll the loop so there is no performance hit. And if we are ever able to use consts as generic paramters, we could make things like `dot_product_vec` accept arrays of any size. 
&gt; Blow was never going to use Rust. Things like the borrow checker are antithetical to the way he looks at development. I know strictly the title here is 'graphics' not fully 'gamedev', but i'm addressing his exact use case, i.e. gamedev including consoles (including the most pathological past platforms ps2/360/ps3, and extrapolating that this experience may relate to other niches in the embedded software world, outside of games).
https://github.com/insanitybit/queue-delay-app And yeah - it's definitely intended to be actory. The pattern I use is MPSC or MPMC actors everywhere.
Yup, we basically take patches for the first edition, but don't actively work on it. Updating the post is a good idea.
I haven't kept up with the details here, but #228 is https://github.com/alexcrichton/futures-rs/issues/228 I think the solution is foo.boxed() turns into Box::new(foo)
It's technically possible to have a token whose owner is able to call non-thread safe function. For instance, this code I believe ensures `Token` can be only in a single thread at the same time while allowing for neat patterns like sharing a token in a mutex. This example depends on existence of only a single `Token` instance, so `Token` structure should be in a separate module to ensure it cannot be constructed using struct constructor syntax. If you are in environment which requires a particular thread (say GUI on Mac OS X which can only be ran in main thread) you also should remove `Send` bound from a `Token` which would prevent token from changing a thread. use std::cell::Cell; use std::marker::PhantomData; use std::sync::{Arc, Mutex, Once, ONCE_INIT}; use std::thread; #[derive(Debug)] pub struct Token { _no_sync: PhantomData&lt;Cell&lt;()&gt;&gt;, } impl Token { pub fn new() -&gt; Self { static INITIALIZER: Once = ONCE_INIT; let mut token = None; INITIALIZER.call_once(|| { token = Some(Token { _no_sync: PhantomData, }); }); token.expect("Token initializer function called multiple times") } pub fn async_unsafe(&amp;self) { println!("This function is not safe to call from multiple threads."); } } fn main() { let token = Token::new(); { let token_ref = &amp;token; token.async_unsafe(); token_ref.async_unsafe(); } let thread = thread::spawn(|| { let token = token; token.async_unsafe(); token }); // token.async_unsafe(); // not possible - token was moved let token = thread.join().unwrap(); // Token can be safely shared in a mutex let mutex = Arc::new(Mutex::new(token)); let mut threads = Vec::new(); for i in 0..10 { let mutex = mutex.clone(); threads.push(thread::spawn(move || { let token = mutex.lock().unwrap(); println!("Call {}:", i); token.async_unsafe(); })); } for thread in threads { thread.join().unwrap(); } }
Whoops! Thanks for pointing that out. 
That's a good idea. In my actual use case these points are coming out of rust-geo's geojson crate, so this would probably help. Is the convention here to make such an optional dependency available through a flag?
I considered doing this, but opted not to because arrays can only have one type in them, which is also how vectors work. Once slice patterns become available on stable, there's no usability difference. 
...unfortunately it also consumes the stream :(
Many of the operators -- not `&amp;` and `|` but certainly `+` and `&lt;&lt;` -- now have default definitions including "panic on overflow." You cannot merely port C-like pseudocode for bit manipulation. The standard library forces you to deal with a new abstraction `Wrapping&lt;T&gt;`. That's what I mean by pioneering: "How do I tell the compiler and std lib to get out of my way, allow overflow or whatever 'unsafe' behaviour I accept or actively desire?" For example, if you have a Wrapping&lt;u16&gt; accumulator and a u8 byte, you can't say `accum += byte;` because the std library has an implicit opinion about mixing wrapping and checked types: you don't. You have to write `accum += w(byte as _)` and that's the best I've been able to do without competly rewriting `Wrapping`. 
`by_ref` does not change the fact that `take` consumes the stream as well :-\ But maybe the manual polling could be a solution :)
&gt; In graphics programming you mostly[1] don't deal with external input and memory safety isn't that big of a deal. Anyone who's run homebrew on the DS appreciates that. When I think about it, maybe C++ development is a good thing for those.
But wouldn't it be better if the language you used *forced* you to deal with all of that before it compiles? Then you wouldn't have bugs like that years after release
The other crates don't have good documentation, and I never intended to use them. They took much less time to write. For this reason, I didn't really count them--they're just going be one of those many crates forever languishing at 3 downloads, hardly better than those hello world crates you link to. That's not to say this crate won't have the same fate, but I see it being actually used in practice. I hope I don't seem dishonest with the wording of my post. Above all, I wrote it that way to attract useful feedback from those who know how to make usable crates. 
Awesome work. I just downloaded it. Perhaps you could also make a script to automate the entire process :)
Is this what people call constructive criticism today? 
But `by_ref` gives `&amp;mut Self`, which is the `Stream` that `take` would consume (due to the `impl&lt;'a, S: ?Sized + Stream&gt; Stream for &amp;'a mut S`). So `by_ref` should basically turn the move into a mutable borrow, leaving the owned value in a valid state.
There it is! It wouldn't be a proper Reddit thread without someone throwing shade my way. In the future, please try phrase things in a more constructive way. You wouldn't speak to a colleague in that supercilious tone, and you shouldn't speak to strangers that way either.
he's talking about a user tweakable parameter. using the UI to enter a value that is NaN, when I want code that assumes no NaNs exist. This UI must be adjusted not to allow entering NaNs, or FoV's of zero that gets inverted (whatever crazy scenario he has found). (if its an FoV of zero... whats the game going to do? it's a DESIGN BUG. it takes action to avert this at the *design* level, to change the behaviour to prevent that e..g "minimum/maxium" values of the FoV slider.. a special UI element for a non zero value.. whatever.) key points:- * the bulk of the system should assume no NaNs exist, for maximum performance. * any entry of new values into the system must go through some check (i.e. interface with UI or network, or files). * any processing of safe values still requires empirically tested logic that wont generate NaNs. (e.g. you don't try to compute the normal of a degenerate triangle, you check its not degenerate first, etc)
Last week I did TWiR, wrote [more](https://github.com/rust-lang-nursery/rust-clippy/pull/1982) [clippy lints](https://github.com/rust-lang-nursery/rust-clippy/pull/1992), added a [suggestion](https://github.com/rust-lang/rust/pull/44104) to rustc, made bytecount `no-std`-compliant and blogged a bit. However, I didn't make much progress on my RustFest talk. In preparation I'll blog some general tips about public speaking and hopefully start the slides. I'll still do TWiR and may do some more stuff with Rust and/or clippy. Also I'm recently thinking about how to use parallelism (e.g. rayon) in [bytecount](https://github.com/llogiq/bytecount). 
http://rustjobs.rs/ exists, that's about the extent of where you may find new Rust work. Unfortunately, the market currently is rather thin. A lot of the current work is the type of work developers like: fresh code on new projects. Older languages have a lot of existing code that requires maintenance, and that's where a lot of the money comes from. If you just want to make money you may be better off learning how to maintain C or COBOL. It will be a while before Rust is going to have similar stability.
&gt; weird corner-cases Indeed :)
Ah. Hmm. I'll add that to the to-do list. (In practice, the to-do list is often "have a look at GEOS and copy their implementation, unless it's utterly terrible")
Ha! The pun genuinely wasn't intended.
&gt; The standard library forces you to deal with a new abstraction Wrapping&lt;T&gt;. That's what I mean by pioneering: "How do I tell the compiler and std lib to get out of my way, allow overflow or whatever 'unsafe' behaviour I accept or actively desire?" Putting the ergonomic issues aside, `Wrapping` is actually better for performance than the C equivalent. The C equivalent always invokes UB if you don't have a check. Rust's never will meaning that you can often elide a check. You also have the benefit of rustc giving LLVM the wrapping intrinsic and thus allowing for more (sound) optimizations.
its worth experimenting with library features, I should try the vector idea at some point etc
&gt;which I think is in maintenance hell due to some package permissions Which are these? I've had the best time using iron 
If you're looking at std::sync::mpsc channel, you may be interested in this article on [designing a channel](https://stjepang.github.io/2017/08/13/designing-a-channel.html) that was in a [recent](https://this-week-in-rust.org/blog/2017/08/15/this-week-in-rust-195/) "This Week in Rust". 
When I get a library conflict `library "X" is being linked by more than version of the same package` How can I see those conflicts. Currently using `cargo tree` but it is showing everything has the same version.
The function you're writing must dispose of the `Entry` value before returning. This can be done with a consume-method: `insert` or `into_ref` (or `into_key`). Otherwise it becomes the caller's responsibility to drop the Entry -- and that's not very helpful. Here it is in [my style](https://play.rust-lang.org/?gist=da430975f2d7620c88802a6c8af54365&amp;version=stable) and [yours](https://play.rust-lang.org/?gist=8bd07942eccbf1fdb6472a700dcd48d2&amp;version=stable). Point #2 is solved by getting the key from the function arguments, not the `Entry`. I'm not sure why it seems more obvious to me to do it that way, but it does sidestep confusion over when exactly the `VacantEntry` ceases to exist. If the `default` function consumes its argument, then it's not possible to have that value to store in the tree. Here's [a version with `Clone`](https://play.rust-lang.org/?gist=4e5a92b49e82c65800d4a1331841ce0f&amp;version=stable) so you can see how that might work. And here's one [where `default` only borrows the key](https://play.rust-lang.org/?gist=58c6fc3f04fb8381b601ebd2307022dc&amp;version=stable).
Yeah that's bad design - look what happened with all the `_r` variants that had to be added to POSIX functions. In Rust, it *really* doesn't make sense to call that a property of the function - the function is *inherently unsafe*, that is, it's *fundamentally* impossible to use it safely - the only potentially implementable rule I can think of is *disallowing* threading entirely in the rest of the program if you use it. Now if you make the function take an unforgeable token (`pub struct Foo { _nothing: () }`) created on *one thread* (via an `Once` check, for example) and which isn't `Send` or `Sync`, then you have added a (zero-sized) resource *symbolizing* the true global variable resource, and have successfully restricted the function and its use of the variable to one thread.
/u/dbaupp your suggestion [has been implemented](https://github.com/ucarion/line_segment_intersection/commit/2488a57895bcbcd08ece3c6be3a1f32f092077ea). Thanks again!
Not specifically a Rust question but one which should be easy to answer regardless. Is the ISO-8859-1 charset compatible with UTF-8? Its Wikipedia article implies that it should be but I'd like to know for sure.
These are fun and helpful. That's a great way to get peers to help you learn to fix ownership, borrowing, and lifetime errors for Rusty glory. EDIT: I just finished doing the challenges and wish there will be advanced challenges coming soon.
This is all very illuminating; thank you! :) EDIT: &gt; I'm not sure why it seems more obvious to me to do it that way, That is because I failed to notice that having `K: Copy` means that `key` is still available after passing it to `entry()`.
I've got this same problem. It's very annoying! Please report back if you figure out a fix/workaround.
It's a little difficult to navigate at first but this information should be in your `Cargo.lock`. Search for `X` (whatever it is) and see where it appears in a package's `dependencies` table.
Ah. I missed that you need to just borrow it. You can't exactly do that. I tend to write clients that take `self` and return a future of `(Response,Self)`
No all extended ASCII charsets are incompatible with UTF-8, only 7 bit ASCII is encoded in bytes is compatible. If you happen to have ASCII text that is encoded in any extended ASCII charset it will work however (since the upper bit is never set).
 C's signed types are very poorly behaved forerunners of Rust's `iint` types. They invoke UB on overflow instead of panicking. (Nice!) C's unsigned types are like `Wrapping&lt;uint&gt;` but better. Rust also has bound-checked `uint` types suitable for heading off overflow exploits (legitimately a nice feature) and `Wrapping&lt;iint&gt;` types that... I've no idea what they would be used for. If Rust were to do it over from the beginning, I think it should probably have had modular, non-negative, and signed integer types on equal footing with each other. The `io` traits, `as_bytes` and similar would handle [m8] data. Casting from non-negative to modular or small to larger would be considered safe casts. 
Thanks for posting. Can you share the reasoning for using a separated rocksdb instance for raft log? What are the benefits over a column family?
It depends on what you mean by "compatible". All ISO 8859-1 characters exist in Unicode and are associated with the same character numbers ("scalars"). However there are two problems. First: UTF-8 encodes every character past #127 as multiple bytes. ISO 8859-1 encodes all characters as one byte. Second: Microsoft abused the label "ISO 8859-1" as a synonym for "Windows-1252" so much that if you process 8859-1 text, you should expect it to in fact be 1252. (1252 has more printable characters in place of the C1 control characters of 8859 and Unicode).
&gt; casting from non-negative to modular or small to larger would be considered safe casts. For what its worth this is safe, and it would be backwards compatible to add implicit up-casts to the compiler. There are some wrinkles around up-casting to `usize` because the safe casts are platform dependent. The `Wrapping` ergonomics can be improved for similar reasons through stuff like [auto-ref for `Copy` types](https://github.com/rust-lang/rfcs/pull/2111), implicit up-casting, and enhancements to generics so that say you can always use a u8 on the right side of a shift operator. You can do the later now, it just involves a ton of boiler plate for the implementer of the `ops` traits. I'm hoping with integer generics that we'll someday be able to write operation generic over int sizes, e.g.: struct uint&lt;const bytes: usize&gt;; impl&lt;const left: usize, const right: usize&gt; Shr&lt;unit&lt;right&gt;&gt; for unit&lt;left&gt; { type Output = Self; // or maybe const math to ensure the right number of bytes fn shr(&amp;self, other: unit&lt;right&gt;) { // ... } }
[This table](https://en.wikipedia.org/wiki/Western_Latin_character_sets_\(computing\)#Comparison_table) actually just helped me actually realize what Wikipedia means when it says: &gt; ISO/IEC 8859-1 or Latin-1 is the most used and also defines the first 256 codes in Unicode Unicode and ISO 8859-1 codepoints are the same for `00` through `FF`, however UTF-8 encodes `80` through `FF` (and beyond) using two bytes, so it isn't strictly compatible with 8859-1.
Thanks. I've decided for my crate that handling anything other than UTF-8 input is left as an exercise for the user.
The tokio core can be built at any time before you build you client. I usually build it directly before building the hyper client, then store them together.
Safety in Rust isn't just about security vulnerabilities, it is also about productivity. In C++, I frequently got page faults and data races which took many, many hours to fix. It led me to give up on all but simple concurrency models and data flows in my projects. With Rust, I hack together whatever I want to, and as long as it compiles, bugs are local and mostly trivial to find. But maybe it's just that I am a terrible C++ programmer since nobody else seems to value this gain in productivity.
Thanks Steve!
[chessground](https://github.com/niklasf/rust-chessground), a chessboard widget for Relm/GTK. Having some fun with Cairo to draw everything.
I'm also working on my own after getting inspired by JB. I have a different vision too. I've been thinking about it for years and he finally gave me the impetus to go ahead and work on it. It's my main long-term hobby project now and I'm amazed at the progress I've been able to make after pushing myself. I don't have any delusions of creating something that will ever be popular, but it doesn't really have to be. I just want to create *my* vision. I don't think you should stop working on your project. Even if your project eventually fails in some respect (other than you giving up on it), you can still use the knowledge gained for your next project or your next collaboration. Maybe other people will recognise some of the merit of your design and use it in their own language/compiler.
I recently moved from hyper to reqwest specifically because I didn't need the async interface. It was a pretty easy migration, much easier than working with tokio directly if you just want to upgrade.
How good is `c++` support, is it possible to create `std::string` or `std::vector` and give it to `c++` code?
Including the rest of your code would be helpful. 
Neat, thanks.
heh is it on GitHub? i'd be vey curious to take a look , compare notes :)
`bindgen` isn't a C++ compiler, and can't create new monomorphizations of template functions or template classes' methods. It also doesn't support specialization, which is used fairly heavily in the STL. If you are working with some C++ function that returns template instantiations that already have monomorphizations in some C++ shared library or object file that you're linking to, then `bindgen` should allow you to shepherd instances or pointers to instances back and forth between Rust and C++.
For anyone else following along, I fixed some typos (like extra parentheses) and provided a minimal example of their code that produces the issue: main.rs: #[macro_use] extern crate nickel; extern crate hyper; use hyper::method::Method; use nickel::{Nickel, StaticFilesHandler, HttpRouter}; use std::io::Read; mod myMod { pub struct myMod { pub File: Vec&lt;u8&gt; } impl myMod { pub fn new() -&gt; Self { myMod { File: Vec::new() } } } } fn main() { let mut myMod = myMod::myMod::new(); let mut server = Nickel::new(); server.add_route(Method::Post, "/sendFile", middleware!{|req| req.origin.read_to_end(&amp;mut myMod.File).unwrap(); println!("You posted {} bytes", myMod.File.len()) }); server.utilize(StaticFilesHandler::new("./Client")); server.listen("10.0.0.4:8080").unwrap(); } Cargo.toml: [dependencies] hyper = "0.10.0" nickel = "0.10.0"
That's just for dogfooding. I understand why that choice was made, and I agree with it. But if I were writing a language, Rust would not be at the top of my list of choices for implementation language (nor at the bottom).
So, fundamentally, the issue is that you're looking at a closure here: |req| { req.origin.read_to_end(&amp;mut myMod.File)).unwrap(); println!("You posted {} bytes", myMod.File.len()) } A closure is an anonymous function. So, you declare this function. It *captures* (by reference) `myMod.File`. Now, what if the closure were run multiple times? They could even be running simultaneously. Rust does not allow you to have multiple mutable references to the same piece of memory at a specific moment in time. Eventually, if you tweak the code enough, you'll get this error message: error[E0596]: cannot borrow captured outer variable in an `Fn` closure as mutable &gt; I'd like to write to a struct outside of this method because I'd like for the lifetime of the vector to be the duration of the running time of the server. Rust doesn't see it this way. It even believes that `main` could exit before that closure finishes, since it isn't guaranteed that a closure will run serially. &gt; Is there a way I can write into the field of the struct or is there a better way to accomplish this? It really depends what your objective is. Why do you want the Vec to live for the duration of the program?
Plus, compilers have a lot of complex data transformation and little IO, so things like monadic IO are definitely worth the effort.
Thank you, that is much better! I modified it a bit so it would also return the filepath for the errored file, and was wondering if this was a good way to do it? #[derive(Debug)] struct FileProblem&lt;E: std::error::Error&gt; { file_path: PathBuf, err: E, } fn parallel_write(filenames: &amp;[PathBuf]) -&gt; Vec&lt;FileProblem&lt;io::Error&gt;&gt; { filenames .par_iter() .map(|path| { File::create(path) .and_then(|mut x| write!(x, "Hello from reddit!")) .map_err(|e| { FileProblem { file_path: path.clone(), err: e, } }) }) .filter_map(|x| x.err()) .collect() } fn main() { let mut filenames = Vec::new(); filenames.push(PathBuf::from("hello.txt")); filenames.push(PathBuf::from("hi.txt")); filenames.push(PathBuf::from("cat.txt")); filenames.push(PathBuf::from("dog.txt")); for error in parallel_write(&amp;filenames) { println!("{:?} failed with error: {}", error.file_path, error.err); } } I'm a rust noob, so still trying to figure out the rustic way to do things. Thanks!
&gt; If you are working with some C++ function that returns template instantiations that already have monomorphizations in some C++ shared library `std::string` is template instantition of `basic_string` and I am pretty sure that at least `libstdc++*.so` include compiled code that work with this specific instantion. So for `std::string` answer is yes? 
They could just make `slice::sort`, `cmp::{min, max}` all just require `PartialOrd` rather than `Ord` and make everyone happy. ;)
`ordered-float` seems a bit heavy weight. Is there a crate that defines `r32` and `r64` structs (with `#[allow(no_camel_case)]`) which behave like floats without NaNs with `debug_assert(!self.0.is_nan())`?
the vector is going to contain binary data that dictates the way the server should behave for, at the moment, the duration of the server. there will be other routes added to the server that will tell the server to perform certain functions based on the data in the vector. The plan is to eventually add code to delete that file and allow the server to put another in its place without having to restart the server The client is set up not to allow multiple posts to the sendFile route while the current file is active, but the server obviously doesn't know that. For reference, the server will eventually function as a console emulator, and the file will be the rom file. I have written several in the past in Javascript and Go using a similar structure, but decided to use Rust for this project because I needed a powerful and fast language that allows for execution of dynamically allocated memory. 
Also see [`quickersort::sort_floats`](https://docs.rs/quickersort/3.0.0/quickersort/fn.sort_floats.html).
Okay, so in that case, I think a [`RwLock`](https://doc.rust-lang.org/std/sync/struct.RwLock.html) might be a good choice instead of a `Mutex`. Something like this: let mut server = Nickel::new(); let mut myMod = Arc::new(RwLock::new(myMod::myMod::new())); server.add_route(Method::Post, "/sendFile", middleware!(|req| { let mut myMod = myMod.write().unwrap(); req.origin.read_to_end(&amp;mut myMod.File).unwrap(); format!("You posted {} bytes", myMod.File.len()) })); `sendFile` would be the route that "sets" the current ROM file. Other routes would use `myMod.read().unwrap()` instead of `write()`, where they would then receive read-only access. With `RwLock`, there can be either many readers, or one writer. If there are readers when you call `write()`, it will block until there are no readers. If you call `read()` when there is a writer, it will block until the writer is finished.
I'm putting together exercises for the series of workshops Mozilla is sponsoring in Brazil, so this is quite handy, thank you!
why does my small loop have a trailing % as the last print, but only when I use print! but not println! use std::io; fn main() { println!("Please enter a number to find its dividens"); let mut user_number = String::new(); io::stdin().read_line(&amp;mut user_number) .expect("Failed to read line"); let user_number: u64 = user_number.trim().parse().unwrap(); let mut v: Vec&lt;u64&gt; = Vec::new(); for i in 1..user_number{ if user_number % i == 0 { v.push(i); } } print!("{} is divisble by: ", user_number); for element in v.iter() { print!("{}, ", element); } } example: 72 returns 72 is divisble by: 1, 2, 3, 4, 6, 8, 9, 12, 18, 24, 36, %
Assuming it isn't using specialization at all, yes. If it is, then marking it opaque, so it is just represented as a bag of bytes, should do the trick.
This will probably work, the server will eventually function as a console emulator, and the file will be the rom file. For now I'm only implementing a single instance of the console, so, only one user will be able to send a post request with a game file to the server, and any additional requests will be ignored in the handler, ill look into the mpsc channel as well.
It is, but it's not very well documented and you can't do that much with it yet. It's hard to condense into a few sentences, but I'm going for a minimalistic approach that could maybe be described as "LISP with static typing and a richer syntax". Metaprogramming is done with macros/quote/eval and use the exact same syntax as the rest of the language. No keywords (fun, if, return, etc.) are special-cased in the parser, but are implemented simply as "built-in macros", which means you can extend the language in libraries and get exactly the same amount of power as those built-in primitives. I'm trying to keep the syntax intuitive and familiar to a certain base of developers (so e.g. infix arithmetic operators, C-like blocks and expressions, traditional if, for, while, etc.) as languages with too many strange sigils and unfamiliar conventions really put me off learning them. But there are of course trade-offs to be made in syntax. I'm also going for typical systems programming with a no/minimal/optional built-in runtime and no automatic GC, so ideally you'd be able to write an OS kernel or even stuff like bootloaders in it. I'm hoping the metaprogramming facilities will be so strong as to allow things like exceptions to be implemented in a library as opposed to in the base language, but that's far away yet and there's a lot to consider.
There's [noisy_float](https://docs.rs/noisy_float/0.1.2/noisy_float/), that defines `N32` and `N64` and also `R32` and `R64`, which also disallow infinities.
wrong sub. you want /r/playrust
This is extremely exciting!
I'm a big fan of generators, so this is very good news.
The `%` is a common way for your terminal/shell to indicate there was no newline character at the end of the output. If there were a newline, it would be natural for the next prompt to be on its own line. When there is no newline, a `%` is inserted to indicate the end of the stream, then the prompt is put on its own line anyway. Otherwise you could get behavior like the prompt starting in the middle of the line immediately following whatever was just printed. So using `println!` includes the newline, which means no `%`, while using `print!` has the opposite effect.
Hey, I just implemented this a few weeks ago, very nice! I'm very curious to read it all once I get home. For my problem, I was trying to find the intersection of lines, line segments, and rays, which is an easy extension, essentially just changing the interval of intersection from [0,1] to [0,inf) and (-inf,inf) for rays and lines respectively. Its actually very useful that the floating point format supports infinity for that particular reason!
They'd probably be upvoted more often if they weren't so combative, if they included fewer misunderstandings of things people have said to you, or if they were more willing to work with the rest of the community.
One Idea that would take a crap-ton of work to implement, and an another crap-ton of work to learn, could be [Dependant Types](https://en.wikipedia.org/wiki/Dependent_type). Being able to specify that a number should never be bigger than the array you're indexing or that a float should never be NaN moves the check to the numbers creation or when calling the function, where it would only need to be checked once for subsequent accesses. A precursor to dependant types would be integral generics. It's hard to implement `safe_unchecked_get&lt;N, I: LessThan&lt;N&gt;, T&gt;(array: &amp;[T; N], idx: I) -&gt; T` without being able to specify N as an integer.
**Dependent type** In computer science and logic, a dependent type is a type whose definition depends on a value. A "pair of integers" is a type. A "pair of integers where the second is greater than the first" is a dependent type because of the dependence on the value. It is an overlapping feature of type theory and type systems. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.26
This is very curious. If you try to do *anything* to read the value of `new_foo`, you get an error about the value being moved. You even noticed this, it would seem, because you're cheating with the `assert_eq!` at the end. Of course `bar.foo.id` is not equal to `2`, it was set to `1`, but if you try to do assert_eq!(new_foo.id, bar.foo.id); then you immediately get: error[E0382]: use of moved value: `new_foo.id` --&gt; src/main.rs:21:16 | 16 | bar.foo = new_foo; | ------- value moved here ... 21 | assert_eq!(new_foo.id, bar.foo.id); | ^^^^^^^^^^ value used here after move | = note: move occurs because `new_foo` has type `Foo`, which does not implement the `Copy` trait So, this is something that I think I've seen before, but it's been a long time. I think the reasoning is that the `new_foo.id = 2;` line is re-initializing the struct. There are some who would prefer to allow you to re-use structures after they have been fully re-initialized, and some who would prefer this to be consider an error. Right now, it's acceptable to re-initialize a variable previously bound to value that has been moved away, but you can't do anything with it. I would prefer to see this generate an error message, but since `new_foo` becomes write-only, the only possible harm is that you would silently drop a mutation that you expected to occur. Since this mutation is dropped *every* time, it should never pass unit testing unnoticed, let alone actual using of the software. [There is an issue here](https://github.com/rust-lang/rust/issues/21232) tracking it.
You can consider splitting the emulation code in a separate library, so other people could use it. Maybe someone would like to build a GUI for it and teach the kids assembly, for example.
And to be fair, the compile time thing isn't necessarily a language issue, but an implementation issue, so it *can* and *will* improve. What this means is that while Rust may not be ready for AAA titles *now*, it can be once either the language saves enough time to make up for the tools or the tools catch up. I'm hopeful that we'll have lots of improvements soon with incremental builds and other similar performance tweaks. However, until that happens, I'm going to mostly use it for things that don't need a quick iteration cycle or where rapid iteration can't be solved with scripting.
&gt; I'm going for a minimalistic approach that could maybe be described as "LISP with static typing and a richer syntax" Come on, you can't say that and not put the link :p More seriously, I saw some kind of static typed rust-inspired LISP some time ago which looked quite cool and I wondered if it was the same project or another one?
Hi all, i have one custom board with the lpc43xx chip and ftdi communication. but i got an error launching gdb: https://gist.github.com/elsuizo/eecedf30c935aafee6b8c122f16a13a0 https://gist.github.com/elsuizo/177772ae9b2d27d039decc31c14fcad1 Thanks in advance!!! 
I'm eagerly... erm... "awaiting" this feature. ;)
Oh, that's interesting. I guess I should know more about my own terminal's behavior! Thanks :)
That's a good point! In principle, I could add `interval_of_intersection: [f32; 2]` to `LineSegment` (which would have to be renamed), and it would support this use-case. If you want, you can open a PR for this! But the current API is very segment-oriented, and `LineRelation` handles cases irrelevant to infinite lines. I'd do this myself, but I won't be stress-testing this use-case. I'm not very good at computational geometry, so I don't trust myself to do it right on the first try, even with some test coverage.
Awesome news! So if I follow: an expirmental RFC was approved and an implementation has been merged, however before it will be allowed into stable another RFC will be opened first and must be accepted. Is this correct? If it is, is there a ballpark guess on how long until this feature would might show up in stable (assuming the second RFC was accepted).
Yes, that's roughly correct. This is the first one so the process is a bit fuzzy, but there certainly needs to be at least one more RFC. There's no good way to make a ballpark guess; the whole point of this is that *we don't know yet*.
There's [an RFC](https://github.com/rust-lang/rfcs/issues/1930) for a version of that, split into 3 sub-RFCs. Part 1 is on-going discussion; the latter 2 have been postponed.
Relevant StackOverflow question (with answer): https://stackoverflow.com/questions/36048741/why-does-compilation-not-fail-when-a-member-of-a-moved-value-is-assigned-to
Switching tracks on my attempt to make Way Cooler compatible with awesomeWM. Going to no longer build it as a separate library, as that's too difficult, and instead use rlua directly in Way Cooler in order to define the interface. This means I'm be building it more iteratively, eventually building up to full compliance with the default rc.lua _maybe_. 
Hell yeah! Rust just keeps getting more expressive. 
I gotta read and run this. Thanks for giving me something to waste my day off on.
The emulation is a separate library. The only Space Invaders specific code is in `src/bin/invaders.rs` which can be completely ignored.
It looks like coroutines, but is it asynchronous by default? Kind of confusing how would it look like to combine multiple asynchronous calls.
Could be worse: could be "left-pad".
From what I understand, coroutines are the new language feature. The `#[async]` macro will convert a function's body to a coroutine(`yield` in the right places) and then wrap it in an helper function that converts that coroutine to a `Future`. So the code you write to create an asynchronous function does not need to use coroutines syntax(`yield`) directly - but the macro will generate that syntax behind the scenes.
Well, I work on graphics programming pretty much exclusively these days. Even back when I was on the core team and involved with language design I was starting to do a fair bit of graphics. I don't think it's fair to say that Rust doesn't care about graphics/games programming. In a sense that's just factually wrong, in fact, as I was involved a lot in the design and I certainly cared about graphics. Like everyone else, I do have issues with Rust for graphics programming. That said: 1. Like all programming languages that aren't Java in the '90s, Rust has limited resources. The Rust teams are very big on prioritizing what the needs of their customers are, based on the feedback they get through the surveys and conferences and whatnot. A lot more users care about IDE support, for example, than MIR optimizations to eliminate bounds checks. We want to have everything, but the realities are that we have to prioritize. 2. Rust evolves really fast. The only languages I can think of that are as mature as Rust and evolve at the same pace are, like, Swift? TypeScript? Certainly C++, Go, JS, Python, PHP, etc. don't evolve as fast. It always feels to us deeply involved with Rust that it moves slowly, but Rust is actually quite well resourced compared to most other languages, especially the major open source ones that have been around for a long time. Consider that all the most popular dynamic languages not used in Web browsers still don't even have JITs in their primary implementations. 3. Everything that annoys me about Rust pales in comparison to all the little pains about C++ that just add up: no package manager, a huge choice of build systems that are all bad in one way or another, portability issues, verbose iteration APIs, error messages, memory safety problems, most vexing parses, include guards, capture clauses, verbose smart pointers, verbose header files, poor pattern matching, `-&gt;` vs. `.`, every project reinventing the standard library, lack of idiomatic wrappers for platform libraries, lack of standard naming conventions, worrying about undefined behavior, lack of libraries that are as ergonomic as `regex`, `rayon`, `nom`, Diesel, Rocket, `clap`, `serde`, etc. The grass just isn't always greener. So yeah, there are definitely issues with Rust for graphics. I'd put faster compiles, non-lexical borrows, and float sorting at the top of the list. (Note that I didn't think we made a mistake with float sorting, as all the options are basically bad, but we could definitely go further with ergonomic solutions.) Finally, I don't really think it's fair to compare *upcoming* *work-in-progress* C++ features like modules unfavorably to Rust while discounting Rust features that are in a similar state like const generics.
Agreed. Having a crate dedicated to equating two simple linear equations makes as much sense as having a crate dedicated to padding strings. This is what happens when you lower the barrier of entry too much. Good thing the broader programming community (rightfully) frowns upon negative patterns like this.
I'm ok with using Futures as it's the base of an async framework like that, but I wouldn't want this to use tokio. Tokio is controversial and complex. I really don't want rust's async story to be tied to tokio. There has to be better way of doing async io that what tokio currently offers.
The always interesting Request For Explanation podcast did an [episode](https://request-for-explanation.github.io/podcast/ep6-everything-and-the-kitchen-async/index.html) with Alex Crichton on this eRFC about a month ago. 
I'm working on a voxel-based procedural planet generator on my spare time. It uses the naive surface nets algorithm [described here](https://0fps.net/2012/07/12/smooth-voxel-terrain-part-2/) to generate the isosurface of a planet-shaped scalar field. I'm using octrees to implement basically unlimited LOD (in practice limited by floating point precision) so you can zoom in all the way from orbit down to the planet surface. Still rough around the edges, I've just offloaded chunk generation to a background thread so that you can get a consistent 60fps on the main thread. - Screenshots: http://imgur.com/a/m62hD - Video: https://streamable.com/h4jmq - Repo: https://github.com/coreh/universe Currently requires OpenGL 4.3, only tested on NVidia + Windows. 
The compiler knows nothing about Tokio, and will continue to not. What gives the impression otherwise?
https://github.com/PistonDevelopers/vecmath Does just about everything you should need.
As far as I can see this RFC doesn't add futures to the rust language. It merely adds a kind of python-style generators. So hopefully it will also be useful for constructing simple synchronous iterators in rust. EDIT: Here is a nice description of the feature https://github.com/Zoxc/rust/blob/a996d5eec70ba6733e23f2e56e762f58e60bb4ff/src/doc/unstable-book/src/language-features/generators.md
Is there a reason why closures can't be `.clone()`d, even if all the captured variable types implement `Clone` themselves? Or is it just a missing language feature for now? What is the best way to effectively "clone" a single closure so that I can get it executed separetely across multiple identical worker threads? I'm thinking about perhaps implementing a struct that manually "encapsulates" the captured variables, and passing clones of that to the threads via a Trait bound, but it would be less flexible and way more verbose than just cloning the closures.
Depending how how much you use them, a simple type alias like `type Point&lt;T&gt; = (T, T);` can be good enough. You can then rewrite things like `(isize, isize)` as `Point&lt;isize&gt;`.
To play with corutines it would be good to have async io, for example motivation code for this rfc https://github.com/rust-lang/rfcs/blob/master/text/2033-experimental-coroutines.md uses `TcpStream::connect` that can return `Async::Ready` and `Async::NotReady`, stdlib have no such functionality, so `mio`, `tokio` should be used to play with corutines to emulate real life examples?
There is an external implementation of futures using this coroutine support in the compiler. But that's the thing: it's an external implementation. It's just making use of the part the compiler does know about, which are coroutines/generators. The compiler knows absolutely nothing about Tokio. It just happens to be a nice fit, and so a lib exists to use the compiler features.
No. If you do this a *lot* and don't care about semantic messiness, you can define an enum to represent this: enum Either&lt;L, R&gt; { Left(L), Right(R) } fn thing() -&gt; Either&lt;TypeA, TypeB&gt; { ... }
Note that if you go this route you should use the either crate on crates.io, which is this type but with like a dozen bonus methods.
That works if it's two types. What if it's three? four?
Then you need three, four, more enums. As I said, you *cannot* do the thing you want. You'll just have to make do.
The implementation of async/await in the RFC mentions futures, so I'm assuming that means futures-rs. That library doesn't provide async io on its own, so tokio is the logical choice to provide io coupled with futures.
Thanks
I'm sure I'm not the only one, but I love these podcasts, and I get excited every time I see a new one pop up on my feed. 
I use [euclid](https://crates.io/crates/euclid) for simple game 2D stuff. There's also [cgmath](https://crates.io/crates/cgmath) which may be better when doing 3D game geometry.
I'm a game dev and I see this "we'll just avoid doing bugs and otherwise we'll catch them in testing" attitude all the time... and I'm calling absolute bull**** :) - First of all, you can definitely afford the checks. Unless it's really core code, but really, you can very likely afford them. There's people that make games in Unity, so a 5% hit is not going to tank your product unless you're a massive AAA. - Test doesn't catch even half the issues. If you actually want a bug free game, that's a lie we tell each other. On the other hand, nobody actually wants a bug free game I guess. - Finally, test costs (a lot) of money while Rustc works for free, so either way you should aim to minimize using QA as much as possible, either through better language, unit tests, functional tests, code reviews, etc. So yeah, you're actually correct that NaN and generally memory safety isn't a problem in games - but not because gamedevs are superhuman or controlled environments, just because gamers will put up with anything. I don't like that and I think that the "unpleasant strictness" that Rust brings would be good for gamedev, if gamedev was mature software engineering and didn't consider quantity of product over any other goal. /rant :)
There's an open issue for it https://github.com/rust-lang/rfcs/issues/1369 Something like this might work for now https://play.rust-lang.org/?gist=6c4aedae6abbfa01ab39acce0161d49e&amp;version=nightly
You can make a macro that expands to `Either&lt;Either&lt;Either&lt;A, B&gt;, C&gt;, D&gt;` for 4 types, for example. You can prolly use specialization to let you create a value of it directly from one of the 4 types. When you impl a trait for it you can usually handle just the `Either&lt;A, B&gt;` case and have the nested ones work for free FWIW this feature as `A | B | ...`, specifically for trait dispatch, has come up during `impl Trait` discussion and I'd be happy to have it in the language, just needs someone to RFC it (or if there an RFC, which there may be, to get it accepted).
You can use a Box for dynamic dispatch, but I don't know if this is considered good style https://rustbyexample.com/error/boxing_errors.html Alternatively the error-chain crate makes it easy to define custom error types https://github.com/rust-lang-nursery/error-chain 
You can try this https://github.com/Taymindis/atomic_hashtable It is been using for quite some multiplater games server, currently release for open sources.
And the macros that enable async/await on top of generators don't know about Tokio either, just futures.
Dependent types is good and all, and can provide static checks for index out of bounds, but in many cases you would have to manually prove this. To make dependent (or refined) types really smooth to use you need to combine them with an SMT solver, like it's done in F*, LiquidHaskell etc. This is very much an active research field though, probably not suitable for inclusion in Rust yet.
Woo! Gonna try this out if I can :)
I hope i can find some time this weekend to try this awesome work out! Thanks for all the effort you put into this. its a pleasure to see people having such a fun time and also come to such great results. I am very interested in the debugging part :) shoutout @ /u/perplexinglyemma :3
Thought `map` was iterator only :D meh
Looks good to me! For a more efficient implementation, I would probably avoid cloning the path, which means changing the `FileProblem` struct to borrow from the original path. struct FileProblem&lt;'a&gt; { file_path: &amp;'a Path, err: io::Error, } Note that I got rid of the generic error parameter since I think all errors here are going to be io-related, I could be wrong though. Given that change, the function would look like this: fn parallel_write(filenames: &amp;[PathBuf]) -&gt; Vec&lt;FileProblem&gt; { filenames .par_iter() .filter_map(|path| { File::open(path) .and_then(|mut x| write!(x, "Hello!")) .err() .map(|x| { FileProblem { file_path: path.as_path(), err: x, } }) }) .collect() } A little refactoring happened here with `map` and `filter_map` being collapsed into a single `filter_map`. It's interesting how much type-juggling you can do with Result and Option types. 
Side note: You don't want `isize`. Choose whether `i32` or `i64` fits for you problem space, but I have yet to find a use case where pointer-sized coordinates make sense (one also wouldn't use `uintptr_t` for coordinates in C).
I'm not the author, I just found this new blog post available through https://planet.kde.org/ aggregator.
It is not supercilious and I can speak to my collegues that way and they can to me. These are probably cultural differences between us. Essentially you have translated stackoverflow answer which involves trivial math to Rust. If this is not trivial then I don't know what is.
Well, writing into a value that you can't read anymore isn't unsafe anyway, is it? I guess it is useless to do so, but it would be better as a lint in my opinion.
Do you have more infos on these workshops? When and where will they happen?
Can the core be used to do something else? Is there a cost to them? Should I pool them or share them?
Try to write a simple mutable iterator but got "cannot infer an appropriate lifetime" error, code is [here](https://play.rust-lang.org/?gist=4c60e87095640d50d397ae12918d7696&amp;version=stable) but the immutable version is fine, [code](https://play.rust-lang.org/?gist=160351d60917e1bf2ca966416edc3df2&amp;version=stable)
That's simply not true. Padding a string is really easy: almost any programmer should be able to figure it out in a new programming language in minutes. This actually has some corner cases that need to be handled properly. Writing the algorithm (+ tests!) can easily take an hour, especially if you have no experience in graphics programming or geometric calculations. I would rather have people be able to download this single implementation, or use it as a reference for their own code, than have people reinvent the wheel all the time. Probably with bugs.
Thanks for the link! Really looking forward to playing around with this on Nightly.
cc /u/jinqueeny
Will do, I can't believe this isn't a common thing that people want / already documented..
You're looking for /r/playrust.
Public speaking tips will be very much appreciated! 😄
I'd say it's a fairly reasonable choice, especially now that the ecosystem is starting to mature, at least if you care about the performance of your compiler.
When will the playground be updated so I can try it out? Even using nightly locally doesn't seem to make the examples [posted here](https://github.com/Zoxc/rust/blob/a996d5eec70ba6733e23f2e56e762f58e60bb4ff/src/doc/unstable-book/src/language-features/generators.md) work. What's required to try this out?
Having a syntax sugar here is good for making future code less complicated. It's just like the ? operator.
I would definitely expect at least a warning for this.
Damn, I thought it was going to be an article. :-( Never seen a podcast on github.io before, maybe I'm just out of touch. :-)
I'd argue that finding the intersection of a line is equally as easy. Programmers that need to preform this calculation in their projects will be familiar with the associated edge cases, all two of them. If they aren't they'll probably mess up somewhere higher up the stack. Also, let's say for the sake of argument that these one-trick-pony crates are justified, where would you draw the line (hue hue hue)? Should we also provide crates that calculate rectangular areas? what about circles and spheres? should those be separated in their own crates? 
Nightly hasn't been updated since 2017-08-27, so this hasn't yet landed.
Hi, I'm a maintainer of rust-sfml. I'm going to investigate this over at https://github.com/jeremyletang/rust-sfml/issues/166. You can follow how this resolves over there.
I'll also mention since all the other popular ones have already been mentioned: [nalgebra](http://nalgebra.org/) has points and vectors (among a lot of other stuff) which is also used by [ncollide](http://ncollide.org/) which is also used by [nphysics](http://nphysics.org/). If you plan on ever using ncollide or nphysics using nalgebra for your points and vecs early could save some headache later.
If your mutable iterator would compile, we could `.collect::&lt;Vec&lt;&amp;mut String&gt;&gt;`. The resulting vector would contain three aliasing`&amp;mut String` instances. This is illegal, so the iterator you envision is not possible. Streaming iterators would solve this problem, but Rust doesn't really support them (they're not in the standard library and not very ergonomic). What are you trying to accomplish?
Turns out this is actually fixed on git master of rust-sfml. The problem was that `Transform` didn't impl `Copy`, even though it should have. There will be a new release of rust-sfml in September, and it will fix this issue.
The dates are off-by-one; info: latest update on 2017-08-28, rust version 1.21.0-nightly (e26688824 2017-08-27) that said alex just told me that the nightly failed last night, so it *is* gonna be until tomorrow.
When you do `Vec::with_capacity(read_size as usize)`, the capacity is set to `read_size` but then *length* is still 0. `ReadProcessMemory`, being a simple FFI function that works with raw pointers, does not change the length of your `Vec`, so when you later attempt to index your `chunk`, you're accessing elements that are beyond the length, despite being within the capacity. Try doing `let mut chunk: Vec&lt;u8&gt; = vec![0; read_size];` instead.
Emma presented this work at Akademy. The video is forthcoming. I'm looking forward to trying it out. https://conf.kde.org/en/akademy2017/public/events/394
As far as I know the RFC is mentioning async/await as something that can be implemented as a macro on top of the generator support in the compiler. I wouldn't be surprised if futures do make it into the stdlib, along with something like tokio-io but as far as I know that isn't happening anytime soon.
Yes, `&amp;*c` is definitely one of the most ugly and (at first glance) illogical syntaxes of Rust :) Is there an open issue about improving the diagnostics for that? I think that's easier than adding new logic/rules to deref. I'd love for `Path::new(&amp;c)` to work. In the meantime, you can also write `Path::new(c.as_ref())` in this case.
Afaik, Rust compiler can't enforce this statically. It should be impossible to write such a function in safe Rust, and the way you'd warn about this property is with `unsafe fn` - which is how external non-Rust functions are declared. The simplest way to protect a library that is not thread-reentrant is with a lazy_static `Mutex&lt;()&gt;`. let ret_value; unsafe { let guard = LIBRARY_MUTEX .unlock().unwrap(); ret_value = library_call(args); drop(guard); } This enforces the rule that only one thread will be in the library at once - good enough to protect C `static`s - by making each thread wait its turn. The next level of protection would be guaranteeing that only one designated thread ever touches a library. This can be done with a lazy_static `ThreadId`, and might be necessary if a library makes system calls into a driver that requires this. 
I might be wrong, but I think that's a cool idea and you should [submit an RFC](https://github.com/rust-lang/rfcs/issues/new)! :)
This is a valid complaint... when I first saw it I instinctively thought "eh that's not really much of a problem", then thought of an instance just yesterday where a deref coercion was not doing what I thought was obvious first off and I just grumpily fiddled around with variations of `&amp;*` until it did what I want. (Turns out `&amp;**` was what I needed.) Better error messages would be very nice. But might be tricky because there can be a lot of generic inference involved, maybe? Auto-derefing is very nice but when you push it too far it can easily end up in "do what I mean" territory where the compiler is flailing around trying to guess what the heck you're trying to do... which can often lead to even *more* obtuse error messages. And when a deref isn't actually a deref but just some arbitrary method that may or may not really act like a deref... can get tricky. Maybe we should just stop pretending `*` dereferences something and call it something else? I vote "splat", a la INTERCAL. :-)
Use euclid, cgmath or nalgebra. They're all suitable for games, and listed in more or less increasing order of power/complexity/theoretical coolness.
It doesn't, _but_ it provides the foundation to create the async/await pattern using procedural macros which could be provided by futures-rs.
Maybe not splat as thats what the expansion operators in python and javascript are called.
The only issue I have with this is possible namespace pollution. There are currently several libraries that deal with computational geometry. It would be preferable if all those crates offered this, but they don't. That's the reason the author wrote this in the first place. The name of the crate is too specific, so I don't believe this is squatting on a name. I can't imagine anything else using this name. The relevant code is 50 lines long. If I have functionality that spans over 50 lines and is useful in other places, I split it off in a function. The only dependency is the general `num-traits` crate, and it's easily compatible with other geometry crates. This crate can also be expanded in the future: * Multiple algorithms with different numerical properties, if they exist. * Also include 3D lines and (semi-)infinite rays. Or maybe even nD lines in the general case. That would completely solidify the existence of this library. Sometimes it's better to split off functionality, instead of hoarding it in a massive geometry library. I do believe the left-pad incident was a little bit ridiculous, but this incident can't occur in our ecosystem: you can't delete code from crates.io. So namesquatting is not a problem with a name this specific, you can't have anything like the left-pad incident, and there is possibility for growth. I really see no reason to not allow this. Don't use it if you don't want to, and do if you do. We have the functionality to share compilable code easily with a central repository, why not use it?
I found [this](http://www.1024cores.net/home/lock-free-algorithms/queues/bounded-mpmc-queue), it’s a bounded MPMC algorithm from the same site that the rust MPSC implementation references. [It was also a part of the standard library at some point](https://github.com/brayniac/mpmc). It's probably better to base my implementation on this instead. It is safe to use get_unchecked() and ptr::write() to make sure we never panic or call drop?
No it doesn't. * No useful conversions (`From` trait impl's) * No separate point and vec types, a distinction which is more useful than you expect * No Rect type, which is useful for 2D stuff * No quaternions or other dedicated rotation maths * As far as I can tell, no shortcuts to construct useful matrices (projections, rotations, etc) * Irritating C-style type-specific functions instead of methods and traits (I mean, implementing `Add` and `Mul` and such for your vector types seems a no-brainer) * Last significant change was 2 years ago, so it will probably never improve unless you're the one who wants to improve it (that's right, YOU, the person reading this)
Call it the "squished-bug" operator then, maybe?
The effect of your code is: - Subslice to remove the leading and trailing elements for which `pred` is false. And I believe /u/Nijaitchy may have wanted: - Subslice to remove the leading and trailing elements which are equal to the first and last. 
I replaced it but it didnt really change anything, the function still fails and returns 0 and the chunk/buffer stays full of zeroes only
`skip_while` produces an iterator, otherwise yes you're on the right track.
Wow, great to know this is being worked on then! Haven't used impl trait before, would this be a rough equivalent in stable rust? (With dynamic dispatch) https://play.rust-lang.org/?gist=a44f34b930d5c46925080158dca7cd3c&amp;version=stable If so it is really clever, since the "struct" becomes implicit with the outer function arguments.
&gt; * No quaternions or other dedicated rotation maths Yeah I just noticed that a few minutes ago, I wouldn't have mentioned it if I had known when I saw this thread.
What do you mean exactly by "asynchronous by default?" Generators as implemented here are pretty close to the traditional understanding of coroutines where multiple functions run on the same thread and co-operatively yield control. These generators strike me as being very similar to [wren Fibers](http://wren.io/fibers.html). One of the examples is almost identical!
You might also find this useful: https://github.com/luser/read-process-memory
It looks like the current async/await only uses yield without a value, however I saw that in the RFC it mentioned the ability to yield a value. It seems like yielding a value could be used to implement a Steam, is this possible in the new nightly? Ideally it would look something like this (ignoring some slight syntaxes errors): // I think this is the right return type. It would get converted to Stream&lt;Baz&gt; #[async] fn foo(stream: Stream&lt;Bar&gt;) -&gt; impl Iterator&lt;Result&lt;Baz&gt;&gt; { #[async] for bar in stream { let baz = bar_to_baz(bar); yield baz; } } 
You have to indent four spaces, reddit doesn't support triple backticks. (I don't know the answer to your question)
I'm not sure what they want exactly, but surely passing the predicate `|e| e == input[0]` or something along those lines would have desired effect for your interpretation?
Whoops, thanks! That what happens when I write it from my phone. 
You also need to use [as_ptr\(\)](https://doc.rust-lang.org/std/vec/struct.Vec.html#method.as_ptr), probably, to get a pointer to the actual data in memory, rather than a pointer to the Vec (which has a pointer to the actual data)
yeah that was the problem, I got it working now, unfortunately it is awfully slow gotta think about optimizing this now somehow Thanks for the help!
I have started to work on a pretty big project called [Chorus Studio](https://github.com/Lisoph/chorus_studio). Essentially it's an open source clone of [Ohm Studio](https://www.ohmstudio.com/). TL;DR: Open source clone of the Ohm Studio DAW.
nvm compiling it in release makes it fast enough
thanks for reply. i use coroutines in unity3d but i wondered how are they different from generators. i didnt use coroutines outside unity so i may lack understanding. what would be a typical use for coroutines and why current solutions are not enough? i also checked fibers article, very clean, thanks!
What would be the issue with an iterator in this case? 
Ah, cool, I was trying to find if there was a recording or slides or anything some time ago without success. Apparently I just need to be patient. :)
if you need to make it faster, try using "with_capacity" and "set_len" rather than initializing the values to a useless value first.
If the first and last elements are not equal?
[Happened here](https://reps.mozilla.org/e/dive-into-rust-senai-santa-cecilia/), will [happen here](https://reps.mozilla.org/e/dive-into-rust-thoughtworks-brazil/) and hopefully will happen a lot more, there seems to be a lot of demand!
We're just too lazy to come up with a better publishing mechanism.
Apologies in advance if you already know most of this! So right now the primary method of concurrency in Rust is via OS threads. Lately, an alternative has arisen in the form of Futures that can be scheduled into a `tokio_core::reactor::Core` or a `futures_cpupool::CpuPool`, or maybe something else down the road that does cool multiplexing of them across threads kind of like Go's goroutine scheduler does. Threads have the problem of being rather heavy and requiring a lot of time-consuming context switching, so you don't want to spin up a lot of them. Futures are a lot lighter and can be scheduled independently of OS threads. The annoying thing is where you have to build up your "futures chain" with lots of `.and_then(|r1| { // do something with r1}).and_then(|r2| { // do something with r2})...` calls. If you've done much with futures/tokio/etc., You've probably seen how cumbersome this gets and how it can get difficult to manage borrowing, mutability, and lifetimes in the chain. Enter Gernarators/Coroutines. At their core, they're a lot more powerful than basic Futures since they can return values mid-computation and then jump right back into the same point, so they can be used as a new form of iterator like in the classic [python example](http://intermediatepythonista.com/python-generators). In the context of Futures and the proposed async/await features, they let us build up something that's, at the end of the day, pretty similar to the old `.and_then(...)` chains, but much much **much** more ergonomic to write. You can think of each 'await' as the divider between the `and_then` blocks of old, since both represent a point at which the current future could return control back to someone else because some IO thing isn't ready yet. So what does that have to do with Generators? Async/await is all just sugar around generators, wrapped up in Futures. Where you see `await` in an async function, it's actually just checking the Future that you're waiting on, `yield`'ing if it's not ready, and returning an `Async::NotReady` result from the wrapping Future's `poll` method. And then when the scheduler decides to try this Future again, it jumps straight back to the point in the generator where it previously yielded and can try the `await` again. TL/DR: We can actually do a lot of what generators give us with the async/await use-case today, albeit nowhere near as ergonomically. See this [javascript example](http://voidcanvas.com/explaining-async-await-es7-examples/) of their implementation of async/await vs the old "promise chaining" method (which is almost identical to our current "future chaining").
Thanks! I'll be in Rio next month for a couple of weeks, so if you happen to know any Rustaceans there let me know, I'd be happy to meet some.
IIRC there are plans ("asref coercions") that will fix this.
Cool, thanks for the answer.
Generally you'll want one core per application - it acts as the 'handler' for all the async / futures operations on one thread. The general idea with futures / tokio is that all your IO work is done on the tokio thread, and any CPU intensive stuff can be put onto a thread pool which is still linked to that same tokio core. You'd start the application or service by creating a tokio core and hyper client, then using `core.run` on the `Future` which drives your program until it's competed.
`&amp;*` violates my personal style. Alternatives: - Call `.as_ref()` explicitly. - `let var: &amp;Target = &amp;source` - Call `.deref()` or `.deref::&lt;Target&gt;()` explicitly. Preferred techniques are `.as_ref()` and `let` assignment. This has several advantages. - It's better documented for Rust newbies: `as_ref` is in the standard prelude, `deref` is in the standard library index, `&amp;*` is not. - The explicit `deref` says what type you want. `&amp;*` vs `&amp;**` only say how to get what you want. - It doesn't look like bad APL. (Admittedly a matter of taste.) (edit: `as_ref` is in the prelude, making it preferred)
I can't see the gifs for some reason.
&gt; There's people that make games in Unity, so a 5% hit unity: C# 'application code' sitting on a C++ underlying engine. I am of course referring to the underlying engine, not the C#-esque use-case. &gt; Finally, test costs (a lot) of money while Rustc works for free, **Rustc isn't free.** You pay up-front in *extra annotation*, and *searching for the right helper-functions* to do things which you can do by composing basic operators in other languages (it is documented that some pieces of code that are known to be safe can be rejected by the borrow checker). **if it could prove safety without extra work on my part I wouldn't be complaining**. The point is it doesn't stop me having to write tests *for other reasons*, so I'm sometimes forced to pay a 'debugging cost' twice. A chunk of the testing can be automated: games sometimes train their AI by playing themselves (hell, today, we have people training general-purpose AI by playing games), or run their levels through pre-rendering tools for lighting / optimising streaming, or to check the frame rate. If you've got the infrastructure for all that setup, running part of that in debug/check mode is hardly difficult. But ultimately human testing *can't* be avoided, because the fundemental purpose of a game is a subjective response in a player, not merely completing some objective mechanical task. Now, the current crop of CPUs aren't as difficult to use as the last wave, but I assert: *that any language aiming itself as a C/C++ replacement must consider every possible niche in which C/C++ has been used* . Machines and niches evolve: computers 'get faster' but they also get *shrunk* to be placed in new resource-constrained scenarios (i.e. running on light batteries for [[drones]](https://community.arm.com/iot/embedded/b/embedded-blog/posts/how-to-bring-mobile-processing-power-to-robotics-and-drone-designs), [IoT](https://en.wikipedia.org/wiki/Internet_of_things)). Those extreme performance hazards may recur. There's a trend toward generalised vectorisation (enabled by [scatter/gather instructions](https://software.intel.com/en-us/node/523826)). &gt; didn't consider quantity of product over any other goal. "'quality/budget/time - pick 2" There's always a tradeoff. It just happens that in game-dev timescales can matter a lot, e.g. you might have to cut corners to get something out in time for a marketing campaign or christmass, or ahead of rivals .. whatever. Gamedev has it's own optimal tradeoffs. If 'being 100% bug-free, and onetime' mattered most, then of course you'd use a GC'd language. My perspective comes from a scenario where performance on [awkward machines](https://www.gamespot.com/forums/playstation-nation-1000002/which-is-initially-harder-to-develop-for-the-ps2-o-25700792/) was of the utmost importance. That might not be the scenario you are working in now, but Rust must be able to handle every C/C++ use case if the world really wants a replacement for C/C++; There are flaws with C++ that are nothing to do with unsafely. My motivations for looking for a replacement are completely different.
You can use [`ManuallyDrop`](https://doc.rust-lang.org/std/mem/union.ManuallyDrop.html) to prevent automatic drops from occurring. As for panicks, it depends on the specific implementation.
If you're looking for byte signatures, may I advertise my crate [pelite](https://github.com/CasualX/pelite)? It works with the binary files on disk rather than ReadProcessMemory'ing it. It's pretty simple to use (I hope) and supports really nice signature scanning facilities so it may be of use, see the [examples](https://github.com/CasualX/pelite/tree/features/pattern_examples/examples) to see what is possible.
Links to the videos are at the end of the article (but I also had the same problem)
Interesting, thank you. Is there a way to use the master branch through Cargo?
&gt; I don't really think it's fair to compare upcoming work-in-progress C++ features I mention these because I do point out where I'm aware Rust *is* going to get features (I know Rust does want constants in type-paramerters etc... - it's just being aware that *both* languages have a roadmap. C++ went unchallenged so stagnated for a long time , but with rivals gaining ground, I think it has given them a bit more imputus to actually improve. I do accept that Rust being younger can move *faster*, that was part of the hope I had coming to it earlier. I need to draw some sort of overlapping projected S-curves here; rusts 'rate of feature gain' can be much higher, C++ is further along .. higher but levelled off.. but C++ already has a few extra features that matter to me , so there's an unknown in the projection. C++ has made enough progress recently to keep it in the running.
Not without import of std::ops
What is your approach to GUI in such an interface-heavy program?
hehe ok. I'll bear it in mind for the next one. 
those are indeed interesting , and I realise that Rust (being younger &amp; more modern from the outset) has a higher chance of getting something like that than C++. I have certainly not 'rejected Rust out of hand'.. I continue to dabble with it. What I *haven't* been able to do is make a switch to it as my 'primary language', which is what I was really hoping it to be. 
The reason I've been using i/usize is that I often use the coordinate components to index vectors and slices. Why is it better practice to store them as integer types and cast for indexing?
We'll just have to wait for them to post the desired specs so we can be sure of the intended behavior. I think the supplied code example should at least be able to serve as inspiration for their own solution. 
Generators, also known as semicoroutines, differ from fibers, or "full" coroutines, in that generators are stackless whereas fibers are stackful. That means that when you call a generator, you don't need to allocate a separate stack for it, since it can run on the "top" of the stack of the OS thread. On the other hand, stackful coroutines have their own stack, which means that you have to do a memory allocation when you start one. That makes stackful coroutines a bit "heavier" than generators, but still not as heavy as OS threads. However, generators have the downside of being less flexible. They can't yield from a deep stackframes, because the stack, sitting on the top of the caller stack, must be unwound for the caller to be able to continue execution. On the other hand, stackful coroutines can yield from anywhere, because their stack is on the heap, and doesn't block anybody else from continuing.
The reason I've been using i/usize is that I often use the coordinate components to index vectors and slices. Additionally, one of my projects is implementing [Infinite Minesweeper](https://github.com/Inityx/minesweepfinity), in which I thought it was appropriate to use the platform's native size for indexing. 
The implementations of push() and pop() dosen't ever drop or panic, but to be absolutely sure that doesn’t happen, I want remove those code paths from binary completely. This means we assume Options are empty when Nodes are empty, so no need for drop the old value and we skip bounds checking. This would be safe and equivalent to C++ implementation right?
There's an issue for this, can't link on mobile, but it's trickier than it might sound because of generics which are compiled when you compile your binary, not when you compile your deps. So in particular std collections and iterators and all that would still be compiled at your level. 
Well you can't remove it from the binary, since someone else might want to panic. I don't know what you mean by the rest of this comment, I would have to see an implementation.
&gt;Why is it better practice to store them as integer types and cast for indexing? You didn't ask me, but the obvious answer (which may or may not be right) is that now you have different behavior based on the hardware you're running on, which probably isn't intended -- you're tying your coordinate space to your pointer size, which is arbitrary and feels kind of wrong. You can always do `num as i64`, or something. 
There's an open RFC issue for this kind of feature, [RFC 294](https://github.com/rust-lang/rfcs/issues/294).
[RFC 294](https://github.com/rust-lang/rfcs/issues/294) has been a thing for a loooong time :P
Yes, you can use [dependencies.sfml] git = "https://github.com/jeremyletang/rust-sfml.git" EDIT: I failed to mention that the master branch requires Rust 1.20, which will release on September 1st. You can either use nightly or 1.20 beta until then.
Your dreams will come true soon-ish https://github.com/rust-lang/cargo/issues/1359 
That allows matching so not really, it's probably [RFC 1154](https://github.com/rust-lang/rfcs/pull/1154) that's closer in spirit to what I'm talking about.
I wonder if this is the same as the problem that we recently ran into with Askama: https://users.rust-lang.org/t/troubles-creating-generic-function-interface-for-iterables-their-references/12595 It seems similar in that coercion through references doesn't seem to work right for function arguments. I really think this kind of thing ought to work, as it seems like a bit of a weird cliff effect in the type system: something works if you keep the code together -- but put part of your code in a well-typed function, and it doesn't work anymore.
[Here is what i want to do] (https://gist.github.com/henninglive/adb28728a5fac8e42e7880175dc20e07) 
This is still unhelpful. Are you using `ptr::write` to avoid dropping the previous value? You can do that safely by using `mem::replace` and `mem::forget`. You clearly aren't using it because you only have access to it via a raw pointer (which is the normal use-case for `ptr::write`). Also, are you sure that the indexing checks are a bottleneck? Have you benchmarked both? LLVM can often elide bounds checks. If you can use 100% safe code you can have much more confidence, so it's important to know that the tradeoffs are worthwhile.
I imagine that's because, globally, web developers outnumber the rest by an order of magnitude.
Thanks this also looks like it will work, its likely only a single thread will be parsing the ROM at any given time so a mutex seems unnecessary.
I'm working on deadlock detection for [parking_lot](https://github.com/Amanieu/parking_lot/pull/39).
Your right `mem::replace()` and `mem::forget()` is probably better. Speed is not the problem here, it’s safety. I am calling `push()` from a signal handler where [most system calls are unsafe](http://man7.org/linux/man-pages/man7/signal-safety.7.html). Panics and Drops will not work correctly in this context.
That's what I'm working on currently. I'm rolling my own hardware accelerated GUI solution, based on Glium for rendering. I think writing a blog or reddit post, explaining how it works, wouldn't be a bad idea, since it's pretty interesting in it's concept. I strongly seperate the layouting and widgets from another, for example, and the layouting is inspired by HTML's box model. At this point it's mostly an experiment tho. The GUI system could theoretically even be extracted into it's own crate. This is something that I'm planing on doing in the future. So far the basic layouting system is implemented. Next step is adding text rendering based on rusttype.
[Hmm?](https://play.rust-lang.org/?gist=f200d67d5806996d2c43aaa19fd9b714&amp;version=stable)
Left-Middle-Right is sometimes used for three options. The same as two eithers.
Hey, did you have any luck with this?
You sound like you need [an `Either` type with `N` variants](https://github.com/cramertj/EitherN-rs)! ;)
Presently we have been using https://github.com/AtherEnergy/rumqtt . It isn't async, it's blocking threads using a callback. The MQTT3 protocol crate that it uses is sound but the API design is irritating. Otherwise the client lib has performed quite well. The same author is working on https://github.com/rumqueue/rumqttd which is a broker and, despite the similar name, is a complete rewrite using Tokio. The current focus is on the broker, but it turns out you need to implement about 95% of a client lib in order to implement a broker. So a solid Tokio based client lib would spin off of that. My recommendation would be to use rumqtt in the short term, and help build rumqttd (+ client) into a solid product. That is my plan. 
I should point out that, for clients, a polling/non-blocking sockets approach is rarely necessary, since the number of threads involved should never exceed 2 or 3. It is also quite easy to run rumqtt in a thread and have the callback emit messages into a MPSC (or whatever) queue that could be easily consumed by futures. 
How about a "squished-bug" emoji?
I completely agree. Rust's strengths are also a barrier in certain areas (in the same way Haskell might be). Much of my time working in reverse engineering game servers using Rust was spent wrangling around Rust semantics and exhaustingly long compile times to experiment. I think the former is not really something the Rust project should be shooting to solve, however. There _are_ better languages for that kind of work, and most of the time you don't need the benefits of Rust there. Not that C++ is one of them. The compile times issue for me was more egregious. And also the lack of integer generics. I'm not even doing anything particularly fancy, I just want all the traits applied to arrays of all sizes where applicable.
I'm too lazy to publish even as much as that, so I can't comment. :-U
Sounds like a smart move. UTF-16 or UTF-32, _maybe_, but anything other than that and I'll just say "good luck" and awkwardly sidle out of the room.
It kind of works though. You could think of an `Option&lt;T&gt;` as a sequence of either 1 or 0 elements, which would mean that `Option::map` is really the same thing as a `map` over a sequence.
What's a good way to express a loop over all possible byte values, while we wait for inclusive range syntax to arrive? I currently have the below, but I'm not keen on having that cast lying around, I'd like to do everything in `u8`. for a in 0..256u16 { let byte = a as u8; } 
I don't see a particular emoji for it... maybe we should replace `&amp;*` with 👏🐛?
Those are... all probably pretty good solutions. But writing `&amp;*` is neat! It feels like pulling the tablecloth out from under the set table without disturbing the vase of flowers. :-P
For the benefit of the uninformed: could someone please explain how exactly this works (why &amp;* doesn't just "cancel out")? I know I've had to use it in the past but it was more of a 'hey this compiles it must be right' than actually understanding *why* it worked.
The counter (within the iterator's state structure) needs to be bigger than `u8` because otherwise it's not possible to distinguish "not yet started" from "now finished". [The cleanest solution in Rust is this.](https://play.rust-lang.org/?gist=e23bab946c7fdc5571bfe45f2e6ecdbf&amp;version=stable) The same problem comes up in C or assembly. The idiomatic solution was to be clever with your loop variables and jumps. In 8-bit pseudo-assembly: move 0 -&gt; C loop: // loop stuff add 1 -&gt; C jump_not_zero loop BASIC and C have `do ... while` loops for exactly this reason. Rust doesn't, but you can kinda fake it, being [good](https://play.rust-lang.org/?gist=3730fb3f35064e81d3d4cc530e9308a3&amp;version=stable) or [evil](https://play.rust-lang.org/?gist=7f3c60dfb93b3a22e3ec91f9f566e44c&amp;version=stable) or [taking pains to not overflow](https://play.rust-lang.org/?gist=0622b741e601d2814a02e95ccf717980&amp;version=stable). 
Drop by our Telegram group and ask around! https://t.me/rustlangbr
Plans, but no RFC yet?
Da hora! I hope Rust adoption keeps growing in there.
As best as I understand - and this is just programmer level without knowing the compiler intimately: &amp; puts its argument in lvalue context - something that computes a memory address (plus length) rather than computing data. * in this context doesn't try to load data, it *only* converts between reference types. This is just like how `*a_ref = value` stores to the referenced location; it's not a load. It would be pointless in C - loads data to a temp location, returns address of that location. Rust's * operator (technically, I think it might not be an operator, "lvalue path component" maybe) is much smarter than C's. This is related to why Rust doesn't need `-&gt;` for structure field access and things like that. 
Has the "delay" blocked you from executing other work?
&gt; &gt; No separate point and vec types, a distinction which is more useful than you expect [`nalgebra` distinguishes betweeen a lot of stuff](http://nalgebra.org/quick_reference/) like vectors, points, translations, quaternions, and more.
As an aside, you can also skip the compile step by using [`cargo-script`](https://crates.io/crates/cargo-script). Just make sure your script has a `#!/usr/bin/env run-cargo-script` hashbang, and remove the file extension, and you're good to go. *This even works on Windows.*
thanks, now i think i understand it! what about tokio? could it be replaced by coroutines? tokio uses threads, but coroutines would be simpller to manage... would there be still need for a asyn library or it would be easy enough to write everything with coroutines?
That's still doing a cast though, it's just moved up into the loop header. I'll go play around some more.
If you have some data structure `SelfBorrow` where `refer` may be a reference to `value`: #[derive(Debug)] struct SelfBorrow&lt;'a&gt; { value: u32, refer: Option&lt;&amp;'a u32&gt; } And you initialize it like this: let mut self_borrow = SelfBorrow { value: 10, refer: None }; self_borrow.refer = Some(&amp;self_borrow.value); `self_borrow` can be read immutably, so `println!("{:?}", self_borrow)` works and returns `SelfBorrow { value: 10, refer: Some(10) }`, but if you try to move it, maybe by assigning to another variable like `let x = self_borrow;` you will get a compiler error: error[E0505]: cannot move out of `self_borrow` because it is borrowed --&gt; src/main.rs:25:9 | 21 | self_borrow.refer = Some(&amp;self_borrow.value); | ------- borrow of `self_borrow.value` occurs here ... 25 | let x = self_borrow; | ^ move out of `self_borrow` occurs here You can also have a `SelfBorrowMut` with a `refer: Option&lt;&amp;'a mut u32&gt;`, and while you can compile `self_borrow_mut.refer = Some(&amp;mut self_borrow_mut.value);`, you won't be able to read or write to it, since it is mutably borrowed. So to answer: &gt; Does the process of moving and borrowing invalidates any underlying ownership and reference in a struct? No, there is no invalidation of references when a struct moves, the compiler won't be allow you to move the struct nor break the borrowing rules. 
This sounds like it could be a potential clippy lint.
I was going to suggest the same thing. This removes the whole headache about sharing binaries too (and the compiled hook script will be cached on each machine).
The type of `&amp;*x` is always `&amp;T` for some `T`, but `x` can be anything that can be dereferenced, e.g. smart pointers like `Box&lt;T&gt;`, and so the combination is a way to explicitly get a reference to the contents of a smart pointer. (If `x` is `&amp;T` then it does cancel out/do nothing.)
Somehow, reading this made me go from being annoyed by the pattern to liking it myself
Well, contrary to most people on the thread, it doesn't really bother me. The compiler is pretty good at telling you if you need it or not. And on the other side it sometimes teaches you what you are doing (that `iter().find()` is borrowing twice!). I wouldn't like having to write `.as_ref()` as it is much longer than `*` or `**`. Teaching the compiler to properly deref is ok of course.
What is the purpose of this post? Are you trying to "alert" the community to cherrypicked version of a particular person's actions, or maybe to the precedent the mod decision sets and just happening to target the subject in the crossfire?
Transparency; establishing the fact that the mod team has no problem with sexism/bigotry as long as it's "on your time" and directed to the groups disfavored by liberals in the USA.
"Kill all men" is maybe a little less funny if you suffer from suicidal depression regarding social issues like this
Look, as much as I dislike those tweets, I think the Rust mods did the right thing: comments denigrating any group are unacceptable in this channel, but there's no need to go trawling through someone's history to find out all they've done outside. That aside, I don't think this post should be here at all because it contributes little to Rust the language.
What can I do with a trait that cannot be made into an object? error[E0038]: the trait `serde_json::ser::Formatter` cannot be made into an object --&gt; formatter.rs:7:5 | 7 | formatter: Box&lt;Formatter&gt;, | ^^^^^^^^^^^^^^^^^^^^^^^^^ the trait `serde_json::ser::Formatter` cannot be made into an object | = note: method `write_null` has generic type parameters ... I have a piece of code where I would substitue a `impl Formatter for CompactFormatter` and `impl&lt;'a&gt; Formatter for PrettyFormatter&lt;'a&gt;`, so I thought using Formatter or Box&lt;Formatter&gt; would work. Do I have any recourse, or is this the way Formatter is designed and I cannot use it this way?
No, there are a class of rough ideas in this space which will eventually RFCify i guess.
I'd be a little more sympathetic if they hadn't also threatened to ban me over an outside social media post that was merely making fun of the CoC as opposed to violently targeting whole groups of people.
Personally, I don't mind the syntax in itself. What I mind is that 1) the error is quite confusing the first time(s) you stumble on it 2) I find it a bit inconsistent. I mean in my mind whether you declare a function as taking `&amp;str` or a generic parameter which includes `&amp;str`, it should be the same way to call it for a variable that can be dereferenced to `&amp;str`. If there was no deref coercion at all, I guess I'd get used to it, but it's weird that sometimes you need it explicitly for cases that are really quite similar to the cases where you don't. 
For those who aren't aware, Ms. Williams is a member of the Rust community team in addition to her role in Node.
It's because of a quirk of the Deref trait. When a type A derefs into a type B, it doesn't mean A is *actually* a pointer to B, it just means that it's kinda-sorta a pointer if you squint at it. A function or method that requires an actual pointer to B won't accept an A, so you have to do the `&amp;*` dance to shuffle the types around: - you have some value of type A, a sort-of pointer to B - you apply `*`, giving you a value of type B - you apply `&amp;`, giving you a value of type `&amp;B`, an actual pointer to B 
I, for one, have almost always viewed "splat" as a synonym to "asterisk". That's what it was called in *C++ For Dummies*, one of my first programming books. Basically what I'm saying is that I'm fine with this.
It is possible to overload `*` with the `Deref` trait, typically for containers (`Box`, `Rc`, ...). So if x is `Box::new(42)`, `*x` will be `42` and thus `&amp;*x` will be similar to `&amp;42`, not `&amp;Box::new(42)`. In many cases you don't have to explicitly use it, as the compiler will do it for you (deref coercion). E.g., if you have a function that takes a `&amp;u32`, you can give it `&amp;Box::new(42)`, you don't have to give `&amp;*Box::new(42)`.
For a very long, and slightly dated, anser: http://cglab.ca/~abeinges/blah/too-many-lists/book/
Is this something I'd use if I want to create a Rust program that interacts with a softwares C++ SDK/API? I've heard Rust and C++ don't play too great, we have a bunch of VFX tools that only provide C++ APIs, am I going to have a bad time if I want to call a couple methods with Rust, is this something Python or another language would be better at?(I've noticed bindings for things like Qt/QML, OpenCV, etc are a bit lacking(and a strong point of Python), not sure if this is due to community or language, or both). I loathe Python and would really prefer something like Rust for this sort of thing if I have to implement the wrapper/binding myself. Alternatively we just hire a C++ dev I guess?
It's good to know, thanks! 
Sorry for the late reply, @arthurprs. Using a dedicated RocksDB instance now is to get ready for a more efficient engine to store Raft log in the future because of write amplification in RocksDB.
This would also be a good case for `duct`
Would generator support improve iterators? It's been a while, but I remember wanting to use iterators for processing a stream of data that wasn't able to implement the iterator trait due to how state was handled requiring a while loop calling a function instead. I think the problem was to do with borrowing/ownership of the data structure being iterated on and the current way to implement an iterator didn't lend well to what I was doing(but I did want to process it like an iterator with take/skip/etc).
The cast is necessary because you have more than 256 states. (257)
I can't answer your actual question, but I think this will help you: https://github.com/dtolnay/erased-serde
I can't say for sure without seeing the problem, but it's likely that your problem stems from the fact that the iterator trait requires the iterator items to have the lifetime of the underlying iterable, or be static. (No other lifetimes can be named in the context of Iterator trait) The problem with this is that you can't express an iterator whose items live only a single iteration. If you could, these kind of iterators are called "streaming iterators". I think that generators suffer from the same limitations. The problem is with the inexpressibility of the type system: you can't define generic associated type items. However, there is an RFC that is attempting to fix this. It has been recently accepted and is waiting for merge: https://github.com/rust-lang/rfcs/pull/1598
Yes, Box&lt;T&gt; was what I was thinking of. That makes sense now, thanks.
I had a hell of a time a few months ago trying to figure out how to avoid `&amp;*`. Any tips on how it could be done better?(GPU lib and CPU native rust versions) [Playground example](https://play.rust-lang.org/?gist=9192e1db18346721556c4a68e91272e1&amp;version=stable) I remember trying a few other variations, one of which might have worked better but due to borrowchecker even though it should have worked wasn't going to happen, non-lexical lifetime support in future might make that feasible. As it currently is, it looks god awful to read/maintain, any help appreciated :)
&gt; It feels like pulling the tablecloth out from under the set table without disturbing the vase of flowers. :-P As I mentioned in [my response](https://www.reddit.com/r/rust/comments/6wrcr3/would_there_be_a_way_to_fix_the_pattern/dmbakvj/), I didn't feel that way :P Granted newish to Rust, was quite frusting to figure out the incantation to make the compiler happy :( Doesn't look like code I'd feel proud of.
match generator.resume() { GeneratorState::Yielded(x) =&gt; {}, GeneratorState::Complete(x) =&gt; {}, _ =&gt; {} } Hmm... seems a bit verbose. Here's my pre-pre-RFC: if yield x = generator.resume() { } // or even if yield x = generator() {
[I've already started experimenting with it.](https://twitter.com/mgattozzi/status/902730041603354625) I'm super excited to try this out and do some fun stuff with it.
If you read my commented out versions you'll see similar for prior CPU approach that just uses Rust: ` *a -= *b + *c; *a ^= *c &gt;&gt; x[0];` The active method includes comments showing actual fn signature and actual computations when used with ArrayFire library. I couldn't use those as it wouldn't compile on playground so they're commented out and replaced with a compilable version. You'll find that what you propose doesn't work with ArrayFire and the approach I used was neccessary :( Would love to be proved wrong of course :) a/b/c in the GPU version are &amp;af::Array iirc, the library provides references to the C objects or something I guess.
Looks like a few "expected" things are missing at the moment (which is fine, of course): * function style generators * generators with arguments * using generators as iterators I'm sure such things are easier to work on after the giant PR got merged in-tree :)
I think what's happening is that it casts the `usize` to `u64`. Essentially your call is actually `u64::max_value()`which means `decomped_bc` can never be bigger than it which is why clippy is throwing the error. If you're worried about the types not lining up due to their size you can check the size it takes up in memory using [`mem::size_of`](https://doc.rust-lang.org/std/mem/fn.size_of.html). You could do something like: if mem::size_of&lt;u64&gt;() &gt; mem::size_of&lt;usize&gt; { return Err(ErrorKind::DecompressLargerThanWord.into()); } I think this is what you might be looking to actually do since it seems you need it to work with a `u64`.
I think clippy is just assuming the pointer size is the same as the host's. That lint could probably be improved. Generally, the way to check the size of the pointer type is to use `if cfg!(target_pointer_width = "32") {}`. I think the check you have is pretty good at expressing the intent, though. You could also do `decomped_bc: usize` perhaps?
&gt; I can't say for sure without seeing the problem I believe this was the snippet I wanted to treat as an iterator: [Playground snippet](https://play.rust-lang.org/?gist=92430e21508bd55f36e5d28edcc16dde&amp;version=stable) I'm not able to compile it right now, but the references to self(impl trait on a data struct) probably don't help. It's also returning references, whereas I think an iterator internally wants to own the state/data and provide references to the consumer? Usage was to iterate until a stop condition or no more data was left(None). I wanted to have some configurability where I could compose/chain several iterators together into one to cater to several use cases. I think a highly anticipated feature to do with impl trait(can't recall name or syntax exactly) that will arrive in Rust within a year or so is meant to help with this type of situation, did something with the data and provided a method to return an iterator I think while avoiding the present issues with some compiler magic/smarts. &gt; If you could, these kind of iterators are called "streaming iterators". I recall that term! I think this is what someone said I was wanting, but when I looked into it, still had some trouble implementing. &gt; you can't define generic associated type items. However, there is an RFC that is attempting to fix this. It has been recently accepted and is waiting for merge: https://github.com/rust-lang/rfcs/pull/1598 That may very well be what I was referring to above, I also recall a lengthy blogpost about better type expressions and the problems they'd solve, that RFC looks similar. HKT(Higher Kinded Types) I think along with some other fancy name :) --- So in short, it's probably not an easily(at least desirable) problem to solve with Rust right now but could be in the near future?
https://youtu.be/9ZVzT0zx_pk?t=2m22s ?
[removed]
Okay, thanks!
This is amazing, I can't wait to try this out a bit. 
Thanks for the tip. cargo-script looks fantastic!
So in that case the problem is indeed the self references. (Btw. I spoke in my earlier post about "the lifetime of the iterator" but actually what I meant is the lifetime of the underlying iterable. The self references here make it impossible to implement Iterator. Streaming iterator would help as it makes it possible to return an item with the lifetime of of &amp;self, and the type system would ensure that the lifetime of the previous item has ended before calling next() the next time. You could make it so that the iterator only takes a reference to the Array, which allows the returned items to live longer, and the lifetime would be expressible in the current type system. However, there are a few buts: you can't add new items to the array during the iteration, unless that array somehow implements internal mutability. Also, you must store all the generated values until the end of the lifetime of the iterator. If the lifetime is unbounded, and you don't use the old values for anything, you have a memory leak. (Or not an actual leak, but... you are using memory for nothing) If you want to have such an iterator, consider using an arena as your backing storage. Streaming iterators remedy these problems by allowing only a single value to exist at a time, so you can always reuse the memory space, and mutate it inside the iterator, as it isn't borrowed yet during the next() call. Edit: impl Trait doesn't help with expressivity here, it helps mostly with ergonomics and API versioning story. Only thing it does to improve expressivity, is that it allows having iterators of unboxed closures.
Btw. I'm super excited about the forthcoming type system features such as generic associated types. impl Trait, and const value generics. Fingers crossed that we'll see these on the nightly by the end of the year and hopefully stabilised next year! (Disclaimer: I know nothing about rustc development so this is just my wishful daydreaming :D)
And Ruby. `*array` and `**hash` are super fun
Your description is what the OP wants to happen. As they said, if usize is 32 bit, the max value will be less than the largest u64 and the comparison will be able to succeed. The 64 bit case can never fail, true, but the runtime error being checked here is entirely impossible on such platforms and so the code is correct: expressing the condition as it is avoids needing special cases. In any case, your code will always return the error on 32 bit platforms, which isn't correct: it should only be returned if the actual dynamic value of the `decomped_bc` can't fit in a usize.
I was very eager to try this but sadly I experienced another show-stopper in my own project due to custom attributes and the *proc_macro* requirement ([#39347](https://github.com/rust-lang/rust/issues/39347)).
Thanks! I like how you commented everything. It made for a nice read.
Interesting, makes total sense. I guess FIFO compaction would work reasonably well for that though. Thank you. 
From the look of it, rumqtt uses mqtt-protocol, not mqtt3. But thanks a lot for your insights, I'll go and try to implement a client with rumqtt.
Oh, I see. If it's application code (not a library) you can use `#[no_std]` and override the panic functions. An infinite loop would work.
Exactly :3
Most welcome. The FIFO compaction style may also be a good choice, but WAL and SST files might cause duplicated IO for this situation, and we just need WAL.
One more question: how do you use this ? The version published on crates.io is outdated wrt the docs on github (the example doesn't compile). Do you use the version from crates.io and a local docs repository, or do you clone a github snapshot and if yes, do you have a stable ref to share ?
You could use [unfold](https://docs.rs/itertools/*/itertools/fn.unfold.html) from the itertools crate. let all_bytes = itertools::unfold(Some(0u8), |s|{ let ret = *s; *s = s.and_then(|n| n.checked_add(1)); ret });
Wow, just came across this. Thanks so much!
What interesting problems can be solved with stackless coroutines? So far I have only used stackful coroutines for a task system. I am not sure if there are problems that can only be solved with coroutines. To me they seem mostly to simplify some code. I could also see that some problems may also be solved a bit more efficient with stackless coroutines. 
Does Rust really need more implicit coercions? While they can sometimes be convenient, I fear they would make the language harder to learn.
Hi. I've stopped developing rumqtt because there is no way to implement TLS safely by reading incoming messages from one thread and publishing from another. I gave a try with tokio but combinators and error messages were too painful. Generators landed in nightly yesterday and I'm going to give it a try and see how it goes :)
Sometimes you don't need something as advanced as an ARM. You might just want to control some LED's via a switch. AVR's fit that job perfectly
Another way to do the check is to do the usize conversion, then convert back to u64 and make sure you get what you started with: fn decompress(&amp;mut self, decomped_bc: u64) -&gt; Result&lt;()&gt; { let decomped_bc_usize = decomped_bc as usize; if decomped_bc_usize as u64 != decomped_bc { return Err(ErrorKind::DecompressLargerThanWord.into()); } let mut decompressed_bytes: Vec&lt;u8&gt; = Vec::with_capacity(decomped_bc_usize); 
Awesome! I think it will even be better if we can just write `...?` instead of `await!(...)?` [as per your suggestion](https://github.com/alexcrichton/futures-await/issues/3#issuecomment-322419631). `await!(...)?` feels unnecessary and redundant to me. If we do it that way, I don't even see the need for a separate `#[async_await]` macro. The `await` macro itself still makes sense to keep when one doesn't want to propagate the error, eg:- match await!(...) { Ok(result) =&gt; { /* do something with the result */ } Err(error) =&gt; { /* handle the error */ } } Both styles can coexist within the same `#[async]` block. Of course one could still do `await!(...)?` if they prefer but I hope we will have a lint for that.
Thanks. I'll try to do the same for the Formatter trait in serde-json.
 #[cfg(target_pointer_width = "...")] where `...` is `32` on 32-bit platforms and `64` on 64-bit platforms. #[cfg(target_pointer_width = "32")] fn decompress(&amp;mut self, decomped_bc: u64) -&gt; Result&lt;()&gt; { return Err(ErrorKind::DecompressLargerThanWord.into()); } #[cfg(target_pointer_width = "64")] fn decompress(&amp;mut self, decomped_bc: u64) -&gt; Result&lt;()&gt; { let mut decompressed_bytes: Vec&lt;u8&gt; = Vec::with_capacity(decomped_bc as usize); 
Speaking for clippy, we always go with what the target spec defines (the machine you compile stuff *for*, not necessarily the machine you compile *on*). This is by design. The canonical way would be to use `#[cfg(any(target_pointer_width = "16", target_pointer_width="32"))]` to only include the check on non-64-bit systems. Once the `try_from` feature is stabilzed, using `usize::try_from(decomped_bc).is_error()` would be my preferred solution.
I don't really have a lot to add here but I thought I would mention that issues like this make me want tooling that helps a programmer figure out how to convert from one type into another. This is especially true when learning a language. I often find myself thinking gee I have this Foo and the function I want to call takes a &amp;Bar and I know that a Foo can be converted to a &amp;Bar I just don't know how. Perhaps a tool could help search the potential paths to get from one type to another? I doubt this is a 100% solvable issue but even something that does a search and says 'Sorry, couldn't help you' would be useful.
Could this same model of converting coroutines into a state machine be used to implement F# computation expressions or haskells do notation in an efficient manner? The reason I ask is it seems like a more generic approach with potentially fewer pitfalls. C# falls prey to a few pitfalls with its async/await approach that fsharp protects you from ([async gotchas](http://tomasp.net/blog/csharp-async-gotchas.aspx/)) and while I'm guessing the proposed rust approach doesn't have these same issues it would be nice if rust got a more powerful system out of it.
What does this give over an iterator? I'm trying to understand generator use cases.
F# already does this for its `seq` expressions, but that's special-cased – all other computation expressions are naive, including `async` expressions, unfortunately.
It's to make things like iterators easier to implement, not necessarily easier to consume.
I use `unfold` _all the time_ in F#... I've always wondered why it isn't in the stdlib for Rust – is there some idiom I'm missing that makes it unnecessary (aside from nightly-only generators)?
Hexathorpe.
I believe stackless coroutines are much lighter on the system, as there is potentially no allocations, or just one or two small allocations to keep track of the Task. Stackful coroutines need to allocate a pretty big blob of data (the stack) and I believe are heavier when switching from one to another. On the other hand, stackful coroutines make it easier (or possible I guess I should say) to implement MxN processing. This, however, seems to complicate the system for Rust a little bit due to how the language works, as most things on that coroutine should be Send+Sync so they could be passed from one thread to another. I'm no expert in this topic, but I believe the reasoning towards stackless coroutines was something along those lines. The discussion can be found on the eRFC for async/await.
I understand that. However what would you recommend to use in the interim, even if not 100% safe ?
great although im against using generators in a library until the api stabilize... it was released yesterday, give it few days or make an option not to use it so when the api changes the library wont be broken
It's necessary if we do it _in this way_, but that's why I was looking for other ways where we _don't_ need the cast. E.g. I could write an iterator type that tracks a `u8` and a `bool` (or an `Option&lt;u8&gt;` probably), no casting required. Maybe someone knows another, nicer, approach.
Thanks!
What is the difference between stackful/stackless here? Is it even possible to implement coroutines without saving full cpu context onto stack (on x86 _PUSHAD_ instruction)? Discounting _naive_ (Knuth) coroutine implementation. 
Nice idea! There's a bit of dancing around with `ret` and the reference, but I wonder if I could build a helper method around `unfold` to streamline things a bit. IIRC `checked_add` isn't on a trait, right?
It definitely looks like something I might start using a lot!
It's just a branch.
Very exciting! I'm trying it out, but already finding the limitations on borrowing to be difficult. Is the expectation that a better solution to that will be figured out before this stabilizes? Or are we really going to have to sacrifice methods that take `&amp;self` to use this for the foreseeable future? IMO it's a pretty awful trade off. From the caveats section: &gt; You'll either need to take self by value or defer to a different #[async] function. It'd be helpful to see an example of deferring to a different async function. async is infectious, so if your `&amp;self` method is calling an async function, doesn't it still have to be async itself? I don't quite get how that can work around the issue. Compiler errors from inside code generated by the proc macros are also difficult to work with. The span it shows is the `#[async]` annotation, but you see messages about a type signature that appear elsewhere: error[E0310]: the associated type `&lt;E as ruma_api::Endpoint&gt;::Request` may not live long enough --&gt; src/lib.rs:136:15 | 1 | | //! Crate `ruma_client` is a [Matrix](https://matrix.org/) client library. | |___^ ... 13| #[async(boxed)] | _______________^ | = help: consider adding an explicit lifetime bound `&lt;E as ruma_api::Endpoint&gt;::Request: 'static`... note: ...so that the type `impl futures::Future` will meet its required lifetime bounds
We had [`unfold`](https://doc.rust-lang.org/1.2.0/std/iter/struct.Unfold.html) but it got removed.
The `checked_add` methods are defined directly on the primitive types.
Stackful coroutines allocate their stack on the heap, which means that you can resume them from anywhere. (Even nested coroutines) At least that is what I think it means.
For me at least, the biggest pain of implementing iterators in Rust is handling the 3 cases: `move` vs `&amp;` vs `&amp;mut`, how do generators improve this?
If you handle where to store the stacks you can build stackfull coroutines on top of stackless ones as a library. E.g. https://play.rust-lang.org/?gist=82d1422cdb0539d5d145910160b670f9&amp;version=nightly
I'm speaking from my experience with coroutines in C++, F# sequence expressions, C# iterators, etc. I've not yet used them in Rust, so there are surely practicalities and impracticalities that I'm not yet aware of. I should say, I certainly _**hope**_ they ultimately aid with implementing iterators in Rust... ;-/
How would this compare to https://github.com/edef1c/libfringe? Is there any benefit from using stackless coroutines to implement stackful coroutines? Portability maybe?
I'm pretty sure that you don't know how stack/callstack/stackframe [actually works](https://stackoverflow.com/a/40106523/882600) (and [here](https://en.wikipedia.org/wiki/Setcontext) is how you would implement coroutines in POSIX C via CPU context saving/restoring).
&gt; Is the expectation that a better solution to that will be figured out before this stabilizes? The expectation with all of this is "play around, report issues, we'll check it out". This is all extremely experimental.
Nope, I've opened a feature request on pdcuses :) 
It seems like you could write down such a trait; is that just an optimisation thing or am I missing something?
I've updated my comment above with an actual working example :) &gt; How would this compare to https://github.com/edef1c/libfringe? It is basically the same thing, but the compiler does all the work for you. You just write `|| { ... yield x; }` and be happy with it.
Have you consider using a C library, paho comes to mind. 
Why? A stack is just a data-structure. A program can have multiple stacks, and it can implement their own and allocate them wherever it wants.
Nope, I was hoping to have the least external dependencies possible (apart from OpenSSL perhaps). I think that if I have to use a C MQTT client lib I'll use C for the entire project.
Implementing iterators in Rust is trivial when compared with iterators/ranges in C++, even when the C++ ranges are implemented using the coroutines TS. A Rust iterator is about 1-10% LOC of a C++ range. The main difference with C++ is that in Rust one often wants to implement an iterator for 1x ownership and 2x borrowing: `move` (when you move the type into the iterator, e.g., a vector that is consumed), `&amp;` (when you iterate over the range without mutating it), and `&amp;mut` (when you iterate over the range while mutating it). So... if one could abstract over ownership/borrowing, one could cut the LOCs in Rust by 3. That would be a significant improvement. Otherwise, I don't see how generators can simplify writing iterators that much, since the iterator logic in Rust is already pretty minimal.
What I really want is a feature similar to Haskell's [applicative do notation](https://ghc.haskell.org/trac/ghc/wiki/ApplicativeDo), in which some monadic expressions can be made concurrent whenever they don't have data dependencies. In the context of Rust's async-await, it would be something like: let x = await!(test1)?; let y = await!(test2)?; // rest of function If y depends on x, it compiles as today; if y *doesn't* depend on x, the two async computations happens concurrently. For the case where x and y are independent, the implementation can be something like this (I'm not sure if this compiles..) let (x, y) = await!(test1.join(test2))?; // rest of function
Yes thanks for clearing that up!
I'm glad you enjoyed it!
It depends what kind of iterator you are implementing. Let's say you want to implement the `chain` adaptor: you need to think about all the possible states your iterator could be in (iterating first iterator, iterating second iterator, completed) then you need to manually convert that to an enum and state machine. With generators, you can just yield all the elements from the first iterator, and then yield all the elements from the second iterator. The compiler automatically extracts the state machine from your normal control flow and use of local variables.
The Servo team is also trying to make Servo run on iOS: https://github.com/servo/servo/issues/18154 I believe it requires the devices to be jailbroken though.
I do have a repo with changes. I'll make it available. TLS has not been a requirement for us (we use mqtt.js and websockets for device -&gt; server and rumqtt for server -&gt; server) /u/kteza1 - I'll see what it'll take to add TLS support to rumqtt. but I think we should work with https://github.com/rumqueue/rumqttd/ to make a complete client and broker. It really is not far off. 
Strictly speaking that is true, stack is indeed just a memory mapped somewhere in VAS. His comment &gt;Stackful coroutines allocate their stack on the heap, which means that you can resume them from anywhere. doesn't actually say anything and the implication suggests that he doesn't understand the topic. Don't really wanna go into details on how does stack actually work on different architectures (stack/register machines) operating systems and their executable formats (PExe/Elf/a.out), suffice to say that it's not that simple. Feel free to explain to me how his implication makes sense (in context), I'd definitely like to hear it. Also feel free to explain how does your comment makes any sense as a reply to my comment which basically said "I don't think you understand how stack works".
Yes, stackless coroutines are implemented within the compiler as a code transformation, so no context switching is needed (the code transformation converts the async code into synchronous code implementing a state machine, and happens prior to register allocation or any codegen). This is possible due to two related limitations: 1) The stack frame size of coroutines *at any suspension point* is finite and known at compile time (stackless coroutines cannot recurse into themselves without a heap allocation). Importantly, between suspension points, the stack can grow freely. 2) You can only resume a coroutine which you have a mutable reference to. This combined with the previous point means that the space needed to store the coroutine's stack frame was allocated as part of the coroutine object itself, and that coroutine object may well exist on the stack, so no heap allocation at all is necessary. 
Why isn't there a dot product?
Thanks. Should have reread [this](https://www.chiark.greenend.org.uk/~sgtatham/coroutines.html) before asking, "stackless" went against my understanding of how coroutines that I have implemented in past worked (setjmp/longjmp, setcontext and their assembly counterparts).
That's a concern I have about async/await. As my company has moved from promises to async/await in Javascript the code bases have suddenly become a lot less concurrent. Promises are slightly more ergonomic to use `all` rather than threading previous results through, async/await flips the equation. The benefits are totally worth it though. I think rust has a better chance of fixing it because futures are cold.
The standard library doesn't really include a lot of numeric traits. The choice was made to leave such things to the crates.io ecosystem.
&gt; doesn't actually say anything and the implication suggests that he doesn't understand the topic. His comment says it all. Every single programming language with stackful coroutines as a language feature stores the stack of each coroutine on the heap using a stack data-structure in one form or another (e.g. segmented stacks). That is, you have the program stack, which is the stack of the main program thread, but then you also have a collection of stacks in the heap, and you can use it to store the stacks of each coroutine. That way, you can use a thread-pool to execute millions of coroutines concurrently, by loading and storing their stacks on the heap. 
Sounds interesting, do let us know how it goes with updates!
One step at a time! I'm personally hoping that this is a *significant* step forward from where we were previously, but time will tell. Thanks for the suggestion for the README, I'll add an example. As for the error message, thanks for that as well! I've got some idea of how to improve the spans and I'll see if I can get them implemented soon
https://www.bignerdranch.com/blog/building-an-ios-app-in-rust-part-1/ is from around the 1.0 timeframe, and so doesn't use rustup, but is an example of this. There was also a game on the app store; "A snake's tale".
Dunno.
I was following you right until this part: &gt; loading and storing stacks to the heap You don't actually store/load stack, you (re)store CPU context (registers IP, SP, BP, ...) which is basically crux of my argument here and I don't believe you can use these terms interchangeably. There is no point in saving/loading entire "stack" onto/from heap is there. And to be fair I am arguing about terminology here, your comments are _essentially_ correct of course. With that squared away, his implication doesn't really make any fucking sense and I'm not sure anybody has actually read it before explaining how wrong I am. &gt; Stackful coroutines allocate their stack on the heap, which means that you can resume them from anywhere. I have genuinely no idea how can you conclude latter part of that sentence from earlier part, but hey, english isn't my primary language.
&gt; You don't actually store/load stack Except that, _yes, you do_, with stackful coroutines – that's what makes them _stackful_. ;-] A stackful coroutine needs to allocate memory for its runtime stack, and consequently the context switch is more expensive compared to a stackless coroutine, but this allows the coroutine to be suspended from within a nested stackframe. EDIT: my grammar is terrible this morning...
We need folks to experiment with it though, in order to get feedback on how well it actually works in practice.
Great read! Comments are really helpful to get the reasoning behind the code :) However, to be sure that all the work has been done, shouldn't the loop (at line 376) that counts down the received results be done on `jobs_total` instead of `jobs_remaining`?
Stack can grow to arbitrary lengths, are you saying that you load/store, for sake of the argument, 10GB stack. That's ridiculous, that 10GB stack is mapped somewhere in the virtual adress space and all you do is set ("load") few CPU control registers so that you can resume execution flow. I have no idea whether we speak the same language at this point. And feel free to replace that 10GB with 10 exabytes if you want.
&gt; Auto-derefing is very nice but when you push it too far it can easily end up in "do what I mean" territory where the compiler is flailing around trying to guess what the heck you're trying to do... which can often lead to even more obtuse error messages. I predict this will get even more "fun" when we get also auto-`ref`ing (in `match` patterns).
This is the runtime stack for local variables that we're talking about. Who the hell puts 10GB on the stack? In Windows the default stack size for a new thread is 1MB... For coroutines, the compiler should be able to calculate the required stack size, and it's typically on the order of bytes, not kilobytes or megabytes or more.
Just FYI, you can totally use Promise.all with async/await in js, because it just returns a promise.
Yes, you're absolutely right; I'll fix that.
Current implementation switches between publishing and receiving on a single thread using timeouts. I'll try to fix the examples by tomorrow. 
Would it be possible to have a crates.io release with matching docs ?
The compiler treats indexing as an arbitrary function call, so it doesn't know that these two rows don't alias. The simplest workaround would to use indexing everywhere (so no `for block in row`), but you can also use the [`split_at_mut`](https://static.rust-lang.org/doc/master/std/primitive.slice.html#method.split_at_mut) function here: let (rows_above, rows_beneath) = rows.split_at_mut(row_index + 1); let row_beneath = rows.beneath.first(); let row = rows_above.last_mut(); Also, for row_index in 18..0 this won't do a single iteration, because the ranges iterate upwards. Try something like `(0..18).rev()` (or maybe `(0..19).rev()`).
Yeah I would like to push this to finish line. Apart from finding more time, tokio combinator experience has been little painful. Also I've just started implementing rumqtt client using generators and I'm so happy with the ergonomics. On a different note, will you be willing to work on nightly if both client &amp; broker are implemented using generators?
One solution is to use the `split_at_mut` method for the rows vector to slice it into separate mutable slices, and then grabbing the corresponding values you need. Rust's compiler isn't yet able to determine that what you're doing is safe. If you really wanted to notify the compiler that you know what you're doing, you'd have to use the unsafe keyword, as the `split_at_mut` method does internally. And you shouldn't need to type out `&amp;mut` and `&amp;` on your matched variants if you dereference your match inputs. It would also be more efficient if your input to the function had the `&amp;mut [Vec&lt;Option&lt;Block&gt;&gt;]` type.
I won't find time to fix all the documentation but I'll add examples and publish to crates.io. Do you need anything apart from publish and subscribe examples?
I think that'll be perfect. Thanks !
&gt; This is the runtime stack for local variables that we're talking about. There is only one stack "active" in single execution context and apart from storing local variables it also stores "stack frames" used for function calls. I have linked stack overflow answer somewhere in previous threads, it does graphically explain how that one works. &gt; Who the hell puts 10GB on the stack? In Windows the default stack size for a new thread is 1MB... Whole bunch of recursion based algorithms (such as stack based sorts). &gt; For coroutines, the compiler should be able to calculate the required stack size, and it's typically on the order of bytes, not kilobytes or megabytes or more. Stack can actually grow dynamically to arbitrary lenghts, regardless of what is it's size upon program load (this is not true for example linux kernel stack which is of static size and kernel developers are aware of it). But that's besides the point, original argument of somebody here was that entire stack gets loaded/saved which is fucking ridiculous, excuse my french.
We (Snips) were doing it last year for our Secure Aggregator. It was working well, but the application is now dormant. We have another use-case looming: we are porting our on-device voice assistant technology to mobile platforms. Android is on good tracks, and we are confident iOS will work. We have also contributed "dinghy" a cargo extension to run tests and bench on mobile (a real pain in iOS).
There is no need for function style generators. You can just return the generator from a function: fn foo() -&gt; impl Generator { || { ... } } I like this a lot. It's obvious that the function now returns the actual generator instead of magically becoming a wrapper like in e.g. python. When generators are allowed to take arguments we can write those in the argument part of the closure syntax instead of doing let (arg1, arg2, arg3) = yield value; somewhere in the body
&gt; But that's besides the point, which was that the whole idea that entire stack gets loaded or saved is fucking ridiculous The stack is allocated when the coroutine is started; a _pointer to the stack_ is what gets loaded and saved during context switches (in addition to usual CPU context). I really don't understand the confusion here; I'm sure someone else will be able to explain this better than I.
Definitely, the problem is that `await` is more ergonomic so people default to that. `.all` is more ergonomic then `.then` chains of independent results so that used to be the default. This is a problem in Haskell even and the reason behind the applicative `do` proposal.
Yep. I'd like to build the core using only Tokio and futures though. With a generator wrapper layer, so that people have the option. 
You're right, I didn't notice the mutable alias problem and it seems not easy to overcome, right now I just use plain loop instead of implementing the Iterator Trait. Never heard of Steaming Iterator, I'll have a look at it, Thank you!
/u/bestouff and /u/kteza1 Here is an updated rumqtt that builds on stable. Also I have added an example crate which demonstrates two communicating clients. https://github.com/QuiverMedia/rumqtt You should be able to just cd into example and run `Cargo run` (after standing up a broker on localhost:1883) 
Ah, this is for client-side checks. I've written lots of *server side* checks in Rust already [here](https://crates.io/crates/git-checks). Most of them are content-based rather than things like "make sure that tests pass", but they may still be of use. It'd be interesting to see them be able to be run on the local side as well. Note that it does not use `libgit2` because it (at the C level) does not provide all of the plumbing that is necessary to implement the logic required to get good performance out of checking large repositories.
Jail broken to run Rust or Servo? It doesn't need to be jailbroken for standard apps that incorporate Rust code. The biggest issue I'm aware of is with the bitcode submission requirements from. Apple for Watch and TV. iPhone has no such requirement (yet?). Getting bitcode into a format which Apple would accept is not obvious right now.
You can't use streaming iterators with Rust's `for` loops, and they're really tricky, so I wouldn't bother if I were you :)
I think you meant to post on /r/playrust
A web browser is not a standard apps. Firefox has to use WebKit for iOS, see https://en.wikipedia.org/wiki/Firefox_for_iOS.
Sorry Reddit expert
I think the plans here are for very limited coercions (specifically, function call sites) that act as the analogue of deref coercions for references.
Yes. That's why I was drawing a distinction. The original post made that unclear. 
If you actually follow the context of the discussion here that is what I'm telling everyone here and for some reason people still argue with me ¯\_(ツ)_/¯
Honestly I think it would be cool to have something like let c = for g in generator { ... } where g is the yield and c is the complete 
I saw lots of you calling other people wrong without much explanation; if this was what you were getting at, it certainly didn't come across that way. At all. Miscommunications happen, no big deal. That said, I thought it was pretty obvious that loading and storing "the stack", when "the stack" is allocated on the heap specifically _so it doesn't have to physically move_, referred to its address and not its contents. But at least we're on the same page now. ;-D
I don't have any experience with developing on iOS so to be honest, I wouldn't know. I expect this to be specific to Servo.
Seeing everyone violently agreeing with each other has at least convinced me that yes, my previous understanding of stackless and stackfull coroutines was correct :)
Tokio and generators don't actually overlap a whole lot in their functionality. Tokio provides you the means by which to obtain futures representing asynchronous IO and the engine used to schedule and drive those futures to completion. So you'd compose your future taskss with async/await, and then use tokio's Core to actually run them. To my knowledge, Tokio doesn't actually do anything with threads. A Core just runs on a single thread. Maybe you're thinking of futures_cpupool? Regardless, you're always going to need something that's responsible for telling your coroutines when/where to run. Right now, the best we have is Tokio, but I'd love to see something that's able to move them between threads if it needs to.
https://github.com/kryptco/kryptonite-ios uses a Rust library for both that iOS app and their Android app. The Rust code is housed here: https://github.com/kryptco/ssh-wire I am also attempting to use a library of my own in an iOS app, but I haven't gotten past the linking stage successfully yet.
It's been discussed [here](https://github.com/rust-lang/rfcs/issues/376), but AFAIK no one is actively pushing for it right now.
I might have overlooked it but is there a way to query if the generator can be resumed? If my understanding is correct, it would be possible for a function to maliciously or not send you a generator which is "Done". If you assume that the generator is safe to call/resume you could run into trouble. 
So does a single transistor Anyway what is the point of using a less powerful chip when the more powerful chip is just as cheap, almost as easy to program, smaller on-board, and future-proofs your design against wanting to add more features?
These examples look great! I've been working on an IRC bot that's built entirely on top of futures and streams. As time has gone on, the code has gotten a bit ugly because of the amount chaining futures and streams together. I'm probably going to convert it to nightly tonight and start converting it over to async/await to start testing generators. In my previous experience with other languages that support async/await I'm really hoping to see big gains in code quality and the ability to actually follow along with the code.
After some complaints about wrong links and double entries I now use ripgrep to search for and count pull request numbers. 😎
There's a formal RFC [here](https://github.com/rust-lang/rfcs/pull/1935).
If you are not limited to MQTT, Mles is a simpler pub/sub protocol option with example client available using futures: https://github.com/jq-rs/mles-rs
If I remember right, there are some keywords that are already reserved but don't have a purpose yet. Maybe "yield" was one?
`yield` is already a keyword, yes. The full list of reserved keywords [is here](https://doc.rust-lang.org/book/second-edition/appendix-01-keywords.html).
How do I specify the lifetimes for a function parameter? I have this struct: struct MaxHeap&lt;T, F&gt; { heap_arr: Vec&lt;T&gt;, size: usize, compare_function: F, } This struct has an array of a generic type, a size, and a function used to compare items in the array. Then I have I want to create a new one with a specific time of compare_function: impl&lt;T, F&gt; MaxHeap&lt;T, F&gt; { fn new&lt;'a, 'b&gt;(f: F) -&gt; Self where F: Fn(&amp;'a T, &amp;'b T) -&gt; &amp;'a T { MaxHeap { heap_arr: Vec::new(), size: 0, compare_function: f, } } } I thought this would say that I wanted to take in a function where there was two arguments, and return the value with a lifetime equal to that of the first argument. So I have two questions then. First, how do I specify the correct lifetimes for this function parameter? Second, How can I make it so the return value can have either the return value of the first or second argument? __Edit:__ I was able to solve this with the following solution. Because I need the lifetime based on the closures invocation and not the new functions invocation I have to use the for keyword as follows. impl&lt;T, F&gt; MaxHeap&lt;T, F&gt; { fn new&lt;'a, 'b&gt;(f: F) -&gt; Self where F: for&lt;'a&gt; Fn(&amp;'a T, &amp;'b T) -&gt; &amp;'a T { MaxHeap { heap_arr: Vec::new(), size: 0, compare_function: f, } } } 
I really hope they're not going to keep adding stuff to Rust until it becomes C++...
C++ doesn't have yield, async and await unless a lot has changed in the last few years...
Would you share your code?
There's a TS for it currently with experimental implementations in MSVC and Clang, and there's some chance of it making it into C++20. But no, nothing standardized yet.
MFW this actually compiles and works: https://github.com/sevagh/erased-serde-json https://github.com/sevagh/pq/pull/69/files#diff-ada1bf8cc1f8631fb6e0fadaf98b843bR2 This is the most black magic I've ever done with Rust.
I took a non-object-safe Trait (serde_json::ser::Formatter) and made it object-safe (following the example of https://github.com/dtolnay/erased-serde): The repo: https://github.com/sevagh/erased-serde-json See it in use here, `Box&lt;ErasedFormatter&gt;`: https://github.com/sevagh/pq/pull/69/files#diff-ada1bf8cc1f8631fb6e0fadaf98b843bR2 I did all of this just to be able to use `Box` on `CompactFormatter` and `PrettyFormatter` - which both have `Formatter` impls (but again, Formatter is [unsafe](https://docs.serde.rs/serde_json/ser/trait.Formatter.html)). Learned something though... Or a lot of things. **edit** Blog post: https://sevagh.github.io/post/erase/
Generators won't *replace* iterators, they will be one (possibly the most ergonomic) way to implement iterators. Anyway, for iterators, the interface, or trait, that is relevant to iterating, will still be `Iterator`. Whether that is implemented with a generator or "by hand", is an implementation detail.
should be `yield a; yield b;` for the first two numbers furthermore, instead of `yield a + b;` it should first do the addition and then just `yield b` inside the loop
&gt; With generators, you can just yield all the elements from the first iterator, and then yield all the elements from the second iterator. The compiler automatically extracts the state machine from your normal control flow and use of local variables. On the other hand, if you were to write an alternating iterator... then I am not sure it would easier.
Are those functions that use those methods exposed by the library / used somewhere in the `main` path? If not, then they will still be registered to the compiler as "unused"
I find itertools::chain to be the simplest. extern crate itertools; use itertools::chain; fn main() { for byte in chain(0u8..0xffu8, vec![0xffu8]) { print!("{} ", byte); } } Unfortunately, if you try to wrap it in a function, the type signature gets ugly.
How will you distinguish on the call site between async functions (that will block your coroutine) and sync functions (that return immediately). I think the async macro improves understandability quite substantially.
This is probably your problem. If your methods are only called from 'unused' methods then they count as 'unused' as well.
Wouldn't that more or less just be: fn alternating(prev: &amp;mut impl Iterator, next: &amp;mut impl Iterator) -&gt; impl Iterator { while prev.is_some() &amp;&amp; next.is_some() { std::mem::swap(prev, next); yield prev.next(); } } Edit: forgot the enclosing loop.
I think you might want better showcases in your documentation. Although your describe your crate as "a set of higher-level abstractions", reading the page you linked (index page of your documentation) it is not clear that your `WorkQueue` provides anything at all on top of a simple synchronized queue structure (though that it a crucial component for a parallelism library This is especially true since there already exists a well-known crate for parallelism, [rayon](https://crates.io/crates/rayon), which seems to provide a higher-level interface, with a `ThreadPool` that offers a `spawn` method taking a closure.
&gt; - using generators as iterators I'm excited about that one. It currently requires a wrapper type, which isn't included in libstd but can be implemented externally: https://play.rust-lang.org/?gist=7f8d2960361237403fe4d19161c00731&amp;version=nightly UPDATE: Someone also published the [gen-iter](https://github.com/tinaun/gen-iter) crate, which contains a similar wrapper type. It would be even nicer if the generator itself implemented the Iterator trait (with no wrapper type required). This can't be done with a generic impl since it would conflict with other generic impls of Iterator, but the compiler could generate both `impl Generator` and `impl Iterator` for each generator literal.
Here is the commit that shows the difference: https://github.com/mehcode/shio-rs/commit/8078a34c075bf2f52c0d6287f4ed0e3a912a3f7c#diff-924ded2c751ccd30e8769f459e61a362 I'm interested in the performance &amp; compile time implications of this change. Did you do any measurements?
Hmm, well I have a lot of functions calling other functions and the main point is that it is used in a library, so my main is more for show and testing rather than anything functional. Should I maybe be calling my lib.rs from my main?
Hey I responded to the parent. Think you can take a look?
Hmm... it's a huge project. 10s(?) of thousands of lines, so I'm not sure what to share.
&gt; if you try to wrap it in a function, the type signature gets ugly. Story of my rust life so far :-/
Its probably trying to do a u8 to char cast. Try that literal with a u32 on the end so that it knows what you mean. Edit: something like `0xEF8888u32`
If you make a library, functions/methods marked `pub`, that are exported to users of your library, are never considered 'unused'. They are entry points as well as `main()` and everything called from those (recursively) will not be considered 'unused'.
Only a `u8` may directly be cast to `char`. I believe this is because all `u8` values are valid utf8, whereas not all `u32` values are a single, valid character in utf8. I see that there is an nightly-only `TryFrom&lt;u32&gt;` for `char`, which will attempt to do the conversion and return an error if it is invalid. I've never worked with individual `char`s directly before, so maybe someone else can provide guidance on what the best way to approach this on stable is. EDIT: aha! [I found the answer.](https://doc.rust-lang.org/std/char/fn.from_u32.html)
Ahhh this makes so much sense. Thank you, I think I have an idea of what's going wrong now! Also, just a quick side question. When making test folders how do you safely expose methods from a crate? Or is it just advisable to make a test within the src directory so that you aren't showing external entry points to test your code?
Well there's your answer I guess :/ There might be some function for it in the `char` module or on the `char` type.
`char`s are not numbers. The method you're looking for is `std::char::from_u32` - however, `0xEF8888` is not a valid [unicode scalar value](http://www.unicode.org/glossary/#unicode_scalar_value) because it is outside the allowed range.
if you want to test functions that are not `pub` then you'll need to put the tests in the same module as those functions. For those tests you can use the common pattern of declaring a `mod tests` in the same file: #[cfg(test)] mod tests { use super::non_pub_function; // works because inside the module #[test] fn test_non_pub_function() { assert_eq!(non_pub_function(), 42); } }
Awesome, thank you!
Sure is a confusing error message. Thanks for the docs. That's helpful.
I'm going to need more context to be able to help. I think Sieve of Erastosthenes is pretty easy so I wouldn't expect that to be a semester project unless you plan to do multiple implementations and explore some aspect of it fairly deeply.
I'm not exactly sure what you're going for here, but one issue is with sized-ness: struct A { child: A } This struct's size cannot be determined at compile time since the parent-&gt;child relationship could repeat itself any number of times. In fact, since the field isn't optional it actually must be of infinite size at runtime, so let's fix that: struct B { child: Option&lt;B&gt; } But because `Option`'s size will be equivalent to the size of `B` we still can't determine size at compile time. The solution as I see it is to introduce some sort of indirection---either a reference or a `Box`. For example: struct C&lt;'a&gt; { child: Option&lt;&amp;'a C&lt;'a&gt;&gt; } fn main() { let child = C{ child: None }; let c = C{ child: Some(&amp;child) }; } Now we can build a chain of structs as long as we want and the compile-time size is still the same since references have size `usize`. Something similar would be done with `Box` if you don't mind heap allocation.
Is there any widely used way to fake it?
I'm not aware of one. What's your end goal?
Hangman is pretty simple too... What else can I do? Rust programming isn't as easy to do since documentation and help is limited compared to C++ for a college project... And there aren't any Rust projects floating on internet unlike C++ or Java to get cues from..
Typically, people don't care as much about Variadic Templates in Rust because the macro system is amazing and keeps getting better. If you need to accomplish something complicated at compile time, odds are you can do it much more clearly with !macros or procedural macros. https://github.com/paholg/dimensioned is an example of something which would fall within the wheelhouse of the Boost metaprogramming library, but can be implemented in Rust with some templates and the help of macros. 
&gt; And there aren't any Rust projects floating on internet [I don't understand](https://github.com/search?utf8=%E2%9C%93&amp;q=language%3ARust&amp;type=Repositories&amp;ref=advsearch&amp;l=Rust&amp;l=).
Here's [the current discussion on the internals forum](https://internals.rust-lang.org/t/help-test-async-await-generators-coroutines/5835), which includes links to past discussions. Here's [the experimental RFC](https://github.com/rust-lang/rfcs/pull/2033).
You can just await a `Promise.all` though: See: var a = await asyncA(); var b = await asyncB(); var c = await asyncC(); Vs. var [a, b, c] = await Promise.all(asyncA(), asyncB(), asyncC()); No `.then` required. Edit: Also, that said, I agree it would be great to see a solution that would automatically do this concurrency for you.
it is dangerous to go alone take this `crates.io`
You might impress the heck out of everyone if you built a spellchecker. Thanks to some handy crates by https://github.com/BurntSushi you could accomplish this quickly. And your C++ friends will look at their hangman games and hang their heads in shame. 
The issue isn't whether `.all` returns a promise its where the ergonomics of the language guide you. With `await` awaiting all three is easy and linear. var a = await asyncA(); var b = await asyncB(); var c = await asyncC(); That desugared is: return asyncA().then((a) =&gt; { return asyncB.then((b) =&gt; { return asyncC.then((c) =&gt; { return [a, b, c] } } } The average js developer at this point says use `Promise.all`, duh. But with `await` it shifts the costs around, and now you need to know to reach for `all` *and* are forced to use less pretty syntax. Since we started using async/await at work this has been a common performance pitfall for even experienced JS devs who grew up in the `Promise` era and should know better. This is also a problem even in haskell with `do` notation which is inherently linear, people forget to or don't realize they code that can run concurrently when you make the sequential code easy to write.
&gt; How will you distinguish on the call site between async functions (that will block your coroutine) and sync functions (that return immediately). `await!()` doesn't actually block the thread, so there is really no need to make this distinction. [The README](https://github.com/alexcrichton/futures-await/blob/e09c87c7aa796bf6e5b6d8d45d6cba2078d12210/README.md#what-is-this) states:- &gt; The `await!` macro allows blocking on a future to completion. This does not actually block the thread though, it just "blocks" the future returned from `fetch_rust_lang` from continuing. You can think of the `await!` macro as a function from a future to its `Result`, consuming the future along the way. `#[async]` at the top tells you that code block is asynchronous so you know it returns a `Future`. Inserting visual cues in the body of the implementation to separate futures from sync functions doesn't provide much benefit. They both return immediately.
Brushing up on data structures, gonna work through some textbook examples of B trees, but in Rust.
Sorry, looks like I was misunderstanding the issue. So if I have the situation below: ``` file a.rs: pub fn cool_function(){} file b.rs: use a::cool_function; c = a::cool_function(actions) ``` This is producing a warning in my case. I need the pub in a.rs in order to use cool_function in b.rs. If I do not have it it will produce an error. If I do have it it will throw a warning, which is bad for a file that has function used all over the place. 
Yeah I realized that today when I wasn't so tired. I've since made a better version of this that allows iteration.
Does it mean that my ipad 1 could come back from the dead? I would be very happy if it could run servo! I mainly use it for ebooks since the default ios browser does not work for many sites (including reddit!)
It sounds like your argument basically boils down to joining Promises is a little unwieldy to the point where it almost requires you to use `Promise.all` to keep your sanity, whereas `await` is ergonomic enough that you may not reach for `Promise.all` costing you performance, right? I can't argue with that, I guess that's true. I think that ship has sailed on JS, I don't think there's a real solution to that. Your suggestion here for Rust would solve that, at the cost of making things a little less explicit (you would have to understand the data dependencies to know how the function will execute concurrently). Such analysis is not really possible in JS so the same solution couldn't really apply there without significantly more work.
Why did you do a `!` macro instead of a custom derive? Not saying that was wrong, just curious why.
You can directly write `'\u{EF8888}'`, but then rustc will say "error: invalid unicode character escape," because the Unicode range is only 0-0x10FFFF.
Pardon my inexperience, but can these constructs be used to model Go's style of concurrency? This is purely an exploratory question and I am not suggesting this is what I think is best. After looking at some of the examples people have shared, it's not totally obvious yet. Thanks for your time.
This is a custom derive. Are you following the link to the crate that this is inspired by?
Rust qualifications a plus at Amazon whaaaat.
~~Just FYI, that article is flat out wrong on (at least) several of the C# examples.~~ EDIT:: I was incorrect, my tests were flawed (Thread.Sleep vs Task.Sleep). In C#, the "async" keyword that you decorate functions with just enables you to use the "await" keyword in your function ("async" does nothing on it's own). You "await" tasks in order to continue processing the rest of your function after that task has completed (think of it as taking the rest of your function and putting it inside a task.ContinueWith, but with the added benefit of automatic exception propagation, readability, and allowing the continuation to easily continue execution on the thread that ran before you hit your "await" (depending on the continuation context, of course)). Correct me if I am wrong, but rust generators seem to be far more similar to C# IEnumerables (using the yield keyword). In C#, when you use the yield keyword, the compiler rewrites your function into a statemachine, very much like this proposal is doing, but without exposing the actual state machine itself.
Currently, no, you as a caller would have to track that to see when you get back `Returned`
Filed https://github.com/rust-lang/rust/issues/44201 
I'm an idiot... Good job haha
&gt; `c = a::cool_function(actions)` I suspect using the full path here makes `use a::cool_function;` redundant. The `use` statement allows you access `cool_function` like a locally defined function - `c = cool_function(actions)`. But you are not doing this - you are only accessing the function with it's full path. This will work even without the `use` statement - which means you can remove it, and this is why you get a warning.
Generators impling `Iterator` would give you weird things like futures implementing `Iterator&lt;Item=()&gt;` though.
&gt;\[disposition: merge] Generic associated types (associated type constructors). &gt; &gt;\[disposition: merge] Evolving Rust through checkpoints. &gt; &gt; ... &gt; &gt; Clarify and streamline paths and visibility. Really happy to see the progress on these fronts in advance of the "impl period". 😃
I could have written my announcement a bit more clear so no worries.
Thank you so much! Looks like spellchecker is going to be the one.
In the [rfc](https://github.com/nikomatsakis/nll-rfc/blob/master/0000-nonlexical-lifetimes.md) for non-lexical borrow checker, there's this line: &gt; The reason is that the map is borrowed as part of the call to get_mut, and that borrow must encompass not only the call to get_mut, but also the Some branch of the match in reference to this code: fn process_or_default() { let mut map = ...; let key = ...; match map.get_mut(&amp;key) { // -------------+ 'lifetime Some(value) =&gt; process(value), // | None =&gt; { // | map.insert(key, V::default()); // | // ^~~~~~ ERROR. // | } // | } // &lt;--------------------------------+ } I don't understand why the borrow needs to encompass the the Some branch in particular. I get that map is borrowed in the match, but why is it still needed for Some(value)'s branch but not the None branch?
To select an user by login, you have to use [filter](http://docs.diesel.rs/diesel/prelude/trait.FilterDsl.html#tymethod.filter) users::table.filter(users::login.eq("whatever")).first::&lt;User&gt;(&amp;conn) I suggest you read the getting started guide here: http://diesel.rs/guides/getting-started/ It contains everything you want to know about basic queries and more :p 
and by login and password?
 users::table.filter(users::login.eq("whatever").and(users::password.eq("pouet")).first::&lt;User&gt;(&amp;conn) Everything is explained in the getting started guide. You shouldn't be selecting by password though...
it's not
Right, there are no filter chaining in it, it's been a while since I last read it.
Do I use a specific name many times? `use` that name. Do I use many things from a module, but only use them infrequently? `use` the module. Do I use something only very rarely? I don't `use` it, I write the full path out, or I `use` only within the functions it's actually needed. I don't wildcard unless I'm being sloppy and writing something I don't care about. Whether I export a module structure or just flat symbols depends entirely on expected uses. Does the module structure *matter* to the user? No? `pub use` to create a flatter API. Does the module structure help with having lots of names? Keep it.
The only possible way you can select based on the password is by doing passwords entirely wrong. Please read https://crackstation.net/hashing-security.htm and check out https://github.com/bryant/argon2rs
[removed]
The ergonomics are *so* much better here, it's astounding!
Test it yourself! Any benchmark provided will be dependent on OS, compiler, compiler flags, CPU, memory, etc. If you write and run your own, you can be sure that the results you get are relevant to you.
I think for each shared clone for both Arc and Rc two extra operation - one to increment on clone and one to decrement the counter on drop. So the real diff is atomic +/- to non-atomics. The linked stackoverflow post might help https://stackoverflow.com/questions/2538070/atomic-operation-cost 
I'm a bit surprised the number of lines didn't go down more. Though obviously more complex syntax (hello loops) will benefit more.
await doesn't block the thread, but it does block your coroutine. &gt; They both return immediately. No, the await will block further progress in the coroutine. If you don't await, you have a future that you can pass around or await on later after doing other work, or something. Imagine someone wrote some code like: for foo in bar { do_stuff(foo)? } How would you feel if you found out long later that this code is actually doing an async network call or something for every iteration of the loop and awaiting automatically, without it even being visible that this might happen (without looking for the attribute in the `do_stuff` definition). In this case, probably making things a lot slower than they need to be without visually indicating this may be the case. It's nice to see at a call-site that something may be async and that we're explicitly waiting on the async action to complete before continuing. That said, something like this can happen in just the regular sync world without generators etc, but why not make it easier to see what's going on here regardless? It's not very noisy, though I would like it to be a keyword though honestly. But that can come in due time. Maybe something like: let foo = await? do_stuff()
&gt; It's nice to see at a call-site that something may be async and that we're explicitly waiting on the async action to complete before continuing. I never understood this argument. Something similar could be said about normal blocking operations and yet, we don't propagate sync io call 'markers'. I don't see what's the big deal about an async io operation. If anything, the lack of visual difference is better in my opinion. let a = foo(); bar(b); The snippet above expresses the idea that foo provides the value a and bar requires it. It does not matter how foo obtains a, bar has to wait anyway.
The point about sync IO is a good and and I don't have a very strong argument there. It's more of a feeling that I want to know that some fancy async stuff is going on. It might just be a case of what we're used to as programmers. It's typical (in other languages) to call out when we wait for async operations to complete, so doing it the same in Rust gives me the warm fuzzies. That said, I believe awaiting actually does significant code transformation for you, specifically turning your code into a state machine. I'm not sure callers need to be constantly reminded of that and any associated cost, but it does seem like a good idea to know it's happening. But maybe the `[#async]` attribute is good enough for that. BTW, are there any languages that do what is being suggested? I'd be interested in looking at code examples to get a feel for it.
Yeah, the Postgres example, being the most complex, is much more readable. I wasn't really counting lines as much as just reading the code before and after.
mind giving me any help on this? i'm not that good at rust and have no clue how a spellchecker works in code terms
I quite like the idea of building something with s-expression syntax, and allowing other syntax to be layered ontop ('the program is [defined as an AST](https://en.wikipedia.org/wiki/Intentional_programming), but you can visualise and edit it in other formats' .. end all the arguments about [significant whitespace](http://wiki.c2.com/?SyntacticallySignificantWhitespaceConsideredHarmful) vs [braces](https://en.wiktionary.org/wiki/curly-bracket_language), etc.)
It's because `value` is a reference pointing into the map. If we say that `map` is of type `HashMap&lt;K, V&gt;`, then [`get_mut`](https://doc.rust-lang.org/std/collections/struct.HashMap.html#method.get_mut) is going to return an `Option&lt;&amp;mut V&gt;`, which makes `value` a `&amp;V`. `map` must remain borrowed as long as that reference is alive.
You could use [`std::iter::once(0xff)`](https://doc.rust-lang.org/std/iter/fn.once.html) instead of the vec I think.
I was recently thinking a out tackling something like this also. Any chance you're ready to share a little code? 
I think you're missing an `extern crate diesel_codegen;`. Anyway, the full examples are here: https://github.com/diesel-rs/diesel/tree/master/examples/postgres.
Does Rust have a string/sequence matching algorithm that works on `&amp;[u8]`? That search implementation will be slow for longer signatures.
I believe `await* [AsyncA(), AsyncB(), AsyncC()]` was considered in the original JS proposal, and could be a solution to this...
In general, I agree with that sentiment. However, seeing how some code (eg. with futures) can be greatly simplified and made more readable with generators (and I'm one of the people who have no real problem coding with the monadic combinator style), I think they'll be a valuable addition. I think one of the important things that'll be needed is clear explanation what they are actually inside. Like, a closure is basically a function pointer + struct with variables. A generator, in my understanding, is a function pointer, struct with variables and „continue here when called again“ position.
Looks awesome, and also circular looks very useful :)
I'm not too familiar with ARM, but Avrs were designed as microcontrollers, with their primary functionality being just that - controlling. Hence, it is incredibly easy to control a pin, write to serial, etc. They do a very specialised job there, and they're pretty good at it. Not to mention the massive community of AVR users, and the Arduino community, which makes it easier to find help or get support. ARMs iirc are more designed as microprocessors, and as far as I have heard they accel at that, but controlling pins directly is more difficult. So, you're right: for some projects you might want to use ARM for power or future capability, but AVRs are simpler. 
The cost of atomic operations in `Arc&lt;T&gt;` is completely independent of `T`. However `Arc&lt;Cell&lt;f64&gt;&gt;` doesn’t seem particularly useful: `Cell` makes it `!Sync`, so it might as well be a non-atomatic `Rc&lt;Cell&lt;f64&gt;&gt;`. If you do need `Sync` (thread-safety) and mutation of nothing more than `f64`, you could consider using `AtomicU64` together with `f64::to_bits` and `f64::from_bits`.
Hi! Where can I find a simple example (getting started-like) about generators in Rust?
Those podcasts are awesome. Thanks a lot for doing that!
I have done some small experiments with Rust on iOS but nothing concrete to show. My experience has been that Rust is certainly usable on iOS, however the experience varies a lot depending on what you want to achieve: Using Rust to write a library and interacting with that library using C ffi is pretty easy. If the Rust library wants to interact with iOS frameworks, things can get more complicated. If you want to write a whole App in Rust, using OpenGL for the UI, that is possible as well -- although requires some fiddly set-up (I finally managed to get things working with SDL + glium; glutin, gfx or piston all had some problems).
This is 100% hardware dependent. But the in general i believe its something like : Arc is at least in the ball park of being 100x slower then Rc. But! It is possible that branch prediction gets it right ( but i wouldn't count on it ). which in turn could cut the cost by 99% , or cut the cost by 0.00001% depending on what you are actually doing with result. 
&gt; Some basic examples I saw from blog posts and the like used generators in an async sense, with an await!(...) macro. Is this going to be first class for generators, or are libraries like tokio expected to handle the async side on their own? I believe generators are on a level with Futures , not on a level with mio or tokio . in a sense that they are polled/resumed which will run code until it yiels/returns. For async io side of things you will still need something like mio ( the event buffer and OS interface) and tokio-core (an eventloop to execute on a thread , parking and unparking ). ( I think . not 100% sure ) 
&gt; That said, I believe awaiting actually does significant code transformation for you, specifically turning your code into a state machine. This. I think this is the important distinction here. &gt; Something similar could be said about normal blocking operations and yet, we don't propagate sync io call 'markers'. There is an important difference for me. I am thinking in three categories: single threaded - I can reason about changes to everything just reading the current function (this is blocking IO in this case). Concurrent: I might get paused and when I resumed things might have changed under me, but there is no potential for data races (coroutines). Parallel: Things might change while I am running and I have to account for that. Async signalizes the second for me. The program goes to do other things while my execution is paused.
This subreddit topic is about [Rust programming language](https://www.rust-lang.org/en-US/), not the video game. You might want to re-post your question on /r/playrust/
While I'm not sure how fast they are compared to each other, Rust will always tell you if you can't use Rc, so if you are writing a library, you can use Higher Kinded Types to allow the user to choose which one to use, so they can always use the one that works best in their situation.
&gt; you can use Higher Kind Types Considering that GATs are not available in Rust yet, what wizardry is this?
Only ASCII 7-bit characters are valid UTF-8, as far as I know.