I couldn't say. Everytime I try to learn about monads, the experience feels like trying to learn what words mean with a dictionary. "A monad is a type that fulfills the monadic property" -.-
Good point about `asset!`/`assert!`. Maybe `assets!` is sufficient semantic distance?
&gt; I have seen include_dir before, and it's a fantastic crate Wow, it's nice to hear people are enjoying my `include_dir` crate! I hadn't really thought of it since I rewrote it to use `proc_macro_hack` several months ago.
[It (was) also a C/C++ preprocessor](https://github.com/facebookarchive/warp)
During development, I constantly compared warp's performance to hyper. The plaintext and json benchmarks from TechEmpower, nothing more complicated. There was no noticeable difference.
Each successful test ends with `... ok`, so why not just grep for that? cargo test --all | grep -c cargo test --all | grep -F '... ok' -c-F '... ok'
Nope, just the lib they depend on
Would it be possible to define `Lazy::new` as a `const fn`? pub const fn new&lt;F: FnOnce() -&gt; T&gt;(f: F) -&gt; LazyCell&lt;T&gt; If so, then we wouldn't need macros at all: static GLOBAL_DATA: Lazy&lt;Mutex&lt;HashMap&lt;i32, String&gt;&gt;&gt; = LazyCell::new(|| { let mut m = HashMap::new(); m.insert(13, "Spica".to_string()); m.insert(74, "Hoyten".to_string()); Mutex::new(m) }); That'd be nice to have in `libcore`. To me, the `lazy_static!` macro has always felt like a hack, so perhaps `const` can solve the static initialization problem more elegantly?
I see a man of culture. Well Yukikaze-sama is beautiful, elegant, lucky and indestructible. Last two are just must have qualities for network :D
From a software perspective, a monad is just a design pattern. It lets you compose things. You'll already be familiar with lots of them (there are many in the standard library). It has two methods. One wraps up a "thing", the other performs an action on it and gives you another "thing". In Rust, a monad might look something like this: impl MyMonad&lt;A&gt; { fn new(a: A) -&gt; MyCoolMonad&lt;A&gt;; fn and_then&lt;B&gt;(a: MyCoolMonad&lt;A&gt;, func: fn(MyCoolMonad&lt;A&gt;) -&gt; MyCoolMonad&lt;B&gt;) -&gt; MyCoolMonad&lt;B&gt;; } That's it. The real power comes when you are able to abstract over them - "give me anything that I can put some value into, then `.and_then()` a bunch of times". I hope this helps. If you are interested in the maths behind it, and are interested in learning what all those exciting words you've no doubt heard, I can highly recommend Bartosz Milewski's [Category Theory for Programmers](https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/), and the accompanying video series.
 use std::alloc::System; #[global_allocator] static GLOBAL: System = System; What happening here ? How can `System` be both a type and an instance of this type ?
Sure, yeah, both `Lazy::new` and `OnceCell::new` should be const, as soon as that is stable. The ideal API would probably even use a free-standing function: ```Rust const fn lazy(f: impl Fn() -&gt; T) -&gt; Lazy&lt;T&gt; { Lazy::new(f) } ``` I am also all for including some version of this into core/std. As a fact, this crate was born exactly from the discussion about moving `lazy_static!` to libcore: https://internals.rust-lang.org/t/pre-rfc-lazy-static-move-to-std/7993/36 :-) Note, however, that `sync::OnceCell` would probably not fit libcore with the current semantics: it needs `sync::Once` to provide a guarantee that only only `f` is called, and there's no `Once` in libcore.
`System` is a unit struct; it's type is System but it can also be constructed as such as well. You can see it's definition here: https://github.com/rust-lang/rust/blob/master/src/liballoc_system/lib.rs#L71
Could you give a few examples of how to use this please?
Create an enum? ```rust enum Value { String(String), Int(u32), } ```
I haven't dug into the Rust web ecosystem at all yet, but that's not surprising to hear. From my work on the Haskell side, the two biggest issues were: * Figuring out a streaming interface * Figuring out the right way to do resource allocation To some extent, these are solved problems in Rust, so that's nice. On the other hand, error handling is probably something that will throw a curve ball into the mix. If there's anyone working on this, I'd certainly be happy to join the discussion if anything from the Haskell world can be useful.
I assume one would prefer to use `i64` instead of `u32`, because OP probably wants to use `f64` instead of `f32` for the floats, so the enum needs to be large enough for that anyway, and also because they might expect "integer" to also include negative number. Apart from that, yup, such an enum is the way to go.
I disagree. If you need to own the data, make the user give it to you. If you do not need to own it take a reference. This way the cost is transparent instead of hidden. This is the only factor that matters for this case. Otherwise why tf are you using Rust anyway. Use a gc language if you are just disregarding the cost of allocation and copy.
You're probably familiar with how one instantiates a struct that has fields: struct Foo { a: i32 } let f = Foo {a: 1}; Consider what happens when you define a struct that has no fields: struct Foo; let f = Foo; If you choose to explicitly annotate the type there, that would turn into: let f: Foo = Foo; Finally, since items at global scope require type annotations, that's just how it looks.
Yes, that is how it works. How large are your json input files, that you worry about size? And strictly speaking they would be 126 bit because enums also store a discriminant.
&gt; static GLOBAL: System = System; I thought I had seen talk of switching to `System` as the default once this was available because those who need jemalloc could now opt-in to it. Is this still on the table?
Yes http://www.ovpworld.org/processor-model-variant-mips-warrior-mips64r6 Also MIPS recently purchased themselves back from their parent company. They've been doing rather good, been a rather dark horse in the current lower-power-embedded war between ARM/RISC-V.
What happens if you have a non zero u8 and then subtract enough that it becomes zero? Do weird things randomly keep happening to your program after this? 
My lay person's understanding is Haskell uses monads to allow escaping from the lazy type system. Monads force sequential evaluation on a series of functions. Without monads, Haskell would have no way to ensure the order of IO operations, like: print "What's your name: " name = input() print "Hi " + name Could end up waiting for input before printing anything.
NonZero types don't implement any operations, so you can't e.g. do `x = a - b` where a and b are NonZeros. If those are ever implemented, I suppose it would result in a panic if the result is 0.
I think that is a terrible decision if true. Some systems have poor allocators and jemalloc works very well. Allocator performance is much more important than binary size in general. Switching to system by default requires everyone to know that on some system you have to switch to jemalloc. 
Note that [`NonZeroU8`](https://doc.rust-lang.org/nightly/std/num/struct.NonZeroU8.html#implementations) doesn't implement any operators. To do your subtraction, you'd have to `get()` the raw value, then call `NonZeroU8::new()` again which checks for 0. (Or use the unsafe `new_unchecked()`, and the responsibility is on you.)
It's still on the table, but it's not clear if and when it will happen.
I've been hoping that the `http` crate with something like the `Service` trait would be exactly that in Rust.
I have a quadtree crate I'm writing. Currently, the quadtree struct contains a vec for the data in it, ie pub struct Quadtree&lt;'a, T: 'a&gt; { split_threshold: usize, bb: BoundingBox, data: Vec&lt;&amp;'a T&gt;, st: Vec&lt;Quadtree&lt;'a, T&gt;&gt;, } My question is whether there is any way to have the data be stored directly in the struct, rather than on the heap. The reason it is currently in a Vec is to allow a user to create a quadtree with an arbitrary split threshold, but I think it would be more efficient to have something like this pub struct Quadtree8&lt;'a, T: 'a&gt; { bb: BoundingBox, data: [Option&lt;&amp;'a T&gt;; 8], st: Vec&lt;Quadtree&lt;'a, T&gt;&gt;, } so that when it comes to creating and populating the quadtree, there are at lot fewer heap allocations to do, as well as removing a layer of indirection.
Some applications may have a memory pattern that works better with a different allocator. At the distro level, we want everything to use the system allocator as much as possible, but acknowledge that it's not perfect for everyone. Fedora devel has recently been discussing [allocator guidelines](https://lists.fedoraproject.org/archives/list/devel@lists.fedoraproject.org/thread/43MAKHBKT5HHO6QEXQASWATMMK3TSH3K/) too.
I think you're overreacting. In addition, if you were unable to come up with the `enum` idea yourself, you are entirely unqualified to use `unsafe`. Of course if you know in advance that the data type for a column will be the same for each row, you can do much better. But without more information you're stuck with the big enum. Still, I doubt for most inputs that the csv file would be that much smaller than the loaded data.
My understanding is that this paper talks about Finagle
You are definitely write about Acquire/Release! What would be the best way to write a test that exposes the current bug though? I can trigger it with a stress-test if I manually point the pointer to the unint memory, and *then* copy the contents of the `Box` over. But if I leave the code unchanged, then the bug is not triggered. is there some magical compiler switch for "reshuffle instructions such that the weakest possible memory model is exposed"?
You can store the data column-wise: enum ValueVec { String(Vec&lt;String&gt;), Int(Vec&lt;Int&gt;), }
&gt; I'm not sure if Monads are actually frequently used to abstract over things They certainly are! When you're using monads as an abstraction over things, it's easier to think of monads as containers. Since you seem familiar with Haskell, I'll do some examples in that. -- "Given some container of integers, return a new version of that container with all the elements doubled" monadic_double_int :: (Monad m) =&gt; m Integer -&gt; m Integer monadic_double_int i = liftM (*2) -- Double everything in the list monadic_double_int [1, 2, 3] -- Double 6 monadic_double_int (Just 6) -- Nothing - there's nothing to double! monadic_double_int Nothing -- Double the CPU time - I was going to use "double an integer read from the command line", but it was a bit more verbose monadic_double_int getCPUTime So you wrote one function, but could abstract over many different container types. It doesn't know or care that it's doing IO, or using a list, it just gets on with it. 
The spin crate provides primitives with spinning based waiting.
I'll admit I thought this was a Rustkell joke at first, but nope, it's legit (and awesome).
I'm not really knowledgable enough to be able to discuss the technical merits of either allocator being the default, but the most common complaint I see from people who've only taken a quick look at Rust is that the binaries for a simple Hello World are much bigger than they'd expected for a "systems" language. They don't realize that the default compiler behavior produces unoptimized binaries with with all of the debugging information included, as well as a separate memory allocator. I don't have any hard numbers to back it up, but I can recall at least 1 embedded developer on HN saying that they were considering using Rust but the size of the Hello World binary made them think it was a bloated language that wouldn't be practical on resource constrained devices. Some Rust devs replied and told them about the reason for the large binaries, and how to make them smaller, and the dev said they'd give Rust a more thorough look.
I played around with the allocator recently (1.27.2) and using the system allocator instead of jemalloc is not giving you that much of a benefit to a size comparison. Its noticeable but `strip` yields better results for that matter. Of course everything counts if you're after minimal size :) 
So I did what I should have done to start with and I ran some benchmarks comparing the two: test benches::find_100_000 ... bench: 311,102 ns/iter (+/- 35,534) test benches::find_10_000 ... bench: 25,958 ns/iter (+/- 96) test benches::find_1_000 ... bench: 3,355 ns/iter (+/- 25) test benches::find_q10_100_000 ... bench: 244,465 ns/iter (+/- 10,664) test benches::find_q10_10_000 ... bench: 20,821 ns/iter (+/- 122) test benches::find_q10_1_000 ... bench: 2,889 ns/iter (+/- 15) test benches::insert_100_000 ... bench: 19,798,805 ns/iter (+/- 338,625) test benches::insert_10_000 ... bench: 1,325,673 ns/iter (+/- 3,414) test benches::insert_1_000 ... bench: 92,757 ns/iter (+/- 270) test benches::insert_q10_100_000 ... bench: 19,430,253 ns/iter (+/- 682,693) test benches::insert_q10_10_000 ... bench: 1,248,408 ns/iter (+/- 4,927) test benches::insert_q10_1_000 ... bench: 84,503 ns/iter (+/- 498) The q10s are the ones with the change to arrays in the struct. So while the new version is \~20% faster with finding elements, it isn't faster at being populated, so I guess it isn't really worth it
You might want to look at the csv crate for csv parsing: [https://docs.rs/csv/1.0.0/csv/](https://docs.rs/csv/1.0.0/csv/).
This is great! It would be cool to have some form of this merged into clippy/rustc so that one can turn on/off warnings about shadowed variables.
Thanks for the feedback! In a future version I'm going to add some sort of verbosity flag that will enable this kind of output. I also want to add something that will display any variables that use a shadowed variable. I'll open an issue!
I thought they all will be banned in a second, then I noticed it's another subreddit :).
Thanks, this is a neat example!
&gt;sing Rust anyway. Use a gc language if you are just disregarding the cost of allocation and copy. The cost of function is something that you describe in docs. You cannot guarantee that a function is transparent (though you should strive to write them that way yourself). So trying to calculate the cost of a function solely by looking at parameters or result types is ill-advised.
Awesome, thank you!
This is a nice idea - I'll give it some thought. You could accomplish the decompression part of this with the format above: `\`resource!("filename.gz", some_gunzip_function)\``. Of course, this would require you to gzip your assets in advance, maybe in a build script. I'll keep thinking though - might be a more ergonomic way of doing this.
&gt; The cost of function is something that you describe in docs. This is besides the point. Making the user give the data to you gives him a chance to provide you with data without an allocation and copy. He might not need it anymore anyway. 
I actually just got done implementing something similar to this for a data science team and the approach worked perfectly (although I used a lot more Int(Vec&lt;Option&lt;Int&gt;&gt;)...).
&gt; 128 bit because enums also store a discriminant Interesting, the discriminant doesn't get optimized to a u8? (With 256 or less variants)
Thanks :D! It seems that clippy already has a lint that looks for shadowed variables. I think these are the lints: `#[warn(shadow_same, shadow_reuse, shadow_unrelated)]`
&gt; A common idiom seems to be state stored behind a Rc&lt;Cell&lt;T&gt;&gt; This way leads only to pain, IMHO, and I extremely rarely do it for this reason.
Sorry, proper documentation will come after API stabilization in 0.1.0 For now you can refer to my tests https://gitlab.com/Douman/yukikaze/blob/master/tests/client.rs
My original point is that if your current implementation allocates a vector, it may as well be a temporary solution, e g. cos of some poorly written library underneath, which you expect to be fixed one day. In that case I prefer api stability (taking a slice) to changing the api after the optimization.
GNU Emacs and [Programming Rust: Fast, Safe Systems Development - Jim](https://www.amazon.com/Programming-Rust-Fast-Systems-Development/dp/1491927283)
And the already-existing versions of rustc that don't have this bug?
Allow me to hijack a bit this, but as explained here [https://www.reddit.com/r/rust/comments/93ikxz/emulating\_japlkdb\_take\_2\_how\_store\_columnar\_data/](https://www.reddit.com/r/rust/comments/93ikxz/emulating_japlkdb_take_2_how_store_columnar_data/) I'm also debating how represent the data for a relational language. On it, you load a table, then do operation across columns/rows. So I also noted that some KV stores just store Vec&lt;u8&gt; or &amp;\[u8\]. Is this a good overall idea instead of use a enum like: pub enum Column { I64(RVec&lt;i64&gt;), BOOL(RVec&lt;bool&gt;), } and do instead: #[derive(Debug, Clone)] pub enum Layout { Scalar, Row, Col, } struct Relation { layout: Layout, data: Vec&lt;u8&gt; } 
An excellent question! The `LazyCell` from `lazycell` is exactly the `unsync::OnceCell`, and the `unsyc::Lazy` is just a thin convenience wrapper, so no much difference here. The `AtomicLazyCell` is, however, very different from `sync::OnceCell`. `AtomicLazyCell` never blocks the thread, but it can't provide `or_init` functionality. So, you probably shouldn't use `once_cell` instead of `lazy_cell`: for unsync case, they are equivalent, for sync case they do different things. The comparison with `lazy_static` is more interesting. The most important difference is that the api of `sync::OnceCell` is more powerful. First, `OnceCell` doesn't have to be static, you can use it as a local variable or as a field. Second, you can initialize `OnceCell` with arbitrary closure. The example with logger from the docs is nice: you initialize global logger with data you only get in `main`, something which is not really possible with lazy_static in a convenient way. And of course there's a question of surface API: lazy static is a macro which defines a fresh hidden type, while with `once_cell` it is basically a boring `Lazy&lt;T&gt;` which you can name. I also **really** like how API of `OnceCell` looks like! It feels like a middle ground between `Cell/AtomicX` and `RefCell/Mutex`, you can easily build conventional `Lazy` values on top of them, and this all just works in `const` context. I feel there's a value in this reshaping of existing APIs into something more orthogonal. 
Has anyone proposed to an RFC or PR to merge `parking_lot` into `std` that goes over the benefits, drawbacks, and alternatives in detail? Performance improvements are usually welcomed into `std`. I think you would find that the fact that it hasn't been merged is an indicator that it *isn't* "obviously superior", although it may perform better in many cases. I've glanced at `parking_lot` a few times over the years, but I've never personally been motivated enough about Mutex performance to do more than glance, since the standard library implementations are usually good enough. None of my applications have ever been bottlenecked by Mutexes when I've analyzed their performance.
The stride is still 128 bits, and "size" usually means stride.
The contents of the enum must still be aligned. The discriminant may be `u8`, but alignment makes it big. Even if you do the discriminant last (so e.g. first a `f64`, then a discriminant byte) alignment will still ensure that another 7 bytes would go unused between a subsequent instance (e.g. in a `[enum; 2]`).
But can't you already make syscalls through the standard library? How the syscall interface for UEFI for any different?
Compiler plugins are not ever planned to be stable; the closest we're going to get is procedural macros. For an overview of all of our macro stuff: https://words.steveklabnik.com/an-overview-of-macros-in-rust
I thought there was one but can't quite find it yet. I swear I read something somewhere...
Thank you, that was helpful. Perhaps an add to the docks a description of how `lazy_static`will not work for each of the examples. Also how does this compare performance wize? How many layers of runtime indirection are involved?
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/learnrust] [\[r\/rust crosspost\] What are some vital concept you wish you knew earlier in Rust?](https://www.reddit.com/r/learnrust/comments/9421ny/rrust_crosspost_what_are_some_vital_concept_you/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Yeah, that's what I've found however this pattern is often mentioned for app state.
I appreciate /u/steveklabnik1 even more when I go to one of these non /r/rust threads and see that he's already there providing excellent answers to hairy questions.
Yeah, disabling debug info and stripping symbols is probably going to have a larger effect on binary size than using the system allocator. The question is, why do `--release` builds come with debug-info, and why doesn't cargo automatically `strip` the release binary?
^The linked tweet was tweeted by [@copyconstruct](https://twitter.com/copyconstruct) on Aug 02, 2018 01:46:09 UTC (5 Retweets | 34 Favorites) ------------------------------------------------- . [@bcantrill ](https://twitter.com/bcantrill ) begins his talk about his summer with [@rustlang ](https://twitter.com/rustlang ) saying “I’m not in the midst of a mid life crisis, I assure you...” Well, that’s as good a start as any. [Attached photo](https://pbs.twimg.com/media/Djjw0L5U8AAB5Vc.jpg:orig) | [imgur Mirror](https://i.imgur.com/Dcl8jKh.jpg) ------------------------------------------------- ^^• Beep boop I'm a bot • Find out more about me at /r/tweettranscriberbot/ •
Why is \`Option&lt;u8&gt;\` two bytes large?
Q: Is it normal to not find a need for Traits and Lifetimes? When learning Rust from the Book v2, I spent a good deal of time trying and failing to understand these. Now that I'm a bit steeped in Rust programming and have enough command of the language to make my own projects, I haven't found a use for these. Are they mainly for making public APIs that might take varied data, eg different types of numbers or objects? I had a few cases where the compiler failed, suggesting lifetimes, but these were due to bugs in the code that when fixed, precluded this need.
[This](https://github.com/rust-lang/rfcs/pull/1632)?
I wrote a comment over on HN explaining https://news.ycombinator.com/item?id=17672998 Let me know if that doesn't clear things up!
Yes; I wonder why my search didn't turn it up.
If you're on x86, a lot of instructions are just sequentially consistent by default. (Although you should never rely on this.) If you want to try and see the issue, maybe try a architecture with worse guarantees? 
Thanks! If you notice, I have a specific style about it too. You don't want to barge into someone else's space, especially in threads like this where things can get heated. It's better to just answer factual questions, and be honest about shortcomings.
Sticking to objective points when there are so many subjective things about Rust that we love is a very challenging skill!
Understood. I'm curious about it even for nightly.
Ah, of course. Thank you for the explanation :)
And we're done: [https://crates.io/crates/resource](https://crates.io/crates/resource) I decided to go with `resource!` like I said. Give it a look if you're interested! :)
You _can_ do the syscall to get the memory, but you still need a way to inform things like `Vec::new()` to use that memory. That's what the global allocator gives you.
That sounds like a possible breaking change (in performance) to me.
You need 1 byte for the u8 then 1 bit to decide whether or the optional is empty or not. Since the smallest unit of memory is usually the byte, that one bit takes up a byte, hence two bytes.
Cool tool! I like shadowing myself, but it's true that it could accidentally cause problems. Great to see somebody offering a tool to help!
I assume its because you have to be able to represent 0-255 and None, which can't all fit in a single byte, if your u8 is non-zero then you only have to represent 1-255 and None, which can fit in a single byte
Release builds by default use `debuginfo=1` which provides only line tables, which are for printing backtraces on panic. Debug builds use `debuginfo=2` which includes variable and type information.
As XKCD points out ["Every change breaks someone's workflow"](https://xkcd.com/1172/). From when I saw the discussions, they didn't consider it a breaking change but they wanted to ensure there was a path forward for the people whose use case is best served by jemalloc. imo If we could never change the default allocator, that'd be pretty restrictive. Could we never upgrade jemalloc if it wasn't a perfect improvement for everyone? 
Currently alacritty uses Core Text on OSX, Freetype on other *nix, and rusttype on Windows. The main advantage would be DirectWrite rasterizing, but even just the fact that it's code alacritty doesn't have to maintain anymore could be useful.
because you want to get a useful backtrace out of release builds when they panic in production. Being left with "process aborted" as the only clue sucks so hard. The debug information in release builds is nowhere near as extensive (by default) as the debug info in debug builds, but it is enough to get _something_ useful out of a backtrace.
Which systems have poor allocators? 
You can make the largest variants be behind pointers. That will force indirection for some variants but leave others in-line. A struct like enum Value { String(Box&lt;String&gt;), Int(i64), Float(f64), Bool(bool) } is 16 bytes (8 for the data, 1 for discriminant, 7 for padding), while the version with the `String` inline is 32 bytes. https://play.rust-lang.org/?gist=bc417f11b442048a9003bb27b9367bcb&amp;version=stable&amp;mode=debug&amp;edition=2015
I think this has broken the search bar in Chromium over at https://doc.rust-lang.org/std/ When I type something, nothing happens, and this shows up in the console: ``` Uncaught TypeError: Cannot read property 'length' of undefined at findArg (main.js:689) at execQuery (main.js:954) at execSearch (main.js:1342) at search (main.js:1427) ``` This is with Chromium Version 68.0.3440.84. Still works fine in Firefox 61.0.1.
Please file a bug!
Thank you for the terminology!
The nonzero types are neat. Would they be recommended for use cases outside of performance, such as a parameter that is guaranteed to be nonzero?
They're mostly for layout optimizations, not for validating stuff like that. There's no math operations defined on them, for example, so you'd have to pull them back out into a regular number type everywhere.
Maybe [hyper](https://hyper.rs)?
Rust is arguably the nicest low-level, non-GC, systems-level language. Its generally as fast/lightweight as C[++], but includes features of modern languages like a best-in-class package manager, centralized documentation, neat iteration, high-level functional concepts etc.
There's actually an interesting history here: Ancient (pre-1.0) Rust allowed both `Foo{}` and `Foo` for instantiating empty structs, but because of parser ambiguities the `Foo{}` form was removed for some years. Eventually the parser ambiguity was resolved, and in the eleventh hour before 1.0 an RFC was accepted to make `Foo{}` legal again, in order to make things easier on macro authors (who otherwise would have needed to special-case their treatment of zero-field structs (this is also why we allow otherwise-useless one-element tuples)), and to minimize annoyance on people who are rapidly prototyping and may be wantonly removing or adding fields from structs ("prototyping" being the key word there; idiomatic Rust still prefers `Foo`).
Oh, just on time. I was just done compiling 1.27.
Thx. :)
Btw, I've had discussions with Gil Tene of Azul, and he's said that today he would actually write the the Azul JVM in Java... so I'm not sure that using a low-level language to write a better higher-level one is actually required.
Great!
You're right that they're mostly useful in libraries. They want to give the user maximum flexibility, so they'll do things like `fn split&lt;'a, T: AsRef&lt;&amp;'a str&gt;&gt;(input: T) -&gt; impl Iterator&lt;Item=&amp;'a str&gt;` so that the user can pass in whatever data format they have and put the results in whatever container they want or even no container at all, with maximum efficiency thanks to the lack of copying. In your own code, you know what you need and may not need 100% performance, so `fn split(input: &amp;str) -&gt; Vec&lt;String&gt;` can work just fine.
I'm getting really bad performance out of `std::io::copy` when reading from a Response using the `reqwest` crate. Here's some test code: extern crate reqwest; use std::fs::File; use std::io::copy; use std::time::Instant; fn main() { let mut resp = reqwest::get("https://nocduro.com/post/gp2s60b-breakout/images/header.jpg").unwrap(); println!("response done"); let mut f = File::create("header.jpg").unwrap(); println!("file creation done"); let start = Instant::now(); copy(&amp;mut resp, &amp;mut f).unwrap(); println!("Copy done in: {:?}", Instant::now().duration_since(start)); } And its output (the image I'm downloading is 360KB): response done file creation done Copy done in: 5.25180736s It's taking over 5 seconds to copy 360KB from the response body to a file (!?). I've tried doing it with BufReader and BufWriter with the exact same result. I tried separating the reading and the writing, and reading from the `Response` buffer is what is taking so long, even if I read into a `Vec&lt;u8&gt;` instead of writing to file. Am I doing something wrong? Why does it take 5 seconds to read from an in memory buffer? compiled with: cargo run --release cargo version: cargo 1.28.0 (96a2c7d16 2018-07-13) rustc version: rustc 1.28.0 (9634041f0 2018-07-30) reqwest version: 0.8.7 OS: Windows 10
Rust and Haskell are my favorite programming languages at the moment. For web frameworks, I still prefer [Yesod](https://www.yesodweb.com/book) especially in combination with [Esqueleto](https://github.com/bitemyapp/esqueleto) to write type-checked SQL queries. Diesel is nice, but can do only a fraction of what's possible with Esqueleto. 
Go is not a systems language. A web server is nearly as far from "systems software" as you can get. Good examples of system software include: - Operating systems - Device drivers - Hypervisors - Embedded/bare metal programs - Control systems Go depends on several high level features usually provided by an operating system, including threads and various concurrency primitives, whilst also having its own runtime to provide goroutine support and garbage collection. One of the great things about Rust is that it can do *all* of these things. There are still limitations, like limited LLVM support for more obscure architectures, or various legacy reasons, why you might still choose to use C in these areas, but Rust provides many compelling advantages in this space. One really great thing about Rust is that you can use the same language to build both these low-level foundations, *and* higher level constructs (like web servers) and even business applications. Also, regarding your test, you should know that actix is currently number 1 on the tech-empower benchmarks, above *all* other web frameworks: https://www.techempower.com/benchmarks/#hw=ph&amp;test=plaintext
I strongly disagree that "Java is arguably messier and more difficult to code in than Rust". I can't see what you base that on. I admit that my experience with Rust is limited as this point, but a review of the standard library code for both Rust and the JVM (I always start there, since if the language creators can't write easily understood code - what hope is there for the rest of us) makes Java the clear winner IMO. Java may be more verbose in many aspects, but that provides significant clarity. Java limits your options to provide clarity as well - GO takes this even farther.
Interesting question. I'm coming from Python and recently had to decide to learn Go or Rust. I chose Rust because it's more complicated and I'd learn more (general programming). But if you look at how fast Go is already, I would say that for most applications it's kind of hard to justify using Rust. 
You can write device drivers in Rust? I thought I had read that this is strictly out of scope - for now anyway. Again though, why would you ever want to build "higher level constructs" like "web servers or business applications" in a non-GC environment - that's just silly at this point. That debate was settled decades ago, and certainly not up for debate given the current hardware performance and advances in GC technology.
Also, I checked your performance chart - there are fractional performance differences between Rust and the GC systems implementations - I will GUARANTEE the GC based systems are easier to develop and work with.
You are right: You *are* missing something. GC may be fine for some workloads, but even Gil will admit that folks in the high-speed java space are trying their darndest to keep the GC idle during normal operation (I should know – it's what I do by day). Also the complexity is not incidental – it enables (and sometimes nudges) you to write less complex code for the same task. E.g. the rules for borrows are actually quite simple and once you've mastered them (with Rust, the compiler will get you there if you work with it), you'll find that you write safer, better code just naturally. So, in short, Rust enables folks who'd otherwise write high level (Java, Python, Ruby) code to work on a systems level (read C/C++) without having to fear UB and a host of other footguns. It's most-beloved language on the stack overflow survey three times in a row for that. So. What's your problem with that?
Is there a rationale behind nonzero not implementing match operations? Something like [this comment](https://www.reddit.com/r/rust/comments/940ewr/announcing_rust_128/e3hse8r) suggests would make sense.
But Go is second place at 99.8% of the speed of actix. And the source code is probably a lot shorter/easier. Why is it that Rust isn’t faster even though it doesn’t have a GC? I have a non-CS background, so I don’t have any clue about the details, but Rust only being 0.2% faster seems a bit disappointing. 
I think `NonZeroU64` would make a better example than `NonZeroU8` since `Option&lt;u64&gt;` completely wastes 7 bytes.
I disagree. I did HFT for the past 7 years. As soon as you have highly concurrent systems that need any sort of dynamic memory, a GC implementation is usually faster than any C or C++ based one - because the latter need to rely on either 1) copying the data, or 2) use atomic reference counting - both slower than GC systems. If you can write your system without any dynamic memory, than it can be faster, but I would argue it is probably a system that has pretty low complexity/functionality.
I'm not sure; the RFC text does not address it at all.
The main goal of reqwest is to provide applications with a batteries-included easy-to-use HTTP client. The async part of reqwest being in an "unstable" module was because there was strong pressure to stabilize reqwest at 1.0, but since futures are changing, the async portion would certainly need breaking changes later. Since it seems the "unstable" part scares people, perhaps that should just be done away with, since reqwest isn't 1.0 still...
&gt; If you're talking device drivers, etc. you can't write those in Rust anyway... Why not? What can you write in C that you can't in Rust? And what about all the other items that don't need a GC or large runtime or crappy FFI? Why is it only device drivers you can't write in Rust? Also, you seem not to mention or concern yourself with the security aspect at all.
Fedora is actually [actively trying to limit](https://lwn.net/Articles/761502/) the use of non-glibc allocators. Is it really so controversial that rust should use the system-provided allocator by default? 
&gt; You can write device drivers in Rust? There are multiple, full operating system projects in Rust. Rust can do anything C can do. &gt; I thought I had read that this is strictly out of scope - for now anyway. Maybe you read that you need nightly-only features to work on this stuff; that *is* true.
Of course you can... &gt; That debate was settled decades ago, and certainly not up for debate given the current hardware performance and advances in GC technology. You're simply wrong here. For these higher level applications, whether or not you use GC is mostly irrelevant. What matters is correctness, security and productivity. Rust excels in all three of these areas. I work at a company which is currently moving towards using Rust as our primary language on the back-end (from python) precisely because of these benefits. We've found empirically that our Rust services require an order of magnitude less maintenance to keep running, and so even if building them were to take longer (which we have not found any evidence of) it saves us a huge amount of time and money overall. There is simply no other language which gives this benefit without sacrificing in other areas.
could it be your antivirus slowing down a file copy? running your example on my windows 10 machine gives times around 80ms for the copy.
I understand because of the memory safety that general Rust (not using unsafe) etc. will be in most cases far more secure than similar code in C or C++ (due to programmer error). I would also argue that the same code in a GC (especially functional/immutable designer) would be far safer than the Rust code.
Good point! Just tried disabling windows defender and am still getting 3-4 seconds to copy :(
&gt; the only thing Rust offers is an attempt at memory safety - which is already solved by GC systems [...] I must be missing something. Boy are you ever. The only thing it offers? I am beginning to suspect trolling if it is not clear that it offers more than an attempt at memory safety. 35 years and you tell people to "use a GC language anyway" :-( &gt; The only knocks on GC based languages is [...] Wrong. Where do you get your information to so confidently say you know the only two knocks on GC based languages? What about runtime size? For example, I want to write an extension to something that as a C interface like a Java JNI native piece. Now, what safe lang would you choose for this? Have you ever used Go for a task like this? Even options like Swift, Kotlin Native, D, etc leave a lot to be desired with their weight.
Is it safe to allocate space via std::alloc::alloc then pass that pointer to a Box via Box::from_raw? Or would one need to wrap it in a type and impl drop to call std::alloc::dealloc?
It would be a poor argument to say far safer. And there are a lot of environments where you don't have a GC. Just because you don't use them daily doesn't mean they are there. Take WASM for example. Want to ship an entire GC with your WASM code? Please stop saying you can use a GC for everything. Please stop making false statements like you can't write drivers in Rust, Rust only offers one thing, GC's only have two knocks against them, etc. Instead phrase them as questions so you don't build your conclusions on a made-up false foundation.
Mmmm, to be fair to anyone who sees this comment, the Fortunes test (which is the most real-world of them that I see) still has a Rust project cracking the top 10, and it's a much newer project to boot... so I'm willing to believe there's a lot of room for growth. Now, whether it catches fasthttp &amp; co is a different story, but a lot of those top 10 ones become somewhat arcane to work with anyway (i.e, I wouldn't write something with h20). Comparatively I've found that the Rust one is still enjoyable to work with. Ultimately a web server doesn't matter too much since you'd end up scaling horizontally anyway past a certain point, but I tend to write some things in Rust because I prefer how strict the compiler is.
&gt; I would also argue that the same code in a GC (especially functional/immutable designer) would be far safer than the Rust code. What would be your evidence to support this argument?
A more seasoned Go expert can (and should) feel free to correct me here, but that Go variant is specifically fasthttp... which is good for larger projects, but from what I understand not fully compatible with everything else out there. In short it gets speed from being opinionated as hell. Which can be good, mind you. This all comes with the caveat that the project may have changed since I last worked with it, so...
What kind of HFT algorithm needs dynamic allocation? You must have had a *very* luxurious cycle budget then. In my experience you preallocate all you need, then just go through your buffers. See for example [LMAX disruptor](https://lmax-exchange.github.io/disruptor/). You'd be surprised how far you can get with this scheme in terms of functionality. Also in Rust you can often forgo atomic reference counting, as long as you have one canonical owner. Don't try that in C, btw.
&gt; One could implement […] You can't implement a trait on a struct if you didn't define either the trait or the struct yourself though. Such thing needs to be done in `std`.
So that only maters if your production binaries can `panic!` right? (most of mine use `panic = abort`)
Is `debuginfo=1` if `panic=abort` on release?
It also applies to crates that use `failure` (or similar crates), since `failure` can generate backtraces using that same DWARF information without actually invoking a panic.
I think you’re confusing GC and memory safety? I know I’ve written plenty of memory-unsafe things in GC’d languages...
&gt; In short it gets speed from being opinionated as hell. Ironically enough as we discuss it in a Rust thread, it gets speed from asking the developer to respect certain object lifetimes it can't enforce in code.
&gt; Release builds by default use `debuginfo=1` I don't think that's true -- the [profile docs](https://doc.rust-lang.org/cargo/reference/manifest.html#the-profile-sections) say the default is `debug = false`. Then it doesn't pass any `-g` or `-Cdebuginfo` flag to `rustc` at all, and `rustc` defaults to 0 (`NoDebugInfo`). You'll still pick up some debuginfo from `libstd.rlib` etc. though, according to the way it was precompiled in the Rust release. (But not `std` code marked `inline` or yet to be monomorphized.) The linker will just include that existing debuginfo unless you actively strip it.
Rust doesn't use jemalloc on Windows though, only the system allocator.
Watch
I'd rather approach `OnceCell` as "another kind of interior mutability, which have many uses, including the `lazy_static!` one of creating global data with complex initialization", and not as a direct competitor to `lazy_static!`. Though undeniably the second one makes for a more clickbaity reddit link title :) I have added a comparison with standard interior mutability facilities though: https://github.com/matklad/once_cell/tree/pretty-table#comparison-with-std As for the performance, I have made exactly zero benchmarks, but I think it should be pretty much the same as for `lazy_static!`, at least in theory? The `get` operation is one `load(Acquire)`, `set` calls `.call_once` directly, just like the `lazy_static!`. 
Ah, yes, I was mistaken about the default for release builds.
It turns out I was mistaken, and the default for all release builds is actually equivalent to \`debuginfo=0\`. But note that \`panic=abort\` doesn't affect backtrace printing, so line tables can still be useful whether unwinding is enabled or not.
Stability without stagnation.
First off, as other readers have pointed out. You need to use nightlies to write drivers. I’m sorry but at this point it is not a C replacement. Also, have you seen the size of a GO executable ? I’m on the road right now so I can’t give the hard numbers but when I last looked it was fairly trivial. Most GC is fairly trivial code. It is not large. And yes you can. Your bias against GC is somewhat alarming. Most trivial GO programs won’t have any GC anyway due to escape analysis. I don’t think you know what you are talking about. I’ve written plenty of systems in C, C++ and multiple assembly languages. I agree that GC is not appropriate right now for low level systems code, but what percentage of developer effort around the world is this? And on top of that it has been proven (via Linux) that these systems are easily written in C. 
Even with `panic=abort`, it will be able to print out the stacktrace.
Im not. I would guess that greater than 99% percent of security exploits are due to buffer overruns which are not possible in GC/safe environments. The others being injection exploits or really exotic cpu bugs. We probably have a different definition of unsafe. I consider unsafe being a security exploit, a program crashing due to panic/exception is not unsafe. 
From what I've seen, rust isn't that much faster than GCed languages, but it uses much less memory, at least compared to idiomatic implementations.
The fact your company built everything with python originally doesn’t give me warm feelings about the engineering. Sorry. Have you read “dreaming in code”? Any seasoned engineer would of stopped that nonsense. 
Btw, the LMAX guys have given up on garbage free. These use Azul Zing. Java is just not the language but the extensive libraries - which are not garbage free - so trying to write GC free Java is a fools errand unless you rewrite all of the stdlib and third party libs. 
Yawn
Traditionally, “memory safety” includes things like: use after free, dereferencing dangling pointers, memory leaks, race conditions. Rust does a very good job at tackling these problems. And a GC doesn’t not solve all of them.
 That some users require nightly means it's not a C replacement? What kind of logic is that? I don't have bias against GC, I do far more Go and JVM work than Rust. I know every Go program comes with the GC regardless of size. I'm just not foolish enough to make flat out false statements (I've counted 5 so far with no admission) and build assumptions based of that. I try not to assume the worst, but I have to assume troll at this point.
Yep, you’re right, its been proven time and time again that C code is completely memory safe. No segfaults or security exploits have ever been found in any system written in C. It’s time to close the Rust project down guys. It was fun while it lasted.
That sounds like Rust's memory allocator requested more WASM pages which then invalidated the original ArrayBuffer (which got replaced by a new, larger ArrayBuffer).
You don't need to download entire video - you can just Ctrl-V the YouTube link into VLC window and it will start playing right away (alternative for mouse users: Media-&gt;Open Network Stream...)
Box allocates memory with `std::alloc::alloc`.
Your totaly right and here is the Rust API guideline advices: https://rust-lang-nursery.github.io/api-guidelines/flexibility.html#c-caller-control
Wait so `RUST_BACKTRACE=1` prints an accurate backtrace even when `panic=abort` ? I just always assumed that with `panic=abort` libbacktrace would not be linked and the only message I get is file and line number where the abort was triggered, but that's it. I need to try this.
Seems like the documentation for `Box::from_raw`and similar functions should be updated to reflect that.
How come? The guarantees are the same if you were building a box from a pointer you got from C.
Not sure, but at the instruction level comparisons with zero are always fast whereas comparisons with arbitrary integers can be slow.
i'm making my first indie game. I got c, c++, rust. I'm not an expert of even intermediate coder for any of these. I find Rust to be the simplest of the bunch, by far.
Application developer here. I burned that gc language ship when I arrived at rust. No regrets. Only hope and inspiration now.
You might be looking for /r/playrust - this sub is for enthusiasts of the chemical process of iron oxidation
Thanks for the response! \&gt; That sounds like Rust's memory allocator requested more WASM pages which then invalidated the original ArrayBuffer I think so, too. \&gt; which got replaced by a new, larger ArrayBuffer Would you mind explaining how i can access the new \`ArrayBuffer\` or provide an alternative solution?
Some languages default to having user-defined types being allocated on the heap, meaning fundamentally the data that live son the stack is a reference, and what you defined is stored on the heap. This is your typical OO object with smart pointers. In Rust you can define a value type with a known size which can be placed on the stack, In Rust, while you can define something to fundamentally be a reference type, you can instead define a value type (the direct data) and then later wrap that in the right smart pointer for your task at hand. So reference types are more ad hoc when/how you want them, rather than being default or one-size-fits-all. `Box` is just your standard pointer to heap allocated memory. When the reference on the stack is popped, a call to free the data in the heap is inserted by the compiler before doing so, so that you never leak the data. An `Rc` is like a `Box` except its for use-case when it might not be clear when to free the memory, (e.g. when you want to give out multiple handles and these might be stored in heap allocated memory. meaning you won't determinstically know which will be the last reference.) Basically once the last reference to `Rc` wrapped data goes away, a call to `free` is dynamically dispatched. made. `Arc` is just a version that is thread safe (multiple threads can be taking handles to whats inside the `Arc`.) This is a good resources (It's from the first edition): https://doc.rust-lang.org/book/first-edition/choosing-your-guarantees.html It also helps to be familiar with the stack and the heap, which the 1st edition also has a nice little refresher on.
That's going to be difficult. Here's translated list of songs on that album, with YouTube links to the original and to the arrange (when found). 1. Lunar Clock ~ Luna Dial [orig WAV](https://www.youtube.com/watch?v=01skyPMeeoc) [orig MIDI](https://www.youtube.com/watch?v=EIprf6DgNRc) 2. Theme of Morroc from Ragnarok Online [orig](https://www.youtube.com/watch?v=v4b7XJPjNWI) 3. is not listed [here](https://altneuland.net/gallery/the-citadel-where-to-return/) 4. Flight of the Bamboo Cutter ～ Lunatic Princess [orig](https://www.youtube.com/watch?v=pw2WQjqKpVM) [arrange](https://www.youtube.com/watch?v=aQkmCW9hmSw) 5. Shanghai Teahouse ～ Chinese Tea [orig](https://www.youtube.com/watch?v=UbqoImQZ-EY) [arrange](https://www.youtube.com/watch?v=o6TYhPRIW2E) 6. Hiroari Shoots a Strange Bird ～ Till When? [orig](https://www.youtube.com/watch?v=xi_ZC_Hyhmo) 7. Bloom Nobly, Ink-black Cherry Blossom ~ Border of Life [orig](https://www.youtube.com/watch?v=VcfQxcRGg8s) 8. Necrofantasia [orig](https://www.youtube.com/watch?v=195XntreoMc) 9. U.N. Owen was Her? [orig WAV](https://www.youtube.com/watch?v=SyC5eWJhCr8) [orig MIDI](https://www.youtube.com/watch?v=cgRF6ixQ0bs) [arrange - skip to 21:19 - which will take you forever without premium so I didn't verify this one](http://www.nicovideo.jp/watch/sm3873401) Other songs by アルトノイラント I found meanwhile: - Centennial Festival for Magical Girls [orig](https://www.youtube.com/watch?v=Zs7zF8AxQCc) [arrange](https://www.youtube.com/watch?v=3THQ147V6KY) - Lunatic Eyes ~ Invisible Full Moon [orig](https://www.youtube.com/watch?v=lY_KORpQPdI) [arrange](https://www.youtube.com/watch?v=F3C4UIUmYzI) - Voyage 1969 [orig](https://www.youtube.com/watch?v=ptQHyJiZM_g) [arrange - first minute of this](http://www.nicovideo.jp/watch/sm3823059) The album can be bought e.g. [here](https://www.jauce.com/auction/d192040527) but I'm not willing to pay &gt;$20 for shipping alone.
Finally `std::alloc` is stable! This means that for the first time in 3 years, [Inko](https://inko-lang.org) (VM is written in Rust) can be built using stable Rust! Now if intrinsics ever stabilise I could ditch nightly all together, but I don't see that happening any time soon.
I've seen you make this statement before. If your intention is to attract effort toward bringing up more platforms to tier-1 status then this isn't a helpful way to characterize the situation.
Getting the first N elements of a slice is really convenient (`&amp;myslice[..N]`). But getting the last N elements is a drag. I have to call `len()` twice and do some subtraction. And if I'm taking a mutable slice, it doesn't even compile right now (at least until NLL lands) without a temporary variable. Has anyone thought about adding a convenience method for this? Is there maybe a method already that I'm missing?
Shouldn't `Lazy` require `FnOnce` instead of `Fn`?
Thank you. So why do they have separate docs?
That's a good point, but the `NonZero` types [are in `std`](https://doc.rust-lang.org/stable/std/num/index.html), so it's pretty much fair game. :)
Check https://medium.com/learning-rust/rust-beyond-the-basics-4fc697e3bf4f
I think this is because efail.
Oh, didn't even think about that. You're probably right, my connection isn't the fastest where I am right now. In a web browser with cache off it should take around a second or two on my connection. I'll double check on one of my VPSs later that has more speed. Thanks!
Maybe this sub-thread will help? https://www.reddit.com/r/rust/comments/940ewr/announcing_rust_128/e3hoo7r/?st=jkdctvha&amp;sh=e4d8205a
GC is bad for systems languages because it makes FFI much more difficult. In a systems context, you generally have the C ABI as your basic interoperability mechanism, with all of the components of the system written in various languages (though mostly C and C++). What happens if you try to write a standard library in Go? How does non-Go code link with that library? Go didn't have any support for this at *all* until Go 1.5, released in 2015. Other popular GC'd languages (Java, Python) are similar -- integrating with them from other languages is a massive pain (and integrating two codebases/libraries in different GC'd languages is particularly hard). If you want to write a library at the "system" level (where you can expect that it will be used by binaries in a different language), then your library will be far easier to use (and therefore *better*) if it is written in a non-GC'd language. Side note: I used to do robotics, and the Azul JVM is the only GC'd language implementation I've heard of that's even remotely suitable for hard-realtime robotics uses. I don't see it picking up much popularity because it's not open source, plus it supports Java (and newer languages -- such as Rust -- are far nicer to program in than Java). Can it even run on a system as limited as an Atom-based nettop? I know it originally required specialized hardware; I've never seen server-grade/high-powered hardware that fits in a form factor suitable for robotics.
let mut slice: [u8; 4]; = [0; 4]; slice.copy_from_slice(bytes[..4]); Probably something like that, I'm not at my pc to check it - but you want the copy_from_slice method.
A very trivial reason I use Rust: `Option` is a savior. Kotlin is the only JVM language that I've actually used that has explicit nullability, .NET doesn't have an option (well, F#, but I'm not counting purely functional for my second point), and Swift isn't a GC language (it's reference counted). Go has `nil`. There's a reason that implicit nullability has been called the million-dollar mistake. A secondary, slightly trivial: my (personal) productivity is greater in procedural languages with functional influences (like Rust or Kotlin) than pure functional languages. Most classical algorithms are explained in a mutable manner (though you probably shouldn't be implementing classical algorithms...), and most domain-specific algorithms are also specified in a mutable manner (CRUD, etc). And as a final note, language comparisons tend to ignore build tools. `cargo` and `rustup` are lifesavers for dependencies and Rust updates respectively. Solutions exist for other languages but none of them are as well adopted and integrated as `cargo` is. I also enjoy the rapid update pace of the language: I can rest easy knowing that my tools are improving, and that I can help improve them. Rust's ownership models have made me a better programmer all around. The hidden benefit that is easy to look past though that applies to any problem domain is the sticker feature of Rust: _fearless concurrency_. While writing safe Rust code, I don't have to worry about thread safety and can just push the make-me-parallel button (`rayon`) or write more explicitly threaded operations (`crossbeam`) or even just write async code without fear. For GUI programming and mobile platforms, sticking to the native language that those libraries were built around will always be the best option. When developing against industry-standard tooling/engines, you use the language they were designed for as well. For pretty much everything else, though, I prefer to choose Rust where I can. The strong, expressive type system and the compiler/clippy pushing me to write better code make me a better and more productive programmer. Rust allows me to refactor without fear of breaking things in subtle ways. I've never achieved that in other languages.
yes
Thanks, I appreciate an alternative. I remember when I first had to touch code with `lazy_static!` it first seemed magical but then occasionally frustrating because the type was hidden behind the macro, it made it non-obvious in what situations it would cause problems and how to resolve them. A non-macro version will probably help make this clear. But now I'm more familiar with Rust, the motivation to migrate is less, especially since I don't knowingly have problems with the use of `lazy_static` in my code. To lower the barrier for considering porting, mind providing a migration guide, even if it is "before" and "after" example code?
I've not messed much with Swig. Does it have higher level binding descriptions (e.g. something like IDL) than C? I'm curious if we could provide easier to use, higher performance bindings by generating that directly rather than generating SWIG bindings off of a C header.
Interesting. I have a bunch of question. Which GC do you use? Does it have a STW? How large is your heap?
I used the Azul JVM with heaps larger than 64 GB. Pauses were very infrequent, and typically under 100 us. Using the latest 1.10 GO, it appears to very similar pause times, although I have not tested it extensively with large heaps.
Your compile times may not be the best but while you wait you can head over to /r/playrust.
I would write it in C or C++, although I think if you did extensive reviews of JIT compilation in a modern JVM you would find very little need. Typically the only native code I've needed to write in a while has been to access OS specific features that are not exposed in Java, and in these cases I've used C, and its fairly trivial to do so. In fact a lot of the JVM stdlib is in native code, but a lot of the native was moved to Java with OpenJDK. I am definitely not trolling. I have been evaluating Rust. As far as I know, even with Rust, if you want to call a C function it needs to be in an unsafe block - so there goes your safety. I'll reiterate my point - if you are not doing dynamic memory allocation, then the application is probably straightforward (and possible trivial) - and so something like C is simple to write and maintain. If you are using complex object hierarchies and lots of dynamic memory - you are going to essentially write your own manually controlled GC. If you think that every developer can do this better than the teams of developers that write the actual GC code you're kidding yourself - and if you just let the GC do its work the code and structure is much simpler - ESPECIALLY for highly concurrent systems.
As I said earlier, I used the basic 'hello world' web server using the built-in go stdlib, and the Rust one - the GO server was 4x faster... I was surprised at that, but thinking about the concurrency, and stream processing, it's possible. There are many studies that show GC is far faster than malloc when both allocation and de-allocation are measured (do a google search). The only time malloc type memory management is faster is with highly customized allocators designed for the task at hand - no one should need to do this for business or general apps... Look at the Linux kernel - lots of specialized memory memory management based on the task and usage.
That is simply not true. Many studies show manually managed memory to be the number one cause of bugs in software. Here is a great study from US Davis - [http://web.cs.ucdavis.edu/\~filkov/papers/lang\_github.pdf](http://web.cs.ucdavis.edu/~filkov/papers/lang_github.pdf)
You misread - the nightlies are required in order to do device driver development. Sorry, but when evaluating a language/platform I'm apt to look at the 'release/stable' version - much easier for me at least to get questions answered.
Azul Zing does not require specialized hardware, but it is not suitable for Arm/Embedded systems. That being said, all of Android is GC - so the most embedded devices in the world are GC based. I am impressed at the GC pause times in GO, and the new Shenandoah GC in OpenJDK is amazing - fully open source - see [https://wiki.openjdk.java.net/display/shenandoah/Main](https://wiki.openjdk.java.net/display/shenandoah/Main) 
It says the exact opposite of what he is proposing. &gt; If a function requires ownership of an argument, it should take ownership of the argument rather than borrowing and cloning the argument.
I am not disagreeing that Rust is a better C, just that I see no real use for it. No one is rewriting the Linux OS in Rust. Almost all other applications can be written faster and better in a GC language. And for stuff where C (systems programming) is required most really good programmers understand the memory dynamics anyway, and to me it seems overly verbose when you get into highly complex concurrent code. Some of this too, as I stated earlier, look at the Rust code that makes up the stdlib and compared with the Java stdlib, there is no comparison as to readability, especially in the highly complex concurrent structures.
Finch is a Scala combinator library for building Finagle HTTP services. In some ways the Finch API is a superset of Finagle. 
You can use `&amp;myslice[myslice.len() - N..]`. [Playground](http://play.rust-lang.org/?gist=b8ec8b555e4f7e17b53a6a3195d61cb6&amp;version=stable&amp;mode=debug&amp;edition=2015).
&gt;There are currently only 2 ways to do it:
Absolutely interested.
&gt; GC implementation is usually faster It depends. Moving GC can do wonders. But Java doesn't have C/C++/Rust's structs. If you need an array with a million objects then you'll get a million and one allocation in Java while in Rust you'll get one. You'll also lose cache locality in Java. Due to type erasure in Java there won't be as many opportunities for monomorphization as you get in Rust or C++. Then there are traps that are easier to avoid in Rust than Java - dynamic dispatch is the default in Java and closures don't require allocation in Rust. After that there's a host of other niceties, one feature among them that Rust can hold over most other nonacademic languages is data-race safety. Java doesn't do anything at the language level to prevent you from getting ConcurrentModificationException exceptions and headaches due to concurrency heisenbug are probably the ones we would most want to avoid. 
As I said earlier, arguably the largest computing platform in the world by device count is Android - which is GC. Even the originally Objective-C was GC (using ref counts), but they've moved to a Rust ownership model - anyone with an iOS device care to comment on how many more app crashes they started to experience ? I know I have. I don't have huge experience with cargo, but I would offer that Maven, or npm, are pretty complete - not sure what they are missing that cargo would offer. I will investigate the rayon, and crossbeam. That's interesting, because the concurrent code examples I've reviewed in Rust are pretty horrible IMO. I've written million LOC+ systems in Java, and NPE's were never a problem, certainly not one that typically exposed itself in production, but that being said, not having null/nil definitely makes things safer, but the null object reference is importent in GC languages because it makes it easier to avoid unnecessary allocations - essentially lazy creation - without it you need to have a Option class and JVM support there.
I am not sure that is the case, again you can reference that Android runs on some pretty low-end devices - granted not 8k embedded SOC, but typically larger heap gives the collector more head-room so under stress it can avoid large pauses because it can keep allocating "until it gets a chance to clean-up".
Can you clarify that? AFAIK you don't need to respect any object lifetimes in Go (or any GC language) - outstanding traceable references determine the lifetime - that is the whole point of GC.
Loving the tutorial so far. Good job showing how to make robust, idiomatic code without making things too hard to follow. I'm pretty much a Rust n00b and the biggest hurdle I've run up against is getting code completion to work in lesson 6. I am getting a little tired of this triangle though ;)
S/match/math FTFY. :)
That is not what I stated - I said the code readability of the implementation. You are referring to the public API - two different things, and even then most of what you cite is just a lack of understanding of boxing, memory usage, and the language specification. For example, I can override hashCode() and return 0. Nothing bad happens... now depending on how you use that object and the library you might have problems, but I'm fairly certain nothing in the stdlib will have issues with that. As an anther example, even with Rust, you can't tell me the memory usage of Vec(int8) with 1 million elements... I'm sorry but I don't think your criticisms are not well thought out.
Ok, at this point you're asking me to believe a pro with 35 years of experience, 7 of which in the HFT space only now creates a reddit account to...spew FUD about Rust and Java? That's stretching credulity.
There was a loss of throughput but it varied greatly based on the type of code being executed. Math/computational code shows little degradation, highly allocation intensive code seemed worse. We saw some loss up to 20%, but later releases of Zing were much better. I would suggest looking at the Go or Shenendoah projects for more publicly available up to date information on the state of the world. I think the latest Go release raised the pause times in order to improve throughput?
I never claimed GC was unacceptable for embedded -- only questioned what hardware the *Azul* VM (specifically) could run on. Note that Robot-controlled systems are embedded systems, though larger ones may be x86/amd64-based. On the other hand, Android is not the epitome of performant OSes; I frequently observe stuttering and delays while operating its UI (which may or may not be GC-caused). For some applications, 1.8ms (from the Shenandoah link you posted) is a *fantastic* pause time. For others, it's enormous. I'll reevaluate the performance impact of GCs when I see a GC'd game that doesn't stutter.
There are subjective tradeoffs in Go that I personally don't appreciate like no generics or [structural typing](https://en.wikipedia.org/wiki/Structural_type_system). &gt; you can run Go programs with 'data race' detection This is done at runtime so either there's a performance penalty or it isn't comprehensive. Rust will throw error at compile time if it detects it. I wouldn't be surprised if the only other mainstream languages that can do that are purely functional ones because in imperative land you pretty much need rust's lifetime system to insure at compile time that only one thread can write to a piece of memory at the same time without anyone else reading it. 
I used plenty of non-GCd games that stutter. I do think a lot of the stutter in Android is related to GC, but it has certainly gotten better. And I use iOS all the time, and it stutters/hangs like crazy as well - especially when playing games (I am betting most of it is the VM system paging...) Also, the Go GC is claiming pauses less than 1 ms now, which aligns with my testing.
The unsafe keyword does not come at the cost of safety, it comes at the cost of guaranteed safety. That's why the keyword exists, you explicitly tell the compiler to trust you as the programmer. Canonical example is the implementation of a vector, it requires uninitialized memory. It's not unsafe in that context, but the compiler doesn't know that. When you call C functions you're implicitly trusting that it's safe, since the compiler doesn't have any idea it's unsafe. That said, iirc not all FFI calls are unsafe. Just most useful ones, like passing around arrays or anonymous structs. 
Ok, and as soon as you do that - you are leaving it up to the developer. Not to different than using NULL and uninitialized objects in Java. If the developer uses it wrong you're going to have a problem - still not going to be a security hole though - but certainly could be one in Rust (as you can double free, etc. all the protections are gone I assume).
Well, you say that, but... my experience is different. ¯\\_(ツ)_/¯ 
As I said in another comment, I agree with many of the criticisms of Go as a language. I don't know enough about the data race safety in Go, but I can't see how it can work in a concurrent program using ARC - you need higher level synchronizations to know whether there is an actual data race and the synchronization can happen in a variety of ways. Simiarlly in Java, because there is a runtime, often times the synchronization code is essentially bypassed because it can detect that the data is not shared - impossible to do I think in an environment without a runtime.
From https://github.com/valyala/fasthttp: &gt; VERY IMPORTANT! Fasthttp disallows holding references to RequestCtx or to its' members after returning from RequestHandler. Otherwise data races are inevitable.
As soon as you do what? Use `unsafe`? It's quite the opposite really, you use `unsafe` code underneath a safe interface. The only time you as a developer need to use unsafe blocks is if you're intentionally and explicitly bypassing the compiler to do something you know is safe that the compiler doesn't (for example, raw pointer arithmetic to avoid a bounds check on a buffer you know is a certain size), or if you're calling through FFI and the compiler can't guarantee some arbitrary binary is safe. 
&gt;the code readability of the implementation My bad. &gt; I can override hashCode() and return 0. Sorry my point was unclear and it is also a rather minor point. Java String cache their hashes. It works by using 0 as a special value that represents not-computed yet. A possible attack for a system that relies implicitly on this optimization is to send it strings which hashCode is 0. (the cache will not work for these values) &gt; Rust, you can't tell me the memory usage of Vec(int8) with 1 million elements... My point was that being forced to box primitives to put them in a collection is extremely expensive. I did not mean that the memory usage is more unpredictable in Java. 
Oh, you were referring to the fasthttp web server... To be honest, 'object pools' in most cases have been proven to be slower than direct allocation except for the largest of objects with complex initialization. Just by reading that warning it appears the RequestCtx is being reused between requests with probably no reason to do so... but there is probably no reason to retain a reference to it on the previous callback either.
Oh wow, I was totally forgetting I could leave off the second range bound there :p
Just out of curiosity, where does exception handling standard in HFT? What’s your typical time budget? I knows some companies (non financial related ) discourage the use of exception, and it would be interesting to know how exception is used in HFT.
Awesome! I live in Hampton Roads so Charlottesville is only a couple hours away and is worth the commute every time. Thanks for organizing this!
Sorry, I actually had it backwards - it was originally non referenced counted ownership based, and they added automatic reference counting. So my theory on the crashes was wrong... Most of IntelliJ is written in Java not Kotlin. You can view the IDE source at https://github.com/JetBrains/intellij-community I don't think you can have a proper OO system without GC. It is just too hard if you have complex object graphs and concurrency. That being said, I used C++ to do OO, but never in a multi-threaded context. 
Yeah man. Hopefully things will get better with rust 2018. 
There is a branch that supports poisoning.
I actually won't disagree there. Part of OOP is references everywhere and that doesn't really work well with the kind of strict ownership Rust's model is. I will concede that if you want to do OOP (as in message passing) that a GCd language is your best choice. If personally point you in the direction of the JVM, as Kotlin is my second favorite language and JVM language interop is magically seamless. But I'll argue that for a large percentage of cases, OOP (as in message passing) is not the best solution. The industry is increasingly turning to functional and data-oriented designs, and Rust is great at the latter and as good or better at the former as any other primarily OOP language. Any modern approach has to have some approach towards multithreading. The growth dimension of computers is no longer straight-line speed but rather parallel capacity and throughput. Rust's is scoped mutability and Send/Sync guarantees. All of that said, I know the value of a GC in use cases where ownership is shared, and am one of the people on-and-off experimenting with what a GC design would look like implemented in Rust for use in safe Rust. The power of Rust is choosing your abstractions. Having a GC as one of those options can only broaden the expressive power of the language. And you'll find that _new_ code added to IDEA is primarily Kotlin. The main point of Kotlin was seamless Java interop so that JetBrains could incrementally write new development in Kotlin. New plugins by JetBrains people are typically pure Kotlin, such as IntelliJ Rust even.
I guess when you break it down, I see at least 5 different memory access methods : value, reference, RC, ARC, raw pointer and there are probably others. Contrast this with Go - where there is one - and the computer figures out the best method (escape analysis, shared data detection, etc.). I think often there is the human fragile ego at work - where we as humans don't want to acknowledge the machine is better, and it just gets worse when there are thousands of talented developers making the machine (GC) better. Contrast that with a single developer trying to get the memory references and ownership correct in the highly concurrent system - extermely difficult. I think many people prefer the latter just to "prove I can". I guess as I get older I prefer to be productive, and spend my free time with friends and family rather than figuring out complex structures (that should be simple). As I referred to prior, look at the source file for vec.rs and compare that with LinkedList.java - no comparison - and the performance is essentially the same.
doesn't jemalloc cut down on the number of syscalls dramatically? that's pretty big for performance, even on Linux. I've been very happy with jemalloc as default, but I'm glad it is now able to be swapped.
(Raw pointers are not a part of safe Rust; if you're considering them, you have to consider Go's unsafe as well.) If you want to compare list implementations, compare apples to apples, or in this case, linked lists to linked lists. Vec is closer to ArrayList. But the average person isn't writing these building blocks anyway, or at least shouldn't be. (Also, don't forget to include superclasses' complexity into the budget.) There are multiple ways of having a handle to data in Rust, but they're all semantically meaningful. In Go as I understand it, you just have your data blob and it's mutable. In Rust you either own the data, thus can mutate it (`Type` or `Box&lt;Type&gt;`), are borrowing it from someone else (`&amp;Type`) and might be allowed to mutate it (`&amp;mut`) if the loaner allows, or it's shared ownership (`Rc`) and you need to coordinate access. It's not just a different handle to data, there's different semantics to each one, thus Rust separating them out. I'm not one on the Rust train for low-level control, but these semantics are important enough that I'd include an owned, borrowed, and shared state into a language of my own design.
As others have mentioned, the memory has changed. I recommend checking for this before each memory access with something like the following: function check_mem_realloc() { if (bytes.byteLength != exports.memory.buffer.byteLength) { let bytes = new Uint8ClampedArray(exports.memory.buffer, ptr, size); } } In my own code, it looks like: _check_mem_realloc() { if (this._mem.byteLength != this._raw_mem.byteLength) { this._mem = new DataView(this._raw_mem.buffer); } } If you want to have a cleaner experience and have this kind of stuff handled, I would recommend using the [`stdweb`](https://github.com/koute/stdweb) rust crate. It adds a minimal amount of code in exchange for automatically handling things like this.
Go and Java do it at runtime and not comprehensively because they are imperative languages with shared global state. &gt; but I can't see how it can work That's why you should first learn the language before coming gung-ho to the sub implying there's no good reason and [everyone is mistaken](https://www.rust-lang.org/en-US/friends.html) for using the language. I can only attempt to explain why it works, and the explanation won't be accurate in every way, by saying that rust tracks one additional piece of information for each variable (not value behind that variable but variable itself!). Most static languages only explicitly track type (compiler can then track host of other stuff like whether variable has been initialised at certain points), Rust's novelty is that it explicitly tracks types and another construct called lifetimes for each variable. Both variable's type and lifetime are tracked at compile time and "evaporate" at runtime. Through lifetimes and carefully constructed standard library the system knows how long each variable lives and prevents you from using them when they overlap and standard library is written such that it works over threads. Now this is the part that I hope blows your pants off and makes you jump over to Rust - the same system automatically works over handles to resources as well. It can, at compile time, prevent you from accessing the same file handle or the same sql connection handle inappropriately. It's bloody ingenious. :)
I'm interested as well. Thanks.
I am in Portland and am very intrested 
Thanks, I figured that's the biggest issue with Rust - learning the new patterns. No worries about the triangle: try to run incoming lessons 15-x, 16-x and 17-x ;). However if I were to restructure the lessons a bit, I would certainly introduce at least one more geometric shape _before_ the error handling.
About JSX and the procedural macro, I'm quite fond of `horrorshow`'s [syntax](https://github.com/Stebalien/horrorshow-rs). Maybe you could consider something like that instead? I don't really like JSX either, and requiring nightly for it seems unfortunate. And about your mutability woes, I think I've seen some approaches (maybe in [`relm`](https://github.com/antoyo/relm)?) where the event handlers don't mutate the DOM directly, but fire events. This way you might avoid the `Rc&lt;RefCell&lt;T&gt;&gt;` mess. Actually nevermind, you mention that [here](https://www.reddit.com/r/rust/comments/933tke/writing_smithy_a_frontend_webassembly_framework/e3afl95/).
Note that Rust already used (and still uses) the system allocator in some targets, so if you were only targeting Windows, for example, this always worked.
Yea I was too! The syn crate has a really useful API for parsing Rust code!
I haven't done any benchmarking but from what I have read the original ctypes binding I created for my library was probably faster than the Swig version. However, since I also wanted R and Java bindings, the convenience of being able to generate them from one file more than made up for any performance hit from using Swig in my case.
Fair enough. But from a utility perspective, if I'm using an integer, I'm more likely to want to store a value of `0` than `MAX`.
I'm struggling with lifetimes. I'm reading through the second edition of the book, now on Chapter 12 (making minigrep) and I have a hard time visualizing when to apply lifetimes. I kinda understand what they are, but when would I need to annotate them explicitly in my own code goes over my head. Do any of you guys maybe have some sources on lifetimes in Rust? I'd love some more visual ones, with diagrams etc. if they exist.
We're waiting for const generics to add a generalized mechanism for doing this.
It makes it a bit easier to take baby-steps towards interoperation between shared and static Rust libraries (which are build with different allocators by default). It's not there yet, as handing a `Vec` from one library to another is UB (could have different layouts), but small steps.
`NonZeroX` is nice because you save space on `Option&lt;NonZeroX&gt;` ( it just has the size of `X` and `None` is represented by `0` instead of an additional flag). What would be the use-case for `NonMaxX` ? 
&gt; Are they mainly for making public APIs that might take varied data, eg different types of numbers or objects? That's basically it. Traits are for specifying the few aspects of a type that you actually care about in that piece of code, instead of specifying a single exact type. It's like saying "I want something Small and Heavy" instead of saying "I want a Brick". 
This is the same as unsync flavor from once_cell and the LazyCell from lazycell.
Ideally it should yeah, I just haven’t implemented that yet.
 The implication would be that you'd also save space on `Option&lt;NonMaxX&gt;`. The max value would be used internally for representing `None` instead of 0.
Thanks. Is there a discussion you can link to which explains the connection?
ah ok. It could also be `Non42X` then ;)
Note that `lazy_static` only uses macros for convenience, and its type doesn't require `const fn` because of associated consts, and passing the initializer to the getter, as opposed to having e.g. a function pointer - but you can use a function pointer in a wrapper around it with only public fields, so again no need for `const fn`.
Best I can find is this: https://github.com/rust-lang/rfcs/pull/2307#issuecomment-361070510
`NonZeroU8` might be useful for storing an ASCII value, `NonZeroUsize` for `descendants_or_self.len()`. But you wouldn't use `NonZeroUsize` to store `descendants.len()` because 0 is a value that you'd need. Wanting a `NonMaxUsize` for this scenario isn't so esoteric.
I am far too amused that `NonZeroX::new()` and `NonZeroX::new_unchecked()` should compile down to the exact same code.
My only question is this - where are all the Rust jobs? Self-learning only goes so far, and the few I have seen listed are blockchain jobs with almost shady descriptions. Anybody know where to seek out Rust jobs? Remote would not only be fine, but welcome!
Thanks. Maybe I don't follow this completely, but what's wrong with something like: trait InvalidValue&lt;T&gt; { const INVALID: T; } struct NonZeroU32(u32); struct NonMaxU32(u32); impl InvalidValue&lt;u32&gt; for NonZeroU32 { const INVALID: u32 = 0; } impl InvalidValue&lt;u32&gt; for NonMaxU32 { const INVALID: u32 = std::u32::MAX; } Is it to avoid too much custom treatment of types by the compiler?
Actually for the `Vec&lt;u8&gt;` with 1 million elements it is 1 pointer for the heap allocation, one `usize` for the length, 1 `usize` for the capacity, a continuous 1 million bytes for the contents (might be more, depending on whether the programmer requested that exact capacity, or the `Vec&lt;u8&gt;` was grown dynamically). Possibly a few additional bytes because whatever allocator is being used has to do bookkeeping. But then we don't really count those things for Java either, so I would say the answer is 1 million bytes on the heap, and 24 bytes on the stack (assuming 64 bits).
IIRC OSX really isn't great (based on dlmalloc which is quite outdated) and glibc is (was?) meh especially when the number of threads is lower than the number of cpus. They also have fragmentation issues.
Why is that? Doesn't the plain `new()` additionally perform checking against zero?
panic=abort just keep panic from unwinding. In many applications I prefer an abort as unwinding a thread does not kill the application until the main thread tries to take a poisoned lock which may or may not happen very often. 
I've had pretty bad results using jemalloc on anything not a modern x86_64 with small caches and what not. It was many years since I tried it so maybe jemalloc has improved by now. I tried it on mips and arm if I remember correctly. 
Unfortunately, as currently implemented, `staticslot` is unsound. It tries to build an "AtomicBox with swap" primitive, and that is an impossible object: if you `.get` the reference, and then `.swap` the object, the reference will dangle. That said, I believe we can build another primitive, which would kinda combine properties of OnceCell and StaticSlot. It would store an `AtomicPtr&lt;T&gt;` inside and would have a `fn set(&amp;self, value: T) -&gt; Result&lt;(), T&gt;` method which moves `value` to the heap and tries to swap nullptr with a pointer to `value`. That is, we can build an "write once" pointer without `Once`. Then, the `get_or_init` semantics for this thing would be slightly different from `OnceCell`: the `f` *might* be called several times, but only one will win. This is worse than *exactly once* guarantee of `Once` but, on the flip-side, `.get_or_init` now never blocks. Does anyone knows of a crate which provides such primitive? I actually need it for one of my projects. Does anyone knows of a good name for such racy OnceCell? The best I can think of is "кто успел, того и тапки", but that won't work until we accept the unicod identifiers RFC :-)
Rust plays nicely with c - it allow incremental adoption, or you can have the 2 languages coexisting quite happily.
The thing I like most about rust is its aesthetic beauty. Maybe not the best reason but there you go.
`NonZeroX::new()` returns an `Option&lt;NonZeroX&gt;`. Technically, `NonZeroX::new_unchecked()` only returns `NonZeroX`, but assembly doesn't care about types.
&gt; No one is rewriting the Linux OS in Rust There is Redox and there are some references to Rust in Fuchsia, so you might be wrong. Even if not, this has more to do with status quo than with any technical decisions. &gt; Almost all other applications can be written faster and better in a GC language I've found Rust to be very compelling choice even for cases where I could use something with GC due to well designed, modern language. You could have that in other languages... but Rust beats them in many areas. &gt; And for stuff where C (systems programming) is required most really good programmers understand the memory dynamics anyway No, they don't, they all keep creating bugs. And not all programmers are "really good" (whatever it means), so unless you somehow fix the universe, better language is the best way to go. &gt; look at the Rust code that makes up the stdlib and compared with the Java stdlib While I understand the argument, "how stdlib looks like inside" is a factor I care least about, as long as its maintained, same way most Java developers don't care how jvm code looks like. How the code that uses it looks like matters to me.
Sort of. It's harder to ensure you're handling types uniformly if trait impls are involved. Also you can't easily add multiple ranges like with a wrapper.
&gt; `NonZeroX::new()` returns an `Option&lt;NonZeroX&gt;` I didn't know that, now it makes sense! Thanks! :)
There is no stable release but you can get a preview using the nightly compiler. See &lt;https://rust-lang-nursery.github.io/edition-guide/&gt;
Just wanted to say, this is awesome! I have played with a bunch of web frameworks across languages and this is the most excited I have been by one! It reminds me of Haskell's Servant, but manages to provide all of the type safety in what feels like a more general and easy to use approach. I'll def be looking at the code to see how you achieve the type safety here - I expect I'll have a bunch of other potential uses for the approach!
This is fairly specific, but I had an "ahaa" effect when I finally understood it: **`move` closures cannot return references to their environment/borrowed from `self`.** Because every closure can also be used as `FnOnce` which is consuming (`fn call(self)`). Thus the captured environment only lives for that single call, making it impossible to have a reference to it. To be fair, even if `FnOnce` would not be the super trait of the other two, it would still be impossible, since we would need GATs to correctly define a trait able to return references to `self` but also owned values. Also note that closure can capture the environment by reference and return that reference. But we cannot move environment variables into a closure and return references to those. But yeah, I tried this several times and was always puzzled why it didn't work. Until it finally *clicked*.
Where is this usage of malloc coming from? Someone please correct me if I'm wrong, but the only reason I can think of to use malloc in a Rust program is if you're using a C library that expects you to allocate memory which it then frees. Aside from that, there's only [mem::uninitialized](https://doc.rust-lang.org/std/mem/fn.uninitialized.html) and [mem::zeroed](https://doc.rust-lang.org/std/mem/fn.zeroed.html), which will still attempt to drop, though it's undefined behaviour for the type to drop in an uninitialized state and probably for it to drop in a zeroed state.
Sorry, I just restart from scratch and no more warning 
For unsigned integers, you'd probably drop the max. For signed integers, it might make more sense to drop the minimum value, since there's already more negative values than positive values. For floats, you'd probably want to drop one of the many equivalent NaN bit patterns.
I just learned that there is an impl of Default for &amp;mut T[]. The most recent release added one for &amp;mut str. I have no idea how that could possibly work! ```rust fn main() { let foo: &amp;mut [usize]; foo = Default::default(); println!("foo = {:p}", foo); } ``` Prints: ``` foo = 0x560c3fd3b600 ``` So foo is a unique reference to a an array of usizes somewhere. But where? Who owns that array? How does it live long enough to be borrowed? This goes against my whole mental model of how references work! 
I don't have any visual aids, sorry, but I can offer some framing of the concepts. The first clue is that the compiler knows when all the instances are destroyed, ie. it knows the scope of the objects you create. The compiler also knows when you create references to existing variables. Since the compiler knows for what scope objects are valid, and knows when references are created, all that is needed is to be able to track which reference came from which variable. As long as the compiler can track which references originate from which variables, it is obvious that the compiler can prevent these references from 'escaping' or otherwise living longer than the variable from which it was created. The tricky bit is in the pudding, or in this case in tracking where a reference original came from. As long as there are no function calls involved this is again trivial. The references are right there. Even C++ compilers can/will warn you that you're returning a pointer to a local variable. However once a pointer goes through a function the origin of a reference becomes obfuscated. It is important that just passing a reference as an input to a function is always valid (proof is left as an exercise to the reader). What matter is what to do with references _returned_ (or otherwise _escaping_) from functions. These returned references still have an origin, but without some help from the programmer the compiler has lost track. You may imagine the compiler looking _into_ the function body and observe where the reference came from but this has issues. The key insight here is that all references returned from a function *must* originate from some of its inputs. A relationship between an input and an output that lets the compiler track that, hey this returned reference has the same origin as this input reference. Once that relationship is established, we're back to the land where the compiler can figure everything out itself. Earlier I said that this relationship cannot be observed/inferred by the compiler. Actually for simple cases it totally can. However this can create subtle problems where changing the body of a function changes how the function can be called. Observe: fn switch(left: &amp;str, right: &amp;str) -&gt; &amp;str { left } Here the returned reference is tied to the left argument and the compiler will check that the returned reference will not outlive whatever variable owns the memory from which the left argument was created. You can see how changing the body of switch to return the right reference changes this, now the compiler infers that the returned reference may not outlive whatever variable owns the memory from which the right argument was created! Further imagine if there is some condition which is used to return either left or right. There are also some details about unsafe code and how references are created from raw pointers. How does the compiler determine here which variable was the origin of the reference? Rust does not like this brittleness and requires that you instruct the compiler what the relationships are between input and output references, it does so through the _lifetime_ syntax. The lifetime syntax is a way of tagging which output is related to which input reference. Then the scope of the output reference is limited to that of the input reference. fn switch&lt;'a&gt;(left: &amp;'a str, right: &amp;str) -&gt; &amp;'a str { left } Here we've told the compiler, 'hey that output reference? yeah that, that comes from the left argument. in your analysis make sure that the returned reference does not leave the scope from which the left argument was created'. It gets a bit more complex if you annotate multiple arguments with the same lifetime parameter. This indicates that the returned reference may come from _either_ argument, and the compiler makes sure that the returned reference does not outlive the smallest scope. Finally Rust has lifetime elision to avoid the bother of annotating trivial or common situations so you aren't always bothered by this extra burden. Sorry for the wall of text, I just started writing and this is what I ended up with. I hope it was helpful to understanding how Rust validates references and how lifetimes are used to annotate how output references relate to input references at the function call boundary.
The trick is that the slice is empty, its size is zero. It doesn't matter that it's mutable, because in practice you can't *do* anything with it. In this sense this empty slice reference has much in common with other zero sized types (ZST): let unit = &amp;(); println!("unit = {:p}", unit); Prints: unit = 0x7ffe5ccc3068 The result is that the pointer value is not really relevant. As long as it's not null. This is very important as Rust likes to use the null pattern as an optimization for enums (eg. Option and Result). This has implications for low level memory manipulation! If you are not aware of zero-sized types your unsafe code could be trivially broken. (imagine calling malloc(0) and interpreting the returned null pointer as out of memory.) So when you take an address of a ZST Rust needs to come up with a non-null pointer value, but the actual bit pattern doesn't matter all that much. In practice that is probably a pointer within the stack frame of the function.
I'd love it if Rust had a functioning async i/o web framework instead of several functional async i/o web frameworks
I'm just an outside observer who won't be going there anyway, but, to me, selling them to the highest bidder would feel more efficient, in an economics sense, and thus more satisfying. :-)
I know what you mean, every time I'm fighting the borrow-checker, I can't stand it when it blasts me with the BFG-9000.
r/j3kl_ has mentioned the book. It explains the lifetimes, ownerships concepts very well. As for the IDE - I found that using an editor would be enough. The compiler messages are helpful enough. You just have to install a plugin (for auto-completion, syntax, etc. stuff) https://areweideyet.com and you are good to go. You don't need a full blown IDE.
What is the relationship between (unstable) [`alloc:Alloc`](https://doc.rust-lang.org/nightly/std/alloc/trait.Alloc.html) and `alloc::GlobalAlloc`? `System` implements both, while [`Global`](https://doc.rust-lang.org/nightly/std/alloc/struct.Global.html) implements only `Alloc`, and custom allocators should implement `GlobalAlloc`?
I once ported a low-latency service (99%&lt;1ms, including I/O) from Java to Rust. Rust made it so much easier to build as concurrent and provided so many benefits: much smoother latency, less CPU usage, RAM usage divided by at least 3, no more introducing buffers everywhere to fight against the GC, and I'd even say less bugs. I won't go back.
&gt;Nope. This is what OpenPGP's MDC system and AE solves: authentication without identification. I think MDC/AE actually means providing integrity without providing identity authentication. This is not what vks\_ wants.
Rust is pretty enormous, but the core useful parts of the language are basically a very ML-y core (not much different from OCaml or F# apart from the syntax), plus traits, macros and the borrow checker. A lot of the hairy gnarly complicated bits are things you usually won't encounter very often. (I still don't know how `for&lt;'a&gt;` works; I've needed it a grand total of once so far, and that instance turned out to be a bad way to write the code I was making anyway.) I tend to give this advice to all the beginners, but: you can learn a lot of useful things just by taking an afternoon or two and skimming the docs for the standard library. I particularly found a lot of handy bits and pieces in `Iterator`, `slice`, `Vec`, `Result`, `Option`, and the numeric types. You don't have to memorize it, just have some idea in your head so that sometime when you try to do something you go "oh wait, wasn't there a method for..."
Looks like most of the objections were "parking lot is too new". So perhaps now (two years later) it might be possible.
Yes, macro is unsafe-free, and the macro-less version is just safe.
You're quite welcome to not see a use for Rust, and indeed not use it. But there are a lot of people who see value in it, myself included. In my spare time I had been designing a better C/C++ for years, with some of the same ideas (single-owner pointers etc), and even started on some implementation. (Before that I also wrote a transpiler that would let me write C++ in a Java style.) But the guys behind Rust are way brighter than me, and developed something to fit the same constraints, but that goes much much further. If you don't see the importance and value of the constraints that Rust was designed to fit within, then I guess it will never make sense.
I'd argue that generally, yes they do understand, but they are human and mae mistakes. But why try to track what you can offload to the compiler?
That's great. If the unsafe code here has been scrutinized by the community this is enough reason for me to switch.
&gt;The frustrating part is that most of the startups using Rust as part of their stack seem to be focusing only on Blockchain, and while that is an interesting technology, it is still (in my opinion) not very stable from a job security point of view. That's true and I also noticed that. &gt;That is very good advice. Thank you! You're welcome ;-) Good luck with finding a Rust job!
Now you mention it I do remember I saw that somewhere. Then again that still falls into managing ownership rather than explicitly deallocating, rigth? From cambridge dictionary: garbage collector - a program that automatically removes unwanted data from a computer's memory In that sense rust is garbage collected, it's just rust doesn't depend on timing, scheduler, locks and the like to know when to remove data from the memory, instead it depends on scope, ownership and lifetmies.
I believe the libs team does not yet have solid answers to what should happen on NonZero(1) - 1 and such so we've not yet added these impls; it's also true that most of the use cases for NonZero that we're aware of do not involve actual math: rather, using an integer as an index into some array where you can easily just not use the 0th element or such.
I'm interested!
`Alloc` is for general allocators, `GlobalAlloc` is for global allocators. `GlobalAlloc` is much smaller than `Alloc`; it's a subset. So, I would imagine that most implement both.
It definitely falls under managing ownership. But calling a function like drop, possibly with a different name, is how you communicate to the compiler that you are done with `T`, in that sense delete and drop are similar. To me GCs have to be runtime process that act on conditions only known at runtime, where as delete and drop are known at compile time.
Ok. I guess I was expecting `Alloc: GlobalAlloc` or something.
Yeah, that's interesting. I wonder if that's planned...
Well, IMO what you really want is to split debuginfo - like e.g. RPM/deb etc. have been doing for years and years with C/C++. But, split debuginfo conflicts with the simplicity of skipping rpm/deb type build processes and directly embedding the binary in a container which is what a lot of people do. (And generating debuginfo containers is just weird...maybe someday though we'll get to teaching container build processes this level of sophistication)
I guess you could put a check every math operation, but I think that goes against the point of using this type.
Indeed, staticslot was cobbled together for a single-threaded project that involved a decent amount of unsafe. It would probably be best if all of the existing methods were unsafe. I don't know of a crate that provides the primitive you are describing, but it would be pretty easy to adapt StaticSlot to that. I could throw a OnceSlot together (and make it safe) and add it to the staticslot crate.
Hey, that makes sense! Thanks!
First off all, I’m glad you added that link to search. Did you by chance do the same search on “chrome” - it literally had 10x the number... The first link use cite, which has no supporting details was a marketing move. All of the browser vendors have always had far more vulnerabilities. Which was my point. If you examine the actual Java vunerablilities the are in the backing native code which is used universally - including by the browsers. 
Interested
When use you Box you are using malloc. You are putting the object on the heap. Eventually it is removed from the heap. Again, take a look at the very simple vec.rs file, you will see the machinations required for a simple vector. Contrast that with LinkedList.java. No comparison. Both do exactly the same thing. 
Your criticism on String hashes is not correct it will still work, the value of the hash will just be recompiled each time. The caching of the hash is an optimization that works in almost all cases - which makes it a very good optimization. 
I have to say IntelliJ+Rust plugin is preferable in my view. One nice feature is inline type annotations (i think in vscode you have to hover over variables to see those), in IntelliJ it looks like this [https://i.redd.it/1er3hcxevvmz.png](https://i.redd.it/1er3hcxevvmz.png) (annotation next to \`let mut vec\` and \`let elem\`).
I'm not sure what the best implementation would be, but it'd be nice if there was a way to indicate shadowing in the code, which would then prevent it from appearing in the output. Similar to the `[[fallthrough]]` attribute in C++17 to indicate where `switch` fallthrough occurs (and suppress any warnings about it). Sinc
Sorry I didn’t realize Vec was array backed. So a better comparison would be ArrayList.java which is even simpler.
I appreciate you driving so far. I know I would do the exact same thing 😅. I left a comment on the page asking what times are good for people if you would like to chime in.
Is this a question? Statement? What are you wanting to know? Try posting some kind of explanation of what you are looking for or what you are trying to say.
As I already discovered Vec is really ArrayList.java which is even simpler. But, I think you are incorrect on ago, you cannot use unsafe, only the stdlib and language authors can, but I could be wrong - this is a criticism of the opinionated nature of Go. 
No, it’s a trade off. Rust pays the cost with every allocation and deallocation. With GC the runtime is free to delay the GC until a more opportune time, trading memory usage for performance. If you cap the heap size you will essentially force the GC to run more often adversely affecting performance. 
That's right, you didn't realize. Then again you've had your mind made up about this from the beginning, so I doubt I can tell you anything that will change your mind about Rust. But, for the sake of argument, if `ArrayList` is so much simpler, please tell me: how much memory (in bytes) does `ArrayList&lt;Integer&gt;` use, when it contains 1 million elements?
If I create a struct with fixed sized Vec (and no remove is possible), can I take multiple mut ref to its elemets safely ? (Cause I know the memory will always stay valid and not get reallocated)?
[removed]
The problem (using vs code) is that I put "hello". and never autocomplete for me "to_string". I have Rust(rls) plugin. I need something else? 
I love it. I immediately thought of that one guy's problem being solved.
Probably 24 * 1 million, but only a bad programmer would do that, a structure like IntList would be far more memory efficient and performant.
So does any GC when it needs more memory from the OS, so I'm not sure what your point is in bringing up malloc. Again, I can't see a reason why you could call malloc directly unless you are writing the allocator or for the FFI reasons I mentioned above. Also, a vector is *not* a linked list, they're two completely different ways of storing lists of data.
Im sorry but how would that be more efficient? They would have to setup some kind of payment system instead of just choosing at random
You are missing the point. Every^(123) language is underpinned by code not covered by memory safety guarantees of its core language. Java unavoidably touches C in standard library and through JNI, C# has unsafe keyword just like Rust and for the same reasons, Haskell has its own FFI, etc. The reason Java Applets were so dangerous, and it may be that they had an order of magnitude fewer vulnerabilities, is that it wasn't completely self-contained completely memory safe language that you want it to be and one would need it to be. Unsafe is unavoidable, Rust is no different that any other language. ^1 Unless you are running purpose-built OS with compatible memory model, e.g. [Singularity](https://en.wikipedia.org/wiki/Singularity_(operating_system)). But then you are only pushing unsafety to the kernel. ^2 Unless you formally prove unsafe subset to be safe with automatic theorem provers. This is exceedingly hard. ^3 You want to build a space heater and need no FFI or runtime. 
I am curious, you say "need to use unsafe blocks is if you're intentionally and explicitly bypassing the compiler to do something you know is safe that the compiler doesn't ", doesn't that mean that the expressiveness of the 'borrow checker' is not sufficient for a large swath of programs ? Seems like it is used a lot in the stdlib for even trivial things like linked lists (a simple data structure). Contrast this with Java where the only 'unsafe' code in the stdlib deals with OS level or very low-level concurrency primitives.
&gt; Data-oriented architectures Any examples or documentation of this?
I amclose by in Seattle and definitely interested!
I'm not saying it would be _easier_ (or be worth the effort for just two tickets), but _efficient_ in the sense of homing in on the value people are ascribing to such tickets.
Thanks! I am hoping with this tool bugs of that nature can be more easily found.
I guess I mean, what kind of code would use this? I understand why saving a byte is sweet, but what kinds of procedures need numbers that can't change that are guaranteed to be non-zero?
&gt; A mutex on a type does not cover all of the common shared state concurrency issues And I didn't claim otherwise. Use of mutexes isn't enforced in other languages is my point. And lifetimes go way beyond mutexes. I'll attempt one last explanation - you can imagine them to serve similar purpose to try-with-resource statements in Java but generalised so that resources can be tracked across function boundaries and not just inside their lexical enclosure. &gt; Unlike you, I did the reading Thus far you demonstrated that you didn't or you misread everything you found. What are you trying to achieve? Surely you would know trying to spreading FUD in Rust community won't bear any fruit (that's what slashdot is for) so at this point we can start assuming you are not here in good faith. So you trolled us, good on you, you got us! Guess what, I'm like you, I like to argue. ;) 
As joshmatthews said, it's just a link. I don't have a question, just trying to draw attention to the fact that this exists. RISC-V has been gaining some momentum lately and it is possible that it will be an important architecture to support in the future.
Indeed: https://godbolt.org/g/HNK93g
Oh wow, I totally missed this. It didn't show up in my google search, but I guess I should have used github search. Thanks!
My personal is has been pointers; I’m not totally sure. Maybe check out the RFC.
Oh that's absolutely makes sense. Thanks!
The game dev stuff applies outside of game dev too; check out Raph’s latest talk on GUIs at the SF rust meetup for an example of applying it there.
/r/playrust
There are already plenty of CLI tools coded in Rust. But what you really want is /r/playrust
Actually, we've known [rust is coming to consoles](https://www.reddit.com/r/rust/comments/78bowa/hey_this_is_kyren_from_chucklefish_we_make_and/) for nearly a year. But you may want to post in /r/playrust
The reason it is not an issue is that there is very little applicability for a list of pure integers, and if there is in your case an IntList or array is trivial. 
That’s my point. Calling Java insecure is disingenuous when applications of far greater reach have orders of magnitude more vulnerabilities. . 
Agreed. I use malloc as a shorthand for manually managed memory. I’ve addressed the Vec issue multiple timed. 
As a general advice, relaxed ordering is usually subtly wrong, and so is the case here. Consider using `Relaxed`/`Acquire` orderings here.
Just a note inferring extern crate with use is available now in the 2018 preview. 
There was thread about rust on consoles before: https://www.reddit.com/r/rust/comments/68tvdk/rust_on_ps4_and_xbox1_this_actually_belongs_on/ 
I get by with vscode it’s a little shakey at times but generally does the job. Also the programming rust book is excellent. 
It should show errors. You must have it set up wrong.
It is a valid question though, I was confused myself until OP explained under. He should probably have used a text post with some context.
Perfectly balanced, as all things should be.
Note there are panic that can be checked this way like Array out of bound, Arithmetic overflow on debug, Out of memory, Division by Zero, Stack overflow ...
At the bottom everything is unsafe. Using `Box` for heap allocation? There is 'unsafe' code inside. `Vec&lt;T&gt;` uses unsafe. But if you accept `Box&lt;T&gt;` as a building block you need no additional unsafe code to implement a singly linked list. With `Rc&lt;T&gt;` and `Weak&lt;T&gt;` you can implement a doubly linked list without additional unsafe. So I don't get your point.
Those applications are C/C++. Rust was made to address that. 99.99% of code you will write in Rust will be without unsafes, just like in C# or Java. What's disingenuous is you desperately trying to equate Rust to C/C++ or that most languages aren't underpinned by unsafe code. Lastly, a bonus point. [Here's what think whenever someone mentions their "experience".](https://www.youtube.com/watch?v=ix4cewdC3gA) You are behaving like you know everything and definitely more than this community. At this points large multinationals are using Rust, including Google, Microsoft and EA. Are you saying they are all mistaken and you are right? We aren't having a conversation here. You aren't right just because you might be older. 
awesome, big fan of RISC-V
Strictly speaking, all these operators boil down to methods and their panics can be traced.
I'm really not sure what you are asking. In your example you need a single struct to make it work with derives and serialize. What do you want the struct to look like, what are you expecting Serde to do for you?
Then why are you going on about forgetting to free memory when you're done? This does not leak memory: fn foo() { let thing = Box::new(3); }
In my experience the IntelliJ IDEA plugin worked great when I first started learning the language about two years ago. 
Why does rustc need support for a new ISA? I thought it was a LLVM front-end.
Why is there no type alias for \`&amp;'static str\`? It would save some otherwise "hard" typing
No, the LMAX guys are working on Aeron.
Hey, thanks for taking your time. I am using Serde as a standard for serializing binary data: [current state](https://github.com/CasualX/pelite/commit/63bd42d67e1321befebb60df8d0c9d2c42f1672b#diff-9b7b8e38e5f184fc06719bc968249e52). The source comes from complex interlinked data structures in the PE file format. The raw image structs are already serialized by slapping `#[derive(Serialize)]` on them. But there's far more to it than that, interpreting complex interlinked binary format requires some manual hand holding. In the link earlier you can see I already wrote a wrapper to call `serialize.collect_seq` on Iterators (the wrapper is called `SerdeIterator`). My question was if I could do without creating structs and implement Serialize. Eg. if I could just provide ad-hoc `serialize&lt;S: Serializer&gt;` functions instead of a trait implementation that would help a lot for this case.
Majority of that will be false positives, unless you use program analysis.
I'm just about to start a new low-level project, and I've decided to give Rust a shot instead of C. I'm sold on the foundation and idea, and really want to kick its tires a bit. I'm writing a .so library that will be linked to by C and other languages. So.. what are the common pitfalls and mistakes made by beginners?
It might be a trade off in Java. It isn't in Rust, nor in C/C++. You get both at once, without the unpredictable slowdowns of a garbage collector. What's more, in Rust, you get it with compile-time memory safety. The cost of deallocation will always be paid. Rust just does it in a consistent, predictable fashion without the extra overhead of a garbage collector.
You are right, if GCs become a neglible language detail, rust does not have advantage regarding to speed. But I want to add another point which did not came up so far: Rust can lift a lot of brainwork of your shoulders by simply having unclonable/uncopyable types. Instances of these types can only be moved, and such type can even have a size of 0. This allows you to express surprisingly many kinds of rules and restrictions regarding the use of an API or resource. (including inner APIs) If this is done everywhere (and it is done in most public crates), this makes your live so much easier. I think many people who switch to rust from C or C++ don't realize that taking care of suble API details (e.g. is it allowed to call this method twice?) was consuming some of their brain power (and creating a small feeling of constant fear over the years). In rust you think once and than just try, if some new code would violate any system invariants you simply cannot find a program that compiles. In my opinion the lifetime system is only a detail that helps temporarily sharing uncopyable types in a manner that does not restrict the ability to encode arbitrary rules into types. Rust is just a sweet spot between functional/procedural with a helpful typesystem and little pleasures like OR-types (aka enums) . As some rustacean said a few years ago: The best that can happen to rust is to be replaced by a language that is even better. PS: Why is this Post voted down? Can't we take a little critisism? 
Not knowing convenient stuff. Seriously, read the rust book
And fighting the borrow checker. Aka, you'll sometimes hate the borrow checker but it'll always have your back
We trade time for explictness
The subreddit for the Rust game is /r/playrust
You are right that there isn't really a need for rust outside of replacing C++ in certain situations. That's why it was created, and it does that job well. Rust's existance doesn't make exisiting managed code worse. But a big part of Rust's success is the rich type system combined with practical imperative programming. Programming in Rust can be a lot of fun, and even in non performance-critical applications, I find that borrow checker is very helpful in forcing me to actually understand the logic of my code, instead of letting a garbage collector do the thinking for me. In short, Rust is a fun language to use. If you don't enjoy it, that's fine. I do. There is a *need* for rust, but there is a much bigger *want* for rust.
\+1 for this question, got me curious now... Maybe for performance issues, etc.. rustc being able to know more specific details about the platform and generate optimized code? But as a sub-question: would it be a \*strict\* requirement? Eventually rustc could output LLVM intermediate code that could run on RISCV? (even if sub-optimal, not using extensions, fixed for 32b, etc etc... but that for a 'basic' code would run). Cheers
Strings. `str` vs `String`, it took me a long while to decide which to use where and why.
On balance, probably.
Maine rustacean chiming in!
&gt;To be honest, 'object pools' in most cases have been proven to be slower than direct allocation except for the largest of objects with complex initialization. Such an assertion would warrant a good number of citations.
Also in `const` and `static` items Rust will default the lifetime to `'static` where applicable.
 fn divide(a: u32, b: NonZeroU32) -&gt; u32 { a / b.get() }
In some cases, my [`dont_panic`](https://crates.io/crates/dont_panic) crate might be useful...
This post summarizes everything I found out while researching this topic in the past few weeks (I desperately needed a solution for a project of mine). I think it's surprising what is possible already today. I hope this is useful to some of you -- either because you enjoy to test the limits of Rust's current type system or because you need a workaround, too. If anything is unclear, please just ask! :)
One trick is to offset your integer; when unsigned, for example, you can simply add 1 when storing and remove 1 when getting it out. Wrap `NonZeroX` in your type of choice, and here you go, same interface but now MAX is the invalid value.
Awesome! I read ~half the book online awhile back when I was initially interested. I bought the book on Kindle last night and started over. Definitely gonna have a solid foundation before I start writing code!
Yeah, the sizes of platform-dependent types are all fixed to something specific in LLVM IR. Besides target boilerplate, the PR also had to implement the call ABI for FFI here: https://github.com/rust-lang/rust/pull/52787/files#diff-27aeed9bf150ab5e48222c78f21a56f0
Procedural macros aren't stable unfortunately. Otherwise we probably wouldn't need a build script for code generation at all, we would just directly generate the code with a procedural macro.
Go can indeed [`import "unsafe"`](https://golang.org/pkg/unsafe/) and doing so throws out all guarantees of portability and stability.
Motorola 68000 support is on it's way, too :-) * [LLVM part](https://github.com/M680x0/M680x0-llvm) * [Rust part](https://github.com/glaubitz/rust/tree/m68k-linux)
On the waitlist. Also interested. My company is interested in making it to the conference in the hope of hiring some rust devs.
You can do a google search. Especially for concurrent systems. The map lookup or linked list maintenance is more expensive than the allocation. Not to mention the problems it causes with retained references. Though if the object is immutable it is less of an issue. 
Interesting timing, today I also needed GATs for a graphics api, but I needed generics and not lifetimes. pub trait Backend: Sized { type Buffer&lt;T&gt;: BufferApi&lt;T, Self&gt;; // &lt;- GAT type Image&lt;T&gt;: ImageApi&lt;T, Self&gt;; // .. } struct Vulkan; impl Backend for Vulkan { type Buffer&lt;T&gt; = Buffer&lt;T&gt;; //... } pub struct Buffer&lt;T, Backend: Backend&gt; { buffer: Backend::Buffer&lt;T&gt;, } //... let context = Context::create::&lt;Vulkan&gt;(); // or //let context = Context::create::&lt;OpenGL&gt;(); // infers the API based on the context let vertex_buffer = Buffer::from_slice(&amp;context, &amp;data); Took me the whole day to find a good workaround. 
This looks excellent, and is pretty much everything I want out of a framework in terms of the actual API to wire things together. I'd tried to throw some ideas on this around in the past, but this is much more elegant than I'd have expected. Awesome! Not to be that person who always pipes up on performance, but have you considered adding it to [TechEmpower](https://github.com/TechEmpower/FrameworkBenchmarks)? I know you have Hyper on there as well; it would be interesting to see how little (or much) overhead there is when using Warp. 
Yes, interested!
In some code I am reading, I have come across something like this: use std::{ self, io::Read, }; What does it mean to put the `self` there? What are the relevant keywords I should look up in the Rust book to figure this out?
When reading code, how does one differentiate between whether something being imported is a trait, versus something else (e.g. a struct)? When making traits, should one name them something like `TraitRead`, to make it clear that `Read` is a trait? 
When reading code, how does one differentiate between whether something being imported is a trait, versus something else (e.g. a struct)? When making traits, should one name them something like `TraitRead`, to make it clear that `Read` is a trait? 
I don't have any references but `use std::io::self;` is equivalent to `use std::io;`. I'm going to assume you meant to use `std::io::self` instead of `use std::self` as the latter doesn't make much sense. As for why this syntax exists is to make using traits more convenient. In order for traits to be at their most convenient they must be `use`d directly: use std::io::Read; However, and specifically in the case of the io module, you may want to refer to other items in the io namespace indirectly: use std::io; It is convenient to combine these two statements (using io and using the Read trait) using the syntax you're asking about: use std::io::{self, Read}; 
That may be a stupid question, but is there anything that still uses the 68k architecture these days? Or is this intended for these old devices? We still had to learn 68k assembler in university, and I remember my Palm and TI-89 having it, but I haven't had anything like that in a long time.
No, I definitely mean what I typed, which is `use std::self`, it is not `use std::io::self`.
With the recent [stabilization of `#[global_allocator]`](https://blog.rust-lang.org/2018/08/02/Rust-1.28.html#global-allocators), I wanted to play around with a few tests to get some insight into how allocations were being made. So, I created this crate to wrap a `GlobalAlloc` allocator. Then I can get statistics about the change in allocations between any two points in the program or in tests. It should be noted that the counting is done with atomic counters, but the counters aren't atomic together, and the counters are enforcing `Ordering::SeqCst`, and so probably slow down the allocation path a good deal. I may try benchmarking with a `parking_lot` mutex or such. While I haven't done it yet, I am also planning to add a stats wrapper around the `Alloc` trait as well. Big bonus, this crate works on stable.
I'm not too far from Maine, just a skip over New Hampshire from Massachusetts, but I can make it up to York relatively easily (not sure how much further into Maine you live). My main meetup is the Boston meetup, which seems to be the largest gathering in the New England area.
AFAIU, this crate can be used only with no_std or something similar.
Yes, a whitelist will be useful.
What good workaround did you come up with? The code you posted doesn't contain that workaround, right?
So... it's a bit more complicated :) First of all, a language is not JUST a language, it also generally comes with a run-time, even if said run-time can be rather minimal, which is automatically linked by the compiler. Adding a new platform requires: 1. Adapting the run-time for the new platform, 2. Teaching the set of features available in this run-time to the front-end, 3. Ensuring the front-end properly picks up the right run-time when comes to the time to build the library/binary. This *could* be done via configuration files, but historically it's been embedded as code directly into the compiler binary. Secondly, the ABI of a language is generally NOT universal; it varies from platform to platform. Part of the ABI is handled by the backend, such as calling conventions, but the front-end nonetheless needs to know about such things as the size and alignment of the basic types, if only to appropriately deduce the answer to `std::mem::size_of`. Thirdly, languages with a C FFI need to interact with C. This once again requires the front-end to learn about an ABI (the C one) on the target platform so as to be able to know the size and alignment of C-compatible types. I invite you to check the patch [for RISC-V](https://github.com/rust-lang/rust/pull/52787/files). There are only 139 lines across 11 files changed, so it's easy to scan, and it'll give you an idea of how a target needs to be customized. And it should definitely let you appreciate how much heavylifting LLVM does. I contrast, the LLVM support is likely to require thousands or tens of thousands of lines.
Although it's been lost in the new layout, we usually discourage bare links unless it's really obvious what was going on, and instead encourage using the "Text" option so as to include a small explanation of why you thought the link to be of interest. In this case, it could even have been handled in the title: if the title had read "RISC-V support coming to rustc" for example, it would immediately have given a hint as to why it was linked, without having to know that it's not supported right now.
Thanks for clearing that up. I just have one more follow-up question: Why did you say &gt;I think it makes more sense to write http microservices in Go than Rust I keep hearing/reading the term "fearless concurrency". Isn't that exactly what's needed to build a web server? And why is Rust not as good as Go for this? And also the other way round: Why is Rust more suitable to build a web browser? Is it because Rust is still a bit faster because of the missing GC?
Have you watched Matt Godbolt's talk: When a microsecond is an eternity? (CppCon 2017 I think) In this talk he mentions that Optiver (he's a former employee) was managing to have reliable 2.5us latency with C++ trading systems. From experience at IMC (another HFT firm), this is achieved by (1) using good hardware, (2) using user-space network stack, (3) using iso-cpus to keep anything but your application running on the cores you pick, (4) using core pinning to avoid costly/disrupting core hoping and (5) using spin loops to avoid the OS. None of this is rocket science, but properly configured this means that you have an OS-free experience in your critical loop, at which point real-time and near real-time is definitely achievable. On a standard Linux distribution.
&gt; Please calm down, and stop spewing stuff you don’t understand. Unlike you, I did the reading, and as expected you are incorrect. Excellent advice, please do calm down and avoid such abrasive sentences, they do not lead to constructive discussions. 
Yeah agreed; I already observed "cheating" in various implementations (not checking methods, responding with different headers, etc). I'm mainly curious about the impact of Warp over Hyper itself, out of interest rather than "I need the fastest!". I find it interesting to see how some abstractions play nicer than others. Since that's the one Hyper benchmark I know of, figured I'd ask. Anyway, totally understand priorities ;) thanks for the reply! 
I am surprised that a HFT trading system could get away with 100 us pauses, in the trading systems I develop, a 10 us reaction delay is cause for an alert. Were you involved in more slow-paced (aka smarter) layers?
Well then, it brings `std` into scope so you can start paths with `std::stuff` instead of having to type `::std::stuff`.
Just out of curiosity, what sort of applications are you developing in Rust? (I'm looking for new ideas about things I could try doing with Rust)
&gt; I would also argue that the same code in a GC (especially functional/immutable designer) would be far safer than the Rust code. Actually, most GC'ed languages fail to enforce data-race freedom, leading in many nasty bugs. And in some cases, such as the current Go implementation, it's undefined behavior to have data races on slices/interfaces, which is definitely *less* safe than Rust.
My dream is still that one day someone will come along and automate all of this stuff, so you can skip those intermediate steps. Here's a very hacky attempt I made at doing just that: [https://github.com/samscott89/swiggen](https://github.com/samscott89/swiggen) (so hacky, I think it doesn't work any more after \`syn\` had a major update) . 
Looks pretty good. Have you thought about using thread local counters and combining them for the output? Then you would have almost no performance overhead. I noticed that you checked your `.idea` files in. IMO, these are IDE specific and have no place in git. Same thing for `.editorconfig`. You can add the files of your IDE to a global git ignore file.
&gt; Yes it does, although I would not consider race conditions a "memory safety" issue. It definitely depends on the language: - In Java, a data race may lead to violated struct invariants or bogus answers, but is not a memory safety issue. - In Go, a data race on a slice or interface is Undefined Behavior, so it *definitely* is a memory safety issue.
The same would go for many functions that assert any condition internally. The OP is asking about potential panics, not about a crate that ensures that your input parameters don't lead to a panic.
&gt; Some people avoid exceptions due to legacy performance concerns, but modern branching CPUs and compilers/VMs I believe make the overhead negligible. Yes... and no. Let's start with the Yes. The typical modern implementation of exceptions is the so called Zero-Cost Exception model which is table-driven. In this case, at run-time, exceptions that are not thrown have zero-cost. And yet, no. The first most obvious cost is that in exchange for being fast when not thrown, when they are thrown exceptions are incredibly costly: 1. The mechanism to unwind the stack, and performing clean-up actions as appropriate, requires quite a fair amount of overhead (on top of the cost of the clean-up actions themselves, which are unavoidable). 2. The tables being separated from the normal code path are costly to fetch; and should always be, for if they are used often enough than they seat in the CPU cache, then the exceptions are not exceptional enough! In terms of throughput, this is nice. In terms of latency, this can kill any SLA you have, which is why C++ game developers traditionally disable exception support. The less obvious cost is the missed opportunities cost. In the presence of exceptions, many optimizations fly out of the window, both at code level and optimizer level. For example, I invite you to write a `std::vector::insert` method in the presence of throwing move/copy constructors. It's possible, *of course*, but the amount of convolutions necessary to achieve it is a performance killer (cue, `std::is_relocatable` proposal).
OK that's actually what I would have said/thought also. Great. Thanks so much for the explanation again!
This is a common fallacy. Total available wealth usually has at least as big an effect on willingness to pay as value ascribed to the object in question. This is why gigs/concerts for famous artists don't auction their tickets, even though they could much more for them if they did.
Nice! I wrapped it up in an attribute macro to better fit how I would like to use it: [`#[no_panic]`](https://github.com/dtolnay/no-panic). #[no_panic] fn demo(s: &amp;str) -&gt; &amp;str { &amp;s[1..] } fn main() { println!("{}", demo("input string")); } And I found a way to have it tell you which function contains the problematic panic (in this case `demo`): undefined reference to `RUST_PANIC_IN_FUNCTION&lt;demo&gt;'
You could check out https://rust-lang-nursery.github.io/api-guidelines/naming.html to see if that floats your boat. I don't think just calling it `Read` introduces a lot of ambiguity, since traits are generally used in very different spots than types and enum variants. Otoh, spotting type objects is difficult sometimes, but `dyn Trait` will resolve that I think.
&gt; I must be missing something. You are missing two: - `NullPointerException`, - `ConcurrentModificationException`. `null` is the Billion Dollars Mistake, yet most languages also include `null` (deferring error checks to run-time) because it's easier to do so. Proper support for sum types neatly solves the issue, and Rust has such support. Most languages have no support for guaranteeing exclusive access to an object. The `ConcurrentModificationException` in Java is thrown, notably, when iterating over a collection which is modified: - either because it is modified with the loop (typical), - or because it is modified from another thread (ouch). Languages when mutability is pervasive (aka, the top 10 languages) simply shrug at the inevitability of it. Languages when immutability is pervasive tout their superiority, and assume anybody understands that it comes at the cost of not being able to efficiently support arrays, with all that entails performance-wise. Rust, instead, supports compile-time verification of exclusive access (`&amp;mut`) via ownership and borrowing, and makes `ConcurrentModificationException` a compile-time error. --- Why Rust? Because I like correct code.
Also, you can probably get away with relaxed ordering, because ordering only affects non-atomic data, which you don't share. Correct me if I'm wrong.
A fantastic article even if you don’t need GAT it really helps when trying to understand advanced lifetime and trait bounds. 
Which school?
Awesome, definitely down when I come back next month.
Rust needs annotations on function signatures which makes the type inference local. Perhaps this make it easier?
Well the official rust compiler has a huge dependency with LLVM that makes it impractical for embedding purpose. You could try an alternative compiler, cranelift for ex, but none of theme are mature enough yet. 
It's a hobby, mostly, at least for me. But I always learn a lot in the process.
there are some good first issues, if you fix one of those that will definitely earn a ticket :) [https://github.com/solana-labs/solana/issues?page=1&amp;q=is%3Aissue+is%3Aopen](https://github.com/solana-labs/solana/issues?page=1&amp;q=is%3Aissue+is%3Aopen) 
A compacting garbage collector, like the nursery in HotSpot uses, doesn't actually "free" memory in the same way malloc does. The algorithm instead moves live objects over the top of memory that isn't associated with another still-live object, so it essentially "ignores to death" the garbage, and the cost of a GC sweep is proportional to the amount of *live* data, not the amount of garbage. Allocating to the nurserey is usually a couple-instruction pointer bump. If you want to get something comparable in Rust, you'll either use the stack or an arena. Both can give you pointer-bump allocation, and they don't have to occasionally scan the entire heap. 
That’s very convenient. The more time I spend here the more I think this community has some mental blocks. The fact that GC languages are clearly superior in all aspects for general purpose development is settled. I cited a seminal paper and there are many others. You can argue Rust is superior for systems level development but that is going to change as well. The OS itself will be GC in order to ensure security and reliability. 
Malloc is far slower than that. If you confine Rust to no dynamic memory, fine, but you might as well use C. 
A single system call is on the order of 2-3 us. Our software traded over 20% of the volume on the CME and ICE. Not a lot of equity work which is lower latency, but in general yes, always better to be smart and fast than stupid and faster to a point. 
I really like your unix-y dedup solution. Found a use of pv I really never imagined: $ rdedup_store.sh #!/bin/bash read -sp 'Password: ' password; echo renice -n 19 -p $$ ionice -c 3 -p $$ rdup -x /dev/null "$1" 2&gt; $RDEDUP_DIR/rdup_error.log |\ pv -ptrbTCB 700M -i0.25 |\ RDEDUP_PASSPHRASE=$password rdedup store "$2" 2&gt; $RDEDUP_DIR/rdedup_error.log $ cat rdedup_load.sh #!/bin/bash renice -n 19 -p $$ ionice -c 3 -p $$ rdedup load "$1" 2&gt; $RDEDUP_DIR/rdedup_error.log |\ pv -ptrbTCB 700M -i0.25 |\ rdup-up "$2" 2&gt; $RDEDUP_DIR/rdup_error.log I set pv to update every 250ms and have a 700M buffer and have been using overlayfs to balance disk usage. Keep up the good work.
The whole point of `.editorconfig` is to be shared configuration. That file _should_ be checked in such that everyone uses the same indents, line endings, and such. For IDE files, if it's a large open source project, they should definitely be ignored. For smaller personal projects or those where most people are expected to be using the same IDE, [the standard template](https://github.com/github/gitignore/blob/master/Global/JetBrains.gitignore) ignores personal configuration but shares useful global information. As an example, with the suggested ignore, run configurations are not shared, unless you check the UI box to share them, in which case they are checked into source control.
Alrighty, I'll make a new post too, but for all those following this post, here's the hurriedly put together raffle form: [https://goo.gl/forms/gBbtSrbd7FnbtQWk1](https://goo.gl/forms/gBbtSrbd7FnbtQWk1) Thanks all! 
[removed]
This is something that I need to test further to ensure will produce reasonably accurate values. I did start with relaxed, but will see how it goes.
You’re a gift to everyone you meet.
Nice. Agism. I've found that this formula generally holds on any forum: quality of engineers = 1 / (level of childish vitriol spewed) The original question was what's the point, and I've summarized the responses in my OP. No one has yet to mention any major system written in Rust - AFAIK even the inventors only use it for a small portion of the Gecko engine. Just about every major open source software today is written in a JVM/GC based language - Hadoop, Spark, Intellij, GMail, Android, and the list goes on...
Actually, the "unsafe" import is not part of the 1.0 standard and is subject to being removed. probably won't happen, although it should IMO.
Creating a trait and implementing it for both structs seems like the proper way to me. You could do something with macros, but it's not worth it imo.
Well said, and that I can understand. I am into problem solving as must as the next person. Just most of the time, at least in a job situation, I have a fiduciary responsibility to be efficient too.
They could. There is the hifive1 dev board that is riscv based. Which I can't wait for it to be Nativitly supported.
The guy working on the m68k backend for LLVM is an LLVM upstream developer, so no worries here.
No, you don't need those. But possibly it's a little faster this way, just like a specialized `IntList` might be better in Java than an `ArrayList&lt;Integer&gt;`.
Fine you win. 
Ohhh, I see. Guess I misunderstood that. Thanks :)
Maybe a better way to say it is Monads allow abstracting over sequentiality?
You're looking for /r/playrust
Thanks for this, I'm really really interested in contributing to the language but I'm afraid to do so. This will motivate me significantly. 
But LLVM is necessary to be installed apart of rust? If I take the rustc binary it not work stand-alone?
&gt;Aeron is a messaging system written in Java, I am not sure what that has to do with the LMAX exchange using Zing. &gt; &gt; &gt; &gt;"Aeron is a high-performance messaging system written in Java built with mechanical sympathy in mind, and can run over UDP, Infiniband or Shared Memory, using lock-free and wait-free structures. In this talk, Martin explores the design of Aeron to share what was learned while building Aeron to achieve high performance and low latency."
The admins are investing your brigading you fucking shills.
I think clippy might be able to, but don't quote me on that.
I recommend deleting this before you start losing karma
I read the post, but don't really agree. If you look at the 'pattern matching' and 'recovering from error' sections the example code is unmaintainable in my opinion, and will lead to obscure hard to fix runtime bugs. The Java you cite is just part of the language - most likely done for performance, and for compatibility with lots of C code that expects that behavior. Btw, Java 8 has Math.addExact that throws exceptions in overflow conditions.
No, it won't.
Announcing Rust 2018 prematurely was a mistake. C++0x become C++1x and finally C++ 11 without any issue; nobody had to stick to releasing it in 2009 just because it had 0x in the name.
Derive MyTrait seems exactly for that. Why isn't it worth it?
Then, the burden is placed on you of managing your threads and making sure there are no data races.
This is because the `iter()` function's signature is something like `fn iter(&amp;self) -&gt; VecIter`. Since it takes a reference to self, it can work with a `Vec` or a `&amp;Vec`. With a `Vec`, it's borrowed for the duration of the function call. With the `&amp;Vec`, it's already borrowed and is simply passed to and from the function.
Then, you could just drop the `let`. Use `(new_a, new_b) = some_new_computed_pair()` instead.
Because since you're only doing it for two structs, you could just do it manually.
If you write software in a GC language, you are limiting your software to just that language. There's good reason why most of the libraries in a Linux system are C libraries, with C++ second. Rust can generate C-compatible libraries, which every language can build bindings from. Optimizing a Rust library / application is much easier than doing so for C or C++. Going a step further, making your highly optimized application take advantage of multiple cores is simple with crates like rayon and crossbeam. If you want to build some open source software that's built to last, your going to want it in Rust. Runtime GC is also neither necessary nor sufficient. If you run perf on a GC'd binary, you'll see that a significant portion of your cycles are wasted in the runtime of the GC, rather than your program. Those developing with GC languages need to go to great lengths to attempt to fix this. Rust provides the tools to write high level APIs and applications with algebraic data types, pattern matching, trait-based generics, and a functional paradigm. Cargo is a powerful build tool that makes publishing and importing crates easy. Compiler macros are even making it trivial to accomplish complex tasks with minimal to zero code. Rust is only complex if you're unfamiliar with the many concepts it implements. Knowing these concepts makes for a better programmer. These are tools that enable you to build better software with less effort. When building complex software, you'll want to reach for the tools that can make those complex problems simple. Rust does this really well.
That's not correct rust syntax ^^ otherwise I'd do that ;) you cannot have a deconstruction on the left hand side without let
GATs have nothing to do with GADTs. GATs are higher-kinded associated types.
The only correct statement you made in the entire post was that if you are writing a library, using Rust (or C for that matter) is the best choice for the widest audience to be able to utilize it.
The original insert was much easier to follow than the one the author wrote. It is how it is coded in most other languages, and while it might not be rust-idiomatic code, it looks like code someone in languages ranging from Kotlin to C would write.
It's the other way around: no_std crates can use my crate without a problem too, not just std crates. :)
I suppose it's just what you're familiar with because I found the new insert to be far simpler, personally. "Head is the new item, and the next points to whatever head used to be." Wrapping types and checking whether there's actually something"there" are, to me, actually just tangential to the core logic of the data manipulation.
Good article! Note: `mem::replace(&amp;mut self.head, None)` could as well be the more readable `self.head.take()`.
There should be a lint for that.
Yes but but if you do this, nearly every method will be reported.
I didn't realize that rustc had its own SmallVec implementation. If anyone would like to work on switching it to the [smallvec](https://crates.io/crates/smallvec) crate, see [this GitHub issue](https://github.com/rust-lang/rust/issues/51640). As a maintainer of the smallvec crate, I'd be happy to provide help or advice.
I haven't used VS-code. I use Vim with https://github.com/rust-lang/rust.vim The autocompletion is not that smart - as you want it to be. Although I have heard YCM provides a smart auto completion.
By doing more analysis then rustc does. let a = [1,2,3][2]; Does not panic, the panic will even now probably not end up in the binary. You obviously can't prove all cases, but interesting ones. For example, that `unwrap` on `Result&lt;T,!&gt;` does not reach the panic inside.
"not practical" is very different from "can't be checked". libstd is _full_ of methods that panic, especially around data structures, making in very hard to write code that avoids panics.
This was my first thought, esp. after seeing the 3 variants.
[rustig](https://github.com/Technolution/rustig/) &gt; A tool to detect code paths leading to Rust's panic handler
Moreover, I really think stuff in the standard library should be a shining example of idiomatic rust, so newcomers to the language have a great reference to learn from.
How do you hack on rustc_data_structures without having to recompile the whole compiler every single time ? 
They are in beta.
Yeah this is what I do as well: on failure, die fast and restart.
&gt; Runtime GC is also neither necessary nor sufficient. If you run perf on a GC'd binary, you'll see that a significant portion of your cycles are wasted in the runtime of the GC, rather than your program. Those developing with GC languages need to go to great lengths to attempt to fix this. This is absolutely true, even if you don't see it. Maybe you're upset that your years of experience learning these skills are being replaced by the compiler?
perf.rust-lang.org does not _only_ group changes by day, you can go to the Compare page and choose 1) a commit before your change and 2) the commit you want to benchmark. It'll then display all the benchmarks numbers to see the impact of the chosen range of changes 
Sean, keep doing what you do. Love your work!
Crossposting is not brigading.
Trying to hack on the compiler is really sad sometimes, just because the code quality is... really not great in places :/ I already proposed this somewhere else: **what about a compiler cleanup month after the Rust 2018 release?** In that time feature PRs are not merged, only cleanup changes are allowed (including simple formatting). It would be nice to make the whole compiler idiomatic Rust 2018 code. I think such a special month would be useful, because many people have reservations about things like "run `rustfmt` on a crate", because it creates a huge diff and makes merging with other changes super hard. Because honestly, I stopped hacking on the compiler multiple times already because of the lack of code quality in some places (no comments at all, incorrect formatting, hard to read code, ...). I really greatly appreciate PRs trying to improve this situation!
If any subreddit had awesome solutions to an annoying problem, it’s this one. Love you all. 
I know that it can seem overwhelming at times, but don't be discouraged. We don't need concerted action we just need action. Perhaps doing a rustc cleanup workshop during a Meetup would be cool enough to spread...
Thank you! And you are right, that would improve readability. Note that I kept the changes purposefully small – the `mem::replace` was already there, so it appeared less risky than to change it.
You're very welcome. This is what I was hoping for!
How accurate is this? I have a tiny allocator for this: https://www.reddit.com/r/rust/comments/8z83wc/is_there_any_way_to_benchmark_memory_usage_in_rust/e2h4dp9/ but it isn't very accurate.
I never thought like that. Good point.
I've implemented a simple wrapper around std and parking_lot RwLock and now I'm able to switch between them seamlessly. https://docs.rs/shkeleton/0.3.2/src/shkeleton/shkeleton.rs.html#7 So I could for instance use std in debug and parking_lot in release.
Great tutorial! The explanations are clear and very pedagogical! I have a few remarks that may be useful to make some things simpler: * Instead of `type MyMatrixType = Matrix&lt;i32, U4, U27, MatrixArray&lt;i32, U4, U27&gt;&gt;;` you can write `type MyMatrixType = MatrixMN&lt;i32, U4, U27&gt;;`. This hides the need of specifying the storage type explicitly. * Similarly, instead of `type MyMatrixType = Matrix&lt;f64, U49, Dynamic, MatrixVec&lt;f64, U49,Dynamic&gt;&gt;;` you can write `type MyMatrixType = MatrixMN&lt;f64, U49, Dynamic&gt;`. * It is possible to index a matrix in the following way: `matrix[(i, j)]`. Here the index is a tuple of two `usize` where `i` is the 0-based row index and `j` the column index. * You mention that a slice cannot be modified. This is true, except for mutable slices. For example you can write `let mut s2 = m.slice_mut(start, shape);`. This will create a mutable view of a matrix and any modification will modify the original matrix. Would you mind if I add a link to your tutorial somewhere on `nalgebra.org`? Not sure where yet, but I guess I could add an “External/other resources” chapter.
I think that blog posts "let's do X with Rust, no previous knowledge of Rust assumed" have a tremendous value. People want to use Rust with a specific goal in mind, and when we show that hey, it is possible, and even somewhat familiar, we thread the path - so to speak - through uncertainty.
&gt; If the system is commonly encountering exceptions, then it is using them for flow control which is clearly improper. I agree, which is why in general I am not too worried about the run-time cost myself. On the other hand, the lost optimization opportunities affect the code generation of the "happy" path, which is always a concern. You can see proposals in the C++ community about avoiding the situation: - In his Value Exception proposal, Herb Sutter proposed to make allocation failure abort instead of throwing `std::bad_alloc` so as to eliminate ~90% of throwing functions from the standard library. I was surprised at the switch of strategy, and expected widespread refusal, instead a majority of the committee agreed to the direction. - `std::is_relocatable` is being proposed, allowing libraries to use `memmove`/`realloc` to move objects around in bulk and with no exception instead of moving them one at a time using move/copy constructors, and having to handle potential exceptions in the middle of it. 
You can just go to the module's folder (/src/librustc\_data\_structures) and compile it with cargo like a regular crate.
Thank you for all that effort, reading this really is quite interesting! :)
`malloc` is not slow... in average. It's the spikes that kill you. Which is why I mentioned specialized memory allocators and memory pools, as well as avoiding allocations in the critical path (which does not mean avoiding allocations everywhere, or every time).
Edited ;)
You have to be joking.
Let's check https://robotics.rs for your field. It is my site.
&gt; We don't need concerted action, we just need action. I think I disagree. Several things: - Cleanup PRs usually don't get that much recognition as PRs implementing a feature. Understandable, but somewhat sad. Having a "now is cleanup time" in the mind of the whole community, big cleanup PRs will be more prominent, increasing the incentive to work on such a change. - Similarly, putting "cleanup" as the focus of a month or so shows the community that we value cleanups and that we think good code quality is important. Otherwise, many people invest their limited time in feature-changes, because they might think it's valued more by the community. - I already mentioned: many people have reservations about doing cleanup work, because "other people could be working on 'real' changes in the same file, thus my cleanup gets in the way of their 'real' work". Again, explicitly prioritizing cleanup would surely help with that. - It takes the pressure of the core compiler contributors to do "great things". Some important cleanup work can only be done by people who have a full overview over the compiler and the whole system. But those people are rare and the community usually expects them to implement the next big thing. Of course, one cleanup month doesn't replace the continuous efforts that have to happen to keep the code clean and improve small things. But it's more difficult to check a new PR for good code quality, when the surrounding code isn't that great. If the whole code would have helpful comments, a lack of comments in a new PR would be easier to notice. If everything would be formatted according to `rustfmt`, one could automatically check that a new PR is also correctly formatted. Maybe "cleanup month" sounds like we loose a month of work. But I am pretty sure that easier to read code would mean that more people could work on the compiler (and in particular, would be able to do more on their own). That would more than make up for the "lost" time IMO.
Reaching for `unsafe` when it's not needed. Trying to do object oriented programming in Rust.
Hardly, in the general case. I think the lifetime elision rules and type inference are good examples to the contrary. More recently (and less popularly, depending on who you ask) "in-band lifetimes". But I suppose you're right in this particular instance.
how about integrate warp with diesel?
Great tutorial, thank you.
I hope they update the data in the foreseeable future &gt; With its first stable version released this year, Rust is designed to make concurrent systems easier to program reliably. And put on an embedded badge like Python and a mobile badge like C on it while there at it :)
Following convention, the site should have been https://arewerobotyet.com or something similar
I don't think we need to disagree, but I think having a steady stream of small improvements without impacting other work is better than stopping other work for a month to get a bigger wave of improvements. 
A runtime GC 'might' be faster than a naive malloc implementation in a few cases, but an efficient malloc implementation pools memory so that the program rarely needs to waste time with allocating or deallocating. If I were to run perf on a Go binary, more than 60% of the total runtime is spent in the garbage collector constantly sweeping in the background and invoking context switches to do it, whereas for an equivalent Rust implementation, it would only be a small fraction of that spent in free and malloc. I've yet to see any real world software that benefits from having a runtime GC, though. It's pretty common to hear about the efforts that people using D, Java, and Go go through in order to fix throughput issues due to their runtime GCs -- disabling the GC at various times, forcing the GC to clean up objects that hold file descriptors at other times (to ensure that their service doesn't crash from the GC never getting around to calling the destructors and running out of sockets), or also having to force it to run because otherwise the program will trigger OOM due to making inefficient use of memory and performing a lot of allocations in a short time frame.
Android is also the least efficient platform. The battery life of an Android phone is abysmal compared to an iPhone with a smaller battery. Can you guess why? It has a lot to do with Java and the runtime GC. It's very inefficient compared to Swift, which uses basic reference counting instead of a complete runtime GC running in a virtual machine. Cargo is much more complete than Maven or NPM, too. For one, it's easier to add crates to a project. Cargo uses a TOML config in the project root, and adding a dependency to that config is as easy as 'cargo add crate', or just typing the name of the crate followed by the version you want to use. No IDE required to manage your config. It's human readable and writeable, which is more than I can say for Maven XML files. Cargo includes a lot of subcommands by default, but it's also extendible through installing extra subcommands. cargo profile, cargo flame, cargo watch, cargo make, cargo vendor, cargo fmt, cargo add, etc.
The code in the standard library is not a useful example for how Rust is written in the wild. It has much more restrictions than that. First, it was written before Rust was standardized, and well before many of the conveniences that exist today were created. Second, it has to largely make do with some crates which cannot rely on the standard library. From my casual look into some areas of the codebase, there's quite a bit of usage of unsafe that's not necessary anymore. NLL will drive that even further.
Did you read the criticism of this argument? Please provide any direct evidence of this? Applets being unsafe was not the problem. Java was a sandboxed environment from the start - by design - for web security. Do designers make mistakes? Sure and they are usually fixed. Compare Applets with the far more exposed tech at the time of Flash - no comparison which was more secure. I will stand by the statement that Applets were secure. The Java plugin had which is a completely different technology was a different matter at times, since it allowed - with permission - to run unsafe code. Additionally, since Microsoft wrote their own Applet runtime (and own Java) it was much more unsafe since they exposed unsafe features since they wanted to expand the functionality - and did so in a proprietary way in an attempt to control the browser.
Do you guys prefer ndarray or nalgebra, and why?
A JIT based language (at least a good one) does not suffer from this, at the exception handling is removed at runtime - now it is more expensive when one does occur, as the OSR needs to occur to fix up the code, but still, the fast path is not affected.
I disagree a bit here. We had a more generic system so more was moved to Java, but even when we did native code, the native code had to be orders of magnitude faster to even come close to making sense due to the overhead of the Java -&gt; native transition for complex functions. It's pretty clear if you look at properly done benchmarks including warm-up (which is usually not an issue for server based systems), the JIT compiler code is in fact in many cases faster tha AOT compilers. There's advantages on both sides - in our particular case the speed was never going to match FPGA anyway, so it was more cost effective to be smarter and do more in Java.
[removed]
Now that is what I would call readable code. Still, by the API methods it would appear that all entries must be a copy due to the inserts taking a T? So you can't have a linked list of references to T? But I am probably wrong because I just don't understand the Rust type syntax well enough. (Or maybe you need a struct that contains the reference to the objects if you want to store refs)? 
And if there are no more objects available in the pool? No more predictability. Now most apps can "pre-size", but if they could do that really accurately they could just use arrays.
Btw, I've been taking a lot of the comments to heart and am working on a The Point of Rust Part II (I know - everyone is thrilled) to address issues like this. It seems that there is more content here of the tone "GC is bad, if you're are using it you must be stupid, or your programs or slow, yada yada yada" and I can't believe seasoned professionals steering Rust honestly believe this.
Well, if the Go webserver is more than 10% faster than the Rust ones in almost all of the webserver tests, and it spends 60% of its time in GC, how slow is Rust??? Clearly you are just completely wrong here. Maybe the Rust proponents that can speak freely will chime in to keep their engineering creds, and then people will stop posting comments like this.
I remember chasing down an elusive 1ms pause. As the code was instrumented to understand where it happened, it would shift to another place. Then we realized it was simply a major page fault on the first access to a page in the `.text` section (first time the code was called). That's the sneakiest syscall I've ever seen so far. Otherwise, with paging disabled and a warmup sequence to touch all the memory that'll you need to ensure that the OS commits it, you can avoid those paging issues. I fully agree that it's an uphill battle, though, and when you finally think you've worked around all the tricky syscalls, there's always a new one to pop up.
&gt; That is completely true, but sometimes hard to achieve and manage with very complex systems Indeed. Thankfully C++ offers a pretty expressive interface so you can generally wrap the complexity under an intuitive interface, but there are difficult cases... such as sending messages between threads.
A generic type only needs to impl Copy if the Copy constraint is added to the type signature. IE, the signature would read `T: Copy`, rather than just `T`. A generic type with no constraints can be anything. A reference or an owned value. The point of the constraint is to enable you to use methods from that trait. Yet if all you're doing is storing and moving values, and references to these values, then you have no need for a constraint.
By predictability, I refer to being able to profile the program between runs with the same input and get the same behavior. The same amount of memory will be allocated at any given point. Jemalloc usually improves performance, but you may also disable it and use the system allocator if you prefer an allocator with less heap management.
I copy pasted [websockets_chat.rs](https://github.com/seanmonstar/warp/blob/master/examples/websockets_chat.rs) in my project and it does not build. For every line, on l1.get::&lt;meter&gt;(), rustc says error[0061]: this function takes 1 parameter but 0 parameter where supplied
Not quite. You may construct a thread scope which shares a reference to the data with all threads, without the need for Arc. Though I also don't see your issue with Arc, as what a runtime GC is doing is much more complex and expensive.
Thanks, I like this kind of error reporting!
That is not true - runtime GC is more efficient that Arc since there are no atomic operations that are needed. Think about that happens in Arc, with the last dereference, that caller will still execute the destructor/free code in their calling space (or you need to have a threaded clean-up )
R, the new Language of the Web?
Can you provide a little more here: how (using the code provided) does the code (Rust lifetimes) provided prevent the caller from allocating an object, adding it to the list, then freeing it - meaning that subsequent retrievals of the object will return an invalid reference ? I'm not at all saying it can't, I just don't see anything in the API that shows me how that is prevented ?
Agreed - in the end I always test all changes with `test --stage 1` when I can. I was referring to the tinkering part.
Atomic operations are always needed when managing memory across thread boundaries. Runtime GCs aren't using magic tricks to avoid the unavoidable.
Nope, not true.
Mostly because support for non-zero already existed in the compiler, for non-NULL pointers (in Box, Vec, etc.)
You can look through the comments here, there is a site that will all of the performance metrics. In fact in the more complex cases, the Go system (and Java ones for that matter) show even better performance metrics.
I'm super excited for i18n to be coming out next! That's the last thing I have to do before making the jump from https://ErichDonGubler.github.io to https://ErichDonGubler.com. :) I may be willing to contribute simply out of self-interest after discovering how easy it was to fix the nested pathing and theb adding the indented syntax for SASS!
Move semantics. When you put a value into it, it is thereby owned by the map and can no longer be accessed except though requesting it from the map. You can't free it without taking it out of the map, and if you take it out of the map, then the map no longer owns it (unless you are borrowing a reference instead of transferring ownership). You can't have two owners of the same data. Additionally, None is used to convey the absence of a value.
&gt; I'm super excited for i18n to be coming out next! Make sure to comment on the RFC so I know who to ping when looking for testers! &gt; I may be willing to contribute simply out of self-interest after discovering how easy it was to fix the nested pathing and theb adding the indented syntax for SASS! Cool! `sass-sys` already supports indented syntax AFAIK so it should be trivial to add
Also, if what you store into the map is a reference, then the compiler will not allow you to transfer ownership of the original data until all references no longer exist. You therefore cannot free a value which is borrowed.
Are these equivalent too: let s: u32 = some_u32; let s: u32 = NonZeroU32::new(some_u32).map(NonZeroU32::get).unwrap_or(0); ? 
OK, then how do you implement a shared cache of commonly used immutable (for simplicity) objects? Clearly the cache (map?) holds the object (or more likely a reference to the heap allocated object), now I want to get the object from the cache and use it (but still have the object in the cache for future requests). Now make it more advanced. Say there is a background pruning step in order to limit cache growth. In a concurrent environment it would seem that the only suitable storage mechanism would be an ARC reference to the original object. Correct ?
In case you end up working with \`String\`s and other structs, there are some really nice tricks you can apply using Swig, which I wrote up here: [https://libpasta.github.io/blog/bindings/](https://libpasta.github.io/blog/bindings/). I would also keep an eye on what /u/Michael-F-Bryan is working on if you start getting deeper into FFI :) he's writing this guide - [https://s3.amazonaws.com/temp.michaelfbryan.com/index.html](https://s3.amazonaws.com/temp.michaelfbryan.com/index.html)
Also, although swiggen was a PoC using proc macros to do it all inline, it could/should probably be extracted out into just a basic codegen utility, used similarly to cbindgen. 
&gt; Cool! `sass-sys` already supports indented syntax AFAIK so it should be trivial to add Heh, I think you misunderstand me -- I was the one who authored PRs for [fixing nested SASS path handling](https://github.com/Keats/gutenberg/pull/255) and [adding indented syntax availability to SASS](https://github.com/Keats/gutenberg/pull/264) in Gutenberg while 0.3.4 (IIRC) was still in development. I'm giving Gutenberg a plug for how easy it was to jump in! :)
When I `rustup install nightly` I get a very very old nightly: info: syncing channel updates for 'nightly-x86_64-pc-windows-msvc' info: latest update on 2017-05-08, rust version 1.19.0-nightly (d985625b3 2017-05-07) That can't be right, what I am I doing wrong and how do I install a toolchain that tracks the nightly channel for `x86_64-pc-windows-msvc`? I can manually specify a specific nightly, but I want a toolchain that updates itself automatically.
As previously explained, borrowing is an option which does not take the value. It simply returns a reference that points to the value. In doing so, you will also be unable to free or mutate the map until all references to it have been dropped. A common pattern is an ECS model. As for sharing between threads, you would wrap the entire map in the Arc, and then you can share the map across threads. If you're wanting a solution which does not require a Mutex to modify the map, then you would want to look into using one of the available lock-free concurrent data structures that already exists in crate form.
I don't use Windows but I thought MSVC Rust isn't supported (well / anymore). I think you need MinGW.
Doesn't that change the drop order?
With that trick the IDE work great now. Wish to have debug support but as I'm learning now is good enough.
Try running the command with `--verbose`. Post the error here or create an issue thread in the rustup repo.
 rustup install --verbose nightly error: Found argument '--verbose' which wasn't expected, or isn't valid in this context I'll put an issue in the rustup issue tracker.
I'm not sure, but I don't think any code in rustc relies on it.
Have you ever tried to edit a Maven XML by hand? It's a monstrosity. Maven assumes that you're interacting with it through a specific IDE, rather than being able to comfortably edit it by hand with any editor of your choice. There's also the issue of dependencies and dependency management with Maven. It's very far behind what Rust is doing with Cargo. Searching for a dependency is very difficult, and getting documentation for that dependency is even harder. Even popular libraries like Spring ship with unreadable documentation. That's very different from the experience of using crates.io &amp; docs.rs.
That's not my experience, but I use the IntelliJ maven support, and almost never edit by hand. I was never a fan of using a strictly hierarchical format when most of the elements are flat. I like Gradle for its power and expressiveness, but it is also complex. npm is trivial to work with and use, and coupled with Node and the nested dependencies and modules is pretty powerful.
Ah sorry about that!
The API is not expecting anything. `T` can be anything, even a reference to a type. `T` is only defined based on how you use it. If you are storing `&amp;str` values into it, then it becomes a `List&lt;&amp;str&gt;`. If you store a `Vec&lt;String&gt;` into it, then it becomes a `List&lt;Vec&lt;String&gt;&gt;`. The API does not care, or need to care, about threads. Types that can will automatically derive the `Send` and `Sync` traits if they are safe to share across threads. How you share your data across threads depends on how you implement threading. Take this rayon scope, for example, which does not require `Arc` because the scope ensures that threads are joined before it returns, and the borrow checker is happy knowing that: let cache = generate_cache(); let mut a = None; let mut b = None; let mut c = None; { let cache = &amp;cache; rayon::scope(|s| { s.spawn(|s1| a = do_thing_with(cache)); s.spawn(|s1| b = do_other_thing_with(cache)); s.spawn(|s1| c = also_with(cache)); }); } eprintln!("{:?}; {:?}; {:?}", a, b, c); Crossbeam scopes work similarly, too. Rayon joins are also useful: let (a, b): (io::Result&lt;X&gt;, io::Result&lt;Y&gt;) = rayon.join( || do_this_with(cache), || do_that_with(cache) ); a.and(b)?; As are parallel iterators in rayon: let cache = &amp;cache; vector_with_many_values .par_iter() .map(|value| do_with(cache, value)) .collect(); You only need to resort to `Arc` &amp;&amp; / || `Mutex` when you aren't able to guarantee that the values will exist for longer than the threads that you spawn (ie: not using a thread scope). Then you can just take `List&lt;T&gt;` and make it either an `Arc&lt;List&lt;T&gt;&gt;` or an `Arc&lt;Mutex&lt;List&lt;T&gt;&gt;`. I write a lot of Rust professionally at work, and write a lot of software with threads, so I can give real world examples of a number of scenarios.
I agree that the above samples are far easier to understand and thus maintain. But why then, given the propensity of parallel systems these days isn't this 'core Rust' and all of the other low-level stuff completely hidden (unsafe).
I found the issue, it was because I still had a RUSTUP_DIST_VERSION for checking out the 1.28.0 stable early... Removing that fixed the issue.
You do realize that code at that link wasn't intended as like production-ready beautiful and clean code, right? 
Rather than asking "What's the Point of Rust?", which is well-explained in multiple venues including the documentation, I'd like to ask, what is your point? You don't really seem to have one. You don't like Rust. Good for you. What's your point? You have none that I can tell.
&gt; I have a friend that is a big time automotive engineer and you'd be surprised at the number of in car systems using Java. I was surprised at a point to learn how big Java was in the embedded world, but no longer :) I am still unclear on whether Java is used for real-time, though.
&gt; It seems that there is more content here of the tone "GC is bad, if you're are using it you must be stupid, or your programs or slow, yada yada yada" and I can't believe seasoned professionals steering Rust honestly believe this. If there are such comments, I'd like them pointed out. Ad hominems are not tolerated here. 
See, i'm confused here. Python is one of those garbage collected perfect languages you love so much, why is using it bad if GC is so great? Mere moments ago you were saying doing web stuff in anything but a GC was stupid, but now you're questioning the engineering for using a GC language for web stuff?
Works fine for me in Firefox 61.0.1 and Chrome 68.0.3440.84
One note: this gives predictability in terms of memory corruption, but not in terms of run-time. That is, since calling into the OS to allocate/free memory has unbounded latency, there is no guarantee that two consecutive runs will have the same run-time.
Standardizing commonly used functions eases language adoption. You are wrong about the Java and go apis. Go in particular. Java has remained backwards compatible with 1.9 while drastically increasing the functions in stdlib. 
That’s my point. That’s the same reason of a lot of the bad C in the world. I need this to work now, do I’ll do it as quickly as possible and clean it up later. Never happens. The promoted promise of Rust is that it is going to cause clean code before it will even compile. Not the case. 
It is the first time I am writing a blog so I have no idea of how to correct it. I am using jekyll and did not find anything in my [config.md](https://config.md) file. Is there something particular I should modify ?
I think there is confusion about the potential of Rust, and the current state of Rust here. For example, looking at [Techempower 16 - Fortunes](https://www.techempower.com/benchmarks/#section=data-r16&amp;hw=ph&amp;test=fortune) will show Go's fasthttp framework well ahead of Rust's actix-raw. In the absence of `async`, and `async` database drivers, the performance of actix-raw is clearly lagging behind fasthttp's, itself at only 80% of the performance of C's h2o. However, I would note that there's a lot of "cheating" going on here: - Go fasthttp uses pooling, so has strict instructions (in the documentation) about NOT keeping some objects in use after a certain point, - actix-raw is not actix-web, it's a stripped down version which shows the raw power of actix but is not really "practical". I also think that comparing async vs non-async is not very interesting. Yes, Rust code that does I/O is currently slow when using the ergonomic sync calls instead of less ergonomic callbacks (when available). It's unsurprising, and uninteresting: Rust needs good async support, we all know it, it's being worked on, let's wait for it? Once Rust gets proper `async` support we'll see if how async Rust fares... and draw lessons if it fares poorly.
The more complex ones (such as Fortune) are uninteresting now because they teach a lesson that the community already knows: [Rust needs good async support](such as https://www.reddit.com/r/rust/comments/942nik/the_point_of_rust/e3llfgr). It's known, it's the work, nothing to learn from those benchmarks until the support is there.
Please reread the thread. Every other post contains a statement calling GC slow. 
In case it matters, I'm on Windows 10 Pro Version 10.0.16299 Build 16299 (basically just standard windows, latest updates). Is there a way I can get more details about this error? The browser isn't very helpful here. 
&gt; or 2) use atomic reference counting which requires CAS semantics to know when the event object E should be destroyed Actually, no, you don't need CAS. You only need `fetch_sub` which is significantly simpler (no retry necessary). This still implies contention on the counter; obviously.
I agree completely. Fortunately for me I knew C++ before coming to rust so the transition was not so difficult but I could easily understand that a Python programmer would have much more difficulty to do it. Having documentation about simple things to do to solve a problem without relying in complicated Rust features would be very helpful to help people coming from other languages.
Any chance the Spectrum data tried to sort out references to Rust the video game from Rust the programming language and overcompensated? Any reasonable way of validating the data?
Hey thanks for replying, looks like I fixed the issue. Some time ago I hard-coded the IPs in the hosts file for rust related websites. I did this because DNS lookups kept returning only IPv6 addresses and I don't have IPv6 at my current ISP. So I threw in some valid (at the time) IPv4 addresses to work around the issue. Removing the hard-coded hosts entries fixed the problem.
I have no problems with people expressing their opinions, although I do wish they were substantiated and quantified, providing relevant/representative benchmarks is hard, since no two people have the same requirements. I only care about ad hominems, such as "you must be stupid". Those are not tolerated.
I fail to see how it eases language adoption. Importing a crate is no different than importing functionality from the `std` or `core` libraries. The process is the same. Cargo will automatically pull in the dependencies for you when you build / check the project. Backwards compatibility is not the issue. The issue is in having inferior APIs included in the std, and where the standing recommendation is to ignore the libraries in std and use superior third party implementations, instead. Case in point, the http library that ships in Go since it was launched is really bad. The general recommendation is to avoid it and use a superior third party HTTP library instead. As for the rest of Go's standard library, it's very anaemic / inadequate. They need to throw the whole thing out and start over.
Indeed the documentation of nalgebra is really beautiful and contains many examples which is wonderful for beginners. However, the first time I tried to use it I still had some problems. For example, the documentation explains that we can compute inverses but it took me a long time to understand how. Same for the factorizations, I did not understand that after being factorized we obtain a data structure containing the different elements of the factorization. It may seem easy with insight but I really stumbled on it the first time I needed to use it. Besides, when writing this post I needed obviously to check that my code was compiling and working as expected. Well, let's say that I learned a lot on things I thought I understood haha. 
Thank you very much! I hope it will serve many people wanting to use nalgebra :)
Bah, six minutes of compiling only for it to finally blow up on linking because it can't find `libstdc++`. Setting LIBRARY_PATH to point to one of gcc's just makes the linker explode with undefined references to std::* - all from libsass-sys from the look of it. Anyone else got this to compile under FreeBSD?
&gt; me was Strings. I mean it's a bunch of characters, why is it so complicated Because Strings, no matter what language you use, *aren't* just a bunch of characters. Strings are really complicated. Other languages hide this and make your life harder because you need to fight their them on it now. Unicode is complicated.
Unrelated, but you might be interested in https://github.com/evomata/gridsim.
I did the same thing before I started. I just poured over the book for two weeks. In retrospect, there was too much information to absorb purely from reading. Take advantage of the buttons to try the code samples on the rust playground; it will help a lot!
Yes of course. That is why I wrote this: &gt; and once you understand that this is more of a CS than a Rust problem, the anger can be managed easier. It's specifically about the contrast of Rust vs Python and how they handle Strings.
&gt; And on top of that it has been proven (via Linux) that these systems are easily written in C. Let's avoid such flawed arguments please. Pyramids have proven that thousands of workers and decades of work suffice to build large and imposing stone buildings; this does not mean that there is no point to using steel, concrete, and modern construction techniques and tools. The only thing that Linux being successful means is that writing an OS in C is possible. It gives no clue as to whether another language would not have made the development easier, the resulting product faster, influenced the design in different ways, etc...
Libsass is also causing issues with Muslim :/ can we add freebsd to Travis? I also maintain sass-rs so I would welcome a way to make it work on as many platforms as possible
&gt; You need to use nightlies to write drivers. Do you? You need a Nightly to create a `no_std` binary, but drivers are libraries, and `no_std` libraries are possible on stable. Unless there are other features I am missing, I would expect it is possible to write drivers without nightly. Are you sure you didn't mean that you needed `unsafe`?
&gt; I try not to assume the worst, but I have to assume troll at this point. Please avoid assuming.
Only promise of Rust is memory safety (and being data race free?). Are you talking about error handling? That is difficult task in any language. Difference between C and Rust is that when you decide to cleanup your code, compiler will make sure that code remains memory safe. Nothing more, nothing less.
&gt; Also, have you seen the size of a GO executable ? I’m on the road right now so I can’t give the hard numbers but when I last looked it was fairly trivial. Most GC is fairly trivial code. It is not large. 1.9MB for the typical Hello World according to [this question on Stack Overflow](https://stackoverflow.com/questions/28576173/reason-for-huge-size-of-compiled-executable-of-go). For reference a statically linked C Hello World is said to be 750KB according to the Go FAQ, leaving 1.15MB of overhead.
**Egon Spengler:** Don't cross the streams. **Peter Venkman:** Why? **Egon Spengler:** It would be bad. **Peter Venkman:** I'm fuzzy on the whole good/bad thing. What do you mean, "bad"? **Egon Spengler:** Try to imagine all life as you know it stopping instantaneously, and every molecule in your body exploding at the speed of light. Seriously, Rust is probably mostly a fine language for writing Racket extensions. You may have trouble with memory management, though: Racket's GC and Rust's arena are going to have to get along somehow: you could end up with a ton of copying if you're not careful. Honestly, having written a lot of Scheme and Rust, I'd just write in Rust.
Too many people, both Rust evangelists and its opponents, focus on memory safety and ignore all the other things that make Rust great. IMO the most important thing is that possibility of data races is compile-time error - which greatly simplifies concurrent programming. Introducing many functional programming patterns into imperative language. Trait system that makes it very easy to extend functionality of pre-existing libraries, as well as provide unified interface for writing both static and dynamic polymorphism. And the no-exceptions approach to error handling makes ensuring all execution paths are covered much easier. The point of Rust is to create software that will never, ever break.*
*mumbles something incomprehensible about monoids and endofunctors*
I've no idea, maybe look for `baseurl` and `JEKYLL_ENV=production` on https://jekyllrb.com/docs/configuration/. Are you using any plugins like [this one](https://jekyll.github.io/jekyll-seo-tag/)?
I removed this. You generated enough pointless discussion the first time around. Please take your arguments somewhere else.
(Not every platform uses jemalloc; Windows for example)
Would be great to link to this from the [here](http://www.arewelearningyet.com/scientific-computing/). I'll try and add it soon, but PR to add it is also very welcome and tends to make it happen faster.
The documentation clearly states the return type tho, which you can click on to go to it. That type's description will then tell you more about it. Usage guides like you've made are nice tho! Personally, I also use them as go-tos for a quick intro. Understanding docs is important for advanced stuff tho :)
`mem::transmute`, your friend and helper in all matters related to shooting yourself in the foot. [Example on playground](https://play.rust-lang.org/?gist=1740da5f8700bcde780181838fadc331&amp;version=nightly&amp;mode=debug&amp;edition=2015).
I've sent a [pull request to fix building sass-rs on FreeBSD](https://github.com/compass-rs/sass-rs/pull/31) - mercifully simple. Unfortunately Travis doesn't support FreeBSD :(
For whatever reason pom.xml does not use attributes. It would be much less eye-bleedingly terrible if it did.
Note that accessing crates from the prelude is experimental in Rust 2015, but is stable in Rust 2018. Which means you can just do `std::stuff` instead of `::std::stuff` without needing `use std;`
If you click on the screenshot i linked you'll see, but yes.
Maybe not, but they could use it, or something like it, if they needed to. The option is there, whereas with a runtime GC the option is not.
I'm looking forward to this, it seems like a nice and useful thing.
Wow. Well I guess that says I a lot. I never made a derogatory statement about anyone. I backed up every point I made with references. I was 35% upvoted - which means I am clearly not a pariah. And you still kill it. I guess the emperor really does have not clothes.
I didn't say that, I was just repeating what another poster had said, who I assumed had much more experience.
I wanted to try `async`/`await` and started with this: ``` async fn foo() -&gt; i32 { 42 } ``` That worked fine, but this: ``` struct Foo {} impl Foo { async fn foo() -&gt; i32 { 42 } } ``` ... didn't work, it failed with ``` error: missing `fn`, `type`, or `const` for impl-item declaration \--&gt; src/client/oauth.rs:141:11 | 141 | impl Foo { | \_\_\_\_\_\_\_\_\_\_\_\^ 142 | | async fn foo() -&gt; i32 { | |\_\_\_\_\^ missing `fn`, `type`, or `const` ``` What's up with that?
But you don't need that to determine if an object is reachable, which is the heart of GC. If an object can't be reached it can't be mutated. That is why it is more efficient. With generational and regional collectors the amount of memory that needs to be scanned gets smaller and smaller.
Java doesn't have async and await and has always performed well. sync IO with threads is faster until the number of connections grows beyond a point. Granted, Java has async IO, zero copy networking, and other tools for writing an ideal webserver, but it's my understanding Rust has these too. In my experience the number one factor that determines the performance of the resulting system is the ability to refactor, which is usually controlled by simplicity, proper abstraction and encapsulation. The design, data structures and algorithms control the performance more than anything else. We routinely tested the performance of our Java based system against competitive, well established native (C/C++) systems and we came out on top in almost all cases. The few times we were behind is when someone had created a brand new system from whole cloth, using the previous domain knowledge - the assumption being the old code was just too hard to change without breaking, so better start fresh - we NEVER found this to be the case in our system.
I'm not infallible. If you disagree with my decision, please contact all moderators via modmail. Your comments often crossed the line into language bashing. This is something we don't condone here. Also apropos emperors clothes, you failed to supply evidence when asked, just asking everyone to google. There are a lot of studies and you can't expect folks to guess the ones you picked for your argument. Finally I should let you know that we approved your last post despite numerous reports. This one got 4 reports before I blocked it.
Yeah, that's not what Rust promises. There's basically no way to promise that in a language without being super-strict. What lead you to believe this was a Rust promise?
`async` isn't expected in an `impl` block right now. I'm not sue if it is intended behaviour, and it's not mentioned in the RFC text, so I'm not sure what is up with that.
That is completely untrue, I told one person to Google because it was more than obvious. The first (and only other) I told to Google I also included a direct reference to a paper. Teaching someone to fish... I have to expect you have some significant experience to be a moderator. The stuff I am bringing up is well established - the fact it is being questioned is kind of frightening. I never bashed the language, I asked for concrete examples of the sweet spot, and why they thought in the end it would turn out and better than the current C/C++ fiasco/problems. But even more scary, my posts were reported on what grounds? Making people question their choices so their egos were bruised? I never spoke harshly about anyone - the harsh tones came the other way. And so the second post is rejected, even though after being called out that I shouldn't reference stdlib code (because nobody works with it), so I found user level Rust code that I was referred to from other Reddit posts and used that as a basis to enhance the argument that things were broken - and then that is not acceptable. You guys aren't doing engineering, you're doing religion.
Brilliant job! I'd love to see where this goes.
Aha! the `.get` method signature [changed](https://github.com/iliekturtles/uom/commit/ff5bccdcfb96818b8ac44eb61b55fb7219a944af#diff-a9bf580625791b184ea1d4031373c735) since 0.19 was released. If you copy the [`si.rs`](https://github.com/iliekturtles/uom/blob/v0.19.0/examples/si.rs) that goes with the 0.19 release you'll fix the compile error.
That will be cool. Currently I use a portable version of python that I ship complete, so I'm not worry too much if all weight some megabytes.. but if is getting to heavy then is not ideal.
This is the subreddit for the Rust programming language. You're looking for /r/playrust
Cheers dude 
H￸o￸w c￸a￸n I di￸sab￸le ht￸tp1￸.1 or lower in a￸ct￸ix? Say I got the following code: [https://github.com/actix/examples/blob/master/tls/src/main.rs](https://github.com/actix/examples/blob/master/tls/src/main.rs)
Honestly, I tried to find the exact comment in my Part I, and I couldn't so maybe it was a general impression, and even if I could of found it, one person stating it shouldn't make it so. So back to the general impression, by reading the comments here and elsewhere the writers are correct that there is a lot of horrid C/C++ in the world, and typically this horrid code has lead to a lot of security holes. It is my impression Rust was created to sort of "start fresh" with a better systems language , to create more secure, more efficient to program, and more maintainable code. I could be wrong there, but I don't think so. I used this example for a person using a tool (Rust) that was clearly not needed by the problem domain, and in the process wrote some pretty ugly code that is not maintainable. If this person used a more restrictive language, he would of got better results with possibly some degredation in performance, and we would of been left with more maintainable = useful code in the process.
I don't understand, I am pretty sure you're enhancing my point. I got half way through and I was crying about the trials and errors due to the use of unsafe. This just doesn't happen in Java, or even Go (not in the standard). I 20+ years of using java I have used less than 10 libraries that have native code. I could definitely get behind "safe rust", but at that point, in complex apps, I would still contend that a GC language would be far more productive.
I ended up using work in a demo for a company I was interviewing at and it worked just as I expected, but I feel like I keed to learn how to implement custom Futures so that the computationally heavy tasks don't clog the thread pool. But thanks, overall :)
Do you have a link/writeup for what you worked on that you'd care to share?
You can use Grammarly if you don't feel confident writing English. 
I would suggest using bluss' `ndarray` for handling multidimensional arrays, it has a couple subcrates that are quite useful, in your use case there's a `ndarray-parallel` crate that implements parallel iterators on ndarrays (so you don't have to fight the borrow checker too much or go unsafe)
Doing some more research, I came across this [https://www.reddit.com/r/rust/comments/8s7gei/unsafe\_rust\_in\_actixweb\_other\_libraries/](https://www.reddit.com/r/rust/comments/8s7gei/unsafe_rust_in_actixweb_other_libraries/) and followed it around. How you can claim Applets unsafe with a straight face is pretty unbelievable. The Java system has had from the beginning the ability to prevent any running and usage of non-public API methods (e.g. cannot use the sun.misc package). This was always enabled in Applets, and by default in WebStart applications. The user needed to specifically allow the application "unsafe access". Contrast this with Rust applications. There is not guarantee - other than OS level protections that the code isn't doing something nefarious. Rust has nothing like Applets and never will. Rust programs by definition will always be subject to security holes until "safe rust" is the required standard.
Interestingly enough if you do a custom ranking using just hackernews Rust comes out on top.
I don't think that is true - pretty sure Swift (and ObjectiveC) are secure, and don't use GC (unless you consider reference counting GC, but most (including myself) don't).
I'm kind of done with this. You keep making very broad claims based on very flimsy foundations. That fig leaf of a "maybe I'm stupid" in your original post is absolutely no substitute for actual humility and trying to understand things before claiming they suck.
You told both myself and others to Google, and I followed most of your discussion without ever seeing a link to a paper – or even a citation I could look up. You can deny the facts all you want. Also I'm neither doing religion nor engineering on reddit. I do community stuff (some of which is moderation) here. But feel free to label. People reported your post for language bashing, nonconstructive comments and trolling. Read the sidebar and the reddiquette for more information. Again. I may be wrong and I urge you that if you feel you've been unfairly treated, ask the other moderators to cross-check.
You told both myself and others to Google, and I followed most of your discussion without ever seeing a link to a paper – or even a citation I could look up. You can deny the facts all you want. Also I'm neither doing religion nor engineering on reddit. I do community stuff (some of which is moderation) here. But feel free to label. People reported your post for language bashing, nonconstructive comments and trolling. Read the sidebar and the reddiquette for more information. Again. I may be wrong and I urge you that if you feel you've been unfairly treated, ask the other moderators to cross-check.
If you honestly believe that, then please help me. I understand the difference in use cases, and clearly this person used Rust for a task where it was not appropriate. That you can't agree with that says more about you than me, or even Rust. If you really think there are limitations in Java memory mapped files, as an expert in them, I will be more than glad to help you to understand how they are performant and safe.
I would ask, as a personal favor, that you reread my posts. If you'd like I can go back through them and send links to all of them to you. I have included many references. I am not a troll. I honestly came here because I am concerned about the 'fanboy/marketing' nature of what is happening in the software world. JS is abysmal, luckily people are waking up to it, and using transpiling to get around. I've been doing a lot of investigation on "hot" languages/platforms like Go, Node, Rust, etc. and was genuinely confused about the focus and direction, and the results, or Rust. I can guess that I have a bit more experience than you, and probably the ability to be a bit more open minded - my career or livelyhood does not and will not depend on the success of Rust. I honestly think the best solution is when there is a great fit between the tool and the task, and clearly not all tasks are the same. Disparaging my character, or my engineering abilities, is just not right. This thread https://www.reddit.com/r/rust/comments/8s7gei/ alone should give all of the Rust proponents great pause. It is a huge, and real, concern.
I see; I guess that's a bit of a letdown given that something like 90% of all functions are in `impl`...?
Thanks! I'll have to check that out. Fighting the borrow checker over parallelism was the most difficult part of writing the heat dispersion simulation.
Python is a language for embedded programming? By that definition Bash is an embedded language.
I don't. It was my first Rust project, only my second real programming project, and it was pretty bad. I thought about restarting it recently, but it doesn't compile any more. I planned out a re-write, but then found [Duplicati](https://www.duplicati.com) which is basically what I had planned out and decided to play with actix-web instead.
Depends IMO. If the entire app is contunually creating objects and destroying them (consider a message processor, without pools, etc.) I would much prefer to spend a 10% overhead and have clean code that was easier and faster to write, and use the productivity savings to buy better hardware if needed - but even better to give the money out to the developers as bonuses for making it happen.
Btw, the post I really meant to reference was [https://www.reddit.com/r/rust/comments/8zpp5f/auditing\_popular\_crates\_how\_a\_oneline\_unsafe\_has/](https://www.reddit.com/r/rust/comments/8zpp5f/auditing_popular_crates_how_a_oneline_unsafe_has/)
Or write it in Rust with 0% overhead with clean code that is also easy and fast to write.
You would have to provide more evidence on why Swift/ObjC are not secure. They have bounds checking, no pointer arithmetic, and automatic memory management, and by unsafe I mean memory exploits, not injection attacks, etc.
Did you miss my link to Swift's UnsafeMutablePointer type? It's.... unsafe...
I tried to play with custom rankings and I found a pretty disturbing error. If it's set to Stack Overflow only and 2018, seven languages get 0.0 points, and one of them is Rust. If SO is ignored in the "Trending" ranking, Rust goes from 20th place to 13th.
You don't necessarily need to learn a *whole* bunch about custom futures, you could try some things first, like increase the number of workers in the tokio threadpool. It defaults to the number of cores you have, which is probably best if CPU bound (if CPU is already at 100%, more threads are just bad), but if some tasks are waiting on IO, you may want more workers...
Sorry, I didn't realize the Nope was a link. Thank you, I didn't know that. Not sure that was possible in ObjC? That explains more why the later iOS apps crash more... :)
&gt;Not sure that was possible in ObjC? ObjC is a strict superset of C. It absolutely has unsafe pointers
Haskell is marked for embedded applications as well...
I took over the maintenance of an large open-source OSX app, Seashore, written in ObjC, and I never encountered it, so I was just assuming - but thats also the only ObjC I've ever done. I know, Google is my friend... Thanks.
WTH is ClamD?
To back-peddle/clarify this a bit. There are numerous JCP proposals for value types in Java, usually under the need for speed, or lower memory consumption. In my gut, in almost all cases the speed issue is neglible, since just about all applications of value do significant IO, and this is orders of magnitude slower than the memory access that support them, so combined with intelligent prefetching, it just isn't that big of a deal - only really shows up in micro-benchmarks. The memory size issue seems not very important either, considering in most cases the largest data processing apps are JVM based, and they just partition and scale out.
Sure! I hope it can be of help.
I think your evidence is unsubstantial and Java has most of the same problems because of multithreading. 
As a wordpress developer, i am so confused because Gutenberg is the new upcoming text editor of wordpress https://wordpress.org/plugins/gutenberg/ But its just a name, the software you wrote looks good so keep up the good work
Boy that project has a bright future lolol
What garbage data. Rust is one of the fastest growing languages in open source yet they put it near the bottom on trending languages. If anything with all the hype around Rust it should be #1 in that ranking.
Java has had concurrency constructs designed into the language from the beginning. People can argue about the best way to do concurrency, CSP, etc. but almost all java programs are concurrent to an extent - give the nature of Swing UI and background processes, etc. Programming is hard. Concurrent programming is harder. Doing both for a long time, I would much rather use a GC language for highly complex, highly concurrent applications. And multithreading doesn't cause memory issues - at least not in Java - it does in many cases in non-GC languages due to double free, and no free. It can lead to data race issues, but often programs are highly concurrent in the pursuit of performance from the beginning, so having the right amount of synchronization is paramount to proper performance - but this is not always done correctly.
&gt; Java has had concurrency constructs designed into the language from the beginning. People can argue about the best way to do concurrency, CSP, etc. but almost all java programs are concurrent to an extent - give the nature of Swing UI and background processes, etc. Programming is hard. Concurrent programming is harder. Doing both for a long time, I would much rather use a GC language for highly complex, highly concurrent applications. &gt; &gt; This is a list of excuses. You're being a Java apologetic. Concurrent programming is hard because you need to keep track of ownership yourself. Rust solves this automatically for you and will prevent compilation if you would have race conditions. You're also completely ignoring the way computer hardware is going and has been going. If you want to write fast software you MUST be multithreaded or multi-process.
Not sure what you are referring to. Java has been fully concurrent since its inception by design. You don't need to keep track of ownership in GC at least for object lifetime. For concurrent access you need additional constructs, and at what level you do that is determined by the complexity and performance requirements, e.g. course mutex, fine mutex, CAS etc. You have these same concerns in Rust. Oh I get it, you are trolling. Ah thanks. Cool.
Alright, I just want to say one more thing, just to make sure you understand my point. The issue isn't with Rust or Swift or Objetive-C in particular, pretty much every language is built on an unsafe foundation. Many language runtimes are implemented in either C or C++, and bugs in the runtime can absolutely cause memory unsafety in the managed language. Many languages also have CFFI that can also introduce memory unsafety when used incorrectly. In general, in programming, there are safe abstractions that can only be implemented in unsafe code. Additionally, the idea of separating safe code from unsafe code isn't unique to Rust, it was AFAIK first implemented in the 80s in[ Modula-3](https://en.wikipedia.org/wiki/Modula-3#Safe_vs_Unsafe). Many modern languages try to make unsafety explicit, in e.g. Rust or C# with a keyword, or in e.g. Swift or Haskell with a convention that unsafe types/operations are prefixed with "unsafe"
I only meant to compliment you. 
Open source antivirus software https://www.clamav.net/
I know, I know.. but I had the name before dammit!
Btw u know what would be an epic troll?
You're talking to a wall. I'd lock both threads and move on.
That's cool. Maybe you can help out [https://github.com/robaho/seashore](https://github.com/robaho/seashore) I only learned ObjC and fixed the code in a day, so I'm sure you can do a lot more.
Great tutorial!
The daemon interface for Clam AV - https://linux.die.net/man/8/clamd
I hope that is a joke? Both posts were/are upwards of 30%+ upvoted, so is that portion of the community not worth being listened to, or are you just assuming they are all trolls out to "get Rust" - to what end? The post that was removed probably demonstrated the Rust issues I'm having more clearly, and probably more clearly to others that may be having the same reservations - no wonder it was killed.
You found a Rustmander! They are weak to Ferriscon and strong vs Memorus!
Although you added an \*, the following trivial, correctly compiling code, deadlocks: **use** std::sync::{Arc, Mutex}; **use** std::thread; **use** std::time::Duration; **fn** main() { **let** m1 = Arc::*new*(Mutex::*new*(0)); **let** m2 = Arc::*new*(Mutex::*new*(0)); **let mut** h1; **let mut** h2; { **let** m1 = m1.clone(); **let** m2 = m2.clone(); h1 = thread::spawn(**move** || { **let mut** data = m1.lock().unwrap(); thread::sleep(Duration::*new*(5,0)); **let mut** data2 = m2.lock().unwrap(); }); } { **let** m1 = m1.clone(); **let** m2 = m2.clone(); h2 = thread::spawn(**move** || { **let mut** data = m2.lock().unwrap(); thread::sleep(Duration::*new*(5,0)); **let mut** data2 = m1.lock().unwrap(); }); } h1.join(); h2.join(); }
also, the following correctly compiling, trivial code, deadlocks - Rust is not immune. once you get into concurrent systems, there are a whole other set of issues you need to deal with... **use** std::sync::{Arc, Mutex}; **use** std::thread; **use** std::time::Duration; **fn** main() { **let** m1 = Arc::*new*(Mutex::*new*(0)); **let** m2 = Arc::*new*(Mutex::*new*(0)); **let mut** h1; **let mut** h2; { **let** m1 = m1.clone(); **let** m2 = m2.clone(); h1 = thread::spawn(**move** || { **let mut** data = m1.lock().unwrap(); thread::sleep(Duration::*new*(5,0)); **let mut** data2 = m2.lock().unwrap(); }); } { **let** m1 = m1.clone(); **let** m2 = m2.clone(); h2 = thread::spawn(**move** || { **let mut** data = m2.lock().unwrap(); thread::sleep(Duration::*new*(5,0)); **let mut** data2 = m1.lock().unwrap(); }); } h1.join(); h2.join(); }
At least it makes binaries.
Amazing how far people can come with shit for brains. Kinda jelly. I thought you were just a bad troll, now I wonder if you got fired from that gig in chicago.. Either we are all morons or you are. It's you. For the record it wasn't ageism. If you bothered to understand the clip you'd see that. Saying that you have 35+ experience means nothing because people can just sit on their asses and be little code monkeys for that amount of time. Or in Louis's terms be garbage men getting bjs.. The fact that you felt the need to bring it up betrays your insecurity about your inexperience. Despite being an ass to you I wish you get better man. :/ And just give Rust a shot, Azul might be great, but not everyone can afford it and waiting 20 years for yet another Sufficiently Smart Compiler™️ pipe dream for that tech to be available to everyone in a language that doesn't target junior devs doesn't seem to be worth it. 
I just came across this as well [https://www.reddit.com/r/rust/comments/938fmh/stackoverflow\_on\_huge\_boxed\_element/](https://www.reddit.com/r/rust/comments/938fmh/stackoverflow_on_huge_boxed_element/) People are drinking some serious KoolAid around here.
Again, my formula for any forum: quality of engineers = 1 / (level of childish vitriol spewed)
Like Open CV but not bindings is exactly what I was looking for recently for a work project.
Assuming you're not the creator, have they considered using sixels or Unicode half-blocks to make it look nicer?
Dude how much you spewed? You're bad at trolling and you don't troll throwaways man. :P
Ok time to close this throwaway. You win, I spent way too much too much time on you. Hope you get better you magnificent moron. :)
I wasn't trolling, and I never spewed. I looked at your other posts in other reddits - somethings not right and I'm pretty sure I could never help with it. I wish you a good day. 
&gt; in other reddits - somethings not right and I'm pretty Ad hominem. And sure. I can't control myself and am a giant arse like you lol. Mostly I try to behave on this sub tho. And dude if I were a bigger arse I'd dox you to your bosses. What kind of absolute moron posts this garbage under their own name. I ramble on the internets for fun, you fucking sign under it. Just fucking delete it. It's so fucking stupid half the time I'm thinking you aren't OptionsCity dude and have to be trying to make him look bad by impersonating him. Which is equally fucking moronic. Did he sleep with your lady or something? Anywho I don't keep accounts around for too long, don't worry I'll be deleting this account after I shower in 30 mins, because I spend too much time obsessing about morons like you on reddit. Wonder what asinine reply I get out of you tho..... 
I _was_ going to post this but then I forgot to. Here's his video documenting the process of starting it: https://youtu.be/8KP8rjUCa9w
This is Hopson learning the language, so they're sticking to something they knew roughly how to do beforehand -- in this case, ASCII display. They've done other similar things with C++ and OpenGL and/or SFML (such as their biggest view-wise series of recreating Minecraft).
Sometimes one wonders how you survived in hft. Do quants whisper in your ears and you mash your keyboard until passable solution appears? That how you spent 35 years? I wish I had seven figures for being complete duffus like you. Data races, look it up. Nobody claimed dead locks. For the hundredth time.
The concepts of GATs, GADTs and the type families are closely related and in many cases may be expressed with each other. The [GATs RFC](https://github.com/rust-lang/rfcs/blob/master/text/1598-generic_associated_types.md) even has some comparisons with Haskell. A regular generic data is parameterized by the type that its constructor holds. GADTs is a generalization over this. The data type parameter is associated with the constructor and may not match the type of the value in the constructor. Type families are more abstract, they just associate the types with each other. If they are used as a type parameter of a data type, you can express a GADT. GATs is indeed a fairly Rust specific and quite complex feature. From a simplified Haskell perspective it looks like this is type families that can only be made within the traits and can operate both on types and lifetimes. Even if this is not entirely accurate, I think that here the type inference challenges are more related to this type system feature than the language.
I am sorry if I bothered people. It wasn't my intent to troll, and things have gone off the rails. I won't be posting here anymore, and again, I'm sorry if my comments offended people.
You've been given explanation that Rust doesn't make guarantees about dead locks, it does make data races impossible however (outside of unsafe code). You replied to parent with unrelated factoid that nobody disputed demonstrating effects that you were informed of beforehand (by at least me, probably others as well because this is half of Rust's schtick). 
I'm working in a relational language, that also is a columnar engine like kdb+ [https://www.tutorialspoint.com/kdbplus/index.htm](https://www.tutorialspoint.com/kdbplus/index.htm) and need a lot of work in the areas of join optimizations, indexes, query planner, etc. Is \*not\* a full rdbms, but can look alike one in-memory with zero transactions or all that. Or similar to work with python/pandas or ndarray but more generic. If wanna help, I'm interested in how implement: [http://www.frankmcsherry.org/dataflow/relational/join/2015/04/11/genericjoin.html](http://www.frankmcsherry.org/dataflow/relational/join/2015/04/11/genericjoin.html) (the code is in rust but I need to adapt to my lang.) Also, columnar processing benefit for late-materialization: [http://db.csail.mit.edu/pubs/abadi-column-stores.pdf](http://db.csail.mit.edu/pubs/abadi-column-stores.pdf) My constrain? This is a scripting language for everyday use, not a central datastore, so I need to make it as practical/close as work with python...
Personally I would love to have a rust alternative to OMPL. I know your crate rrt (thanks for it!) and I think it is a good one but it is just one motion planner. It would be great to have the equivalent of OMPL with all the benchmarking possibility and the easy switching between motion planners.
TLDR: Have your iterator hold a `std::slice::IterMut` of your pixel vector, and everything will work. More detail: The gist of what's going wrong here is, Rust isn't sure that you're giving out a different pixel every time. _You_ know that you're always giving out a different pixel, but that's based on a bunch of other facts that the compiler either doesn't understand or doesn't want to rely on. (No other code will ever decrement your counter, your counter never overflows, members of a `Vec` at different indices are disjoint, etc.) All the compiler sees is that you're giving out a pointer that's supposed to have a lifetime independent from `&amp;mut self` -- so that it's safe to e.g. build a collection of items from your iterator instead of using them only one at a time -- but you're getting that reference via `&amp;mut self`, and that's impossible. So why does this work in the shared case, if not the mutable one? I don't remember the name for this rule, but Rust is happy to "extract" a long-lived reference out of a short-lived one, as long as the reference is shared, like this: fn extract_reference&lt;'a, 'b&gt;(x: &amp;'a &amp;'b i32) -&gt; &amp;'b i32 { *x } The idea here is that even though `x` might be some very short lived double reference, we can reach through it while we have it, and make a _copy_ of the inner reference it's pointing to. And here's the key: Making copies of shared references is totally cool, but making copies of a `&amp;mut` is fundamentally illegal, because it would violate the uniqueness guarantee that `&amp;mut` makes. `let pixel = &amp;self.image.pixels[self.i];` is reaching through the short lived `&amp;mut self` of your shared iterator, and copying out a longer lived reference to the underlying vector. Cool for shared reference, not cool for mutable ones. At the lowest level, you basically need unsafe code to solve this. You would audit all your logic and prove to yourself that you're never giving out the same reference twice. Luckily, `Vec` and `std::slice` have already done this for you, in the form of the `iter_mut()` method that they provide. The implementation of that guy has tons of unsafe code in it, but it's probably correct :-D So if you store the `IterMut` that you get from that, and call `next` on it directly, the compiler won't give you any trouble.
Google closure and Fn/FnOnce.
To expand on this, your fn can take a generic `F: Fn(T, T) -&gt; T` and then you can pass the functions corresponding to the operators. You would pass these functions as `|x, y| x + y` or, equivalently, `std::ops::Add::add`, etc. An operator on its own does not represent a function.
In `rdedup` the `store` path is heavily parallelized and documented (which I think is a good education material) but the `load` one is not. It shouldn't be too difficult, and all the tests are already there so instant feedback is possible. And the there's no math, just a lot of data. :)
I think I solved it. Thanks for your help!
I probably deserve my downvotes but unoriginal software names really make me cringe.
Rust sucks 
Why would you box up these closures?
&gt; nalgebra just re-exports them so you don't have to explicitly depend on typenum to use them. That's not exactly true. nalgebra really defines the structs U1 to U127. If greater values are necessary, the user can still depend on typenum and use its U128 and greater type-level integers. nalgebra defines those structs up to U127 so the compiler can generate better error messages. That way, for all type-level integers bellow 127, the compiler will actually output `U1`, `U3`, etc. in its error messages instead of something like `typenum::UInt&lt;typenum::UInt&lt;typenum::UTerm, typenum::B1&gt;, typenum::B0&gt;` (this is how tymenum represents its integers) which would be unreadable.
Looks like it is to extend the life of the closure out of the scope of the match.
Unknown size at compile time?
Look at my other post, &amp;dyn Fn would work too among other things.
I see. If I had enough time, I wanted to do that, but it's impossible for me to implement a lot of algorithms now. I'm also so happy if someone do that.
Does `cargo build --target nvptx64-nvidia-cuda.json" not work?
I think what you want is `u32::add`, and the trait that represents it it `Fn(u32, u32) -&gt; u32`.
I'm not certain there is a specific function, but it will be included as part of the wrapper around exported Rust functions. I believe the build time `cargo-web` packager inserts code to trigger rebuilding the views into rust memory into the rust code's allocator as well. I'm not 100% sure it would be the best thing for your work, but it could help. If you were using `stdweb`, you would want to use its exporting interface completely and trust it to do the interop. Then you could pass something like [`stdweb::UnsafeTypedArray`](https://docs.rs/stdweb/*/stdweb/struct.UnsafeTypedArray.html#method.new) to JS I think.
Stabilizing the `intrinsics` module as a whole is probably not gonna happen, but we do sometimes re-export a stable wrapper for individual intrinsics for example in `std::mem` or now in `std::hint`. Consider writing an RFC (or maybe start with a pre-RFC on internals) that describes your use case and how it could be solved.
Huh, yeah that makes sense actually. Thanks for the correction!
I am also using gutenberg and it is pretty amazing how easy it is to use. But I always wanted to switch to hugo for one reason. I write a lot of code in my blog posts, and I do want to write the code once in a separate cargo project, and not in the markdown itself. Is it currently possible to include extern files? It would be nice if I could do something like {{ playpen(path="fizzbuzz.rs") }} And then get a codeblock, a playpen run button, and a playpen output. 
Are there other static site generators with that name? Gutenberg sounds like a pretty good option, all things considered. Websites are modern day printing presses.
Why should it give us pause? An experimental web framework got an audit, was found to contain UB and got fixed in two weeks. Given how relatively small the Rust community is and how quick the numerous issues were fixed, I'd say it's a testament to Rust's success.
gutenberg sounds like a good name for what it does, but its too famous, too likely to collide. You can see it already happened.
Looks like I skimmed through too quickly. Good point about the name being "too famous", I suppose that increases the chance of name collisions later on
Cargo.toml and Cargo.lock reference version 0.19.0. I finally updated the code of the .get method to make it compliant with 0.18.0 signature and it finally works
Hey I'm the one who made this repo, may I ask what a Rustmander is? :D
It's a portmanteau of Rust and Charmander in a playful mashup. Got to have a little fun once in a while!
Ohh I see it now :D
it is supported (in inherent impls only, not in traits), but only if the 2018 edition flag is set. only standalone async fns are allowed in rust 2015. https://play.rust-lang.org/?gist=0a4cedfff4d7df2260850d80a0adfcbc&amp;version=nightly&amp;mode=debug&amp;edition=2018
Then what are you doing on the subreddit? 
The Rust Cookbook is working on a science section. That would be lovely if we can get your input: [https://github.com/rust-lang-nursery/rust-cookbook/issues/362](https://github.com/rust-lang-nursery/rust-cookbook/issues/362)
It currently cannot load files from a template but adding a global fn to do so seems trivial. &gt; And then get a codeblock, a playpen run button, and a playpen output. Would that be an iframe?
I am not a webdev at all, I hacked something together. https://github.com/MaikKlein/maikklein.github.io/blob/source/templates/shortcodes/playpen.html https://github.com/MaikKlein/maikklein.github.io/blob/source/static/site.js But I still have to copy/paste the source code from the file into the markdown manually.
Wow, I was thinking I need some shopping list app few days ago. Awesome!
One thing I really need [for the Pikelet specification](https://github.com/pikelet-lang/pikelet/issues/109) is math support. Are there any plans on improving/adding this in the future? I also would need the ability to add a custom language for syntax highlighting (currently I use a fork of highlight.js with mdbook). Would this be possible too?
you could take a look at the [sozu HTTP reverse proxy](https://github.com/sozu-proxy/sozu). Performance is a first class concern for a load balancer. You won't find much concurrency work though, as it's heavily single threaded
👌 thanks!
If the playground has an endpoint like that then I guess the issue is only loading the content from a file, which shouldn't be too hard. We'll also need to add a `highlight` filter since we currently don't have one (https://github.com/Keats/gutenberg/issues/226). Can you open an issue to track that?
Creator here, thanks for the post! It is probably worth mentioning that the project is in really early stages, all you can do really is walk around the world (for now). 
shit wrong subreddit 
You're posting in the wrong subreddit. This subreddit is about a programming language with the same name. Take a look at /r/playrust 
&gt; Destructors with a runtime GC can be deferred until the GC decides to enact cleanup of the stale object. This can be dangerous. Yes, RAII does not work well with GCs. Whenever I see a `try/finally` to close a file or socket I cringe :x
I had missed it :/ Thanks for confirming!
Deadlocks aren't data races. Yes, they make program hang up, and it's bad - but it's entirely different category of bad than data corruption.
We would appreciate any help with this project! (I'm one of the contributors)
If you try to compile the code, the compiler gives you a hint :) error[E0507]: cannot move out of borrowed content --&gt; src/main.rs:9:11 | 9 | match *val { | ^^^^ cannot move out of borrowed content 10 | MyEnum::A(string) =&gt; string.as_str(), | ------ hint: to prevent move, use `ref string` or `ref mut string` 11 | MyEnum::B(string) =&gt; string.as_str(), | ------ ...and here (use `ref string` or `ref mut string`) What it tells you is that it cannot move val, because it is a reference. So let's do what the compiler suggests: take a reference instead. Solution: http://play.integer32.com/?gist=5e0ac5fbe99a1d186a9af0b2434a8a50&amp;version=stable&amp;mode=debug&amp;edition=2015 
Insert `/*` before `enum MyEnum` and `*/` before `fn main() { }`. Alternatively, change the match line to `match val` instead of `match *val` to use some relatively recent match magic. Alternatively, and I assume this is the solution they have in mind, insert the keyword `ref` in both matchings, e.g. `MyEnum::A(string)` =&gt; `MyEnum::A(ref string)`.
Thanks for taking a look at it! Are there any things unclear to you about the library after reading the readme/docs?
Clever! Wouldn't that change the definition though?
I’m pretty sure that the definition that shouldn’t be changed is only the enum which would only be violated by the commenting solution.
Fair enough, we could rewrite `fn matcher` to the following to also strictly meet the restrictions of changing two lines and getting it to compile :p fn matcher(val: &amp;MyEnum) -&gt; &amp;str { /*match *val { MyEnum::A(string) =&gt; string.as_str(), MyEnum::B(string) =&gt; string.as_str() }*/"now it works" } 
Seems people are downvoting you for this, but I agree. The code could have looked a lot like the following, and then there would have been no allocation, no virtual dispatch, etc. But it seems the ion shell is beyond criticism :p let action = { let f: fn(f64, f64) -&gt; f64 = match operator { Operator::Add =&gt; std::ops::Add::add, Operator::Divide =&gt; std::ops::Div::div, Operator::Subtract =&gt; std::ops::Sub::sub, Operator::Multiply =&gt; std::ops::Mul::mul, _ =&gt; /* Current error message + return */ panic!("noooo") }; move|src|f(src, value) };
This *does* seem eerily familiar to the various pass-ordering issues Rust has - LLVM missing seemingly obvious optimizations because it's not running the optimization pass that would perform it at the right point in the compilation pipeline.
I am working on a molecular simulation package: [lumol](https://github.com/lumol-org/lumol). This should tick all your demands: run time performance is really important (simulations can run for weeks on supercomputers, and even a 10% improvement is really worth it), the math is relatively simple, and you can visualize the atoms moving once the simulation is finished. We already started working on shared-memory parallelism, but there are many other area that could see improvements. I also have some ideas to try to work with distributed computing in Rust. If you are interested, ping me here or by email ! 
Pass ordering is a dark art. With more than a hundred passes, each taking advantage of the others work to various degree, figuring out which sequence^1 is optimal it tough, and varies depending on the program being compiled :/ ^1 *Not just order, some passes are executed multiple times!*
The compile-time impact of "let the compiler deal with it" worries me. Wouldn't it be better to just specialize the code for non-dropping types?
I’d always ignorantly assumed it ran a series of passes until it ‘converged’, i.e., until `x = f(x)`.
Interesting idea! After working on a Redux+React project I always wondered what a typesafe implementation would look like or what benefits it could have. I really like the `Merge` trait to let the user decide what to combine. But the merging logic could get quite complex. Maybe a new variant of `MergeResult` could be useful, which allows the user to rewrite two actions into two new actions. This could be used in the readme example to rewrite `DecrementBy` actions as `IncrementBy`actions with a negated value and then combine only the `IncrementBy`actions. But I don't know how much this would complicate the `compress` logic. `Chain` could also track if the current set of actions has been compressed. In the current API it is possible to simply forget to compress a chain before adding it to a `Timeline`. It could be useful to expose iterators from `Timeline` to get a convenient view of the sequence of states. Maybe this could require `Clone` for the state as the `Timeline` holds only one instance of the state.
My compiler experience consists of once making a simple expression evaluator, like (+ (- 5 3) X) for some many number of X, and then realizing, hey (- 5 3) is just a constant, I can go through the tree find all the constant expressions and replace them with a constant. but then you have NEW constant expressions. I'm sure there is some algorithm for this but I just kept running my "find constants" pass until it didn't find any new ones!
You should be able to reduce all constant expressions in a single pass no? If you have a recursive "reduce" algorithm, for example: fn reduce(node: Node) -&gt; Node { let left = reduce(node.left); let right = reduce(node.right); if is_constant(left) &amp;&amp; is_constant(right) { do_reduce(node.op, left, right) } else { Node { op: node.op, left, right } } } Unless other passes were also creating constant expressions? *(Note: and of course, there's also the `x - x` case which is fairly interesting for integers, and not as simple for floating points)*
See also [this thread](https://news.ycombinator.com/item?id=17595468) and Walter Bright's post about the performance of the D compiler. It's quite possible that a more monolithic compiler could be faster.
&gt; why does read_to_end() require a Vec? A heap-allocated array is way slower than a stack-allocated array. Stack array's size must be known at the time of compilaction, so it's impossible in this case. &gt; Is it OK to use a BufReader to read the whole file? Also, should I use a BufWriter to print everything to the standard output? Usually, using BufReader is a good idea when you have small reads/writes. But in your case, you want to copy as big chunk at once as possible, so BufReader is not necessary. (Actually, if you're doing bigger-than-buffer operations with a BufReader/Writer, it bypasses the buffer and uses the underlying Reader/Writer directly) Also, the most idiomatic way to copy from a reader to writer is [`io::copy`](https://static.rust-lang.org/doc/master/std/io/fn.copy.html): io::copy(&amp;mut file, &amp;mut stdout)?; Internally, it uses a reasonably-sized buffer. And one more thing about stdout – by default it's mutex protected and line-buffered, if you're writing huge amount of data, use the `lock` method: let stdout = io::stdout(); let stdout = stdout.lock(); Bonus: `StdoutLock` implements `BufRead` trait so it behaves just like a `BufReader`.
More reliable and easier to write probably not, since you'd need to write, test and maintain two code paths instead of a single one.
Yes I am sure you are right, I just coded the thing on a lark and didn't think through how. And yeah I figured out a whole host of similar optimizations. I probably reinvented compiler optimization from gcc version 0.00.01 
&gt; And yeah I figured out a whole host of similar optimizations. I probably reinvented compiler optimization from gcc version 0.00.01 Isn't it fun, though :) ? 
It's easier to handle one concern twice than to juggle two at the same time. Writing `pub fn truncate(&amp;mut self, len: usize) { unsafe { self.set_len(0); })` is a lot easier than figuring out how to get `truncate` to optimize into that statement.
&gt; why does `read_to_end()` require a `Vec`? How will you read to the end if you have a fixed size buffer which is to small? The `Read` trait must be usable for things that don't have a length known in advance, e.g. stdin, a stream of data from the internet, etc. &gt; A heap-allocated array is way slower than a stack-allocated array. When you're performing IO you probably won't be able to measure the extra allocation. Also stack overflows would be a real danger when you have more than a few MB of data. &gt; Is it OK to use a BufReader to read the whole file? I don't see why not :-) &gt; Also, should I use a BufWriter to print everything to the standard output? Sure. I suggest you do some profiling so you can find out which is best. I think the "official" `cat` changes strategy based on the type of input and output stream, and I *think* there are some system calls that allow you to bypass having to buffer the bytes in your application, but don't quote me on that :-)
Thanks for spotting that :) Actually, I've meant `StoutLock: BufWrite`
But it doesn't: https://doc.rust-lang.org/std/io/struct.StdoutLock.html.
Haha. `BufRead` exists for things that really need buffering, like reading to the end of the line. There wasn't any need for a `BufWrite` trait (though there is a `BufWriter` struct, of course).
&gt;Maybe a new variant of MergeResult could be useful, which allows the user to rewrite two actions into two new actions. This could be used in the readme example to rewrite DecrementBy actions as IncrementByactions with a negated value and then combine only the IncrementByactions. But I don't know how much this would complicate the compress logic. That is definitely possible (and I will rewrite the compress logic anyways). It could sometimes also be benificial for execution speed (swapping an expensive action with a cheaper one). &gt;But the merging logic could get quite complex. I agree. I have been experimenting a bit with different approaches for merging. I don't like that most merges have to be defined twice where the order does not matter (`DecrementBy` first and `IncrementBy` second and vice versa). Maybe I can change it somehow so that the API consumer can define those merges just once. The merging implementation is not much more than an idea right now, so suggestions are very welcome :) &gt;In the current API it is possible to simply forget to compress a chain before adding it to a Timeline. It is not always desirable to compress the chain though. I can imagine that users want to be able to undo specific actions which would be removed if compressed. Also, compressing is pretty expensive as of right now (copying actions). To me, compressing seems more like something you would want to do if you want to keep a difference-log of an application. For example, if you are writing an editor for a game-engine. Whenever the user saves, the current chain could be compressed an stored to the drive. It could then be used to show the differences between saves to the user (and even at the level of single actions required to get to the new state: "You moved this object", etc.). &gt; It could be useful to expose iterators from Timeline to get a convenient view of the sequence of states. Maybe this could require Clone for the state as the Timeline holds only one instance of the state. Good idea! That will be implemented! Thanks for taking the time to look at the library, your suggestions are really helpful!
There are only three optimizations: partial evaluation, instruction selection, and data layout optimization, and only two of these are the responsibility of the compiler.
I’ve been a .Net developer on windows for years but I completely switched to linux for amazing terminal/shell/vim goodies. I recommend you to try neovim with plugins (I’ll share my dotfiles on github if you are interested). As terminal, I use alacritty, zsh (with oh-my-zsh) as shell and tmux for terminal multiplexer. When I need to write c#, I switch to Rider and use it with vim plugin. When you get used to vim, you can’t do anything without it
&gt; Screenshpts In the readme.
Why is it that ECSes tend to be the best choice in Rust? Would love to hear more!
How does it compare performance wise to actix et al?
Traits are not interfaces, and `B: A` does not imply inheritance. Yes, implementing `B` requires that a type also implement `A`, but that doesn't mean `B` *includes* the functionality of `A`. Consider that traits can have cyclical requirements, so there's no strict hierarchy like there is in conventional single-inheritance OOP languages. This also means it's impossible in general for a trait's vtable to include every transitively required trait's methods. If you want to use `A`'s methods from a `&amp;B`, your best bet is to add a `fn as_a(&amp;self) -&gt; &amp;A { self }` method to `B`.
&gt; And why use any functions at all when you can just move the match into the loop? ``` let array = [ 1 2 3 4 5 6 7 8 9 10 ] let array += 5 echo @array 6 7 8 9 10 11 12 13 14 15 ``` It's applying the same action to every key / value pair that the operation, so that would require re-evaluating which operation to use in every
Have a look at https://github.com/rust-lang/rust/issues/32220 especially the first comment
that would be cool. As long as the arguments are numerals. Merge also has to take side effects into account. For example a certain operation might modify a seemingly unrelated field, and thus merging would not be equivalent to running the changes in order.
This is very nice. The refactoring looks very laborious but not very complicated. You don't even need to understand what's happening. Probably some parts could be automated (getting all the assessors, both ways etc...).
You can always use `split_at_mut`.
A deadlock is still "correct" in the sense that it is safe and will perform as expected. Logic bugs are easy to track down and resolve. What would be "incorrect" is passing a reference to a value to your children, and then dropping that value in the parent. Rust will prevent that from happening. Also of note is that it is "incorrect" to send values and references across threads which are not thread-safe. Passing a reference counter (`Rc`) instead of an atomic reference counter (`Arc`), for example. Rust automatically derives the `Send` + `Sync` traits for types which are safe to send + share across threads, which is based on all the types that make up a structure. If you have a raw pointer or `Rc` within that structure, it won't derive, and thus you'll be barred from using it in a threaded context.
Yup, the example in the book says so, and is essentially a reimplementation of `split_at_mut`
A goal of the library is to minimize/eliminate side effects by design. It forces the programmer to think about avoiding side effects, as you *must* return the inverse of each action. And I'm not entirely sure but I thiink that it is impossible to get an mutable reference across the "actions-border" in safe Rust (if using the Timeline). That means that it cannot affect anything it does not own. In the case you are describing the actions should have never been marked mergable :) (there are different MergeResults, you can find them in the docs) And I already wrote a test that compares the behaviour of merged actions versus the unmerged actions. That can be generalized so that users can test if the compressed version really produces the same output as the uncompressed output.
They’re “flat”, instead of in a hierarchy. Additionally, the data is split up into different chunks. Both of these things make the borrow checker much happier. You should watch the talk I reference downthread :)
That‘s likely still faster than the indirect function call. It would probably need to be measured but a predictable branch costs almost nothing.
I wonder if it's even allowed to do such optimization: ``` void test(unsigned long long* a) { if (*a != 0) { *a = 0; } // optmize to a 'simple' store } ``` I would say that it's not allowed since it's semantically different.
An implementation of the Fast Marching Method could be interesting, and in terms of math, perhaps at a similar level of difficulty as heat diffusion. The algorithm is quite useful for things like robotic path planning: https://pythonhosted.org/scikit-fmm/ https://en.wikipedia.org/wiki/Fast_marching_method http://cosc.ok.ubc.ca/__shared/assets/Fast_Marching_Methods_and_Level_Set_Methods_21116.pdf https://math.berkeley.edu/~sethian/ 
[Here](https://github.com/japaric/nvptx) is japaric's repo on NVPTX support. I believe `cargo-xbuild` is the one used nowadays, as described in phil-opp's article [here](https://os.phil-opp.com/minimal-rust-kernel/)
Maybe, `fn update(&amp;mut self, action: &amp;Action) -&gt; Result&lt;()&gt;`?
I would not put more than necessary inside of an unsafe block: pub fn truncate(&amp;mut self, len: usize) { let current_len = self.len; let mut ptr = unsafe { self.as_mut_ptr().offset(self.len as isize) }; let mut local_len = SetLenOnDrop::new(&amp;mut self.len); // drop any extra elements for _ in len..current_len { local_len.decrement_len(1); unsafe { ptr = ptr.offset(-1); ptr::drop_in_place(ptr); } } } This is all unsafe is needed. Unsafe code does silently overflow integers right now (even in debug mode), which has led to bugs in my experience when people start to suddenly include mathematics inside of the unsafe code. I.e. let a: usize = 3 - 2; // panics in debug mode, as it should but: unsafe { let a: usize = 3 - 2; // 18446744073709551614 } This is why I only use unsafe blocks at the point of calling the unsafe function. In this example it doesn't matter, but especially with calculating length for vectors, you can't be too careful (i.e. the `decrement_len` function could silently overflow if it wasn't inside of the loop). Rather than asking "how many times is unsafe in this library" I'd rather ask "how many lines are inside an unsafe block"? In my example, I have 2 unsafe markers, but only 3 lines of unsafe code. He has one unsafe marker, but 6 lines of unsafe code.
You're right, I wonder why that is, since it's not declared as `volatile`. You could, of course, construct some cases where the optimization would change the behaviour (make `a` point to a read-only page), but..
I have the following bit of code (please let me know if you need more context): impl QueueFamilyIds { fn new(adapter: &amp;mut hal::Adapter&lt;hal::Backend&gt;) -&gt; Self { ... The error I get at this line is: fn new(adapter: &amp;mut hal::Adapter&lt;hal::Backend&gt;) -&gt; Self { ^^^^^^^^^^^^ the trait `hal::Backend` cannot be made into an object `QueueFamilyIds` is a struct. `hal::Adapter` is a struct defined like this: pub struct Adapter&lt;B: Backend&gt; { ... where `Backend` is `hal::Backend`, which is imported for use, so just put as `Backend`. What is the error telling me?
This is awesome! The tooling seems to have come so far from when I last used it! I'll give it a go again. &lt;3 rust-numpy.
Rust heavily relies on the "let the optimizer deal with it". And yes that's partially the cause for Rust's bad compile times.
Where exactly can one find the "fixme" problems in the resources for the course? I can't seem to hit upon them.
What you did is running until the earliest fixpoint, which is absolutely fine and is how you solve a lot of problems when you can prove that you'll reach a fix point :)
You'd think so, but the following *will* include the comparison in the assembly. if (*a != 0) { *a = 0; } else { *a = *a; } I guess it has something to do with the order of the optimization passes, optimizing out all instances of `*&lt;thing&gt; = *&lt;thing&gt;`, before other passes get a chance to take a look. If the optimizer were to somehow preserve that `*a` gets assigned on all code paths, then it might be smarter.
it is in the git repo linked on this site [https://web.stanford.edu/class/cs140e/assignments/1-shell/](https://web.stanford.edu/class/cs140e/assignments/1-shell/)
[1] https://www.youtube.com/watch?v=VGk95NXaafs Could someone comment on why this tutorial[1] uses $id1 and $id2 in the macro definition? Why do we need the part `$id1: ident | $id2: ident &lt;-`? As far as I understood, the author mentions it's a list comprehension similar to python. In python, you have `[x for x in range(start, end+1) if even(x)]` but here the author doesn't do anything with x in the macro. Moreover, when I play with the example and try to do something with x in the macro, I get an error. Thanks! macro_rules! compr { ($id1: ident | $id2: ident &lt;- [$start: expr; $end: expr], $cond: expr) =&gt; { { let mut vec = Vec::new(); for num in $start..$end + 1 { if $cond(num) { vec.push(num); } } vec } }; } fn even(x: i32) -&gt; bool { x%2 == 0 } fn main() { let mut result = compr![x | x &lt;- [1;10], even]; println!("{:?}", result); result = compr![y | y &lt;- [1;10], even]; println!("{:?}", result); }
That's interesting, there's a few ideas in there that it looks like I should use, but I'm still at the stage where I don't have a good feel for what is a good idea and what is not. I had a go at adding in a new automata type to my project, and it was nowhere near as smooth as I had hoped. I had to refactor a few things, I now have methods in the public interface that I didn't really want to make public, and there is a good bit of duplication in my constructors that I assume could be tidied up.
I don't think the overflow thing is true. It panics in debug mode for me. In general, `unsafe` doesn't change the behaviour of code, it just allows additional code to compile (e.g. calls to `unsafe fn`s).
FYI the title says `B: A` whereas your example says `A: B`, so it's a bit confusing. Is this separate notation or just a typo? Unlike in other languages, Rust traits aren't just interfaces that describe abstract APIs and data types, they're also contracts that specify how borrow semantics, ownership, and lifetimes (like if the trait is parametric over a lifetime) apply to a type. When Rust converts a sized type to a trait object, all of that extra type and `where` clause information is lost. Although you can call `B`'s functions from `&amp;dyn A`'s vtable, Rust knows absolutely nothing about `&amp;dyn A`'s `B` implementation other than some basic type information for the vtable signatures. It would be extremely unsound to convert `&amp;dyn A` to `&amp;dyn B` without all of that extra type information, much of which is lost by definition when you use trait objects. If you're not afraid of a little type level programming, check out [frunk](https://github.com/lloydmeta/frunk). If you know all of the trait types (i.e. this is application code, not generic library code going out to tons of consumers), you can use a coproduct (generic enum) as inputs to your functions and return errors when you find invalid types. Essentially, you write all of your functions as generics that take in a `type Interfaces = Coprod!(&amp;'a dyn Trait1, &amp;'a dyn Trait2, &amp;'a dyn Trait3, ...)` and map over them using a frunk macro, returning errors or panicking when they receive an invalid type. Since `type Interfaces` is a concrete type, you can have a base trait called `Recast` with a function `recast(&amp;self) -&gt; Result&lt;Interfaces, Error&gt;` or something that converts a `&amp;'a dyn Trait[x]` to the coprod. The inside of a trait implementation knows the type it's implementing for so it should be able to return a the Coprod trait object. You could make it a lot more ergonomic by write a simple macro that creates the list of mapping functions for all of your traits except for the ones you override via macro inputs.
Traits with associated constants (and maybe types as well) can't be made into an object. so trait Something {} can be made into a trait object and trait WithConst { const X: usize = 0; } cannot.
&gt;FnMut(&amp;T, &amp;T) -&gt; bool. I'm not sure what you actually want to achieve. Yeah, I mean to pass any operator, but was unable to edit the title. So I have one branch for boolean operators and other for math. Then maybe looking how collapse all into a single op...
Happy to have inspired you! And for everyone else: Many improvements don't even require this level of sophistication.
I am just planning to have a small server for my friends and I. Maybe like 10 people. Also, what is stack? Thanks
So you are talking about the game Rust. Read the part in parentheses then :)
You can get more generic and make it work over `FnMut(&amp;T, &amp;T) -&gt; U`, then you can pass `PartialEq::eq`, `PartialOrd::cmp`, `PartialOrd::lt`, etc... and all the operator ones eg `ops::Add::add`. They all have in common they take two arguments by reference and output a result (`T` by value for the math operators and `bool` for the comparisons).
Smart constructors are a clever way to deal rewrite ordering. The JVM uses this technique. For each expression type you define a function that constructs that expression but it first simplifies the expression as much as possible. Then instead of constructing expressions directly you always construct them via those functions. If a smart constructor does a rewrite then you construct the rewritten expression with smart constructors too. This ensures that your expressions are always fully simplified. You can then be sure that the expressions passed to the smart constructors are already in simplified form, so smart constructors don't need to do any recursive simplification. They only need to apply rewrite rules that directly involve the expression that is being constructed.
To further explain, this subreddit about the Rust programming language, which can be used to write servers. /r/playrust is for the game. Aside: You may want to consider another provider such as Digital Ocean or Vultr. These are often significantly cheaper than AWS.
I wouldn't call this sophistication, maybe curiosity and a bit of persistence.
You could also add another `&amp;` to the argument type.
In the elm style pattern I have been using I use traits to handle different categories of message/action. This helps separate the code somewhat. I don’t find a massive need to improve efficiently by merging actions/messages. At most I send an array of messages to be processed as a batch on a fail I rewind based on the undo messages. I like the pattern but it can be verbose.
&gt;This is… a… placeholder crate – i.e. it doesn’t have anything exposed. People do this to book a crate name for later. I highly dislike that and people doing this should really be warned not to do this, because they prevent others – who actually have some code to put there – from using the name. Looking at the last commit for [spline](https://github.com/hilbert-space/spline/commit/244cf2c7ee5addc006f487eb1c64fabf29d4e34e) it looks like the author just deprecated the crate by nuking the source. Which is shortsighted, can you file an issue with crates.io to request them to delete the crate? 
Thank you for giving me a name for this. I ended up figuring this pattern out because you're right, it really does simplify quite a bit. I used it for [regex's HIR](https://docs.rs/regex-syntax/0.6.2/regex_syntax/hir/struct.Hir.html). Note that you can _look_ at the [syntax tree](https://docs.rs/regex-syntax/0.6.2/regex_syntax/hir/enum.HirKind.html), but there is no public way to build an `Hir` directly from an `HirKind`. Instead, you must rebuild it using one of the "smart" constructors. (I've only begun scratching the surface here... Still so much to do.)
Thanks for checking the library out! &gt; I don’t find a massive need to improve efficiently by merging actions/messages. [...] I like the pattern but it can be verbose. I agree that merging is a lot of the times unnecessary (It is probably most of the time cheaper to just execute the actions than to merge them before executing :) ). `Merge` is an optional trait though and I believe it is useful in a couple specific cases. This is what I said in a comment above: &gt; To me, compressing seems more like something you would want to do if you want to keep a difference-log of an application. For example, if you are writing an editor for a game-engine. Whenever the user saves, the current chain could be compressed an stored to the drive. It could then be used to show the differences between saves to the user (and even at the level of single actions required to get to the new state: "You moved this object", etc.). &gt; At most I send an array of messages to be processed as a batch on a fail I rewind based on the undo messages. That is a pretty nice use-case I did not think about! Cool 
Thinking a bit more about it. I guess the merging aspect could also be helpful when undo'ing. I.e. if I shift an on screen object using the arrow keys 23 pixels to the left, and then want to "undo", I am more likely wanting to undo all 23 shifts to the left rather than each individual shift. Hence undoing a merged action (of the 23 actions) would also make more sense.
Sadly, I won't be able to attend, but thumbs up for your initiative!
You can see the Lanton's Ant example [here](https://github.com/evomata/gridsim-ui/blob/master/examples/langtons_ant.rs).
The author of `spline` is not at fault here, it's just the system working as intended. The best course of action would probably be to contact the author to transfer the ownership.
I wonder why people would downmod this. What is wrong with building a good XSLT engine? Last time I checked (a couple of months ago) Rust was still somewhat lacking in XML support. XSLT is a very interesting language and, despite being rather verbose, really nice when you have to transform some XML to HTML/XML. Once you have XSLT (and thus XPath, by necessity) it wouldn't be such a big step to also do XQuery.
You are right! That is an essential part that definitely should be included in the library. But I'm not sure if that should be the same trait though. It feels for me like there is quite a big difference between 'grouping' certain actions or 'compressing' a lot of actions. Maybe dynamically merging actions inside the `Timeline` would be a good idea. But more as in the concept of 'grouping' actions. So the API consumer could specify: I want to group these actions if they appear sequentially: "moving an object to the left".
AFAIK crates.io will not delete a crate for something like this situation. Actually the only crate I know to have been deleted is [NUL](https://crates.io/crates/nul) for [breaking shit on Windows](https://www.reddit.com/r/rust/comments/68hemz/i_think_a_crate_called_nul_is_causing_errors_for/). In this case the only possible way is to ask if the original author would like to transfer ownership of the crate on crates.io as you wrote.
I'm not completely against renaming it but it is already in the majority of package managers under that name now so that would be a big hassle.
But the .iter() method even on a Vec of i32s will give you an iterator that returns &amp;i32 so you still have the same problem. The answer is that the AddAssign trait (the += operator) is implemented for both i32 and &amp;i32
2. Smart pointers carry a runtime cost and often an allocation with them. Many times you do not want to have to create a smart pointer for many items and instead you can use lifetime specifiers to satisfy the compiler that you’re not breaking any safety rules, and then your reference compiles down to just a raw pointer.
Trying to bend the borrow checker to your will to implement data or logic structures you are familiar with from other languages. Often that is a sign that concept just doesn’t translate well to Rust (because it may have some fundamental safety flaw that just went unnoticed in other languages). 
gfx-rs pre-ll 
it is an impractically large undertaking probably
I suppose it is kind of a lot for a 'learn Rust' project. Sadly an XPath parser (without actually executing it) doesn't really qualify as a 'performance intensive' project.
Paper and Cone author here. Just ran across your post. Thanks for sharing it. I always welcome feedback or questions.
Btw, do you think frunk's Coproduct could also be used to solve [this issue](https://github.com/DenisKolodin/yew/issues/350), similar to how Coproducts are used in [PureScript/Halogen](https://github.com/slamdata/purescript-halogen/blob/5534d1dcc0e691b56691e897f3d4a05d4d4f1fec/docs/5%20-%20Parent%20and%20child%20components.md#multiple-types-of-child-component)?
Solving the problem with half the budget should get extra points..
&gt;If you choose not to use it, it might still give you some inspiration. It's not that I don't want to use it, it's more that my goal was to practice writing Rust code, not really to prototype automata. When I say there are ideas I should use, I don't mean specifically for this project, but in general.
What's interesting is that some passes *do* run until a fix point internally. The main peephole pass (InstSimplify I think) keeps going until it runs out of simplifications it can make. The pass itself is run multiple times though. 
&gt; I'm not seeing what baking it all into the language actually accomplishes. I agree with you: that would be a poor choice. Fortunately that's not what I am doing with Cone. The various memory managers (allocators) will indeed be distinct and selectable libraries/packages. But: some mechanisms **do** need to be baked into the language in order for libraries to leverage certain capabilities (which is true for Rust too). For example, accurate garbage collection requires tracing maps, safe points, and auto-generation of write barriers. Similarly, static and runtime permissions require underlying mechanisms built into the compiler's type system for enforcement. Rust is still missing some of these mechanisms, and has no near-term plan (that I know of) to close these gaps.
I think you might want something like this: ``` fn foo&lt;T: Shr&lt;u8, Output=T&gt;&gt;(a: T, b: u8) -&gt; T { a &gt;&gt; b } ```
"rah-eeeeeeeeeeeeeeeeeeeee"
You are right, I believe that the two identifiers are not being used. There is also a more elegant way to write the same thing: ``` fn even(x: &amp;i32) -&gt; bool { *x % 2 == 0 } fn main() { // Note: collect not necessary, it returns an Iterator let result = 1..=10.filter(even).collect::&lt;Vec&lt;_&gt;&gt;(); println!("{:?}", result); } ```
At this point, [termion](https://docs.rs/termion) is pretty stable, and doesn't seem to be changing, so you could write a relatively stable `ranger` in Rust with that. As for file listings, etc. that is totally stable and set in stone. However, the wrappers around `termion` are still plentiful and not stable. Consider using `cursive`, which can fall back directly to `termion` but still use `curses` if necessary
Awesome, thank you!
Awesome, thank you!
Nice, this should work for writing a computer algebra system as well. (Except that it is probably necessary to still keep dumb constructors.)
if you write scripts to port hugo themes to gutenberg, I'd give gutenberg a shot
Posted twice
Why is this? It seems kind of weird, considering that the constant will stay the same type.
&gt; Why can't I cast a &amp;dyn A into a &amp;dyn B when B: A? As far as I am aware there are no good reasons other than "it's harder to implement". It causes some really bizarre situations and most people are unaware of it. For instance actix-web has a subtrait from `Fail` but because of that, none of the useful methods on fail work. As a result it has this awful workaround in the code: https://github.com/actix/actix-web/blob/f4fba5f481e7147ee8910757cf3b33b68cb04e70/src/error.rs#L107-L127
Possibly. It's hard to answer that question because working with heterogeneous types like hlist and coprod is full of limitations - at least until Chalk lands in Rust nightly. Without a lot of experience with both type level programming in Rust and your codebase, I can't give a more satisfactory answer. You can try implementing `Handles&lt;M&gt; for State&lt;T&gt; where T: Handles&lt;Coprod&lt;H, T&gt;&gt;, Coprod&lt;H, T&gt;: Inject&lt;M&gt; // I don't remember the actual trait name from frunk`. Then, using specialization in nightly (`default` trait implementations), create a mapper over an hlist of your types and their children that recursively applies the message to each component in the chain - since most of them will be noops and inlined away (experiment with #[inline(always)]), you'll get really nice zero cost abstraction updates (except for when you have dynamic children hidden by trait objects, i.e. situations when you need to provide a `key` prop in React, which would required Vec/Hashmap/other collections). You can either have a marker trait for each child that has an associated type Coprod of all the messages it takes or if you're using a closure + builder pattern like I am, you just build up a master Coprod that just sums up all the children's coprods. The closure+builder pattern is nice because it allows the builder struct to track the relationship of messages and make sure that the parent component can handle all of the messages its children expect. It also allows the builder to generate its own closures for dispatching events directly to children with a captured index instead of using specialization to iterate over the hlist of state and dispatching everywhere (which, although no-oped in release builds, can increase compile times significantly). Note, however, this just makes it easier to work with many different nested types. You'll still need to come up with a way of checking whether a given child is supposed to receive an update. I'm trying to do it with type level indices but you can do it with integer or string IDs or whatever. I've uploaded a chunk of code [here](https://github.com/akiselev/vtable-rs) for a `rustc` ICE bug report and you can sort of see what I mean by closure-builder pattern in [`core/lib.rs`](https://github.com/akiselev/vtable-rs/blob/master/core/src/lib.rs) in the commented out test at the bottom. This is incomplete because it doesn't yet store the message types in a Coprod but it'll give you an idea of what it's like working with frunk in general. If you remove the offending `testfunky()` function that causes the compiler crash, you can uncomment and run the test. The turbofish syntax is a requirement because I haven't switched to type level indexes for referring back to the structs stored in the builder. If you want a type-level seizure, check out how it works in `core/builder.rs`. You can see some of the fold implementations in `core/folds.rs`, which is what you would use to fold over the parent hlist of children in order to dispatch the message.
Some Rust practitioners are weird. You have all those great approaches about correctness, and then you see here and there developers trying random explicitly unsafe hacks depending on internals of data types, that not even C++ developers would try. I mean, when was the last time when you randomly tried to nuke the internal length of an C++ std::vector to speed things up, using simultaneously dangerous means and a complex reasoning (but probably undocumented, and maybe incorrect) justifying that there actually was no risk in your opinion? And on top of that do it in, not in some kind of insane computation kernel trying to grab the last bit of perf, but in merely an HTTP library? Well at least I'm happy this particular missed optim has been fixed. One less reason to resort to insane unsafe hacks. (On a sidenote, it is *sometimes* better to only modify a variable in memory if it doesn't already has the good value, but this would be mostly for scaling concurrent accesses of the same cacheline on multiple CPU, so this does not really apply here) 
You're looking for /r/playrust. This subreddit is dedicated to the Rust programming language.
I've been writing tests against well-known allocating types to verify the correctness of the results as well as the overhead incurred by running with the instrumentation enabled. 
Oh thankyou
Oh, I sorry. I just meant that some of original developers of LMAX decided that it was basically a broken idea and went on to do Aeron instead. (It wasn't for or against you position, per se. I was just saying the the "LMAX guys" you mentioned may not be the same "LMAX guys" that people may associate with the project.)
I think it's a matter of guarantees. `Vec::set_len` is unsafe, yes, but it's also exactly documented what it does, and what the requirements for safety are. It's also deliberately public for this kind of use case. Safe Rust is about making it that you don't have to worry about this kind of unsafety in the common case. `unsafe` Rust is about letting you have C-level control over your code if desired.
I get it, however I think that this is a strong departure from the classical design approach where methods exist with the intent of enforcing invariants of their classes, to a point that even languages intrinsically unsafe everywhere do not even propose that kind of operations (the ambient unsafety they have is usually way more subtle than having random lets-blindly-change-this-attribute-to-arbitrary-values-despite-it-being-part-of-crucial-invariants methods) So yes, I was surprised to discover that set_len even exists, and I still don't really know what it is good for? (short of micro-optims of the nature described here, which you understand I'm not a fan of...)
As I understand, the pluralization is part of translations file. So it is impossible to change translation without update application binary with intl_pluralrules? What about format, is it possible to use some other formats, like qt translation files?
&gt; I am also excited as I wait for an Asus Tinkerboard That's 32-bit though… &gt; U-Boot as an EFI payload hm, interesting. For FreeBSD, we typically use U-Boot as the EFI *implementation*, i.e. U-Boot loads our `loader.efi`. Already having UEFI out of the box is considered better. I wonder if it's possible to make an infinite loop of U-Boot loading itself as an EFI payload :D
There's an algorithm that does constant folding and dead code elimination at the same time that is better than any finite sequence of either. https://en.wikipedia.org/wiki/Sparse_conditional_constant_propagation
Write a simple path-tracer that can handle large chunks of geo and runs on multiple threads. You'll need thread and instruction level parallelism to get good performance and there is enough documentation out there to get you started. Plus the math is relatively simply and the result can't get more graphic than this.
&gt; And on top of that do it in, not in some kind of insane computation kernel trying to grab the last bit of perf, but in merely an HTTP library? That's what kills it IMO. It's not just a dangerous game - it's pointless as well. :(
&gt; Consider that traits can have cyclical requirements *Whaaaaaaaaaaaaaat* I guess traits involved in a cycle must all be identical, since anything added to one trait in the cycle must be supported by all of them?
&gt; Consider that traits can have cyclical requirements I went to give an example of this, but the compiler refused to build it. I'm *convinced* I demonstrated this the last time I remember this coming up, but I can't find that post or the example. So now I'm not sure whether I managed to find an edge case that did work, or if I'm misremembering.
Very useful library. Thanks! 
You probably want /r/playrust.
I've found something that's sort of a circular dependency and which the compiler accepts, but using a where clause, instead of a subtrait. trait A: B {} trait B {} impl A for Foo {} impl B for Foo where Self: A {} And the compiler decides that `Foo` now implements both `A` and `B`. The opposite would be just as logically acceptable.
I feel like many of these "puzzles" are mostly trivial because the compiler is so helpful. I used to feel like Rust was super intimidating, but now the compiler teaches me thinks I didn't know about the language. For example, I was thinking that there would be a lifetime issue, but the `&amp;str`s come from the same enum (thus they have the same lifetime), so it's safe if you return a reference. rustc is just too helpful for things like this :)
Thanks for adding an incorrect comment.
Maybe a playground of discover ability. People can learn the tools without reading the docs. Or arrogance.
I make it work if decode by each operator, but can't make it fully generic. This is my try: /// Comparing 2 columns fn _compare_both&lt;'r, 's, T, F&gt;(left:&amp;'r [T], right:&amp;'s [T], mut apply:F) -&gt; Column where T: PartialEq, F: FnMut(&amp;'r T, &amp;'s T) -&gt; bool { let x:Vec&lt;bool&gt; = left.into_iter() .zip(right.into_iter()) .map( |(x, y)| apply(x, y)) .collect(); Column::from(x) } fn decode_both&lt;T, F&gt;(left:&amp;Column, right: &amp;Column, apply:F) -&gt; Column where F: FnMut(&amp;T, &amp;T) -&gt; bool { match (left, right) { (Column::I64(lhs), Column::I64(rhs)) =&gt; { _compare_both(lhs.as_slice(), rhs.as_slice(), apply) } (Column::UTF8(lhs), Column::UTF8(rhs)) =&gt; { _compare_both(lhs.as_slice(), rhs.as_slice(), apply) } (Column::BOOL(lhs), Column::BOOL(rhs)) =&gt;{ _compare_both(lhs.as_slice(), rhs.as_slice(), apply) } (Column::ROW(lhs), Column::ROW(rhs)) =&gt;{ _compare_both(lhs.as_slice(), rhs.as_slice(), apply) } (x , y) =&gt; panic!(" Incompatible {:?} and {:?}", x, y) } } pub fn equal_both(left:&amp;Column, right:&amp;Column) -&gt; Column { decode_both(left, right, PartialEq::eq) } I get hit by: 52 | _compare_both(lhs.as_slice(), rhs.as_slice(), apply) | ^^^^^^^^^^^^^ the trait `std::ops::FnMut&lt;(&amp;i64, &amp;i64)&gt;` is not implemented for `F` and repeat by each type.
Thanks for the response. Can you elaborate on how Rust addresses the (returns Future&lt;X&gt;) composability problem. I think this is key for vNext language design - concurrency, but only when needed (not mandated by past sync/async descisions).
WANT.
Oh, I completely misunderstood the point =X. Glad there are more people interested in automatas in the community!
Can I get one for my bday too?
You might also be interested in using [cbindgen](https://github.com/eqrion/cbindgen). If it generates a header file that matches the one you are trying to implement then I'd say it worked. I also have made [a primitive template for combining the Conan package manager with cbindgen to automatically generate the header files and expose them to consumers](https://github.com/vadixidav/conan-cbindgen-test). It is not heavily tested, but it might help if you want to develop your library against test code in the test_package (its a folder with a C++ CMake consumer) to see if you are exposing all the symbols and the test code works properly for both your library and the one you are replacing (you can run a test against two different packages with `conan test test_package &lt;package ref&gt;`). This will be especially easy if the library you are replacing already has a Conan package since you won't have to do anything.
AWESOME. But, my kids would steal Ferris immediately. 👍
I'm working on \`webnis\`, a replacement for NIS/YP that uses https/rest/json instead of sunrpc. Right now I have nss client library, a pam client library, a connection-handling daemon on the client side, and a server that serves NIS maps and does authentication. All in rust. I had to patch gdbm-rust (patches accepted by maintainer) and pamsm (no pull request sent yet). I implemented tokio-tls support for hyper::server::Server. It's \*\*almost\*\* finished but that last polishing takes 80% of the time :)
A simple [Dice Bot](https://github.com/Lokathor/dice-bot-rs), just for rpg dice stuff.
&gt; I am going to read-up more on the data race safeties in Rust because I can't see how it can possibly work in all cases given specialized concurrency structures. Rust does this using a clever compile-time checking, using the [Send and Sync traits](https://doc.rust-lang.org/beta/nomicon/send-and-sync.html).
It takes a lot more sophistication than how I fixed trait impls for trait objects containing copies of marker traits being distinct on the number of copies. That was literally just sort and deduplicate.
Can we train a recognizer on post bodies and titles to figure out if they want /r/playrust or not?
I got a couple of the KS Ferrises and my daughter keeps trying to steal them.
I think this is phenomenal. Tangentially, I also think there's this big knowledge gap of where all the fun tutorials and books and stuff ends and all the necessary but not-well-communicated knowledge of toolchain management/creation begins. I've been trying to do my own exploration of bring Rust to fun new places and damn, there's a serious knowledge gap there for me. If anyone is exploring this space, I'd love to participate (/dogfood whatever you write, because I am but a beginner).
I discussed this with kennytm on IRC back on [May 20th](https://botbot.me/mozilla/rust-lang/2018-05-20/?msg=100246713&amp;page=1). Even said I was going to write an RFC to allow this, but I forgot to actually write it. Whoops.
Learning web dev in rust by making a REST api with Rocket for soccer teams/leagues/matches and statistics that I will eventually try to make an easy to use and intuitive client side for. 
What's interesting is that it doesn't depend on any Rust package (everything gets compiled statically). However it depends on.. libgcc1?
Continuing work on [elba](https://github.com/dcao/elba), a lil package manager for Idris. I've finished up all the basic functionality (building, installing, using the repl with dependencies), so now I just have to make sure everything works correctly with tests and the like.
I wish Rust had a online free book with the "Start programming" philosophy, like the [doc.rust-lang.org/book/](https://doc.rust-lang.org/book/) but much easier and more explained on what are we doing, maybe making one big project step by step or like [https://automatetheboringstuff.com/](https://automatetheboringstuff.com/) is :)
I'm working on an pretty basic JVM to learn how these things work and to play around with rust in a "bigger" project. Classfile parsing works pretty well using \`nom\`, currently working on a simple bytecode-interpreter. 
In the next couple of days I will release [Thumbcloud](https://github.com/flofriday/thumbcloud) 0.0.3. Thumbcloud is a server application to share files on your local network (built with actix-web). After that I will write a reddit post about Thumbcloud and my experience with rust.
I believe they were more commenting on the dependency for libgcc1, which does seem odd to me, personally. They mentioned the static compilation of rust binaries.
Not sure if I would like to introduce this tool for my family based on the name. :D
libgcc1 is a common dependency, for example chromium and emacs depend on it (and 5000 more packages).
Not quite sure what you mean. `include_bytes!` yields a `&amp;'static [u8; N]` so you always end up with a reference to the data segment. The only way you could get an owned instance is by explicitely copying the data onto the stack.
Hey! I'm doing my senior project in rust right now. It's also my 'first *real*' rust project. 
pluralization is part of internationalization. It allows string variants to be selected. This crate can be used for localization and for internationalization APIs. In case you're asking about, a localization crate such as Fluent will load translation resources at startup, so it is possible to change translations without updating application binary. What is bundled inside `intl_pluralrules` is just the logic to resolve plural rules for each locale. I'm not familiar with the QT localization API, but if it can hook into Unicode CLDR Plural Rules model, then one could write a QT l10n API in Rust now and use the `intl_pluralrules` crate.
So you are not able to reproduce the results of that code?
I don't know about the the '\1' write and reading, possible that that is a adtix thing. 4seconds is really slow/unuseable. I never had such slow performance. I tested it before a couple of commits and got allways around 20ms. How many files were in those big directories? The only reason I can rhink of right now is that the icen detection is slow. For that the server compares the file extension of every file against ~1350 extensions to detect if its an audio, video etc file. The code for that is in the files::categories module.
For Qt 5 translation file (Russian as in your example): ``` &lt;message numerus="yes"&gt; &lt;source&gt;%n Ошибка&lt;/source&gt; &lt;translation&gt; &lt;numerusform&gt;%n Ошибка&lt;/numerusform&gt; &lt;numerusform&gt;%n Ошибки&lt;/numerusform&gt; &lt;numerusform&gt;%n Ошибок&lt;/numerusform&gt; &lt;/translation&gt; &lt;/message&gt; ``` So in Qt has compiled in rules for Russian language, and translator only need add translation for three plural form. For other language there are different amount of plural forms. And there is no way to specify this is for `n == 1` and so on.
Suppose my method takes a `&amp;mut T` reference, is it undefined behavior to alias it with multiple `*mut T` raw pointers? I don't want to make my caller use an UnsafeCell as it would be leaky and cumbersome.
Code of C lib from GNU has many symbols to tell GCC how to compile. Those symbols are GCC-specific, not understood by other compilers (except those compilers are designed to be compatible with GCC), and when importing those symbol, the libgcc is needed.
It's a syphilist!
Wow. I should have used that name instead. But yeah, I'm native Finnish speaker (so pun intended).
Indeed the Tinkerboard is 32bit but we're trying to get there soon:) Indeed UEFI is better and I'm excited of the work Linaro did to bring it to HiKey960! My issue is that I am not sure how to port the version of Redox that is loaded through U-Boot to UEFI. So, as noted on SuSE's paper, I'm going through the known known steps :) If you have any tips though you're welcome to have a look at the source code and especially the aarch64 branch and offer your valuable comments!
I *think* it is okay so long as you do not use (read from / write to) those `*mut T` pointers in any way for as long as the `&amp;mut T` reference is alive.
Nominally, yes. However, [the glibc docs suggest](https://www.gnu.org/software/libc/manual/html_node/Adding-Platform_002dspecific.html#Adding-Platform_002dspecific) that for features that are platform-specific and not tied to an operating system (like `memset` and `memcpy`) that it's better to implement them in GCC so all software can benefit from them, rather than only software that links with glibc. Also, on some platforms I bet "memset" and "memcpy" are specialised CPU instructions rather than functions, which puts them firmly in GCC's domain. That said, `readelf --dyn-syms /lib/x86_64-linux-gnu/libgcc_s.so.1` also mentions things like `malloc` and `free` which are surely handled by libc at some level, so I could have misunderstood something.
What about reading/writing? This is what I want to do: http://play.rust-lang.org/?gist=451536b3d4bf39958692867d02b5e04a
&gt; You could, of course, construct some cases where the optimization would change the behaviour (make `a` point to a read-only page), but.. But what? :p This is a real possibility, and would generally make that optimization invalid for the C example. From LLVM's perspective, that's a possibility for `truncate` as well: pub fn truncate(&amp;mut self, len: usize) { unsafe { // drop any extra elements while len &lt; self.len { // decrement len before the drop_in_place(), so a panic on Drop // doesn't re-drop the just-failed value. self.len -= 1; let len = self.len; ptr::drop_in_place(self.get_unchecked_mut(len)); } } } If `len &gt;= self.len`, then `self.len` will never be touched, so for all LLVM knows, `self` might point to read-only memory. From Rust's perspective, it's illegal to have an `&amp;mut` reference to read-only memory, but LLVM doesn't know that. LLVM does know that the pointer is not *completely* invalid, because rustc marked it `dereferenceable`, which according to the [LLVM language reference](https://llvm.org/docs/LangRef.html) means that it "can be loaded from speculatively without a risk of trapping". However, that doesn't mean it can be stored to without a risk of trapping! Also, speaking generally, that optimization is not always a performance win; sometimes it can be a loss! If a bunch of cores are repeatedly reading from some cache line, they can each have a copy of it in their private L1 cache, in the "shared" state. But if some core tries to write somewhere within that cache line, it needs to get exclusive access to it, so it will notify the other cores to invalidate their copies – requiring them to reload the data from the (shared) L2 cache if they need to read it again. That happens [even if the value being written is the same as the previous value](https://stackoverflow.com/questions/46749422/performance-for-writing-the-same-value-again-into-cache-line). So, if there's enough contention, it can actually be faster to 'check before you write', and people sometimes do that intentionally as a manual optimization. Now, for `truncate` in particular that's not the most likely scenario; if nothing else, `&amp;mut` of course requires obtaining exclusive access, so any other cores that might want to read the length value would have to be blocked on a lock or something anyway. But LLVM doesn't know that either. Besides, it's still possible that other cores are reading data from somewhere else in the same cache line...
Of course they are, your app depends on the binary form of that lib.
Or you mean why libgcc is not statical linked to the app. Don't know. That is the choice of the maintainer/packager.
&gt; But what? :p This is a real possibility, and would generally make that optimization invalid for the C example. But C doesn't mention anything about memory protection. I'm not saying you're wrong, just that I'm a little surprised. By the way, see [this](https://godbolt.org/g/FKjhqG) for C++'s take on the equivalent code. /u/mewloz already mentioned the cache invalidation issue, I agree with that. Of course, the compiler won't decide to insert loads and comparisons just to save you extra memory stores, so I suppose it's fair that it doesn't always do the opposite either.
I think it's possible to link them statically but you do benefit from it being dynamic. For example, on platforms where SIMD is available it might be used to copy larger amounts of data at once (I think)
Nice explanation, thanks.
&gt; Nominally, yes. However, the glibc docs suggest that for features that are platform-specific and not tied to an operating system (like memset and memcpy) that it's better to implement them in GCC so all software can benefit from them, rather than only software that links with glibc. &gt; Also, on some platforms I bet "memset" and "memcpy" are specialised CPU instructions rather than functions, which puts them firmly in GCC's domain. That… makes no sense. libc (or equivalent e.g. on OSX it's called libSystem and covers libc, libinfo, libkvm, libm and libpthread) is the standard C library provided by the platform itself, surely the people who build the platform itself are most likely to know how to interface with it?
Have you considered [just not using `unsafe`](https://play.rust-lang.org/?gist=d6ff5de05da7ef8065411184719e069c)? Because then you don't have to think about this kind of stuff.
I still find people's allergy on unsafe kinda strange. But if there is safe alternative, that produces equal result, there is no problem to change it ofc.
Well, it's a matter of being able to focus your code reviews. In case of \`actix-web\` there was a lot of "unsafely unsafe" code, and there might still be some remaining. I've also seen too many \`unsafe impl Send for Foo\` in code using \`actix-web\` to feel comfortable using it. This one was trivially safe, but it's better to spend your "unsafe budget" somewhere else.
by the way, `.collect::&lt;Vec&lt;_&gt;&gt;()` is not necessary in `main`. You can pass an iterator to your `parse_args` function because there is a blanket implementation of `IntoIterator` for all iterators.
it is serious blow to Rust reputation when we write safe code that is slower than unsafe though. Regardless how you phrase it, even if difference single instruction, it still matters
Speak for yourself
Pushing to get a 0.4 release of [graphql-client](https://github.com/tomhoule/graphql-client) out of the door. It mainly needs documentation and a way to specify additional traits users want derived for their generated types. With this release the last major features will all be covered and I can move on to specific client libraries (native, wasm-bindgen, yew at first).
I still would like to know the rules regarding aliasing mutable references because there are cases when you can't avoid it. ``` let mut x = 1u32; let r = &amp;mut x; unsafe { some_ffi_fn(r); } ``` I think if my `parse_args` example invoked UB then the above basic FFI example would too.
I will be publishing a version 0.2 shortly, which will include the tests and benchmarks. Nonetheless, I've been getting very accurate, reproducible results, enough that the tests can assert the amount of memory allocated and verify that everything allocated gets deallocated.
I would love to follow this blog, but it seems that the RSS feed is broken :(. Could you fix it?
Sure, but `memset` and `memcpy` aren't functions provided by the kernel, or functions that can meaningfully differ between operating systems on the same hardware, so it's quite possible for the compiler to have a better implementation than the OS, Also, there's no compatibility risk for using a custom implementation (as there would be for `malloc` and `free`, for example). According to [the GCC docs](https://gcc.gnu.org/onlinedocs/gcc/Other-Builtins.html#Other-Builtins) there are dozens of C standard library functions that can be replaced with a custom implementation if GCC thinks it can do better in some specific situation than the general-purpose implementation in libc.
&gt; Interesting idea! After working on a Redux+React project I always wondered what a typesafe implementation would look like or what benefits it could have. Then you should definitely look at [PureScript-Halogen](https://github.com/slamdata/purescript-halogen) :) It's a great example of using the type system to provide a very elegant and convenient way to write reactive UIs, also being able to do async things like ajax and querying child components **inline** (like async/await but with monads) (unlike in Elm where it requires handling the result in another Msg variant handler).
The new signature wont be published until 0.20.0. The README in the repository hasn't been updated and still mentions 0.19.0.
With a whole bunch of free time this last weekend I made a lot of progress on [`uom`](https://github.com/iliekturtles/uom) (type-safe zero-cost dimensional analysis). CI build scripts were improved to run faster and run more tests. I also changed from kcov to tarpaulin for code coverage (thanks /u/xd009642 for effort making this tool). I'm also getting close to merging support for multiple quantities with the same dimensions, adding temperature interval, and supporting degree Celsius/Fahrenheit. These issues are some of the oldest in the `uom` repository so I'm excited to get them polished up and in to master.
Fantastic work here, I remember attempting to use Amethyst for a game jam a long time ago and being really frustrated by it because it felt like it could be exactly what I wanted but there were constant irritations and bugs. Now it looks like it's getting closer to what I wanted all along and I couldn't be happier.
Do you have a citation? My understanding is this is more of an implementation issue (that is, the compiler doesn't support it, and there's a few trade-offs that no-one has yet argued convincingly-enough for/against) rather than some purposeful design decision. (For instance, one of the trade-offs is that you don't need to splat every parent vtable into your own, possibly at the cost of performance for method calls.)
oh no
... do I have a citation on the part that I crossed out because I realised I couldn't prove it? &amp;nbsp; No. No, I do not. &amp;nbsp; &amp;nbsp; But if you're asking about the second part: if you have some non-zero number of cycles, how do you order the vtables? There's no coherent parent/child relationship, even less so when a trait could potentially be its own grandparent through multiple paths.
I've been disappointed with the state of Rust's GPGPU story (especially CUDA) for a while now, and I've finally decided to do something about it. I thought the first step would be to compile some known-good CUDA code (japaric's [nvptx](https://github.com/japaric/nvptx) example). That turned out to be quite a rabbit-hole. Rustc [segfaults](https://github.com/rust-lang/rust/issues/53099) when trying to cross-compile `libcore` for PTX. It took me a while to figure that out because Cargo returns a generic error message (exit code 101) with no details - the same generic error that it returns when the target is invalid. So I figured I'd try to build rustc myself with debug symbols so I could figure out the segfault and fix it. Haven't even been able to build rustc, and the rust-internals IRC channel has been deserted all weekend. It's all been quite frustrating. Since today is a holiday here, I'm going to take the day off and do other things and come back to it later. I expect this will be an ongoing project with a number of contributions to the Rust repo. I've read the [Rustc Guide](https://rust-lang-nursery.github.io/rustc-guide/), but it would be helpful to have someone who could help me figure out build-system problems and provide advice.
&gt; Sprite renderer pass for improved sprite support and easier manipulation. As someone who does entirely 2d stuff, should I wait for this before jumping in to Amethyst?
There is already nice stuff for 2D in Amethyst, but the new render pass will definitely make it a treat. What I'd recommend is that as soon as [this PR](https://github.com/amethyst/amethyst/pull/830) gets merged, you could use the develop branch to have all the features of 0.8 + the sprite pass. If you've got any question on how to do that, feel free to visit us on our Discord server!
This isn't accurate, its purely an implementation issue about how to layout the vtables so that casts like this could be enabled. B absolutely does "include" the functionality of A in the only way that makes sense in Rust: you can call methods from `A` on a value of type `T: B`.
The impl essentially "breaks" the cycle. Consider this example: trait A { } trait B: A { } impl&lt;T: B&gt; A for T { } There's no problem of a "cyclic vtable" here, because the calls to methods from `A` will ultimately resolve to this impl.
Well that's just out of my own experience. Trait constraints don't *behave* like inheritance in any other conventional language I've used that uses that term. You could probably construct an argument that they *do*, but I've seen that cause so many practical misunderstandings that I find trying to liken the two counter-productive.
There are significant differences between traits and inheritance, but none of them are relevant here that I can see. I think your answer is very misleading and you should be careful not to overstate things.
Playing with Rust as a shared library for Android and iOS. I have used Dropbox's djinni before for cpp, but I want to see how far I can get towards automated binding generation with rust instead of cpp. No idea where it will take me 
Nice! I will consider integrating into [`configure_me`](https://crates.io/crates/configure_me)
There are multiple libc implementations available on Linux that people want to use with GCC, so it makes sense to move some things out of glibc.
Ok, I never tested such big directories (I know I should have). I have to admit, that the frontend was created in a rush and uses slow/bad code like innerHTML. I will fix this issue before the next release :)
They are (tautologically) but bits of glibc because they are not part of glibc and not distributed with glibc. They are OS-independent compiler intrinsics that GCC can use regardless of your choice of libc. I really don't understand what's counterintuitive about that.
I was very frustrated with the fact that, for the longest time, there was basically no documentation for someone who's new to both Rust and (to a lesser extent, because I'm not expecting them to teach the basics of game development) game development. I felt like I was just flying blind, and I didn't even get a Hello, World style game going.
I can not seem to find an `IntoInterator` implementation for `config::Config`. Are you sure that's the exact code you are running?
Can this be integrated with \`clap\` or \`structopt\` at some point?
We made a full chapter of the book dedicated to the concepts of Amethyst (most notably the ECS). We would love to hear feedback from people that did not know what it was before reading it so that we can make sure it works well!
Fantastic!
No biggie :-). I've commented before on one of your posts. I'm working on a similar thing, but mine is more focused on performance. I wanted to test `thumbcloud` to see how it looks and works. The other issue I encountered was that it behaves weirdly if `static/` is missing. If you're using `askama` for the templates, you may want to consider including the static assets in the binary to make it portable.
Can you provide an example config file and the full code? 
Why don't you just `println!("{:?}", hashmap)` ?
&gt; println!("{:?}", hashmap) Because it errors with: &gt; thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: invalid type: map, expected a string', libc
From the example file: https://github.com/mehcode/config-rs/blob/master/examples/simple/src/main.rs Make sure to deserialize the config into a HashMap: let hash_map = map.deserialize::&lt;HashMap&lt;String, String&gt;&gt;().unwrap(); ... or whatever you need. I don't know how *config* handles nested maps. For hierarchical configs you probably want to check out this one: https://github.com/mehcode/config-rs/tree/master/examples/hierarchical-env
I've found Rosetta Code really useful to learn by comparison. Depending on the problem you will either see very different idioms, or the equivalence.
This will be up to whoever's giving the presentation
“There are moments when it certainly feels like the Rust compiler is gaslighting you.” That one made me lol
With async/await. Nothing particularly fancy there. 
You're trying to deserialize into a `HashMap` where the type of the value is not always the same, the error above explains that. You're saying, load my config file into `HashMap&lt;String, String&gt;` but the possible values are `String` and `HashMap`.
WOW 3 MB installed size
I honestly can't tell whether you're impressed or disgusted
Performance is not such a big focus on my project, but it wouldn't hurt it either. If your project turns out to be more stable and faster I might use it as a kind of backend. What exactly is your project anyway? A nextcloud clone? I would love to contribute to something like that. I already tought about that, and I think it can be easily done. 
This is incorrect. Rather: Dynamic dispatch is a pretty second class citizen in rust, and is intended to be something you don't need much, which means it's not so powerful. The reason traits don't work this way is that you'd need to include parent vtable pointers in the vtable (or inline the vtable) which is an extra overhead. Having an \`as\_a\` method makes this explicit and we only pay the cost when we need it.
Considering that [`grep`](https://packages.debian.org/sid/grep) uses 1 MB, I think it's not that bad. Plus I wonder how much can be removed with `strip`.
If they have ripgrep's binary down to ~3MB, then it's likely they already ran `strip`.
considering the added features and performance, I honestly don't think it's bad at all
Yeah, after I wrote the message, I've tested it, and without `strip` I've got 41 MB.
Yup! ripgrep's Cargo.toml enables debug info even in release builds.
I know this was just a joke and I hate to concentrate on such minor issues but, as an African, I find including "~~slaves~~" in your README example tasteless and a bit offensive.
&gt; What exactly is your project anyway? A nextcloud clone? Haha, no way. I didn't announce it because the code is really icky and the web interface is really ugly -- it's my first "webby" project, and I didn't want to include Bootstrap for something so small. I'm using plain `hyper`, without a router or anything. Right now: - it's reasonably fast - can stream `tar` archives; you can navigate to a 3 GB directory and download it without the server breaking a sweat. If you cancel the download, nothing bad happens. - has a bonus audio player ¯\\\_(ツ)\_/¯ - it's kinda' stuck in `futures-await` hell Planned features: - logins with per-user access to shared folders (partially implemented) - uploads - sharing links (so you can download something without having an account) - I want it to have a good story for non-UTF-8 filenames (partially implemented)
The thing to be careful of is that unsafe blocks don't just pollute the code they wrap, but also any other code in that module. If you really want to count something, count the number of lines of code in modules with unsafe in them at all. For reference, here's the relevant part of the Rustonomicon: https://doc.rust-lang.org/nomicon/working-with-unsafe.html
I am about to finish a scraping project that provides api for our school meals. I am going to use that for a bot.
&gt; One of the world's fastest ECS It uses specs right? I always thought it would use different storages under the hood, which I assume means that you don't get linear access all the time. Has anyone benched it against other ECS that have guaranteed linear access, like the new Unity ECS? Also have you looked at the assembly? I tried to write high performance Rust a [couple of times](https://github.com/MaikKlein/hpr/blob/master/src/intersect.rs) but it is very hard sometimes to get Rust to auto-vectorize. I never could get anywhere close to what unity's burst compiler can do. Essentially I would have to manually vectorize everything. Unity burst compiler can even automatically turn some AoS code into SoA, I am sure something similar could be implemented in rustc. But even with the SoA format, I couldn't get rustc to auto-vectorize everything. There was a [working-group](https://github.com/nox/wg-codegen/issues/1) for this, but I am not sure if it is still alive. 
[https://rosettacode.org/wiki/Category:Rust](https://rosettacode.org/wiki/Category:Rust)
The perfectionist in me says that it could probably be a lot less (by using dynamic linking or dead code elimination across crates) but then again 3 MB is also not that much these days. I guess it depends on what you compare it against.
I have a `&amp;dyn std::iter::Iterator` that I'd like to iterate over using a `for` loop. How would I accomplish that?
Can it generate [mdoc](http://mandoc.bsd.lv/) instead?
No worries; I blame reddit ;P
Good luck! I really enjoyed mine. What is your project?
I think the fundamental misunderstanding here is that glibc and gcc are not tied together. You can use GCC with any libc. The reasoning behind memset and memcpy being in GCC is that GCC can make specific use of information within the program to use more optimized variants, such as using splice to memcpy a file, or converting a memcpy of just a few bytes into a regular load and store, neither of which could be handled by libc as it requires a completely generic implementation. 
You can derive FromSqlRow and As Expression now. That issue links to the right file, but a commit from 0.7. See https://github.com/diesel-rs/diesel/blob/master/diesel_tests/tests/custom_types.rs
That would be better IMO.
`VecStorage`s is the most common storage in practice and in fact has its components available in O(1). Regarding the assembly, the best way to know the answer to that question is to ask the people maintaining the specs project directly.
I assume then that the order matters. Russian, according to CLDR [0] has four cardinal plural categories - one, few, many, other. I'm not sure how it applies onto Qt. It seems that Qt either doesn't follow CLDR, or does and skips one form. If order matters and the order applies to the plural categories, then you can use `intl_pluralrules` to retrieve the plural categories available for Russian, and select the correct one from the example XML you provided. But since there's the mismatch between three and four, I'm concerned that maybe Qt uses its own database of plural rules and CLDR cannot be applied. [0] http://www.unicode.org/cldr/charts/33/supplemental/language_plural_rules.html#ru
Indeed, by policy all binaries in Debian are stripped and debug-info carried separately: [https://www.debian.org/doc/debian-policy/ch-files.html#binaries](https://www.debian.org/doc/debian-policy/ch-files.html#binaries)
[removed]
Can't read the link at work, but... &gt; In C++ you can’t portably nuke the length of a vector. In Rust, you can. What's "nuke"? Vector has a well defined length and capacity, as well as well defined memory layout. You have algorithms available in std lib to affect them both in various ways (incl. directly, in case of capacity), while keeping the results consistent. What's missing? &gt; In C++ you can’t mutate set elements in place easily, in Rust, you can. In case of mutating elements such that their relative position doesn't change, I don't see the problem? So what does Rust do in case I mutate an element in-situ so that it needs to be reinserted in a different position? &gt; In Rust, unsafe makes the danger of these operations stand out from the rest of the code. In C++ the committee just thinks that these operations are too dangerous; they are right, these operations are too dangerous for C++. I am not sure that simply marking something "unsafe" makes the thing safe... You can still break all sorts of shit inside the block, can you not? Happy to be proven wrong here wrt Rust semantics. 
Wait - there's software outside of Chromium and Emacs? News to me.
It doesn't automatically make it safe, but it does make the potential unsafety clearly visible in the code: if something turns out to be wrong with the implementation of whatever trickery you were doing, you know right where to look for the cause.
&gt; what’s missing ? The ability to change the length of the vector at will. Suppose you read bytes from the network. It is a common pattern in Rust to reserve enough capacity, let the network stack write into the vector, and then manually change the length. C++ vector API doesn’t let you do that. &gt; In case of mutating elements such that their relative position doesn't change, I don't see the problem? Me neither and neither does Rust. C++ API forces you to remove the element from the set, and reinsert it. That’s often way more expensive than mutating the element in place of doing so doesn’t change the order of the element within the set.
So these could be implemented in Rust and/or statically linked to avoid the extra run-time dependency?
A final release build should be using LTO, which in my mind would accomplish the same thing as aggressive tree shaking. Is my impression wrong here?
Have you tried using `&amp;mut dyn`? `&amp;mut` references to an iterator are themselves iterators, but shared `&amp;` references aren't, because you can't call `next` through them.
It's 3/4 of my L3 cache. Not that little.
I think you mean FFI (foreign function interface) rather than EFI (extensible firmware interface) But the issue here is that rust doesn't have a standard ABI so your users would constantly need to recompile their components to match whatever version of rust you used to compile it. Using a C style FFI will provide a consistent way for your code to talk to any components, though is slightly cumbersome and can incur a slight performance degradation as you convert data back and forth.
You need a `&amp;mut std::iter::Iterator&lt;Item=...&gt;`. You can't iterate over an immutable reference.
I did mean FFI. This makes a lot of sense. Thanks so much. 
Thanks! Yeah, that was a blunder on my part.
Thanks. Now that I think about it it's obvious that an immutable reference wouldn't work.
Now _this_ is cool! Will definitely be needing this at some point. 
&gt; I'd think Rust would be uniquely suited to something like this Rust aims to be compatible with C conventions, and that is in direct conflict with such goal. There are some libraries for Rust, but not nearly as convenient as languages designed with green threads in mind. See those projects: https://crates.io/search?q=green%20threads
The current focus for concurrent programming seems to be on the "Futures"/(Promise)-concept. It tends to allow for even higher concurrency (avoid unused stack-allocations and the overhead of context-switching that fibers incurr). The cost is slightly harder to write and read the code, and debugging can be harder.
there's a lot of `coroutine` based libraries on `crates.io`. I authored one myself. The `async`/`await` syntax should be be incorporated in future versions. 